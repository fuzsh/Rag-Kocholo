{
    "id": "correct_foundationPlace_00055_2",
    "rank": 87,
    "data": {
        "url": "https://machinelearning.apple.com/",
        "read_more_link": "",
        "language": "en",
        "title": "Overview",
        "top_image": "https://mlr.cdn-apple.com/media/Home_1200x630_48225d82e9.png",
        "meta_img": "https://mlr.cdn-apple.com/media/Home_1200x630_48225d82e9.png",
        "images": [
            "https://mlr.cdn-apple.com/media/Discover_1440x420_2x_9c465d585e.jpg"
        ],
        "movies": [],
        "keywords": [],
        "meta_keywords": [
            ""
        ],
        "tags": null,
        "authors": [],
        "publish_date": null,
        "summary": "",
        "meta_description": "Apple machine learning teams are engaged in state of the art research in machine learning and artificial intelligence. Learn about the latest advancements.",
        "meta_lang": "en",
        "meta_favicon": "/favicon.ico",
        "meta_site_name": "Apple Machine Learning Research",
        "canonical_link": "https://machinelearning.apple.com",
        "text": "At the 2024 Worldwide Developers Conference, we introduced Apple Intelligence, a personal intelligence system integrated deeply into iOSÂ 18, iPadOSÂ 18, and macOSÂ Sequoia.\n\nApple Intelligence is comprised of multiple highly-capable generative models that are specialized for our usersâ everyday tasks, and can adapt on the fly for their current activity. The foundation models built into Apple Intelligence have been fine-tuned for user experiences such as writing and refining text, prioritizing and summarizing notifications, creating playful images for conversations with family and friends, and taking in-app actions to simplify interactions across apps.\n\nIn the fast-evolving world of natural language processing (NLP), there is a strong demand for generating coherent and controlled text, as referenced in the work Toward Controlled Generation of Text. Traditional autoregressive models such as GPT, which have long been the industry standard, possess inherent limitations that sometimes manifest as repetitive and low-quality outputs, as seen in the work The Curious Case of Neural Text Degeneration. This is primarily due to a phenomenon known as \"exposure bias,\" as seen in the work Scheduled Sampling for Sequence Prediction with Recurrent Neural Networks. This imperfection arises due to a mismatch between how these models are trained and their actual use during inference, often leading to error accumulation during text generation.\n\nApple is sponsoring the International Conference on Machine Learning (ICML) 2024, which is taking place in person from July 21 to 27 in the Messe Wien Exhibition and Congress Center, Vienna Austria. ICML is globally renowned for presenting and publishing cutting-edge research on all aspects of machine learning used in closely related areas like artificial intelligence, statistics and data science, as well as important application areas such as machine vision, computational biology, speech recognition, and robotics. Below is the schedule of our sponsored workshops and events at ICML 2024.\n\nApple is sponsoring the International ACM Conference on Research and Development in Information Retrieval (SIGIR), which is taking place from July 14 to 18 in Washington D.C. SIGIR is an international forum focused on presenting new research in the information retrieval field. Below are accepted Apple papers at SIGIR 2024."
    }
}
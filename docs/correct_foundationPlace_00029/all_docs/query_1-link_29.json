{
    "id": "correct_foundationPlace_00029_1",
    "rank": 29,
    "data": {
        "url": "https://www.linkedin.com/pulse/building-data-lake-foundation-siebel-aws-james-chan",
        "read_more_link": "",
        "language": "en",
        "title": "Building a Data Lake Foundation for Siebel on AWS",
        "top_image": "https://media.licdn.com/dms/image/C5612AQG5NyfyV8StUw/article-cover_image-shrink_600_2000/0/1564749795841?e=2147483647&v=beta&t=NLzwWuoxjhQzo3hEBinx5ggJ6CMIK4ytE44uxUp4NK8",
        "meta_img": "https://media.licdn.com/dms/image/C5612AQG5NyfyV8StUw/article-cover_image-shrink_600_2000/0/1564749795841?e=2147483647&v=beta&t=NLzwWuoxjhQzo3hEBinx5ggJ6CMIK4ytE44uxUp4NK8",
        "images": [
            "https://media.licdn.com/dms/image/C5612AQG5NyfyV8StUw/article-cover_image-shrink_600_2000/0/1564749795841?e=2147483647&v=beta&t=NLzwWuoxjhQzo3hEBinx5ggJ6CMIK4ytE44uxUp4NK8",
            "https://media.licdn.com/dms/image/C5612AQFymnZmnbxNZQ/article-inline_image-shrink_1000_1488/0/1564746692266?e=1726704000&v=beta&t=p4VT76dBe8H7p59Pbx4ha-6GDI3aC3yuKY4Vjdj00uQ",
            "https://media.licdn.com/dms/image/C5612AQHowYyQoQxr5g/article-inline_image-shrink_1000_1488/0/1564746772955?e=1726704000&v=beta&t=evSsSKaaOVznjJC_UlxYzDPfI3vJuf86R4-1SfhssyI",
            "https://media.licdn.com/dms/image/C5612AQFvNk6AiOjSaw/article-inline_image-shrink_1000_1488/0/1564747009887?e=1726704000&v=beta&t=NpyvdLcQ9mqiHK8Ahzqfre49xAEv3GhI788rZNTbXdc",
            "https://media.licdn.com/dms/image/C5612AQHA5bfvoNcbYg/article-inline_image-shrink_1000_1488/0/1564747193701?e=1726704000&v=beta&t=NTXxp2Ub67Gct8f-dht3ywJDhr7MwpOvRPAFSDFnU6U"
        ],
        "movies": [],
        "keywords": [],
        "meta_keywords": [
            ""
        ],
        "tags": null,
        "authors": [
            "James Chan"
        ],
        "publish_date": "2019-08-02T12:51:38+00:00",
        "summary": "",
        "meta_description": "Nowadays, companies needed a new tech stack giving them a whole bunch of benefits including taking the data from their premise and pulling it into the cloud, helping them to deliver better customer experiences. In running customer data and analytics functions in the cloud rather than on-premises, te",
        "meta_lang": "en",
        "meta_favicon": "https://static.licdn.com/aero-v1/sc/h/al2o9zrvru7aqj8e1x2rzsrca",
        "meta_site_name": "",
        "canonical_link": "https://www.linkedin.com/pulse/building-data-lake-foundation-siebel-aws-james-chan",
        "text": "Nowadays, companies needed a new tech stack giving them a whole bunch of benefits including taking the data from their premise and pulling it into the cloud, helping them to deliver better customer experiences. In running customer data and analytics functions in the cloud rather than on-premises, teams can spend their time focused on customer outcomes instead of the “heavy lifting” of building and maintaining data analytics pipelines and systems. Siebel is a popular and widely used customer relationship management (CRM) platform. It lets you store and manage prospect and customer information—like contact info, accounts, leads, and sales opportunities—in one central location.\n\nThis post discusses how to extract data from the on-premise Siebel database and save it on a centralized AWS data lake repository with various formats. Although Amazon S3 provides the foundation of a data lake, other data stores are added to tailor the data lake for business needs. It also addresses the integration with other AWS services for data processing and analytics.\n\nDesign Concerns\n\nHere are some of the design points that we considered,\n\nData Lake should be able to store all structured, semi-structures and un-structured data at any scale.\n\nThe platform can run different types of analytics, from visualisations to big data processing, real-time analytics, and machine learning to guide better decision.\n\nThe data lake solution should impose no/low impact on production workloads.\n\nThe architecture should de-couples of storage from compute and data processing, and able to integrate with cluster-less and server-less AWS services.\n\nThe data processing component follows an architecture pattern called Lambda Architecture. It is designed to handle massive quantities of data by taking advantage of both batch and stream-processing methods. Please refer the Wikipedia for more information.\n\nData Architecture Overview\n\nThe architecture in Figure 2 describes conceptually how we build our data transform pipeline. At the far left, we have incoming data source from on-premise Siebel database. Data is extracted using AWS Data Migration Service (DMS) with Amazon Kinesis Data Stream as a target. The persistent layer stores incoming data on S3 for batch processing. While the streaming layer transforms the CDC records for near real-time references and reporting. The right side illustrates how the data lake can be read from various Amazon services for business needs.\n\nData Ingestion\n\nWe use AWS DMS in the data integration pipeline to replicate data in near real time directly into Kinesis Data streams. With this approach, we can build a decoupled and eventually consistent view of database on various data stores such as S3 and DynamoDB.\n\nAWS Database Migration Service serialises each database record to Kinesis data stream with JSON format. With object mapping, users can determine how to structure the data records in the stream and define the partition key for grouping the data into its shards.\n\nWith AWS DMS version 2.3.0 and later, you can create an Oracle CDC task that uses an Active Data Guard standby instance as a source for replicating ongoing changes. Doing this eliminates overloading the primary server. For migrations with a high volume of changes, CDC performance is usually much better when using Binary Reader than when using Oracle LogMiner. To use Oracle Active Standby as a source with Binary Reader for CDC in AWS DMS, apply the configurations as follow,\n\nCreate the following directories on the primary server for the standby instance\n\nCREATE OR REPLACE DIRECTORY \"DMS_MIGRATION_REDO\" AS '<<REDO_LOG_LOCATION>>'; CREATE OR REPLACE DIRECTORY \"DMS_MIGRATION_ARCHIVE\" AS '<<ARCHIVE_LOG_LOCATION>>';\n\nCreate a source endpoint for Oracle Active Standby with extra connection attributes\n\nuseLogminerReader=N;useBfile=Y;archivedLogDestId=1;additionalArchivedLogDestId=2\n\nAs AWS DMS supports multiple Target Endpoints from a single Source Endpoints, a single replication instance can host one or more replication tasks to achieve such implementation. Figure 3 shows an alternative data integration pipeline,\n\nRecently AWS DMS announced support for migrating data to Amazon S3 from any AWS-supported source in Apache Parquet data format, which is commonly used for data analytics.\n\nAWS DMS uses table mapping to specify the data source schema, data, and any transformations that should occur during the task. You can use source filters to limit the number and type of records transferred from your source to your target. As Lambda architecture mainly depends on a data model with immutable data source, you can use filters to exclude records in transient states, such as work-in-progress transactions.\n\nData Processing\n\nLambda architecture takes two processing paths, a persistent (batch) layer and a streaming (speed) layer. Persistent layer manages the master data and precomputes the batch views. Normally it might take a large amount of time for files to be processed in batch mode. Streaming layer processes the data in near real time as they arrived. In the point of performance, the latency of batch processing will be in minutes to hours while the latency of stream processing will be in seconds or milliseconds.\n\nPersistent Layer\n\nWe have our data sources which can be structured and unstructured. They all integrate into a raw data store that consumes data in the purest possible form i.e. use least transformations. This is known as Raw Data Lake and kept as single source of truth. It provides data scientists an avenue to explore data and create hypothesis, or as a repository for data analysts to analyse data and find patterns.\n\nThe pipeline persists the streaming data to Amazon S3 using Amazon Kinesis Firehose. Once the raw data stored on S3, users can define the lifecycle policies which determine when objects transition to another storage class. For example, you might choose to transition objects to the Standard-Infrequent Access storage class 30 days after created and archive to the Amazon Glacier a year after.\n\nWe have batch processing engine that processes the raw data into something that can be consumed by the users i.e. a structure that can be used to reporting to the end-used. We called the second tier of transformed data as Analytic Data Lake. It provides a data store for business users to explore data and create analytic reports.\n\nColumnar data formats are key for BI and data analytics workloads. They help to optimise the analytics query performance, because they drastically reduce the overall disk I/O requirements, and also reduce the amount of data to load from disk with data compression. One of most commonly used formats is Apache Parquet.\n\nAWS Glue is used to transform the raw data into Apache Parquet format and save it into Analytic Data Lake. First, we define a database and configure a crawler to explore data in Raw Data Lake. Then create an ETL job which perform the parquet conversion and save the output to destination. In addition, AWS Glue’s DynamicFrames support native partitioning using a sequence of keys. Systems like Amazon Athena, Amazon Redshift Spectrum, and AWS Glue itself can use these partitions to filter data by value without making unnecessary calls to Amazon S3. This can significantly improve the performance of applications that need to read only a few partitions.\n\nIt is important to schedule a Crawler job to keep AWS Glue Data Catalog in sync with Amazon S3 for both Raw and Analytic Data Lakes.\n\nStreaming Layer\n\nReal time processing deals with streams of data that are captured in real-time and processed with minimal latency to generate near real-time cache store or reporting. Two main tasks are done in the speed layer: first, the customer related data, such as contact info, accounts, leads, is updated in Amazon DynamoDB for cache reference; Second, is performed the analyse and report of workflow statistics such as aggregate transaction response times.\n\nCache Streaming\n\nFor customer related information, we need to build a cache layer which provide single-digit milli-seconds latency for accesses from API requests. Amazon DynamoDB is a fully managed NoSQL database service that provides fast and predictable performance with seamless scalability. It has dynamic schema for semi-structured/unstructured data, which is stored in many ways: it can be column-oriented, document-oriented, graph-based or organised as a key-value store.\n\nFigure 4 illustrates the mapping of relational data model with NoSQL data store. The sample has 3 relational entities (<companies>, <orders> and <order_items>) with the parent-child relationships defined in relational database. On the DynamoDB side, one single table is created to capture all the business information among these relational tables. The hash key of the NoSQL table is company's unique identifier. Please read my previous post, Near Zero Downtime Migration From Oracle Database to DynamoDB, for more details about the data modeling and mapping design.\n\nAWS Lambda allows for stateless invocation of code in response to events. A Lambda function would be triggered by a batch of events on Kinesis Data stream, transforms the data based on the mapping rules and store it into DynamoDB.\n\nAnalytic Streaming\n\nThe incoming data from the Kinesis Data stream is fed into Kinesis Analytics that provides an easy way to process the data in real time. Analytics allows writing standard SQL queries to extract specific components from the incoming records and perform real-time ETL on it. You can write a time-based windowed query with analytic functions such as computing average or count, e.g.\n\nCREATE OR REPLACE STREAM \"srm_request_agg_stream\" ( status VARCHAR(10), avg_tran_time INTEGER, tran_count INTEGER); CREATE OR REPLACE PUMP srm_request_pump AS INSERT INTO srm_request_agg_stream SELECT STREAM status ,AVG(TSDIFF(actl_start_dt - actl_end_dt)) ,COUNT(*) FROM source_srm_request_stream WHERE status in ('SUCCESS', 'ERROR') WINDOWED BY STAGGER ( PARTITION BY status RANGE INTERVAL '5' MINUTE);\n\nThe processed data from the query is fed into Firehose delivery streams, which batch the data into CSV files and store it in Analytic Data Lake. This layer's views may not be as accurate or complete as the ones eventually produced by the persistent layer, but they are available almost immediately after data is received. They can be purged with schedule job when the batch layer's views for the same data become available.\n\nData Consumption\n\nThe data integration pipeline stores the Siebel information on various AWS repositories,\n\nSiebel raw data on Amazon S3 (Raw Data Lake)\n\nBatch processed analytic data on Amazon S3 (Analytic Data Lake)\n\nNear real-time processed analytic data on Amazon S3 (Analytic Data Lake)\n\nBusiness cache data on Amazon DynamoDB (Cache Data Store)\n\nWith Lambda architecture, the Streaming Layer is responsible for handling low latency updates, which occur due to high latency batch view creation in the Persistent Layer. Queries on Analytic Data Lake with combination of both batch and real-time views provides a complete picture of analytic data including the latest updates.\n\nThe Siebel Data Lake can be the reference data stores for some of AWS services, such as\n\nAmazon Lambda\n\nAWS Lambda is a compute service that lets you run codes without provisioning or managing servers. Lambda functions perform reading an item (GetItem operation) or multiple items (BatchGetItem operation) from DynamoDB. Users can access these Lambda functions through an API provided by the AWS API Gateway service.\n\nAmazon Athena\n\nAmazon Athena is a managed interactive query service that makes it easy to analyse data in Amazon S3 using standard SQL. Athena uses the AWS Glue Data Catalog as a central location to store and retrieve table metadata. The Athena execution engine requires table metadata that instructs it where to read data, how to read it, and other information necessary to process the data. Within the Analytic Data Lake, the processed data is stored in Apache Parquet and CSV formats for batch and speed layers respectively.\n\nAmazon QuickSight\n\nAmazon QuickSight is best known for stunning visualisations, interactive dashboards, and accurate machine learning insights. Data sets are built from processed CSV files on Amazon S3 (Streaming Layer) or Athena data source (Persistent Layer). It’s super-fast, parallel, in-memory, calculation engine (SPICE) parses the ingested data and allows users to create a variety of visualisations with different graph types.\n\nAmazon SageMaker\n\nAmazon SageMaker is a service that enables a developer to build and train machine learning models for predictive or analytical applications in the AWS public cloud. Machine learning offers a variety of benefits for enterprises, such as advanced analytics for customer data. Amazon SageMaker provides built-in and common machine learning algorithms, along with other tools, to simplify and accelerate the process.\n\nConclusion\n\nObtaining a 360-degree customer view means having a holistic customer profile record that captures different types of data from across channels and systems, aggregates that data to understand what’s important to customers, and applies those insights to deliver personalised, engaging customer experiences. The road to obtaining a customer 360 view usually involves the development of a data lake that unifies data from all customer touch-points – streaming, structured data files, unstructured documents, emails, webchats, web logs, transaction logs, and social posts. Oracle Siebel is branded as one of the most comprehensive customer relationship management (CRM) solutions and provides an extensive set of customer information in structured format.\n\nBuilding a data lake on Amazon S3 provides an organisation with countless benefits. It allows you to access diverse data sources, determine unique relationships, build AI/ML models to provide customised customer experiences, and accelerate the curation of new data sets for consumption."
    }
}
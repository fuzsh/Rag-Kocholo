{
    "id": "correct_foundationPlace_00029_1",
    "rank": 33,
    "data": {
        "url": "https://www.cisco.com/c/en/us/td/docs/unified_computing/ucs/UCS_CVDs/ucs_oracle_siebel.html",
        "read_more_link": "",
        "language": "en",
        "title": "Oracle Siebel CRM on Cisco Unified Computing System with EMC VNX Storage",
        "top_image": "",
        "meta_img": "",
        "images": [
            "https://www.cisco.com/c/dam/en/us/td/docs/unified_computing/ucs/UCS_CVDs/ucs_oracle_siebel.fm/_jcr_content/renditions/ucs_oracle_siebel-01.jpg",
            "https://www.cisco.com/c/dam/en/us/td/i/templates/blank.gif",
            "https://www.cisco.com/c/dam/en/us/td/i/templates/blank.gif",
            "https://www.cisco.com/c/dam/en/us/td/i/templates/blank.gif",
            "https://www.cisco.com/c/dam/en/us/td/i/templates/blank.gif",
            "https://www.cisco.com/c/dam/en/us/td/i/templates/blank.gif",
            "https://www.cisco.com/c/dam/en/us/td/i/templates/blank.gif",
            "https://www.cisco.com/c/dam/en/us/td/i/templates/blank.gif",
            "https://www.cisco.com/c/dam/en/us/td/i/templates/blank.gif",
            "https://www.cisco.com/c/dam/en/us/td/i/templates/blank.gif",
            "https://www.cisco.com/c/dam/en/us/td/i/templates/blank.gif",
            "https://www.cisco.com/c/dam/en/us/td/i/templates/blank.gif",
            "https://www.cisco.com/c/dam/en/us/td/i/templates/blank.gif",
            "https://www.cisco.com/c/dam/en/us/td/i/templates/blank.gif",
            "https://www.cisco.com/c/dam/en/us/td/i/templates/blank.gif",
            "https://www.cisco.com/c/dam/en/us/td/i/templates/blank.gif",
            "https://www.cisco.com/c/dam/en/us/td/i/templates/blank.gif",
            "https://www.cisco.com/c/dam/en/us/td/i/templates/blank.gif",
            "https://www.cisco.com/c/dam/en/us/td/i/templates/blank.gif",
            "https://www.cisco.com/c/dam/en/us/td/i/templates/blank.gif",
            "https://www.cisco.com/c/dam/en/us/td/i/templates/blank.gif",
            "https://www.cisco.com/c/dam/en/us/td/i/templates/blank.gif",
            "https://www.cisco.com/c/dam/en/us/td/i/templates/blank.gif",
            "https://www.cisco.com/c/dam/en/us/td/i/templates/blank.gif",
            "https://www.cisco.com/c/dam/en/us/td/docs/unified_computing/ucs/UCS_CVDs/ucs_oracle_siebel.fm/_jcr_content/renditions/ucs_oracle_siebel-02.jpg",
            "https://www.cisco.com/c/dam/en/us/td/i/templates/blank.gif",
            "https://www.cisco.com/c/dam/en/us/td/i/templates/blank.gif",
            "https://www.cisco.com/c/dam/en/us/td/i/templates/blank.gif",
            "https://www.cisco.com/c/dam/en/us/td/i/templates/blank.gif",
            "https://www.cisco.com/c/dam/en/us/td/i/templates/blank.gif",
            "https://www.cisco.com/c/dam/en/us/td/i/templates/blank.gif",
            "https://www.cisco.com/c/dam/en/us/td/i/templates/blank.gif",
            "https://www.cisco.com/c/dam/en/us/td/i/templates/blank.gif",
            "https://www.cisco.com/c/dam/en/us/td/i/templates/blank.gif",
            "https://www.cisco.com/c/dam/en/us/td/i/templates/blank.gif",
            "https://www.cisco.com/c/dam/en/us/td/i/templates/blank.gif",
            "https://www.cisco.com/c/dam/en/us/td/i/templates/blank.gif",
            "https://www.cisco.com/c/dam/en/us/td/i/templates/blank.gif",
            "https://www.cisco.com/c/dam/en/us/td/i/templates/blank.gif",
            "https://www.cisco.com/c/dam/en/us/td/i/templates/blank.gif",
            "https://www.cisco.com/c/dam/en/us/td/docs/unified_computing/ucs/UCS_CVDs/ucs_oracle_siebel.fm/_jcr_content/renditions/ucs_oracle_siebel-03.jpg",
            "https://www.cisco.com/c/dam/en/us/td/docs/unified_computing/ucs/UCS_CVDs/ucs_oracle_siebel.fm/_jcr_content/renditions/ucs_oracle_siebel-04.jpg",
            "https://www.cisco.com/c/dam/en/us/td/i/templates/blank.gif",
            "https://www.cisco.com/c/dam/en/us/td/i/templates/blank.gif",
            "https://www.cisco.com/c/dam/en/us/td/i/templates/blank.gif",
            "https://www.cisco.com/c/dam/en/us/td/i/templates/blank.gif",
            "https://www.cisco.com/c/dam/en/us/td/i/templates/blank.gif",
            "https://www.cisco.com/c/dam/en/us/td/i/templates/blank.gif",
            "https://www.cisco.com/c/dam/en/us/td/docs/unified_computing/ucs/UCS_CVDs/ucs_oracle_siebel.fm/_jcr_content/renditions/ucs_oracle_siebel-05.jpg",
            "https://www.cisco.com/c/dam/en/us/td/i/templates/blank.gif",
            "https://www.cisco.com/c/dam/en/us/td/i/templates/blank.gif",
            "https://www.cisco.com/c/dam/en/us/td/i/templates/blank.gif",
            "https://www.cisco.com/c/dam/en/us/td/i/templates/blank.gif",
            "https://www.cisco.com/c/dam/en/us/td/i/templates/blank.gif",
            "https://www.cisco.com/c/dam/en/us/td/docs/unified_computing/ucs/UCS_CVDs/ucs_oracle_siebel.fm/_jcr_content/renditions/ucs_oracle_siebel-06.jpg",
            "https://www.cisco.com/c/dam/en/us/td/docs/unified_computing/ucs/UCS_CVDs/ucs_oracle_siebel.fm/_jcr_content/renditions/ucs_oracle_siebel-07.jpg",
            "https://www.cisco.com/c/dam/en/us/td/docs/unified_computing/ucs/UCS_CVDs/ucs_oracle_siebel.fm/_jcr_content/renditions/ucs_oracle_siebel-08.jpg",
            "https://www.cisco.com/c/dam/en/us/td/docs/unified_computing/ucs/UCS_CVDs/ucs_oracle_siebel.fm/_jcr_content/renditions/ucs_oracle_siebel-09.jpg",
            "https://www.cisco.com/c/dam/en/us/td/docs/unified_computing/ucs/UCS_CVDs/ucs_oracle_siebel.fm/_jcr_content/renditions/ucs_oracle_siebel-10.jpg",
            "https://www.cisco.com/c/dam/en/us/td/docs/unified_computing/ucs/UCS_CVDs/ucs_oracle_siebel.fm/_jcr_content/renditions/ucs_oracle_siebel-11.jpg",
            "https://www.cisco.com/c/dam/en/us/td/docs/unified_computing/ucs/UCS_CVDs/ucs_oracle_siebel.fm/_jcr_content/renditions/ucs_oracle_siebel-12.jpg",
            "https://www.cisco.com/c/dam/en/us/td/i/templates/blank.gif",
            "https://www.cisco.com/c/dam/en/us/td/i/templates/blank.gif",
            "https://www.cisco.com/c/dam/en/us/td/i/templates/blank.gif",
            "https://www.cisco.com/c/dam/en/us/td/i/templates/blank.gif",
            "https://www.cisco.com/c/dam/en/us/td/docs/unified_computing/ucs/UCS_CVDs/ucs_oracle_siebel.fm/_jcr_content/renditions/ucs_oracle_siebel-13.jpg",
            "https://www.cisco.com/c/dam/en/us/td/docs/unified_computing/ucs/UCS_CVDs/ucs_oracle_siebel.fm/_jcr_content/renditions/ucs_oracle_siebel-14.jpg",
            "https://www.cisco.com/c/dam/en/us/td/docs/unified_computing/ucs/UCS_CVDs/ucs_oracle_siebel.fm/_jcr_content/renditions/ucs_oracle_siebel-15.jpg",
            "https://www.cisco.com/c/dam/en/us/td/docs/unified_computing/ucs/UCS_CVDs/ucs_oracle_siebel.fm/_jcr_content/renditions/ucs_oracle_siebel-16.jpg",
            "https://www.cisco.com/c/dam/en/us/td/i/templates/blank.gif",
            "https://www.cisco.com/c/dam/en/us/td/i/templates/blank.gif",
            "https://www.cisco.com/c/dam/en/us/td/i/templates/blank.gif",
            "https://www.cisco.com/c/dam/en/us/td/i/templates/blank.gif",
            "https://www.cisco.com/c/dam/en/us/td/i/templates/blank.gif",
            "https://www.cisco.com/c/dam/en/us/td/i/templates/blank.gif",
            "https://www.cisco.com/c/dam/en/us/td/i/templates/blank.gif",
            "https://www.cisco.com/c/dam/en/us/td/i/templates/blank.gif",
            "https://www.cisco.com/c/dam/en/us/td/i/templates/blank.gif",
            "https://www.cisco.com/c/dam/en/us/td/i/templates/blank.gif",
            "https://www.cisco.com/c/dam/en/us/td/i/templates/blank.gif",
            "https://www.cisco.com/c/dam/en/us/td/i/templates/blank.gif",
            "https://www.cisco.com/c/dam/en/us/td/i/templates/blank.gif",
            "https://www.cisco.com/c/dam/en/us/td/i/templates/blank.gif",
            "https://www.cisco.com/c/dam/en/us/td/i/templates/blank.gif",
            "https://www.cisco.com/c/dam/en/us/td/i/templates/blank.gif",
            "https://www.cisco.com/c/dam/en/us/td/i/templates/blank.gif",
            "https://www.cisco.com/c/dam/en/us/td/i/templates/blank.gif",
            "https://www.cisco.com/c/dam/en/us/td/i/templates/blank.gif",
            "https://www.cisco.com/c/dam/en/us/td/i/templates/blank.gif",
            "https://www.cisco.com/c/dam/en/us/td/i/templates/blank.gif",
            "https://www.cisco.com/c/dam/en/us/td/i/templates/blank.gif",
            "https://www.cisco.com/c/dam/en/us/td/i/templates/blank.gif",
            "https://www.cisco.com/c/dam/en/us/td/i/templates/blank.gif",
            "https://www.cisco.com/c/dam/en/us/td/i/templates/blank.gif",
            "https://www.cisco.com/c/dam/en/us/td/i/templates/blank.gif",
            "https://www.cisco.com/c/dam/en/us/td/i/templates/blank.gif",
            "https://www.cisco.com/c/dam/en/us/td/i/templates/blank.gif",
            "https://www.cisco.com/c/dam/en/us/td/i/templates/blank.gif",
            "https://www.cisco.com/c/dam/en/us/td/i/templates/blank.gif",
            "https://www.cisco.com/c/dam/en/us/td/i/templates/blank.gif",
            "https://www.cisco.com/c/dam/en/us/td/i/templates/blank.gif",
            "https://www.cisco.com/c/dam/en/us/td/i/templates/blank.gif",
            "https://www.cisco.com/c/dam/en/us/td/i/templates/blank.gif",
            "https://www.cisco.com/c/dam/en/us/td/i/templates/blank.gif",
            "https://www.cisco.com/c/dam/en/us/td/i/templates/blank.gif",
            "https://www.cisco.com/c/dam/en/us/td/i/templates/blank.gif",
            "https://www.cisco.com/c/dam/en/us/td/i/templates/blank.gif",
            "https://www.cisco.com/c/dam/en/us/td/i/templates/blank.gif",
            "https://www.cisco.com/c/dam/en/us/td/i/templates/blank.gif",
            "https://www.cisco.com/c/dam/en/us/td/i/templates/blank.gif",
            "https://www.cisco.com/c/dam/en/us/td/i/templates/blank.gif",
            "https://www.cisco.com/c/dam/en/us/td/i/templates/blank.gif",
            "https://www.cisco.com/c/dam/en/us/td/i/templates/blank.gif",
            "https://www.cisco.com/c/dam/en/us/td/i/templates/blank.gif",
            "https://www.cisco.com/c/dam/en/us/td/docs/unified_computing/ucs/UCS_CVDs/ucs_oracle_siebel.fm/_jcr_content/renditions/ucs_oracle_siebel-17.jpg",
            "https://www.cisco.com/c/dam/en/us/td/i/templates/blank.gif",
            "https://www.cisco.com/c/dam/en/us/td/i/templates/blank.gif",
            "https://www.cisco.com/c/dam/en/us/td/i/templates/note.gif",
            "https://www.cisco.com/c/dam/en/us/td/i/templates/blank.gif",
            "https://www.cisco.com/c/dam/en/us/td/i/templates/blank.gif",
            "https://www.cisco.com/c/dam/en/us/td/i/templates/blank.gif",
            "https://www.cisco.com/c/dam/en/us/td/i/templates/blank.gif",
            "https://www.cisco.com/c/dam/en/us/td/i/templates/blank.gif",
            "https://www.cisco.com/c/dam/en/us/td/i/templates/blank.gif",
            "https://www.cisco.com/c/dam/en/us/td/i/templates/blank.gif",
            "https://www.cisco.com/c/dam/en/us/td/i/templates/note.gif",
            "https://www.cisco.com/c/dam/en/us/td/i/templates/blank.gif",
            "https://www.cisco.com/c/dam/en/us/td/i/templates/blank.gif",
            "https://www.cisco.com/c/dam/en/us/td/i/templates/blank.gif",
            "https://www.cisco.com/c/dam/en/us/td/i/templates/blank.gif",
            "https://www.cisco.com/c/dam/en/us/td/i/templates/blank.gif",
            "https://www.cisco.com/c/dam/en/us/td/i/templates/blank.gif",
            "https://www.cisco.com/c/dam/en/us/td/i/templates/blank.gif",
            "https://www.cisco.com/c/dam/en/us/td/i/templates/blank.gif",
            "https://www.cisco.com/c/dam/en/us/td/i/templates/blank.gif",
            "https://www.cisco.com/c/dam/en/us/td/i/templates/blank.gif",
            "https://www.cisco.com/c/dam/en/us/td/i/templates/blank.gif",
            "https://www.cisco.com/c/dam/en/us/td/i/templates/note.gif",
            "https://www.cisco.com/c/dam/en/us/td/i/templates/blank.gif",
            "https://www.cisco.com/c/dam/en/us/td/i/templates/blank.gif",
            "https://www.cisco.com/c/dam/en/us/td/i/templates/blank.gif",
            "https://www.cisco.com/c/dam/en/us/td/i/templates/blank.gif",
            "https://www.cisco.com/c/dam/en/us/td/i/templates/blank.gif",
            "https://www.cisco.com/c/dam/en/us/td/docs/unified_computing/ucs/UCS_CVDs/ucs_oracle_siebel.fm/_jcr_content/renditions/ucs_oracle_siebel-18.jpg",
            "https://www.cisco.com/c/dam/en/us/td/i/templates/blank.gif",
            "https://www.cisco.com/c/dam/en/us/td/i/templates/blank.gif",
            "https://www.cisco.com/c/dam/en/us/td/i/templates/note.gif",
            "https://www.cisco.com/c/dam/en/us/td/i/templates/blank.gif",
            "https://www.cisco.com/c/dam/en/us/td/i/templates/blank.gif",
            "https://www.cisco.com/c/dam/en/us/td/i/templates/blank.gif",
            "https://www.cisco.com/c/dam/en/us/td/i/templates/blank.gif",
            "https://www.cisco.com/c/dam/en/us/td/i/templates/blank.gif",
            "https://www.cisco.com/c/dam/en/us/td/i/templates/blank.gif",
            "https://www.cisco.com/c/dam/en/us/td/i/templates/blank.gif",
            "https://www.cisco.com/c/dam/en/us/td/i/templates/note.gif",
            "https://www.cisco.com/c/dam/en/us/td/i/templates/blank.gif",
            "https://www.cisco.com/c/dam/en/us/td/i/templates/blank.gif",
            "https://www.cisco.com/c/dam/en/us/td/i/templates/blank.gif",
            "https://www.cisco.com/c/dam/en/us/td/i/templates/blank.gif",
            "https://www.cisco.com/c/dam/en/us/td/i/templates/blank.gif",
            "https://www.cisco.com/c/dam/en/us/td/i/templates/blank.gif",
            "https://www.cisco.com/c/dam/en/us/td/i/templates/blank.gif",
            "https://www.cisco.com/c/dam/en/us/td/i/templates/blank.gif",
            "https://www.cisco.com/c/dam/en/us/td/i/templates/blank.gif",
            "https://www.cisco.com/c/dam/en/us/td/i/templates/blank.gif",
            "https://www.cisco.com/c/dam/en/us/td/i/templates/blank.gif",
            "https://www.cisco.com/c/dam/en/us/td/i/templates/note.gif",
            "https://www.cisco.com/c/dam/en/us/td/i/templates/blank.gif",
            "https://www.cisco.com/c/dam/en/us/td/i/templates/blank.gif",
            "https://www.cisco.com/c/dam/en/us/td/i/templates/blank.gif",
            "https://www.cisco.com/c/dam/en/us/td/i/templates/blank.gif",
            "https://www.cisco.com/c/dam/en/us/td/i/templates/blank.gif",
            "https://www.cisco.com/c/dam/en/us/td/docs/unified_computing/ucs/UCS_CVDs/ucs_oracle_siebel.fm/_jcr_content/renditions/ucs_oracle_siebel-19.jpg",
            "https://www.cisco.com/c/dam/en/us/td/i/templates/blank.gif",
            "https://www.cisco.com/c/dam/en/us/td/i/templates/blank.gif",
            "https://www.cisco.com/c/dam/en/us/td/i/templates/blank.gif",
            "https://www.cisco.com/c/dam/en/us/td/i/templates/blank.gif",
            "https://www.cisco.com/c/dam/en/us/td/i/templates/blank.gif",
            "https://www.cisco.com/c/dam/en/us/td/i/templates/blank.gif",
            "https://www.cisco.com/c/dam/en/us/td/i/templates/blank.gif",
            "https://www.cisco.com/c/dam/en/us/td/i/templates/blank.gif",
            "https://www.cisco.com/c/dam/en/us/td/i/templates/blank.gif",
            "https://www.cisco.com/c/dam/en/us/td/i/templates/blank.gif",
            "https://www.cisco.com/c/dam/en/us/td/i/templates/blank.gif",
            "https://www.cisco.com/c/dam/en/us/td/docs/unified_computing/ucs/UCS_CVDs/ucs_oracle_siebel.fm/_jcr_content/renditions/ucs_oracle_siebel-20.jpg",
            "https://www.cisco.com/c/dam/en/us/td/i/templates/blank.gif",
            "https://www.cisco.com/c/dam/en/us/td/i/templates/blank.gif",
            "https://www.cisco.com/c/dam/en/us/td/i/templates/blank.gif",
            "https://www.cisco.com/c/dam/en/us/td/i/templates/blank.gif",
            "https://www.cisco.com/c/dam/en/us/td/i/templates/blank.gif",
            "https://www.cisco.com/c/dam/en/us/td/i/templates/blank.gif",
            "https://www.cisco.com/c/dam/en/us/td/i/templates/blank.gif",
            "https://www.cisco.com/c/dam/en/us/td/i/templates/blank.gif",
            "https://www.cisco.com/c/dam/en/us/td/i/templates/blank.gif",
            "https://www.cisco.com/c/dam/en/us/td/i/templates/blank.gif",
            "https://www.cisco.com/c/dam/en/us/td/i/templates/blank.gif",
            "https://www.cisco.com/c/dam/en/us/td/i/templates/blank.gif",
            "https://www.cisco.com/c/dam/en/us/td/i/templates/blank.gif",
            "https://www.cisco.com/c/dam/en/us/td/docs/unified_computing/ucs/UCS_CVDs/ucs_oracle_siebel.fm/_jcr_content/renditions/ucs_oracle_siebel-21.jpg",
            "https://www.cisco.com/c/dam/en/us/td/i/templates/blank.gif",
            "https://www.cisco.com/c/dam/en/us/td/i/templates/note.gif",
            "https://www.cisco.com/c/dam/en/us/td/i/templates/blank.gif",
            "https://www.cisco.com/c/dam/en/us/td/i/templates/blank.gif",
            "https://www.cisco.com/c/dam/en/us/td/i/templates/blank.gif",
            "https://www.cisco.com/c/dam/en/us/td/i/templates/blank.gif",
            "https://www.cisco.com/c/dam/en/us/td/i/templates/blank.gif",
            "https://www.cisco.com/c/dam/en/us/td/i/templates/blank.gif",
            "https://www.cisco.com/c/dam/en/us/td/docs/unified_computing/ucs/UCS_CVDs/ucs_oracle_siebel.fm/_jcr_content/renditions/ucs_oracle_siebel-22.jpg",
            "https://www.cisco.com/c/dam/en/us/td/i/templates/blank.gif",
            "https://www.cisco.com/c/dam/en/us/td/i/templates/note.gif",
            "https://www.cisco.com/c/dam/en/us/td/i/templates/blank.gif",
            "https://www.cisco.com/c/dam/en/us/td/i/templates/blank.gif",
            "https://www.cisco.com/c/dam/en/us/td/i/templates/blank.gif",
            "https://www.cisco.com/c/dam/en/us/td/i/templates/blank.gif",
            "https://www.cisco.com/c/dam/en/us/td/i/templates/blank.gif",
            "https://www.cisco.com/c/dam/en/us/td/i/templates/blank.gif",
            "https://www.cisco.com/c/dam/en/us/td/i/templates/blank.gif",
            "https://www.cisco.com/c/dam/en/us/td/i/templates/blank.gif",
            "https://www.cisco.com/c/dam/en/us/td/i/templates/blank.gif",
            "https://www.cisco.com/c/dam/en/us/td/i/templates/blank.gif",
            "https://www.cisco.com/c/dam/en/us/td/i/templates/blank.gif",
            "https://www.cisco.com/c/dam/en/us/td/i/templates/blank.gif",
            "https://www.cisco.com/c/dam/en/us/td/i/templates/blank.gif",
            "https://www.cisco.com/c/dam/en/us/td/i/templates/blank.gif",
            "https://www.cisco.com/c/dam/en/us/td/i/templates/blank.gif",
            "https://www.cisco.com/c/dam/en/us/td/i/templates/blank.gif",
            "https://www.cisco.com/c/dam/en/us/td/i/templates/blank.gif",
            "https://www.cisco.com/c/dam/en/us/td/i/templates/blank.gif",
            "https://www.cisco.com/c/dam/en/us/td/i/templates/blank.gif",
            "https://www.cisco.com/c/dam/en/us/td/i/templates/blank.gif",
            "https://www.cisco.com/c/dam/en/us/td/i/templates/blank.gif",
            "https://www.cisco.com/c/dam/en/us/td/i/templates/blank.gif",
            "https://www.cisco.com/c/dam/en/us/td/i/templates/blank.gif",
            "https://www.cisco.com/c/dam/en/us/td/i/templates/blank.gif",
            "https://www.cisco.com/c/dam/en/us/td/i/templates/blank.gif",
            "https://www.cisco.com/c/dam/en/us/td/i/templates/note.gif",
            "https://www.cisco.com/c/dam/en/us/td/i/templates/blank.gif",
            "https://www.cisco.com/c/dam/en/us/td/docs/unified_computing/ucs/UCS_CVDs/ucs_oracle_siebel.fm/_jcr_content/renditions/ucs_oracle_siebel-23.jpg",
            "https://www.cisco.com/c/dam/en/us/td/i/templates/blank.gif",
            "https://www.cisco.com/c/dam/en/us/td/i/templates/blank.gif",
            "https://www.cisco.com/c/dam/en/us/td/i/templates/blank.gif",
            "https://www.cisco.com/c/dam/en/us/td/i/templates/blank.gif",
            "https://www.cisco.com/c/dam/en/us/td/i/templates/blank.gif",
            "https://www.cisco.com/c/dam/en/us/td/i/templates/blank.gif",
            "https://www.cisco.com/c/dam/en/us/td/i/templates/blank.gif",
            "https://www.cisco.com/c/dam/en/us/td/docs/unified_computing/ucs/UCS_CVDs/ucs_oracle_siebel.fm/_jcr_content/renditions/ucs_oracle_siebel-24.jpg",
            "https://www.cisco.com/c/dam/en/us/td/i/templates/blank.gif",
            "https://www.cisco.com/c/dam/en/us/td/i/templates/blank.gif",
            "https://www.cisco.com/c/dam/en/us/td/i/templates/blank.gif",
            "https://www.cisco.com/c/dam/en/us/td/i/templates/blank.gif",
            "https://www.cisco.com/c/dam/en/us/td/i/templates/blank.gif",
            "https://www.cisco.com/c/dam/en/us/td/i/templates/blank.gif",
            "https://www.cisco.com/c/dam/en/us/td/i/templates/blank.gif",
            "https://www.cisco.com/c/dam/en/us/td/i/templates/blank.gif",
            "https://www.cisco.com/c/dam/en/us/td/i/templates/blank.gif",
            "https://www.cisco.com/c/dam/en/us/td/i/templates/blank.gif",
            "https://www.cisco.com/c/dam/en/us/td/i/templates/blank.gif",
            "https://www.cisco.com/c/dam/en/us/td/i/templates/blank.gif",
            "https://www.cisco.com/c/dam/en/us/td/i/templates/blank.gif",
            "https://www.cisco.com/c/dam/en/us/td/i/templates/note.gif",
            "https://www.cisco.com/c/dam/en/us/td/i/templates/blank.gif",
            "https://www.cisco.com/c/dam/en/us/td/i/templates/blank.gif",
            "https://www.cisco.com/c/dam/en/us/td/i/templates/blank.gif",
            "https://www.cisco.com/c/dam/en/us/td/i/templates/blank.gif",
            "https://www.cisco.com/c/dam/en/us/td/i/templates/blank.gif",
            "https://www.cisco.com/c/dam/en/us/td/i/templates/blank.gif",
            "https://www.cisco.com/c/dam/en/us/td/i/templates/blank.gif",
            "https://www.cisco.com/c/dam/en/us/td/i/templates/blank.gif",
            "https://www.cisco.com/c/dam/en/us/td/i/templates/blank.gif",
            "https://www.cisco.com/c/dam/en/us/td/i/templates/blank.gif",
            "https://www.cisco.com/c/dam/en/us/td/i/templates/blank.gif",
            "https://www.cisco.com/c/dam/en/us/td/i/templates/blank.gif",
            "https://www.cisco.com/c/dam/en/us/td/i/templates/blank.gif",
            "https://www.cisco.com/c/dam/en/us/td/i/templates/blank.gif",
            "https://www.cisco.com/c/dam/en/us/td/i/templates/blank.gif",
            "https://www.cisco.com/c/dam/en/us/td/i/templates/blank.gif",
            "https://www.cisco.com/c/dam/en/us/td/i/templates/blank.gif",
            "https://www.cisco.com/c/dam/en/us/td/i/templates/blank.gif",
            "https://www.cisco.com/c/dam/en/us/td/i/templates/blank.gif",
            "https://www.cisco.com/c/dam/en/us/td/i/templates/blank.gif",
            "https://www.cisco.com/c/dam/en/us/td/i/templates/blank.gif",
            "https://www.cisco.com/c/dam/en/us/td/i/templates/blank.gif",
            "https://www.cisco.com/c/dam/en/us/td/i/templates/blank.gif",
            "https://www.cisco.com/c/dam/en/us/td/i/templates/blank.gif",
            "https://www.cisco.com/c/dam/en/us/td/i/templates/blank.gif",
            "https://www.cisco.com/c/dam/en/us/td/i/templates/blank.gif",
            "https://www.cisco.com/c/dam/en/us/td/i/templates/blank.gif",
            "https://www.cisco.com/c/dam/en/us/td/i/templates/blank.gif",
            "https://www.cisco.com/c/dam/en/us/td/i/templates/blank.gif",
            "https://www.cisco.com/c/dam/en/us/td/i/templates/blank.gif",
            "https://www.cisco.com/c/dam/en/us/td/i/templates/blank.gif",
            "https://www.cisco.com/c/dam/en/us/td/i/templates/blank.gif",
            "https://www.cisco.com/c/dam/en/us/td/i/templates/blank.gif",
            "https://www.cisco.com/c/dam/en/us/td/i/templates/blank.gif",
            "https://www.cisco.com/c/dam/en/us/td/i/templates/blank.gif",
            "https://www.cisco.com/c/dam/en/us/td/i/templates/blank.gif",
            "https://www.cisco.com/c/dam/en/us/td/i/templates/blank.gif",
            "https://www.cisco.com/c/dam/en/us/td/i/templates/blank.gif",
            "https://www.cisco.com/c/dam/en/us/td/i/templates/blank.gif",
            "https://www.cisco.com/c/dam/en/us/td/i/templates/blank.gif",
            "https://www.cisco.com/c/dam/en/us/td/i/templates/blank.gif",
            "https://www.cisco.com/c/dam/en/us/td/i/templates/blank.gif",
            "https://www.cisco.com/c/dam/en/us/td/i/templates/blank.gif",
            "https://www.cisco.com/c/dam/en/us/td/i/templates/blank.gif",
            "https://www.cisco.com/c/dam/en/us/td/i/templates/blank.gif",
            "https://www.cisco.com/c/dam/en/us/td/i/templates/blank.gif",
            "https://www.cisco.com/c/dam/en/us/td/i/templates/blank.gif",
            "https://www.cisco.com/c/dam/en/us/td/i/templates/blank.gif",
            "https://www.cisco.com/c/dam/en/us/td/i/templates/blank.gif",
            "https://www.cisco.com/c/dam/en/us/td/i/templates/blank.gif",
            "https://www.cisco.com/c/dam/en/us/td/i/templates/blank.gif",
            "https://www.cisco.com/c/dam/en/us/td/i/templates/blank.gif",
            "https://www.cisco.com/c/dam/en/us/td/docs/unified_computing/ucs/UCS_CVDs/ucs_oracle_siebel.fm/_jcr_content/renditions/ucs_oracle_siebel-25.jpg",
            "https://www.cisco.com/c/dam/en/us/td/i/templates/blank.gif",
            "https://www.cisco.com/c/dam/en/us/td/i/templates/blank.gif",
            "https://www.cisco.com/c/dam/en/us/td/i/templates/blank.gif",
            "https://www.cisco.com/c/dam/en/us/td/i/templates/blank.gif",
            "https://www.cisco.com/c/dam/en/us/td/docs/unified_computing/ucs/UCS_CVDs/ucs_oracle_siebel.fm/_jcr_content/renditions/ucs_oracle_siebel-26.jpg",
            "https://www.cisco.com/c/dam/en/us/td/i/templates/blank.gif",
            "https://www.cisco.com/c/dam/en/us/td/i/templates/blank.gif",
            "https://www.cisco.com/c/dam/en/us/td/i/templates/blank.gif",
            "https://www.cisco.com/c/dam/en/us/td/i/templates/blank.gif",
            "https://www.cisco.com/c/dam/en/us/td/i/templates/blank.gif",
            "https://www.cisco.com/c/dam/en/us/td/i/templates/blank.gif",
            "https://www.cisco.com/c/dam/en/us/td/i/templates/blank.gif",
            "https://www.cisco.com/c/dam/en/us/td/i/templates/blank.gif",
            "https://www.cisco.com/c/dam/en/us/td/i/templates/blank.gif",
            "https://www.cisco.com/c/dam/en/us/td/i/templates/blank.gif",
            "https://www.cisco.com/c/dam/en/us/td/i/templates/blank.gif",
            "https://www.cisco.com/c/dam/en/us/td/i/templates/blank.gif",
            "https://www.cisco.com/c/dam/en/us/td/i/templates/blank.gif",
            "https://www.cisco.com/c/dam/en/us/td/i/templates/blank.gif",
            "https://www.cisco.com/c/dam/en/us/td/i/templates/note.gif",
            "https://www.cisco.com/c/dam/en/us/td/i/templates/blank.gif",
            "https://www.cisco.com/c/dam/en/us/td/docs/unified_computing/ucs/UCS_CVDs/ucs_oracle_siebel.fm/_jcr_content/renditions/ucs_oracle_siebel-27.jpg",
            "https://www.cisco.com/c/dam/en/us/td/i/templates/blank.gif",
            "https://www.cisco.com/c/dam/en/us/td/i/templates/blank.gif",
            "https://www.cisco.com/c/dam/en/us/td/i/templates/blank.gif",
            "https://www.cisco.com/c/dam/en/us/td/i/templates/blank.gif",
            "https://www.cisco.com/c/dam/en/us/td/i/templates/blank.gif",
            "https://www.cisco.com/c/dam/en/us/td/i/templates/blank.gif",
            "https://www.cisco.com/c/dam/en/us/td/i/templates/blank.gif",
            "https://www.cisco.com/c/dam/en/us/td/i/templates/blank.gif",
            "https://www.cisco.com/c/dam/en/us/td/i/templates/blank.gif",
            "https://www.cisco.com/c/dam/en/us/td/i/templates/blank.gif",
            "https://www.cisco.com/c/dam/en/us/td/i/templates/blank.gif",
            "https://www.cisco.com/c/dam/en/us/td/i/templates/blank.gif",
            "https://www.cisco.com/c/dam/en/us/td/i/templates/blank.gif",
            "https://www.cisco.com/c/dam/en/us/td/i/templates/blank.gif",
            "https://www.cisco.com/c/dam/en/us/td/i/templates/blank.gif",
            "https://www.cisco.com/c/dam/en/us/td/i/templates/blank.gif",
            "https://www.cisco.com/c/dam/en/us/td/i/templates/blank.gif",
            "https://www.cisco.com/c/dam/en/us/td/i/templates/blank.gif",
            "https://www.cisco.com/c/dam/en/us/td/i/templates/blank.gif",
            "https://www.cisco.com/c/dam/en/us/td/i/templates/note.gif",
            "https://www.cisco.com/c/dam/en/us/td/i/templates/blank.gif",
            "https://www.cisco.com/c/dam/en/us/td/docs/unified_computing/ucs/UCS_CVDs/ucs_oracle_siebel.fm/_jcr_content/renditions/ucs_oracle_siebel-28.jpg",
            "https://www.cisco.com/c/dam/en/us/td/i/templates/blank.gif",
            "https://www.cisco.com/c/dam/en/us/td/i/templates/blank.gif",
            "https://www.cisco.com/c/dam/en/us/td/docs/unified_computing/ucs/UCS_CVDs/ucs_oracle_siebel.fm/_jcr_content/renditions/ucs_oracle_siebel-29.jpg",
            "https://www.cisco.com/c/dam/en/us/td/i/templates/blank.gif",
            "https://www.cisco.com/c/dam/en/us/td/docs/unified_computing/ucs/UCS_CVDs/ucs_oracle_siebel.fm/_jcr_content/renditions/ucs_oracle_siebel-30.jpg",
            "https://www.cisco.com/c/dam/en/us/td/i/templates/blank.gif",
            "https://www.cisco.com/c/dam/en/us/td/docs/unified_computing/ucs/UCS_CVDs/ucs_oracle_siebel.fm/_jcr_content/renditions/ucs_oracle_siebel-31.jpg",
            "https://www.cisco.com/c/dam/en/us/td/i/templates/blank.gif",
            "https://www.cisco.com/c/dam/en/us/td/i/templates/blank.gif",
            "https://www.cisco.com/c/dam/en/us/td/i/templates/blank.gif",
            "https://www.cisco.com/c/dam/en/us/td/i/templates/blank.gif",
            "https://www.cisco.com/c/dam/en/us/td/i/templates/blank.gif",
            "https://www.cisco.com/c/dam/en/us/td/i/templates/blank.gif",
            "https://www.cisco.com/c/dam/en/us/td/i/templates/blank.gif",
            "https://www.cisco.com/c/dam/en/us/td/i/templates/blank.gif",
            "https://www.cisco.com/c/dam/en/us/td/i/templates/blank.gif",
            "https://www.cisco.com/c/dam/en/us/td/i/templates/blank.gif",
            "https://www.cisco.com/c/dam/en/us/td/i/templates/blank.gif",
            "https://www.cisco.com/c/dam/en/us/td/i/templates/blank.gif",
            "https://www.cisco.com/c/dam/en/us/td/i/templates/blank.gif",
            "https://www.cisco.com/c/dam/en/us/td/i/templates/blank.gif",
            "https://www.cisco.com/c/dam/en/us/td/i/templates/blank.gif",
            "https://www.cisco.com/c/dam/en/us/td/i/templates/blank.gif",
            "https://www.cisco.com/c/dam/en/us/td/i/templates/blank.gif",
            "https://www.cisco.com/c/dam/en/us/td/i/templates/blank.gif",
            "https://www.cisco.com/c/dam/en/us/td/i/templates/blank.gif",
            "https://www.cisco.com/c/dam/en/us/td/i/templates/blank.gif",
            "https://www.cisco.com/c/dam/en/us/td/i/templates/blank.gif",
            "https://www.cisco.com/c/dam/en/us/td/i/templates/note.gif",
            "https://www.cisco.com/c/dam/en/us/td/i/templates/blank.gif",
            "https://www.cisco.com/c/dam/en/us/td/i/templates/blank.gif",
            "https://www.cisco.com/c/dam/en/us/td/i/templates/blank.gif",
            "https://www.cisco.com/c/dam/en/us/td/i/templates/blank.gif",
            "https://www.cisco.com/c/dam/en/us/td/i/templates/blank.gif",
            "https://www.cisco.com/c/dam/en/us/td/i/templates/blank.gif",
            "https://www.cisco.com/c/dam/en/us/td/i/templates/blank.gif",
            "https://www.cisco.com/c/dam/en/us/td/i/templates/blank.gif",
            "https://www.cisco.com/c/dam/en/us/td/i/templates/blank.gif",
            "https://www.cisco.com/c/dam/en/us/td/i/templates/blank.gif",
            "https://www.cisco.com/c/dam/en/us/td/i/templates/blank.gif",
            "https://www.cisco.com/c/dam/en/us/td/i/templates/blank.gif",
            "https://www.cisco.com/c/dam/en/us/td/i/templates/blank.gif",
            "https://www.cisco.com/c/dam/en/us/td/i/templates/blank.gif",
            "https://www.cisco.com/c/dam/en/us/td/i/templates/blank.gif",
            "https://www.cisco.com/c/dam/en/us/td/i/templates/blank.gif",
            "https://www.cisco.com/c/dam/en/us/td/i/templates/blank.gif",
            "https://www.cisco.com/c/dam/en/us/td/i/templates/blank.gif",
            "https://www.cisco.com/c/dam/en/us/td/i/templates/blank.gif",
            "https://www.cisco.com/c/dam/en/us/td/i/templates/blank.gif",
            "https://www.cisco.com/c/dam/en/us/td/i/templates/blank.gif",
            "https://www.cisco.com/c/dam/en/us/td/i/templates/blank.gif",
            "https://www.cisco.com/c/dam/en/us/td/i/templates/blank.gif",
            "https://www.cisco.com/c/dam/en/us/td/i/templates/blank.gif",
            "https://www.cisco.com/c/dam/en/us/td/i/templates/blank.gif",
            "https://www.cisco.com/c/dam/en/us/td/i/templates/note.gif",
            "https://www.cisco.com/c/dam/en/us/td/i/templates/blank.gif",
            "https://www.cisco.com/c/dam/en/us/td/i/templates/blank.gif",
            "https://www.cisco.com/c/dam/en/us/td/i/templates/blank.gif",
            "https://www.cisco.com/c/dam/en/us/td/i/templates/blank.gif",
            "https://www.cisco.com/c/dam/en/us/td/i/templates/blank.gif",
            "https://www.cisco.com/c/dam/en/us/td/i/templates/blank.gif",
            "https://www.cisco.com/c/dam/en/us/td/i/templates/blank.gif",
            "https://www.cisco.com/c/dam/en/us/td/i/templates/blank.gif",
            "https://www.cisco.com/c/dam/en/us/td/i/templates/blank.gif",
            "https://www.cisco.com/c/dam/en/us/td/i/templates/blank.gif",
            "https://www.cisco.com/c/dam/en/us/td/i/templates/note.gif",
            "https://www.cisco.com/c/dam/en/us/td/i/templates/blank.gif",
            "https://www.cisco.com/c/dam/en/us/td/i/templates/blank.gif",
            "https://www.cisco.com/c/dam/en/us/td/i/templates/blank.gif",
            "https://www.cisco.com/c/dam/en/us/td/i/templates/blank.gif",
            "https://www.cisco.com/c/dam/en/us/td/i/templates/blank.gif",
            "https://www.cisco.com/c/dam/en/us/td/i/templates/blank.gif",
            "https://www.cisco.com/c/dam/en/us/td/i/templates/blank.gif",
            "https://www.cisco.com/c/dam/en/us/td/i/templates/blank.gif",
            "https://www.cisco.com/c/dam/en/us/td/i/templates/blank.gif",
            "https://www.cisco.com/c/dam/en/us/td/i/templates/blank.gif",
            "https://www.cisco.com/c/dam/en/us/td/i/templates/blank.gif",
            "https://www.cisco.com/c/dam/en/us/td/i/templates/blank.gif",
            "https://www.cisco.com/c/dam/en/us/td/i/templates/blank.gif",
            "https://www.cisco.com/c/dam/en/us/td/i/templates/blank.gif",
            "https://www.cisco.com/c/dam/en/us/td/i/templates/blank.gif",
            "https://www.cisco.com/c/dam/en/us/td/i/templates/blank.gif",
            "https://www.cisco.com/c/dam/en/us/td/i/templates/blank.gif",
            "https://www.cisco.com/c/dam/en/us/td/i/templates/blank.gif",
            "https://www.cisco.com/c/dam/en/us/td/i/templates/blank.gif",
            "https://www.cisco.com/c/dam/en/us/td/i/templates/blank.gif",
            "https://www.cisco.com/c/dam/en/us/td/i/templates/blank.gif",
            "https://www.cisco.com/c/dam/en/us/td/i/templates/blank.gif",
            "https://www.cisco.com/c/dam/en/us/td/i/templates/blank.gif",
            "https://www.cisco.com/c/dam/en/us/td/i/templates/blank.gif",
            "https://www.cisco.com/c/dam/en/us/td/i/templates/blank.gif",
            "https://www.cisco.com/c/dam/en/us/td/i/templates/blank.gif",
            "https://www.cisco.com/c/dam/en/us/td/i/templates/blank.gif",
            "https://www.cisco.com/c/dam/en/us/td/i/templates/blank.gif",
            "https://www.cisco.com/c/dam/en/us/td/i/templates/blank.gif",
            "https://www.cisco.com/c/dam/en/us/td/i/templates/blank.gif",
            "https://www.cisco.com/c/dam/en/us/td/i/templates/blank.gif",
            "https://www.cisco.com/c/dam/en/us/td/i/templates/blank.gif",
            "https://www.cisco.com/c/dam/en/us/td/i/templates/blank.gif",
            "https://www.cisco.com/c/dam/en/us/td/i/templates/blank.gif",
            "https://www.cisco.com/c/dam/en/us/td/i/templates/blank.gif",
            "https://www.cisco.com/c/dam/en/us/td/i/templates/blank.gif",
            "https://www.cisco.com/c/dam/en/us/td/i/templates/blank.gif",
            "https://www.cisco.com/c/dam/en/us/td/i/templates/blank.gif",
            "https://www.cisco.com/c/dam/en/us/td/i/templates/blank.gif",
            "https://www.cisco.com/c/dam/en/us/td/i/templates/blank.gif",
            "https://www.cisco.com/c/dam/en/us/td/i/templates/blank.gif",
            "https://www.cisco.com/c/dam/en/us/td/i/templates/blank.gif",
            "https://www.cisco.com/c/dam/en/us/td/i/templates/blank.gif",
            "https://www.cisco.com/c/dam/en/us/td/i/templates/blank.gif",
            "https://www.cisco.com/c/dam/en/us/td/i/templates/blank.gif",
            "https://www.cisco.com/c/dam/en/us/td/i/templates/blank.gif",
            "https://www.cisco.com/c/dam/en/us/td/i/templates/blank.gif",
            "https://www.cisco.com/c/dam/en/us/td/i/templates/blank.gif",
            "https://www.cisco.com/c/dam/en/us/td/i/templates/blank.gif",
            "https://www.cisco.com/c/dam/en/us/td/i/templates/note.gif",
            "https://www.cisco.com/c/dam/en/us/td/i/templates/blank.gif",
            "https://www.cisco.com/c/dam/en/us/td/i/templates/blank.gif",
            "https://www.cisco.com/c/dam/en/us/td/i/templates/note.gif",
            "https://www.cisco.com/c/dam/en/us/td/i/templates/blank.gif",
            "https://www.cisco.com/c/dam/en/us/td/i/templates/blank.gif",
            "https://www.cisco.com/c/dam/en/us/td/i/templates/blank.gif",
            "https://www.cisco.com/c/dam/en/us/td/i/templates/note.gif",
            "https://www.cisco.com/c/dam/en/us/td/i/templates/blank.gif",
            "https://www.cisco.com/c/dam/en/us/td/i/templates/blank.gif",
            "https://www.cisco.com/c/dam/en/us/td/i/templates/blank.gif",
            "https://www.cisco.com/c/dam/en/us/td/i/templates/blank.gif",
            "https://www.cisco.com/c/dam/en/us/td/i/templates/blank.gif",
            "https://www.cisco.com/c/dam/en/us/td/i/templates/blank.gif",
            "https://www.cisco.com/c/dam/en/us/td/i/templates/note.gif",
            "https://www.cisco.com/c/dam/en/us/td/i/templates/blank.gif",
            "https://www.cisco.com/c/dam/en/us/td/i/templates/blank.gif",
            "https://www.cisco.com/c/dam/en/us/td/i/templates/blank.gif",
            "https://www.cisco.com/c/dam/en/us/td/i/templates/blank.gif",
            "https://www.cisco.com/c/dam/en/us/td/i/templates/blank.gif",
            "https://www.cisco.com/c/dam/en/us/td/i/templates/blank.gif",
            "https://www.cisco.com/c/dam/en/us/td/i/templates/blank.gif",
            "https://www.cisco.com/c/dam/en/us/td/i/templates/note.gif",
            "https://www.cisco.com/c/dam/en/us/td/i/templates/blank.gif",
            "https://www.cisco.com/c/dam/en/us/td/i/templates/blank.gif",
            "https://www.cisco.com/c/dam/en/us/td/i/templates/blank.gif",
            "https://www.cisco.com/c/dam/en/us/td/i/templates/note.gif",
            "https://www.cisco.com/c/dam/en/us/td/i/templates/blank.gif",
            "https://www.cisco.com/c/dam/en/us/td/i/templates/blank.gif",
            "https://www.cisco.com/c/dam/en/us/td/i/templates/blank.gif",
            "https://www.cisco.com/c/dam/en/us/td/i/templates/blank.gif",
            "https://www.cisco.com/c/dam/en/us/td/i/templates/blank.gif",
            "https://www.cisco.com/c/dam/en/us/td/i/templates/blank.gif",
            "https://www.cisco.com/c/dam/en/us/td/i/templates/note.gif",
            "https://www.cisco.com/c/dam/en/us/td/i/templates/blank.gif",
            "https://www.cisco.com/c/dam/en/us/td/i/templates/blank.gif",
            "https://www.cisco.com/c/dam/en/us/td/i/templates/blank.gif",
            "https://www.cisco.com/c/dam/en/us/td/i/templates/blank.gif",
            "https://www.cisco.com/c/dam/en/us/td/i/templates/blank.gif",
            "https://www.cisco.com/c/dam/en/us/td/i/templates/blank.gif",
            "https://www.cisco.com/c/dam/en/us/td/docs/unified_computing/ucs/UCS_CVDs/ucs_oracle_siebel.fm/_jcr_content/renditions/ucs_oracle_siebel-32.jpg",
            "https://www.cisco.com/c/dam/en/us/td/i/templates/blank.gif",
            "https://www.cisco.com/c/dam/en/us/td/i/templates/blank.gif",
            "https://www.cisco.com/c/dam/en/us/td/i/templates/blank.gif",
            "https://www.cisco.com/c/dam/en/us/td/i/templates/blank.gif",
            "https://www.cisco.com/c/dam/en/us/td/i/templates/blank.gif",
            "https://www.cisco.com/c/dam/en/us/td/i/templates/blank.gif",
            "https://www.cisco.com/c/dam/en/us/td/i/templates/blank.gif",
            "https://www.cisco.com/c/dam/en/us/td/i/templates/blank.gif",
            "https://www.cisco.com/c/dam/en/us/td/i/templates/blank.gif",
            "https://www.cisco.com/c/dam/en/us/td/i/templates/blank.gif",
            "https://www.cisco.com/c/dam/en/us/td/i/templates/note.gif",
            "https://www.cisco.com/c/dam/en/us/td/i/templates/blank.gif",
            "https://www.cisco.com/c/dam/en/us/td/i/templates/blank.gif",
            "https://www.cisco.com/c/dam/en/us/td/i/templates/blank.gif",
            "https://www.cisco.com/c/dam/en/us/td/i/templates/blank.gif",
            "https://www.cisco.com/c/dam/en/us/td/i/templates/note.gif",
            "https://www.cisco.com/c/dam/en/us/td/i/templates/blank.gif",
            "https://www.cisco.com/c/dam/en/us/td/i/templates/blank.gif",
            "https://www.cisco.com/c/dam/en/us/td/docs/unified_computing/ucs/UCS_CVDs/ucs_oracle_siebel.fm/_jcr_content/renditions/ucs_oracle_siebel-33.jpg",
            "https://www.cisco.com/c/dam/en/us/td/i/templates/blank.gif",
            "https://www.cisco.com/c/dam/en/us/td/i/templates/blank.gif",
            "https://www.cisco.com/c/dam/en/us/td/i/templates/blank.gif",
            "https://www.cisco.com/c/dam/en/us/td/i/templates/note.gif",
            "https://www.cisco.com/c/dam/en/us/td/i/templates/blank.gif",
            "https://www.cisco.com/c/dam/en/us/td/i/templates/blank.gif",
            "https://www.cisco.com/c/dam/en/us/td/i/templates/blank.gif",
            "https://www.cisco.com/c/dam/en/us/td/i/templates/blank.gif",
            "https://www.cisco.com/c/dam/en/us/td/i/templates/blank.gif",
            "https://www.cisco.com/c/dam/en/us/td/docs/unified_computing/ucs/UCS_CVDs/ucs_oracle_siebel.fm/_jcr_content/renditions/ucs_oracle_siebel-34.jpg",
            "https://www.cisco.com/c/dam/en/us/td/i/templates/note.gif",
            "https://www.cisco.com/c/dam/en/us/td/i/templates/blank.gif",
            "https://www.cisco.com/c/dam/en/us/td/docs/unified_computing/ucs/UCS_CVDs/ucs_oracle_siebel.fm/_jcr_content/renditions/ucs_oracle_siebel-35.jpg",
            "https://www.cisco.com/c/dam/en/us/td/i/templates/blank.gif",
            "https://www.cisco.com/c/dam/en/us/td/i/templates/blank.gif",
            "https://www.cisco.com/c/dam/en/us/td/i/templates/blank.gif",
            "https://www.cisco.com/c/dam/en/us/td/i/templates/blank.gif",
            "https://www.cisco.com/c/dam/en/us/td/i/templates/blank.gif",
            "https://www.cisco.com/c/dam/en/us/td/i/templates/blank.gif",
            "https://www.cisco.com/c/dam/en/us/td/i/templates/blank.gif",
            "https://www.cisco.com/c/dam/en/us/td/docs/unified_computing/ucs/UCS_CVDs/ucs_oracle_siebel.fm/_jcr_content/renditions/ucs_oracle_siebel-36.jpg",
            "https://www.cisco.com/c/dam/en/us/td/docs/unified_computing/ucs/UCS_CVDs/ucs_oracle_siebel.fm/_jcr_content/renditions/ucs_oracle_siebel-37.jpg",
            "https://www.cisco.com/c/dam/en/us/td/i/templates/blank.gif",
            "https://www.cisco.com/c/dam/en/us/td/i/templates/blank.gif",
            "https://www.cisco.com/c/dam/en/us/td/i/templates/blank.gif",
            "https://www.cisco.com/c/dam/en/us/td/i/templates/blank.gif",
            "https://www.cisco.com/c/dam/en/us/td/i/templates/blank.gif",
            "https://www.cisco.com/c/dam/en/us/td/i/templates/blank.gif",
            "https://www.cisco.com/c/dam/en/us/td/i/templates/blank.gif",
            "https://www.cisco.com/c/dam/en/us/td/i/templates/blank.gif",
            "https://www.cisco.com/c/dam/en/us/td/i/templates/blank.gif",
            "https://www.cisco.com/c/dam/en/us/td/i/templates/blank.gif",
            "https://www.cisco.com/c/dam/en/us/td/i/templates/blank.gif",
            "https://www.cisco.com/c/dam/en/us/td/i/templates/note.gif",
            "https://www.cisco.com/c/dam/en/us/td/i/templates/blank.gif",
            "https://www.cisco.com/c/dam/en/us/td/i/templates/blank.gif",
            "https://www.cisco.com/c/dam/en/us/td/i/templates/blank.gif",
            "https://www.cisco.com/c/dam/en/us/td/i/templates/blank.gif",
            "https://www.cisco.com/c/dam/en/us/td/i/templates/blank.gif",
            "https://www.cisco.com/c/dam/en/us/td/i/templates/blank.gif",
            "https://www.cisco.com/c/dam/en/us/td/i/templates/blank.gif",
            "https://www.cisco.com/c/dam/en/us/td/i/templates/blank.gif",
            "https://www.cisco.com/c/dam/en/us/td/i/templates/note.gif",
            "https://www.cisco.com/c/dam/en/us/td/i/templates/blank.gif",
            "https://www.cisco.com/c/dam/en/us/td/i/templates/blank.gif",
            "https://www.cisco.com/c/dam/en/us/td/i/templates/blank.gif",
            "https://www.cisco.com/c/dam/en/us/td/i/templates/blank.gif",
            "https://www.cisco.com/c/dam/en/us/td/i/templates/blank.gif",
            "https://www.cisco.com/c/dam/en/us/td/i/templates/blank.gif",
            "https://www.cisco.com/c/dam/en/us/td/docs/unified_computing/ucs/UCS_CVDs/ucs_oracle_siebel.fm/_jcr_content/renditions/ucs_oracle_siebel-38.jpg",
            "https://www.cisco.com/c/dam/en/us/td/i/templates/blank.gif",
            "https://www.cisco.com/c/dam/en/us/td/i/templates/blank.gif",
            "https://www.cisco.com/c/dam/en/us/td/i/templates/blank.gif",
            "https://www.cisco.com/c/dam/en/us/td/i/templates/blank.gif",
            "https://www.cisco.com/c/dam/en/us/td/i/templates/blank.gif",
            "https://www.cisco.com/c/dam/en/us/td/i/templates/blank.gif",
            "https://www.cisco.com/c/dam/en/us/td/i/templates/blank.gif",
            "https://www.cisco.com/c/dam/en/us/td/i/templates/blank.gif",
            "https://www.cisco.com/c/dam/en/us/td/docs/unified_computing/ucs/UCS_CVDs/ucs_oracle_siebel.fm/_jcr_content/renditions/ucs_oracle_siebel-39.jpg",
            "https://www.cisco.com/c/dam/en/us/td/docs/unified_computing/ucs/UCS_CVDs/ucs_oracle_siebel.fm/_jcr_content/renditions/ucs_oracle_siebel-40.jpg",
            "https://www.cisco.com/c/dam/en/us/td/i/templates/blank.gif",
            "https://www.cisco.com/c/dam/en/us/td/i/templates/blank.gif",
            "https://www.cisco.com/c/dam/en/us/td/i/templates/blank.gif",
            "https://www.cisco.com/c/dam/en/us/td/i/templates/blank.gif",
            "https://www.cisco.com/c/dam/en/us/td/i/templates/blank.gif",
            "https://www.cisco.com/c/dam/en/us/td/i/templates/blank.gif",
            "https://www.cisco.com/c/dam/en/us/td/i/templates/blank.gif",
            "https://www.cisco.com/c/dam/en/us/td/i/templates/blank.gif",
            "https://www.cisco.com/c/dam/en/us/td/i/templates/blank.gif",
            "https://www.cisco.com/c/dam/en/us/td/i/templates/blank.gif",
            "https://www.cisco.com/c/dam/en/us/td/i/templates/blank.gif",
            "https://www.cisco.com/c/dam/en/us/td/i/templates/blank.gif",
            "https://www.cisco.com/c/dam/en/us/td/i/templates/blank.gif",
            "https://www.cisco.com/c/dam/en/us/td/i/templates/blank.gif",
            "https://www.cisco.com/c/dam/en/us/td/i/templates/blank.gif",
            "https://www.cisco.com/c/dam/en/us/td/i/templates/blank.gif",
            "https://www.cisco.com/c/dam/en/us/td/i/templates/blank.gif",
            "https://www.cisco.com/c/dam/en/us/td/docs/unified_computing/ucs/UCS_CVDs/ucs_oracle_siebel.fm/_jcr_content/renditions/ucs_oracle_siebel-41.jpg",
            "https://www.cisco.com/c/dam/en/us/td/docs/unified_computing/ucs/UCS_CVDs/ucs_oracle_siebel.fm/_jcr_content/renditions/ucs_oracle_siebel-42.jpg",
            "https://www.cisco.com/c/dam/en/us/td/docs/unified_computing/ucs/UCS_CVDs/ucs_oracle_siebel.fm/_jcr_content/renditions/ucs_oracle_siebel-43.jpg",
            "https://www.cisco.com/c/dam/en/us/td/docs/unified_computing/ucs/UCS_CVDs/ucs_oracle_siebel.fm/_jcr_content/renditions/ucs_oracle_siebel-44.jpg",
            "https://www.cisco.com/c/dam/en/us/td/docs/unified_computing/ucs/UCS_CVDs/ucs_oracle_siebel.fm/_jcr_content/renditions/ucs_oracle_siebel-45.jpg",
            "https://www.cisco.com/c/dam/en/us/td/docs/unified_computing/ucs/UCS_CVDs/ucs_oracle_siebel.fm/_jcr_content/renditions/ucs_oracle_siebel-46.jpg",
            "https://www.cisco.com/c/dam/en/us/td/docs/unified_computing/ucs/UCS_CVDs/ucs_oracle_siebel.fm/_jcr_content/renditions/ucs_oracle_siebel-47.jpg",
            "https://www.cisco.com/c/dam/en/us/td/docs/unified_computing/ucs/UCS_CVDs/ucs_oracle_siebel.fm/_jcr_content/renditions/ucs_oracle_siebel-48.jpg",
            "https://www.cisco.com/c/dam/en/us/td/docs/unified_computing/ucs/UCS_CVDs/ucs_oracle_siebel.fm/_jcr_content/renditions/ucs_oracle_siebel-49.jpg",
            "https://www.cisco.com/c/dam/en/us/td/docs/unified_computing/ucs/UCS_CVDs/ucs_oracle_siebel.fm/_jcr_content/renditions/ucs_oracle_siebel-50.jpg",
            "https://www.cisco.com/c/dam/en/us/td/docs/unified_computing/ucs/UCS_CVDs/ucs_oracle_siebel.fm/_jcr_content/renditions/ucs_oracle_siebel-51.jpg",
            "https://www.cisco.com/c/dam/en/us/td/docs/unified_computing/ucs/UCS_CVDs/ucs_oracle_siebel.fm/_jcr_content/renditions/ucs_oracle_siebel-52.jpg",
            "https://www.cisco.com/c/dam/en/us/td/docs/unified_computing/ucs/UCS_CVDs/ucs_oracle_siebel.fm/_jcr_content/renditions/ucs_oracle_siebel-53.jpg",
            "https://www.cisco.com/c/dam/en/us/td/docs/unified_computing/ucs/UCS_CVDs/ucs_oracle_siebel.fm/_jcr_content/renditions/ucs_oracle_siebel-54.jpg",
            "https://www.cisco.com/c/dam/en/us/td/i/templates/blank.gif",
            "https://www.cisco.com/c/dam/en/us/td/i/templates/blank.gif",
            "https://www.cisco.com/c/dam/en/us/td/i/templates/blank.gif",
            "https://www.cisco.com/c/dam/en/us/td/i/templates/blank.gif",
            "https://www.cisco.com/c/dam/en/us/td/i/templates/blank.gif",
            "https://www.cisco.com/c/dam/en/us/td/i/templates/blank.gif",
            "https://www.cisco.com/c/dam/en/us/td/i/templates/blank.gif",
            "https://www.cisco.com/c/dam/en/us/td/i/templates/blank.gif",
            "https://www.cisco.com/c/dam/en/us/td/i/templates/blank.gif",
            "https://www.cisco.com/c/dam/en/us/td/i/templates/blank.gif",
            "https://www.cisco.com/c/dam/en/us/td/i/templates/blank.gif",
            "https://www.cisco.com/c/dam/en/us/td/i/templates/blank.gif",
            "https://www.cisco.com/c/dam/en/us/td/i/templates/note.gif",
            "https://www.cisco.com/c/dam/en/us/td/i/templates/blank.gif",
            "https://www.cisco.com/c/dam/en/us/td/i/templates/blank.gif",
            "https://www.cisco.com/c/dam/en/us/td/i/templates/blank.gif",
            "https://www.cisco.com/c/dam/en/us/td/i/templates/blank.gif",
            "https://www.cisco.com/c/dam/en/us/td/i/templates/blank.gif",
            "https://www.cisco.com/c/dam/en/us/td/i/templates/blank.gif",
            "https://www.cisco.com/c/dam/en/us/td/i/templates/blank.gif",
            "https://www.cisco.com/c/dam/cdc/i/Feedback_OceanBlue.png",
            "https://www.cisco.com/etc/designs/cdc/fw/i/icon_lock_small.png",
            "https://cisco.112.2o7.net/b/ss/cisco-mobile/5/12345"
        ],
        "movies": [],
        "keywords": [],
        "meta_keywords": [
            ""
        ],
        "tags": null,
        "authors": [],
        "publish_date": "2021-09-21T10:00:10",
        "summary": "",
        "meta_description": "Oracle Siebel CRM on Cisco Unified Computing System with EMC VNX Storage",
        "meta_lang": "en",
        "meta_favicon": "",
        "meta_site_name": "Cisco",
        "canonical_link": "https://www.cisco.com/c/en/us/td/docs/unified_computing/ucs/UCS_CVDs/ucs_oracle_siebel.html",
        "text": "Table Of Contents\n\nAbout the Authors\n\nAcknowledgements\n\nAbout Cisco Validated Design (CVD) Program\n\nOracle Siebel CRM on Cisco Unified Computing System with EMC VNX Storage\n\nIntroduction\n\nTarget Audience\n\nPurpose of this Guide\n\nBusiness Needs\n\nSolution Overview\n\nTechnology Overview\n\nOracle Siebel Customer Relationship Management Solution\n\nMulti-Tier Architecture\n\nCisco Unified Computing System\n\nFabric Interconnect\n\nCisco UCS 2100 and 2200 Series Fabric Extenders\n\nCisco UCS Blade Chassis\n\nCisco UCS Manager\n\nCisco UCS Blade Server Types\n\nCisco UCS Service Profiles\n\nCisco Nexus 5548UP Switch\n\nI/O Adapters\n\nEMC VNX Storage Family\n\nEMC VNX Storage Platforms\n\nFAST Cache Technology\n\nFAST VP\n\nDesign Considerations for Oracle Siebel Implementation on Cisco Unified Computing System\n\nScalable Architecture Using Cisco UCS Servers\n\nBoot from SAN\n\nEMC VNX5500 - Block and File Storage Required for Oracle Siebel\n\nInfrastructure Setup\n\nConfiguring the Cisco Unified Computing System\n\nLogging into the UCS Manager\n\nEditing Chassis Discovery Policy\n\nEnabling Network Components\n\nCreating MAC Address Pools\n\nCreating WWPN Pools\n\nCreating WWNN Pools\n\nCreating UUID suffix pools\n\nCreating VLANs\n\nCreating Uplink Ports Channels\n\nCreating VSANs\n\nCreating Boot Policies\n\nCreating Service Profile Templates\n\nCreating Service Profile from the Template and associating it to a Blade\n\nConfiguring the EMC VNX5500\n\nCreation of Storage Pools/RAID Groups\n\nCreation of File System (NFS Share)\n\nConfiguring the Nexus Switches\n\nSetting up the Nexus 5548 Switch\n\nEnabling Nexus 5548 Switch Licensing\n\nConfiguration of Ports 29-32 as FC ports\n\nCreating VSAN and Adding FC Interfaces\n\nCreating VLANs and Managing Traffic\n\nCreation and Configuration of Virtual Port Channel (VPC)\n\nCreation of Zoneset and Zones\n\nCisco UCS Manager Service Profile update\n\nModifying Service Profile for Boot Policy\n\nHost - Storage Connectivity\n\nRedHat Linux OS Installation\n\nEMC PowerPath Setup\n\nOracle Siebel Installation\n\nPlanning/Pre-Requisites\n\nUnderstanding the Hardware and Software Prerequisites\n\nHardware Sizing\n\nInstallation of Oracle Database\n\nInstallation of Database Client on the Oracle Siebel Gateway/Application Servers.\n\nPreparing the Siebel File system\n\nDownloading the Oracle Siebel Installation Archives and Running the Oracle Siebel Image Creator\n\nOther Installation Prerequistes\n\nOracle Siebel Installation\n\nOracle HTTP Server Installation\n\nGateway Server Installation\n\nSiebel Server/ DB Utilities Installation\n\nOracle Siebel Configuration\n\nGateway Server / Enterprise Server Configuration\n\nCreation of SWSE Logical Profile\n\nDatabase Configuration\n\nOracle Siebel Server Configuration\n\nSWSE Install and Configuration\n\nApplying the SWSE Logical profile\n\nCustomizations\n\nEAI Configuration\n\nWorkflow Configuration\n\nLessons Learnt & Best Practices\n\nOracle Siebel Performance and Scalability\n\nBusiness Transactions & Workload Mix\n\nTest Environment Setup\n\nSmall Enterprise\n\nCPU / Memory Usage\n\nMedium Enterprise\n\nLarge Enterprise\n\nTransaction Throughput and Response Time\n\nIO Characterization\n\nSizing Recommendations\n\nBest Practices & Tuning Recommendations\n\nWeb Tier\n\nApplication server / Oracle Siebel Enterprise\n\nDatabase Tier\n\nConclusion\n\nBill of Materials\n\nOracle Siebel CRM on Cisco Unified Computing\n\nSystem with EMC VNX Storage\n\nA Cisco Validated Design for Oracle Siebel CRM 8.1.1.4 with Oracle 11g R2 Database on Cisco UCS B-Series Blade Servers for Workloads Scaling up to 10,000 Users\n\nLast Updated: April 27, 2012\n\nBuilding Architectures to Solve Business Problems\n\nAbout the Authors\n\nBabu Mahadevan V, Technical Marketing Engineer, Server Access Virtualization Business Unit, Cisco Systems\n\nBabu has over 15 years of experience in large systems performance in Financial, Telecom and Retail industry verticals focusing on optimizing both custom-made applications as well commercial product deployments. Babu was a performance engineering consultant with TCS prior to Cisco and holds a Master's degree in Electronics Engineering.\n\nVadiraja Bhatt, Performance Architect, Server Access Virtualization Business Unit, Cisco Systems\n\nVadiraja Bhatt is a Performance Architect at Cisco, managing the solutions and benchmarking effort on Cisco Unified Computing System Platform. Vadi has over 17 years of experience in performance and benchmarking the large enterprise systems deploying mission critical applications. Vadi specializes in optimizing and fine tuning complex hardware and software systems and has delivered many benchmark results on TPC and other industry standard benchmarks. Vadi has 6 patents to his credits in the Database (OLTP and DSS) optimization area.\n\nAcknowledgements\n\nFor their support and contribution to the design, validation, and creation of the Cisco Validated Design, we would like to thank:\n\n Deepak Adiga-Cisco\n\n Radhakrishnan Manga-EMC\n\n Kathy Sharp-EMC\n\nAbout Cisco Validated Design (CVD) Program\n\nThe CVD program consists of systems and solutions designed, tested, and documented to facilitate faster, more reliable, and more predictable customer deployments. For more information visit www.cisco.com/go/designzone.\n\nALL DESIGNS, SPECIFICATIONS, STATEMENTS, INFORMATION, AND RECOMMENDATIONS (COLLECTIVELY, \"DESIGNS\") IN THIS MANUAL ARE PRESENTED \"AS IS,\" WITH ALL FAULTS. CISCO AND ITS SUPPLIERS DISCLAIM ALL WARRANTIES, INCLUDING, WITHOUT LIMITATION, THE WARRANTY OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT OR ARISING FROM A COURSE OF DEALING, USAGE, OR TRADE PRACTICE. IN NO EVENT SHALL CISCO OR ITS SUPPLIERS BE LIABLE FOR ANY INDIRECT, SPECIAL, CONSEQUENTIAL, OR INCIDENTAL DAMAGES, INCLUDING, WITHOUT LIMITATION, LOST PROFITS OR LOSS OR DAMAGE TO DATA ARISING OUT OF THE USE OR INABILITY TO USE THE DESIGNS, EVEN IF CISCO OR ITS SUPPLIERS HAVE BEEN ADVISED OF THE POSSIBILITY OF SUCH DAMAGES.\n\nTHE DESIGNS ARE SUBJECT TO CHANGE WITHOUT NOTICE. USERS ARE SOLELY RESPONSIBLE FOR THEIR APPLICATION OF THE DESIGNS. THE DESIGNS DO NOT CONSTITUTE THE TECHNICAL OR OTHER PROFESSIONAL ADVICE OF CISCO, ITS SUPPLIERS OR PARTNERS. USERS SHOULD CONSULT THEIR OWN TECHNICAL ADVISORS BEFORE IMPLEMENTING THE DESIGNS. RESULTS MAY VARY DEPENDING ON FACTORS NOT TESTED BY CISCO.\n\nCCDE, CCENT, Cisco Eos, Cisco Lumin, Cisco Nexus, Cisco StadiumVision, Cisco TelePresence, Cisco WebEx, the Cisco logo, DCE, and Welcome to the Human Network are trademarks; Changing the Way We Work, Live, Play, and Learn and Cisco Store are service marks; and Access Registrar, Aironet, AsyncOS, Bringing the Meeting To You, Catalyst, CCDA, CCDP, CCIE, CCIP, CCNA, CCNP, CCSP, CCVP, Cisco, the Cisco Certified Internetwork Expert logo, Cisco IOS, Cisco Press, Cisco Systems, Cisco Systems Capital, the Cisco Systems logo, Cisco Unity, Collaboration Without Limitation, EtherFast, EtherSwitch, Event Center, Fast Step, Follow Me Browsing, FormShare, GigaDrive, HomeLink, Internet Quotient, IOS, iPhone, iQuick Study, IronPort, the IronPort logo, LightStream, Linksys, MediaTone, MeetingPlace, MeetingPlace Chime Sound, MGX, Networkers, Networking Academy, Network Registrar, PCNow, PIX, PowerPanels, ProConnect, ScriptShare, SenderBase, SMARTnet, Spectrum Expert, StackWise, The Fastest Way to Increase Your Internet Quotient, TransPath, WebEx, and the WebEx logo are registered trademarks of Cisco Systems, Inc. and/or its affiliates in the United States and certain other countries.\n\nAll other trademarks mentioned in this document or website are the property of their respective owners. The use of the word partner does not imply a partnership relationship between Cisco and any other company. (0809R)\n\n 2012 Cisco Systems, Inc. All rights reserved\n\nOracle Siebel CRM on Cisco Unified Computing System with EMC VNX Storage\n\nIntroduction\n\nThe Oracle Siebel CRM software helps organizations to differentiate their businesses to achieve maximum top-and bottom-line growth by delivering a combination of transactional, analytical, and engagement features to manage all customer-facing operations. Oracle Siebel CRM provides comprehensive CRM solutions tailored with industry needs along with pre-built integrations makes it a widely used CRM software.\n\nSince companies compete to attract new businesses, increase customer loyalty through CRM tools, the demand can scale rapidly, forcing datacenters to expand system resources quickly to meet increasing workloads. Oracle Siebel CRM allows datacenters to scale horizontally (adding more servers at each tier as they grow), vertically (adding more powerful servers (CPUs and RAM)) or both, since it is built on Service Oriented Architecture foundation.\n\nOracle Siebel CRM applications, running on the Cisco Unified Computing System, can reduce the total cost of ownership at the platform, site, and organizational levels and increase IT staff productivity and business agility.\n\nTarget Audience\n\nThis document is intended to assist Solution Architects, Sales Engineers, Field Engineers and Consultants in planning, design, and deployment of Oracle Siebel CRM hosted on Cisco Unified Computing System servers. This document assumes that the reader has an architectural understanding of the Cisco UCS servers, Oracle Siebel CRM software, EMC VNX5500 Storage array, and related software.\n\nPurpose of this Guide\n\nThis Cisco Validated Design demonstrates how enterprises can apply best practices for Cisco Unified Computing System, Cisco Nexus family of switches and EMC VNX5500 storage array for Oracle Siebel CRM implementation.\n\nDesign validation was achieved by conducting tests for Oracle Siebel CRM workloads using HP's LoadRunner tool. Workloads were defined as small enterprise sized workloads of 600 concurrent users, medium enterprise workloads of 3000 concurrent users and large workloads of 10000 concurrent users with proportionately increased data volumes.\n\nBusiness Needs\n\nThe business needs for Oracle Siebel CRM on Cisco UCS servers are:\n\n As companies compete to expand to new businesses and improve customer satisfaction using CRM tools such as Oracle Siebel, it can result in high system resource demands in shortest possible time.\n\n Large Oracle Siebel CRM implementations require high compute, highly available system resources as optimum service levels are required to ensure business continuity.\n\n Cost containment and reduced complexities when adding new system resources to meet the ever increasing Siebel CRM demands.\n\n Ability of the system resources to upgrade/adopt to newer technologies such as cloud computing etc. without a significant impact on TCO (Total Cost of Ownership).\n\nTo meet these business needs the Cisco Unified Computing System server platform provides a new data center architecture that is ideal for supporting and managing mission-critical applications. The Cisco UCS is the next-generation data center server platform that unites compute, network, storage access, and virtualization into a cohesive system designed to outperform previous server architectures, increase operational agility and flexibility while potentially dramatically reducing overall data center costs.\n\nTesting exercises conducted at Cisco labs using a well-defined Siebel CRM workload have shown that, the Cisco UCS servers scale easily to accommodate a wide range of workloads - thus validating the Siebel CRM performance requirements can be adequately met in enterprise deployments.\n\nCombined with EMC VNX5500 storage array, Cisco UCS servers provide a compelling proposition for Oracle Siebel CRM implementation, as VNX5500 offers unified storage (both block and file access together) along with optimal response time suiting Oracle Siebel CRM requirements. EMC VNX5500 is high-performing Unified Storage with unsurpassed simplicity and efficiency, and offers new levels of performance, protection, compliance, and ease of management.\n\nSolution Overview\n\nThe Solution demonstrates the ease of deployment of Oracle Siebel CRM version 8.1.1.4 on a fully configured Cisco UCS system with an end to end deployment of a typical Oracle Siebel CRM implementation. The solution describes the following aspects of Siebel CRM deployment on Cisco Unified Computing System:\n\n Configuring Cisco UCS for Oracle Siebel\n\n Configuring Fabric Interconnect\n\n Configuring Cisco UCS Blades\n\n Configuring EMC VNX5500 series storage enclosures for Oracle Siebel CRM\n\n Configuring the storage and creating LUNs\n\n Associating LUNs with the hosts\n\n Installing and configuring Oracle Siebel CRM 8.1.1.4\n\n Provisioning the required server resource\n\n Installing and Configuring Oracle Web Server, Siebel Gateway Server and Siebel Application Server\n\n Configuring the Oracle RAC database\n\n Performance characterization of Oracle Siebel CRM on Cisco Unified Computing System\n\n Testing and tuning Cisco UCS hardware components for Oracle Siebel CRM\n\n Tuning OS level parameters\n\n Tuning Oracle Siebel Components - Web server, Siebel server and Database server\n\n Sizing guide for Oracle Siebel CRM applications on Cisco UCS\n\n Sizing criteria and guidelines\n\nThe solution is limited to a typical, minimally customized Oracle Siebel CRM application which deals with Order Management workload. However, Oracle Siebel CRM has several modules and diverse workload characteristics which can be configured to meet the requirements of broad range of enterprise demands. Although this solution does not address all those scenarios, it provides metrics and guidelines which can be used as a baseline for extending this solution to include specific Oracle Siebel needs to deploy using Cisco Unified Computing System.\n\nTechnology Overview\n\nOracle Siebel Customer Relationship Management Solution\n\nOracle Siebel 8.1 uses a multi-tiered application framework. The Oracle Siebel environment consists of client, application, and database tiers. The client tier comprises of devices that access the application via the Web. The application tier can be broken down to two different functions, services that terminate client connections and Application Object Managers (AOM) that perform the business logic. Multiple application components can reside in the application tier providing different business functions. The database tier contains the Relational Data Base Management System (RDBMS) and shared file system. The database serves as a repository for data collections. The shared file system is for storing attachments such as Adobe Acrobat files, fax quotes, and other documents. The Oracle Siebel architecture is shown in Figure 1.\n\nFigure 1 Oracle Siebel Architecture\n\nMulti-Tier Architecture\n\nClient Tier\n\nThere are multiple client types available on the Oracle Siebel platform. The major client types are Web client, mobile client, and dedicated Web client. Wireless and handheld clients are used in specialized applications such as retail and manufacturing.\n\n Oracle Siebel Web clientOracle Siebel Web client uses a Web browser on the local PC. It connects to the Siebel Web server via http (port 80) or https (port 443). No additional software is required. The Web client is easy to maintain since it does not require any software upgrades.\n\n Oracle Siebel mobile client and dedicated clientThe mobile client and dedicated client require additional software installed on the PC. The additional software provides faster throughput with less data transfers for a given transaction by sending only changed data between the client and the server. For remote users, the dedicated client allows disconnected mode and synchronizes with the database when network connectivity is restored.\n\n Oracle Siebel wireless client and Oracle Siebel handheld clientThese two clients are specialized clients for vertical applications. The wireless client has a translator for Hypertext Transfer Protocol (HTTP) to Wireless Application Protocol (WAP), which is suitable for mobile phones. The handheld client can accommodate information on smaller screens.\n\nApplication Tier\n\nThe application tier contains two functional areas, services that terminate client connections and business logic.\n\nThe former component is called the Siebel Web Server Extension (SWSE). It is an add-on to Oracle HTTP Web server. SWSE is responsible for handling Web requests from users. It forwards user requests to the Application Object Managers (AOM) via Siebel Internet Session API (SISNAPI) protocol. Oracle Siebel provides native server load balancing for highly-available Web servers. Third party load balancers are supported as well.\n\nThere are numerous Siebel application servers that provide different business applications. Each Oracle Siebel application component can be run on a single or multiple physical servers. Application components can be load balanced at the component level across different physical server pools. Load balancing can be configured with native Siebel load balancer or a third-party load balancer.\n\nDatabase Tier\n\nThe database tier provides a repository to Oracle Siebel application data. It consists of a RDBMS and a separate file system store.\n\n File systemThe Oracle Siebel File System (SFS) is a server with a shared directory that provides NFS access to other Oracle Siebel servers. The SFS is a shared storage area for images, reports, documents, and other data. A pointer in the database record locates the file in the SFS.\n\n Database ServerThe database server is the main data store for the Oracle Siebel application. The Oracle Siebel application servers connect directly to the database server. Oracle Database 11g is used in this deployment.\n\nGateway Name Server\n\nGateway name server is a repository for configuration information for each Siebel server. It has configuration information about the Siebel Enterprise.\n\nCisco Unified Computing System\n\nThe Cisco Unified Computing System is a next-generation data center platform that unites compute, network, and storage access. The platform, optimized for virtual environments, is designed using open industry-standard technologies and aims to reduce total cost of ownership (TCO) and increase business agility. The system integrates a low-latency; lossless 10 Gigabit Ethernet unified network fabric with enterprise-class, x86-architecture servers. It is an integrated, scalable, multi chassis platform in which all resources participate in a unified management domain.\n\nThe main components of Cisco Unified Computing System are:\n\nComputingThe system is based on an entirely new class of computing system that incorporates blade servers based on Intel Xeon 5500/5600 Series Processors. Selected Cisco UCS blade servers offer the patented Cisco Extended Memory Technology to support applications with large datasets and allow more virtual machines per server.\n\nNetworkThe system is integrated onto a low-latency, lossless, 10-Gbps unified network fabric. This network foundation consolidates LANs, SANs, and high-performance computing networks which are separate networks today. The unified fabric lowers costs by reducing the number of network adapters, switches, and cables, and by decreasing the power and cooling requirements.\n\nVirtualizationThe system unleashes the full potential of virtualization by enhancing the scalability, performance, and operational control of virtual environments. Cisco security, policy enforcement, and diagnostic features are now extended into virtualized environments to better support changing business and IT requirements.\n\nStorage accessThe system provides consolidated access to both SAN storage and Network Attached Storage (NAS) over the unified fabric. By unifying the storage access the Cisco Unified Computing System can access storage over Ethernet, Fibre Channel, Fibre Channel over Ethernet (FCoE), and iSCSI. This provides customers with choice for storage access and investment protection. In addition, the server administrators can pre-assign storage-access policies for system connectivity to storage resources, simplifying storage connectivity, and management for increased productivity.\n\nManagementThe system uniquely integrates all system components which enable the entire solution to be managed as a single entity by the Cisco UCS Manager. The Cisco UCS Manager has an intuitive graphical user interface (GUI), a command-line interface (CLI), and a robust application programming interface (API) to manage all system configuration and operations.\n\nThe Cisco Unified Computing System is designed to deliver:\n\n A reduced Total Cost of Ownership and increased business agility.\n\n Increased IT staff productivity through just-in-time provisioning and mobility support.\n\n A cohesive, integrated system which unifies the technology in the data center. The system is managed, serviced and tested as a whole.\n\n Scalability through a design for hundreds of discrete servers and thousands of virtual machines and the capability to scale I/O bandwidth to match demand.\n\n Industry standards supported by a partner ecosystem of industry leaders.\n\nCisco Unified Computing System is designed from the ground up to be programmable and self integrating. A server's entire hardware stack, ranging from server firmware and settings to network profiles, is configured through model-based management. With Cisco virtual interface cards, even the number and type of I/O interfaces is programmed dynamically, making every server ready to power any workload at any time.\n\nWith model-based management, administrators manipulate a model of a desired system configuration, associate a model's service profile with the hardware components, and the system configures automatically to match the model. This automation speeds provisioning and workload migration with accurate and rapid scalability. The result is increased IT staff productivity, improved compliance, and reduced risk of failures due to inconsistent configurations.\n\nCisco Fabric Extender technology reduces the number of system components to purchase, configure, manage, and maintain by condensing three network layers into one. It eliminates both blade server and hypervisor-based switches by connecting fabric interconnect ports directly to individual blade servers and virtual machines. Virtual networks are now managed exactly as physical networks are, but with massive scalability. This represents a radical simplification over traditional systems, reducing capital and operating costs while increasing business agility, simplifying and speeding deployment, and improving performance. Figure 2 shows the Cisco UCS components.\n\nFigure 2 Cisco Unified Computing System Components\n\nFabric Interconnect\n\nThe Cisco UCS 6200 Series Fabric Interconnect is a core part of the Cisco Unified Computing System, providing both network connectivity and management capabilities for the system. The Cisco UCS 6200 Series offers line-rate, low-latency, lossless 10 Gigabit Ethernet, Fibre Channel over Ethernet (FCoE) and Fibre Channel functions.\n\nThe Cisco UCS 6200 Series provides the management and communication backbone for the Cisco UCS B-Series Blade Servers and Cisco UCS 5100 Series Blade Server Chassis. All chassis, and therefore all blades, attached to the Cisco UCS 6200 Series Fabric Interconnects become part of a single, highly available management domain. In addition, by supporting unified fabric, the Cisco UCS 6200 Series provides both the LAN and SAN connectivity for all blades within its domain.\n\nFrom a networking perspective, the Cisco UCS 6200 Series uses a cut-through architecture, supporting deterministic, low-latency, line-rate 10 Gigabit Ethernet on all ports, 1Tb switching capacity, 160 Gbps bandwidth per chassis, independent of packet size and enabled services. The product family supports Cisco low-latency, lossless 10 Gigabit Ethernet unified network fabric capabilities, which increase the reliability, efficiency, and scalability of Ethernet networks. The Fabric Interconnect supports multiple traffic classes over a lossless Ethernet fabric from a blade server through an interconnect. Significant TCO savings come from an FCoE-optimized server design in which network interface cards (NICs), host bus adapters (HBAs), cables, and switches can be consolidated. The Cisco Fabric Interconnect is shown in Figure 3.\n\nFigure 3 Cisco 6200 Series Fabric interconnect\n\nThe following are the different types of Cisco Fabric Interconnects:\n\nCisco UCS 6296UP Fabric Interconnect\n\nCisco UCS 6248UP Fabric Interconnect\n\nCisco UCS U6120XP 20-Port Fabric Interconnect\n\nCisco UCS U6140XP 40-Port Fabric Interconnect\n\nCisco UCS 6296UP Fabric Interconnect\n\nThe Cisco UCS 6296UP 96-Port Fabric Interconnect is a 2RU 10 Gigabit Ethernet, FCoE and native Fibre Channel switch offering up to 1920-Gbps throughput and up to 96 ports. The switch has 48 1/10-Gbps fixed Ethernet, FCoE and Fiber Channel ports and three expansion slots. It doubles the switching capacity of the data center fabric to improve workload density from 960Gbps to 1.92 Tbps, reduces end-to-end latency by 40 percent to improve application performance and provides flexible unified ports to improve infrastructure agility and transition to a fully converged fabric.\n\nCisco UCS 6248UP Fabric Interconnect\n\nThe Cisco UCS 6248UP 48-Port Fabric Interconnect is a one-rack-unit (1RU) 10 Gigabit Ethernet, FCoE and Fiber Channel switch offering up to 960-Gbps throughput and up to 48 ports. The switch has 32 1/10-Gbps fixed Ethernet, FCoE and FC ports and one expansion slot.\n\nCisco UCS U6120XP 20-Port Fabric Interconnect\n\nThe Cisco UCS U6120XP 20-Port Fabric Interconnect is a 1RU, 10 Gigabit Ethernet, IEEE DCB, and FCoE interconnect providing more than 500 Gbps throughput with very low latency. It has 20 fixed 10 Gigabit Ethernet, IEEE DCB, and FCoE SFP+ ports.One expansion module slot can be configured to support up to six additional 10 Gigabit Ethernet, IEEE DCB, and FCoE SFP+ ports.\n\nCisco UCS U6140XP 40-Port Fabric Interconnect\n\nThe Cisco UCS U6140XP 40-Port Fabric Interconnect is a 2RU, 10 Gigabit Ethernet, IEEE DCB, and FCoE interconnect built to provide 1.04 Tbps throughput with very low latency. It has 40 fixed 10 Gigabit Ethernet, IEEE DCB, and FCoE SFP+ ports.\n\nTwo expansion module slots can be configured to support up to 12 additional 10 Gigabit Ethernet, IEEE DCB, and FCoE SFP+ ports.\n\nCisco UCS 2100 and 2200 Series Fabric Extenders\n\nThe Cisco UCS 2100 and 2200 Series Fabric Extenders multiplex and forward all traffic from blade servers in a chassis to a parent Cisco UCS fabric interconnect over from 10-Gbps unified fabric links. All traffic, even traffic between blades on the same chassis or virtual machines on the same blade, is forwarded to the parent interconnect, where network profiles are managed efficiently and effectively by the fabric interconnect. At the core of the Cisco UCS fabric extender are application-specific integrated circuit (ASIC) processors developed by Cisco that multiplex all traffic.\n\nUp to two fabric extenders can be placed in a blade chassis.\n\n The Cisco UCS 2104XP Fabric Extender has eight 10GBASE-KR connections to the blade chassis midplane, with one connection per fabric extender for each of the chassis' eight half slots. This configuration gives each half-slot blade server access to each of two 10-Gbps unified fabric-based networks through SFP+ sockets for both throughput and redundancy. It has four ports connecting the fabric interconnect.\n\n The Cisco UCS 2208XP is first product in the Cisco UCS 2200 Series. It has eight 10 Gigabit Ethernet, FCoE-capable, and Enhances Small Form-Factor Pluggable (SFP+) ports that connect the blade chassis to the fabric interconnect. Each Cisco UCS 2208XP has thirty-two 10 Gigabit Ethernet ports connected through the midplane to each half-width slot in the chassis. Typically configured in pairs for redundancy, two fabric extenders provide up to 160 Gbps of I/O to the chassis.\n\nCisco UCS Blade Chassis\n\nThe Cisco UCS 5100 Series Blade Server Chassis is a crucial building block of the Cisco Unified Computing System, delivering a scalable and flexible blade server chassis.\n\nThe Cisco UCS 5108 Blade Server Chassis, is six rack units (6RU) high and can mount in an industry-standard 19-inch rack. A single chassis can house up to eight half-width Cisco UCS B-Series Blade Servers and can accommodate both half-width and full-width blade form factors.\n\nFour single-phase, hot-swappable power supplies are accessible from the front of the chassis. These power supplies are 92 percent efficient and can be configured to support non-redundant, N+ 1 redundant and grid-redundant configurations. The rear of the chassis contains eight hot-swappable fans, four power connectors (one per power supply), and two I/O bays for Cisco UCS 2104XP Fabric Extenders.\n\nA passive mid-plane provides up to 20 Gbps of I/O bandwidth per server slot and up to 40 Gbps of I/O bandwidth for two slots. The chassis is capable of supporting future 40 Gigabit Ethernet standards. The Cisco UCS Blade Server Chassis is shown in Figure 4.\n\nFigure 4 Cisco Blade Server Chassis (front and back view)\n\nCisco UCS Manager\n\nCisco UCS Manager provides unified, embedded management of all software and hardware components of the Cisco Unified Computing System through an intuitive GUI, a command line interface (CLI), or an XML API. The Cisco UCS Manager provides unified management domain with centralized management capabilities and controls multiple chassis and thousands of virtual machines.\n\nCisco UCS Blade Server Types\n\nThe following are the different types of Cisco Blade Servers:\n\nCisco UCS B200 M3 Server\n\nCisco UCS B200 M2 Server\n\nCisco UCS B250 M2 Extended Memory Blade Server\n\nCisco UCS B230 M2 Blade Servers\n\nCisco UCS B440 M2 High-Performance Blade Servers\n\nCisco UCS B200 M3 Server\n\nDelivering performance, versatility and density without compromise, the Cisco UCS B200 M3 Blade Server addresses the broadest set of workloads, from IT and Web Infrastructure through distributed database.\n\nBuilding on the success of the Cisco UCS B200 M2 blade servers, the enterprise-class Cisco UCS B200 M3 server, further extends the capabilities of Cisco's Unified Computing System portfolio in a half blade form factor. The Cisco UCS B200 M3 server harnesses the power and efficiency of the Intel Xeon E5-2600 processor product family, up to 768 GB of RAM, 2 drives or SSDs and up to 2 x 20 GbE to deliver exceptional levels of performance, memory expandability and I/O throughput for nearly all applications. In addition, the Cisco UCS B200 M3 blade server offers a modern design that removes the need for redundant switching components in every chassis in favor of a simplified top of rack design, allowing more space for server resources, providing a density, power and performance advantage over previous generation servers. The Cisco UCS B200M3 Server is shown in Figure 5.\n\nFigure 5 Cisco UCS B200 M3 Blade Server\n\nCisco UCS B200 M2 Server\n\nThe Cisco UCS B200 M2 Blade Server is a half-width, two-socket blade server. The system uses two Intel Xeon 5600 Series Processors, up to 96 GB of DDR3 memory, two optional hot-swappable small form factor (SFF) serial attached SCSI (SAS) disk drives, and a single mezzanine connector for up to 20 Gbps of I/O throughput. The server balances simplicity, performance, and density for production-level virtualization and other mainstream data center workloads. The Cisco UCS B200 M2 Server is shown in Figure 6.\n\nFigure 6 Cisco UCS B200 M2 Blade Server\n\nCisco UCS B250 M2 Extended Memory Blade Server\n\nThe Cisco UCS B250 M2 Extended Memory Blade Server is a full-width, two-socket blade server featuring Cisco Extended Memory Technology. The system supports two Intel Xeon 5600 Series Processors, up to 384 GB of DDR3 memory, two optional SFF SAS disk drives, and two mezzanine connections for up to 40 Gbps of I/O throughput. The server increases performance and capacity for demanding virtualization and large-data-set workloads with greater memory capacity and throughput. The Cisco UCS Extended Memory Blade Server is shown in Figure 7.\n\nFigure 7 Cisco UCS B250 M2 Extended Memory Blade Server\n\nCisco UCS B230 M2 Blade Servers\n\nThe Cisco UCS B230 M2 Blade Server is a full-slot, 2-socket blade server offering the performance and reliability of the Intel Xeon processor E7-2800 product family and up to 32 DIMM slots, which support up to 512 GB of memory. The Cisco UCS B230 M2 supports two SSD drives and one CNA mezzanine slot for up to 20 Gbps of I/O throughput. The Cisco UCS B230 M2 Blade Server platform delivers outstanding performance, memory, and I/O capacity to meet the diverse needs of virtualized environments with advanced reliability and exceptional scalability for the most demanding applications.\n\nCisco UCS B440 M2 High-Performance Blade Servers\n\nThe Cisco UCS B440 M2 High-Performance Blade Server is a full-slot, 2-socket blade server offering the performance and reliability of the Intel Xeon processor E7-4800 product family and up to 512 GB of memory. The Cisco UCS B440 M2 supports four SFF SAS/SSD drives and two CNA mezzanine slots for up to 40 Gbps of I/O throughput. The Cisco UCS B440 M2 blade server extends Cisco UCS by offering increased levels of performance, scalability, and reliability for mission-critical workloads.\n\nCisco UCS Service Profiles\n\nProgrammatically Deploying Server Resources\n\nCisco UCS Manager provides centralized management capabilities, creates a unified management domain, and serves as the central nervous system of the Cisco UCS. Cisco UCS Manager is embedded device management software that manages the system from end-to-end as a single logical entity through an intuitive GUI, CLI, or XML API. Cisco UCS Manager implements role- and policy-based management using service profiles and templates. This construct improves IT productivity and business agility. Now infrastructure can be provisioned in minutes instead of days, shifting IT's focus from maintenance to strategic initiatives.\n\nDynamic Provisioning with Service Profiles\n\nCisco UCS resources are abstract in the sense that their identity, I/O configuration, MAC addresses and WWNs, firmware versions, BIOS boot order, and network attributes (including QoS settings, pin groups, and threshold policies) all are programmable using a just-in-time deployment model. The manager stores this identity, connectivity, and configuration information in service profiles that reside on the Cisco UCS 6100 Series Fabric Interconnect. A service profile can be applied to any blade server to provision it with the characteristics required to support a specific software stack. A service profile allows server and network definitions to move within the management domain, enabling flexibility in the use of system resources. Service profile templates allow different classes of resources to be defined and applied to a number of resources, each with its own unique identities assigned from predetermined pools.\n\nService Profiles and Templates\n\nA service profile contains configuration information about the server hardware, interfaces, fabric connectivity, and server and network identity. The Cisco UCS Manager provisions servers utilizing service profiles. The Cisco UCS Manager implements a role-based and policy-based management focused on service profiles and templates. A service profile can be applied to any blade server to provision it with the characteristics required to support a specific software stack. A service profile allows server and network definitions to move within the management domain, enabling flexibility in the use of system resources.\n\nService profile templates are stored in the Cisco UCS 6100 Series Fabric Interconnects for reuse by server, network, and storage administrators. Service profile templates consist of server requirements and the associated LAN and SAN connectivity. Service profile templates allow different classes of resources to be defined and applied to a number of resources, each with its own unique identities assigned from predetermined pools.\n\nThe Cisco UCS Manager can deploy the service profile on any physical server at any time. When a service profile is deployed to a server, the Cisco UCS Manager automatically configures the server, adapters, Fabric Extenders, and Fabric Interconnects to match the configuration specified in the service profile. A service profile template parameterizes the UIDs that differentiate between server instances.\n\nThis automation of device configuration reduces the number of manual steps required to configure servers, Network Interface Cards (NICs), Host Bus Adapters (HBAs), and LAN and SAN switches. Figure 8 shows the Service profile which contains abstracted server state information, creating an environment to store unique information about a server.\n\nFigure 8 Service Profile\n\nCisco Nexus 5548UP Switch\n\nThe Cisco Nexus 5548UP is a 1RU 1 Gigabit and 10 Gigabit Ethernet switch offering up to 960 gigabits per second throughput and scaling up to 48 ports. It offers 32 1/10 Gigabit Ethernet fixed enhanced Small Form-Factor Pluggable (SFP+) Ethernet/FCoE or 1/2/4/8-Gbps native FC unified ports and three expansion slots. These slots have a combination of Ethernet/FCoE and native FC ports. The Cisco Nexus 5548UP switch is shown in Figure 9.\n\nFigure 9 Cisco Nexus 5548UP switch\n\nI/O Adapters\n\nThe Cisco UCS Blade Server has various Converged Network Adapters (CNA) options. The Cisco UCS M81KR Virtual Interface Card (VIC) option is used in this Cisco Validated Design.\n\nThis Cisco UCS M81KR VIC is unique to the Cisco UCS blade system. This mezzanine card adapter is designed around a custom ASIC that is specifically intended for VMware-based virtualized systems. It uses custom drivers for the virtualized HBA and 10-GE network interface card. As is the case with the other Cisco CNAs, the Cisco UCS M81KR VIC encapsulates fibre channel traffic within the 10-GE packets for delivery to the Fabric Extender and the Fabric Interconnect.\n\nThe Cisco UCS M81KR VIC provides the capability to create multiple VNICs (up to 128 in version 1.4) on the CNA. This allows complete I/O configurations to be provisioned in virtualized or non-virtualized environments using just-in-time provisioning, providing tremendous system flexibility and allowing consolidation of multiple physical adapters.\n\nSystem security and manageability is improved by providing visibility and portability of network policies and security all the way to the virtual machines. Additional M81KR features like VN-Link technology and pass-through switching, minimize implementation overhead and complexity. The Cisco UCS M81KR VIC is as shown in Figure 10.\n\nFigure 10 Cisco UCS M81KR VIC\n\nVIC1240\n\nThe Cisco UCS Virtual Interface Card 1240 is a 4-port 10 Gigabit Ethernet, Fibre Channel over Ethernet (FCoE)-capable modular LAN on motherboard (mLOM) designed exclusively for the M3 generation of Cisco UCS B-Series Blade Servers. When used in combination with an optional Port Expander, the Cisco UCS VIC 1240 capabilities can be expanded to eight ports of 10 Gigabit Ethernet. The Cisco UCS VIC 1240 enables a policy-based, stateless, agile server infrastructure that can present up to 256 PCIe standards-compliant interfaces to the host that can be dynamically configured as either network interface cards (NICs) or host bus adapters (HBAs). In addition, the Cisco UCS VIC 1240 supports Cisco Data Center Virtual Machine Fabric Extender (VM-FEX) technology, which extends the Cisco UCS fabric interconnect ports to virtual machines, simplifying server virtualization deployment.\n\nVIC1280\n\nThe Cisco UCS Virtual Interface Card 1280 is an eight-port 10 Gigabit Ethernet, Fibre Channel over Ethernet (FCoE)-capable mezzanine card designed exclusively for Cisco UCS B-Series Blade Servers. The card enables a policy-based, stateless, agile server infrastructure that can present up to 256 PCIe standards-compliant interfaces to the host that can be dynamically configured as either network interface cards (NICs) or host bus adapters (HBAs). In addition, the Cisco UCS VIC 1280 supports Cisco Virtual Machine Fabric Extender (VM-FEX) technology, which extends the Cisco UCS Fabric Interconnect ports to virtual machines, simplifying server virtualization deployment.\n\nEMC VNX Storage Family\n\nThe EMC VNX family of storage systems represents EMC's next generation of unified storage, optimized for virtual environments, while offering a cost effective choice for deploying mission-critical enterprise applications such as Oracle Siebel. The massive virtualization and consolidation trends with servers demand a new storage technology that is dynamic and scalable. The EMC VNX series meets these requirements and offers several software and hardware features for optimally deploying enterprise applications such as Oracle Siebel. The EMC VNX family is shown in Figure 11.\n\nFigure 11 The EMC VNX Family of Unified Storage Platforms\n\nA key distinction of this new generation of platforms is, support for both block and file-based external storage access over a variety of access protocols, including Fibre Channel (FC), iSCSI, FCoE, NFS, and CIFS network shared file access. Furthermore, data stored in one of these systems, whether accessed as block or file-based storage objects, is managed uniformly via Unisphere, a web-based interface window. Additional information on Unisphere can be found on emc.com in the white paper titled: Introducing EMC Unisphere: A Common Midrange Element Manager, see:\n\nhttp://www.emc.com/collateral/software/white-papers/h8017-unisphere-element-manager.pdf.\n\nEMC VNX Storage Platforms\n\nThe new EMC VNX family of unified storage platforms continues the EMC tradition of providing some of the highest data reliability and availability in the industry. Apart from this they also include in their design a boost in performance and bandwidth to address the sustained data access bandwidth rates. The new system design has also placed heavy emphasis on storage efficiencies and density, as well as crucial green storage factors, such as a smaller data center footprint, lower power consumption, and improvements in power reporting. The VNX5500 model was used in this Oracle Siebel implementation exercise. All models in EMC's new VNX storage family now support the 2.5\" SAS drives in a 2U disk array enclosure (DAE) that can hold up to 25 drives, one of the densest offerings in the industry. For example, compared to the older-generation technology of storing 15 x 600 GB worth of data using the 3.5\" FC drives in a 3U DAE, the new DAE which uses 25 x 600 GB drives in a 2U footprint means an increase of 2.5 times. The power efficiency of the new DAEs also makes it more cost-effective to store the increased data in this much more compact footprint without the need to increase power consumption and cooling. For more information on VNX Series, see: http://www.emc.com/collateral/hardware/data-sheets/h8520-vnx-family-ds.pdf.\n\nKey efficiency features available with the VNX series include FAST Cache and FAST VP.\n\nFAST Cache Technology\n\nFAST cache is a storage performance optimization feature that provides immediate access to frequently accessed data. In traditional storage arrays, the DRAM caches are too small to maintain the hot data for a long period of time. Very few storage arrays give an option to non-disruptively expand DRAM cache, even if they support DRAM cache expansion. FAST Cache extends the available cache to customers by up to 2 TB using enterprise Flash drives. FAST Cache tracks the data temperature at 64 KB granularity and copies hot data to the Flash drives once its temperature reaches a certain threshold. After a data chunk gets copied to FAST Cache, the subsequent accesses to that chunk of data will be served at Flash latencies. Eventually, when the data temperature cools down, the data chunks get evicted from FAST Cache and will be replaced by newer hot data. FAST Cache uses a simple Least Recently Used (LRU) mechanism to evict the data chunks.\n\nFAST Cache is built on the premise that the overall applications' latencies can improve when most frequently accessed data is maintained on a relatively smaller sized, but faster storage medium, like Flash drives. FAST Cache identifies the most frequently accessed data which is temporary and copies it to the flash drives automatically and non-disruptively. The data movement is completely transparent to applications, thereby making this technology application-agnostic and management-free. For example, FAST Cache can be enabled or disabled on any storage pool simply by selecting/clearing the \"FAST Cache\" storage pool property in advanced settings.\n\nFAST Cache can be selectively enabled on a few or all storage pools within a storage array, depending on application performance requirements and SLAs.\n\nThere are several distinctions to EMC FAST Cache:\n\n It can be configured in read/write mode, which allows the data to be maintained on a faster medium for longer periods, irrespective of application read-to-write mix and data re-write rate.\n\n FAST Cache is created on a persistent medium like Flash drives, which can be accessed by both the storage processors. In the event of a storage processor failure, the surviving storage processor can simply reload the cache rather than repopulating it from scratch. This can be done by observing the data access patterns again, which is a differentiating factor.\n\n Enabling FAST Cache is completely non-disruptive. It is as simple as selecting the Flash drives that are part of FAST Cache and does not require any array disruption or downtime.\n\n Since FAST Cache is created on external Flash drives, adding FAST Cache will not consume any extra PCI-E slots inside the storage processor.\n\nEMC FAST Cache used in this Oracle Siebel architecture is as shown in Figure 12.\n\nFigure 12 EMC FAST Cache\n\nAdditional information on EMC Fast Cache is documented in the white paper titled EMC FAST Cache - A Detailed Review which is available at:\n\nhttp://www.emc.com/collateral/software/white-papers/h8046-clariion-celerra-unified-fast-cache-wp.pdf.\n\nFAST VP\n\nVNX FAST VP is a policy-based auto-tiering solution for enterprise applications. FAST VP operates at a granularity of 1 GB, referred to as a \"slice\". The goal of FAST VP is to efficiently utilize storage tiers to lower customers' TCO by tiering colder slices of data to high-capacity drives, such as NL-SAS, and to increase performance by keeping hotter slices of data on performance drives, such as Flash drives. This occurs automatically and transparently to the host environment. High locality of data is important to realize the benefits of FAST VP. When FAST VP relocates data, it will move the entire slice to the new storage tier. To successfully identify and move the correct slices, FAST VP automatically collects and analyzes statistics prior to relocating data. Customers can initiate the relocation of slices manually or automatically by using a configurable, automated scheduler that can be accessed from the Unisphere management tool. The multi-tiered storage pool allows FAST VP to fully utilize all the storage tiers: Flash, SAS, and NL-SAS. The creation of a storage pool allows for the aggregation of multiple RAID groups, using different storage tiers, into one object. The LUNs created out of the storage pool can be either thickly or thinly provisioned. These \"pool LUNs\" are no longer bound to a single storage tier. Instead, they can be spread across different storage tiers within the same storage pool. If you create a storage pool with one tier (Flash, SAS, or NL-SAS), then FAST VP has no impact on the performance of the system. To operate FAST VP, you need at least two tiers.\n\nAdditional information on EMC FAST VP for Unified Storage is documented in the white paper titled EMC FAST VP for Unified Storage System - A Detailed Review, see: http://www.emc.com/collateral/software/white-papers/h8058-fast-vp-unified-storage-wp.pdf.\n\nFAST Cache and FAST VP are offered in a FAST Suite package as part of the VNX Total Efficiency Pack. This pack includes the FAST Suite which automatically optimizes for the highest system performance and lowest storage cost simultaneously. In addition, this pack includes the Security and Compliance Suite which keeps data safe from changes, deletions, and malicious activity. For additional information on this Total Efficiency Pack as well as other offerings such as the Total Protection Pack, see: http://www.emc.com/collateral/software/data-sheet/h8509-vnx-software-suites-ds.pdf\n\nDesign Considerations for Oracle Siebel Implementation on Cisco Unified Computing System\n\nIn this section, the key design considerations such as scalability, high availability, and performance are addressed for Oracle Siebel Implementation on Cisco Unified Computing System. Since most organizations use Siebel as front end application dealing with their customers/partners, the demands on non-functional aspects are very critical.\n\nScalable Architecture Using Cisco UCS Servers\n\nThe target workloads for the small, medium, and large enterprise were modeled based on the real world CRM implementation for this exercise is as shown in Table 1.\n\nThe classification is based on a typical deployment scenario in an enterprise, keeping in mind the load and data size requirements. These broad classifications can be taken as framework for sizing the Cisco UCS servers.\n\nThe deployment configurations for small, medium and large Oracle Siebel enterprises are shown in Figure 13. These configurations were built by leveraging the modular architecture of Oracle Siebel as well by the choice of a range of Cisco UCS servers based on their capacity. In order to achieve high availability, redundancy at the level of the database servers, Siebel application servers, web servers with gateway servers are considered.\n\nFigure 13 Oracle Siebel Deployment Options for Small, Medium and Large Enterprises\n\nFigure 14 shows a deployment topology configured for a small Oracle Siebel Enterprise, which uses Cisco UCS B200/B230 servers, Cisco 6100 series Fabric Interconnects, Nexus 5000 series switches and EMC VNX5500 storage. The B200 M2 blade server is an entry level blade server which is suitable for low to moderate compute/ memory workloads such as web servers and gateway servers. However with increased CPU capacity, it can also serve as Application server in small enterprise Oracle Siebel setup.\n\nFor medium size enterprise setup, an additional blade is added at database tier to facilitate for a two node Oracle RAC (Real Application Cluster) implementation. With a typical customer scenario having 3000 concurrent users and 150 GB of database size, high availability of database is critical which is achieved using Oracle RAC. Since redundancy is essential for every critical component (such as network and I/O paths) within the database nodes, full width B250 blade server is chosen to host individual Oracle RAC nodes.\n\nFigure 14 Small Oracle Siebel Enterprise - Topology\n\nFor large deployments, additional application servers are added as shown in Figure 15. Each of the components (Web, Application and DB server) can be scaled for increase in the workload.\n\nFigure 15 Large Oracle Siebel Enterprise - Topology\n\nBoot from SAN\n\nBoot from SAN is a critical feature which helps to achieve stateless computing in which there is no static binding between a physical server and the OS / applications hosted on that server. The OS is installed on a SAN LUN and is booted using the service profile. When the service profile is moved to another server, the server policy and the PWWN of the HBAs will also move along with the service profile. The new server takes the identity of the old server and looks identical to the old server.\n\nThe following are the benefits of boot from SAN:\n\n Reduce Server Footprint - Boot from SAN eliminates the need for each server to have its own direct-attached disk (internal disk) which is a potential point of failure. The following are the advantages of diskless servers:\n\n Require less physical space\n\n Require less power\n\n Require fewer hardware components\n\n Less expensive\n\n Disaster and Server Failure RecoveryBoot information and production data stored on a local SAN can be replicated to another SAN at a remote disaster recovery site. When server functionality at the primary site goes down in the event of a disaster, the remote site can take over with a minimal downtime.\n\n Recovery from server failuresRecovery from server failures is simplified in a SAN environment. Data can be quickly recovered with the help of server snapshots, and mirrors of a failed server in a SAN environment. This greatly reduces the time required for server recovery.\n\n High AvailabilityA typical data center is highly redundant in nature with redundant paths, redundant disks and redundant storage controllers. The operating system images are stored on SAN disks which eliminates potential problems caused due to mechanical failure of a local disk.\n\n Rapid RedeploymentBusinesses that experience temporary high production workloads can take advantage of SAN technologies to clone the boot image and distribute the image to multiple servers for rapid deployment. Such servers may only need to be in production for hours or days and can be readily removed when the production need has been met. Highly efficient deployment of boot images makes temporary server usage highly cost effective.\n\n Centralized Image ManagementWhen an operating system images are stored on SAN disks, all upgrades and fixes can be managed at a centralized location. Servers can readily access changes made to disks in a storage array.\n\nWith boot from SAN, the server image resides on the SAN and the server communicates with the SAN through a Host Bus Adapter (HBA). The HBA BIOS contain instructions that enable the server to find the boot disk. After Power OnSelf Test (POST), the server hardware component fetches the designated boot device in the hardware BOIS settings. Once the hardware detects the boot device, it follows the regular boot process.\n\nEMC VNX5500 - Block and File Storage Required for Oracle Siebel\n\nOracle Siebel data is traditionally stored in any of the supported RDBMS such as Oracle using block storage. In our current implementation, the EMC VNX5500 storage system is used for block storage. The EMC VNX5500's capability of storing files and block access in unified manner is leveraged in this solution. LUNs are carved out using heterogeneous storage pools (FAST Virtual Pool with Flash drives, SAS, and NL-SAS disks) to ensure meeting the Oracle Siebel CRM storage capacity and performance demands. FAST Cache is enabled for the entire array to ensure faster response times for both read and write operations.\n\nStoring files such as PDFs, Word Documents etc., is also common requirement along with captured data. Familiar examples are adding resolution steps of Service Requests in MS word document format (or) generating an invoice document (typically a PDF) for a newly captured order. The EMC VNX5500 has datamover components which allow accessing these files using NFS/CIFS protocols and hence reduces the data management challenges.\n\nInfrastructure Setup\n\nThis section describes the configuration and setup details for:\n\n Cisco UCS with and without boot Policy\n\n EMC Storage\n\n Nexus Switch\n\nConfiguring the Cisco Unified Computing System\n\nThis section details the Cisco UCS configuration that is done as part of the infrastructure build for deployment of Oracle Siebel. The racking, power and installation of the chassis are described in the install guide: http://www.cisco.com/en/US/docs/unified_computing/Cisco UCS/hw/chassis/install/Cisco UCS5108_install.html.\n\nOne of the important aspects of configuring a physical blade in the Cisco UCS 5108 chassis is to build a Service Profile through the Cisco UCS Manager. Service profile is an extension of the virtual machine abstraction applied to physical servers. The definition has been expanded to include elements of the environment that span the entire data center, encapsulating the server identity (LAN and SAN addressing, I/O configurations, firmware versions, boot order, network VLAN, physical port, and quality-of-service [QoS] policies) in logical service profiles that can be dynamically created and associated with any physical blade in the system within minutes as compared to the considerable time consumption in a conventional approach. The association of service profiles with the physical servers is performed as a simple, single operation. It enables migration of identities between servers in the environment without requiring any physical configuration changes and facilitates rapid Cisco UCS Server provisioning for replacements of failed servers. Service profiles can be created either from an existing template or from cloning an existing profile or from a new service profile.\n\nLogging into the UCS Manager\n\nTo log into Cisco UCS Manager, perform the following steps:\n\n1. Open the Web browser with the Cisco UCS 6120 Fabric Interconnect cluster address.\n\n2. Click Launch to download the Cisco UCS Manager software.\n\n3. You might be prompted to accept security certificates; accept as necessary.\n\n4. In the login page, enter \"admin\" in username text box and the password set during the initial setup in the password text box.\n\n5. Click Login to access the Cisco UCS Manager software.\n\nVerification: The Cisco UCS Manager software must show up after clicking \"Login\".\n\nEditing Chassis Discovery Policy\n\nTo edit the chassis discovery policy, perform the following steps:\n\n1. Navigate to the Equipment tab in the right pane of the UCS Manager.\n\n2. In the right pane, click the Policies tab.\n\n3. Under Global Policies, change the Chassis Discovery Policy to 4-link.\n\n4. Click Save Changes in the bottom right corner.\n\nEnabling Network Components\n\nTo enabling Fiber Channel, servers, and uplink ports, perform the following steps:\n\n1. Select the Equipment tab on the top left of the UCS Manager window.\n\n2. Select Equipment>Fabric Interconnects >Fabric Interconnect A (primary) >Fixed Module.\n\n3. Expand the Unconfigured Ethernet Ports section.\n\n4. Select ports 1-12 that are connected to the UCS chassis and right-click on them and select Configure as Server Port.\n\n5. Click Yes to confirm, and then click OK to continue.\n\n6. Select ports 17 and 19. These ports are connected to the Cisco Nexus 5548 switches. Right-click on them and select Configure as Uplink Port.\n\n7. Click Yes to confirm, and then click OK to continue.\n\n8. Select Equipment > Fabric Interconnects >Fabric Interconnect A (primary) > Expansion Module 2.\n\n9. Ensure the FC ports 1-2 are not disabled.\n\n10. Click Yes to confirm, and then click OK to continue.\n\n11. Select Equipment > Fabric Interconnects >Fabric Interconnect A (primary).\n\n12. Right click, and select Set FC Switching Mode to put the Fabric Interconnect in Fiber Channel Switching Mode.\n\n13. Click Yes to confirm.\n\n14. A message displays stating that the \"Fiber Channel Switching Mode has been set and the switch will reboot\". Click OK to continue. Wait until the UCS Manager is available again and log back into the interface.\n\n15. Select Equipment > Fabric Interconnects > Fabric Interconnect B (subordinate) > Fixed Module.\n\n16. Expand the Unconfigured Ethernet Ports section.\n\n17. Select ports 1-12. These ports are connected to the UCS chassis and right-click on them and select Configure as Server Port.\n\n18. Click Yes to confirm and then click OK to continue.\n\n19. Select ports 17 and 19. These ports are connected to the Cisco Nexus 5548 switches. Right-click on them, and select Configure as Uplink Port.\n\n20. Click Yes to confirm, and then click OK to continue.\n\n21. Select Equipment > Fabric Interconnects > Fabric Interconnect B (subordinate) > Expansion Module 2.\n\n22. Ensure the FC ports 1-2 are not disabled.\n\n23. Click Yes to confirm, and then click OK to save changes and exit.\n\nVerification: Check if all configured links show their status as \"up\" as shown in Figure 16 for Fabric Interconnect A. This can also be verified on the Cisco Nexus switch side by running \"show int status\" and all the ports connected to the Cisco UCS fabric interconnects are shown as \"up\".\n\nFigure 16 Configured Links Shown on Fabric Interconnect A\n\nCreating MAC Address Pools\n\nTo create MAC Address pools, perform the following steps:\n\n1. Select the LAN tab on the left of the Cisco UCS Manager window.\n\n2. Under Pools > root.\n\nNote Two MAC address pools will be created, one for fabric A and one for fabric B.\n\n3. Right-click MAC Pools under the root organization and select Create MAC Pool to create the MAC address pool for fabric A.\n\n4. Enter Siebel_MAC_Pool_A for the name of the MAC pool for fabric A.\n\n5. Enter a description of the MAC pool in the description text box. This is optional; you can choose to omit the description.\n\n6. Click Next to continue.\n\n7. Click Add to add the MAC address pool.\n\n8. Specify a starting MAC address for fabric A.\n\nNote The default is fine, but it is recommended to change the second before last octet to be \"0A\" to differentiate between MAC addresses in fabric A and fabric B (00:25:85:00:0A:00).\n\n9. Specify the size as 32 for the MAC address pool for fabric A.\n\n10. Click OK.\n\n11. Click Finish.\n\n12. A pop-up message box appears, click OK to save changes.\n\n13. Right-click MAC Pools under the root organization and select Create MAC Pool to create the MAC address pool for fabric B.\n\n14. Enter Siebel_MAC_Pool_B for the name of the MAC pool for fabric B.\n\n15. Enter a description of the MAC pool in the description text box. This is optional; you can choose to omit the description.\n\n16. Click Next to continue.\n\n17. Click Add to add the MAC address pool.\n\n18. Specify a starting MAC address for fabric B.\n\nNote The default is fine, but it is recommended to change the second before last octet to be \"0B\" to differentiate between MAC addresses in fabric A and fabric B (00:25:85:00:0B:00).\n\n19. Specify the size as 32 for the MAC address pool for fabric B.\n\n20. Click OK.\n\n21. Click Finish.\n\n22. A pop-up message box appears; click OK to save changes and exit.\n\nVerification: Select LAN tab > Pools > root. Select MAC Pools and it expands to show the MAC pools created. On the right pane, details of the MAC pools are displayed as shown in Figure 17.\n\nFigure 17 MAC Pool Details\n\nCreating WWPN Pools\n\nTo create WWPN pools, perform the following steps:\n\n1. Select the SAN at the top left of the Cisco UCS Manager window.\n\n2. Select WWPN Pools > root.\n\nNote Two WWPN pools will be created, one for fabric A and one for fabric B.\n\n3. Right-click WWPN Pools and select Create WWPN Pool.\n\n4. Enter Siebel_WWPN_Pool_A as the name for the WWPN pool for fabric A.\n\n5. Enter a description of the WWPN pool in the description text box. This is optional; you can choose to omit the description.\n\n6. Click Next.\n\n7. Click Add to add a block of WWPNs.\n\n8. Enter 20:00:00:25:B5:00:0A:00 as the starting WWPN in the block for fabric A.\n\nNote It is recommended to change the octet next to 25 to some number to identify the FI pair in the Datacenter (here Oracle Siebel FIs are in fifth rack hence used `B5') in order to avoid the same WWPNs get published in Nexus switch level which may be talking to multiple FI pair. Also suggested to change the second-to-last octet to be \"0A\" to differentiate between WWPNs in fabric A and fabric B (20:00:00:25:B5:00:0A:00).\n\n9. Set the size of the WWPN block to 48.\n\n10. Click OK to continue.\n\n11. Click Finish to create the WWPN pool.\n\n12. Click OK to save changes.\n\n13. Right-click WWPN Pools and select Create WWPN Pool.\n\n14. Enter Siebel_WWPN_Pool_B as the name for the WWPN pool for fabric B.\n\n15. Enter a description of the WWPN pool in the description text box. This is optional; you can choose to omit the description.\n\n16. Click Next.\n\n17. Click Add to add a block of WWPNs.\n\n18. Enter 20:00:00:25:B5:00:0B:00 as the starting WWPN in the block for fabric B.\n\nNote It is recommended to change the octet next to 25 as `B5' (from 00) and second-to-last octet to be \"0B\" to identify as fabric B (20:00:00:25:B5:00:0B:00).\n\n19. Set the size of the WWPN block to 48.\n\n20. Click OK to continue.\n\n21. Click Finish to create the WWPN pool.\n\n22. Click OK to save changes and exit.\n\nVerification: The new name with the 48 block size displays in the right panel when WWPN pools is selected on the left panel. Also verify that the second-to-last octet reflects the fabric ID as shown in Figure 18.\n\nFigure 18 WWPN Pool Details\n\nCreating WWNN Pools\n\nTo create WWNN pools, perform the following steps:\n\n1. Select the SAN tab at the top left of the UCS manager window.\n\n2. Select Pools > root.\n\n3. Right-click WWNN Pools and select Create WWNN Pool.\n\n4. Enter Oracle Siebel_WWNN_Pool as the name of the WWNN pool.\n\n5. Enter a description of the WWNN pool in the description text box. This is optional; you can choose to omit the description.\n\n6. Click Next to continue.\n\n7. A pop-up window \"Add WWN Blocks\" appears; click Add at the bottom of the page.\n\n8. A pop-up window \"Create WWN Blocks\" appears; set the size of the WWNN block to 32.\n\n9. Click OK to continue.\n\n10. Click Finish.\n\n11. Click OK to save changes and exit.\n\nVerification: The new name with the 32 block size displays in the right panel when WWNN pools is selected on the left panel as shown in Figure 19.\n\nFigure 19 WWNN Pool Details\n\nCreating UUID suffix pools\n\nTo create UUID suffix pools, perform the following steps:\n\n1. Select the Servers tab on the top left of the Cisco UCS Manager window.\n\n2. Select Pools > root.\n\n3. Right-click UUID Suffix Pools and select Create UUID Suffix Pool.\n\n4. Enter the name the UUID suffix pool as Siebel_UUID_Pool.\n\n5. Enter a description of the UUID suffix pool in the description text box. This is optional; you can choose to omit the description.\n\n6. Prefix is set to \"derived\" by default. Do not change the default setting.\n\n7. Click Next to continue.\n\n8. A pop-up window \"Add UUID Blocks\" appears. Click Add button at the bottom of the window to add a block of UUID suffixes.\n\n9. The \"Form\" field will be in default setting. Do not change the \"From\" field.\n\n10. Set the size of the UUID suffix pool to 32.\n\n11. Click OK to continue.\n\n12. Click Finish to create the UUID suffix pool.\n\n13. Click OK to save changes and exit.\n\nVerification: Ensure that the UUID suffix pools created are displayed as shown in Figure 20.\n\nFigure 20 UUID Suffix Pool Details\n\nCreating VLANs\n\nTo create VLANs, perform the following steps:\n\n1. Select the LAN tab on the left of the Cisco UCS Manager window.\n\nNote Three VLANs will be created for Management Traffic, Data traffic and Oracle RAC database inter-node private traffic.\n\n2. Right-click VLANs in the tree and click Create VLANs.\n\n3. Enter MGMT-VLAN for the name of the VLAN (for example, 809). This name will be used for traffic management.\n\n4. Keep the option \"Common/Global\" selected for the scope of the VLAN.\n\n5. Enter a VLAN ID for the management VLAN. Keep the sharing type as \"none\".\n\n6. Similarly create VLANs for Application data traffic (for example, 812) and Oracle Private traffic (192).\n\nVerification: Select LAN tab > LAN Cloud > VLANs. Open VLANs and all of the created VLANs are displayed. The right pane gives the details of all individual VLANs as shown in Figure 21.\n\nFigure 21 Details of Created VLANs\n\nCreating Uplink Ports Channels\n\nTo create uplink port channels to Nexus 5548 switches, perform the following steps:\n\n1. Select the LAN tab on the left of the Cisco UCS Manager window.\n\nNote Two port channels are created, one from fabric A to both Cisco Nexus 5548 switches and one from fabric B to both Cisco Nexus 5548 switches.\n\n2. Expand the \"Fabric A\" tree.\n\n3. Right-click on the \"Port Channels\" and click Create Port Channel.\n\n4. Enter \"13\" as the unique ID of the port channel.\n\n5. Enter \"Siebel_Po13\" as the name of the port channel.\n\n6. Click Next.\n\n7. Select ports 1/17 and 1/19 to be added to the port channel.\n\n8. Click >> to add the ports to the Port Channel.\n\n9. Click Finish to create the port channel.\n\n10. A pop-up message box appears, click OK to continue.\n\n11. In the left pane, click the newly created port channel.\n\n12. In the right pane under \"Actions\", choose Enable Port Channel option.\n\n13. In the pop-up box, click Yes, and then click OK to save changes.\n\n14. Expand the \"Fabric B\" tree.\n\n15. Right-click on the \"Port Channels\" and click Create Port Channel.\n\n16. Enter \"14\" as the unique ID of the port channel.\n\n17. Enter \"Siebel_Po14\" as the name of the port channel.\n\n18. Click Next.\n\n19. Select ports 1/17 and 1/19 to be added to the Port Channel.\n\n20. Click >> to add the ports to the Port Channel.\n\n21. Click Finish to create the port channel.\n\n22. A pop-up message box appears, click OK to continue.\n\n23. In the left pane, click the newly created port channel.\n\n24. In the right pane under \"Actions\", choose Enable Port Channel option.\n\n25. In the pop-up box, click Yes, and then click OK to save changes.\n\nVerification: Select LAN tab > LAN Cloud. On the Right Pane, select the LAN Uplinks and expand the Port channels listed as shown in the following Figure 22.\n\nNote In order for the Fabric Interconnect Port Channels to get enabled, the vpc needs to be configured first at Nexus 5548 Switches as described in Creation and Configuration of Virtual Port Channel (VPC).\n\nFigure 22 Details of Port channels\n\nCreating VSANs\n\nTo create VSANs, perform the following steps:\n\n1. Select the SAN tab at the top left of the Cisco UCS Manager window.\n\n2. Expand the SAN cloud tree.\n\n3. Right-click on the \"VSANs\" and click Create VSAN.\n\n4. Enter \"Siebel_VSAN\" as the VSAN name for Fabric A.\n\n5. Enter \"2\" as the VSAN ID.\n\n6. Enter \"2\" as the FCoE VLAN ID.\n\n7. Click OK to create the VSANs.\n\nVerification: Select SAN tab >SAN Cloud >VSANs on the left panel. The right panel displays the created VSANs as shown in the following Figure 23.\n\nFigure 23 Details of Created VSANs\n\nCreating Boot Policies\n\nDo not select any boot policy at this time. It must be done after creating LUNs in EMC VNX5500 storage system and establishing connectivity. To modify the Service Profile, see the section Modifying Service Profile for Boot Policy.\n\nCreating Service Profile Templates\n\nTo create service profile templates, perform the following steps:\n\n1. Select the Servers tab at the top left of the Cisco UCS Manager window.\n\n2. Select Service Profile Templates > root. In the right window, click Create Service Profile Template under the Actions tab.\n\n3. The Create Service Profile Template window appears.\n\na. Identify the Service Profile Template section.\n\n Enter the name of the service profile template as \"Cisco UCS-Oracle Siebel\".\n\n Select the type as \"Initial Template\".\n\n In the UUID section, select \"Siebel_UUID_Pool\" as the UUID pool.\n\n Click Next to continue to the next section.\n\nb. Storage Section\n\n Select \"RAID 1\" for the Local Storage field for local hard disk resiliency.\n\n Select the option \"Expert\" for the field \"How would you like to configure SAN connectivity\".\n\n In the WWNN Assignment field, select \"Siebel_WWNN_Pool\".\n\n Click Add button at the bottom of the window to add vHBAs to the template.\n\nNote We need to create four vHBAs and First pair of vHBA's will be used for SAN Boot LUN and Second pair of vHBA's will be used for Oracle Siebel Application purposes.\n\n The Create vHBA window appears. Ensure that the vHBA is \"vhba0\".\n\n In the WWPN Assignment field, select \"Siebel_WWPN_Pool_A\".\n\n Ensure that the Fabric ID is set to \"A\".\n\n In the \"Select VSAN\" field, select \"Siebel_VSAN\".\n\n Click OK to save changes.\n\n Click Add button at the bottom of the window to add vHBAs to the template.\n\n The Create vHBA window appears. Ensure that the vHBA is \"vhba1\".\n\n In the WWPN Assignment field, select \"Siebel_WWPN_Pool_B\".\n\n Ensure that the Fabric ID is set to \"B\".\n\n In the \"Select VSAN\" field, select \"Siebel_VSAN\".\n\n Click OK to save changes.\n\n Click the Add button at the bottom of the window to add vHBAs to the template.\n\n Similarly create \"vhba2\" (with Fabric ID A) and \"vhba3\" (with Fabric ID B)\n\n Ensure that both the vHBAs are created.\n\n Click Next to continue.\n\nc. Network Section\n\n Restore the default setting for \"Dynamic vNIC Connection Policy\" field.\n\n Select the option \"Expert\" for the field \"How would you like to configure LAN connectivity\".\n\n Click Add to add a vNIC to the template.\n\n The Create vNIC window appears. Enter the name of the vNIC as \"eth0\".\n\n Select the MAC address assignment field as \"Siebel_MAC_Pool_A\".\n\n Select Fabric ID as \"Fabric A\".\n\n Select appropriate VLANs (812) in the VLANs.\n\n Click OK to save changes.\n\n Click Add to add a vNIC to the template.\n\n The Create vNIC window appears. Enter the name of the vNIC \"eth1\".\n\n Select the MAC address assignment field as \"Siebel_MAC_Pool_B\".\n\n Select Fabric ID as \"Fabric B\".\n\n Select appropriate VLANs (812) in the VLANs.\n\n Click OK to add the vNIC to the template.\n\n Ensure that both the vHBAs are created.\n\n Click Next to continue.\n\nd. vNIC/vHBA Placement section\n\n Restore the default setting as \"Let System Perform Placement\" in the Select Placement field.\n\n Ensure that all the vHBAs are created.\n\n Click Next to continue.\n\ne. Server Boot Order section\n\n You need not select any boot. For boot policy creation and association of service profile, see section Creating Boot Policies.\n\nf. Maintenance Policy, Server assignment, and operation policy Section\n\n Select default settings for all these sections.\n\n Custom policies can be defined for each of the three cases, for instance, in operational policy one can disable `quiet boot' in the BIOS policy\n\n Click Finish to complete the creation of Service profile template.\n\nCreating Service Profile from the Template and associating it to a Blade\n\nTo create a service profile from the template and associating it to a blade, perform the following steps:\n\n1. Select the Servers tab at the top left of the Cisco UCS Manager window.\n\n2. Select Service Profile Templates > root > Sub-Organizations > Service Template Cisco UCS-Oracle Siebel.\n\n3. Click Create Service Profiles From Template in the Actions tab of the right pane of the window.\n\n4. Enter \"sb-as\" in the Naming Prefix text box and the number as \"1\"\n\n5. Click OK to create service Profile\n\n6. Select the created Service profile Servers > Service profiles > root > sb-as-1 and go to \"Change Service Profile Association\"\n\n7. Select \"Existing Server\" under the option \"Server Assignment\" and from the list shown\n\n8. Select the right server based on Chassis ID/Slot number.\n\n9. Click OK to associate the service profile to that blade. The successful association of the service profile is as shown in Figure 24.\n\nFigure 24 Successful Association of the Service Profile\n\nConfiguring the EMC VNX5500\n\nFigure 25 shows the disk layout carved on the EMC VNX5500 storage which is connected to the Cisco UCS system. It leverages the FAST Cache as well as FAST Virtual Pool and Unified Storage capabilities of the VNX5500 as well the EMC best practices to carve the LUNs required for this exercise.\n\n Oracle Siebel CRM data stored in Oracle database accessed randomly and requires fastest for read/write operations. For such access pattern a storage pool with SAS drives and Flash disks (SSD disks) are chosen with RAID level 5. SSD disks provide faster response time for Random access and tiered storage with SAS disks provides the necessary capacity.\n\n Oracle Database Log is sequential write intensive operation, hence SAS drives with RAID level 10 is chosen.\n\n For the Siebel File System where sequential writes are more than the reads (based on the workload designed in this exercise), SAS drives alone without Flash disks in the storage pool are used with RAID level 10. However based on the actual access pattern and I/O mix, appropriate drivers and RAID level can be chosen.\n\n For boot LUNs, since the I/O operations are mostly sequential, SAS drives with RAID level 1 is chosen.\n\nLocal hard disk partitions are used for OS level /swap and /tmp partitions in this exercise; however it was observed that they were not used during the test executions.\n\nFor Back up LUNs, NL SAS drives are chosen since speed is not very critical with RAID level 10.\n\nTable 2 provides Storage Pool/RAID Groups and Figure 25 shows the same.\n\nFigure 25 Disk Layout - EMC VNX5500\n\nTable 3 lists LUNs that were created for Oracle Siebel small setup:\n\nTable 4 lists LUNs that were created for Oracle Siebel Medium and Large setup:\n\nFrom the above table, it is clear that you need to choose an appropriate storage processor as the default owner, so that the IOs are evenly balanced.\n\nCreation of Storage Pools/RAID Groups\n\nTo create storage pools/RAID groups, perform the following steps:\n\n1. Login to the EMC VNX Unisphere to create storage pools.\n\n2. To create Storage Pool, click Storage > Storage Configuration > storage pools > Pools tab and the click Create. The \"create storage pool\" pop-up window appears.\n\na. Ensure that the Storage Pool type is \"Pool\".\n\nb. Enter an appropriate name for the storage pool name in the text box.\n\nc. Select appropriate RAID group from the drop-down list.\n\nd. Select the required disks from the disk selection popup window and the click OK.\n\n3. To create LUNs from the storage pool, right-click on the desired storage pool. A pop window \"Create LUN\" appears. In the General tab of Create LUN pop-up box.\n\na. Click General tab of the Create LUN window. Enter the required LUN size in the LUN properties text box.\n\nb. Enter the name for the LUN in the \"LUN Name\" text box.\n\nc. In the Advanced tab, ensure the right SP is chosen as \"Default owner\" as mentioned in Table 3 and Table 4 on Oracle Siebel Small and Medium/Large setups respectively.\n\nd. Ensure that the Database LUNs (Oracle data) are selected as \"Highest Available Tier\" and Application LUNs are selected as \"Lowest Available Tier\" in the Tiering Policy.\n\n4. To associate LUNs to the host, Navigate to Hosts > Storage Group and the click Create, A pop-up window \"Create Storage Group\" appears.\n\na. Enter an appropriate name in the \"Storage Group Name\" text box; click OK and then click Yes to confirm. Click LUNs tab, a pop-up window \"Storage Group properties\" appears.\n\nb. Select the LUN from the respective SPA / SPB and click Add in the \"Available LUNs\" to add the selected LUNs. In the \"Show LUN\" drop-down list, select the option \"All\" instead of \"Not in other storage groups\".\n\nNote The Host ID which is typically 0 for the first LUN attached to the storage group and this Host Id should match with Cisco UCS Manager Service Profile > Create Boot Policy > LUN ID for SAN boot as shown in Figure 26.\n\nFigure 26 Selecting a Storage Group Name in EMC Unisphere\n\n5. To create RAID Groups, click Storage > Storage Configuration > Storage pools > RAID Groups tab and click Create. A pop-up window \"create storage pool\" appears.\n\na. Ensure that the selected Storage Pool type is \"RAID Group\".\n\nb. Select the required disks from the \"Disk Selection\" popup window and click OK.\n\n6. To create LUNs from the storage pool, right-click on the desired storage pool. A pop window \"Create LUN\" appears. In the General tab of Create LUN pop-up box.\n\nc. Ensure that the selected Storage Pool type is \"RAID Group\"\n\nd. Enter the required LUN size in the\" LUN Properties\" text box.\n\ne. Enter the name of the LUN in the \"LUN Name\" text box.\n\nf. In the Advanced tab, ensure the right SP is chosen as \"Default owner\" as mentioned in Table 3 and Table 4 on Oracle Siebel Small and Medium/Large setups respectively.\n\nCreation of File System (NFS Share)\n\nTo create Oracle Siebel File System, perform the following steps:\n\n1. Login to the EMC VNX Unisphere to create a Storage Pool for file system requirements for Oracle Siebel.\n\n2. To create the Storage Pools, click Storage > Storage Configuration > Storage pools and the click Create. A pop-up window \"Create Storage Pool\" appears.\n\na. Ensure the option \"Storage Pool\" is selected in the window.\n\nb. Enter the name of the storage pool as \"Pool 1- Siebel File System\" in the Storage Pool Name text box and select RAID group as \"10\".\n\nc. Select the required disks from the \"Disk selection\" pop-up window and then click OK.\n\n3. To create LUNs from the created storage pool, right-click on storage pool. A new window \"Create LUN\" appears. Click the General tab in the \"Create LUN\" window.\n\na. Enter the required LUN size in the \"LUN properties\" text box.\n\nb. Enter the name of the LUN as \"SiebelFile System LUN\" in the \"LUN Name\" text box.\n\nc. Click the Advanced tab, ensure that the right SP is chosen as \"Default owner\" as mentioned in Table 3 and Table 4 on Oracle Siebel Small and Medium/ Large setups respectively.\n\nd. For the Tiering Policy, select the \"Auto tier\" option from the drop-down list. Since the file access workload implementation is not more than 15% of total workload.\n\ne. The created LUN must be added to the LUN in the default File Storage Group. For this, click Hosts > Storage Groups. Select the Storage Group named \" ~filestorage\" and click Connect LUNs and then click OK.\n\nNote The Host ID (also called HLU) must be greater than or equal to 16 for the LUN, otherwise it may not show up in Volumes list.\n\nFigure 27 shows creation of LUNs and addition of the created LUN to the default File Storage Group.\n\nFigure 27 Connecting LUNs in Storage group Properties Window\n\nf. To create disk volumes, click Storage > Rescan Storage Systems. The progress of the scanning process can be observed from System > Monitoring and Alerts > Background Tasks for File. To view the disk volumes, click Storage > Storage Configuration > Volumes.\n\ng. To create Meta Volume, click Storage > Storage Configuration > storage pools for pool and select the \"Pool 1 - Siebel File System\". From the Create menu, select \"Meta Volumes\" for \"create from\" field. Enter the Pool name as \"Siebel FS\" and select all the volumes listed below. Select the check-box \"Slice pool Volumes by Default\" and click OK. Once the process is complete a new Storage pool for file (with type as user pool) is created called \"Siebel FS\" as shown in Figure 28.\n\nFigure 28 Creating Siebel FS from Meta Volume\n\nh. To create File System, click Storage > Storage Configuration > File Systems and click Create. A pop-up window \"Create File System\" appears. Enter the appropriate name and size for the file system. Select \"Pool 1- Siebel File System\" option from the Storage Pool drop-down list. Click OK. This is shown in Figure 29.\n\nFigure 29 Creating Siebel File System from Storage Pool\n\ni. You can export the created File System. To export the created File System, click Storage > Shared Folders > NFS and select the path as \"/SiebelFileSystem\" and click Create. A pop-up window \"Create NFS Export\" appears. Enter the Path name and list of Host IPs that need to access this share. Click OK. This is shown in Figure 30.\n\nFigure 30 Exporting the Created FileSystem\n\nThis completes the file system creation on the EMC storage system.\n\nConfiguring the Nexus Switches\n\nTo configure the Nexus 5548 Switch, perform the following steps:\n\nSetting up the Nexus 5548 Switch\n\nTo setup the Nexus 5548 switch, perform the following steps for Cisco Nexus 5548 Switch A (Siebel_Nexus_Switch_A):\n\n1. After the initial boot and connection to the serial or console port of the switch, the NX-OS setup should automatically start.\n\n2. Enter \"yes\" to enforce secure password standards.\n\n3. Enter the password for the admin user.\n\n4. Enter the password a second time to commit the password.\n\n5. Enter \"yes\" to enter the basic configuration dialog.\n\n6. Create another login account (yes/no) [n]: \"Enter\".\n\n7. Configure read-only SNMP community string (yes/no) [n]: \"Enter\".\n\n8. Configure read-write SNMP community string (yes/no) [n]: \"Enter\".\n\n9. Enter the switch name as \"Siebel_Nexus_Switch_A\" \"Enter\".\n\n10. Continue with out-of-band (mgmt0) management configuration? (yes/no) [y]: \"Enter\".\n\n11. Mgmt0IPv4 address: \"10.104.xxx.xxx\". Enter\".\n\n12. Mgmt0IPv4 netmask: \"255.255.255.0\" Enter\".\n\n13. Configure the default gateway? (yes/no) [y]: \"Enter\".\n\n14. IPv4 address of the default gateway: \"10.104.xxx.xxx\" \"Enter\".\n\n15. Enable the telnet service? (yes/no) [n]: \"Enter\".\n\n16. Enable the ssh service? (yes/no) [y]: \"Enter\".\n\n17. Type of ssh key you would like to generate (dsa/rsa): rsa.\n\n18. Number of key bits <768-2048>: \"1024\" \"Enter\".\n\n19. Configure the ntp server? (yes/no) [n]: \"Enter\".\n\n20. Enter basic FC configurations (yes/no) [n]: \"Enter\".\n\n21. Would you like to edit the configuration? (yes/no) [n]: \"Enter\".\n\nNote Ensure to review the configuration summary before enabling it.\n\n22. Use this configuration and save it? (yes/no) [y]: \"Enter\".\n\n23. You may continue configuration from the console or using SSH. To use SSH, connect to Mgmt0 IP given in step 11.\n\n24. Log in as user \"admin\" with the password entered above.\n\nTo setup the Nexus 5548 switch, perform the following steps for Cisco Nexus 5548 Switch B (Siebel_Nexus_Switch_B):\n\n1. After the initial boot and connection to the serial or console port of the switch, the NX-OS setup should automatically start.\n\n2. Enter \"yes\" to enforce secure password standards.\n\n3. Enter the password for the admin user.\n\n4. Enter the password a second time to commit the password.\n\n5. Enter \"yes\" to enter the basic configuration dialog.\n\n6. Create another login account (yes/no) [n]: \"Enter\".\n\n7. Configure read-only SNMP community string (yes/no) [n]: \"Enter\".\n\n8. Configure read-write SNMP community string (yes/no) [n]: \"Enter\".\n\n9. Enter the switch name: \"Siebel_Nexus_Switch_B\" Enter\".\n\n10. Continue with Out-of-band (mgmt0) management configuration? (yes/no) [y]: \"Enter\".\n\n11. Mgmt0IPv4 address: \": \"10.104.xxx.xxx Enter\".\n\n12. Mgmt0IPv4netmask: \"255.255.255.0\" Enter\".\n\n13. Configure the default gateway? (yes/no) [y]: \"Enter\".\n\n14. IPv4 address of the default gateway: \"10.104.108.xxx\" Enter\".\n\n15. Enable the telnet service? (yes/no) [n]: \"Enter\".\n\n16. Enable the ssh service? (yes/no) [y]: \"Enter\".\n\n17. Type of ssh key you would like to generate (dsa/rsa): rsa.\n\n18. Number of key bits <768-2048>: \"1024 Enter\".\n\n19. Configure the ntp server? (yes/no) [n]: \"Enter\".\n\n20. Enter basic FC configurations (yes/no) [n]: \"Enter\".\n\n21. Would you like to edit the configuration? (yes/no) [n]: \"Enter\".\n\nNote Ensure to review the configuration summary before enabling it.\n\n22. Use this configuration and save it? (yes/no) [y]: \"Enter\".\n\n23. You may continue configuration from the console or using SSH. To use SSH, connect to Mgmt0 IP given in step 11.\n\n24. Log in as user \"admin\" with the password entered above.\n\nEnabling Nexus 5548 Switch Licensing\n\nTo enable appropriate Nexus 5548 switch licensing, perform the following steps for both Cisco Nexus 5548 A - (Siebel_Nexus_Switch_A), and Cisco Nexus 5548 B - (Siebel_Nexus_Switch_B) separately:\n\n1. Type \"config t\" to enter into the global configuration mode.\n\n2. Type \"feature lacp\".\n\n3. Type \"feature fcoe\".\n\n4. Type \"feature npiv\".\n\n5. Type \"feature vpc\".\n\n6. Type \"feature fport-channel-trunk\".\n\nNote FCoE feature needs to be enabled first before enabling npiv.\n\nVerification: The command \"show feature | include enabled\" should list the enabled features.\n\nSiebel_Nexus_Switch_A# sh feature | inc enabled\n\nassoc_mgr 1 enabled\n\nfcoe 1 enabled\n\nfex 1 enabled\n\nlacp 1 enabled\n\nlldp 1 enabled\n\nnpiv 1 enabled\n\nsshServer 1 enabled\n\ntelnetServer 1 enabled\n\nvpc 1 enabled\n\nConfiguration of Ports 29-32 as FC ports\n\nTo configure the ports 29-32 as FC ports, perform the following steps for both Cisco Nexus 5548 A - (Siebel_Nexus_Switch_A), and Cisco Nexus 5548 B - (Siebel_Nexus_Switch_B) separately:\n\n1. Type \"config t\" to enter into the global configuration mode.\n\n2. Type \"slot 1\".\n\n3. Type \"interface fc 1/29-32\".\n\n4. Type \"switchport mode F\".\n\n5. Type \"no shut\".\n\nVerification: The command \"show interface brief\" should list these interfaces as FC (Admin Mode \"F\").\n\nSiebel_Nexus_Switch_A# sh interface brief\n\nInterface Vsan AdminMode AdminTrunkMode Status SFP OperMode OperSpeed(Gbps)\n\n------------------------------------------------------------------------------------\n\nfc1/29 2 F on up swl F 4\n\nfc1/30 2 F on up swl F 4\n\nfc1/31 2 F on up swl F 4\n\nfc1/32 2 F on up swl F 4\n\nCreating VSAN and Adding FC Interfaces\n\nTo create VSAN and adding FC interfaces, perform the following steps for both Cisco Nexus 5548 A - (Siebel_Nexus_Switch_A), and Cisco Nexus 5548 B - (Siebel_Nexus_Switch_B) separately:\n\n1. Type \"config t\" to enter into the global configuration mode.\n\n2. Type \"vsan database\".\n\n3. Type \"vsan2 name UCS- Siebel\".\n\n4. Type \"vsan 2 interface fc1/29-32\".\n\n5. Type \"y\" on the \"Traffic on fc1/29 may be impacted. Do you want to continue? (y/n) [n]\".\n\n6. Similarly type \"y\" for fc1/30, fc1/31 and fc1/32 interfaces.\n\nVerification: The command \"show vsan membership\" should list fc1/29-32 under \"vsan 2\".\n\nSiebel_Nexus_Switch_A# show vsan membership\n\nvsan 2 interfaces:\n\nfc1/29 fc1/30 fc1/31 fc1/32\n\nCreating VLANs and Managing Traffic\n\nTo create necessary VLANs for example, VLAN 809 and managing data traffic for example, VLAN 812 - data traffic, perform the following steps for both Cisco Nexus 5548 A - (Siebel_Nexus_Switch_A), and Cisco Nexus 5548 B - (Siebel_Nexus_Switch_B) separately:\n\n1. Type \"config t\" to enter into the global configuration mode.\n\n2. From the global configuration mode, type \"vlan809\" and press \"Enter\".\n\n3. Type \"name MGMT-VLAN\" to enter a descriptive name for the VLAN.\n\n4. Type \"exit\".\n\n5. Type \"vlan812\".\n\n6. Type \"name Data-VLAN\".\n\n7. Type \"Interface ethernet1/1-20\" (make sure to choose the Ethernet interfaces where Fabric Interconnects are connected).\n\n8. Type \"switchport mode trunk\".\n\n9. Type \"switchport trunk allowed vlan 809,812\".\n\n10. Type \"exit\".\n\nVerification: The command \"show vlan\" should list the vlans and interfaces assigned to it. Or, the command \"show run interface <interface name> should show the configuration for a given interface or port channel.\n\nSiebel_Nexus_Switch_A# show vlan\n\nVLAN Name Status Ports\n\n---- -------------------------------- --------- ----------------\n\n809 VLAN0809 active Eth1/1,Eth1/2, Eth1/3, Eth1/4\n\nEth1/5,Eth1/6, Eth1/7, Eth1/8\n\nEth1/9,Eth1/10, Eth1/11, Eth1/12\n\nEth1/13,Eth1/14, Eth1/15, Eth1/16\n\nEth1/17,Eth1/18, Eth1/19, Eth1/20\n\nVLAN Name Status Ports\n\n---- -------------------------------- -------- ------------------\n\n812 VLAN0812 active Eth1/1, Eth1/2, Eth1/3, Eth1/4\n\nEth1/5, Eth1/6, Eth1/7, Eth1/8\n\nEth1/9, Eth1/10, Eth1/11, Eth1/12\n\nEth1/13, Eth1/14, Eth1/15, Eth1/16\n\nEth1/17,ETH1/18, Eth1/19, Eth1/20\n\nCreation and Configuration of Virtual Port Channel (VPC)\n\nTo create and configure the VPC, perform the following steps for both Cisco Nexus 5548 A - (Siebel_Nexus_Switch_A), and Cisco Nexus 5548 B - (Siebel_Nexus_Switch_B) separately:\n\n1. In the global configuration mode, type \"vpc domain 108\".\n\n2. Type \"role priority 1000\".\n\n3. Type \"peer-keepalive destination 10.x.x.x\". (This IP is the Siebel_Nexus_Switch_BMgmt IP)\n\n4. Type \"int port-channel 108\".\n\n5. Type \"switchport mode trunk\".\n\n6. Type \"switchport trunk allowed vlan45, 809-812\".\n\n7. Type \"vpc peer-link\".\n\n8. Type \"int ethernet 1/5\" (peer link port).\n\n9. Type \"switchport mode trunk\".\n\n10. Type \"switchport trunk allowed vlan 192, 809-812\".\n\n11. Type \"channel-group 108 mode active\".\n\n12. Type \"Exit\".\n\n13. Type \"int port-channel 109\".\n\n14. Type \"switchport mode trunk\".\n\n15. Type \"switchport trunk allowed vlan 192, 809-812\".\n\n16. Type \"vpc 109\".\n\n17. Type \"Exit\".\n\n18. Type \"int ethernet 1/1\".\n\n19. Type \"channel-group 109 mode active\".\n\n20. Type \"switchport mode trunk\".\n\n21. Type \"switchport trunk allowed vlan 192, 809-812\".\n\n22. Type \"Exit\".\n\n23. Type \"int ethernet 1/1\".\n\n24. Type \"channel-group 109 mode active\".\n\n25. Type \"switchport mode trunk\".\n\n26. Type \"switchport trunk allowed vlan 192, 809-812\".\n\n27. Type \"Exit\".\n\nVerification: \"show vpc\" command should list the vpc properties with vpc peer-link status as \"success\" and Consistency status as \"success\"\n\nSiebel_Nexus_Switch_A# show vpc\n\nLegend:\n\n(*) - local vPC is down, forwarding via vPC peer-link\n\nvPC domain id : 108\n\nPeer status : peer adjacency formed ok\n\nvPC keep-alive status : peer is alive\n\nConfiguration consistency status : success\n\nPer-vlan consistency status : success\n\nType-2 consistency status : success\n\nvPC role : secondary\n\nNumber of vPCs configured : 1\n\nPeer Gateway : Disabled\n\nDual-active excluded VLANs : -\n\nGraceful Consistency Check : Enabled\n\nvPC Peer-link status\n\n------------------------------------------------\n\nid Port Status Active vlans\n\n-- ---- ------ -----------------------------\n\n1 Po108 up 192,809-812\n\nvPC status\n\n------------------------------------------------------------------\n\nid Port Status Consistency Reason Active vlans\n\n------ ----------- ------ ----------- --------------------------\n\n109 Po109 up success success 192,809-812\n\nCreation of Zoneset and Zones\n\nTo create zoneset and zone, perform the following steps for Cisco Nexus 5548 Switch A - (Siebel_Nexus_Switch_A):\n\nNote As mentioned in Service profile template creation, the first pair of vHBAs (vHBA0 and vHBA1) are created for SAN boot process and Second pair of vHBAs (vHBA2 and vHBA3) are created for application. Hence each pair is zoned separately.\n\n1. From the global configuration mode, type \"zoneset name UCS-Siebel vsan 2\"\n\nNote The VSAN id should match to the VSAN Id Created in Fabric Interconnect.\n\n2. Type \"zone name ucs-sb-med-as1-boot-a\".\n\n3. Type \"member pwwn 20:00:00:25:b5:05:0A:0d\".\n\nNote This is WWPN of vHBA0 in the associated service profile of B200 blade.\n\n4. Type \"member pwwn 50:06:01:6f:3e:a0:05:68\".\n\n5. Type \"member pwwn 50:06:01:66:3e:a0:05:68\".\n\n6. Type \"exit\".\n\n7. Type \"zone name ucs-sb-med-as1-app-a\".\n\n8. Type \"member pwwn 20:00:00:25:b5:05:0B:0d\".\n\nNote This is WWPN of vHBA2 in the associated service profile of B200 blade.\n\n9. Type \"member pwwn 50:06:01:6f:3e:a0:05:68\".\n\n10. Type \"member pwwn 50:06:01:66:3e:a0:05:68\".\n\n11. Type \"exit\".\n\n12. Type \"Zoneset activate name UCS-Siebel vsan 2\".\n\n13. Type \"copy r s\".\n\nTo create zoneset and zone, perform the following steps for Cisco Nexus 5548 Switch B - (Siebel_Nexus_Switch_B):\n\n1. From the global configuration mode, type \"zoneset name UCS- Siebel vsan 2\"\n\nNote The VSAN id should match to the VSAN Id Created in Fabric Interconnect.\n\n2. Type \"zone name ucs-sb-med-as1-boot-b\".\n\n3. Type \"member pwwn 20:00:00:25:b5:05:0A:2d\".\n\nNote This is WWPN of vHBA0 in the associated service profile of B200 blade.\n\n4. Type \"member pwwn 50:06:01:6e:3e:a0:05:68\".\n\n5. Type \"member pwwn 50:06:01:67:3e:a0:05:68\".\n\n6. Type \"exit\".\n\n7. Type \"zone name ucs-sb-med-as1-app-b\".\n\n8. Type \"member pwwn 20:00:00:25:b5:05:0B:2d\".\n\nNote This is WWPN of vHBA2 in the associated service profile of B200 blade.\n\n9. Type \"member pwwn50:06:01:6e:3e:a0:05:68\".\n\n10. Type \"member pwwn50:06:01:67:3e:a0:05:68\".\n\n11. Type \"exit\".\n\n12. Type \"Zoneset activate name UCS-Siebel vsan 2\".\n\n13. Type \"copy r s\".\n\nSimilarly create Zones for the other blades in both Nexus switches. This can be verified by executing `\n\n`show zoneset active vsan 2' command in the Nexus switch.\n\nSiebel_Nexus_Switch_A# show zoneset active vsan 2\n\nzoneset name UCS-Siebel vsan 2\n\nzone name ucs-sb-med-as1-boot-a vsan 2\n\n* fcid 0x440016 [pwwn 20:00:00:25:b5:05:0a:0d]\n\n* fcid 0x4405ef [pwwn 50:06:01:6f:3e:a0:05:68]\n\n* fcid 0x4403ef [pwwn 50:06:01:66:3e:a0:05:68]\n\nzone name ucs-sb-med-as1-app-a vsan 2\n\n* fcid 0x440017 [pwwn 20:00:00:25:b5:05:0a:2d]\n\n* fcid 0x4405ef [pwwn 50:06:01:6f:3e:a0:05:68]\n\n* fcid 0x4403ef [pwwn 50:06:01:66:3e:a0:05:68]\n\nFigure 31 shows the zoning configuration:\n\nFigure 31 SAN Zoning Configuration\n\nCisco UCS Manager Service Profile update\n\nSince the Nexus switches are connected with the storage array and the host, you must modify the boot policy in the Service Profile to add the Storage Ports. Perform the following steps to modify the boot policy:\n\nModifying Service Profile for Boot Policy\n\nIn this setup, vhba0 and vhba1 are used for SAN Boot and the other two configured HBA's that is, vhba2 and vhba3 are for Oracle Siebel application server installation. Storage SAN WWPN ports will be connected in the boot policy as:\n\nvhba0\n\n Storage Port SP-B0 Primary Target - 50:06:01:6f:3e:a0:05:68\n\n Storage Port SP-A0 Secondary Target - 50:06:01:66:3e:a0:05:68\n\nvhba1\n\n Storage Port SP-B1 Primary Target - 50:06:01:6e:3e:a0:05:68\n\n Storage Port SP-A1 Secondary Target - 50:06:01:67:3e:a0:05:68\n\nTo modify the Service Profile for boot policy, perform the following steps:\n\n1. Login to the Cisco UCS Manager. Click Servers tab > Policies > Boot Policies and then click Add. A pop-up window \"Create Boot Policy\" appears.\n\n2. Enter the name as \"UCS-Siebel\" in the \"Name\" text box and in the Description text box enter \"for Siebel blades\" and ensure that the check box \"Reboot on Boot Order Change\" is checked.\n\n3. Add the first target as CD-ROM, as this will enable you to install Operating System through KVM Console.\n\n4. Click Add SAN Boot on the vHBAs section; in the \"Add SAN Boot\" pop-up window, type \"vHBA0\" and select the type as Primary and click OK. This will be the SAN Primary Target.\n\n5. Click Add SAN Boot Target to add a target to the SAN Boot Primary in the vHBAs window. In the \"Add SAN Boot Target\" pop-up window, type \"0\" in the \"Boot Target LUN\". Enter \"50:06:01:6e:3e:a0:05:68\" in the \"Boot Target WWPN\" and select the type as \"Primary\" and then click OK.\n\n6. To add another target to the SAN Boot Primary, click Add to add another SAN Boot Target in the vHBAs window; in the \"Add SAN Boot Target\" pop-up box, type \"0\" in the Boot Target LUN; type \"50:06:01:67:3e:a0:05:68\" in the Boot Target WWPN and ensure that the type selected is \"Primary\" and click OK.\n\nNote These WWPNs are from storage SPB0/ SPA0 ports. For more details, see: SAN Zoning Configuration, Figure 31.\n\n7. Similarly for the SAN Secondary Target, click \"Add SAN Boot\" in the vHBAs window; in the \"Add SAN Boot\" pop-up window, type \"vHBA1\" and select the type as \"Secondary\" and then click OK.\n\n8. Click Add SAN Boot Target to add a target to the SAN Boot Primary in the vHBAs window. In the \"Add SAN Boot Target\" pop-up window, type \"0\" in the \"Boot Target LUN\". Enter \"50:06:01:6e:3e:a0:05:68\" in the \"Boot Target WWPN\" and select the type as \"Secondary\" and then click OK.\n\n9. To add another target to the SAN Boot Primary, click Add to add another SAN Boot Target in the vHBAs window; in the \"Add SAN Boot Target\" pop-up box, type \"0\" in the Boot Target LUN; type \"50:06:01:67:3e:a0:05:68\" in the Boot Target WWPN and ensure that the type selected is \"Secondary\" and click OK.\n\nNote These WWPNs are from storage SPB1 / SPA1 ports. For more details, see: SAN Zoning Configuration, Figure 31.\n\n10. Click Save Changes to save all the settings. The Boot Policy window in Cisco UCS Manager is as shown in Figure 32.\n\nFigure 32 Boot Policy in Service Profile\n\n11. To add this boot policy to the Service Profile, click Servers tab > Service Profiles > root > sb-as-1. Select the Boot Order on the right pane and click Modify Boot Policy. A pop-up window \"Modify Boot Policy\" appears. Select the newly created Boot Policy \"UCS-Siebel\" and click OK. This will reboot the blade, as \"Reboot on Boot order change\" is enabled in the Boot policy.\n\nUpdate the other service profiles in similar way with the boot policy \"UCS-Siebel\" to boot from the SAN after creating necessary LUNs / Storage groups in Storage array and Zones in Nexus Switches.\n\nHost - Storage Connectivity\n\nTo establish the Host connectivity, you need to connect the host at the EMC VNX5500 array.\n\nConnecting Storage to the Host\n\nSince the zones are configured in the Cisco Nexus switches with the Host HBA WWPNs, they will appear in the EMC VNX5500 Unisphere.\n\nTo connect storage to the host, perform the following steps:\n\n1. Login to the EMC VNX Unisphere, click Hosts> Connectivity Status under \"Host management\" on the right side of the window. A pop-u"
    }
}
{
    "id": "dbpedia_3567_3",
    "rank": 39,
    "data": {
        "url": "https://arxiv.org/html/2405.10288v2",
        "read_more_link": "",
        "language": "en",
        "title": "Timeline-based Sentence Decomposition with In-Context Learning for Temporal Fact Extraction",
        "top_image": "",
        "meta_img": "",
        "images": [
            "https://arxiv.org/html/x1.png",
            "https://arxiv.org/html/x2.png",
            "https://arxiv.org/html/x3.png"
        ],
        "movies": [],
        "keywords": [],
        "meta_keywords": [
            ""
        ],
        "tags": null,
        "authors": [],
        "publish_date": null,
        "summary": "",
        "meta_description": "",
        "meta_lang": "en",
        "meta_favicon": "",
        "meta_site_name": "",
        "canonical_link": null,
        "text": "Jianhao Chen1, Haoyuan Ouyang1, Junyang Ren1, Wentao Ding2, Wei Hu1, Yuzhong Qu1\n\n1State Key Laboratory for Novel Software Technology, Nanjing University, China\n\n2State Key Laboratory of General Artificial Intelligence\n\nBeijing Institute for General Artificial Intelligence (BIGAI), China\n\n{jh_chen, hyouyang, jyren}@smail.nju.edu.cn\n\ndingwentao@bigai.ai, {whu,yzqu}@nju.edu.cn\n\nAbstract\n\nFacts extraction is pivotal for constructing knowledge graphs. Recently, the increasing demand for temporal facts in downstream tasks has led to the emergence of the task of temporal fact extraction. In this paper, we specifically address the extraction of temporal facts from natural language text. Previous studies fail to handle the challenge of establishing time-to-fact correspondences in complex sentences. To overcome this hurdle, we propose a timeline-based sentence decomposition strategy using large language models (LLMs) with in-context learning, ensuring a fine-grained understanding of the timeline associated with various facts. In addition, we evaluate the performance of LLMs for direct temporal fact extraction and get unsatisfactory results. To this end, we introduce TSDRE , a method that incorporates the decomposition capabilities of LLMs into the traditional fine-tuning of smaller pre-trained language models (PLMs). To support the evaluation, we construct ComplexTRED, a complex temporal fact extraction dataset. Our experiments show that TSDRE achieves state-of-the-art results on both HyperRED-Temporal and ComplexTRED datasets.\n\n\\UseTblrLibrary\n\nbooktabs\n\nTimeline-based Sentence Decomposition with In-Context Learning for Temporal Fact Extraction\n\nJianhao Chen1, Haoyuan Ouyang1, Junyang Ren1, Wentao Ding2, Wei Hu1, Yuzhong Qu1 1State Key Laboratory for Novel Software Technology, Nanjing University, China 2State Key Laboratory of General Artificial Intelligence Beijing Institute for General Artificial Intelligence (BIGAI), China {jh_chen, hyouyang, jyren}@smail.nju.edu.cn dingwentao@bigai.ai, {whu,yzqu}@nju.edu.cn\n\n1 Introduction\n\nAcquiring knowledge has long been a fundamental challenge in the field of artificial intelligence. Typically, the acquired knowledge is stored in knowledge graphs (KGs) as triples, comprising a head entity, a relation, and a tail entity. Recently, there has been a significant surge in the need Chen et al. (2023); Liu et al. (2023); Ding et al. (2022); Xu et al. (2023); Li et al. (2022); Mezni (2022); Xiong et al. (2024) for acquiring temporal facts. For instance, users may seek information from the KGs, such as ‚ÄúWhen did Michael Jordan win the NBA Finals MVP?‚Äù. However, traditional triples like (NBA Finals MVP, winner, Michael Jordan) cannot meet such needs due to the lack of time dimension. This can be solved by adding time information like (Point In Time, 1996) to the triple. A temporal fact can be formalized as a quintuple (eh‚Å¢e‚Å¢a‚Å¢dsubscriptùëí‚Ñéùëíùëéùëëe_{head}italic_e start_POSTSUBSCRIPT italic_h italic_e italic_a italic_d end_POSTSUBSCRIPT, rùëüritalic_r, et‚Å¢a‚Å¢i‚Å¢lsubscriptùëíùë°ùëéùëñùëôe_{tail}italic_e start_POSTSUBSCRIPT italic_t italic_a italic_i italic_l end_POSTSUBSCRIPT, qùëûqitalic_q, tùë°titalic_t), where the time qualifier qùëûqitalic_q and time value tùë°titalic_t indicate the time dimension information of the triple.\n\nAlthough many works Wang et al. (2011); Liu et al. (2021); Chia et al. (2022) have explored the task of temporal fact extraction, the challenge of establishing correspondence between time and triples remains unresolved. Figure 1 shows a difficult example of temporal fact extraction. It can be seen that the difficulty of this example mainly comes from the use of ‚Äúthe other\" to imply that the other two players also won three awards in one year. Besides, natural language expressions are typically succinct, with \"Michael Jordan in 1996 and 1998\" alone expressing six temporal facts. Existing methods cannot handle this situation without explicitly stating what event occurred at a certain point in time, leading to suboptimal performance in addressing complex narratives with interwoven timelines.\n\nTo address this gap, we propose a timeline-based sentence decomposition strategy. Our strategy involves breaking down sentences according to their timelines to capture the temporal dimensions of facts, ensuring a fine-grained understanding of the timeline associated with various facts. In the past, sentence decomposition often required a large amount of training corpus. But now, the in-context learning capability of large language models (LLMs) empowers us to perform timeline-based sentence decomposition without training corpus.\n\nTo the best of our knowledge, we are the first to investigate the application of LLMs for temporal fact extraction tasks. We conduct an evaluation to assess the performance of employing LLMs directly for temporal fact extraction. However, it is unsatisfactory that the performance of LLMs does not surpass the traditional approach of fine-tuning smaller PLMs. To enhance the performance of temporal fact extraction methods, we come up with the idea of combining the surprising sentence decomposition capability of LLMs with the traditional way of fine-tuning small pre-trained language models. Experiments show that our method based on the combining idea achieves SOTA results.\n\nIn addition to research on extraction methods, there are currently few benchmarks Chia et al. (2022); Wang et al. (2012) for temporal fact extraction, and they do not pay much attention to sentences involving complex time related narratives (we call them complex sentences for short). To this end, we construct a temporal fact extraction dataset composed of complex sentences for evaluation.\n\nIn summary, our main contributions in this paper are outlined as follows:\n\n1.\n\nWe summarize the unique challenge of the temporal fact extraction task and propose a timeline-based sentence decomposition method on natural language using in-context learning enhanced by human feedback.\n\n2.\n\nWe propose timeline-based sentence decomposition (TSD) for the temporal fact extraction task. Evaluation results demonstrate that TSD indeed helps models understand the correspondence between events and time.\n\n3.\n\nWe conduct evaluations of the methods that utilize LLMs in the task of temporal extraction. Moreover, we build a novel dataset ComplexTRED, which consists of 19,148 complex sentences with multiple time expressions or temporal facts.\n\n2 Related Work\n\n2.1 Temporal Fact Extraction\n\nTemporal fact extraction methods are mainly divided into two categories: pattern-based methods and deep learning-based methods.\n\nPattern-based methods\n\nExisting work attempts to treat relationships and qualifiers as a whole to mine corresponding patterns in text, or use sequence annotation and classification methods. T-Yago Wang et al. (2010) and a similar study Kuzey and Weikum (2012) extract temporal instances from semi-structured data like Wikipedia‚Äôs Infoboxes, Categories, and Lists, limiting coverage to freely available text. Pravda Wang et al. (2011) uses textual patterns to represent candidate facts and labels them through a graph-based label propagation algorithm. Liu et al. (2021) applies the idea of distant supervision, leveraging existing temporal facts to learn corresponding patterns from web text and subsequently applying them to the extraction process.\n\nDeep learning-based methods\n\nCubeRE Chia et al. (2022) first employs a sequence labeling approach to identify entities and time, followed by classification of the relations and qualifiers between them.\n\nPrevious Studies have not fully addressed the challenges posed by complex sentences in temporal fact extraction. Moreover, we are the first to explore the application of LLMs for the task of temporal fact extraction.\n\n2.2 Sentence Decomposition\n\nSentence decomposition is a common method used for tasks in the NLP field. From a technical perspective, sentence decomposition can be roughly divided into two categories: supervised learning methods Huang et al. (2023) and rule-based methods Hu et al. (2021). Supervised learning requires sufficient training corpus, while rule-based methods are labor-intensive and often suffer from poor coverage issues. In the era of large models, we explore using in-context learning for sentence decomposition, which does not require task-specific training data and has sufficient coverage.\n\n2.3 In-Context Learning\n\nLarge language models, exemplified by the GPT series Brown et al. (2020); OpenAI (2023) and the LlaMA Touvron et al. (2023a, b) family, are known to have impressive in-context learning abilities. These LLMs have been proven to be able to solve a completely new problem with a small number of examples without the need for task-specific training leading to a surge in exploration within the domain of in-context learning.\n\nGenerally, research on in-context learning can be categorized into two main areas. The first focuses on the strategy for selecting examples Li et al. (2023) to enhance performance, while the second delves into the interpretability aspects Han et al. (2023) of in-context learning. Our study focuses on example selection. We iteratively constructed demonstrative examples for task scenarios without training corpus. Moreover, we incorporate human feedback to guide the model and prevent common errors.\n\n3 Preliminary\n\n3.1 Problem Formulation\n\nGiven an input sentence of nùëõnitalic_n words sùë†sitalic_s = {x1subscriptùë•1x_{1}italic_x start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT, x2subscriptùë•2x_{2}italic_x start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT, ‚Ä¶, xnsubscriptùë•ùëõx_{n}italic_x start_POSTSUBSCRIPT italic_n end_POSTSUBSCRIPT}, the objective of the temporal fact extraction task is to extract all existing temporal facts in sùë†sitalic_s. Formally, a temporal fact is represented as (eh‚Å¢e‚Å¢a‚Å¢dsubscriptùëí‚Ñéùëíùëéùëëe_{head}italic_e start_POSTSUBSCRIPT italic_h italic_e italic_a italic_d end_POSTSUBSCRIPT, rùëüritalic_r, et‚Å¢a‚Å¢i‚Å¢lsubscriptùëíùë°ùëéùëñùëôe_{tail}italic_e start_POSTSUBSCRIPT italic_t italic_a italic_i italic_l end_POSTSUBSCRIPT, qùëûqitalic_q, tùë°titalic_t). An entity eùëíeitalic_e is a consecutive span of words where eùëíeitalic_e = {xisubscriptùë•ùëñx_{i}italic_x start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT, xi+1subscriptùë•ùëñ1x_{i+1}italic_x start_POSTSUBSCRIPT italic_i + 1 end_POSTSUBSCRIPT, ‚Ä¶, xjsubscriptùë•ùëóx_{j}italic_x start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT}, i,j‚ààùëñùëóabsenti,j\\initalic_i , italic_j ‚àà {1111, ‚Ä¶, nùëõnitalic_n}. rùëüritalic_r represents the relation between head entity eh‚Å¢e‚Å¢a‚Å¢dsubscriptùëí‚Ñéùëíùëéùëëe_{head}italic_e start_POSTSUBSCRIPT italic_h italic_e italic_a italic_d end_POSTSUBSCRIPT and tail entity et‚Å¢a‚Å¢i‚Å¢lsubscriptùëíùë°ùëéùëñùëôe_{tail}italic_e start_POSTSUBSCRIPT italic_t italic_a italic_i italic_l end_POSTSUBSCRIPT. r‚ààRùëüùëÖr\\in Ritalic_r ‚àà italic_R, where RùëÖRitalic_R is the predefined set of relation labels. The qualifier qùëûqitalic_q and the time value tùë°titalic_t indicates the time dimension of the relation triplet (eh‚Å¢e‚Å¢a‚Å¢dsubscriptùëí‚Ñéùëíùëéùëëe_{head}italic_e start_POSTSUBSCRIPT italic_h italic_e italic_a italic_d end_POSTSUBSCRIPT, rùëüritalic_r, et‚Å¢a‚Å¢i‚Å¢lsubscriptùëíùë°ùëéùëñùëôe_{tail}italic_e start_POSTSUBSCRIPT italic_t italic_a italic_i italic_l end_POSTSUBSCRIPT). q‚ààQùëûùëÑq\\in Qitalic_q ‚àà italic_Q, where QùëÑQitalic_Q is the predefined set of qualifier labels.\n\n3.2 Complex Sentence in Temporal Fact Extraction\n\nIn this paper, we delineate the distinctions between simple and complex sentences in the context of temporal fact extraction. Prior research has shown a lack of emphasis on complex sentences. However, this paper shifts its focus to the temporal fact extraction of complex sentences.\n\nA simple sentence comprises only one time element and one temporal fact, presenting relatively lower difficulty in relation extraction and time selection. Example 1 provides a concrete instance of a simple sentence in the context of temporal fact extraction.\n\nExample 1.\n\nPeter Whittle was elected a fellow of the Royal Society in 1981 .\n\nExtracted Temporal Facts:\n\n(Peter Whittle, member of, Royal Society, start time, 1981)\n\nOn the other hand, a complex sentence involves two or more time elements or two or more facts, often introducing challenges in time selection or relation extraction. Understanding the correspondence between time and facts becomes more challenging in the presence of multiple time references compared to a situation with a single time reference. Figure 1 has shown a complex sentence with more than two time elements.\n\nExample 2 provides another concrete instance of a complex sentence in the context of temporal fact extraction.\n\nExample 2.\n\n20 November 1883, Jules Ferry succeeds Challemel-Lacour as Minister of Foreign Affairs.\n\nExtracted Temporal Facts:\n\n(Jules Ferry, position held, Minister of Foreign Affairs, start time, 20 November 1883)\n\n(Challemel-Lacour, position held, Minister of Foreign Affairs, end time, 20 November 1883)\n\n(Jules Ferry, replaces, Challemel-Lacour, point in time, 20 November 1883)\n\nAlthough there is only one point in time in Example 2, this sentence contains three temporal facts. This is mainly because the word \"succeeds\" expresses the relationship of succession at this moment. Implicit expressions such as these imply the connection of events in the time dimension, which brings challenges to the complete extraction of temporal facts.\n\n4 Method\n\nIn this section, we begin by presenting two direct approaches to leverage LLMs for temporal fact extraction. Following that, we introduce our extraction methods, encompassing a timeline-based sentence decomposition strategy and the fine-tuning of generative models using decomposition results for training.\n\n4.1 In-Context Learning with ChatGPT3.5 and Fine-tuning Open-Source LLM\n\nWe try to employ LLMs directly for temporal fact extraction through in-context learning and fine-tuning.\n\nSpecifically, we apply in-context learning to ChatGPT3.5. To construct the prompt, we first give a task description: Extract all the quintuples [subject, relation, object, qualifier, time point] from the input text, followed by the specified relationship list and qualifier list. We then select 48 examples at random from the train set to ensure that all relations and qualifiers have at least one-shot examples. The complete prompt is more than 2,000 tokens and will be shown in Appendix E.\n\nWe also try to LoRA fine-tune Llama2 (7B). We still give Llama2 the task description as the instruction to make Llama2 better understand the task.\n\n4.2 Timeline-based Sentence Decomposition\n\nTimeline-based sentence decomposition can be used to understand and process texts containing temporal information. It helps organize and present the timeline in sentences, making it easier for models to understand and follow the development of events.\n\nAn example of decomposition is as follows:\n\nText: Shaquille O‚ÄôNeal is one of only three players to win NBA MVP, All-Star game MVP, and Finals MVP awards in the same year (2000); the other players are Willis Reed in 1970 and Michael Jordan in 1996 and 1998.\n\nTime: [‚Äò2000‚Äô, ‚Äò1970‚Äô, ‚Äò1996‚Äô, ‚Äò1998‚Äô]\n\nDecomposition: 2000: Shaquille O‚ÄôNeal is one of only three players to win NBA MVP, All-Star game MVP, and Finals MVP awards in the same year. 1970: Willis Reed won NBA MVP, All-Star game MVP, and Finals MVP awards in the same year. 1996: Michael Jordan won NBA MVP, All-Star game MVP, and Finals MVP awards in the same year. 1998: Michael Jordan won NBA MVP, All-Star game MVP, and Finals MVP awards in the same year.\n\nDue to the absence of annotated data, we consider leveraging the in-context learning capability of LLMs to accomplish the decomposition task. Before starting in-context learning, we use SUTime Chang and Manning (2012) to identify time expressions in sentences, as specialized tools excel in time recognition compared to large LLMs. Without even a decomposition example, we iteratively construct the prompt for in-context learning. First, we only give ChatGPT3.5 the task description and one test sentence:\n\nInstruction: First, I will give you a Text, and secondly, I will give you the time contained in the Text. You need to sort out the events that occurred at each time I provided based on the Text. The Decomposition format is required to contain a time point and corresponding events in each sentence. Each sentence contains the complete elements of the event, such as subject, predicate, and object.\n\nWe undertake this exploration to understand the output format preferences of ChatGPT3.5. This knowledge will guide us in uncovering the potential of the model to achieve optimal performance by aligning with these preferences. Subsequently, we make slight modifications to the output to generate a demonstrative example that is both accurate and in line with the model‚Äôs preferred output format. After several iterations like this, we get the demonstrative examples needed for in-context learning.\n\nHuman Feedback Enhanced In-Context Learning\n\nWe find that even though we have shown ChatGPT3.5 high-quality demonstrative examples, it still makes mistakes when outputting. We add negative examples in the prompt to help the model avoid common mistakes. We carefully select the examples that we believe are representative of the mistakes made by ChatGPT3.5. Subsequently, we add human feedback to point out the mistakes and provide corrected answers. The complete prompt will be shown in Appendix E.\n\n4.3 Fine-tuning Models with Timeline-based Sentence Decomposition\n\nWe combine the natural language understanding and reasoning capabilities emerging from very large LMs with smaller LMs fine-tuned for specific tasks. Specifically, we fine-tune generative language models (Llama2 and Flan-T5) with timeline-based sentence decomposition. We splice the text in the dataset with the decomposition results generated by ChatGPT3.5 as a new input for training. The following is an example of the input we provide for models:\n\nInput: Text: Lamberto Visconti di Eldizio ( died 1225 ) was the Judge of Gallura from 1206 , when he married the heiress Elena , to his own death . Decomposition: 1225: Lamberto Visconti di Eldizio passed away and ended serving as the Judge of Gallura. 1206: Lamberto Visconti di Eldizio became the Judge of Gallura and married the heiress Elena.\n\nFigure 2 illustrates the distinction between training directly and training with the utilization of temporal-based sentence decomposition (TSD) information in the framework. We name the method of fine-tuning Flan-T5 using TSD as TSDRE.\n\n5 ComplexTRED: A Complex Temporal Fact Extraction Dataset\n\nThere are very few temporal fact extraction datasets available to date. Pravda Wang et al. (2012) lacks annotation labels and therefore cannot be used for supervised learning. The Wiki-People Dataset Liu et al. (2021) is not open source. HyperRED Chia et al. (2022) is a hyper-relational fact extraction dataset, 48% of which are temporal facts. However, many samples in HyperRED are overly simplistic and insufficient to depict the potential difficulties in complex practical scenarios. Specifically, the majority of sentences in HyperRED consist solely of a single temporal expression, and their respective extraction results comprise a singular temporal fact.\n\nWe need a complex temporal fact extraction dataset to evaluate our method and train existing models on their ability to extract temporal facts from complex temporal sentences. Below we will introduce data collection and dataset statistics.\n\n5.1 Data Collection\n\nIt is very challenging to construct a large-scale, diverse complex temporal fact extraction dataset. The two major difficulties are the number of complex temporal sentences and high-quality annotated labels. In order to collect enough data, we use two methods to obtain data: using distant supervision to obtain the alignment of web text and temporal fact in KG, and manually correcting the samples in HyperRED. We will introduce our means of controlling the quality of the dataset in the introduction of each method.\n\nDistant Supervision\n\nWe collect the introduction sections of 343,603 DBpedia articles, the full text of 401,796 Wikipedia articles, and 3,002,373 Wikidata temporal facts for alignment. Specifically, we use DBpedia Spotlight Mendes et al. (2011) for entity linking and SUTime Chang and Manning (2012) to extract time entities from DBpedia and Wikidata articles. In distant supervision, if a sentence contains the head entity, tail entity and temporal entity of a temporal fact, then we align the fact to the sentence.\n\nDistant supervision encounters two significant challenges: noise and incomplete facts (The sentence may express facts that are not in the knowledge graph). However, by aligning the two entities plus time, the noise problem is greatly mitigated. To tackle the incomplete facts problem, we leverage ChatGPT to complete the facts contained in the sentence. Subsequently, we organize 50 computer science undergraduate students to manually label the correctness of the added facts by ChatGPT3.5. Finally, we obtain about 17,000 complex sentences through distant supervision.\n\nManual Correction from HyperRED\n\nSamples in HyperRED are alignment with Wikipedia text and Wikidata data, which are from the same source as our samples. Therefore, we correct the labels of some complex sentences from HyperRED and absorb them into our dataset. We first select about 7,000 sentences in HyperRED that contain more than or equal to two time expressions as candidates. Then we organize 70 computer science undergraduates to correct the labels of each sentence, and the corrected samples will be added to our dataset. Finally, we get 2,589 corrected samples.\n\n5.2 Dataset Statistics\n\nTo ensure that the train set, dev set, and test set data adhere to an independent and identical distribution, we employ stratified sampling on the original data based on the relation types. Our goal is to maintain an 8:1:1 ratio for each relation type in the train, dev, and test set, respectively. The resulting splitting of the dataset is presented in Table 1. We manually check all samples in the dev and test set to ensure the dataset‚Äôs quality.\n\nThe analysis of the sample is shown in the Table 2. ComplexTRED significantly outperforms HyperRED-Temporal in terms of sentence length, temporal expressions, and temporal facts contained per sentence, which reflects the difficulty of the dataset to a certain extent.\n\n6 Experiments\n\n6.1 Experimental Setup\n\nDatasets\n\nWe assess temporal fact extraction using HyperRED Chia et al. (2022), a publicly available benchmark, and ComplexTRED, a dataset specifically crafted by us for complex temporal fact extraction. We select samples in HyperRED whose labels are temporal facts, delete the relations that have little correlation with time, and finally form HyperRED-Temporal as the evaluation dataset. The statistics of the two datasets are shown in Table 1.\n\nCompared Methods\n\nWe compare our approach with the SOTA extraction method reported on CubeRE Chia et al. (2022) and several baseline methods. We evaluated the performance of directly employing LLMs for temporal fact extraction, including LoRA fine-tuning the 7B version of Llama2 Touvron et al. (2023b) and naive in-context learning with ChatGPT3.5. We also tried to leverage our Timeline-based Sentence Decomposition (TSD) to enhance LoRA fine-tuned Llama2.\n\nIn addition to directly employing LLMs, we also evaluated the performance of Flan-T5-Large Chung et al. (2022), a relatively smaller pre-trained language model (PLM). We transplant the method of Wadhwa et al. (2023) on relation extraction (triple extraction) to temporal fact extraction, which trains Flan-T5 with LLM-generated explanations. We use ChatGPT3.5 to generate the explanations required for training instead of GPT-3 in the Wadhwa et al. (2023), because ChatGPT3.5 has not yet come out when Wadhwa et al. (2023) was published and we believe ChatGPT3.5 performs better than GPT-3.\n\nEvaluation metrics\n\nWe report the precision (denoted as PùëÉPitalic_P), recall (denoted as RùëÖRitalic_R), and F1subscriptùêπ1F_{1}italic_F start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT score of the evaluation results. When calculating metrics, we employ an exact match (at string level) approach without following Wadhwa et al. (2023)‚Äôs method of utilizing manual assessment for the outputs of large-scale models.\n\n6.2 Main results\n\nWe first report the results of methods directly based on LLMs, and subsequently, we report the results of approaches that involve combining LLMs and smaller PLMs.\n\nLLMs results.\n\nBased on Table 3, it is evident that both Fine-tuned open-source LLMs and in-context learning LLMs do not perform satisfactorily on both datasets. Among them, in-context ChatGPT3.5 consistently gets the lowest F1subscriptùêπ1F_{1}italic_F start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT score. It is reasonable to achieve such performance without training. HyperRED-Temporal includes 45 relation types and 4 qualifier types, while ComplexTRED includes 40 relation types and 4 qualifier types. Despite providing detailed task settings for ChatGPT3.5, it is expected to be exceedingly challenging to provide accurate answers across all 180 (45*4)/160 (40*4) categories based solely on a few-shot examples. Fine-tuned Llama2 (7B) performs better than in-context ChatGPT3.5, but its F1subscriptùêπ1F_{1}italic_F start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT score still falls considerably short of CubeRE. This could be attributed to the insufficient training samples to effectively support such a large language model, leading to suboptimal fitting. Additionally, we conducted experiments using our decomposition method to enhance Llama2 training. This resulted in Llama2 achieving an 11-point F1subscriptùêπ1F_{1}italic_F start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT score improvement on HyperRED-Temporal, and a 9-point F1subscriptùêπ1F_{1}italic_F start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT score improvement on ComplexTRED. This result indicates that integrating our decomposition method into training enables the model to better learn the relationships between sentence features and temporal facts. Another possible reason we speculate is that high-quality decomposition results are suitable as training data and make the training of Llama2 more sufficient.\n\nResults of combining LLMs and smaller PLMs.\n\nFlan-T5 (Large) achieved surprising results on both datasets. We believe that compared to LLMs, smaller PLMs are easier to fine-tune and fit when the training data is insufficient. However, enhancing Flan-T5 with ChatGPT3.5-generated explanation resulted in a tiny drop in F1subscriptùêπ1F_{1}italic_F start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT on both datasets. This method may be disadvantageous under exact match, which we have mentioned in the evaluation metrics in Section 6.1. Another reason we believe is that the temporal fact extraction task is inherently more challenging to interpret compared to the fact (triple) extraction task. Finally, our method TSDRE, which enhances Flan-T5 with timeline-based sentence decomposition (TSD), achieves state-of-the-art results on both datasets. We achieve remarkable results by combining the strengths of both large and small LMs: Large LMs excel at effectively organizing timelines in natural language, while small LMs prove more adept at precise fine-tuning for specific tasks.\n\n6.3 Decomposition Quality\n\nWe randomly select 100 sentences and invite three experts to evaluate our decomposition results of these sentences. We still use precision and recall as evaluation metrics. The goal of our decomposition is to divide different events in the text into different points in time. In this scenario, True Positive refers to the count of events correctly classified at their respective time points in the prediction results. False Positive indicates the count of events erroneously classified at the wrong time point. False Negative represents the count of events that were not predicted. It must be pointed out that the relatively ambiguous aspect is that opinions differ among individuals regarding what should be considered an event in natural language text.\n\nTable 4 shows the scores of manual evaluation. Overall, our decomposition results surpass 90 in both precision and recall metrics, indicating that we have acquired high-quality auxiliary information for training. This establishes a solid foundation for enhancing model performance. Moreover, prompt enhanced by human feedback has further improved precision and recall. This shows that human feedback on demonstrative examples can enable LLMs to understand the task requirement better and avoid some common errors.\n\n6.4 Error Analysis\n\nWe select 50 sentences from each of the two datasets whose F1subscriptùêπ1F_{1}italic_F start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT scores are less than 1 to analyze TSDRE‚Äôs performance. The results are illustrated in Table 5.\n\nWe classify error types according to the elements of the quintuple as Named Entity Recognition (NER) error, Relation Extraction error, Time Selection error, Qualifier Classification error, False Negative and Missing Facts. It is worth noting that the error rates of Time selection and Qualifier Classification are very low, showing that TSDRE perform well when faced with time-to-fact correspondences. In addition, under relaxed standards, the prediction results of entities overlapping with the answer entities and the prediction results of false negatives can both be counted as correct. This means that the actual performance of the model is much better than the scores under the exact match measurement. Finally, completely wrong NER, wrong Relation Extraction, and Missing Facts are still the legacy problems of fact (triple) extraction.\n\n6.5 Case Study\n\nAs is shown in Figure 2, Flan-T5 fails to capture information regarding the awards of the other two players, aside from O‚ÄôNeal. However, after we incorporate decomposition into training, Flan-T5 successfully outputs all temporal facts. Smaller language models do have limited learning capabilities for implicit expressions, so introducing LLMs with powerful understanding capabilities for natural language can effectively make up for this shortcoming.\n\n7 Conclusion\n\nIn this paper, we explore the application of large language models (LLMs) in the extraction of temporal facts. Our attempts indicate that directly employing LLMs for temporal fact extraction falls short of achieving satisfactory results. To tackle this issue, we introduce a timeline-based sentence decomposition (TSD) method. Building upon this, we propose TSDRE, which employs a relatively smaller PLM as its foundation, combined with LLM-driven TSD to achieve the extraction. Experiments demonstrate that TSDRE achieves SOTA results on two datasets and incorporating TSD into the training process can enhance the performance of LLMs on temporal fact extraction tasks. In the future, an interesting topic would be to explore the extraction of temporal facts from text that necessitate inferring the occurrence time based on existing temporal references, such as ‚Äúthree days later‚Äù, which has not yet received widespread attention.\n\nAcknowledgments\n\nThis work was supported by the National Natural Science Foundation of China (No. 62272219) and the Collaborative Innovation Center of Novel Software Technology & Industrialization.\n\nLimitations\n\nOur contribution does have important limitations. First, our decomposition results rely on ChatGPT for completion, and decomposition without training the open-source LLMs cannot achieve the desired results.\n\nSecond, we only tested the effect of ChatGPT on the GPT3.5-turbo model, but not on the latest GPT4 or GPT4-turbo, due to the significantly higher cost involved.\n\nThird, for document-level temporal fact extraction, when combined with time-based sentence decomposition results, the input may exceed the maximum length allowed by the generative model.\n\nFinally, our dataset construction inevitably introduces noise problems due to the use of distant supervision. Additionally, due to limited resources, we only checked the validation set and test set of ComplexTRED, which may result in some noise issues in the training set.\n\nEthics Statement\n\nFirst and foremost, our proposed TSDRE method strives to enhance the overall performance of RE models in extracting temporal facts. However, given the inherent black-box nature of the generative model, it is inevitable that the extracted facts may possess certain quality issues. Hence, when employing our method to extract temporal facts and utilize them for downstream tasks, users must exercise caution in discerning the authenticity of these facts in order to mitigate potential real-world consequences arising from erroneous information.\n\nSecondly, for dataset construction, we have gathered text from Wikipedia and DBpedia, as well as facts from Wikidata. These are publicly available datasets commonly utilized for dataset construction. Wikidata facts are under the Creative Commons CC0 License , while the texts obtained from both Wikipedia and DBpedia are licensed under the Creative Commons Attribution-ShareAlike 3.0 Unported License . Thus, we are able to freely utilize this data to construct our dataset, and our dataset will be released under the same license. Furthermore, we have organized some human annotators throughout the dataset construction process, and each annotator has been duly compensated based on their respective working hours.\n\nReferences\n\nBrown et al. (2020) Tom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel M. Ziegler, Jeffrey Wu, Clemens Winter, Christopher Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, and Dario Amodei. 2020. Language models are few-shot learners. In Advances in Neural Information Processing Systems 33: Annual Conference on Neural Information Processing Systems 2020, NeurIPS 2020, December 6-12, 2020, virtual.\n\nChang and Manning (2012) Angel X. Chang and Christopher D. Manning. 2012. Sutime: A library for recognizing and normalizing time expressions. In Proceedings of the Eighth International Conference on Language Resources and Evaluation, LREC 2012, Istanbul, Turkey, May 23-25, 2012, pages 3735‚Äì3740. European Language Resources Association (ELRA).\n\nChen et al. (2023) Ziyang Chen, Jinzhi Liao, and Xiang Zhao. 2023. Multi-granularity temporal question answering over knowledge graphs. In Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), ACL 2023, Toronto, Canada, July 9-14, 2023, pages 11378‚Äì11392. Association for Computational Linguistics.\n\nChia et al. (2022) Yew Ken Chia, Lidong Bing, Sharifah Mahani Aljunied, Luo Si, and Soujanya Poria. 2022. A dataset for hyper-relational extraction and a cube-filling approach. In Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing, EMNLP 2022, Abu Dhabi, United Arab Emirates, December 7-11, 2022, pages 10114‚Äì10133. Association for Computational Linguistics.\n\nChung et al. (2022) Hyung Won Chung, Le Hou, Shayne Longpre, Barret Zoph, Yi Tay, William Fedus, Eric Li, Xuezhi Wang, Mostafa Dehghani, Siddhartha Brahma, Albert Webson, Shixiang Shane Gu, Zhuyun Dai, Mirac Suzgun, Xinyun Chen, Aakanksha Chowdhery, Sharan Narang, Gaurav Mishra, Adams Yu, Vincent Y. Zhao, Yanping Huang, Andrew M. Dai, Hongkun Yu, Slav Petrov, Ed H. Chi, Jeff Dean, Jacob Devlin, Adam Roberts, Denny Zhou, Quoc V. Le, and Jason Wei. 2022. Scaling instruction-finetuned language models. CoRR, abs/2210.11416.\n\nDing et al. (2022) Wentao Ding, Hao Chen, Huayu Li, and Yuzhong Qu. 2022. Semantic framework based query generation for temporal question answering over knowledge graphs. In Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing, EMNLP 2022, Abu Dhabi, United Arab Emirates, December 7-11, 2022, pages 1867‚Äì1877. Association for Computational Linguistics.\n\nGast et al. (2016) Volker Gast, Lennart Bierkandt, Stephan Druskat, and Christoph Rzymski. 2016. Enriching timebank: Towards a more precise annotation of temporal relations in a text. In Proceedings of the Tenth International Conference on Language Resources and Evaluation (LREC‚Äô16), pages 3844‚Äì3850.\n\nHan et al. (2023) Xiaochuang Han, Daniel Simig, Todor Mihaylov, Yulia Tsvetkov, Asli Celikyilmaz, and Tianlu Wang. 2023. Understanding in-context learning via supportive pretraining data. In Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), ACL 2023, Toronto, Canada, July 9-14, 2023, pages 12660‚Äì12673. Association for Computational Linguistics.\n\nHu et al. (2021) Xixin Hu, Yiheng Shu, Xiang Huang, and Yuzhong Qu. 2021. Edg-based question decomposition for complex question answering over knowledge bases. In The Semantic Web - ISWC 2021 - 20th International Semantic Web Conference, ISWC 2021, Virtual Event, October 24-28, 2021, Proceedings, volume 12922 of Lecture Notes in Computer Science, pages 128‚Äì145. Springer.\n\nHuang et al. (2023) Xiang Huang, Sitao Cheng, Yiheng Shu, Yuheng Bao, and Yuzhong Qu. 2023. Question decomposition tree for answering complex questions over knowledge bases. In Thirty-Seventh AAAI Conference on Artificial Intelligence, AAAI 2023, Thirty-Fifth Conference on Innovative Applications of Artificial Intelligence, IAAI 2023, Thirteenth Symposium on Educational Advances in Artificial Intelligence, EAAI 2023, Washington, DC, USA, February 7-14, 2023, pages 12924‚Äì12932. AAAI Press.\n\nKuzey and Weikum (2012) Erdal Kuzey and Gerhard Weikum. 2012. Extraction of temporal facts and events from wikipedia. In 2nd Temporal Web Analytics Workshop, TempWeb ‚Äô12, Lyon, France, April 16-17, 2012, pages 25‚Äì32. ACM.\n\nLewis et al. (2019) Mike Lewis, Yinhan Liu, Naman Goyal, Marjan Ghazvininejad, Abdelrahman Mohamed, Omer Levy, Ves Stoyanov, and Luke Zettlemoyer. 2019. Bart: Denoising sequence-to-sequence pre-training for natural language generation, translation, and comprehension. arXiv preprint arXiv:1910.13461.\n\nLi et al. (2023) Xiaonan Li, Kai Lv, Hang Yan, Tianyang Lin, Wei Zhu, Yuan Ni, Guotong Xie, Xiaoling Wang, and Xipeng Qiu. 2023. Unified demonstration retriever for in-context learning. In Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), ACL 2023, Toronto, Canada, July 9-14, 2023, pages 4644‚Äì4668. Association for Computational Linguistics.\n\nLi et al. (2022) Yujia Li, Shiliang Sun, and Jing Zhao. 2022. Tirgn: Time-guided recurrent graph network with local-global historical patterns for temporal knowledge graph reasoning. In Proceedings of the Thirty-First International Joint Conference on Artificial Intelligence, IJCAI 2022, Vienna, Austria, 23-29 July 2022, pages 2152‚Äì2158. ijcai.org.\n\nLiu et al. (2023) Yonghao Liu, Di Liang, Mengyu Li, Fausto Giunchiglia, Ximing Li, Sirui Wang, Wei Wu, Lan Huang, Xiaoyue Feng, and Renchu Guan. 2023. Local and global: Temporal question answering via information fusion. In Proceedings of the Thirty-Second International Joint Conference on Artificial Intelligence, IJCAI 2023, 19th-25th August 2023, Macao, SAR, China, pages 5141‚Äì5149. ijcai.org.\n\nLiu et al. (2021) Yu Liu, Wen Hua, and Xiaofang Zhou. 2021. Temporal knowledge extraction from large-scale text corpus. World Wide Web, 24(1):135‚Äì156.\n\nMendes et al. (2011) Pablo N. Mendes, Max Jakob, Andr√©s Garc√≠a-Silva, and Christian Bizer. 2011. Dbpedia spotlight: shedding light on the web of documents. In Proceedings the 7th International Conference on Semantic Systems, I-SEMANTICS 2011, Graz, Austria, September 7-9, 2011, ACM International Conference Proceeding Series, pages 1‚Äì8. ACM.\n\nMezni (2022) Haithem Mezni. 2022. Temporal knowledge graph embedding for effective service recommendation. IEEE Trans. Serv. Comput., 15(5):3077‚Äì3088.\n\nOpenAI (2023) OpenAI. 2023. GPT-4 technical report. CoRR, abs/2303.08774.\n\nTouvron et al. (2023a) Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timoth√©e Lacroix, Baptiste Rozi√®re, Naman Goyal, Eric Hambro, Faisal Azhar, Aur√©lien Rodriguez, Armand Joulin, Edouard Grave, and Guillaume Lample. 2023a. Llama: Open and efficient foundation language models. CoRR, abs/2302.13971.\n\nTouvron et al. (2023b) Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, Dan Bikel, Lukas Blecher, Cristian Canton-Ferrer, Moya Chen, Guillem Cucurull, David Esiobu, Jude Fernandes, Jeremy Fu, Wenyin Fu, Brian Fuller, Cynthia Gao, Vedanuj Goswami, Naman Goyal, Anthony Hartshorn, Saghar Hosseini, Rui Hou, Hakan Inan, Marcin Kardas, Viktor Kerkez, Madian Khabsa, Isabel Kloumann, Artem Korenev, Punit Singh Koura, Marie-Anne Lachaux, Thibaut Lavril, Jenya Lee, Diana Liskovich, Yinghai Lu, Yuning Mao, Xavier Martinet, Todor Mihaylov, Pushkar Mishra, Igor Molybog, Yixin Nie, Andrew Poulton, Jeremy Reizenstein, Rashi Rungta, Kalyan Saladi, Alan Schelten, Ruan Silva, Eric Michael Smith, Ranjan Subramanian, Xiaoqing Ellen Tan, Binh Tang, Ross Taylor, Adina Williams, Jian Xiang Kuan, Puxin Xu, Zheng Yan, Iliyan Zarov, Yuchen Zhang, Angela Fan, Melanie Kambadur, Sharan Narang, Aur√©lien Rodriguez, Robert Stojnic, Sergey Edunov, and Thomas Scialom. 2023b. Llama 2: Open foundation and fine-tuned chat models. CoRR, abs/2307.09288.\n\nUzZaman et al. (2012) Naushad UzZaman, Hector Llorens, James Allen, Leon Derczynski, Marc Verhagen, and James Pustejovsky. 2012. Tempeval-3: Evaluating events, time expressions, and temporal relations. arXiv preprint arXiv:1206.5333.\n\nWadhwa et al. (2023) Somin Wadhwa, Silvio Amir, and Byron C. Wallace. 2023. Revisiting relation extraction in the era of large language models. In Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), ACL 2023, Toronto, Canada, July 9-14, 2023, pages 15566‚Äì15589. Association for Computational Linguistics.\n\nWang et al. (2012) Yafang Wang, Maximilian Dylla, Marc Spaniol, and Gerhard Weikum. 2012. Coupling label propagation and constraints for temporal fact extraction. In The 50th Annual Meeting of the Association for Computational Linguistics, Proceedings of the Conference, July 8-14, 2012, Jeju Island, Korea - Volume 2: Short Papers, pages 233‚Äì237. The Association for Computer Linguistics.\n\nWang et al. (2011) Yafang Wang, Bin Yang, Lizhen Qu, Marc Spaniol, and Gerhard Weikum. 2011. Harvesting facts from textual web sources by constrained label propagation. In Proceedings of the 20th ACM Conference on Information and Knowledge Management, CIKM 2011, Glasgow, United Kingdom, October 24-28, 2011, pages 837‚Äì846. ACM.\n\nWang et al. (2010) Yafang Wang, Mingjie Zhu, Lizhen Qu, Marc Spaniol, and Gerhard Weikum. 2010. Timely YAGO: harvesting, querying, and visualizing temporal knowledge from wikipedia. In EDBT 2010, 13th International Conference on Extending Database Technology, Lausanne, Switzerland, March 22-26, 2010, Proceedings, volume 426 of ACM International Conference Proceeding Series, pages 697‚Äì700. ACM.\n\nXiong et al. (2024) Siheng Xiong, Ali Payani, Ramana Kompella, and Faramarz Fekri. 2024. Large language models can learn temporal reasoning.\n\nXu et al. (2023) Yi Xu, Junjie Ou, Hui Xu, and Luoyi Fu. 2023. Temporal knowledge graph reasoning with historical contrastive learning. In Thirty-Seventh AAAI Conference on Artificial Intelligence, AAAI 2023, Thirty-Fifth Conference on Innovative Applications of Artificial Intelligence, IAAI 2023, Thirteenth Symposium on Educational Advances in Artificial Intelligence, EAAI 2023, Washington, DC, USA, February 7-14, 2023, pages 4765‚Äì4773. AAAI Press.\n\nAppendix A Environments and Parameters\n\nTSDRE‚Äôs results were achieved using a Python implementation running on a workstation with an Intel(R) Xeon(R) Gold 5222 CPU @ 3.80GHz, 376GB RAM and 3 NVIDIA RTX3090 graphics cards. Llama2‚Äôs results were achieved on a workstation with an Intel(R) Xeon(R) Gold 6248 CPU @ 2.50GHz, 472GB RAM, and 8 NVIDIA Tesla V100 graphics cards. The hyperparameter settings of CubeRE, Flan-T5, Llama2 and BART are shown in Table 6. When applying LoRA to fine-tune Llama2, we used a rank of 8 and an alpha value of 32. Besides, We set the temperature value of ChatGPT3.5 to 0 to facilitate reproduction.\n\nAppendix B TSDRE with BART\n\nIn this paper, we propose a pipeline architecture TSDRE which is backbone-free. We have reported in Section 6.2 that our model TSDRE w Flan-T5 achieves the SOTA results. We test the performance of replacing Flan-T5 Large (770M) with a slightly weaker generative model BART Large (340M). The results are shown in table 7. From the experimental results, it can be seen that the performance of BART as base model is indeed lower than that of Flan-T5 (mentioned in Table 3). However, TSD still enhances the performance of BART.\n\nAppendix C Error Analysis\n\nHere we present specific examples of three major types of errors.\n\nC.1 NER Error\n\nInput: Fifteen locomotives of British Rail Class 83 were built between 1960 and 1962 by English Electric at Vulcan Foundry , as part of British Rail ‚Äôs policy to develop a standard electric locomotive .\n\nPrediction: [[‚ÄôBritish Rail Class 83‚Äô, ‚Äômanufacturer‚Äô, ‚ÄôVulcan Foundry‚Äô, ‚Äôend time‚Äô, ‚Äô1962‚Äô], [‚ÄôBritish Rail Class 83‚Äô, ‚Äômanufacturer‚Äô, ‚ÄôVulcan Foundry‚Äô, ‚Äôstart time‚Äô, ‚Äô1960‚Äô]]\n\nError Analysis: The tail entity should be ‚ÄôEnglish Electric‚Äô.\n\nC.2 Relation Extraction Error\n\nInput: He was appointed manager of the Highland Railway ‚Äô s Lochgorm Works in 1903 , and promoted to Assistant to the Chief Mechanical Engineer , Peter Drummond in 1903 .\n\nPrediction: [[‚ÄôPeter Drummond‚Äô, ‚Äôemployer‚Äô, ‚ÄôHighland Railway‚Äô, ‚Äôstart time‚Äô, ‚Äô1903‚Äô], [‚ÄôPeter Drummond‚Äô, ‚Äôemployer‚Äô, ‚ÄôHighland Railway‚Äô, ‚Äôend time‚Äô, ‚Äô1903‚Äô]]\n\nError Analysis: Relation should be ‚Äôdirector/ manager‚Äô.\n\nC.3 Missing Facts\n\nInput: Sir Hubert Edward Henry Jerningham , ( 18 October 1842 - 3 April 1914 ) was a British Liberal Party politician and Governor of Mauritius 1892 - 1897 , then Governor of Trinidad and Tobago between 1897 and 1900 .\n\nPrediction: [[‚ÄôHubert Edward Henry Jerningham‚Äô, ‚Äôposition held‚Äô, ‚ÄôGovernor of Trinidad and Tobago‚Äô,‚Äôstart time‚Äô, ‚Äô1897‚Äô], [‚ÄôHubert Edward Henry Jerningham‚Äô, ‚Äôposition held‚Äô, ‚ÄôGovernor of Mauritius‚Äô, ‚Äôend time‚Äô, ‚Äô1897‚Äô], [‚ÄôHubert Edward Henry Jerningham‚Äô, ‚Äôposition held‚Äô, ‚ÄôGovernor of Mauritius‚Äô,‚Äôstart time‚Äô, ‚Äô1892‚Äô]]\n\nError Analysis: [‚ÄôHubert Edward Henry Jerningham‚Äô, ‚Äôposition held‚Äô, ‚ÄôGovernor of Trinidad and Tobago‚Äô,‚Äôend time‚Äô, ‚Äô1900‚Äô] missed.\n\nAppendix D Difference from the temporal relation extraction task\n\nIndeed, it is easy to confuse temporal relation extraction with temporal fact extraction based on their names, but they are actually very different tasks. Specifically, temporal relation extraction aims to identifying the temporal relation (e.g., BEFORE, AFTER, OVERLAPS) between events and times, while temporal fact extractions (please refer to Section 3.1) aims to to extract facts with temporal attributes (e.g., start_time, end_time). Therefore, datasets used for temporal relation extraction tasks such as TimeBank Gast et al. (2016) and TempEval UzZaman et al. (2012) are not suitable for evaluating our task.\n\nAppendix E Prompt\n\nE.1 In-Context ChatGPT3.5 Prompt\n\nTask: Extract all the quintuples [subject, relation, object, qualifier, time point] from the input text.\n\nTask requirements: Extract all the quintuples [subject, relation, object, qualifier, time point] from the input text.\n\nHere are some concrete examples, the output format is a list of quintuples:\n\nInput: He received the 1921 Nobel Prize in Physics for his sÃàervices to theoretical physics ,Ãà in particular his discovery of the law of the photoelectric effect , a pivotal step in the evolution of quantum theory .\n\nOutput: [[‚ÄôNobel Prize‚Äô, ‚Äôwinner‚Äô, ‚ÄôHe‚Äô, ‚Äôpoint in time‚Äô, ‚Äô1921‚Äô], [‚ÄôHe‚Äô, ‚Äôaward received‚Äô, ‚ÄôNobel Prize‚Äô, ‚Äôpoint in time‚Äô, ‚Äô1921‚Äô]]\n\nInput: It won the 1991 Nebula Award for Best Novelette and was nominated for the 1991 Hugo Award for Best Novelette .\n\nOutput: [[‚ÄôIt‚Äô, ‚Äôaward received‚Äô, ‚ÄôNebula Award‚Äô, ‚Äôpoint in time‚Äô, ‚Äô1991‚Äô], [‚ÄôIt‚Äô, ‚Äônominated for‚Äô, ‚ÄôBest Novelette‚Äô, ‚Äôpoint in time‚Äô, ‚Äô1991‚Äô]]\n\nInput: They are currently the only club in Ulster to have won an All - Ireland Senior Club Hurling Championship , which they first won in 1983 .\n\nOutput: [[‚ÄôAll - Ireland Senior Club Hurling Championship‚Äô, ‚Äôwinner‚Äô, ‚ÄôThey‚Äô, ‚Äôpoint in time‚Äô, ‚Äô1983‚Äô]]\n\nInput: Alexander Mackenzie , PC ( January 28 , 1822 2ÃÜ013 April 17 , 1892 ) , was a building contractor and newspaper editor , and was the second Prime Minister of Canada , from November 7 , 1873 to October 8 , 1878 .\n\nOutput: [[‚ÄôCanada‚Äô, ‚Äôhead of government‚Äô, ‚ÄôAlexander Mackenzie‚Äô, ‚Äôend time‚Äô, ‚ÄôOctober 8 , 1878‚Äô], [‚ÄôCanada‚Äô, ‚Äôhead of government‚Äô, ‚ÄôAlexander Mackenzie‚Äô, ‚Äôstart time‚Äô, ‚ÄôNovember 7 , 1873‚Äô], [‚ÄôAlexander Mackenzie‚Äô, ‚Äôposition held‚Äô, ‚ÄôPrime Minister‚Äô, ‚Äôend time‚Äô, ‚ÄôOctober 8 , 1878‚Äô], [‚ÄôAlexander Mackenzie‚Äô, ‚Äôposition held‚Äô, ‚ÄôPrime Minister‚Äô, ‚Äôstart time‚Äô, ‚ÄôNovember 7 , 1873‚Äô]]\n\nInput: There has been a resident Treasury or Downing Street cat employed as a mouser and pet since the reign of Henry VIII , when Cardinal Wolsey placed his cat by his side while acting in his judicial capacity as Lord Chancellor , an office he assumed in 1515 .\n\nOutput: [[‚ÄôCardinal Wolsey‚Äô, ‚Äôposition held‚Äô, ‚ÄôLord Chancellor‚Äô, ‚Äôstart time‚Äô, ‚Äô1515‚Äô]]\n\nInput: He had 24 caps for Japan , from 1974 to 1984 , scoring 3 tries , 5 conversions , 14 penalties and 3 drop goals , in an aggregate of 73 points .\n\nOutput: [[‚ÄôHe‚Äô, ‚Äômember of sports team‚Äô, ‚ÄôJapan‚Äô, ‚Äôstart time‚Äô, ‚Äô1974‚Äô], [‚ÄôHe‚Äô, ‚Äômember of sports team‚Äô, ‚ÄôJapan‚Äô, ‚Äôend time‚Äô, ‚Äô1984‚Äô]]\n\nInput: He later played professional football in the American Football League , appearing in 42 games as a tackle and defensive end for the New York Titans ( later renamed the Jets ) from 1960 to 1962 .\n\nOutput: [[‚ÄôHe‚Äô, ‚Äômember of sports team‚Äô, ‚ÄôNew York Titans‚Äô, ‚Äôstart time‚Äô, ‚Äô1960‚Äô], [‚ÄôHe‚Äô, ‚Äômember of sports team‚Äô, ‚ÄôNew York Titans‚Äô, ‚Äôend time‚Äô, ‚Äô1962‚Äô]]\n\nInput: \" Suedehead \" is the debut solo single from Morrissey , released in February 1988 .\n\nOutput: [[‚ÄôSuedehead‚Äô, ‚Äôperformer‚Äô, ‚ÄôMorrissey‚Äô, ‚Äôpublication date‚Äô, ‚ÄôFebruary 1988‚Äô]]\n\nInput: It closed on 1 December 2003 when operation of the line was suspended between Kabe Station and Sandanky0ÃÜ14d Station .\n\nOutput: [[‚ÄôIt‚Äô, ‚Äôadjacent station‚Äô, ‚ÄôKabe‚Äô, ‚Äôend time‚Äô, ‚Äô1 December 2003‚Äô], [‚ÄôIt‚Äô, ‚Äôadjacent station‚Äô, ‚ÄôSandanky0ÃÜ14d‚Äô, ‚Äôend time‚Äô, ‚Äô1 December 2003‚Äô]]\n\nInput: The 2008 presidential campaign of Barack Obama , then junior United States Senator from Illinois , was announced at an event on February 10 , 2007 in Springfield , Illinois .\n\nOutput: [[‚ÄôBarack Obama‚Äô, ‚Äôcandidacy in election‚Äô, ‚Äô2008 presidential campaign‚Äô, ‚Äôstart time‚Äô, ‚ÄôFebruary 10 , 2007‚Äô]]\n\nInput: Among his victories were in reconquering Ji ‚Äô an in Jiangxi Province in 1856 , as well as leading the assault on the Taiping capital at Nanjing in 1864 .\n\nOutput: [[‚ÄôNanjing‚Äô, ‚Äôcapital of‚Äô, ‚ÄôTaiping‚Äô, ‚Äôend time‚Äô, ‚Äô1864‚Äô]]\n\nInput: He was also one of the original correspondents on Comedy Central ‚Äô s The Daily Show from 1996 to 1998 .\n\nOutput: [[‚ÄôThe Daily Show‚Äô, ‚Äôcast member‚Äô, ‚ÄôHe‚Äô, ‚Äôend time‚Äô, ‚Äô1998‚Äô], [‚ÄôThe Daily Show‚Äô, ‚Äôcast member‚Äô, ‚ÄôHe‚Äô, ‚Äôstart time‚Äô, ‚Äô1996‚Äô]]\n\nInput: He was International President of WWF from 1996 to 1999 succeeding Prince Philip , the Duke of Edinburgh .\n\nOutput: [[‚ÄôWWF‚Äô, ‚Äôchairperson‚Äô, ‚ÄôHe‚Äô, ‚Äôend time‚Äô, ‚Äô1999‚Äô], [‚ÄôWWF‚Äô, ‚Äôchairperson‚Äô, ‚ÄôHe‚Äô, ‚Äôstart time‚Äô, ‚Äô1996‚Äô]]\n\nInput: Timothy Fok Tsun - ting ( born 14 February 1946 in Hong Kong ) , GBS , JP , the eldest son of Henry Fok , is a Member of the Legislative Council of Hong Kong , representing the Sports , Performing Arts , Culture and Publication functional constituency .\n\nOutput: [[‚ÄôHenry Fok‚Äô, ‚Äôchild‚Äô, ‚ÄôTimothy Fok Tsun - ting‚Äô, ‚Äôstart time‚Äô, ‚Äô14 February 1946‚Äô]]\n\nInput: The 2013 Philadelphia Eagles season was the franchise ‚Äô s 81st season in the National Football League , and the first under head coach Chip Kelly .\n\nOutput: [[‚ÄôChip Kelly‚Äô, ‚Äôcoach of sports team‚Äô, ‚ÄôPhiladelphia Eagles‚Äô, ‚Äôstart time‚Äô, ‚Äô2013‚Äô]]\n\nInput: Shek Kip Mei Station served as a terminus in the very early phase of the Kwun Tong Line ( Shek Kip Mei to Kwun Tong , 1 October 1979 to 31 December 1979 ) .\n\nOutput: [[‚ÄôShek Kip Mei‚Äô, ‚Äôconnecting line‚Äô, ‚ÄôKwun Tong Line‚Äô, ‚Äôstart time‚Äô, ‚Äô1 October 1979‚Äô]]\n\nInput: K0ÃÜ0f6nigsberg was transferred to Soviet control in 1945 after World War II .\n\nOutput: [[‚ÄôK0ÃÜ0f6nigsberg‚Äô, ‚Äôcountry‚Äô, ‚ÄôSoviet‚Äô, ‚Äôstart time‚Äô, ‚Äô1945‚Äô]]\n\nInput: She was the Director of the Walter and Eliza Hall Institute of Medical Research ( WEHI ) , from 1996 until 30 June 2009 and remains a faculty member , having rejoined the institute ‚Äô s Molecular Genetics of Cancer Division .\n\nOutput: [[‚ÄôWEHI‚Äô, ‚Äôdirector / manager‚Äô, ‚ÄôShe‚Äô, ‚Äôend time‚Äô, ‚Äô30 June 2009‚Äô], [‚ÄôWEHI‚Äô, ‚Äôdirector / manager‚Äô, ‚ÄôShe‚Äô, ‚Äôstart time‚Äô, ‚Äô1996‚Äô]]\n\nInput: He previously served as Commander , United States Transportation Command from September 2005 to August 2008 .\n\nOutput: [[‚ÄôUnited States Transportation Command‚Äô, ‚Äôdirector / manager‚Äô, ‚ÄôHe‚Äô, ‚Äôend time‚Äô, ‚ÄôAugust 2008‚Äô], [‚ÄôUnited States Transportation Command‚Äô, ‚Äôdirector / manager‚Äô, ‚ÄôHe‚Äô, ‚Äôstart time‚Äô, ‚ÄôSeptember 2005‚Äô]]\n\nInput: He graduated from Pennsylvania State University in State College , PA in 1969 , and earned a J . D .\n\nOutput: [[‚ÄôHe‚Äô, ‚Äôeducated at‚Äô, ‚ÄôPennsylvania State University‚Äô, ‚Äôend time‚Äô, ‚Äô1969‚Äô]]\n\nInput: She married in 1936 , and took up her first post in Liverpool University , where she studied for the rest of her working life .\n\nOutput: [[‚ÄôShe‚Äô, ‚Äôemployer‚Äô, ‚ÄôLiverpool University‚Äô, ‚Äôstart time‚Äô, ‚Äô1936‚Äô]]\n\nInput: Air Union was merged with four other French airlines to become Air France on 7 October 1933 .\n\nOutput: [[‚ÄôAir Union‚Äô, ‚Äôfollowed by‚Äô, ‚ÄôAir France‚Äô, ‚Äôpoint in time‚Äô, ‚Äô7 October 1933‚Äô]]\n\nInput: The Duchy of Magdeburg ( German : Herzogtum Magdeburg ) was a province of Brandenburg - Prussia from 1680 to 1701 and a province of the German Kingdom of Prussia from 1701 to 1807 .\n\nOutput: [[‚ÄôBrandenburg - Prussia‚Äô, ‚Äôfollowed by‚Äô, ‚ÄôPrussia‚Äô, ‚Äôpoint in time‚Äô, ‚Äô1701‚Äô], [‚ÄôMagdeburg‚Äô, ‚Äôlocated in the administrative territorial entity‚Äô, ‚ÄôPrussia‚Äô, ‚Äôstart time‚Äô, ‚Äô1701‚Äô], [‚ÄôMagdeburg‚Äô, ‚Äôlocated in the administrative territorial entity‚Äô, ‚ÄôBrandenburg - Prussia‚Äô, ‚Äôstart time‚Äô, ‚Äô1680‚Äô]]\n\nInput: In 1998 , the studio moved from Studio City , California to Burbank in celebration of a new facility , and was renamed Nickelodeon Animation Studio .\n\nOutput: [[‚ÄôNickelodeon Animation Studio‚Äô, ‚Äôheadquarters location‚Äô, ‚ÄôStudio City , California‚Äô, ‚Äôend time‚Äô, ‚Äô1998‚Äô], [‚ÄôNickelodeon Animation Studio‚Äô, ‚Äôheadquarters location‚Äô, ‚ÄôBurbank‚Äô, ‚Äôstart time‚Äô, ‚Äô1998‚Äô]]\n\nInput: Kimera Walusimbi was Kabaka of the Kingdom of Buganda between 1374 and 1404 .\n\nOutput: [[‚ÄôKimera‚Äô, ‚Äônoble title‚Äô, ‚ÄôKabaka‚Äô, ‚Äôstart time‚Äô, ‚Äô1374‚Äô], [‚ÄôKimera‚Äô, ‚Äônoble title‚Äô, ‚ÄôKabaka‚Äô, ‚Äôend time‚Äô, ‚Äô1404‚Äô]]\n\nInput: For the start of the 1982 season , the Minnesota Vikings moved from Metropolitan Stadium to the Hubert H . Humphrey Metrodome .\n\nOutput: [[‚ÄôMetrodome‚Äô, ‚Äôoccupant‚Äô, ‚ÄôMinnesota Vikings‚Äô, ‚Äôstart time‚Äô, ‚Äô1982‚Äô], [‚ÄôMinnesota Vikings‚Äô, ‚Äôhome venue‚Äô, ‚ÄôMetrodome‚Äô, ‚Äôstart time‚Äô, ‚Äô1982‚Äô]]\n\nInput: It was established as the official legislature of Kampuchea on January 5 , 1976 , consisting of 250 members .\n\nOutput: [[‚ÄôKampuchea‚Äô, ‚Äôlegislative body‚Äô, ‚ÄôIt‚Äô, ‚Äôstart time‚Äô, ‚ÄôJanuary 5 , 1976‚Äô]]\n\nInput: They were designed by R . J . Billinton and built at Brighton works from 1895 to 1897 .\n\nOutput: [[‚ÄôThey‚Äô, ‚Äômanufacturer‚Äô, ‚ÄôBrighton works‚Äô, ‚Äôstart time‚Äô, ‚Äô1895‚Äô]]\n\nInput: Dee Palmer ( formerly David Palmer ; born 2 July 1937 ) is an English composer , arranger , and keyboardist best known for having been a member of the progressive rock group Jethro Tull from 1977 to 1980 .\n\nOutput: [[‚ÄôDavid Palmer‚Äô, ‚Äômember of‚Äô, ‚ÄôJethro Tull‚Äô, ‚Äôend time‚Äô, ‚Äô1980‚Äô], [‚ÄôDavid Palmer‚Äô, ‚Äômember of‚Äô, ‚ÄôJethro Tull‚Äô, ‚Äôstart time‚Äô, ‚Äô1977‚Äô]]\n\nInput: In 1931 , he joined Joseph Lyons and several other members in leaving the Labor Party and joining with the Nationalists to create the United Australia Party .\n\nOutput: [[‚ÄôJoseph Lyons‚Äô, ‚Äômember of political party‚Äô, ‚ÄôUnited Australia Party‚Äô, ‚Äôstart time‚Äô, ‚Äô1931‚Äô], [‚ÄôJoseph Lyons‚Äô, ‚Äômember of political party‚Äô, ‚ÄôLabor Party‚Äô, ‚Äôend time‚Äô, ‚Äô1931‚Äô]]\n\nInput: He was in the United States Army during World War II , from 1943 to 1946 .\n\nOutput: [[‚ÄôHe‚Äô, ‚Äômilitary branch‚Äô, ‚ÄôUnited States Army‚Äô, ‚Äôend time‚Äô, ‚Äô1946‚Äô], [‚ÄôHe‚Äô, ‚Äômilitary branch‚Äô, ‚ÄôUnited States Army‚Äô, ‚Äôstart time‚Äô, ‚Äô1943‚Äô]]\n\nInput: She was named after the title character of the 1866 opera Mignon , written by her godfather , French composer Ambroise Thomas .\n\nOutput: [[‚ÄôAmbroise Thomas‚Äô, ‚Äônotable work‚Äô, ‚ÄôMignon‚Äô, ‚Äôpublication date‚Äô, ‚Äô1866‚Äô]]\n\nInput: It was best known as the home of the Detroit Red Wings hockey team of the National Hockey League from its opening until 1979 .\n\nOutput: [[‚ÄôDetroit Red Wings‚Äô, ‚Äôhome venue‚Äô, ‚ÄôIt‚Äô, ‚Äôend time‚Äô, ‚Äô1979‚Äô], [‚ÄôIt‚Äô, ‚Äôoccupant‚Äô, ‚ÄôDetroit Red Wings‚Äô, ‚Äôend time‚Äô, ‚Äô1979‚Äô]]\n\nInput: He became a solicitor in 1900 and a barrister in 1913 , being a member of both King ‚Äô s Inns , Dublin , and Gray 2ÃÜ019 s Inn , London .\n\nOutput: [[‚ÄôHe‚Äô, ‚Äôoccupation‚Äô, ‚Äôsolicitor‚Äô, ‚Äôstart time‚Äô, ‚Äô1900‚Äô], [‚ÄôHe‚Äô, ‚Äôoccupation‚Äô, ‚Äôbarrister‚Äô, ‚Äôstart time‚Äô, ‚Äô1913‚Äô]]\n\nInput: The Portuguese Air Force ( PoAF ) operated 50 LTV A - 7 Corsair II aircraft in the anti - ship , air interdiction and air defense roles between 1981 and 1999 .\n\nOutput: [[‚ÄôLTV A - 7 Corsair II‚Äô, ‚Äôoperator‚Äô, ‚ÄôPortuguese Air Force‚Äô, ‚Äôstart time‚Äô, ‚Äô1981‚Äô], [‚ÄôLTV A - 7 Corsair II‚Äô, ‚Äôoperator‚Äô, ‚ÄôPortuguese Air Force‚Äô, ‚Äôend time‚Äô, ‚Äô1999‚Äô]]\n\nInput: Brickleberry is an American animated comedy that premiered on September 25 , 2012 on Comedy Central .\n\nOutput: [[‚ÄôBrickleberry‚Äô, ‚Äôoriginal broadcaster‚Äô, ‚ÄôComedy Central‚Äô, ‚Äôstart time‚Äô, ‚ÄôSeptember 25 , 2012‚Äô]]\n\nInput: Shellen joined Google in 2003 when the company acquired Pyra Labs , which developed the Blogger blogging platform .\n\nOutput: [[‚ÄôPyra Labs‚Äô, ‚Äôowned by‚Äô, ‚ÄôGoogle‚Äô, ‚Äôstart time‚Äô, ‚Äô2003‚Äô]]\n\nInput: Volkswagen purchased the Bugatti trademark in June 1998 and incorporated Bugatti Automobiles S . A . S .\n\nOutput: [[‚ÄôBugatti‚Äô, ‚Äôowned by‚Äô, ‚ÄôVolkswagen‚Äô, ‚Äôstart time‚Äô, ‚ÄôJune 1998‚Äô]]\n\nInput: In 1986 , the company was acquired by Penguin Group and split into two imprints : Dutton and Dutton Children ‚Äô s Books .\n\nOutput: [[‚ÄôDutton‚Äô, ‚Äôparent organization‚Äô, ‚ÄôPenguin Group‚Äô, ‚Äôstart time‚Äô, ‚Äô1986‚Äô], [‚ÄôDutton Children ‚Äô s Books‚Äô, ‚Äôparent organization‚Äô, ‚ÄôPenguin Group‚Äô, ‚Äôstart time‚Äô, ‚Äô1986‚Äô]]\n\nInput: In 1990 , following the Iraqi invasion of Kuwait , Saudi Arabia participated in the Gulf War to expel Iraqi forces from the country .\n\nOutput: [[‚ÄôGulf War‚Äô, ‚Äôparticipant‚Äô, ‚ÄôSaudi Arabia‚Äô, ‚Äôpoint in time‚Äô, ‚Äô1990‚Äô]]\n\nInput: On November 18 , 1928 the first Mickey Mouse cartoon released to the public , Steamboat Willie , debuted at the Colony .\n\nOutput: [[‚ÄôMickey Mouse‚Äô, ‚Äôpresent in work‚Äô, ‚ÄôSteamboat Willie‚Äô, ‚Äôpoint in time‚Äô, ‚ÄôNovember 18 , 1928‚Äô]]\n\nInput: The Third Republic of South Korea was replaced in 1972 by the Fourth Republic of South Korea under the Third Republic of South Korea ‚Äô s president Park Chung - hee .\n\nOutput: [[‚ÄôFourth Republic of South Korea‚Äô, ‚Äôreplaces‚Äô, ‚ÄôThird Republic of South Korea‚Äô, ‚Äôpoint in time‚Äô, ‚Äô1972‚Äô]]\n\nInput: He returned to favor in 1942 and was recalled to Moscow .\n\nOutput: [[‚ÄôHe‚Äô, ‚Äôresidence‚Äô, ‚ÄôMoscow‚Äô, ‚Äôstart time‚Äô, ‚Äô1942‚Äô]]\n\nInput: On 20 July 2012 , the Constable welcomed the Olympic Torch to London at the Tower one week in advance of the London 2012 Summer Olympic Games , as part of the Olympic torch relay .\n\nOutput: [[‚ÄôLondon‚Äô, ‚Äôsignificant event‚Äô, ‚ÄôLondon 2012 Summer Olympic Games‚Äô, ‚Äôpoint in time‚Äô, ‚Äô20 July 2012‚Äô]]\n\nInput: and Max Verstappen , who in 2015 became the youngest driver in Formula One history at just 17 years old .\n\nOutput: [[‚ÄôMax Verstappen‚Äô, ‚Äôsport‚Äô, ‚ÄôFormula One‚Äô, ‚Äôstart time‚Äô, ‚Äô2015‚Äô]]\n\nInput: She was married to internationally famous writer Jorge Amado from 1945 until his death in 2001 .\n\nOutput: [[‚ÄôShe‚Äô, ‚Äôspouse‚Äô, ‚ÄôJorge Amado‚Äô, ‚Äôstart time‚Äô, ‚Äô1945‚Äô], [‚ÄôShe‚Äô, ‚Äôspouse‚Äô, ‚ÄôJorge Amado‚Äô, ‚Äôend time‚Äô, ‚Äô2001‚Äô]]\n\nInput: It was first listed on the London Stock Exchange in 2005 is now a constituent of the FTSE 100 Index .\n\nOutput: [[‚ÄôIt‚Äô, ‚Äôstock exchange‚Äô, ‚ÄôLondon Stock Exchange‚Äô, ‚Äôstart time‚Äô, ‚Äô2005‚Äô]]\n\nInput: The Valencia Street Circuit ( Valencian : Circuit Urb0ÃÜ0e0 de Val0ÃÜ0e8ncia , Spanish : Circuito Urbano de Valencia ) was a street circuit in Valencia , Spain which hosted the Formula One European Grand Prix for five years ( 2008 2ÃÜ013 2012 ) .\n\nOutput: [[‚ÄôValencia Street Circuit‚Äô, ‚Äôused by‚Äô, ‚ÄôFormula One‚Äô, ‚Äôstart time‚Äô, ‚Äô2008‚Äô], [‚ÄôValencia Street Circuit‚Äô, ‚Äôused by‚Äô, ‚ÄôFormula One‚Äô, ‚Äôend time‚Äô, ‚Äô2012‚Äô]]\n\nNow please extract the quintuples from the following input:\n\nInput:\n\nE.2 Decomposition Prompt with Feedback\n\nFirst, I will give you a TEXT, and secondly, I will give you the TIME contained in the TEXT. You need to sort out the events that occurred at each time I provided based on the TEXT. The DECOMPOSITION format is required to contain a time point and corresponding events in each sentence. Each sentence contains the complete elements of the event, such as subject, predicate, and object. DECOMPOSITION ends with </s>. There are two evaluation criteria for this task: Precision and Recall. The definitions of Precision and Recall under this task are as follows:\n\nPrecision: Whether the time at which each event occurred is given correctly.\n\nRecall: Whether all events that occurred at each time are given completely.\n\nI will show you some examples. At the same time, i will give you human feedback for decomposition results in these examples. I need you to learn from these examples and feedback how to decompose TEXT correctly and completely:\n\nEXAMPLE1:\n\nTEXT: The Duchy of Magdeburg ( German : Herzogtum Magdeburg ) was a province of Brandenburg - Prussia from 1680 to 1701 and a province of the German Kingdom of Prussia from 1701 to 1807 .\n\nTIME: [‚Äôfrom 1680 to 1701‚Äô, ‚Äôfrom 1701 to 1807‚Äô]\n\nDECOMPOSITION: From 1680 to 1701: The Duchy of Magdeburg was a province of Brandenburg-Prussia. From 1701 to 1807: The Duchy of Magdeburg was a province of the German Kingdom of Prussia. </s>\n\nfeedback: Precision: very good. Recall: very good.\n\nEXAMPLE2:\n\nTEXT: From its foundation in 1921 until moving to Ashton Gate in 2014 the Memorial Ground was the home to Bristol Rugby .\n\nTIME: [‚Äô1921‚Äô, ‚Äô2014‚Äô]\n\nDECOMPOSITION: 1921: The Memorial Ground was founded and became the home to Bristol Rugby. 2014: The Memorial Ground moved to Ashton Gate. </s>\n\nfeedback: Precision: Not good. It was not the Memorial Ground that was founded in 1921 but Bristol Rugby. Besides, it was not the Memorial Ground but Bristol Rugby that moved to Ashton Gate. Recall: Not good. Two events are not recalled: In 1921 Bristol Rugby was founded and in 2014 Bristol Rugby moved to Ashton Gate. A correct and complete decomposition is as follows: 1921: Bristol Rugby was founded and the Memorial Ground the home to Bristol Rugby. 2014: Bristol Rugby moved to Ashton Gate.\n\nEXAMPLE3:\n\nTEXT: Alexander Mackenzie , PC ( January 28 , 1822 2ÃÜ013 April 17 , 1892 ) , was a building contractor and newspaper editor , and was the second Prime Minister of Canada , from November 7 , 1873 to October 8 , 1878 .\n\nFeedback: Precision: very good. Recall: very good.\n\nEXAMPLE4:\n\nTEXT: Pope Benedict XV ( Latin : Benedictus XV ; Italian : Benedetto XV ) born Giacomo Paolo Giovanni Battista della Chiesa , ( 21 November 1854 2ÃÜ013 22 January 1922 ) , was Pope from 3 September 1914 until his death in 1922 .\n\nTIME: [‚Äô21 November 1854‚Äô, ‚Äô22 January 1922‚Äô, ‚Äô3 September 1914‚Äô, ‚Äô1922‚Äô]\n\nDECOMPOSITION: 21 November 1854: Pope Benedict XV, born Giacomo Paolo Giovanni Battista della Chiesa, was born. 22 January 1922: Pope Benedict XV passed away. 3 September 1914: Pope Benedict XV started serving as Pope. 1922: Pope Benedict XV ended serving as Pope. </s>\n\nfeedback: Precision: very good. Recall: not good. In 22 January 1922, Pope Benedict XV passed away and was no longer Pope. In 1922: Pope Benedict XV passed away and ended serving as Pope. A correct and complete decomposition is as follows: 21 November 1854: Pope Benedict XV, born Giacomo Paolo Giovanni Battista della Chiesa, was born. 22 January 1922: Pope Benedict XV passed away and ended serving as Pope. 3 September 1914: Pope Benedict XV started serving as Pope. 1922: Pope Benedict XV passed away and ended serving as Pope.\n\nEXAMPLE5:\n\nTEXT: He received : Serbian NIN Prize 1978 and Andri0ÃÜ107 Award 1987 , Swedish PEN Tucholsky Award ( 1993 ) , German Herder Prize ( 1995 ) , Montenegrin Njego0ÃÜ161 Award ( 2009 ) and 13th July award ( 2004 ) , Slovenian Vilenica Award ( 2003 ) , Croatian Vladimir Nazor Award ( 2008 ) etc .\n\nTIME: [‚Äô1978‚Äô, ‚Äô1987‚Äô, ‚Äô1993‚Äô, ‚Äô1995‚Äô, ‚Äô2003‚Äô, ‚Äô2004‚Äô, ‚Äô2008‚Äô, ‚Äô2009‚Äô]\n\nDECOMPOSITION: 1978: He received the Serbian NIN Prize. 1987: He received the Andriƒá Award. 1993: He received the Swedish PEN Tucholsky Award. 1995: He received the German Herder Prize. 2003: He received the Slovenian Vilenica Award. 2004: He received the 13th July award. 2008: He received the Croatian Vladimir Nazor Award. 2009: He received the Montenegrin Njego≈° Award. </s>\n\nfeedback: Precision: very good. Recall: very good.\n\nEXAMPLE6:\n\nTEXT: It is one of the most famous old districts in Poland today and was the center of Poland ‚Äô s political life from 1038 until King Sigismund III Vasa relocated his court to Warsaw in 1596 .\n\nTIME: [‚Äô1038‚Äô, ‚Äô1596‚Äô]\n\nDECOMPOSITION: 1038: It was the center of Poland‚Äôs political life. 1596: King Sigismund III Vasa relocated his court to Warsaw. </s>\n\nfeedback: Precision: very good. Recall: not good. In 1596 it was no longer the center of Poland ‚Äô s political life. A correct and complete decomposition is as follows: 1038: It was the center of Poland‚Äôs political life. 1596: It was no longer the center of Poland ‚Äô s political life since King Sigismund III Vasa relocated his court to Warsaw.\n\nEXAMPLE7:\n\nTEXT: He played for Gloucestershire between 1920 and 1921 .\n\nTIME: [‚Äô1920‚Äô, ‚Äô1921‚Äô]\n\nDECOMPOSITION: 1920: He played for Gloucestershire. 1921: He played for Gloucestershire. </s>\n\nfeedback: Precision: not good. This decomposition does not convey the meaning that he started playing for Gloucestershire in 1920 and ended playing for Gloucestershire in 1921. Recall: very good. A correct and complete decomposition is as follows: 1920: He started playing for Gloucestershire. 1921: He ended playing for Gloucestershire.\n\nNow process the following TEXT and follow my examples above for the DECOMPOSITION format:\n\nAppendix F Relations Deleted from HyperRED\n\nThere are 57 types of relationships with time qualifier in the HyperRED data set. We have deleted the following relations:\n\nDistant supervision of these relations results in excessive noise:\n\nShares_Border_With, Country_Of_Citizenship, Instance_Of, League, Subclass_Of, Part_Of, Partner_In_Business_Or_Sport.\n\nEasily confused relations:\n\nHead_Of_State (Head_Of_Government), Location (Located_In_The_Administrative_Territorial_Entity), Part_Of (Member_of), Participating_team (Participant), Voice_Actor (Performer).\n\nIn ComplexTRED, we delete 5 relations from HyperRED-Temporal:\n\nDistant supervision of these relations results in excessive noise:\n\nFollowed_By (Replaces), Award_Received (Winner), Head_Of_Government (Position held),\n\nEasily confused relations:\n\nConnecting_Line, Stock_Exchange."
    }
}
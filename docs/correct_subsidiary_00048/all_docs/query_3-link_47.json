{
    "id": "correct_subsidiary_00048_3",
    "rank": 47,
    "data": {
        "url": "https://arxiv.org/html/2310.19791v4",
        "read_more_link": "",
        "language": "en",
        "title": "Lilo: Learning Interpretable Libraries by Compressing and Documenting Code",
        "top_image": "",
        "meta_img": "",
        "images": [
            "https://arxiv.org/html/x1.png",
            "https://arxiv.org/html/x2.png",
            "https://arxiv.org/html/x3.png",
            "https://arxiv.org/html/x4.png",
            "https://arxiv.org/html/x5.png",
            "https://arxiv.org/html/x6.png",
            "https://arxiv.org/html/x7.png",
            "https://arxiv.org/html/x8.png",
            "https://arxiv.org/html/x9.png",
            "https://arxiv.org/html/x10.png",
            "https://arxiv.org/html/x11.png",
            "https://arxiv.org/html/x12.png",
            "https://arxiv.org/html/x13.png",
            "https://arxiv.org/html/x14.png",
            "https://arxiv.org/html/x15.png",
            "https://arxiv.org/html/x16.png",
            "https://arxiv.org/html/x17.png",
            "https://arxiv.org/html/x18.png",
            "https://arxiv.org/html/extracted/5473947/figures/appendix/lilo_stitch_midjourney.png"
        ],
        "movies": [],
        "keywords": [],
        "meta_keywords": [
            ""
        ],
        "tags": null,
        "authors": [],
        "publish_date": null,
        "summary": "",
        "meta_description": "",
        "meta_lang": "en",
        "meta_favicon": "",
        "meta_site_name": "",
        "canonical_link": null,
        "text": "HTML conversions sometimes display errors due to content that did not convert correctly from the source. This paper uses the following packages that are not yet supported by the HTML conversion tool. Feedback on these issues are not necessary; they are known and are being worked on.\n\nfailed: mwe\n\nfailed: minitoc\n\nfailed: algpseudocodex\n\nfailed: inconsolata\n\nAuthors: achieve the best HTML results from your LaTeX submissions by following these best practices.\n\nLicense: arXiv.org perpetual non-exclusive license\n\narXiv:2310.19791v4 [cs.CL] 15 Mar 2024\n\nLilo: Learning Interpretable Libraries by\n\nCompressing and Documenting Code\n\nGabriel Grand1,2 Lionel Wong2 Maddy Bowers1 Theo X. Olausson1 Muxin Liu3\n\nJoshua B. Tenenbaum1,2 Jacob Andreas1\n\n1MIT CSAIL 2MIT Brain and Cognitive Sciences 3Harvey Mudd College\n\nCorrespondence to gg@mit.edu. Code for this paper is available at: github.com/gabegrand/lilo.\n\nAbstract\n\nWhile large language models (LLMs) now excel at code generation, a key aspect of software development is the art of refactoring: consolidating code into libraries of reusable and readable programs. In this paper, we introduce Lilo, a neurosymbolic framework that iteratively synthesizes, compresses, and documents code to build libraries tailored to particular problem domains. Lilo combines LLM-guided program synthesis with recent algorithmic advances in automated refactoring from Stitch: a symbolic compression system that efficiently identifies optimal ŒªùúÜ\\lambdaitalic_Œª-abstractions across large code corpora. To make these abstractions interpretable, we introduce an auto-documentation (AutoDoc) procedure that infers natural language names and docstrings based on contextual examples of usage. In addition to improving human readability, we find that AutoDoc boosts performance by helping Lilo‚Äôs synthesizer to interpret and deploy learned abstractions. We evaluate Lilo on three inductive program synthesis benchmarks for string editing, scene reasoning, and graphics composition. Compared to existing methods‚Äîincluding the state-of-the-art library learning algorithm DreamCoder‚ÄîLilo solves more complex tasks and learns richer libraries that are grounded in linguistic knowledge.\n\n\\doparttoc\\faketableofcontents\n\n1 Introduction\n\nLarge language models (LLMs) are growing highly adept at programming in many settings: completing partially-written code (Chen et al., 2021; Fried et al., 2022; Li et al., 2023), conversing with human programmers (Austin et al., 2021; Nijkamp et al., 2023), and even solving competition-level programming puzzles (Hendrycks et al., 2021; Li et al., 2022; Haluptzok et al., 2022; OpenAI, 2023). However, beyond solving the immediate task at hand, software engineers are concerned with building libraries that can be applied to entire problem domains. To this end, a key aspect of software development is the art of refactoring (Brown et al., 1998; Abrahamsson et al., 2017): identifying abstractions that make the codebase more concise (consolidating shared structure), reusable (generalizing to new tasks), and readable (intuitive to other programmers). Solving this multi-objective optimization will require broadening the scope of existing code completion tools‚Äîwhich are already used by millions of programmers‚Äîto approach the problem of library learning.\n\nIn this paper, we combine language models with recent algorithmic advances in automated refactoring from the programming languages (PL) literature to learn libraries of reusable function abstractions. We introduce Lilo, a neurosymbolic framework for Library Induction from Language Observations, which consists of three interconnected modules (Fig. 1):\n\n‚Ä¢\n\nA dual-system synthesis module, which searches for solutions to programming tasks using two distinct methods: LLM-guided search imbues the system with strong domain-general priors, and enumerative search enables the discovery of domain-specific expressions;\n\n‚Ä¢\n\nA compression module, which identifies useful abstractions from the existing solution set via Stitch (Bowers et al., 2023), a high-performance symbolic compression system;\n\n‚Ä¢\n\nAn auto-documentation (AutoDoc) module, which generates human-readable function names and docstrings, yielding better interpretability and improving downstream LLM-guided search.\n\nOur architecture draws inspiration from DreamCoder (Ellis et al., 2021), an iterative Wake-Sleep algorithm that alternates between searching for solutions to programming tasks (Wake phase) and refactoring shared abstractions into a library (Sleep phase) that in turn helps to guide search. Unlike standard deep learning approaches, DreamCoder can make strong generalizations from just a handful of examples, and the model‚Äôs conceptual knowledge is represented symbolically in the learned library. However, DreamCoder‚Äôs search procedure is extremely computationally intensive, requiring more than two CPU-months just to learn a single domain (see Ellis et al., 2021, Apx. J). Much of this search time is spent ‚Äúgetting off the ground‚Äù: discovering a basic set of abstractions that human programmers typically already know, or might be able to grok quickly based on having solved problems in other domains. Moreover, DreamCoder libraries are not necessarily interpretable, requiring both domain expertise and knowledge of lambda calculus to decipher.\n\nTo address these issues, Lilo leverages LLMs in two novel ways: (1) to expedite the discovery of program solutions during search, and (2) to improve the interpretability of learned libraries through documentation. We evaluate Lilo against a language-guided DreamCoder variant on three challenging program synthesis domains: string editing with regular expressions (Andreas et al., 2018), scene reasoning on the CLEVR dataset (Johnson et al., 2017), and graphics composition in the 2D Logo turtle graphics language (Abelson & diSessa, 1986). On all three domains, Lilo solves more tasks than DreamCoder and learns empirically richer libraries containing abstractions that are intractable to discover with existing methods. For instance, Lilo learns the concept of a vowel (Fig. 2), which is a key stepping-stone in the string editing domain that would otherwise require searching over 265superscript26526^{5}26 start_POSTSUPERSCRIPT 5 end_POSTSUPERSCRIPT possible disjunctions of character primitives. Unlike LLM-only baselines‚Äîwhich can solve similar tasks‚ÄîLilo compresses this knowledge into symbolic abstractions that are useful for both traditional search methods and LLM-guided synthesis. Key to this neurosymbolic integration is our AutoDoc module, which not only improves interpretability, but also helps the LLM synthesizer use the library more effectively. Lilo illustrates how ideas and tools from the PL community can be integrated with recent breakthroughs in language models, representing a new evolution in a long line of work in inductive program synthesis, which we review below.\n\n2 Preliminaries: Program Search and Library Learning\n\nProgram synthesis. In inductive program synthesis (Gulwani et al., 2017), we are given a library of primitives ‚Ñí={f1,f2,‚Ä¶}‚Ñísubscriptùëì1subscriptùëì2‚Ä¶\\mathcal{L}=\\{f_{1},f_{2},\\ldots\\}caligraphic_L = { italic_f start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , italic_f start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT , ‚Ä¶ } that forms a domain-specific language (DSL). For a given programming task t={(xi,yi)}ùë°subscriptùë•ùëñsubscriptùë¶ùëñt=\\{(x_{i},y_{i})\\}italic_t = { ( italic_x start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT , italic_y start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ) } specified as a set of input-output pairs, the goal is to find a program œÄ:‚àÄiœÄ‚Å¢(xi)=yi:ùúãsubscriptfor-allùëñùúãsubscriptùë•ùëñsubscriptùë¶ùëñ\\pi:\\forall_{i}\\pi(x_{i})=y_{i}italic_œÄ : ‚àÄ start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT italic_œÄ ( italic_x start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ) = italic_y start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT that correctly maps all inputs to outputs, denoted œÄ‚ä¢tprovesùúãùë°\\pi\\vdash titalic_œÄ ‚ä¢ italic_t. However, a typical task admits many such solutions that will not necessarily generalize (for instance, a simple lookup table). To address this inherent under-specification, concern is given to finding an optimal program œÄ^‚ä¢tproves^ùúãùë°\\hat{\\pi}\\vdash tover^ start_ARG italic_œÄ end_ARG ‚ä¢ italic_t with respect to descriptive complexity (Solomonoff, 1964; Kolmogorov, 1965; Chaitin, 1966). This optimization is naturally framed in terms of probabilistic inference:\n\narg‚Å¢maxœÄ‚Å°log‚Å°p‚Å¢(œÄ‚à£t,‚Ñí)=arg‚Å¢maxœÄ‚Å°[log‚Å°p‚Å¢(t‚à£œÄ)+log‚Å°p‚Å¢(œÄ‚à£‚Ñí)]subscriptargmaxùúãùëùconditionalùúãùë°‚Ñísubscriptargmaxùúãùëùconditionalùë°ùúãùëùconditionalùúã‚Ñí\\operatorname*{arg\\,max}_{\\pi}\\log p(\\pi\\mid t,\\mathcal{L})=\\operatorname*{arg% \\,max}_{\\pi}\\left[\\log p(t\\mid\\pi)+\\log p(\\pi\\mid\\mathcal{L})\\right]start_OPERATOR roman_arg roman_max end_OPERATOR start_POSTSUBSCRIPT italic_œÄ end_POSTSUBSCRIPT roman_log italic_p ( italic_œÄ ‚à£ italic_t , caligraphic_L ) = start_OPERATOR roman_arg roman_max end_OPERATOR start_POSTSUBSCRIPT italic_œÄ end_POSTSUBSCRIPT [ roman_log italic_p ( italic_t ‚à£ italic_œÄ ) + roman_log italic_p ( italic_œÄ ‚à£ caligraphic_L ) ] (1)\n\nIn a typical setting, the likelihood p‚Å¢(t‚à£œÄ)‚âúùüôœÄ‚ä¢t‚âúùëùconditionalùë°ùúãsubscript1provesùúãùë°p(t\\mid\\pi)\\triangleq\\mathbbm{1}_{\\pi\\vdash t}italic_p ( italic_t ‚à£ italic_œÄ ) ‚âú blackboard_1 start_POSTSUBSCRIPT italic_œÄ ‚ä¢ italic_t end_POSTSUBSCRIPT is computed via program execution, while the prior p‚Å¢(œÄ‚à£‚Ñí)‚âú‚àèf‚ààœÄp‚Å¢(f‚à£‚Ñí)‚âúùëùconditionalùúã‚Ñísubscriptproductùëìùúãùëùconditionalùëì‚Ñíp(\\pi\\mid\\mathcal{L})\\triangleq\\prod_{f\\in\\pi}p(f\\mid\\mathcal{L})italic_p ( italic_œÄ ‚à£ caligraphic_L ) ‚âú ‚àè start_POSTSUBSCRIPT italic_f ‚àà italic_œÄ end_POSTSUBSCRIPT italic_p ( italic_f ‚à£ caligraphic_L ) is defined under a probabilistic context free grammar (PFCG; Johnson, 1998) that assigns a weight 0‚â§Œ∏‚â§10ùúÉ10\\leq\\theta\\leq 10 ‚â§ italic_Œ∏ ‚â§ 1 to each production rule. This is equivalent to a weighted description length prior, where longer programs have lower probability.\n\nThis formulation highlights the central challenge of program synthesis: historically, approaches to Eq. 1 have inevitably involved enumerative search through a combinatoral space of programs. A range of techniques have been proposed to improve search tractability, including type-directed synthesis (Polikarpova et al., 2016), Monte Carlo approximation (Liang et al., 2010; Allamanis et al., 2018; Shin et al., 2019), and neural network guidance (Gulwani et al., 2015; Balog et al., 2017; Parisotto et al., 2017; Devlin et al., 2017; Nye et al., 2019; Ellis et al., 2021). However, even with these methods, traditional program synthesis hinges critically on DSL design. Omission of key primitives can make complex tasks unsolvable, while inclusion of extraneous primitives can make search intractable. Consequently, DSL engineering is a painstaking process that requires significant expertise to anticipate common patterns across tasks in a domain.\n\nLibrary learning. While classical approaches focus on synthesizing the best program for a task specification given a fixed DSL (as in Eq. 1), programmers in the wild are typically concerned with solving entire problem domains. Given the difficulty of manual DSL engineering, a natural evolution is to include ‚Ñí‚Ñí\\mathcal{L}caligraphic_L itself as part of the optimization problem. This is the main intuition behind library learning methods (Liang et al., 2010; Dechter et al., 2013; Lake et al., 2015; Shin et al., 2019; L√°zaro-Gredilla et al., 2019; Ellis et al., 2018; 2021), which start with a collection of tasks ùíØ={t1,t2,‚Ä¶}ùíØsubscriptùë°1subscriptùë°2‚Ä¶\\mathcal{T}=\\{t_{1},t_{2},\\ldots\\}caligraphic_T = { italic_t start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , italic_t start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT , ‚Ä¶ } and a base library ‚Ñí0subscript‚Ñí0\\mathcal{L}_{0}caligraphic_L start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT, and jointly infer an expanded library ‚Ñí=‚Ñí0‚à™{f1*,‚Ä¶,fk*}‚Ñísubscript‚Ñí0subscriptsuperscriptùëì1‚Ä¶subscriptsuperscriptùëìùëò\\mathcal{L}=\\mathcal{L}_{0}\\cup\\{f^{*}_{1},\\ldots,f^{*}_{k}\\}caligraphic_L = caligraphic_L start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT ‚à™ { italic_f start_POSTSUPERSCRIPT * end_POSTSUPERSCRIPT start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , ‚Ä¶ , italic_f start_POSTSUPERSCRIPT * end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT } that includes additional abstractions f*superscriptùëìf^{*}italic_f start_POSTSUPERSCRIPT * end_POSTSUPERSCRIPT built from ‚Ñí0subscript‚Ñí0\\mathcal{L}_{0}caligraphic_L start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT (Fig. 1B) and programs Œ†={œÄ1,œÄ2,‚Ä¶}Œ†subscriptùúã1subscriptùúã2‚Ä¶\\Pi=\\{\\pi_{1},\\pi_{2},\\ldots\\}roman_Œ† = { italic_œÄ start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , italic_œÄ start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT , ‚Ä¶ } written in terms of ‚Ñí‚Ñí\\mathcal{L}caligraphic_L:\n\narg‚Å¢maxŒ†,‚Ñí‚Å°log‚Å°p‚Å¢(Œ†,‚Ñí‚à£ùíØ,‚Ñí0)=arg‚Å¢maxŒ†,‚Ñí‚Å°[‚àët‚ààùíØlog‚Å°p‚Å¢(t‚à£œÄt)+log‚Å°p‚Å¢(œÄt‚à£‚Ñí)]+log‚Å°p‚Å¢(‚Ñí‚à£‚Ñí0)subscriptargmaxŒ†‚ÑíùëùŒ†conditional‚ÑíùíØsubscript‚Ñí0subscriptargmaxŒ†‚Ñísubscriptùë°ùíØùëùconditionalùë°subscriptùúãùë°ùëùconditionalsubscriptùúãùë°‚Ñíùëùconditional‚Ñísubscript‚Ñí0\\operatorname*{arg\\,max}_{\\Pi,\\mathcal{L}}\\log p(\\Pi,\\mathcal{L}\\mid\\mathcal{T% },\\mathcal{L}_{0})=\\operatorname*{arg\\,max}_{\\Pi,\\mathcal{L}}\\left[\\sum_{t\\in% \\mathcal{T}}\\log p(t\\mid\\pi_{t})+\\log p(\\pi_{t}\\mid\\mathcal{L})\\right]+\\log p(% \\mathcal{L}\\mid\\mathcal{L}_{0})start_OPERATOR roman_arg roman_max end_OPERATOR start_POSTSUBSCRIPT roman_Œ† , caligraphic_L end_POSTSUBSCRIPT roman_log italic_p ( roman_Œ† , caligraphic_L ‚à£ caligraphic_T , caligraphic_L start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT ) = start_OPERATOR roman_arg roman_max end_OPERATOR start_POSTSUBSCRIPT roman_Œ† , caligraphic_L end_POSTSUBSCRIPT [ ‚àë start_POSTSUBSCRIPT italic_t ‚àà caligraphic_T end_POSTSUBSCRIPT roman_log italic_p ( italic_t ‚à£ italic_œÄ start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ) + roman_log italic_p ( italic_œÄ start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ‚à£ caligraphic_L ) ] + roman_log italic_p ( caligraphic_L ‚à£ caligraphic_L start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT ) (2)\n\nThis objective carries over the program prior and likelihood from Eq. 1, but introduces a distribution over libraries p‚Å¢(‚Ñí‚à£‚Ñí0)ùëùconditional‚Ñísubscript‚Ñí0p(\\mathcal{L}\\mid\\mathcal{L}_{0})italic_p ( caligraphic_L ‚à£ caligraphic_L start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT ), typically also defined in terms of description length. Intuitively, Eq. 2 is optimized by inventing abstractions that are both reusable, simplifying the solutions to multiple tasks in ùíØùíØ\\mathcal{T}caligraphic_T; and concise, ideally building on one another hierarchically so as to share logic. Ellis et al. (2021) approximate Eq. 2 via coordinate ascent, alternating between a search step, which holds the library fixed and searches for task solutions Œ†Œ†\\Piroman_Œ†, and a refactoring step, which extracts common structure from the solution set to update ‚Ñí‚Ñí\\mathcal{L}caligraphic_L. The tractability of this approach hinges critically on the ability to do efficient refactoring, which we discuss further in Section 3.\n\nLeveraging language guidance. Given the size of the search space, generic priors such as description length are not always sufficient to solve Eq. 1; for this reason, a line of work considers natural language task descriptions dtsubscriptùëëùë°d_{t}italic_d start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT as an additional source of learning signal (Manshadi et al., 2013; Raza et al., 2015; Shin et al., 2019). Traditionally, making use of such descriptions has required learning a domain-specific semantic parsing model (Liang et al., 2011; Artzi & Zettlemoyer, 2013; Chen & Mooney, 2011). More recent work (Rahmani et al., 2021; Yin et al., 2022; Zelikman et al., 2022) uses LLMs, which excel when ‚Ñí‚Ñí\\mathcal{L}caligraphic_L resembles a common programming language that is well-represented in pretraining.\n\nIn library learning settings‚Äîwhere ‚Ñí‚Ñí\\mathcal{L}caligraphic_L is novel by construction‚Äîit is currently less clear how to leverage language. In LAPS (Language for Abstraction and Program Search), Wong et al. (2021) generalize Eq. 2 to condition on dtsubscriptùëëùë°d_{t}italic_d start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT by fitting an inverted ‚Äúprogram-to-language‚Äù translation model. However, learning this mapping from scratch necessitates the use of a small alignment model (IBM Model 4; Brown et al., 1993) that makes strict token-to-token decomposition assumptions. In Lilo, we take the opposite approach: we start with a large model that already has strong priors over the joint distribution of language and code; then, we adapt the library to resemble this distribution by building up contextual examples and documentation. In contrast to simply picking a more common ‚Ñí‚Ñí\\mathcal{L}caligraphic_L (e.g., Python) to work in, this procedure enables us to learn a new ‚Ñí‚Ñí\\mathcal{L}caligraphic_L on-the-fly that is both optimized for the domain and grounded in natural language.\n\n3 LILO: Library Induction with Language Observations\n\nOur method, Lilo, aims to combine the strong inductive search biases encoded by LLMs with the key ideas from classical methods in library learning‚Äînamely, the ability to discover new symbolic abstractions through refactoring. Algorithmically, Lilo (Alg. 1) has a similar structure to existing approaches that optimize Eq. 2 via coordinate ascent, alternating between search and refactoring. However, unlike prior work, our use of LLMs for search necessitates an additional step‚ÄîAutoDoc‚Äîto render the learned abstractions legible to the synthesizer. We detail each of these modules below.\n\nDual-system program search (Fig. 1A). Inspired by dual process theories of cognition (Sloman, 1996; Evans, 2003; Kahneman, 2011), Lilo is equipped with two distinct search procedures. We use a LLM as a ‚Äúfast‚Äù approximate search model in string space that leverages strong inductive biases learned in pretraining. After this first pass, we perform ‚Äúslow‚Äù enumerative search in program space, using a task-conditioned PCFG inferred by a ‚Äúrecognition network‚Äù for guidance. We focus here on LLM-guided synthesis and refer to Ellis et al., 2021 (Apx. E and I) for details on enumeration.\n\nFormally, we write pLLM‚Å¢(y‚à£x)subscriptùëùLLMconditionalùë¶ùë•p_{\\text{LLM}}(y\\mid x)italic_p start_POSTSUBSCRIPT LLM end_POSTSUBSCRIPT ( italic_y ‚à£ italic_x ) to denote the distribution over strings yùë¶yitalic_y produced by a language model prompted with string xùë•xitalic_x. Then, for some target task t^^ùë°{\\hat{t}}over^ start_ARG italic_t end_ARG, our goal is to approximate the conditional distribution over programs\n\np‚Å¢(œÄt^‚à£‚Ñí,Œ†,dt^)‚âàpLLM‚Å¢(‚ü®œÄt^‚ü©‚à£‚ü®f‚à£f‚àà‚Ñí‚ü©‚èülibrary functions‚àò‚ü®(dt,œÄt)‚à£œÄt‚àºŒ†‚ü©‚èüprogram examples‚àò‚ü®dt^‚ü©‚èütask desc.)ùëùconditionalsubscriptùúã^ùë°‚ÑíŒ†subscriptùëë^ùë°subscriptùëùLLMconditionaldelimited-‚ü®‚ü©subscriptùúã^ùë°subscript‚èüinner-productùëìùëì‚Ñílibrary functionssubscript‚èüinner-productsubscriptùëëùë°subscriptùúãùë°similar-tosubscriptùúãùë°Œ†program examplessubscript‚èüdelimited-‚ü®‚ü©subscriptùëë^ùë°task desc.p(\\pi_{\\hat{t}}\\mid\\mathcal{L},\\Pi,d_{\\hat{t}})\\approx p_{\\text{LLM}}(\\langle% \\pi_{\\hat{t}}\\rangle\\mid\\underbrace{\\langle f\\mid f\\in\\mathcal{L}\\rangle}_{% \\text{library functions}}\\circ\\underbrace{\\langle(d_{t},\\pi_{t})\\mid\\pi_{t}% \\sim\\Pi\\rangle}_{\\text{program examples}}\\circ\\!\\!\\!\\underbrace{\\langle d_{% \\hat{t}}\\rangle}_{\\begin{subarray}{c}\\text{task desc.}\\end{subarray}}\\!\\!\\!)italic_p ( italic_œÄ start_POSTSUBSCRIPT over^ start_ARG italic_t end_ARG end_POSTSUBSCRIPT ‚à£ caligraphic_L , roman_Œ† , italic_d start_POSTSUBSCRIPT over^ start_ARG italic_t end_ARG end_POSTSUBSCRIPT ) ‚âà italic_p start_POSTSUBSCRIPT LLM end_POSTSUBSCRIPT ( ‚ü® italic_œÄ start_POSTSUBSCRIPT over^ start_ARG italic_t end_ARG end_POSTSUBSCRIPT ‚ü© ‚à£ under‚èü start_ARG ‚ü® italic_f ‚à£ italic_f ‚àà caligraphic_L ‚ü© end_ARG start_POSTSUBSCRIPT library functions end_POSTSUBSCRIPT ‚àò under‚èü start_ARG ‚ü® ( italic_d start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT , italic_œÄ start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ) ‚à£ italic_œÄ start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ‚àº roman_Œ† ‚ü© end_ARG start_POSTSUBSCRIPT program examples end_POSTSUBSCRIPT ‚àò under‚èü start_ARG ‚ü® italic_d start_POSTSUBSCRIPT over^ start_ARG italic_t end_ARG end_POSTSUBSCRIPT ‚ü© end_ARG start_POSTSUBSCRIPT start_ARG start_ROW start_CELL task desc. end_CELL end_ROW end_ARG end_POSTSUBSCRIPT ) (3)\n\nwhere ‚ü®‚Ä¶‚ü©delimited-‚ü®‚ü©‚Ä¶\\langle\\ldots\\rangle‚ü® ‚Ä¶ ‚ü© and ‚àò\\circ‚àò denote string serialization and concatenation, respectively. To sample from the distribution in Eq. 3, we procedurally construct few-shot prompts consisting of three parts: (1) A library description that enumerates the available primitives and any learned abstractions, (2) a set of exemplars consisting of description-solution pairs (dt,œÄt)‚àºŒ†similar-tosubscriptùëëùë°subscriptùúãùë°Œ†(d_{t},\\pi_{t})\\sim\\Pi( italic_d start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT , italic_œÄ start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ) ‚àº roman_Œ† sampled from the set of solved tasks, and (3) a description of the target task dt^subscriptùëë^ùë°d_{\\hat{t}}italic_d start_POSTSUBSCRIPT over^ start_ARG italic_t end_ARG end_POSTSUBSCRIPT. For each completion, we run parsing, type inference, and execution checks to identify valid programs that solve the target task. (Appendix A.1 illustrates the composition of a typical prompt; A.3 contains additional details on how examples are sampled.)\n\nRefactoring via Stitch compression (Fig. 1B). As the learner solves more tasks, the solution set will grow to contain many recurring program fragments that we wish to refactor into a set of reusable abstractions. In library learning systems that rely on enumeration, refactoring improves search efficiency by avoiding the need to rediscover key building blocks for each new task. Analogously, in Lilo, refactoring makes the generation task easier: a LLM equipped with a library of abstractions can deploy entire blocks of code with just a few tokens. Additionally, refactoring helps to mitigate LLM context window limits by minimizing the description length of the few-shot examples.\n\nVarious algorithms for refactoring have been proposed using combinatory logic (Liang et al., 2010), tree substitution grammars (Allamanis & Sutton, 2014; Allamanis et al., 2018; Shin et al., 2019), version spaces (Lau & Weld, 1998; Ellis et al., 2021), and e-graphs (Cao et al., 2023). In Lilo, we cast refactoring as a compression problem over a corpus of programs\n\nf*=compress‚Ñí‚Å¢(Œ†)=arg‚Å¢minf‚Å°|f|+‚àëœÄ‚ààŒ†|rewrite‚Ñí‚à™{f}‚Å¢(œÄ)|superscriptùëì‚ÑícompressŒ†subscriptargminùëìùëìsubscriptùúãŒ†‚Ñíùëìrewriteùúãf^{*}=\\underset{\\mathcal{L}}{\\textsc{compress}}(\\Pi)=\\operatorname*{arg\\,min}_% {f}{|f|+\\sum_{\\pi\\in\\Pi}|\\underset{\\mathcal{L}\\cup\\{f\\}}{\\textsc{rewrite}}(\\pi% )|}italic_f start_POSTSUPERSCRIPT * end_POSTSUPERSCRIPT = undercaligraphic_L start_ARG compress end_ARG ( roman_Œ† ) = start_OPERATOR roman_arg roman_min end_OPERATOR start_POSTSUBSCRIPT italic_f end_POSTSUBSCRIPT | italic_f | + ‚àë start_POSTSUBSCRIPT italic_œÄ ‚àà roman_Œ† end_POSTSUBSCRIPT | start_UNDERACCENT caligraphic_L ‚à™ { italic_f } end_UNDERACCENT start_ARG rewrite end_ARG ( italic_œÄ ) | (4)\n\nwhere the goal is to identify abstractions with minimal description length |f|ùëì|f|| italic_f | that facilitate efficient rewriting of Œ†Œ†\\Piroman_Œ†. However, performing even a single round of compression as in Eq. 4 necessitates an efficient search strategy. In Lilo, we leverage recent algorithmic advances from Stitch (Bowers et al., 2023): a symbolic compression system that uses branch-and-bound search to identify reusable abstractions in large datasets of lambda calculus programs. While Bowers et al. demonstrate that Stitch is 1000‚Äì10000x faster and 100x more memory efficient than DreamCoder‚Äôs compression algorithm, prior analyses were limited to static corpora of ground truth programs. In Lilo, we deploy Stitch for the first time in a synthesis loop and find it similarly performant, typically running in seconds on a single CPU. These efficiency improvements enable us to re-derive the entire library from ‚Ñí0subscript‚Ñí0\\mathcal{L}_{0}caligraphic_L start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT at every iteration (Alg. 1 line 6). While many abstractions remain stable across iterations, this ‚Äúdeep refactoring‚Äù allows Lilo to discard suboptimal abstractions discovered early in learning.\n\nLibrary auto-documentation (Fig. 1C). Unlike traditional program synthesis methods, language models draw inferences about the semantics of programs from their lexical content. In other words, LLMs (like human programmers) care what functions are named. However, PL tools are typically not equipped to write human-readable function names, instead outputting anonymous lambda abstractions (in Stitch, these are represented numerically; e.g., fn_42). In early experiments, we observed that naively providing a LLM with Stitch abstractions measurably degraded its ability to solve tasks (4.1). We hypothesized that rewriting program examples in terms of these anonymous lambdas during the compression step (Eq. 4) was acting as a form of code obfuscation (Srikant et al., 2021; Zeng et al., 2022; Miceli-Barone et al., 2023).\n\nMotivated by these findings, as part of Lilo, we introduce a library auto-documentation (AutoDoc) procedure that writes human-readable names and docstrings for abstractions generated by Stitch. AutoDoc leverages the fact that LLMs are naturally suited to code deobfuscation (Lachaux et al., 2021; Sharma et al., 2022; Cambronero et al., 2023). During AutoDoc, we sequentially prompt an instruction-tuned LLM to produce a human-readable name and docstring for each abstraction in the library. Fig. 2 gives an overview of this workflow (the full prompt is reproduced in A.2). In this example in the REGEX domain, the LLM has solved some problems that require vowel substitutions. During compression, Stitch pulls out the expression (or ‚Äôa‚Äô (or ‚Äôe‚Äô (or ‚Äôi‚Äô (or ‚Äôo‚Äô ‚Äôu‚Äô))))} for occurring commonly in the solution set and defines it as an anonymous arity-0 function (i.e., a constant). Subsequently, AutoDoc names this abstraction \\mintinline[breaklines]lispvowel_regex, which forms the basis for more complex expressions. For instance, consonant is expressed as (not vowel_regex)}, which in turn supports an abstraction for consonant replacement. In \\crefsec:experiments, we explore how AutoDoc benefits downstream synthesis performance, yielding both richer and more interpretable libraries.\n\n4 Experiments and Results\n\nDomains. Our goal is to build a system that can develop expertise in novel technical domains and leverage natural language guidance to bootstrap learning. We evaluate our approach on three inductive program synthesis domains: string editing (REGEX), scene reasoning (CLEVR), and graphics composition (LOGO). Detailed descriptions and metrics pertaining to each domain are provided in B.1. These three domains were introduced in LAPS (Wong et al., 2021) as more challenging versions of the domains evaluated in DreamCoder and our evaluations are directly comparable to prior results (B.3). Following Wong et al., we use synthetic ground truth task descriptions in our primary experiments and evaluate on a noisier set of human language annotations in B.4. Experiment setup. Our experiments are designed to simulate a ‚Äúlifelong learning‚Äù setting where the learner starts from a small seed set of simple examples that it must generalize to solve a broader set of training tasks that range in complexity. Performance is measured as the percentage of tasks solved from an i.i.d. test set. We sequentially perform two experiments that test different aspects of models‚Äô learning. First, in online synthesis, each model runs for a fixed number of iterations, continually updating its library (if applicable) and attempting to solve test tasks. These experiments serve as a general benchmark of language-guided synthesis capabilities. Next, in offline synthesis, we freeze the final library ‚Ñífsubscript‚Ñíùëì\\mathcal{L}_{f}caligraphic_L start_POSTSUBSCRIPT italic_f end_POSTSUBSCRIPT from each online synthesis run and perform enumerative search with no language guidance for a fixed time budget. (To compare against non-library learning baselines, we run Stitch to generate ‚Ñífsubscript‚Ñíùëì\\mathcal{L}_{f}caligraphic_L start_POSTSUBSCRIPT italic_f end_POSTSUBSCRIPT post-hoc.) We hold the hyperparameters of the search fixed so that performance depends entirely on ‚Ñífsubscript‚Ñíùëì\\mathcal{L}_{f}caligraphic_L start_POSTSUBSCRIPT italic_f end_POSTSUBSCRIPT and not on the original model. Thus, the offline synthesis evaluations provide a controlled comparison of the off-the-shelf utility of different learned libraries in the absence of language guidance. Throughout, comparisons between models are expressed in terms of absolute percentage point changes in mean solve rates on the test set. Implementation details. For LLM-guided search, we queried OpenAI‚Äôs Codex model (code-davinci-002) with up to 4 prompts per task, sampling 4 completions per prompt. For AutoDoc (which requires significantly fewer queries) we found that OpenAI‚Äôs newer instruction-tuned models (gpt-3.5-turbo and gpt-4) better adhered to the AutoDoc task and schema. Further implementation details can be found in A.4‚ÄìA.5.\n\n4.1 Evaluation of Synthesis Performance\n\nWe compare Lilo against two baseline models: a language-guided DreamCoder variant from Wong et al. (2021) and a language model (LLM Solver) that does not perform library learning (3). To study the effects of the different Lilo components, we introduce ablated variants that remove the enumerative search and/or AutoDoc steps. To ensure fair comparison and improve overall runtimes, Stitch is used as the compression module for all library learning models, including the DreamCoder baseline. Tab. 1 gives the full breakdown of task solution rates.\n\nLLMs facilitate effective search over lambda calculus programs. Our first question is whether the LLM-based search procedure introduced in Alg. 1 can match the accuracy of enumerative search. Compared to DreamCoder, we find that the LLM Solver, with no library learning, performs significantly better on REGEX (+32.2032.20+32.20+ 32.20), slightly worse on CLEVR (‚àí5.835.83-5.83- 5.83), and comparably on LOGO (+3.603.60+3.60+ 3.60). The improvements on REGEX are primarily attributable to LLMs‚Äô ability to generate expressions for concepts like vowel and consonant that invoke prior knowledge. Lilo achieves the strongest overall performance. As observed in 3, Lilo significantly outperforms DreamCoder on REGEX (+33.1433.14+33.14+ 33.14) and LOGO (+20.4220.42+20.42+ 20.42). It also achieves small improvements on CLEVR (+2.262.26+2.26+ 2.26), though the DreamCoder baseline is already quite high for this domain (Œº=94.50ùúá94.50\\mu=94.50italic_Œº = 94.50, œÉ=2.44ùúé2.44\\sigma=2.44italic_œÉ = 2.44). Moreover, Lilo also improves on the LLM Solver baseline by +0.940.94+0.94+ 0.94 (REGEX), +8.098.09+8.09+ 8.09 (CLEVR), and +16.8216.82+16.82+ 16.82 (LOGO). A key advantage of Lilo is the ability to perform enumerative search, which aids in the discovery novel programs that differ structurally from existing solutions in the LLM prompts. To isolate the effects of search, we ran an ablation [Lilo (‚úÇ Search)] as well as an augmented baseline [LLM Solver (+ Search)]. We find that enumerative search is most helpful on LOGO, which requires certain domain-specific program structures (e.g., how to draw a ‚Äúsnowflake‚Äù or ‚Äústaircase‚Äù; see 5) that are difficult to infer from language alone. Auto-documentation unlocks effective contextual usage of abstractions. Early experiments interfacing the LLM Solver with Stitch [Tab. 1, Lilo (‚úÇ Search / AutoDoc)] revealed a puzzling finding: providing the LLM with abstractions did not help‚Äîand in some cases, significantly hurt‚Äîdownstream synthesis performance. Relative to the LLM Solver baseline, we observed solution rate changes of ‚àí30.6030.60-30.60- 30.60 (REGEX), ‚àí2.912.91-2.91- 2.91 (CLEVR), and ‚àí11.1111.11-11.11- 11.11 (LOGO) after introducing Stitch compression. Qualitative inspection found that GPT struggled to deploy anonymous abstractions in contextually-appropriate ways, which motivated our exploration of de-obfuscation methods (Section 3). After introducing AutoDoc [Tab. 1, Lilo (‚úÇ Search)], we see mean improvements of +9.739.73+9.73+ 9.73 (REGEX) and +2.272.27+2.27+ 2.27 (CLEVR) over the naive condition. On LOGO, AutoDoc does not improve performance (+0.000.00+0.00+ 0.00). We attribute this to the occurrence of semantic errors, which we analyze further in 4.2. Lilo rivals symbolic search in terms of computational efficiency. Compared to enumerative search, we find that we are able to achieve faster overall wall clock runtimes with LLM-based search due to orders of magnitude better sample efficiency (see C.1). Indeed, in terms of dollar cost, we estimate that one round of LLM synthesis is equivalent to 48 CPU-hours of DreamCoder search. Of course, LLM-based and enumerative search are not mutually exclusive: our main Lilo variant integrates both of these procedures and achieves the highest solve rates of all models we evaluated. Our findings make a strong case that LLMs can reduce the need for exhaustive search in synthesis domains where language annotations are available.\n\nLilo libraries generalize well even in the absence of language. In our offline synthesis experiments, we tested each model‚Äôs final library ‚Ñífsubscript‚Ñíùëì\\mathcal{L}_{f}caligraphic_L start_POSTSUBSCRIPT italic_f end_POSTSUBSCRIPT in an off-the-shelf setting with no language guidance. 4 shows the results of these evaluations (metrics are given in Tab. 1, lower). As the baseline for each domain, we measure synthesis performance in ‚Ñí0subscript‚Ñí0\\mathcal{L}_{0}caligraphic_L start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT (Base DSL). As expected, we can significantly outperform ‚Ñí0subscript‚Ñí0\\mathcal{L}_{0}caligraphic_L start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT using library learning: DreamCoder‚Äôs ‚Ñífsubscript‚Ñíùëì\\mathcal{L}_{f}caligraphic_L start_POSTSUBSCRIPT italic_f end_POSTSUBSCRIPT improves on the ‚Ñí0subscript‚Ñí0\\mathcal{L}_{0}caligraphic_L start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT solve rates by +19.619.6+19.6+ 19.6 (REGEX), +62.4662.46+62.46+ 62.46 (CLEVR), and +29.7329.73+29.73+ 29.73 (LOGO). Moreover, in each domain, Lilo‚Äôs ‚Ñífsubscript‚Ñíùëì\\mathcal{L}_{f}caligraphic_L start_POSTSUBSCRIPT italic_f end_POSTSUBSCRIPT improves further on DreamCoder, showing solution rate gains of +42.2742.27+42.27+ 42.27 (REGEX), +63.4363.43+63.43+ 63.43 (CLEVR), and +43.2443.24+43.24+ 43.24 (LOGO) over ‚Ñí0subscript‚Ñí0\\mathcal{L}_{0}caligraphic_L start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT. Lilo‚Äôs ‚Ñífsubscript‚Ñíùëì\\mathcal{L}_{f}caligraphic_L start_POSTSUBSCRIPT italic_f end_POSTSUBSCRIPT also outperforms libraries derived post-hoc from the two LLM Solver baselines, highlighting the benefits of performing compression and documentation in-the-loop. As these results demonstrate, Lilo learns high-quality libraries that generalize well to downstream synthesis tasks and can be used even in the absence of language guidance.\n\n4.2 Qualitative Analysis of Library Abstractions\n\nIn all three domains, the libraries learned by Lilo exhibit examples of compositional and hierarchical reuse. For instance, in the LOGO library (5 and B.2.3), the top abstraction is a general method for drawing polygons that is invoked by several higher-level abstractions. Similarly, the CLEVR library (Fig. 1 and B.2.2) contains two layers of abstractions: a lower layer implements filter operations over size, color, shape, and material attributes that support a higher layer of more specialized abstractions for counting and filtering. These examples showcase how Lilo builds on one of the main strengths of DreamCoder‚Äîthe ability to bootstrap hierarchies of learned concepts‚Äîwhile improving the richness and interpretability of the learned libraries through auto-documentation. While the GPT models do a remarkable job at inferring program semantics, we observe various cases where they produce unclear or even incorrect documentation. For instance, in LOGO (5), the two polygon functions (fn_27 and fn_34) are assigned relatively uninformative names that emphasize their implementation (looping move and rotate) but not their behavior (drawing polygons). Moreover, because AutoDoc works sequentially, it occasionally ‚Äúdoubles down‚Äù on particular statements that may be correct in one context but not another. For example, AutoDoc correctly notes that fn_27 works by ‚Äúincrementing the angle of each side on each iteration,‚Äù but this idea is ambiguous in fn_31 (which angle?) and incorrect in fn_34 (the length is constant, not doubling). In addition to affecting interpretability, these semantic errors may also impact downstream synthesis performance in LLM-guided search. Future work could adopt self-consistency and verification techniques (Wang et al., 2022; Dhuliawala et al., 2023) to improve the quality of AutoDoc generations.\n\n5 Discussion and Conclusion\n\nIn this work, we introduced Lilo, a neurosymbolic framework for learning interpretable libraries for program synthesis. In our experiments, we found that Lilo performs favorably compared to both DreamCoder and an LLM-only baseline. Much of Lilo‚Äôs advantage comes from synergy between neural and symbolic components: LLM-guided search enables Lilo to learn concepts that are intractable to discover with enumerative search; compression allows Lilo to consolidate these solutions into reusable abstractions available to symbolic search; and finally, AutoDoc makes these abstractions legible for both humans and LLMs. While Lilo improves on prior library learning approaches, notably, the LLM-only baseline also demonstrates the ability to bootstrap its performance over time. This result aligns with recent successes in automated prompting (Zhou et al., 2023; Yao et al., 2022), suggesting that transformer attention can be viewed as implementing a form of non-compressive library learning where task-relevant information is accumulated in the prompt. However, it is unclear whether this approach will scale to large software libraries: as context length grows, key information may be ‚Äúlost in the middle‚Äù or ignored due to ordering effects (O‚ÄôConnor & Andreas, 2021; Lu et al., 2022; Liu et al., 2023). Accordingly, an important line of research looks to develop new strategies for equipping LLMs with long-term memory through retrieval (Wu et al., 2022; Park et al., 2023), self-reflection (Shinn et al., 2023), or combinations of both that enable learning libraries of programmatic skills in embodied environments (Wang et al., 2023). Currently, these approaches face the common challenge of determining what information to preserve, leading to a large space of ad hoc heuristics. Lilo offers a principled approach to the consolidation of knowledge in a lifelong learning setting, adding program compression to a growing toolkit of LLM integrations with symbolic computation (Schick et al., 2023; Wolfram, 2023). Moreover, given the generality of Stitch‚Äôs algorithmic approach, extending Lilo to modern imperative languages (e.g., Python) reduces to a PL research problem that is both presently tractable and technically compelling. In contrast to the view that LLMs will subsume formal accounts of programming languages, Lilo offers a blueprint for collaboration between the ML and PL communities towards the longstanding goal of learning interpretable software libraries that enable solutions to novel problem domains.\n\nAcknowledgments\n\nThis work benefited greatly from discussions with colleagues in academia and industry, including Sam Acquaviva, Ekin Aky√ºrek, Jacob Austin, Armando Solar-Lezama, and Belinda Li. We are thankful to Jack Feser for his OCaml wizardry. Finally, Lilo owes a significant debt to ideas and infrastructure pioneered by Kevin Ellis, and we are deeply grateful for his feedback and guidance. The authors gratefully acknowledge support from the MIT Quest for Intelligence, the MIT-IBM Watson AI Lab, the Intel Corporation, AFOSR, DARPA, and ONR. GG and MB are supported by the National Science Foundation (NSF) under Grant No. 2141064. GG was additionally supported by the MIT Presidential Fellowship. TXO is supported by the Herbert E. Grier (1933) and Dorothy J. Grier Fund Fellowship and the DARPA ASKEM program (award #HR00112220042). GG, MB, TXO, and JA are supported by the National Science Foundation (NSF) and Intel through NSF Grant CCF:2217064. JA is supported by NSF Grant IIS-2238240. LW and JBT received support from AFOSR Grant #FA9550-19-1-0269, the MIT-IBM Watson AI Lab, ONR Science of AI and the DARPA Machine Common Sense program. Any opinions, findings, and conclusions or recommendations expressed in this material are those of the author(s) and do not necessarily reflect the views of sponsors.\n\nReferences\n\nAbelson & diSessa (1986) Harold Abelson and Andrea diSessa. Turtle Geometry: The Computer as a Medium for Exploring Mathematics. The MIT Press, 1986. ISBN 9780262362740. doi: 10.7551/mitpress/6933.001.0001. URL https://doi.org/10.7551/mitpress/6933.001.0001.\n\nAbrahamsson et al. (2017) Pekka Abrahamsson, Outi Salo, Jussi Ronkainen, and Juhani Warsta. Agile software development methods: Review and analysis. ArXiv preprint, abs/1709.08439, 2017. URL https://arxiv.org/abs/1709.08439.\n\nAllamanis & Sutton (2014) Miltiadis Allamanis and Charles Sutton. Mining idioms from source code. In Proceedings of the 22nd ACM SIGSOFT International Symposium on Foundations of Software Engineering, FSE 2014, pp. 472‚Äì483, New York, NY, USA, 2014. Association for Computing Machinery. ISBN 9781450330565. doi: 10.1145/2635868.2635901. URL https://doi.org/10.1145/2635868.2635901.\n\nAllamanis et al. (2018) Miltiadis Allamanis, Earl T. Barr, Christian Bird, Premkumar Devanbu, Mark Marron, and Charles Sutton. Mining Semantic Loop Idioms. IEEE Transactions on Software Engineering, 44(7):651‚Äì668, 2018. ISSN 0098-5589, 1939-3520, 2326-3881. doi: 10.1109/TSE.2018.2832048.\n\nAndreas et al. (2016) Jacob Andreas, Marcus Rohrbach, Trevor Darrell, and Dan Klein. Neural module networks. In 2016 IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2016, Las Vegas, NV, USA, June 27-30, 2016, pp. 39‚Äì48. IEEE Computer Society, 2016. doi: 10.1109/CVPR.2016.12. URL https://doi.org/10.1109/CVPR.2016.12.\n\nAndreas et al. (2018) Jacob Andreas, Dan Klein, and Sergey Levine. Learning with latent language. In Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers), pp. 2166‚Äì2179, New Orleans, Louisiana, 2018. Association for Computational Linguistics. doi: 10.18653/v1/N18-1197. URL https://aclanthology.org/N18-1197.\n\nArtzi & Zettlemoyer (2013) Yoav Artzi and Luke Zettlemoyer. Weakly supervised learning of semantic parsers for mapping instructions to actions. Transactions of the Association for Computational Linguistics, 1:49‚Äì62, 2013. doi: 10.1162/taclÀôaÀô00209. URL https://aclanthology.org/Q13-1005.\n\nAustin et al. (2021) Jacob Austin, Augustus Odena, Maxwell Nye, Maarten Bosma, Henryk Michalewski, David Dohan, Ellen Jiang, Carrie Cai, Michael Terry, Quoc Le, et al. Program synthesis with large language models. ArXiv preprint, abs/2108.07732, 2021. URL https://arxiv.org/abs/2108.07732.\n\nBalog et al. (2017) Matej Balog, Alexander L. Gaunt, Marc Brockschmidt, Sebastian Nowozin, and Daniel Tarlow. Deepcoder: Learning to write programs. In 5th International Conference on Learning Representations, ICLR 2017, Toulon, France, April 24-26, 2017, Conference Track Proceedings. OpenReview.net, 2017. URL https://openreview.net/forum?id=ByldLrqlx.\n\nBowers et al. (2023) Matthew Bowers, Theo X. Olausson, Lionel Wong, Gabriel Grand, Joshua B. Tenenbaum, Kevin Ellis, and Armando Solar-Lezama. Top-down synthesis for library learning. Proc. ACM Program. Lang., 7(POPL), 2023. doi: 10.1145/3571234. URL https://doi.org/10.1145/3571234.\n\nBrown et al. (1993) Peter F. Brown, Stephen A. Della Pietra, Vincent J. Della Pietra, and Robert L. Mercer. The mathematics of statistical machine translation: Parameter estimation. Computational Linguistics, 19(2):263‚Äì311, 1993. URL https://aclanthology.org/J93-2003.\n\nBrown et al. (1998) William H Brown, Raphael C Malveau, Hays W ‚ÄúSkip‚Äù McCormick, and Thomas J Mowbray. AntiPatterns: refactoring software, architectures, and projects in crisis. John Wiley & Sons, Inc., 1998.\n\nCambronero et al. (2023) Jos√© Cambronero, Sumit Gulwani, Vu Le, Daniel Perelman, Arjun Radhakrishna, Clint Simon, and Ashish Tiwari. FlashFill++: Scaling Programming by Example by Cutting to the Chase. Proceedings of the ACM on Programming Languages, 7(POPL):952‚Äì981, 2023. ISSN 2475-1421. doi: 10.1145/3571226.\n\nCao et al. (2023) David Cao, Rose Kunkel, Chandrakana Nandi, Max Willsey, Zachary Tatlock, and Nadia Polikarpova. babble: Learning better abstractions with e-graphs and anti-unification. Proc. ACM Program. Lang., 7(POPL):396‚Äì424, 2023. doi: 10.1145/3571207. URL https://doi.org/10.1145/3571207.\n\nChaitin (1966) Gregory J Chaitin. On the length of programs for computing finite binary sequences. Journal of the ACM (JACM), 13(4):547‚Äì569, 1966.\n\nChen & Mooney (2011) David L. Chen and Raymond J. Mooney. Learning to interpret natural language navigation instructions from observations. In Wolfram Burgard and Dan Roth (eds.), Proceedings of the Twenty-Fifth AAAI Conference on Artificial Intelligence, AAAI 2011, San Francisco, California, USA, August 7-11, 2011. AAAI Press, 2011. URL http://www.aaai.org/ocs/index.php/AAAI/AAAI11/paper/view/3701.\n\nChen et al. (2021) Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Ponde de Oliveira Pinto, Jared Kaplan, Harri Edwards, Yuri Burda, Nicholas Joseph, Greg Brockman, et al. Evaluating large language models trained on code. ArXiv preprint, abs/2107.03374, 2021. URL https://arxiv.org/abs/2107.03374.\n\nDechter et al. (2013) Eyal Dechter, Jonathan Malmaud, Ryan P. Adams, and Joshua B. Tenenbaum. Bootstrap learning via modular concept discovery. In Francesca Rossi (ed.), IJCAI 2013, Proceedings of the 23rd International Joint Conference on Artificial Intelligence, Beijing, China, August 3-9, 2013, pp. 1302‚Äì1309. IJCAI/AAAI, 2013. URL http://www.aaai.org/ocs/index.php/IJCAI/IJCAI13/paper/view/6890.\n\nDevlin et al. (2017) Jacob Devlin, Jonathan Uesato, Surya Bhupatiraju, Rishabh Singh, Abdel-rahman Mohamed, and Pushmeet Kohli. Robustfill: Neural program learning under noisy I/O. In Doina Precup and Yee Whye Teh (eds.), Proceedings of the 34th International Conference on Machine Learning, ICML 2017, Sydney, NSW, Australia, 6-11 August 2017, volume 70 of Proceedings of Machine Learning Research, pp. 990‚Äì998. PMLR, 2017. URL http://proceedings.mlr.press/v70/devlin17a.html.\n\nDhuliawala et al. (2023) Shehzaad Dhuliawala, Mojtaba Komeili, Jing Xu, Roberta Raileanu, Xian Li, Asli Celikyilmaz, and Jason Weston. Chain-of-verification reduces hallucination in large language models. arXiv preprint arXiv:2309.11495, 2023.\n\nEllis et al. (2018) Kevin Ellis, Daniel Ritchie, Armando Solar-Lezama, and Josh Tenenbaum. Learning to infer graphics programs from hand-drawn images. In Samy Bengio, Hanna M. Wallach, Hugo Larochelle, Kristen Grauman, Nicol√≤ Cesa-Bianchi, and Roman Garnett (eds.), Advances in Neural Information Processing Systems 31: Annual Conference on Neural Information Processing Systems 2018, NeurIPS 2018, December 3-8, 2018, Montr√©al, Canada, pp. 6062‚Äì6071, 2018. URL https://proceedings.neurips.cc/paper/2018/hash/6788076842014c83cedadbe6b0ba0314-Abstract.html.\n\nEllis et al. (2021) Kevin Ellis, Catherine Wong, Maxwell Nye, Mathias Sabl√©-Meyer, Lucas Morales, Luke Hewitt, Luc Cary, Armando Solar-Lezama, and Joshua B Tenenbaum. Dreamcoder: Bootstrapping inductive program synthesis with wake-sleep library learning. In Proceedings of the 42nd ACM SIGPLAN International Conference on Programming Language Design and Implementation, pp. 835‚Äì850, 2021.\n\nEvans (2003) Jonathan St.B.T. Evans. In two minds: dual-process accounts of reasoning. Trends in Cognitive Sciences, 7(10):454‚Äì459, 2003. ISSN 1364-6613. doi: https://doi.org/10.1016/j.tics.2003.08.012. URL https://www.sciencedirect.com/science/article/pii/S1364661303002250.\n\nFried et al. (2022) Daniel Fried, Armen Aghajanyan, Jessy Lin, Sida Wang, Eric Wallace, Freda Shi, Ruiqi Zhong, Wen-tau Yih, Luke Zettlemoyer, and Mike Lewis. Incoder: A generative model for code infilling and synthesis. ArXiv preprint, abs/2204.05999, 2022. URL https://arxiv.org/abs/2204.05999.\n\nGothoskar et al. (2021) Nishad Gothoskar, Marco F. Cusumano-Towner, Ben Zinberg, Matin Ghavamizadeh, Falk Pollok, Austin Garrett, Josh Tenenbaum, Dan Gutfreund, and Vikash K. Mansinghka. 3DP3: 3D scene perception via probabilistic programming. In Marc‚ÄôAurelio Ranzato, Alina Beygelzimer, Yann N. Dauphin, Percy Liang, and Jennifer Wortman Vaughan (eds.), Advances in Neural Information Processing Systems 34: Annual Conference on Neural Information Processing Systems 2021, NeurIPS 2021, December 6-14, 2021, virtual, pp. 9600‚Äì9612, 2021. URL https://proceedings.neurips.cc/paper/2021/hash/4fc66104f8ada6257fa55f29a2a567c7-Abstract.html.\n\nGulwani et al. (2015) Sumit Gulwani, Jos√© Hern√°ndez-Orallo, Emanuel Kitzelmann, Stephen H Muggleton, Ute Schmid, and Benjamin Zorn. Inductive programming meets the real world. Communications of the ACM, 58(11):90‚Äì99, 2015.\n\nGulwani et al. (2017) Sumit Gulwani, Oleksandr Polozov, and Rishabh Singh. Program synthesis. Foundations and Trends¬Æ in Programming Languages, 4(1-2):1‚Äì119, 2017. ISSN 2325-1107. doi: 10.1561/2500000010. URL http://dx.doi.org/10.1561/2500000010.\n\nHaluptzok et al. (2022) Patrick Haluptzok, Matthew Bowers, and Adam Tauman Kalai. Language models can teach themselves to program better. ArXiv preprint, abs/2207.14502, 2022. URL https://arxiv.org/abs/2207.14502.\n\nHendrycks et al. (2021) Dan Hendrycks, Steven Basart, Saurav Kadavath, Mantas Mazeika, Akul Arora, Ethan Guo, Collin Burns, Samir Puranik, Horace He, Dawn Song, et al. Measuring coding challenge competence with apps. ArXiv preprint, abs/2105.09938, 2021. URL https://arxiv.org/abs/2105.09938.\n\nHu et al. (2017) Ronghang Hu, Jacob Andreas, Marcus Rohrbach, Trevor Darrell, and Kate Saenko. Learning to reason: End-to-end module networks for visual question answering. In IEEE International Conference on Computer Vision, ICCV 2017, Venice, Italy, October 22-29, 2017, pp. 804‚Äì813. IEEE Computer Society, 2017. doi: 10.1109/ICCV.2017.93. URL https://doi.org/10.1109/ICCV.2017.93.\n\nJohnson et al. (2017) Justin Johnson, Bharath Hariharan, Laurens van der Maaten, Li Fei-Fei, C. Lawrence Zitnick, and Ross B. Girshick. CLEVR: A diagnostic dataset for compositional language and elementary visual reasoning. In 2017 IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2017, Honolulu, HI, USA, July 21-26, 2017, pp. 1988‚Äì1997. IEEE Computer Society, 2017. doi: 10.1109/CVPR.2017.215. URL https://doi.org/10.1109/CVPR.2017.215.\n\nJohnson (1998) Mark Johnson. PCFG models of linguistic tree representations. Computational Linguistics, 24(4):613‚Äì632, 1998. URL https://aclanthology.org/J98-4004.\n\nKahneman (2011) Daniel Kahneman. Thinking, fast and slow. Farrar, Straus and Giroux, New York, 2011. ISBN 9780374275631 0374275637.\n\nKersten et al. (2004) Daniel Kersten, Pascal Mamassian, and Alan Yuille. Object perception as Bayesian inference. Annu. Rev. Psychol., 55:271‚Äì304, 2004.\n\nKnill & Richards (1996) David C Knill and Whitman Richards. Perception as Bayesian inference. Cambridge University Press, 1996.\n\nKolmogorov (1965) Andrei Nikolaevich Kolmogorov. Three approaches for defining the concept of information quantity. Problemy peredaci informacii, 1:3‚Äì11, 1965.\n\nLachaux et al. (2021) Marie-Anne Lachaux, Baptiste Rozi√®re, Marc Szafraniec, and Guillaume Lample. DOBF: A deobfuscation pre-training objective for programming languages. In Marc‚ÄôAurelio Ranzato, Alina Beygelzimer, Yann N. Dauphin, Percy Liang, and Jennifer Wortman Vaughan (eds.), Advances in Neural Information Processing Systems 34: Annual Conference on Neural Information Processing Systems 2021, NeurIPS 2021, December 6-14, 2021, virtual, pp. 14967‚Äì14979, 2021. URL https://proceedings.neurips.cc/paper/2021/hash/7d6548bdc0082aacc950ed35e91fcccb-Abstract.html.\n\nLake et al. (2015) Brenden M. Lake, Ruslan Salakhutdinov, and Joshua B. Tenenbaum. Human-level concept learning through probabilistic program induction. Science, 350(6266):1332‚Äì1338, 2015. doi: 10.1126/science.aab3050. URL https://www.science.org/doi/abs/10.1126/science.aab3050.\n\nLau & Weld (1998) Tessa A. Lau and Daniel S. Weld. Programming by demonstration: An inductive learning formulation. In Proceedings of the 4th International Conference on Intelligent User Interfaces, pp. 145‚Äì152, Los Angeles California USA, 1998. ACM. ISBN 978-1-58113-098-0. doi: 10.1145/291080.291104.\n\nL√°zaro-Gredilla et al. (2019) Miguel L√°zaro-Gredilla, Dianhuan Lin, J Swaroop Guntupalli, and Dileep George. Beyond imitation: Zero-shot task transfer on robots by learning concepts as cognitive programs. Science Robotics, 4(26):eaav3150, 2019.\n\nLee & Mumford (2003) Tai Sing Lee and David Mumford. Hierarchical Bayesian inference in the visual cortex. JOSA A, 20(7):1434‚Äì1448, 2003.\n\nLi et al. (2023) Raymond Li, Loubna Ben Allal, Yangtian Zi, Niklas Muennighoff, Denis Kocetkov, Chenghao Mou, Marc Marone, Christopher Akiki, Jia Li, Jenny Chim, Qian Liu, Evgenii Zheltonozhskii, Terry Yue Zhuo, Thomas Wang, Olivier Dehaene, Mishig Davaadorj, Joel Lamy-Poirier, Jo√£o Monteiro, Oleh Shliazhko, Nicolas Gontier, Nicholas Meade, Armel Zebaze, Ming-Ho Yee, Logesh Kumar Umapathi, Jian Zhu, Benjamin Lipkin, Muhtasham Oblokulov, Zhiruo Wang, Rudra Murthy, Jason Stillerman, Siva Sankalp Patel, Dmitry Abulkhanov, Marco Zocca, Manan Dey, Zhihan Zhang, Nour Fahmy, Urvashi Bhattacharyya, Wenhao Yu, Swayam Singh, Sasha Luccioni, Paulo Villegas, Maxim Kunakov, Fedor Zhdanov, Manuel Romero, Tony Lee, Nadav Timor, Jennifer Ding, Claire Schlesinger, Hailey Schoelkopf, Jan Ebert, Tri Dao, Mayank Mishra, Alex Gu, Jennifer Robinson, Carolyn Jane Anderson, Brendan Dolan-Gavitt, Danish Contractor, Siva Reddy, Daniel Fried, Dzmitry Bahdanau, Yacine Jernite, Carlos Mu√±oz Ferrandis, Sean Hughes, Thomas Wolf, Arjun Guha, Leandro von Werra, and Harm de Vries. StarCoder: May the source be with you!, 2023. URL https://arxiv.org/abs/2305.06161.\n\nLi et al. (2022) Yujia Li, David Choi, Junyoung Chung, Nate Kushman, Julian Schrittwieser, R√©mi Leblond, Tom Eccles, James Keeling, Felix Gimeno, Agustin Dal Lago, et al. Competition-level code generation with AlphaCode. Science, 378(6624):1092‚Äì1097, 2022.\n\nLiang et al. (2010) Percy Liang, Michael I. Jordan, and Dan Klein. Learning programs: A hierarchical Bayesian approach. In Johannes F√ºrnkranz and Thorsten Joachims (eds.), Proceedings of the 27th International Conference on Machine Learning (ICML-10), June 21-24, 2010, Haifa, Israel, pp. 639‚Äì646. Omnipress, 2010. URL https://icml.cc/Conferences/2010/papers/568.pdf.\n\nLiang et al. (2011) Percy Liang, Michael Jordan, and Dan Klein. Learning dependency-based compositional semantics. In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies, pp. 590‚Äì599, Portland, Oregon, USA, 2011. Association for Computational Linguistics. URL https://aclanthology.org/P11-1060.\n\nLiu et al. (2022) Jiachang Liu, Dinghan Shen, Yizhe Zhang, Bill Dolan, Lawrence Carin, and Weizhu Chen. What makes good in-context examples for GPT-3? In Proceedings of Deep Learning Inside Out (DeeLIO 2022): The 3rd Workshop on Knowledge Extraction and Integration for Deep Learning Architectures, pp. 100‚Äì114, Dublin, Ireland and Online, 2022. Association for Computational Linguistics. doi: 10.18653/v1/2022.deelio-1.10. URL https://aclanthology.org/2022.deelio-1.10.\n\nLiu et al. (2023) Nelson F. Liu, Kevin Lin, John Hewitt, Ashwin Paranjape, Michele Bevilacqua, Fabio Petroni, and Percy Liang. Lost in the Middle: How Language Models Use Long Contexts, 2023. URL https://arxiv.org/abs/2307.03172.\n\nLu et al. (2022) Yao Lu, Max Bartolo, Alastair Moore, Sebastian Riedel, and Pontus Stenetorp. Fantastically ordered prompts and where to find them: Overcoming few-shot prompt order sensitivity. In Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pp. 8086‚Äì8098, Dublin, Ireland, 2022. Association for Computational Linguistics. doi: 10.18653/v1/2022.acl-long.556. URL https://aclanthology.org/2022.acl-long.556.\n\nManshadi et al. (2013) Mehdi Manshadi, Daniel Gildea, and James Allen. Integrating programming by example and natural language programming. In Proceedings of the Twenty-Seventh AAAI Conference on Artificial Intelligence, AAAI‚Äô13, pp. 661‚Äì667. AAAI Press, 2013.\n\nMiceli-Barone et al. (2023) Antonio Valerio Miceli-Barone, Fazl Barez, Ioannis Konstas, and Shay B. Cohen. The Larger They Are, the Harder They Fail: Language Models do not Recognize Identifier Swaps in Python, 2023. URL https://arxiv.org/abs/2305.15507.\n\nNijkamp et al. (2023) Erik Nijkamp, Bo Pang, Hiroaki Hayashi, Lifu Tu, Huan Wang, Yingbo Zhou, Silvio Savarese, and Caiming Xiong. CodeGen: An open large language model for code with multi-turn program synthesis. In The Eleventh International Conference on Learning Representations, 2023. URL https://openreview.net/forum?id=iaYcJKpY2B_.\n\nNye et al. (2019) Maxwell I. Nye, Luke B. Hewitt, Joshua B. Tenenbaum, and Armando Solar-Lezama. Learning to infer program sketches. In Kamalika Chaudhuri and Ruslan Salakhutdinov (eds.), Proceedings of the 36th International Conference on Machine Learning, ICML 2019, 9-15 June 2019, Long Beach, California, USA, volume 97 of Proceedings of Machine Learning Research, pp. 4861‚Äì4870. PMLR, 2019. URL http://proceedings.mlr.press/v97/nye19a.html.\n\nO‚ÄôConnor & Andreas (2021) Joe O‚ÄôConnor and Jacob Andreas. What context features can transformer language models use? In Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers), pp. 851‚Äì864, Online, 2021. Association for Computational Linguistics. doi: 10.18653/v1/2021.acl-long.70. URL https://aclanthology.org/2021.acl-long.70.\n\nOpenAI (2023) OpenAI. GPT-4 Technical Report, 2023. URL https://arxiv.org/abs/2303.08774.\n\nParisotto et al. (2017) Emilio Parisotto, Abdel-rahman Mohamed, Rishabh Singh, Lihong Li, Dengyong Zhou, and Pushmeet Kohli. Neuro-symbolic program synthesis. In 5th International Conference on Learning Representations, ICLR 2017, Toulon, France, April 24-26, 2017, Conference Track Proceedings. OpenReview.net, 2017. URL https://openreview.net/forum?id=rJ0JwFcex.\n\nPark et al. (2023) Joon Sung Park, Joseph C. O‚ÄôBrien, Carrie J. Cai, Meredith Ringel Morris, Percy Liang, and Michael S. Bernstein. Generative Agents: Interactive Simulacra of Human Behavior, 2023. URL https://arxiv.org/abs/2304.03442.\n\nPoesia et al. (2022) Gabriel Poesia, Alex Polozov, Vu Le, Ashish Tiwari, Gustavo Soares, Christopher Meek, and Sumit Gulwani. Synchromesh: Reliable code generation from pre-trained language models. In The Tenth International Conference on Learning Representations, ICLR 2022, Virtual Event, April 25-29, 2022. OpenReview.net, 2022. URL https://openreview.net/forum?id=KmtVD97J43e.\n\nPolikarpova et al. (2016) Nadia Polikarpova, Ivan Kuraj, and Armando Solar-Lezama. Program synthesis from polymorphic refinement types. ACM SIGPLAN Notices, 51(6):522‚Äì538, 2016.\n\nRahmani et al. (2021) Kia Rahmani, Mohammad Raza, Sumit Gulwani, Vu Le, Daniel Morris, Arjun Radhakrishna, Gustavo Soares, and Ashish Tiwari. Multi-modal program inference: a marriage of pre-trained language models and component-based synthesis. Proceedings of the ACM on Programming Languages, 5(OOPSLA):1‚Äì29, 2021.\n\nRaza et al. (2015) Mohammad Raza, Sumit Gulwani, and Natasa Milic-Frayling. Compositional program synthesis from natural language and examples. In Qiang Yang and Michael J. Wooldridge (eds.), Proceedings of the Twenty-Fourth International Joint Conference on Artificial Intelligence, IJCAI 2015, Buenos Aires, Argentina, July 25-31, 2015, pp. 792‚Äì800. AAAI Press, 2015. URL http://ijcai.org/Abstract/15/117.\n\nSchick et al. (2023) Timo Schick, Jane Dwivedi-Yu, Roberto Dess√¨, Roberta Raileanu, Maria Lomeli, Luke Zettlemoyer, Nicola Cancedda, and Thomas Scialom. Toolformer: Language models can teach themselves to use tools. ArXiv preprint, abs/2302.04761, 2023. URL https://arxiv.org/abs/2302.04761.\n\nSharma et al. (2022) Pratyusha Sharma, Antonio Torralba, and Jacob Andreas. Skill induction and planning with latent language. In Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pp. 1713‚Äì1726, Dublin, Ireland, 2022. Association for Computational Linguistics. doi: 10.18653/v1/2022.acl-long.120. URL https://aclanthology.org/2022.acl-long.120.\n\nShin et al. (2019) Eui Chul Richard Shin, Miltiadis Allamanis, Marc Brockschmidt, and Alex Polozov. Program synthesis and semantic parsing with learned code idioms. In Hanna M. Wallach, Hugo Larochelle, Alina Beygelzimer, Florence d‚ÄôAlch√©-Buc, Emily B. Fox, and Roman Garnett (eds.), Advances in Neural Information Processing Systems 32: Annual Conference on Neural Information Processing Systems 2019, NeurIPS 2019, December 8-14, 2019, Vancouver, BC, Canada, pp. 10824‚Äì10834, 2019. URL https://proceedings.neurips.cc/paper/2019/hash/cff34ad343b069ea6920464ad17d4bcf-Abstract.html.\n\nShinn et al. (2023) Noah Shinn, Federico Cassano, Beck Labash, Ashwin Gopinath, Karthik Narasimhan, and Shunyu Yao. Reflexion: Language Agents with Verbal Reinforcement Learning, 2023. URL https://arxiv.org/abs/2303.11366.\n\nSloman (1996) Steven A Sloman. The empirical case for two systems of reasoning. Psychological bulletin, 119(1):3, 1996.\n\nSolomonoff (1964) Ray J Solomonoff. A formal theory of inductive inference. Part I. Information and control, 7(1):1‚Äì22, 1964.\n\nSrikant et al. (2021) Shashank Srikant, Sijia Liu, Tamara Mitrovska, Shiyu Chang, Quanfu Fan, Gaoyuan Zhang, and Una-May O‚ÄôReilly. Generating adversarial computer programs using optimized obfuscations. In 9th International Conference on Learning Representations, ICLR 2021, Virtual Event, Austria, May 3-7, 2021. OpenReview.net, 2021. URL https://openreview.net/forum?id=PH5PH9ZO_4.\n\nTheis & Wong (2017) Thomas N. Theis and H.-S. Philip Wong. The end of Moore‚Äôs law: A new beginning for information technology. Computing in Science & Engineering, 19(2):41‚Äì50, 2017. doi: 10.1109/MCSE.2017.29.\n\nWang et al. (2023) Guanzhi Wang, Yuqi Xie, Yunfan Jiang, Ajay Mandlekar, Chaowei Xiao, Yuke Zhu, Linxi Fan, and Anima Anandkumar. Voyager: An Open-Ended Embodied Agent with Large Language Models, 2023. URL https://arxiv.org/abs/2305.16291.\n\nWang et al. (2022) Xuezhi Wang, Jason Wei, Dale Schuurmans, Quoc Le, Ed Huai hsin Chi, and Denny Zhou. Self-consistency improves chain of thought reasoning in language models. ArXiv preprint, abs/2203.11171, 2022. URL https://arxiv.org/abs/2203.11171.\n\nWolfram (2023) Stephen Wolfram. ChatGPT gets its ‚ÄúWolfram Superpowers‚Äù, 2023. URL https://writings.stephenwolfram.com/2023/03/chatgpt-gets-its-wolfram-superpowers/.\n\nWong et al. (2021) Catherine Wong, Kevin Ellis, Joshua B. Tenenbaum, and Jacob Andreas. Leveraging language to learn program abstractions and search heuristics. In Marina Meila and Tong Zhang (eds.), Proceedings of the 38th International Conference on Machine Learning, ICML 2021, 18-24 July 2021, Virtual Event, volume 139 of Proceedings of Machine Learning Research, pp. 11193‚Äì11204. PMLR, 2021. URL http://proceedings.mlr.press/v139/wong21a.html.\n\nWu et al. (2015) Jiajun Wu, Ilker Yildirim, Joseph J. Lim, Bill Freeman, and Joshua B. Tenenbaum. Galileo: Perceiving physical object properties by integrating a physics engine with deep learning. In Corinna Cortes, Neil D. Lawrence, Daniel D. Lee, Masashi Sugiyama, and Roman Garnett (eds.), Advances in Neural Information Processing Systems 28: Annual Conference on Neural Information Processing Systems 2015, December 7-12, 2015, Montreal, Quebec, Canada, pp. 127‚Äì135, 2015. URL https://proceedings.neurips.cc/paper/2015/hash/d09bf41544a3365a46c9077ebb5e35c3-Abstract.html.\n\nWu et al. (2017) Jiajun Wu, Joshua B. Tenenbaum, and Pushmeet Kohli. Neural scene de-rendering. In 2017 IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2017, Honolulu, HI, USA, July 21-26, 2017, pp. 7035‚Äì7043. IEEE Computer Society, 2017. doi: 10.1109/CVPR.2017.744. URL https://doi.org/10.1109/CVPR.2017.744.\n\nWu et al. (2022) Yuhuai Wu, Markus Norman Rabe, DeLesley Hutchins, and Christian Szegedy. Memorizing transformers. In The Tenth International Conference on Learning Representations, ICLR 2022, Virtual Event, April 25-29, 2022. OpenReview.net, 2022. URL https://openreview.net/forum?id=TrjbxzRcnf-.\n\nYao et al. (2022) Shunyu Yao, Jeffrey Zhao, Dian Yu, Nan Du, Izhak Shafran, Karthik Narasimhan, and Yuan Cao. ReAct: Synergizing Reasoning and Acting in Language Models, 2022. URL https://arxiv.org/abs/2210.03629.\n\nYi et al. (2018) Kexin Yi, Jiajun Wu, Chuang Gan, Antonio Torralba, Pushmeet Kohli, and Josh Tenenbaum. Neural-symbolic VQA: disentangling reasoning from vision and language understanding. In Samy Bengio, Hanna M. Wallach, Hugo Larochelle, Kristen Grauman, Nicol√≤ Cesa-Bianchi, and Roman Garnett (eds.), Advances in Neural Information Processing Systems 31: Annual Conference on Neural Information Processing Systems 2018, NeurIPS 2018, December 3-8, 2018, Montr√©al, Canada, pp. 1039‚Äì1050, 2018. URL https://proceedings.neurips.cc/paper/2018/hash/5e388103a391daabe3de1d76a6739ccd-Abstract.html.\n\nYildirim et al. (2020) Ilker Yildirim, Mario Belledonne, Winrich Freiwald, and Josh Tenenbaum. Efficient inverse graphics in biological face processing. Science Advances, 6(10):eaax5979, 2020. doi: 10.1126/sciadv.aax5979. URL https://www.science.org/doi/abs/10.1126/sciadv.aax5979.\n\nYin et al. (2022) Pengcheng Yin, Wen-Ding Li, Kefan Xiao, Abhishek Rao, Yeming Wen, Kensen Shi, Joshua Howland, Paige Bailey, Michele Catasta, Henryk Michalewski, Alex Polozov, and Charles Sutton. Natural Language to Code Generation in Interactive Data Science Notebooks, 2022. URL https://arxiv.org/abs/2212.09248.\n\nYuille & Kersten (2006) Alan Yuille and Daniel Kersten. Vision as Bayesian inference: analysis by synthesis? Trends in cognitive sciences, 10(7):301‚Äì308, 2006.\n\nZelikman et al. (2022) Eric Zelikman, Qian Huang, Gabriel Poesia, Noah D. Goodman, and Nick Haber. Parsel: A (De-)compositional Framework for Algorithmic Reasoning with Language Models, 2022. URL https://arxiv.org/abs/2212.10561.\n\nZeng et al. (2022) Zhengran Zeng, Hanzhuo Tan, Haotian Zhang, Jing Li, Yuqun Zhang, and Lingming Zhang. An extensive study on pre-trained models for program understanding and generation. In Proceedings of the 31st ACM SIGSOFT International Symposium on Software Testing and Analysis, pp. 39‚Äì51, Virtual South Korea, 2022. ACM. ISBN 978-1-4503-9379-9. doi: 10.1145/3533767.3534390.\n\nZhou et al. (2023) Yongchao Zhou, Andrei Ioan Muresanu, Ziwen Han, Keiran Paster, Silviu Pitis, Harris Chan, and Jimmy Ba. Large language models are human-level prompt engineers. In The Eleventh International Conference on Learning Representations, 2023. URL https://openreview.net/forum?id=92gvk82DE-.\n\nPart\n\nPart Appendix\n\n\\parttoc\n\nAppendix A Methods\n\nA.1 LLM Solver Prompt\n\nA.2 Auto-Documentation Prompt\n\nFor reproducibility, we provide an example of the full text of an AutoDoc prompt sequence for the REGEX domain below. The prompt is composed of multiple pieces that are sent in serial as messages to the ChatGPT interface. The sequence begins with a header message describing the DSL. For pedagogical clarity, we consider the case where every abstraction except the final one have already assigned names. Thus, the header contains a mostly-documented library with the final fn_51 remaining anonymous.\n\n|Youarewritingsoftwaredocumentation.Yourgoalistowritehuman-readablenamesforthefollowinglibraryfunctions:|\n\n\\parvowel_or ::tsubstr\n\n(regex_or ‚Äôa‚Äô (regex_or ‚Äôe‚Äô (regex_or ‚Äôi‚Äô (regex_or ‚Äôo‚Äô ‚Äôu‚Äô))))\n\n{-Matchesanysinglevowelcharacter (‚Äôa‚Äô, ‚Äôe‚Äô, ‚Äôi‚Äô, ‚Äôo‚Äô, ‚Äôu‚Äô)using ‚Äôregex_or‚Äôfunction. -}\n\n\\parreplace_and_flatten ::tfullstr ->tsubstr ->tsubstr ->tfullstr\n\n(lambda (lambda (lambda (regex_flatten (regex_map (lambda (regex_if (regex_match$2$0)$1$0)) (regex_split$1$2))))))\n\n{-Replacesallinstancesofagivensubstringwithanothersubstring,andreturnstheresultingstringflattenedintoonestring.Thefirstargumentistheinputstring,thesecondargumentisthesubstringtobereplaced,andthethirdargumentisthesubstringtouseinsteadofthereplacedsubstring. -}\n\n\\par| ‚Ä¶ <fn\\_44 -fn\\_50omittedforconcision> ‚Ä¶|\n\n\\parfn_51 ::tfullstr ->tsubstr ->tsubstr ->tfullstr\n\n(lambda (lambda (lambda (regex_flatten (regex_cons$0 (regex_cons$1 (regex_cdr (split_string_into_list$2))))))))\n\nWe then send a message prompting the LLM to document fn_51. At the end of the message, we request that the LLM encode the reply into a particular JSON format to facilitate downstream parsing.\n\n|Considerthefollowinganonymousfunction:|\n\n\\par%****iclr2024_conference.texLine 475 ****fn_51 ::tfullstr ->tsubstr ->tsubstr ->tfullstr\n\n(lambda (lambda (lambda (regex_flatten (regex_cons$0 (regex_cons$1 (regex_cdr (split_string_into_list$2))))))))\n\n\\par|Herearesomeexamplesofitsusage:|\n\n\\par‚Äìifthewordstartswithconsonantanyletterreplacethatwithvd\n\n(lambda (regex_if (regex_match (regex_notvowel_or) (regex_car (split_string_into_list$0))) (fn_51 (regex_flatten (regex_cdr (split_string_into_list$0))) ‚Äôd‚Äô ‚Äôv‚Äô)$0))\n\n\\par‚Äìifthewordstartswithanylettervoweladdqbeforethat\n\n(lambda (regex_if (regex_matchvowel_or (regex_car (regex_cdr (split_string_into_list$0)))) (fn_51$0 (regex_car (split_string_into_list$0)) ‚Äôq‚Äô)$0))\n\n\\par‚Äìifthewordstartswithvowelreplacethatwithuc\n\n(lambda (regex_if (regex_matchvowel_or (regex_car (split_string_into_list$0))) (fn_51 (regex_flatten (split_string_into_list$0)) ‚Äôc‚Äô ‚Äôu‚Äô)$0))\n\n\\par| ‚Ä¶ <additionalusageexamplesomittedforconcision> ‚Ä¶|\n\n\\par|Pleasewriteahuman-readablenameanddescriptionfor ‚Äòfn\\_51‚ÄòintheJSONformatshownbelow.\n\nYour ‚Äòreadable\\_name‚Äòshouldbeunderscore-separatedandshouldnotcontainanyspaces.\n\nItshouldalsobeunique (notexistinginthefunctionlibraryabove).\n\nIfyoucannotcomeupwithagoodname,pleaseset ‚Äòreadable\\_name‚Äòto ‚Äònull‚Äò.|\n\n\\par{\n\n‚Äùanonymous_name‚Äù: ‚Äùfn_51‚Äù,\n\n‚Äùreadable_name‚Äù:TODO,\n\n‚Äùdescription‚Äù:TODO\n\n}\n\nWe encountered difficulties in coaxing Codex to perform the AutoDoc task: the resulting function names were variable in quality, did not reliably capture the function semantics, and were embedded in generations that did not always adhere to the desired output specification. Instead, we take advantage of OpenAI‚Äôs instruction-tuned gpt-3.5-turbo and gpt-4 models, which we found adhered to the desired output JSON schema 100% of the time and never chose to return null for readable_name. We experimented with both gpt-3.5-turbo and gpt-4 for AutoDoc and found both resulted in comparable synthesis performance on REGEX. However, GPT-4 was significantly slower: whereas gpt-3.5-turbo averaged 10-20 seconds for one iteration of AutoDoc, gpt-4 averaged upwards of 2 minutes per iteration. We therefore chose to use gpt-3.5-turbo in the experiments reported in 4. Unlike for the LLM Solver, we do not provide any few-shot examples of the desired transformations; all of this behavior is zero-shot, making AutoDoc an extremely domain-general technique that is easy to implement across a variety of settings.\n\nA.3 Task-Example Selection Methods\n\nOur LLM-guided program synthesis method (Eq. 3) requires selecting a set of few-shot examples for prompting. As the set of solved tasks grows, the set of possible examples exceeds the size of the LLM context window. This issue particularly affects non-compressive methods, such as the LLM Solver baseline. However, even with program compression‚Äîwhich substantially reduces the length of the program examples‚ÄîLilo still requires subsampling from the total set of possible examples. We experimented with two different methods for task example selection: a naive random sampling method and a task-example selection method (Liu et al., 2022) based on cosine similarity between the task descriptions of the example dx‚Üí‚Üísubscriptùëëùë•\\vec{d_{x}}over‚Üí start_ARG italic_d start_POSTSUBSCRIPT italic_x end_POSTSUBSCRIPT end_ARG and the target dt‚Üí‚Üísubscriptùëëùë°\\vec{d_{t}}over‚Üí start_ARG italic_d start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT end_ARG:\n\nscore‚Å¢(dx‚Üí,dt‚Üí)=dx‚Üí‚ãÖdt‚Üí‚Äñdx‚Üí‚Äñ‚Å¢‚Äñdt‚Üí‚Äñscore‚Üísubscriptùëëùë•‚Üísubscriptùëëùë°‚ãÖ‚Üísubscriptùëëùë•‚Üísubscriptùëëùë°norm‚Üísubscriptùëëùë•norm‚Üísubscriptùëëùë°\\text{score}(\\vec{d_{x}},\\vec{d_{t}})=\\frac{\\vec{d_{x}}\\cdot\\vec{d_{t}}}{\\|% \\vec{d_{x}}\\|\\|\\vec{d_{t}}\\|}score ( over‚Üí start_ARG italic_d start_POSTSUBSCRIPT italic_x end_POSTSUBSCRIPT end_ARG , over‚Üí start_ARG italic_d start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT end_ARG ) = divide start_ARG over‚Üí start_ARG italic_d start_POSTSUBSCRIPT italic_x end_POSTSUBSCRIPT end_ARG ‚ãÖ over‚Üí start_ARG italic_d start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT end_ARG end_ARG start_ARG ‚à• over‚Üí start_ARG italic_d start_POSTSUBSCRIPT italic_x end_POSTSUBSCRIPT end_ARG ‚à• ‚à• over‚Üí start_ARG italic_d start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT end_ARG ‚à• end_ARG\n\nIn our implementation, we used embeddings from text-embedding-ada-002 via the OpenAI API to pre-compute pairwise similarities between all task descriptions in each domain. For both selection methods, we construct the prompt dynamically to fit as many examples as possible. We ran a head-to-head comparison between the two sampling methods for our main Lilo model. As 7 and Tab. 2 show, we did not observe a significant improvement from the cosine similarity example selection method, though introducing determinism did have the effect of reducing the variance across runs in the REGEX domain. In absence of evidence justifying additional methods complexity, we chose to use random sampling for the results reported in 4. It is possible that the use of compression in Lilo reduces the need for targeted example selection, since we are able to fit approx. 20-40 examples per prompt across all domains. We also noted a tendency for the cosine similarity sampling to be oversensitive to superficial lexical overlap in the task descriptions; e.g., two tasks might involve very different programs but both include the word ‚Äúsix‚Äù as an argument, resulting in high cosine similarity. Thus, methods that explicitly finetune a model to infer similarity between (observed) example and (unobserved) target programs (i.e., Target Similarity Tuning from Poesia et al., 2022) could offer clearer performance advantages.\n\nA.4 Implementation Details\n\nWe provide a brief summary of key implementation details relevant to the experiments that are not reported in 4. We ran all experiments on AWS EC2 instances with machine specs tailored to suit the computational workload of each experiment. Enumerative search. For experiments involving enumerative search, which is an embarrassingly parallel workload that scales linearly with the number of available CPUs, we ran on 96-CPU c5.24xlarge instances. These machines have the highest CPU count in the c5 machine class. To take maximal advantage of the CPU parallelism, we set batch_size=96 for these experiments (i.e., each iteration searches for solutions for a subset of 96 tasks). A convenient consequence of this implementation choice is that each task is allocated to a single, dedicated CPU, so the overall wall clock runtime of a single search iteration is equal to the per-task enumeration time budget. We set the enumeration budget on a per-domain basis using the timeouts from Wong et al. (2021) (REGEX = 1000s, CLEVR = 600s, LOGO = 1800s). We ran DreamCoder until convergence on all domains. For CLEVR and LOGO, we performed 10 iterations of search, while for REGEX, we observed that the solve rate was still increasing at iteration 10, so we used a higher search budget of 16 iterations for this domain. Following Wong et al. (2021) and based on a common practice in machine learning, we limited evaluation of the test set to every 3 iterations due to the computational cost of enumerative search. GPT language models. For experiments in which GPT LLMs perform program search, the bulk of the computational workload is effectively offloaded to OpenAI‚Äôs servers. Locally, the only requirements are that our machine is able to make API queries, process the results, and run compression. Accordingly, these experiments are run on c5.2xlarge machines with 8 CPUs each. (For experiments involving combinations of GPT queries and DreamCoder search, we use the larger c5.24xlarge machines.) To ensure comparability in solver performance between LLM-based and enumerative search-based experiments, we also run the LLM experiments with batch_size=96 so that the learning timelines are aligned. Stitch. For compression, we make use of the Stitch Python bindings, which interface with a fast backend written in Rust (https://stitch-bindings.readthedocs.io/en/stable/). Stitch exposes various hyperparameters, the most important of which are iterations, which governs the number of abstractions produced, and max-arity, which governs the maximum number of arguments that each abstraction can take. For all experiments, we set these to a constant iterations=10 and max-arity=3. We note that Stitch will only produce an abstraction if it is compressive; i.e., it appears in multiple programs, and rewriting the corpus in terms of the abstraction reduces the overall description length. For this reason, in rare cases early on in learning, when only a handful of solved programs are available, the actual library size can be smaller than iterations. This behavior is beneficial in that it avoids introducing abstractions that have no utility and that might potentially negatively affect performance. A summary of hyperparameters can be found in A.5. For further implementation details, we refer to our codebase: github.com/gabegrand/lilo.\n\nA.5 Hyperparameters\n\nWe provide a summary of all key hyperparameters used in each component of Lilo.\n\nDreamCoder\n\nBatch size: 96 tasks Global iterations: 10 (CLEVR, LOGO), 16 (REGEX) Search timeouts: 600s (CLEVR), 1000s (REGEX), 1800s (LOGO) Neural recognition model: 10K training steps / iteration\n\nStitch\n\nMax iterations: 10 (Controls max library size) Max arity: 3 (Controls max arity of abstractions)\n\nLilo: LLM Synthesizer\n\nPrompts per task: 4 Samples per prompt: 4 GPT Model: code-davinci-002 Temperature: 0.90 Max completion tokens Œ≤ùõΩ\\betaitalic_Œ≤: 4.0x (Multiplier w/r/t the final prompt program.)\n\nAppendix B Experiments and Results\n\nB.1 Domain Details\n\nREGEX: String editing. We evaluate on a domain of structured string transformation problems‚Äìa classic task in inductive program synthesis (Lau & Weld, 1998). The dataset, originally introduced in Andreas et al. (2018), contains procedurally-generated regular expressions that implement transformations on strings (e.g., if the word ends with a consonant followed by ‚Äús‚Äù, replace that with b). Task examples consist of input/output pairs where the inputs are strings randomly sampled from an English dictionary and the outputs are the result of applying a particular string transformation. Following prior work (Ellis et al., 2021; Wong et al., 2021), the base DSL in this domain contains functional various programming primitives for string manipulation (map, fold, cons, car, cdr, length, index) and character constants. Each example comes with a synthetic language description of the task, which was generated by template based on human annotations (Andreas et al., 2018). CLEVR: Scene reasoning. We extend our approach to a visual question answering (VQA) task based on the CLEVR dataset (Johnson et al., 2017). Following successful efforts in modeling VQA as program synthesis (Andreas et al., 2016; Hu et al., 2017), each synthesis task is specified by a structured input scene and a natural language question. Outputs can be one of several types, including a number (how many red rubber things are there?), a boolean value (are there more blue things than green?), or another scene (what if all of the red things turned blue?). The dataset, designed by Wong et al. (2021), uses a modified subset of the original CLEVR tasks and introduces new task types that require imagining or generating new scenes (e.g., how many metal things would be left if all the blue cylinders were removed?) that require learning new abstractions. The base DSL includes functional programming primitives similar to the regular expression domain, with domain-specific query functions and constants (e.g., get_color(x); get_shape(x); blue; cube). Input scenes are specified symbolically as scene graphs consisting of an array of structured objects defined as a dictionary of their attributes, and programs are designed to manipulate these structured arrays. Synthetic language annotations were generated based on the original high-level templates in Johnson et al. (2017) and human annotations were collected by Wong et al. (2021). LOGO: Turtle graphics. Following in a long tradition of modeling vision as inverse graphics, (Knill & Richards, 1996; Kersten et al., 2004; Yuille & Kersten, 2006; Lee & Mumford, 2003; Wu et al., 2015; Yildirim et al., 2020; Wu et al., 2017; Yi et al., 2018; Gothoskar et al., 2021) we evaluate on a domain of compositional drawing problems. The dataset, originally introduced in (Wong et al., 2021) and based on a simpler dataset from (Ellis et al., 2021), contains programs that generate shapes and designs in a vector graphics language. The DSL is based on Logo Turtle graphics (Abelson & diSessa, 1986), which originated from early symbolic AI research. Program expressions control the movement and direction of a pen (classically represented as a Turtle) on a canvas and can involve complex symmetries and recursions (e.g., a seven sided snowflake with a short line and a small triangle as arms; a small triangle connected by a big space from a small circle). The base DSL includes for loops, a stack for saving/restoring the pen state, and arithmetic on angles and distances (Ellis et al., 2021). Synthetic language annotations were generated with high-level templates over the objects and relations in each task; human annotations were collected by Wong et al. (2021).\n\nB.2 Learned Libraries and Graphical Maps\n\nWe generated graphical visualizations of the libraries learned by the best Lilo model for each domain. Each graph includes the DSL primitives, the learned and named abstractions, and a random sample of 3 solved tasks that invoke each abstraction. Arrows indicate direction of reference; i.e., fn_1 -> fn_2 indicates that fn_1 invokes fn_2, and analogously for the tasks.\n\nB.2.1 Library for REGEX\n\n(fn_42)vowel_regex ::tsubstr\n\n(regex_or ‚Äôa‚Äô (regex_or ‚Äôe‚Äô (regex_or ‚Äôi‚Äô (regex_or ‚Äôo‚Äô ‚Äôu‚Äô))))\n\n{-Regularexpressionthatmatchesanyvowel (‚Äôa‚Äô, ‚Äôe‚Äô, ‚Äôi‚Äô, ‚Äôo‚Äô, ‚Äôu‚Äô).Usedinvariousfunctionstoidentifyandmodifywordsbasedonvowelpresenceandposition. -}\n\n\\par{-Exampleusages -}\n\n‚Äìifthereisconsonantaddsafterthat\n\n(|$\\lambda$| (replace_substring_if_match ‚Äôs‚Äô (regex_notvowel_regex)$0))\n\n‚Äìifthewordstartswithvowelreplacethatwithjl\n\n(|$\\lambda$| (regex_if (regex_match (regex_notvowel_regex) (regex_car (split_fullstring$0)))$0 (replace_first_occurrence$0 ‚Äôl‚Äô ‚Äôj‚Äô)))\n\n‚Äìifthewordstartswithvowelreplacethatwithuc\n\n(|$\\lambda$| (replace_if_match_substring$0 (replace_first_occurrence$0 ‚Äôc‚Äô ‚Äôu‚Äô)vowel_regex))\n\n\\par(fn_43)replace_substr ::tfullstr ->tsubstr ->tsubstr ->tfullstr\n\n(|$\\lambda$| (|$\\lambda$| (|$\\lambda$| (regex_flatten (regex_map (|$\\lambda$| (regex_if (regex_match$1$0)$2$0)) (regex_splitempty_string$2))))))\n\n{-Replacesallinstancesofagivensubstring$1inafullstring$0withanothersubstring$2.Thesubstringsareseparatedbyemptyspaces. -}\n\n\\par{-Exampleusages -}\n\n‚Äìifthereisdreplacethatwithy\n\n%****iclr2024_conference.texLine 700 ****(|$\\lambda$| (replace_substr$0 ‚Äôy‚Äô ‚Äôd‚Äô))\n\n‚Äìifthereisireplacethatwithkt\n\n(|$\\lambda$| (replace_substr$0 (regex_concat ‚Äôk‚Äô ‚Äôt‚Äô) ‚Äôi‚Äô))\n\n‚Äìifthereissreplacethatwithtq\n\n(|$\\lambda$| (replace_substr$0 (regex_concat ‚Äôt‚Äô ‚Äôq‚Äô) ‚Äôs‚Äô))\n\n\\par(fn_44)replace_first_occurrence ::tfullstr ->tsubstr ->tsubstr ->tfullstr\n\n(|$\\lambda$| (|$\\lambda$| (|$\\lambda$| (regex_flatten (regex_cons$0 (regex_cons$1 (regex_cdr (regex_split ‚Äô.‚Äô$2))))))))\n\n{-Replacesthefirstoccurrenceofasubstring$1inafullstring$0withanothersubstring$2.Thesubstringsareseparatedbyperiods. -}\n\n\\par{-Exampleusages -}\n\n‚Äìifthewordstartswithvowelreplacethatwithqb\n\n(|$\\lambda$| (replace_if_match_substring$0 (replace_first_occurrence$0 ‚Äôb‚Äô ‚Äôq‚Äô)vowel_regex))\n\n‚Äìifthewordstartswithconsonantreplacethatwithi\n\n(|$\\lambda$| (replace_first_occurrence$0empty_string ‚Äôi‚Äô))\n\n‚Äìifthewordstartswithvowelreplacethatwithla\n\n(|$\\lambda$| (regex_if (regex_match (regex_notvowel_regex) (regex_car (split_fullstring$0)))$0 (replace_first_occurrence$0 ‚Äôa‚Äô ‚Äôl‚Äô)))\n\n\\par(fn_45)replace_each_substring ::tfullstr -> (tsubstr ->tsubstr) ->tfullstr\n\n(|$\\lambda$| (|$\\lambda$| (regex_flatten (regex_map$0 (regex_split ‚Äô.‚Äô$1)))))\n\n{-Replaceseachsubstringseparatedbyperiodsinagivenfullstringwithanewsubstring.Thenewsubstringcanbemanipulatedwitha |$\\lambda$|functionthattakeseachsubstringasinput. -}\n\n\\par{-Exampleusages -}\n\n‚Äìifthereistreplacethatwithax\n\n(|$\\lambda$| (replace_each_substring$0 (|$\\lambda$| (regex_if (regex_match ‚Äôt‚Äô$0) (regex_concat ‚Äôa‚Äô ‚Äôx‚Äô)$0))))\n\n%****iclr2024_conference.texLine 725 ****‚Äìifthereisvowelreplacethatwithaf\n\n(|$\\lambda$| (replace_each_substring$0 (|$\\lambda$| (regex_if (regex_matchvowel_regex$0) (regex_concat ‚Äôa‚Äô ‚Äôf‚Äô)$0))))\n\n‚Äìifthereiscreplacethatwithkb\n\n(|$\\lambda$| (replace_each_substring$0 (|$\\lambda$| (regex_if (regex_match ‚Äôc‚Äô$0) (regex_concat ‚Äôk‚Äô ‚Äôb‚Äô)$0))))\n\n\\par(fn_46)replace_if_match_substring ::tfullstr ->tfullstr ->tsubstr ->tfullstr\n\n(|$\\lambda$| (|$\\lambda$| (|$\\lambda$| (regex_if (regex_match$0 (regex_car (regex_split ‚Äô.‚Äô$2)))$1$2))))\n\n{-Replacesagivensubstring$2inafullstring$0withanothersubstring$1ifthebeginningofthestringmatchesthetargetsubstring.Allsubstringsareseparatedbyperiods. -}\n\n\\par{-Exampleusages -}\n\n‚Äìifthewordstartswithvoweladdpbeforethat\n\n(|$\\lambda$| (replace_if_match_substring$0 (regex_flatten (regex_cons ‚Äôp‚Äô (split_fullstring$0)))vowel_regex))\n\n‚Äìifthewordstartswithconsonantanyletterreplacethatwithf\n\n(|$\\lambda$| (replace_if_match_substring$0 (regex_flatten (regex_cons ‚Äôf‚Äô (regex_cdr (regex_cdr (split_fullstring$0))))) (regex_notvowel_regex)))\n\n‚Äìifthewordstartswithvowelanyletterreplacethatwithw\n\n(|$\\lambda$| (replace_if_match_substring$0 (regex_flatten (regex_cons ‚Äôw‚Äô (regex_cdr (regex_cdr (split_fullstring$0)))))vowel_regex))\n\n\\par(fn_47)add_new_substring_if_match ::tsubstr ->tsubstr ->tfullstr ->tfullstr\n\n(|$\\lambda$| (|$\\lambda$| (|$\\lambda$| (replace_each_substring$0 (|$\\lambda$| (regex_if (regex_match$2$0) (regex_concat$3$0)$0))))))\n\n{-Replaceseachsubstringseparatedbyperiodsinagivenfullstringwithanewsubstring,ifaspecifiedsubstringisfound.Thenewsubstringcanbemanipulatedwitha |$\\lambda$|functionthattakeseachsubstringasinput. -}\n\n\\par{-Exampleusages -}\n\n‚Äìifthereisgaddwbeforethat\n\n(|$\\lambda$| (add_new_substring_if_match ‚Äôw‚Äô ‚Äôg‚Äô$0))\n\n‚Äìifthereisanyletteraddlbeforethat\n\n%****iclr2024_conference.texLine 750 ****(|$\\lambda$| (add_new_substring_if_match ‚Äôl‚Äô ‚Äô.‚Äô$0))\n\n‚Äìifthereisraddbbeforethat\n\n(|$\\lambda$| (add_new_substring_if_match ‚Äôb‚Äô ‚Äôr‚Äô$0))\n\n\\par(fn_48)append_reverse_cdr ::tfullstr ->tsubstr ->tfullstr\n\n(|$\\lambda$| (|$\\lambda$| (regex_flatten (regex_append$0 (regex_reverse_cdr (regex_split ‚Äô.‚Äô$1))))))\n\n{-Appendsanewsubstringtotheendofthegivenfullstringandreversestheorderofallsubstringsexceptforthelastone (whichisremoved). -}\n\n\\par{-Exampleusages -}\n\n‚Äìifthewordendswithconsonantreplacethatwithog\n\n(|$\\lambda$| (append_reverse_cdr$0 (regex_concat ‚Äôo‚Äô ‚Äôg‚Äô)))\n\n‚Äìifthewordendswithconsonantreplacethatwithna\n\n(|$\\lambda$| (regex_if (regex_match ‚Äôe‚Äô (regex_tail (split_fullstring$0)))$0 (append_reverse_cdr$0 (regex_concat ‚Äôn‚Äô ‚Äôa‚Äô))))\n\n‚Äìifthewordendswithanyletterreplacethatwithoj\n\n(|$\\lambda$| (append_reverse_cdr$0 (regex_concat ‚Äôo‚Äô ‚Äôj‚Äô)))\n\n\\par(fn_49)replace_substring_if_match ::tsubstr ->tsubstr ->tfullstr ->tfullstr\n\n(|$\\lambda$| (|$\\lambda$| (|$\\lambda$| (replace_each_substring$0 (|$\\lambda$| (regex_if (regex_match$2$0) (regex_concat$0$3)$0))))))\n\n{-Replaceseachsubstringseparatedbyperiodsinagivenfullstringwithanewsubstring,ifaspecifiedsubstringisfound,usinga |$\\lambda$|functionthattakesthecurrentsubstringasinputandreplacesitwithanewsubstringbasedonacondition. -}\n\n\\par{-Exampleusages -}\n\n‚Äìifthereisvoweladdiafterthat\n\n(|$\\lambda$| (replace_substring_if_match ‚Äôi‚Äôvowel_regex$0))\n\n‚Äìifthereiscaddeafterthat\n\n(|$\\lambda$| (replace_substring_if_match ‚Äôe‚Äô ‚Äôc‚Äô$0))\n\n%****iclr2024_conference.texLine 775 ****‚Äìifthereisnaddeafterthat\n\n(|$\\lambda$| (replace_substring_if_match ‚Äôe‚Äô ‚Äôn‚Äô$0))\n\n\\par(fn_50)split_fullstring ::tfullstr ->list(tsubstr)\n\n(|$\\lambda$| (regex_split ‚Äô.‚Äô$0))\n\n{-Splitsagivenfullstringintoalistofsubstringsseparatedbyperiods. -}\n\n\\par{-Exampleusages -}\n\n‚Äìifthewordendswithanyletteranyletteraddfafterthat\n\n(|$\\lambda$| (regex_flatten (regex_append (regex_concat ‚Äôf‚Äôempty_string) (split_fullstring$0))))\n\n‚Äìifthewordstartswithanyletterreplacethatwithwi\n\n(|$\\lambda$| (regex_flatten (regex_cons (regex_concat ‚Äôw‚Äô ‚Äôi‚Äô) (regex_cdr (split_fullstring$0)))))\n\n‚Äìifthereisanyletteraddvafterthat\n\n(|$\\lambda$| (replace_each_substring$0 (|$\\lambda$| (regex_tail (regex_map (|$\\lambda$| (regex_concat$1 ‚Äôv‚Äô)) (split_fullstring$1))))))\n\n\\par(fn_51)replace_consonant_with_substring ::tsubstr ->tsubstr ->tfullstr ->tfullstr\n\n(|$\\lambda$| (|$\\lambda$| (|$\\lambda$| (replace_if_match_substring$0 (replace_first_occurrence$0$1$2) (regex_notvowel_regex)))))\n\n{-Replacesthefirstoccurrenceofaconsonantatthebeginningofagivenfullstringwithaspecifiedsubstring.Thetargetsubstringcanalsobemodifiedbeforereplacementusinganotherspecifiedsubstring. -}\n\n\\par{-Exampleusages -}\n\n‚Äìifthewordstartswithconsonantreplacethatwithiq\n\n(|$\\lambda$| (replace_consonant_with_substring ‚Äôi‚Äô ‚Äôq‚Äô$0))\n\n‚Äìifthewordstartswithconsonantreplacethatwithgd\n\n(|$\\lambda$| (replace_consonant_with_substring ‚Äôg‚Äô ‚Äôd‚Äô$0))\n\n‚Äìifthewordstartswithconsonantreplacethatwithpb\n\n%****iclr2024_conference.texLine 800 ****(|$\\lambda$| (replace_consonant_with_substring ‚Äôp‚Äô ‚Äôb‚Äô$0))\n\nB.2.2 Library for CLEVR\n\n(fn_54)filter_by_size ::tclevrsize ->list(tclevrobject) ->list(tclevrobject)\n\n%****iclr2024_conference.texLine 825 ****(|$\\lambda$| (|$\\lambda$| (clevr_fold$0$0 (|$\\lambda$| (|$\\lambda$| (clevr_map (|$\\lambda$| (clevr_if (clevr_eq_size (clevr_query_size$0)$4)$0$2))$0))))))\n\n{-Returnsalistofobjectsintheinputlistthathavethespecifiedsize. -}\n\n\\par(fn_55)filter_by_color ::tclevrcolor ->list(tclevrobject) ->list(tclevrobject)\n\n(|$\\lambda$| (|$\\lambda$| (clevr_fold$0clevr_empty (|$\\lambda$| (|$\\lambda$| (clevr_if (clevr_eq_color (clevr_query_color$1)$3) (clevr_add$1$0)$0))))))\n\n{-Returnsalistofobjectsintheinputlistthathavethespecifiedcolor. -}\n\n\\par{-Exampleusages -}\n\n‚Äìwhatcoloristhesmallmetalthingbehindthesmallpurplemetalthing\n\n(|$\\lambda$| (clevr_query_color (clevr_car (filter_objects_by_material (filter_objects_by_small_size (clevr_relate (clevr_car (filter_by_colorclevr_purple (filter_objects_by_material (filter_objects_by_small_size$0))))clevr_behind$0))))))\n\n‚Äìwhatisthesizeofthegraything\n\n(|$\\lambda$| (clevr_query_size (clevr_car (filter_by_colorclevr_gray$0))))\n\n‚Äìhowmanythingsareredthingsorlargegreenthings\n\n(|$\\lambda$| (clevr_count (clevr_union (filter_by_colorclevr_red$0) (filter_large_objects_by_size (filter_by_colorclevr_green$0)))))\n\n\\par(fn_56)filter_by_material ::tclevrmaterial ->list(tclevrobject) ->list(tclevrobject)\n\n(|$\\lambda$| (|$\\lambda$| (clevr_fold$0clevr_empty (|$\\lambda$| (|$\\lambda$| (clevr_if (clevr_eq_material (clevr_query_material$1)$3) (clevr_add$1$0)$0))))))\n\n{-Returnsalistofobjectsintheinputlistthathavethespecifiedmaterial. -}\n\n\\par(fn_57)filter_objects_by_shape ::tclevrshape ->list(tclevrobject) ->list(tclevrobject)\n\n(|$\\lambda$| (|$\\lambda$| (clevr_fold$0clevr_empty (|$\\lambda$| (|$\\lambda$| (clevr_if (clevr_eq_shape (clevr_query_shape$1)$3) (clevr_add$1$0)$0))))))\n\n{-Filtersalistofobjectstoincludeonlythosewiththespecifiedshape. -}\n\n\\par{-Exampleusages -}\n\n‚Äìfindthecubes\n\n%****iclr2024_conference.texLine 850 ****(|$\\lambda$| (filter_objects_by_shapeclevr_cube$0))\n\n‚Äìfindtherubbercube\n\n(|$\\lambda$| (filter_objects_by_rubber_material (filter_objects_by_shapeclevr_cube$0)))\n\n‚Äìifyouremovedthecylindershowmanylargethingswouldbeleft\n\n(|$\\lambda$| (clevr_count (clevr_difference (filter_large_objects_by_size$0) (filter_objects_by_shapeclevr_cylinder$0))))\n\n\\par(fn_58)filter_objects_by_color ::tclevrcolor ->list(tclevrobject) ->list(tclevrobject)\n\n(|$\\lambda$| (|$\\lambda$| (clevr_fold$0$0 (|$\\lambda$| (|$\\lambda$| (clevr_map (|$\\lambda$| (clevr_if (clevr_eq_color (clevr_query_color$0)$4)$0$2))$0))))))\n\n{-Returnsalistofobjectsintheinputlistthathavethespecifiedcolor. -}\n\n\\par{-Exampleusages -}\n\n‚Äìfindthegrayrubberthing\n\n(|$\\lambda$| (filter_objects_by_rubber_material (filter_objects_by_colorclevr_gray$0)))\n\n‚Äìwhatisthethingthatisfrontthebrownthingmadeof\n\n(|$\\lambda$| (clevr_query_material (clevr_car (clevr_relate (clevr_car (filter_objects_by_colorclevr_brown$0))clevr_front$0))))\n\n‚Äìwhatnumberofsmallobjectsareeithermetalcubesorredrubberthings\n\n(|$\\lambda$| (clevr_count (filter_objects_by_small_size (clevr_union (filter_objects_by_material (filter_objects_by_shapeclevr_cube$0)) (filter_objects_by_rubber_material (filter_objects_by_colorclevr_red$0))))))\n\n\\par(fn_59)filter_objects_by_small_size ::list(tclevrobject) ->list(tclevrobject)\n\n(|$\\lambda$| (filter_by_sizeclevr_small$0))\n\n{-Returnsalistofobjectsintheinputlistthataresmallinsize. -}\n\n\\par{-Exampleusages -}\n\n‚Äìfindthesmallredthing\n\n(|$\\lambda$| (filter_objects_by_small_size (filter_objects_by_colorclevr_red$0)))\n\n%****iclr2024_conference.texLine 875 ****‚Äìfindthesmallthings\n\n(|$\\lambda$| (filter_objects_by_small_size$0))\n\n‚Äìwhatnumberofsmallobjectsareeitherbluemetalthingsorrubberthings\n\n(|$\\lambda$| (clevr_count (filter_objects_by_small_size (clevr_union (filter_objects_by_rubber_material$0) (filter_objects_by_material (filter_objects_by_colorclevr_blue$0))))))\n\n\\par(fn_60)filter_objects_by_material ::list(tclevrobject) ->list(tclevrobject)\n\n(|$\\lambda$| (filter_by_materialclevr_metal$0))\n\n{-Returnsalistofobjectsintheinputlistthathavethespecifiedmaterial. -}\n\n\\par{-Exampleusages -}\n\n‚Äìthereisametalcylinderrightthesmallpurplemetalthingwhatisitssize\n\n(|$\\lambda$| (clevr_if (clevr_eq_shapeclevr_cube (clevr_query_shape (clevr_car (clevr_relate (clevr_car (clevr_union$0 (filter_objects_by_material$0)))clevr_right$0))))clevr_smallclevr_large))\n\n‚Äìwhatifyouremovedallofthebluemetalthings\n\n(|$\\lambda$| (clevr_difference$0 (filter_objects_by_colorclevr_blue (filter_objects_by_material$0))))\n\n‚Äìfindthesmallmetalcylinder\n\n(|$\\lambda$| (filter_objects_by_small_size (filter_objects_by_material (filter_objects_by_shapeclevr_cylinder$0))))\n\n\\par(fn_61)count_remaining_objects_by_color_and_shape ::list(tclevrobject) ->tclevrcolor ->tclevrshape ->int\n\n(|$\\lambda$| (|$\\lambda$| (|$\\lambda$| (clevr_count (clevr_difference (filter_objects_by_shape$0$2) (filter_objects_by_color$1$2))))))\n\n{-Countsthenumberofobjectsthatremainafterremovingobjectsofaspecifiedcolorandshapefromtheinputlistofobjects. -}\n\n\\par{-Exampleusages -}\n\n‚Äìifyouremovedthebrownthingshowmanysphereswouldbeleft\n\n(|$\\lambda$| (count_remaining_objects_by_color_and_shape$0clevr_brownclevr_sphere))\n\n‚Äìifyouremovedtheredcubeshowmanycubeswouldbeleft\n\n%****iclr2024_conference.texLine 900 ****(|$\\lambda$| (count_remaining_objects_by_color_and_shape$0clevr_redclevr_cube))\n\n‚Äìifyouremovedthecyancylindershowmanycylinderswouldbeleft\n\n(|$\\lambda$| (count_remaining_objects_by_color_and_shape$0clevr_cyanclevr_cylinder))\n\n\\par(fn_62)filter_objects_by_rubber_material ::list(tclevrobject) ->list(tclevrobject)\n\n(|$\\lambda$| (filter_by_materialclevr_rubber$0))\n\n{-Returnsalistofobjectsintheinputlistthathaverubberastheirmaterial. -}\n\n\\par{-Exampleusages -}\n\n‚Äìwhatnumberofspheresaresmallcyanmetalthingsorsmallrubberthings\n\n(|$\\lambda$| (clevr_count (clevr_union (filter_objects_by_material (filter_objects_by_small_size (filter_by_colorclevr_cyan (filter_objects_by_shapeclevr_sphere$0)))) (filter_objects_by_rubber_material (filter_objects_by_small_size (filter_objects_by_shapeclevr_sphere$0))))))\n\n‚Äìwhatnumberofrubberobjectsarepurplethingsorcylinders\n\n(|$\\lambda$| (clevr_count (filter_objects_by_rubber_material (clevr_union (filter_objects_by_shapeclevr_cylinder$0) (filter_objects_by_colorclevr_purple$0)))))\n\n‚Äìwhatnumberofcylindersareeitherlargerubberthingsorsmallbluerubberthings\n\n(|$\\lambda$| (clevr_count (clevr_intersect (filter_objects_by_rubber_material$0) (filter_objects_by_shapeclevr_cylinder$0))))\n\n\\par(fn_63)filter_large_objects_by_size ::list(tclevrobject) ->list(tclevrobject)\n\n(|$\\lambda$| (filter_by_sizeclevr_large$0))\n\n{-Returnsalistofobjectsintheinputlistthatarelargeinsize. -}\n\n\\par{-Exampleusages -}\n\n‚Äìfindthelargemetalsphere\n\n(|$\\lambda$| (filter_large_objects_by_size (filter_objects_by_material (filter_objects_by_shapeclevr_sphere$0))))\n\n‚Äìthereisalargethingfrontthesmallmetalcubewhatisitsshape\n\n(|$\\lambda$| (clevr_query_shape (clevr_car (filter_large_objects_by_size (clevr_relate (clevr_car (filter_objects_by_small_size (filter_objects_by_material (filter_objects_by_shapeclevr_cube$0))))clevr_front$0)))))\n\n%****iclr2024_conference.texLine 925 ****‚Äìwhatnumberofcylindersareeitherlargerubberthingsorsmallbluerubberthings\n\n(|$\\lambda$| (clevr_count (filter_objects_by_shapeclevr_cylinder (clevr_union (filter_objects_by_rubber_material (filter_large_objects_by_size$0)) (filter_objects_by_small_size (filter_by_colorclevr_blue (filter_objects_by_rubber_material$0)))))))\n\nB.2.3 Library for LOGO\n\n%****iclr2024_conference.texLine 950 ****(fn_27)turtle_loop_move_rotate ::turtle ->int ->tlength ->turtle\n\n(|$\\lambda$| (|$\\lambda$| (|$\\lambda$| (logo_for_loop$1 (|$\\lambda$| (|$\\lambda$| (logo_move_pen_forward_rotate$2 (logo_divide_anglelogo_unit_angle$3)$0)))$2))))\n\n{-Repeatedlymovetheturtleforwardandrotateitbyaspecifiedangle,creatingaloopofaspecificnumberofsideswithagivenlinelength. -}\n\n\\par{-Exampleusages -}\n\n‚Äìasmallsquare\n\n(|$\\lambda$| (turtle_loop_move_rotate$0 4logo_unit_line))\n\n‚Äìasmall 7gon\n\n(|$\\lambda$| (turtle_loop_move_rotate$0 7logo_unit_line))\n\n‚Äìashortline\n\n(|$\\lambda$| (turtle_loop_move_rotate$0 1logo_unit_line))\n\n\\par(fn_28)turtle_staircase ::turtle ->int ->turtle\n\n(|$\\lambda$| (|$\\lambda$| (logo_for_loop$0 (|$\\lambda$| (|$\\lambda$| (logo_move_pen_forward_rotatelogo_unit_line (logo_divide_anglelogo_unit_angle 4) (logo_move_pen_forward_rotatelogo_unit_line (logo_subtract_angleslogo_unit_angle (logo_divide_anglelogo_unit_angle 4))$0))))$1)))\n\n{-Createsastaircasepatternbyrepeatedlymovingtheturtleforwardandrotatingitataspecificangle.Thenumberofstepsinthestaircaseisdeterminedbythefunctionargument. -}\n\n\\par{-Exampleusages -}\n\n‚Äìa 4steppedstaircase\n\n(|$\\lambda$| (turtle_staircase$0 4))\n\n‚Äìa 7steppedstaircase\n\n(|$\\lambda$| (turtle_staircase$0 7))\n\n‚Äìa 4steppedstaircase\n\n(|$\\lambda$| (turtle_staircase$0 4))\n\n\\par(fn_29)turtle_loop_draw_pentagon_spiral ::turtle ->int ->turtle\n\n%****iclr2024_conference.texLine 975 ****(|$\\lambda$| (|$\\lambda$| (logo_for_loop$0 (|$\\lambda$| (|$\\lambda$| (logo_move_pen_forward_rotatelogo_zero_line (logo_multiply_anglelogo_epsilon_angle 8) (logo_for_loop 9 (|$\\lambda$| (|$\\lambda$| (logo_move_pen_forward_rotatelogo_unit_line (logo_multiply_anglelogo_epsilon_angle 8)$0)))$0))))$1)))\n\n{-Createsaspiralofpentagonsbyrepeatedlydrawingapentagonandincrementingtheangleofeachsideoneachiteration.Thenumberofpentagonsinthespiralisdeterminedbythefunctionargument. -}\n\n\\par{-Exampleusages -}\n\n‚Äì4small 5gonsinarow\n\n(|$\\lambda$| (turtle_loop_draw_pentagon_spiral$0 4))\n\n‚Äì3small 5gonsinarow\n\n(|$\\lambda$| (turtle_loop_draw_pentagon_spiral$0 3))\n\n‚Äì6small 5gonsinarow\n\n(|$\\lambda$| (turtle_loop_draw_pentagon_spiral$0 6))\n\n\\par(fn_30)turtle_square_row ::turtle ->int ->t"
    }
}
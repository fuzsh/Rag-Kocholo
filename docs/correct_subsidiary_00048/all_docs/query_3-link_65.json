{
    "id": "correct_subsidiary_00048_3",
    "rank": 65,
    "data": {
        "url": "https://www.usenix.org/conference/nsdi24/technical-sessions",
        "read_more_link": "",
        "language": "en",
        "title": "NSDI '24 Technical Sessions",
        "top_image": "https://www.usenix.org/sites/default/files/nsdi24_banner_social_media_1200x630.png",
        "meta_img": "https://www.usenix.org/sites/default/files/nsdi24_banner_social_media_1200x630.png",
        "images": [
            "https://www.usenix.org/sites/default/files/styles/neat_conference_menu_logo/public/nsdi24_logo_wordmark_500.png?itok=fFLF_fM9",
            "https://www.usenix.org/modules/file/icons/application-pdf.png",
            "https://www.usenix.org/modules/file/icons/application-pdf.png",
            "https://www.usenix.org/modules/file/icons/application-pdf.png",
            "https://www.usenix.org/modules/file/icons/application-pdf.png",
            "https://www.usenix.org/modules/file/icons/application-pdf.png",
            "https://www.usenix.org/sites/all/modules/usenix/usenix_files/images/usenix-locked.png",
            "https://www.usenix.org/modules/file/icons/application-pdf.png",
            "https://www.usenix.org/sites/all/themes/custom/neat_conference/images/icons/pdf.svg",
            "https://www.usenix.org/sites/all/themes/custom/neat_conference/images/icons/pdf.svg",
            "https://www.usenix.org/sites/all/themes/custom/neat_conference/images/icons/video.svg",
            "https://www.usenix.org/sites/all/themes/custom/neat_conference/images/icons/pdf.svg",
            "https://www.usenix.org/sites/all/themes/custom/neat_conference/images/icons/slides.svg",
            "https://www.usenix.org/sites/all/themes/custom/neat_conference/images/icons/video.svg",
            "https://www.usenix.org/sites/all/themes/custom/neat_conference/images/icons/pdf.svg",
            "https://www.usenix.org/sites/all/themes/custom/neat_conference/images/icons/slides.svg",
            "https://www.usenix.org/sites/all/themes/custom/neat_conference/images/icons/video.svg",
            "https://www.usenix.org/sites/all/themes/custom/neat_conference/images/icons/pdf.svg",
            "https://www.usenix.org/sites/all/themes/custom/neat_conference/images/icons/slides.svg",
            "https://www.usenix.org/sites/all/themes/custom/neat_conference/images/icons/video.svg",
            "https://www.usenix.org/sites/all/themes/custom/neat_conference/images/icons/pdf.svg",
            "https://www.usenix.org/sites/all/themes/custom/neat_conference/images/icons/pdf.svg",
            "https://www.usenix.org/sites/all/themes/custom/neat_conference/images/icons/slides.svg",
            "https://www.usenix.org/sites/all/themes/custom/neat_conference/images/icons/video.svg",
            "https://www.usenix.org/sites/all/themes/custom/neat_conference/images/icons/pdf.svg",
            "https://www.usenix.org/sites/all/themes/custom/neat_conference/images/icons/pdf.svg",
            "https://www.usenix.org/sites/all/themes/custom/neat_conference/images/icons/slides.svg",
            "https://www.usenix.org/sites/all/themes/custom/neat_conference/images/icons/video.svg",
            "https://www.usenix.org/sites/all/themes/custom/neat_conference/images/icons/pdf.svg",
            "https://www.usenix.org/sites/all/themes/custom/neat_conference/images/icons/slides.svg",
            "https://www.usenix.org/sites/all/themes/custom/neat_conference/images/icons/video.svg",
            "https://www.usenix.org/sites/all/themes/custom/neat_conference/images/icons/pdf.svg",
            "https://www.usenix.org/sites/all/themes/custom/neat_conference/images/icons/slides.svg",
            "https://www.usenix.org/sites/all/themes/custom/neat_conference/images/icons/video.svg",
            "https://www.usenix.org/sites/all/themes/custom/neat_conference/images/icons/pdf.svg",
            "https://www.usenix.org/sites/all/themes/custom/neat_conference/images/icons/pdf.svg",
            "https://www.usenix.org/sites/all/themes/custom/neat_conference/images/icons/slides.svg",
            "https://www.usenix.org/sites/all/themes/custom/neat_conference/images/icons/video.svg",
            "https://www.usenix.org/sites/all/themes/custom/neat_conference/images/icons/pdf.svg",
            "https://www.usenix.org/sites/all/themes/custom/neat_conference/images/icons/slides.svg",
            "https://www.usenix.org/sites/all/themes/custom/neat_conference/images/icons/video.svg",
            "https://www.usenix.org/sites/all/themes/custom/neat_conference/images/icons/pdf.svg",
            "https://www.usenix.org/sites/all/themes/custom/neat_conference/images/icons/video.svg",
            "https://www.usenix.org/sites/all/themes/custom/neat_conference/images/icons/pdf.svg",
            "https://www.usenix.org/sites/all/themes/custom/neat_conference/images/icons/slides.svg",
            "https://www.usenix.org/sites/all/themes/custom/neat_conference/images/icons/video.svg",
            "https://www.usenix.org/sites/all/themes/custom/neat_conference/images/icons/pdf.svg",
            "https://www.usenix.org/sites/all/themes/custom/neat_conference/images/icons/video.svg",
            "https://www.usenix.org/sites/all/themes/custom/neat_conference/images/icons/pdf.svg",
            "https://www.usenix.org/sites/all/themes/custom/neat_conference/images/icons/slides.svg",
            "https://www.usenix.org/sites/all/themes/custom/neat_conference/images/icons/video.svg",
            "https://www.usenix.org/sites/all/themes/custom/neat_conference/images/icons/pdf.svg",
            "https://www.usenix.org/sites/all/themes/custom/neat_conference/images/icons/pdf.svg",
            "https://www.usenix.org/sites/all/themes/custom/neat_conference/images/icons/slides.svg",
            "https://www.usenix.org/sites/all/themes/custom/neat_conference/images/icons/video.svg",
            "https://www.usenix.org/sites/all/themes/custom/neat_conference/images/icons/pdf.svg",
            "https://www.usenix.org/sites/all/themes/custom/neat_conference/images/icons/pdf.svg",
            "https://www.usenix.org/sites/all/themes/custom/neat_conference/images/icons/video.svg",
            "https://www.usenix.org/sites/all/themes/custom/neat_conference/images/icons/pdf.svg",
            "https://www.usenix.org/sites/all/themes/custom/neat_conference/images/icons/video.svg",
            "https://www.usenix.org/sites/all/themes/custom/neat_conference/images/icons/pdf.svg",
            "https://www.usenix.org/sites/all/themes/custom/neat_conference/images/icons/video.svg",
            "https://www.usenix.org/sites/all/themes/custom/neat_conference/images/icons/pdf.svg",
            "https://www.usenix.org/sites/all/themes/custom/neat_conference/images/icons/video.svg",
            "https://www.usenix.org/sites/all/themes/custom/neat_conference/images/icons/pdf.svg",
            "https://www.usenix.org/sites/all/themes/custom/neat_conference/images/icons/video.svg",
            "https://www.usenix.org/sites/all/themes/custom/neat_conference/images/icons/pdf.svg",
            "https://www.usenix.org/sites/all/themes/custom/neat_conference/images/icons/pdf.svg",
            "https://www.usenix.org/sites/all/themes/custom/neat_conference/images/icons/slides.svg",
            "https://www.usenix.org/sites/all/themes/custom/neat_conference/images/icons/video.svg",
            "https://www.usenix.org/sites/all/themes/custom/neat_conference/images/icons/pdf.svg",
            "https://www.usenix.org/sites/all/themes/custom/neat_conference/images/icons/pdf.svg",
            "https://www.usenix.org/sites/all/themes/custom/neat_conference/images/icons/slides.svg",
            "https://www.usenix.org/sites/all/themes/custom/neat_conference/images/icons/video.svg",
            "https://www.usenix.org/sites/all/themes/custom/neat_conference/images/icons/pdf.svg",
            "https://www.usenix.org/sites/all/themes/custom/neat_conference/images/icons/slides.svg",
            "https://www.usenix.org/sites/all/themes/custom/neat_conference/images/icons/video.svg",
            "https://www.usenix.org/sites/all/themes/custom/neat_conference/images/icons/pdf.svg",
            "https://www.usenix.org/sites/all/themes/custom/neat_conference/images/icons/slides.svg",
            "https://www.usenix.org/sites/all/themes/custom/neat_conference/images/icons/video.svg",
            "https://www.usenix.org/sites/all/themes/custom/neat_conference/images/icons/pdf.svg",
            "https://www.usenix.org/sites/all/themes/custom/neat_conference/images/icons/video.svg",
            "https://www.usenix.org/sites/all/themes/custom/neat_conference/images/icons/pdf.svg",
            "https://www.usenix.org/sites/all/themes/custom/neat_conference/images/icons/slides.svg",
            "https://www.usenix.org/sites/all/themes/custom/neat_conference/images/icons/video.svg",
            "https://www.usenix.org/sites/all/themes/custom/neat_conference/images/icons/pdf.svg",
            "https://www.usenix.org/sites/all/themes/custom/neat_conference/images/icons/video.svg",
            "https://www.usenix.org/sites/all/themes/custom/neat_conference/images/icons/pdf.svg",
            "https://www.usenix.org/sites/all/themes/custom/neat_conference/images/icons/pdf.svg",
            "https://www.usenix.org/sites/all/themes/custom/neat_conference/images/icons/slides.svg",
            "https://www.usenix.org/sites/all/themes/custom/neat_conference/images/icons/video.svg",
            "https://www.usenix.org/sites/all/themes/custom/neat_conference/images/icons/pdf.svg",
            "https://www.usenix.org/sites/all/themes/custom/neat_conference/images/icons/slides.svg",
            "https://www.usenix.org/sites/all/themes/custom/neat_conference/images/icons/video.svg",
            "https://www.usenix.org/sites/all/themes/custom/neat_conference/images/icons/pdf.svg",
            "https://www.usenix.org/sites/all/themes/custom/neat_conference/images/icons/slides.svg",
            "https://www.usenix.org/sites/all/themes/custom/neat_conference/images/icons/video.svg",
            "https://www.usenix.org/sites/all/themes/custom/neat_conference/images/icons/pdf.svg",
            "https://www.usenix.org/sites/all/themes/custom/neat_conference/images/icons/video.svg",
            "https://www.usenix.org/sites/all/themes/custom/neat_conference/images/icons/pdf.svg",
            "https://www.usenix.org/sites/all/themes/custom/neat_conference/images/icons/pdf.svg",
            "https://www.usenix.org/sites/all/themes/custom/neat_conference/images/icons/video.svg",
            "https://www.usenix.org/sites/all/themes/custom/neat_conference/images/icons/pdf.svg",
            "https://www.usenix.org/sites/all/themes/custom/neat_conference/images/icons/pdf.svg",
            "https://www.usenix.org/sites/all/themes/custom/neat_conference/images/icons/video.svg",
            "https://www.usenix.org/sites/all/themes/custom/neat_conference/images/icons/pdf.svg",
            "https://www.usenix.org/sites/all/themes/custom/neat_conference/images/icons/slides.svg",
            "https://www.usenix.org/sites/all/themes/custom/neat_conference/images/icons/video.svg",
            "https://www.usenix.org/sites/all/themes/custom/neat_conference/images/icons/pdf.svg",
            "https://www.usenix.org/sites/all/themes/custom/neat_conference/images/icons/slides.svg",
            "https://www.usenix.org/sites/all/themes/custom/neat_conference/images/icons/video.svg",
            "https://www.usenix.org/sites/all/themes/custom/neat_conference/images/icons/pdf.svg",
            "https://www.usenix.org/sites/all/themes/custom/neat_conference/images/icons/slides.svg",
            "https://www.usenix.org/sites/all/themes/custom/neat_conference/images/icons/video.svg",
            "https://www.usenix.org/sites/all/themes/custom/neat_conference/images/icons/pdf.svg",
            "https://www.usenix.org/sites/all/themes/custom/neat_conference/images/icons/slides.svg",
            "https://www.usenix.org/sites/all/themes/custom/neat_conference/images/icons/video.svg",
            "https://www.usenix.org/sites/all/themes/custom/neat_conference/images/icons/pdf.svg",
            "https://www.usenix.org/sites/all/themes/custom/neat_conference/images/icons/pdf.svg",
            "https://www.usenix.org/sites/all/themes/custom/neat_conference/images/icons/slides.svg",
            "https://www.usenix.org/sites/all/themes/custom/neat_conference/images/icons/video.svg",
            "https://www.usenix.org/sites/all/themes/custom/neat_conference/images/icons/pdf.svg",
            "https://www.usenix.org/sites/all/themes/custom/neat_conference/images/icons/slides.svg",
            "https://www.usenix.org/sites/all/themes/custom/neat_conference/images/icons/video.svg",
            "https://www.usenix.org/sites/all/themes/custom/neat_conference/images/icons/pdf.svg",
            "https://www.usenix.org/sites/all/themes/custom/neat_conference/images/icons/video.svg",
            "https://www.usenix.org/sites/all/themes/custom/neat_conference/images/icons/pdf.svg",
            "https://www.usenix.org/sites/all/themes/custom/neat_conference/images/icons/slides.svg",
            "https://www.usenix.org/sites/all/themes/custom/neat_conference/images/icons/video.svg",
            "https://www.usenix.org/sites/all/themes/custom/neat_conference/images/icons/pdf.svg",
            "https://www.usenix.org/sites/all/themes/custom/neat_conference/images/icons/pdf.svg",
            "https://www.usenix.org/sites/all/themes/custom/neat_conference/images/icons/video.svg",
            "https://www.usenix.org/sites/all/themes/custom/neat_conference/images/icons/pdf.svg",
            "https://www.usenix.org/sites/all/themes/custom/neat_conference/images/icons/pdf.svg",
            "https://www.usenix.org/sites/all/themes/custom/neat_conference/images/icons/video.svg",
            "https://www.usenix.org/sites/all/themes/custom/neat_conference/images/icons/pdf.svg",
            "https://www.usenix.org/sites/all/themes/custom/neat_conference/images/icons/slides.svg",
            "https://www.usenix.org/sites/all/themes/custom/neat_conference/images/icons/video.svg",
            "https://www.usenix.org/sites/all/themes/custom/neat_conference/images/icons/pdf.svg",
            "https://www.usenix.org/sites/all/themes/custom/neat_conference/images/icons/pdf.svg",
            "https://www.usenix.org/sites/all/themes/custom/neat_conference/images/icons/video.svg",
            "https://www.usenix.org/sites/all/themes/custom/neat_conference/images/icons/pdf.svg",
            "https://www.usenix.org/sites/all/themes/custom/neat_conference/images/icons/pdf.svg",
            "https://www.usenix.org/sites/all/themes/custom/neat_conference/images/icons/video.svg",
            "https://www.usenix.org/sites/all/themes/custom/neat_conference/images/icons/pdf.svg",
            "https://www.usenix.org/sites/all/themes/custom/neat_conference/images/icons/video.svg",
            "https://www.usenix.org/sites/all/themes/custom/neat_conference/images/icons/pdf.svg",
            "https://www.usenix.org/sites/all/themes/custom/neat_conference/images/icons/pdf.svg",
            "https://www.usenix.org/sites/all/themes/custom/neat_conference/images/icons/slides.svg",
            "https://www.usenix.org/sites/all/themes/custom/neat_conference/images/icons/video.svg",
            "https://www.usenix.org/sites/all/themes/custom/neat_conference/images/icons/pdf.svg",
            "https://www.usenix.org/sites/all/themes/custom/neat_conference/images/icons/slides.svg",
            "https://www.usenix.org/sites/all/themes/custom/neat_conference/images/icons/video.svg",
            "https://www.usenix.org/sites/all/themes/custom/neat_conference/images/icons/pdf.svg",
            "https://www.usenix.org/sites/all/themes/custom/neat_conference/images/icons/pdf.svg",
            "https://www.usenix.org/sites/all/themes/custom/neat_conference/images/icons/video.svg",
            "https://www.usenix.org/sites/all/themes/custom/neat_conference/images/icons/pdf.svg",
            "https://www.usenix.org/sites/all/themes/custom/neat_conference/images/icons/pdf.svg",
            "https://www.usenix.org/sites/all/themes/custom/neat_conference/images/icons/pdf.svg",
            "https://www.usenix.org/sites/all/themes/custom/neat_conference/images/icons/slides.svg",
            "https://www.usenix.org/sites/all/themes/custom/neat_conference/images/icons/video.svg",
            "https://www.usenix.org/sites/all/themes/custom/neat_conference/images/icons/pdf.svg",
            "https://www.usenix.org/sites/all/themes/custom/neat_conference/images/icons/video.svg",
            "https://www.usenix.org/sites/all/themes/custom/neat_conference/images/icons/pdf.svg",
            "https://www.usenix.org/sites/all/themes/custom/neat_conference/images/icons/slides.svg",
            "https://www.usenix.org/sites/all/themes/custom/neat_conference/images/icons/video.svg",
            "https://www.usenix.org/sites/all/themes/custom/neat_conference/images/icons/pdf.svg",
            "https://www.usenix.org/sites/all/themes/custom/neat_conference/images/icons/pdf.svg",
            "https://www.usenix.org/sites/all/themes/custom/neat_conference/images/icons/video.svg",
            "https://www.usenix.org/sites/all/themes/custom/neat_conference/images/icons/pdf.svg",
            "https://www.usenix.org/sites/all/themes/custom/neat_conference/images/icons/pdf.svg",
            "https://www.usenix.org/sites/all/themes/custom/neat_conference/images/icons/video.svg",
            "https://www.usenix.org/sites/all/themes/custom/neat_conference/images/icons/pdf.svg",
            "https://www.usenix.org/sites/all/themes/custom/neat_conference/images/icons/slides.svg",
            "https://www.usenix.org/sites/all/themes/custom/neat_conference/images/icons/video.svg",
            "https://www.usenix.org/sites/all/themes/custom/neat_conference/images/icons/pdf.svg",
            "https://www.usenix.org/sites/all/themes/custom/neat_conference/images/icons/video.svg",
            "https://www.usenix.org/sites/all/themes/custom/neat_conference/images/icons/pdf.svg",
            "https://www.usenix.org/sites/all/themes/custom/neat_conference/images/icons/slides.svg",
            "https://www.usenix.org/sites/all/themes/custom/neat_conference/images/icons/video.svg",
            "https://www.usenix.org/sites/all/themes/custom/neat_conference/images/icons/pdf.svg",
            "https://www.usenix.org/sites/all/themes/custom/neat_conference/images/icons/slides.svg",
            "https://www.usenix.org/sites/all/themes/custom/neat_conference/images/icons/video.svg",
            "https://www.usenix.org/sites/all/themes/custom/neat_conference/images/icons/pdf.svg",
            "https://www.usenix.org/sites/all/themes/custom/neat_conference/images/icons/slides.svg",
            "https://www.usenix.org/sites/all/themes/custom/neat_conference/images/icons/video.svg",
            "https://www.usenix.org/sites/all/themes/custom/neat_conference/images/icons/pdf.svg",
            "https://www.usenix.org/sites/all/themes/custom/neat_conference/images/icons/pdf.svg",
            "https://www.usenix.org/sites/all/themes/custom/neat_conference/images/icons/slides.svg",
            "https://www.usenix.org/sites/all/themes/custom/neat_conference/images/icons/video.svg",
            "https://www.usenix.org/sites/all/themes/custom/neat_conference/images/icons/pdf.svg",
            "https://www.usenix.org/sites/all/themes/custom/neat_conference/images/icons/slides.svg",
            "https://www.usenix.org/sites/all/themes/custom/neat_conference/images/icons/video.svg",
            "https://www.usenix.org/sites/all/themes/custom/neat_conference/images/icons/pdf.svg",
            "https://www.usenix.org/sites/all/themes/custom/neat_conference/images/icons/slides.svg",
            "https://www.usenix.org/sites/all/themes/custom/neat_conference/images/icons/video.svg",
            "https://www.usenix.org/sites/all/themes/custom/neat_conference/images/icons/pdf.svg",
            "https://www.usenix.org/sites/all/themes/custom/neat_conference/images/icons/video.svg",
            "https://www.usenix.org/sites/all/themes/custom/neat_conference/images/icons/pdf.svg",
            "https://www.usenix.org/sites/all/themes/custom/neat_conference/images/icons/slides.svg",
            "https://www.usenix.org/sites/all/themes/custom/neat_conference/images/icons/video.svg",
            "https://www.usenix.org/sites/all/themes/custom/neat_conference/images/icons/pdf.svg",
            "https://www.usenix.org/sites/all/themes/custom/neat_conference/images/icons/slides.svg",
            "https://www.usenix.org/sites/all/themes/custom/neat_conference/images/icons/video.svg",
            "https://www.usenix.org/sites/all/themes/custom/neat_conference/images/icons/pdf.svg",
            "https://www.usenix.org/sites/all/themes/custom/neat_conference/images/icons/slides.svg",
            "https://www.usenix.org/sites/all/themes/custom/neat_conference/images/icons/video.svg",
            "https://www.usenix.org/sites/all/themes/custom/neat_conference/images/icons/pdf.svg",
            "https://www.usenix.org/sites/all/themes/custom/neat_conference/images/icons/pdf.svg",
            "https://www.usenix.org/sites/all/themes/custom/neat_conference/images/icons/slides.svg",
            "https://www.usenix.org/sites/all/themes/custom/neat_conference/images/icons/video.svg",
            "https://www.usenix.org/sites/all/themes/custom/neat_conference/images/icons/pdf.svg",
            "https://www.usenix.org/sites/all/themes/custom/neat_conference/images/icons/pdf.svg",
            "https://www.usenix.org/sites/all/themes/custom/neat_conference/images/icons/slides.svg",
            "https://www.usenix.org/sites/all/themes/custom/neat_conference/images/icons/video.svg",
            "https://www.usenix.org/sites/all/themes/custom/neat_conference/images/icons/pdf.svg",
            "https://www.usenix.org/sites/all/themes/custom/neat_conference/images/icons/slides.svg",
            "https://www.usenix.org/sites/all/themes/custom/neat_conference/images/icons/video.svg",
            "https://www.usenix.org/sites/all/themes/custom/neat_conference/images/icons/pdf.svg",
            "https://www.usenix.org/sites/all/themes/custom/neat_conference/images/icons/slides.svg",
            "https://www.usenix.org/sites/all/themes/custom/neat_conference/images/icons/video.svg",
            "https://www.usenix.org/sites/all/themes/custom/neat_conference/images/icons/pdf.svg",
            "https://www.usenix.org/sites/all/themes/custom/neat_conference/images/icons/slides.svg",
            "https://www.usenix.org/sites/all/themes/custom/neat_conference/images/icons/video.svg",
            "https://www.usenix.org/sites/all/themes/custom/neat_conference/images/icons/pdf.svg",
            "https://www.usenix.org/sites/all/themes/custom/neat_conference/images/icons/slides.svg",
            "https://www.usenix.org/sites/all/themes/custom/neat_conference/images/icons/video.svg",
            "https://www.usenix.org/sites/all/themes/custom/neat_conference/images/icons/pdf.svg",
            "https://www.usenix.org/sites/all/themes/custom/neat_conference/images/icons/video.svg",
            "https://www.usenix.org/sites/all/themes/custom/neat_conference/images/icons/pdf.svg",
            "https://www.usenix.org/sites/all/themes/custom/neat_conference/images/icons/slides.svg",
            "https://www.usenix.org/sites/all/themes/custom/neat_conference/images/icons/video.svg",
            "https://www.usenix.org/sites/all/themes/custom/neat_conference/images/icons/pdf.svg",
            "https://www.usenix.org/sites/all/themes/custom/neat_conference/images/icons/video.svg",
            "https://www.usenix.org/sites/all/themes/custom/neat_conference/images/icons/pdf.svg",
            "https://www.usenix.org/sites/all/themes/custom/neat_conference/images/icons/video.svg",
            "https://www.usenix.org/sites/all/themes/custom/neat_conference/images/icons/pdf.svg",
            "https://www.usenix.org/sites/all/themes/custom/neat_conference/images/icons/pdf.svg",
            "https://www.usenix.org/sites/all/themes/custom/neat_conference/images/icons/video.svg",
            "https://www.usenix.org/sites/all/themes/custom/neat_conference/images/icons/pdf.svg",
            "https://www.usenix.org/sites/all/themes/custom/neat_conference/images/icons/slides.svg",
            "https://www.usenix.org/sites/all/themes/custom/neat_conference/images/icons/video.svg",
            "https://www.usenix.org/sites/all/themes/custom/neat_conference/images/icons/pdf.svg",
            "https://www.usenix.org/sites/all/themes/custom/neat_conference/images/icons/slides.svg",
            "https://www.usenix.org/sites/all/themes/custom/neat_conference/images/icons/video.svg",
            "https://www.usenix.org/sites/all/themes/custom/neat_conference/images/icons/pdf.svg",
            "https://www.usenix.org/sites/all/themes/custom/neat_conference/images/icons/pdf.svg",
            "https://www.usenix.org/sites/all/themes/custom/neat_conference/images/icons/slides.svg",
            "https://www.usenix.org/sites/all/themes/custom/neat_conference/images/icons/video.svg",
            "https://www.usenix.org/sites/all/themes/custom/neat_conference/images/icons/pdf.svg",
            "https://www.usenix.org/sites/all/themes/custom/neat_conference/images/icons/video.svg",
            "https://www.usenix.org/sites/all/themes/custom/neat_conference/images/icons/pdf.svg",
            "https://www.usenix.org/sites/all/themes/custom/neat_conference/images/icons/video.svg",
            "https://www.usenix.org/sites/all/themes/custom/neat_conference/images/icons/pdf.svg",
            "https://www.usenix.org/sites/all/themes/custom/neat_conference/images/icons/slides.svg",
            "https://www.usenix.org/sites/all/themes/custom/neat_conference/images/icons/video.svg",
            "https://www.usenix.org/sites/all/themes/custom/neat_conference/images/icons/pdf.svg",
            "https://www.usenix.org/sites/all/themes/custom/neat_conference/images/icons/pdf.svg",
            "https://www.usenix.org/sites/all/themes/custom/neat_conference/images/icons/slides.svg",
            "https://www.usenix.org/sites/all/themes/custom/neat_conference/images/icons/video.svg",
            "https://www.usenix.org/sites/all/themes/custom/neat_conference/images/icons/pdf.svg",
            "https://www.usenix.org/sites/all/themes/custom/neat_conference/images/icons/pdf.svg",
            "https://www.usenix.org/sites/all/themes/custom/neat_conference/images/icons/slides.svg",
            "https://www.usenix.org/sites/all/themes/custom/neat_conference/images/icons/video.svg",
            "https://www.usenix.org/sites/all/themes/custom/neat_conference/images/icons/pdf.svg",
            "https://www.usenix.org/sites/all/themes/custom/neat_conference/images/icons/slides.svg",
            "https://www.usenix.org/sites/all/themes/custom/neat_conference/images/icons/video.svg",
            "https://www.usenix.org/sites/all/themes/custom/neat_conference/images/icons/pdf.svg",
            "https://www.usenix.org/sites/all/themes/custom/neat_conference/images/icons/pdf.svg",
            "https://www.usenix.org/sites/all/themes/custom/neat_conference/images/icons/slides.svg",
            "https://www.usenix.org/sites/all/themes/custom/neat_conference/images/icons/video.svg",
            "https://www.usenix.org/sites/all/themes/custom/neat_conference/images/icons/pdf.svg",
            "https://www.usenix.org/sites/all/themes/custom/neat_conference/images/icons/pdf.svg",
            "https://www.usenix.org/sites/all/themes/custom/neat_conference/images/icons/slides.svg",
            "https://www.usenix.org/sites/all/themes/custom/neat_conference/images/icons/video.svg",
            "https://www.usenix.org/sites/all/themes/custom/neat_conference/images/icons/pdf.svg",
            "https://www.usenix.org/sites/all/themes/custom/neat_conference/images/icons/slides.svg",
            "https://www.usenix.org/sites/all/themes/custom/neat_conference/images/icons/video.svg",
            "https://www.usenix.org/sites/all/themes/custom/neat_conference/images/icons/pdf.svg",
            "https://www.usenix.org/sites/all/themes/custom/neat_conference/images/icons/video.svg",
            "https://www.usenix.org/sites/all/themes/custom/neat_conference/images/icons/pdf.svg",
            "https://www.usenix.org/sites/all/themes/custom/neat_conference/images/icons/pdf.svg",
            "https://www.usenix.org/sites/all/themes/custom/neat_conference/images/icons/slides.svg",
            "https://www.usenix.org/sites/all/themes/custom/neat_conference/images/icons/video.svg",
            "https://www.usenix.org/sites/all/themes/custom/neat_conference/images/icons/pdf.svg",
            "https://www.usenix.org/sites/all/themes/custom/neat_conference/images/icons/pdf.svg",
            "https://www.usenix.org/sites/all/themes/custom/neat_conference/images/icons/video.svg",
            "https://www.usenix.org/sites/all/themes/custom/neat_conference/images/icons/pdf.svg",
            "https://www.usenix.org/sites/all/themes/custom/neat_conference/images/icons/pdf.svg",
            "https://www.usenix.org/sites/all/themes/custom/neat_conference/images/icons/slides.svg",
            "https://www.usenix.org/sites/all/themes/custom/neat_conference/images/icons/video.svg",
            "https://www.usenix.org/sites/all/themes/custom/neat_conference/images/icons/pdf.svg",
            "https://www.usenix.org/sites/all/themes/custom/neat_conference/images/icons/slides.svg",
            "https://www.usenix.org/sites/all/themes/custom/neat_conference/images/icons/video.svg",
            "https://www.usenix.org/sites/all/themes/custom/neat_conference/images/icons/pdf.svg",
            "https://www.usenix.org/sites/all/themes/custom/neat_conference/images/icons/pdf.svg",
            "https://www.usenix.org/sites/all/themes/custom/neat_conference/images/icons/slides.svg",
            "https://www.usenix.org/sites/all/themes/custom/neat_conference/images/icons/video.svg",
            "https://www.usenix.org/sites/all/themes/custom/neat_conference/images/icons/pdf.svg",
            "https://www.usenix.org/sites/all/themes/custom/neat_conference/images/icons/pdf.svg",
            "https://www.usenix.org/sites/all/themes/custom/neat_conference/images/icons/video.svg",
            "https://www.usenix.org/sites/all/themes/custom/neat_conference/images/icons/pdf.svg",
            "https://www.usenix.org/sites/all/themes/custom/neat_conference/images/icons/pdf.svg",
            "https://www.usenix.org/sites/all/themes/custom/neat_conference/images/icons/slides.svg",
            "https://www.usenix.org/sites/all/themes/custom/neat_conference/images/icons/video.svg",
            "https://www.usenix.org/sites/all/themes/custom/neat_conference/images/icons/pdf.svg",
            "https://www.usenix.org/sites/all/themes/custom/neat_conference/images/icons/slides.svg",
            "https://www.usenix.org/sites/all/themes/custom/neat_conference/images/icons/video.svg",
            "https://www.usenix.org/sites/all/themes/custom/neat_conference/images/icons/pdf.svg",
            "https://www.usenix.org/sites/all/themes/custom/neat_conference/images/icons/pdf.svg",
            "https://www.usenix.org/sites/all/themes/custom/neat_conference/images/icons/slides.svg",
            "https://www.usenix.org/sites/all/themes/custom/neat_conference/images/icons/video.svg",
            "https://www.usenix.org/sites/all/themes/custom/neat_conference/images/icons/pdf.svg",
            "https://www.usenix.org/sites/all/themes/custom/neat_conference/images/icons/pdf.svg",
            "https://www.usenix.org/sites/all/themes/custom/neat_conference/images/icons/slides.svg",
            "https://www.usenix.org/sites/all/themes/custom/neat_conference/images/icons/video.svg",
            "https://www.usenix.org/sites/all/themes/custom/neat_conference/images/icons/pdf.svg",
            "https://www.usenix.org/sites/all/themes/custom/neat_conference/images/icons/video.svg",
            "https://www.usenix.org/sites/all/themes/custom/neat_conference/images/icons/pdf.svg",
            "https://www.usenix.org/sites/all/themes/custom/neat_conference/images/icons/pdf.svg",
            "https://www.usenix.org/sites/all/themes/custom/neat_conference/images/icons/slides.svg",
            "https://www.usenix.org/sites/all/themes/custom/neat_conference/images/icons/video.svg",
            "https://www.usenix.org/sites/all/themes/custom/neat_conference/images/icons/pdf.svg",
            "https://www.usenix.org/sites/all/themes/custom/neat_conference/images/icons/pdf.svg",
            "https://www.usenix.org/sites/all/themes/custom/neat_conference/images/icons/slides.svg",
            "https://www.usenix.org/sites/all/themes/custom/neat_conference/images/icons/video.svg",
            "https://www.usenix.org/sites/all/themes/custom/neat_conference/images/icons/pdf.svg",
            "https://www.usenix.org/sites/all/themes/custom/neat_conference/images/icons/pdf.svg",
            "https://www.usenix.org/sites/all/themes/custom/neat_conference/images/icons/video.svg",
            "https://www.usenix.org/sites/all/themes/custom/neat_conference/images/icons/pdf.svg",
            "https://www.usenix.org/sites/all/themes/custom/neat_conference/images/icons/pdf.svg",
            "https://www.usenix.org/sites/all/themes/custom/neat_conference/images/icons/slides.svg",
            "https://www.usenix.org/sites/all/themes/custom/neat_conference/images/icons/video.svg",
            "https://www.usenix.org/sites/all/themes/custom/neat_conference/images/icons/pdf.svg",
            "https://www.usenix.org/sites/all/themes/custom/neat_conference/images/icons/video.svg",
            "https://www.usenix.org/sites/all/themes/custom/neat_conference/images/icons/pdf.svg",
            "https://www.usenix.org/sites/all/themes/custom/neat_conference/images/icons/slides.svg",
            "https://www.usenix.org/sites/all/themes/custom/neat_conference/images/icons/video.svg",
            "https://www.usenix.org/sites/all/themes/custom/neat_conference/images/icons/pdf.svg",
            "https://www.usenix.org/sites/all/themes/custom/neat_conference/images/icons/video.svg",
            "https://www.usenix.org/sites/all/themes/custom/neat_conference/images/icons/pdf.svg",
            "https://www.usenix.org/sites/all/themes/custom/neat_conference/images/icons/slides.svg",
            "https://www.usenix.org/sites/all/themes/custom/neat_conference/images/icons/video.svg",
            "https://www.usenix.org/sites/all/themes/custom/neat_conference/images/icons/pdf.svg",
            "https://www.usenix.org/sites/all/themes/custom/neat_conference/images/icons/slides.svg",
            "https://www.usenix.org/sites/all/themes/custom/neat_conference/images/icons/video.svg",
            "https://www.usenix.org/sites/all/themes/custom/neat_conference/images/icons/pdf.svg",
            "https://www.usenix.org/sites/all/themes/custom/neat_conference/images/icons/slides.svg",
            "https://www.usenix.org/sites/all/themes/custom/neat_conference/images/icons/video.svg"
        ],
        "movies": [],
        "keywords": [],
        "meta_keywords": [
            ""
        ],
        "tags": null,
        "authors": [
            "Mayur Patel",
            "Amr Sabaa",
            "Arjun Singh",
            "Alex Smirnov",
            "Manish Verma",
            "Prerepa V Viswanadham",
            "Zhuotao Liu",
            "Biao Lyu",
            "Amin Vahdat",
            "Ennan Zhai"
        ],
        "publish_date": "2024-01-29T17:21:01-08:00",
        "summary": "",
        "meta_description": "The 21st USENIX Symposium on Networked Systems Design and Implementation (NSDI '24) will take place April 16–18, 2024, at the Hyatt Regency Santa Clara in Santa Clara, CA, USA. NSDI focuses on the design principles, implementation, and practical evaluation of networked and distributed systems.",
        "meta_lang": "en",
        "meta_favicon": "https://www.usenix.org/sites/default/files/waves_favicon.ico",
        "meta_site_name": "USENIX",
        "canonical_link": "https://www.usenix.org/conference/nsdi24/technical-sessions",
        "text": "Short-lived tasks are prevalent in modern interactive datacenter applications. However, designing schedulers to assign these tasks to workers distributed across the whole datacenter is challenging, because such schedulers need to make decisions at a microsecond scale, achieve high throughput, and minimize the tail response time. Current task schedulers in the literature are limited to individual racks. We present Horus, a new in-network task scheduler for short tasks that operates at the datacenter scale. Horus efficiently tracks and distributes the worker state among switches, which enables it to schedule tasks in parallel at line rate while optimizing the scheduling quality. We propose a new distributed task scheduling policy that minimizes the state and communication overheads, handles dynamic loads, and does not buffer tasks in switches. We compare Horus against the state-of-the-art in-network scheduler in a testbed with programmable switches as well as using simulations of datacenters with more than 27K hosts and thousands of switches handling diverse and dynamic workloads. Our results show that Horus efficiently scales to large datacenters, and it substantially outperforms the state-of-the-art across all performance metrics, including tail response time and throughput.\n\nVector query processing powers a wide range of AI applications. While GPUs are optimized for massive vector operations, today's practice relies on CPUs to process queries for large vector datasets, due to limited GPU memory.\n\nWe present RUMMY, the first GPU-accelerated vector query processing system that achieves high performance and supports large vector datasets beyond GPU memory. The core of RUMMY is a novel reordered pipelining technique that exploits the characteristics of vector query processing to efficiently pipeline data transmission from host memory to GPU memory and query processing in GPU. Specifically, it leverages three ideas: (i) cluster-based retrofitting to eliminate redundant data transmission across queries in a batch, (ii) dynamic kernel padding with cluster balancing to maximize spatial and temporal GPU utilization for GPU computation, and (iii) query-aware reordering and grouping to optimally overlap transmission and computation. We also tailor GPU memory management for vector queries to reduce GPU memory fragmentation and cache misses. We evaluate RUMMY with a variety of billion-scale benchmarking datasets. The experimental results show that RUMMY outperforms IVF-GPU with CUDA unified memory by up to 135×. Compared to the CPU-based solution (with 64 vCPUs), RUMMY (with one NVIDIA A100 GPU) achieves up to 23.1× better performance and is up to 37.7× more cost-effective.\n\nKernel bypass systems have demonstrated order of magnitude improvements in throughput and tail latency for network-intensive applications relative to traditional operating systems (OSes). To achieve such excellent performance, however, they rely on dedicated resources (e.g., spinning cores, pinned memory) and require application rewriting. This is unattractive to cloud operators because they aim to densely pack applications, and rewriting cloud software requires a massive investment of valuable developer time. For both reasons, kernel bypass, as it exists, is impractical for the cloud.\n\nIn this paper, we show these compromises are not necessary to unlock the full benefits of kernel bypass. We present Junction, the first kernel bypass system that can pack thousands of instances on a machine while providing compatibility with unmodified Linux applications. Junction achieves high density through several advanced NIC features that reduce pinned memory and the overhead of monitoring large numbers of queues. It maintains compatibility with minimal overhead through optimizations that exploit a shared address space with the application. Junction scales to 19–62× more instances than existing kernel bypass systems and can achieve similar or better performance without code changes. Furthermore, Junction delivers significant performance benefits to applications previously unsupported by kernel bypass, including those that depend on runtime systems like Go, Java, Node, and Python. In a comparison to native Linux, Junction increases throughput by 1.6–7.0× while using 1.2–3.8× less cores across seven applications.\n\nPacket schedulers play a crucial role in determining the order in which packets are served. They achieve this by assigning a rank to each packet and sorting them based on these ranks. However, when dealing with a large number of flows at high packet rates, sorting functions can become extremely complex and time-consuming. To address this issue, fast-approximating packet schedulers have been proposed, but they come with the risk of producing scheduling errors, or packet inversions, which can lead to undesirable consequences. We present Sifter, a programmable packet scheduler that offers high accuracy and large capacity while ensuring inversion-free operation. Sifter employs a unique sorting technique called “Sift Sorting” to coarsely sort packets with larger ranks into buckets, while accurately and finely sorting those with smaller ranks using a small Push-In-First-Out (PIFO) queue in parallel. The sorting process takes advantage of the “Speed-up Factor”, which is a function of the memory bandwidth to output link bandwidth ratio, to achieve Sift Sorting and ensure accurate scheduling with low resource consumption. Sifter combines the benefits of PIFO’s accuracy and FIFO-based schedulers’ large capacity, resulting in guaranteed delivery of packets in an accurate scheduling order. Our simulation results demonstrate Sifter’s efficiency in achieving inversion-free scheduling, while the FPGA-based hardware prototype validates that Sifter supports a throughput of 100Gbps without packet inversion errors.\n\nMost existing data center network (DCN) flow scheduling solutions aim to minimize flow completion times (FCT). However, these solutions either require precise flow information (e.g., per-flow size), which is challenging to implement on commodity switches (e.g., pFabric), or no prior flow information at all, which is at the cost of performance (e.g., PIAS). In this work, we present QCLIMB, a new flow scheduling solution designed to minimize FCT by utilizing imprecise flow information. Our key observation is that although obtaining precise flow information can be challenging, it is possible to accurately estimate each flow's lower and upper bounds with machine learning techniques.\n\nQCLIMB has two key parts: i) a novel scheduling algorithm that leverages the lower bounds of different flows to prioritize small flow over large flows from the beginning of transmission, rather than at later stages; and ii) an efficient out-of-order handling mechanism that addresses practical reordering issues resulting from the algorithm. We show that QCLIMB significantly outperforms PIAS (88% lower average FCT of small flows) and is surprisingly close to pFabric (around 9% gap) while not requiring any switch modifications.\n\nCongestion control (CC) plays a pivotal role in cloud gaming services. However, existing CC methods often cause self-induced bottleneck queuing. As a result, they may largely delay game frame transmission and undermine the player's gaming experience. We present a new end-to-end CC algorithm named Pudica that strives to achieve near-zero queuing delay and high link utilization while respecting cross-flow fairness. Pudica introduces several judicious approaches to utilize the paced frame to probe the bandwidth utilization ratio (BUR) instead of bandwidth itself. By leveraging BUR estimations, Pudica designs a holistic bitrate adjustment policy to balance low queuing, efficiency, and fairness. We conducted thorough and comprehensive evaluations in real networks. In comparison to baseline methods, Pudica reduces the average and tailed frame delay by 3.1× and 4.9× respectively, and cuts down the stall rate by 10.3×. Meanwhile, it increases the frame bitrate by 12.1\\%. Pudica has been deployed in a large-scale cloud gaming platform, serving millions of players.\n\nCongestion control is a key enabler for lossless Ethernet at scale. In this paper, we revisit this classic topic from a new perspective, i.e., understanding and exploiting the intrinsic properties of the underlying lossless network. We experimentally and analytically find that the intrinsic properties of lossless networks, such as packet conservation, can indeed provide valuable implications in estimating pipe capacity and the precise number of excessive packets. Besides, we derive principles on how to treat congested flows and victim flows individually to handle HoL blocking efficiently. Then, we propose ACK-driven congestion control (ACC) for lossless Ethernet, which simply resorts to the knowledge of ACK time series to exert a temporary halt to exactly drain out excessive packets of congested flows and then match its rate to pipe capacity. Testbed and large-scale simulations demonstrate that ACC ameliorates fundamental issues in lossless Ethernet (e.g., congestion spreading, HoL blocking, and deadlock) and achieves excellent low latency and high throughput performance. For instance, compared with existing schemes, ACC improves the average and 99th percentile FCT performance of small flows by 1.3~3.3× and 1.4~11.5×, respectively.\n\nAchieving resource efficiency while preserving end-user experience is non-trivial for cloud application operators. As cloud applications progressively adopt microservices, resource managers are faced with two distinct levels of system behavior: end-to-end application latency and per-service resource usage. Translating between the two levels, however, is challenging because user requests traverse heterogeneous services that collectively (but unevenly) contribute to the end-to-end latency. We present Autothrottle, a bi-level resource management framework for microservices with latency SLOs (service-level objectives). It architecturally decouples application SLO feedback from service resource control, and bridges them through the notion of performance targets. Specifically, an application-wide learning-based controller is employed to periodically set performance targets—expressed as CPU throttle ratios—for per-service heuristic controllers to attain. We evaluate Autothrottle on three microservice applications, with workload traces from production scenarios. Results show superior CPU savings, up to 26.21% over the best-performing baseline and up to 93.84% over all baselines.\n\nServerless computing promises automatic resource provisioning to relieve the burden of developers. Yet, developers still have to manually configure resources on current serverless platforms to satisfy application-level requirements. This is because cloud applications are orchestrated as serverless workflows with multiple stages, exhibiting a complex relationship between resource configuration and application requirements.\n\nWe propose Jolteon, an orchestrator to unleash the promise of automatic resource provisioning for serverless workflows. At the core of Jolteon is a stochastic performance model that combines the benefits of whitebox modeling to capture the execution characteristics of serverless computing and blackbox modeling to accommodate the inherent performance variability. We formulate a chance constrained optimization problem based on the performance model, and exploit sampling and convexity to find optimal resource configurations that satisfy user-defined cost or latency bounds. We implement a system prototype of Jolteon and evaluate it on AWS Lambda with a variety of serverless workflows. The experimental results show that Jolteon outperforms the state-of-the-art solution, Orion, by up to 2.3× on cost and 2.1× on latency.\n\nCloud providers offer spot instances alongside on-demand instances to optimize resource utilization. While economically appealing, spot instances’ preemptible nature causes them ill-suited for deadline-sensitive jobs. To allow jobs to meet deadlines while leveraging spot instances, we propose a simple idea: use on-demand instances judiciously as a backup resource. However, due to the unpredictable spot instance availability, determining when to switch between spot and on-demand to minimize cost requires careful policy design. In this paper, we first provide an in-depth characterization of spot instances (e.g., availability, pricing, duration), and develop a basic theoretical model to examine the worst and average-case behaviors of baseline policies (e.g., greedy). The model serves as a foundation to motivate our design of a simple and effective policy, Uniform Progress, which is parameter-free and requires no assumptions on spot availability. Our empirical study, based on three-month-long real spot availability traces on AWS, demonstrates that it can (1) outperform the greedy policy by closing the gap to the optimal policy by 2× in both average and bad cases, and (2) further reduce the gap when limited future knowledge is given. These results hold in a variety of conditions ranging from loose to tight deadlines, low to high spot availability, and on single or multiple instances. By implementing this policy on top of SkyPilot, an intercloud broker system, we achieve 27%-84% cost savings across a variety of representative real-world workloads and deadlines. The spot availability traces are open-sourced for future research.\n\nAn intelligent cockpit is now crucial in automobiles, not just to provide digital instrumentation and in-vehicle controls but also to offer a wide range of entertainment functionalities. To cater to the demands of these intelligent vehicles, the automotive industry starts employing virtualization technology to offer a unified hardware and software architecture that can simplify system management and enhance resource utilization. Particularly in the domain of intelligent cockpits, virtualization can tightly integrate systems with different criticality levels (e.g., safety and real-time) on a single hardware platform, improving inter-system communication quality and the timely response to user-initiated requests. Currently, microhypervisor virtualization has been used in production to achieve intelligent automobile cockpit. However, in addition to the performance concern and high production costs, this solution is suffering from the global shortage of chips capable of running microhypervisor systems.\n\nOur key insight is that, most functions within intelligent cockpit systems are non-safety-critical and non-real-time multimedia tasks. Based on this characteristic, in this paper we present AutoVP, a new cockpit virtualization architecture. The hardware foundation of AutoVP consists of two low-cost chips: 1) a consumer-grade System-on-Chip (SoC) multi-core processor as the main chip; 2) a typical automotive-grade Microcontroller Unit (MCU) as the auxiliary chip. The MCU auxiliary chip is responsible for hosting real-time and safety-critical tasks, while the SoC main chip primarily handles multimedia tasks, such as entertainment systems and digital instrumentation. Further more, we construct an Android container virtual environment on the SoC main chip. This environment integrates multiple media functions onto a single chip, resulting in efficient utilization of chip computational resources and high system scalability. Our comparative performance evaluation demonstrates that AutoVP is a cost-effective and efficient solution to build intelligent cockpits.\n\nThis paper introduces MuCache, a framework for extending arbitrary microservice applications with inter-service caches. MuCache significantly improves the performance of microservice graphs (commonly found in large applications like Uber or Twitter) by eliminating the need for one microservice to call another when the relevant state has not changed. MuCache is enabled by a novel non-blocking cache coherence and invalidation protocol for graph topologies that minimizes critical-path overhead. For this protocol, we prove a strong correctness result: any execution observed by the cache-enabled microservice application could have been observed by the original application without caches. Our evaluation on well-known microservice benchmarks shows that MuCache reduces the median request latency by up to 2.5×, and increases throughput by up to 60%.\n\nThis paper describes the process and operational experiences of deploying the Data Center TCP (DCTCP) protocol in a large-scale data center network. In contrast to legacy congestion control protocols that rely on loss as the primary signal of congestion, DCTCP signals in-network congestion (based on queue occupancy) to senders and adjusts the sending rate proportional to the level of congestion. At the time of our deployment, this protocol was well-studied and fairly established with proven efficiency gains in other networks. As expected, we also observed improved performance, and notably decreased packet losses, compared to legacy protocols in our data centers. Perhaps unexpectedly, however, we faced numerous hurdles in rolling out DCTCP; we chronicle these unexpected challenges, ranging from its unfairness (to other classes of traffic) to implementation bugs. We close by discussing some of the open research questions and challenges.\n\nIn this paper, we present TECC, a system based on collaborative transmission control that mitigates the mismatch of sending behavior between the inner and outer connections to achieve efficient QUIC tunneling. In TECC, a feedback framework is implemented to enable end hosts to collect more precise network information that is sensed on the tunnel server, which assists the inner end-to-end connection to achieve better congestion control and loss recovery. Extensive experiments in emulated networks and real-world large-scale A/B tests demonstrate the efficiency of TECC. Specifically, compared with the state-of-the-art QUIC tunneling solution, TECC significantly reduces flow completion time. In emulated networks, TECC decreases flow completion time by 30% on average and 53% at the 99th percentile. TECC also gains a reduction in RPC (Remote Procedure Call) request completion time of 3.9% on average and 13.3% at the 99th percentile in large-scale A/B tests.\n\nNamed Data Networking (NDN) shifts the network from host-centric to data-centric with a clean-slate design, in which packet forwarding is based on names, and the data plane maintains per-packet state. Different forwarders have been implemented to provide NDN capabilities for various scenarios, however, there is a lack of a network stack that is integrated with operating systems (OS) for general purpose. Designing a stateful and entirely name-based protocol stack in OS kernel remains a challenge due to three factors: (i) an in-kernel name resolution architecture for packet demultiplexing is necessary, (ii) an entirely name-based stack requires to be compatible with the current address (MAC/IP/port)-based architecture in OS kernel, and (iii) maintaining per-packet state introduces a trade-off between performance and resource consumption.\n\nIn this paper, for the first time, we take NDN into OS kernel by proposing iStack, an Information-Centric Networking (ICN) protocol stack. The main innovations of iStack are threefold. First, we propose a name resolution architecture to support both network-layer forwarding and local packet demultiplexing. Second, a two-layer face system is proposed to provide abstraction of address-based network interfaces. Third, we design socket-compatible interfaces to keep the uniformity of current network stack in OS. Besides, we design compact forwarding data structures for fast packet processing with low memory footprint. We have implemented prototypes on multiple platforms. The evaluation results show that iStack achieves 6.50 Gbps throughput, outperforming the NDN-testbed forwarder by a factor of 16.25x, and reduces 46.08% forwarding latency for cached packets with its inkernel packet caching. iStack is not just another forwarder for NDN, but a step forward for practical development of ICN.\n\nBulk data replication across multiple cloud regions and providers is essential for large organizations to support data analytics, disaster recovery, and geo-distributed model serving. However, data multicast in the cloud can be expensive due to network egress costs and slow due to cloud network constraints. In this paper, we study the design of high-throughput, cost-optimized overlay multicast for bulk cloud data replication that exploits trends in modern provider pricing models along with techniques like ephemeral waypoints to minimize cloud networking costs.\n\nTo that end, we design an optimization algorithm that uses information about cloud network throughput and pricing to identify cost-minimizing multicast replication trees under user-given runtime budgets. Our open-source implementation, Cloudcast, is used for cloud overlay multicast that supports pluggable algorithms for determining the multicast tree structure. Our evaluations show that Cloudcast achieves 61.5% cost reduction and 2.3× replication speedup compared to both academic and commercial baselines (e.g., AWS multi-region bucket) for multi-region replication.\n\nRoutable PCIe has become the predominant cluster interconnect to build emerging composable infrastructures. Empowered by PCIe non-transparent bridge devices, PCIe transactions can traverse multiple switching domains, enabling a server to elastically integrate a number of remote PCIe devices as local ones. However, it is unclear how to move data or perform communication efficiently over the routable PCIe fabric without understanding its capabilities and limitations.\n\nThis paper presents the design and implementation of rPCIeBench, a software-hardware co-designed benchmarking framework to systematically characterize the routable PCIe fabric. rPCIeBench provides flexible data communication primitives, exposes end-to-end PCIe transaction observability, and enables reconfigurable experiment deployment. Using rPCIeBench, we first analyze the communication characteristics of a routable PCIe path, quantify its performance tax, and compare it with the local PCIe link. We then use it to dissect in-fabric traffic orchestration behaviors and draw three interesting findings: approximate max-min bandwidth partition, fast end-to-end bandwidth synchronization, and interference-free among orthogonal data paths. Finally, we encode gathered characterization insights as traffic orchestration rules and develop an edge constraints relaxing algorithm to estimate PCIe flow transmission performance over a shared fabric. We validate its accuracy and demonstrate its potential to provide an optimization guide to design efficient flow schedulers.\n\nTraditional Byzantine Fault Tolerance (BFT) state machine replication protocols assume a partial synchrony model, leading to a design where a leader replica drives the protocol and is replaced after a timeout. Recently, we witnessed a surge of asynchronous BFT protocols, which use randomization to remove the need for bounds on message delivery times, making them more resilient to adverse network conditions. However, existing research proposals still fall short of gaining practical adoption, plausibly because they are not able to combine good performance with a simple design that can be readily understood and adopted. In this paper, we present Alea-BFT, a simple and highly efficient asynchronous BFT protocol, which is gaining practical adoption, namely in Ethereum distributed validators. Alea-BFT brings the key design insight from classical protocols of concentrating part of the work on a single designated replica and incorporates this principle in a simple two-stage pipelined design, with an efficient broadcast led by the designated replica, followed by an inexpensive binary agreement. The evaluation of our research prototype implementation and two real-world integrations in cryptocurrency ecosystems shows excellent performance, improving on the fastest protocol (Dumbo-NG) in terms of latency and displaying good performance under faults.\n\nDatacenter networks today provide best-effort delivery—messages may observe unpredictable queueing, delays, and drops due to switch buffer overflows within the network. Such weak guarantees reduce the set of assumptions that system designers can rely upon from the network, thus introducing inefficiency and complexity in host hardware and software.\n\nWe present Harmony, a datacenter network architecture that provides powerful \"congestion-free\" message delivery guarantees—each message, once transmitted by the sender, observes bounded queueing at each switch in the network. Thus, network delays are bounded in failure-free scenarios, and congestion-related drops are completely eliminated. We establish, both theoretically and empirically, that Harmony provides such powerful guarantees with near-zero overheads compared to best-effort delivery networks: it incurs a tiny additive latency overhead that diminishes with message sizes, while achieving near-optimal network utilization.\n\nCloud services improve their availability by replicating data across sites in different geographical regions. A variety of state-machine replication protocols have been proposed for this setting that reduce the latency under workloads with low contention. However, when contention increases, these protocols may deliver lower performance than Paxos. This paper introduces SwiftPaxos—a protocol that lowers the best-case latency in comparison to Paxos without hurting the worst-case one. SwiftPaxos executes a command in 2 message delays if there is no contention, and in 3 message delays otherwise. To achieve this, the protocol allows replicas to vote on the order in which they receive state-machine commands. Differently from previous protocols, SwiftPaxos permits a replica to vote twice: first for its own ordering proposal, and then to follow the leader. This mechanism avoids restarting the voting process when a disagreement occurs among replicas, saving computation time and message delays. Our evaluation shows that the throughput of SwiftPaxos is up to 2.9x better than state-of-the-art alternatives.\n\nByzantine Fault-Tolerant (BFT) protocols cover a broad spectrum of design dimensions from infrastructure settings, such as the communication topology, to more technical features, such as commitment strategy and even fundamental social choice properties like order-fairness. The proliferation of different protocols has made it difficult to navigate the BFT landscape, let alone determine the protocol that best meets application needs. This paper presents Bedrock, a unified platform for BFT protocols analysis, implementation, and experimentation. Bedrock proposes a design space consisting of a set of dimensions and explores several design choices that capture the trade-offs between different design space dimensions. Within Bedrock, a wide range of BFT protocols can be implemented and uniformly evaluated under a unified deployment environment.\n\nSerializable distributed in-memory transactions are important building blocks for data center applications. To achieve high throughput and low latency, existing distributed transaction systems eschew the kernel networking stack and rely heavily on kernel-bypass networking techniques such as RDMA and DPDK. However, kernel-bypass networking techniques generally suffer from security, isolation, protection, maintainability, and debuggability issues, while the kernel networking stack supports these properties well, but performs poorly.\n\nWe present DINT, a kernel networking stack-based distributed transaction system that achieves kernel-bypass-like throughput and latency. To gain the performance back under the kernel stack, DINT offloads frequent-path transaction operations directly into the kernel via eBPF techniques without kernel modifications or customized kernel modules, avoiding most of the kernel stack overheads. DINT does not lose the good properties of the kernel stack, as eBPF is a kernel-native technique on modern OSes. On typical transaction workloads, DINT even achieves up to 2.6× higher throughput than using a DPDK-based kernel-bypass stack, while only adding at most 10%/16% average/99th-tail unloaded latency.\n\nThe emerging programmable networks sparked significant research on Intelligent Network Data Plane (INDP), which achieves learning-based traffic analysis at line-speed. Prior art in INDP focus on deploying tree/forest models on the data plane. We observe a fundamental limitation in tree-based INDP approaches: although it is possible to represent even larger tree/forest tables on the data plane, the flow features that are computable on the data plane are fundamentally limited by hardware constraints. In this paper, we present BoS to push the boundaries of INDP by enabling Neural Network (NN) driven traffic analysis at line-speed. Many types of NNs (such as Recurrent Neural Network (RNN), and transformers) that are designed to work with sequential data have advantages over tree-based models, because they can take raw network data as input without complex feature computations on the fly. However, the challenge is significant: the recurrent computation scheme used in RNN inference is fundamentally different from the match-action paradigm used on the network data plane. BoS addresses this challenge by (i) designing a novel data plane friendly RNN architecture that can execute unlimited RNN time steps with limited data plane stages, effectively achieving line-speed RNN inference; and (ii) complementing the on-switch RNN model with an off-switch transformer-based traffic analysis module to further boost the overall performance. We implement a prototype of BoS using a P4 programmable switch as our data plane, and extensively evaluate it over multiple traffic analysis tasks. The results show that BoS outperforms state-of-the-art in both analysis accuracy and scalability.\n\nWeb centralization and consolidation has created potential single points of failure, e.g., in areas such as content hosting, name resolution, and certification. The \"Decentralized Web\", led by open-source software implementations, attempts to build decentralized alternatives. The InterPlanetary File System (IPFS) is part of this effort and attempts to provide a decentralized layer for object storage and retrieval. This comes with challenges, though: Decentralization can increase complexity, overhead, as well as compromise performance and scalability. As the core maintainers of IPFS, we have therefore begun to explore more hybrid approaches. This paper reports on our experiences building three centralized components within IPFS: (i) InterPlanetary Network Indexers, which provides an alternative centralized method for content indexing; (ii) Hydra Boosters, which are strategic DHT nodes that assist IPFS in content routing; and (iii) HTTP Gateways, which are a public access point for users to retrieve IPFShosted content. Through this approach, we trade-off the level of decentralization within IPFS in an attempt to gain certain benefits of centralization. We evaluate the performance of these components and demonstrate their ability to successfully address the challenges that IPFS faces.\n\nThe need for fairness, strong isolation, and fine-grained control over network traffic in multi-tenant cloud settings has engendered a rich literature on packet scheduling in switches and programmable hardware. Recent proposals for hardware scheduling primitives (e.g., PIFO, PIEO, BMW-Tree) have enabled run-time programmable packet schedulers, considerably expanding the suite of scheduling policies that can be applied to network traffic. However, no existing solution can be practically deployed on modern switches and NICs because they either do not scale to the number of elements required by these devices or fail to deliver good throughput, thus requiring an impractical number of replicas.\n\nIn this work, we ask: is it possible to achieve priority packet scheduling at line-rate while supporting a large number of flows? Our key insight is to leverage a scheduling primitive used previously in software – called Hierarchical Find First Set – and port this to a highly pipeline-parallel hardware design. We present the architecture and implementation of the Bitmapped Bucket Queue (BBQ), a hardware-based integer priority queue that supports a wide range of scheduling policies (via a PIFO-like abstraction). BBQ, for the first time, supports hundreds of thousands of concurrent flows while guaranteeing 100 Gbps line rate (148.8 Mpps) on FPGAs and 1 Tbps (1,488 Mpps) line rate on ASICs. We demonstrate this by implementing BBQ on a commodity FPGA where it is capable of supporting over 100K flows and 32K priorities at 300 MHz, 3× the packet rate of similar hardware priority queue designs. On ASIC, we can synthesize 100K elements at 3.1 GHz using a 7nm process.\n\nAlibaba Cloud designs and deploys P4-capable gateway to accelerate the processing of the diverse business traffics in the edge cloud. Since the programmable ASIC in the gateway only accepts a monolithic, pipelined P4 program, the dozens network function chains for different business traffics have to be composed into one. This is non-trivial due to the contention between the complexity of network function chains and the limited resource in the programmable ASIC. In this paper, we present Sirius, a system that automates network function chain composition process. Sirius synthesizes tables to identify which business traffic the input packet belongs to, pipelines loops in the merged network function graph via recirculations, and partitions the graph between programmable ASIC and CPU when the required memory consumption exceeds the ASIC’s capability. So far, Sirius has automated network function arrangement in hundreds of gateways, and has effectively decreased our programmers’ workload by three orders of magnitude, from weeks to minutes.\n\nProgrammable pipeline offers flexible and high-throughput packet processing capability, but only to some extent. When more advanced dataplane functions beyond basic packet processing and forwarding are desired, the pipeline becomes handicapped. The fundamental reason is that most stateful operations require backward cross-stage data passing and pipeline stalling for state update and consistency, which are anomalous to a standard pipeline. To solve the problem, we augment the pipeline with a low-cost, yet fast side ring to facilitate the backward data passing. We further apply the speculative execution technique to avoid pipeline stalling. The resulting architecture, RAPID, supports native and generic stateful function programming using the enhanced P4 language. We build an FPGA-based prototype to evaluate the system, and a software emulator to assess the cost and performance of an ASIC implementation. We realize several stateful applications enabled by RAPID to show how it extends a programmable dataplane's potential to a new level.\n\nIn real-time video communication, retransmitting lost packets over high-latency networks is not viable due to strict latency requirements. To counter packet losses without retransmission, two primary strategies are employed—encoder-based forward error correction (FEC) and decoder-based error concealment. The former encodes data with redundancy before transmission, yet determining the optimal redundancy level in advance proves challenging. The latter reconstructs video from partially received frames, but dividing a frame into independently coded partitions inherently compromises compression efficiency, and the lost information cannot be effectively recovered by the decoder without adapting the encoder.\n\nWe present a loss-resilient real-time video system called GRACE, which preserves the user’s quality of experience (QoE) across a wide range of packet losses through a new neural video codec. Central to GRACE’s enhanced loss resilience is its joint training of the neural encoder and decoder under a spectrum of simulated packet losses. In lossless scenarios, GRACE achieves video quality on par with conventional codecs (e.g., H.265). As the loss rate escalates, GRACE exhibits a more graceful, less pronounced decline in quality, consistently outperforming other loss-resilient schemes. Through extensive evaluation on various videos and real network traces, we demonstrate that GRACE reduces undecodable frames by 95% and stall duration by 90% compared with FEC, while markedly boosting video quality over error concealment methods. In a user study with 240 crowdsourced participants and 960 subjective ratings, GRACE registers a 38% higher mean opinion score (MOS) than other baselines.\n\nVideo codecs are essential for video streaming. While traditional codecs like AVC and HEVC are successful, learned codecs built on deep neural networks (DNNs) are gaining popularity due to their superior coding efficiency and quality of experience (QoE) in video streaming. However, using learned codecs built with sophisticated DNNs in video streaming leads to slow decoding and low frame rate, thereby degrading the QoE. The fundamental problem is the tight frame referencing design adopted by most codecs, which delays the processing of the current frame until its immediate predecessor frame is reconstructed. To overcome this limitation, we propose LiFteR, a novel video streaming system that operates a learned video codec with loose frame referencing (LFR). LFR is a unique frame referencing paradigm that redefines the reference relation between frames and allows parallelism in the learned video codec to boost the frame rate. LiFteR has three key designs: (i) the LFR video dispatcher that routes video data to the codec based on LFR, (ii) LFR learned codec that enhances coding efficiency in LFR with minimal impact on decoding speed, and (iii) streaming supports that enables adaptive bitrate streaming with learned codecs in existing infrastructures. In our evaluation, LiFteR consistently outperforms existing video streaming systems. Compared to the existing best-performing learned and traditional systems, LiFteR demonstrates up to 23.8% and 19.7% QoE gain, respectively. Furthermore, LiFteR achieves up to a 3.2× frame rate improvement through frame rate configuration.\n\nCamera orientations (i.e., rotation and zoom) govern the content that a camera captures in a given scene, which in turn heavily influences the accuracy of live video analytics pipelines. However, existing analytics approaches leave this crucial adaptation knob untouched, instead opting to only alter the way that captured images from fixed orientations are encoded, streamed, and analyzed. We present MadEye, a camera-server system that automatically and continually adapts orientations to maximize accuracy for the workload and resource constraints at hand. To realize this using commodity pan-tilt-zoom (PTZ) cameras, MadEye embeds (1) a search algorithm that rapidly explores the massive space of orientations to identify a fruitful subset at each time, and (2) a novel knowledge distillation strategy to efficiently (with only camera resources) select the ones that maximize workload accuracy. Experiments on diverse workloads show that MadEye boosts accuracy by 2.9-25.7% for the same resource usage, or achieves the same accuracy with 2-3.7× lower resource costs.\n\nVideo conferencing systems suffer from poor user experience when network conditions deteriorate because current video codecs simply cannot operate at extremely low bitrates. Recently, several neural alternatives have been proposed that reconstruct talking head videos at very low bitrates using sparse representations of each frame such as facial landmark information. However, these approaches produce poor reconstructions in scenarios with major movement or occlusions over the course of a call, and do not scale to higher resolutions. We design Gemino, a new neural compression system for video conferencing based on a novel high-frequency-conditional super-resolution pipeline. Gemino upsamples a very low-resolution version of each target frame while enhancing high-frequency details (e.g., skin texture, hair, etc.) based on information extracted from a single high-resolution reference image. We use a multi-scale architecture that runs different components of the model at different resolutions, allowing it to scale to resolutions comparable to 720p, and we personalize the model to learn specific details of each person, achieving much better fidelity at low bitrates. We implement Gemino atop aiortc, an open-source Python implementation of WebRTC, and show that it operates on 1024x1024 videos in real-time on a Titan X GPU, and achieves 2.2–5x lower bitrate than traditional video codecs for the same perceptual quality.\n\nLive streaming of segmented videos over the Hypertext Transfer Protocol (HTTP) is increasingly popular and serves heterogeneous clients by offering each segment in multiple representations. A bitrate ladder expresses this choice as an ordered list of bitrate-resolution pairs. Whereas existing solutions for HTTP-based live streaming use a static bitrate ladder, the fixed ladders struggle to appropriately accommodate the dynamics in the video content and network-conditioned client capabilities. This paper proposes ARTEMIS as a practical scalable alternative that dynamically configures the bitrate ladder depending on the content complexity, network conditions, and clients' statistics. ARTEMIS seamlessly integrates with the end-to-end streaming pipeline and operates transparently to video encoders and clients. We develop a cloud-based implementation of ARTEMIS and conduct extensive real-world and trace-driven experiments. The experimental comparison vs. existing prominent bitrate ladders demonstrates that live streaming with ARTEMIS outperforms all baselines, reduces encoding computation by 25%, end-to-end latency by 18%, and increases quality of experience by 11%.\n\nPacket buffers in datacenter switches are shared across all the switch ports in order to improve the overall throughput. The trend of shrinking buffer sizes in datacenter switches makes buffer sharing extremely challenging and a critical performance issue. Literature suggests that push-out buffer sharing algorithms have significantly better performance guarantees compared to drop-tail algorithms. Unfortunately, switches are unable to benefit from these algorithms due to lack of support for push-out operations in hardware. Our key observation is that drop-tail buffers can emulate push-out buffers if the future packet arrivals are known ahead of time. This suggests that augmenting drop-tail algorithms with predictions about the future arrivals has the potential to significantly improve performance.\n\nThis paper is the first research attempt in this direction. We propose CREDENCE, a drop-tail buffer sharing algorithm augmented with machine-learned predictions. CREDENCE can unlock the performance only attainable by push-out algorithms so far. Its performance hinges on the accuracy of predictions. Specifically, CREDENCE achieves near-optimal performance of the best known push-out algorithm LQD (Longest Queue Drop) with perfect predictions, but gracefully degrades to the performance of the simplest drop-tail algorithm Complete Sharing when the prediction error gets arbitrarily worse. Our evaluations show that CREDENCE improves throughput by 1.5x compared to traditional approaches. In terms of flow completion times, we show that CREDENCE improves upon the state-of-the-art approaches by up to 95% using off-the-shelf machine learning techniques that are also practical in today’s hardware. We believe this work opens several interesting future work opportunities both in systems and theory that we discuss at the end of this paper.\n\nState-intensive network and distributed applications rely heavily on online caching heuristics for high performance. However, there remains a fundamental performance gap between online caching heuristics and the optimal offline caching algorithm due to the lack of visibility into future state access requests in an online setting. Driven by the observation that state access requests in network and distributed applications are often carried in incoming network packets, we present Seer, an online caching solution for networked systems, that exploits the delays experienced by a packet inside a network—most prominently, transmission and queuing delays—to notify in advance of future packet arrivals to the target network nodes (switches/routers/middleboxes/end-hosts) implementing caching. Using this as a building block, Seer presents the design of an online cache manager that leverages visibility into (partial) set of future state access requests to make smarter prefetching and cache eviction decisions. Our evaluations show that Seer achieves up to 65% lower cache miss ratio and up to 78% lower flow completion time compared to LRU for key network applications over realistic workloads.\n\nThe switch buffers in datacenters today are shared by traffic classes with different loss tolerance and reaction to congestion signals. In particular, while legacy applications use loss-tolerant transport, e.g., DCTCP, newer applications require lossless datacenter transport, e.g., RDMA over Converged Ethernet. The allocation of buffers for this diverse traffic mix is managed by a buffer-sharing scheme. Unfortunately, as we analytically show in this paper, the buffer-sharing practices of today's datacenters pose a fundamental limitation to effectively isolate RDMA and TCP while also maximizing burst absorption. We identify two root causes: (i) the buffer-sharing for RDMA and TCP relies on two independent and often conflicting views of the buffer, namely ingress and egress; and (ii) the buffer-sharing scheme micromanages the buffer and overreacts to the changes in its occupancy during transient congestion.\n\nIn this paper, we present Reverie, a buffer-sharing scheme, which, unlike prior works, is suitable for both lossless and loss-tolerant traffic classes, providing isolation as well as superior burst absorption. At the core of Reverie lies a unified (consolidated ingress and egress) admission control that jointly optimizes the buffers for both traffic classes. Reverie, allocates buffer based on a low-pass filter that naturally absorbs bursty queue lengths during transient congestion within the buffer limits. Our evaluation shows that Reverie can improve the performance of RDMA as well as TCP in terms of flow completion times by up to 33%.\n\nData center traffic engineering (TE) routes flows over a set of available paths following custom weight distributions to achieve optimal load balancing or flow throughput. However, as a result of hardware constraints, it is challenging, and often impossible for larger data center networks, to precisely implement the TE weight distributions on the data plane switches. The resulting precision loss in the TE implementation causes load imbalances that can result in congestion and traffic loss.\n\nInstead of treating all flows equally, we adapt the hardware resource allocation to a flow’s traffic volume and its contribution to the overall precision loss. We intelligently prune select ports in weight distributions and merge identical distributions to free up hardware resources. Evaluation using realistic traffic loads shows that our techniques approximate ideal TE solutions under various scenarios within 7% error, compared to a 67% error for today’s state-of-the-art approach. In addition, our design avoids traffic loss triggered by switch rule overflow. Finally, the execution time is 10× faster than the current approach.\n\nWe propose a practical approach to implementing multitenancy on programmable network switches to make in-network acceleration accessible to cloud users. We introduce a Switch Virtual Machine (SwitchVM), that is deployed on the switches and offers an expressive instruction set and program state abstractions. Tenant programs, called Data-Plane filters (DPFs), are executed on top of SwitchVM in a sandbox with memory, network and state isolation policies controlled by network operators. The packets that trigger DPF execution include the code to execute or a reference to the DPFs deployed in the switch. DPFs are Turing-complete, may maintain state in the packet and in switch virtual memory, may form a dynamic chain, and may steer packets to desired destinations, all while enforcing the operator’s policies.\n\nWe demonstrate that this idea is practical by prototyping SwitchVM in P4 on Intel Tofino switches. We describe a variety of use cases that SwitchVM supports, and implement three complex applications from prior works – key-value store cache, Load-aware load balancer and Paxos accelerator. We also show that SwitchVM provides strong performance isolation, zero-overhead runtime programmability, may hold two orders of magnitude more in-switch programs than existing techniques, and may support up to thirty thousand concurrent tenants each with its private state.\n\nLarge Language Models (LLMs) have presented impressive performance across several transformative tasks. However, it is non-trivial to efficiently utilize large-scale cluster resources to develop LLMs, often riddled with numerous challenges such as frequent hardware failures, intricate parallelization strategies, and imbalanced resource utilization. In this paper, we present an in-depth characterization study of a six-month LLM development workload trace collected from our GPU datacenter Acme. Specifically, we investigate discrepancies between LLMs and prior task-specific Deep Learning (DL) workloads, explore resource utilization patterns, and identify the impact of various job failures. Our analysis summarizes hurdles we encountered and uncovers potential opportunities to optimize systems tailored for LLMs. Furthermore, we introduce our system efforts: (1) fault-tolerant pretraining, which enhances fault tolerance through LLM-involved failure diagnosis and automatic recovery. (2) decoupled scheduling for evaluation, which achieves timely performance feedback via trial decomposition and scheduling optimization.\n\nDeep learning recommendation models play an important role in online companies and consume a major part of the AI infrastructure dedicated to training and inference. The accuracy of these models highly depends on how quickly they are published on the serving side. One of the main challenges in improving the model update latency and frequency is the model size, which has reached the order of Terabytes and is expected to further increase in the future. The large model size causes large latency (and write bandwidth) to update the model in geo-distributed servers. We present QuickUpdate, a system for real-time personalization of large-scale recommendation models, that publishes the model in high frequency as part of online training, providing serving accuracy that is comparable to that of a fully fresh model. The system employs novel techniques to minimize the required write bandwidth, including prioritized parameter updates, intermittent full model updates, model transformations, and relaxed consistency. We evaluate QuickUpdate using real-world data, on one of the largest production models in Meta. The results show that QuickUpdate provides serving accuracy that is comparable to a fully fresh model, while reducing the average published update size and the required bandwidth by over 13x. It provides a scalable solution for serving production models in real-time fashion, which is otherwise not feasible at scale due to the limited network and storage bandwidth.\n\nWe present the design, implementation and engineering experience in building and deploying MegaScale, a production system for training large language models (LLMs) at the scale of more than 10,000 GPUs. Training LLMs at this scale brings unprecedented challenges to training efficiency and stability. We take a full-stack approach that co-designs the algorithmic and system components across model block and optimizer design, computation and communication overlapping, operator optimization, data pipeline, and network performance tuning. Maintaining high efficiency throughout the training process (i.e., stability) is an important consideration in production given the long extent of LLM training jobs. Many hard stability issues only emerge at large scale, and in-depth observability is the key to address them. We develop a set of diagnosis tools to monitor system components and events deep in the stack, identify root causes, and derive effective techniques to achieve fault tolerance and mitigate stragglers. MegaScale achieves 55.2% Model FLOPs Utilization (MFU) when training a 175B LLM model on 12,288 GPUs, improving the MFU by 1.34x compared to Megatron-LM. We share our operational experience in identifying and fixing failures and stragglers. We hope by articulating the problems and sharing our experience from a systems perspective, this work can inspire future LLM systems research.\n\nTPUv4 (Tensor Processing Unit) is Google’s 3rd generation accelerator for machine learning training, deployed as a 4096-node supercomputer with a custom 3D torus interconnect. In this paper, we describe our experience designing and operating the software infrastructure that allows TPUv4 supercomputers to operate at scale, including features for automatic fault resiliency and hardware recovery. We adopt a software-defined networking (SDN) approach to manage TPUv4’s high-bandwidth inter-chip interconnect (ICI) fabric, using optical circuit switching to dynamically configure routes to work around machine, chip and link failures. Our infrastructure detects failures and automatically triggers reconfiguration to minimize disruption to running workloads, as well as initiating remediation and repair workflows for the affected components. Similar techniques interface with maintenance and upgrade workflows for both hardware and software. Our dynamic reconfiguration approach allows our TPUv4 supercomputers to achieve 99.98% system availability, gracefully handling hardware outages experienced by ~1% of the training jobs.\n\nA physical-layer modulator is a vital component for an IoT gateway to map the symbols to signals. However, due to the soldered hardware chipsets on the gateway's motherboards or the diverse toolkits on different platforms for the software radio, the existing solutions either have limited extensibility or are platform specific. Such limitation is hard to ignore when modulation schemes and hardware platforms have become extremely diverse. This paper presents a new paradigm of using neural networks as an abstraction layer for physical layer modulators in IoT gateway devices, referred to as NN-defined modulators. Our approach addresses the challenges of extensibility and portability for multiple technologies on various hardware platforms. The proposed NN-defined modulator uses a model-driven methodology rooted in solid mathematical foundations while having native support for hardware acceleration and portability to heterogeneous platforms. We conduct the evaluation of NN-defined modulators on different platforms, including Nvidia Jetson Nano, Raspberry Pi. Evaluations demonstrate that our NN-defined modulator effectively operates as conventional modulators and provides significant efficiency gains (up to 4.7× on Nvidia Jetson Nano and 1.1× on Raspberry Pi), indicating high portability. Furthermore, we show the real-world applications using our NN-defined modulators to generate ZigBee and WiFi packets, which are compliant with commodity TI CC2650 (ZigBee) and Intel AX201 (WiFi NIC) respectively.\n\nMulti-tenant Low Earth Orbit (LEO) satellites emerge as a cost-effective win-win solution for direct 4G/5G access to our regular phones/IoTs anywhere on Earth. However, the current hop-by-hop stateful cellular session impedes this effort due to its need for tight functional coupling and stable service relationships among satellite operators, mobile operators, and users. Our empirical study with real satellite data shows that, it restricts LEO satellites' serviceable areas, limits the use of available (possibly competitive) satellites, and suffers from signaling storms and dynamic many-to-many relationships in extreme LEO mobility. We thus devise MOSAIC to strive for self-serve multi-tenant LEO satellites. MOSAIC defines policy-embedded one-time tokens for pay-as-you-go local satellite access. These tokens allow satellites to self-serve users anywhere without relying on remote mobile operators, alleviate inter-satellite coordinations to enjoy competitive satellites, and simplify many-to-many service relationships for on-demand multi-tenancy. MOSAIC is attack-resilient and incrementally deployable using our SIM-based solution. Our evaluations with the real satellite data and commodity 3GPP NTN protocol stack validate MOSAIC's viability.\n\nEarth observation satellites, in low Earth orbits, are increasingly approaching near-continuous imaging of the Earth. Today, these satellites capture an image of every part of Earth every few hours. However, the networking capabilities haven’t caught up, and can introduce delays of few hours to days in getting these images to Earth. While this delay is acceptable for delay-tolerant applications like land cover maps, crop type identification, etc., it is unacceptable for latency-sensitive applications like forest fire detection or disaster monitoring. We design Serval to enable near-realtime insights from Earth imagery for latency-sensitive applications despite the networking bottlenecks by leveraging the emerging computational capabilities on the satellites and ground stations. The key challenge for our work stems from the limited computational capabilities and power resources available on a satellite. We solve this challenge by leveraging predictability in satellite orbits to bifurcate computation across satellites and ground stations. We evaluate Serval using trace-driven simulations and hardware emulations on a dataset comprising ten million images captured using the Planet Dove constellation comprising nearly 200 satellites. Serval reduces end-to-end latency for high priority queries from 71.71 hours (incurred by state of the art) to 2 minutes, and 90-th percentile from 149 hours to 47 minutes.\n\nLow Earth Orbit satellite constellations are gaining traction for providing connectivity to low-power outdoor Internet of Things (IoT) devices. This is made possible by the development of low-cost, low-complexity pico-satellites that can be easily launched, offering global connectivity without the need for Earth-based gateways. In this paper, we report the space-to-Earth communication bottlenecks derived from our experience of deploying an IoT satellite. Specifically, we characterize the challenges posed by the low link budgets, satellite motion, and packet collisions. To address these challenges, we design a new class of technique that use the Doppler shift caused by the satellite's motion as a unique signature for packet detection and decoding, even at low signal-to-noise ratios and in the presence of collisions. We integrate these techniques into our system, called Spectrumize, and evaluate its performance through both simulations and real-world deployments. Our evaluation shows that Spectrumize performs 3x better compared to classic approach in detecting packet with over 80% average accuracy in decoding.\n\nThis paper presents Zipper, a novel Radio Access Network (RAN) slicing system that provides assurances of application-level throughput and latency. Existing RAN slicing systems optimize for slice-level assurance, but these methods fail to provide predictable network performance to individual mobile apps. Extending the slice-level formulation to app-level introduces an intractable optimization problem with exploding state and action spaces. To simplify the search space, Zipper casts the problem as a model predictive controller, and explicitly tracks the network dynamics of each user. It uses an efficient algorithm to compute slice bandwidth allocations that meet each app's requirements. To assist operators with interfacing admission control policies, Zipper exposes a primitive that estimates if there is bandwidth available to accommodate an incoming app's requirements.\n\nWe implemented Zipper on a production-class 5G virtual RAN testbed integrated with hooks to control slice bandwidth, and we evaluated it on real workloads, including video conferencing and virtual reality apps. On a representative RAN workload, our real-time implementation supports up to 200 apps and over 70 slices on a 100 MHz channel. Relative to a slice-level service assurance system, Zipper reduces tail throughput and latency violations, measured as a ratio of violation of the app's request, by 9×.\n\nNetwork slicing reserves a portion of the physical resources of radio access networks and makes them available to consumers. Slices guarantee traffic isolation, strict bandwidth and quality of service. However, the abstraction of slicing has been limited to access networks. We develop CHISEL, a system that dynamically carves slices of the wide-area network (WAN), enabling an end-to-end network slicing abstraction. CHISEL creates optical slices between WAN endpoints to avoid queueing and congestion delays inherent in packet switched paths in WANs. CHISEL incrementally allocates optical spectrum on long-haul fiber to provision slices. This task is made challenging by the co-existence of data-carrying channels on the fiber and numerous physical constraints associated with provisioning optical paths e.g., spectrum contiguity, continuity and optical reach constraints. CHISEL leverages the empirical finding that cloud WANs have abundant optical spectrum to spare — 75% of optical spectrum on 75% of fiber spans is unused. CHISEL can optimally allocate terabits of slice requests while consuming minimal optical spectrum within seconds without increasing spectral fragmentation on fiber. CHISEL trades-off optimality of slice bandwidth allocation for faster run-time, provisioning slices within 2% of optimal in less than 30 seconds in a commercial cloud WAN. Finally, CHISEL reduces the latency of provisioning optical slices on hardware by 10X. Compared to IP tunnels of equivalent capacity, CHISEL consumes 3.3X fewer router ports.\n\nEdge clouds are expected to be a key revenue growth driver for cloud vendors in the next decade; however, simply replicating the network infrastructure for the public cloud to the edge experiences deployment issues. At the edge, the challenge for cloud network design is to deliver the required performance under the stringent restrictions of hardware budget and deployment footprints, while retaining functionality equivalence. To this end, we propose LuoShen, a hyper-converged gateway for multi-tenant multi-service edge clouds by consolidating the entire cloud network infrastructure into a 2U server switch with a P4-centric architecture. At the data plane, LuoShen conducts pipeline folding and fits the original overlay and underlay devices into the switch pipeline via meticulous on-chip resource budgeting. At the control plane, LuoShen relies on BGP peering to ensure inter-component reachability. LuoShen achieves 1.2Tbps throughput and reduces the upfront cost, deployment size and power usage by 75%, 87%, 60%, compared with the original cloud network architecture. It has been deployed in Alibaba Cloud at hundreds of edge sites.\n\nCrawling the web at scale forms the basis of many important systems: web search engines, smart assistants, generative AI, web archives, and so on. Yet, the research community has paid little attention to this workload in the last decade. In this paper, we highlight the need to revisit the notion that web crawling is a solved problem. Specifically, to discover and fetch all page resources dependent on JavaScript and modern web APIs, crawlers today have to employ compute-intensive web browsers. This significantly inflates the scale of the infrastructure necessary to crawl pages at high throughput.\n\nTo make web crawling more efficient without any loss of fidelity, we present Sprinter, which combines browser-based and browserless crawling to get the best of both. The key to Sprinter’s design is our observation that crawling workloads typically include many pages from every site that is crawled and, unlike in traditional user-facing page loads, there is significant potential to reuse client-side computations across pages. Taking advantage of this property, Sprinter crawls a small, carefully chosen, subset of pages on each site using a browser, and then efficiently identifies and exploits opportunities to reuse the browser’s computations on other pages. Sprinter was able to crawl a corpus of 50,000 pages 5x faster than browser-based crawling, while still closely matching a browser in the set of resources fetched.\n\nInteractive streaming requires minimizing stuttering events (or deadline misses for video frames) to ensure seamless interaction between users and applications. However, existing packet loss recovery mechanisms uniformly optimize redundancy for initial transmission and retransmission, which still could not satisfy the delay requirements of interactive streaming, but also introduces considerable bandwidth costs. Our insight is that in edge-based interactive streaming, differentiating retransmissions on redundancy settings can often achieve a low bandwidth cost and a low deadline miss rate simultaneously. In this paper, we propose Hairpin, a new packet loss recovery mechanism for edge-based interactive streaming. Hairpin finds the optimal combination of data packets, retransmissions, and redundant packets over multiple rounds of transmissions, which significantly reduces the bandwidth cost while ensuring the end-to-end latency requirement. Experiments with production deployments demonstrate that Hairpin can simultaneously reduce the bandwidth cost by 40% and the deadline miss rate by 32% on average in the wild against state-of-the-art solutions.\n\nProduction systems use heuristics because they are faster or scale better than their optimal counterparts. Yet, practitioners are often unaware of the performance gap between a heuristic and the optimum or between two heuristics in realistic scenarios. MetaOpt is a system that helps analyze these heuristics. Users specify the heuristic and the optimal (or another heuristic) as input, and MetaOpt encodes these efficiently for a solver to find performance gaps and their corresponding adversarial inputs. Its suite of built-in optimizations helps it scale to practical problem sizes. We used MetaOpt to analyze heuristics from three domains (traffic engineering, vector bin packing, and packet scheduling). We found a production traffic engineering heuristic can require 30\\% more capacity than the optimal in realistic cases. We modified the heuristic based on the patterns in the adversarial inputs MetaOpt discovered and reduced the performance gap by 12.5×. We examined adversarial inputs to a vector bin packing heuristic and proved a new lower bound on its performance.\n\nWe seek to ease the design of congestion control algorithms (CCAs) that provably perform well under diverse network scenarios including, cellular links, policers, token bucket filters, operating system jitter, etc. Guaranteeing performance under such conditions is hard as it requires considering combinatorial possibilities of CCA and network interactions. We build a framework that allows us to reason about CCAs. It describes (1) the necessary actions that any performant CCA must take, and (2) a provably sufficient amount of information for CCAs to consider when deciding their sending rate. Combining this framework with techniques in formal methods, we synthesize CCAs that provably perform across a diverse set of network conditions. Our methodology also led us to discover and prove fundamental impossibility results.\n\nData plane verification is designed to automatically verify network correctness by directly analyzing the data plane. Recent data plane verifiers have achieved sub-millisecond verification for per rule update by partitioning packets into equivalence classes (ECs). A large number of data plane updates can be generated in a short interval, known as update storms, due to network events such as end-to-end establishments, disruption or recovery. When it comes to update storms, however, the verification speed of current EC-based methods is often slowed down by the maintenance of their EC-based network model (EC-model).\n\nThis paper presents EPVerifier, a fast, partitioned data plane verification for update storms to further accelerate update storms verification. EPVerifier uses a novel edge-predicate-based (EP-based) local modeling approach to avoid drastic oscillations of the EC-model caused by changes in the set of equivalence classes. In addition, with local EPs, EPVerifier can achieve a partition of verification tasks by switches that EC-based methods cannot to get better parallel performance. We implement EPVerifier as an easy-to-use tool, allowing users to quickly get the appropriate verification results at any moment by providing necessary input. Both dataset trace-driven simulations and deployments in the wild show that EPVerifier achieves robustly fast update storm verification and superior parallel performance and these advantages expand with the data plane's complexity and storm size growth. The verification time of EPVerifier for an update storm of size 1M is around 10s on average, a 2-10× improvement over the state-of-the-art.\n\nNetwork operators have long struggled to achieve reliability. Increased complexity risks surprising interactions, increased downtime, and lost person-hours trying to debug correctness and performance problems in large systems. For these reasons, network operators have also long pushed back on deploying promising network research, fearing the unexpected consequences of increased network complexity. Despite the changes’ potential benefits, the corresponding increase in complexity may result in a net loss.\n\nThe method to build reliability despite complexity in Software Engineering is testing. In this paper, we use statistics from a large-scale network to identify unique challenges in network testing. To tackle the challenges, we develop Netcastle: a system that provides continuous integration/continuous deployment (CI/CD) network testing as a service for 11 different networking teams, across 68 different use-cases, and O(1k) of test devices. Netcastle supports comprehensive network testing, including device-level firmware, datacenter distributed control planes, and backbone centralized controllers, and runs 500K+ network tests per day, a scale and depth of test coverage previously unpublished. We share five years of experiences in building and running Netcastle at Meta.\n\nComplex network protocols like the Border Gateway Protocol (BGP) are prone to implementation errors that cause unintended behaviors with potentially global consequences. We introduce an approach and tool called MESSI (Modular Exploration of State and Structure Inclusively) to automatically generate tests for black-box BGP implementations. Our approach is model-based, leveraging an executable model of BGP to generate behavioral tests. However, doing so effectively requires addressing new challenges such as the stateful nature of BGP and the need to generate complex structures like regular expressions in route maps. We used MESSI to generate roughly 150K tests that capture different aspects of BGP, such as route-map filtering, the decision process, route aggregation, and dynamics. These tests identified 22 correctness bugs across several widely used open-source BGP implementations (FRR, Quagga, GoBGP, BIRD, Batfish) and one closed-source implementation. Eight of these errors have already been fixed. While our models are BGP-specific our approach is not: thus we expect it can be adapted to test other stateful protocols with complex structures.\n\nNovel low-diameter network topologies such as Slim Fly (SF) offer significant cost and power advantages over the established Fat Tree, Clos, or Dragonfly. To spearhead the adoption of low-diameter networks, we design, implement, deploy, and evaluate the first real-world SF installation. We focus on deployment, management, and operational aspects of our test cluster with 200 servers and carefully analyze performance. We demonstrate techniques for simple cabling and cabling validation as well as a novel high-performance routing architecture for InfiniBand-based low-diameter topologies. Our real-world benchmarks show SF's strong performance for many modern workloads such as deep neural network training, graph analytics, or linear algebra kernels. SF outperforms non-blocking Fat Trees in scalability while offering comparable or better performance and lower cost for large network sizes. Our work can facilitate deploying SF while the associated (open-source) routing architecture is fully portable and applicable to accelerate any low-diameter interconnect.\n\nThis paper presents the design, implementation, evaluation, and deployment of Crescent, ByteDance’s network emulation platform, for preventing change-induced network incidents. Inspired by prior art such as CrystalNet, Crescent achieves high fidelity by running switch vendor images inside containers. But, we explore a different route to scaling up the emulator with unique challenges. First, we analyze our past network incidents to reveal the difficulty in identifying a safe emulation boundary. Instead of emulating the entire network, we exploit the inherent symmetry and modularity of data center network architectures to strike a balance between coverage and resource cost. Second, we study the node-to-host assignment by formulating it as a graph partitioning problem. Evaluation results show that our partitioning algorithm reduces the testbed bootup time by up to 20× compared with random partitioning. Third, we developed an incremental approach to modify the emulated network on the fly. This approach can be 30× faster than creating a new testbed of the same scale. Crescent has been actively used for three and a half years, which led to a significant reduction in change-induced network incidents. We also share Crescent’s success in many other use cases and the critical lessons learned from its deployment.\n\nThis paper presents Jingubang, the first reported system for checking network traffic load properties (e.g., if any link’s utilization would exceed 80% during a network change) in a production Wide Area Network (WAN). Motivated by our network operators, Jingubang should meet three important requirements: (R1) comprehensive support for complex traffic behavior under BGP, IS-IS, policy-based routes (PBR), and segment routes (SR), (R2) reasoning on traffic load of billions of flows across a period of time, (R3) real-time failure-tolerance analysis. These requirements pose challenges in modeling the complex traffic behavior and maintaining the checking efficiency. Jingubang has successfully addressed these challenges. First, we propose the traffic distribution graph (or TDG), capable of modeling equal-cost multi-path (ECMP), packet rewriting, and tunneling, introduced by BGP/IS-IS, PBR, and SR, respectively. Second, we design an algorithm based on TDG to simulate traffic distribution for billions of flows across a time period both efficiently and accurately. Third, Jingubang proposes an incremental traffic simulation approach that first computes an incremental TDG and then simulates only the differential traffic distribution, avoiding the need to simulate the entire network traffic distribution from scratch. Jingubang has been used in the daily checking of our WAN for more than one year and prevented service downtime resulting from traffic load violations.\n\nAs the cloud rapidly expands in scale, the virtual network controller must manage an increasing number of devices with higher update frequencies. Furthermore, the emergence of cloud-native services has substantially intensified program-triggered updates, leading to more frequent API invocations. To enhance performance and extensibility, we propose Poseidon, a novel virtual network control framework. Specifically, to reduce operational expenses (OpEx), we have consolidated the common functions of multiple service controllers into a single controller. To manage heterogeneous devices and eliminate the multi-table lookup complexity due to config dependencies, we introduce Trident, a tree-based service- and device-independent abstraction, so that config dependency calculation can be replaced by more efficient tree traversal. After deploying Poseidon on Alibaba Cloud, we observed a 21x increase in the throughput of virtual network configuration tasks, along with a 4.4x decrease in the P99 API processing latency. Poseidon completes the task of enabling hundreds of Elastic IP addresses (EIPs) 1.8 to 55 times faster than Vendors A and B, both of which are among the top 5 providers, for identical network configuration jobs.\n\nReal-world application deployments have hundreds of inter-dependent configuration parameters, many of which significantly influence performance and efficiency. With today's complex and dynamic services, operators need to continuously monitor and set the right configuration values (configuration tuning) well after a service is widely deployed. This is challenging since experimenting with different configurations post-deployment may reduce application performance or cause disruptions. While state-of-the-art ML approaches do help to automate configuration tuning, they do not fully address the multiple challenges in end-to-end configuration tuning of deployed applications.\n\nThis paper presents OpperTune, a service that enables configuration tuning of applications in deployment at Microsoft. OpperTune reduces application interruptions while maximizing the performance of deployed applications as and when the workload or the underlying infrastructure changes. It automates three essential processes that facilitate post-deployment configuration tuning: (a) determining which configurations to tune, (b) automatically managing the scope at which to tune the configurations, and (c) using a novel reinforcement learning algorithm to simultaneously and quickly tune numerical and categorical configurations, thereby keeping the overhead of configuration tuning low. We deploy OpperTune on two enterprise applications in Microsoft Azure's clusters. Our experiments show that OpperTune reduces the end-to-end P95 latency of microservice applications by more than 50% over expert configuration choices made ahead of deployment. The code and datasets used are made available at https://aka.ms/OPPerTune.\n\nDeep neural networks (DNNs) are becoming progressively large and costly to train. This paper aims to reduce DNN training costs by leveraging preemptible instances on modern clouds, which can be allocated at a much lower price when idle but may be preempted by the cloud provider at any time. Prior work that supports DNN training on preemptive instances employs a reactive approach to handling instance preemptions and allocations after their occurrence, which only achieves limited performance and scalability.\n\nWe present Parcae, a system that enables cheap, fast, and scalable DNN training on preemptible instances by proactively adjusting the parallelization strategy of a DNN training job to adapt to predicted resource changes before instance preemptions and allocations really happen, which significantly reduces the cost of handling these events. Parcae optimizes liveput, a novel metric that measures the expected training throughput of a DNN job under various possible preemption scenarios. Compared to existing reactive, throughput-optimized systems, Parcae's proactive, live-optimized solution considers both the throughput of a job and its robustness under preemptions. To optimize liveput, Parcae supports lightweight instance migration and uses an availability predictor to forecast future preemptions. It then uses a liveput optimizer to discover an optimal strategy to parallelize DNN training under predicted preemptions. We evaluate Parcae on a variety of DNNs and preemption traces and show that Parcae outperforms existing spot-instance DNN training systems by up to 10×. More importantly, Parcae achieves near-optimal performance for training large DNNs under frequent preemptions, in which case existing approaches cannot make any progress.\n\nDeep learning recommendation models (DLRM) are extensively adopted to support many online services. Typical DLRM training frameworks adopt the parameter server (PS) in CPU servers to maintain memory-intensive embedding tables, and leverage GPU workers with embedding cache to accelerate compute-intensive neural network computation and enable fast embedding lookups. However, such distributed systems suffer from significant communication overhead caused by the embedding transmissions between workers and PS. Prior work reduces the number of cache embedding transmissions by compromising model accuracy, including oversampling hot embeddings or applying staleness-tolerant updates.\n\nThis paper reveals that many of such transmissions can be avoided given the predictability and infrequency natures of in-cache embedding accesses in distributed training. Based on this observation, we explore a new direction to accelerate distributed DLRM training without compromising model accuracy, i.e., embedding scheduling—with the core idea of proactively determining \"where embeddings should be trained\" and \"which embeddings should be synchronized\" to increase the cache hit rate and decrease unnecessary updates, thus achieving a low communication overhead. To realize this idea, we design Herald, a real-time embedding scheduler consisting of two main components: an adaptive location-aware inputs allocator to determine where embeddings should be trained and an optimal communication plan generator to determine which embeddings should be synchronized. Our experiments with real-world workloads show that Herald reduces 48%-89% embedding transmissions, leading up to 2.11× and up to 1.61× better performance with TCP and RDMA, respectively, over 100 Gbps Ethernet for end-to-end DLRM training.\n\nMultimodal model training takes multiple types of inputs to process with differently structured submodules, and aggregates outcomes from the submodules to learn the relationship among various types of inputs, e.g., correlating text to image for text-to-image generation. The differences of submodule architectures as well as their inputs lead to heterogeneity in terms of computation efficiency. Failing to account for such heterogeneity, existing distributed training systems treat all submodules as a monolithic entity and thus have sub-optimal performance. Moreover, the outcome aggregation phase introduces cross-sample dependencies with contrasting positive and negative sample pairs (i.e., contrastive loss). Such dependencies make the existing pipeline parallelism scheduling algorithms not applicable for multimodal training with contrastive loss.\n\nTo address the limitations of existing solutions, we propose DISTIMM. For a given multimodal model, DISTIMM exploits the heterogeneity among submodules, applying different distributed parallelism strategies for each submodule, e.g., using Tensor Parallelism for a computation-intensive submodule, and Data Parallelism for a submodule with a small number of parameters. DISTIMM balances the computation of parallelized submodules to reduce the computing resource idle time of waiting for the slowest submodule. DISTIMM further optimizes the locality of submodules by leveraging the heterogeneous bandwidth of interconnections among accelerators. To address the limitation of existing pipeline execution schedules, we propose a new pipeline execution primitive, called batch-sync instruction, and a corresponding schedule, called DISTIMM-Pipe. We build a prototype of DISTIMM and evaluate it with existing solutions on models with various sizes ranging from 1.1 billion to 26 billion parameters and observe 1.32-3.27 × speedup over Megatron-LM.\n\nText-to-image generation using diffusion models has seen explosive popularity owing to their ability in producing high quality images adhering to text prompts. However, diffusion-models go through a large number of iterative denoising steps, and are resource-intensive, requiring expensive GPUs and incurring considerable latency. In this paper, we introduce a novel approximate-caching technique that can reduce such iterative denoising steps by reusing intermediate noise states created during a prior image generation. Based on this idea, we present an end-to-end text-to-image generation system, NIRVANA, that uses approximate-caching with a novel cache management policy to provide 21% GPU compute savings, 19.8% end-to-end latency reduction, and 19% dollar savings on two real production workloads. We further present an extensive characterization of real production text-to-image prompts from the perspective of caching, popularity and reuse of intermediate states in a large production environment.\n\nDeep neural networks (DNNs) are the de facto standard for essential use cases, such as image classification, computer vision, and natural language processing. As DNNs and datasets get larger, they require distributed training on increasingly larger clusters. A main bottleneck is the resulting communication overhead where workers exchange model updates (i.e., gradients) on a per-round basis. To address this bottleneck and accelerate training, a widely-deployed approach is compression. However, previous deployments often apply bi-directional compression schemes by simply using a uni-directional gradient compression scheme in each direction. This results in significant computational overheads at the parameter server and increased compression error, leading to longer training and lower accuracy.\n\nWe introduce Tensor Homomorphic Compression (THC), a novel bi-directional compression framework that enables the direct aggregation of compressed values and thus eliminating the aforementioned computational overheads. Moreover, THC is compatible with in-network aggregation (INA), which allows for further acceleration. Our evaluation shows that training representative vision and language models with THC reaches target accuracy by 1.40× to 1.47× faster using INA and 1.28× to 1.33× faster using a software PS compared with state-of-the-art systems.\n\nDistributed databases suffer from performance degradation under skewed workloads. Such workloads cause high contention, which is exacerbated by cross-node network latencies. In contrast, single-machine databases better handle skewed workloads because their centralized nature enables performance optimizations that execute contended requests more efficiently. Based on this insight, we propose a novel hybrid architecture that employs a single-machine database inside a distributed database and present TurboDB, the first distributed database that leverages this hybrid architecture to achieve up to an order of magnitude better performance than representative solutions under skewed workloads.\n\nTurboDB introduces two designs to tackle the core challenges unique to its hybrid architecture. First, Hybrid Concurrency Control is a specialized technique that coordinates the single-machine and distributed databases to collectively ensure process-ordered serializability. Second, Phalanx Replication provides fault tolerance for the single-machine database without significantly sacrificing its performance benefits. We implement TurboDB using CockroachDB and Cicada as the distributed and single-machine databases, respectively. Our evaluation shows that TurboDB significantly improves the performance of CockroachDB under skewed workloads.\n\nCaching is an indispensable technique for low-cost and fast data serving. The eviction algorithm, at the heart of a cache, has been primarily designed to maximize efficiency—reducing the cache miss ratio. Many eviction algorithms have been designed in the past decades. However, they all trade off throughput, simplicity, or both for higher efficiency. Such a compromise often hinders adoption in production systems.\n\nThis work presents SIEVE, an algorithm that is simpler than LRU and provides better than state-of-the-art efficiency and scalability for web cache workloads. We implemented SIEVE in five production cache libraries, requiring fewer than 20 lines of code changes on average. Our evaluation on 1559 cache traces from 7 sources shows that SIEVE achieves up to 63.2% lower miss ratio than ARC. Moreover, SIEVE has a lower miss ratio than 9 state-of-the-art algorithms on more than 45% of the 1559 traces, while the next best algorithm only has a lower miss ratio on 15%. SIEVE's simplicity comes with superior scalability as cache hits require no locking. Our prototype achieves twice the throughput of an optimized 16-thread LRU implementation. SIEVE is more than an eviction algorithm; it can be used as a cache primitive to build advanced eviction algorithms just like FIFO and LRU.\n\nMany applications can benefit from data that increases performance but is not required for correctness (commonly referred to as soft state). Examples include cached data from backend web servers and memoized computations in data analytics systems. Today's systems generally statically limit the amount of memory they use for storing soft state in order to prevent unbounded growth that could exhaust the server's memory. Static provisioning, however, makes it difficult to respond to shifts in application demand for soft state and can leave significant amounts of memory idle. Existing OS kernels can only spend idle memory on caching disk blocks—which may not have the most utility—because they do not provide the right abstractions to safely allow applications to store their own soft state.\n\nTo effectively manage and dynamically scale soft state, we propose soft memory, an elastic virtual memory abstraction with unmap-and-reconstruct semantics that makes it possible for applications to use idle memory to store whatever soft state they choose while guaranteeing both safety and efficiency. We present Midas, a soft memory management system that contains (1) a runtime that is linked to each application to manage soft memory objects and (2) OS kernel support that coordinates soft memory allocation between applications to maximize their performance. Our experiments with four real-world applications show that Midas can efficiently and safely harvest idle memory to store applications' soft state, delivering near-optimal application performance and responding to extreme memory pressure without running out of memory.\n\nWe present PReQuaL (Probing to Reduce Queuing and Latency), a load balancer for distributed multi-tenant systems. PReQuaL is designed to minimize real-time request latency in the presence of heterogeneous server capacities and non-uniform, time-varying antagonist load. To achieve this, PReQuaL actively probes server load and leverages the power of d choices paradigm, extending it with asynchronous and reusable probes. Cutting against received wisdom, PReQuaL does not balance CPU load, but instead selects servers according to estimated latency and active requests-in-flight (RIF). We explore the major design features of PReQuaL on a testbed system and describe our experience using it to balance load within YouTube, where it has been running for more than a year. PReQuaL has dramatically decreased tail latency, error rates, and resource use, enabling YouTube and other production systems at Google to run at much higher utilization.\n\nThe existing ambient backscatter systems suffer from either more spectrum utilization or low throughput. we propose Orthcatter, the first in-band OFDM backscatter system that provides a higher throughput while consuming fewer spectrum resources. Our key innovation is the designed over-the-air code division technique that enables the cancellation of the co-channel interferences, solving the core challenge of the in-band backscatter communication. Unlike the common code-division systems that generate orthogonal codewords locally, we construct the quasi-orthogonal backscatter codewords by swapping the subcarriers of each excitation OFDM symbol and concrete this design passively with a double side-band symbol construction method. Armed with these quasi-orthogonal codewords, we design a two-step interference cancellation scheme, significantly improving reliability. We prototype and test Orthcatter. The results show that Orthcatter can achieve throughput of 248kbps and a BER of 10^-4 under OFDM WiFi exciter, improving by over 4.6× and 300× compared with the state-of-the-art in-band backscatter system. Our throughput and BER can even be 11kbps higher and 59× better than the prior side-band backscatter systems, and the exciter-to-tag communication range is 3× of prior OFDM backscatter systems.\n\nRadio Access Networks (RAN) are increasingly softwarized and accessible via data-collection and control interfaces. RAN intelligent control (RIC) is an approach to manage these interfaces at different timescales. In this paper, we introduce EdgeRIC, a real-time RIC co-located with the Distributed Unit (DU). It is decoupled from the RAN stack, and operates at the RAN timescale. EdgeRIC serves as the seat of real-time AI-in-the-loop for decision and control. It can access RAN and application-level information to execute AI-optimized and other policies in real-time (sub-millisecond). We demonstrate that EdgeRIC operates as if embedded within the RAN stack. We showcase RT applications called μApps over EdgeRIC that significantly outperforms a cloud-based near real-time RIC (> 15 ms latency) in terms of attained system throughput. Further, our over-the-air experiments with AI-based policies showcase their resilience to channel dynamics. Remarkably, these AI policies outperform model-based strategies by 5% to 25% in both system throughput and end user application-level benchmarks across diverse mobile scenarios.\n\nThe wireless channel between gaming console and accessories e.g. controllers and headsets, experiences extremely rapid variations due to abrupt head and hand movements amidst an exciting game. In the absence of prior studies on wireless packet losses for console gaming, through extensive evaluations and user studies, we find that state-of-the-art rate adaptation schemes, unable to keep up with these rapid changes, experience packet loss rates of 2-10% while loss rates that are 10× lower (0.1-0.5%) are required to ensure a high quality gaming experience. We present ADR-X, an ANN-based contextual multi-armed bandit rate adaptation technique that continuously predicts and tracks the channel and picks appropriate data rates. A key challenge for ADR-X is that it must run on power and compute constrained embedded devices under realtime constraints. ADR-X addresses this challenge by meticulously crafting an ANN that leverages existing communication theory results to incorporate domain knowledge. This allows ADR-X to achieve 10× lower packet losses than existing schemes while also running 100× faster than state-of-the-art reinforcement learning schemes, making it suitable for deployment on embedded gaming devices.\n\nIn the fast-paced landscape of UHF RFID technology, achieving precise spatial-selective identification is of critical importance in the logistics and retail domain. This work introduces RFID+, a magnetically-driven UHF RFID system that leverages the matching loops of commercial-off-the-shelf UHF RFID tags for efficient energy harvesting from tailored magnetic fields. The RFID+ delivers a level of spatial precision comparable to that of HF NFC systems, effectively mitigating issues of miss-reading and cross-reading. Our primary contributions reside in the development of a specialized multi-turn, capacitor-segmented coil antenna and an innovative fast inventory algorithm. The RFID+ seamlessly integrates traditional radiative coupling with the innovative magnetic coupling in UHF RFID systems, bolstering their overall performance and efficiency. Real-world pilot studies in warehouses and logistics settings reveal that RFID+ significantly diminishes the miss-reading rate from 22.9% down to a remarkable 1.06%, while entirely eliminating cross-reading challenges. Moreover, our RFID+ variant demonstrates better resilience against materials traditionally challenging for UHF RFID, such as water bottles and containers. These advancements make RFID+ exceedingly relevant for practical applications in logistical networks.\n\nWi-Fi direct transport provides versatile connectivity that enables convenient data sharing and improves the productivity of mobile end users. However, as today's smartphones are capable of near-Gbps wireless data rates, current solutions do not efficiently utilize the available bandwidth in this single-hop environment. We show that existing transport schemes suffer from resource-intensive reliable delivery mechanisms, inadequate congestion control, and inefficient flow control for achieving line-rate transmission in peer-to-peer Wi-Fi direct links. In this paper, we present SMUFF, a reliable file transfer service that achieves nearly the practical line rate of the underlying wireless bandwidth. We note a unique feature of direct transport—the sender can monitor each buffer along the data path and determine an optimal sending rate accordingly. Therefore, SMUFF can maximize throughput by strategically backlogging the appropriate amount of data in the bottleneck buffer. We have deployed SMUFF on four different phone models, and our evaluations with other transport schemes show that SMUFF achieves up to 94.7% of the practical line rate and 22.6% throughput improvement with a 37% reduction in CPU usage and a 15% reduction in power consumption, compared to state-of-the-art solutions.\n\nLive ML analytics have gained increasing popularity with large-scale deployments due to recent evolution of ML technologies. To serve live ML queries, experts nowadays still need to perform manual query planning, which involves pipeline construction, query configuration, and pipeline placement across multiple edge tiers in a heterogeneous infrastructure. Finding the best query plan for a live ML query requires navigating a huge search space, calling for an efficient and systematic solution.\n\nIn this paper, we propose Vulcan, a system that automatically generates query plans for live ML queries to optimize their accuracy, latency, and resource consumption. Based on the user query and performance requirements, Vulcan determines the best pipeline, placement, and query configuration for the query with low profiling cost; it also performs fast online adaptation after query deployment. Vulcan outperforms state-of-the-art ML analytics systems by 4.1×-30.1× in terms of search cost while delivering up to 3.3× better query latency.\n\nWe present CASSINI, a network-aware job scheduler for machine learning (ML) clusters. CASSINI introduces a novel geometric abstraction to consider the communication pattern of different jobs while placing them on network links. To do so, CASSINI uses an Affinity graph that finds a series of time-shift values to adjust the communication phase"
    }
}
{
    "id": "correct_subsidiary_00048_2",
    "rank": 22,
    "data": {
        "url": "https://thenewstack.io/what-developers-need-to-know-about-intels-2024-server-chips/",
        "read_more_link": "",
        "language": "en",
        "title": "What Developers Need to Know about Intel’s 2024 Server Chips",
        "top_image": "https://cdn.thenewstack.io/media/2023/11/a637506f-newsroom-inside-btb-don-soltis-1920x1080.png.rendition.intel_.web_.1920.1080.jpg",
        "meta_img": "https://cdn.thenewstack.io/media/2023/11/a637506f-newsroom-inside-btb-don-soltis-1920x1080.png.rendition.intel_.web_.1920.1080.jpg",
        "images": [
            "https://cdn.thenewstack.io/media/2023/11/a637506f-newsroom-inside-btb-don-soltis-1920x1080.png.rendition.intel_.web_.1920.1080-1024x576.jpg",
            "https://thenewstack.io/wp-content/uploads/2017/06/501d027b-agam-shah_avatar_1497985030.-600x600.jpg"
        ],
        "movies": [],
        "keywords": [],
        "meta_keywords": [
            ""
        ],
        "tags": null,
        "authors": [
            "Agam Shah",
            "Loraine Lawson",
            "Scott M. Fulton III",
            "Joab Jackson",
            "Steven J. Vaughan-Nichols",
            "Manas Chowdhury",
            "Raghavan \"Rags\" Srinivas",
            "Jeffrey Burt",
            "Jack Wallen",
            "Kim Lewandowski"
        ],
        "publish_date": "2023-11-10T11:00:18+00:00",
        "summary": "",
        "meta_description": "Intel's new chips are engineering marvels, but software developers will need to recompile and rewrite software to exploit their performance benefits.",
        "meta_lang": "en",
        "meta_favicon": "https://thenewstack.io/favicon.ico",
        "meta_site_name": "The New Stack",
        "canonical_link": "https://thenewstack.io/what-developers-need-to-know-about-intels-2024-server-chips/",
        "text": "Intel’s server chips currently coming off the factory floors are engineering marvels, with some of the most radical chip overhauls in decades.\n\nThe new chips, which will ship next year, are shape-shifters: They can run conventional applications such as databases and operating systems, but can switch over to AI applications, which are predicated on vector processors and arithmetic units.\n\nSoftware developers will need to recompile and rewrite software to exploit the performance benefits.\n\nThe duality of those chips adds more choice for developers, but also creates confusion on which processor to choose for conventional applications (which use CPUs) and AI (which need accelerators). The on-chip AI features are a luxury; if developers can exploit it, it’s only gravy for Intel.\n\nGranite Rapids and Sierra Forest\n\nThe upcoming server chips, code-named Granite Rapids and Sierra Forest, also bring mainstream support for new technologies like Compute Express Link (CXL) interconnect, which can convert a data center into one giant computer.\n\nThe new server chips will support existing x86 applications but may require some tweaks in code and recompilation. At the same time, the growing complexity of chips may further lock buyers into Intel hardware, which already dominates data centers.\n\nRecompiling code should be a breeze if developers have a history of compiling code to x86 hardware, Intel’s Ronak Singhal, chief architect of Xeon, told The New Stack.\n\nBut recompiling should bring performance benefits by adapting code to the new features on the chips. “Specifically, when you’re comparing to something like an ARM competitor, you have to go and migrate all of that software to an ARM environment,” Singhal said.\n\nMajor cloud providers are changing data center designs for AI, with more network bandwidth and separate memory and storage tiers. The new server chips are designed for those installations, and that should narrow the number of programmers who code directly to resource allocation on hardware.\n\nSome new technologies in Granite Rapids chips are designed to scale up performance in tandem across CPUs and accelerators. As a result, there are more fine-grained layers and complexity to the provisioning and sharing of accelerators, memory and storage.\n\nCXL\n\nThe CXL technology gives servers faster access to more memory and storage with faster connections. Intel’s current server chips have early versions of CXL, but the technology will go mainstream with Granite Rapids, which is Intel’s server chip coming next year.\n\nTypically, servers rely on internal memory, but CXL has superfast bandwidth that allows the creation of storage and memory tiers outside servers. That means a server can use memory stored in a box that may be physically distant.\n\nResearchers are still trying to unpack the implications of CXL’s ability to use remote memory and cache, and how applications will perform.\n\nResearchers from the University of Illinois at Urbana-Champaign published a paper in October benchmarking the performance of Nginx and Redis on CXL. The research also noted the promise of physical CXL memory by comparing it to emulated CXL memory systems.\n\nCustomers are looking at CXL memory for different reasons, which could include memory expansion, bandwidth or getting cheaper memory outside of standard DDR5, Singhal said.\n\nIntel is building in CXL software that blurs the lines between direct-attached and tiers of attached memory. The software allows the hardware to manage under the covers and make sure that the right data is in the right place, Singhal said.\n\n“Software doesn’t need to be aware of the different tiers — this notion of software-transparent memory management with CXL can be used in some cases,” Singhal said.\n\nGranite Rapids will have more processing cores than the current generation chips, code-named Sapphire Rapids. It will be compiled of chiplets, in which server silicon by piecing together modular blocks of processors, memory and storage.\n\nFor developers, there are many things to consider as the new chips get closer to release.\n\nThe biggest change in Granite Rapids is the APX instruction set, which doubles the number of registers from 16 to 32. APX provides more registers to load and restore applications faster, and compilers can manage variables in registers and cut reliance on memory.\n\nCoders will need to recompile applications to get incremental performance boosts, which matters in critical applications. Intel is working to mainstream APX-related tools in the open source ecosystem.\n\nIntel pushed some APX developer tools to GCC over the last two months, and is doing the same for LLVM. Intel is building in APX support into its OneAPI parallel programming framework and is working with Microsoft and other companies for APX support.\n\nIntel is also activating features for developers who need more time to fine-tune applications to the new chips.\n\nThe chip maker is slowly shifting to an on-demand purchase model in which customers will pay only for the chip features they need. For example, customers can pay a fee to turn on an on-chip accelerator like AMX — which is designed for AI acceleration — when it needs inferencing features.\n\nSoftware development cycles are long, and a software stack may not initially be ready for AMX. Customers do not have to pay for AMX until the software has been tested.\n\nIntel is also pairing its discrete Gaudi3 AI accelerator with the Granite Rapids chip, which is similar to how Nvidia paired its Grace CPU with its red-hot H100 GPU. Intel may ultimately take a traditional server-client approach to offload some of the AI inferencing on Gaudi to the AMX accelerator.\n\nThe chip maker’s OneAPI provides the tools to optimize code for its chips and has libraries for TensorFlow, PyTorch and its own distribution of Python. Intel is also a big advocate for SYCL, which slices proprietary code — such as CUDA — and rewrites programs to work on a wide range of CPUs and accelerators.\n\nThe security features on Granite Rapids remain a mystery, but Singhal said the chip will include Trusted Domain Execution (TDX), a confidential computing feature that only allows authorized users to access programs or code. The authentication feature keeps code secure.\n\nIntel is also shipping a low-power chip called Sierra Forrest, which will come with 288 cores. The chip has power-efficient cores and will compete with low-power ARM-based chips developed by Ampere and Amazon. Sierra Forest will also have many of the features in Granite Rapids, including APX and AVX2, but is designed more for web applications.\n\nThe goal for Sierra Forest is to prevent x86 customers from defecting to ARM.\n\n“Having this portfolio of Granite Rapids and Sierra Forest, our value proposition to them is saying ‘you want to build your infrastructure based on x86, whether it is your legacy software or new software… developing it to run on x86 going forward,” Singhal said."
    }
}
{
    "id": "correct_subsidiary_00048_3",
    "rank": 1,
    "data": {
        "url": "https://www.linkedin.com/posts/sunil-shenoy-05ab4b7_congratulations-to-shlomit-weiss-we-are-activity-6930147390079340544-kZst",
        "read_more_link": "",
        "language": "en",
        "title": "Sunil Shenoy on LinkedIn: Congratulations to Shlomit Weiss. We are privileged to have her leadership…",
        "top_image": "https://media.licdn.com/dms/image/C4E05AQFNKrE94Cmo6g/videocover-high/0/1652274908453?e=2147483647&v=beta&t=C6hgSY5D9v0Xn6tccpzEWPTOhW6E565ihDIOGr-EaBM",
        "meta_img": "https://media.licdn.com/dms/image/C4E05AQFNKrE94Cmo6g/videocover-high/0/1652274908453?e=2147483647&v=beta&t=C6hgSY5D9v0Xn6tccpzEWPTOhW6E565ihDIOGr-EaBM",
        "images": [
            "https://static.licdn.com/aero-v1/sc/h/5q92mjc5c51bjlwaj3rs9aa82"
        ],
        "movies": [
            "https://dms.licdn.com/playlist/vid/C4E05AQFNKrE94Cmo6g/mp4-640p-30fp-crf28/0/1652274917492?e=2147483647&v=beta&t=-oo-wZDmbw0dKjQB6RB0I33k7O05U5obAu4g7SRTY7A"
        ],
        "keywords": [],
        "meta_keywords": [
            ""
        ],
        "tags": null,
        "authors": [
            "Sunil Shenoy"
        ],
        "publish_date": "2022-05-11T13:31:33.692000+00:00",
        "summary": "",
        "meta_description": "Congratulations to Shlomit Weiss. We are privileged to have her leadership at Intel Corporation.",
        "meta_lang": "en",
        "meta_favicon": "https://static.licdn.com/aero-v1/sc/h/al2o9zrvru7aqj8e1x2rzsrca",
        "meta_site_name": "",
        "canonical_link": "https://www.linkedin.com/posts/sunil-shenoy-05ab4b7_congratulations-to-shlomit-weiss-we-are-activity-6930147390079340544-kZst",
        "text": "Some time ago I blogged about the growing carbon emissions from data centers despite the vast generational improvements in energy efficiency. Burgeoning demand cancels out the efficiencies and more. I mused about the impact of the AI revolution expressing cautious optimism that AI computing is inherently “efficient” and offers a palette of trade-offs between energy consumption and model scale and accuracy. That optimism was backed by Google researchers in the IEEE Computer paper titled “The Carbon Footprint of Machine Learning Training Will Plateau, Then Shrink” where they presented measurements and analysis circa 2021 to support this prognosis. However recent sustainability reports from cloud providers including Google and Microsoft report double digit increases in annual greenhouse gas (GHG) emissions which are steeply climbing since 2022. In practice AI computing is far from carbon neutral and despite the best efforts will take a long time to get there. Will AI computing GHGs pay for themselves by realizing some other global good (besides increasing the market value of a handful of companies)? There is a veritable cottage industry forecasting productivity and GDP gains attributable to AI. Notably MIT researcher Acemoglu has published a highly technical but widely available article (“The Simple Macroeconomics of AI\", May 2024) estimating the incremental cumulative GDP uplift over a decade to be under 2%. This is coming mostly from the productivity increase from automating routine tasks. Squaring with more optimistic forecasts maybe it is double that to reach 0.5% annual uplift for 0.5% annual GHG emission growth. No silver lining here. Perhaps AI can help solve or mitigate the world’s big problems like climate change and the economic uplift of the billion people still left behind in dire poverty. On climate change AI contribution seems to be improved forecasting of weather and other natural events, static and dynamic energy demand. AI can optimize control actions to reduce GHGs or mitigation reactions to floods, heat waves, wildfires etc. This is typically the domain of (by now traditional) machine learning AI. Generational AI and LLMs offer more promise for poverty reduction. The biggest opportunities could lie in improving the speed and quality of primary and secondary school level education. AI can help with the two biggest improvement prescriptions – teaching students at their individual level and providing teachers with help and automated feedback. For a balanced article on this see the July 7 issue of The Economist (“Will artificial intelligence transform school?”). AI can also help in disseminating reliable health information to less literate people who lack easy access to it today and improve their access to government and NGO services which are available to them (reducing friction, corruption, delay etc.). AI to realize these benefits must get its unfair share of attention and funding to make the world a fairer and happier place!\n\nWe are just past the summer solstice and here in Oregon the sun creeps up soon after 5 AM and light lingers well past 9 PM making for long, delightful, warm, bright days. 20 solar panels on our roof have already generated over 5 MWh of electricity this year – enough for all our total needs (including EV charging). Even in our cloudy corner of the world our total cost of solar energy is just 12 cents (10 rupees) per KwH. The levelized cost of solar energy has come down by a factor of 5 in 15 years. If that exponential reminds you of something else, you are not alone. Cost per transistor also decreased at a similar rate during the heyday of Moore’s Law. Photovoltaic cells were discovered about the same time as the transistor, about 75 years ago. Solar power lacked Gordon Moore who forecast the exponential over 50 years ago. Even the most optimistic proponents of solar power have underestimated its cost curves and hence ramp. Today without subsidies solar power can generate energy at the lowest total cost (including cost of capital at contemporary, competitive interest rates) compared to any alternative. Soon investing in any other form of energy would defy rationality and economics and be like continuing to build computers from vacuum tubes 50 years ago. Moore’s law measured strictly by decreasing cost per transistor probably ended 10 years ago although the “value” of transistors measured by power, performance, diversity continues to improve and aid in system cost and scaling. Solar manufacturing is simpler compared to the thousands of steps needed to manufacture a modern transistor and the innovations needed to achieve scaling and improved characteristics. The input is mostly sand, and the output power is an undifferentiated commodity - power. Most of the cost reduction required to be lowest cost has been achieved even if the remainder comes from scale and learning effects. Cheaper and faster computing helped to design and manufacture cheaper and faster computers. This virtuous cycle works for solar as well with lower energy cost contributing to cheaper manufacturing. Hence capital investments are lower risk (remember various delays in new process technology deployments and ramps). Clean energy demand is likely endless since it has been highly correlated with economic well-being for centuries. This is the case even if you skirt the issue of reducing or replacing fossil fuel-based energy. What is particularly sweet is that most of the billion or so people who are still extremely poor reside where the sun shines longest and brightest. Satisfaction of their energy demand must have the highest utility for humanity. For once fortune will literally shine directly on them and benefit them disproportionally.\n\nIn the last of a series of blogs to commemorate the 50th anniversary of the 8080 microprocessor I muse about the future trajectory of GPGPUs and AI and ML accelerators. The AI revolution has arrived with a bang and NVIDIA hardware shipments, revenues and market value seem to respect no ceiling. But there are at least one hundred serious accelerators from over thirty major companies that have been commercially released and sold across embedded, autonomous, mobile, personal computer and datacenter markets. Check out the Lincoln AI Computing Survey (LAICS) from MIT researchers which provides information about and metrics for most of these. The papers classify and graph them by power vs. peak performance (Tera-Ops per sec) and normed by release date, technology node and data type width. Benchmarking learning and AI accelerators is a more bounded challenge than CPUs and the MLPerf benchmark seems thorough, representative of production workloads and well respected. All signs of a healthy competitive field. The technical debates focus on “GPGPU versus the TPU” philosophy or more generally SIMD vs Systolic (or similar) hardware approaches. The former typically wins on flexibility, programmability, and compute unit utilization across and within workloads. The latter on compute density, power efficiency and complexity of design. In practice hybrid approaches are common with systolic processing units being a big component of datacenter GPGPUs. Industry dynamics are quite different from those that led to the horizontalization of the PC industry by Windows and the x86 architecture. Despite the recent NVIDIA phenomenon datacenter customers wield vast power, enjoy huge balance sheets, and have all built up large hardware design capabilities. They will have to select the best hardware (and software) but will not easily allow an architecture or company to dominate in the long run. If you do not have to worship at the altar of binary compatibility software should be a differentiator and a large source of value but not an unsurmountable moat. CUDA enjoys a wide adoption and technology lead. But there are alternatives like OpenCL. As other companies develop better hardware, more software developers will design for these platforms, and CUDA dominance could diminish. Of course, the datacenter customers are powerhouses of software talent, expertise, and experience as well. While not optimal, compiling CUDA (or PTX intermediate) to other hardware is legal. AMD and Intel have tools to source convert CUDA programs to their ROCm and OpenAPI platforms, respectively. I envision an innovative, diverse, and robustly competitive and healthy future for AI accelerators!\n\nMy recent blog commemorating the 50th anniversary of the 8080 microprocessor wondered whether the AI revolution would cause the “CPU” to lose value and relevance quickly. The CPU was never a slouch and has not slowed down with age. Per socket CPU performance has increased by almost 10X even in the last decade. The CPU has also lived up to its promise of flexibility and versatility by addressing the needs of new workloads with instruction set and data type enhancements, vector processing extensions, multi-threading, support for security and kernel functions etc. Parallel applications can be tuned to perform quite well on modern multi-core CPUs (see “Limits of GPU acceleration” paper by Georgia Tech researchers, “Debunking the 100X GPU vs. CPU myth” paper by Intel engineers). Except for graphics and modem processing few major applications have escaped the CPU’s reach. The silicon area footprint of “cores” is minor compared to caches, graphics units, memory, and IO controllers etc. Capital efficiency has favored the continued optimization for and deployment of CPUs. The AI revolution has shaken up this status quo with a “hockey stick” deployment of GPGPUs and accelerators. NVIDIA’s revenues have increased from a one third to thrice of Intel ‘s and AMD’s combined datacenter revenues! Model training and inference will migrate away from CPUs. With so many GPGPUs and accelerators in the socket and the datacenter there is an even playing field now for optimizing other applications across their components that run on the CPU and the GPU. On the other hand, there is still a whole lot done on CPUs. Non-ML and AI applications use an estimated 75% of activity and energy. CPU workloads are tremendously diverse with no “killer app.” Over fifty hottest binaries are needed to cover 50% of CPU cycles (see “Profiling a warehouse-scale computer” by Google and university researchers). It is hard to envision that AI and ML will replace or displace most such applications or that they will be recast to run on accelerators or GPGPUs. CPUs have lost a major “design win” but still have jobs to do. How should CPUs evolve? Continue to strive for universal compute domination as they tried to (and succeeded until recently)? Or strategically focus? (Linus Torvalds famously ranted against the AVX-512 vector extension as overreach). Intel and AMD are bifurcating micro-architectures (E-core, P-core, Zen, Zen-C) with each positioned as equally important. ARM has been doing this for generations. There will not be hyper-threading on the recently announced Intel Lunar Lake processors. What about pruning and deprecating legacy instructions and features which are obsolete? RISC-V promises that you can do better if you start from scratch. It also offers disjoint instruction set extensions which can be layered above a basic instruction set. The CPU may well need to internalize its changed job post the GPU/accelerator ramp and optimize for this future to thrive for the next 50 years.\n\nMy last blog which commemorated the golden anniversary of the 8080 CPU concluded by posing three important questions. The first was whether the x86-ARM-stice (relative market share and prominence between these architectures which has been stable for over a decade) would shift significantly and quickly. This is unlikely to happen due to any technical innovations or insights. Researchers have concluded that neither architecture has a “inherent” handicap, and this has been evidenced by commercial implementations. Instead he factor driving change is likely to be customer power. The advent of the PC, in 1981, famously resulted in the “horizontalization” of the computer industry. Soon most computers were built on the x86, and Windows standards and system vendors had to rely on other sources of differentiation. The found little and hence value migrated to the providers of the horizontal layers – Microsoft, Intel, and AMD. Their profits and market caps boomed funding a virtuous cycle of innovation and investment to preserve and enhance the horizontal layers and extend it to other markets. But value migration is not an inevitable outcome of horizontal disaggregation. Michael G. Jacobides and John Paul MacDuffie explain this in “How to Drive Value Your Way”, From the HBR Magazine (July–August 2013) through the example of the car industry. Car manufacturers have retained their relative market caps despite extensive outsourcing and acceptance of horizontal standards. This has been the case in the datacenter market – the other big customer for the x86 architecture. Datacenter and cloud providers’ profits and market caps have exploded. They have used their balance sheets to extend vertically downstream, sideways and will do so upstream as well. In the last decade Microsoft, Amazon, Google, and Meta have acquired and/or developed impressive silicon component architecture and design capabilities. Their output is already evident and will grow. Since these designs must rely on open or licensable architectures like ARM it follows that the relative share of ARM will expand. This will be the major driver for tilting the status quo. Intel and AMD must continue to provide excellent x86 CPUs. They will. Recently announced or discussed products like AMD’s Turin, Intel’s Lunar Lake and “Forests” will be outstanding. In addition, they are seeking other ways to make up for or increase their share of wallet in datacenter silicon. Both have AI and accelerator investments, technologies, and products. Intel of course also aspires to manufacture other’s silicon as a foundry. The next question is whether value and prominence in the silicon chain will rapidly migrate there away from any CPU (ARM or x86).\n\nThis is the 50th anniversary of the launch of the Intel 8080 which is credited with starting the microcomputer industry. Following that many companies launched their own microprocessors with diverse architectures. They considered instruction set innovation important for delivering value. Subsequently RISC researchers and champions argued that a simple instruction set augmented with capable compilers was better. RISC was “winning” this debate. Then in 1981 the “mother of all CISCs” – the x86 architecture snagged the IBM PC design win with the 8088. The personal computer revolution filled Intel and AMD coffers. They innovated, invested, and executed very well on the x86 architecture while preserving strict binary compatibility across implementations. When the next big market – enterprise and cloud servers came along there remained no viable alternative to the x86. But the third time was not a charm. ARM – a RISC like architecture founded in 1990 won the mobile revolution. By 2010 these two architectures had settled to a draw in market share and prominence. Meanwhile NVIDIA had launched and promoted GPGPUs but despite CUDA, OpenCL etc. they had made little headway except in graphics and gaming applications. “CPUs” remained unchallenged as the main source of value in systems. Founded in 1993 NVIDIAs market cap never exceeded $5B until 2015! Two interesting research articles were published around then. “Revisiting the RISC vs. CISC debate on contemporary ARM and x86 architectures” by Emily Blem et al argued through measurement and extrapolation that microarchitecture mattered much more than architecture and neither ARM nor x86 had an “inherent” advantage or handicap. “Limits of GPU acceleration” by researchers at Georgia Tech showed that well-tuned multi-core processors can perform as well as GPGPUs on many parallel applications. It threw a “small wet blanket” on the potential of GPUs to be big disruptors. These and follow-on papers predicted a continuation of the equilibrium. Indeed, that status quo did persist for a decade until it did not! Recently the fourth AI revolution has brought changes. Almost all cloud server capital spending in the last 12 months has been on accelerators and GPGPUs instead of CPUs. Intel and Microsoft have launched and promoted the “AI PC” stressing the capabilities and value of their accelerators. Microsoft’s Copilot+ PC was launched with an ARM processor and will be available soon from all leading PC manufacturers. NVIDIAs market cap is around $3 trillion. ARM’s cap has risen to match Intel! Hence the following questions: 1. Will the market share equilibrium between x86 and ARM shift significantly and quickly? 2. As it turns fifty will the “CPU” rapidly cede its role as the primary source of value in computer systems to accelerators and “GPGPUs”? 3. Will “architecture” matter and be sticky for accelerators and “GPGPUs” as it did for “CPUs”? Happy Golden Anniversary to 8080 and the first CPUs!\n\nEarlier I talked about the increasing energy consumption and carbon footprint of datacenters even after marvelous improvements in efficiency. And now we are amid the AI revolution! How might AI applications and accelerators figure into this? While training draws attention as an intensive compute workload inference dominates energy consumption just because the models are used for inference a lot more than they are trained. AI applications including inference have been running on general purpose, multi-core CPUs for a long time mostly to optimize for capital costs. When they were a small part of the total computing workload it was better to amortize the much larger CPU capital for them. But going forward companies are deploying GPGPUs and accelerators rapidly. These are at least two times more energy efficient at inference and this migration will conserve energy. More significantly companies are designing and deploying specialized hardware (systolic arrays, tensor cores) with widespread deployment by Google, Amazon and now Microsoft. This shows the willingness to make promising tradeoffs in favor of energy efficiency versus design and capital cost. Then there is algorithm and software efficiency. We can characterize that as number of flops (plural of a flop) per inference per unit of “accuracy.” Optimizing lower precision formats is an example of economizing on flops per inference at little loss of model accuracy. But to make models more accurate their parameters increase, and the structure of the inference network may become more complex demanding more flops. Compensating for this is the generational power efficiency of delivering these flops characterized by FLOPS (floating point operation per second) per Watt. Divide flops per inference by FLOPS per Watt and you get energy (joules) per inference. (Check that … the units do match 😊). This is what matters. The most accurate models demand more flops, and their inference energy vastly outstrips improvements in power efficiency in delivering the flops. However, we do not have to deploy the most advanced models for all inferences. We can make tradeoffs in accuracy to keep the flops per inference constant and reduce energy instead. All this to say that AI computation is inherently more “efficient” than general purpose computation and can be more easily made more efficient through algorithmic, hardware and software innovations. Companies are doing this versus the lazy approach of using CPUs and GPGPUs. This is a good trend but of course the increase in AI usage will swamp this. Everything will get intelligent and need to make inferences. Each of us will do things in our work and play which will trigger more inferences. Hopefully, other compensating benefits will accrue from all this tsunami of (energy efficient) inferences."
    }
}
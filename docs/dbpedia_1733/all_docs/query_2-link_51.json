{
    "id": "dbpedia_1733_2",
    "rank": 51,
    "data": {
        "url": "https://sse.tulane.edu/previous-colloquia",
        "read_more_link": "",
        "language": "en",
        "title": "Previous Colloquia",
        "top_image": "https://sse.tulane.edu/themes/custom/tulane_tailwindcss/favicon.ico",
        "meta_img": "https://sse.tulane.edu/themes/custom/tulane_tailwindcss/favicon.ico",
        "images": [
            "https://sse.tulane.edu/sites/default/files/2023-08/D9_Web_Lockups_sse-113.svg",
            "https://sse.tulane.edu/themes/custom/tulane_tailwindcss/img/tulane_footer.svg"
        ],
        "movies": [],
        "keywords": [],
        "meta_keywords": [
            ""
        ],
        "tags": null,
        "authors": [],
        "publish_date": null,
        "summary": "",
        "meta_description": "",
        "meta_lang": "en",
        "meta_favicon": "/themes/custom/tulane_tailwindcss/favicon.ico",
        "meta_site_name": "",
        "canonical_link": "https://sse.tulane.edu/previous-colloquia",
        "text": "Summer 2023 Colloquia\n\nAll colloquia, unless otherwise noted, will be available for in-person attendance as well as remote attendance via Zoom. Current Tulane faculty, staff, and students are encouraged to attend in person. Zoom details will be provided via the announcement listserv, or you may email dramil1@tulane.edu to request the corresponding link. If you would like to receive notices about upcoming seminars, you can subscribe to the announcement listserv.\n\nMay 22\n\nMachine Learning for Engineering\n\nPascal Van Hentenryck | Georgia Tech\n\nThis talk will be held on Monday, May 22nd, at 4:00 p.m. in Stanley Thomas 302. Please note the special weekday and time for this event.\n\nAbstract: The fusion of machine learning and optimization has the potential to achieve breakthroughs in science and engineering that the two technologies cannot accomplish independently. This talk reviews a number of research avenues in this direction, including the concept of optimization proxies and end-to-end learning. Principled combinations of machine learning and optimization are illustrated on case studies in energy systems, mobility, and supply chains. Preliminary results show how this fusion makes it possible to perform real-time risk assessment in energy systems, find near-optimal solutions quickly in supply chains, and implement model-predictive control for large-scale mobility systems..\n\nAbout the Speaker: Pascal Van Hentenryck is an A. Russell Chandler III Chair and Professor in the H. Milton Stewart School of Industrial and Systems Engineering at Georgia Tech. He is also the the director of the NSF Artificial Intelligence Institute for Advances in Optimization. Prior to this appointment, he was a professor of Computer Science at Brown University for about 20 years, he led the optimization research group (about 70 people) at National ICT Australia (NICTA) (until its merger with CSIRO), and was the Seth Bonder Collegiate Professor of Engineering at the University of Michigan. Van Hentenryck is also an Honorary Professor at the Australian National University.\n\nVan Hentenryck is a Fellow of AAAI (the Association for the Advancement of Artificial Intelligence) and INFORMS (the Institute for Operations Research and Management Science). Van Hentenryck’s research focuses on artificial intelligence and operations research for engineering applications: He explores methodologies that includes large-scale optimization and machine learning and applies them in challenging applications in energy, mobility, supply chains and logistics, privacy, and resilience. In particular, he leads the NSF AI Institute for Advances in Optimization, the Socially-Aware Mobility (SAM) project with a major focus on on-demand multi-modal transit systems, and the RAMC Project whose goal is to study risk-aware market clearing algorithms to integrate large shares of renewable energy. Together with MARTA, he piloted MARTA Reach, an on-demand service to explore the viability of on-demand multi-modal transit systems in Atlanta. Earlier in his career, Van Hentenryck designed and implemented several widely used optimization systems, including the constraint programming language CHIP (the foundation of modern constraint-programming systems) and the modeling language OPL (now an IBM product).\n\nJuly 18\n\nInterpretable Visual Domain Adaption from Feature Representation to Multi-modal Semantics\n\nTaotao Jing | Computer Science PhD Student, Tulane University\n\nPlease join us for Taotao Jing’s PhD dissertation defense talk, which is described below.\n\nThis talk will be held via ZOOM ONLY on Tuesday, July 18th, at 10:00 a.m. Zoom details will be provided via the announcement listserv, or you may email dramil1@tulane.edu to request the corresponding link.\n\nAbstract: Transfer learning has revolutionized the field of deep learning, allowing the utilization of pre-trained models to address challenges such as limited training data and expensive computational resources. However, the lack of interpretability and transparency in transfer learning methods poses significant obstacles to their practical deployment and trustworthiness. This doctoral dissertation is dedicated to enhancing the transparency and interpretability of visual domain adaptation, a critical task of transfer learning, encompassing feature representation analysis and integration of multimodal semantic knowledge. By addressing the domain shift across domains and providing human-friendly explanations simultaneously, we seek to provide deeper insights into the transfer learning process and facilitate more interpretable and trustworthy outcomes in real-world applications. The research provides valuable insights for integrating AI systems across domains, promoting transparency, interpretability, and trustworthiness in decision-making. Overall, it contributes to the development of interpretable transfer learning techniques, enhancing the understanding and practical application of deep learning models, and fostering transparent and collaborative human-AI interactions.\n\nJuly 21\n\nAvik Bhattacharya | Computer Science PhD Student, Tulane University\n\nPlease join us for Avik Bhattacharya’s PhD dissertation defense talk, which is described below.\n\nThis talk will be held in Stanley Thomas 302 on Friday, July 21st, at 10:00 a.m. Zoom details will be provided via the announcement listserv, or you may email dramil1@tulane.edu to request the corresponding link.\n\nAbstract: CD4+ T-cell receptors recognize peptide-MHCII complexes displayed on the surface of antigen-presenting cells to induce an immune response. A fundamental problem in immunology is to characterize which peptides (i.e., epitopes) in an antigen induce such a response; this is the problem of computational epitope prediction. Almost all state-of-the-art CD4+ T-cell epitope prediction tools rely exclusively on peptide-MHCII binding affinity scores to make predictions and ignore the crucial phenomena of antigen processing. In this dissertation, we developed computational approaches that incorporate a structure-based model of antigen processing as a crucial part of epitope prediction framework. We discuss the application of the our approaches in multiple settings ranging from epitope prediction of pathogenic antigens to personalized epitope prediction for cancer neo-antigens. Our approaches show significant improvement over state-of-the-art approaches and provide better understanding of the MHCII pathway.\n\nSpring 2023 Colloquia\n\nUnless otherwise noted, the seminars in Spring 2023 will meet on Thursdays at 12:45 pm in Stanley Thomas 316. All colloquia this semester, unless otherwise noted, will be available for in-person attendance as well as remote attendance via Zoom. Current Tulane faculty, staff, and students are encouraged to attend in person. Zoom details will be provided via the announcement listserv, or you may email dramil1@tulane.edu to request the corresponding link. If you would like to receive notices about upcoming seminars, you can subscribe to the announcement listserv.\n\nJan 24\n\nUnderstand, Predict and Enhance User Behavior in Mixed Reality\n\nYukang Yan | Carnegie Mellon University\n\nThis talk will be held on Tuesday, January 24th, at 12:45 p.m. in Stanley Thomas 316. Please note the special weekday for this event.\n\nAbstract: My research is based on the observation that Mixed Reality blurs the boundary between virtuality and reality, and significantly impacts how humans perceive the world and themselves. To understand this impact, I conduct user studies to observe and model the user’s behavioral and perceptual patterns when they interact with Mixed Reality. Based on the obtained findings, I build interaction techniques to facilitate the two way communication of how users deliver the interaction intents to the computer and how the computer displays the information and feedback to the user. More specifically, I develop multimodal input techniques leveraging the user’s hand gestures, head movements and facial expressions and adaptive user interfaces considering the user’s mental state and environmental context to optimize the information display. A step further, I explore leveraging the unique behavioral patterns that users have in Mixed Reality to enhance their interactions. In previous projects, I enabled users to embody healthier virtual avatars and non-humanoid avatars to gain novel experiences that are less possible to have in reality.\n\nAbout the Speaker: My name is Yukang Yan. I’m a postdoc at Carnegie Mellon University, working with David Lindlbauer. Before that, I obtained my Ph.D. degree at Tsinghua University under the supervision of Yuanchun Shi. My research interests lie in the intersection area between Human-Computer Interaction (HCI) and Mixed Reality (MR). I publish at top HCI venues (ACM CHI, UIST, Ubicomp) and MR venues (IEEE VR), with one Best Paper Honorable Mention Award at CHI 2020. I served on the program committee of CHI 22-23, UIST 21-22, and the SIGCHI Communications Committee.\n\nJan 30\n\nContent Experience in 3D Virtual Reality Worlds: Lens on Learning Transformation\n\nGayathri Sadanala | Missouri Valley College\n\nThis talk will be held on Monday, January 30th in Boggs 600 at 3:00 p.m. Please note the special weekday for this event.\n\nAbstract: A little over the past two decades, the advent of digital technology has transformed online learning drastically. With the increase of online students every year and especially during the period of pandemic that suggests social distancing, online orientation is necessary for the students to have a smooth transition into online learning. This talk will present the important factors that contribute successful student learning, engagement, and experiences within the educational 3d virtual reality orientation and research contribution to intersection of education, human computer interaction and computer science.\n\nAbout the Speaker: Gayathri Sadanala is an Assistant professor at Missouri Valley College. She is a doctoral candidate at the University of Missouri majoring in Human-Computer Interaction from Information Science and Learning Technologies. She holds Master’s in Engineering Technology and bachelor’s in computer science Engineering. Her research interests are interactions in Virtual Reality Environments, data science, visualization, Usability, and User experience studies.\n\nFeb 7\n\nDesigning Adaptive and Context-aware AR/VR Interactions\n\nRawan Alghofaili | George Mason University\n\nThis talk will be held on Tuesday, February 7th, at 12:45 p.m. in Stanley Thomas 316. Please note the special weekday for this event.\n\nAbstract: Anyone who has witnessed the adoption of the Internet remembers the static and non-context-aware websites of the past. Compare that with the powerful engines behind the websites of today. These context-aware engines are equipped with machine learning and optimization algorithms that allow them to adapt and cater to their user's behavior and environment. This deep understanding of the user and their needs created a more personalized and efficient experience. Current AR/VR systems are not quite as static as the websites of yesteryear but still a long way to go from becoming as powerful and context-aware as browsing the web today. Rawan aspires to facilitate the road to achieving context-aware AR/VR systems that elegantly adapt their interactions to their user's behavior and environment. Rawan will discuss her work in AR/VR adaptive navigation aids, VR environment design via visual attention, and in-situ mobile AR content-authoring via 2D to 3D curve projection.\n\nAbout the Speaker: Rawan Alghofaili received her Ph.D. in Computer Science graduate from George Mason University under the mentorship of Professor Craig (Lap-Fai) Yu. She received an M.S. from the University of Massachusetts-Boston in 2018 and an MEng. from Cornell University in 2014. Rawan was a research intern with Adobe's Creative Intelligence lab in the Summer of 2020 and a research intern with Meta Reality Labs in the Summer and Fall of 2021. Her work focuses on creating context-aware adaptive interactions for AR/VR via computational interaction design, which involves blending the wonderful worlds of ML, Computer Graphics, and HCI.\n\nFeb 9\n\nHolistic Data Protection: Policy-based Privacy-preserving Data Management\n\nPrimal Pappachan | Pennsylvania State University\n\nAbstract: The era of Big Data, AI, and the IoT is resulting in a significant increase in the generation of personally identifying data. Protecting personal information has become more critical than ever, especially as new privacy laws like the General Data Protection Regulation (GDPR), the California Consumer Privacy Act (CCPA), and the Virginia Consumer Data Protection Act (VCDPA) recognize the importance of both privacy and security in data management. However, effective data protection involves more than just addressing security challenges like unauthorized access. It also requires addressing privacy concerns, including the risk of inferences that can potentially threaten individuals' anonymity and safety.\n\nIn this talk, I will discuss my research on designing and implementing data protection methods that take into account the trade-offs between privacy, utility, and user customization. First, I will delve into policy-based approaches, which use access control as their primary defense against unauthorized access to personal data and provide users with the ability to customize their own data protection preferences. I will also explore the challenges of scalability and inference control and present algorithmic solutions that can help address these challenges. Next, I will present my work on a privacy-preserving mechanism based on Differential Privacy, which provides stronger guarantees for protecting personal data under certain assumptions. Specifically, I will discuss a new privacy-preserving mechanism for location privacy, which offers end-user customization while preserving strong privacy guarantees.\n\nLastly, I will outline my research plans for developing the next generation of data management systems, which integrate these two data protection methods to support secure, private, and reliable computing for individuals. Through this holistic approach, my work aims at providing a balanced approach to data protection that prioritizes both security and privacy, while also providing users with the ability to customize their own data protection preferences.\n\nAbout the Speaker: Primal Pappachan is a Postdoctoral Scholar at Penn State University's College of Information Sciences and Technology, where he works with Professor Anna Squicciarini. He received his Ph.D. in Computer Science from the University of California, Irvine in 2021, where he was advised by Professor Sharad Mehrotra. Primal's research focuses on data management and privacy, particularly on designing and implementing data protection mechanisms. He has published his work in top-tier venues including VLDB and SIGSPATIAL and has received awards to support his postdoctoral work including the Penn State Center for Security Research and Education Seed Grant and the Penn State Center for Socially Responsible AI Collaboration Pilot Funding. In addition to his research, Primal has served on program committees for conferences and workshops and is currently co-organizing a workshop (ASTRIDE) at ICDE 2023.\n\nFeb 13\n\nKnowledge in Pieces and Computing\n\nSuleman Mahmood | University of Illinois at Urbana Champaign\n\nThis talk will be held on Monday, February 13th in Boggs 600 at 1:00 p.m. Please note the special weekday, venue, and time for this event.\n\nAbstract: Knowledge in pieces is one of the prominent perspectives on how students access different concepts to solve a problem. This perspective emphasizes the role of context in how students solve problems. This perspective has been used in computing education research to explain the fragile nature of students’ knowledge in early programming classes. In my presentation, I will explain knowledge in pieces perspective of conceptual change. I will discuss several examples from prior research where knowledge in pieces perspective has been used to explain the observations about how students solve problems. I will also describe how I used this theoretical framework explains my own observation of students’ knowledge of cache memories. In the end I will discuss, I will discuss the implications of knowledge in pieces perspective for classroom teaching.\n\nAbout the Speaker: Suleman Mahmood is a Ph.D. candidate in the Department of Computer Science at University of Illinois Urbana-Champaign. He is interested in exploring how students learn computing concepts, effectiveness of assessments and building tools to help students learn. His current research is focused on exploring how students learn caches within the context of an undergraduate computer science course. He has also developed tools to help students learn memory system performance analysis and coding techniques that can improve memory system performance. He has been included in the list of students ranked excellent by students twice at University of Illinois. Suleman completed his master’s in computer science at Lahore University of Management Sciences. He also worked as a software engineer for 5 years.\n\nFeb 14\n\nParty Tricks: On Primaries and Gerrymandering\n\nOmer Lev | Ben-Gurion University of the Negev\n\nThis talk will be held on Tuesday, February 14th, at 1:00 p.m. in Stanley Thomas 316. Please note the special weekday and time for this event.\n\nAbstract: In the last few years, the effects that primaries and gerrymandering have on election outcomes became a commonly debated issue, particularly in the US. Both primaries (candidate selection by parties) and district-based elections are examples of mechanisms that involve adding stages to a decison-making procedure. I will discuss several results on how these particular stages change elections, including the effect parties may have on the quality of winners, how population distribution affects the possibility of manipulation of the districts (\"gerrymandering power”), and some new results showing gerrymandering effects in real-world data.\n\nJoint work (in different papers) with Yoram Bachrach, Allan Borodin, Yoad Lewenberg, Jeffrey S. Rosenschein, Nisarg Shah, Tyrone Strangway, and Yair Zick.\n\nAbout the Speaker: Omer Lev is a faculty member at the department of Industrial Engineering and Management in Ben-Gurion University of the Negev. Before joining Ben-Gurion University, he did his post doctoral fellowship at the department of Computer Science of the University of Toronto, following his PhD at the Hebrew University. Omer is interested in various areas of AI and theoretical computer science, mostly involving a game-theoretical analysis trying to understand various phenomena, while keeping the research closely related to real-world data and observable behavior. Lately, his focus has been on multi-staged decision mechanisms, peer evaluation, and crowd-activities.\n\nFeb 24\n\nData Science for Software Engineering\n\nTung Thanh Nguyen | Auburn University\n\nThis talk will be held on Friday, February 24th, in Boggs 102 at 1:00 p.m. Please note the special weekday, venue, and time for this event.\n\nAbstract: I believe that software and data are changing the world, as said by famous quotes \"Software is eating the world\" and \"Data is feeding the world\". Therefore, I am doing research on software and data, especially, how to use data to help people build software faster, smarter, and safer. In this talk, I will present our intelligent tool for code completion as a representative example. Our tool uses hidden Markov models to learn programming patterns from more than 200,000 mobile apps. It then uses those models to provide context-sensitive method call suggestions. The empirical evaluation shows that it is highly effective with top-3 accuracy of 90%.\n\nAbout the Speaker: Dr. Tung Nguyen is an Assistant Professor from the Department of Computer Science and Software Engineering at Auburn University. He has a PhD in Computer Engineering from Iowa State University. He has more than 10 years of research and teaching experience in Software Engineering and Data Science. He has published nearly 70 peer-reviewed papers at the top venues in the field and won three Best Paper Awards.\n\nFeb 28\n\nEnabling Visualization of Data Analysis Programs\n\nRebecca Faust | Virginia Tech\n\nThis talk will be held on Tuesday, February 28th, at 12:45 p.m. in Stanley Thomas 316. Please note the special weekday for this event.\n\nAbstract: Data analysis methods inherently lose information when moving from initial data to final results, such as context from the underlying data features. The loss of such information inhibits analysts' ability to reason about the processes performed on their data and interpret the results. In this talk, I will discuss my efforts to design visualization and interaction methods that re-expose some of the information lost during analysis. First, I will focus on using visualization to re-introduce underlying data features in dimension reductions. I will demonstrate how my DimReader method provides lost context of the underlying data features via gradient visualization. Additionally, I will discuss how explainable, interactive projections provide context while enabling data exploration through feature-based reorganization. Then, I will broaden the scope and illustrate how visualizing analysis scripts, via my Anteater method, provides context from the internal script data that illuminates how the analysis reaches its final results.\n\nAbout the Speaker: Rebecca Faust is a postdoctoral researcher and Computing Innovations Fellow in the Sanghani Center for AI and Data Analytics at Virginia Tech working with Dr. Chris North. She received her Ph.D. from the University of Arizona, where she was advised by Dr. Carlos Scheidegger. Her research interests lie at the intersection of data visualization and data analysis, with an emphasis on designing interactive visualization methods to enable explainability and interpretability of analysis methods and results.\n\nMar 2\n\nDesigning AI-Based Applications to Benefit Deaf and Hard-of-Hearing Individuals and Sign-Language Users: A Human–Computer Interaction Perspective\n\nSaad Hassan | Rochester Institute of Technology\n\nThis talk will be held on Thursday, March 2nd, at 12:45 p.m. in Stanley Thomas 316.\n\nAbstract: Recent advancements in speech and language-based technologies have opened up new opportunities to design innovative technologies for Deaf and hard-of-hearing people and other sign-language users. In this talk, I will discuss my research at the intersection of accessible computing, human-computer interaction (HCI), and computational linguistics. This includes three main areas of research: sign-language interfaces, caption evaluation and enhancements, and detection of intersectional ableist biases in language models. To investigate designing and evaluating sign-language technologies, I have used HCI methodologies such as interviews, prototyping, task-based experiments, and observational studies. DHH users face challenges during conversational interaction or when viewing media, and individuals who are learning sign-language face challenges in looking up the meaning of unfamiliar sign-language words. To address the challenges experienced by both these groups, I have explored the best ways to structure the user experience, given the limitations of current sign-recognition technologies. This talk will focus on two systems I have developed for learners and two systems for DHH individuals, and I will share findings from studies that revealed how these systems benefit sign-language users. This research required collaboration with AI researchers, two Deaf professional organizations, and industry research partners. Finally, I discuss my future plans to address barriers faced by people with disabilities when using technologies or accessing information, to support their social interactions in digital spaces.\n\nAbout the Speaker: Saad Hassan (https://saadh.info/) is a Ph.D. candidate in the Computing and Information Science program at Rochester Institute of Technology; his research focuses on Human-computer Interaction, Computing Accessibility, and AI for Social Good. This involves designing and evaluating linguistic and speech technologies to benefit individuals with disabilities, with a focus on deaf or hard-of-hearing (DHH) individuals. He has also worked on the critical and interpretive analysis of language models to uncover intersectional biases against individuals within language models. During his graduate studies, he has worked as a research scientist intern and visiting researcher with the sign-language understanding group at Google AI and audio experiences and central accessibility teams at Meta Reality Labs, where he helped build, evaluate, and deploy innovative technologies. Saad has published research at highly selective computing venues, including ACM Conference on Human Factors in Computing Systems (CHI), ACM SIGACCESS Conference on Computers and Accessibility (ASSETS), ACM Transactions on Accessible Computing (TACCESS), and Conference on Empirical Methods in Natural Language Processing (EMNLP). In 2022, Saad was awarded a Duolingo dissertation research grant to support his future research on language learning with technology. The same year, he received a best paper nomination at ASSETS. He has served on the program committees for ASSETS and CHI. Saad has served as a teaching assistant for several graduate and undergraduate courses and mentored more than 22 undergraduate and graduate students.\n\nApr 11\n\nMachine Learning and Physically Based Modeling Application to Coastal and Riverine Environments\n\nEhab Meselhe, Kelin Hu, and Laura Manuel | River-Coastal Science and Engineering, Tulane University\n\nThis talk will be held on Tuesday, April 11th, at 12:45 p.m. in Stanley Thomas 316. Please note the special weekday for this event.\n\nAbstract: Coastal and riverine environments are facing challenges due to a host of natural and anthropogenic factors. Climate change, aggressive urbanization, and improperly designed grey infrastructure are examples of factors that impacting natural systems. Computer models are widely used to improve our understanding of system dynamics, evaluate restoration strategies, develop long term plans. They are also used for real time forecasting. Advancements in computer modeling include integration of physical, biological and socioeconomic components. These complex models, especially when applied to large spatial scales and for long-term simulations, require substantial computational resources. Machine learning tools are an attractive tool for environmental applications to supplement physically-based models. In this presentation, we share modeling applications for hurricanes, compound flooding, and coastal restoration strategies. We will examine impacts on marsh creation, health of marine mammals, sustainability of oysters, and land loss rates.\n\nAbout the Speakers: Laura Manuel is a 3rd year PhD student at the River-Coastal Science and Engineering (RCSE) Department. Her research focus is on real-time forecasting and evaluation of restoration strategies. Kelin Hu is an Assistant Professor-Research at RCSE. He is an expert in coastal modeling applications especially storm surge, hydrodynamics, waves, and water quality. Ehab Meselhe is a Professor at RCSE. His research focus is development and application of computer models to coastal, deltaic and riverine systems.\n\nApr 13\n\nVisual Cross-Domain Adaption Under Various Data Access Privileges\n\nHaifeng Xia | Computer Science PhD Student, Tulane University\n\nPlease join us for Haifeng Xia’s PhD dissertation defense talk, which is described below.\n\nThis talk will be held on Thursday, April 13th, at 2:00 p.m. CST in Gibson Hall, Room # 308. Please note the special time and venue for this event.\n\nAbstract: Deep neural networks have achieved tremendous progress on solving computer vision tasks such as image classification and object detection by training model on large-scale label-sufficient dataset. However, the manual annotations on abundant instances are laborious and expensive and distribution shift across training (source) and test (target) sets significantly reduces model performance. These causes hinder the widespread application of deep learning techniques. To breakthrough this bottleneck, transfer learning is a feasible and effective solution paradigm by borrowing and adapting the existing source knowledge into target domain with the similar tasks. This thought facilitates the exploration on unsupervised domain adaptation using source and target samples to fulfill cross-domain alignment. Although UDA methods have indicated powerful ability on overcoming cross-domain learning problem, accessing data of all domains is unrealistic and impossible in real-world industrial applications. Hence, this dissertation mainly discusses domain adaptation under various data access privileges and effectively overcomes these scenarios including source-free domain adaptation, imbalanced domain generalization and incomplete multi-view domain adaptation.\n\nKeywords: Domain Adaptation, Data Access Privileges, Knowledge Transfer\n\nApr 21\n\nTitle: VR Implementations in Computer Science\n\nGulsebnem Bishop| Campbellsville University\n\nThis talk will be held on Friday, April 21st, at 3:00 p.m. Venue: TBA. Please note the special weekday for this event.\n\nAbstract: The most challenging aspect of teaching is still delivering the course material to the students in an environment where engagement and learning take place. So many new technologies are at our disposal to use, and we are reluctant to use them for many reasons. Implementing new technologies is scary. As instructors, we think we need to have complete mastery of these technologies before we even think about implementing them in our classrooms. We try to learn as time permits. We do our best to develop new and exciting solutions to eliminate boredom and promote student interaction and excitement. We get lost. We quit. In this presentation, Dr. Bishop talks about how she started implementing VR in her classroom, why it is important to use these new technologies and how we can implement them in computer science education as well as other disciplines. We will have time to do a demo session of a classroom environment using a visor at the end of the talk.\n\nAbout the Speaker: Dr. Gulsebnem (Sheb) Bishop is currently teaching at Campbellsville University, as an Assistant Professor of Computer Information Systems, in the Department of Business, Economics, and Technology. She graduated with a doctorate in Computer Science and Information Systems from Pace University, NY, in 2006. She holds an MBA and a Cybersecurity degree from Stratford University, VA. She is a certified PMP and serves as an at-large board member in the Syracuse PMI chapter. Her area of interest is software development, design and security, data warehousing and database administration, and data analytics. Dr. Bishop worked in non-profit IT, specifically in database administration, for over thirteen years and in other IT areas, such as web design, systems integration/conversion, and IT project management. She worked for several non-profits in New York and Washington, DC (American Museum of Natural History, NAACP, Legal Defense and Education Fund, and Human Rights Campaign). Dr. Bishop is currently working on incorporating VR technology into her own classes and experimenting with different platforms.\n\nApr 25\n\nComputational Models of User Engagement with Online News\n\nKarthik Shivaram | Computer Science PhD Student, Tulane University\n\nPlease join us for Karthik Shivaram’s PhD dissertation defense talk, which is described below.\n\nThis talk will be held on Tuesday, April 25th, at 1:00 p.m. CST in Stanley Thomas Hall, Room # 316. Please note the special date and time for this event.\n\nAbstract: The shift from traditional print media to online platforms has revolutionized the way people consume and engage with current events. To enhance user involvement, these platforms typically employ personalization algorithms like recommendation systems, that learn about users’ preferences from their past interactions and suggest relevant content. Nevertheless, the use of such algorithms may result in biased engagement patterns caused by confounded data, leading to concerns about \"filter bubbles\" and \"echo chambers\". Such entities cause users to be over-exposed to information that conforms with their pre-existing beliefs while limiting exposure to opposing viewpoints. As a result, this type of news consumption habits can bias users, leading to negative consequences such as the hyper- partisanship, online polarization, and the spread of misinformation. In this dissertation we aim to better understand factors that affect short-term and long-term news engagement behavior on social media. To achieve this, we conduct simulation studies to understand which aspects of recommendation systems contribute to filter bubble formation. We propose attention-based neural networks to mitigate these effects in content-based recommenders. In addition, long-term news engagement behavior is examined by analyzing observational data collected from Twitter over a decade. Our analysis focuses on a specific type of engagement behavior where users exhibit distrust towards the news media they engage with and examine its impact on engagement diversity. Finally, we propose forecasting methods to predict future news engagement behavior of users which reveal factors that shape long-term news consumption habits on social media.\n\nApr 27\n\nAugmented Reality for Computer Science Education\n\nSing Chun Lee | Johns Hopkins University\n\nThis talk will be held on Thursday, April 27th, at 1:00 p.m. in Stanley Thomas 316. Please note the special time for this event.\n\nAbstract: Augmented Reality (AR) has revolutionized how we interact with the world. By overlaying virtual digital content into the real world, AR enriches and supplements our sensing, opening up new ways to interact with the environment around us. Nowadays, we see AR everywhere, such as in the football game on TV (the virtual 10-yard line), in entertainment (Pokemon Go), and in healthcare (enhanced medical data visualization), etc. In education, AR promotes active learning and improves students’ engagement. In this talk, we will first see some examples of AR and discuss two key components of successful AR applications – calibration/registration and user acceptance. Then, we will explore the directions for applying AR to computer science education – where and how we should integrate AR to improve learning outcomes and make learning more fun.\n\nAbout the Speaker: Sing Chun Lee is a PhD Candidate at Johns Hopkins University, where he discovered his passion for teaching and is interested in integrating research into undergraduate education. His research focuses on Augmented Reality and Geometry Processing, specifically on intuitive data augmentation and geometric calculus. At Hopkins, he received Professor Joel Dean Excellence in Teaching Award in 2022, recognizing his effort and dedication to improving undergraduate computer science education. Besides teaching and research, he immerses himself in virtual reality escape room games.\n\nFall 2022 Colloquia\n\nSept 6\n\nIntroduction to NSF OAC Research Programs Supporting Software and Data Cyberinfrastructure Development\n\nSeung-Jong Jay Park | Louisiana State University/National Science Foundation\n\nThis talk will be held on Tuesday, September 6th, at 1:00 p.m. in Boggs 600. Please note the special weekday, time, and venue for this event.\n\nAbout the Speaker: Dr. Seung-Jong Jay Park is the Dr. Fred H. Fenn Memorial Professor of Computer Science and Engineering at Louisiana State University, where he has worked in cyberinfrastructure development for large-scale scientific and engineering applications since 2004. He received Ph.D. in the school of Electrical and Computer Engineering from the Georgia Institute of Technology. He has performed interdisciplinary research projects including (1) big data and deep learning research including developing software frameworks for large-scale science applications; and (2) cyberinfrastructure development using cloud computing, high-performance computing, and high-speed networks. Those projects have been supported by federal and state funding programs including NSF, NASA, NIH, ONR, and AFRL. He received IBM faculty research awards between 2015-2017. He also served an associate director for the Center for Computation and Technology of LSU between 2016-2018. Since 2021 he has served at the U.S. National Science Foundation (on leave from LSU) as a program director managing research support programs, such as Cyberinfrastructure for Sustained Scientific Innovation (CSSI), Principles and Practice of Scalable Systems (PPoSS), Computational and Data-Enabled Science and Engineering (CDS&E), and others.\n\nNov 28\n\nRecent Advances in Robust Machine Learning\n\nMasashi Sugiyama| RIKEN/The University of Tokyo\n\nThis talk will be held on Monday, November 28th, at 11:00 a.m. in Boggs 600. Please note the special weekday, time, and venue for this event.\n\nAbstract: When machine learning systems are trained and deployed in the real world, we face various types of uncertainty. For example, training data at hand may contain insufficient information, label noise, and bias. In this talk, I will give an overview of our recent advances in robust machine learning, including weakly supervised classification (positive-unlabeled classification, positive-confidence classification, complementary-label classification, etc), noisy label learning (noise transition estimation, instance-dependent noise, clean sample selection, etc.), and domain adaptation (joint importance-predictor learning for covariate shift adaptation, dynamic importance-predictor learning for full distribution shift, etc.).\n\nAbout the Speaker: Masashi Sugiyama received a Ph.D. in Computer Science from Tokyo Institute of Technology in 2001. He has been a Professor at the University of Tokyo since 2014 and concurrently Director of the RIKEN Center for Advanced Intelligence Project (AIP) since 2016. His research interests include theories and algorithms of machine learning. He served as Program Co-chairs for Neural Information Processing Systems (NeurIPS) Conference in 2015, International Conference on Artificial Intelligence and Statistics (AISTATS) in 2019, and Asian Conference on Machine Learning (ACML) in 2010 and 2020. He (co)authored Machine Learning in Non-Stationary Environments (MIT Press, 2012), Density Ratio Estimation in Machine Learning (Cambridge University Press, 2012), and Machine Learning from Weak Supervision (MIT Press, 2022).\n\nDec 8\n\nThe National Science Data Fabric: Democratizing Data Access for Science and Society\n\nValerio Pasucci| University of Utah\n\nThis talk will be held at 3:30 p.m. in Stanley Thomas 302. Please note the special time for this event.\n\nAbstract: Effective use of data management techniques for the analysis and visualization of massive scientific data is a crucial ingredient for the success of any experimental facility, supercomputing center, or cyberinfrastructure that supports data-intensive scientific investigations. Data movements have become a central component that can enable or stifle innovation in the progress towards high-resolution experimental data acquisition (e.g., APS, SLAC, NSLS II). However, universal data delivery remains elusive, limiting the scientific impacts of these facilities. This is particularly true for high-volume/high-velocity datasets and resource-constrained institutions.\n\nThis talk will present the National Science Data Fabric (NSDF) testbed, which introduces a novel trans-disciplinary data fabric integrating access to and use of shared storage, networking, computing, and educational resources. The NSDF technology addresses the key data management challenges involved in constructing complex streaming workflows that take advantage of data processing opportunities that may arise while data is in motion. This technology finds practical use in many research and industrial applications, including materials science, precision agriculture, ecology, climate modeling, astronomy, connectomics, and telemedicine.\n\nThis NSDF overview will include several techniques that allow building a scalable data movement infrastructure for fast I/O while organizing the data in a way that makes it immediately accessible for processing, analytics, and visualization with resources from Campus Computing Cybeinfrastructures, the Open Storage Network, the Open Science Grid, NSF/DOE leadership computing facilities, the CloudLab, Camelion, and Jetstream, just to name a few. For example, I will present a use case for the real-time data acquisition from an Advanced Photon Source (APS) beamline to allow remote users to monitor the progress of an experiment and direct integration in the Materials Commons community repository. We accomplish this with an ephemeral NSDF installation that can be instantiated via Docker or Singularity at the beginning of the experiment and removed right after. In general, the advanced use of containerized applications with automated deployment and scaling makes the practical use of clients, servers, and data repositories straightforward in practice, even for non-expert users. Full integration with Python scripting facilitates the use of external libraries for data processing. For example, the scan of a 3D metallic foam can be easily distributed with the following Jupyter notebook https://tinyurl.com/bdzhf2nx.\n\nOverall, this leads to building flexible data streaming workflows for massive imaging models without compromising the interactive nature of the exploratory process, the most effective characteristic of discovery activities in science and engineering. The presentation will be combined with a few live demonstrations of the same technology including notebooks which are being used to provide undergraduate students of a minority-serving institution (UTEP) with real-time access to large-scale data normally used only by established scientists in well-funded research groups. About the Speaker: Valerio Pascucci is the Inaugural John R. Parks Endowed Chair, the founding Director of the Center for Extreme Data Management Analysis and Visualization (CEDMAV), a Faculty of the Scientific Computing and Imaging Institute, and a Professor of the School of Computing of the University of Utah. Valerio is also the President of ViSOAR LLC, a University of Utah spin-off, and the founder of Data Intensive Science, a 501(c) nonprofit providing outreach and training to promote the use of advanced technologies for science and engineering. Before joining the University of Utah, Valerio was the Data Analysis Group Leader of the Center for Applied Scientific Computing at Lawrence Livermore National Laboratory and an Adjunct Professor of Computer Science at the University of California, Davis. Valerio's research interests include Big Data management and analytics, progressive multi-resolution techniques in scientific visualization, discrete topology, and compression. Valerio is the coauthor of more than two hundred refereed journal and conference papers and was an Associate Editor of the IEEE Transactions on Visualization and Computer Graphics.\n\nAbout the Speaker: Valerio Pascucci is the Inaugural John R. Parks Endowed Chair, the founding Director of the Center for Extreme Data Management Analysis and Visualization (CEDMAV), a Faculty of the Scientific Computing and Imaging Institute, and a Professor of the School of Computing of the University of Utah. Valerio is also the President of ViSOAR LLC, a University of Utah spin-off, and the founder of Data Intensive Science, a 501(c) nonprofit providing outreach and training to promote the use of advanced technologies for science and engineering. Before joining the University of Utah, Valerio was the Data Analysis Group Leader of the Center for Applied Scientific Computing at Lawrence Livermore National Laboratory and an Adjunct Professor of Computer Science at the University of California, Davis. Valerio's research interests include Big Data management and analytics, progressive multi-resolution techniques in scientific visualization, discrete topology, and compression. Valerio is the coauthor of more than two hundred refereed journal and conference papers and was an Associate Editor of the IEEE Transactions on Visualization and Computer Graphics.\n\nDec 13\n\nInterdisciplinary Project Presentations\n\nFang Qi, Linsen Li, and Xiaolin Sun | Computer Science PhD Students, Tulane University\n\nThis event will be held at 12:30 p.m. in Stanley Thomas 302. Please note the special weekday for this event.\n\nFang Qi\n\nQuantum Vulnerability Analysis to Accurately Estimate the Quantum Algorithm Success Rate\n\nAbstract: Quantum technology is still in its infancy, but superconducting circuits have made great progress toward pushing forward the computing power of the quantum state of the art. Due to limited error characterization methods and temporally varying error behavior, quantum operations can only be quantified to a rough percentage of successful execution, which fails to provide an accurate description of real quantum execution in the current noisy intermediate-scale quantum (NISQ) era. State-of-the-art success rate estimation methods either suffer from significant prediction errors or unacceptable computation complexity. Therefore, there is an urgent need for a fast and accurate quantum program estimation method that provides stable estimation with the growth of the program size. Inspired by the classical architectural vulnerability factor (AVF) study, we propose and design Quantum Vulnerability Factor (QVF) to locate any manifested error which generates Cumulative Quantum Vulnerability (CQV) to perform SR prediction. By evaluating it with well-known benchmarks on three 27-qubit and one 65-qubit quantum machines, CQV outperforms the state-of-the-art prediction technique ESP by achieving on average 6 times less relative prediction error, with best cases at 20 times, for benchmarks with a real SR rate above 0.1%.\n\nLinsen Li\n\nOnline Reviews Are Leading Indicators of Changes in K-12 School Attributes\n\nAbstract: School rating websites are increasingly used by parents to assess the quality and fit of U.S. K-12 schools for their children. These online reviews often contain detailed descriptions of a school’s strengths and weaknesses, which both reflect and inform perceptions of a school. Existing work on these text reviews has focused on finding words or themes that underlie these perceptions, but have stopped short of using the textual reviews as leading indicators of school performance. In this paper, we investigate to what extent the language used in online reviews of a school is predictive of changes in the attributes of that school, such as its socio-economic makeup and student test scores. Using over 300K reviews of 70K U.S. schools from a popular ratings website, we apply language processing models to predict whether schools will significantly increase or decrease in an attribute of interest over a future time horizon. We find that using the text improves predictive performance significantly over a baseline model that does not include text but only the historical time-series of the indicators themselves, suggesting that the review text carries predictive power. A qualitative analysis of the most predictive terms and phrases used in the text reviews indicates a number of topics that serve as leading indicators, such as diversity, changes in school leadership, a focus on testing, and school safety.\n\nXiaolin Sun\n\nPandering in a (Flexible) Representative Democracy\n\nAbstract: In representative democracies, the election of new representatives in regular election cycles is meant to prevent corruption and other misbehavior by elected officials and to keep them accountable in service of the “will of the people.\" This democratic ideal can be undermined when candidates are dishonest when campaigning for election over these multiple cycles or rounds of voting. Much of the work on COMSOC to date has investigated strategic actions in only a single round. We introduce a novel formal model of pandering, or strategic preference reporting by candidates seeking to be elected and examine the resilience of two democratic voting systems to pandering within a single round and across multiple rounds. The two voting systems we compare are Representative Democracy (RD) and Flexible Representative Democracy (FRD). For each voting system, our analysis centers on the types of strategies candidates employ and how voters update their views of candidates based on how the candidates have pandered in the past. We provide theoretical results on the complexity of pandering in our setting for a single cycle, formulate our problem for multiple cycles as a Markov Decision Process, and use reinforcement learning to study the effects of pandering by both single candidates and groups of candidates across a number of rounds.\n\nPrevious Colloquia\n\nSpring 2022 Colloquia\n\nJanuary 24\n\nAI-Driven Big Data Analytics\n\nJustin Zhan | University of Arkansas\n\nAbstract: Data has become the central driving force to new discoveries in science, informed governance, insight into society, and economic growth in the 21st century. Abundant data is a direct result of innovations including the Internet, faster computer processors, cheap storage, the proliferation of sensors, etc, and has the potential to increase business productivity and enable scientific discovery. However, while data is abundant and everywhere, people do not have a fundamental understanding of data. Traditional approaches to decision making under uncertainty are not adequate to deal with massive amounts of data, especially when such data is dynamically changing or becomes available over time. These challenges require novel techniques in AI-driven data analytics, In this seminar, a number of recent funded AI-driven big data analytics projects will be presented to address various data analytics, mining, modeling and optimization challenges.\n\nAbout the Speaker: Dr. Justin Zhan is the Arkansas Research Alliance Scholar and Professor of Data Science at the Department of Computer Science and Computer Engineering, University of Arkansas. He is the Director of Data Science, Arkansas Integrative Metabolic Research Center. He is a joint professor at the Department of Biomedical Informatics, School of Medicine, University of Arkansas for Medical Sciences. He received his PhD degree from University of Ottawa, Master degree from Syracuse University, and Bachelor degree from Liaoning University of Engineering and Technology. His research interests include Data Science, Biomedical Informatics, Deep Learning & Big Data Analytics, Cyber Security & Blockchain, Network Science & Social Computing. He has served as a conference general chair, a program chair, a publicity chair, a workshop chair, or a program committee member for over one-hundred and fifty international conferences and an editor-in-chief, an editor, an associate editor, a guest editor, an editorial advisory board member, or an editorial board member for about thirty journals. He has published 246 articles in peer-reviewed journals and conferences and delivered 30 keynote speeches and invited talks. His research has been extensively funded by National Science Foundation, Department of Defense, and National Institute of Health.\n\nJanuary 28\n\nThe Privacy Policy Permission Model: A Methodology for Modeling Privacy Policies\n\nMaryam Majedi |University of Calgary\n\nThis event will be held on Friday, January 28th. Please note the special weekday for this event.\n\nAbstract: Organizations use privacy policies to communicate their data collection practices to their clients. A privacy policy is a set of statements that specifies how an organization gathers, uses, discloses, and maintains clients' data. However, most privacy policies lack a clear, complete explanation of how data providers' information is used. In this talk, I will present a modeling methodology, called the Privacy Policy Permission Model (PPPM), that provides a uniform, easy-to-understand representation of privacy policies, which can show how data is used within an organization's practice.\n\nAbout the Speaker: Dr. Maryam Majedi completed a teaching stream postdoc at the University of Toronto, where she worked with the Embedded Ethics Education Initiative (E3I) team and developed and delivered ethics modules for computer science courses. Dr. Majedi completed her Ph.D. in data privacy at the University of Calgary, where she introduced a new technique to model privacy policies. She holds a M.Sc. in High-Performance Scientific Computing from the University of New Brunswick and a Fellowship in Medical Innovation from Western University.\n\nJanuary 31\n\nDesigning the Next Generation of User Interfaces for Children\n\nJulia Woodward | University of Florida\n\nAbstract: Children are increasingly interacting with technology, such as touchscreen devices, at home, in the classroom, and at museums. However, these devices are not designed to take into account that children interact with devices differently than adults. As children’s everyday use of technology increases, these devices need to be tailored towards children. In this talk, I will present research exploring the differences between how children and adults interact and think about different technology. Our findings lead to a better understanding of how to design technology for children. I will also present some recent work examining how to design information in augmented reality (AR) headsets for both children’s and adults’ task performance. I will conclude with some takeaways and plans for future work in designing the next generation of user interfaces for children.\n\nAbout the Speaker: Julia Woodward is a Doctoral Candidate studying Human-Centered Computing in the Department of Computer and Information Science and Engineering at the University of Florida, as well as a National Science Foundation Graduate Research Fellow. Her main research areas include examining how to design better user interfaces tailored towards children, and understanding how children think about and use technology. Through her research, she has identified specific differences between how adults and children interact with technology and has provided recommendations for designing technology for children. Her current dissertation work focuses on understanding how to design information in augmented reality (AR) headsets to aid in adults’ and children’s task performance and how it differs between the two populations. Julia is graduating this year and plans to continue researching and designing technology tailored for children.\n\nFebruary 4\n\nPersonality and Emotion in Strong-Story Narrative Planning\n\nAlireza Shirvani |Saint Louis University\n\nThis talk will be held online only on Friday, February 4th, at 4:00 p.m. CST. Please note the special weekday for this event. Zoom details will be provided via the announcement listserv, or you may email dramil1@tulane.edu to request the corresponding link.\n\nAbstract: Interactive virtual worlds provide an immersive and effective environment for training, education, and entertainment purposes. Virtual characters are an essential part of every interactive narrative. I propose models of personality and emotion that are highly domain independent and integrate those models into multi-agent strong-story narrative planning systems. My models of emotion and personality enable the narrative generation system to create more opportunities for players to resolve conflicts using certain behavior types. In doing so, the author can encourage the player to adopt and exhibit those behaviors.\n\nAbout the Speaker: Dr. Alireza Shirvani is a visiting professor at Saint Louis University in the Department of Computer Science. He received his PhD in Computer Science from the University of Kentucky in 2021. His research focuses on Computational Narrative, with a more general interest in Artificial Intelligence for Games. He is particularly interested in generating believable behavior by integrating emotion and personality into virtual characters. One of his major projects, called Camelot, provides a free easy-to-use 3D tool to visualize interactive stories. This engine acts as a presentation layer to an external program, called the experience manager, which can be written in any programming language.\n\nFebruary 14\n\nOvercoming Heterogeneity in Autonomous Cyber-Physical Systems\n\nIvan Ruchkin |University of Pennsylvania\n\nAbstract: From autonomous vehicles to smart grids, cyber-physical systems (CPS) play an increasingly important role in today's society. Often, CPS operate autonomously in highly critical settings, and thus it is imperative to engineer these systems to be safe and trustworthy. However, it is particularly difficult to do so due to CPS heterogeneity -- the high diversity of components and models used in these systems. This heterogeneity substantially contributes to fragmented, incoherent assurance as well as to inconsistencies between different models of the system.\n\nThis talk will present two complementary techniques for overcoming CPS heterogeneity: confidence composition and model integration. The former technique combines heterogeneous confidence monitors to produce calibrated estimates of the run-time probability of safety in CPS with machine learning components. The latter technique discovers inconsistencies between heterogeneous CPS models using a logic-based specification language and a verification algorithm. The application of these techniques will be demonstrated on an unmanned underwater vehicle and a power-aware service robot. These techniques serve as stepping stones towards the vision of engineering autonomous systems that are aware of their own limitations.\n\nAbout the Speaker: Ivan Ruchkin is a postdoctoral researcher in the PRECISE center at the University of Pennsylvania. He received his PhD in Software Engineering from Carnegie Mellon University. His research develops integrated high-assurance methods for modeling, analyzing, and monitoring modern cyber-physical systems. His contributions were recognized with multiple Best Paper awards, a Gold Medal in the ACM Student Research Competition, and the Frank Anger Memorial Award for crossover of ideas between software engineering and embedded systems. More information can be found at https://www.seas.upenn.edu/~iruchkin.\n\nFebruary 21\n\nUsing Kernel-level Data Provenance for Intrusion Detection\n\nXueyuan (Michael) Han |Harvard University\n\nAbstract: Attacks today are increasingly difficult to detect and their damage continues to skyrocket. For example, it takes an average of over 200 days to identify a data breach and costs about $4 million to rectify. More than 18,000 organizations were affected in the late 2020 SolarWinds supply chain attack. Devastating attacks that make headlines (e.g., Equifax, Target, and Kaseya) are no longer isolated, rare incidents.\n\nIn this talk, I will present my work on leveraging kernel-level data provenance to detect system intrusions. Kernel-level data provenance describes system activity as a directed acyclic graph that represents interactions between low-level kernel objects such as processes, files, and sockets. I will describe CamFlow, an OS infrastructure that captures such provenance graphs with negligible performance overhead. I will then describe a host intrusion detection system (IDS), called Unicorn, that uses provenance graphs to detect particularly dangerous attacks called advanced persistent threats (APTs). APTs are the main cause of many of today’s large-scale data breaches. Unicorn applies machine learning to provenance graphs to identify system anomalies caused by APTs in real time without a priori attack knowledge.\n\nI will close the talk by discussing challenges and opportunities in provenance-based intrusion detection, including efforts to develop a robust IDS that not only provides timely anomaly detection, but also explains the manner in which an attack unfolds.\n\nAbout the Speaker: Xueyuan (Michael) Han is a computer science doctoral candidate advised by Professor James Mickens at Harvard University and Professor Margo Seltzer at the University of British Columbia. His research interests lie at the intersection of systems, security, and privacy. His work focuses on combining practical system design and machine learning to detect host intrusions, and designing language-level frameworks that respect user directives for handling private data. He has previously spent time at the University of Cambridge, Microsoft Research, and NEC Labs America. He is a Siebel Scholar and holds a B.S. in computer science from UCLA.\n\nMarch 7\n\nAutomatic Program Repair and Inconsistency Detection\n\nThibaud Lutellier |University of Waterloo\n\nAbstract: From bug detection to bug repair, software reliability is involved in all parts of the development cycle. Automation is desirable as it can reduce developers' time on these tasks and discover issues that would be hard to find manually. This job talk will present our recent advancements in automatic program repair and inconsistency detection. In the first part of the talk, I will introduce a new automatic program repair technique that uses ensemble learning and a new neural machine translation (NMT) architecture to automatically fix bugs in multiple programming languages. We then extend this work by introducing a pre-trains programming language model and a new code-aware search strategy. This extended approach outperforms all existing automatic program repair techniques on popular benchmarks. In the second part of the talk, I will explain how we propose a new automated inconsistency detection technique to find bugs in PDF readers and files and how we extended it to find bugs in another domain.\n\nAbout the Speaker: Thibaud Lutellier is a PostDoc Fellow in the Electrical and Computer Engineering Department at the University of Waterloo. His research interests lie at the crossroad between software engineering and artificial intelligence. His recent work includes proposing new AI-driven program repair techniques and new solutions for detecting bugs in deep learning libraries. He got an ACM SIGSOFT Distinguished Paper Award at ASE'20 for his work on analysing variance in DL training.\n\nApril 4\n\nMore Benign Point-Based Additive Manufacturing Through Process Planning\n\nLee Clemon |University of Technology Sydney\n\nThis talk will be held on Monday, April 4th, at 3:00 p.m. in Boggs 239. Please note the special time, and venue for this event.\n\nAbstract: Additive manufacturing is a rapidly growing consumer and commercial fabrication industry that may invert the design and manufacturing paradigm for many products. However, this suite of technologies is currently limited by slow cycle times, and a direct trade-off between throughput and precision. Current deposition methods rely on simple algorithms and sequential fabrication of each layer. Improvements in deposition planning can accelerate throughput and reduce resource use. We establish new approaches that leverage the structure of the intended model and relax unnecessary fabrication constraints to circumvent current speed limitations and maximize value adding operations. These efforts explore multiple algorithms to construct an improved toolpath. In addition, material use and energy consumption in additive manufacturing pose a challenge in production scale-up, particularly when considering climate change and waste generation. We characterize the resource intensity in current machines and by typical users to enable designers to make more informed decisions and identify opportunities for waste reduction. With these advances in deposition planning and enabled by multi-material fabrication we propose, new opportunities for creating circular economies leveraging additive manufacturing to give new live to waste materials. We then evaluate the structural implications of material sequestration to enable a redesign of products and product lifecycles for these circular economies. ----\n\nAbout the Speaker: Dr. Lee Clemon, P.E. is a research scientist in advanced manufacturing and high consequence design and licensed professional engineer. He focuses on the interplay of materials, design, and manufacturing for a more reliable and environmentally conscious industrial world. His current research interests are in process improvement and material property manipulation in advanced manufacturing processes, with an emphasis on additive and hybrid additive-subtractive manufacturing through particulate, wire, layer, and ensemble fabrication methods. He is a management member of the Centre for Advanced Manufacturing, program co-lead of the ARC Training Centre for Collaborative Robotics in Advanced Manufacturing, and member of the RF and Communications Laboratory. Lee also serves the mechanical engineering profession as an active volunteer for ASME providing professional development and training. Lee M Clemon holds a Ph.D. and a M.S. in Mechanical Engineering from the University of California at Berkeley, and a B.S. in Mechanical Engineering from the University of Kansas. He was previously a staff member at Sandia National Laboratories as a design and Research and Development engineer on hazardous substance processing systems and manufacturing process development. Lee became a Lecturer at the University of Technology Sydney, in the School of Mechanical and Mechatronic Engineering.\n\nMay 2\n\nA Life in Logic: Colloquium in Honor of Michael Mislove\n\nThis event will be held in honor of Prof. Michael Mislove, who is retiring from Tulane University after over 50 years of service. Please join us in celebrating his work and his time at Tulane.\n\nThis event will be held on Monday, May 2nd, from 4:00 p.m. - 6:30 p.m. (CDT) in Boggs 600. Please note the special venue for this event.\n\n4:00 p.m - Welcome and short intro, Carola Wenk (Computer Science) and Morris Kalka (Math)\n\n4:10 p.m. - 4:40 p.m.: Mike Mislove and Tulane, Some Reminiscences\n\nJimmie Lawson |Louisiana State University\n\nAbstract: Mike Mislove's professional career at Tulane has intersected significantly with mine over the years, and I would like to share some (by no means complete) reminiscences, recollections, and reflections from those years of his research, leadership, and service, with some important Tulane personalities forming a backdrop.\n\n4:40 p.m. - 5:10 p.m.: Additional Talks\n\nPeter Bierhost |University of New Orleans\n\nEllis Fenske |United States Naval Academy\n\n5:10 p.m.: Video\n\nReception with drinks and snacks to follow.\n\nThe event will also be available over Zoom for those that cannot attend in person. As referenced above, Zoom details will be provided via the announcement listserv, or you may email dramil1@tulane.edu to request the corresponding link.\n\nMay 11\n\nTitle: Security Analysis of Binary Code via Architectural and Compiler Support\n\nJiang Ming |University of Texas at Arlington\n\nThis talk will be held on Wednesday, May 11, at 4:00 p.m. in Stanley Thomas 302. Please note the special weekday and venue for this event.\n\nAbstract: Software security has become a hugely important consideration in all aspects of our lives. As software vulnerabilities and malware occupy a large portion of cyberattacks, automated security analysis of binary code is booming over the past few years. Binary code is everywhere, from IoT device firmware to malicious programs. However, binary code analysis is highly challenging due to binary code's very low-level and complicated nature. My research explores cross-disciplinary methodologies to effectively address the security problems in binary code. In this talk, I will first present my recent work on the security hardening of embedded systems (ASPLOS '22). We take advantage of architectural support to safely eliminate unused code of shared libraries on embedded systems. Our work can significantly reduce the code-reuse attacking surface with zero runtime overhead. Then, I will look into the most common source leading to binary code differences: compiler optimization. Our PLDI '21 awarded paper takes the first step to systematically studying the effectiveness of compiler optimization on binary code differences. We provide an important new viewpoint on the established binary diffing research area and challenge long-held optimization-resistance claims with compelling evidence.\n\nAbout the Speaker: Jiang Ming is an Assistant Professor in the Department of Computer Science and Engineering at the University of Texas at Arlington. He received his Ph.D. from Pennsylvania State University in 2016. His research interests span Software and Systems Security, with a focus on binary code analysis, hardware-assisted software security analysis, mobile systems security, and language-based security. His work has been published in prestigious conferences, including IEEE S&P, ASPLOS, PLDI, USENIX Security, ACM CCS, NDSS, ICSE, FSE, ASE, and MobiSys. Jiang has been funded by multiple NSF grants, Cisco research award, and UT System Rising STARs program. He was the recipient of UTA College of Engineering Outstanding Early Career Research Award, ACM SIGPLAN Distinguished Paper Award, and ACM SIGSOFT Distinguished Paper Nomination.\n\nMay 26\n\nTitle: Puzzles in Life, Work, and Computer Science\n\nMatthew Toups |University of New Orleans\n\nThis talk will be held on Thursday, May 26, at 4:00 p.m. in Stanley Thomas 302. Please note the special weekday and venue for this event.\n\nAbstract: Jigsaw puzzles have been produced for centuries, and printed puzzles have been a daily part of newspapers since the early 20th century. Puzzles can provide lightweight recreation, can serve as a distraction during the recent pandemic, or can even be the subject of rigorous computational complexity analysis. Puzzles and puzzle-solving also inform how I approach teaching Computer Science at the undergraduate level. Not only are puzzles a way to add both fun and challenge to courses, but more broadly I have several observations on CS pedagogy which I use puzzles to illustrate. Puzzles not only teach problem-solving, but they can motivate both through competition and co-operation, and can scale up and down in difficulty as needed. We can also step back from small puzzle pieces to examine the larger picture of what we want students to synthesize. I will also figuratively put together some puzzle pieces from my time as a student as a way to introduce my perspective on our discipline.\n\nAbout the Speaker: Matthew Toups, born and raised in New Orleans, holds a B.S. in Computer Science from Carnegie Mellon University and an M.S. from the University of New Orleans. Since 2016 he has served as I.T. Director for the University of New Orleans' Computer Science Department, providing a wide range of research and teaching technology needs. Additionally he has taught numerous undergraduate systems courses at UNO, and he sponsors a student cybersecurity competition team. He also enjoys solving puzzles.\n\nFall 2021 Colloquia\n\nOct 4\n\nComplex Proteoform Identification by Top-down Mass Spectrometry\n\nXiaowen Liu | Biomedical Informatics and Genomics Division, Tulane University\n\nAbstract: Mass spectrometry-based proteomics has been rapidly developed in the past decade, but researchers are still in the early stage of exploring the world of complex proteoforms, which are protein products with various primary structure alterations resulting from gene mutations, alternative splicing, post-translational modifications, and other biological processes. Proteoform identification is essential to mapping proteoforms to their functions and discovering novel proteoforms and new protein functions. Top-down mass spectrometry is the method of choice for identifying complex proteoforms because it provides a “bird’s eye view” of intact proteoforms. The combinatorial explosion of various alterations on a protein may result in billions of possible proteoforms, making proteoform identification a challenging computational problem. We propose to use mass graphs to efficiently represent proteoforms and design mass graph alignment algorithms for proteoform identification by top-down mass spectrometry. Experiments on top-down mass spectrometry data sets show that the proposed methods are capable of identifying complex proteoforms with various alterations.\n\nAbout the Speaker: Dr. Xiaowen Liu is a professor of bioinformatics in the Division of Biomedical Informatics and Genomics, Tulane University School of Medicine. He received his Ph.D. degree in computer science from the City University of Hong Kong in 2008. After 4-year postdoc training at the University of Western Ontario, the University of Waterloo, and the University of California, San Diego, Dr. Liu took positions as an Assistant Professor and Associate Professor at the Department of BioHealth Informatics, Indiana University-Purdue University Indianapolis from 2012 to 2021. Recently, he joined Tulane University School of Medicine. His research focuses on developing computational methods for analyzing mass spectrometry data, especially top-down mass spectrometry data. His lab developed TopPIC suite, a widely used software package for proteoform identification by top-down mass spectrometry.\n\nNov 8\n\nInterdisciplinary Project Presentations\n\nTianyi Xu, Taotao Jing, and Haifeng Xia | Computer Science PhD Students, Tulane University\n\nTianyi Xu\n\nContextual Bandits With Probing\n\nAbstract: In various practical applications from clinical trials to recommendation systems and anomaly detection, the problem of sequential decision-making is often encountered. Usually, each action (for example, the user's profile) has related information or context, and only the reward for the selected action is revealed (i.e., the bandit feedback). Since the reward is often a random variable, a statistical approach (contextual bandits) can be applied to solving these problems. Different from choosing one arm each time, we consider a novel extension of the bandit learning framework to incorporate joint probing and play. We assume that before the decision maker chooses an arm to play in each round, it can probe a subset of arms and observe their rewards (in that round). The decision maker then picks an arm to play according to the observations obtained in the probing stage and historical data. Our bandit learning model and its extensions can potentially be applied to a large body of sequential decision-making problems that involve joint probing and play under uncertainty. We will present an efficient algorithm for these problems and establish the regret bound using tools from online learning and statistics.\n\nTaotao Jing\n\nAugmented Multi-Modality Fusion for Generalized Zero-Shot Sketch-based Visual Retrieval\n\nAbstract: Augmented Multi-Modality Fusion for Generalized Zero-Shot Sketch-based Visual Retrieval Abstract: Zero-shot sketch-based image retrieval (ZS-SBIR) has attracted great attention recently, due to the potential application of sketch-based retrieval under the zero-shot scenario, where the categories of query sketches and gallery photo pool are not observed in the training stage. However, it is still under insufficient exploration for the general and practical scenario when the query sketches and gallery photos contain both seen and unseen categories. Such problem is defined as generalized zero-shot sketch-based image retrieval (GZS-SBIR), which is also the focus of this work. To this end, we propose a novel Augmented Multi-modality Fusion (AMF) framework to generalize seen concepts to unobserved one efficiently. Specifically, a novel knowledge discovery module named cross-domain augmentation is designed in both visual and semantic space to mimic novel knowledge unseen from the training stage, which is the key to handle GZS-SBIR challenge. Moreover, a triplet domain alignment module is proposed to couple the cross-domain distribution across photo and sketch in visual space. To enhance the robustness of our model, we explore embedding propagation to refine both visual and semantic features by removing undesired noise. Eventually, visual-semantic fusion representations are concatenated for further domain discrimination and task-specific recognition, which tends to trigger the cross-domain alignment in both visual and semantic feature space.In addition to popular ZS-SBIR benchmarks, a new evaluation protocol specifically designed for GZS-SBIR problem is constructed from DomainNet dataset with more diverse sub-domains, and the promising results demonstrate the superiority of the proposed solution over other baselines.\n\nHaifeng Xia\n\nPrivacy Protected Multi-Domain Collaborative Learning\n\nAbstract: Unsupervised domain adaptation (UDA) aims to transfer knowledge from one or more well-labeled source domains to improve model performance on the different-yet-related target domain without any annotations. However, existing UDA algorithms fail to bring any benefits to source domains and neglect privacy protection during data sharing. With these considerations, we define Privacy Protected Multi-Domain Collaborative Learning (P2MDCL) and propose a novel Mask-Driven Federated Network (MDFNet) to reach a “win-win” deal for multiple domains with data protected. First, each domain is armed with individual local model via a mask disentangled mechanism to learn domain-invariant semantics. Second, the centralized server refines the global invariant model by integrating and exchanging local knowledge across all domains. Moreover, adaptive self-supervised optimization is deployed to learn discriminative features for unlabeled domains. Finally, theoretical studies and experimental results illustrate rationality and effectiveness of our method on solving P2MDCL.\n\nNov 29\n\nHard Instances From Generalized Error Correcting Codes\n\nVictor Bankston | Department of Computer Science, Tulane University\n\nAbstract: Understanding the tractability of certain instances of NP-hard problems is an important issue in modern complexity theory. For example, when restricted to perfect graphs, Independent Set, $\\alpha(G)$ can be solved in polynomial time by computing a semidefinite relaxation, Lovasz's theta function $\\vartheta(G)$. To better understand the intractability of Independent Set, we seek families of instances of graphs for which $\\vartheta(G)$ and $\\alpha(G)$ differ to the largest possible extent. We propose a method for constructing such instances based on the fact that $\\vartheta(G) > \\alpha(G)$ may be viewed as a Bell Inequality. We present a toy model where measurements may be viewed as codes in $\\mathbb{F}_5^n$. We then study the Bell Inequalities using the techniques for analyzing error correcting codes. We then study the more natural Pauli measurements and observe that these also have the structure of a generalized error correcting code, an association scheme. Thus, we argue that we can construct hard instances of Independent Set by constructing 2-designs of Pauli measurements.\n\nDec 6\n\nAutomated Deep Learning for Open & Inclusive AI\n\nJun (Luke) Huan | StylingAI Inc.\n\nAbstract: Big Data, AI, and cloud computing are transforming our society. In areas such as game playing, image classification, and speech recognition, AI algorithms may have already surpassed human experts’capability. We are observing transformations AI and data science produce to industry sectors such as e-commerce, social-networking, finance, health, and transportation among others. My talk covers two parts: (1) a brief introduction to the Baidu AutoDL project where we use deep learning to design deep learning networks and (2) the applications of automated deep learning in different vertical areas including content generation and digital human.\n\nAbout the Speaker: Dr. Jun (Luke) Huan is the founder, president, and chief scientist of StylingAI Inc., a start-up aiming to developing and applying AI capabilities for automated content generation. Before that he served as a distinguished scientist and the head of Baidu Big Data Laboratory at Baidu Research. Before joining industry, he was the Charles E. and Mary Jane Spahr Professor in the EECS Department at the University of Kansas. From 2015-2018, Dr. Huan worked as a program director at the US NSF in charge of its big data program.\n\nDr. Huan works on Data Science, AI, Machine Learning and Data Mining. Dr. Huan's research is recognized internationally. He has published more than 150 peer-reviewed papers in leading conferences and journals and has graduated eleven Ph.D. students. He was a recipient of the NSF Faculty Early Career Development Award in 2009. His group won several best paper awards from leading international conferences. Dr. Huan service record includes Program Co-Chair of IEEE BIBM in 2015 and IEEE Big Data 2019.\n\nDec 8\n\nSpecial Joint Talk in Both the Algebraic Geometry and Geometric Topology Seminar of the Department of Mathematics and the Department of Computer Science Colloquium\n\nErin Wolf Chambers| Saint Louis University\n\nThis talk will be held on Wednesday, December 8th, at 3:00 p.m. in Gibson 310. Please note the special weekday, time, and venue for this event.\n\nReeb Graph Metrics From the Ground Up\n\nAbstract: The Reeb graph has been utilized in various applications including the analysis of scalar fields. Recently, research has been focused on using topological signatures such as the Reeb graph to compare multiple scalar fields by defining distance metrics on the topological signatures themselves. In this talk, we will introduce and study five existing metrics that have been defined on Reeb graphs: the bottleneck distance, the interleaving distance, functional distortion distance, the Reeb graph edit distance, and the universal edit distance. This talk covers material from a recent survey paper, which has multiple contributions: (1) provide definitions and concrete examples of these distances in order to develop the intuition of the reader, (2) visit previously proven results of stability, universality, and discriminativity, (3) identify and complete any remaining properties which have only been proven (or disproven) for a subset of these metrics, (4) expand the taxonomy of the bottleneck distance to better distinguish between variations which have been commonly miscited, and (5) reconcile the various definitions and requirements on the underlying spaces for these metrics to be defined and properties to be proven.\n\nAbout the Speaker: Dr. Erin Wolf Chambers is a Professor at Saint Louis University in the Department of Computer Science, with a secondary appointment in the Department of Mathematics. Her research focus is on computational topology and geometry, with a more general interest in combinatorics and algorithms. Complementing this work, she is also active in projects to support and improve the culture and climate in computer science and mathematics at all levels. She currently serves as editor for several journals, on the board of trustees for the Society for Computational Geometry, and on the SafeToC organizing committee and as an advocate. She received her PhD in Computer Science from the University of Illinois at Urbana-Champaign in 2008, and was a Visiting Research Professor at Saarland University in Summer 2011.\n\nDec 13\n\nArchitectural Support for Interdisciplinary Data Science Research\n\nLu Peng | Louisiana State University\n\nAbstract: The ever-increasing amount of global data introduces big challenges to computer systems in the aspect of performance, power consumption, reliability, and security. Deep learning and other advanced algorithms have been proposed to handle the problems in the layers of software, however, they require significant computing resources and memory bandwidth. In consequence, traditional CPU-based platforms are no longer the best choices for deploying these algorithms because they do not provide sufficient parallelism. Graphics Processing Units (GPUs) can provide improved performance but at the cost of higher power consumption. FPGAs and ASICs have garnered attention due to their application-specific nature, ability to achieve high degrees of parallelism, and high energy efficiency. In this talk, I will introduce our recent work in the computer system and architectural support for interdisciplinary data science research including hardware accelerator for deep neural networks, accelerator design for smart contracts processing, and an application of blockchain in contact tracing against COVID-19. Other recent work including adapting the B+ tree on Graphics Processing Units (GPUs) and improving resilience for Big Data kernels will be briefly introduced.\n\nAbout the Speaker: Lu Peng is the Gerard L. “Jerry” Rispone professor with the Division of Electrical and Computer Engineering, Louisiana State University, Baton Rouge, Louisiana. His research interests include computer systems and architecture focusing on many design issues on CPUs and GPUs, hardware accelerators, and applications for deep learning neural networks and blockchains. As PI or Co-PI, he has led or co-led several interdisciplinary research projects with collaborating researchers from different fields: Computer Science, Electrical Engineering, Statistics, Chemistry, Pathobiological Sciences, and Meteorology. His work has been supported by multiple federal and state agencies including NSF, NIH, NRL, DOE/LLNL, ORAU, NASA/LaSpace, BoR, and LSU RoC, as well as industrial companies including Chevron and Xilinx. He was a recipient of the ORAU Ralph E. Power junior faculty enhancement awards in 2007 and the Best Paper Award from IEEE IGSC in 2019 and IEEE ICCD in 2001.\n\nSummer 2021 Colloquia\n\nCheck back soon for more information on the computer science seminar series. If you would like to receive notices about upcoming seminars, you can subscribe to the announcement listserv. Unless otherwise noted, the seminars meet on Mondays at 4pm in Stanley Thomas 302.\n\nJuly 29\n\nDissertation Defense Talk\n\nMajid Mirzanezhad | Computer Science PhD Student, Tulane University\n\nPlease join us for Majid Mirzanezhad’s PhD dissertation defense as described below. This is our first in-person colloquium in a long time! While there is an option to join remotely, we sincerely hope you will be able to join in person. There will be a reception with snacks afterwards.\n\nThis talk will be held on Thursday, July 29th, at 2:00 p.m. CST in Stanley Thomas, Room # 302. Please note the special weekday and time for this event. This presentation will also be delivered online at https://tulane.zoom.us/j/99919478181?pwd=d2wvR0ltbjhWSUF6bVZ6VHIrSHVVZz09.\n\nAbstract: The rapid growth of the need for using Geographic Information Systems (GIS), for a better understanding of the environment, has led many researchers and practitioners of various disciplines to design efficient algorithmic methods for confronting the real-world problems arising in the realm of intelligent transportation systems, urban planning, mobility, surveillance systems, and other disciplines over the past few decades. In this dissertation, we consider several topics in computational geometry that involve applications in maps and networks in GIS. We first propose several algorithms that capture the similarity between linear features, notably curves, whose edges are relatively long. One of the popular metrics to capture the similarity between curves is the Fréchet distance. We give a linear-time greedy algorithm deciding and approximating the Fréchet distance and a near linear-time algorithm computing the exact Fréchet distance between two curves in any constant dimension. We also propose several efficient data structures for the approximate nearest-neighbor problem and distance oracle queries among curves under the Fréchet distance.\n\nWe exploit the metric studied above for simplification purposes. We specifically consider the problem of simplifying a feature, e.g., graph/tree/curve with an alternative feature of minimum-complexity such that the distance between the input and simplified features remains at most some threshold. We propose several algorithmic and NP-hardness results based on the distance measure we use and the vertex placement of the simplified feature that can be selected from the input's vertices, or its edges, or any points in the ambient space.\n\nAbout the Speaker: Majid Mirzanezhad is a Ph.D. candidate in the Department of Computer Science at Tulane University. His research area is on computational geometry with applications in GIS and primarily focused on approximation algorithms and data structures for curves and graphs. Prior to pursuing his Ph.D., Majid received his MSc and BSc in computer science from Amirkabir University of Technology (Tehran Polytechnic), Tehran, Iran.\n\nSpring 2021 Colloquia\n\nCheck back soon for more information on the computer science seminar series. If you would like to receive notices about upcoming seminars, you can subscribe to the announcement listserv. Unless otherwise noted, the seminars meet on Mondays at 4pm in Stanley Thomas 302. However, due to the current pandemic, all colloquia are being conducted virtually.\n\nApr 26\n\nInterdisciplinary Project Presentations\n\nKarthik Shivaram and Xintian Li | Computer Science PhD Students, Tulane University\n\nThese presentations will be delivered online. You may access the presentations on Monday, April 26th, at 4:00 pm CST via the following link: https://tulane.zoom.us/j/94777604484?pwd=MGNHUGNWb09va05NQ0taNDI1NDduUT09 . Meeting ID: 947 7760 4484. Passcode: 511369. Please be sure to mute your microphone when you log on.\n\nKarthik Shivaram\n\nCombating Partisan Homogenization in Content-Based News Recommendation Systems\n\nAbstract: Content-based news recommendation systems build user profiles to identify important terms and phrases that correlate with the user’s engagement to make accurate recommendations. Prior work by Ping et al. [1] suggests that these recommendation systems tend to have a homogenization effect when a user’s political views are diverse over a set of topics. In this work we propose a novel attention-based neural network architecture in a multitask learning setting to overcome this problem of partisan homogenization.\n\nXintian Li\n\nEvacuation Diffusion Modeling from Twitter\n\nAbstract: Evacuations have a significant impact on saving human lives during hurricanes. However, as a complex dynamic process, it is typically difficult to know the individual evacuation decisions in real time. Since a large amount of information is continuously posted through social media platforms from all populations, we can use them to predict individual evacuation behavior. In this project, we collect tweets during Hurricane Irma 2017, and train a text classifier in an active-learning way to identify tweets indicating positive evacuation decisions from both negative and irrelevant ones. We predict the demographic information for each identified evacuee, based on which we use time series modeling to predict evacuation rate changes over time. We also use the demographic information to help predict possible evacuees in different time ranges. The results can be used to help inform planning strategies of emergency response agencies.\n\nFall 2020 Colloquia\n\nOct 19\n\nInterdisciplinary Project Presentations\n\nDemi Qin and Akshay Mehra | Computer Science PhD Students, Tulane University\n\nThese presentations will be delivered online. You may access the presentations on Monday, October 19th, at 4:00 pm CST via the following link: https://tulane.zoom.us/j/95685637204 . Meeting ID: 956 8563 7204. Please be sure to mute your microphone when you log on.\n\nDemi Qin\n\nFast Prostate Cancer Diagnosis using Topological Data Analysis\n\nAbstract: Topological data analysis (TDA) is attracting increasing interest among researchers in machine learning due to the power of capturing shapes and structure in data. In this talk, we particularly consider biopsy image classification of prostate cancer with TDA that can utilize the topological summaries of images in machine learning tasks. We begin with the theoretical background of TDA and show our previous work on prostate cancer diagnosis by applying TDA in machine learning applications. Next, we define two aspects to improve the use of TDA: 1) A parallel computation pipeline of our previous work; 2) Comparing distance metric on topological summaries. Our results give new insights on when topological summaries could be more suitable and can be used to design better feature-based learning models with TDA.\n\nAkshay Mehra\n\nPenalty Method for Inversion-Free Deep Bilevel Optimization\n\nAbstract: Bilevel optimization problems are at the center of several important machine learning problems such as hyperparameter tuning, learning with noisy labels, meta-learning, and adversarial attacks. In this presentation, I will talk about our algorithm for solving bilevel problems using the penalty method and discuss its convergence guarantees and show that it has linear time and constant space complexities. Small space and time complexities of our algorithm make it an effective solver for large-scale bilevel problems involving deep neural networks. I will present results of the proposed algorithm on data denoising, few-shot learning, and data poisoning problems in a large-scale setting and show that it outperforms or is comparable to previously proposed algorithms based on automatic differentiation and approximate inversion in terms of accuracy, run-time and convergence speed.\n\nNov 9\n\nInterdisciplinary Project Presentations\n\nErfan Hosseini, Pan Fang, and Henger Li | Computer Science PhD Students, Tulane University\n\nThese presentations will be delivered online. You may access the presentations on Monday, November 9th, at 4:30 pm CST via the following link: https://tulane.zoom.us/j/93662271094 . Meeting ID: 936 6227 1094. Please be sure to mute your microphone when you log on.\n\nErfan Hosseini\n\nThe Study of Gentrification on Social Urban Simulation - How Income and Interest Can Shape Neighborhoods\n\nAbstract: Gentrification is well-known among sociologists for its complexity and vast effects on urban life. In fact, gentrification can be so complex that sociologists only study specific instances of it. The study of gentrification is important since changes in gentrified urban areas directly affect surrounding suburban and rural areas hence a huge population is involved. In this project, we aim to simulate an urban environment and observe how gentrification starts and how it can affect the city in different situations. Furthermore, we experiment with the factors of gentrification to find possible bottlenecks and try to prevent it. This study can help us understand gentrification better and manage the city in a proper manner while facing it.\n\nPan Fang\n\nDistance Measures for Embedded Graphs\n\nAbstract: Measuring similarity of two objects is an essential step in many applications, particularly in comparing objects that can be modeled as graphs. In this project, we explore and learn the existing distance measures for planar graphs. When comparing their performance regarding different factors such as computability, quality of similarity and robustness, none of them have desired result in all these aspects. Besides these computational parts, we also take account of theoretic parts in mathematics for comparison. Specifically, if a distance measure of planar graphs is a metric, we investigate some topological properties of the metric space (e.g., connectedness, completeness and compactness). We comprehensively summarize the existing work in this area and analyze the strengths and weaknesses of these methods. This project will present a critical assessment and concise review of this field that is directly accessible to most people.\n\nHenger Li\n\nLearning to Pool: Multi-Arm Bandit for COVID-19 Group Testing\n\nAbstract: The worldwide pandemic coronavirus (COVID-19) has grown exponentially and caused huge life and economic loss. Due to its highly contagious nature, it is vital to have a large scale and rapid testing to screen for the virus's presence to control its spread. The recent RT-PCR based group testing or pooled testing seems like an effective method to vastly reduce the number of tests. However, the current group testing suffers from the dilution in pooled samples, which makes it harder to detect early-stage infection with low viral load. We propose a multi-arm bandit framework to balance the trade-off between the number of tests and false-negative rate through dynamically decide the group size and which group to test according to the historical test result during the group testing.\n\nNov 25\n\nThesis Defense Talk\n\nSushovan Majhi | Mathematics PhD Student, Tulane University\n\nThis presentation will be delivered online. You may access the presentation on Tuesday, November 25th, at 10:00 am CST via the following link: https://tulane.zoom.us/j/99521555848 .\n\nTopological Methods in Shape Reconstruction and Comparison\n\nAbstract: Most of the modern technologies at our service rely on \"shapes\" in some way or the other. Be it the Google Maps showing you the fastest route to your destination or the 3D printer on your desk creating an exact replica of a relic---shapes are being repeatedly sampled, reconstructed, and compared by intelligent machines. With the advent of modern sampling technologies, shape reconstruction and comparison techniques have matured profoundly over the last two decades. In this defense talk, we will catch a glimpse of the provable topological methods we propose to advance the study of Euclidean shape reconstruction and comparison. We investigate how topological concepts and results---like the Vietoris-Rips and Cech complexes, Nerve Lemma, discrete Morse theory, etc---lend themselves well to the reconstruction of geodesic spaces from a noisy sample. Our study also delves into the approximation of Gromov-Hausdorff distance, which is deemed as a robust shape comparison framework. We address some of the pivotal questions and challenges pertaining to its efficient computation---particularly for Euclidean subsets. Finally, we present an approximation algorithm, with a tight approximation factor of (1+1/4), for the Gromov-Hausdorff distance on the real line. .\n\nSpring 2020 Colloquia\n\nFeb 10\n\nInteractive Visual Analysis at Scale: From Data to Actionable Insights\n\nFabio Miranda | New York University\n\nAbstract: Over the past decade, technological innovations have produced a wealth of large and complex data sets on almost every aspect of human life, from natural science to business and social science. The analysis of this data is usually an exploratory process in which domain expertise plays an important role. It is, therefore, essential to integrate the user into the analysis loop, enabling them to formulate hypotheses and gain actionable insights into domain-specific problems. Interactive visualization is central in the support of this process, but the scale and complexity of the data present several challenges. My research focuses on proposing new methods and systems that allow for the interactive visual analysis of large data of different types, such as time-series, spatio-temporal, geometry, and image data. By combining visualization, machine learning, data management, and computer graphics, my work tackles fundamental challenges in data science, enabling effective analysis of large data to untangle real-world problems. In this talk, I will present my mos"
    }
}
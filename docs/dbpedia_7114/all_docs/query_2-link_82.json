{
    "id": "dbpedia_7114_2",
    "rank": 82,
    "data": {
        "url": "https://www.diag.uniroma1.it/node/25631",
        "read_more_link": "",
        "language": "en",
        "title": "Dipartimento di Ingegneria informatica, automatica e gestionale",
        "top_image": "",
        "meta_img": "",
        "images": [
            "https://www.diag.uniroma1.it/sites/all/themes/sapienza_bootstrap/logo.png",
            "https://www.diag.uniroma1.it/sites/default/files/add_calendar.png"
        ],
        "movies": [],
        "keywords": [],
        "meta_keywords": [
            ""
        ],
        "tags": null,
        "authors": [],
        "publish_date": null,
        "summary": "",
        "meta_description": "",
        "meta_lang": "it",
        "meta_favicon": "https://www.diag.uniroma1.it/sites/all/themes/sapienza_bootstrap/favicon.ico",
        "meta_site_name": "",
        "canonical_link": "https://www.diag.uniroma1.it/node/25631",
        "text": "Data and metadata in datasets experience many different kinds of change. Values are inserted, deleted or updated; rows appear and disappear; columns are added or repurposed, etc. In such a dynamic situation, users might have many questions related to changes in the dataset, for instance which parts of the data are trustworthy and which are not? Users will wonder: How many changes have there been in the recent minutes, days or years? What kind of changes were made at which points of time? How dirty is the data? Is data cleansing required? The fact that data changed can hint at different hidden processes or agendas: a frequently crowd-updated city name may be controversial; a person whose name has been recently changed may be the target of vandalism; and so on. We show various use cases that benefit from recognizing and exploring such change. We present a system and methods to interactively explore such change, addressing the variability dimension of big data challenges. To this end, we propose a model to capture change and the process of exploring dynamic data to identify salient changes. We provide exploration primitives along with motivational examples and measures for the volatility of data. Finally, we identify technical challenges that need to be addressed to make our vision a reality, show some use cases of change exploration and propose directions of future work.\n\nShort Bio. Felix Naumann studied mathematics, economy, and computer sciences at the University of Technology in Berlin. After receiving his diploma (MA) in 1997 he completed his PhD thesis in the area of data quality at Humboldt University of Berlin in 2000. In 2001 and 2002 he worked at the IBM Almaden Research Center on topics around data integration. From 2003 - 2006 he was assistant professor for information integration again at the Humboldt-University of Berlin. Since 2006 he holds the chair for information systems at the Hasso Plattner Institute at the University of Potsdam in Germany. He has been visiting researcher at QCRI, AT&T Research and IBM Research. His research interests include data profiling, data cleansing, and text mining. More details are at https://hpi.de/naumann."
    }
}
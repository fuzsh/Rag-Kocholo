{
    "id": "dbpedia_6819_0",
    "rank": 58,
    "data": {
        "url": "https://arxiv.org/html/2405.14782v1",
        "read_more_link": "",
        "language": "en",
        "title": "Lessons from the Trenches on Reproducible Evaluation of Language Models",
        "top_image": "",
        "meta_img": "",
        "images": [
            "https://arxiv.org/html/extracted/5594376/images/Timeline_v1.png",
            "https://arxiv.org/html/extracted/5594376/images/taskoverview.png",
            "https://arxiv.org/html/extracted/5594376/images/typesoftasks.png"
        ],
        "movies": [],
        "keywords": [],
        "meta_keywords": [
            ""
        ],
        "tags": null,
        "authors": [],
        "publish_date": null,
        "summary": "",
        "meta_description": "",
        "meta_lang": "en",
        "meta_favicon": "",
        "meta_site_name": "",
        "canonical_link": null,
        "text": "Lessons from the Trenches on\n\nReproducible Evaluation of Language Models\n\nStella Biderman Hailey Schoelkopf Lintang Sutawika\n\nLeo Gao Jonathan Tow Baber Abbasi Alham Fikri Aji Pawan Sasanka Ammanamanchi Sidney Black Jordan Clive Anthony DiPofi Julen Etxaniz Benjamin Fattori Jessica Zosa Forde Charles Foster Mimansa Jaiswal Wilson Y. Lee Haonan Li Charles Lovering Niklas Muennighoff Ellie Pavlick Jason Phang Aviya Skowron Samson Tan Xiangru Tang Kevin A. Wang Genta Indra Winata François Yvon Andy Zou\n\nAbstract\n\nEffective evaluation of language models remains an open challenge in NLP. Researchers and engineers face methodological issues such as the sensitivity of models to evaluation setup, difficulty of proper comparisons across methods, and the lack of reproducibility and transparency. In this paper we draw on three years of experience in evaluating large language models to provide guidance and lessons for researchers. First, we provide an overview of common challenges faced in language model evaluation. Second, we delineate best practices for addressing or lessening the impact of these challenges on research. Third, we present the Language Model Evaluation Harness (lm-eval): an open source library for independent, reproducible, and extensible evaluation of language models that seeks to address these issues. We describe the features of the library as well as case studies in which the library has been used to alleviate these methodological concerns.\n\n1 Introduction\n\nEvaluation on shared benchmark tasks is a crucial tool used to track and communicate progress in the machine learning and language modeling communities (Ruder, 2021). Benchmarks are used to track progress toward shared community goals and to demonstrate the improvements of newly proposed methods over prior baselines. Evaluation practices thus play a crucial role in the direction of the field: inconsistencies or biases in evaluation practices can lead to skewed performance comparisons, which may influence the direction of future research and the adoption of new methods by the community (Dehghani et al., 2021) or lead to adverse effects from deploying suboptimal or harmful models (Bender & Friedman, 2018) on tasks for which they are ill-suited (Raji et al., 2022).\n\nIn this work, we detail our lessons learned that have been especially beneficial to obtaining useful and rigorous findings. By sharing these lessons, we aim to foster a more robust and reliable evaluation ecosystem. Our contributions are threefold:\n\n1.\n\nWe highlight several commonly-faced challenges in evaluating language models, including the difficulty of assessing the correctness of natural language responses, challenges in benchmark design, and the dependence upon implementation details that are often obscured or unreported (Section 2).\n\n2.\n\nWe then discuss best practices we’ve identified to improve how to communicate results and improve evaluation rigor in the language modeling community, despite–or to mitigate the impact of—these challenges (Section 3).\n\n3.\n\nTo enable researchers and engineers to easily utilize the best practices we have identified, we present lm-eval, our open-source library for reproducible evaluation of language models (Section 4). lm-eval’s framework defines a flexible API for both model implementation and evaluation task implementation, and then handles all of the work to orchestrate evaluations internally, allowing users to evaluate their chosen combinations of models + tasks, dependent on use case. We additionally present a set of case studies in which lm-eval was used to improve the level of rigor of a set of evaluations (Section 5).\n\n2 Challenges in Evaluating Language Models\n\n2.1 Evaluating and Scoring Natural Language Abilities\n\nThe biggest challenge in language model evaluation is a concept we term the Key Problem: When evaluating language models, there can be many semantically equivalent but syntactically different ways of expressing the same idea. In an ideal world, we would have a way to automatically detect when two sentences express the same content but in different words. Unfortunately, our best tools for determining whether two sentences are semantically equivalent are the very models we are seeking to evaluate. This problem drives many of the approaches to LM benchmarking, and many problems in LM evaluation stem from there not being any silver bullets for solving the Key Problem.\n\nIn principle, this would be solvable by simply having expert human annotators score model responses for correctness. The main reason this is not ubiquitous is cost: performing accurate human studies is not only difficult and time-consuming but also very expensive due to fair compensation, pricing smaller actors or organizations out of performing such evaluations. Additionally, there are other reasons relying on solely human assessments must be done with caution: they can be flawed and biased, especially for complex judgments such as factuality (Hosking et al., 2024; Xu et al., 2023; Wu & Aji, 2023). Expert, trained human judgment can alleviate these issues but is inherently non-scalable.\n\nTo address the high costs of manual human evaluation, automated metrics are often used. These offer notable advantages in that they are (theoretically) fully reproducible, far easier and cheaper to compute, and can avoid some of the issues faced by human studies (Wei & Jia, 2021; Freitag et al., 2021; Amidei et al., 2020). Automated metrics such as BLEU (Papineni et al., 2002) and ROUGE (Lin, 2004) seek to directly solve the Key Problem by measuring the distance from a generated response to a gold-standard one, such as via counting the n-gram overlap between the two texts. Heuristic-based metrics such as BLEU (and its derivatives) have flaws (Callison-Burch et al., 2006) and present reproducibility challenges (Marie et al., 2021), but can be useful. More recently, model-based metrics have recently gained momentum through evaluation methods that leverage large language models as a grader (Kim et al., 2024; Wang et al., 2024; Liu et al., 2023b), especially as proxies for human preference evaluation (Zheng et al., 2023), but these are known to be flawed (Wang et al., 2023; Huang et al., 2024; Shen et al., 2023; Zeng et al., 2024; Hu et al., 2024; Liu et al., 2023c; Chen et al., 2024) and suffer from similar reproducibility issues as BLEU, ROUGE, and their variants.\n\nThe Key Problem can alternately be sidestepped by artificially restricting the answer space. The most prevalent way to achieve this is to reframe questions as multiple choice problems, with a single gold target answer and a finite, static set of possible responses (Hendrycks et al., 2020; Srivastava et al., 2022; Li’evin et al., 2022; Lin et al., 2022; Robinson et al., 2023; Holtzman et al., 2022). Alternatively, when a reference answer is known, one can perform string-matching approaches heuristically to determine whether the model’s answer matches the ground truth (Dua et al., 2019; Joshi et al., 2017; Hendrycks et al., 2021).\n\nThis challenge does not necessarily impact other applications of language models and related technologies, such as playing games where it easy to check that the game has ended (Romstad et al., 2008; Silver et al., 2018; † et al.(2022)(FAIR)†, Bakhtin, Brown, Dinan, Farina, Flaherty, Fried, Goff, Gray, Hu, et al., FAIR), more constrained scientific applications (Jumper et al., 2021; Ahdritz et al., 2022), or domains where we have practically usable verifiers even when the solutions are not checkable in all contexts (Biderman, 2020; Biderman & Raff, 2022; Lewkowycz et al., 2022). In the case of LLMs, the most notable cases where this ground-truth verifier is known are coding and mathematics problems, although the verifiers used, such as unit tests, may still break down in edge cases (Liu et al., 2023a)\n\n2.2 Benchmark Design and Validity\n\nTypically, we do not care about the actual numeric score of a model on a benchmark. Instead, we desire the benchmark to be a useful proxy for some real-world phenomenon. The validity of an evaluation is the extent to which these correlate (Messick, 1994). For a recent overview of validity concerns in NLP benchmarking, see Subramonian et al. (2023). Also see Raji et al. (2021); Saphra et al. (2023); Davis (2023) for extended discussion of construct validity in LLM evaluation.\n\nWhile validity is an ongoing problem in language model evaluation, we focus on mitigating other concerns first: as we will describe, lm-eval is designed to ensure measurements are consistent across runs and models, regardless of (construct) validity.\n\n2.3 Implementation Difficulties and (Ir)Reproducibility\n\nOnce a benchmark has been designed, it then needs to be implemented by machine learning researchers around the world to see use in driving progress in the field. This introduces a host of new challenges that need to be addressed in order to ensure that everyone is evaluating models on a benchmark in the same fashion when comparing results. This adaptation process can introduce inconsistencies and make it difficult to draw conclusions across different implementations. Researchers must adapt it to their own workflows and libraries for the purposes of actually adopting the benchmark in their research.\n\n2.3.1 “Minor” Implementation Details Matter\n\nThe importance of interoperability and full reproducibility stems from the fact that language models are incredibly sensitive to precise details that may not be obvious to practitioners. Even minor variations in prompts, formatting, or other implementation details can significantly impact the performance and validity of evaluations (Weber et al., 2023; Sclar et al., 2023; Mizrahi et al., 2024; Alzahrani et al., 2024; Lu et al., 2022; Webson & Pavlick, 2022; Min et al., 2022). Without access to the original evaluation code, when re-implementing evaluation procedures from scratch is required, it is nearly impossible to account for all the subtle details that can affect outcomes. As a result, these implementations are likely to diverge in ways that make it extremely difficult to ensure fair comparisons across works, even when evaluating on the same benchmark. Even having the prompts reported in a paper is no substitute for having access to the actual evaluation code: prompts in papers are often incorrect or difficult to map to the exact code implementation because they’ve been stylized to be human-readable.\n\n2.3.2 Lack of Agreement About “Apples to Apples”\n\nEven assuming that benchmarks are implemented consistently across works, the question of how to draw fair comparisons across models and methods is still difficult for LMs.\n\nFor instance, different instruction-tuned models may be trained to expect certain formats (Taori et al., 2023; Sanh et al., 2022; Wei et al., 2022) – using these models’ intended prompt formats can make the evaluation tasks inherently different or change their difficulty, but not using these can also bias against models trained with formats not matching tasks’ “standard” prompting styles. Likewise, if an original benchmark implementation (including prompting and postprocessing) is tailored for a specific model, other models trained differently will suffer, artificially skewing perceptions of what techniques are effective.\n\nLikewise, some questions of how to set up controlled experiments are still open–is it ideal to normalize performance and comparisons by the number of parameters? Training FLOPs? Inference cost? Must training data be held equal? How should models which can leverage external resources such as retrieved documents or external tools be compared? These questions are all context-dependent but can impact findings significantly. For example, Wang et al. (2022) explore comparisons across architectures and training objectives, and choose to normalize for FLOPs, thus comparing encoder-decoder models with double the parameters to decoder-only models. Comparing results of models with equivalent training FLOPs, regardless of the allocation of those FLOPs, is commonplace (Hoffmann et al. (2022); Peng et al. (2023); Touvron et al. (2023), inter alia). However, in a more memory-constrained setting, comparing models equi-parameter may be more logical. While this is not inherently problematic, as different application contexts motivate different evaluation criteria, it is common to gloss headline claims as “model X is better than model y” or “model x doesn’t really work as advertised” without paying significant attention to how comparisons were made.\n\n2.3.3 Comparisons with Prior Work are Expensive (and Sometimes Impossible)\n\nSetting aside the question of establishing fair comparisons between methods or models, an additional key challenge in language modeling research is that many barriers prevent thorough comparison with related work.\n\nMany LMs developed by industrial labs, often used as reference points for benchmarks, have never been released externally (Chowdhery et al., 2023; Hoffmann et al., 2022), preventing comparisons except by pulling unverified evaluation numbers from technical reports. Those models that have been made available via APIs may non-transparently not match the published versions or otherwise be modified for deployment. Additionally, these API models are quickly deprecated and no longer accessible, rendering slews of work no longer reproducible . API access, especially for large volumes of evaluation, is quite expensive.\n\nFurther, a growing number of companies no longer make base language models available, but enforce interaction via a chat interface or API, which may include product features such as personalization , safety controls or domain-specific tooling . Attempts to compare these closed systems, which integrate a language model along with proprietary features, introduce a whole new set of complications.\n\n2.4 Fast-changing Progress and Conventions\n\nDue to the time-consuming nature of developing good benchmarks and the rapid pace of change in NLP research in the past decade, many widely used language model evaluation benchmarks do not represent the current paradigm of how language models are trained. This has two major impacts:\n\n•\n\nBenchmarks are being used for purposes they were not originally designed for or designed for validity under: for example, a large number of benchmarks have been built around fine-tuning on a known training set and closed space of labels (Wang et al., 2019b; a).\n\n•\n\nThere is no “ground-truth” implementation from the original benchmark authors for many of these popular benchmarks “retrofitted” to be used with prompted autoregressive LMs. In the absence of a clear standard, the community’s methodology for evaluating on these benchmarks may be fragmented or undocumented (Clark et al., 2018; Paperno et al., 2016).\n\nTo illustrate the effects of this development timeline, Figure 1 shows how many prominent LM benchmarks were designed prior to shifts such as in-context learning and chat interaction, and therefore were not designed to take these formats and approaches into account. This can affect validity or difficulty in unforeseen ways.\n\n3 Best Practices for Language Model Evaluation\n\nWhile LM evaluation is difficult and suffers from a number of challenges as we have described, there are measures that can be taken to significantly improve current practices. We provide our high-level recommendations regarding such measures, and detail our motivations briefly for each.\n\nAlways share your exact prompts and code\n\n•\n\nIf possible, full evaluation code including the full prompts used should also be provided for reproducible evaluation runs, as well as further identifiers such as links to specific commits used. Failing this, sharing prompts is often not done, but can drastically improve reproducibility.\n\n•\n\nFor fair comparison against other models, evaluation should be done with the same set of prompts unless there’s a good reason not to. Prompts should not be optimized for performance on a given model but not others, and the amount of prompt engineering done should be disclosed.\n\nAvoid copying results from other implementations\n\n•\n\nComparing results across papers can be misleading due to a wide range of experimental differences, including prompts, sample size, metric calculation, and more (Marie et al., 2021).\n\n•\n\nResults should not be copied or reported from other papers (Marie, 2022) whenever possible, unless one can verify that the exact same code has been used to run the experiments in those papers. If such copying is unavoidable, it should be clearly marked as such and treated carefully.\n\nAlways provide model outputs\n\n•\n\nProviding model outputs alongside evaluation code can allow others to recalculate scores based on these artifacts, which can be useful for performing statistical significance testing and for assessing the impact of different evaluation metrics or scoring approaches.\n\n•\n\nEvaluation of large models or APIs can be quite costly–sharing such artifacts allows researchers without access to significant compute to participate in evaluation research.\n\n•\n\nFinally, sharing outputs can allow results on API models to be reproduced to some extent, even if the models are subsequently deprecated.\n\nPerform qualitative analyses\n\n•\n\nQualitatively review a small batch of results and outputs before testing at scale: it is very easy to have bugs in your generation code, especially when working with multiple sets of benchmarks, prompts, and models of different architectures. Catching issues early can save a lot of time and compute, and increase confidence in results.\n\n•\n\nQuantitative scores only provide so much information. To understand why a model is scoring so well or so poorly, it is important to do some sort of qualitative error analysis. This can sometimes reveal superficial errors that are easier to correct with post-processing Bawden & Yvon (2023), or more fundamental errors.\n\nPerform statistical significance testing\n\n•\n\nMost works on language modeling do not perform statistical significance testing (Marie et al., 2021). This simple addition can dramatically boost the reliability of claimed results.\n\n•\n\nAlthough costly, reporting results run over more than one random seed can dramatically boost the validity and utility of results. For example, averaging across model runs (Sellam et al., 2022), or averaging over multiple selections of few-shot examples.\n\n4 The Language Model Evaluation Harness\n\nInformed by these practices we have built lm-eval. Unlike prior work on unified benchmarking libraries (Liang et al., 2023; Srivastava et al., 2022), the Evaluation Library does not seek to solely prescribe what the correct benchmark or evaluation protocols to use are, and allows users to select their desired tasks and use cases.\n\nThe role of the lm-eval is to solve the orchestration problem: previously, performing thorough LM evaluations would require painstaking re-implementation of previous tasks (likely to introduce subtle methodological divergences) or the individual installation and usage of extant code for each benchmark. Our goal is to make it easy to allow researchers or library users to simply install one codebase, and run their method plus selected baselines on their desired tasks in a controlled fashion. We hope to make it more effort-intensive to not follow minimum best practices for evaluation rigor.\n\n4.1 Design\n\nWe provide an overview of lm-eval’s major components and design philosophy. At its core, lm-eval allows for the contribution of two types of implementations: evaluation Tasks and integrations with novel LM implementations.\n\nTasks\n\nlm-eval is built around modular implementations of evaluation tasks, implemented as a Task class using a common API. This allows tasks to be collected in a common library, for new tasks to be extended or implemented easily, and for novel tasks to be easily shared reproducibly among practitioners or other library users. Users can implement tasks either via YAML-based configuration files or via subclassing the provided Task class and providing custom code for specific methods. In Figure 2, we show an example of the evaluation logic packaged within a Task class.\n\nWe provide a number of implementations for common tasks, and accept new tasks sourced from the community. We strive to match the paper originally introducing a benchmark dataset in its methodology, including using the same prompts if applicable. For tasks such as those introduced prior to prompted evaluation becoming the standard, we source evaluation methodology from the paper first posing the evaluation dataset as a prompted task. For example, we implement many tasks as adapted for in-context learning by Brown et al. (2020).\n\nLMs\n\nThe next core piece of lm-eval is the LM API. Because effective orchestration is our core goal, we allow arbitrary software libraries or (autoregressive) language model architectures to extend a provided interface for LM objects.\n\nFor ease of use, and compartmentalization of the model definition and external library integrations for custom models away from core evaluation logic, we assume that LMs operate upon dispatched Requests which consist of mapping string inputs to some string or probability as output. We thus abstract tokenizers away within the LM class, and treat a neural language model combined with its tokenizer as a single system being evaluated.\n\nLMs implement a simple interface, consisting of several types of Requests in order to be used within the library for all supported tasks.\n\nRequest Types\n\nWe allow for 3 core types of Requests that may be sent to a language model, which consist of distinct types of measurements that can be performed to observe a model’s response or latent capabilities in a prompted format. These are:\n\n•\n\n(Conditional) Loglikelihoods (loglikelihood, multiple_choice) - computing the probability of given output string(s), conditioned on some provided input.\n\n•\n\nPerplexities (loglikelihood_rolling) - measuring the average loglikelihood or probability of producing the tokens in a given dataset.\n\n•\n\nGeneration (generate_until) - generating text until a given stopping condition is reached, from a model conditioned on some provided input.\n\nProvided with these three primitive operations, we are able to implement the major ways in the literature that have been used to evaluate LMs (Gao et al. (2020), Brown et al. (2020), inter alia). While these high-level approaches are standard, they all contain a number of subtle implementation decisions which are often not disclosed in papers. Therefore, we include a full formal description of common implementation details involved in ours and others’ approaches within Appendix A for completeness, which we hope will be a useful contribution to the literature.\n\n4.2 Addressing Challenges and Incorporating Best Practices\n\nHere we detail how we position lm-eval to address the issues mentioned in Section 2 and incorporate the recommendations in Section 3, in order to encourage a more robust evaluation ecosystem.\n\nReproducibility\n\nlm-eval encourages and enables reproducible evaluation in several ways. First, by providing a standardized implementation of many common tasks, practitioners can report on these tasks and ensure they are evaluating on the same prompt and implementation as other users of the library.\n\nAlongside task results we report a version field, incremented each time a task must be modified in a way that affects its scoring. Therefore, in the case where task implementations have bugs or must otherwise be updated, one can still reference the version of the task used, to ensure future research can reproduce reported results.\n\nWhile this is not a panacea for the costs of comparing to prior work, and rerunning baselines oneself is advised, when prior work uses our library one can be confident that the results from prior work match what one would have gotten had one rerun it oneself using that version of the library (Beeching et al., 2023).\n\nQualitative Analysis\n\nlm-eval provides support for performing qualitative analysis of evaluation scores. In keeping with our recommended best practices, we implement the following, which allow for qualitative checks to be a core part of the evaluation workflow when using lm-eval:\n\n•\n\nWe allow for artificially limiting the amount of samples used for a given evaluation run, to enable code to be tested and outputs to be reviewed in small batches prior to full evaluation runs.\n\n•\n\nPer-sample logging is supported, for post-hoc reproduction of scores or error analysis of model mistakes or evaluation implementation.\n\nStatistical Testing\n\nlm-eval reports the standard error (SE) of most supported metrics, calculated by either bootstrapping or dividing the sample standard deviation by the root of the sample size.\n\nBy reporting these SE calculations prominently in every evaluation run, we make it trivial for practitioners to add simple statistical measures such as confidence intervals to their results. While we believe more rigorous and widespread statistical testing in LM evaluation is still needed, we hope that this will spur the community to report and be more aware of statistical significance concerns by lowering the difficulty of reporting such measures.\n\n5 Case Studies\n\nFinally, we demonstrate lm-eval’s utility for improving evaluation rigor and understanding via case studies of its successful usage. We provide additional case studies within Appendix B.\n\n5.1 Prompts Massively Affect Results\n\nAs mentioned in Section 2.3.1, language models can be very sensitive to the specific prompt or framing of an evaluation task.\n\nHere we detail how lm-eval has been used to study this sensitivity, and can be used to improve confidence in the comparison of scores across models. We focus our attention on two popular language modeling benchmarks: the ARC question answering benchmark (Clark et al., 2018) and MMLU (Hendrycks et al., 2021). However, not all papers evaluate on these tasks in the same way as the original formats.\n\nARC is first adapted to the in-context learning setting by Brown et al. (2020), who implement the dataset as a “cloze” task: the model is prompted via ‘‘Question: {question}\\\\\\backslash\\nAnswer:’’ and the likelihood of each potential completion string’s text is compared. Comparatively, MMLU is implemented in Hendrycks et al. (2020) via providing the model with the question text, each of the 4 possible answers preceded by an answer letter A, B, C, or D, and scoring over the answer letters’ probabilities.\n\nHowever, if models do not adopt these approaches, or disclose their exact settings, it is impossible to reliably compare stated model performance. In Table 1, we compare evaluation on the Easy and Challenge sets of ARC using the prompt from Brown et al. (2020) (“Cloze”) and using an MMLU-style answer letter with explicit multiple choice options (“MMLU-style”). We additionally compare MMLU scores between the original MMLU prompting style (“MMLU-style”) and an approach we term “Hybrid”, consisting of an MMLU-style prompt but using the answer strings instead of answer letters as the set of continuations over which we can score. As described further in Appendix B.1, this can be done by modifying two lines in lm-eval’s ARC and MMLU config files. Comparing these prompting styles, the degree to which models differ in performance widely varies, as well as which prompt performs better. If certain model creators chose one prompting style and certain other model creators chose the other, and each used the cited numbers from the respective technical reports for comparing their model to other baselines, the comparisons would be nonsensical and not provide information on which model were “truly” performant.\n\nThis demonstrates the vital importance of not copying numbers from across other papers’ reported evaluation scores, and of sharing full details on one’s own evaluation setup. We thus hope the use of lm-eval will boost rigor and confidence in novel evaluation results and encourage better communication of evaluation setups.\n\n5.2 Empowering Benchmark Creators and LM Evaluation Research\n\nProviding a library for evaluation orchestration that is configurable as we have described in Section 4 has many other uses, and we have observed the community leveraging lm-eval effectively for these purposes.\n\nExperimentation on the complexity or difficulties in LM evaluation has been made easier via our configurable task design. Alzahrani et al. (2024) and Lyu et al. (2024) explore the effects of prompting and other distractors on model robustness and performance using lm-eval, as well as investigate the role of evaluation methodology such as the tradeoffs of loglikelihood versus generative evaluation, as we also detail in Appendix A.\n\nlm-eval has been adopted by the community to make the design of novel benchmarks easier: our extensible Task configurations, and corresponding codebase have been used by the community to prototype the evaluation of their new benchmark datasets in lm-eval. By providing this location for community members to design and contribute novel evaluation code, we sidestep the challenging problem of tracking down and using extant evaluation code from various papers entirely: the reference implementations for these new tasks are directly in lm-eval in the first place. As described in Section 4.1, we strive to reduce barriers to task development and contribution, such as providing low-friction modes of development (modular configuration files) or examples implementing tasks “in the style of MMLU”.\n\nlm-eval has recently received contributions for a variety of datasets relying on lm-eval for their evaluation and benchmark prototyping/design (Faysse et al., 2024; Son et al., 2024a; b; Kweon et al., 2024; Li et al., 2024). By directly contributing their new evaluation tasks to lm-eval, benchmark authors also get to have full control over the dissemination (Hendrycks & Woodside, 2024) and implementation of their evaluation, making it far easier for the language modeling community to discover and recognize new benchmarking contributions that might otherwise go unrecognized or unadopted (Dehghani et al., 2021). This is the power of orchestration–the goal is to put new evaluation benchmarks in the hands of the community, and put tools for creating benchmarks given an evaluation dataset in the hands of evaluation developers, while smoothing over potential roadblocks we discuss in Section 2. As an additional concrete example, tasks in lm-eval have been used to back not only the popular Open LLM Leaderboard (Beeching et al., 2023), but also the construction of arbitrary novel leaderboards, which have been used to make custom comparisons between models on more specific use cases, such as non-English languages.\n\nThus, the orchestration viewpoint we take allows downstream users and developers to create their own approaches which best fit their goals and applications, allowing for the fostering of more evaluation development and a more interoperable ecosystem, rather than setting a few chosen metrics in stone.\n\n6 Conclusion\n\nWe have presented a number of common challenges in LM evaluation and our recommendations to mitigate the worst of these pitfalls. We introduce lm-eval, a library for evaluation orchestration built to enable easier and more reproducible benchmarking across common evaluation tasks and model implementations.\n\nWe hope that lm-eval will continue to be used by the community to improve rigor and our collective understanding of LM evaluations.\n\nReferences\n\nAhdritz et al. (2022) Gustaf Ahdritz, Nazim Bouatta, Sachin Kadyan, Qinghui Xia, William Gerecke, Timothy J O’Donnell, Daniel Berenberg, Ian Fisk, Niccolò Zanichelli, Bo Zhang, et al. Openfold: Retraining alphafold2 yields new insights into its learning mechanisms and capacity for generalization. Biorxiv, pp. 2022–11, 2022.\n\nAlam et al. (2024) Mohammad Mahmudul Alam, Edward Raff, Stella Biderman, Tim Oates, and James Holt. Holographic global convolutional networks for long-range prediction tasks in malware detection. arXiv preprint arXiv:2403.17978, 2024.\n\nAlzahrani et al. (2024) Norah Alzahrani, Hisham Abdullah Alyahya, Yazeed Alnumay, Sultan Alrashed, Shaykhah Alsubaie, Yusef Almushaykeh, Faisal Mirza, Nouf Alotaibi, Nora Altwairesh, Areeb Alowisheq, M Saiful Bari, and Haidar Khan. When benchmarks are targets: Revealing the sensitivity of large language model leaderboards, 2024.\n\nAmidei et al. (2020) Jacopo Amidei, Paul Piwek, and Alistair Willis. Identifying annotator bias: A new IRT-based method for bias identification. In Donia Scott, Nuria Bel, and Chengqing Zong (eds.), Proceedings of the 28th International Conference on Computational Linguistics, pp. 4787–4797, Barcelona, Spain (Online), December 2020. International Committee on Computational Linguistics. doi: 10.18653/v1/2020.coling-main.421. URL https://aclanthology.org/2020.coling-main.421.\n\nArora et al. (2024) Simran Arora, Sabri Eyuboglu, Michael Zhang, Aman Timalsina, Silas Alberti, Dylan Zinsley, James Zou, Atri Rudra, and Christopher Ré. Simple linear attention language models balance the recall-throughput tradeoff, 2024.\n\nAskell et al. (2021) Amanda Askell, Yuntao Bai, Anna Chen, Dawn Drain, Deep Ganguli, Tom Henighan, Andy Jones, Nicholas Joseph, Ben Mann, Nova DasSarma, Nelson Elhage, Zac Hatfield-Dodds, Danny Hernandez, Jackson Kernion, Kamal Ndousse, Catherine Olsson, Dario Amodei, Tom Brown, Jack Clark, Sam McCandlish, Chris Olah, and Jared Kaplan. A general language assistant as a laboratory for alignment, 2021.\n\nBach et al. (2022) Stephen H. Bach, Victor Sanh, Zheng-Xin Yong, Albert Webson, Colin Raffel, Nihal V. Nayak, Abheesht Sharma, Taewoon Kim, M Saiful Bari, Thibault Fevry, Zaid Alyafeai, Manan Dey, Andrea Santilli, Zhiqing Sun, Srulik Ben-David, Canwen Xu, Gunjan Chhablani, Han Wang, Jason Alan Fries, Maged S. Al-shaibani, Shanya Sharma, Urmish Thakker, Khalid Almubarak, Xiangru Tang, Dragomir Radev, Mike Tian-Jian Jiang, and Alexander M. Rush. Promptsource: An integrated development environment and repository for natural language prompts. In Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics: System Demonstrations, pp. 93–104, 2022.\n\nBawden & Yvon (2023) Rachel Bawden and François Yvon. Investigating the translation performance of a large multilingual language model: the case of BLOOM. In Proceedings of the 24th Annual Conference of the European Association for Machine Translation, 2023. URL https://arxiv.org/abs/2303.01911.\n\nBeeching et al. (2023) Edward Beeching, Clémentine Fourrier, Nathan Habib, Sheon Han, Nathan Lambert, Nazneen Rajani, Omar Sanseviero, Lewis Tunstall, and Thomas Wolf. Open llm leaderboard. https://huggingface.co/spaces/HuggingFaceH4/open_llm_leaderboard, 2023.\n\nBender & Friedman (2018) Emily M. Bender and Batya Friedman. Data statements for natural language processing: Toward mitigating system bias and enabling better science. Transactions of the Association for Computational Linguistics, 6:587–604, 2018. doi: 10.1162/tacl˙a˙00041. URL https://aclanthology.org/Q18-1041.\n\nBentivogli et al. (2009) Luisa Bentivogli, Peter Clark, Ido Dagan, and Danilo Giampiccolo. The fifth pascal recognizing textual entailment challenge. TAC, 7(8):1, 2009.\n\nBiderman (2020) Stella Biderman. Magic: the gathering is as hard as arithmetic. arXiv preprint arXiv:2003.05119, 2020.\n\nBiderman & Raff (2022) Stella Biderman and Edward Raff. Fooling moss detection with pretrained language models. In Proceedings of the 31st ACM international conference on information & knowledge management, pp. 2933–2943, 2022.\n\nBiderman et al. (2023) Stella Biderman, Hailey Schoelkopf, Quentin Gregory Anthony, Herbie Bradley, Kyle O’Brien, Eric Hallahan, Mohammad Aflah Khan, Shivanshu Purohit, Usvsn Sai Prashanth, Edward Raff, Aviya Skowron, Lintang Sutawika, and Oskar Van Der Wal. Pythia: A suite for analyzing large language models across training and scaling. In Andreas Krause, Emma Brunskill, Kyunghyun Cho, Barbara Engelhardt, Sivan Sabato, and Jonathan Scarlett (eds.), Proceedings of the 40th International Conference on Machine Learning, volume 202 of Proceedings of Machine Learning Research, pp. 2397–2430. PMLR, 23–29 Jul 2023. URL https://proceedings.mlr.press/v202/biderman23a.html.\n\nBisk et al. (2020) Yonatan Bisk, Rowan Zellers, Jianfeng Gao, Yejin Choi, et al. Piqa: Reasoning about physical commonsense in natural language. In Proceedings of the AAAI conference on artificial intelligence, volume 34, pp. 7432–7439, 2020.\n\nBlack et al. (2022) Sid Black, Stella Biderman, Eric Hallahan, Quentin Anthony, Leo Gao, Laurence Golding, Horace He, Connor Leahy, Kyle McDonell, Jason Phang, Michael Pieler, USVSN Sai Prashanth, Shivanshu Purohit, Laria Reynolds, Jonathan Tow, Ben Wang, and Samuel Weinbach. GPT-NeoX-20B: An open-source autoregressive language model. In Proceedings of the ACL Workshop on Challenges & Perspectives in Creating Large Language Models, 2022. URL https://arxiv.org/abs/2204.06745.\n\nBrown et al. (1992) Peter F Brown, Stephen A Della Pietra, Vincent J Della Pietra, Jennifer C Lai, and Robert L Mercer. An estimate of an upper bound for the entropy of English. Computational Linguistics, 18(1):31–40, 1992.\n\nBrown et al. (2020) Tom B Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are few-shot learners. arXiv preprint arXiv:2005.14165, 2020. URL https://arxiv.org/abs/2005.14165.\n\nCallison-Burch et al. (2006) Chris Callison-Burch, Miles Osborne, and Philipp Koehn. Re-evaluating the role of Bleu in machine translation research. In Diana McCarthy and Shuly Wintner (eds.), 11th Conference of the European Chapter of the Association for Computational Linguistics, pp. 249–256, Trento, Italy, April 2006. Association for Computational Linguistics. URL https://aclanthology.org/E06-1032.\n\nChen et al. (2024) Guiming Hardy Chen, Shunian Chen, Ziche Liu, Feng Jiang, and Benyou Wang. Humans or llms as the judge? a study on judgement biases. ArXiv, abs/2402.10669, 2024. URL https://api.semanticscholar.org/CorpusID:267740522.\n\nChowdhery et al. (2023) Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam Roberts, Paul Barham, Hyung Won Chung, Charles Sutton, Sebastian Gehrmann, Parker Schuh, Kensen Shi, Sasha Tsvyashchenko, Joshua Maynez, Abhishek Rao, Parker Barnes, Yi Tay, Noam Shazeer, Vinodkumar Prabhakaran, Emily Reif, Nan Du, Ben Hutchinson, Reiner Pope, James Bradbury, Jacob Austin, Michael Isard, Guy Gur-Ari, Pengcheng Yin, Toju Duke, Anselm Levskaya, Sanjay Ghemawat, Sunipa Dev, Henryk Michalewski, Xavier Garcia, Vedant Misra, Kevin Robinson, Liam Fedus, Denny Zhou, Daphne Ippolito, David Luan, Hyeontaek Lim, Barret Zoph, Alexander Spiridonov, Ryan Sepassi, David Dohan, Shivani Agrawal, Mark Omernick, Andrew M. Dai, Thanumalayan Sankaranarayana Pillai, Marie Pellat, Aitor Lewkowycz, Erica Moreira, Rewon Child, Oleksandr Polozov, Katherine Lee, Zongwei Zhou, Xuezhi Wang, Brennan Saeta, Mark Diaz, Orhan Firat, Michele Catasta, Jason Wei, Kathy Meier-Hellstern, Douglas Eck, Jeff Dean, Slav Petrov, and Noah Fiedel. Palm: Scaling language modeling with pathways. Journal of Machine Learning Research, 24(240):1–113, 2023. URL http://jmlr.org/papers/v24/22-1144.html.\n\nClark et al. (2019) Christopher Clark, Kenton Lee, Ming-Wei Chang, Tom Kwiatkowski, Michael Collins, and Kristina Toutanova. Boolq: Exploring the surprising difficulty of natural yes/no questions, 2019.\n\nClark et al. (2018) Peter Clark, Isaac Cowhey, Oren Etzioni, Tushar Khot, Ashish Sabharwal, Carissa Schoenick, and Oyvind Tafjord. Think you have Solved Question Answering? Try ARC, the AI2 Reasoning Challenge, 2018.\n\nDagan et al. (2005) Ido Dagan, Oren Glickman, and Bernardo Magnini. The pascal recognising textual entailment challenge. In Machine learning challenges workshop, pp. 177–190. Springer, 2005.\n\nDavis (2023) Ernest Davis. Benchmarks for automated commonsense reasoning: A survey. ACM Computing Surveys, 56(4):1–41, 2023.\n\nDe et al. (2024) Soham De, Samuel L. Smith, Anushan Fernando, Aleksandar Botev, George Cristian-Muraru, Albert Gu, Ruba Haroun, Leonard Berrada, Yutian Chen, Srivatsan Srinivasan, Guillaume Desjardins, Arnaud Doucet, David Budden, Yee Whye Teh, Razvan Pascanu, Nando De Freitas, and Caglar Gulcehre. Griffin: Mixing gated linear recurrences with local attention for efficient language models, 2024.\n\nde Marneffe et al. (2019) Marie-Catherine de Marneffe, Mandy Simons, and Judith Tonhauser. The commitmentbank: Investigating projection in naturally occurring discourse. 2019. URL https://api.semanticscholar.org/CorpusID:203595067.\n\nDehghani et al. (2021) Mostafa Dehghani, Yi Tay, Alexey A. Gritsenko, Zhe Zhao, Neil Houlsby, Fernando Diaz, Donald Metzler, and Oriol Vinyals. The benchmark lottery, 2021.\n\nDevlin et al. (2018) Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: Pre-training of deep bidirectional transformers for language understanding. arXiv preprint arXiv:1810.04805, 2018.\n\nDua et al. (2019) Dheeru Dua, Yizhong Wang, Pradeep Dasigi, Gabriel Stanovsky, Sameer Singh, and Matt Gardner. Drop: A reading comprehension benchmark requiring discrete reasoning over paragraphs. arXiv preprint arXiv:1903.00161, 2019.\n\n(31) Meta Fundamental AI Research Diplomacy Team (FAIR)†, Anton Bakhtin, Noam Brown, Emily Dinan, Gabriele Farina, Colin Flaherty, Daniel Fried, Andrew Goff, Jonathan Gray, Hengyuan Hu, et al. Human-level play in the game of diplomacy by combining language models with strategic reasoning. Science, 378(6624):1067–1074, 2022.\n\nFaysse et al. (2024) Manuel Faysse, Patrick Fernandes, Nuno M. Guerreiro, António Loison, Duarte M. Alves, Caio Corro, Nicolas Boizard, João Alves, Ricardo Rei, Pedro H. Martins, Antoni Bigata Casademunt, François Yvon, André F. T. Martins, Gautier Viaud, Céline Hudelot, and Pierre Colombo. Croissantllm: A truly bilingual french-english language model, 2024.\n\nFreitag et al. (2021) Markus Freitag, George Foster, David Grangier, Viresh Ratnakar, Qijun Tan, and Wolfgang Macherey. Experts, errors, and context: A large-scale study of human evaluation for machine translation. Transactions of the Association for Computational Linguistics, 9:1460–1474, 2021. doi: 10.1162/tacl˙a˙00437. URL https://aclanthology.org/2021.tacl-1.87.\n\nFu et al. (2023) Daniel Y. Fu, Tri Dao, Khaled K. Saab, Armin W. Thomas, Atri Rudra, and Christopher Ré. Hungry hungry hippos: Towards language modeling with state space models, 2023.\n\nGao (2021) Leo Gao. Multiple Choice Normalization in LM Evaluation. https://blog.eleuther.ai/multiple-choice-normalization/, 2021.\n\nGao et al. (2020) Leo Gao, Stella Biderman, Sid Black, Laurence Golding, Travis Hoppe, Charles Foster, Jason Phang, Horace He, Anish Thite, Noa Nabeshima, Shawn Presser, and Connor Leahy. The Pile: an 800GB dataset of diverse text for language modeling. arXiv preprint arXiv:2101.00027, 2020. URL https://arxiv.org/abs/2101.00027.\n\nGiampiccolo et al. (2007) Danilo Giampiccolo, Bernardo Magnini, Ido Dagan, and William B Dolan. The third pascal recognizing textual entailment challenge. In Proceedings of the ACL-PASCAL workshop on textual entailment and paraphrasing, pp. 1–9, 2007.\n\nGu & Dao (2023) Albert Gu and Tri Dao. Mamba: Linear-time sequence modeling with selective state spaces. arXiv preprint arXiv:2312.00752, 2023.\n\nGu et al. (2022) Albert Gu, Karan Goel, and Christopher Re. Efficiently modeling long sequences with structured state spaces. In International Conference on Learning Representations, 2022. URL https://openreview.net/forum?id=uYLFoz1vlAC.\n\nHaim et al. (2006) R Bar Haim, Ido Dagan, Bill Dolan, Lisa Ferro, Danilo Giampiccolo, Bernardo Magnini, and Idan Szpektor. The second pascal recognising textual entailment challenge. In Proceedings of the Second PASCAL Challenges Workshop on Recognising Textual Entailment, volume 7, pp. 785–794, 2006.\n\nHendrycks & Woodside (2024) Dan Hendrycks and Thomas Woodside. Devising ml metrics. https://www.safe.ai/blog/devising-ml-metrics, 2024.\n\nHendrycks et al. (2020) Dan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn Song, and Jacob Steinhardt. Measuring massive multitask language understanding. In International Conference on Learning Representations, 2020.\n\nHendrycks et al. (2021) Dan Hendrycks, Collin Burns, Saurav Kadavath, Akul Arora, Steven Basart, Eric Tang, Dawn Song, and Jacob Steinhardt. Measuring mathematical problem solving with the math dataset, 2021.\n\nHoffmann et al. (2022) Jordan Hoffmann, Sebastian Borgeaud, Arthur Mensch, Elena Buchatskaya, Trevor Cai, Eliza Rutherford, Diego de Las Casas, Lisa Anne Hendricks, Johannes Welbl, Aidan Clark, Tom Hennigan, Eric Noland, Katie Millican, George van den Driessche, Bogdan Damoc, Aurelia Guy, Simon Osindero, Karen Simonyan, Erich Elsen, Jack W. Rae, Oriol Vinyals, and Laurent Sifre. Training compute-optimal large language models, 2022.\n\nHoltzman et al. (2020) Ari Holtzman, Jan Buys, Li Du, Maxwell Forbes, and Yejin Choi. The curious case of neural text degeneration, 2020.\n\nHoltzman et al. (2022) Ari Holtzman, Peter West, Vered Shwartz, Yejin Choi, and Luke Zettlemoyer. Surface form competition: Why the highest probability answer isn’t always right, 2022.\n\nHosking et al. (2024) Tom Hosking, Phil Blunsom, and Max Bartolo. Human feedback is not gold standard, 2024.\n\nHu et al. (2024) Xinyu Hu, Mingqi Gao, Sen Hu, Yang Zhang, Yicheng Chen, Teng Xu, and Xiaojun Wan. Are LLM-based Evaluators Confusing NLG Quality Criteria? ArXiv, abs/2402.12055, 2024. URL https://api.semanticscholar.org/CorpusID:267750948.\n\nHuang et al. (2024) Hui Huang, Yingqi Qu, Jing Liu, Muyun Yang, and Tiejun Zhao. An empirical study of llm-as-a-judge for llm evaluation: Fine-tuned judge models are task-specific classifiers. 2024. URL https://api.semanticscholar.org/CorpusID:268247854.\n\nJelinek et al. (2005) F. Jelinek, R. L. Mercer, L. R. Bahl, and J. K. Baker. Perplexity—a measure of the difficulty of speech recognition tasks. The Journal of the Acoustical Society of America, 62(S1):S63–S63, 08 2005. ISSN 0001-4966. doi: 10.1121/1.2016299. URL https://doi.org/10.1121/1.2016299.\n\nJiang et al. (2023) Albert Q. Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bamford, Devendra Singh Chaplot, Diego de las Casas, Florian Bressand, Gianna Lengyel, Guillaume Lample, Lucile Saulnier, Lélio Renard Lavaud, Marie-Anne Lachaux, Pierre Stock, Teven Le Scao, Thibaut Lavril, Thomas Wang, Timothée Lacroix, and William El Sayed. Mistral 7b, 2023.\n\nJiang et al. (2024) Albert Q. Jiang, Alexandre Sablayrolles, Antoine Roux, Arthur Mensch, Blanche Savary, Chris Bamford, Devendra Singh Chaplot, Diego de las Casas, Emma Bou Hanna, Florian Bressand, Gianna Lengyel, Guillaume Bour, Guillaume Lample, Lélio Renard Lavaud, Lucile Saulnier, Marie-Anne Lachaux, Pierre Stock, Sandeep Subramanian, Sophia Yang, Szymon Antoniak, Teven Le Scao, Théophile Gervet, Thibaut Lavril, Thomas Wang, Timothée Lacroix, and William El Sayed. Mixtral of experts, 2024.\n\nJoshi et al. (2017) Mandar Joshi, Eunsol Choi, Daniel S. Weld, and Luke Zettlemoyer. Triviaqa: A large scale distantly supervised challenge dataset for reading comprehension, 2017.\n\nJumper et al. (2021) John Jumper, Richard Evans, Alexander Pritzel, Tim Green, Michael Figurnov, Olaf Ronneberger, Kathryn Tunyasuvunakool, Russ Bates, Augustin Žídek, Anna Potapenko, et al. Highly accurate protein structure prediction with alphafold. Nature, 596(7873):583–589, 2021.\n\nKhashabi et al. (2018) Daniel Khashabi, Snigdha Chaturvedi, Michael Roth, Shyam Upadhyay, and Dan Roth. Looking beyond the surface: A challenge set for reading comprehension over multiple sentences. In Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers), pp. 252–262, 2018.\n\nKim et al. (2024) Seungone Kim, Jamin Shin, Yejin Cho, Joel Jang, Shayne Longpre, Hwaran Lee, Sangdoo Yun, Seongjin Shin, Sungdong Kim, James Thorne, and Minjoon Seo. Prometheus: Inducing fine-grained evaluation capability in language models, 2024.\n\nKweon et al. (2024) Sunjun Kweon, Byungjin Choi, Minkyu Kim, Rae Woong Park, and Edward Choi. Kormedmcqa: Multi-choice question answering benchmark for korean healthcare professional licensing examinations, 2024.\n\nLevesque et al. (2012) Hector Levesque, Ernest Davis, and Leora Morgenstern. The winograd schema challenge. In Thirteenth international conference on the principles of knowledge representation and reasoning, 2012.\n\nLewkowycz et al. (2022) Aitor Lewkowycz, Anders Andreassen, David Dohan, Ethan Dyer, Henryk Michalewski, Vinay Ramasesh, Ambrose Slone, Cem Anil, Imanol Schlag, Theo Gutman-Solo, et al. Solving quantitative reasoning problems with language models. Advances in Neural Information Processing Systems, 35:3843–3857, 2022.\n\nLhoest et al. (2021) Quentin Lhoest, Albert Villanova del Moral, Yacine Jernite, Abhishek Thakur, Patrick von Platen, Suraj Patil, Julien Chaumond, Mariama Drame, Julien Plu, Lewis Tunstall, Joe Davison, Mario Šaško, Gunjan Chhablani, Bhavitvya Malik, Simon Brandeis, Teven Le Scao, Victor Sanh, Canwen Xu, Nicolas Patry, Angelina McMillan-Major, Philipp Schmid, Sylvain Gugger, Clément Delangue, Théo Matussière, Lysandre Debut, Stas Bekman, Pierric Cistac, Thibault Goehringer, Victor Mustar, François Lagunas, Alexander Rush, and Thomas Wolf. Datasets: A community library for natural language processing. In Heike Adel and Shuming Shi (eds.), Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing: System Demonstrations, pp. 175–184, Online and Punta Cana, Dominican Republic, November 2021. Association for Computational Linguistics. doi: 10.18653/v1/2021.emnlp-demo.21. URL https://aclanthology.org/2021.emnlp-demo.21.\n\nLi et al. (2016) Jiwei Li, Will Monroe, Alan Ritter, Dan Jurafsky, Michel Galley, and Jianfeng Gao. Deep reinforcement learning for dialogue generation. In Jian Su, Kevin Duh, and Xavier Carreras (eds.), Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, pp. 1192–1202, Austin, Texas, November 2016. Association for Computational Linguistics. doi: 10.18653/v1/D16-1127. URL https://aclanthology.org/D16-1127.\n\nLi et al. (2024) Nathaniel Li, Alexander Pan, Anjali Gopal, Summer Yue, Daniel Berrios, Alice Gatti, Justin D. Li, Ann-Kathrin Dombrowski, Shashwat Goel, Long Phan, Gabriel Mukobi, Nathan Helm-Burger, Rassin Lababidi, Lennart Justen, Andrew B. Liu, Michael Chen, Isabelle Barrass, Oliver Zhang, Xiaoyuan Zhu, Rishub Tamirisa, Bhrugu Bharathi, Adam Khoja, Zhenqi Zhao, Ariel Herbert-Voss, Cort B. Breuer, Samuel Marks, Oam Patel, Andy Zou, Mantas Mazeika, Zifan Wang, Palash Oswal, Weiran Liu, Adam A. Hunt, Justin Tienken-Harder, Kevin Y. Shih, Kemper Talley, John Guan, Russell Kaplan, Ian Steneker, David Campbell, Brad Jokubaitis, Alex Levinson, Jean Wang, William Qian, Kallol Krishna Karmakar, Steven Basart, Stephen Fitz, Mindy Levine, Ponnurangam Kumaraguru, Uday Tupakula, Vijay Varadharajan, Yan Shoshitaishvili, Jimmy Ba, Kevin M. Esvelt, Alexandr Wang, and Dan Hendrycks. The wmdp benchmark: Measuring and reducing malicious use with unlearning, 2024.\n\nLiang et al. (2023) Percy Liang, Rishi Bommasani, Tony Lee, Dimitris Tsipras, Dilara Soylu, Michihiro Yasunaga, Yian Zhang, Deepak Narayanan, Yuhuai Wu, Ananya Kumar, Benjamin Newman, Binhang Yuan, Bobby Yan, Ce Zhang, Christian Cosgrove, Christopher D. Manning, Christopher Ré, Diana Acosta-Navas, Drew A. Hudson, Eric Zelikman, Esin Durmus, Faisal Ladhak, Frieda Rong, Hongyu Ren, Huaxiu Yao, Jue Wang, Keshav Santhanam, Laurel Orr, Lucia Zheng, Mert Yuksekgonul, Mirac Suzgun, Nathan Kim, Neel Guha, Niladri Chatterji, Omar Khattab, Peter Henderson, Qian Huang, Ryan Chi, Sang Michael Xie, Shibani Santurkar, Surya Ganguli, Tatsunori Hashimoto, Thomas Icard, Tianyi Zhang, Vishrav Chaudhary, William Wang, Xuechen Li, Yifan Mai, Yuhui Zhang, and Yuta Koreeda. Holistic evaluation of language models, 2023.\n\nLi’evin et al. (2022) Valentin Li’evin, Christoffer Egeberg Hother, and Ole Winther. Can large language models reason about medical questions? Patterns, 5, 2022. URL https://api.semanticscholar.org/CorpusID:250627547.\n\nLin (2004) Chin-Yew Lin. Rouge: A package for automatic evaluation of summaries. In Text summarization branches out, pp. 74–81, 2004.\n\nLin et al. (2022) Stephanie Lin, Jacob Hilton, and Owain Evans. TruthfulQA: Measuring how models mimic human falsehoods. In Smaranda Muresan, Preslav Nakov, and Aline Villavicencio (eds.), Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pp. 3214–3252, Dublin, Ireland, May 2022. Association for Computational Linguistics. doi: 10.18653/v1/2022.acl-long.229. URL https://aclanthology.org/2022.acl-long.229.\n\nLiu et al. (2023a) Jiawei Liu, Chunqiu Steven Xia, Yuyao Wang, and Lingming Zhang. Is your code generated by chatgpt really correct? rigorous evaluation of large language models for code generation, 2023a.\n\nLiu et al. (2023b) Yang Liu, Dan Iter, Yichong Xu, Shuohang Wang, Ruochen Xu, and Chenguang Zhu. G-eval: NLG evaluation using gpt-4 with better human alignment. In Houda Bouamor, Juan Pino, and Kalika Bali (eds.), Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, pp. 2511–2522, Singapore, December 2023b. Association for Computational Linguistics. doi: 10.18653/v1/2023.emnlp-main.153. URL https://aclanthology.org/2023.emnlp-main.153.\n\nLiu et al. (2023c) Yixin Liu, Alexander R Fabbri, Jiawen Chen, Yilun Zhao, Simeng Han, Shafiq Joty, Pengfei Liu, Dragomir Radev, Chien-Sheng Wu, and Arman Cohan. Benchmarking generation and evaluation capabilities of large language models for instruction controllable summarization. arXiv preprint arXiv:2311.09184, 2023c.\n\nLu et al. (2022) Yao Lu, Max Bartolo, Alastair Moore, Sebastian Riedel, and Pontus Stenetorp. Fantastically ordered prompts and where to find them: Overcoming few-shot prompt order sensitivity. In Smaranda Muresan, Preslav Nakov, and Aline Villavicencio (eds.), Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pp. 8086–8098, Dublin, Ireland, May 2022. Association for Computational Linguistics. doi: 10.18653/v1/2022.acl-long.556. URL https://aclanthology.org/2022.acl-long.556.\n\nLyu et al. (2024) Chenyang Lyu, Minghao Wu, and Alham Fikri Aji. Beyond probabilities: Unveiling the misalignment in evaluating large language models, 2024.\n\nMagnusson et al. (2023) Ian Magnusson, Akshita Bhagia, Valentin Hofmann, Luca Soldaini, Ananya Harsh Jha, Oyvind Tafjord, Dustin Schwenk, Evan Pete Walsh, Yanai Elazar, Kyle Lo, Dirk Groeneveld, Iz Beltagy, Hannaneh Hajishirzi, Noah A. Smith, Kyle Richardson, and Jesse Dodge. Paloma: A benchmark for evaluating language model fit, 2023.\n\nMarie (2022) Benjamin Marie. Comparing the uncomparable to claim the state of the art: A concerning trend. Blog post, August 2022.\n\nMarie et al. (2021) Benjamin Marie, Atsushi Fujita, and Raphael Rubino. Scientific credibility of machine translation research: A meta-evaluation of 769 papers. In Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers), pp. 7297–7306, Online, August 2021. Association for Computational Linguistics. doi: 10.18653/v1/2021.acl-long.566. URL https://aclanthology.org/2021.acl-long.566.\n\nMerity et al. (2016) Stephen Merity, Caiming Xiong, James Bradbury, and Richard Socher. Pointer sentinel mixture models, 2016.\n\nMessick (1994) Samuel Messick. Standards-based score interpretation: Establishing valid grounds for valid inferences. ETS Research Report Series, 1994(2):291–305, 1994. doi: https://doi.org/10.1002/j.2333-8504.1994.tb01630.x. URL https://onlinelibrary.wiley.com/doi/abs/10.1002/j.2333-8504.1994.tb01630.x.\n\nMihaylov et al. (2018) Todor Mihaylov, Peter Clark, Tushar Khot, and Ashish Sabharwal. Can a suit of armor conduct electricity? a new dataset for open book question answering. arXiv preprint arXiv:1809.02789, 2018.\n\nMin et al. (2022) Sewon Min, Xinxi Lyu, Ari Holtzman, Mikel Artetxe, Mike Lewis, Hannaneh Hajishirzi, and Luke Zettlemoyer. Rethinking the role of demonstrations: What makes in-context learning work?, 2022.\n\nMizrahi et al. (2024) Moran Mizrahi, Guy Kaplan, Dan Malkin, Rotem Dror, Dafna Shahaf, and Gabriel Stanovsky. State of what art? a call for multi-prompt LLM evaluation, 2024.\n\nPaperno et al. (2016) Denis Paperno, Germán Kruszewski, Angeliki Lazaridou, Quan Ngoc Pham, Raffaella Bernardi, Sandro Pezzelle, Marco Baroni, Gemma Boleda, and Raquel Fernández. The lambada dataset: Word prediction requiring a broad discourse context. arXiv preprint arXiv:1606.06031, 2016.\n\nPapineni et al. (2002) Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu. Bleu: a method for automatic evaluation of machine translation. In Pierre Isabelle, Eugene Charniak, and Dekang Lin (eds.), Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics, pp. 311–318, Philadelphia, Pennsylvania, USA, July 2002. Association for Computational Linguistics. doi: 10.3115/1073083.1073135. URL https://aclanthology.org/P02-1040.\n\nPenedo et al. (2023) Guilherme Penedo, Quentin Malartic, Daniel Hesslow, Ruxandra Cojocaru, Alessandro Cappelli, Hamza Alobeidli, Baptiste Pannier, Ebtesam Almazrouei, and Julien Launay. The refinedweb dataset for falcon llm: Outperforming curated corpora with web data, and web data only, 2023.\n\nPeng et al. (2023) Bo Peng, Eric Alcaide, Quentin Anthony, Alon Albalak, Samuel Arcadinho, Huanqi Cao, Xin Cheng, Michael Chung, Matteo Grella, Kranthi Kiran GV, et al. Rwkv: Reinventing rnns for the transformer era. arXiv preprint arXiv:2305.13048, 2023.\n\nPeng et al. (2024) Bo Peng, Daniel Goldstein, Quentin Anthony, Alon Albalak, Eric Alcaide, Stella Biderman, Eugene Cheah, Teddy Ferdinan, Haowen Hou, Przemysław Kazienko, et al. Eagle and finch: Rwkv with matrix-valued states and dynamic recurrence. arXiv preprint arXiv:2404.05892, 2024.\n\nPerez et al. (2021) Ethan Perez, Douwe Kiela, and Kyunghyun Cho. True few-shot learning with language models, 2021.\n\nPilehvar & Camacho-Collados (2018) Mohammad Taher Pilehvar and Jose Camacho-Collados. Wic: the word-in-context dataset for evaluating context-sensitive meaning representations. arXiv preprint arXiv:1808.09121, 2018.\n\nPoli et al. (2023) Michael Poli, Stefano Massaroli, Eric Nguyen, Daniel Y Fu, Tri Dao, Stephen Baccus, Yoshua Bengio, Stefano Ermon, and Christopher Ré. Hyena hierarchy: Towards larger convolutional language models. In International Conference on Machine Learning, pp. 28043–28078. PMLR, 2023.\n\nPress et al. (2020) Ofir Press, Noah A. Smith, and Mike Lewis. Shortformer: Better language modeling using shorter inputs. CoRR, abs/2012.15832, 2020. URL https://arxiv.org/abs/2012.15832.\n\nProvilkov et al. (2020) Ivan Provilkov, Dmitrii Emelianenko, and Elena Voita. BPE-dropout: Simple and effective subword regularization, 2020.\n\nQin et al. (2024a) Zhen Qin, Songlin Yang, Weixuan Sun, Xuyang Shen, Dong Li, Weigao Sun, and Yiran Zhong. Hgrn2: Gated linear rnns with state expansion, 2024a.\n\nQin et al. (2024b) Zhen Qin, Songlin Yang, and Yiran Zhong. Hierarchically gated recurrent neural network for sequence modeling. Advances in Neural Information Processing Systems, 36, 2024b.\n\nRadford et al. (2019) Alec Radford, Jeff Wu, Rewon Child, David Luan, Dario Amodei, and Ilya Sutskever. Language models are unsupervised multitask learners. OpenAI Blog, 2019.\n\nRaji et al. (2021) Inioluwa Deborah Raji, Emily M. Bender, Amandalynne Paullada, Emily Denton, and Alex Hanna. AI and the Everything in the Whole Wide World Benchmark, 2021.\n\nRaji et al. (2022) Inioluwa Deborah Raji, I. Elizabeth Kumar, Aaron Horowitz, and Andrew Selbst. The fallacy of ai functionality. In 2022 ACM Conference on Fairness, Accountability, and Transparency, FAccT ’22. ACM, June 2022. doi: 10.1145/3531146.3533158. URL http://dx.doi.org/10.1145/3531146.3533158.\n\nRobinson et al. (2023) Joshua Robinson, Christopher Michael Rytting, and David Wingate. Leveraging large language models for multiple choice question answering, 2023.\n\nRoemmele et al. (2011) Melissa Roemmele, Cosmin Adrian Bejan, and Andrew S Gordon. Choice of plausible alternatives: An evaluation of commonsense causal reasoning. In 2011 AAAI Spring Symposium Series, 2011.\n\nRomstad et al. (2008) Tord Romstad, Marco Costalba, Joona Kiiski, Gary Linscott, Yu Nasu, Motohiro Isozaki, and Hisayori Noda. Stockfish, 2008. URL https://github.com/official-stockfish/Stockfish.\n\nRuder (2021) Sebastian Ruder. Challenges and Opportunities in NLP Benchmarking. http://ruder.io/nlp-benchmarking, 2021.\n\nSakaguchi et al. (2019) Keisuke Sakaguchi, Ronan Le Bras, Chandra Bhagavatula, and Yejin Choi. Winogrande: An adversarial winograd schema challenge at scale, 2019.\n\nSanh et al. (2022) Victor Sanh, Albert Webson, Colin Raffel, Stephen Bach, Lintang Sutawika, Zaid Alyafeai, Antoine Chaffin, Arnaud Stiegler, Teven Le Scao, Arun Raja, et al. Multitask prompted training enables zero-shot task generalization. In The Tenth International Conference on Learning Representations, 2022.\n\nSaphra et al. (2023) Naomi Saphra, Eve Fleisig, Kyunghyun Cho, and Adam Lopez. First tragedy, then parse: History repeats itself in the new era of large language models. arXiv preprint, 2023.\n\nSchaeffer et al. (2023) Rylan Schaeffer, Brando Miranda, and Sanmi Koyejo. Are emergent abilities of large language models a mirage?, 2023.\n\nSclar et al. (2023) Melanie Sclar, Yejin Choi, Yulia Tsvetkov, and Alane Suhr. Quantifying language models’ sensitivity to spurious features in prompt design or: How I learned to start worrying about prompt formatting, 2023.\n\nSellam et al. (2022) Thibault Sellam, Steve Yadlowsky, Ian Tenney, Jason Wei, Naomi Saphra, Alexander D’Amour, Tal Linzen, Jasmijn Bastings, Iulia Raluca Turc, Jacob Eisenstein, Dipanjan Das, and Ellie Pavlick. The multiBERTs: BERT reproductions for robustness analysis. In International Conference on Learning Representations, 2022. URL https://openreview.net/forum?id=K0E_F0gFDgA.\n\nShannon (1948) C. E. Shannon. A mathematical theory of communication. The Bell System Technical Journal, 27(3):379–423, 1948. doi: 10.1002/j.1538-7305.1948.tb01338.x.\n\nShen et al. (2023) Chenhui Shen, Liying Cheng, Xuan-Phi Nguyen, Yang You, and Lidong Bing. Large language models are not yet human-level evaluators for abstractive summarization. In Houda Bouamor, Juan Pino, and Kalika Bali (eds.), Findings of the Association for Computational Linguistics: EMNLP 2023, pp. 4215–4233, Singapore, December 2023. Association for Computational Linguistics. doi: 10.18653/v1/2023.findings-emnlp.278. URL https://aclanthology.org/2023.findings-emnlp.278.\n\nSilver et al. (2018) David Silver, Thomas Hubert, Julian Schrittwieser, Ioannis Antonoglou, Matthew Lai, Arthur Guez, Marc Lanctot, Laurent Sifre, Dharshan Kumaran, Thore Graepel, et al. A general reinforcement learning algorithm that masters chess, shogi, and go through self-play. Science, 362(6419):1140–1144, 2018.\n\nSon et al. (2024a) Guijin Son, Hanwool Lee, Sungdong Kim, Seungone Kim, Niklas Muennighoff, Taekyoon Choi, Cheonbok Park, Kang Min Yoo, and Stella Biderman. Kmmlu: Measuring massive multitask language understanding in korean, 2024a.\n\nSon et al. (2024b) Guijin Son, Hanwool Lee, Suwan Kim, Huiseo Kim, Jaecheol Lee, Je Won Yeom, Jihyu Jung, Jung Woo Kim, and Songseong Kim. Hae-rae bench: Evaluation of korean knowledge in language models, 2024b.\n\nSrivastava et al. (2022) Aarohi Srivastava, Abhinav Rastogi, Abhishek B Rao, Abu Awal Md Shoeb, Abubakar Abid, Adam Fisch, Adam R. Brown, Adam Santoro, Aditya Gupta, Adrià Garriga-Alonso, Agnieszka Kluska, Aitor Lewkowycz, Akshat Agarwal, Alethea Power, Alex Ray, Alex Warstadt, Alexander W. Kocurek, Ali Safaya, Ali Tazarv, Alice Xiang, Alicia Parrish, Allen Nie, Aman Hussain, Amanda Askell, Amanda Dsouza, Ameet Annasaheb Rahane, Anantharaman S. Iyer, Anders Johan Andreassen, Andrea Santilli, Andreas Stuhlmuller, Andrew M. Dai, Andrew D. La, Andrew Kyle Lampinen, Andy Zou, Angela Jiang, Angelica Chen, Anh Vuong, Animesh Gupta, Anna Gottardi, Antonio Norelli, Anu Venkatesh, Arash Gholamidavoodi, Arfa Tabassum, Arul Menezes, Arun Kirubarajan, Asher Mullokandov, Ashish Sabharwal, Austin Herrick, Avia Efrat, Aykut Erdem, Ayla Karakacs, Bridget R. Roberts, Bao Sheng Loe, Barret Zoph, Bartlomiej Bojanowski, Batuhan Ozyurt, Behnam Hedayatnia, Behnam Neyshabur, Benjamin Inden, Benno Stein, Berk Ekmekci, Bill Yuchen Lin, Blake Stephen Howald, Cameron Diao, Cameron Dour, Catherine Stinson, Cedrick Argueta, C’esar Ferri Ram’irez, Chandan Singh, Charles Rathkopf, Chenlin Meng, Chitta Baral, Chiyu Wu, Chris Callison-Burch, Chris Waites, Christian Voigt, Christopher D. Manning, Christopher Potts, Cindy Tatiana Ramirez, Clara E. Rivera, Clemencia Siro, Colin Raffel, Courtney Ashcraft, Cristina Garbacea, Damien Sileo, Daniel H Garrette, Dan Hendrycks, Dan Kilman, Dan Roth, Daniel Freeman, Daniel Khashabi, Daniel Levy, Daniel Gonz’alez, Danny Hernandez, Danqi Chen, Daphne Ippolito, Dar Gilboa, David Dohan, D. Drakard, David Jurgens, Debajyoti Datta, Deep Ganguli, Denis Emelin, Denis Kleyko, Deniz Yuret, Derek Chen, Derek Tam, Dieuwke Hupkes, Diganta Misra, Dilyar Buzan, Dimitri Coelho Mollo, Diyi Yang, Dong-Ho Lee, Ekaterina Shutova, Ekin Dogus Cubuk, Elad Segal, Eleanor Hagerman, Elizabeth Barnes, Elizabeth P. Donoway, Ellie Pavlick, Emanuele Rodolà, Emma FC Lam, Eric Chu, Eric Tang, Erkut Erdem, Ernie Chang, Ethan A. Chi, Ethan Dyer, Ethan Jerzak, Ethan Kim, Eunice Engefu Manyasi, Evgenii Zheltonozhskii, Fan Xia, Fatemeh Siar, Fernando Mart’inez-Plumed, Francesca Happ’e, François Chollet, Frieda Rong, Gaurav Mishra, Genta Indra Winata, Gerard de Melo, Germán Kruszewski, Giambattista Parascandolo, Giorgio Mariani, Gloria Wang, Gonzalo Jaimovitch-L’opez, Gregor Betz, Guy Gur-Ari, Hana Galijasevic, Han Sol Kim, Hannah Rashkin, Hanna Hajishirzi, Harsh Mehta, Hayden Bogar, Henry Shevlin, Hinrich Schütze, Hiromu Yakura, Hongming Zhang, Hubert Wong, Ian Aik-Soon Ng, Isaac Noble, Jaap Jumelet, Jack Geissinger, John Kernion, Jacob Hilton, Jaehoon Lee, Jaime Fernández Fisac, J. Brooker Simon, James Koppel, James Zheng, James Zou, Jan Koco’n, Jana Thompson, Jared Kaplan, Jarema Radom, Jascha Sohl-Dickstein, Jason Phang, Jason Wei, Jason Yosinski, Jekaterina Novikova, Jelle Bosscher, Jenni Marsh, Jeremy Kim, Jeroen Taal, Jesse Engel, Jesujoba Oluwadara Alabi, Jiacheng Xu, Jiaming Song, Jillian Tang, Jane W Waweru, John Burden, John Miller, John U. Balis, Jonathan Berant, Jorg Frohberg, Jos Rozen, José Hernández-Orallo, Joseph Boudeman, Joseph Jones, Joshua B. Tenenbaum, Joshua S. Rule, Joyce Chua, Kamil Kanclerz, Karen Livescu, Karl Krauth, Karthik Gopalakrishnan, Katerina Ignatyeva, Katja Markert, Kaustubh D. Dhole, Kevin Gimpel, Kevin Ochieng’ Omondi, Kory Wallace Mathewson, Kristen Chiafullo, Ksenia Shkaruta, Kumar Shridhar, Kyle McDonell, Kyle Richardson, Laria Reynolds, Leo Gao, Li Zhang, Liam Dugan, Lianhui Qin, Lidia Contreras-Ochando, Louis-Philippe Morency, Luca Moschella, Luca Lam, Lucy Noble, Ludwig Schmidt, Luheng He, Luis Oliveros Col’on, Luke Metz, Lutfi Kerem cSenel, Maarten Bosma, Maarten Sap, Maartje ter Hoeve, Madotto Andrea, Maheen Saleem Farooqi, Manaal Faruqui, Mantas Mazeika, Marco Baturan, Marco Marelli, Marco Maru, M Quintana, Marie Tolkiehn, Mario Giulianelli, Martha Lewis, Martin Potthast, Matthew Leavitt, Matthias Hagen, M’aty’as Schubert, Medina Baitemirova, Melissa Arnaud, Melvin Andrew McElrath, Michael A. Yee, Michael Cohen, Mi Gu, Michael I. Ivanitskiy, Michael Starritt, Michael Strube, Michal Swkedrowski, Michele Bevilacqua, Michihiro Yasunaga, Mihir Kale, Mike Cain, Mimee Xu, Mirac Suzgun, Monica Tiwari, Mohit Bansal, Moin Aminnaseri, Mor Geva, Mozhdeh Gheini, T MukundVarma, Nanyun Peng, Nathan Chi, Nayeon Lee, Neta Gur-Ari Krakover, Nicholas Cameron, Nicholas S. Roberts, Nicholas Doiron, Nikita Nangia, Niklas Deckers, Niklas Muennighoff, Nitish Shirish Keskar, Niveditha Iyer, Noah Constant, Noah Fiedel, Nuan Wen, Oliver Zhang, Omar Agha, Omar Elbaghdadi, Omer Levy, Owain Evans, Pablo Casares, Parth Doshi, Pascale Fung, Paul Pu Liang, Paul Vicol, Pegah Alipoormolabashi, Peiyuan Liao, Percy Liang, Peter W. Chang, Peter Eckersley, Phu Mon Htut, Pi-Bei Hwang, P. Milkowski, Piyush S. Patil, Pouya Pezeshkpour, Priti Oli, Qiaozhu Mei, QING LYU, Qinlang Chen, Rabin Banjade, Rachel Etta Rudolph, Raefer Gabriel, Rahel Habacker, Ram’on Risco Delgado, Raphaël Millière, Rhythm Garg, Richard Barnes, Rif A. Saurous, Riku Arakawa, Robbe Raymaekers, Robert Frank, Rohan Sikand, Roman Novak, Roman Sitelew, Ronan Lebras, Rosanne Liu, Rowan Jacobs, Rui Zhang, Ruslan Salakhutdinov, Ryan Chi, Ryan Lee, Ryan Stovall, Ryan Teehan, Rylan Yang, Sahib J. Singh, Saif M. Mohammad, Sajant Anand, Sam Dillavou, Sam Shleifer, Sam Wiseman, Samuel Gruetter, Sam Bowman, Samuel S. Schoenholz, Sanghyun Han, Sanjeev Kwatra, Sarah A. Rous, Sarik Ghazarian, Sayan Ghosh, Sean Casey, Sebastian Bischoff, Sebastian Gehrmann, Sebastian Schuster, Sepideh Sadeghi, Shadi Sameh Hamdan, Sharon Zhou, Shashank Srivastava, Sherry Shi, Shikhar Singh, Shima Asaadi, Shixiang Shane Gu, Shubh Pachchigar, Shubham Toshniwal, Shyam Upadhyay, Shyamolima Debnath, Siamak Shakeri, Simon Thormeyer, Simone Melzi, Siva Kumar Reddy, Sneha Priscilla Makini, Soo hwan Lee, Spencer Bradley Torene, Sriharsha Hatwar, Stanislas Dehaene, Stefan Divic, Stefano Ermon, Stella Rose Biderman, Stephanie C. Lin, Stephen Prasad, Steven T. Piantadosi, Stuart M. Shieber, Summer Misherghi, Svetlana Kiritchenko, Swaroop Mishra, Tal Linzen, Tal Schuster, Tao Li, Tao Yu, Tariq A. Ali, Tatsuo Hashimoto, Te-Lin Wu, Theo Desbordes, Theodore Rothschild, Thomas Phan, Tianle Wang, Tiberius Nkinyili, Timo Schick, T. N. Kornev, Timothy Telleen-Lawton, Titus Tunduny, Tobias Gerstenberg, Trenton Chang, Trishala Neeraj, Tushar Khot, Tyler O. Shultz, Uri Shaham, Vedant Misra, Vera Demberg, Victoria Nyamai, Vikas Raunak, Vinay V. Ramasesh, Vinay Uday Prabhu, Vishakh Padmakumar, Vivek Srikumar, William Fedus, William Saunders, William Zhang, W Vossen, Xiang Ren, Xiaoyu F Tong, Xinyi Wu, Xudong Shen, Yadollah Yaghoobzadeh, Yair Lakretz, Yang Song, Yasaman Bahri, Ye Ji Choi, Yichi Yang, Yiding Hao, Yifu Chen, Yonatan Belinkov, Yu Hou, Yu Hou, Yuntao Bai, Zachary Seid, Zhao Xinran, Zhuoye Zhao, Zi Fu Wang, Zijie Jay Wang, Zirui Wang, and Ziyi Wu. Beyond the imitation game: Quantifying and extrapolating the capabilities of language models. In arXiv preprint 2206.04615, 2022. URL https://arxiv.org/abs/2206.04615.\n\nSubramonian et al. (2023) Arjun Subramonian, Xingdi Yuan, Hal Daumé III, and Su Lin Blodgett. It takes two to tango: Navigating conceptualizations of NLP tasks and measurements of performance. arXiv preprint arXiv:2305.09022, 2023.\n\nSun et al. (2023) Yutao Sun, Li Dong, Shaohan Huang, Shuming Ma, Yuqing Xia, Jilong Xue, Jianyong Wang, and Furu Wei. Retentive network: A successor to transformer for large language models. arXiv preprint arXiv:2307.08621, 2023.\n\nTaori et al. (2023) Rohan Taori, Ishaan Gulrajani, Tianyi Zhang, Yann Dubois, Xuechen Li, Carlos Guestrin, Percy Liang, and Tatsunori B. Hashimoto. Stanford alpaca: An instruction-following llama model. https://github.com/tatsu-lab/stanford_alpaca, 2023.\n\nTay et al. (2021) Yi Tay, Mostafa Dehghani, Samira Abnar, Yikang Shen, Dara Bahri, Philip Pham, Jinfeng Rao, Liu Yang, Sebastian Ruder, and Donald Metzler. Long range arena : A benchmark for efficient transformers. In 9th International Conference on Learning Representations, ICLR 2021, Virtual Event, Austria, May 3-7, 2021. OpenReview.net, 2021. URL https://openreview.net/forum?id=qVyeW-grC2k.\n\nTouvron et al. (2023) Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, Dan Bikel, Lukas Blecher, Cristian Canton Ferrer, Moya Chen, Guillem Cucurull, David Esiobu, Jude Fernandes, Jeremy Fu, Wenyin Fu, Brian Fuller, Cynthia Gao, Vedanuj Goswami, Naman Goyal, Anthony Hartshorn, Saghar Hosseini, Rui Hou, Hakan Inan, Marcin Kardas, Viktor Kerkez, Madian Khabsa, Isabel Kloumann, Artem Korenev, Punit Singh Koura, Marie-Anne Lachaux, Thibaut Lavril, Jenya Lee, Diana Liskovich, Yinghai Lu, Yuning Mao, Xavier Martinet, Todor Mihaylov, Pushkar Mishra, Igor Molybog, Yixin Nie, Andrew Poulton, Jeremy Reizenstein, Rashi Rungta, Kalyan Saladi, Alan Schelten, Ruan Silva, Eric Michael Smith, Ranjan Subramanian, Xiaoqing Ellen Tan, Binh Tang, Ross Taylor, Adina Williams, Jian Xiang Kuan, Puxin Xu, Zheng Yan, Iliyan Zarov, Yuchen Zhang, Angela Fan, Melanie Kambadur, Sharan Narang, Aurelien Rodriguez, Robert Stojnic, Sergey Edunov, and Thomas Scialom. Llama 2: Open foundation and fine-tuned chat models, 2023.\n\nWang et al. (2019a) Alex Wang, Yada Pruksachatkun, Nikita Nangia, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, and Samuel Bowman. Superglue: A stickier benchmark for general-purpose language understanding systems. Advances in neural information processing systems, 32, 2019a.\n\nWang et al. (2019b) Alex Wang, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, and Samuel R. Bowman. GLUE: A multi-task benchmark and analysis platform for natural language understanding. 2019b. In the Proceedings of ICLR.\n\nWang et al. (2023) Peiyi Wang, Lei Li, Liang Chen, Dawei Zhu, Binghuai Lin, Yunbo Cao, Qi Liu, Tianyu Liu, and Zhifang Sui. Large language models are not fair evaluators. ArXiv, abs/2305.17926, 2023. URL https://api.semanticscholar.org/CorpusID:258960339.\n\nWang et al. (2022) Thomas Wang, Adam Roberts, Daniel Hesslow, Teven Le Scao, Hyung Won Chung, Iz Beltagy, Julien Launay, and Colin Raffel. What language model architecture and pretraining objective work best for zero-shot generalization?, 2022.\n\nWang et al. (2024) Yidong Wang, Zhuohao Yu, Zhengran Zeng, Linyi Yang, Wenjin Yao, Cunxiang Wang, Hao Chen, Chaoya Jiang, Rui Xie, Jindong Wang, Xing Xie, Wei Ye, Shikun Zhang, and Yue Zhang. PandaLM: An automatic evaluation benchmark for LLM instruction tuning optimization. In The Twelfth International Conference on Learning Representations, 2024. URL https://openreview.net/forum?id=5Nn2BLV7SB.\n\nWeber et al. (2023) Lucas Weber, Elia Bruni, and Dieuwke Hupkes. The ICL consistency test, 2023.\n\nWebson & Pavlick (2022) Albert Webson and Ellie Pavlick. Do prompt-based models really understand the meaning of their prompts? In Marine Carpuat, Marie-Catherine de Marneffe, and Ivan Vladimir Meza Ruiz (eds.), Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pp. 2300–2344, Seattle, United States, July 2022. Association for Computational Linguistics. doi: 10.18653/v1/2022.naacl-main.167. URL https://aclanthology.org/2022.naacl-main.167.\n\nWei et al. (2022) Jason Wei, Maarten Bosma, Vincent Y. Zhao, Kelvin Guu, Adams Wei Yu, Brian Lester, Nan Du, Andrew M. Dai, and Quoc V. Le. Finetuned language models are zero-shot learners, 2022.\n\nWei & Jia (2021) Johnny Tian-Zheng Wei and Robin Jia. The statistical advantage of automatic NLG metrics at the system level, 2021.\n\nWorkshop et al. (2023) BigScience Workshop, :, Teven Le Scao, Angela Fan, Christopher Akiki, Ellie Pavlick, Suzana Ilić, Daniel Hesslow, Roman Castagné, Alexandra Sasha Luccioni, François Yvon, Matthias Gallé, Jonathan Tow, Alexander M. Rush, Stella Biderman, Albert Webson, Pawan Sasanka Ammanamanchi, Thomas Wang, Benoît Sagot, Niklas Muennighoff, Albert Villanova del Moral, Olatunji Ruwase, Rachel Bawden, Stas Bekman, Angelina McMillan-Major, Iz Beltagy, Huu Nguyen, Lucile Saulnier, Samson Tan, Pedro Ortiz Suarez, Victor Sanh, Hugo Laurençon, Yacine Jernite, Julien Launay, Margaret Mitchell, Colin Raffel, Aaron Gokaslan, Adi Simhi, Aitor Soroa, Alham Fikri Aji, Amit Alfassy, Anna Rogers, Ariel Kreisberg Nitzav, Canwen Xu, Chenghao Mou, Chris Emezue, Christopher Klamm, Colin Leong, Daniel van Strien, David Ifeoluwa Adelani, Dragomir Radev, Eduardo González Ponferrada, Efrat Levkovizh, Ethan Kim, Eyal Bar Natan, Francesco De Toni, Gérard Dupont, Germán Kruszewski, Giada Pistilli, Hady Elsahar, Hamza Benyamina, Hieu Tran, Ian Yu, Idris Abdulmumin, Isaac Johnson, Itziar Gonzalez-Dios, Javier de la Rosa, Jenny Chim, Jesse Dodge, Jian Zhu, Jonathan Chang, Jörg Frohberg, Joseph Tobing, Joydeep Bhattacharjee, Khalid Almubarak, Kimbo Chen, Kyle Lo, Leandro Von Werra, Leon Weber, Long Phan, Loubna Ben allal, Ludovic Tanguy, Manan Dey, Manuel Romero Muñoz, Maraim Masoud, María Grandury, Mario Šaško, Max Huang, Maximin Coavoux, Mayank Singh, Mike Tian-Jian Jiang, Minh Chien Vu, Mohammad A. Jauhar, Mustafa Ghaleb, Nishant Subramani, Nora Kassner, Nurulaqilla Khamis, Olivier Nguyen, Omar Espejel, Ona de Gibert, Paulo Villegas, Peter Henderson, Pierre Colombo, Priscilla Amuok, Quentin Lhoest, Rheza Harliman, Rishi Bommasani, Roberto Luis López, Rui Ribeiro, Salomey Osei, Sampo Pyysalo, Sebastian Nagel, Shamik Bose, Shamsuddeen Hassan Muhammad, Shanya Sharma, Shayne Longpre, Somaieh Nikpoor, Stanislav Silberberg, Suhas Pai, Sydney Zink, Tiago Timponi Torrent, Timo Schick, Tristan Thrush, Valentin Danchev, Vassilina Nikoulina, Veronika Laippala, Violette Lepercq, Vrinda Prabhu, Zaid Alyafeai, Zeerak Talat, Arun Raja, Benjamin Heinzerling, Chenglei Si, Davut Emre Taşar, Elizabeth Salesky, Sabrina J. Mielke, Wilson Y. Lee, Abheesht Sharma, Andrea Santilli, Antoine Chaffin, Arnaud Stiegler, Debajyoti Datta, Eliza Szczechla, Gunjan Chhablani, Han Wang, Harshit Pandey, Hendrik Strobelt, Jason Alan Fries, Jos Rozen, Leo Gao, Lintang Sutawika, M Saiful Bari, Maged S. Al-shaibani, Matteo Manica, Nihal Nayak, Ryan Teehan, Samuel Albanie, Sheng Shen, Srulik Ben-David, Stephen H. Bach, Taewoon Kim, Tali Bers, Thibault Fevry, Trishala Neeraj, Urmish Thakker, Vikas Raunak, Xiangru Tang, Zheng-Xin Yong, Zhiqing Sun, Shaked Brody, Yallow Uri, Hadar Tojarieh, Adam Roberts, Hyung Won Chung, Jaesung Tae, Jason Phang, Ofir Press, Conglong Li, Deepak Narayanan, Hatim Bourfoune, Jared Casper, Jeff Rasley, Max Ryabinin, Mayank Mishra, Minjia Zhang, Mohammad Shoeybi, Myriam Peyrounette, Nicolas Patry, Nouamane Tazi, Omar Sanseviero, Patrick von Platen, Pierre Cornette, Pierre François Lavallée, Rémi Lacroix, Samyam Rajbhandari, Sanchit Gandhi, Shaden Smith, Stéphane Requena, Suraj Patil, Tim Dettmers, Ahmed Baruwa, Amanpreet Singh, Anastasia Cheveleva, Anne-Laure Ligozat, Arjun Subramonian, Aurélie Névéol, Charles Lovering, Dan Garrette, Deepak Tunuguntla, Ehud Reiter, Ekaterina Taktasheva, Ekaterina Voloshina, Eli Bogdanov, Genta Indra Winata, Hailey Schoelkopf, Jan-Christoph Kalo, Jekaterina Novikova, Jessica Zosa Forde, Jordan Clive, Jungo Kasai, Ken Kawamura, Liam Hazan, Marine Carpuat, Miruna Clinciu, Najoung Kim, Newton Cheng, Oleg Serikov, Omer Antverg, Oskar van der Wal, Rui Zhang, Ruochen Zhang, Sebastian Gehrmann, Shachar Mirkin, Shani Pais, Tatiana Shavrina, Thomas Scialom, Tian Yun, Tomasz Limisiewicz, Verena Rieser, Vitaly Protasov, Vladislav Mikhailov, Yada Pruksachatkun, Yonatan Belinkov, Zachary Bamberger, Zdeněk Kasner, Alice Rueda, Amanda Pestana, Amir Feizpour, Ammar Khan, Amy Faranak, Ana Santos, Anthony Hevia, Antigona Unldreaj, Arash Aghagol, Arezoo Abdollahi, Aycha Tammour, Azadeh HajiHosseini, Bahareh Behroozi, Benjamin Ajibade, Bharat Saxena, Carlos Muñoz Ferrandis, Daniel McDuff, Danish Contractor, David Lansky, Davis David, Douwe Kiela, Duong A. Nguyen, Edward Tan, Emi Baylor, Ezinwanne Ozoani, Fatima Mirza, Frankline Ononiwu, Habib Rezanejad, Hessie Jones, Indrani Bhattacharya, Irene Solaiman, Irina Sedenko, Isar Nejadgholi, Jesse Passmore, Josh Seltzer, Julio Bonis Sanz, Livia Dutra, Mairon Samagaio, Maraim Elbadri, Margot Mieskes, Marissa Gerchick, Martha Akinlolu, Michael McKenna, Mike Qiu, Muhammed Ghauri, Mykola Burynok, Nafis Abrar, Nazneen Rajani, Nour Elkott, Nour Fahmy, Olanrewaju Samuel, Ran An, Rasmus Kromann, Ryan Hao, Samira Alizadeh, Sarmad Shubber, Silas Wang, Sourav Roy, Sylvain Viguier, Thanh Le, Tobi Oyebade, Trieu Le, Yoyo Yang, Zach Nguyen, Abhinav Ramesh Kashyap, Alfredo Palasciano, Alison Callahan, Anima Shukla, Antonio Miranda-Escalada, Ayush Singh, Benjamin Beilharz, Bo Wang, Caio Brito, Chenxi Zhou, Chirag Jain, Chuxin Xu, Clémentine Fourrier, Daniel León Periñán, Daniel Molano, Dian Yu, Enrique Manjavacas, Fabio Barth, Florian Fuhrimann, Gabriel Altay, Giyaseddin Bayrak, Gully Burns, Helena U. Vrabec, Imane Bello, Ishani Dash, Jihyun Kang, John Giorgi, Jonas Golde, Jose David Posada, Karthik Rangasai Sivaraman, Lokesh Bulchandani, Lu Liu, Luisa Shinzato, Madeleine Hahn de Bykhovetz, Maiko Takeuchi, Marc Pàmies, Maria A Castillo, Marianna Nezhurina, Mario Sänger, Matthias Samwald, Michael Cullan, Michael Weinberg, Michiel De Wolf, Mina Mihaljcic, Minna Liu, Moritz Freidank, Myungsun Kang, Natasha Seelam, Nathan Dahlberg, Nicholas Michio Broad, Nikolaus Muellner, Pascale Fung, Patrick Haller, Ramya Chandrasekhar, Renata Eisenberg, Robert Martin, Rodrigo Canalli, Rosaline Su, Ruisi Su, Samuel Cahyawijaya, Samuele Garda, Shlok S Deshmukh, Shubhanshu Mishra, Sid Kiblawi, Simon Ott, Sinee Sang-aroonsiri, Srishti Kumar, Stefan Schweter, Sushil Bharati, Tanmay Laud, Théo Gigant, Tomoya Kainuma, Wojciech Kusa, Yanis Labrak, Yash Shailesh Bajaj, Yash Venkatraman, Yifan Xu, Yingxin Xu, Yu Xu, Zhe Tan, Zhongli Xie, Zifan Ye, Mathilde Bras, Younes Belkada, and Thomas Wolf. Bloom: A 176b-parameter open-access multilingual language model, 2023.\n\nWu & Aji (2023) Minghao Wu and Alham Fikri Aji. Style over substance: Evaluation biases for large language models. arXiv preprint arXiv:2307.03025, 2023.\n\nXu et al. (2023) Fangyuan Xu, Yixiao Song, Mohit Iyyer, and Eunsol Choi. A critical evaluation of evaluations for long-form question answering. In Anna Rogers, Jordan Boyd-Graber, and Naoaki Okazaki (eds.), Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pp. 3225–3245, Toronto, Canada, July 2023. Association for Computational Linguistics. doi: 10.18653/v1/2023.acl-long.181. URL https://aclanthology.org/2023.acl-long.181.\n\nYang et al. (2024) Songlin Yang, Bailin Wang, Yikang Shen, Rameswar Panda, and Yoon Kim. Gated linear attention transformers with hardware-efficient training, 2024.\n\nZellers et al. (2019) Rowan Zellers, Ari Holtzman, Yonatan Bisk, Ali Farhadi, and Yejin Choi. Hellaswag: Can a machine really finish your sentence?, 2019.\n\nZeng et al. (2024) Zhiyuan Zeng, Jiatong Yu, Tianyu Gao, Yu Meng, Tanya Goyal, and Danqi Chen. Evaluating large language models at evaluating instruction following. In The Twelfth International Conference on Learning Representations, 2024. URL https://openreview.net/forum?id=tr0KidwPLc.\n\nZhang et al. (2018) Sheng Zhang, Xiaodong Liu, Jingjing Liu, Jianfeng Gao, Kevin Duh, and Benjamin Van Durme. Record: Bridging the gap between human and machine commonsense reading comprehension. arXiv preprint arXiv:1810.12885, 2018.\n\nZhang et al. (2022) Susan Zhang, Stephen Roller, Naman Goyal, Mikel Artetxe, Moya Chen, Shuohui Chen, Christopher Dewan, Mona Diab, Xian Li, Xi Victoria Lin, Todor Mihaylov, Myle Ott, Sam Shleifer, Kurt Shuster, Daniel Simig, Punit Singh Koura, Anjali Sridhar, Tianlu Wang, and Luke Zettlemoyer. Opt: Open pre-trained transformer language models, 2022.\n\nZheng et al. (2023) Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang, Zhanghao Wu, Yonghao Zhuang, Zi Lin, Zhuohan Li, Dacheng Li, Eric P. Xing, Hao Zhang, Joseph E. Gonzalez, and Ion Stoica. Judging llm-as-a-judge with mt-bench and chatbot arena. CoRR, abs/2306.05685, 2023. doi: 10.48550/ARXIV.2306.05685. URL https://doi.org/10.48550/arXiv.2306.05685.\n\nZhu et al. (2015) Yukun Zhu, Ryan Kiros, Richard Zemel, Ruslan Salakhutdinov, Raquel Urtasun, Antonio Torralba, and Sanja Fidler. Aligning books and movies: Towards story-like visual explanations by watching movies and reading books, 2015.\n\nAppendix A Formalizing Measurements\n\nHere we provide a formal description of the most common approaches to obtaining outputs or measurements from LMs for evaluation, as we implement in lm-eval. We include this for a number of reasons: as a reference for future work, notes on the history of certain LM eval practices, and as an illustrative example of just how many implementation details or methodological choices do not typically make it into evaluation papers and yet can vitally impact results or findings.\n\nA.1 Preliminaries\n\nThroughout, we consider an auto-regressive language model (LM), with vocabulary V𝑉Vitalic_V. Given an input consisting of tokens x0,x1,…,xn−1subscript𝑥0subscript𝑥1…subscript𝑥𝑛1x_{0},x_{1},...,x_{n-1}italic_x start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT , italic_x start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , … , italic_x start_POSTSUBSCRIPT italic_n - 1 end_POSTSUBSCRIPT, the model outputs a probability distribution over the vocabulary, P⁢(xn|x0,x1,…,xn−1)𝑃conditionalsubscript𝑥𝑛subscript𝑥0subscript𝑥1…subscript𝑥𝑛1P(x_{n}|x_{0},x_{1},...,x_{n-1})italic_P ( italic_x start_POSTSUBSCRIPT italic_n end_POSTSUBSCRIPT | italic_x start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT , italic_x start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , … , italic_x start_POSTSUBSCRIPT italic_n - 1 end_POSTSUBSCRIPT ). Internally, this is represented as returning “logits” of shape (1,|V|)1𝑉(1,|V|)( 1 , | italic_V | ), which when taking a log-softmax over the vocabulary dimension, yields log probabilities (“logprobs” or “loglikelihoods”) of each token in V𝑉Vitalic_V. Logits are the raw, unnormalized predictions of the model before applying the softmax function. Crucially, due to the parallel training and causal masking of autoregressive LMs, it is possible to obtain from a single LM call with x0,x1,…,xn−1subscript𝑥0subscript𝑥1…subscript𝑥𝑛1x_{0},x_{1},...,x_{n-1}italic_x start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT , italic_x start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , … , italic_x start_POSTSUBSCRIPT italic_n - 1 end_POSTSUBSCRIPT as input, logits of shape (n,|V|)𝑛𝑉(n,|V|)( italic_n , | italic_V | ) with the i𝑖iitalic_i-th element of these logits representing P⁢(xi|x0,x1,…,xi−1)𝑃conditionalsubscript𝑥𝑖subscript𝑥0subscript𝑥1…subscript𝑥𝑖1P(x_{i}|x_{0},x_{1},...,x_{i-1})italic_P ( italic_x start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT | italic_x start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT , italic_x start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , … , italic_x start_POSTSUBSCRIPT italic_i - 1 end_POSTSUBSCRIPT ) for all 1≤i≤n1𝑖𝑛1\\leq i\\leq n1 ≤ italic_i ≤ italic_n. (That is, for every token position of the input, we obtain concurrently the model’s prediction for the subsequent token, starting from its prediction for the value of x1subscript𝑥1x_{1}italic_x start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT and ending with the model’s predictions for the (not provided) “xnsubscript𝑥𝑛x_{n}italic_x start_POSTSUBSCRIPT italic_n end_POSTSUBSCRIPT” token.)\n\nA.2 Ranking-Based Multiple Choice QA\n\nGiven our language model, we aim to compute the conditional (log) probability (or “loglikelihood”) of a target string y𝑦yitalic_y conditioned on input x𝑥xitalic_x, denoted as log⁡P⁢(y|x)𝑃conditional𝑦𝑥\\log P(y|x)roman_log italic_P ( italic_y | italic_x ). This can be performed in a single LM call.\n\nLet x=x0,x1,…,xn−1𝑥subscript𝑥0subscript𝑥1…subscript𝑥𝑛1x=x_{0},x_{1},...,x_{n-1}italic_x = italic_x start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT , italic_x start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , … , italic_x start_POSTSUBSCRIPT italic_n - 1 end_POSTSUBSCRIPT be an input sequence of n𝑛nitalic_n tokens and y=y0,y1,…,ym−1𝑦subscript𝑦0subscript𝑦1…subscript𝑦𝑚1y=y_{0},y_{1},...,y_{m-1}italic_y = italic_y start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT , italic_y start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , … , italic_y start_POSTSUBSCRIPT italic_m - 1 end_POSTSUBSCRIPT be the target sequence of m𝑚mitalic_m tokens, where xisubscript𝑥𝑖x_{i}italic_x start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT and yisubscript𝑦𝑖y_{i}italic_y start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT represent individual tokens. To compute log⁡P⁢(y|x)𝑃conditional𝑦𝑥\\log P(y|x)roman_log italic_P ( italic_y | italic_x ), we follow these steps:\n\n1.\n\nConcatenate x𝑥xitalic_x and y𝑦yitalic_y to form a new sequence, but discard the final token ym−1subscript𝑦𝑚1y_{m-1}italic_y start_POSTSUBSCRIPT italic_m - 1 end_POSTSUBSCRIPT. The resulting sequence is x0,x1,…,xn−1,y0,y1,…,ym−2subscript𝑥0subscript𝑥1…subscript𝑥𝑛1subscript𝑦0subscript𝑦1…subscript𝑦𝑚2x_{0},x_{1},...,x_{n-1},y_{0},y_{1},...,y_{m-2}italic_x start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT , italic_x start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , … , italic_x start_POSTSUBSCRIPT italic_n - 1 end_POSTSUBSCRIPT , italic_y start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT , italic_y start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , … , italic_y start_POSTSUBSCRIPT italic_m - 2 end_POSTSUBSCRIPT.\n\n2.\n\nPass this concatenated sequence through the language model to obtain logits l𝑙litalic_l of shape (n+m−1,|V|)𝑛𝑚1𝑉(n+m-1,|V|)( italic_n + italic_m - 1 , | italic_V | ), where |V|𝑉|V|| italic_V | is the size of the vocabulary. The last m𝑚mitalic_m positions in these logits correspond to the predicted probability distributions for the target tokens y0subscript𝑦0y_{0}italic_y start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT to ym−1subscript𝑦𝑚1y_{m-1}italic_y start_POSTSUBSCRIPT italic_m - 1 end_POSTSUBSCRIPT, conditioned on the input x𝑥xitalic_x and the preceding target tokens.\n\n3.\n\nApply a log-softmax function to the last m𝑚mitalic_m logits to obtain log probabilities for the completion tokens only.\n\n4.\n\nCalculate the conditional loglikelihood of the target string y𝑦yitalic_y given the input x𝑥xitalic_x by summing the log probabilities of each target token:\n\nlog⁡P⁢(y|x)=∑i=0m−1log⁡p⁢(yi|x,y0,…,yi−1)=∑i=0m−1l⁢(n+i,yi),𝑃conditional𝑦𝑥superscriptsubscript𝑖0𝑚1𝑝conditionalsubscript𝑦𝑖𝑥subscript𝑦0…subscript𝑦𝑖1superscriptsubscript𝑖0𝑚1𝑙𝑛𝑖subscript𝑦𝑖\\log P(y|x)=\\sum_{i=0}^{m-1}\\log p(y_{i}|x,y_{0},...,y_{i-1})=\\sum_{i=0}^{m-1}% l(n+i,y_{i}),roman_log italic_P ( italic_y | italic_x ) = ∑ start_POSTSUBSCRIPT italic_i = 0 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_m - 1 end_POSTSUPERSCRIPT roman_log italic_p ( italic_y start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT | italic_x , italic_y start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT , … , italic_y start_POSTSUBSCRIPT italic_i - 1 end_POSTSUBSCRIPT ) = ∑ start_POSTSUBSCRIPT italic_i = 0 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_m - 1 end_POSTSUPERSCRIPT italic_l ( italic_n + italic_i , italic_y start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ) , (1)\n\nwhere log⁡p⁢(yi|x,y0,…,yi−1)𝑝conditionalsubscript𝑦𝑖𝑥subscript𝑦0…subscript𝑦𝑖1\\log p(y_{i}|x,y_{0},...,y_{i-1})roman_log italic_p ( italic_y start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT | italic_x , italic_y start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT , … , italic_y start_POSTSUBSCRIPT italic_i - 1 end_POSTSUBSCRIPT ) is the log probability of the i𝑖iitalic_i-th target token conditioned on the full input x𝑥xitalic_x and the preceding target tokens. (and where x,y0,…⁢y−1𝑥subscript𝑦0…subscript𝑦1x,y_{0},...y_{-1}italic_x , italic_y start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT , … italic_y start_POSTSUBSCRIPT - 1 end_POSTSUBSCRIPT denotes conditioning on only x𝑥xitalic_x.)\n\nWith this primitive for computing log⁡P⁢(y|x)𝑃conditional𝑦𝑥\\log P(y|x)roman_log italic_P ( italic_y | italic_x ), several options for evaluation (and decisions regarding hyperparameters) become available.\n\nMultiple Choice QA\n\nEquation 1 determines how to compute log⁡P⁢(y|x)𝑃conditional𝑦𝑥\\log P(y|x)roman_log italic_P ( italic_y | italic_x ). We now describe how to perform loglikelihood-based multiple choice as described by Brown et al. (2020): given k𝑘kitalic_k possible answer strings a1,a2,…,aksubscript𝑎1subscript𝑎2…subscript𝑎𝑘a_{1},a_{2},...,a_{k}italic_a start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , italic_a start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT , … , italic_a start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT, we compute the model’s answer to be argmax⁢(log⁡P⁢(a1|x),log⁡P⁢(a2|x),…,log⁡P⁢(ak|x))argmax𝑃conditionalsubscript𝑎1𝑥𝑃conditionalsubscript𝑎2𝑥…𝑃conditionalsubscript𝑎𝑘𝑥\\texttt{argmax}(\\log P(a_{1}|x),\\log P(a_{2}|x),...,\\log P(a_{k}|x))argmax ( roman_log italic_P ( italic_a start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT | italic_x ) , roman_log italic_P ( italic_a start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT | italic_x ) , … , roman_log italic_P ( italic_a start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT | italic_x ) ). In other words, the model selects the answer string with the highest conditional log probability given the input x𝑥xitalic_x.\n\nThis can be performed with worst-case k𝑘kitalic_k LM calls using the approach to calculate log⁡P⁢(y|x)𝑃conditional𝑦𝑥\\log P(y|x)roman_log italic_P ( italic_y | italic_x ) for each ai=ysubscript𝑎𝑖𝑦a_{i}=yitalic_a start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT = italic_y described above. However, the number of LM calls can be reduced if one or more answer strings are only a single token in length. Assume some aisubscript𝑎𝑖a_{i}italic_a start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT is only encoded by a single token z𝑧zitalic_z. Then, when calculating the loglikelihood of another answer string a0subscript𝑎0a_{0}italic_a start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT, we obtain the (log-softmaxed) logits of shape (n+m−1,|V|)𝑛𝑚1𝑉(n+m-1,|V|)( italic_n + italic_m - 1 , | italic_V | ) as an intermediate output. These logits contain the predicted log probabilities for each token in the vocabulary at each position, conditioned on the input x𝑥xitalic_x and the preceding tokens. To extract the loglikelihood of predicting the single-token answer aisubscript𝑎𝑖a_{i}italic_a start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT conditioned on x𝑥xitalic_x, we can simply select the element in l𝑙litalic_l corresponding to token z𝑧zitalic_z at position n𝑛nitalic_n. This logit represents the log probability of predicting token z𝑧zitalic_z immediately after the input sequence x0,x1,…,xn−1subscript𝑥0subscript𝑥1…subscript𝑥𝑛1x_{0},x_{1},...,x_{n-1}italic_x start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT , italic_x start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , … , italic_x start_POSTSUBSCRIPT italic_n - 1 end_POSTSUBSCRIPT.\n\nThus, we can calculate the loglikelihood of a single-token continuation “for free” and remove an additional LM call for each such single-token aisubscript𝑎𝑖a_{i}italic_a start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT.\n\nNormalization\n\nWhile the above approach uses the raw loglikelihoods of each given answer choice to select a model answer, other options are available. For instance, if each answer string aisubscript𝑎𝑖a_{i}italic_a start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT is different in length, this process may frequently default to selecting the shortest aisubscript𝑎𝑖a_{i}italic_a start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT simply because loglikelihoods are the sum over individual tokens’ log probabilities. Several options for normalizing these loglikelihoods are possible, as also described in Gao (2021):\n\n•\n\nToken-length normalization: each aisubscript𝑎𝑖a_{i}italic_a start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT’s loglikelihood is divided by misubscript𝑚𝑖m_{i}italic_m start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT, its length in tokens, to gain the per-token loglikelihood of each answer. This approach requires no additional LM calls, and is used alternately with raw loglikelihoods for most tasks by Brown et al. (2020).\n\n•\n\nByte-length normalization: each aisubscript𝑎𝑖a_{i}italic_a start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT’s loglikelihood is divided by its length in bytes, removing the dependence on the model’s tokenizer but still normalizing by answer string length. lm-eval provides this metric where applicable as acc_norm.\n\n•\n\nMutual Information: each aisubscript𝑎𝑖a_{i}italic_a start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT’s loglikelihood is defined as log⁡P⁢(ai|x)−log⁡P⁢(ai|n⁢u⁢l⁢l)𝑃conditionalsubscript𝑎𝑖𝑥𝑃conditionalsubscript𝑎𝑖𝑛𝑢𝑙𝑙\\log P(a_{i}|x)-\\log P(a_{i}|null)roman_log italic_P ( italic_a start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT | italic_x ) - roman_log italic_P ( italic_a start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT | italic_n italic_u italic_l italic_l ), where n⁢u⁢l⁢l𝑛𝑢𝑙𝑙nullitalic_n italic_u italic_l italic_l is either the empty string, a BOS token, or a placeholder such as \"Answer:\". This can be thought of as a notion of the pointwise mutual information (Shannon, 1948; Askell et al., 2021), log⁡(P⁢(ai|x)P⁢(ai))𝑃conditionalsubscript𝑎𝑖𝑥𝑃subscript𝑎𝑖\\log\\left(\\frac{P(a_{i}|x)}{P(a_{i})}\\right)roman_log ( divide start_ARG italic_P ( italic_a start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT | italic_x ) end_ARG start_ARG italic_P ( italic_a start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ) end_ARG ), which measures the increase in the likelihood of outputting aisubscript𝑎𝑖a_{i}italic_a start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT when conditioned on the input x𝑥xitalic_x, compared to the likelihood of outputting aisubscript𝑎𝑖a_{i}italic_a start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT unconditionally. Intuitively, this measure of mutual information captures the extent to which introducing x𝑥xitalic_x makes aisubscript𝑎𝑖a_{i}italic_a start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT more likely. Although this approach is nonstandard, it is provided in lm-eval under the option acc_mutual_info, and used selectively by Brown et al. (2020) and Askell et al. (2021) for certain tasks.\n\nComputing Exact Match\n\nIn addition to computing loglikelihoods and normalized loglikelihoods, we may also want to determine whether a given target string y𝑦yitalic_y would be produced by greedily decoding from the input x𝑥xitalic_x. Let z𝑧zitalic_z be the concatenation of x𝑥xitalic_x and y𝑦yitalic_y as defined in the previous sections, and let l𝑙litalic_l be the logits of shape (n+m−1,|V|)𝑛𝑚1𝑉(n+m-1,|V|)( italic_n + italic_m - 1 , | italic_V | ) obtained by passing z𝑧zitalic_z through the language model. To compute the exact match, we compute ∑i=0m−1𝟙⁢[yi=argmax⁢(l⁢(n+i,⋅))]superscriptsubscript𝑖0𝑚11delimited-[]subscript𝑦𝑖argmax𝑙𝑛𝑖⋅\\sum_{i=0}^{m-1}\\mathbb{1}[y_{i}=\\texttt{argmax}(l(n+i,\\cdot))]∑ start_POSTSUBSCRIPT italic_i = 0 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_m - 1 end_POSTSUPERSCRIPT blackboard_1 [ italic_y start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT = argmax ( italic_l ( italic_n + italic_i , ⋅ ) ) ], where 𝟙⁢[⋅]1delimited-[]⋅\\mathbb{1}[\\cdot]blackboard_1 [ ⋅ ] is the indicator function that returns 1 if the condition is true and 0 otherwise, and l⁢(n+i,⋅)𝑙𝑛𝑖⋅l(n+i,\\cdot)italic_l ( italic_n + italic_i , ⋅ ) represents the logits vector corresponding to the model’s output logits predicting the n+1+i𝑛1𝑖n+1+iitalic_n + 1 + italic_i-th token and therefore the i𝑖iitalic_i-th token position in y𝑦yitalic_y (0-indexed). Intuitively, this sum checks whether each token yisubscript𝑦𝑖y_{i}italic_y start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT in the target string y𝑦yitalic_y matches the most probable (argmax) token predicted by the model at each step of greedy decoding. If the sum equals m𝑚mitalic_m (the length of y𝑦yitalic_y), it means that all tokens in y𝑦yitalic_y would be produced by greedily generating m𝑚mitalic_m tokens starting from x𝑥xitalic_x. In this case, we return True to indicate an exact match. Otherwise, if the sum is less than m𝑚mitalic_m, we return False, indicating that y𝑦yitalic_y would not be produced by greedy decoding. Computing the exact match can b"
    }
}
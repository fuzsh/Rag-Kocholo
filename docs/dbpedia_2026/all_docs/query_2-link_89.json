{
    "id": "dbpedia_2026_2",
    "rank": 89,
    "data": {
        "url": "https://www.ncbi.nlm.nih.gov/pmc/articles/PMC7859568/",
        "read_more_link": "",
        "language": "en",
        "title": "Population health AI researchers’ perceptions of the public portrayal of AI: A pilot study",
        "top_image": "https://www.ncbi.nlm.nih.gov/corehtml/pmc/pmcgifs/pmc-card-share.jpg?_=0",
        "meta_img": "https://www.ncbi.nlm.nih.gov/corehtml/pmc/pmcgifs/pmc-card-share.jpg?_=0",
        "images": [
            "https://www.ncbi.nlm.nih.gov/coreutils/uswds/img/favicons/favicon-57.png",
            "https://www.ncbi.nlm.nih.gov/coreutils/uswds/img/icon-dot-gov.svg",
            "https://www.ncbi.nlm.nih.gov/coreutils/uswds/img/icon-https.svg",
            "https://www.ncbi.nlm.nih.gov/coreutils/nwds/img/logos/AgencyLogo.svg",
            "https://www.ncbi.nlm.nih.gov/corehtml/pmc/pmcgifs/logo-sageopen.png",
            "https://www.ncbi.nlm.nih.gov/pmc/articles/PMC7859568/bin/10.1177_0963662520965490-img1.jpg",
            "https://www.ncbi.nlm.nih.gov/pmc/articles/PMC7859568/bin/10.1177_0963662520965490-img1.jpg"
        ],
        "movies": [],
        "keywords": [],
        "meta_keywords": [
            ""
        ],
        "tags": null,
        "authors": [
            "Gabrielle Samuel",
            "Heilien Diedericks",
            "Gemma Derrick"
        ],
        "publish_date": "2021-02-11T00:00:00",
        "summary": "",
        "meta_description": "This article reports how 18 UK and Canadian population health artificial\nintelligence researchers in Higher Education Institutions perceive the\nuse of artificial intelligence systems in their research, and how this\ncompares with their perceptions about ...",
        "meta_lang": "en",
        "meta_favicon": "https://www.ncbi.nlm.nih.gov/coreutils/nwds/img/favicons/favicon.ico",
        "meta_site_name": "PubMed Central (PMC)",
        "canonical_link": "https://www.ncbi.nlm.nih.gov/pmc/articles/PMC7859568/",
        "text": "1. Introduction\n\nScholars in the field of the sociology of expectations have long shown how news reporting of innovative health technologies is over-emphasised through ‘breakthrough narratives’ (Brown, 2003; Fortun, 2008; Hilgartner, 2015; Petersen, 2018; Samuel and Kitzinger, 2013), and how this hype is not simply a by-product of innovation, but rather constitutes an innovation process itself: by envisaging futures in the present, it creates a positive vision of the technology, which acts performatively by securing funding in the present (Lehoux et al., 2017; Samuel and Farsides, 2017; Van Lente, 2012). The consequences of these performative effects have been explored in terms of both stakeholders and the public (Hilgartner, 2015; Samuel et al., 2017; Samuel and Kitzinger, 2013), as well as on how clinicians and researchers manage such expectations in their day-to-day practices (Gardner et al., 2015; Will, 2010). Some of this work has pointed to a disconnect between sensationalised public depictions of innovative technologies, and researchers’ more sober interpretations (see, for example, Beaulieu, 2002; Dumit, 2004). Pickersgill (2016) describes this as researchers’ ‘epistemic modesty’ (Will, 2010) whereby researchers admit the uncertainty, ambiguity and opacity of their field of study, and juxtapose this with what they see as overly optimistic media representations. As Pickersgill describes, this ‘epistemic modesty’ underscores a general reflexive awareness of the limits of scientific knowledge. Collins (1997, 1999) explains that this disconnect emerges because ‘distance leads to enchantment’ (Collins, 1997): those close to science (researchers working on the science projects) are often aware of the uncertainties of the methods they use, but because scientists tend to ‘black box’ areas of controversy and uncertainty, ‘those distant from the research front [other researchers, policymakers, funders, the public], and thus not exposed to the art and craft of scientific practice, get a view of science relatively free of doubts and uncertainties’ (p. 165). As such, explains MacKenzie (1998), the perceived uncertainty about matters such as the reliability, safety or predictability of a technology is higher for those closer to knowledge production. The further removed from science one becomes, the less uncertain the research appears, leaving an ‘alien science’ (Collins 1999; Hedgecoe, 2006), in which a habitable ‘space’ emerges for socio-technical futuristic expectations to proliferate (Brown and Michael, 2003; Hedgecoe, 2006).\n\nIn the health domain, artificial intelligence (AI) systems are an example of an innovative health technology. AI is a branch of computer science involving the development of a range of heterogeneous computer algorithms to accomplish tasks traditionally associated with human intelligence, such as the ability to learn and solve problems (Tang et al., 2018). The general aim of AI research is to use computer algorithms to analyse their environment through data, uncover relevant information from the data, and make predictions about the data in order to take actions to achieve specific goals (He et al., 2019). In the health and medical domain, machine learning is probably the most commonly used AI system. In comparison to a general AI system, which is intended to perform most activities that humans can perform, machine learning constitutes ‘narrow AI’, and can perform only one or a few specific tasks. It includes, for example, deep learning, neural networks and natural language processing (Ho, 2019). Machine learning systems have been used to help in the development of diagnostic aids for, for example, eye disease, heart disease, stroke, Alzheimer’s Disease (Loh, 2018; Tran et al., 2019). They are also being used to improve medical imagining, where standardised images (x-rays, pathology slides, photographic images, etc) provide relatively uncomplex datasets for AI-based visual pattern recognition. Here, much work has explored the utility of AI-based systems in the prediction and diagnosis of various cancers, such as skin cancer and breast cancer. Alongside this, AI systems have been incorporated into robotic surgical devices; used for the detection of genetic disorders (Gurovich et al., 2019); and tested as a predictor of responses to certain medications (Loh, 2018). In population health research, researchers are exploring how AI systems can help predict epidemics and disease outbreaks (Bengtsson et al., 2015; Harris et al., 2017; Naghavi et al., 2010), as well as control current pandemics.1 They are also exploring how AI systems can help individuals with various health and mental health states through the use of ‘digital phenotyping’, for example, by using AI systems as a predictor for suicide attempts and depression (Birk and Samuel, 2020; Blasimme and Vayena, 2019).\n\nSensationalist discourses have often accompanied the reporting of AI research in a series of AI-hype bubbles that have repeatedly peaked and troughed over the past 70 years (Bory, 2019; Chuan et al., 2019; Elish and Boyd, 2018; Fast and Horvitz, 2016; Krijgsman, 2018; Laï et al., 2020; Natale and Ballatore, 2017). Such discourses closely link to two competing imaginaries of, on one hand, pessimism, with concerns around dystopian surveillance, and on the other hand, with utopian views of AI systems spurring innovation and acting as a powerful tool to address various societal ills (Boyd and Crawford, 2012; Brennen et al., 2018; Elish and Boyd, 2018; Fast and Horvitz, 2016; Krijgsman, 2018). While media discourses of AI have long been explored, to date, little is known about how the media specifically portray AI in the context of health research, or how health researchers navigate this.\n\nThe aim of this pilot paper is to draw on work studying promissory discourses and the sociology of expectations to explore how population health researchers working with AI systems in Higher Education Institutions (HEIs) navigate their work with relation to the public presentation of AI systems. The aim is to also triangulate these findings with a small scoping analysis of how the news media portray AI specifically in the context of health research. Our study focuses on the United Kingdom and Canada, with the latter from English-speaking regions only. We conducted 18 interviews with UK and English-speaking Canadian researchers, and a small pilot analysis of the way in which UK and English-speaking Canadian newspaper articles portray the use of AI systems for population health research.\n\n4. Discussion\n\nAs has been shown in similar studies (Laï et al., 2020), our interviewees viewed AI systems solely as a methodological instrument, one of a number of research tools, non-exceptionalist, not exciting, and uncertain in terms of its capabilities. They imagined the field of implementing AI-based solutions as far from reality, and that predictions are ‘goals that motivate and drive their work, not accurate depictions of the state of the art ..[..].. as with any nascent and emerging field, what is behind the curtain is full of contested boundaries and uncertainties, methodological challenges and epistemological pitfalls’ (Elish and Boyd, 2018: 69). Our researchers juxtaposed these views of AI systems with their perceptions of how AI systems are publicly constructed as either over-enthusiastic in terms of their benefits, or as posing an imminent danger. While these perceptions are supported by the literature (Elish and Boyd, 2018), they were only partially supported by our news article analysis. While several articles reported on dystopian eliciting stories associated with AI surpassing health practitioners’ abilities, this (almost) dystopian narrative was of much less focus than our interviewees suggested in their narratives. Our news articles overwhelmingly constructed AI health research using the socio-technical imaginary (Jasanoff and Kim, 2009) of optimism, newness and certainty. As such, interviewees were likely drawing their perceptions about the public portrayal of AI from the reporting of AI more generally rather than the reporting of health research specifically. This is important to note, since if AI researchers are unable to see the potential impact of their work through the public’s eyes (or have misconceptions about this), this could have implications for the way in which they frame their own research when communicating about it to both professional and public stakeholders.\n\nThe separation of sober representations and uncertainty of AI methods by interviewees against the more sensationalist expectations of the media is unsurprising given the literature on ‘alien science’, uncertainty and the generation of expectations about science (Brown and Michael, 2010; Collins, 1999; Hedgecoe, 2006). Another way of interpreting this separation, and a way that also reflects the non-exceptionalist properties our interviewees gave to AI systems, is that our interviewees’ closeness to their work normalises them to their AI research methods and tools. What at a distance is perceived as exciting and new, close up, with the monotony of daily use, is something less exciting, more ‘normal’ and therefore non-exceptional.\n\nOur interviewees’ focus on the uncertainty of their work is in line with Pickersgill’s (2016) concept of epistemic modesty, in which researchers admit the uncertainty, ambiguity and opacity of their field of study. The role for describing their work as uncertain seemed to relate to the knowledge machinery of scientific practice, whereby interviewees’ discussed problems associated with their work just like when they are talking informally to colleagues (Pickersgill, 2016). It is likely that such discussions of uncertainty were also used by interviewees to dampen the expectations around AI systems, whereby interviewees were acting responsibly by communicating with scientific professionalism and rendering problematic the research of competitors (Pickersgill, 2016). This, explains Pickersgill (2016), is associated with an ‘empathetic citizenship’ (p. 197), whereby researchers have a long-standing, intrinsic desire to try and ensure that false hopes and expectations on the part of non-scientists are lamented and, where possible, minimised (p. 197). In fact, interviewees drew on the ‘them and us’ narrative to situate their responsible selves at a distance from other researchers who they perceived as being inappropriately ‘epistemically ostentatiousness’ (Pickersgill, 2016) – framing AI research as new, innovative, ‘cool’ or exciting for the self-interested purposes of attracting investment.\n\nFinally, we cannot conclude from the above that our interviewees enacted these responsibilities themselves, or that they did not adopt similar epistemic ostentatiousness outside of the interview setting. It has long been known that researchers describe their research using a number of context specific discourses, reporting it to be more or less uncertain depending on the specific circumstance (Evans et al., 2009; Gilbert and Mulkay, 1984). In fact, researchers must often learn to manage sensationalist expectations (Gardner et al., 2015) by navigating the regimes of hope in their own work (epistemic ostentation) with regimes of truth (Moreira and Palladino, 2005; epistemic modesty). However, our interviewees’ narratives hinted at the authenticity of their self-proclaimed responsibilities and their desire to maintain this epistemic modesty as much as possible. Interviewees seemed to be concerned about communicating their AI research to health professionals appropriately, and seemed to ‘care about the production of knowledge, about the enthusiasm that this can stimulate in non-scientists, and about the importance of precision in communicating developments . . . to those without sufficient expertise themselves to adjudicate new claims’ (Pickersgill, 2016: 197). This was described by our interviewees as sometimes being difficult because some health practitioners were perceived to actively want to lose the uncertainties surrounding AI systems when using the technologies, preferring the science to remain ‘alien’, just as long as the AI system promised to provide a rapid and efficient decision-making aid. This is problematic for a range of reasons, not least because when ‘alien science’ comes to be seen as ‘simply science’ (Pickersgill, 2016: 194) outside of the research specific environment, decision-making about research funding and use can ultimately be made with little attention paid to the research’s inherent uncertainties.\n\nAs a final note, two implications from our findings do not directly relate to this article’s aims – being more related to the initial aim of our research project to explore ethical issues related to AI population health research – but nevertheless, deserve mention. First, our researchers perceived that unrealistic expectations around AI could be addressed by better communication with health professionals/the public, which would improve understanding and build trust. At times, our interviewees’ narratives suggested a deficit-type one-way approach to engagement (researcher to public/stakeholder) to help fill the knowledge gap to build this trust. However, the relationship between communicating AI knowledge and trust in AI is complex. Individuals may not always need (and as we have shown above, want) to understand a technology to trust it (and vice versa), and trust in a technology may be more underpinned by trust in broader social systems and networks, such as trust in the institutions developing or implementing the technology, or trust in technology governance more generally, than trust in the technology itself. Trust in a technology may also be related more to experiences, such as individual/organisational experiences of working with technologies and/or beliefs about how technology can be accommodated in their workflows. Also, trust does not always equal trustworthiness.\n\nSecond, reflecting on the fact that our researchers seemed to view AI as an unproblematic methodological research tool, social science scholars have long alerted us to the fact that methods and methodological tools are not uncontroversial in that they can be separated from ethics. Rather, methodological tools and design embed ethical choices (Markham, 2006; Markham et al., 2018). It is worth therefore re-iterating, that when developing or using AI software, AI researchers must remain ethically alert to the fact that all research tools, including AI, have biases and assumptions built into them (Berendt et al., 2015; Elish and Boyd, 2018; Gillespie, 2014; Luka and Millette, 2018; McQuillan, 2018; Vis, 2013; Zimmer, 2018)."
    }
}
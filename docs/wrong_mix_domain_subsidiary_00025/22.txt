11. The Start-ups (1987-90)

by Piero Scaruffi

The Software Industry

Since CSC's IPO of 1963, very few software companies had gone public. The first software company to "go public" after CSC had been Cullinet in 1978, and Cullinet had also been the first software company to be valued at over a billion dollars. In 1971 the Nasdaq had opened to raise capital specifically for small high-tech firms, but it had remained a niche stock market. Software, however, ended up benefiting from a change enacted in 1978 by the USA that allowed pension funds to invest in venture capital funds. It wasn't that venture capitalists fell in love with software companies but simple statistics: the amount that venture capital firms managed in 1977 was $2.5 billion; in 1983 it had skyrocketed to $12 billion. The US economy had suffered a decade of high inflation and stagnation following the 1973 OPEC crisis. It had come out of the tunnel in 1983, which turned out to be the best year for IPOs since 1969. There was also pressure on investors to diversify because the US economy was transitioning out of the auto, steel, oil and aircraft economy that had dominated the country for almost a century. In 1950 the USA had produced about two-thirds of all the cars manufactured in the world, and 47% of the world's steel, but by 1980 Japan was threatening US supremacy in both cars and steel (the US share of car production had fallen to 21%, the US share of steel production was down to 14%). High-tech startups benefited from all these factors, simply because there was so much money and so much enthusiasm. The existence of the Nasdaq had certainly helped steer some of the available money towards young, small, software startups. Computer Associates had gone public in 1981. Lotus had been one of the beneficiaries of the 1983 boom. That year there were, in fact, 173 IPOs in the high-tech industry of the USA. For the first time in the history of computers, in 1983 investors had moved away from hardware and towards software. The feeling was that hardware had little left to say after IBM had standardized the world of both small and large computers, and after the rapid rise of Japan in the semiconductors industry. In 1984 stock prices of high-tech companies had falled across the board, but the momentum remained and in 1986 several software companies (Microsoft, Oracle, Adobe) went public. In 1985 Lotus replaced Cullinet as the largest software company in the USA (by sales). In 1990 Microsoft would cross the $1 billion mark in revenues and, thus, overtake Lotus. By 1990 software was receiving (17.4%) twice as much venture capital money as hardware and biotech, just before "the dotcom boom". Improved access to public equity finance helped the young and small high-tech companies succeed in fields dominated by large well-established corporations. The latter started losing market share to the newcomers in the 1980s and would continue to bleed throughout the 1990s.

The late 1980s witnessed a flurry of new computer models to replace the old text-only systems with graphical environments, and to add more power and more memory. The main beneficiaries were the software companies, whose power kept increasing.

However, no West Coast company could compete with the giants of the East Coast. IBM remained by far the largest software company in the world: only 13% of IBM's 1989 revenues came from software, but that corresponded to $8 billion (IBM's total revenues of $62 billion constituted about a third of the world's total revenues for hardware and software). The first software company to reach one billion dollar in sales was Computer Associates in 1989, based in New York and still mainly focused on applications for mainframe computers. Massachusetts-based Lotus, which had sold over four million copies of 1-2-3 by 1988, now trailed behind Oracle, with sales of $692 million in 1990. In january 1990 Microsoft released Office for Windows, which integrated Word, Powerpoint and Excel. The days of Lotus domination in the application business were numbered.

Data Management in the Bay Area

At the turn of the decade the largest software company in the Bay Area was Oracle, whose relational database management system had become the de-facto international standard after IBM had involuntarily legitimized SQL in the mid 1980s. The difference between IBM and Oracle is that Oracle had targeted minicomputers and notably Unix. In 1982 (before IBM introduced the DB2) Oracle had 24 employees, a customer base of 75 companies and revenues of $2.5 million. In 1987 revenues had reached the $100 million mark. In 1989 they skyrocketed to $584 million, and then they almost doubled in one year, just short of a billion dollars for the fiscal year ending in may 1990. This was one case in which a much bigger competitor had helped, not killed, the independent by legitimizing its (better) technology. Seattle-based Microsoft was slightly ahead of Oracle with revenues of $804 millions in 1989.

The other database manufacturers of the Bay Area were also booming. In 1988 Sybase became the darling of the stock market by introducing a client/server relational database, SQL Server, and signing a deal with Microsoft to port SQL Server to Windows (the joint project would originate Microsoft's own namesake product). In 1989 Phil White took over at Informix and turned it around in a dramatic way. By 1993 it would become one of the Bay Area's most valuable companies.

In april 1988 Michael Pliner (former founder of networking company Sytek) starteed Verity, basically a spin-off from Advanced Decision Systems and largely staffed with MIT alumni such as Clifford Reid, to capitalize on ADS' text retrieval system for topic-based queries. Topic, initially designed for the Strategic Air Command of the Air Force by David Glazer and Phil Nelson, was one of the first commercial "search engines".

Legato, founded in september 1988 in Mountain View by Bob Lyon and Russell Sandberg (the SUN engineers who had helped Bill Joy create NFS), introduced a device to improve the performance of SUN's NFS, but it would soon become a leader in the more strategic field of cross-platform automatic data backup and recovery.

The increasing importance of sharing data across applications on a variety of software platforms in real time spawned a new industry of "middleware". In 1987 Indian-born Boston-based Vivek Ranadive founded Teknekron Software Systems (which in 1997 would evolve into TIBCO and relocate to Palo Alto) to market an "Information Bus" capable of shuttling mission-critical data between software programs. It rapidly digitized Wall Street's transactions.

At the hardware level, SanDisk founded in 1988 by two EEPROM experts from Intel, Israeli-born Eli Harari and Indian-born Sanjay Mehrotra, was soon to become a significant competitor of Asian leaders Toshiba and Samsung in the strategic field of flash memory cards.

Xerox's Inventions at Work

In january 1990 Adobe released Photoshop for the Macintosh, thus completing the desktop-publishing revolution. Photoshop, actually developed by Thomas Knoll, a student at the University of Michigan, and acquired by Adobe in 1988, allowed ordinary computer users to do things that were difficult even for a professional printer. IBM itself introduced the Interleaf desktop-publishing software for its personal computers.

In 1988 a Los Angeles-based company, Elixir, ported the Xerox Star windowing GUI to the IBM PC, but it was too little too late to compete with the Macintosh. Smalltalk, the first object-oriented environment, had never been marketed by its inventor, Xerox. Los Angeles-based Digitalk had already developed in 1983 a version of Smalltalk for the IBM PC, the first commercial release of Smalltalk. In 1989 Adele Goldberg, who had taken over management of Smalltalk at Xerox PARC from Alan Kay, founded ParcPlace in Mountain View to market a version of Smalltalk for Unix, Windows, and Macintosh platforms. Both companies (that eventually merged) were trying to capitalize on the appeal of Smalltalk's elegant software development environment and graphical user interface, but Smalltalk was an interpreted language and therefore very slow. Last but not least, it was not endorsed by any major company.

Documentum, based in Pleasanton and founded in january 1990 by Howard Shao of Xerox PARC and John Newton of Ingres, was incubated by Xerox' own venture-capital arm, Xerox Technology Ventures, to consolidate PARC's efforts in document management software.

In 1988 Alan Cooper (of Digital Research and General Ledger fame) sold Microsoft a visual form generator ("Ruby") that, combined with Microsoft's BASIC programming language, would become Visual Basic. The spirit was pure PARC: a friendly environment to develop business applications on personal computers. It also introduced a new way for people to augment a system by seamlessly integrating third-party software code ("widgets") into the system.

Meanwhile, the PARC's other great invention, the Ethernet, had spawned an industry of its own, and the demand for "broadband" was increasingly dramatically. Sensing an opportunity, Indian-born inventor Vinod Bhardwaj founded Sunnyvale-based Kalpana in 1987 that in 1990 introduced the first Ethernet switch. A "switch" is a device that connects the computers of a Local Area network (LAN). The switch optimizes the traffic of data packets, thereby reducing bandwidth usage and increasing the overall bandwidth of the network. Another early player in that market was Crescendo Communications, founded in 1990 by Italian-born former Olivetti executives Luca Cafiero and Mario Mazzola.

Software for Manufacturing

The ERP industry came to the Bay Area in 1987, when David Duffield founded PeopleSoft in the east bay. SAP dominated that market. In the USA the leader was JD Edwards. Both their platforms ran on mainframes. PeopleSoft started with the idea of taking the human-resource management system developed for the mainframe by Duffield's previous company, Integral Systems (that he had founded in 1972 in New Jersey but relocated in 1976 to Walnut Creek, near Berkeley), and port it to a client-server architecture. The product rapidly overtook the mainframe-based competition, generating revenues of $1.9 million in 1989 and $6.1 million in 1990. PeopleSoft went on to overtake JD Edwards and eventually absorb it.

Meanwhile, a new sector of software for manufacturing was being invented in Detroit. In 1982 General Motors began to plan a new car, the Saturn. Meanwhile GM had hired Wharton Business School's professor Morris Cohen to improve its factories. Cohen's proposals went into the new Saturn factory, and basically amounted to linking all GM dealers with the factory's mainframe via satellite. Cohen had modeled GM's business as a series of "supply chains", each one representing a stream of "resources" (such as raw materials and third-party components) towards a finished product that is delivered to the end customer. By doing that, the Saturn factory had built the first Supply Chain Management (SCM) system, integrating suppliers, factory and customers. A company named Intellection (later renamed i2) was founded in Dallas in 1988 by two former Texas Instruments employees, Indian-born Sanjiv Sidhu (who had worked in the Artificial Intelligence lab) and Myanmar-born Ken Sharma to create an independent SCM software package (called Rhythm). Needless to say, the border between ERP and SCM was blurred at best.

Locally in the Bay Area, there were many companies serving the needs of the semiconductor industry. For example, Rasna Corporation, founded in november 1987 in San Jose by George Henry and other engineers of IBM's Almaden Research Center (plus Keith Krach, founder of the robotics division of General Motors), sold computer-aided engineering tools for the semiconductor industry (and it was eventually purchased by Parametric Technology of Boston that had just purchased CAD pioneers Evans & Sutherland).

3D Printing

Silicon Valley also missed the train on one of the most important innovations in manufacturing: 3D printing. There were multiple inventors, because "additive manufacturing" can be implemented in many different ways. In 1967 Wyn Swainson, still a student in Denmark, applied for a patent titled "Method of Producing a 3D Figure by Holography" that was probably the first kind of 3D printing. He completed his studies in chemistry at UC Berkeley, obtained a patent in 1971, and opened a company called Formigraphic in Bolinas, north of San Francisco. In 1974 Formigraphic (later renamed Omtec Replication) demonstrated the printing of a 3D object. Charles Hull filed a patent in 1984 for "stereolithography" or SLA, a laser-based process that works with liquid resins (he was still working for a company called UVP in Los Angeles). In 1986 Carl Deckard at the University of Texas invented Selective Laser Sintering (SLS) that applied the laser to the ancient technology of sintering and could print a wide range of materials: plastics, ceramic, metal, etc; but Deckard's idea had been preceded in 1979 by a similar invention by Ross Housholder in Las Vegas. In 1987 Michael Feygin in Los Angeles invented Laminated Object Manufacturing (LOM), also known as "paper 3D printing" because it uses paper. In 1988 Frank Arcella at Westinghouse in Pittsburgh invented Laser Additive Manufacturing (LAM) for making metal parts, a technique that used a high-power laser and titanium powder. Fused Filament Fabrication (FFF) or Fused Deposition Modeling (FDM) was developed by Scott Crump in Minnesota in 1989, who instead used a filament of molten plastic. As it is often the case with US inventions, some foreigners had come first. In 1980 Hideo Kodama of Nagoya Municipal Industrial Research Institute in Japan had already published the general idea of 3D Printing. Alain LeMehaute working at General Electric in France had filed an SLA patent a few weeks before Charles Hull for the same technology. In 1984 Yoji Marutani of the Osaka Prefectural Industrial Research Institute (OPIRI) invented his own version of stereolithography; and in 1986 patents for 3D printing were filed by Takashi Morihara of Fujitsu in Japan and Itzchak Pomerantz of Cubital in Israel. Hull introduced his first commercial 3D printer, called SLA-1, in 1988 with his Los Angeles-based company 3D Systems. Crump founded Stratasys in 1989 and shipped his first 3D printer, called 3D Modeler, in 1991 (FFF was patented by Stratesys, FDM is very similar but more open). In 1989 Deckard founded Nova Automation, later renamed Desk Top Manufacturing (DTM) that manufactured the first SLS printer in 1990, the Mod A. Feygin started selling LOM printers in 1991 under the company name Helisys, later renamed Cubic.

3D Scanning was the complementary technology to 3D printing and it was also born at about the same time. LIDAR (the name was born as a combination of "light" and "radar", but now it stands for "Light Detection and Ranging") is a radar device that uses light instead of sound. In 1960 NASA began experiments on a device projecting lasers and then capturing the signal that bounces off objects. In 1972 the first satellites of the Global Positioning System (GPS) were launched, and the lidar started being used to rapidly capture shapes of landscapes and to construct three-dimensional models. In 1977 Michel Clerget, Francois Germain and Jiri Kryze at the national French laboratory IRIA (later renamed INRIA) invented the laser-based 3D scanner. In 1989 Jeremy Dunn built the first lidar "gun" for the police, that helps detect drivers violating the speed limit. The first commercial product was a head scanner introduced in 1987 by David and Lloyd Addleman's Cyberware Laboratories of Monterey (south of the Bay Area). The lidar basically offered the best of two worlds: a cameras is an optical system, that, like the eye, sees the shape and size of objects but doesn't see well in the dark or in bad weather; the radar, on the other hand, can detect objects and how far away they are, but cannot provide information about their shape and size.

Outsourcing the Fab

In april 1989 Intel introduced the 80486, which contained 1.2 million transistors and performed 20 million instructions per second. The truth is that software (both system software and application software) had trouble keeping up with the progress in hardware. Hardware indirectly enabled a lot of functions that software was not exploiting yet. So, in a sense, IBM was right to ignore (yet again) the new microprocessor, while its competitors Compaq, Olivetti and Zenith rushed to introduce 486-based computers.

However, the real news for Silicon Valley's semiconductor industry was not technological but logistical. In 1985 the government of Taiwan hired Chinese-born Texas Instruments' vice-president Morris Chang to run the Industrial Technology Research Institute (ITRI). Chang promoted the outsourcing of semiconductor manufacturing by USA companies to Taiwanese companies, and in 1987 he personally founded the Taiwan Semiconductor Manufacturing Corporation (TSMC). Taiwanese companies were able to slash costs, mainly because of cheap labor. This led to the establishment in Silicon Valley of "fab-less" semiconductor companies, i.e. hardware companies that did not own a factory but instead used TSMC for the actual manufacturing process. The model was pioneered by Chips and Technologies ((founded in 1985 by Gordon Campbell and Filipino-born Dado Banatao) and Xilinx (founded in 1984 by Zilog's engineers Ross Freeman and Bernard Vonderschmitt), and particularly by their executives Gordon Campbell and Bernard Vonderschmitt. Cirrus Logic (founded in 1981 by Indian-born MIT alumnus Suhas Patil) and Adaptec (founded in 1981 by Laurence Boucher) were other early fab-less chip "manufacturers" in the valley.

The fab-less phenomenon became a form of inter-firm cooperation in disguise. Whenever a Silicon Valley manufacturer outsourced a project to a Taiwanese fab, it directly improved the Taiwanese plant both by injecting capital and by the project's new requirements. This indirectly constituted a favor to the competitor who would outsource next to the same plant. As long as this meant that the shared Taiwanese plant was going to be always better at serving one's needs, it was accepted that it would also be better at serving the competition's needs. It was like living in a polyandrous relationship in which it is ok to share the same wife as long as that wife gets better and better at being a housewife for all the husbands.

Unix and Internet

While Microsoft operating systems were spreading from computer to computer, Unix continued to stumble. Unix had been stuck in a time warp of sorts after AT&T and SUN had taken separate roads with incompatible implementations. Just when peace was being signed by AT&T and SUN, the war resumed in 1988 on another front: IBM, DEC, Hewlett-Packard and others formed the Open Software Foundation (OSF) to enact an open Unix standard, clearly a preemptive strike against the blossoming AT&T/Sun alliance.

Elsewhere in the USA, the late 1980s continued the trend towards networking at all levels. In 1988 Bellcore, the descendant of the glorious Bell Labs (that AT&T had to abandon when the government broke it up in 1984), invented "Digital Subscriber Line" (DSL), a communications technology that provided broadband on a regular phone line. Price permitting, this allowed every household in the world to use their existing phone line to establish a high-speed connection with a computer, i.e. this enabled the transmission of bulky files (such as the ones produced by scanning a document or by a digital camera).

In order to access the Internet, a user needed an account (a login name and password) at a university or research laboratory or government agency. With few exceptions the user interface was Unix, so the Internet user needed to be fluent in Unix commands. The old UUCP protocol was still the main way to provide access to Usenet and e-mail. UUCP was the invisible "language" spoken by the nodes of the Internet as they transferred and forwarded data. Rick Adams, a system administrator at the Center for Seismic Studies in Virginia, pioneered the idea of providing those UUCP-based services commercially to companies in 1987 when he founded Uunet, the first independent Internet Service Provider (ISP). Initially it simply provided access to Usenet and e-mail, but in 1990 it also launched its AlterNet that made the Internet even easier to access. Uunet's customers were mainly companies that needed to do research on the Internet. At the same time in 1989 Barry Shein in Boston started the ambitiously named "The World", another pioneering ISP. Every small business willing to purchase a modem could get on the Internet. There was precious little for ordinary households on the Internet other than e-mail, but every household could do the same. In 1989 CompuServe, Ohio's pioneer of "dial-up" time-sharing services, connected its proprietary e-mail service to the Internet, thus allowing its customers to exchange e-mail with Internet users. CompuServe was pushing information to their customers, providing some support for special-interest discussion groups and offering a chat system. Cheap computer models with built-in modems began to appear.

The main addition to the Internet was perhaps the one developed by Jarkko Oikarinen in Finland and debuted in 1988: Internet Relay Chat, basically an Internet version of "Relay", developed by Jeff Kell in Tennessee in 1985 for the academic network Bitnet. That was the birth of real-time text transmission over the Internet, i.e. of instant messaging and online chat.

Fighting computer viruses became big business. The vast majority was propagated by simply copying files from one disk to another, but several were already spreading over networks. In 1987 John McAfee founded in Santa Clara one of the earliest companies specializing in anti-virus software. In 1988 Eva Chen founded Trend Micro in Taiwan and Tjark Auerbach founded Avira in Germany. In 1991 Symantec released Norton Anti-Virus.

The bad news is that in 1988 "Morris", the first Internet "worm", unleashed by a graduate student at Cornell University, infected most of the Internet, an omen of the dangers of networking.

As computer networks mushroomed, it became economically unfeasible to provide every terminal user with expensive computers such as Unix workstations. In 1984 the MIT had created the specifications for a "thin client", i.e. a very simple machine to connect to a network. Simply called "X", it created an industry of "X terminals". They also came to be associated with graphics because the X protocol mandated a graphical user interface (GUI) on the terminal and enabled the transmission and remote display of high-resolution images. For example, Network Computing Devices was founded in 1987 in Mountain View by Doug Klein and others (including a young Martin Eberhard, who had worked at San Jose's ASCII terminal maker Wyse Technology) and soon run by Judy Estrin, a pupil of Vint Cerf at Stanford and the founder of Bridge Communications, to sell cheap Unix terminals.

However, the Internet was still mainly a tool for research labs and government agencies. In 1988 the first implementation of the Integrated Services for Digital Network (ISDN), an international communications standard enacted by the International Telegraph and Telephone Consultative Committee (CCITT), heralded the age of digital transmissions of voice, video, data over traditional telephone networks and telephone copper wires.

In 1990 DARPA transferred control over the Internet to the National Science Foundation. Note that commercial activities were still banned on the Internet itself, although common on several private networks such as CompuServe (Ohio), Prodigy (formed in 1984 in New York state by CBS, IBM and Sears), GEnie (a General Electric spin-off of 1985 based in Maryland), Quantum Link (launched in 1985 in Virginia and later renamed America OnLine) and Delphi (founded by Wes Kussmaul in Boston in 1983). Email, electronic bulletin boards and online news were already the norm for many users of the Internet and for many customers of those private for-pay networks.

Collaborative software platforms called "groupware" (a term coined by Peter and Trudy Johnson-Lenz in 1978) became popular in the 1990s. Ray Ozzie and other former students of the University of Illinois' CERL, who had grown up with the online community PLATO Notes, ported Notes to the personal computeri with funding from Lotus. The result in 1990 was Lotus Notes, a system for interconnected personal-computer users to share a project. Because it allowed people at the bottom of the organization to interact directly, without the mediation of the management, it was the precursor of social networking.

By 1986 the GPS had matured into a reliable technology and the US government was ready to allow commercial use of it. That year Ed Tuck, a Los Angeles-based telecom veteran who was also an aviator, came up with the idea of a hand-held device to connect ordinary people with the GPS. He founded Magellan with Don Rea of Omicron Labs. Using the gallium-arsenide chip made by Oregon startup TriQuint Semiconductor, an Intel 8086 processor and a digital ASIC by VLSI Logic, in 1989 they produced the first commercial navigation device: the Magellan NAV 1000. Their GPS devices were used in 1991 during the invasion of Iraq, which became the first widely-publicized success story of the GPS. Magellan was followed by Garmin in 1991.

While little of this was done in the Bay Area, someone in Silicon Valley understood what it all meant. Mark Weiser at Xerox PARC coined the term "ubiquitous computing" in 1988, prophesizing that the world was just beginning to enter a new era of computing, not personal computing but ubiquitous computing.

Laptops and Videogames

Two off-shoots of the personal computer were becoming increasingly significant in terms of revenues, and both were dominated by Japanese companies.

After Toshiba had revolutionized the field, there had also been rapid progress in creating smaller mobile computers with the power of a PC: in 1988 Compaq introduced its first laptop PC with VGA graphics, the Compaq SLT/286; in 1989 NEC released the UltraLite, an even lighter laptop (but still based on the older 8086 processor); and finally in september 1989 Apple released the first Macintosh portable. In 1989 GRiD Systems introduced what was possibly the world's first pen-based handheld computer, the GRiDPAD. In 1990 the number of mobile personal computers sold in the USA skyrocketed to more than one million (versus eight million desktops).

The leadership for videogame consoles had decisively shifted to Japan: in 1988 Sega introduced the Mega Drive/Genesis and in 1990 Nintendo introduced the Super Nintendo Entertainment System, both destined to sell tens of millions of units. No video console introduced in the USA in those years would even remotely compete with the Japanese numbers. However, a new concept was being born in the Bay Area that would have wide-ranging consequences. SimCity was a simulation game, first released in 1989 and created by game designer Will Wright for Jeff Braun's Maxis in Emeryville (near Oakland). It was different in that there was no winning and no losing: the player simply created a city.

The "Wintel" personal computer (Windows operating system, Intel microprocessor) was becoming a serious game platform thanks to a generation of VGA-compatible graphics cards. In 1990 Texas' family-run Origin released an epochal game, Chris Roberts' Wing Commander, whose almost three-dimensional graphics represented a quantum leap forward for the industry. The belief that games running on personal computers could not compete with games running on consoles was shattered forever.

The first Computer Game Developers Conference (later renamed Game Developers Conference) was organized in April 1988 by Chris Crawford and took place in his San Jose living room.

Biotech and Nanotech

The futuristic industries of the Bay Area, biotech and nanotech, were at different stages of growth.

In 1990 Swiss pharmaceutical giant La Roche acquired a majority stake in Genentech, that had become the largest biotech company in the world with revenues of about $400 million. The takeover legitimized the whole industry.

The government entered the picture of the bioscience in a grand way. In october 1988 the National Institutes of Health in collaboration with the Department of Energy established the Office for Human Genome Research, later renamed the National Human Genome Research Institute (NHGRI), and named James Watson, one of the co-discoverers of the structure of DNA, as its first director. The USA and other nations (eventually Britain, China, Japan, Germany, France, Canada and New Zealand) launched the International Human Genome Project with the mission to determine the sequence of the human DNA and to map the 25,000 genes of the human genome.

The term "nanotechnology" (originally introduced by Japanese scientist Norio Taniguchi in 1974) had been popularized by Eric Drexler's book "Engines of Creation - The Coming Era of Nanotechnology" (1986). Drexler also founded the Foresight Institute in Menlo Park with Christine Peterson. "Nano" referred to technology that operates at the atomic and molecular scale, 100 nanometers or smaller. Materials are built from atoms. The configuration of the atoms can produce materials with completely different properties, like coal versus diamond, or sand versus silicon. Molecular manufacturing would open a new era for the fabrication of materials. This vision was originally concocted by theoretical physicist Richard Feynman in 1959. Progress in nanotechnology was enabled by the invention of the Scanning Tunneling Microscope (STM) in 1981 that allowed scientists to work on individual atoms, and by the invention of the Atomic Force Microscope in 1986. In 1989 Don Eigler at IBM's San Jose Almaden labs carried out a spectacular manipulation of atoms that resulted in the atoms forming the three letters "IBM". The problem with nanotechnology was, of course, that its tools were extremely expensive. The electron synchrotron nicknamed "Advanced Light Source" in construction at the Lawrence Berkeley Lab (completed in 1993) was designed to generate laser-like beams 10,000 times brighter than the brightest light ever produced on Earth. It was the ideal tool for exploring the structure of materials, for observing the nucleus of biological cells and for building subatomic microstructures; but even that wasn't "nano" enough.

The Culture of Risk

While all of this was going on, the Bay Area contributed relatively little to groundbreaking technological innovation. It was mostly incremental evolution. The real innovation was in another dimension: Silicon Valley did not depend anymore on the military industry, and its financial independence had created a new business model that questioned the old world order in many ways. Funding from the military had helped create a very stable (hardware) industry that, a few decades later, had spawned a new highly unstable industry of small software companies. Behind it there was a new attitude towards risk, that somehow may have been part of the regional psyche since the days of the Far West but was also due to the very nature of the old semiconductor business. Building chips had always been a tricky business. As chips got smaller and faster, that business began to border on magic. By the late 1980s companies such as Intel had to build a completely new plant to create a new generation of semiconductors, and each new plant easily cost in the neighborhood of $1 billion. The smaller the chips, the higher the demand for capital. At the same time these products had a very short lifespan, in most cases less than two years. During their lifetime these products were also subject to price wars that reduced their return on investment. The rule of thumb in the 1980s was that 25% of the new electronics firms failed within a few years. Nonetheless, semiconductor components (memory chips, programmable logic, micro-processors, custom-made circuits) constituted the heart of the most successful appliances ever, from calculators to videogames. It was worth the risk. Investors learned that it was a statistical game: invest in several start-ups, and only a few will survive, but those will make a lot of money for a short period of time. Any company that stayed in the business learned to live with high risk, aware that they could be bestsellers today, and broke in two years. That culture of risk remained after its very creators had succumbed to it, and reincarnated in the software industry of the late 1980s. The creation of wealth continued, and now depended on the venture capital available locally rather than on the defense industry. Many of the new investors were former start-up founders themselves, who recycled their money in the same environment in which they had made it. They had been raised in that culture of risk.

And it was more than a culture: it was a whole infrastructure designed to promote, assist and reward risk-takers in new technologies. That infrastructure consisted not only in laboratories, plants and offices, but also in corporate lawyers, marketing agencies, and, of course, venture capitalists. Coupled with the continuous flow of international students from the local universities, this world represented an entire ecosystem at the service of a risk-taking culture.

Law firms, for example, specialized not only in incorporating start-ups and documenting the early funding process, but also in protecting intellectual property. No other region in the world boasted attorneys that were more efficient and skilled in protecting a company's intellectual property. This hidden economy of corporate lawyers preexisted the boom of the 1980s. It certainly helped that California law always displayed a bias towards the small company. This was probably the legacy of having had to fight the East-Coast conglomerates in the old days when the West Coast did not have any large conglomerate. The task of defending California businesses was left to the state, and the state enacted laws to that extent. Even more important might be the laws that did not exist here: the best-known example was probably the lack of a law forbidding employees of a firm to migrate to a competing firm. Other states had such laws because the local conglomerates lobbied for them in the old days to protect their dominant status.

By then Silicon Valley represented a significant demographic change for the USA: a shift in political and economic power from the old industrial and financial capitals of the Northeast and Midwest towards a new pole of industry and finance based on the West Coast.

This had even wider geopolitical implications: the biggest competitor of California was Japan, not Western Europe. The old "Atlantic" economy, whose industrial and financial centers stretched from the East Coast and Midwest of the USA to Western Europe, was being replaced by a new "Pacific" economy, whose industrial and financial centers stretched from the Far East to California and Texas. It wasn't just the technology that had moved to the Pacific, it was also the capital: the venture capitalists based in Menlo Park and San Francisco accounted for a rapidly growing share of the world's venture capital. In fact, the semiconductor industry, which was slowing down, would have dragged the Bay Area down with it if it weren't for the large amounts of capital available to a completely new industry, the software industry. There was really no compelling reason for a software company to open business in the Silicon Valley. After all, Microsoft was based in Seattle, and Apple computers supported many fewer third-party developers than Microsoft. IBM, still the largest hardware, software and consulting company in the world, was based far away. The reason that a new industry boomed in Silicon Valley was, ultimately, that there was a lot of money around. The universities and companies of the region had attracted bright highly educated brains from all over the USA and the world. The combination of capital and brains replaced a declining industry with a booming one.

As a consequence, the symbiosis between venture capital and educational centers (Stanford, Berkeley and now also San Jose State University, which churned out software engineers by the hundreds) had become an addiction.

The downside, of course, was the temptation to generate a profit as quickly as possible. The great investor of the 1950s and 1960s, the military, thought long-term, with no interest in return on investment. The new investors of the 1980s had a short-term view of business. Investors began to promote corporate strategies that were focused not on innovation but on return on investment. This fear became national when Japan's MITI funded ambitious long-term projects that no Silicon Valley start-up could possibly match because no investor would invest in long-term prospects.

However, the short-term approach helped communicate effectively with the market. The Silicon Valley start-up was usually "visionary" but grounded in the reality of technological feasibility and of market readiness. Furthermore, the Darwinian system of small start-ups as a whole was more likely to find a solution to a problem than, say, a large bureaucratic company. Progress was incremental, but was rapid.

Basically, the venture-capital firms had created a ghost industry that evolved in parallel to the technological one. This ghost industry was focused on making money, and typically through either of two schemes: the IPO (the start-up goes public for a value much higher than the capitalist's investment) or an acquisition (a large company typically based outside the Bay Area, buys the start-up for a price much higher than the capitalist's investment). Venture-capital firms did not incubate healthy, long-lasting businesses as much as they incubated their own prospects for profit. This ghost economy was purely concerned with IPOs and acquisitions, independently of the intrinsic (social, scientific, human) value of a start-up's technology. It was as much a gamble to make money as stock gambling at Wall Street, except that the multiplicator could potentially be much bigger. Considering the voracious nature of the whole process of IPOs and acquisitions, it is quite amazing that, at the end of the day, Silicon Valley as a whole did generate real, groundbreaking products that triggered social changes all over the world.

The precarious lives of start-ups also created a culture of employment promiscuity. Engineers became accustomed with the idea that they may have to change jobs many times in their career. Each new job was a bet on the chances of a company. This culture created a further order of flexibility in that people were also more willing to change job on their own. A new start-up could easily find the right brains to develop a new technology, whereas elsewhere in the world people were less prepared to switch jobs. It helped that so many Silicon Valley residents were immigrant (from other states or from other nations): they were not afraid to move to a new environment and start a new life. The goal in Europe and on the East Coast was the career: ascend the ladders of the company's hierarchy. This was hard to conceive in Silicon Valley, where a company's (business) life expectancy was much lower than its employee's (biological) life expectancy. The fact that people living in Europe and on the East Coast were, instead, more reluctant to change jobs was, in fact, an important factor in determining the evolution of their economies. Silicon Valley's dream was a linear progression from engineer in a start-up to founder of a start-up to investor in a start-up. This dream encouraged people to take chances working for a start-up, to take chances creating start-ups, and to take chances investing in start-ups. It was a self-fulfilling prophecy and a self-sustaining metabolic cycle.

Now that venture capitalists employed or were themselves technology specialists their role had changed in a subtle way. A venture capitalist had always had a vested interest in helping the company s/he funded, but the technology-savvy venture capitalist, totally immersed in the community of innovators and with strong ties to the academia, could do more: s/he had become a knowledge broker, helping shape companies and their businesses through her/his network of contacts. Venture-capital firms had become more and more active in guiding if not running the business of their protege, from whom to hire for the executive team to which companies to choose as partners.

Part of this passion for start-ups was also the legacy of the anti-establishment (and therefore anti-corporate) sentiment of the 1960s. Software, in particular, appealed to the long-haired alternative type of kid, being novel, invisible and creative.

It was at about this time that Silicon Valley witnessed the rise of a cult of personality that went beyond mere admiration. Jobs, Ellison, Noyce and McNealy became more than founders or leaders: they became myths and prophets. The Fairchild founders had not been folk legends at all. The HP founders had been respected, but not over-hyped. They had been role models for a selected number of people who knew their responsible and dignified role in running the company. On the other hand, the leaders of Apple, Oracle, Intel and SUN acquired semi-god status in Silicon Valley. These mythical figures fought epic battles (typically against Bill Gates, Silicon Valley's public enemy number one) and the local media chronicled their fantastic odysseys. Their charisma replaced the charisma of the engineers who had truly invented their technologies (the likes of Faggin, Wozniak and Bechtolsheim). This phenomenon had a net effect on the ambitious youth of Silicon Valley: the trend had been shifting from inventing a product to starting a company, with the emphasis on the business plan rather than on the technological breakthrough.

One of the most influential figures of the era, Robert Noyce, cofounder of Fairchild and Intel, and coinventor of the integrated circuit, nicknamed "the Mayor of Silicon Valley", died in June 1990. That event, metaphorically, marked the end of the era of hardware and the beginning of the era of software. The following year something truly monumental was going to happen that would change the world forever.

Culture and Society

None of this could have happened if the Bay Area had not continued attracting brains from all over the world. The old model of lifetime employment in a large, safe company still prevailed on the East Coast, in Western Europe and in Japan. The West Coast, instead, spearheaded the preference for the small, dynamic company, even if it didn't offer benefits, it required to work on weekends and was likely to go bankrupt. Silicon Valley's agile, dynamic and brutal Darwinian system won over all the other technology centers in the USA and Europe. Silicon Valley came to embody the old myth of the "the land of opportunity", and therefore became an even bigger attractor for young educated people. Between 1970 and 1990 the population of San Jose alone almost doubled, from 445,779 to 782,248 people. In 1989 San Jose passed San Francisco in population. The San Francisco Bay Area was now officially a misnomer.

While it was attracting engineers, the Bay Area continued to attract artists and musicians who thrived on diversity. The collective of chamber electronic ethnic music that revolved around Lights In A Fat City and Trance Mission, or the iconoclastic collective of Thinking Fellers Union Local 282 were emblematic of what the Usenet had labeled "alt-rock". Visual artists formed the multidisciplinary gallery Luggage Store (properly the 509 Cultural Center) in 1987. This followed the Lab, which was also founded by interdisciplinary artists (in 1984), which in turn followed the Capp Street Project, founded in 1983 by Ann Hatch to promote avantgarde installations. Michael Gosney launched the "Digital Be-in" in january 1989 in San Francisco, an event mixing technology and art that transferred the ideals of the counterculture to the digital age.

The strand of anti-technological spirituality was still proceeding in parallel with the monumental achievements of the high-tech industry: the two behaved like entangled Siamese twins. The human-potential movement that had mutated into the new-age movement was still quite popular in the Bay Area. In august 1987 psychedelic painter Jose Arguelles organized the Harmonic Convergence (1987) in Sedona (Arizona) to celebrate what he believed to be a planetary alignment correlated with the Mayan calendar. Believers rushed to power centers were the phenomenon would be maximized, one being Mount Shasta, 400 kilometers north of San Francisco.

Mark Jury's documentary "Dances Sacred and Profane" (1987) popularized "body modification", pioneered by the likes of Roland Loomis, aka Fakir Musafar, who liked to hang himself in the air via hooks pierced into his chest, thereby resurrecting the Native American practice of O-Kee-Pa during their Sun Dance festival while simultaneously inventing the subgenre of performance art called "flesh hook suspension" and jumpstarting the "modern primitive" movement. Promptly, Vale Hamanaka (aka V Vale) and Andrea Juno paid tribute to the booming art of body piercing and tattooing in their book "Modern Primitives" (1989).

Another quasi-religious movement was born in Los Angeles. The "extropian" movement believed in the power of science and technology to yield immortality. Its members practiced cryogenics to preserve their brain after death. The term "extropy" was coined by Tom Bell, juxtaposing it to "entropy". Max More, an Oxford philosopher, had helped set up the first cryonic service in Europe (later renamed Alcor). Relocating to Los Angeles, in 1988 More founded the magazine "Extropy", subtitled "journal of transhumanist thought" and founded the "Extropy Institute", which in 1991 had its own online forum. The extropian movement had strong anti-government libertarian/anarchic political views, predicting a technocratic society in which the power would shift to the people. By the time Wired published the influential article "Meet The Extropians" in 1994, the extropian movement included members and sympathyzers such as Hans Moravec, Ralph Merkle, Nick Szabo, Hal Finney, as well as co-founders Tom Bell (Tom Morrow) and Perry Metzger. Merkle would go on to become a leader in nanotechnology, Szabo and Finley would pioneer Bitcoin, Metzger would launch the cryptography mailing list, and Moravec would lead the "singularity" movement.

In 1990 Burning Man came into its own. The first effigy had been built and burned by Larry Harvey and Jerry James in 1986 at one of Suicide Club alumna (and old-school hippy) Mary Grauberger's summer solstice beach parties on Baker Beach in San Francisco, but it was in 1990 that Kevin Evans and John Law decided to transplant the event to the Black Rock Desert in northern Nevada (simply because the San Francisco police had shut down the traditional one at Baker Beach). Law and Evans had been members of the Suicide Club. The Suicide Club had disbanded in 1983 when visionary Gary Warren had died, but the group had reorganized in 1986 as the Cacophony Society, inspired by the Dada cultural movement. Carrie Galbraith was leading the "zone trips" of the Cacophony Society, inspired by Andrei Tarkovsky's film "Stalker" (1979), and in 1988 artists such as Kevin Evans had started joining the society. This was a less secret and more inclusive and public society that indulged in what would later be called "flash mobs". Jerry James was a humble carpenter and the other two were jobless and penniless. That september about 70 people attended the first real Burning Man, which originally had simply been planned as a "zone trip". The number went up to 200 in 1991 and 500 in 1992. By 1996 more than 10,000 people flocked to the desert. Many, if not all, of the activists around the original Burning Man had apprenticed at Survival Research Laboratories. Burning Man was where the hippy, punk, machine art, Briarpatch and many other cultures met and merged.

From the 1960s through the 1980s in San Francisco the philosophy of the counterculture had largely been one of "public deeds", not mere theory.

The Bay Area had by then assumed a physiognomy that wouldn't change for a while. Passed by San Jose, San Francisco was no longer the biggest city of the Bay Area, but remained a major financial center and virtually its only cultural center. In terms of music, art and culture, Berkeley was its eastern appendix, with Berkeley University still producing a lively counterculture. During the 1980s Berkeley was a unique center for alternative music, boasting three of the largest record stores in the world within the space of three blocks; and non-stop improvised collective drumming in the lower Sprawl Plaza. Next to it, Oakland was still a poor crime-ridden city. The East Bay between Oakland and San Jose was home to large immigrant communities from the Indian subcontinent and from Latin America. Silicon Valley from San Jose to Mountain View was a breathtaking industrial area with few equals in the world: virtually any non-residential street was lined with multi-story office buildings, a veritable "who's who" of the high-tech industry. North of Mountain View was the Peninsula, which included Stanford University. The human landscape around Stanford was quite different from the one emanating from Berkeley: less extravagant and more business-oriented. The Peninsula contained some of the wealthiest communities in the world: Atherton, Woodside, Portola Valley, Menlo Park, Los Altos Hills and Palo Alto itself (originally an industrial and student area, but turned into an expensive town by the boom). An impressive amount of private capital was held in this area. The 101 corridor (along the namesake freeway) was still industrial (especially between Redwood City and the San Francisco airport), including Oracle and Genentech, but towards the hills the Peninsula was the habitat of new and old multimillionaires.
{
    "id": "dbpedia_3991_2",
    "rank": 24,
    "data": {
        "url": "https://www.cawcr.gov.au/projects/verification/",
        "read_more_link": "",
        "language": "en",
        "title": "",
        "top_image": "",
        "meta_img": "",
        "images": [
            "https://www.cawcr.gov.au/projects/verification/JWGFVR-logo-250.png",
            "https://www.cawcr.gov.au/projects/verification/WWRPlogo.gif",
            "https://www.cawcr.gov.au/projects/verification/WCRPlogo.gif",
            "https://www.cawcr.gov.au/projects/verification/timeseries.gif",
            "https://www.cawcr.gov.au/projects/verification/DWDmaps.gif",
            "https://www.cawcr.gov.au/projects/verification/Accuracy.gif",
            "https://www.cawcr.gov.au/projects/verification/BIAS.gif",
            "https://www.cawcr.gov.au/projects/verification/POD.gif",
            "https://www.cawcr.gov.au/projects/verification/FAR.gif",
            "https://www.cawcr.gov.au/projects/verification/POFD.gif",
            "https://www.cawcr.gov.au/projects/verification/SR.gif",
            "https://www.cawcr.gov.au/projects/verification/TS.gif",
            "https://www.cawcr.gov.au/projects/verification/ETSa.gif",
            "https://www.cawcr.gov.au/projects/verification/ETSb.gif",
            "https://www.cawcr.gov.au/projects/verification/HK.gif",
            "https://www.cawcr.gov.au/projects/verification/HSSnew.gif",
            "https://www.cawcr.gov.au/projects/verification/ExpectedCorrect.gif",
            "https://www.cawcr.gov.au/projects/verification/OddsRatio.gif",
            "https://www.cawcr.gov.au/projects/verification/ORSS.gif",
            "https://www.cawcr.gov.au/projects/verification/histogram.gif",
            "https://www.cawcr.gov.au/projects/verification/ACCmulti.gif",
            "https://www.cawcr.gov.au/projects/verification/HSSmulti.gif",
            "https://www.cawcr.gov.au/projects/verification/HKKmulti.gif",
            "https://www.cawcr.gov.au/projects/verification/GSS.gif",
            "https://www.cawcr.gov.au/projects/verification/GSS_sii.gif",
            "https://www.cawcr.gov.au/projects/verification/GSS_sij.gif",
            "https://www.cawcr.gov.au/projects/verification/GSS_ai.gif",
            "https://www.cawcr.gov.au/projects/verification/scatterplot.gif",
            "https://www.cawcr.gov.au/projects/verification/error_scatterplot.gif",
            "https://www.cawcr.gov.au/projects/verification/boxplot.gif",
            "https://www.cawcr.gov.au/projects/verification/MeanError.gif",
            "https://www.cawcr.gov.au/projects/verification/multbias.gif",
            "https://www.cawcr.gov.au/projects/verification/MAE.gif",
            "https://www.cawcr.gov.au/projects/verification/RMSE.gif",
            "https://www.cawcr.gov.au/projects/verification/MSE.gif",
            "https://www.cawcr.gov.au/projects/verification/LEPS.gif",
            "https://www.cawcr.gov.au/projects/verification/LEPSdiagram.gif",
            "https://www.cawcr.gov.au/projects/verification/LEPSrevised.gif",
            "https://www.cawcr.gov.au/projects/verification/SEEPS.gif",
            "https://www.cawcr.gov.au/projects/verification/SEEPSdiagram.gif",
            "https://www.cawcr.gov.au/projects/verification/SEEPS_scoring_matrix.gif",
            "https://www.cawcr.gov.au/projects/verification/Corr.gif",
            "https://www.cawcr.gov.au/projects/verification/AnomCorr.gif",
            "https://www.cawcr.gov.au/projects/verification/S1.gif",
            "https://www.cawcr.gov.au/projects/verification/Skill.gif",
            "https://www.cawcr.gov.au/projects/verification/ReliabilityDiagram.gif",
            "https://www.cawcr.gov.au/projects/verification/BSexpanded.gif",
            "https://www.cawcr.gov.au/projects/verification/BSS.gif",
            "https://www.cawcr.gov.au/projects/verification/ROC.gif",
            "https://www.cawcr.gov.au/projects/verification/Discrimination_diagram.gif",
            "https://www.cawcr.gov.au/projects/verification/RPS.gif",
            "https://www.cawcr.gov.au/projects/verification/CRPS.gif",
            "https://www.cawcr.gov.au/projects/verification/RPSS.gif",
            "https://www.cawcr.gov.au/projects/verification/value_determ.gif",
            "https://www.cawcr.gov.au/projects/verification/value.gif",
            "https://www.cawcr.gov.au/projects/verification/value_EPS.gif",
            "https://www.cawcr.gov.au/projects/verification/IS_MSEskill.gif",
            "https://www.cawcr.gov.au/projects/verification/FSSplot.gif",
            "https://www.cawcr.gov.au/projects/verification/FSS.gif",
            "https://www.cawcr.gov.au/projects/verification/CRA_entities0.gif",
            "https://www.cawcr.gov.au/projects/verification/MODE.gif",
            "https://www.cawcr.gov.au/projects/verification/DASexample.gif",
            "https://www.cawcr.gov.au/projects/verification/RankHistogram.gif",
            "https://www.cawcr.gov.au/projects/verification/CorrespondenceRatio.gif",
            "https://www.cawcr.gov.au/projects/verification/Venn.gif",
            "https://www.cawcr.gov.au/projects/verification/DetLim.gif",
            "https://www.cawcr.gov.au/projects/verification/EDS.gif",
            "https://www.cawcr.gov.au/projects/verification/SEDS.gif",
            "https://www.cawcr.gov.au/projects/verification/EDI.gif",
            "https://www.cawcr.gov.au/projects/verification/SEDI.gif",
            "https://www.cawcr.gov.au/projects/verification/TaylorDiagram.gif",
            "https://www.cawcr.gov.au/projects/verification/Roebber/PerformanceDiagram_small.gif",
            "https://www.cawcr.gov.au/projects/verification/RMSF.gif",
            "https://www.cawcr.gov.au/projects/verification/NashEfficiency.gif",
            "https://www.cawcr.gov.au/projects/verification/AlphaIndex.gif",
            "https://www.cawcr.gov.au/projects/verification/Koh/VectorError.gif",
            "https://www.cawcr.gov.au/projects/verification/Koh/VarianceEllipse_medium.gif",
            "https://www.cawcr.gov.au/projects/verification/Koh/SD.gif",
            "https://www.cawcr.gov.au/projects/verification/Koh/eccentricity.gif",
            "https://www.cawcr.gov.au/projects/verification/Jenkner/fall_bothint_querrclim.gif",
            "https://www.cawcr.gov.au/projects/verification/TampereRain_small.jpg"
        ],
        "movies": [],
        "keywords": [],
        "meta_keywords": [
            ""
        ],
        "tags": null,
        "authors": [],
        "publish_date": null,
        "summary": "",
        "meta_description": "",
        "meta_lang": "",
        "meta_favicon": "",
        "meta_site_name": "",
        "canonical_link": null,
        "text": "WWRP/WGNE Joint Working Group on Forecast Verification Research\n\nNew: 7th International Verification Methods Workshop\n\nTheme: Forecast Verification methods Across Time and Space Scales\n\nVenue: Berlin, Germany\n\nTutorial: May 3-10, 2017 | Applications due: January 31, 2017\n\nScience Conference: May 8-11, 2017 | Abstracts Due: February 27, 2017 Introduction - what is this web site about?\n\nIssues:\n\nWhy verify?\n\nTypes of forecasts and verification\n\nWhat makes a forecast good?\n\nForecast quality vs. value\n\nWhat is \"truth\"?\n\nValidity of verification results\n\nPooling vs. stratifying results\n\nMethods:\n\nStandard verification methods:\n\nMethods for dichotomous (yes/no) forecasts\n\nMethods for multi-category forecasts\n\nMethods for forecasts of continuous variables\n\nMethods for probabilistic forecasts\n\nScientific or diagnostic verification methods:\n\nMethods for spatial forecasts\n\nMethods for probabilistic forecasts, including ensemble prediction systems\n\nMethods for rare events\n\nOther methods\n\nSample forecast datasets:\n\nFinley tornado forecasts\n\nProbability of precipitation forecasts\n\nFreely available verification tools and packages\n\nSome frequently asked questions\n\nDiscussion group\n\nReferences:\n\nLinks to other verification sites\n\nReferences and further reading\n\nContributors to this site\n\nVerification and art\n\nIntroduction\n\nThis web site:\n\nDescribes methods for forecast verification, including their characteristics, pros and cons. The methods range from simple traditional statistics and scores, to methods for more detailed diagnostic and scientific verification.\n\nGives examples for each method, with links and references for further information. The examples are all drawn from the meteorological world (since the people creating this web site are themselves meteorologists or work with meteorologists), but the verification methods can easily be applied in other fields. They are appropriate for verifying estimates as well as forecasts.\n\nDemonstrates the verification techniques on a handful of forecast examples. These data will be available for download if you want to try out some of the techniques.\n\nDoes not provide source code (sorry, but what language would we use?). However, the simple methods are relatively easy to code, and the complex ones give references to people who have developed them or are working on them.\n\nIs a dynamic site - please contribute your ideas and verification methods, also suggestions for making the site better.\n\nIssues\n\nWhat is forecast verification?\n\nIf we take the term forecast to mean a prediction of the future state (of the weather, stock market prices, or whatever), then forecast verification is the process of assessing the quality of a forecast.\n\nThe forecast is compared, or verified, against a corresponding observation of what actually occurred, or some good estimate of the true outcome. The verification can be qualitative (\"does it look right?\") or quantitative (\"how accurate was it?\"). In either case it should give you information about the nature of the forecast errors.\n\nWhy verify?\n\nA forecast is like an experiment -- given a set of conditions, you make a hypothesis that a certain outcome will occur. You wouldn't consider an experiment to be complete until you determined its outcome. In the same way, you shouldn't consider a forecast experiment to be complete until you find out whether the forecast was successful.\n\nThe three most important reasons to verify forecasts are:\n\nto monitor forecast quality - how accurate are the forecasts and are they improving over time?\n\nto improve forecast quality - the first step toward getting better is discovering what you're doing wrong.\n\nto compare the quality of different forecast systems - to what extent does one forecast system give better forecasts than another, and in what ways is that system better?\n\nTypes of forecasts and verifications\n\nThere are many types of forecasts, each of which calls for slightly different methods of verification. The table below lists one way of distinguishing forecasts, along with verification methods that are appropriate for that type of forecast. David Stephenson has proposed a classification scheme for forecasts. It is often possible to convert from one type of forecast to another simply by rearranging, categorizing, or thresholding the data.\n\nNature of forecast: Example(s) Verification methods deterministic (non-probabilistic) quantitative precipitation forecast visual, dichotomous, multi-category, continuous, spatial probabilistic probability of precipitation, ensemble forecast visual, probabilistic, ensemble qualitative (worded) 5-day outlook visual, dichotomous, multi-category\n\nSpace-time domain: time series daily maximum temperature forecasts for a city visual, dichotomous, multi-category, continuous, probabilistic spatial distribution map of geopotential height, rainfall chart visual, dichotomous, multi-category, continuous, probabilistic, spatial, ensemble pooled space and time monthly average global temperature anomaly dichotomous, multi-category, continuous, probabilistic, ensemble\n\nSpecificity of forecast: dichotomous (yes/no) occurrence of fog visual, dichotomous, probabilistic, spatial, ensemble multi-category cold, normal, or warm conditions visual, multi-category, probabilistic, spatial, ensemble continuous maximum temperature visual, continuous, probabilistic, spatial, ensemble object- or event-oriented tropical cyclone motion and intensity visual, dichotomous, multi-category, continuous, probabilistic, spatial\n\nWhat makes a forecast \"good\"?\n\nAllan Murphy, a pioneer in the field of forecast verification, wrote an essay on what makes a forecast \"good\" (Murphy, 1993). He distinguished three types of \"goodness\":\n\nConsistency - the degree to which the forecast corresponds to the forecaster's best judgement about the situation, based upon his/her knowledge base\n\nQuality - the degree to which the forecast corresponds to what actually happened\n\nValue - the degree to which the forecast helps a decision maker to realize some incremental economic and/or other benefit\n\nSince we're interested in forecast verification, let's look a bit closer at the forecast quality. Murphy described nine aspects (called \"attributes\") that contribute to the quality of a forecast. These are:\n\nBias - the correspondence between the mean forecast and mean observation.\n\nAssociation - the strength of the linear relationship between the forecasts and observations (for example, the correlation coefficient measures this linear relationship)\n\nAccuracy - the level of agreement between the forecast and the truth (as represented by observations). The difference between the forecast and the observation is the error. The lower the errors, the greater the accuracy.\n\nSkill - the relative accuracy of the forecast over some reference forecast. The reference forecast is generally an unskilled forecast such as random chance, persistence (defined as the most recent set of observations, \"persistence\" implies no change in condition), or climatology. Skill refers to the increase in accuracy due purely to the \"smarts\" of the forecast system. Weather forecasts may be more accurate simply because the weather is easier to forecast -- skill takes this into account.\n\nReliability - the average agreement between the forecast values and the observed values. If all forecasts are considered together, then the overall reliability is the same as the bias. If the forecasts are stratified into different ranges or categories, then the reliability is the same as the conditional bias, i.e., it has a different value for each category.\n\nResolution - the ability of the forecast to sort or resolve the set of events into subsets with different frequency distributions. This means that the distribution of outcomes when \"A\" was forecast is different from the distribution of outcomes when \"B\" is forecast. Even if the forecasts are wrong, the forecast system has resolution if it can successfully separate one type of outcome from another.\n\nSharpness - the tendency of the forecast to predict extreme values. To use a counter-example, a forecast of \"climatology\" has no sharpness. Sharpness is a property of the forecast only, and like resolution, a forecast can have this attribute even if it's wrong (in this case it would have poor reliability).\n\nDiscrimination - ability of the forecast to discriminate among observations, that is, to have a higher prediction frequency for an outcome whenever that outcome occurs.\n\nUncertainty - the variability of the observations. The greater the uncertainty, the more difficult the forecast will tend to be.\n\nTraditionally, forecast verification has emphasized accuracy and skill. It's important to note that the other attributes of forecast performance also have a strong influence on the value of the forecast.\n\nForecast quality vs. value\n\nForecast quality is not the same as forecast value. A forecast has high quality if it predicts the observed conditions well according to some objective or subjective criteria. It has value if it helps the user to make a better decision.\n\nImagine a situation in which a high resolution numerical weather prediction model predicts the development of isolated thunderstorms in a particular region, and thunderstorms are indeed observed in the region but not in the particular spots suggested by the model. According to most standard verification measures this forecast would have poor quality, yet it might be very valuable to the forecaster in issuing a public weather forecast.\n\nAn example of a forecast with high quality but little value is a forecast of clear skies over the Sahara Desert during the dry season.\n\nWhen the cost of a missed event is high, the deliberate overforecasting of a rare event may be justified, even though a large number of false alarms may also result. An example of such a circumstance is the occurence of fog at airports. In this case quadratic scoring rules (those involving squared errors) will tend to penalise such forecasts harshly, and a positively oriented score such as \"hit rate\" may be more useful.\n\nKatz and Murphy (1997), Thornes and Stephenson (2001) and Wilks (2001) describe methods for assessing the value of weather forecasts. The relative value plot is sometimes used as a verification diagnostic.\n\nWhat is \"truth\" when verifying a forecast?\n\nThe \"truth\" data that we use to verify a forecasts generally comes from observational data. These could be rain gauge measurements, temperature observations, satellite-derived cloud cover, geopotential height analyses, and so on.\n\nIn many cases it is difficult to know the exact truth because there are errors in the observations. Sources of uncertainty include random and bias errors in the measurements themselves, sampling error and other errors of representativeness, and analysis error when the observational data are analyzed or otherwise altered to match the scale of the forecast.\n\nRightly or wrongly, most of the time we ignore the errors in the observational data. We can get away with this if the errors in the observations are much smaller than the expected error in the forecast (high signal to noise ratio). Even skewed or under-sampled verification data can give us a good idea of which forecast products are better than others when intercomparing different forecast methods. Methods to account for errors in the verification data currently being researched.\n\nValidity of verification results\n\nThe verification results are naturally more trustworthy when the quantity and quality of the verification data are high. It is always a good idea to put some error bounds on the verification results themselves. It is especially important (a) for rare events where the sample size is typically small, (b) when the data shows a lot of variability, and (c)when you want to know whether one forecast product is significantly better (in a statistical sense) than another.\n\nThe usual approach is to determine confidence intervals for the verification scores using analytic, approximate, or bootstrapping methods (depending on the score). Some good meteorological references on this subject are Seaman et al. (1996), Wilks (2011, ch.5), Hamill (1999), and Kane and Brown (2000).\n\nPooling versus stratifying results\n\nTo get reliable verification statistics, a large number of forecast/observations pairs (samples) may be pooled over time and/or space. The larger the number of samples, the more reliable the verification results. You can also get pooled results by aggregating verification statistics over a longer time period, but be careful to handle non-linear scores properly.\n\nThe danger with pooling samples, however, is that it can mask variations in forecast performance when the data are not homogeneous. It can bias the results toward the most commonly sampled regime (for example, regions with higher station density, or days with no severe weather). Non-homegeneous samples can lead to overestimates of forecast skill using some commonly used metrics - Hamill and Juras (2005) provide some clear examples of how this can occur.\n\nStratifying the samples into quasi-homogeneous subsets (by season, by geographical region, by intensity of the observations, etc.) helps to tease out forecast behavior in particular regimes. When doing this, be sure that the subsets contain enough samples to give trustworthy verification results.\n\nStandard verification methods\n\n\"Eyeball\" verification\n\nOne of the oldest and best verification methods is the good old fashioned visual, or \"eyeball\", method: look at the forecast and observations side by side and use human judgment to discern the forecast errors. Common ways to present data are as time series and maps.\n\nThe eyeball method is great if you only have a few forecasts, or you have lots of time, or you're not interested in quantitative verification statistics. Even when you do want statistics, it is a very good idea to look at the data from time to time!\n\nHowever, the eyeball method is not quantitative, and it is very prone to individual, subjective biases of interpretation. Therefore it must be used with caution in any formal verification procedure.\n\nThe following sections give fairly brief descriptions of the standard verification methods and scores for dichotomous, multi-category, continuous, and probabilistic forecasts. For greater detail and discussion of the standard methods see Stanski et al. (1989) or one of the excellent books on forecast verification and statistics.\n\n- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -\n\nMethods for dichotomous (yes/no) forecasts\n\nA dichotomous forecast says, \"yes, an event will happen\", or \"no, the event will not happen\". Rain and fog prediction are common examples of yes/no forecasts. For some applications a threshold may be specified to separate \"yes\" and \"no\", for example, winds greater than 50 knots.\n\nTo verify this type of forecast we start with a contingency table that shows the frequency of \"yes\" and \"no\" forecasts and occurrences. The four combinations of forecasts (yes or no) and observations (yes or no), called the joint distribution, are:\n\nhit - event forecast to occur, and did occur\n\nmiss - event forecast not to occur, but did occur\n\nfalse alarm - event forecast to occur, but did not occur\n\ncorrect negative - event forecast not to occur, and did not occur\n\nThe total numbers of observed and forecast occurrences and non-occurences are given on the lower and right sides of the contingency table, and are called the marginal distribution.\n\nContingency Table Observed yes no Total Forecast yes hits false alarms forecast yes no misses correct negatives forecast no Total observed yes observed no total\n\nThe contingency table is a useful way to see what types of errors are being made. A perfect forecast system would produce only hits and correct negatives, and no misses or false alarms.\n\nA large variety of categorical statistics are computed from the elements in the contingency table to describe particular aspects of forecast performance. We will illustrate these statistics using a (made-up) example. Suppose a year's worth of official daily rain forecasts and observations produced the following contingency table:\n\nObserved yes no Total Forecast yes 82 38 120 no 23 222 245 Total 105 260 365\n\nCategorical statistics that can be computed from the yes/no contingency table are given below. Sometimes these scores are known by alternate names shown in parentheses.\n\n- - - - - - - - - - -\n\nAccuracy (fraction correct) -\n\nAnswers the question: Overall, what fraction of the forecasts were correct?\n\nRange: 0 to 1. Perfect score: 1.\n\nCharacteristics: Simple, intuitive. Can be misleading since it is heavily influenced by the most common category, usually \"no event\" in the case of rare weather.\n\nIn the example above, Accuracy = (82+222) / 365 = 0.83, indicating that 83% of all forecasts were correct.\n\n- - - - - - - - - - -\n\nBias score (frequency bias) -\n\nAnswers the question: How did the forecast frequency of \"yes\" events compare to the observed frequency of \"yes\" events?\n\nRange: 0 to ∞. Perfect score: 1.\n\nCharacteristics: Measures the ratio of the frequency of forecast events to the frequency of observed events. Indicates whether the forecast system has a tendency to underforecast (BIAS<1) or overforecast (BIAS>1) events. Does not measure how well the forecast corresponds to the observations, only measures relative frequencies.\n\nIn the example above, BIAS = (82+38) / (82+23) = 1.14, indicating slight overforecasting of rain frequency.\n\n- - - - - - - - - - -\n\nProbability of detection (hit rate) - (also denoted H)\n\nAnswers the question: What fraction of the observed \"yes\" events were correctly forecast?\n\nRange: 0 to 1. Perfect score: 1.\n\nCharacteristics: Sensitive to hits, but ignores false alarms. Very sensitive to the climatological frequency of the event. Good for rare events.Can be artificially improved by issuing more \"yes\" forecasts to increase the number of hits. Should be used in conjunction with the false alarm ratio (below). POD is also an important component of the Relative Operating Characteristic (ROC) used widely for probabilistic forecasts.\n\nIn the example above, POD = 82 / (82+23) = 0.78, indicating that roughly 3/4 of the observed rain events were correctly predicted.\n\n- - - - - - - - - - -\n\nFalse alarm ratio -\n\nAnswers the question: What fraction of the predicted \"yes\" events actually did not occur (i.e., were false alarms)?\n\nRange: 0 to 1. Perfect score: 0.\n\nCharacteristics: Sensitive to false alarms, but ignores misses. Very sensitive to the climatological frequency of the event. Should be used in conjunction with the probability of detection (above).\n\nIn the example above, FAR = 38 / (82+38) = 0.32, indicating that in roughly 1/3 of the forecast rain events, rain was not observed.\n\n- - - - - - - - - - -\n\nProbability of false detection (false alarm rate) - (also denoted F)\n\nAnswers the question: What fraction of the observed \"no\" events were incorrectly forecast as \"yes\"?\n\nRange: 0 to 1. Perfect score: 0.\n\nCharacteristics: Sensitive to false alarms, but ignores misses. Can be artificially improved by issuing fewer \"yes\" forecasts to reduce the number of false alarms. Not often reported for deterministic forecasts, but is an important component of the Relative Operating Characteristic (ROC) used widely for probabilistic forecasts.\n\nIn the example above, POFD = 38 / (222+38) = 0.15, indicating that for 15% of the observed \"no rain\" events the forecasts were incorrect.\n\n- - - - - - - - - - -\n\nSuccess ratio -\n\nAnswers the question: What fraction of the forecast \"yes\" events were correctly observed?\n\nRange: 0 to 1. Perfect score: 1.\n\nCharacteristics: Gives information about the likelihood of an observed event, given that it was forecast. It is sensitive to false alarms but ignores misses. SR is equal to 1-FAR. POD is plotted against SR in the categorical performance diagram.\n\nIn the example above, SR = 82 / (82+38) = 0.68, indicating that for 68% of the forecast rain events, rain was actually observed.\n\n- - - - - - - - - - -\n\nThreat score (critical success index) - (also denoted CSI)\n\nAnswers the question: How well did the forecast \"yes\" events correspond to the observed \"yes\" events?\n\nRange: 0 to 1, 0 indicates no skill. Perfect score: 1.\n\nCharacteristics: Measures the fraction of observed and/or forecast events that were correctly predicted. It can be thought of as the accuracy when correct negatives have been removed from consideration, that is, TS is only concerned with forecasts that count. Sensitive to hits, penalizes both misses and false alarms. Does not distinguish source of forecast error. Depends on climatological frequency of events (poorer scores for rarer events) since some hits can occur purely due to random chance.\n\nIn the example above, TS = 82 / (82+23+38) = 0.57, meaning that slightly more than half of the \"rain\" events (observed and/or predicted) were correctly forecast.\n\n- - - - - - - - - - -\n\nEquitable threat score (Gilbert skill score)- (also denoted GSS)\n\nwhere\n\nAnswers the question: How well did the forecast \"yes\" events correspond to the observed \"yes\" events (accounting for hits due to chance)?\n\nRange: -1/3 to 1, 0 indicates no skill. Perfect score: 1.\n\nCharacteristics: Measures the fraction of observed and/or forecast events that were correctly predicted, adjusted for hits associated with random chance (for example, it is easier to correctly forecast rain occurrence in a wet climate than in a dry climate). The ETS is often used in the verification of rainfall in NWP models because its \"equitability\" allows scores to be compared more fairly across different regimes. Sensitive to hits. Because it penalises both misses and false alarms in the same way, it does not distinguish the source of forecast error.\n\nIn the example above, ETS = (82-34) / (82+23+38-34) = 0.44. ETS gives a lower score than TS.\n\n- - - - - - - - - - -\n\nHanssen and Kuipers discriminant (true skill statistic, Peirce's skill score) - (also denoted TSS and PSS)\n\nAnswers the question: How well did the forecast separate the \"yes\" events from the \"no\" events?\n\nRange: -1 to 1, 0 indicates no skill. Perfect score: 1.\n\nCharacteristics: Uses all elements in contingency table. Does not depend on climatological event frequency. The expression is identical to HK = POD - POFD, but the Hanssen and Kuipers score can also be interpreted as (accuracy for events) + (accuracy for non-events) - 1. For rare events HK is unduly weighted toward the first term (same as POD), so this score may be more useful for more frequent events. Can be expressed in a form similar to the ETS except the hitsrandom term is unbiased. See Woodcock (1976) for a comparison of HK with other scores.\n\nIn the example above, HK = 82 / (82+23) - 38 / (38+222) = 0.63\n\n- - - - - - - - - - -\n\nHeidke skill score (Cohen's k) -\n\nwhere\n\nAnswers the question: What was the accuracy of the forecast relative to that of random chance?\n\nRange: -1 to 1, 0 indicates no skill. Perfect score: 1.\n\nCharacteristics: Measures the fraction of correct forecasts after eliminating those forecasts which would be correct due purely to random chance. This is a form of the generalized skill score, where the score in the numerator is the number of correct forecasts, and the reference forecast in this case is random chance. In meteorology, at least, random chance is usually not the best forecast to compare to - it may be better to use climatology (long-term average value) or persistence (forecast = most recent observation, i.e., no change) or some other standard.\n\nIn the example above, HSS = 0.61\n\n- - - - - - - - - - -\n\nOdds ratio -\n\nAnswers the question: What is the ratio of the odds of a \"yes\" forecast being correct, to the odds of a \"yes\" forecast being wrong?\n\nOdds ratio - Range: 0 to ∞, 1 indicates no skill. Perfect score: ∞\n\nLog odds ratio - Range: -∞ to ∞, 0 indicates no skill. Perfect score: ∞\n\nCharacteristics: Measures the ratio of the odds of making a hit to the odds of making a false alarm. The logarithm of the odds ratio is often used instead of the original value. Takes prior probabilities into account. Gives better scores for rarer events. Less sensitive to hedging. Do not use if any of the cells in the contingency table are equal to 0. Used widely in medicine but not yet in meteorology -- see Stephenson (2000) for more information.\n\nNote that the odds ratio is not the same as the ratio of the probability of making a hit (hits / # forecasts) to the probability of making a false alarm (false alarms / # forecasts), since both of those can depend on the climatological frequency (i.e., the prior probability) of the event.\n\nIn the example above, OR = (82 x 222) / (23 x 38) = 20.8, indicating that the odds of a \"yes\" prediction being correct are over 20 times greater than the odds of a \"yes\" forecast being incorrect.\n\n- - - - - - - - - - -\n\nOdds ratio skill score (Yule's Q) -\n\nAnswers the question: What was the improvement of the forecast over random chance?\n\nRange: -1 to 1, 0 indicates no skill. Perfect score: 1\n\nCharacteristics: Independent of the marginal totals (i.e., of the threshold chosen to separate \"yes\" and \"no\"), so is difficult to hedge. See Stephenson (2000) for more information.\n\nIn the example above, ORSS = [(82 x 222)-(23 x 38)] / [(82 x 222)+(23 x 38)] = 0.91\n\n- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -\n\nMethods for multi-category forecasts\n\nMethods for verifying multi-category forecasts also start with a contingency table showing the frequency of forecasts and observations in the various bins. It is analogous to a scatter plot for categories.\n\nMulti-category Contingency Table Observed Category Total i,j 1 2 ... K 1 n(F1,O1) n(F1,O2) ... n(F1,OK) N(F1) Forecast 2 n(F2,O1) n(F2,O2) ... n(F2,OK) N(F2) Category ... ... ... ... ... ... K n(FK,O1) n(FK,O2) ... n(FK,OK) N(FK) Total N(O1) N(O2) ... N(OK) N\n\nIn this table n(Fi,Oj) denotes the number of forecasts in category i that had observations in category j, N(Fi) denotes the total number of forecasts in category i, N(Oj) denotes the total number of observations in category j, and N is the total number of forecasts.\n\nThe distributions approach to forecast verification examines the relationship among the elements in the multi-category contingency table. A perfect forecast system would have values of non-zero elements only along the diagonal, and values of 0 for all entries off the diagonal. The off-diagonal elements give information about the specific nature of the forecast errors. The marginal distributions (N's at right and bottom of table) show whether the forecast produces the correct distribution of categorical values when compared to the observations. Murphy and Winkler (1987), Murphy et al. (1989) and Brooks and Doswell (1996) develop this approach in detail.\n\nThe advantage of the distributions approach is that the nature of the forecast errors can more easily be diagnosed. The disadvantage is that it is more difficult to condense the results into a single number. There are fewer statistics that summarize the performance of multi-category forecasts. However, any multi-category forecast verification can be converted to a series of K-1 yes/no-type verifications by defining \"yes\" to be \"in category i\" or \"in category i or higher\", and \"no\" to be \"not in category i\" or \"below category i\".\n\n- - - - - - - - - - -\n\nHistogram - Plot the relative frequencies of forecast and observed categories\n\nAnswers the question: How well did the distribution of forecast categories correspond to the distribution of observed categories?\n\nCharacteristics: Shows similarity between location, spread, and skewness of forecast and observed distributions. Does not give information on the correspondence between the forecasts and observations. Histograms give information similar to box plots.\n\n- - - - - - - - - - -\n\nAccuracy -\n\nAnswers the question: Overall, what fraction of the forecasts were in the correct category?\n\nRange: 0 to 1. Perfect score: 1.\n\nCharacteristics: Simple, intuitive. Can be misleading since it is heavily influenced by the most common category.\n\n- - - - - - - - - - -\n\nHeidke skill score -\n\nAnswers the question: What was the accuracy of the forecast in predicting the correct category, relative to that of random chance?\n\nRange: -∞ to 1, 0 indicates no skill. Perfect score: 1.\n\nCharacteristics: Measures the fraction of correct forecasts after eliminating those forecasts which would be correct due purely to random chance. This is one form of a generalized skill score, where the score in the numerator is the number of correct forecasts, and the reference forecast in this case is random chance. Requires a large sample size to make sure that the elements of the contingency table are all adequately sampled. In meteorology, at least, random chance is usually not the best forecast to compare to - it may be better to use climatology (long-term average value) or persistence (forecast is most recent observation, i.e., no change) or some other standard.\n\n- - - - - - - - - - -\n\nHanssen and Kuipers discriminant (true skill statistic, Peirce's skill score) -\n\nAnswers the question: What was the accuracy of the forecast in predicting the correct category, relative to that of random chance?\n\nRange: -1 to 1, 0 indicates no skill. Perfect score: 1\n\nCharacteristics: Similar to the Heidke skill score (above), except that in the denominator the fraction of correct forecasts due to random chance is for an unbiased forecast.\n\n- - - - - - - - - - -\n\nGerrity score -\n\nwhere sij are elements of a scoring matrix given by\n\n(i = j, diagonal), (i ≠ j, off-diagonal), and\n\nwith the sample probabilities (observed frequencies) given by pi = N(Oi) / N).\n\nAnswers the question: What was the accuracy of the forecast in predicting the correct category, relative to that of random chance?\n\nRange: -1 to 1, 0 indicates no skill. Perfect score: 1\n\nCharacteristics: Uses all entries in the contingency table, does not depend on the forecast distribution, and is equitable (i.e., random and constant forecasts score a value of 0). GS does not reward conservative forecasting like HSS and HK, but rather rewards forecasts for correctly predicting the less likely categories. Smaller errors are penalized less than larger forecast errors. This is achieved through the use of the scoring matrix. A more detailed discussion and examples for 3-category forecasts can be found in Jolliffe and Stephenson (2012).\n\n- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -\n\nMethods for foreasts of continuous variables\n\nVerifying forecasts of continuous variables measures how the values of the forecasts differ from the values of the observations. The continuous verification methods and statistics will be demonstrated on a sample data set of 10 temperature forecasts taken from Stanski et al. (1989):\n\nDay 1 2 3 4 5 6 7 8 9 10 Forecast, Fi (C) 5 10 9 15 22 13 17 17 19 23 Observation, Oi (C) -1 8 12 13 18 10 16 19 23 24\n\nVerification of continous forecasts often includes some exploratory plots such as scatter plots and box plots, as well as various summary scores.\n\n- - - - - - - - - - -\n\nScatter plot - Plots the forecast values against the observed values.\n\nAnswers the question: How well did the forecast values correspond to the observed values?\n\nCharacteristics: Good first look at correspondence between forecast and observations. An accurate forecast will have points on or near the diagonal.\n\nScatter plots of the error can reveal relationships between the observed or forecast values and the errors.\n\n- - - - - - - - - - -\n\nBox plot - Plot boxes to show the range of data falling between the 25th and 75th percentiles, horizontal line inside the box showing the median value, and the whiskers showing the complete range of the data.\n\nAnswers the question: How well did the distribution of forecast values correspond to the distribution of observed values?\n\nCharacteristics: Shows similarity between location, spread, and skewness of forecast and observed distributions. Does not give information on the correspondence between the forecasts and observations. Box plots give information similar to histograms.\n\n- - - - - - - - - - -\n\nMean error -\n\nAnswers the question: What is the average forecast error?\n\nRange: -∞ to ∞. Perfect score: 0.\n\nCharacteristics: Simple, familiar. Also called the (additive) bias. Does not measure the magnitude of the errors. Does not measure the correspondence between forecasts and observations, i.e., it is possible to get a perfect score for a bad forecast if there are compensating errors.\n\nIn the example above, Mean Error = 0.8 C\n\n- - - - - - - - - - -\n\n(Multiplicative) bias -\n\nAnswers the question: How does the average forecast magnitude compare to the average observed magnitude?\n\nRange: -∞ to ∞. Perfect score: 1.\n\nCharacteristics: Simple, familiar. Best suited for quantities that have 0 as a lower or upper bound. Does not measure the magnitude of the errors. Does not measure the correspondence between forecasts and observations, i.e., it is possible to get a perfect score for a bad forecast if there are compensating errors.\n\nIn the example above, Bias = 1.06\n\n- - - - - - - - - - -\n\nMean absolute error -\n\nAnswers the question: What is the average magnitude of the forecast errors?\n\nRange: 0 to ∞. Perfect score: 0.\n\nCharacteristics: Simple, familiar. Does not indicate the direction of the deviations.\n\nIn the example above, MAE = 2.8 C\n\n- - - - - - - - - - -\n\nRoot mean square error -\n\nAnswers the question: What is the average magnitude of the forecast errors?\n\nRange: 0 to ∞. Perfect score: 0.\n\nCharacteristics: Simple, familiar. Measures \"average\" error, weighted according to the square of the error. Does not indicate the direction of the deviations. The RMSE puts greater influence on large errors than smaller errors, which may be a good things if large errors are especially undesirable, but may also encourage conservative forecasting.\n\nIn the example above, RMSE = 3.2 C\n\nThe root mean square factor is similar to RMSE, but gives a multiplicative error instead of an additive error.\n\n- - - - - - - - - - -\n\nMean squared error -\n\nMeasures the mean squared difference between the forecasts and observations.\n\nRange: 0 to ∞. Perfect score: 0.\n\nCharacteristics: Can be decomposed into component error sources following Murphy (1987). Units of MSE are the square of the basic units.\n\nIn the example above, MSE = 10 degrees squared\n\n- - - - - - - - - - -\n\nLinear error in probability space (LEPS) -\n\nMeasures the error in probability space as opposed to measurement space, where CDFo() is the cumulative probability density function of the observations, determined from an appropriate climatology.\n\nRange: 0 to 1. Perfect score: 0.\n\nCharacteristics: Does not discourage forecasting extreme values if they are warranted. Requires knowledge of climatological PDF. Not yet in wide usage -- Potts et al. (1996) derived an improved version of the LEPS score that is equitable and does not \"bend back\" (give better scores for worse forecasts near the extremes): .\n\nIn the example above, suppose the climatological temperature is normally distributed with a mean of 14 C and variance of 50 C. Then according to the first expression, LEPS=0.106.\n\n- - - - - - - - - - -\n\nStable equitable error in probability space (SEEPS) -\n\nwhere n(Fi,Oj) is the joint occurrence of forecast category i and observed category j in the 3x3 contingency table, and the scoring matrix is given by\n\nLike LEPS, SEEPS measures the error in probability space as opposed to measurement space. It was developed to assess rainfall forecasts, where (1-p1) is the climatological probability of rain (i.e., accumulation exceeding 0.2 mm, following WMO guidelines), and p2=2p3 divides the climatological cumulative rainfall distribution into \"light\" (lower 2/3 of rain rates ≥0.2 mm) and \"heavy\" (upper 1/3 of rain rates ≥0.2 mm). Refer to diagram at right, where tL/H is the threshold delineating \"light\" and \"heavy\" rain.\n\nRange: 0 to 1. Perfect score: 0.\n\nCharacteristics: Encourages forecasting of all categories. Resistent to hedging. Requires knowledge of climatological PDF. 1-SEEPS may be preferred as it is positively oriented. Use of locally derived thresholds allows aggregation/comparison of scores across climatologically varying regimes. For further stability require 0.1 < p1 < 0.85, that is, climate not too dry or too wet so that rain (or no rain) is an extreme event. For more information see Rodwell et al. (2010).\n\n- - - - - - - - - - -\n\nCorrelation coefficient -\n\nAddresses the question: How well did the forecast values correspond to the observed values?\n\nRange: -1 to 1. Perfect score: 1.\n\nCharacteristics: Good measure of linear association or phase error. Visually, the correlation measures how close the points of a scatter plot are to a straight line. Does not take forecast bias into account -- it is possible for a forecast with large errors to still have a good correlation coefficient with the observations. Sensitive to outliers.\n\nIn the example above, r = 0.914\n\n- - - - - - - - - - -\n\nAnomaly correlation -\n\nAddresses the question: How well did the forecast anomalies correspond to the observed anomalies?\n\nRange: -1 to 1. Perfect score: 1.\n\nCharacteristics: Measures correspondence or phase difference between forecast and observations, subtracting out the climatological mean at each point, C, rather than the sample mean values. The anomaly correlation is frequently used to verify output from numerical weather prediction (NWP) models. AC is not sensitive to forecast bias, so a good anomaly correlation does not guarantee accurate forecasts. Both forms of the equation are in common use -- see Jolliffe and Stephenson (2012) or Wilks (2011) for further discussion.\n\nIn the example above, if the climatological temperature is 14 C, then AC = 0.904. AC is more often used in spatial verification.\n\n- - - - - - - - - - -\n\nS1 score -\n\nwhere DF (DO) refers to the horizontal gradient in the forecast (observations).\n\nAnswers the question: How well did the forecast gradients correspond to the observed gradients?\n\nRange: 0 to ∞. Perfect score: 0.\n\nCharacteristics: It is usually applied to geopotential height or sea level pressure fields in meteorology. Long historical records in NWP showing improvement in model performance over the years. Because S1 depends only on gradients, good scores can be achieved even when the forecast values are biased. Also depends on spatial resolution of the forecast.\n\n- - - - - - - - - - -\n\nSkill score -\n\nAnswers the question: What is the relative improvement of the forecast over some reference forecast?\n\nRange: Lower bound depends on what score is being used to compute skill and what reference forecast is used, but upper bound is always 1; 0 indicates no improvement over the reference forecast. Perfect score: 1.\n\nCharacteristics: Implies information about the value or worth of a forecast relative to an alternative (reference) forecast. In meteorology the reference forecast is usually persistence (no change from most recent observation) or climatology. The skill score can be unstable for small sample sizes. When MSE is the score used in the above expression then the resulting statistic is called the reduction of variance.\n\n- - - - - - - - - - -\n\nSee also Methods for spatial forecasts for more scientific/diagnostic techniques.\n\nSee also Other methods for additional scores for forecasts of continuous variables.\n\n- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -\n\nMethods for probabilistic forecasts\n\nA probabilistic forecast gives a probability of an event occurring, with a value between 0 and 1 (or 0 and 100%). In general, it is difficult to verify a single probabilistic forecast. Instead, a set of probabilistic forecasts, pi, is verified using observations that those events either occurred (oi=1) or did not occur (oi=0).\n\nAn accurate probability forecast system has:\n\nreliability - agreement between forecast probability and mean observed frequency\n\nsharpness - tendency to forecast probabilities near 0 or 1, as opposed to values clustered around the mean\n\nresolution - ability of the forecast to resolve the set of sample events into subsets with characteristically different outcomes\n\n- - - - - - - - - - -\n\nReliability diagram - (called \"attributes diagram\" when the no-resoloution and no-skill w.r.t. climatology lines are included).\n\nThe reliability diagram plots the observed frequency against the forecast probability, where the range of forecast probabilities is divided into K bins (for example, 0-5%, 5-15%, 15-25%, etc.). The sample size in each bin is often included as a histogram or values beside the data points.\n\nAnswers the question: How well do the predicted probabilities of an event correspond to their observed frequencies?\n\nCharacteristics: Reliability is indicated by the proximity of the plotted curve to the diagonal. The deviation from the diagonal gives the conditional bias. If the curve lies below the line, this indicates overforecasting (probabilities too high); points above the line indicate underforecasting (probabilities too low). The flatter the curve in the reliability diagram, the less resolution it has. A forecast of climatology does not discriminate at all between events and non-events, and thus has no resolution. Points between the \"no skill\" line and the diagonal contribute positively to the Brier skill score. The frequency of forecasts in each probability bin (shown in the histogram) shows the sharpness of the forecast.\n\nThe reliability diagram is conditioned on the forecasts (i.e., given that an event was predicted, what was the outcome?), and can be expected to give information on the real meaning of the forecast. It is a good partner to the ROC, which is conditioned on the observations. Some users may find a reliability table (table of observed relative frequency associated with each forecast probability) easier to understand than a reliability diagram.\n\n- - - - - - - - - - -\n\nBrier score -\n\nAnswers the question: What is the magnitude of the probability forecast errors?\n\nMeasures the mean squared probability error. Murphy (1973) showed that it could be partitioned into three terms: (1) reliability, (2) resolution, and (3) uncertainty.\n\nRange: 0 to 1. Perfect score: 0.\n\nCharacteristics: Sensitive to climatological frequency of the event: the more rare an event, the easier it is to get a good BS without having any real skill. Negative orientation (smaller score better) - can \"fix\" by subtracting BS from 1.\n\n- - - - - - - - - - -\n\nBrier skill score -\n\nAnswers the question: What is the relative skill of the probabilistic forecast over that of climatology, in terms of predicting whether or not an event occurred?\n\nRange: -∞ to 1, 0 indicates no skill when compared to the reference forecast. Perfect score: 1.\n\nCharacteristics: Measures the improvement of the probabilistic forecast relative to a reference forecast (usually the long-term or sample climatology), thus taking climatological frequency into account. Not strictly proper. Unstable when applied to small data sets; the rarer the event, the larger the number of samples needed.\n\n- - - - - - - - - - -\n\nRelative operating characteristic - Plot hit rate (POD) vs false alarm rate (POFD), using a set of increasing probability thresholds (for example, 0.05, 0.15, 0.25, etc.) to make the yes/no decision. The area under the ROC curve is frequently used as a score.\n\nAnswers the question: What is the ability of the forecast to discriminate between events and non-events?\n\nROC: Perfect: Curve travels from bottom left to top left of diagram, then across to top right of diagram. Diagonal line indicates no skill.\n\nROC area: Range: 0 to 1, 0.5 indicates no skill. Perfect score: 1\n\nCharacteristics: ROC measures the ability of the forecast to discriminate between two alternative outcomes, thus measuring resolution. It is not sensitive to bias in the forecast, so says nothing about reliability. A biased forecast may still have good resolution and produce a good ROC curve, which means that it may be possible to improve the forecast through calibration. The ROC can thus be considered as a measure of potential usefulness.\n\nThe ROC is conditioned on the observations (i.e., given that an event occurred, what was the correponding forecast?) It is therefore a good companion to the reliability diagram, which is conditioned on the forecasts.\n\nMore information on ROC can be found in Mason 1982, Jolliffe and Stephenson 2012 (ch.3), and the WISE site.\n\n- - - - - - - - - - -\n\nDiscrimination diagram - Plot the likelihood of each forecast probability when the event occurred and when it did not occur. A summary score can be computed as the absolute value of the difference between the mean values of each distribution.\n\nAnswers the question: What is the ability of the forecast to discriminate between events and non-events?\n\nPerfect discrimination is when there is no overlap between the distributions of forecast probabilities for observed events and non-events. As with the ROC the discrimination diagram is conditioned on the observations (i.e., given that an event occurred, what was the correponding forecast?) Some users may find the discrimination diagram easier to understand than the ROC.\n\n- - - - - - - - - - -\n\nRanked probability score -\n\nwhere M is the number of forecast categories, pk is the predicted probability in forecast category k, and ok is an indicator (0=no, 1=yes) for the observation in category k.\n\nAnswers the question: How well did the probability forecast predict the category that the observation fell into?\n\nRange: 0 to 1. Perfect score: 0.\n\nCharacteristics: Measures the sum of squared differences in cumulative probability space for a multi-category probabilistic forecast. Penalizes forecasts more severely when their probabilities are further from the actual outcome. Negative orientation - can \"fix\" by subtracting RPS from 1. For two forecast categories the RPS is the same as the Brier Score.\n\nContinuous version -\n\n- - - - - - - - - - -\n\nRanked probability skill score -\n\nAnswers the question: What is the relative improvement of the probability forecast over climatology in predicting the category that the observations fell into?\n\nRange: -∞ to 1, 0 indicates no skill when compared to the reference forecast. Perfect score: 1.\n\nCharacteristics: Measures the improvement of the multi-category probabilistic forecast relative to a reference forecast (usually the long-term or sample climatology). Strictly proper. Takes climatological frequency into account. Unstable when applied to small data sets.\n\n- - - - - - - - - - -\n\nRelative value (value score) (Richardson, 2000; Wilks, 2001)\n\nAnswers the question: For a cost/loss ratio C/L for taking action based on a forecast, what is the relative improvement in economic value between climatalogical and perfect information?\n\nRange: -∞ to 1. Perfect score: 1.\n\nCharacteristics: The relative value is a skill score of expected expense, with climatology as the reference forecast. Because the cost/loss ratio is different for different users of forecasts, the value is generally plotted as a function of C/L.\n\nLike ROC, it gives information that can be used in decision making. When applied to a probabilistic forecasts system (for example, an ensemble prediction system), the optimal value for a given C/L may be achieved by a different forecast probability threshold than the optimal value for a different C/L. In this case it is necessary to compute relative value curves for the entire range of probabilities, then select the optimal values (the upper envelope of the relative value curves) to represent the value of the probabilistic forecast system. Click here for more information on the cost/loss model and relative value.\n\n- - - - - - - - - - -\n\nSee also Methods for ensemble prediction systems for more scientific/diagnostic techniques.\n\nScientific or diagnostic verification methods\n\nScientific, or diagnostic, verification methods delve more deeply into the nature of forecast errors. As a result they are frequently more complex than the standard verification measures described earlier. Distributions-oriented approaches and plots such as histograms, box plots, and scatter plots, are standard diagnostic verification methods.\n\nThis section gives very brief descriptions of several recently developed scientific and diagnostic methods, and relies heavily on references and links to other sites with greater detail.\n\nThis is also a place to promote new verification techniques. If you are working in this area, then you are encouraged to share your methods via this web site.\n\n- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -\n\nMethods for spatial forecasts\n\nScale decomposition methods - allow the errors at each scale to be diagnosed:\n\nWavelet decomposition (Briggs and Levine, 1997)\n\n- - - - - - - - - - -\n\nIntensity-scale verification approach (Casati et al. 2004)\n\nAnswers the question: How does the skill of spatial precipitation forecasts depend on both the scale of the forecast error and the intensity of the precipitation events?\n\nThe intensity-scale verification approach bridges traditional categorical binary verification, which provides information about skill for different precipitation intensities, with the more recent techniques which evaluate the forecast skill on different spatial scales (e.g., Zepeda-Arce et al., 2000; Briggs and Levine, 1997). It assesses the forecast on its whole domain, and is well suited for verifying spatially discontinuous fields, such as precipitation fields characterized by the presence of many scattered precipitation events. It provides useful insight on individual forecast cases as well as for forecast systems evaluated over many cases.\n\nForecasts are assessed using the Mean Squared Error (MSE) skill score of binary images, obtained from the forecasts and analyses by thresholding at different precipitation rate intensities. The skill score is decomposed on different spatial scales using a two-dimensional discrete Haar wavelet decomposition of binary error images. The forecast skill can then be evaluated in terms of precipitation rate intensity and spatial scale.\n\nhere to learn more.\n\n- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -\n\nDiscrete cosine transformation (DCT) (Denis et al., 2002a for method; Denis et al., 2002b and de Elia et al., 2002 for application)\n\n- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -\n\nNeighborhood (fuzzy) methods - relax the requirement for an exact match by evaluating forecasts in the local neighborhood of the observations.\n\nMulti-scale statistical organization (Zepeda-Arce et al., 2000)\n\n- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -\n\nFractions skill score (Roberts and Lean, 2008)\n\nAnswers the question: What are the spatial scales at which the forecast resembles the observations?\n\nThis approach directly compares the forecast and observed fractional coverage of grid-box events (rain exceeding a certain threshold, for example) in spatial windows of increasing size. These event frequencies are used directly to compute a Fractions Brier Score, a version of the more familiar (half) Brier score but now the observation can take any value between 0 and 1. The result can be framed as a Fractions Skill Score\n\nwhere Pf is the forecast fraction, Po is the observed fraction, and N is the number of spatial windows in the domain.\n\nFSS has the following properties:\n\nThe Fractions Skill Score ranges from 0 (complete mismatch) to 1 (perfect match).\n\nIf either there are no events forecast and some occur, or some occur and none are forecast the score is always 0.\n\nThe value of FSS above which the forecasts are considered to have useful (better than random) skill is given by FSSuseful = 0.5 + fo/2, where fo is the domain average observed fraction. The smallest window size for which FSS ≥ FSSuseful can be considered the \"skillful scale\".\n\nAs the size of the squares used to compute the fractions gets larger, the score will asympotote to a value that depends on the ratio between the forecast and observed frequencies of the event. The closer the asymptotic value is to 1, the smaller the forecast bias.\n\nThe score is most sensitive to rare events (e.g., small rain areas).\n\n- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -\n\nFuzzy logic (Damrath, 2004)\n\n- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -\n\nPragmatic (neighborhood) method (Theis et al., 2005)\n\n- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -\n\nSpatial multi-event contingency tables - useful for verifying high resolution forecasts (Atger, 2001).\n\nBy using multiple thresholds, a deterministic forecast system can be evaluated across a range of possible decision thresholds (instead of just one) using ROC and relative value. The decision thresholds might be intensity thresholds or even \"closeness\" thresholds (for example, forecast event within 10 km of the location of interest, within 20 km, 30 km, etc.). Such verification results can be used to assess the performance of high resolution forecasts where the exact spatial matching of forecast and observed events is difficult or unimportant. This multi-threshold approach enables a fairer comparison against ensemble prediction systems or other probabilistic forecasts.\n\nClick here to learn more.\n\n(related work: Tremblay et al., 1996)\n\n- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -\n\nPractically perfect hindcasts - assessing relative skill of spatial forecasts (Brooks et al, 1998; Kay, 2002)\n\n- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -\n\nNeighborhood verification framework - 12 neighborhood (a.k.a. fuzzy verification) methods combined into one framework (Ebert, 2008)\n\nNeighborhood verification approaches reward closeness by relaxing the requirement for exact matches between forecasts and observations. Some of these neighborhood methods compute standard verification metrics for deterministic forecasts using a broader definition of what constitutes a \"hit\". Other neighborhood methods treat the forecasts and/or observations as probability distributions and use verification metrics suitable for probability forecasts. Implicit in each neighborhood verification method is a particular decision model concerning what constitutes a good forecast.\n\nThe key to the neighborhood approach is the use of a spatial window or neighborhood surrounding the forecast and/or observed points. The treatment of the points within the window may include averaging (upscaling), thresholding, or generation of a PDF, depending on the neighborhood method used. The size of this neighborhood can be varied to provide verification results at multiple scales, thus allowing the user to determine at which scales the forecast has useful skill. Other windows could be included to represent closeness in time, closeness in intensity, and/or closeness in some other important aspect.\n\n- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -\n\nObject oriented methods:\n\nCRA (entity-based) verification (Ebert and McBride, 2000)\n\nAnswers the question: What is the location error of the (spatial) forecast, and how does the total error break down into components due to incorrect location, volume, and fine scale structure?\n\nThis object-oriented method verifies the properties of spatial forecasts of entities, where an entity is anything that can be defined by a closed contour. Some examples of entities, or blobs, are contiguous rain areas (CRAs, for which the method is named), convective outlook regions, and low pressure minima. For each entity that can be identified in the forecast and the observations, CRA verification uses pattern matching techniques to determine the location error, as well as errors in area, mean and maximum intensity, and spatial pattern. The total error can be decomposed into components due to location, volume, and pattern error. This is a useful property for model developers who need such information to improve the numerical weather prediction models.\n\nIn addition, the verified entities themselves may be classified as \"hits\", \"misses\", etc., according to how close the forecast location was to the observed location, and how well the maximum intensity was represented by the forecast. This event verification can be useful for monitoring forecast performance.\n\nClick here to learn more.\n\n- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -\n\nMethod for Object-based Diagnostic Evaluation (MODE) (Brown et al., 2004; Davis et al., 2006)\n\nAnswers the question: How similar are the forecast objects to the observed objects according to a variety of descriptive criteria ?\n\nMODE uses a convolution filter and thresholding to first identify objects in gridded fields. Performance at different spatial scales can be investigated by varying the values of the filter and threshold parameters. Then a fuzzy logic scheme is used to merge objects within a field, and match them between the forecast and the observations. Several attributes of the matched objects (location, area, volume, intensity, shape, etc.) are compared to see how similar they are. These are combined to give an \"interest value\" that summarizes the goodness of the match.\n\nOutput of the MODE algorithm include:\n\nAttributes of single matched shapes (i.e., hits)\n\nAttributes of single unmatched shapes (i.e., false alarms, misses)\n\nAttributes of clustered objects (i.e., groups of forecast or observed objects that are merged together)\n\nAttributes of interest to specific users (e.g., gaps between storms, for aviation strategic planning)\n\nAttributes can be summarized across many cases to understand how forecasts represent the storm/precipitation climatology, understand systematic errors, and document variability in performance in different situations.\n\nThe MODE verification scheme is part of the Model Evaluation Tools (MET) toolkit freely available from NCAR. More information on MODE is available from the Developmental Testbed Center.\n\n- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -\n\nEvent verification using composites (Nachamkin, 2004)\n\n- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -\n\nCluster analysis (Marzban and Sandgathe, 2006, 2008)\n\n- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -\n\nProcrustes shape analysis (Michaes et al., 2007; Lack et al. 2010)\n\n- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -\n\nStructure-Amplitude-Location (SAL) method (Wernli et al., 2008)\n\n- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -\n\nAutomated east-west phase error calculation (Keith Brill, NOAA/NWS/NCEP/HPC)\n\nAnswers the question: What is the phase error of the (spatial) forecast?\n\nThis approach considers both high and low pressure centers, troughs, and ridges, and takes into account the typical synoptic scale wavelength.\n\nGridded forecasts and analyses of mean sea level pressure are meridionally averaged within a zonal strip to give an east-west series of forecast and analyzed values. Cosine series trigonometric approximations are applied to both series, and the variance associated with each spectral component is computed. These are then sorted in descending order of variance to get the hierarchy of most important waves. If the hierarchies agree between the forecast and analyzed spectral components, then the phase angle (error) can be computed for each component.\n\nIn practice, the first spectral component is usually responsible for most of the variance and is the main one of interest. The phase errors are presented as time series. Click here to learn more.\n\n- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -\n\nFeature calibration and alignment (Hoffman et al., 1995; Nehrkorn et al., 2003)\n\n- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -\n\nField verification methods:\n\nDisplacement and Amplitude Score (DAS) Keil and Craig, 2009)\n\nAnswers the question: What is the distance between forecast and observed features?\n\nMeasuring distance between observed and forecast features is not only an intuitive error measure for many users, but also avoids the double-counting penalty where a feature displaced in space is scored worse than either a complete miss or a false alarm since it is penalized as both at once. The Displacement and Amplitude Score (DAS) is based on an optical flow algorithm that defines a vector field that deforms, or morphs, one image to match another. In DAS distance and amplitude errors are combined to produce a single measure. The optical flow method does not require identification and matching of discrete objects, which is often subjective and sensitive to many parameters in the algorithms.\n\nThe figure to the right shows the observed radar reflectivity (top left), forecast reflectivity (top right), forecast superimposed with displacement vector field matching the forecast onto the observation (bottom left), and morphed forecast (bottom right). The two components of DAS in forecast space comprise the mean displacement vector length and the root mean square error of morphed forecast and observation (bottom right – top left).\n\n- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -\n\nMethods for probabilistic forecasts, including ensemble prediction systems\n\nWilson method for EPS verification (Wilson et al, 1999)\n\n- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -\n\nMulti-category reliability diagram (Hamill, 1997)\n\n- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -\n\nRank histogram (Talagrand et al, 1997; Hamill, 2001)\n\nAnswers the question: How well does the ensemble spread of the forecast represent the true variability (uncertainty) of the observations?\n\nAlso known as a \"Talagrand diagram\", this method checks where the verifying observation usually falls with respect to the ensemble forecast data, which is arranged in increasing order at each grid point. In an ensemble with perfect spread, each member represents an equally likely scenario, so the observation is equally likely to fall between any two members.\n\nTo construct a rank histogram, do the following:\n\n1. At every observation (or analysis) point rank the N ensemble members from lowest to highest. This represents N+1 possible bins that the observation could fit into, including the two extremes\n\n2. Identify which bin the observation falls into at each point\n\n3. Tally over many observations to create a histogram of rank.\n\nInterpretation:\n\nFlat - ensemble spread about right to represent forecast uncertainty\n\nU-shaped - ensemble spread too small, many observations falling outside the extremes of the ensemble\n\nDome-shaped - ensemble spread too large, most observations falling near the center of the ensemble\n\nAsymmetric - ensemble contains bias\n\nNote: A flat rank histogram does not necessarily indicate a good forecast, it only measures whether the observed probability distribution is well represented by the ensemble.\n\n- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -\n\nCorrespondence ratio - ratio of the area of intersection of two or more events to the combined area of those events (Stensrud and Wandishin, 2000)\n\nwhere Fm,i is the value of forecast m at gridpoint i, and Oi is the corresponding observed value. In the diagram CR is the ratio of the dark area to the total shaded area. Click here to learn more.\n\n- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -\n\nLikelihood skill measure - Likelihood is defined very simply as the probability of the observations given the forecast. Likelihood-based measures can be used for binary and continuous probability forecasts, and provide a simple and natural general framework for the evaluation of all kinds of probabilistic forecasts. For more information see Jewson, (2003)\n\n- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -\n\nLogarithmic scoring rule (ignorance score) (Roulston and Smith, 2002)\n\nThe logarithmic scoring rule can be defined as follows: If there are n (mutually exclusive) possible outcomes and fi (i=1,...n) is the predicted probability of the ith outcome occurring then if the jth outcome is the one which actually occurs the score for this particular forecast-realization pair is given by\n\nIGN = -log2 fj\n\nAs defined above, with a negative sign, the logarithmic score cannot be negative and smaller values of the score are better. The minimum value of the score (zero) is obtained if a probability of 100% is assigned to the actual outcome. If a probability of zero is assigned to the actual outcome the logarithmic scoring rule is infinite. Click here to learn more.\n\n- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -\n\nMethods for rare events\n\nDeterministic limit (Hewson, 2007)\n\nAnswers the question: What is the length of time into the forecast in which the forecast is more likely to be correct than incorrect?\n\nThe 'deterministic limit' is defined, for categorical forecasts of a pre-defined rare meteorological event, to simply be the point ahead of issue time at which, across the population, the number of misses plus false alarms equals the number of hits (i.e. critical success index =0.5). A hypothetical example of an accuracy statement that might thus arise would be: 'The deterministic limit for predicting a windstorm, with gusts in excess of 60 kts at one or more low-lying inland stations in NW Europe, is 2.1 days'. The base rate (or event frequency) should also be disclosed. Recalibration of the forecast is often necessary for useful deterministic limit measures to be realised.\n\nAs they provide a clear measure of capability, deterministic limit values for various parameters may in due course be used as year-on-year performance indicators, and also to provide succinct guidelines for warning service provision. They could also be used as the cut-off point to switch from deterministic to probabilistic guidance. In turn this may help elevate the hitherto muted enthusiasm shown, by some customers, for probabilistic forecasts.\n\nClick here to learn more.\n\n- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -\n\nExtreme dependency score -\n\nSymmetric extreme dependency score -\n\nExtremal dependence index -\n\nSymmetric extremal dependence index -\n\nwhere p=(hits+misses)/total is the base rate (climatology), q=(hits+false alarms)/total is the frequency with which the event is forecast, H is the hit rate, also known as the probability of detection, and F is the false alarm rate, also known as the probability of false detection.\n\nAnswer the question: What is the association between forecast and observed rare events?\n\nRange: -1 to 1, 0 indicates no skill. Perfect score: 1\n\nCharacteristics: Scores converge to 2η-1 as event frequency approaches 0, where η is a parameter describing how fast the hit rate converges to zero for rarer events. EDS is independent of bias, so should be presented together with the frequency bias. Both EDI and SEDI are independent of the base rate. SEDI approaches 1 only as the forecast approaches perfection, whereas it is possible to optimize EDS and EDI for biased forecasts. For further details and comparison of the merits of these scores see Ferro and Stephenson (2011).\n\n- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -\n\nProbability model approach (Ferro, 2007) - Probability models that impose parametric forms on the relationships between observations and forecasts can help to quantify forecast quality for rare, binary events by identifying key features of the relationships and reducing sampling variation of verification measures. Click here to learn more.\n\n- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -\n\nOther methods\n\nDiagrams to plot several statistics at one time\n\nTaylor diagram of correlation coefficient, root-mean-square difference, and standard deviation (Taylor, 2001); see also LLNL description. An example of a Taylor diagram is shown at right.\n\nBLT diagram of relative climate mean squared difference, variance ratio, and effective correlation (Boer and Lambert, 2001).\n\n- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -\n\nCategorical performance diagram (Roebber 2009)\n\nIn an approach that is conceptually similar to the Taylor diagram, it is possible to exploit the geometric relationship between four measures of dichotomous forecast performance: probability of detection (POD), false alarm ratio or its opposite, the success ratio (SR), bias and critical success index (CSI; also known as the threat score). For good forecasts, POD, SR, bias and CSI approach unity, such that a perfect forecast lies in the upper right of the diagram. Skill is assessed by plotting the forecast quality measure relative to a reference forecast (climatology, persistence or any other desired baseline).\n\nClick here to learn more.\n\n- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -\n\nRoot mean squared factor (Golding, 1998)\n\nAnswers the question: What is the average multiplicative error?\n\nThe RMSF is the exponent of the root mean square error of the logarithm of the data. The logarithmic transformation is performed to smooth the data, reduce the discontinuities, and make the data more robust. Whereas the RMS error can be interpreted as giving a scale to the additive error, i.e., f = o ± RMS, the RMSF can be interpreted as giving a scale to the multiplicative error, i.e., f = o ×/÷ RMSF (read: \"multiplied or divided by\"), which is a more appropriate measure of accuracy for some variables and more intuitively meaningful than the RMS log error. In order to avoid assigning skill to trivial forecasts, statistics are only accumulated where either the forecast or observations are within specified limits. For example, for visibility verification, the lower and upper limits used by Golding (1998) were 1 m and 5000 m. When either the forecast or the observation lies within the range but the other is outside the range, then limits of half the lower limit or double the upper limit are prescribed on the other.\n\n- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -\n\nNash-Sutcliffe efficiency coefficient (Nash and Sutcliffe, 1970) -\n\nAnswers the question: How well does the forecast predict the observed time series?\n\nRange: -∞ to 1. Perfect score: 1.\n\nCharacteristics: Frequently used to quantify the accuracy of hydrological predictions. If E=0 then the model forecast is no more accurate than the mean of the observations; if E<0 then the mean observed value is a more accurate predictor than the model. The expression is identical to that for the coefficient of determination R2 and the reduction of variance.\n\n- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -\n\nAlpha Index (Koh and Ng, 2009) -\n\nAnswers the question: How does the random error of a forecast compare between regions of different observational variability?\n\nRange: 0 to 2. Perfect score: 0.\n\nCharacteristics: Alpha is a normalized measure of unbiased error variance, where the normalization factor is the reciprocal of the sum of forecast and observation variances. Replace the squares by inner products if the variable is a vector (e.g. wind).\n\nClick here to learn more.\n\n- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -\n\nElliptical representation of vector errors (Koh and Ng, 2009)\n\nAnswers the question: How does the vector error between the model and observation vary about the mean vector error (i.e., bias)?\n\nIn the diagram to the right, the mean vector error is represented by . The error variance ellipse may be represented by:\n\nstandard deviation,\n\neccentricity,\n\norientation of the major axis, θ.\n\nwhere a and b are the semi-major and semi-minor axes of the ellipse.\n\nRange: σ ∈ [0,∞), ε ∈ [0,1], θ ∈ [0,π) Perfect Score: for a vector error σ = 0, ε = 0\n\nCharacteristics: For the error ellipse (i.e., forecast minus observation), σ indicates the overall magnitude of the random error, θ is the preferred direction of the vector random error, and ε denotes the degree of preference for that direction.\n\nClick here to learn more.\n\n- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -\n\nQuantile-based categorical statistics (Jenkner et al., 2008)\n\nDichotomous forecasts can be thought of in terms of statistical frequencies instead of physical amplitudes. If the marginal totals of the contingency table are fixed by means of quantiles, categorical statistics benefit from some inherent advantages. The verification problem automatically is calibrated and the degrees of freedom reduce to one, allowing the potential accuracy of the calibrated forecast to be described using a single score. The total error can be split up into the bias and the potential accuracy, which can be measured by the quantile difference and the debiased Peirce's Skill Score, respectively. These two quantities provide a complete verification set with the ability to assess the full range of intensities. The verification can be computed for certain quantiles, as shown at right for the COSMO model's QPF performance over six predefined regions in Switzerland, or it can be aggregated over intensities by means of appropriate summary measures.\n\nClick here to learn more.\n\nSample forecast datasets\n\nFinley tornado forecasts\n\nThis is a classic example used in many textbooks and talks on forecast verification to illustrate the characteristics of the various categorical verification scores.\n\nIn March 1884 Sergeant John Finley initiated twice daily tornado forecasts for eighteen regions in the United States, east of the Rocky Mountains. Finley claimed 95.6% to 98.6% overall accuracy for the first 3-month period, depending on the time and district, with some districts achieving 100% accuracy for all 3 months. A critic of the results pointed out that 98.2% accuracy could be had by merely forecasting \"no tornado\"! This clearly illustrates the need for more meaningful verification scores.\n\nThe contingency table for Finley's (1884) forecasts is:\n\nObserved tornado no tornado Total Forecast tornado 28 72 100 no tornado 23 2680 2703 Total 51 2752 2803\n\nClick here to see how the different categorical scores rate the Finley (1884) forecasts.\n\nProbability of precipitation forecasts\n\n24-hour and 48-hour forecasts of probability of precipitation were made by the Finnish Meteorological Institute (FMI) during 2003, for daily precipitation in the city of Tampere in south central Finland. Three precipitation categories were used:\n\nCategory 0: RR ≤ 0.2 mm\n\nCategory 1: 0.3 mm ≤ RR ≤ 4.4 mm\n\nCategory 2: RR ≥ 4.5 mm\n\nThe probability of rain in each category was predicted each day, with the probabilities across the three categories adding up to 1.\n\nClick here to view the data and see the standard probabilitistic verification results for these precipitation forecasts. Scores and diagnostic plots that are demonstrated include Brier score and its decomposition, Brier skill score, reliability diagram, relative operating characteristic (ROC), relative value, ranked probability score, and ranked probability skill score.\n\nFreely available verification tools and packages\n\nModel Evaluation Tools (MET)\n\nThe Model Evaluation Tools (MET) verification package was developed by the National Center for Atmospheric Research (NCAR) Developmental Testbed Center (DTC). It is a highly-configurable, state-of-the-art suite of verification tools. It was developed using output from the Weather Research and Forecasting (WRF) modeling system but may be applied to the output of other modeling systems as well. It computes the following:\n\nStandard verification scores comparing gridded model data to point-based observations\n\nStandard verification scores comparing gridded model data to gridded observations\n\nSpatial verification methods comparing gridded model data to gridded observations using neighborhood, object-based, and intensity-scale decomposition approaches\n\nEnsemble and probabilistic verification methods comparing gridded model data to point-based or gridded observations\n\nAggregating the output of these verification methods through time and space\n\nEnsemble Verification System (EVS)\n\nThe Ensemble Verification System is designed to verify ensemble forecasts of hydrologic and hydrometeorological variables, such as temperature, precipitation, streamflow, and river stage, issued at discrete forecast locations (points or areas). It is an experimental prototype developed by the Hydrological Ensemble Prediction group of the NOAA Office of Hydrologic Development.\n\nThis Java application in intended to be flexible, modular, and open to accommodate enhancements and additions by its developers and users. Participation in the continuing development of the EVS toward a versatile and standardized tool for ensemble verification is welcomed. For more information see the EVS web site, or the papers by Brown et al. (2010) and Demargne et al. (2010).\n\nR\n\nThe R Project for Statistical Computing has free software for statistical computing and graphics, including some packages for forecast verification. In particular, the \"verification\" package provides basic verification functions including ROC plots, attributes (reliability) diagrams, contingency table scores, and more, depending on the type of forecast and observation. It verifies\n\nbinary forecasts versus binary observations,\n\nprobabilistic forecasts versus binary observations,\n\ncontinous forecasts versus continuous observations,\n\nensemble forecasts versus continuous observations,\n\nspatial forecasts versus spatial observations using fractions skill score and the intensity-scale method.\n\nClick here to find out how to get the R forecast verification routines.\n\nClimate Explorer\n\nThe Climate Explorer is a web based tool for performing climate analysis that also includes several options for seasonal forecast verification. The user is allowed to select a particular season and variable of interest (e.g., precipitation, 2 metre temperature, sea surface temperature, sea level pressure, etc.) and a seasonal forecast model (e.g., ECMWF, UK Met Office, NCEP/CPF, ECHAM4.5, in addition to a large number of models participating in the EU projects DEMETER and ENSEMBLES, and the corresponding observations prior to performing verification. Climate Explorer offers a large number of deterministic and probabilistic scores for assessing the performance of seasonal ensemble predictions (e.g., correlation; root meansquare error and mean absolute error of the ensemble mean; Brier score and its decomposition into reliability, resolution and uncertainty; reliability diagram; Brier skill score; tercile and quintile ranked probability score; tercile and quintile ranked probability skill score; and relative operating characteristics (ROC) curve). Forecast verification results and scores are displayed as spatial maps, diagrams and single values when the user selects the option for time series verification.\n\nSome Frequently Asked Questions\n\n1. How many samples are needed to get reliable verification results?\n\n2. What is the best statistic for measuring the accuracy of a forecast?\n\n4. How do I compare gridded forecasts (from a model, for example) with observations at point locations?\n\n5. How do I verify worded forecasts?\n\n6. What does \"hedging\" a forecast mean, and how do some scores encourage hedging?\n\n7. Is there a difference between \"verification\" and \"validation\"?\n\n8. What is the relationship between confidence intervals and prediction intervals?\n\n9. How do I know whether one forecast system performs significantly better than another?\n\n10. What are the challenges and strategies to verify weather and climate extremes?\n\n11. Reliability and resolution - how are they different?\n\n...\n\nDiscussion group\n\nWe welcome discussion, questions, and new methods of verification. You may wish to join an e-mail discussion group on verification called \"vx-discuss\". To subscribe, visit the vx-discuss web page. This discussion group was begun in June 2003.\n\nLinks to verification and related sites\n\nGeneral\n\nGlossary of Verification Terms - excellent list of definitions and scores, with equations, compiled by NOAA Space Environment Center\n\nGlossary of Forecast Verification Terms - another excellent list of definitions and scores, with equations, compiled by David Stephenson\n\nHarold Brooks' site - a great reference list and a sample temperature data set to play with\n\nDavid Stephenson's Forecasting page - useful links to books, articles, and other things related to statistics and forecast verification\n\nStatistics A New View of Statistics - Will Hopkins' statistical primer for the health sciences\n\nEngineering Statistics Handbook - NIST / SEMATECH summaries of statistical methods\n\nWeb Interface for Statistics Education (WISE) - teaching resources offered through Introductory Statistics courses, especially in the social sciences\n\nDr. Arsham's Web Page - zillions of links to web-based statistics resources\n\nMeteorological - methods\n\nEUMETCAL Forecast Verification tutorial - terrific hands-on tutorial on basic forecast verification methods, last updated June 2009\n\nSurvey of common verification methods in meteorology - classic WMO publication by Stanski et al. on verification with clear descriptions and examples, 1989\n\nStandardised Verification System (SVS) for Long-Range Forecasts (LRF) - WMO/CBS framework for long range forecast verification. See also New Attachment, 2002 (MSWord document)\n\nWGNE survey of verification methods for numerical prediction of weather elements and severe weather events - excellent summary by Philippe Bougeault on the state of the art in 2002\n\nRecommendations on the verification of local weather forecasts (at ECMWF member states) - consultancy report to ECMWF Operations Department by Pertti Nurmi, October 2003\n\nRecommendations for the verification and intercomparison of QPFs from operational NWP models - WWRP/WGNE Joint Verification Working Group recommendations, December 2004\n\nReview of current methods and tools for verification of numerical forecasts of precipitation - summary report prepared for COST717\n\nIntegrated verification procedures for forecasts and warnings - Ian Mason's 1999 consultancy report for the Bureau of Meteorology\n\nDevelopment of standard verification measures for EPS - document submitted by L. Lefaivre to WMO Commission for Basic Systems, October 2001\n\nWMO verification guidance for public weather services - good overall guidance on verifying public weather forecasts\n\nWMO Climate Information and Prediction Services (CLIPS) curriculum - education on climate model predictions\n\nVerification of Forecasts of Convection: Uses, Abuses, and Requirements - Chuck Doswell speaks out\n\nModel Evaluation Tools - A highly-configurable, state-of-the-art suite of verification tools, freely available for download!\n\nMeteorological - examples\n\nNOAA Forecast Systems Laboratory's (FSL) Real Time Verification System (RTVS) - large variety of real-time verification results with an aviation emphasis\n\nVerification of NCEP model QPFs - rain maps and verification scores for regional and mesoscale models over the USA\n\nMOS Verification over the US - operational verification of temperature and probability of precipitation forecasts using several scores\n\nEnsemble Evaluation and Verification - NCEP ensemble prediction system verification\n\nDEMETER Verification - deterministic and probabilistic verification of EU multi-model ensemble system for seasonal to interannual prediction\n\nWorkshops\n\n6th International Verification Methods Workshop, 13-19 March 2014 New Delhi, India - Presentations and tutorial lectures.\n\n5th International Verification Methods Workshop, 1-7 December 2011, Melbourne, Australia - Presentations and tutorial lectures. Click here to see the 2013 special issue of Meteorological Applications on Forecast Verification featuring papers from the 2011 workshop.\n\n4th International Verification Methods Workshop, 8-10 June 2009, Helsinki, Finland - Presentations and tutorial lectures.\n\n3rd International Verification Methods Workshop, 31 January-2 February, 2007, Reading,UK - Tutorial lecture notes and scientific presentations. Click here to see the 2008 special issue of Meteorological Applications on Forecast Verification that features papers from the workshop.\n\n2nd International Verification Methods Workshop, September 15-17, 2004, Montreal, Canada - Presentations and discussion\n\nWorkshop on Making Verification More Meaningful, Boulder, CO, 30 July - 1 August 2002 - Presentations and posters\n\nRFC River Forecast Verification Workshop, Silver Spring, MD, 27-28 February 2002 - Talks on verification of river forecasts\n\nWWRP/WMO Workshop on the Verification of Quantitative Precipitation Forecasts, Prague, Czech Republic, 14-16 May 2001 - papers on verification of QPFs\n\nSRNWP Mesoscale Verification Workshop 2001, KNMI, De Bilt, The Netherlands, 23-24 April 2001\n\nCenters\n\nNOAA/NCEP Hydrometeorological Prediction Center (HPC) - verification of precipitation and temperature forecasts over the United States\n\nEnvironmental Verification and Analysis Center (EVAC) - U. Oklahoma site with precipitation verification at the GPCP Surface Reference Data Center\n\nThe Met Office (UK) verification page\n\nMiscellaneous\n\nThe Royal Meteorological Society Quality of Weather Forecasts Project - online survey of user-focused forecast assessment\n\nSpatial Verification Methods Inter-comparison Project - comparison of newly proposed spatial methods to give the user information about which methods are appropriate for which types of data, forecasts and desired forecast utility.\n\nReferences and further reading\n\nBooks, technical reports, and journal special issues\n\nKatz, R.W. and A.H. Murphy (eds), 1997: Economic Value of Weather and Climate Forecasts. Cambridge University Press, Cambridge.\n\nJolliffe, I.T., and D.B. Stephenson, 2012: Forecast Verification: A Practitioner's Guide in Atmospheric Science. 2nd Edition. Wiley and Sons Ltd, 274 pp.\n\nMurphy, A.H. and R.W. Katz, ed., 1985: Probability, Statistics, and Decision Making in the Atmospheric Sciences. Westview Press, Boulder, CO.\n\nNurmi, P., 2003: Recommendations on the verification of local weather forecasts (at ECWMF member states). ECMWF Operations Department, October 2003. Click here to access a PDF version (464 kB).\n\nStanski, H.R., L.J. Wilson, and W.R. Burrows, 1989: Survey of common verification methods in meteorology. World Weather Watch Tech. Rept. No.8, WMO/TD No.358, WMO, Geneva, 114 pp. Click here to access a PDF version.\n\nvon Storch, H. and F.W. Zwiers, 1999: Statistical Analysis in Climate Research. Cambridge University Press, Cambridge.\n\nWilks, D.S., 2011: Statistical Methods in the Atmospheric Sciences. 3rd Edition. Elsevier, 676 pp.\n\nSpecial issues of Meteorological Applications on Forecast Verification (2008, 2013)\n\nSpecial collection in Weather and Forecasting (2009-2010) on the Spatial Forecast Verification Methods Inter-Comparison Project (ICP)\n\nJournal articles and conference preprints\n\nAccadia, C., S. Mariani, M. Casaioli, A. Lavagnini, and A. Speranza, 2005: Verification of precipitation forecasts from two limited-area models over Italy and comparison with ECMWF forecasts using a resampling technique. Wea. Forecasting, 20, 276-300.\n\nAhijevych, D., E. Gilleland, B.G. Brown, E.E. Ebert, 2009: Application of spatial verification methods to idealized and NWP-gridded precipitation forecasts. Wea. Forecasting, 24, 1485-1497.\n\nAmodei, M. and J. Stein, 2009: Deterministic and fuzzy verification methods for a hierarchy of numerical models. Met. Appl., 16, 191-203.\n\nAtger, F., 2001: Verification of intense precipitation forecasts from single models and ensemble prediction systems. Nonlin. Proc. Geophys., 8, 401-417. Click here to see the abstract and get the PDF (295 Kb).\n\nAtger, F., 2003: Spatial and interannual variability of the reliability of ensemble-based probabilistic forecasts: Consequences for calibrations. Mon. Wea. Rev., 131, 1509-1523.\n\nAtger, F., 2004: Relative impact of model quality and ensemble deficiencies on the performance of ensemble based probabilistic forecasts evaluated through the Brier score. Nonlin. Proc. Geophys., 11, 399-409.\n\nAtger, F., 2004: Estimation of the expected reliability of ensemble-based probabilistic forecasts. Q. J. R. Meteorol. Soc., 130, 627-646.\n\nBaldwin, M.E. and J.S. Kain, 2006: Sensitivity of several performance measures to displacement error, bias, and event frequency. Wea. Forecasting, 21, 636-648.\n\nBarnes, L.R., E.C. Gruntfest, M.H. Hayden, D.M. Schultz, C. Benight, 2007: False alarms and close calls: A conceptual model of warning accuracy. Wea. Forecasting, 22, 1140-1147.\n\nBarnes, L.R., D.M. Schultz, E.C. Gruntfest, M.H. Hayden and C.C. Benight, 2009: CORRIGENDUM: False alarm rate or false alarm ratio? Wea. Forecasting, 24, 1452-1454.\n\nBarnston, A.G., S. Li, S.J. Mason, D. G. DeWitt, L. Goddard, and X. Gong, 2010: Verification of the first 11 years of IRI's seasonal climate forecasts. J. Appl. Meteor. Climatol., 49, 493-520.\n\nBarnston, A.G. and S.J. Mason, 2011: Evaluation of IRI�s seasonal climate forecasts for the extreme 15% tails. Wea. Forecasting, 26, 545-554.\n\nBieringer, P., and P. S. Ray, 1996: A comparison of tornado warning lead times with and without NEXRAD Doppler radar. Wea. Forecasting, 11, 41-46.\n\nBland, J.M. and D.G. Altman, 1986: Statistical methods for assessing agreement between two methods of clinical measurement. Lancet, i, 307-310.\n\nBlattenberger, G., and F. Lad, 1985: Separating the Brier score into calibration and refinement components: A graphical exposition. The American Statistician, 39, 26-32.\n\nBoer, G.J and S. J. Lambert, 2001: Second-order space-time climate difference statistics. Climate Dynamics, 17, 213-218.\n\nBowler, N.E., 2008: Accounting for the effect of observation errors on verification of MOGREPS. Meteorol. Appl., 15.\n\nBradley, A.A., T. Hashino, and S.S. Schwartz, 2003: Distributions-oriented verification of probability forecasts for small data samples. Wea. Forecasting, 18, 903-917.\n\nBradley, A.A., S.S. Schwartz, and T. Hashino, 2008: Sampling uncertainty and confidence intervals for the Brier score and Brier skill score. Wea. Forecasting, 23, 992-1006.\n\nBrier, G. W., 1950: Verification of forecasts expressed in terms of probability. Mon. Wea. Rev., 78, 1-3.\n\nBriggs, W.M. and R.A. Levine, 1997: Wavelets and field forecast verification. Mon. Wea. Rev., 125, 1329-1341.\n\nBröcker, J. and L.A. Smith, 2007: Increasing the reliability of reliability diagrams. Wea. Forecasting, 22, 651-661.\n\nBröcker, J. and L.A. Smith, 2007: Scoring probabilistic forecasts: the importance of being proper. Wea. Forecasting, 22, 382-388.\n\nBrooks, H.E. and C.A. Doswell III, 1996: A comparison of measures-oriented and distributions-oriented approaches to forecast verification. Wea. Forecasting, 11, 288-303.\n\nBrooks, H.E., M. Kay and J.A. Hart, 1998: Objective limits on forecasting skill of rare events. 19th Conf. Severe Local Storms, AMS, 552-555.\n\nBrown, B.G., R.R. Bullock, C.A. David, J.H. Gotway, M.B. Chapman, A. Takacs, E. Gilleland, K. Manning, J. Mahoney, 2004: New verification approaches for convective weather forecasts. 11th Conf. Aviation, Range, and Aerospace Meteorology, 4-8 Oct 2004, Hyannis, MA.\n\nBrown, B.G. and A.H. Murphy, 1987: Quantification of uncertainty in fire-weather forecasts: Some results of operational and experimental forecasting programs. Wea. Forecasting, 2, 190-205.\n\nBrown, B.G., G. Thompson, R.T. Bruintjes, R. Bullock, and T. Kane, 1997: Intercomparison of in-flight icing algorithms: Part II: Statistical verification results. Wea. Forecasting, 12, 890-914.\n\nBrown J.D., Demargne J., Seo D-J., and Liu Y., 2010: The Ensemble Verification System (EVS): a software tool for verifying ensemble forecasts of hydrometeorological and hydrologic variables at discrete locations. Environmental Modelling and Software, 25, 854-872.\n\nCandille, G., C. Côté, P. L. Houtekamer and G. Pellerin, 2007: Verification of an ensemble prediction system against observations. Mon. Wea. Rev., 135, 1140-1147.\n\nCasati, B., Ross, D.B. Stephenson, 2004: A new intensity-scale approach for the verification of spatial precipitation forecasts, Meteorol. Appl., 11, 141-154.\n\nCasati, B., 2010: New developments of the intensity-scale technique within the Spatial Verification Methods Intercomparison Project. Wea. Forecasting, 25, 113-143.\n\nCasati, B., and L.J. Wilson, 2007: A New spatial-scale decomposition of the Brier score: Application to the verification of pightning probability forecasts. Mon. Wea. Rev., 135, 3052-3069.\n\nCasati, B., L.J. Wilson, D.B. Stephenson, P. Nurmi, A. Ghelli, M. Pocernich, U. Damrath, E.E. Ebert, B.G. Brown and S. Mason, 2008: Forecast verification: current status and future directions. Meteorol. Appl., 15, 3-18.\n\nCase, J.L., J. Manobianco, J. E. Lane, C.D. Immer, and F.J. Merceret, 2004: An objective technique for verifying sea breezes in high-resolution numerical weather prediction models. Wea. Forecasting, 19, 690-705.\n\nClemen, R.T., A.H. Murphy, and R.L. Winkler, 1995: Screening probability forecasts: Contrasts between choosing and combining. Int. J. Forecasting, 11, 133-146.\n\nCloke, H.L. and F. Pappenberger, 2008: Evaluating forecasts of extreme events for hydrological applications: an approach for screening unfamiliar performance measures. Meteorol. Appl., 15, 181-197.\n\nDamrath, U., 2004: Verification against precipitation observations of a high density network - what did we learn? Intl. Verification Methods Workshop, 15-17 September 2004, Montreal, Canada. Click here to download the PDF (980 Kb).\n\nDavis, C. and F. Carr, 2000: Summary of the 1998 Workshop on Mesoscale Model Verification. Bull. Amer. Met. Soc., 81, 809-819.\n\nDavis, C., B. Brown, and R. Bullock, 2006a: Object-based verification of precipitation forecasts. Part I: Methods and application to mesoscale rain areas. Mon. Wea. Rev., 134, 1772-1784.\n\nDavis C.A., B.G. Brown, and R.G. Bullock, 2006b. Object-based verification of precipitation forecasts, Part II: Application to convective rain systems. Mon. Wea. Rev. 134, 1785-1795.\n\nDavis, C.A., B.G. Brown, R. Bullock, and J. Halley-Gotway, 2009: The Method for Object-Based Diagnostic Evaluation (MODE) applied to numerical forecasts from the 2005 NSSL/SPC Spring Program. Wea. Forecasting, 24, 1252-1267.\n\nde Elia, R., R. Laprise, and B. Denis, 2002: Forecasting skill limits of nested, limited-area models: A perfect-model approach. Mon. Wea. Rev., 130, 2006-2023.\n\nde Elia, R. and R. Laprise, 2003: Distribution-oriented verification of limited-area model forecasts in a perfect-model framework. Mon. Wea. Rev., 131, 2492-2509.\n\nDeGroot, M.H., and S.E. Fienberg, 1983: The comparison and evaluation of forecasters. The Statistician, 32, 14-22.\n\nDemargne, J., M. Mullusky, K. Werner, T. Adams, S. Lindsey, N. Schwein, W. Marosi, and E. Welles, 2009: Application of forecast verification science to operational river forecasting in the U.S. National Weather Service. Bull. Amer. Meteorol. Soc., 90, 779-784.\n\nDemargne J., J.D. Brown, Y. Liu Y., D-J. Seo, L. Wu, Z. Toth, and Y. Zhu, 2010: Diagnostic verification of hydrometeorological and hydrologic ensembles. Atmos. Sci. Lett., 11, 114-122.\n\nDenis, B., J. Côté and R. Laprise, 2002a: Spectral decomposition of two-dimensional atmospheric fields on limited-area domains using the discrete cosine transform (DCT). Mon. Wea. Rev., 130, 1812-1829.\n\nDenis, B., R. Laprise, D. Caya, and J. Côté, 2002b: Downscaling ability of one-way nested regional climate models: the Big-Brother Experiment. Climate Dynamics, 18, 627-646.\n\nDoblas-Reyes,, F.J., C. A. S. Coelho, D. B. Stephenson, 2008: How much does simplification of probability forecasts reduce forecast quality? Meteorol. Appl., 15.\n\nDoswell, C.A. III, R. Davies-Jones, and D.L. Keller, 1990: On summary measures of skill in rare event forecasting based on contingency tables. Wea. Forecasting, 5, 576-585.\n\nDuc, L., K. Saito, and H. Seko, 2013: Spatial-temporal fractions verification for high-resolution ensemble forecasts. Tellus A, 65.\n\nEbert, E.E., 2008: Fuzzy verification of high resolution gridded forecasts: A review and proposed framework. Meteorol. Appl., 15, 51-64.\n\nEbert, E.E., 2009: Neighborhood verification: a strategy for rewarding close forecasts. Wea. Forecasting, 24, 1498-1510.\n\nEbert, E.E. and W.A. Gallus, 2009: Toward better understanding of the contiguous rain area (CRA) method for spatial forecast verification. Wea. Forecasting, 24, 1401-1415.\n\nEbert, E.E. and J.L. McBride, 2000: Verification of precipitation in weather systems: Determination of systematic errors. J. Hydrology, 239, 179-202.\n\nEfron, B. and R. Tibshirani, 1986: Bootstrap methods for standard errors, confidence intervals, and other measures of statistical accuracy. Statistical Science, 1, 54-77.\n\nEhrendorfer, M., and A.H. Murphy, 1988: Comparative evaluation of weather forecasting systems: Sufficiency, quality, and accuracy. Mon. Wea. Rev., 11"
    }
}
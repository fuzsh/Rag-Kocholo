{
    "id": "correct_foundationPlace_00011_0",
    "rank": 44,
    "data": {
        "url": "https://slideplayer.com/slide/14145574/",
        "read_more_link": "",
        "language": "en",
        "title": "Question Answering over Linked Data",
        "top_image": "",
        "meta_img": "",
        "images": [
            "https://slideplayer.com/static/blue_design/img/logo_slideplayer.png",
            "https://slideplayer.com/static/blue_design/img/slide-loader4.gif",
            "https://slideplayer.com/slide/14145574/86/images/1/Question+Answering+over+Linked+Data.jpg",
            "https://slideplayer.com/slide/14145574/86/images/2/History+Take+search+engines+as+an+example%3A.jpg",
            "https://slideplayer.com/slide/14145574/86/images/3/Current+Status+Search+engines+%EF%83%A0+Knowledge+engines+%28ex.+Google+Knowledge+Graph%29.jpg",
            "https://slideplayer.com/slide/14145574/86/images/4/Google+Typed+links+to%3A+born%2C+die%2C+place%2C+works%2C+other+related+persons..jpg",
            "https://slideplayer.com/slide/14145574/86/images/5/Current+Status+Question+Answering+over+textual+data+%28unstructured+documents+or+free+text%29+Natural+Language+Interfaces+to+Databases..jpg",
            "https://slideplayer.com/slide/14145574/86/images/6/Current+Status+Question+Answering+over+textual+data+%28unstructured+documents+or+free+text%29.jpg",
            "https://slideplayer.com/slide/14145574/86/images/7/Current+Status+Natural+Language+Interfaces+to+Databases+%28NLIDB%29.jpg",
            "https://slideplayer.com/slide/14145574/86/images/8/Current+Status+Question+Answering+over+Linked+Data.jpg",
            "https://slideplayer.com/slide/14145574/86/images/9/Examples+Natural+Language+Search+Interfaces+over+Structured+Knowledge.jpg",
            "https://slideplayer.com/slide/14145574/86/images/10/Keyword-based+Search+%28Semantic+Search%29+over+Linked+Data.jpg",
            "https://slideplayer.com/slide/14145574/86/images/11/Assumptions+Natural+language+queries+cannot+always+be+converted+into+formal+queries+automatically..jpg",
            "https://slideplayer.com/slide/14145574/86/images/12/Approach+Based+on+a+set+of+user-supplied+keywords%2C+we+first+compute+a+list+of+candidate+IRIs+for+each+of+the+keywords+issued+by+the+user..jpg",
            "https://slideplayer.com/slide/14145574/86/images/13/Approach+User+supply+keywords+%3A+island%E3%80%81+Germany.jpg",
            "https://slideplayer.com/slide/14145574/86/images/14/Approach+Ranking+and+Selecting+Anchor+Points.jpg",
            "https://slideplayer.com/slide/14145574/86/images/15/Approach+Ranking+and+Selecting+Anchor+Points%3A.jpg",
            "https://slideplayer.com/slide/14145574/86/images/16/Approach+Ranking+and+Selecting+Anchor+Points%3A+connectivity+degree%28CD%29.jpg",
            "https://slideplayer.com/slide/14145574/86/images/17/Approach+Ranking+and+Selecting+Anchor+Points%3A.jpg",
            "https://slideplayer.com/slide/14145574/86/images/18/Approach+Another+assumption+based+on+observation+%28log+of+DBpedia+public+SPARQL+endpoint%3A+ftp%3A%2F%2Fdownload.openlinksw.com%2Fsupport%2Fdbpedia%EF%BC%9B%E5%B7%B2%E7%B6%93%E6%B2%92%E4%BA%86%29.jpg",
            "https://slideplayer.com/slide/14145574/86/images/19/Approach.jpg",
            "https://slideplayer.com/slide/14145574/86/images/20/An+example+%E5%8D%B3+P7+and+P8.jpg",
            "https://slideplayer.com/slide/14145574/86/images/21/Approach+%E7%B6%93%E9%81%8E%E5%AF%A6%E9%A9%97%EF%BC%8C%E5%BE%9E+17+%E5%80%8B%E8%AE%8A%E6%88%90%E4%BB%A5%E4%B8%8B%E5%BB%BA%E8%AD%B0%E7%9A%84%E5%85%AB%E5%80%8B+patterns.jpg",
            "https://slideplayer.com/slide/14145574/86/images/22/Query+Generation+SPARQL+QUERY+GENERATION.jpg",
            "https://slideplayer.com/slide/14145574/86/images/23/Examples+Natural+Language+Search+Interfaces+over+Structured+Knowledge.jpg",
            "https://slideplayer.com/slide/14145574/86/images/24/NLIDB%3A+Why+Users+are+thus+freed+from+major+access+requirements+to+%28the+Semantic+Web%29%3A+the+mastery+of+a+formal+query+language+like+SPARQL+or+SQL+and..jpg",
            "https://slideplayer.com/slide/14145574/86/images/25/NLIDB%3A+Challenges+%21+NLIDB+is+still+an+AI-complete+problem..jpg",
            "https://slideplayer.com/slide/14145574/86/images/26/NLIDB%3A+Challenges+%21+Categorized+into+three+problems%3A.jpg",
            "https://slideplayer.com/slide/14145574/86/images/27/NLIDB%3A+Challenges+%21.jpg",
            "https://slideplayer.com/slide/14145574/86/images/28/NLIDB%3A+Challenges+%21.jpg",
            "https://slideplayer.com/slide/14145574/86/images/29/Categories+of+NLIDB+Domain+Specific%3A+four+architectures.jpg",
            "https://slideplayer.com/slide/14145574/86/images/30/Pattern+Matching+Patterns+are+defined+using+some+manually+defined+rules.+The+natural+language+query+is+mapped+to+these+defined+rules..jpg",
            "https://slideplayer.com/slide/14145574/86/images/31/Syntax+based+Systems.jpg",
            "https://slideplayer.com/slide/14145574/86/images/32/Semantic+Grammar+Systems.jpg",
            "https://slideplayer.com/slide/14145574/86/images/33/Semantic+Grammar+a+lexicon%3A+stores+all+the+possible+words+%28ie.+single+word%29+that+the+grammar+is+aware+of..jpg",
            "https://slideplayer.com/slide/14145574/86/images/34/Semantic+Grammar+rules%3A+combine+the+terminal+symbols+in+the+lexicon+to+form+phrases+or+sentences+in+a+specific+way..jpg",
            "https://slideplayer.com/slide/14145574/86/images/35/Intermediate+Representation+Language.jpg",
            "https://slideplayer.com/slide/14145574/86/images/36/Domain+Independent+NLIDB+applications+fail+mostly+dealing+with+indirect+and+complex+requests..jpg",
            "https://slideplayer.com/slide/14145574/86/images/37/NLIDB+Limitations+Databases+are+not+interoperable+and+distributed+over+the+web.jpg",
            "https://slideplayer.com/slide/14145574/86/images/38/QALD+Question+Answering+over+Linked+Data.jpg",
            "https://slideplayer.com/slide/14145574/86/images/39/QALD+QALD+%E6%98%AF%E7%9B%AE%E5%89%8D%E6%9C%80%E7%9F%A5%E5%90%8D%E7%9A%84+Linked+Data+%E5%95%8F%E7%AD%94%E7%B3%BB%E7%B5%B1%E6%87%89%E7%94%A8%E8%A9%95%E4%BC%B0%E5%A4%A7%E6%9C%83.jpg",
            "https://slideplayer.com/slide/14145574/86/images/40/QALD+QALD-7+%E7%9A%84%E5%95%8F%E9%A1%8C%E9%9B%86%E5%88%86%E6%88%90%E5%9B%9B%E9%A1%9E%EF%BC%9A+Multilingual+question+answering+over+DBpedia%E3%80%81.jpg",
            "https://slideplayer.com/slide/14145574/86/images/41/QALD+QALD-7+%E7%9A%84%E5%95%8F%E9%A1%8C%E9%9B%86%E5%88%86%E6%88%90%E5%9B%9B%E9%A1%9E%EF%BC%9A+%E9%A1%8C%E7%9B%AE%E7%9A%84%E6%95%B8%E9%87%8F%E4%B9%9F%E9%80%90%E5%B9%B4%E6%88%90%E9%95%B7%EF%BC%8C%E5%BE%9E+QALD-1+%E7%9A%84+50%E9%A1%8C%E5%88%B0+QALD-6+%E5%BE%8C%E5%B7%B2%E7%B6%93%E6%9C%89+350+%E9%A1%8C.jpg",
            "https://slideplayer.com/slide/14145574/86/images/42/QALD+%E5%A4%A7%E6%9C%83%E6%8C%87%E5%87%BA%E7%9A%84%E7%8F%BE%E6%B3%81.jpg",
            "https://slideplayer.com/slide/14145574/86/images/43/QALD+%E4%BD%BF%E7%94%A8%E7%9A%84%E8%B3%87%E6%96%99%E9%9B%86+DBpedia+MusicBrainz+%28QALD-3+%E4%BB%A5%E5%89%8D%EF%BC%9F%29.jpg",
            "https://slideplayer.com/slide/14145574/86/images/44/QALD+%E9%A1%8C%E7%9B%AE%E7%9A%84%E8%A8%AD%E8%A8%88+Test+question+set+for+DBpedia.jpg",
            "https://slideplayer.com/slide/14145574/86/images/45/QALD+Evaluation+With+respect+to+a+single+question+q%2C.jpg",
            "https://slideplayer.com/slide/14145574/86/images/46/QALD+Evaluation.jpg",
            "https://slideplayer.com/slide/14145574/86/images/47/Which+companies+are+located+in+California%2C+USA.jpg",
            "https://slideplayer.com/slide/14145574/86/images/48/An+example+QALD+Which+companies+are+located+in+California%2C+USA.jpg",
            "https://slideplayer.com/slide/14145574/86/images/49/An+example+QALD+%E6%89%BE%E5%87%BA%E7%9F%A5%E8%AD%98%E5%BA%AB%E4%B8%AD%E5%B0%8D%E6%87%89%E7%9A%84%E5%AF%A6%E9%AB%94+%EF%BC%88%E4%BE%8B%E5%A6%82+Google.jpg",
            "https://slideplayer.com/slide/14145574/86/images/50/An+example+QALD+%E6%A0%B9%E6%93%9A%E6%9F%90%E7%A8%AE%E8%A6%8F%E5%89%87%EF%BC%88evaluation+order%EF%BC%89%E5%AF%A6%E9%AB%94+%EF%BC%88%E4%BE%8B%E5%A6%82+Google+++SPARQL+%E8%AA%9E%E6%B3%95%E5%BE%8C%EF%BC%8C%E6%89%BE%E5%87%BA%E7%AD%94%E6%A1%88..jpg",
            "https://slideplayer.com/slide/14145574/86/images/51/An+example+QALD+%E8%8B%A5%E5%85%88%E6%89%BE%E5%85%AC%E5%8F%B8%EF%BC%8C%E6%80%8E%E9%BA%BC%E6%89%BE%EF%BC%9F%E8%BD%89%E6%88%90+SPARQL+%28.jpg",
            "https://slideplayer.com/slide/14145574/86/images/52/An+example+QALD+%E7%8F%BE%E5%9C%A8%E6%89%80%E6%9C%89%E5%85%AC%E5%8F%B8%E9%83%BD%E6%89%BE%E5%88%B0%E4%BA%86%EF%BC%8C%E5%A6%82%E4%BD%95%E5%8F%AF%E4%BB%A5%E6%9F%A5%E5%88%B0%E4%BD%8D%E6%96%BC%E5%8A%A0%E5%B7%9E%E7%9A%84%E5%85%AC%E5%8F%B8%EF%BC%9F.jpg",
            "https://slideplayer.com/slide/14145574/86/images/53/An+example+QALD+%E8%A8%BB%E8%A8%98%EF%BC%9A+USA+%E6%9C%89%E7%94%A8%E5%97%8E%EF%BC%9F+%E8%A6%81%E6%80%8E%E9%BA%BC%E7%94%A8%EF%BC%9F.jpg",
            "https://slideplayer.com/slide/14145574/86/images/54/Exercises+More+exercises%3A.jpg",
            "https://slideplayer.com/slide/14145574/86/images/55/A+Typical+QALD+Architecture.jpg",
            "https://slideplayer.com/slide/14145574/86/images/56/Knowledge+bases+Knowledge+bases+such+as+DBpedia%2C+Wikidata%2C+YAGO+%E7%AD%89.jpg",
            "https://slideplayer.com/slide/14145574/86/images/57/QALD+%E7%9A%84%E6%9E%B6%E6%A7%8B+%E4%BD%BF%E7%94%A8%E8%80%85%E4%BB%A5%E8%87%AA%E7%84%B6%E8%AA%9E%E8%A8%80%E6%9F%A5%E8%A9%A2+%5B%E9%BB%91%E7%9B%92%E5%AD%90%5D+%E5%90%91+LOD+%E9%80%B2%E8%A1%8C%E6%9F%A5%E8%A9%A2%EF%BC%88ex.+SPARQL+or+SQL%EF%BC%89.jpg",
            "https://slideplayer.com/slide/14145574/86/images/58/%E9%BB%91%E7%9B%92%E5%AD%90%E7%9A%84%E6%9E%B6%E6%A7%8B+%E5%81%87%E8%A8%AD%EF%BC%9A%E4%BD%BF%E7%94%A8%E8%80%85%E8%A9%A2%E5%95%8F%E7%9A%84%E7%9F%A5%E8%AD%98%EF%BC%88%E6%88%96%E8%80%85%E8%B3%87%E6%96%99%EF%BC%89%E5%AD%98%E5%9C%A8%E6%96%BC%E7%9F%A5%E8%AD%98%E5%BA%AB%EF%BC%88%E6%88%96%E8%80%85%E8%B3%87%E6%96%99%E5%BA%AB%EF%BC%89%E4%B8%AD+%E9%81%B8%E5%87%BA%E9%87%8D%E8%A6%81%E7%9A%84%E5%AD%97%E5%BD%99+%E5%B0%8D%E6%87%89%EF%BC%88%E6%9F%A5%E8%A9%A2%EF%BC%89%E5%87%BA%E8%A9%B2%E5%AD%97%E5%BD%99%E5%AD%98%E5%9C%A8%E6%96%BC%E7%9F%A5%E8%AD%98%E5%BA%AB%E4%B8%AD%E7%9A%84%E5%AF%A6%E9%AB%94.jpg",
            "https://slideplayer.com/slide/14145574/86/images/59/Query+pre-processing+Natural+language+queries+are+processed+by+NLP+tools+such+as+Stanford+CoreNLP.+Part-of-Speech+tagging+%28POS%EF%BC%9B%E8%A9%9E%E6%80%A7%E6%A8%99%E8%A8%98%29.jpg",
            "https://slideplayer.com/slide/14145574/86/images/60/Query+pre-processing+Query%EF%BC%9AWhich+actors+were+born+in+Germany+Word+POS.jpg",
            "https://slideplayer.com/slide/14145574/86/images/61/Question+Type+Detection.jpg",
            "https://slideplayer.com/slide/14145574/86/images/62/Entity+mapping+%E5%B0%88%E6%9C%89%E5%90%8D%E8%A9%9E%E3%80%81%E5%90%8D%E8%A9%9E%E3%80%81%E5%8B%95%E8%A9%9E%E7%AD%89%E8%88%87+DBpedia+%E4%B8%8A%E7%9A%84%E5%AF%A6%E9%AB%94%EF%BC%88entities%EF%BC%89%E5%B0%8D%E6%87%89+%E4%BE%8B%E5%A6%82%EF%BC%9ACalifornia.jpg",
            "https://slideplayer.com/slide/14145574/86/images/63/Entity+mapping+%E5%9B%B0%E9%9B%A3%EF%BC%9A%E5%B0%88%E6%9C%89%E5%90%8D%E8%A9%9E%E3%80%81%E5%90%8D%E8%A9%9E%E3%80%81%E5%8B%95%E8%A9%9E%E7%AD%89%E8%88%87+DBpedia+%E4%B8%8A%E7%9A%84%E5%AF%A6%E9%AB%94%EF%BC%88entities%EF%BC%89%E5%B0%8D%E6%87%89.jpg",
            "https://slideplayer.com/slide/14145574/86/images/64/%E9%82%8F%E8%BC%AF%E6%9F%A5%E8%A9%A2%E5%BC%8F%E7%9A%84%E7%94%A2%E7%94%9F+%E5%90%8D%E7%A8%B1%E9%A0%97%E7%82%BA%E7%B4%8A%E4%BA%82%EF%BC%8C%E6%9C%89%E7%A8%B1%E4%B9%8B%E7%82%BA+Query+parsing%E3%80%81query+interpretation+%E7%AD%89.jpg",
            "https://slideplayer.com/slide/14145574/86/images/65/Querying+linked+data+graphs+using+semantic+relatedness%3A+A+vocabulary+independent+approach.jpg",
            "https://slideplayer.com/slide/14145574/86/images/66/An+Example+Query+From+which+university+did+the+wife+of+Barack+Obama+graduate.jpg",
            "https://slideplayer.com/slide/14145574/86/images/67/Query+processing+approach.jpg",
            "https://slideplayer.com/slide/14145574/86/images/68/Entity+search+The+query+mechanism+prioritizes+named+entities+as+pivots.+Select+one+name+entity+as+pivot..jpg",
            "https://slideplayer.com/slide/14145574/86/images/69/Query+parsing.jpg",
            "https://slideplayer.com/slide/14145574/86/images/70/Query+parsing+Rule+Definition+eliminate%28%E7%A7%BB%E9%99%A4%29.jpg",
            "https://slideplayer.com/slide/14145574/86/images/71/Query+parsing+Rule+Definition+replicate%28%E8%A4%87%E8%A3%BD%29+cc%3A+coordination%28%E9%80%A3%E6%8E%A5%E8%A9%9E%29.jpg",
            "https://slideplayer.com/slide/14145574/86/images/72/Query+parsing+dependency+tree+After.jpg",
            "https://slideplayer.com/slide/14145574/86/images/73/Semantic+relatedness+Barack+Obama+wife+graduate+university+1+%E2%80%A6%E2%80%A6.jpg",
            "https://slideplayer.com/slide/14145574/86/images/74/Semantic+relatedness+Barack+Obama+wife+graduate+university+3+1+2.jpg",
            "https://slideplayer.com/slide/14145574/86/images/75/Princeton_University.jpg",
            "https://slideplayer.com/slide/14145574/86/images/76/Problems+Queries+with+aggregation+and+conditional+operators+are+not+covered+in+the+scope+of+this+work..jpg",
            "https://slideplayer.com/slide/14145574/86/images/77/Challenges+Lexical+gap+Ambiguities+Multilingualism+Light+expressions.jpg",
            "https://slideplayer.com/slide/14145574/86/images/78/Lexical+Gap+Approaches.jpg",
            "https://slideplayer.com/slide/14145574/86/images/79/Lexical+Gap+String+normalization+and+similarity+function.jpg",
            "https://slideplayer.com/slide/14145574/86/images/80/Lexical+Gap+Automatic+query+expansion+%28using+lexical+database+such+as+Wordnet%29+%E5%8F%AF%E4%BB%A5%E8%A7%A3%E6%B1%BA+synonym%EF%BC%88%E5%90%8C%E7%BE%A9%E8%A9%9E%EF%BC%89%EF%BC%8C%E4%B8%A6%E5%88%A9%E7%94%A8+Wordnet+%E4%BE%86%E6%89%BE+hypernym%EF%BC%88%E4%B8%8A%E4%BD%8D%E8%A9%9E%EF%BC%89%E3%80%81hyponym%EF%BC%88%E4%B8%8B%E4%BD%8D%E8%A9%9E%EF%BC%89.jpg",
            "https://slideplayer.com/slide/14145574/86/images/81/Lexical+Gap+Pattern+libraries.jpg",
            "https://slideplayer.com/slide/14145574/86/images/82/Lexical+Gap+Pattern+libraries+PATTY+BOA+PARALEX.jpg",
            "https://slideplayer.com/slide/14145574/86/images/83/PATTY+A+Taxonomy+of+Relational+Patterns+with+Semantic+Types.jpg",
            "https://slideplayer.com/slide/14145574/86/images/84/Motivation+WordNet+is+limited+to+single+words.jpg",
            "https://slideplayer.com/slide/14145574/86/images/85/Goal+Aim+at+patterns+that+contain+semantic+types%2C+such+as+%3Csinger%3E+sings+%3Csong%3E.jpg",
            "https://slideplayer.com/slide/14145574/86/images/86/Challenges+The+number+of+possible+patterns+increases+exponentially+with+the+length+of+the+patterns..jpg",
            "https://slideplayer.com/slide/14145574/86/images/87/Challenges.jpg",
            "https://slideplayer.com/slide/14145574/86/images/88/Challenges+We+have+to+handle+pattern+sparseness+and+coincidental+matches..jpg",
            "https://slideplayer.com/slide/14145574/86/images/89/Challenges+Computing+mutual+subsumptions+on+a+large+set+of+patterns+may+be+prohibitively+slow..jpg",
            "https://slideplayer.com/slide/14145574/86/images/90/PATTY+Texture+patterns+extraction+SOL+patterns+extraction.jpg",
            "https://slideplayer.com/slide/14145574/86/images/91/Extract+textual+patterns.jpg",
            "https://slideplayer.com/slide/14145574/86/images/92/Extract+textual+patterns.jpg",
            "https://slideplayer.com/slide/14145574/86/images/93/Extract+textual+patterns.jpg",
            "https://slideplayer.com/slide/14145574/86/images/94/Extract+textual+patterns.jpg",
            "https://slideplayer.com/slide/14145574/86/images/95/Extract+textual+patterns.jpg",
            "https://slideplayer.com/slide/14145574/86/images/96/Syntactic+ontological+lexical+patterns+%28SOL+patterns%29.jpg",
            "https://slideplayer.com/slide/14145574/86/images/97/SOL+patterns+Example+pattern.jpg",
            "https://slideplayer.com/slide/14145574/86/images/98/SOL+patterns+The+type+signature+of+a+pattern+is+the+pair+of+the+entity+placeholders.+In+the+example%2C+the+type+signature+is+person+X+song..jpg",
            "https://slideplayer.com/slide/14145574/86/images/99/SOL+patterns+Pattern+B+is+syntactically+more+general+than+pattern+A+if+every+string+that+matches+A+also+matches+B..jpg",
            "https://slideplayer.com/slide/14145574/86/images/100/SOL+patterns+To+generate+SOL+patterns+from+the+textual+patterns%2C+we+decompose+the+textual+patterns+into+n-grams..jpg",
            "https://slideplayer.com/slide/14145574/86/images/101/SOL+patterns+A+SOL+pattern+contains+only+the+n-grams+that+appear+frequently+in+the+corpus+and+the+remaining+word+sequences+are+replaced+by+wildcards..jpg",
            "https://slideplayer.com/slide/14145574/86/images/102/SOL+patterns+To+find+the+frequent+n-grams+efficiently%2C+we+apply+the+technique+of+frequent+itemset+mining+%28Agrawal+1993%3B+Srikant+1996%29.jpg",
            "https://slideplayer.com/slide/14145574/86/images/103/SOL+patterns+For+a+given+pattern+p+with+type+signature+t1+X+t2+%28ex.+person+X+song%29+the+support+of+p+is+the+size+of+its+support+set..jpg",
            "https://slideplayer.com/slide/14145574/86/images/104/Syntactic+Pattern+Generalization.jpg",
            "https://slideplayer.com/slide/14145574/86/images/105/Syntactic+Pattern+Generalization.jpg",
            "https://slideplayer.com/slide/14145574/86/images/106/Semantic+Pattern+Generalization.jpg",
            "https://slideplayer.com/slide/14145574/86/images/107/Semantic+Pattern+Generalization.jpg",
            "https://slideplayer.com/slide/14145574/86/images/108/Taxonomy+Construction.jpg",
            "https://slideplayer.com/slide/14145574/86/images/109/Taxonomy+Construction.jpg",
            "https://slideplayer.com/slide/14145574/86/images/110/Prefix+tree.jpg",
            "https://slideplayer.com/slide/14145574/86/images/111/Prefix+tree+If+synsets+have+entity+pairs+in+common%2C+they+share+a+common+prefix..jpg",
            "https://slideplayer.com/slide/14145574/86/images/112/Prefix+tree+The+prefix-tree+of+support+sets+is+a+prefix-tree+augmented+with+synset+information+stored+at+the+nodes..jpg",
            "https://slideplayer.com/slide/14145574/86/images/113/Mining+Subsumptions+from+the+Prefix-Tree.jpg",
            "https://slideplayer.com/slide/14145574/86/images/114/DAG+Construction+Once+we+have+generated+subsumptions+between+relational+patterns%2C+there+might+be+cycles+in+the+graph+we+generate..jpg",
            "https://slideplayer.com/slide/14145574/86/images/115/DAG+Construction+We+use+a+greedy+algorithm+for+removing+cycles+and+eliminating+redundancy+in+the+subsumptions..jpg",
            "https://slideplayer.com/slide/14145574/86/images/116/Example+usages+of+PATTY.jpg",
            "https://slideplayer.com/slide/14145574/86/images/117/Example+usages+of+PATTY.jpg",
            "https://slideplayer.com/slide/14145574/86/images/118/Example+usages+of+PATTY.jpg",
            "https://slideplayer.com/slide/14145574/86/images/119/Example+usages+of+PATTY.jpg",
            "https://slideplayer.com/slide/14145574/86/images/120/Example+usages+of+PATTY.jpg",
            "https://slideplayer.com/slide/14145574/86/images/121/Challenges+Ambiguities.jpg",
            "https://slideplayer.com/slide/14145574/86/images/122/Ambiguity+%E5%8D%80%E5%88%86+Homonymy+vs.+Polysemy+Synonym+vs.+Taxonomic+relations.jpg",
            "https://slideplayer.com/slide/14145574/86/images/123/Ambiguity+Homonymy+%EF%BC%88%E5%90%8C%E5%BD%A2%E3%80%8C%E6%88%96%E9%9F%B3%E3%80%8D%E7%95%B0%E7%BE%A9%E8%A9%9E%EF%BC%89+Polysemy%EF%BC%88%E4%B8%80%E8%A9%9E%E5%A4%9A%E7%BE%A9%EF%BC%9B%E5%A4%9A%E7%BE%A9%E8%A9%9E%EF%BC%89.jpg",
            "https://slideplayer.com/slide/14145574/86/images/124/Ambiguity+Synonym+vs.+Taxonomic+relations+%EF%BC%88%E5%88%86%E9%A1%9E%E9%97%9C%E4%BF%82%EF%BC%89+Metonymy+%EF%BC%88%E8%BD%89%E5%96%BB%EF%BC%89.jpg",
            "https://slideplayer.com/slide/14145574/86/images/125/Ambiguity+%E8%8B%B1%E6%96%87%E7%9A%84%E6%AF%94%E5%96%BB%EF%BC%88+figure+of+speech%EF%BC%89%EF%BC%8C%E5%B0%B1%E6%9C%89+simile%E3%80%81+metaphor%E3%80%81+metonymy%E7%AD%89%E9%A1%9E%E5%88%A5%EF%BC%9A.jpg",
            "https://slideplayer.com/slide/14145574/86/images/126/Disambiguation+Approaches.jpg",
            "https://slideplayer.com/slide/14145574/86/images/127/Disambiguation+Corpus-based+methods.jpg",
            "https://slideplayer.com/slide/14145574/86/images/128/Disambiguation+Resource-based+methods.jpg",
            "https://slideplayer.com/slide/14145574/86/images/129/Disambiguation%3A+resource-based+methods.jpg",
            "https://slideplayer.com/slide/14145574/86/images/130/Disambiguation%3A+resource-based+methods.jpg",
            "https://slideplayer.com/slide/14145574/86/images/131/Disambiguation%3A+resource-based+methods.jpg",
            "https://slideplayer.com/slide/14145574/86/images/132/Disambiguation%3A+Answer+Type+Detection.jpg",
            "https://slideplayer.com/slide/14145574/86/images/133/Explicit+Semantic+Analysis.jpg",
            "https://slideplayer.com/slide/14145574/86/images/134/Background+How+related+are+cat+and+mouse+And+what+about+preparing+a+manuscript+and+writing+an+article.jpg",
            "https://slideplayer.com/slide/14145574/86/images/135/Idea+source%3A.jpg",
            "https://slideplayer.com/slide/14145574/86/images/136/Idea.jpg",
            "https://slideplayer.com/slide/14145574/86/images/137/TF-IDF+%E4%B8%80%E7%A8%AE%E7%94%A8%E6%96%BC%E8%B3%87%E8%A8%8A%E6%AA%A2%E7%B4%A2%E8%88%87%E6%96%87%E6%9C%AC%E6%8C%96%E6%8E%98%E7%9A%84%E5%B8%B8%E7%94%A8%E5%8A%A0%E6%AC%8A%E6%8A%80%E8%A1%93%E3%80%82.jpg",
            "https://slideplayer.com/slide/14145574/86/images/138/Idea+%EF%BC%8A+%E7%82%BA%E4%BA%86%E9%81%BF%E5%85%8D%E7%94%A2%E7%94%9F%E9%81%8E%E5%A4%9A%E7%9A%84+keywords%EF%BC%8C%E6%AF%8F%E4%B8%80%E5%80%8B+concept+%E5%8F%AF%E4%BB%A5%E9%81%B8%E6%93%87+Top+N+%E5%80%8B+keywords+cat.jpg",
            "https://slideplayer.com/slide/14145574/86/images/139/Implementation+Details.jpg",
            "https://slideplayer.com/14145574/86/images/slide_140.jpg",
            "https://slideplayer.com/slide/14145574/86/images/141/Semantic+Interpreter+2-2Building+a+Semantic+Interpreter.jpg",
            "https://slideplayer.com/slide/14145574/86/images/142/Inverted+matrix+Panthera+Concept2+Concept+ConceptN+cat+0.92.jpg",
            "https://slideplayer.com/slide/14145574/86/images/143/A+HMM-based+Approach+to+Question+Answering+against+Linked+Data.jpg",
            "https://slideplayer.com/slide/14145574/86/images/144/Architecture+HMM+initialization+stage.+HMM+modeling+stage.jpg",
            "https://slideplayer.com/slide/14145574/86/images/145/Architecture.jpg",
            "https://slideplayer.com/slide/14145574/86/images/146/Architecture.jpg",
            "https://slideplayer.com/slide/14145574/86/images/147/Challenges+Multilingualism.jpg",
            "https://slideplayer.com/slide/14145574/86/images/148/Challenges+Light+expressions.jpg",
            "https://slideplayer.com/slide/14145574/86/images/149/Challenges+Complex+queries.jpg",
            "https://slideplayer.com/slide/14145574/86/images/150/Complex+Queries+Examples.jpg",
            "https://slideplayer.com/slide/14145574/86/images/151/Complex+Queries.jpg",
            "https://slideplayer.com/slide/14145574/86/images/152/Complex+Queries+Nested+queries.jpg",
            "https://slideplayer.com/slide/14145574/86/images/153/Complex+Queries+Indirect+queries.jpg",
            "https://slideplayer.com/slide/14145574/86/images/154/Challenges+Distributional+Knowledge.jpg",
            "https://slideplayer.com/slide/14145574/86/images/155/Challenges%3A+Procedural%2C+Temporal+and+Spatial+Questions.jpg",
            "https://slideplayer.com/slide/14145574/86/images/156/Challenges%3A+Procedural%2C+Temporal+and+Spatial+Questions.jpg",
            "https://slideplayer.com/slide/14145574/86/images/157/Challenges%3A+Procedural%2C+Temporal+and+Spatial+Questions.jpg",
            "https://slideplayer.com/slide/14145574/86/images/158/Challenges+Templates..jpg",
            "https://slideplayer.com/slide/14145574/86/images/159/Challenges+Templates..jpg",
            "https://slideplayer.com/slide/14145574/86/images/160/Challenges+Templates..jpg",
            "https://slideplayer.com/slide/14145574/86/images/161/Challenges+Common+drawbacks+of+template-based+approaches%3A.jpg",
            "https://slideplayer.com/slide/14145574/86/images/162/Summary+of+Challenges.jpg",
            "https://slideplayer.com/slide/14145574/86/images/163/References.jpg",
            "https://slideplayer.com/15/4786125/big_thumb.jpg",
            "https://slideplayer.com/15/4791186/big_thumb.jpg",
            "https://slideplayer.com/15/4800868/big_thumb.jpg",
            "https://slideplayer.com/15/4805176/big_thumb.jpg",
            "https://slideplayer.com/15/4805969/big_thumb.jpg",
            "https://slideplayer.com/15/4808298/big_thumb.jpg",
            "https://slideplayer.com/15/4811121/big_thumb.jpg",
            "https://slideplayer.com/15/4812819/big_thumb.jpg",
            "https://slideplayer.com/15/4816700/big_thumb.jpg",
            "https://slideplayer.com/15/4836945/big_thumb.jpg",
            "https://slideplayer.com/15/4840537/big_thumb.jpg",
            "https://slideplayer.com/15/4853966/big_thumb.jpg",
            "https://slideplayer.com/15/4857874/big_thumb.jpg",
            "https://slideplayer.com/15/4858422/big_thumb.jpg",
            "https://slideplayer.com/16/4910065/big_thumb.jpg",
            "https://slideplayer.com/16/4911695/big_thumb.jpg",
            "https://slideplayer.com/16/4925264/big_thumb.jpg",
            "https://slideplayer.com/16/4937975/big_thumb.jpg",
            "https://slideplayer.com/16/4942853/big_thumb.jpg",
            "https://slideplayer.com/16/4947894/big_thumb.jpg"
        ],
        "movies": [],
        "keywords": [],
        "meta_keywords": [
            "Question Answering over Linked Data"
        ],
        "tags": null,
        "authors": [
            "Barack Obama",
            "Arnold Schwarzenegger",
            "Amy Winehouse",
            "Winehouse",
            "DBpedia",
            "Bill"
        ],
        "publish_date": null,
        "summary": "",
        "meta_description": "History Take search engines as an example: Which U.S. state has the highest income tax? For keyword search (like google, yahoo, etc.), only keywords such as 'state', 'income' and 'tax’ are used. (generally, referred as Information Retrieval; IR) Because text interfaces are",
        "meta_lang": "",
        "meta_favicon": "",
        "meta_site_name": "",
        "canonical_link": "https://slideplayer.com/slide/14145574/",
        "text": "Presentation on theme: \"Question Answering over Linked Data\"— Presentation transcript:\n\n1 Question Answering over Linked Data\n\nSpeaker: 呂瑞麟 國立中興大學資管系教授 URL:\n\n2 History Take search engines as an example:\n\nWhich U.S. state has the highest income tax? For keyword search (like google, yahoo, etc.), only keywords such as 'state', 'income' and 'tax’ are used. (generally, referred as Information Retrieval; IR) Because text interfaces are \"natural\" somehow, it is also regarded as a \"shallow\" natural language search. For natural language search (or query), the whole sentence will be used Source:\n\n3 Current Status Search engines  Knowledge engines (ex. Google Knowledge Graph)\n\n4 Google Typed links to: born, die, place, works, other related persons.\n\n5 Current Status Question Answering over textual data (unstructured documents or free text) Natural Language Interfaces to Databases Natural Language Search Interfaces over Structured Knowledge Question Answering over Linked Data 也稱之 Semantic Question Answering\n\n6 Current Status Question Answering over textual data (unstructured documents or free text) Use IR techniques to process large amounts of unstructured text and as such to locate the documents and paragraphs in which the answer might appear Disadvantages: Documents containing the answer could be easily missed if the answer is expressed in a form that does not match the way the query is formulated, or if the answer is unlikely to be available in one document but must be assembled by aggregating answers from multiple documents Campaigns include TREC, CLEF, and SEALS TREC 2015 and 2016 are based on Yahoo Answer site (QA Track; 2007年起停辦；有點重起爐灶的感覺)\n\n7 Current Status Natural Language Interfaces to Databases (NLIDB)\n\nTo be discussed Natural Language Search Interfaces over Structured Knowledge typically based on data that is by and large manually coded and homogeneous (e.g., True Knowledge; is now acquired by Amazon; not longer just a search engine). Evi gathers information for its database in two ways: importing it from \"credible\" external databases (which for them includes Wikipedia) and from user submission following a consistent format and detailed process for input\n\n8 Current Status Question Answering over Linked Data\n\nScaling QA approaches to the large amount of distributed interlinked data that is available nowadays on the web Automatically finding answers to questions among the publicly available structured sources on the web\n\n9 Examples Natural Language Search Interfaces over Structured Knowledge\n\nNatural Language Interfaces to Databases (NLIDB)\n\n10 Keyword-based Search (Semantic Search) over Linked Data\n\nKeyword-driven SPARQL Query Generation Leveraging Background Knowledge 出處：International Conference on Web Intelligence, 2011. 賴家慧整理\n\n11 Assumptions Natural language queries cannot always be converted into formal queries automatically This is due to the meaning of some of the query elements being either unknown, ambiguous, or implicit. For example: Which are the islands in Germany? The implicit meaning is islands are located in Germany\n\n12 Approach Based on a set of user-supplied keywords, we first compute a list of candidate IRIs for each of the keywords issued by the user. Restrict the set of valid IRIs to those which are related to each other via a link in the background knowledge. Use the filtered set of IRIs to generate SPARQL queries that aim to encompass the semantics of the query supplied by the user.\n\n13 Approach User supply keywords : island、 Germany\n\nAn IRI matching to a keyword is an anchor point Matching entities and keywords such that entities’ label contain ki as a substring or equal to ki of all entities in the knowledge base. This is carried out on all types of entities (i.e., classes, properties and instances). The process of selecting a relevant neighborhood for each of the anchor points is called induction (also known as relation discovery )\n\n14 Approach Ranking and Selecting Anchor Points\n\naims at excluding anchor points which are probably unrelated to any interpretation of the user keyword based on string similarity score and connectivity degree AP(Ki): the set of anchor points of Ki\n\n15 Approach Ranking and Selecting Anchor Points:\n\nString similarity score: measuring the normalized edit distance between urdfs:label and Ki. (ulabel;Ki) = 1 means that the two strings are equal.\n\n16 Approach Ranking and Selecting Anchor Points: connectivity degree(CD)\n\n17 Approach Ranking and Selecting Anchor Points:\n\nThe specificity S for each is finally calculated as follows: top-10 of the IRIs contained in APKi. 這個方式也可能用於解決 lexical gap 和 ambiguity\n\n18 Approach Another assumption based on observation (log of DBpedia public SPARQL endpoint: ftp://download.openlinksw.com/support/dbpedia；已經沒了)\n\n19 Approach\n\n20 An example 即 P7 and P8\n\n21 Approach 經過實驗，從 17 個變成以下建議的八個 patterns\n\n22 Query Generation SPARQL QUERY GENERATION\n\nFor the pair of IRIs dbr:Germany and dbo:Island, would generate the following two queries: 1) SELECT * WHERE { ?island a dbo:Island . ?island ?p dbr:Germany . } 2) SELECT * WHERE { dbr:Germany ?p ?island . SPARQL QUERY GENERATION\n\n23 Examples Natural Language Search Interfaces over Structured Knowledge\n\nNatural Language Interfaces to Databases (NLIDB)\n\n24 NLIDB: Why? Users are thus freed from major access requirements to (the Semantic Web): the mastery of a formal query language like SPARQL or SQL and It translates a natural language request by an untrained user into (database) query knowledge about the specific vocabularies of the knowledge base they want to query\n\n25 NLIDB: Challenges?! NLIDB is still an AI-complete problem.\n\nIn the field of artificial intelligence, the most difficult problems are informally known as AI-complete or AI-hard, implying that the difficulty of these computational problems is equivalent to that of solving the central artificial intelligence problem—making computers as intelligent as people, or strong AI. AI-complete problems are hypothesised to include computer vision, natural language understanding, and dealing with unexpected circumstances while solving any real world problem\n\n26 NLIDB: Challenges?! Categorized into three problems:\n\nUsers are required having expertise domain knowledge without knowing the framework of database and database query language Does it mean the challenges become greater if it is open domain knowledge? Categorized into three problems: Natural language query understanding, keyword-based searching, and query answering.\n\n27 NLIDB: Challenges?! Most of the NLIDB systems are dealing with English language and are domain independent. So, to make these systems portable from one domain to another is a challenging task. Due to ambiguity in natural language query i.e., having multiple meanings, the system's output may be different than the desired one. Semantic analysis of a sentence is still very complex due to polysemy (岐異字，一詞多義)\n\n28 NLIDB: Challenges?! Most of the systems are concerned only with the noun and verb irrespective of the auxiliary words in a sentence, leading to misinterpretation of semantics. Another important issue is word economy; i.e., the user might not formulate the query using required number of words. 例如：七星公園，和平公園\n\n29 Categories of NLIDB Domain Specific: four architectures\n\nPattern matching Syntax based Systems Semantic Grammar Systems Intermediate Representation language Domain Independent\n\n30 Pattern Matching Patterns are defined using some manually defined rules. The natural language query is mapped to these defined rules. Having the disadvantage of simplicity\n\n31 Syntax based Systems The natural language query is parsed syntactically and then mapping the syntax tree into the structure of any formal query language Syntax:語法 Grammar:文法 Lexicon:詞庫\n\n32 Semantic Grammar Systems\n\nSemantic grammar contains hard wired knowledge and is best suited for domain specific systems\n\n33 Semantic Grammar a lexicon: stores all the possible words (ie. single word) that the grammar is aware of ex. (customer -> customer patron member) On the left-hand side of the ‘->’ the word ‘customer’ defines a symbol that can be used in the grammar. When ‘customer’ is used in the grammar, it refers to the English words on the right-hand side of the ‘->’, that is ‘customer’, ‘patron’ or ‘member’. ex. “taken out” cannot be in a lexicon Source:\n\n34 Semantic Grammar rules: combine the terminal symbols in the lexicon to form phrases or sentences in a specific way ex. (TAPE_CUSTOMER -> RENTED by customer ATT_NUMBER) rules are in upper case. 3 rules in the example. ATT_NUMBER refers to a number attribute RENTED represents another rule for synonyms of the verb ‘rented’, such as taken out, borrowed etc. TAPE_CUSTOMER is the non-terminal representation of the phrase. examples are “borrowed by member number 22”, or “taken out by customer 14” etc. Source:\n\n35 Intermediate Representation Language\n\nThese systems parsed the language query syntactically, and then the syntax tree is translated by the semantic analyzer into mediate query.\n\n36 Domain Independent NLIDB applications fail mostly dealing with indirect and complex requests. Domain independent applications use techniques like Ontology. Ontology explicitly represents taxonomy of a class. It allows abstracting information and representing it, explicitly highlighting the concepts and relationships and not the words used to describe them. Ontology is considered as a part of semantic approach that aims to use meta data in order to answer queries.\n\n37 NLIDB Limitations Databases are not interoperable and distributed over the web\n\n38 QALD Question Answering over Linked Data\n\nUses Linked Open Data instead of databases Creates linked types for (un-)structured documents and databases (Big Data)? a series of evaluation campaigns on question answering over linked data. So far, it has been organized as an ESWC workshop and as part of the Question Answering lab at CLEF.\n\n39 QALD QALD 是目前最知名的 Linked Data 問答系統應用評估大會\n\n第一屆 QALD 活動於 2011 年和第 8 屆Extended Semantic Web Conference (ESWC) 在希臘同時舉行 至今年為止已舉辦至第 8 屆 (QALD-8)，目前由Cross Language Evaluation Forum (CLEF)協會協辦 主要的會議名稱是 ISWC (International Semantic Web Conference)\n\n40 QALD QALD-7 的問題集分成四類： Multilingual question answering over DBpedia、\n\nQuestions are provided in 8 different languages (English, German, Spanish, Italian, French, Dutch, Romanian, and Farsi) Hybrid question answering For QALD-7, questions are provided in English and can be answered only by integrating structured data (RDF) and unstructured data (free text available in the DBpedia abstracts)\n\n41 QALD QALD-7 的問題集分成四類： 題目的數量也逐年成長，從 QALD-1 的 50題到 QALD-6 後已經有 350 題\n\nLarge-Scale Question answering over RDF Be able to scale up to a big data volume, handle a vast amount of questions and speed up the question answering process by parallelization English question answering over Wikidata include 100 open-domain factual questions compiled from the previous iteration of the Multiligual set. Factual question: A question that aims to collect information about things for which there is a correct answer. 題目的數量也逐年成長，從 QALD-1 的 50題到 QALD-6 後已經有 350 題\n\n42 QALD 大會指出的現況 Only few systems yet address the fact that the structured data available nowadays is distributed among a large collection of interconnected datasets, and that answers to questions can often only be provided if information from several sources are combined. A lot of information is still available only in textual form, both on the web and in the form of labels and abstracts in linked data sources. Therefore, approaches are needed that can not only deal with the specific character of structured data but also with finding information in several sources, processing both structured and unstructured information, and combining such gathered information into one answer.\n\n43 QALD 使用的資料集 DBpedia MusicBrainz (QALD-3 以前？)\n\nThe LinkedBrainz project is intended to help MusicBrainz publish its database as Linked Data. Since QALD-2, it relies mainly on the Music Ontology.\n\n44 QALD 題目的設計 Test question set for DBpedia\n\nA few out-of-scope questions were added to each question set questions to which the datasets do not contain the answer, in order to test the ability of participating systems to judge whether a failure to provide an answer lies in the dataset or the system itself. A small set of questions that could only be answered by combining information from DBpedia and free text, thus testing a system’s ability to combine several linked information sources when searching for an answer.\n\n45 QALD Evaluation With respect to a single question q,\n\nrecall is defined as the ratio of the number of correct answers provided by the system to the number of gold standard answers (大會提供的), and precision is defined as the ratio of the number of correct answers provided by the system to the number of all answers provided by the system\n\n46 QALD Evaluation\n\n47 Which companies are located in California, USA?\n\nAn example QALD Which companies are located in California, USA?\n\n48 An example QALD Which companies are located in California, USA?\n\n人的處理方式：在知識庫中找出所有的公司，然後確認其位於美國加州（如果有索引的話） 電腦的處理方式（人工智慧？）： 從問句中找出每個字的語法（例如 companies 是名詞，located 是動詞，California 和 USA 是專有名詞等） 從問句中找出每個字的語意（例如 companies 是公司的類別；問句型態是 which companies 等）「why and how?」 找出知識庫中對應公司類別的實體 （例如 Google） 從找出的實體中，確認其位於（利用 located）美國加州\n\n49 An example QALD 找出知識庫中對應的實體 （例如 Google\n\n50 An example QALD 根據某種規則（evaluation order）實體 （例如 Google SPARQL 語法後，找出答案 例如： right-to-left ，先找位於美國的加州？可是 … 例如： left-to-right ，先找公司。但是怎麼找？\n\n51 An example QALD 若先找公司，怎麼找？轉成 SPARQL (https://dbpedia.org/sparql)\n\nselect distinct ?s where { ?s rdf:type dbo:Company . } 但是我怎麼知道是 dbo:Company？ 我們只有 companies\n\n52 An example QALD 現在所有公司都找到了，如何可以查到位於加州的公司？\n\nselect distinct ?s where { ?s rdf:type dbo:Company . ?s dbo:locationCity dbr:California . } 可是，我怎麼知道是 dbo:locationCity 和 dbr:California？而且知道前一個是 predicate，後一個是 object？\n\n53 An example QALD 註記： USA 有用嗎？ 要怎麼用？\n\n54 Exercises More exercises:\n\nWhich universities are located in California, USA? Which universities are located in Taichung? How many universities are located in Taichung?\n\n55 A Typical QALD Architecture\n\n56 Knowledge bases Knowledge bases such as DBpedia, Wikidata, YAGO 等\n\n如何生成知識庫？如何將資料庫轉成 Linked Data？ 可以參考 DBpedia/YAGO 等的建立方式 目前不在我們的研究範圍\n\n57 QALD 的架構 使用者以自然語言查詢 [黑盒子] 向 LOD 進行查詢（ex. SPARQL or SQL）\n\n58 黑盒子的架構 假設：使用者詢問的知識（或者資料）存在於知識庫（或者資料庫）中 選出重要的字彙 對應（查詢）出該字彙存在於知識庫中的實體\n\n根據實體的評估順序，轉換成一個（或者一組）查詢語言\n\n59 Query pre-processing Natural language queries are processed by NLP tools such as Stanford CoreNLP. Part-of-Speech tagging (POS；詞性標記) Word (字詞)、專有名詞 (proper noun) Lemma (詞目) the form usually found in dictionaries 例如，running 和 ran 的 lemma 都是 run\n\n60 Query pre-processing Query：Which actors were born in Germany? Word POS\n\nLemma NER Which WDT O Actors NNS Actor Were VBD Be Born VBN born In IN in Germany NNP LOCATION ? . NNP: proper noun NN: noun WDT: Wh-determiner (What and Which are tagged as Wh-determiner (WDT) when NOT acting as the head of a wh- noun phrase, otherwise (at the head) they are tagged as Wh-pronoun (WP)) VBD: verb; past tense VBN: verb; past participle (participle is a form of a verb that is used in a sentence to modify a noun, noun phrase, verb, or verb phrase) （過去分詞） IN: preposition （介系詞） Source:\n\n61 Question Type Detection\n\n問句的常見句型： When：限制找出答案的資料型態為時間日期 Who：限制找出答案是人或者組織 Is：限制找出答案為是或者否 What, Which, and Give：大多跟其問句後第一個出現的名詞相關 How many：出現 entities 數的總和或者答案的 property 值 ex. how many universities or how many pages\n\n62 Entity mapping 專有名詞、名詞、動詞等與 DBpedia 上的實體（entities）對應 例如：California\n\nselect distinct ?s where { ?s rdfs:label . } 但是查詢結果包含 Category，可以加上 FILTER NOT EXISTS { ?s rdf:type skos:Concept . }\n\n63 Entity mapping 困難：專有名詞、名詞、動詞等與 DBpedia 上的實體（entities）對應\n\ncompanies 和 dbo:Company located 和 dbo:locationCity (或者選擇其他更好的 predicate；例如找尋 predicate 的值是 location 的。當作練習題！） 之後，還有其他說明\n\n64 邏輯查詢式的產生 名稱頗為紊亂，有稱之為 Query parsing、query interpretation 等\n\n其實，我比較喜歡稱它 evaluation order 作法蠻多的，我們舉ㄧ種說明\n\n65 Querying linked data graphs using semantic relatedness: A vocabulary independent approach\n\n出處：Data & Knowledge Engineering (Nov. 2013), pp. 126–141 賴家慧整理\n\n66 An Example Query From which university did the wife of Barack Obama graduate?\n\n67 Query processing approach\n\nEntity recognition (跟之前的做法相同) and entity search Query parsing Semantic relatedness\n\n68 Entity search The query mechanism prioritizes named entities as pivots. Select one name entity as pivot If the query has more than one entity, both entity candidates are sent to the entity search engine the entity cardinality (number of properties connected to the entity) is used to determine the final pivot entities.\n\n69 Query parsing Build partial ordered dependency structure(PODS) by taking as inputs both Stanford dependencies and the detected named entities/pivots and by applying a set of operations over the original Stanford dependencies. Rule Definition merge(結合) nn: noun compound modifier(compound) “Oil price futures” nn(futures, price) advmod: adverb modifier “less often” advmod(often, less) amod: adjectival modifier “Sam eats red meat” amod(meat, red)\n\n70 Query parsing Rule Definition eliminate(移除)\n\nadvcl: adverbial clause modifier “If you know who did it, you should tell the teacher” advcl(tell, know) aux: auxiliary(輔助) \"He should leave.\" aux(leave,should) auxpass: passive auxiliary(被動式輔助) “Kennedy has been killed” auxpass(killed, been) aux(killed,has) ccomp: clausal complement(子句補語) “He says that you like to swim” ccomp(says, like) complm : case ? det: determiner(限定詞) “Which book do you prefer?” det(book, which)\n\n71 Query parsing Rule Definition replicate(複製) cc: coordination(連接詞)\n\n“Bill is big and honest” cc(big, and) conj: conjunct(結合) “Bill is big and honest” conj(big, honest) preconj: preconjunct “Both the boys and the girls are here” preconj(boys, both)\n\n72 Query parsing dependency tree After\n\nmerge: {nn, advmod, amod} eliminate: {advcl, aux, auxpass, ccomp, complm, det} replicate: {cc, conj, preconj} Query parsing dependency tree After case graduate Obama wife did university From which of the Barack nmod aux nsubj det compound(nn) graduate Barack Obama wife university nmod nsubj PODS : Barack Obama → wife → graduate → university\n\n73 Semantic relatedness Barack Obama wife graduate university 1 ……\n\nExplicit Semantic Analysis (ESA) EX: r(wife,spouse)、 r(wife,religion) dbr :Michelle_Obama Honolulu, Hawaii, U.S. dbr:Protestantism …… dbp:spouse dbp:birthPlace dbp:religion 1 dbr: Barack Obama\n\n74 Semantic relatedness Barack Obama wife graduate university 3 1 2\n\ndbo:University Semantic relatedness dbr: Barack Obama dbr: Michelle_Obama dbp:spouse dbo:EducationalInstitution\n\n75 Princeton_University\n\nSemantic relatedness Barack Obama wife graduate university 4 6 3 1 dbo: University 2 5 Semantic relatedness dbr: Princeton_University Harvard_Law_School dbp:almaMater rdf:type dbr: Barack Obama dbr: Michelle_Obama dbp:spouse\n\n76 Problems Queries with aggregation and conditional operators are not covered in the scope of this work. aggregation(比較): Who is the youngest player in the Premier League? conditional(條件): Which presidents were born in 1945? 還有選擇 pivot 以及沒有 name entity 的情形\n\n77 Challenges Lexical gap Ambiguities Multilingualism Light expressions\n\nComplex queries Distributional Knowledge Procedural, Temporal and Spatial Questions Templates Source:\n\n78 Lexical Gap Approaches\n\nThe gap between the vocabulary of the user and that of the ontology cannot always be bridged by the use of string distance metrics or generic dictionaries such as WordNet 例如之前的 located 和 locationCity Approaches String normalization and similarity function Automatic query expansion Pattern libraries Compositional Approaches: mixture of the above.\n\n79 Lexical Gap String normalization and similarity function\n\nmatch different forms of the same word (but not including synonym) Normalization: ran and running  run Similarity functions: edit distance (such as Jaro-Winkler), connective degree, etc. 可以看一下 Similarity scores generally need to be calculated between a phrase and every entity label, which is infeasible on large knowledge bases\n\n80 Lexical Gap Automatic query expansion (using lexical database such as Wordnet) 可以解決 synonym（同義詞），並利用 Wordnet 來找 hypernym（上位詞）、hyponym（下位詞） Allow for more matches and thus increase recall but lead to mismatches between related words and thus can decrease the precision\n\n81 Lexical Gap Pattern libraries\n\nIn general, properties require more than AQE because of complex linguistic patterns Ex. “X wrote Y” and “Y is written by X” Ex. “X wrote Y together with Z” for “X is a coauthor of Y”. Reasoning or inference on existing patterns One approach: entailment (蘊含；Yahoo知識+) Ex. “if X writes a book, X is called the author of it.” (雙向的） Ex. 媽媽發燒了→ 媽媽生病了 (單向的，反向不行）\n\n82 Lexical Gap Pattern libraries PATTY BOA PARALEX\n\nFor each property in the knowledge base, sentences from a corpus are chosen containing examples of subjects and objects for this particular property. PARALEX\n\n83 PATTY A Taxonomy of Relational Patterns with Semantic Types\n\nNdapandula Nakashole、Gerhard Weikum、Fabian Suchanek Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning 歐庭銨整理\n\n84 Motivation WordNet is limited to single words\n\nNot phrases or patterns Patterns can be synonymous, and they can subsume each other E.g. “X is romantically involved with Y” and “X is dating Y” Ex. Both are subsumed by “X knows Y”\n\n85 Goal Aim at patterns that contain semantic types, such as <singer> sings <song> sings her <song> and sings his <song>, into a more general pattern sings [prp] <song>\n\n86 Challenges The number of possible patterns increases exponentially with the length of the patterns. Ex., the string “Amy sings ‘Rehab”’ can give rise to the patterns <singer> sings <song>, <person> sings <artifact>, <person> [vbz] <entity>, etc. If wildcards for multiple words are allowed (such as in <person> sings * <song>), the number of possible patterns explodes.\n\n87 Challenges A pattern can be semantically more general than another pattern (when one relation is implied by the other relation), and it can also be syntactically more general than another pattern (by the use of placeholders such as [vbz]). These two subsumption orders have a non-obvious interplay, and none can be analyzed without the other\n\n88 Challenges We have to handle pattern sparseness and coincidental matches. If the corpus is small, e.g., the patterns <singer> later disliked her song <song> and <singer> sang <song>, may apply to the same set of entity pairs in the corpus. Still, the patterns are not synonymous.\n\n89 Challenges Computing mutual subsumptions on a large set of patterns may be prohibitively slow. Moreover, due to noise and vague semantics, patterns may even not form a crisp taxonomy, but require a hierarchy in which subsumption relations have to be weighted by statistical confidence measures.\n\n90 PATTY Texture patterns extraction SOL patterns extraction\n\nSyntactic Pattern Generalization Semantic Pattern Generalization Taxonomy Construction\n\n91 Extract textual patterns\n\nFirst apply the Stanford Parser Ex. Winehouse effortlessly performed her song Rehab. nsubj(performed-3, Winehouse-1) advmod(performed-3, effortlessly-2) poss(Rehab-6, her-4) nn(Rehab-6, song-5) dobj(performed-3, Rehab-6)\n\n92 Extract textual patterns\n\nEx. Winehouse effortlessly performed her song Rehab.\n\n93 Extract textual patterns\n\nWhenever two named entities appear in the same sentence, we extract a textual pattern Detection of named entities in the parsed corpus (YAGO2 knowledge base). In this example, the entity detection yields the entities Amy Winehouse and Rehab (song). We match noun phrases that contain at least one proper noun against the dictionary. For disambiguation, we use a simple context-similarity prior, as described in (Suchanek 2009). We empirically found that this technique has accuracy well above 80%\n\n94 Extract textual patterns\n\nFind the shortest path that connects the two entities. only consider shortest paths that start with subject-like dependencies, such as nsubj, rcmod and partmod. Path → Winehouse nsubj performed dobj Rehab\n\n95 Extract textual patterns\n\nTo reflect the full meaning of the patterns, we expand the shortest path with adverbial and adjectival modifiers, for example the advmod dependency. The sequence of words on the expanded shortest path becomes our final textual pattern. Ex., the textual pattern: Amy Winehouse effortlessly performed Rehab (song). note: Rehab (song) 似乎不存在於 DBpedia\n\n96 Syntactic ontological lexical patterns (SOL patterns)\n\ntextual patterns  SOL patterns. A SOL pattern is an abstraction of a textual pattern that connects two entities of interest It is a sequence of words, POS-tags, wildcards, and ontological types The special POStag [word] stands for any word of any POS class. A wildcard, denoted *, stands for any (possibly empty) sequence of words. An ontological type is a semantic class name (such as <singer>) that stands for an instance of that class Every pattern contains at least two types 1138\n\n97 SOL patterns Example pattern\n\n<person>’s [adj] voice * <song> matches Amy Winehouse’s soft voice in ‘Rehab’ Elvis Presley’s solid voice in his song ‘All shook up’ 1138\n\n98 SOL patterns The type signature of a pattern is the pair of the entity placeholders. In the example, the type signature is person X song. The support set of a pattern is the set of pairs of entities that appear in the place of the entity placeholders in all strings in the corpus that match the pattern. In the example, the support set of the pattern could be {(Amy,Rehab), (Elvis, AllShookUp)}. Each pair is called a support pair of the pattern. 1138\n\n99 SOL patterns Pattern B is syntactically more general than pattern A if every string that matches A also matches B. Ex. Sing  [VB] Pattern B is semantically more general than A if the support set of B is a superset of the support set of A. Ex. <singer>  <person> If A is semantically more general than B and B is semantically more general than A, the patterns are called synonymous. A set of synonymous patterns is called a pattern synset. Two patterns, of which neither is semantically more general than the other, are called semantically different. 1138\n\n100 SOL patterns To generate SOL patterns from the textual patterns, we decompose the textual patterns into n-grams n-grams allow us to break down a sentence into wildcard-separated subsequences, which yields an SOL pattern.\n\n101 SOL patterns A SOL pattern contains only the n-grams that appear frequently in the corpus and the remaining word sequences are replaced by wildcards. Ex. the sentence “was the first female to run for the governor of” might give rise to the pattern * the first female * governor of, if “the first female” and “governor of” are frequent in the corpus 1138\n\n102 SOL patterns To find the frequent n-grams efficiently, we apply the technique of frequent itemset mining (Agrawal 1993; Srikant 1996) each sentence is viewed as a “shopping transaction” with a “purchase” of several n-grams, and the mining algorithm computes the n-gram combinations with large co-occurrence support\n\n103 SOL patterns For a given pattern p with type signature t1 X t2 (ex. person X song) the support of p is the size of its support set. the confidence of p as the ratio of the support-set sizes of p and pu (an untyped variant pu of p, in which the types t1 and t2 are replaced by the generic type entity [類似 Java 的 java.lang.Object].)\n\n104 Syntactic Pattern Generalization\n\nAlmost every (SOL) pattern can be generalized into a syntactically more general pattern in several ways by replacing words by POS-tags by introducing wildcards (combining more n-grams) by generalizing the types in the pattern. It is not obvious which generalizations will be reasonable and useful\n\n105 Syntactic Pattern Generalization\n\nIf a generalization subsumes multiple patterns with disjoint support sets, we abandon the generalized pattern. For example, <person> [vb] <person> subsumes the two semantically different patterns →<person> loves <person>&<person> hates <person>\n\n106 Semantic Pattern Generalization\n\nOnly subsumption is considered support sets were used for determination of subsumption. Because support sets may contain spurious pairs or be incomplete, a notion of a soft set inclusion, in which one set S can be a subset of another set B to a certain degree, was designed deg(S ⊆ B) = |S ∩ B|/|S|. 1139 compute the Wilson score to estimate the ratio of elements of S’ that are in another set B\n\n107 Semantic Pattern Generalization\n\nHowever, the sample (or support) may be too small to get an appropriate deg(S ⊆ B) value Therefore, the Wilson score interval was used （類似常態分配的有效範圍？） in general, deg(S ⊆ B) = c If sample size is too small, deg(S ⊆ B) = c – d c– d is the lower bound of the interval In file wikipedia-subsumptions.txt, the confidence value is actually deg(S ⊆ B). 1139 compute the Wilson score to estimate the ratio of elements of S’ that are in another set B\n\n108 Taxonomy Construction\n\nFinally, arrange the patterns (SOL? semantic patterns?) in a semantic taxonomy. 感覺上，和前一步驟（Semantic Pattern Generalization）應該屬於同一個步驟 naive approach: compare every pattern support set to every other pattern support set in order to determine inclusion, mutual inclusion, or independence. too slow <Politician> was governor of <State>是pattern A.80 A代表support set →Arnold Schwarzenegger, California 80代表出現頻率 where nodes are entity pairs\n\n109 Taxonomy Construction\n\nConstruct a prefix tree Mining Subsumptions from the Prefix-Tree Remove cycles and eliminating redundancy <Politician> was governor of <State>是pattern A.80 A代表support set →Arnold Schwarzenegger, California 80代表出現頻率 where nodes are entity pairs\n\n110 Prefix tree For example, in the support set for the pattern <Politican> was governor of <State>, the entry A,80 may denote the entity pair Arnold Schwarzenegger, California, with an occurrence frequency 80. <Politician> was governor of <State>是pattern A.80 A代表support set →Arnold Schwarzenegger, California 80代表出現頻率 where nodes are entity pairs\n\n111 Prefix tree If synsets have entity pairs in common, they share a common prefix. To increase the chance of shared prefixes, entity pairs are inserted into the tree in decreasing order of occurrence frequency <Politician> was governor of <State>是pattern A.80 A代表support set →Arnold Schwarzenegger, California 80代表出現頻率 where nodes are entity pairs\n\n112 Prefix tree The prefix-tree of support sets is a prefix-tree augmented with synset information stored at the nodes. Each node (entity pair) stores the identifiers of the pattern sysnets whose support sets contain that entity pair. Each node also stores a link to the next node with the same entity pair (ex. F and G in P3 and P4) <Politician> was governor of <State>是pattern A.80 A代表support set →Arnold Schwarzenegger, California 80代表出現頻率 where nodes are entity pairs\n\n113 Mining Subsumptions from the Prefix-Tree\n\nBy traversing the entire path of a synset Pi, we can reach all the pattern synsets sharing common nodes with Pi. start traversing the tree bottom up, starting at the last node in Pi’s support set, we can determine exactly which paths are subsumed by Pi. Traversing for all patterns gives us the sizes of the support set intersection. The determined intersection sizes can then be used in the Wilson estimator to determine the degree of semantic subsumption and semantic equivalence of patterns.\n\n114 DAG Construction Once we have generated subsumptions between relational patterns, there might be cycles in the graph we generate. We ideally want to remove the minimal total number of subsumptions whose removal results in an a directed acyclic graph (DAG). called the minimum feedback-arc-set problem a well known NP-hard problem\n\n115 DAG Construction We use a greedy algorithm for removing cycles and eliminating redundancy in the subsumptions Greedy algorithm: Starting with a list of subsumption edges ordered by decreasing weights we construct the DAG bottom up by adding the highest-weight subsumption edge. a subsumption is added to the DAG only if it does not introduce cycles or redundancy This process finally yields a DAG of pattern synsets – the PATTY taxonomy.\n\n116 Example usages of PATTY\n\nfile: dbpedia-relation-paraphrases.txt Which companies are located in California, USA?\n\n117 Example usages of PATTY\n\nfile: wikipedia-pattern.txt (SOL pattern?) 格式： pattern_id pattern_text confidence domain range\n\n118 Example usages of PATTY\n\nfile: wikipedia-instances.txt 格式： pattern_id arg1 arg2\n\n119 Example usages of PATTY\n\nfile: wikipedia-subsumptions.txt\n\n120 Example usages of PATTY\n\npatterns and subsumptions an example subsumption:\n\n121 Challenges Ambiguities\n\nAmbiguity is the phenomenon of the same phrase having different meanings structural and syntactic (like “flying planes can be dangerous.”) or lexical and semantic (like “bank”) Structural: Either “to fly planes” or “planes, which fly” Semantic: planes can also refer “a power tool for smoothing or shaping wood”. (刨平) Ambiguity is the flipside of the lexical gap negatively effects its precision Source:\n\n122 Ambiguity 區分 Homonymy vs. Polysemy Synonym vs. Taxonomic relations\n\nSource:\n\n123 Ambiguity Homonymy （同形「或音」異義詞） Polysemy（一詞多義；多義詞）\n\na word that sounds the same (ex. No and Know) or is spelled the same (ex. such as bear the animal, and the verb to bear) as another word but has a different meaning Polysemy（一詞多義；多義詞） A polysemous word has more than one meaning (ex. “right”:正確的、合適的) (ex. “right”:權利、右邊？) 必須 related sense (or concepts) (as in bank as a financial institution vs. bank as a building where a financial institution offers services) Source:\n\n124 Ambiguity Synonym vs. Taxonomic relations （分類關係） Metonymy （轉喻）\n\nthe act of referring to something using a word that describes one of its qualities or features （見下頁說明） Hypernymy （上位詞） Source:\n\n125 Ambiguity 英文的比喻（ figure of speech），就有 simile、 metaphor、 metonymy等類別：\n\nsimile一般譯做「明喻」，用 like、 as等以「好像」的字帶出比喻； She ran like an antelope（她跑得像羚羊一樣快） metaphor一般譯做「暗喻」或「隱喻」 ，不用 like、 as等字而比喻自見； The curtain of night was falling（夜幕漸漸下垂） metonymy一般譯做「轉喻」，則是以一件東西象徵某人或某事物， The hammer and sickle no longer commands anybody's loyalty（鎚子和鐮刀已經沒有人信奉了） 用鎚子和鐮刀象徵共產主義 Source:\n\n126 Disambiguation Approaches\n\nDisambiguation is the process of selecting one of multiple candidate concepts for an ambiguous phrase. Corpus-based methods Resource-based methods A different way to restrict the set of answer candidates and thus handle ambiguity is to determine the expected answer type of a factual question (called Answer Type Detection)\n\n127 Disambiguation Corpus-based methods\n\nused and rely on counts, often used as probabilities, from unstructured text corpora Distributional hypothesis: “difference of meaning correlates with difference of [contextual] distribution” Common context features used are word co-occurrences, such as left or right neighbors, but also synonyms, hyponyms, POS-tags and the parse tree structure. More elaborate approaches also take advantage of the context outside of the question, such as past queries of the user\n\n128 Disambiguation Resource-based methods\n\nBased on a fact: the candidate concepts are RDF resources Resources are then compared using different scoring schemes based of their properties and the connections between them. The higher the score, the better its corresponding resource\n\n129 Disambiguation: resource-based methods\n\nBased on observations RVT used Hidden Markov Model CASIA used Markov Logic Network Underspecification discards certain combinations of possible meanings before the time consuming querying step, by combining restrictions for each meaning each term is mapped to DUDE to be studied\n\n130 Disambiguation: resource-based methods\n\nEntity recognition and disambiguation using Wikipedia-based semantic relatedness Treo EasyESA (Explicit Semantic Analysis) gAnswer The number of connections between the RDF fragments of the resource candidates is then used to score and select them. 其他 …\n\n131 Disambiguation: resource-based methods\n\nSome approaches let the user clarify the exact intent, either in all cases or only for ambiguous phrases SQUALL CrowdQ FREyA\n\n132 Disambiguation: Answer Type Detection\n\nuses type coercion (類似 implicit type casting) it is coerced into other types by calculating the probability of an entity of class A to also be in class B Type-and-Generate (TaG): requires manual analysis of a domain Generate-and-Type (GaT): for example, TyCor. It is shown that GaT outperformed TaG.\n\n133 Explicit Semantic Analysis\n\nJournal of Artiﬁcial Intelligence Research 34 (2009)\n\n134 Background How related are “cat” and “mouse”? And what about “preparing a manuscript” and “writing an article”? Humans interpret the specific wording of a document in the much larger context of their background knowledge and experience. It has long been recognized that in order to process natural language, computers require access to vast amounts of common-sense and domain-specific world knowledge Thus, Wikipedia\n\n135 Idea source:\n\n136 Idea We represent texts as a weighted mixture of a predetermined set of natural concepts (called interpretation vectors), which are defined by humans themselves and can be easily explained a concept: a Wikipedia article weight: TF-IDF\n\n137 TF-IDF 一種用於資訊檢索與文本挖掘的常用加權技術。\n\n字詞的重要性隨著它在文件中出現的次數成正比增加，但同時會隨著它在語料庫中出現的頻率成反比下降。 TF : n(i,j)是該詞在文件dj中的出現次數，而分母則是在文件dj中所有字詞的出現次數之和。 IDF: |D|語料庫中的文件總數，包含詞語ti的文件數目(即n(i,j)≠０的文件數目)\n\n138 Idea ＊ 為了避免產生過多的 keywords，每一個 concept 可以選擇 Top N 個 keywords cat\n\nleopard Roar keyword4 .... KeywordM Panthera 0.92 0.84 0.77 xxx Concept2 0.24 Concept3 ... ConceptN ＊ 為了避免產生過多的 keywords，每一個 concept 可以選擇 Top N 個 keywords\n\n139 Implementation Details\n\nDiscard articles that have fewer than 100 non-stop words or fewer than 5 incoming and outgoing links rare words (occurring in fewer than 3 articles) Discard articles that describe speciﬁc dates, as well as Wikipedia disambiguation pages, category pages 停止詞(Stop Words）指的是在自然環境中出現頻率非常高，但是對文章或頁面的意義沒有實質影響的那類詞。如英文中的“the”,“and”，“of” 等，中文中的“的”，“也”，“啊”等。\n\n140\n\n141 Semantic Interpreter 2-2Building a Semantic Interpreter\n\n142 Inverted matrix Panthera Concept2 Concept3 ... .... ConceptN cat 0.92\n\nxxx leopard 0.84 Roar 0.77 0.24 keyword4 KeywordM Graphically, we can represent a concept vector as the centroid of the word vectors it is composed of. The image below illustrates the centroid of a set of vectors i.e. it is the center or average position of the vectors.\n\n143 A HMM-based Approach to Question Answering against Linked Data\n\nCristina Giannone, Valentina Bellomaria, and Roberto Basili Proceedings of the 2013 CLEF 歐庭銨整理\n\n144 Architecture HMM initialization stage. HMM modeling stage\n\nHMM decoding stage\n\n145 Architecture In the HMM modeling stage, the states, emissions and transitions of the Markov chain are defined. States corresponds to the RDF elements retrieved by a question fragment from DBpedia: these elements may correspond to resources, classes or relations. The HMM best sequence of states is computed in the decoding module, here a disambiguation process is imposed exploiting the combination of statistical and ontological constraints obtaining a state sequence which can be mapped into a RDF subgraph\n\n146 Architecture\n\n147 Challenges Multilingualism\n\nto have SQA systems that can handle multiple input languages, which may even differ from the language used to encode the knowledge 中文啊！！！！\n\n148 Challenges Light expressions\n\nA lot of semantically light expression such as the verbs to be and to have, and prepositions of and with, either refer to an ontological property in a massively underspecified way (e.g., in Give me all movies with Tom Cruise, the proposition with needs to be mapped to the ontological property starring) or do not correspond to any property at all.\n\n149 Challenges Complex queries\n\ncontaining aggregation functions, comparisons, superlatives, and temporal reasoning Nested queries\n\n150 Complex Queries Examples\n\nAggregating functions, e.g., counting, as in How many bands broke up in 2010?, sometimes combined with ordering, as in Which countries have more than two official languages?, or with superlatives, as in How many members does the largest group have? Comparisons, like in Who recorded more singles than Madonna? and Which bridges are of the same type as the Manhattan Bridge? Superlatives, e.g., Which rock album has the most tracks? and What is the highest mountain? Temporal reasoning, as in Which artists have their 50th birthday on May 30? and Who was born on the same day as Frank Sinatra?\n\n151 Complex Queries Queries containing linguistic superlatives or comparatives are not considered complex queries if no mechanisms are required to understand the comparison within the ontology. For example, in What is the highest place of Karakoram?, the superlative is directly mapped to the ontological property highestPlace For example, in What mountain is the highest after the Annapurna? where highest  ontological property elevation and the values for all mountains first need to be filtered, so that only the ones with less elevation than the elevation of the Annapurna are kept, and then need to be sorted in descending order, so the first one can be picked as answer\n\n152 Complex Queries Nested queries\n\nYAGO-QA: allows nested queries when the subquery has already been answered. For example “Who is the governor of New York?” after “What is the state of New York?”\n\n153 Complex Queries Indirect queries\n\nFor example: grand-children  reverse of the parent-to-parent or the child-of-child. For example: uncle  brother-of-parent and male; aunt  sister-of-parent and female\n\n154 Challenges Distributional Knowledge\n\nSome questions are only answerable with multiple knowledge bases\n\n155 Challenges: Procedural, Temporal and Spatial Questions\n\nProcedural Questions Factual, list and yes-no questions are easiest to answer as they conform directly to SPARQL queries using SELECT and ASK. Others, such as why (causal) or how (procedural) questions require more additional processing. Procedural QA can currently not be solved by SQA, since, to the best of our knowledge, there are no existing knowledge bases that contain procedural knowledge\n\n156 Challenges: Procedural, Temporal and Spatial Questions\n\nTemporal Questions allows inferring the temporal relation of events from those of others, for example by using the transitivity of before and after Ex. Interval Based Temporal Logic can be used. Ex. the location and time are of the user posing the question can be used (QALL-ME)\n\n157 Challenges: Procedural, Temporal and Spatial Questions\n\nLOD contains locations which is expressed as 2-dimensional geocoordinates with latitude and longitude, while three-dimensional representations (e.g. with additional height) are not supported by the most often used schema More spatial relations can be considered. Ex. Crossing, inclusion, nearness, etc. (Younis et al.)\n\n158 Challenges Templates CASIA generates the graph pattern templates by using the question type, named entities and POS tags techniques. The generated graph patterns are then mapped to resources using WordNet, PATTY and similarity measures. Finally, the possible graph pattern combinations are used to build SPARQL queries\n\n159 Challenges Templates Xser first assigns semantic labels, i.e., variables, entities, relations and categories, to phrases by casting them to a sequence labelling pattern recognition problem which is then solved by a structured perceptron. The perceptron is trained using features including n-grams of POS tags, NER tags and words. Thus, Xser is capable of covering any complex basic graph pattern\n\n160 Challenges Templates An extension of gAnswer is based on question understanding and query evaluation. First, their approach uses a relation mining algorithm to find triple patterns in queries as well as relation extraction, POS-tagging and dependency parsing. Second, the approach tries to find a matching subgraph for the extracted triples and scores them based on a confidence score. Finally, the top-k subgraph matches are returned\n\n161 Challenges Common drawbacks of template-based approaches:\n\nfilter conditions, aggregations and superlatives\n\n162 Summary of Challenges"
    }
}
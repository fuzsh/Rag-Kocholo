{
    "id": "dbpedia_5658_3",
    "rank": 85,
    "data": {
        "url": "https://www.jneurosci.org/content/42/35/6782",
        "read_more_link": "",
        "language": "en",
        "title": "Task-Dependent Warping of Semantic Representations during Search for Visual Action Categories",
        "top_image": "https://www.jneurosci.org/sites/default/files/highwire/jneuro/42/35.cover-source.jpg",
        "meta_img": "https://www.jneurosci.org/sites/default/files/highwire/jneuro/42/35.cover-source.jpg",
        "images": [
            "https://www.jneurosci.org/sites/default/files/mobile-logo.png",
            "https://www.jneurosci.org/sites/default/files/jneuro%20logo.png",
            "https://www.jneurosci.org/sites/all/modules/contrib/panels_ajax_tab/images/loading.gif",
            "https://www.jneurosci.org/content/jneuro/42/35/6782/F1.medium.gif",
            "https://www.jneurosci.org/content/jneuro/42/35/6782/F1.medium.gif",
            "https://www.jneurosci.org/content/jneuro/42/35/6782/F2.medium.gif",
            "https://www.jneurosci.org/content/jneuro/42/35/6782/F2.medium.gif",
            "https://www.jneurosci.org/content/jneuro/42/35/6782/F3.medium.gif",
            "https://www.jneurosci.org/content/jneuro/42/35/6782/F3.medium.gif",
            "https://www.jneurosci.org/content/jneuro/42/35/6782/F4.medium.gif",
            "https://www.jneurosci.org/content/jneuro/42/35/6782/F4.medium.gif",
            "https://www.jneurosci.org/content/jneuro/42/35/6782/F5.medium.gif",
            "https://www.jneurosci.org/content/jneuro/42/35/6782/F5.medium.gif",
            "https://www.jneurosci.org/content/jneuro/42/35/6782/F6.medium.gif",
            "https://www.jneurosci.org/content/jneuro/42/35/6782/F6.medium.gif",
            "https://www.jneurosci.org/content/jneuro/42/35/6782/F7.medium.gif",
            "https://www.jneurosci.org/content/jneuro/42/35/6782/F7.medium.gif",
            "https://www.jneurosci.org/content/jneuro/42/35/6782/F8.medium.gif",
            "https://www.jneurosci.org/content/jneuro/42/35/6782/F8.medium.gif",
            "https://www.jneurosci.org/content/jneuro/42/35/6782/F9.medium.gif",
            "https://www.jneurosci.org/content/jneuro/42/35/6782/F9.medium.gif",
            "https://www.jneurosci.org/content/jneuro/42/35/6782/F10.medium.gif",
            "https://www.jneurosci.org/content/jneuro/42/35/6782/F10.medium.gif",
            "https://www.jneurosci.org/content/jneuro/42/35/6782/F11.medium.gif",
            "https://www.jneurosci.org/content/jneuro/42/35/6782/F11.medium.gif",
            "https://www.jneurosci.org/content/jneuro/42/35/6782/F12.medium.gif",
            "https://www.jneurosci.org/content/jneuro/42/35/6782/F12.medium.gif",
            "https://www.jneurosci.org/content/jneuro/42/35/6782/F13.medium.gif",
            "https://www.jneurosci.org/content/jneuro/42/35/6782/F13.medium.gif",
            "https://www.jneurosci.org/content/jneuro/42/35/6782/F14.medium.gif",
            "https://www.jneurosci.org/content/jneuro/42/35/6782/F14.medium.gif",
            "https://www.jneurosci.org/content/jneuro/42/35/6782/F15.medium.gif",
            "https://www.jneurosci.org/content/jneuro/42/35/6782/F15.medium.gif",
            "https://www.jneurosci.org/sites/default/files/styles/medium/public/highwire/jneuro/42/35.cover-source.jpg?itok=Bpg0sk_U",
            "https://www.jneurosci.org/sites/all/modules/highwire/highwire/images/twitter.png",
            "https://www.jneurosci.org/sites/all/modules/highwire/highwire/images/fb-blue.png",
            "https://www.jneurosci.org/sites/all/modules/highwire/highwire/images/mendeley.png",
            "https://www.jneurosci.org/sites/default/files/files/JNeurosci_footer_logo_forJCore_386x100.png",
            "https://www.jneurosci.org/sites/default/files/files/SfN_footer_logo_forJCore_336x100.png",
            "https://googleads.g.doubleclick.net/pagead/viewthroughconversion/952157035/?value=0&guid=ON&script=0"
        ],
        "movies": [],
        "keywords": [],
        "meta_keywords": [
            ""
        ],
        "tags": null,
        "authors": [
            "Mo Shahdloo",
            "Emin Çelik",
            "Burcu A. Urgen",
            "Jack L. Gallant",
            "Tolga Çukur"
        ],
        "publish_date": "2022-08-31T00:00:00",
        "summary": "",
        "meta_description": "Object and action perception in cluttered dynamic natural scenes relies on efficient allocation of limited brain resources to prioritize the attended targets over distractors. It has been suggested that during visual search for objects, distributed semantic representation of hundreds of object categories is warped to expand the representation of targets. Yet, little is known about whether and where in the brain visual search for action categories modulates semantic representations. To address this fundamental question, we studied brain activity recorded from five subjects (one female) via functional magnetic resonance imaging while they viewed natural movies and searched for either communication or locomotion actions. We find that attention directed to action categories elicits tuning shifts that warp semantic representations broadly across neocortex and that these shifts interact with intrinsic selectivity of cortical voxels for target actions. These results suggest that attention serves to facilitate task performance during social interactions by dynamically shifting semantic selectivity toward target actions and that tuning shifts are a general feature of conceptual representations in the brain.\n\nSIGNIFICANCE STATEMENT The ability to swiftly perceive the actions and intentions of others is a crucial skill for humans that relies on efficient allocation of limited brain resources to prioritize the attended targets over distractors. However, little is known about the nature of high-level semantic representations during natural visual search for action categories. Here, we provide the first evidence showing that attention significantly warps semantic representations by inducing tuning shifts in single cortical voxels, broadly spread across occipitotemporal, parietal, prefrontal, and cingulate cortices. This dynamic attentional mechanism can facilitate action perception by efficiently allocating neural resources to accentuate the representation of task-relevant action categories.",
        "meta_lang": "en",
        "meta_favicon": "https://www.jneurosci.org/sites/default/files/images/favicon.ico",
        "meta_site_name": "Journal of Neuroscience",
        "canonical_link": "https://www.jneurosci.org/content/42/35/6782",
        "text": "Subjects\n\nFive healthy adult volunteers with normal or corrected-to-normal vision who participated in this study were subject (S)1 (male, age 31), S2 (male, age 27), S3 (female, age 32), S4 (male, age 33), and S5 (male, age 27). Data were collected at the University of California, Berkeley. The experimental protocol was approved by the Committee for the Protection of Human Subjects at the University of California, Berkeley. All participants gave written informed consent before scanning.\n\nStimuli and experimental design\n\nData for the main experiment were collected in six 10 min 50 s runs in a single session. Continuous natural movies were used as the stimulus in the main experiment. Three distinct 10 min movie segments were compiled from short movie clips (10–20 s) without sound. Movie clips were selected from a diverse set of natural movies (Nishimoto et al., 2011). Movie clips were cropped into a square frame and downsampled to 512 × 512 pixels. The movie stimulus was displayed at 15 Hz on an MRI-compatible projector screen that covered a 24 × 24° visual angle. Subjects were instructed to covertly search for target categories in the movies while maintaining fixation. A set of instructions regarding the experimental procedure and exemplars of the search targets were provided to the subjects before the experiment. A color square of 0.16 × 0.16° at the center with color changing at 1 Hz was used as the fixation spot. A cue word was displayed before each run to indicate the attention target: communication or locomotion. The communication target contained actions with the intent of communication, including both verbal communication actions and nonverbal gestural communication actions (e.g., talking, shouting, smirking). The locomotion target contained locomotion-related actions with the intent of moving animate entities, including humans and anthropomorphized animals (e.g., moving, running, driving). The same movie stimuli were used during each of the two attention tasks. The order of attention conditions was interleaved across runs to minimize subject expectation bias. This resulted in the presentation of 1800 s of movies without repetition in each attention condition. Data from the first 20 s and last 30 s of each run were discarded to minimize effects of transient confounds. Following these procedures, 900 data samples for each attention condition were obtained.\n\nA separate set of functional data were collected while the same set of subjects passively viewed 120 min of natural movies., passive-viewing data; this dataset was also used in Huth et al., 2012 but here it was reanalyzed with a focus on action categories). This dataset was used to construct the semantic space and to select voxels subjected to further analyses. Data for the passive-viewing experiment were collected in 12 10 min 50 s runs in which 12 separate movie segments were displayed. Presentation procedures were the same between the main experiment and passive-viewing experiment, save for the number of runs. The passive-viewing dataset contained 3600 data samples.\n\nfMRI data collection\n\nData were collected on a 3T Siemens Tim Trio MRI scanner (Siemens Medical Solutions) via a 32-channel receiver coil. Functional data were collected using a T2*-weighted gradient-echo echoplanar imaging pulse sequence with the following parameters: TR = 2 s, TE = 33 ms, water-excitation pulse with flip angle = 70°, voxel size = 2.24 mm × 2.24 mm × 4.13 mm, field of view = 224 mm × 224 mm, 32 axial slices. To construct cortical surfaces, anatomic data were collected using a three-dimensional T1-weighted magnetization-prepared rapid-acquisition gradient-echo sequence with the following parameters: TR = 2.3 s, TE = 3.45 ms, flip angle = 10°, voxel size = 1 mm × 1 mm × 1 mm, field of view = 256 mm × 212 mm × 256 mm. Surface flattening and visualization were done via FreeSurfer and PyCortex software (Dale et al., 1999; Reuter et al., 2012; Gao et al., 2015).\n\nfMRI data preprocessing\n\nMotion correction was performed using Statistical Parametric Mapping (SPM12) toolbox (Friston et al., 1995). Functional volumes were aligned to the first image from the first run in each subject. Brain tissue was identified using the brain extraction tool (BET) from the Functional MRI of the Brain Software Library software package (Smith, 2002). Low-frequency response components were detected using a third-order Savitzky–Golay low-pass filter with 240 s temporal window and were removed from voxel responses. Voxel responses were then z scored to attain zero mean and unit variance. Voxels within the 2 mm neighborhood of the cortical sheet were identified as cortical voxels in each subject (S1, 37,791 voxels; S2, 32,671 voxels; S3, 36,942 voxels; S4, 42,090 voxels; S5, 39,254 voxels).\n\nDefinition of regions of interest\n\nTo define the anatomic regions of interest (ROIs) in each subject, the cortical surface was segmented into 156 regions of the Destrieux atlas (Destrieux et al., 2010) via FreeSurfer. Segmentation results were projected from the anatomic space onto the functional space using PyCortex, and each voxel was assigned an anatomic label based on the projections. Functional ROIs were identified in each subject using visual category and retinotopic localizers (Huth et al., 2012). Localizer experiments for visual category-selective areas [fusiform face area (FFA), occipital face area (OFA), parahippocampal place area (PPA), retrosplenial cortex RSC)] were performed in six 4.5 min runs of 16 blocks (Huth et al., 2012). Subjects passively viewed 20 random static images from one of the objects, scenes, body parts, faces, or spatially scrambled object groups in each block. Each image was shown for 300 ms following a 500 ms blank period. PPA and RSC were identified as voxels with positive scene versus objects contrast (t test, p < 10−4, uncorrected). FFA and OFA were defined using face-versus-object contrast (t test, p < 10−4, uncorrected). The boundaries of these areas were hand drawn on the cortical surfaces along the contours at which the contrast level reached half of the maximum. A localizer experiment for retinoptic early visual areas (RET; V1, V2, V3) contained four 9 min runs. Subjects viewed clockwise and counterclockwise rotating polar wedges in two runs. In the remaining two runs, subjects viewed expanding and contracting rings. Visual angle and eccentricity maps were used to define visual areas V1–3. Finally, ROIs were refined to voxels inside the drawn boundaries near a 2 mm neighborhood of the cortical sheet.\n\nAbbreviations for regions of interest and important sulci\n\nSeveral regions of interest and important sulci were labeled on the flattened cortical surfaces to guide the reader.\n\nRegions of interest\n\nAbbreviations are pMTG (posterior middle temporal gyrus), pSTS (posterior superior temporal sulcus), AG (angular gyrus), SMG (supramarginal gyrus), IPS (intraparietal sulcus), aIP (anterior intraparietal cortex), PrCu (precuneous), dPMC (dorsal premotor cortex), BA44/45 (Brodmann area 44/45), MFG (middle frontal gyrus), SFG (superior frontal gyrus), ACC (anterior cingulate cortex), RET (retinoptic early visual areas V1–3), FFA (fusiform face area), OFA (occipital face area), PPA (parahippocampal place area), RSC (retrosplenial cortex).\n\nSulci\n\nAbbreviations are TOS (temporo-occipital sulcus), STS (superior temporal sulcus), SF (Sylvian fissure), IFS (inferior frontal sulcus), MFS (middle frontal sulcus), SFS (superior frontal sulcus).\n\nHead motion, eye movement, and physiological noise\n\nTo prevent head motion and physiological noise confounds, estimates of these nuisance factors were regressed out of the BOLD responses. Six affine motion time courses estimated during the motion correction stage were taken as the head motion regressors. The cardiac and respiratory activity during the main experiment was recorded using a pulse oximeter and a pneumatic belt. These data were then used to estimate two regressors to capture respiration and nine regressors to capture cardiac activity (Verstynen and Deshpande, 2011).\n\nTo ensure that eye movements did not unduly bias the results, several control analyses were performed. ViewPoint EyeTracker (Arrington Research) was used to monitor subjects' eye positions at 60 Hz after getting calibrated at the beginning of each experimental run. Kruskal–Wallis tests were used to detect systematic differences in the distribution of eye position and movement. The distribution of eye position during search for communication and locomotion tasks were examined. We find that the distribution of eye position is not affected by search task (p = 0.17) or by target presence or absence (p = 0.74), and no significant interactions are present between these two factors (p = 0.60). To test whether eye movement is affected by target or distractor detection, the distribution of eye position during a 1 s window around target onset and target offset was studied. The eye position distribution is not affected by target onset (p = 0.73) or offset (p = 0.17), and there is no significant interaction between the aforementioned factors (p = 0.83). Furthermore, the moving-average SD of eye position was studied in a 200 ms window to determine systematic differences in rapid moment-to-moment variations in eye position across the two search tasks. There are no significant effects of search task (p = 0.11), target presence or absence (p = 0.32), target onset (p = 0.49), or target offset (p = 0.36), and there are no significant interactions among these factors (p = 0.16). Finally, moving-average SD of eye position was included in the model as a nuisance regressor and was regressed out of the BOLD responses.\n\nTo maintain subject vigilance, the subjects were instructed to depress a button whenever they detected a member of the target category in the stimulus (i.e., either a communication or a locomotion action depending on the search task). The behavioral responses were initially analyzed to ensure that subjects performed the tasks and that task difficulty was balanced across search targets. The target detection rate was 89 ± 9% for the communication and 91 ± 8% for the locomotion targets (mean ± SD across subjects), with no significant difference between the two tasks (bootstrap test, p > 0.05).\n\nCategory features\n\nA category feature space was constructed to encode the information pertaining to object and action categories in the movies. Each second of the movie stimulus was manually labeled using the WordNet lexicon (Miller, 1995) to find the time course for the presence of 922 different object and action categories in the movie stimulus. This yielded an indicator matrix where each row represents a 1 s clip of the movie stimulus, and each column represents a category. Finally, category features were obtained by downsampling the indicator matrix to 0.5 Hz to match the acquisition rate of fMRI.\n\nMotion-energy features\n\nTo infer cortical selectivity for low-level scene features, local spatial frequency and orientation information of each frame of the movie stimulus were quantified using a motion-energy filter bank. The filter bank contained 2139 Gabor filters that were computed at eight directions (0–350° in 45° steps), three temporal frequencies (0, 2, and 4 Hz), and six spatial frequencies (0, 1.5, 3, 6, 12, and 24 cycles/image). Filters were placed on a square grid spanning the 24 × 24° field of view. The luminance channel was extracted from the movie frames and passed through the filter bank. The outputs were then passed through a compressive nonlinearity to yield the motion-energy features (Nishimoto et al., 2011; Lescroart and Gallant, 2019). Finally, the motion-energy features were temporally downsampled to match the fMRI acquisition rate.\n\nSpace-time interest points features\n\nIntermediate-level kinematic information of the movies were quantified by constructing the Space-Time Interest Point (STIP) features using STIP toolbox (Laptev, 2005; Laptev et al., 2008). STIP features have been successfully leveraged in many computer vision applications to recognize human actions. As detailed in Laptev (2005) and Laptev et al. (2008), Harris operators were used to identify spatiotemporal interest points in the movie stimulus at multiple scales (σi2,τj2) = (21+i, 2j), i ∈ {1, …, 6}, j∈{1,2}, where σ and τ are the standard deviations of the Gaussian kernels in spatial and temporal domains, respectively. Histograms of oriented gradients (Dalal and Triggs, 2005), and histograms of optical flow (Holte et al., 2010) were calculated in the (Δx,i,Δy,i,Δt,j) spatiotemporal neighborhood of each interest point, where Δx,i=Δy,i=2kσi and Δt,j=2kτj, and k is the scale factor. The scale factor was set to 9 according to the default configuration of the toolbox. Finally, normalized histograms were concatenated to construct the collection of 162 STIP features and were downsampled to match the acquisition rate of fMRI.\n\nModel estimation and testing\n\nSeparate linearized models were fit in each voxel to estimate model weights that map each set of features (i.e., category, motion-energy, or STIP features) to the measured BOLD responses in each search task in individual subjects. Banded-ridge regression (Nunez-Elizalde et al., 2019) was used to fit the models. To capture the hemodynamic response, delayed feature time courses were concatenated. Delays of two, three, and four samples, corresponding to 4, 6, and 8 s were used. To account for potential correlations between target detection and BOLD responses, a nuisance target-presence regressor was included in the model. The target-presence regressor contained the category regressor for communication during search for the communication task and the category regressor for locomotion during search for the locomotion task. Model fitting for the two search tasks was performed concurrently by concatenating the features and BOLD responses across search tasks (Fig. 2). This procedure ensured consistency between the assigned regularization parameters across search tasks and enabled use of the target regressor (Shahdloo et al., 2020).\n\nA nested cross-validation (CV) procedure was used to choose the regularization parameters and estimate model weights. Data from the main experiment were segmented into 60 30 s blocks. In each of the 10 outer folds, four randomly chosen blocks were held out as validation data. Then, in each of the 10 inner folds, 54 randomly chosen blocks were used as training data, and the 2 remaining blocks were used as test data. To fit models for the passive-viewing data, data were segmented into 144 50 s blocks. In each fold, eight randomly chosen blocks were held out as validation data, 132 randomly chosen blocks were used as training data, and the four remaining blocks were used as test data. For each feature set, regularization parameters were selected with a random search; a thousand normalized regularization parameter candidates were sampled from a Dirichlet distribution and were scaled by 30 log-spaced values ranging from 10−5 to 1020. Training data were used to fit models for each set of regularization parameters independently. Model weights were then used to predict responses in the test data, and prediction scores of the fit models were assessed. Prediction scores were taken as the product-moment correlation coefficient between measured and predicted voxel responses. The set of regularization parameters maximizing the average prediction score across inner CV folds was chosen in each voxel. Finally, the optimal set of parameters was used to fit models on the union of training and test data in each outer fold, and model weights were averaged across the outer folds.\n\nFinally, prediction performance of the fit models were evaluated. In each outer fold, after discarding the nuisance regressors, responses were predicted for the validation data using the fit models, and prediction scores were averaged across the search tasks. Prediction scores were then averaged across the outer folds.\n\nFor each voxel, separate linearized models were estimated to relate each feature representation to the BOLD responses. Specifically, category models were fit to estimate category responses that represented the contribution of each category to single-voxel BOLD responses separately for the data in the main experiment and the passive-viewing data in individual subjects. Furthermore, a motion-energy model and a STIP model were fit in each voxel to represent the contribution of the low- and intermediate-level stimulus features to the responses. These alternative models were further used to select analysis voxels (i.e., semantic voxels).\n\nVariance partitioning\n\nObject-action categories can be correlated with low-level visual features of natural movies (Lescroart and Gallant, 2019), and there is evidence for representation of intermediate-level action features (e.g., action kinematics) across cortex (Jastorff et al., 2010). Therefore, there is a possibility that the estimated category responses are confounded by selectivity for low- and intermediate-level scene features. To control for potential confounds, we performed a variance partitioning analysis. This analysis estimates the response variance that is uniquely explained by the category model after accounting for variance that can be attributed to low- and intermediate-level features captured by the motion-energy and STIP models. To do this, we separately measured the variance explained when all three models (category, motion energy, and STIP) are fit simultaneously (i.e., combined model), and variance explained when only motion-energy and STIP models are fit simultaneously (i.e., control model). Banded ridge regression was used to fit the combined and control models to prevent bias in assigning regularization parameters across different feature sets. The explained variance (R2) was calculated as squared prediction scores, separately for the combined and control models. Note that from a model-fitting perspective, negative prediction scores that correspond to zero explained variance. Finally, unique variance explained by the category model was calculated as follows: R2̂cat=R2comb−R2cont.(1)\n\nHere, R2̂cat is the variance uniquely explained by the category model after accounting for low- and intermediate-level features, R2comb is the variance explained by the combined model, and R2cont is the variance explained by the union of motion-energy and STIP models in each voxel.\n\nAction category responses\n\nThe fit category responses reflect voxel tuning for each of the 922 object and action categories in the movie stimulus. To infer tuning for action categories, 922-dimensional category responses were masked to select only the 109 action categories. This yielded the voxelwise 109-dimensional action category responses.\n\nSemantic representation of actions\n\nPassive-viewing data were used to construct a continuous semantic space for action category representation. In this space, semantically similar action categories would project to nearby points, whereas semantically dissimilar categories would project to distant points (Huth et al., 2012). Category models were fit, and action category responses during passive viewing were estimated. A group semantic space was then obtained using principal component analysis (PCA) on the action category responses of cortical voxels pooled across all subjects. To maximize the quality of the semantic space, voxels in which the category model predicted unique response variance after accounting for the variance attributed to low- and intermediate-level stimulus features were selected. These voxels were further refined to include only the top 3000 best predicted voxels within each subject. The top 12 principal components (PCs) that explained >95% of the variance in responses were selected. Subsequent analyses were also repeated using the top eight PCs that explained >90% of the response variance, but the results remained consistent. The semantic tuning profile for each voxel under each search task was then obtained by projecting the respective action category responses onto the PCs. To illustrate the semantic content of the PCs, characteristic actions of the movie stimulus were clustered in the semantic space, and cluster centers were projected onto the PCs after getting labeled (see Fig. 6).\n\nConsistency of the semantic space across subjects\n\nTo test whether the estimated semantic space is consistent across subjects, we used a leave-one-out cross-validation procedure. In each cross-validation fold, voxels from four subjects were used to derive 12 PCs to construct a semantic space. In the left-out subject, the semantic tuning profile for each voxel was obtained by projecting action category responses during passive viewing onto the derived PCs. Next, the product-moment correlation coefficient was calculated between the tuning profiles in the derived space and the tuning profiles in the original semantic space. Results were averaged across semantic voxels in the left-out subject. The cross-validated semantic spaces consistently correlate with the original semantic space (see Fig. 7).\n\nCharacterizing tuning shifts\n\nAttentional tuning shifts toward or away from targets would be reflected in modulation of semantic selectivity for communication or locomotion action categories. Thus, the magnitude and direction of tuning shifts can be assessed by comparing the semantic selectivity for these categories between the two search tasks. Semantic selectivity for the two target categories was quantified as the similarity between semantic tuning profiles and idealized templates tuned solely for communication or locomotion action categories. First, idealized category responses were constructed as 109-dimensional vectors that contained ones for target categories (either communication or locomotion categories) and zeros previously. Idealized templates were then obtained by projecting these idealized category responses onto the semantic space. Semantic selectivity for each target category was quantified as the product-moment correlation coefficient between the voxel semantic tuning profile and the corresponding template as follows: Ti,C=corr(si,s′C)(2) Ti,L=corr(si,s′L),(3) where Ti,C and Ti,L are the tuning strength for communication (C) and locomotion (L) during condition i ∈ {C, L} denoting attend to communication or attend to locomotion; si is the semantic tuning profile during condition i, and s'C and s'L denote the idealized semantic tuning templates for communication and locomotion, respectively. Finally, voxelwise tuning shift index (TSIall) was quantified as follows: TSIall=(TC,C−TC,L) + (TL,L−TL,C)2−sign(TC,C−TC,L)TC,L−sign(TL,L−TL,C)TL,C.(4)\n\nThe numerator of TSI captures the difference in semantic selectivity for the attended versus unattended category, summed over the two attention tasks (i.e., search for communication and search for locomotion). Observing that the maximum possible selectivity for the attended category is 1, obtained when voxel tuning is equivalent to the idealized template, the denominator is cast to normalize the potential range of the TSI metric between 1 and −1 without affecting its sign. Tuning shifts toward the attended category would yield positive values where a TSIall of 1 indicates a complete match between voxel semantic tuning and idealized templates, whereas negative values would indicate shifts away from the attended category where a TSIall of −1 indicates a complete mismatch between voxel tuning and idealized templates. A TSIall of 0 would indicate that the voxel tuning did not shift between the two search tasks.\n\nThe TSI metric in Equation 4 can also be adopted to calculate tuning changes for any given set of action categories. To do this, the 922-dimensional category responses measured during attention tasks were masked to keep only the responses for the given set of actions. The masked tuning vectors and the idealized template for the given set were then projected onto the 12-dimensional semantic space. Semantic selectivity of a voxel to the given set was taken as the correlation coefficient between the projections of voxel tuning and the idealized template in the semantic space. Attentional modulation of semantic tuning for nontarget categories was examined by calculating a separate tuning shift index (TSInt). Note that this index can be calculated based on Equation 3 but by zeroing out the category responses for communication and locomotion actions before projection onto the semantic space. To study the tuning shifts in an ROI, TSIs were averaged across semantic voxels within the ROI.\n\nThe change in voxelwise tuning during attending to the first target (e.g., communication) versus the second target (e.g., locomotion) was defined as the l1-norm of the tuning difference between the two conditions. This calculated tuning change can be linearly decomposed into a component explained by the target features (i.e., the union of communication and locomotion features) and a component explained by the nontarget features (i.e., all features excluding the target features). The fraction of tuning change for target/nontarget features was computed by taking the ratio of the respective component to the overall tuning change.\n\nCharacterizing target preference during visual search\n\nTo investigate the interaction between tuning shifts and intrinsic selectivity for individual target action categories, we quantified a target preference index (PI; PI∈ [−1,1]) separately during the search for communication actions (PIcom) and during the search for locomotion actions (PIloc). PI during the search for each target action was taken as the difference in selectivity for the attended versus the unattended target as follows: PIcom=TC,C−TC,L1−sign(TC,C−TC,L)TC,L(5) PIloc=TL,L−TL,C1−sign(TL,L−TL,C)TL,C,(6) where PIcom denotes the relative tuning preference for communication actions during the search for communication, and PIloc denotes the relative tuning preference for locomotion actions during search for locomotion. In this scheme, a PI of 1 indicates a complete match between voxel semantic tuning and the idealized template for the target, whereas a PI of −1 indicates a complete mismatch between voxel tuning and the idealized template for the target. Finally, a PI of zero indicates that the voxel semantic tuning does not shift toward any of the target actions.\n\nCharacterizing action category preference during passive viewing\n\nTo investigate the interaction between the calculated preference index for individual targets and intrinsic selectivity for action categories, we quantified a selectivity index (SI; SI ∈ [−1,1]), separately for communication actions (SIcom) and for locomotion actions (SIloc). SI for each target in each voxel was calculated as the product-moment correlation coefficient between the voxel category response and idealized template category tuning for the given target.\n\nAction clustering\n\nTo facilitate interpretation of stimulus information captured by individual PCs, the characteristic action content of the movies was clustered and labeled. Action content (C¯) for each short clip was calculated as the number of frames where each of the 109 actions were present (Na¯) and was normalized by the total number of clip frames (N) as follows: C¯=Na¯N.(7)\n\nThis yielded a 109-dimensional action content vector for each clip. The action content vectors were then projected onto the semantic space and were grouped into 10 clusters using k means. The number of clusters was optimized using the elbow method (Thorndike, 1953). Average action content of each clip (A) was calculated as the mean of the clip's action content vector as follows: A=∑c109,(8) where C¯=[c1,c2,c3,...,c109]. To label the clusters, five clips with the highest average action contents within each cluster were selected. Four candidate labels for each cluster were manually assigned, and 15 evaluators were asked to score (from 1 to 5) the correspondence of the selected clips to each of the four candidate labels. Finally, the label with the highest score was selected to represent each cluster.\n\nStatistical analyses\n\nBootstrap tests were used to assess statistical significance. To assess significance of the prediction scores, single-voxel predicted responses were resampled 5000 times with replacement. For each bootstrap sample, the prediction score was computed. The significance level (p value) of the prediction scores was taken as the fraction of bootstrap samples in which the prediction scores were >0. The significance level of the unique response variance (Eq. 1) was taken as the fraction of bootstrap samples in which the unique variance explained by the category model was >0. All single-voxel significance levels were corrected to account for multiple comparisons using the false discovery rate correction (FDR; Benjamini and Hochberg, 1995).\n\nSignificance of TSIall, TSInt, PIcom, and PIloc was assessed for each ROI across subjects. To do this, ROI-wise metrics were resampled across subjects with replacement 10,000 times. Significance level was taken as the fraction of bootstrap samples where the test metric averaged across resampled subjects is <0 (for right-sided tests) or >0 (for left-sided tests). This procedure was performed in a total of 21 functional ROIs separately. All ROI significance levels were corrected to account for multiple comparisons using FDR.\n\nIn ROIs with a significant metric across subjects, the metric was further tested within individual subjects. To do this, semantic voxels within a given ROI were resampled with replacement 10,000 times. For each bootstrap sample, mean value of a given metric was computed across resampled voxels. The significance level was taken as the fraction of bootstrap samples in which the tested metric was <0 (for right-sided tests) or >0 (for left-sided tests).\n\nData availability\n\nData supporting the findings of this study are available from the corresponding authors on request. Results can be explored online via an interactive brain viewer at http://www.icon.bilkent.edu.tr/brainviewer/shahdloo_etal/. The codes used to estimate spatially informed voxelwise model weights are freely available on GitHub at https://github.com/icon-lab/SPIN-VM.\n\nVisual search warps semantic representation of actions\n\nPrevious studies suggest that the human brain represents visual categories by embedding them in a continuous semantic space (Huth et al., 2012). Here, we used linear encoding models to map category features of natural movies onto the recorded BOLD responses in single voxels. The model features, namely actions, are fundamental semantic concepts in both language and vision. The models successfully predict brain activity in cortical voxels, after controlling for lower levels of features (i.e., motion energy and STIP features). Thus, from a quantitative perspective, it could be argued that there is an explicit representation of the semantic categories of actions in the voxel responses (Naselaris et al., 2011). Note that a theoretical characterization of relationships among semantic concepts is difficult. In computational semantics, an empirical approach is adopted instead that is rooted in the distributional hypothesis. This hypothesis states that concepts with similar statistical distributions have similar meanings. Accordingly, co-occurrence statistics of concepts in corpora are used as a proxy metric for similarity of meaning in many methods for learning semantic relationships (Jurafsky and Martin, 2021). Here, to derive a semantic space underlying action category representations, we performed PCA on the model weights for action categories. Visual search for actions alters category model weights as reported here, so performing PCA on data from search tasks can bias estimates of the semantic space. Instead, we derived the semantic space using the passive-viewing dataset. Action categories that are semantically close to each other should project to nearby points in this space, whereas semantically dissimilar categories should project to distant points. The top 12 PCs that explained >95% of the variance in responses were selected, which showed a high degree of intersubject consistency (r = 0.52 ± 0.02 mean ± SEM across subjects; see Fig. 7). To visually examine the semantic information captured by this space, we projected action categories onto the PCs (Fig. 6a, see Fig. 9, projections onto the first three dimensions that accounted for 72.8% of the response variance; see Fig. 10, loadings for all PCs). All further quantitative analyses regarding tuning shifts were instead conducted in the full semantic space of 12 dimensions, including all the identified PCs.\n\nFigure 6-1\n\nCortical flatmaps of semantic representation for subject S1. a–c, Action category responses during a, passive viewing; b, search for communication; and c, search for locomotion categories were projected onto the semantic space in subject S1. A two-dimensional color map was used to color each voxel based on the projection values along the first and third semantic dimensions (color legend), Voxels where the category model does not explain unique response variance after accounting for low- and intermediate-level stimulus features are masked [bootstrap test, q(FDR) < 0.05]. Regions of interest are illustrated by white borders. Several important sulci are illustrated by dashed gray lines. For abbreviations for regions of interest and sulci, see above, Materials and Methods. Many voxels across occipitotemporal, parietal, and prefrontal cortices shift their tuning toward targets. Download Figure 6-1, TIF file.\n\nFigure 6-2\n\nCortical flatmaps of semantic representation for subject S2. a–c, Action category responses during a, passive viewing; b, search for communication; and c, search for locomotion categories were projected onto the semantic space in subject S2. Formatting is identical to that in Extended Data Figure 6-1. Download Figure 6-2, TIF file.\n\nFigure 6-3\n\nCortical flatmaps of semantic representation for subject S3. a–c, Action category responses during a, passive viewing; b, search for communication; and c, search for locomotion categories were projected onto the semantic space in subject S3. Formatting is identical to that in Extended Figure 6-1. Download Figure 6-3, TIF file.\n\nFigure 6-4\n\nCortical flatmaps of semantic representation for subject S4. a–c, Action category responses during a, passive viewing; b, search for communication; and c, search for locomotion categories were projected onto the semantic space in subject S4. Formatting is identical to that in Extended Data Figure 6-1. Download Figure 6-4, TIF file.\n\nFigure 6-5\n\nCortical flatmaps of semantic representation for subject S5. a–c, Action category responses during a, passive viewing; b, search for communication; and c, search for locomotion categories were projected onto the semantic space in subject S5. Formatting is identical to that in Extended Data Figure 6-1. Download Figure 6-5, TIF file.\n\nPrevious evidence suggests that visual search shifts single-voxel tuning profiles to expand the representation of the targets (Çukur et al., 2013). Thus, it is possible that action-based visual search also shifts semantic tuning in single voxels toward the target category. To investigate this possibility, we projected action category responses onto the semantic space. The first and third PCs maximally differentiated between actions belonging to the target categories (i.e., communication vs locomotion categories; see Fig. 8). Therefore, we visually compared the projections onto these PCs across the two search tasks. We observe that attention causes semantic tuning modulations broadly across cortex (Fig. 6b, Extended Data Figs. 6-1, 6-2, 6-3, 6-4, 6-5, results in individual brain spaces). Specifically, voxels in inferior posterior parietal cortex (PPC), cingulate cortex, and anterior inferior prefrontal cortex shift their tuning toward communication during search for communication actions. Meanwhile, voxels in superior PPC and medial parietal cortex shift their tuning toward locomotion during search for locomotion actions. Several reports suggest involvement of superior PPC in representing locomotion actions (Corbo and Orban, 2017) and inferior PPC in representing communication actions (Rizzolatti and Matelli, 2003; Abdollahi et al., 2013). Therefore, our findings suggest that during search for a given action category, tuning shifts toward the target category are most prominent in voxels that are primarily selective for the target.\n\nVisual search for action categories shifts single-voxel semantic tuning profiles\n\nOur inspection of semantic representations during visual search reveals that attention broadly modulates high-level action representations by shifting semantic tuning profiles in single voxels. To quantify the magnitude and direction of these tuning changes, we separately measured semantic selectivity for communication and locomotion action categories in each search task. The 922-dimensional category responses for individual voxels measured during attention tasks and the idealized template vectors for the targets were projected onto the semantic space. The template vector for a target is constructed as a 109-dimensional indicator vector containing ones for the target category and all its subordinate categories and zeros for the remaining categories. For instance, the locomotion template has ones for locomotion and for walk, run, crawl, move, ride, and so on. As such, the target template vector indexes the target action as well as actions that are semantically related to the target according to the WordNet hierarchy (see above, Materials and Methods). For each attention task, semantic selectivity of a given voxel for a target category was then quantified as the correlation coefficient between projected 12-dimensional vectors characterizing the voxelwise tuning profile and the idealized template in the semantic space. For each voxel, a tuning shift index (TSIall ∈ [−1,1]) was taken as the difference in semantic selectivity for targets when they were attended versus unattended. A positive TSIall indicates shifts toward the target, a negative TSIall indicates shifts away from the target, and a TSIall of 0 suggests no change in between tasks (see above, Materials and Methods).\n\nWe find that voxels across many cortical regions shift their tuning toward the attended category (see Fig. 11a, 11-1a, 11-2a, 11-3a, 11-4a, 11-5a, results in individual brain spaces). The respective tuning shifts are shown in relevant ROIs (see Figure 15a). Tuning shifts are significantly greater than zero in many areas across AON including occipitotemporal cortex (pSTS, pMTG), posterior parietal cortex (IPS; AG, SMG), and premotor cortex [Brodmann's areas 44, 45, BA44/45; bootstrap test q(FDR) < 0.05; see Fig. 15a]. This result suggests that focused attention to specific action categories shifts semantic tuning toward targets in single voxels and that these attentional modulations are present at all levels of the AON hierarchy including occipitotemporal cortex.\n\nFigure 11-1\n\nCortical flatmaps of TSI and PI for subject S1. a, Tuning shift index for all action categories (TSIall). b, Tuning shift for nontarget categories (TSInt). c, Preference index values (PIcom, PIloc) were projected onto the semantic space in subject S1 (Figs. 11, 14, color map legends). Only significant voxels are shown [bootstrap test, q(FDR) < 0.05]. Formatting is identical to that in Extended Data Figure 6-1.\n\nFigure 11-2\n\nCortical flatmaps of TSI and PI for subject S2. a, Tuning shift index for all action categories (TSIall). b, Tuning shift for nontarget categories (TSInt). c, Preference index values (PIcom, PIloc) were projected onto the semantic space in subject S2 (Figs. 11, 14, color map legends). Only significant voxels are shown [bootstrap test, q(FDR) < 0.05]. Formatting is identical to that in Extended Data Figure 6-1. Download Figure 11-2, EPS file.\n\nFigure 11-3\n\nCortical flatmaps of TSI and PI for subject S3. a, Tuning shift index for all action categories (TSIall). b, Tuning shift for nontarget categories (TSInt). c, Preference index values (PIcom, PIloc) were projected onto the semantic space in subject S3 (Figs. 11, 14, color map legends). Only significant voxels are shown [bootstrap test, q(FDR) < 0.05]. Formatting is identical to that in Extended Data Figure 6-1. Download Figure 11-3, EPS file.\n\nFigure 11-4\n\nCortical flatmaps of TSI and PI for subject S4. a, Tuning shift index for all action categories (TSIall). b, Tuning shift for nontarget categories (TSInt). c, Preference index values (PIcom, PIloc) were projected onto the semantic space in subject S2 (Figs. 11, 14, color map legends). Only significant voxels are shown [bootstrap test, q(FDR) < 0.05]. Formatting is identical to that in Extended Data Figure 6-1. Download Figure 11-4, EPS file.\n\nFigure 11-5\n\nCortical flatmaps of TSI and PI for subject S5. a, Tuning shift index for all action categories (TSIall). b, Tuning shift for nontarget categories (TSInt). c, Preference index values (PIcom, PIloc) were projected onto the semantic space in subject S2 (Figs. 11, 14, color map legends). Only significant voxels are shown [bootstrap test, q(FDR) < 0.05]. Formatting is identical to that in Extended Data Figure 6-1. Download Figure 11-5, EPS file.\n\nPrior evidence suggests that during category-based visual search, semantic tuning shifts grow stronger toward later stages of semantic processing (Çukur et al., 2013). Here, we find that semantic tuning shifts in AG and SMG are significantly stronger than those in occipitotemporal (pSTS, pMTG) and premotor cortices (i.e., averaged over AG and SMG, compared with the average over pSTS and pMTG, and with the average over dPMC and BA44/45; Cohen's d = 1.36, p < 0.05). Therefore, the tuning shifts reported here could indicate that AG and SMG are higher nodes in the hierarchy of semantic representation of action categories. In a previous study, we reported that in medial prefrontal cortex, visual search for object categories causes tuning shifts toward targets, whereas it causes tuning shifts away from targets in voxels in PrCu and temporoparietal junction (TPJ; Çukur et al., 2013). Similarly, by qualitative inspection of the flatmaps, here we observe that visual search for action categories causes negative tuning shifts in many voxels across PrCu and TPJ. These results suggest that these areas might be involved in distractor detection and in error monitoring during visual search for actions (Corbetta and Shulman, 2002)."
    }
}
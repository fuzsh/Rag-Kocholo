{
    "id": "dbpedia_1333_2",
    "rank": 32,
    "data": {
        "url": "https://worldwidescience.org/topicpages/t/temporal%2Bdownscaling%2Bmethodology.html",
        "read_more_link": "",
        "language": "en",
        "title": "temporal downscaling methodology: Topics by WorldWideScience.org",
        "top_image": "",
        "meta_img": "",
        "images": [
            "https://worldwidescience.org/sites/www.osti.gov/files/public/image-files/WWSlogo_wTag650px-min.png",
            "https://worldwidescience.org/sites/www.osti.gov/files/public/image-files/OSTIlogo.svg",
            "https://worldwidescience.org/sites/www.osti.gov/files/public/image-files/ICSTIlogo.svg"
        ],
        "movies": [],
        "keywords": [],
        "meta_keywords": [
            ""
        ],
        "tags": null,
        "authors": [],
        "publish_date": null,
        "summary": "",
        "meta_description": "",
        "meta_lang": "en",
        "meta_favicon": "",
        "meta_site_name": "",
        "canonical_link": null,
        "text": "Projected large flood event sensitivity to projection selection and temporal downscaling methodology\n\nEnergy Technology Data Exchange (ETDEWEB)\n\nRaff, D. [U.S. Dept. of the Interior, Bureau of Reclamation, Denver, Colorado (United States)\n\n2008-07-01\n\nLarge flood events, that influence regulatory guidelines as well as safety of dams decisions, are likely to be affected by climate change. This talk will evaluate the use of climate projections downscaled and run through a rainfall - runoff model and its influence on large flood events. The climate spatial downscaling is performed statistically and a re-sampling and scaling methodology is used to temporally downscale from monthly to daily signals. The signals are run through a National Weather Service operational rainfall-runoff model to produce 6-hour flows. The flows will be evaluated for changes in large events at look-ahead horizons from 2011 - 2040, 2041 - 2070, and 2071 - 2099. The sensitivity of results will be evaluated with respect to projection selection criteria and re-sampling and scaling criteria for the Boise River in Idaho near Lucky Peak Dam. (author)\n\nProjected large flood event sensitivity to projection selection and temporal downscaling methodology\n\nInternational Nuclear Information System (INIS)\n\nRaff, D.\n\n2008-01-01\n\nLarge flood events, that influence regulatory guidelines as well as safety of dams decisions, are likely to be affected by climate change. This talk will evaluate the use of climate projections downscaled and run through a rainfall - runoff model and its influence on large flood events. The climate spatial downscaling is performed statistically and a re-sampling and scaling methodology is used to temporally downscale from monthly to daily signals. The signals are run through a National Weather Service operational rainfall-runoff model to produce 6-hour flows. The flows will be evaluated for changes in large events at look-ahead horizons from 2011 - 2040, 2041 - 2070, and 2071 - 2099. The sensitivity of results will be evaluated with respect to projection selection criteria and re-sampling and scaling criteria for the Boise River in Idaho near Lucky Peak Dam. (author)\n\nDeriving temporally continuous soil moisture estimations at fine resolution by downscaling remotely sensed product\n\nScience.gov (United States)\n\nJin, Yan; Ge, Yong; Wang, Jianghao; Heuvelink, Gerard B. M.\n\n2018-06-01\n\nLand surface soil moisture (SSM) has important roles in the energy balance of the land surface and in the water cycle. Downscaling of coarse-resolution SSM remote sensing products is an efficient way for producing fine-resolution data. However, the downscaling methods used most widely require full-coverage visible/infrared satellite data as ancillary information. These methods are restricted to cloud-free days, making them unsuitable for continuous monitoring. The purpose of this study is to overcome this limitation to obtain temporally continuous fine-resolution SSM estimations. The local spatial heterogeneities of SSM and multiscale ancillary variables were considered in the downscaling process both to solve the problem of the strong variability of SSM and to benefit from the fusion of ancillary information. The generation of continuous downscaled remote sensing data was achieved via two principal steps. For cloud-free days, a stepwise hybrid geostatistical downscaling approach, based on geographically weighted area-to-area regression kriging (GWATARK), was employed by combining multiscale ancillary variables with passive microwave remote sensing data. Then, the GWATARK-estimated SSM and China Soil Moisture Dataset from Microwave Data Assimilation SSM data were combined to estimate fine-resolution data for cloudy days. The developed methodology was validated by application to the 25-km resolution daily AMSR-E SSM product to produce continuous SSM estimations at 1-km resolution over the Tibetan Plateau. In comparison with ground-based observations, the downscaled estimations showed correlation (R â¥ 0.7) for both ascending and descending overpasses. The analysis indicated the high potential of the proposed approach for producing a temporally continuous SSM product at fine spatial resolution.\n\nMethodology for Air Quality Forecast Downscaling from Regional- to Street-Scale\n\nScience.gov (United States)\n\nBaklanov, Alexander; Nuterman, Roman; Mahura, Alexander; Amstrup, Bjarne; Hansen Saas, Bent; Havskov SÃ¸rensen, Jens; Lorenzen, Thomas; Weismann, Jakob\n\n2010-05-01\n\nThe most serious air pollution events occur in cities where there is a combination of high population density and air pollution, e.g. from vehicles. The pollutants can lead to serious human health problems, including asthma, irritation of the lungs, bronchitis, pneumonia, decreased resistance to respiratory infections, and premature death. In particular air pollution is associated with increase in cardiovascular disease and lung cancer. In 2000 WHO estimated that between 2.5 % and 11 % of total annual deaths are caused by exposure to air pollution. However, European-scale air quality models are not suited for local forecasts, as their grid-cell is typically of the order of 5 to 10km and they generally lack detailed representation of urban effects. Two suites are used in the framework of the EC FP7 project MACC (Monitoring of Atmosphere Composition and Climate) to demonstrate how downscaling from the European MACC ensemble to local-scale air quality forecast will be carried out: one will illustrate capabilities for the city of Copenhagen (Denmark); the second will focus on the city of Bucharest (Romania). This work is devoted to the first suite, where methodological aspects of downscaling from regional (European/ Denmark) to urban scale (Copenhagen), and from the urban down to street scale. The first results of downscaling according to the proposed methodology are presented. The potential for downscaling of European air quality forecasts by operating urban and street-level forecast models is evaluated. This will bring a strong support for continuous improvement of the regional forecast modelling systems for air quality in Europe, and underline clear perspectives for the future regional air quality core and downstream services for end-users. At the end of the MACC project, requirements on \"how-to-do\" downscaling of European air-quality forecasts to the city and street levels with different approaches will be formulated.\n\nFrom a Real Deployment to a Downscaled Testbed : A Methodological Approach\n\nNARCIS (Netherlands)\n\nStajkic, Andrea; Abrignani, Melchiorre Danilo; Buratti, Chiara; Bettinelli, Andrea; Vigo, Daniele; Verdone, Roberto\n\n2016-01-01\n\nThis paper proposes a novel methodology for the spatial downscaling of real-world deployments of wireless networks, running protocols, and/or applications for the Internet of Things (IoT). These networks are often deployed in environments not easily accessible and highly unpredictable, where doing\n\nUsing a weather generator to downscale spatio-temporal precipitation at urban scale\n\nDEFF Research Database (Denmark)\n\nSÃ¸rup, Hjalte Jomo Danielsen; Christensen, Ole BÃ¸ssing; Arnbjerg-Nielsen, Karsten\n\nIn recent years, urban flooding has occurred in Denmark due to very local extreme precipitation events with very short lifetime. Several of these floods have been among the most severe ever experienced. The current study demonstrates the applicability of the Spatio-Temporal Neyman-Scott Rectangular...... the observed spatio-temporal differences at very fine scale for all measured parameters. For downscaling, perturbation with a climate change signal, precipitation from four different regional climate model simulations has been analysed. The analysed models are two runs from the ENSEMBLES (RACMO...\n\nBioclim deliverable D8a: development of the rule-based down-scaling methodology for BIOCLIM Work-package 3\n\nInternational Nuclear Information System (INIS)\n\n2003-01-01\n\nThe BIOCLIM project on modelling sequential Biosphere systems under Climate change for radioactive waste disposal is part of the EURATOM fifth European framework programme. The project was launched in October 2000 for a three-year period. The project aims at providing a scientific basis and practical methodology for assessing the possible long term impacts on the safety of radioactive waste repositories in deep formations due to climate and environmental change. Five work packages (WP) have been identified to fulfill the project objectives. One of the tasks of BIOCLIM WP3 was to develop a rule-based approach for down-scaling from the MoBidiC model of intermediate complexity in order to provide consistent estimates of monthly temperature and precipitation for the specific regions of interest to BIOCLIM (Central Spain, Central England and Northeast France, together with Germany and the Czech Republic). A statistical down-scaling methodology has been developed by Philippe Marbaix of CEA/LSCE for use with the second climate model of intermediate complexity used in BIOCLIM - CLIMBER-GREMLINS. The rule-based methodology assigns climate states or classes to a point on the time continuum of a region according to a combination of simple threshold values which can be determined from the coarse scale climate model. Once climate states or classes have been defined, monthly temperature and precipitation climatologies are constructed using analogue stations identified from a data base of present-day climate observations. The most appropriate climate classification for BIOCLIM purposes is the Koeppen/Trewartha scheme. This scheme has the advantage of being empirical, but only requires monthly averages of temperature and precipitation as input variables. Section 2 of this deliverable (D8a) outline how each of the eight methodological steps have been undertaken for each of the three main BIOCLIM study regions (Central England, Northeast France and Central Spain) using Mo\n\nDownscaling Surface Water Inundation from Coarse Data to Fine-Scale Resolution: Methodology and Accuracy Assessment\n\nDirectory of Open Access Journals (Sweden)\n\nGuiping Wu\n\n2015-11-01\n\nFull Text Available The availability of water surface inundation with high spatial resolution is of fundamental importance in several applications such as hydrology, meteorology and ecology. Medium spatial resolution sensors, like MODerate-resolution Imaging Spectroradiometer (MODIS, exhibit a significant potential to study inundation dynamics over large areas because of their high temporal resolution. However, the low spatial resolution provided by MODIS is not appropriate to accurately delineate inundation over small scale. Successful downscaling of water inundation from coarse to fine resolution would be crucial for improving our understanding of complex inundation characteristics over the regional scale. Therefore, in this study, we propose an innovative downscaling method based on the normalized difference water index (NDWI statistical regression algorithm towards generating small-scale resolution inundation maps from MODIS data. The method was then applied to the Poyang Lake of China. To evaluate the performance of the proposed downscaling method, qualitative and quantitative comparisons were conducted between the inundation extent of MODIS (250 m, Landsat (30 m and downscaled MODIS (30 m. The results indicated that the downscaled MODIS (30 m inundation showed significant improvement over the original MODIS observations when compared with simultaneous Landsat (30 m inundation. The edges of the lakes become smoother than the results from original MODIS image and some undetected water bodies were delineated with clearer shapes in the downscaled MODIS (30 m inundation map. With respect to high-resolution Landsat TM/ETM+ derived inundation, the downscaling procedure has significantly increased the R2 and reduced RMSE and MAE both for the inundation area and for the value of landscape metrics. The main conclusion of this study is that the downscaling algorithm is promising and quite feasible for the inundation mapping over small-scale lakes.\n\nA space and time scale-dependent nonlinear geostatistical approach for downscaling daily precipitation and temperature\n\nKAUST Repository\n\nJha, Sanjeev Kumar\n\n2015-07-21\n\nA geostatistical framework is proposed to downscale daily precipitation and temperature. The methodology is based on multiple-point geostatistics (MPS), where a multivariate training image is used to represent the spatial relationship between daily precipitation and daily temperature over several years. Here, the training image consists of daily rainfall and temperature outputs from the Weather Research and Forecasting (WRF) model at 50 km and 10 km resolution for a twenty year period ranging from 1985 to 2004. The data are used to predict downscaled climate variables for the year 2005. The result, for each downscaled pixel, is daily time series of precipitation and temperature that are spatially dependent. Comparison of predicted precipitation and temperature against a reference dataset indicates that both the seasonal average climate response together with the temporal variability are well reproduced. The explicit inclusion of time dependence is explored by considering the climate properties of the previous day as an additional variable. Comparison of simulations with and without inclusion of time dependence shows that the temporal dependence only slightly improves the daily prediction because the temporal variability is already well represented in the conditioning data. Overall, the study shows that the multiple-point geostatistics approach is an efficient tool to be used for statistical downscaling to obtain local scale estimates of precipitation and temperature from General Circulation Models. This article is protected by copyright. All rights reserved.\n\nFundamental statistical relationships between monthly and daily meteorological variables: Temporal downscaling of weather based on a global observational dataset\n\nScience.gov (United States)\n\nSommer, Philipp; Kaplan, Jed\n\n2016-04-01\n\nAccurate modelling of large-scale vegetation dynamics, hydrology, and other environmental processes requires meteorological forcing on daily timescales. While meteorological data with high temporal resolution is becoming increasingly available, simulations for the future or distant past are limited by lack of data and poor performance of climate models, e.g., in simulating daily precipitation. To overcome these limitations, we may temporally downscale monthly summary data to a daily time step using a weather generator. Parameterization of such statistical models has traditionally been based on a limited number of observations. Recent developments in the archiving, distribution, and analysis of \"big data\" datasets provide new opportunities for the parameterization of a temporal downscaling model that is applicable over a wide range of climates. Here we parameterize a WGEN-type weather generator using more than 50 million individual daily meteorological observations, from over 10'000 stations covering all continents, based on the Global Historical Climatology Network (GHCN) and Synoptic Cloud Reports (EECRA) databases. Using the resulting \"universal\" parameterization and driven by monthly summaries, we downscale mean temperature (minimum and maximum), cloud cover, and total precipitation, to daily estimates. We apply a hybrid gamma-generalized Pareto distribution to calculate daily precipitation amounts, which overcomes much of the inability of earlier weather generators to simulate high amounts of daily precipitation. Our globally parameterized weather generator has numerous applications, including vegetation and crop modelling for paleoenvironmental studies.\n\nDownscaling Coarse Actual ET Data Using Land Surface Resistance\n\nScience.gov (United States)\n\nShen, T.\n\n2017-12-01\n\nThis study proposed a new approach of downscaling ETWATCH 1km actual evapotranspiration (ET) product to a spatial resolution of 30m using land surface resistance that simulated mainly from monthly Landsat8 data and Jarvis method, which combined the benefits of both high temporal resolution of ETWATCH product and fine spatial resolution of Landsat8. The driving factor, surface resistance (Rs), was chosen for the reason that could reflect the transfer ability of vapor flow over canopy. Combined resistance Rs both upon canopy conditions, atmospheric factors and available water content of soil, which remains stable inside one ETWATCH pixel (1km). In this research, we used ETWATCH 1km ten-day actual ET product from April to October in a total of twenty-one images and monthly 30 meters cloud-free NDVI of 2013 (two images from HJ as a substitute due to cloud contamination) combined meteorological indicators for downscaling. A good agreement and correlation were obtained between the downscaled data and three flux sites observation in the middle reach of Heihe basin. The downscaling results show good consistency with the original ETWATCH 1km data both temporal and spatial scale over different land cover types with R2 ranged from 0.8 to 0.98. Besides, downscaled result captured the progression of vegetation transpiration well. This study proved the practicability of new downscaling method in the water resource management.\n\nDownscaling of coarse resolution LAI products to achieve both high spatial and temporal resolution for regions of interest\n\nKAUST Repository\n\nHouborg, Rasmus; McCabe, Matthew; Gao, Feng\n\n2015-01-01\n\nThis paper presents a flexible tool for spatio-temporal enhancement of coarse resolution leaf area index (LAI) products, which is readily adaptable to different land cover types, landscape heterogeneities and cloud cover conditions. The framework integrates a rule-based regression tree approach for estimating Landsat-scale LAI from existing 1 km resolution LAI products, and the Spatial and Temporal Adaptive Reflectance Fusion Model (STARFM) to intelligently interpolate the downscaled LAI between Landsat acquisitions. Comparisons against in-situ records of LAI measured over corn and soybean highlights its utility for resolving sub-field LAI dynamics occurring over a range of plant development stages.\n\nDownscaling of coarse resolution LAI products to achieve both high spatial and temporal resolution for regions of interest\n\nKAUST Repository\n\nHouborg, Rasmus\n\n2015-11-12\n\nThis paper presents a flexible tool for spatio-temporal enhancement of coarse resolution leaf area index (LAI) products, which is readily adaptable to different land cover types, landscape heterogeneities and cloud cover conditions. The framework integrates a rule-based regression tree approach for estimating Landsat-scale LAI from existing 1 km resolution LAI products, and the Spatial and Temporal Adaptive Reflectance Fusion Model (STARFM) to intelligently interpolate the downscaled LAI between Landsat acquisitions. Comparisons against in-situ records of LAI measured over corn and soybean highlights its utility for resolving sub-field LAI dynamics occurring over a range of plant development stages.\n\nNon-linear statistical downscaling of present and LGM precipitation and temperatures over Europe\n\nDirectory of Open Access Journals (Sweden)\n\nM. Vrac\n\n2007-12-01\n\nFull Text Available Local-scale climate information is increasingly needed for the study of past, present and future climate changes. In this study we develop a non-linear statistical downscaling method to generate local temperatures and precipitation values from large-scale variables of a Earth System Model of Intermediate Complexity (here CLIMBER. Our statistical downscaling scheme is based on the concept of Generalized Additive Models (GAMs, capturing non-linearities via non-parametric techniques. Our GAMs are calibrated on the present Western Europe climate. For this region, annual GAMs (i.e. models based on 12 monthly values per location are fitted by combining two types of large-scale explanatory variables: geographical (e.g. topographical information and physical (i.e. entirely simulated by the CLIMBER model.\n\nTo evaluate the adequacy of the non-linear transfer functions fitted on the present Western European climate, they are applied to different spatial and temporal large-scale conditions. Local projections for present North America and Northern Europe climates are obtained and compared to local observations. This partially addresses the issue of spatial robustness of our transfer functions by answering the question \"does our statistical model remain valid when applied to large-scale climate conditions from a region different from the one used for calibration?\". To asses their temporal performances, local projections for the Last Glacial Maximum period are derived and compared to local reconstructions and General Circulation Model outputs.\n\nOur downscaling methodology performs adequately for the Western Europe climate. Concerning the spatial and temporal evaluations, it does not behave as well for Northern America and Northern Europe climates because the calibration domain may be too different from the targeted regions. The physical explanatory variables alone are not capable of downscaling realistic values. However, the inclusion of\n\nRepresentation of spatial and temporal variability of daily wind speed and of intense wind events over the Mediterranean Sea using dynamical downscaling: impact of the regional climate model configuration\n\nDirectory of Open Access Journals (Sweden)\n\nM. Herrmann\n\n2011-07-01\n\nFull Text Available Atmospheric datasets coming from long term reanalyzes of low spatial resolution are used for different purposes. Wind over the sea is, for example, a major ingredient of oceanic simulations. However, the shortcomings of those datasets prevent them from being used without an adequate corrective preliminary treatment. Using a regional climate model (RCM to perform a dynamical downscaling of those large scale reanalyzes is one of the methods used in order to produce fields that realistically reproduce atmospheric chronology and where those shortcomings are corrected. Here we assess the influence of the configuration of the RCM used in this framework on the representation of wind speed spatial and temporal variability and intense wind events on a daily timescale. Our RCM is ALADIN-Climate, the reanalysis is ERA-40, and the studied area is the Mediterranean Sea.\n\nFirst, the dynamical downscaling significantly reduces the underestimation of daily wind speed, in average by 9 % over the whole Mediterranean. This underestimation has been corrected both globally and locally, and for the whole wind speed spectrum. The correction is the strongest for periods and regions of strong winds. The representation of spatial variability has also been significantly improved. On the other hand, the temporal correlation between the downscaled field and the observations decreases all the more that one moves eastwards, i.e. further from the atmospheric flux entry. Nonetheless, it remains ~0.7, the downscaled dataset reproduces therefore satisfactorily the real chronology.\n\nSecond, the influence of the choice of the RCM configuration has an influence one order of magnitude smaller than the improvement induced by the initial downscaling. The use of spectral nudging or of a smaller domain helps to improve the realism of the temporal chronology. Increasing the resolution very locally (both spatially and temporally improves the representation of spatial\n\nEvaluating Downscaling Methods for Seasonal Climate Forecasts over East Africa\n\nScience.gov (United States)\n\nRoberts, J. Brent; Robertson, Franklin R.; Bosilovich, Michael; Lyon, Bradfield; Funk, Chris\n\n2013-01-01\n\nThe U.S. National Multi-Model Ensemble seasonal forecasting system is providing hindcast and real-time data streams to be used in assessing and improving seasonal predictive capacity. The NASA / USAID SERVIR project, which leverages satellite and modeling-based resources for environmental decision making in developing nations, is focusing on the evaluation of NMME forecasts specifically for use in impact modeling within hub regions including East Africa, the Hindu Kush-Himalayan (HKH) region and Mesoamerica. One of the participating models in NMME is the NASA Goddard Earth Observing System (GEOS5). This work will present an intercomparison of downscaling methods using the GEOS5 seasonal forecasts of temperature and precipitation over East Africa. The current seasonal forecasting system provides monthly averaged forecast anomalies. These anomalies must be spatially downscaled and temporally disaggregated for use in application modeling (e.g. hydrology, agriculture). There are several available downscaling methodologies that can be implemented to accomplish this goal. Selected methods include both a non-homogenous hidden Markov model and an analogue based approach. A particular emphasis will be placed on quantifying the ability of different methods to capture the intermittency of precipitation within both the short and long rain seasons. Further, the ability to capture spatial covariances will be assessed. Both probabilistic and deterministic skill measures will be evaluated over the hindcast period\n\nMultifractal Downscaling of Rainfall Using Normalized Difference Vegetation Index (NDVI) in the Andes Plateau.\n\nScience.gov (United States)\n\nDuffaut Espinosa, L A; Posadas, A N; Carbajal, M; Quiroz, R\n\n2017-01-01\n\nIn this paper, a multifractal downscaling technique is applied to adequately transformed and lag corrected normalized difference vegetation index (NDVI) in order to obtain daily estimates of rainfall in an area of the Peruvian Andean high plateau. This downscaling procedure is temporal in nature since the original NDVI information is provided at an irregular temporal sampling period between 8 and 11 days, and the desired final scale is 1 day. The spatial resolution of approximately 1 km remains the same throughout the downscaling process. The results were validated against on-site measurements of meteorological stations distributed in the area under study.\n\nApplication of physical scaling towards downscaling climate model precipitation data\n\nScience.gov (United States)\n\nGaur, Abhishek; Simonovic, Slobodan P.\n\n2018-04-01\n\nPhysical scaling (SP) method downscales climate model data to local or regional scales taking into consideration physical characteristics of the area under analysis. In this study, multiple SP method based models are tested for their effectiveness towards downscaling North American regional reanalysis (NARR) daily precipitation data. Model performance is compared with two state-of-the-art downscaling methods: statistical downscaling model (SDSM) and generalized linear modeling (GLM). The downscaled precipitation is evaluated with reference to recorded precipitation at 57 gauging stations located within the study region. The spatial and temporal robustness of the downscaling methods is evaluated using seven precipitation based indices. Results indicate that SP method-based models perform best in downscaling precipitation followed by GLM, followed by the SDSM model. Best performing models are thereafter used to downscale future precipitations made by three global circulation models (GCMs) following two emission scenarios: representative concentration pathway (RCP) 2.6 and RCP 8.5 over the twenty-first century. The downscaled future precipitation projections indicate an increase in mean and maximum precipitation intensity as well as a decrease in the total number of dry days. Further an increase in the frequency of short (1-day), moderately long (2-4 day), and long (more than 5-day) precipitation events is projected.\n\nA SPIRAL-BASED DOWNSCALING METHOD FOR GENERATING 30 M TIME SERIES IMAGE DATA\n\nDirectory of Open Access Journals (Sweden)\n\nB. Liu\n\n2017-09-01\n\nFull Text Available The spatial detail and updating frequency of land cover data are important factors influencing land surface dynamic monitoring applications in high spatial resolution scale. However, the fragmentized patches and seasonal variable of some land cover types (e. g. small crop field, wetland make it labor-intensive and difficult in the generation of land cover data. Utilizing the high spatial resolution multi-temporal image data is a possible solution. Unfortunately, the spatial and temporal resolution of available remote sensing data like Landsat or MODIS datasets can hardly satisfy the minimum mapping unit and frequency of current land cover mapping / updating at the same time. The generation of high resolution time series may be a compromise to cover the shortage in land cover updating process. One of popular way is to downscale multi-temporal MODIS data with other high spatial resolution auxiliary data like Landsat. But the usual manner of downscaling pixel based on a window may lead to the underdetermined problem in heterogeneous area, result in the uncertainty of some high spatial resolution pixels. Therefore, the downscaled multi-temporal data can hardly reach high spatial resolution as Landsat data. A spiral based method was introduced to downscale low spatial and high temporal resolution image data to high spatial and high temporal resolution image data. By the way of searching the similar pixels around the adjacent region based on the spiral, the pixel set was made up in the adjacent region pixel by pixel. The underdetermined problem is prevented to a large extent from solving the linear system when adopting the pixel set constructed. With the help of ordinary least squares, the method inverted the endmember values of linear system. The high spatial resolution image was reconstructed on the basis of high spatial resolution class map and the endmember values band by band. Then, the high spatial resolution time series was formed with these\n\nTransferability in the future climate of a statistical downscaling method for precipitation in France\n\nScience.gov (United States)\n\nDayon, G.; BoÃ©, J.; Martin, E.\n\n2015-02-01\n\nA statistical downscaling approach for precipitation in France based on the analog method and its evaluation for different combinations of predictors is described, with focus on the transferability of the method to the future climate. First, the realism of downscaled present-day precipitation climatology and interannual variability for different combinations of predictors from four reanalyses is assessed. Satisfactory results are obtained, but elaborated predictors do not lead to major and consistent across-reanalyses improvements. The downscaling method is then evaluated on its capacity to capture precipitation trends in the last decades. As uncertainties in downscaled trends due to the choice of the reanalysis are large and observed trends are weak, this analysis does not lead to strong conclusions on the applicability of the method to a changing climate. The temporal transferability is then assessed thanks to a perfect model framework. The statistical downscaling relationship is built using present-day predictors and precipitation simulated by 12 regional climate models. The entire projections are then downscaled, and future downscaled and simulated precipitation changes are compared. A good temporal transferability is obtained only with a specific combination of predictors. Finally, the regional climate models are downscaled, thanks to the relationship built with reanalyses and observations, for the best combination of predictors. Results are similar to the changes simulated by the models, which reinforces our confidence in the realism of the models and of the downscaling method. Uncertainties in precipitation change due to reanalyses are found to be limited compared to those due to regional simulations.\n\nHydrologic extremes - an intercomparison of multiple gridded statistical downscaling methods\n\nScience.gov (United States)\n\nWerner, Arelia T.; Cannon, Alex J.\n\n2016-04-01\n\nGridded statistical downscaling methods are the main means of preparing climate model data to drive distributed hydrological models. Past work on the validation of climate downscaling methods has focused on temperature and precipitation, with less attention paid to the ultimate outputs from hydrological models. Also, as attention shifts towards projections of extreme events, downscaling comparisons now commonly assess methods in terms of climate extremes, but hydrologic extremes are less well explored. Here, we test the ability of gridded downscaling models to replicate historical properties of climate and hydrologic extremes, as measured in terms of temporal sequencing (i.e. correlation tests) and distributional properties (i.e. tests for equality of probability distributions). Outputs from seven downscaling methods - bias correction constructed analogues (BCCA), double BCCA (DBCCA), BCCA with quantile mapping reordering (BCCAQ), bias correction spatial disaggregation (BCSD), BCSD using minimum/maximum temperature (BCSDX), the climate imprint delta method (CI), and bias corrected CI (BCCI) - are used to drive the Variable Infiltration Capacity (VIC) model over the snow-dominated Peace River basin, British Columbia. Outputs are tested using split-sample validation on 26 climate extremes indices (ClimDEX) and two hydrologic extremes indices (3-day peak flow and 7-day peak flow). To characterize observational uncertainty, four atmospheric reanalyses are used as climate model surrogates and two gridded observational data sets are used as downscaling target data. The skill of the downscaling methods generally depended on reanalysis and gridded observational data set. However, CI failed to reproduce the distribution and BCSD and BCSDX the timing of winter 7-day low-flow events, regardless of reanalysis or observational data set. Overall, DBCCA passed the greatest number of tests for the ClimDEX indices, while BCCAQ, which is designed to more accurately resolve event\n\nActor groups, related needs, and challenges at the climate downscaling interface\n\nScience.gov (United States)\n\nRÃ¶ssler, Ole; Benestad, Rasmus; Diamando, Vlachogannis; Heike, HÃ¼bener; Kanamaru, Hideki; PagÃ©, Christian; Margarida Cardoso, Rita; Soares, Pedro; Maraun, Douglas; Kreienkamp, Frank; Christodoulides, Paul; Fischer, Andreas; Szabo, Peter\n\n2016-04-01\n\nAt the climate downscaling interface, numerous downscaling techniques and different philosophies compete on being the best method in their specific terms. Thereby, it remains unclear to what extent and for which purpose these downscaling techniques are valid or even the most appropriate choice. A common validation framework that compares all the different available methods was missing so far. The initiative VALUE closes this gap with such a common validation framework. An essential part of a validation framework for downscaling techniques is the definition of appropriate validation measures. The selection of validation measures should consider the needs of the stakeholder: some might need a temporal or spatial average of a certain variable, others might need temporal or spatial distributions of some variables, still others might need extremes for the variables of interest or even inter-variable dependencies. Hence, a close interaction of climate data providers and climate data users is necessary. Thus, the challenge in formulating a common validation framework mirrors also the challenges between the climate data providers and the impact assessment community. This poster elaborates the issues and challenges at the downscaling interface as it is seen within the VALUE community. It suggests three different actor groups: one group consisting of the climate data providers, the other two groups being climate data users (impact modellers and societal users). Hence, the downscaling interface faces classical transdisciplinary challenges. We depict a graphical illustration of actors involved and their interactions. In addition, we identified four different types of issues that need to be considered: i.e. data based, knowledge based, communication based, and structural issues. They all may, individually or jointly, hinder an optimal exchange of data and information between the actor groups at the downscaling interface. Finally, some possible ways to tackle these issues are\n\nOn the downscaling of actual evapotranspiration maps based on combination of MODIS and landsat-based actual evapotranspiration estimates\n\nScience.gov (United States)\n\nSingh, Ramesh K.; Senay, Gabriel B.; Velpuri, Naga Manohar; Bohms, Stefanie; Verdin, James P.\n\n2014-01-01\n\nÂ Downscaling is one of the important ways of utilizing the combined benefits of the high temporal resolution of Moderate Resolution Imaging Spectroradiometer (MODIS) images and fine spatial resolution of Landsat images. We have evaluated the output regression with intercept method and developed the Linear with Zero Intercept (LinZI) method for downscaling MODIS-based monthly actual evapotranspiration (AET) maps to the Landsat-scale monthly AET maps for the Colorado River Basin for 2010. We used the 8-day MODIS land surface temperature product (MOD11A2) and 328 cloud-free Landsat images for computing AET maps and downscaling. The regression with intercept method does have limitations in downscaling if the slope and intercept are computed over a large area. A good agreement was obtained between downscaled monthly AET using the LinZI method and the eddy covariance measurements from seven flux sites within the Colorado River Basin. The mean bias ranged from â16 mm (underestimation) to 22 mm (overestimation) per month, and the coefficient of determination varied from 0.52 to 0.88. Some discrepancies between measured and downscaled monthly AET at two flux sites were found to be due to the prevailing flux footprint. A reasonable comparison was also obtained between downscaled monthly AET using LinZI method and the gridded FLUXNET dataset. The downscaled monthly AET nicely captured the temporal variation in sampled land cover classes. The proposed LinZI method can be used at finer temporal resolution (such as 8 days) with further evaluation. The proposed downscaling method will be very useful in advancing the application of remotely sensed images in water resources planning and management.\n\nDownscaling Meteosat Land Surface Temperature over a Heterogeneous Landscape Using a Data Assimilation Approach\n\nDirectory of Open Access Journals (Sweden)\n\nRihab Mechri\n\n2016-07-01\n\nFull Text Available A wide range of environmental applications require the monitoring of land surface temperature (LST at frequent intervals and fine spatial resolutions, but these conditions are not offered nowadays by the available space sensors. To overcome these shortcomings, LST downscaling methods have been developed to derive higher resolution LST from the available satellite data. This research concerns the application of a data assimilation (DA downscaling approach, the genetic particle smoother (GPS, to disaggregate Meteosat 8 LST time series (3 km Ã 5 km at finer spatial resolutions. The methodology was applied over the Crau-Camargue region in Southeastern France for seven months in 2009. The evaluation of the downscaled LSTs has been performed at a moderate resolution using a set of coincident clear-sky MODIS LST images from Aqua and Terra platforms (1 km Ã 1 km and at a higher resolution using Landsat 7 data (60 m Ã 60 m. The performance of the downscaling has been assessed in terms of reduction of the biases and the root mean square errors (RMSE compared to prior model-simulated LSTs. The results showed that GPS allows downscaling the Meteosat LST product from 3 Ã 5 km2 to 1 Ã 1 km2 scales with a RMSE less than 2.7 K. Finer scale downscaling at Landsat 7 resolution showed larger errors (RMSE around 5 K explained by land cover errors and inter-calibration issues between sensors. Further methodology improvements are finally suggested.\n\nUncertainty Assessment of the NASA Earth Exchange Global Daily Downscaled Climate Projections (NEX-GDDP) Dataset\n\nScience.gov (United States)\n\nWang, Weile; Nemani, Ramakrishna R.; Michaelis, Andrew; Hashimoto, Hirofumi; Dungan, Jennifer L.; Thrasher, Bridget L.; Dixon, Keith W.\n\n2016-01-01\n\nThe NASA Earth Exchange Global Daily Downscaled Projections (NEX-GDDP) dataset is comprised of downscaled climate projections that are derived from 21 General Circulation Model (GCM) runs conducted under the Coupled Model Intercomparison Project Phase 5 (CMIP5) and across two of the four greenhouse gas emissions scenarios (RCP4.5 and RCP8.5). Each of the climate projections includes daily maximum temperature, minimum temperature, and precipitation for the periods from 1950 through 2100 and the spatial resolution is 0.25 degrees (approximately 25 km x 25 km). The GDDP dataset has received warm welcome from the science community in conducting studies of climate change impacts at local to regional scales, but a comprehensive evaluation of its uncertainties is still missing. In this study, we apply the Perfect Model Experiment framework (Dixon et al. 2016) to quantify the key sources of uncertainties from the observational baseline dataset, the downscaling algorithm, and some intrinsic assumptions (e.g., the stationary assumption) inherent to the statistical downscaling techniques. We developed a set of metrics to evaluate downscaling errors resulted from bias-correction (\"quantile-mapping\"), spatial disaggregation, as well as the temporal-spatial non-stationarity of climate variability. Our results highlight the spatial disaggregation (or interpolation) errors, which dominate the overall uncertainties of the GDDP dataset, especially over heterogeneous and complex terrains (e.g., mountains and coastal area). In comparison, the temporal errors in the GDDP dataset tend to be more constrained. Our results also indicate that the downscaled daily precipitation also has relatively larger uncertainties than the temperature fields, reflecting the rather stochastic nature of precipitation in space. Therefore, our results provide insights in improving statistical downscaling algorithms and products in the future.\n\nStatistical downscaling of precipitation using long short-term memory recurrent neural networks\n\nScience.gov (United States)\n\nMisra, Saptarshi; Sarkar, Sudeshna; Mitra, Pabitra\n\n2017-11-01\n\nHydrological impacts of global climate change on regional scale are generally assessed by downscaling large-scale climatic variables, simulated by General Circulation Models (GCMs), to regional, small-scale hydrometeorological variables like precipitation, temperature, etc. In this study, we propose a new statistical downscaling model based on Recurrent Neural Network with Long Short-Term Memory which captures the spatio-temporal dependencies in local rainfall. The previous studies have used several other methods such as linear regression, quantile regression, kernel regression, beta regression, and artificial neural networks. Deep neural networks and recurrent neural networks have been shown to be highly promising in modeling complex and highly non-linear relationships between input and output variables in different domains and hence we investigated their performance in the task of statistical downscaling. We have tested this model on two datasetsâone on precipitation in Mahanadi basin in India and the second on precipitation in Campbell River basin in Canada. Our autoencoder coupled long short-term memory recurrent neural network model performs the best compared to other existing methods on both the datasets with respect to temporal cross-correlation, mean squared error, and capturing the extremes.\n\nDownscaling of MODIS One Kilometer Evapotranspiration Using Landsat-8 Data and Machine Learning Approaches\n\nDirectory of Open Access Journals (Sweden)\n\nYinghai Ke\n\n2016-03-01\n\nFull Text Available This study presented a MODIS 8-day 1 km evapotranspiration (ET downscaling method based on Landsat 8 data (30 m and machine learning approaches. Eleven indicators including albedo, land surface temperature (LST, and vegetation indices (VIs derived from Landsat 8 data were first upscaled to 1 km resolution. Machine learning algorithms including Support Vector Regression (SVR, Cubist, and Random Forest (RF were used to model the relationship between the Landsat indicators and MODIS 8-day 1 km ET. The models were then used to predict 30 m ET based on Landsat 8 indicators. A total of thirty-two pairs of Landsat 8 images/MODIS ET data were evaluated at four study sites including two in United States and two in South Korea. Among the three models, RF produced the lowest error, with relative Root Mean Square Error (rRMSE less than 20%. Vegetation greenness related indicators such as Normalized Difference Vegetation Index (NDVI, Enhanced Vegetation Index (EVI, Soil Adjusted Vegetation Index (SAVI, and vegetation moisture related indicators such as Normalized Difference Infrared IndexâLandsat 8 OLI band 7 (NDIIb7 and Normalized Difference Water Index (NDWI were the five most important features used in RF model. Temperature-based indicators were less important than vegetation greenness and moisture-related indicators because LST could have considerable variation during each 8-day period. The predicted Landsat downscaled ET had good overall agreement with MODIS ET (average rRMSE = 22% and showed a similar temporal trend as MODIS ET. Compared to the MODIS ET product, the downscaled product demonstrated more spatial details, and had better agreement with in situ ET observations (R2 = 0.56. However, we found that the accuracy of MODIS ET was the main control factor of the accuracy of the downscaled product. Improved coarse-resolution ET estimation would result in better finer-resolution estimation. This study proved the potential of using machine learning\n\nStatistical downscaling based on dynamically downscaled predictors: Application to monthly precipitation in Sweden\n\nScience.gov (United States)\n\nHellstrÃ¶m, Cecilia; Chen, Deliang\n\n2003-11-01\n\nA prerequisite of a successful statistical downscaling is that large-scale predictors simulated by the General Circulation Model (GCM) must be realistic. It is assumed here that features smaller than the GCM resolution are important in determining the realism of the large-scale predictors. It is tested whether a three-step method can improve conventional one-step statistical downscaling. The method uses predictors that are upscaled from a dynamical downscaling instead of predictors taken directly from a GCM simulation. The method is applied to downscaling of monthly precipitation in Sweden. The statistical model used is a multiple regression model that uses indices of large-scale atmospheric circulation and 850-hPa specific humidity as predictors. Data from two GCMs (HadCM2 and ECHAM4) and two RCM experiments of the Rossby Centre model (RCA1) driven by the GCMs are used. It is found that upscaled RCA1 predictors capture the seasonal cycle better than those from the GCMs, and hence increase the reliability of the downscaled precipitation. However, there are only slight improvements in the simulation of the seasonal cycle of downscaled precipitation. Due to the cost of the method and the limited improvements in the downscaling results, the three-step method is not justified to replace the one-step method for downscaling of Swedish precipitation.\n\nMultisite rainfall downscaling and disaggregation in a tropical urban area\n\nScience.gov (United States)\n\nLu, Y.; Qin, X. S.\n\n2014-02-01\n\nA systematic downscaling-disaggregation study was conducted over Singapore Island, with an aim to generate high spatial and temporal resolution rainfall data under future climate-change conditions. The study consisted of two major components. The first part was to perform an inter-comparison of various alternatives of downscaling and disaggregation methods based on observed data. This included (i) single-site generalized linear model (GLM) plus K-nearest neighbor (KNN) (S-G-K) vs. multisite GLM (M-G) for spatial downscaling, (ii) HYETOS vs. KNN for single-site disaggregation, and (iii) KNN vs. MuDRain (Multivariate Rainfall Disaggregation tool) for multisite disaggregation. The results revealed that, for multisite downscaling, M-G performs better than S-G-K in covering the observed data with a lower RMSE value; for single-site disaggregation, KNN could better keep the basic statistics (i.e. standard deviation, lag-1 autocorrelation and probability of wet hour) than HYETOS; for multisite disaggregation, MuDRain outperformed KNN in fitting interstation correlations. In the second part of the study, an integrated downscaling-disaggregation framework based on M-G, KNN, and MuDRain was used to generate hourly rainfall at multiple sites. The results indicated that the downscaled and disaggregated rainfall data based on multiple ensembles from HadCM3 for the period from 1980 to 2010 could well cover the observed mean rainfall amount and extreme data, and also reasonably keep the spatial correlations both at daily and hourly timescales. The framework was also used to project future rainfall conditions under HadCM3 SRES A2 and B2 scenarios. It was indicated that the annual rainfall amount could reduce up to 5% at the end of this century, but the rainfall of wet season and extreme hourly rainfall could notably increase.\n\nUsing Random Forest to Improve the Downscaling of Global Livestock Census Data\n\nScience.gov (United States)\n\nNicolas, GaÃ«lle; Robinson, Timothy P.; Wint, G. R. William; Conchedda, Giulia; Cinardi, Giuseppina; Gilbert, Marius\n\n2016-01-01\n\nLarge scale, high-resolution global data on farm animal distributions are essential for spatially explicit assessments of the epidemiological, environmental and socio-economic impacts of the livestock sector. This has been the major motivation behind the development of the Gridded Livestock of the World (GLW) database, which has been extensively used since its first publication in 2007. The database relies on a downscaling methodology whereby census counts of animals in sub-national administrative units are redistributed at the level of grid cells as a function of a series of spatial covariates. The recent upgrade of GLW1 to GLW2 involved automating the processing, improvement of input data, and downscaling at a spatial resolution of 1 km per cell (5 km per cell in the earlier version). The underlying statistical methodology, however, remained unchanged. In this paper, we evaluate new methods to downscale census data with a higher accuracy and increased processing efficiency. Two main factors were evaluated, based on sample census datasets of cattle in Africa and chickens in Asia. First, we implemented and evaluated Random Forest models (RF) instead of stratified regressions. Second, we investigated whether models that predicted the number of animals per rural person (per capita) could provide better downscaled estimates than the previous approach that predicted absolute densities (animals per km2). RF models consistently provided better predictions than the stratified regressions for both continents and species. The benefit of per capita over absolute density models varied according to the species and continent. In addition, different technical options were evaluated to reduce the processing time while maintaining their predictive power. Future GLW runs (GLW 3.0) will apply the new RF methodology with optimized modelling options. The potential benefit of per capita models will need to be further investigated with a better distinction between rural and agricultural\n\nUsing Random Forest to Improve the Downscaling of Global Livestock Census Data.\n\nDirectory of Open Access Journals (Sweden)\n\nGaÃ«lle Nicolas\n\nFull Text Available Large scale, high-resolution global data on farm animal distributions are essential for spatially explicit assessments of the epidemiological, environmental and socio-economic impacts of the livestock sector. This has been the major motivation behind the development of the Gridded Livestock of the World (GLW database, which has been extensively used since its first publication in 2007. The database relies on a downscaling methodology whereby census counts of animals in sub-national administrative units are redistributed at the level of grid cells as a function of a series of spatial covariates. The recent upgrade of GLW1 to GLW2 involved automating the processing, improvement of input data, and downscaling at a spatial resolution of 1 km per cell (5 km per cell in the earlier version. The underlying statistical methodology, however, remained unchanged. In this paper, we evaluate new methods to downscale census data with a higher accuracy and increased processing efficiency. Two main factors were evaluated, based on sample census datasets of cattle in Africa and chickens in Asia. First, we implemented and evaluated Random Forest models (RF instead of stratified regressions. Second, we investigated whether models that predicted the number of animals per rural person (per capita could provide better downscaled estimates than the previous approach that predicted absolute densities (animals per km2. RF models consistently provided better predictions than the stratified regressions for both continents and species. The benefit of per capita over absolute density models varied according to the species and continent. In addition, different technical options were evaluated to reduce the processing time while maintaining their predictive power. Future GLW runs (GLW 3.0 will apply the new RF methodology with optimized modelling options. The potential benefit of per capita models will need to be further investigated with a better distinction between rural\n\nNorth Atlantic atmospheric circulation and surface wind in the Northeast of the Iberian Peninsula: uncertainty and long term downscaled variability\n\nEnergy Technology Data Exchange (ETDEWEB)\n\nGarcia-Bustamante, E.; Jimenez, P.A. [CIEMAT, Departamento de Energias Renovables, Madrid (Spain); Universidad Complutense de Madrid, Departamento de Astrofisica y CC. de la Atmosfera, Madrid (Spain); Gonzalez-Rouco, J.F. [Universidad Complutense de Madrid, Departamento de Astrofisica y CC. de la Atmosfera, Madrid (Spain); Navarro, J. [CIEMAT, Departamento de Energias Renovables, Madrid (Spain); Xoplaki, E. [University of Bern, Institute of Geography and Oeschger Centre for Climate Change Research, Bern (Switzerland); Montavez, J.P. [Universidad de Murcia, Departamento de Fisica, Murcia (Spain)\n\n2012-01-15\n\nThe variability and predictability of the surface wind field at the regional scale is explored over a complex terrain region in the northeastern Iberian Peninsula by means of a downscaling technique based on Canonical Correlation Analysis. More than a decade of observations (1992-2005) allows for calibrating and validating a statistical method that elicits the main associations between the large scale atmospheric circulation over the North Atlantic and Mediterranean areas and the regional wind field. In an initial step the downscaling model is designed by selecting parameter values from practise. To a large extent, the variability of the wind at monthly timescales is found to be governed by the large scale circulation modulated by the particular orographic features of the area. The sensitivity of the downscaling methodology to the selection of the model parameter values is explored, in a second step, by performing a systematic sampling of the parameters space, avoiding a heuristic selection. This provides a metric for the uncertainty associated with the various possible model configurations. The uncertainties associated with the model configuration are considerably dependent on the spatial variability of the wind. While the sampling of the parameters space in the model set up moderately impact estimations during the calibration period, the regional wind variability is very sensitive to the parameters selection at longer timescales. This fact illustrates that downscaling exercises based on a single configuration of parameters should be interpreted with extreme caution. The downscaling model is used to extend the estimations several centuries to the past using long datasets of sea level pressure, thereby illustrating the large temporal variability of the regional wind field from interannual to multicentennial timescales. The analysis does not evidence long term trends throughout the twentieth century, however anomalous episodes of high/low wind speeds are identified\n\nStatistical Downscaling of Gusts During Extreme European Winter Storms Using Radial-Basis-Function Networks\n\nScience.gov (United States)\n\nVoigt, M.; Lorenz, P.; Kruschke, T.; Osinski, R.; Ulbrich, U.; Leckebusch, G. C.\n\n2012-04-01\n\nWinterstorms and related gusts can cause extensive socio-economic damages. Knowledge about the occurrence and the small scale structure of such events may help to make regional estimations of storm losses. For a high spatial and temporal representation, the use of dynamical downscaling methods (RCM) is a cost-intensive and time-consuming option and therefore only applicable for a limited number of events. The current study explores a methodology to provide a statistical downscaling, which offers small scale structured gust fields from an extended large scale structured eventset. Radial-basis-function (RBF) networks in combination with bidirectional Kohonen (BDK) maps are used to generate the gustfields on a spatial resolution of 7 km from the 6-hourly mean sea level pressure field from ECMWF reanalysis data. BDK maps are a kind of neural network which handles supervised classification problems. In this study they are used to provide prototypes for the RBF network and give a first order approximation for the output data. A further interpolation is done by the RBF network. For the training process the 50 most extreme storm events over the North Atlantic area from 1957 to 2011 are used, which have been selected from ECMWF reanalysis datasets ERA40 and ERA-Interim by an objective wind based tracking algorithm. These events were downscaled dynamically by application of the DWD model chain GME â COSMO-EU. Different model parameters and their influence on the quality of the generated high-resolution gustfields are studied. It is shown that the statistical RBF network approach delivers reasonable results in modeling the regional gust fields for untrained events.\n\nA review of downscaling procedures - a contribution to the research on climate change impacts at city scale\n\nScience.gov (United States)\n\nSmid, Marek; Costa, Ana; Pebesma, Edzer; Granell, Carlos; Bhattacharya, Devanjan\n\n2016-04-01\n\nHuman kind is currently predominantly urban based, and the majority of ever continuing population growth will take place in urban agglomerations. Urban systems are not only major drivers of climate change, but also the impact hot spots. Furthermore, climate change impacts are commonly managed at city scale. Therefore, assessing climate change impacts on urban systems is a very relevant subject of research. Climate and its impacts on all levels (local, meso and global scale) and also the inter-scale dependencies of those processes should be a subject to detail analysis. While global and regional projections of future climate are currently available, local-scale information is lacking. Hence, statistical downscaling methodologies represent a potentially efficient way to help to close this gap. In general, the methodological reviews of downscaling procedures cover the various methods according to their application (e.g. downscaling for the hydrological modelling). Some of the most recent and comprehensive studies, such as the ESSEM COST Action ES1102 (VALUE), use the concept of Perfect Prog and MOS. Other examples of classification schemes of downscaling techniques consider three main categories: linear methods, weather classifications and weather generators. Downscaling and climate modelling represent a multidisciplinary field, where researchers from various backgrounds intersect their efforts, resulting in specific terminology, which may be somewhat confusing. For instance, the Polynomial Regression (also called the Surface Trend Analysis) is a statistical technique. In the context of the spatial interpolation procedures, it is commonly classified as a deterministic technique, and kriging approaches are classified as stochastic. Furthermore, the terms \"statistical\" and \"stochastic\" (frequently used as names of sub-classes in downscaling methodological reviews) are not always considered as synonymous, even though both terms could be seen as identical since they are\n\nPrecipitation projections under GCMs perspective and Turkish Water Foundation (TWF) statistical downscaling model procedures\n\nScience.gov (United States)\n\nDabanlÄ±, Ä°smail; Åen, Zekai\n\n2018-04-01\n\nThe statistical climate downscaling model by the Turkish Water Foundation (TWF) is further developed and applied to a set of monthly precipitation records. The model is structured by two phases as spatial (regional) and temporal downscaling of global circulation model (GCM) scenarios. The TWF model takes into consideration the regional dependence function (RDF) for spatial structure and Markov whitening process (MWP) for temporal characteristics of the records to set projections. The impact of climate change on monthly precipitations is studied by downscaling Intergovernmental Panel on Climate Change-Special Report on Emission Scenarios (IPCC-SRES) A2 and B2 emission scenarios from Max Plank Institute (EH40PYC) and Hadley Center (HadCM3). The main purposes are to explain the TWF statistical climate downscaling model procedures and to expose the validation tests, which are rewarded in same specifications as \"very good\" for all stations except one (Suhut) station in the Akarcay basin that is in the west central part of Turkey. Eventhough, the validation score is just a bit lower at the Suhut station, the results are \"satisfactory.\" It is, therefore, possible to say that the TWF model has reasonably acceptable skill for highly accurate estimation regarding standard deviation ratio (SDR), Nash-Sutcliffe efficiency (NSE), and percent bias (PBIAS) criteria. Based on the validated model, precipitation predictions are generated from 2011 to 2100 by using 30-year reference observation period (1981-2010). Precipitation arithmetic average and standard deviation have less than 5% error for EH40PYC and HadCM3 SRES (A2 and B2) scenarios.\n\nHigh-resolution downscaling for hydrological management\n\nScience.gov (United States)\n\nUlbrich, Uwe; Rust, Henning; Meredith, Edmund; Kpogo-Nuwoklo, Komlan; Vagenas, Christos\n\n2017-04-01\n\nHydrological modellers and water managers require high-resolution climate data to model regional hydrologies and how these may respond to future changes in the large-scale climate. The ability to successfully model such changes and, by extension, critical infrastructure planning is often impeded by a lack of suitable climate data. This typically takes the form of too-coarse data from climate models, which are not sufficiently detailed in either space or time to be able to support water management decisions and hydrological research. BINGO (Bringing INnovation in onGOing water management; ) aims to bridge the gap between the needs of hydrological modellers and planners, and the currently available range of climate data, with the overarching aim of providing adaptation strategies for climate change-related challenges. Producing the kilometre- and sub-daily-scale climate data needed by hydrologists through continuous simulations is generally computationally infeasible. To circumvent this hurdle, we adopt a two-pronged approach involving (1) selective dynamical downscaling and (2) conditional stochastic weather generators, with the former presented here. We take an event-based approach to downscaling in order to achieve the kilometre-scale input needed by hydrological modellers. Computational expenses are minimized by identifying extremal weather patterns for each BINGO research site in lower-resolution simulations and then only downscaling to the kilometre-scale (convection permitting) those events during which such patterns occur. Here we (1) outline the methodology behind the selection of the events, and (2) compare the modelled precipitation distribution and variability (preconditioned on the extremal weather patterns) with that found in observations.\n\nDownscaled and debiased climate simulations for North America from 21,000 years ago to 2100AD.\n\nScience.gov (United States)\n\nLorenz, David J; Nieto-Lugilde, Diego; Blois, Jessica L; Fitzpatrick, Matthew C; Williams, John W\n\n2016-07-05\n\nIncreasingly, ecological modellers are integrating paleodata with future projections to understand climate-driven biodiversity dynamics from the past through the current century. Climate simulations from earth system models are necessary to this effort, but must be debiased and downscaled before they can be used by ecological models. Downscaling methods and observational baselines vary among researchers, which produces confounding biases among downscaled climate simulations. We present unified datasets of debiased and downscaled climate simulations for North America from 21âka BP to 2100AD, at 0.5Â° spatial resolution. Temporal resolution is decadal averages of monthly data until 1950AD, average climates for 1950-2005 AD, and monthly data from 2010 to 2100AD, with decadal averages also provided. This downscaling includes two transient paleoclimatic simulations and 12 climate models for the IPCC AR5 (CMIP5) historical (1850-2005), RCP4.5, and RCP8.5 21st-century scenarios. Climate variables include primary variables and derived bioclimatic variables. These datasets provide a common set of climate simulations suitable for seamlessly modelling the effects of past and future climate change on species distributions and diversity.\n\nDownscaling global precipitation for local applications - a case for the Rhine basin\n\nScience.gov (United States)\n\nSperna Weiland, Frederiek; van Verseveld, Willem; Schellekens, Jaap\n\n2017-04-01\n\nWithin the EU FP7 project eartH2Observe a global Water Resources Re-analysis (WRR) is being developed. This re-analysis consists of meteorological and hydrological water balance variables with global coverage, spanning the period 1979-2014 at 0.25 degrees resolution (Schellekens et al., 2016). The dataset can be of special interest in regions with limited in-situ data availability, yet for local scale analysis particularly in mountainous regions, a resolution of 0.25 degrees may be too coarse and downscaling the data to a higher resolution may be required. A downscaling toolbox has been made that includes spatial downscaling of precipitation based on the global WorldClim dataset that is available at 1 km resolution as a monthly climatology (Hijmans et al., 2005). The input of the down-scaling tool are either the global eartH2Observe WRR1 and WRR2 datasets based on the WFDEI correction methodology (Weedon et al., 2014) or the global Multi-Source Weighted-Ensemble Precipitation (MSWEP) dataset (Beck et al., 2016). Here we present a validation of the datasets over the Rhine catchment by means of a distributed hydrological model (wflow, Schellekens et al., 2014) using a number of precipitation scenarios. (1) We start by running the model using the local reference dataset derived by spatial interpolation of gauge observations. Furthermore we use (2) the MSWEP dataset at the native 0.25-degree resolution followed by (3) MSWEP downscaled with the WorldClim dataset and final (4) MSWEP downscaled with the local reference dataset. The validation will be based on comparison of the modeled river discharges as well as rainfall statistics. We expect that down-scaling the MSWEP dataset with the WorldClim data to higher resolution will increase its performance. To test the performance of the down-scaling routine we have added a run with MSWEP data down-scaled with the local dataset and compare this with the run based on the local dataset itself. - Beck, H. E. et al., 2016. MSWEP\n\nExtended-Range High-Resolution Dynamical Downscaling over a Continental-Scale Domain\n\nScience.gov (United States)\n\nHusain, S. Z.; Separovic, L.; Yu, W.; Fernig, D.\n\n2014-12-01\n\nHigh-resolution mesoscale simulations, when applied for downscaling meteorological fields over large spatial domains and for extended time periods, can provide valuable information for many practical application scenarios including the weather-dependent renewable energy industry. In the present study, a strategy has been proposed to dynamically downscale coarse-resolution meteorological fields from Environment Canada's regional analyses for a period of multiple years over the entire Canadian territory. The study demonstrates that a continuous mesoscale simulation over the entire domain is the most suitable approach in this regard. Large-scale deviations in the different meteorological fields pose the biggest challenge for extended-range simulations over continental scale domains, and the enforcement of the lateral boundary conditions is not sufficient to restrict such deviations. A scheme has therefore been developed to spectrally nudge the simulated high-resolution meteorological fields at the different model vertical levels towards those embedded in the coarse-resolution driving fields derived from the regional analyses. A series of experiments were carried out to determine the optimal nudging strategy including the appropriate nudging length scales, nudging vertical profile and temporal relaxation. A forcing strategy based on grid nudging of the different surface fields, including surface temperature, soil-moisture, and snow conditions, towards their expected values obtained from a high-resolution offline surface scheme was also devised to limit any considerable deviation in the evolving surface fields due to extended-range temporal integrations. The study shows that ensuring large-scale atmospheric similarities helps to deliver near-surface statistical scores for temperature, dew point temperature and horizontal wind speed that are better or comparable to the operational regional forecasts issued by Environment Canada. Furthermore, the meteorological fields\n\nImproving GEFS Weather Forecasts for Indian Monsoon with Statistical Downscaling\n\nScience.gov (United States)\n\nAgrawal, Ankita; Salvi, Kaustubh; Ghosh, Subimal\n\n2014-05-01\n\nWeather forecast has always been a challenging research problem, yet of a paramount importance as it serves the role of 'key input' in formulating modus operandi for immediate future. Short range rainfall forecasts influence a wide range of entities, right from agricultural industry to a common man. Accurate forecasts actually help in minimizing the possible damage by implementing pre-decided plan of action and hence it is necessary to gauge the quality of forecasts which might vary with the complexity of weather state and regional parameters. Indian Summer Monsoon Rainfall (ISMR) is one such perfect arena to check the quality of weather forecast not only because of the level of intricacy in spatial and temporal patterns associated with it, but also the amount of damage it can cause (because of poor forecasts) to the Indian economy by affecting agriculture Industry. The present study is undertaken with the rationales of assessing, the ability of Global Ensemble Forecast System (GEFS) in predicting ISMR over central India and the skill of statistical downscaling technique in adding value to the predictions by taking them closer to evidentiary target dataset. GEFS is a global numerical weather prediction system providing the forecast results of different climate variables at a fine resolution (0.5 degree and 1 degree). GEFS shows good skills in predicting different climatic variables but fails miserably over rainfall predictions for Indian summer monsoon rainfall, which is evident from a very low to negative correlation values between predicted and observed rainfall. Towards the fulfilment of second rationale, the statistical relationship is established between the reasonably well predicted climate variables (GEFS) and observed rainfall. The GEFS predictors are treated with multicollinearity and dimensionality reduction techniques, such as principal component analysis (PCA) and least absolute shrinkage and selection operator (LASSO). Statistical relationship is\n\nA hybrid downscaling procedure for estimating the vertical distribution of ambient temperature in local scale\n\nScience.gov (United States)\n\nYiannikopoulou, I.; Philippopoulos, K.; Deligiorgi, D.\n\n2012-04-01\n\nThe vertical thermal structure of the atmosphere is defined by a combination of dynamic and radiation transfer processes and plays an important role in describing the meteorological conditions at local scales. The scope of this work is to develop and quantify the predictive ability of a hybrid dynamic-statistical downscaling procedure to estimate the vertical profile of ambient temperature at finer spatial scales. The study focuses on the warm period of the year (June - August) and the method is applied to an urban coastal site (Hellinikon), located in eastern Mediterranean. The two-step methodology initially involves the dynamic downscaling of coarse resolution climate data via the RegCM4.0 regional climate model and subsequently the statistical downscaling of the modeled outputs by developing and training site-specific artificial neural networks (ANN). The 2.5ox2.5o gridded NCEP-DOE Reanalysis 2 dataset is used as initial and boundary conditions for the dynamic downscaling element of the methodology, which enhances the regional representivity of the dataset to 20km and provides modeled fields in 18 vertical levels. The regional climate modeling results are compared versus the upper-air Hellinikon radiosonde observations and the mean absolute error (MAE) is calculated between the four grid point values nearest to the station and the ambient temperature at the standard and significant pressure levels. The statistical downscaling element of the methodology consists of an ensemble of ANN models, one for each pressure level, which are trained separately and employ the regional scale RegCM4.0 output. The ANN models are theoretically capable of estimating any measurable input-output function to any desired degree of accuracy. In this study they are used as non-linear function approximators for identifying the relationship between a number of predictor variables and the ambient temperature at the various vertical levels. An insight of the statistically derived input\n\nStatistical Downscaling and Bias Correction of Climate Model Outputs for Climate Change Impact Assessment in the U.S. Northeast\n\nScience.gov (United States)\n\nAhmed, Kazi Farzan; Wang, Guiling; Silander, John; Wilson, Adam M.; Allen, Jenica M.; Horton, Radley; Anyah, Richard\n\n2013-01-01\n\nStatistical downscaling can be used to efficiently downscale a large number of General Circulation Model (GCM) outputs to a fine temporal and spatial scale. To facilitate regional impact assessments, this study statistically downscales (to 1/8deg spatial resolution) and corrects the bias of daily maximum and minimum temperature and daily precipitation data from six GCMs and four Regional Climate Models (RCMs) for the northeast United States (US) using the Statistical Downscaling and Bias Correction (SDBC) approach. Based on these downscaled data from multiple models, five extreme indices were analyzed for the future climate to quantify future changes of climate extremes. For a subset of models and indices, results based on raw and bias corrected model outputs for the present-day climate were compared with observations, which demonstrated that bias correction is important not only for GCM outputs, but also for RCM outputs. For future climate, bias correction led to a higher level of agreements among the models in predicting the magnitude and capturing the spatial pattern of the extreme climate indices. We found that the incorporation of dynamical downscaling as an intermediate step does not lead to considerable differences in the results of statistical downscaling for the study domain.\n\nPrecipitation Dynamical Downscaling Over the Great Plains\n\nScience.gov (United States)\n\nHu, Xiao-Ming; Xue, Ming; McPherson, Renee A.; Martin, Elinor; Rosendahl, Derek H.; Qiao, Lei\n\n2018-02-01\n\nDetailed, regional climate projections, particularly for precipitation, are critical for many applications. Accurate precipitation downscaling in the United States Great Plains remains a great challenge for most Regional Climate Models, particularly for warm months. Most previous dynamic downscaling simulations significantly underestimate warm-season precipitation in the region. This study aims to achieve a better precipitation downscaling in the Great Plains with the Weather Research and Forecast (WRF) model. To this end, WRF simulations with different physics schemes and nudging strategies are first conducted for a representative warm season. Results show that different cumulus schemes lead to more pronounced difference in simulated precipitation than other tested physics schemes. Simply choosing different physics schemes is not enough to alleviate the dry bias over the southern Great Plains, which is related to an anticyclonic circulation anomaly over the central and western parts of continental U.S. in the simulations. Spectral nudging emerges as an effective solution for alleviating the precipitation bias. Spectral nudging ensures that large and synoptic-scale circulations are faithfully reproduced while still allowing WRF to develop small-scale dynamics, thus effectively suppressing the large-scale circulation anomaly in the downscaling. As a result, a better precipitation downscaling is achieved. With the carefully validated configurations, WRF downscaling is conducted for 1980-2015. The downscaling captures well the spatial distribution of monthly climatology precipitation and the monthly/yearly variability, showing improvement over at least two previously published precipitation downscaling studies. With the improved precipitation downscaling, a better hydrological simulation over the trans-state Oologah watershed is also achieved.\n\nStatistical downscaling of rainfall: a non-stationary and multi-resolution approach\n\nScience.gov (United States)\n\nRashid, Md. Mamunur; Beecham, Simon; Chowdhury, Rezaul Kabir\n\n2016-05-01\n\nA novel downscaling technique is proposed in this study whereby the original rainfall and reanalysis variables are first decomposed by wavelet transforms and rainfall is modelled using the semi-parametric additive model formulation of Generalized Additive Model in Location, Scale and Shape (GAMLSS). The flexibility of the GAMLSS model makes it feasible as a framework for non-stationary modelling. Decomposition of a rainfall series into different components is useful to separate the scale-dependent properties of the rainfall as this varies both temporally and spatially. The study was conducted at the Onkaparinga river catchment in South Australia. The model was calibrated over the period 1960 to 1990 and validated over the period 1991 to 2010. The model reproduced the monthly variability and statistics of the observed rainfall well with Nash-Sutcliffe efficiency (NSE) values of 0.66 and 0.65 for the calibration and validation periods, respectively. It also reproduced well the seasonal rainfall over the calibration (NSE = 0.37) and validation (NSE = 0.69) periods for all seasons. The proposed model was better than the tradition modelling approach (application of GAMLSS to the original rainfall series without decomposition) at reproducing the time-frequency properties of the observed rainfall, and yet it still preserved the statistics produced by the traditional modelling approach. When downscaling models were developed with general circulation model (GCM) historical output datasets, the proposed wavelet-based downscaling model outperformed the traditional downscaling model in terms of reproducing monthly rainfall for both the calibration and validation periods.\n\nBioclim Deliverable D6b: application of statistical down-scaling within the BIOCLIM hierarchical strategy: methods, data requirements and underlying assumptions\n\nInternational Nuclear Information System (INIS)\n\n2004-01-01\n\nThe overall aim of BIOCLIM is to assess the possible long term impacts due to climate change on the safety of radioactive waste repositories in deep formations. The coarse spatial scale of the Earth-system Models of Intermediate Complexity (EMICs) used in BIOCLIM compared with the BIOCLIM study regions and the needs of performance assessment creates a need for down-scaling. Most of the developmental work on down-scaling methodologies undertaken by the international research community has focused on down-scaling from the general circulation model (GCM) scale (with a typical spatial resolution of 400 km by 400 km over Europe in the current generation of models) using dynamical down-scaling (i.e., regional climate models (RCMs), which typically have a spatial resolution of 50 km by 50 km for models whose domain covers the European region) or statistical methods (which can provide information at the point or station scale) in order to construct scenarios of anthropogenic climate change up to 2100. Dynamical down-scaling (with the MAR RCM) is used in BIOCLIM WP2 to down-scale from the GCM (i.e., IPSL C M4 D ) scale. In the original BIOCLIM description of work, it was proposed that UEA would apply statistical down-scaling to IPSL C M4 D output in WP2 as part of the hierarchical strategy. Statistical down-scaling requires the identification of statistical relationships between the observed large-scale and regional/local climate, which are then applied to large-scale GCM output, on the assumption that these relationships remain valid in the future (the assumption of stationarity). Thus it was proposed that UEA would investigate the extent to which it is possible to apply relationships between the present-day large-scale and regional/local climate to the relatively extreme conditions of the BIOCLIM WP2 snapshot simulations. Potential statistical down-scaling methodologies were identified from previous work performed at UEA. Appropriate station data from the case\n\nFusing MODIS with Landsat 8 data to downscale weekly normalized difference vegetation index estimates for central Great Basin rangelands, USA\n\nScience.gov (United States)\n\nBoyte, Stephen; Wylie, Bruce K.; Rigge, Matthew B.; Dahal, Devendra\n\n2018-01-01\n\nData fused from distinct but complementary satellite sensors mitigate tradeoffs that researchers make when selecting between spatial and temporal resolutions of remotely sensed data. We integrated data from the Moderate Resolution Imaging Spectroradiometer (MODIS) sensor aboard the Terra satellite and the Operational Land Imager sensor aboard the Landsat 8 satellite into four regression-tree models and applied those data to a mapping application. This application produced downscaled maps that utilize the 30-m spatial resolution of Landsat in conjunction with daily acquisitions of MODIS normalized difference vegetation index (NDVI) that are composited and temporally smoothed. We produced four weekly, atmospherically corrected, and nearly cloud-free, downscaled 30-m synthetic MODIS NDVI predictions (maps) built from these models. Model results were strong withÂ R2Â values ranging from 0.74 to 0.85. The correlation coefficients (rÂ â¥Â 0.89) were strong for all predictions when compared to corresponding original MODIS NDVI data. Downscaled products incorporated into independently developed sagebrush ecosystem models yielded mixed results. The visual quality of the downscaled 30-m synthetic MODIS NDVI predictions were remarkable when compared to the original 250-m MODIS NDVI. These 30-m maps improve knowledge of dynamic rangeland seasonal processes in the central Great Basin, United States, and provide land managers improved resource maps.\n\nImplications of the methodological choices for hydrologic portrayals of climate change over the contiguous United States: Statistically downscaled forcing data and hydrologic models\n\nScience.gov (United States)\n\nMizukami, Naoki; Clark, Martyn P.; Gutmann, Ethan D.; Mendoza, Pablo A.; Newman, Andrew J.; Nijssen, Bart; Livneh, Ben; Hay, Lauren E.; Arnold, Jeffrey R.; Brekke, Levi D.\n\n2016-01-01\n\nContinental-domain assessments of climate change impacts on water resources typically rely on statistically downscaled climate model outputs to force hydrologic models at a finer spatial resolution. This study examines the effects of four statistical downscaling methods [bias-corrected constructed analog (BCCA), bias-corrected spatial disaggregation applied at daily (BCSDd) and monthly scales (BCSDm), and asynchronous regression (AR)] on retrospective hydrologic simulations using three hydrologic models with their default parameters (the Community Land Model, version 4.0; the Variable Infiltration Capacity model, version 4.1.2; and the PrecipitationâRunoff Modeling System, version 3.0.4) over the contiguous United States (CONUS). Biases of hydrologic simulations forced by statistically downscaled climate data relative to the simulation with observation-based gridded data are presented. Each statistical downscaling method produces different meteorological portrayals including precipitation amount, wet-day frequency, and the energy input (i.e., shortwave radiation), and their interplay affects estimations of precipitation partitioning between evapotranspiration and runoff, extreme runoff, and hydrologic states (i.e., snow and soil moisture). The analyses show that BCCA underestimates annual precipitation by as much as â250 mm, leading to unreasonable hydrologic portrayals over the CONUS for all models. Although the other three statistical downscaling methods produce a comparable precipitation bias ranging from â10 to 8 mm across the CONUS, BCSDd severely overestimates the wet-day fraction by up to 0.25, leading to different precipitation partitioning compared to the simulations with other downscaled data. Overall, the choice of downscaling method contributes to less spread in runoff estimates (by a factor of 1.5â3) than the choice of hydrologic model with use of the default parameters if BCCA is excluded.\n\nMulti-Site and Multi-Variables Statistical Downscaling Technique in the Monsoon Dominated Region of Pakistan\n\nScience.gov (United States)\n\nKhan, Firdos; Pilz, JÃ¼rgen\n\n2016-04-01\n\nSouth Asia is under the severe impacts of changing climate and global warming. The last two decades showed that climate change or global warming is happening and the first decade of 21st century is considered as the warmest decade over Pakistan ever in history where temperature reached 53 0C in 2010. Consequently, the spatio-temporal distribution and intensity of precipitation is badly effected and causes floods, cyclones and hurricanes in the region which further have impacts on agriculture, water, health etc. To cope with the situation, it is important to conduct impact assessment studies and take adaptation and mitigation remedies. For impact assessment studies, we need climate variables at higher resolution. Downscaling techniques are used to produce climate variables at higher resolution; these techniques are broadly divided into two types, statistical downscaling and dynamical downscaling. The target location of this study is the monsoon dominated region of Pakistan. One reason for choosing this area is because the contribution of monsoon rains in this area is more than 80 % of the total rainfall. This study evaluates a statistical downscaling technique which can be then used for downscaling climatic variables. Two statistical techniques i.e. quantile regression and copula modeling are combined in order to produce realistic results for climate variables in the area under-study. To reduce the dimension of input data and deal with multicollinearity problems, empirical orthogonal functions will be used. Advantages of this new method are: (1) it is more robust to outliers as compared to ordinary least squares estimates and other estimation methods based on central tendency and dispersion measures; (2) it preserves the dependence among variables and among sites and (3) it can be used to combine different types of distributions. This is important in our case because we are dealing with climatic variables having different distributions over different meteorological\n\nIdentification of robust statistical downscaling methods based on a comprehensive suite of performance metrics for South Korea\n\nScience.gov (United States)\n\nEum, H. I.; Cannon, A. J.\n\n2015-12-01\n\nClimate models are a key provider to investigate impacts of projected future climate conditions on regional hydrologic systems. However, there is a considerable mismatch of spatial resolution between GCMs and regional applications, in particular a region characterized by complex terrain such as Korean peninsula. Therefore, a downscaling procedure is an essential to assess regional impacts of climate change. Numerous statistical downscaling methods have been used mainly due to the computational efficiency and simplicity. In this study, four statistical downscaling methods [Bias-Correction/Spatial Disaggregation (BCSD), Bias-Correction/Constructed Analogue (BCCA), Multivariate Adaptive Constructed Analogs (MACA), and Bias-Correction/Climate Imprint (BCCI)] are applied to downscale the latest Climate Forecast System Reanalysis data to stations for precipitation, maximum temperature, and minimum temperature over South Korea. By split sampling scheme, all methods are calibrated with observational station data for 19 years from 1973 to 1991 are and tested for the recent 19 years from 1992 to 2010. To assess skill of the downscaling methods, we construct a comprehensive suite of performance metrics that measure an ability of reproducing temporal correlation, distribution, spatial correlation, and extreme events. In addition, we employ Technique for Order of Preference by Similarity to Ideal Solution (TOPSIS) to identify robust statistical downscaling methods based on the performance metrics for each season. The results show that downscaling skill is considerably affected by the skill of CFSR and all methods lead to large improvements in representing all performance metrics. According to seasonal performance metrics evaluated, when TOPSIS is applied, MACA is identified as the most reliable and robust method for all variables and seasons. Note that such result is derived from CFSR output which is recognized as near perfect climate data in climate studies. Therefore, the\n\nMass Balance Modelling of Saskatchewan Glacier, Canada Using Empirically Downscaled Reanalysis Data\n\nScience.gov (United States)\n\nLarouche, O.; Kinnard, C.; Demuth, M. N.\n\n2017-12-01\n\nObservations show that glaciers around the world are retreating. As sites with long-term mass balance observations are scarce, models are needed to reconstruct glacier mass balance and assess its sensitivity to climate. In regions with discontinuous and/or sparse meteorological data, high-resolution climate reanalysis data provide a convenient alternative to in situ weather observations, but can also suffer from strong bias due to the spatial and temporal scale mismatch. In this study we used data from the North American Regional Reanalysis (NARR) project with a 30 x 30 km spatial resolution and 3-hour temporal resolution to produce the meteorological forcings needed to drive a physically-based, distributed glacier mass balance model (DEBAM, Hock and Holmgren 2005) for the historical period 1979-2016. A two-year record from an automatic weather station (AWS) operated on Saskatchewan Glacier (2014-2016) was used to downscale air temperature, relative humidity, wind speed and incoming solar radiation from the nearest NARR gridpoint to the glacier AWS site. An homogenized historical precipitation record was produced using data from two nearby, low-elevation weather stations and used to downscale the NARR precipitation data. Three bias correction methods were applied (scaling, delta and empirical quantile mapping - EQM) and evaluated using split sample cross-validation. The EQM method gave better results for precipitation and for air temperature. Only a slight improvement in the relative humidity was obtained using the scaling method, while none of the methods improved the wind speed. The later correlates poorly with AWS observations, probably because the local glacier wind is decoupled from the larger scale NARR wind field. The downscaled data was used to drive the DEBAM model in order to reconstruct the mass balance of Saskatchewan Glacier over the past 30 years. The model was validated using recent snow thickness measurements and previously published geodetic mass\n\nUser's Manual for Downscaler Fusion Software\n\nScience.gov (United States)\n\nRecently, a series of 3 papers has been published in the statistical literature that details the use of downscaling to obtain more accurate and precise predictions of air pollution across the conterminous U.S. This downscaling approach combines CMAQ gridded numerical model output...\n\nWave model downscaling for coastal applications\n\nScience.gov (United States)\n\nValchev, Nikolay; Davidan, Georgi; Trifonova, Ekaterina; Andreeva, Nataliya\n\n2010-05-01\n\nDownscaling is a suitable technique for obtaining high-resolution estimates from relatively coarse-resolution global models. Dynamical and statistical downscaling has been applied to the multidecadal simulations of ocean waves. Even as large-scale variability might be plausibly estimated from these simulations, their value for the small scale applications such as design of coastal protection structures and coastal risk assessment is limited due to their relatively coarse spatial and temporal resolutions. Another advantage of the high resolution wave modeling is that it accounts for shallow water effects. Therefore, it can be used for both wave forecasting at specific coastal locations and engineering applications that require knowledge about extreme wave statistics at or near the coastal facilities. In the present study downscaling is applied to both ECMWF and NCEP/NCAR global reanalysis of atmospheric pressure over the Black Sea with 2.5 degrees spatial resolution. A simplified regional atmospheric model is employed for calculation of the surface wind field at 0.5 degrees resolution that serves as forcing for the wave models. Further, a high-resolution nested WAM/SWAN wave model suite of nested wave models is applied for spatial downscaling. It aims at resolving the wave conditions in a limited area at the close proximity to the shore. The pilot site is located in the northern part the Bulgarian Black Sea shore. The system involves the WAM wave model adapted for basin scale simulation at 0.5 degrees spatial resolution. The WAM output for significant wave height, mean wave period and mean angle of wave approach is used in terms of external boundary conditions for the SWAN wave model, which is set up for the western Black Sea shelf at 4km resolution. The same model set up on about 400m resolution is nested to the first SWAN run. In this case the SWAN 2D spectral output provides boundary conditions for the high-resolution model run. The models are implemented for a\n\nA multimodal wave spectrum-based approach for statistical downscaling of local wave climate\n\nScience.gov (United States)\n\nHegermiller, Christie; Antolinez, Jose A A; Rueda, Ana C.; Camus, Paula; Perez, Jorge; Erikson, Li; Barnard, Patrick; Mendez, Fernando J.\n\n2017-01-01\n\nCharacterization of wave climate by bulk wave parameters is insufficient for many coastal studies, including those focused on assessing coastal hazards and long-term wave climate influences on coastal evolution. This issue is particularly relevant for studies using statistical downscaling of atmospheric fields to local wave conditions, which are often multimodal in large ocean basins (e.g. the Pacific). Swell may be generated in vastly different wave generation regions, yielding complex wave spectra that are inadequately represented by a single set of bulk wave parameters. Furthermore, the relationship between atmospheric systems and local wave conditions is complicated by variations in arrival time of wave groups from different parts of the basin. Here, we address these two challenges by improving upon the spatiotemporal definition of the atmospheric predictor used in statistical downscaling of local wave climate. The improved methodology separates the local wave spectrum into âwave families,â defined by spectral peaks and discrete generation regions, and relates atmospheric conditions in distant regions of the ocean basin to local wave conditions by incorporating travel times computed from effective energy flux across the ocean basin. When applied to locations with multimodal wave spectra, including Southern California and Trujillo, Peru, the new methodology improves the ability of the statistical model to project significant wave height, peak period, and direction for each wave family, retaining more information from the full wave spectrum. This work is the base of statistical downscaling by weather types, which has recently been applied to coastal flooding and morphodynamic applications.\n\nCMIP5 based downscaled temperature over Western Himalayan region\n\nScience.gov (United States)\n\nDutta, M.; Das, L.; Meher, J. K.\n\n2016-12-01\n\nLimited numbers of reliable temperature data is available for assessing warming over the Western Himalayan Region (WHR) of India. India meteorological Department provided many stations having more than 30% missing values. Stations having values, were replaced using the Multiple Imputation Chained Equation (MICE) technique. Finally 16 stations having continuous records during 1969-2009 were considered as the \"reference stations\" for assessing the trends in addition to evaluate the Coupled Model Intercomparison, phase 5 (CMIP5) Global Circulation Model(GCMs). Station data indicates higher and rapid (1.41oC) winter warming than the other seasons and least warming was observed in the post monsoon (0.31oC) season. Mean annual warming is 0.84 oC during 1969-2009 indicating the warming over the WHR is more than double the global warming (0.85oC during 1880-2012). The performance of 34 CMIP5 models was evaluated through three different approaches namely comparison of: i) mean seasonal cycle ii) temporal trends and iii) spatial correlation and a rank was assigned to each GCM. How the better performing GCMs able to reproduce the observed spatial details were verified the ERA-interim reanalysis data. Finally station level future downscaled winter temperature has constructed using Empirical Statistical Downscaling (ESD) technique where 2 meter air temperature (T2m) is considered as predictor and station temperature as predictant. Future range of downscaled temperature change for the stations Dheradun, Manali and Gulmarg are 1.3-6.1OC, 1.1-5.8OC and 0.5-5.8OC respectively at the end of 21st century.\n\nProjection of spatial and temporal changes of rainfall in Sarawak of Borneo Island using statistical downscaling of CMIP5 models\n\nScience.gov (United States)\n\nSa'adi, Zulfaqar; Shahid, Shamsuddin; Chung, Eun-Sung; Ismail, Tarmizi bin\n\n2017-11-01\n\nThis study assesses the possible changes in rainfall patterns of Sarawak in Borneo Island due to climate change through statistical downscaling of General Circulation Models (GCM) projections. Available in-situ observed rainfall data were used to downscale the future rainf"
    }
}
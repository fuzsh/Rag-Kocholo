{
    "id": "dbpedia_612_0",
    "rank": 73,
    "data": {
        "url": "https://archive.org/stream/computer-magazine-1990-05/computer-magazine-1990-05_djvu.txt",
        "read_more_link": "",
        "language": "en",
        "title": "Full text of \"Computer Magazine 1990",
        "top_image": "",
        "meta_img": "",
        "images": [
            "https://archive.org/services/img/etree",
            "https://archive.org/services/img/librivoxaudio",
            "https://archive.org/services/img/metropolitanmuseumofart-gallery",
            "https://archive.org/services/img/clevelandart",
            "https://archive.org/services/img/internetarcade",
            "https://archive.org/services/img/consolelivingroom",
            "https://archive.org/images/book-lend.png",
            "https://archive.org/images/widgetOL.png",
            "https://archive.org/services/img/tv",
            "https://archive.org/services/img/911",
            "https://athena.archive.org/0.gif?kind=track_js&track_js_case=control&cache_bust=1567858307",
            "https://athena.archive.org/0.gif?kind=track_js&track_js_case=disabled&cache_bust=978492496"
        ],
        "movies": [],
        "keywords": [],
        "meta_keywords": [
            ""
        ],
        "tags": null,
        "authors": [],
        "publish_date": null,
        "summary": "",
        "meta_description": "",
        "meta_lang": "en",
        "meta_favicon": "https://archive.org/images/glogo.jpg",
        "meta_site_name": "",
        "canonical_link": "https://archive.org/details/computer-magazine-1990-05",
        "text": "Full text of \"Computer Magazine 1990-05\"\n\nSee other formats\n\nMAY 1990 COMPUTER RECENT DEVELOPMENTS If Turn Your Designing Maze Into Amazing Designs. SES/workbench™ System-Level Design Simulation and Modeling Software Now there is an easy to use, elegant tool that guides you through the maze of complex systems design and helps you evaluate design alternatives to discover the consequences of decisions before you commit to them. SES/ workbench. SES/workbench is the premier multi-level design environment from the established leaders in system simulation, modeling and design evaluation, Scientific and Engineering Software, Inc. It can be used to evaluate design decisions throughout the development cycle, from the earliest conceptual stages, through system specification and design, to functional and perform¬ ance verification. E>-S 5®* Jib ftMl BEmZH SES/workbench advances the state of the art in system-level design and evaluation software. # Its unique graphical interface, SES /design, allows you to create a pictorial representation of a system’s structure and to specify its behavior at a high level. • You can quickly analyze design alternatives and tradeoffs with a few clicks of a mouse button. Representation of the system evolves natu¬ rally from a high-level behavioral description to a low-level structural description. The graphical representation of the system is translated into SES /sim, an object- based superset of C and C++ that offers many extensions developed specifically for simulation modeling. Comprehensive statistical reports provide a precise description of the system’s behavior, so that you can evaluate system performance. Free trial licenses are available. Call or write to us for additional information. SES /workbench. A testbed for the imagination. Scientific and Engineering Software, Inc. 1301 West 25th Street, Suite #300 Austin, Texas, U.S.A. Phone: 512/474-4526 Fax: 512/479-6217 Using SIES/design you can easily construct graphical representations of highly complex systems. SBB/design graphs are Reader Service Number 1 New for network analysts and designers Free trial and, if you act now, free training LANNET II.5 Local Area Networks COMNET II.5 Wide Area Networks Fp] Fp] Fp| Fp| ci/pi “ ETHERNET 0 --® 7 0 0 £3 ^ jog Fjj 0 — '^ 7rr _ -2/5—- W. - TOKEN RING 9 © - 1 ~ |57F] E5t 1 Ez^l Ez5l ^ - yj_ - 4/10 L ANNET II.5 uses simulation to predict your LAN performance. You simply describe your LAN and workload. Animated simulation follows immediately --no programming. Easy-to-understand results You get an animated picture of your LAN. System bottlenecks and changing levels of utilization are apparent. Your reports show LAN statistics such as transfer times, delays, and queues. Client, server, and gateway statistics show queue lengths, waiting times, and messages sent. Your LAN simulated You can predict the performance of any LAN. Industry standard protocols such as Ethernet, Token Ring, and Token Bus are built-in. Varia¬ tions can be modeled. C OMNET II.5 uses simulation to predict your network performance. You simply describe your network, traffic load, and routing algorithms. Animated simulation follows immediately -no programming. Easy-to-understand results You get an animated picture of your network. Routing choices and changing levels of network utilization are apparent. Your reports show response times, blocking probabilities, call queueing and packet delays, network throughput, circuit group utilization, and circuit group queue statistics. Your network simulated You can predict the performance of any com¬ munication network-circuit, message, or packet switching. Virtual-circuit or datagram operation can be modeled. Free 60 Day Trial Offer The free trial contains everything you need to try LANNET II.5™ or COMNET II.5™ on your PC, Workstation, or Mainframe. Act now for free training-no cost, no obligation. For immediate information Call Cliff Baker at (619) 457-9681, Fax (619) 457-1184. In Europe, call Nigel McNamara, in the UK, on (01) 332-0122, Fax (01) 332-0112. In Canada, call (613) 747-7467, Fax (613) 747-2224. University faculty should call about our special offer for research and teaching. Rush free trial & training information for: □ LANNET II.5 □ COMNET II.5 LANNET II.5 and COMNET II. : CACI PRODUCTS COMPANY. ©1990 < [ PRODUCTS COMPANY. COMPUTER May 1990 Published by the IEEE Computer Society Vol. 23, No. 5 ARTICLES 5 Guest Editors’ Introduction: Recent Developments in Operating Systems Joseph Boykin and Susan J. LoVerso Q Scalable, Secure, and Highly Available Distributed File Access Mahadev Satyanarayanan Andrew and Coda are distributed Unix file systems that embody many of the recent advances in solving the problem of data sharing in large, physically dispersed workstation environments. 23 The x-kernel: A Platform for Accessing Internet Resources Larry Peterson, Norman Hutchinson, Sean O’Malley, and Herman Rao The x-kemel gives workstation users uniform access to resources across local or wide area networks. The operating system is general, efficient, and easy to program. 35 Scheduling Support for Concurrency and Parallelism in the Mach Operating System David L. Black Traditional time-sharing schedulers are inadequate for parallel and concurrent programs, which require new techniques such as processor allocation and handoff scheduling. 44 Amoeba: A Distributed Operating System for the 1990s Sape J. Mullender, Guido van Rossum, Andrew S. Tanenbaum, Robbert van Renesse, and Hans van Staveren The Amoeba distributed operating system appears to users as a centralized system, but it has the speed, fault tolerance, security safeguards, and flexibility required for the 1990s. 54 Algorithms Implementing Distributed Shared Memory Michael Stumm and Songnian Zhou This article compares several algorithms for implementing distributed shared memory. It shows that the performance of these algorithms is sensitive to the memory access behavior of applications. 65 Distributed Hierarchical Control for Parallel Processing Dror G. Feitelson and Larry Rudolph A novel design using a hierarchy of controllers effectively controls a multiuser, multiprogrammed parallel system. Such a structure allows dynamic repartitioning according to changing job requirements. COMPUTER 79 Standards How not to write commercial standards 82 Computer Society News Cover photo: GabePalmer© Mug Shots BoG adopts statement on IEEE reorganization; Award winners announced at Compcon; Gordon Bell Prize winners announced; Cover design: Jay Simpson, Design & Direction TC provides forum for practical AI 87 Product Reviews Graphical word processors; Getting the most for your laser printer dollars 97 New Products 100 IC/Microsystem Announcements 104 Conferences Compcon Spring 90; NCGA 90; parallel processing; design automation In the next issue Cache architectures for tightly coupled multiprocessors 109 Call for Papers/Calendar 124 Book Reviews 127 New Literature/CS Magazines Career Opportunities, 118; Computer Society Information, 123; Membership Application, 34; 128 Open Channel Change-of-Address Form, 126; Advertiser/ The laws of statistics Product Index, 96; Reader Service Card, 96A May 1990 The joy of Oscape T he C-scape™ Interface Management System frees C programmers from the tedium of coding windows, menus, data validation, help, and text editing functions. C-scape Features Graphics. Combine high-resolution color graphics with text or menus. Object-oriented. Add features and create reusable code modules. Moreover, C-scape is a joy to use. With C-scape’s object-oriented design, you’ll build more functional, more flexible, more portable, and more unique applications—and you’ll have more fun doing it. The industry standout. Many thousands of programmers have quit home-grown libraries and cumbersome, inflexible products for the pleasure of C-scape. The press agrees: “C-scape is by far the best... a joy to use,” wrote IEEE Computer. PC Magazine chose C-scape to produce its Laboratory Benchmark Series 5.0 software because C-scape offers mouse support. Moreover, C-scape simultaneously combines text and graphics. And because C-scape makes it easy to create your own custom routines, mqjor companies have selected C-scape as a standard for software development. C-scape is built around an open architecture, so you can use it with data base management or other C libraries. Mouse. Use any standard mouse for fast screen control. Portability. Write hardware independent code. Supports DOS, OS/2, UNIX, others. Autodetects Hercules, CGA, EGA, VGA. Text editing. Create a full-featured text editor or pop-up note pad. Field flexibility. Create masked, protected and marked fields with complete data validation. Use time, date, money, pop-up list, and many more functions, or create your own. Windows. Choose fh>m pop-up, tiled, bordered and exploding windows, with size and numbers limited only by RAM. Menus. Choose from pop-up, pull-down, 123-style, or slug menus, or create your own. Context-sensitive help. Unk help messages to individual screens or fields. Cross reference messages to create hypertext-like help. Screen design. Build any type of screen or form with the Look and Feel™ Screen Designer, then automatically convert it to C. Screen flexibility. Call screens from files at run time or link them in. And to port from MS-DOS or OS/2 to UNIX, just recompile. Trial with a smile, e scape is not only the most sophisticated, flexible and powerful interface system available, it’s also the most friendly—and easiest to use. Try C-scape on a 30-day trial. It comes with a thorough manual, demo » disk, sample programs with ^ j jA'l source code, an optional /u/ screen designer and code * generator, access to a 24-hour bulletin board, and toll-free support. No royalties, runtime licenses, or runtime modules. After you register, you get complete library source code at no extra cost. Call 800-233-3733 (617-491-7311 in Mass.) to try C-scape now. After the joy of C-scape, programming wil never be the same. MS-DOS, OS/2: $399, library only; with Look & Feel, $499. UNIX, XENIX, Apollo, Sun, Stratus, others please call. Mastercard and Visa accepted. rsnmm Oakland Group, Inc. 675 Massachusetts Ave., Cambridge, MA 02139 USA. FAX: 617-868-4440; Washington 206-746-8767; Benelux (02159)46814; Denmark (02)88 72 49; France (1)46 09 28 28; Germany/Austria/Switzerland (49)07127/5244; Norway (02)44 88 55; Sweden (013)124780; U.K. (0992)500919. C-scape and Look & Feel are trademarks of Oakland Group, Inc. MS-DOS and XENIX are trademarks of Microsoft Corp. OS/2 is a trademark of International Business Machines Corp. UNIX is a trademark of AT&T. HERCULES is a trademark of Hercules Computer Technology, Inc. Prices and terms subject to change. Photo by Jessica A. Boyatt. Kaqji by Kaji Aso. Reader Service Number 2 Guest Editors’ Introduction Recent Developments in Operating Systems Joseph Boykin and Susan J. LoVerso Encore Computer Corporation P rehistory, from an operating sys¬ tem point of view, was 1950. On the earliest digital computers, each user’s job ran separately and had exclusive use of the computer for the duration of the program. The program communicated with I/O devices and managed other re¬ sources without assistance. When the pro¬ gram completed, an operator halted the computer and manually prepared the next job. The mid-1950s saw the dawn of batch processing and early operating systems which did little more than load programs and manage I/O devices. More general- purpose systems did not emerge until the mid-1960s. 1 ' 3 Many of these were njullL ~m 0de sy stems. They not only provided batchprocessing, but also included modes for Tftrtte sharing and some re al-tim e pro- cessfiigT’fHese'behemoths tried,Within a single system, to provide every functional¬ ity to all users. 4 Twenty-year history Operating system refinement and con¬ solidation came in the 1970s, when impor¬ tant concepts were developed, disappeared for a time, and then reappeared. For ex¬ ample, v irtual mem ory, first shown on the At las svstemin~J ?59^~di3rnor^cbme available i n commercial syste ms until 19 72, w hen IBM released it as part of its general product line. For a time, many postulated that the operating system might disappear and that the user had no need to know of the central services provided. The Apple Macintosh was cited as an example. It has no com¬ mand line interpreter and no “system call” interface. The environment is simply a collection of library routines that provide access to the windowing system. But isn’t this an operating system too? Today , operating systems provide secu¬ rity ,jtetworktng73Istnbute3acceis7gr^h- \"ics.~fime^sharmg7 m ultiprocess, mul¬ tithread, and multiprocessor capabilities ^ \"but, unlike their ancestors, are not intended to be all things to all people within a single system. Like the computers on which they run, operating systems vary in size, com¬ plexity, and functionality to fulfill users’ requirements. Some, like Digital Research’s CP/l^f'and Microsoft’s MS- DOS, are little more than program load- environment of .hi gh-performance, single- user graphical workstations connected 'overlocal area networks. LANs filled with ers that also support a local disk drive . TrEFfers, such a~s DEC’S VMS, Data General’s AOS/VS, and AT&T’s Unix, provide support for, and simultane ous / access to, a large number of different pe - , npheral dev ices. Sophisticated environ¬ ments have been developed under these operating systems. Dramatic changes in operating systems have resulted from enhancements to the hardware technology, experience with existing systems, development of new applications and needs, and, of course, imagination. Ten years ago, networking was an interesting concept with a few, unique interconnected systems; today, it’s become a requirement. The combmecT \"developments of LthemeTand commodity microprocessors have provuJeS^^T'new - '' personal workstations have created the demand for efficient use of computing and storage resources. Wide area n etwork s, with the potential for worldwide connec- tivity, provide the possibility of global access to computing resources. Unfortu¬ nately, g lobal agreement over the proto ¬ cols usedto~access these resources seem s unlikely , thereby placing new demands on operating systems to provide users with uniform and integrated data access. Multiprocess or architectures have nfoved from^expeliiUeiual research labs into the mainstream of the computing world. The operating system, as the soft¬ ware foundation for these systems, is being asked to efficiently meet new require¬ ments. Modem operating system imple¬ mentations need to address such issues as load balancing across process ors, gang scheduling, and algorithm locking. Special-issue articles Progress in the 20 short years of operat¬ ing system history has been remarkable. The field has grown with the new, modem computer architectures.^^g^iperming system provides an ever-more -useful view of thesystem. This special issue of Com¬ puter looks at just a few of the recent developments. Its goal is to show where operating systems research has led us and where it might lead us in the future. May 1990 0018-9162/90/0500-0005$01.C I IEEE Early computer users placed the com¬ puter in the center of their world, with one or more large computers serving the needs of many. These systems were maintained by a dedicated staff. T he advent of wor k¬ st ation s shifted the f ocus to the user The resulting problem was that users could no longer share data and now had the respon¬ sibility of administering their own systems — a task they were not trained to do. While no one wanted to eliminate workstations, it was recognized that a balance had to be struck. To accomplish this, we use a few /l arge systems as file and compu t ation serv - ' e fs and leave the user Interface on the workstation. The first article in this issue, ^Scalable/ Secure, and Highly Available Distributed File Access,” by Mahadev Satyanarayanan, discusses the evolution of a distributed file system that provides highly available access to data through a combination of read/write replication and data caching. Networking has become commonplace. What has not become common is the use of a single communications protocol. In some circumstances, this would be useful, but different protocols provide different func¬ tionality and capabilities. Most worksta¬ tions provide only a single protocol family. Access to multiple protocol families, so that users can access different networks, will be an important requirement for the future. In the second article, “The x-Ker- nel: A Platform for Accessing Internet Resources,” Larry Peterson, Norman Hutchinson, Sean O’Malley, and Herman Rao discuss an operating system designed to facilitate the implementation of multiple network protocols. It has always amazed us that, after 20 years of wrestling with multiprocess oper¬ ating systems, we are still trying to deter¬ mine how to best schedule multiple pro¬ cesses. One reason is that the tasks we ask our systems to perform constantly change. Another is that computer architectures have also changed. Scheduling on a multi¬ processor system should be different from scheduling on a uniprocessor — if not, we have not achieved optimal use of this new architecture. David L. Black’s article, “Scheduling Support for Concurrency and Parallelism in the Mach Operating Sys¬ tem,” discusses scheduling techniques and policies for multiprocessor systems. Advances in networking and worksta¬ tions have not been independent activities. A natural outgrowth of these two develop¬ ments has been an effort to make net¬ worked, multiple computer systems ap¬ pear as a single entity. A new operating system that implements this approach is Amoeba. Its design arid implementation are discussed in “Amoeba: A Distributed Operating System for the 1990s,” by Sape J. Mullender, Guido van Rossum, Andrew S. Tanenbaum, Robbert van Renesse, and Hans van Staveren. These first four articles discuss the re¬ sults of software development efforts. “Algorithms Implementing Distributed Shared Memory,” by Michael Stumm and Songnian Zhou, mathematically analyzes the trade-offs of several distributed shared memory algorithms. Multiprocessor systems have gained commercial success in the past few years. These systems, like all computers, con¬ tinue to evolve. The final article, “Distrib¬ uted Hierarchical Control for Parallel Processing” by Dror Feitelson and Larry Rudolph, discusses a new and unique ap¬ proach to multiprocessing that allows dynamic repartitioning. ■ Acknowledgments We would like to thank the many authors who submitted articles to this special issue. The quantity and quality of those submissions was more than we expected. We would also like to thank the many referees who donated countless hours to reading and commenting on papers. It is through their effort that issues such as this are possible. We would also like to thank our colleagues and the management at Encore Computer. Edit¬ ing this issue has taken a great deal of time — our management has been more patient with us than we had any right to expect. Our colleagues were called upon more than once to referee papers during the initial review period and to read and reread the final manuscripts. Finally, we would like to thank Bruce Shriver, editor-in-chief of Computer, for his time, assis¬ tance, and suggestions, which enhanced the quality of this issue. 5. J. Fotheringham, “Dynamic Storage Alloca¬ tion in the Atlas Computer, Including an Automatic Use of a Backing Store,” Comm. ACM, Vol. 4, No. 10, Oct. 1961, pp. 435- 1 Joseph Boykin is the manager of Mach Operat¬ ing System Development with Encore Com¬ puter Corporation. The Mach OS group is re¬ sponsible for the transition of Mach from a research prototype to a commercial product, as well as the use of Mach within research projects pded from sourgcs-bolh- within and o utgide of The Open Software Foundation will usb. Encore’s implementation of Mach as the basis) “ SF/1 operating sys tem. EncorelsMaCh OS group will provide co'hsuitiagr'design, and implementation support to OSF. Boykin has held several leadership positions within the IEEE Computer Society. Currently, he is the society’s treasurer and an Executive Committee member. He has chaired the Techni¬ cal Committee on Operating Systems and the Workshop on Workstation Operating Systems. Boykin holds both an MS in computer science and an MA in psychology. His graduate work was done at Ohio State and Pennsylvania State Universities. References 1. S. Rosen, “Electronic Computers: A His¬ torical Survey,” Computing Surveys, Vol. 1, No. 1, Mar. 1969, pp. 7-36. 2. R.F. Rosin, “Supervisory and Monitor Systems,” Computing Surveys, Vol. 1, No. 1, Mar. 1969, pp. 37-54. 3. N. Weizer, “A History of Operating Systems,” Datamation, Vol. 27, No. 1, Jan. 1981, pp. 119-126. 4. H.M. Deitel, Introduction to Operating Sys¬ tems, Addison-Wesley, Reading, Mass., 1983. Susan J. LoVerso has been a software engineer at Encore Computer Corporation since 1987 as a member of tiie Mach Operating System Devel¬ opment Group. Currently, she is working on parallelizing the 4.4BSD file system for the OSF/1 operating system. LoVerso received her master’s degree in computer science from the State University of New York at Buffalo. As a member of the Education Committee of the Women’s Initiative for Technology Leadership in New England, she often speaks at local high schools about careers in the sciences and mathematics. Readers can write to the guest editors at Encore Computer, 257 Cedar Hill St., Marlbor¬ ough, MA 01752-3089. Their e-mail addresses are boykin@encore.com and sue@encore.com. COMPUTER XEROX That’s the question we've been asking our people since the beginning. And as a result of their valuable contribu¬ tions, Xerox technology has revolutionized the way we work. To continue to develop technologies that will help business make impressive gains in productivity, we need the best talent the industry has to offer. If you have 5-7 years’ experience developing embedded real-time software, you may qualify for a position with Team Xerox. Systems Programmers You’ll design and develop software for advanced printing and electronic reprographic systems using our state- of-the-art highly typed, structured systems programming language (similar to ADA or Modula-2) in a state-of-the-art source language development/debug environment. You must have a BS/MS in electrical engineering, computer engineering or computer science Familiarity with at least one of the following is a plus: structured design techniques, operating system internals, diagnostics, real-time perfor¬ mance analysis, software-hardware interfacing, distributed computing across LANs, page description languages and printer control languages. System Architecture You’ll develop system architectures for advanced elec¬ tronic reprographic, printing, and publishing components and networked services. To do this, you’ll specify hardware, software and supporting elements needed to satisfy product requirements and customer needs for design performance, extensibility and flexibility, using analytical and simulation tools to assess system realization. We require an MS/PhD in computer science, computer engineering or electrical engineering. Knowledge of systems design and distributed processing applications is a plus. Join us and see how your contribution can change the way the world does business in the 1990s. In return, you’ll receive a highly competitive salary and benefits, and membership on the Team that earned last year’s Malcolm Baldridge National Quality Award. To learn more, send your resume indicating position of interest to: Xerox Corporation, Manager, Employment—SWEC, 800 Phillips Road, Bldg. 205-99E, Webster, NY 14580. Xerox is an equal opportunity employer SUMMER 1990 USENIX TECHNICAL CONFERENCE & EXHIBITION Anaheim, CA June 11-15,1990 TUTORIALS The tutorial program provides a detailed exami¬ nation of several UNIX topics. Two days of intensive, in-depth seminars will be presented by leading UNIX experts. Attendees needing to expand their knowledge will benefit from offerings in: Security User Interfaces Networking UNIX Internals Windowing Schemes Operating Systems Distributed Processing System Administration Writing Device Drivers ABOUT THE SPONSOR The USENIX Association is a not-for-profit organi¬ zation of those interested in UNIX and UNIX-like systems. It is dedicated to fostering and communicating the TECHNICAL EXHIBITION development of research and technological information and ideas pertaining to advanced computing systems. The USENIX Exhibition allows hardware and soft¬ ware companies to display their latest, most advanced technical innovations to a highly focused end user com¬ munity. For details on exhibiting, contact: SE The Professional and Technical UNIX Association USENIX Exhibit Office, 5398 Manhattan Circle, Suite 200, Boulder, CO 80303 Telephone (303) 499-2600, FAX (303) 499-2608 For complete conference details, call: (714) 588-8649 or write: TECHNICAL SESSIONS The three day informative program will cover new USENIX Conference Office 22672 Lambert St., Suite 613 El Toro, CA 92630 and interesting work on a variety of technical issues including: Software Release Systems; User Interfaces, Windowing, Graphics; File Systems; Distributed Sys¬ tems; UNIX Kernel Approaches; Compilers, Debuggers, Tools, Runtime Issues; Security and others. UNIX is a registered Trademark of AT&T. Scalable, Secure, and Highly Available Distributed File Access Mahadev Satyanarayanan Carnegie Mellon University F o r theiisers of a distrib uted system to coll aborate -e ffectively, tf ieabil^ the last decade, distributed file systems based on the Unix model have been the subject of growing attention. They are now widely considered an effective means of sharing data in academic and research en¬ vironments. This article presents a sum¬ mary and historical perspective of work done by my colleagues, students, and I in designing and implementing such systems at Carnegie Mellon University. This work began in 1983 in the context of Andrew, a joint project of CMU and IBM to develop a state-of-the-art comput¬ ing facility for education and research at CMU. The project envisioned a dramatic increase in computing power made pos¬ sible by the widespread deployment of powerful personal workstations. Our char¬ ter was to develop a mechanism that would enable the users of these workstations to collaborate and share data effectively. We decided to build a distributed file system for this purpose because it would provide the right balance between functionality and complexity for our usage environment. It was clear from the outset that our distributed file system had to possess two critical attributes: It had to scale well, so that the system could grow to its antici- Andrew and Coda are distributed Unix file systems that embody many of the recent advances in solving the problem of data sharing in large, physically dispersed workstation environments. pated final size of over 5,000 workstations. It also had to be secure, so that users could be confident of the privacy of their data. Neither of these attributes is likely to be present in a design by accident, nor can it be added as an afterthought. Rather, each attribute must be treated as a fundamental constraint and given careful attention dur¬ ing the design and implementation of a system. Our design has evolved over time, re¬ sulting in three distinct versions of the Andrew file system, called AFS-1, AFS-2, and AFS-3. In this article “Andrew file system” or “Andrew” will be used as a collective term referring to all three ver¬ sions. As our user community became more dependent on Andrew, the availability of data in it became more important. Today, a single failure in Andrew can seriously inconvenience many users for significant periods. To address this problem, we be¬ gan the design of an experimental file system called Coda in 1987. Intended for the same computing environment as An¬ drew, Coda retains Andrew’s scalability and security characteristics while provid¬ ing much higher availability. The Andrew architecture The Andrew computing paradigm is a synthesis of the best feat ures of persona l computing ariS jjm jsRann g. It combines 'the flexible and visually rich user interface available in personal computing with the ease of information exchange typical of May 1990 0018-9162/90/0500-0009S01.0 Figure 1. A high-level view of the An¬ drew architecture. The structure la¬ beled “Vice” is a collection of trusted file servers and untrusted networks. The nodes labeled “W” are private or public workstations, or timesharing systems. Software in each such node makes the shared files in Vice appear as an integral part of that node’s file system. Shared files Figure 2. File system view at a work¬ station: how the shared files in Vice appear to a user. The subtree under the directory labeled “afs” is identical at all workstations. The other directo¬ ries are local to each workstation. Symbolic links can be used to make lo¬ cal directories correspond to directo¬ ries in Vice. timesharing. A conceptual view of this model is shown in Figure 1. The large, amoeb a-like structure in the middle, called ViceTis theT riformation- sharingbackboneofthesystem. Although reprcsemed'as'a'stngle entity, it actually consists of a collection of dedicated file servers and a complex local area network. User computing cycles are provided by workstations running the Unix operating system. Data sharing in Andrew is supported by a distributed file system that appears as a single large subtree of the local file system on each workstation. The only files outside the shared subtree are temporary files and files essential for workstation initializa¬ tion. A process called Venus, running on each workstation, mediates shared file access. Venus finds files in Vice, caches them locally, and performs emulation of Unix file system semantics. Both Vice and Venus are invisible to workstation pro¬ cesses, which only see a Unix file system, one subtree of which is identical on all workstations. Processes on two different workstations can read and write files in this subtree just as if they were running on a single timesharing system. Figure 2 de¬ picts the file system view seen by a work¬ station user. Our experience with the Andrew archi¬ tecture over the past six years has been positive. It is simple and easily understood by naive users, and it permits efficient implementation. It also offers a number of benefits that are particularly valuable on a large scale: • Data sharing is simplified. A worksta¬ tion with a small disk can potentially ac¬ cess any file in Andrew by name. Since the file system is location transparent, users do not have to remember the machines on which files are currently located or where files were created. System administrators can move files from one server to another without inconveniencing users, who are completely unaware of such a move. • User mobility is supported. A user can walk to any workstation in the system and access any file in the shared name space. A user’s workstation is personal only in the sense that he owns it. • System administration is easier. Op¬ erations staff can focus on the relatively small number of servers, ignoring the more numerous and physically dispersed clients. Adding a new workstation involves merely connecting it to the network and assigning it an address. • Better security is possible. The servers in Vice are physically secure and run trusted system software. No user programs are executed on servers. Encryption-based authentication and transmission are used to enforce the security of server-worksta¬ tion communication. Although individuals may tamper with the hardware and soft¬ ware on their workstations, their malicious actions cannot affect users at other work¬ stations. • Client autonomy is improved. Work¬ stations can be turned off or physically relocated at any time without inconve¬ niencing other users. Backup is needed only on the servers, since workstation disks are used merely as caches. Scalability in Andrew A scalable distributed system is one that can easily cope with the addition of users and sites, its growth involving minimal expense, performance degradation, and administrative complexity. We have achieved these goals in Andrew by reduc¬ ing static bindings to a bare minimum and by maximizing the number of active clients that can be supported by a server. The following sections describe the evolution of our design strategies for scalability in Andrew. AFS-1. AFS-1 was a prototype with the primary functions of validating the An¬ drew file system architecture and provid¬ ing rapid feedback on key design deci¬ sions. Each server contained a local file system mirroring the structure of the shared file system. Vice file status infor¬ mation, such as access lists, was stored in shadow directories. If a file was not on a server, the search for its name would end in a stub directory that identified the server containing that file. Since server processes could not share memory, their only means of sharing data structures was via the local file system. Clients cached pathname prefix infor¬ mation and used it to direct file requests to appropriate servers. The Vice-Venus inter¬ face named files by their full pathnames. There was no notion of a low-level name, such as the inode in Unix. Venus used a pessimistic approach to maintaining cache coherence. All cached copies of files were considered suspect. Before using a cached file, Venus would contact Vice to verify that it had the latest version. Each open of a file thus resulted in at least one interaction with a server, even if the file was already in the cache and up to date. For the most part, we were pleased with AFS-1. Almost every application was able to use Vice files without recompilation or relinking. There were minor areas of in¬ compatibility with standard Unix seman¬ tics, but these were never serious enough to discourage users. 10 COMPUTER Design principles from Andrew and Coda The design choices of Andrew and Coda were guided by a few simple principles. They were not specified a priori, but emerged in the course of our work. We share these principles and examples of their application in the hope that they will be useful to de¬ signers of other large-scale distributed systems. The principles should not be applied dogmatically but should be used to help crystallize thinking during the design process. • Workstations have the cycles to burn. Whenever there is a choice be¬ tween performing an operation on a workstation and performing it on a central resource, it is preferable to pick the workstation. This enhances the scalability of the design because it less ens the need to increase central rdsgu Fces~as workstations are adde d. ~Tfie~only functions performed by servers in Andrew and Coda are those critical to security, integrity, or location of data. Further, there is very little in¬ terserver traffic. Pathname translation is done on clients rather than on serv¬ ers in AFS-2, AFS-3, and Coda. The parallel update protocol in Coda de¬ pends on the client to directly update all AVSG members, rather than updat¬ ing one of them and letting it relay the update. • Cache whenever possible. Scalability, user mobility, and site au¬ tonomy motivate this principle. Cach¬ ing reduces contention on centralized resources and transparently makes data available wherever it is being used. AFS-1 cached files and location in¬ formation. AFS-2 also cached directo¬ ries, as do AFS-3 and Coda. Caching is the basis of disconnected operation in Coda. • Exploit file usage properties. Knowledge of the nature of file accesses in real systems allows better design choices to be made. Files can often be grouped into a small number of easily identifiable classes that reflect their ac¬ cess and modification patterns. These class-specific properties provide an op¬ portunity for independent optimization and, hence, improved performance. Almost one-third of the file references in a typical Unix system are to temporary files. Since such files are seldom shared, Andrew and Coda make them part of the local name space. The ex¬ ecutable files of system programs are of¬ ten read but rarely written. AFS-2, AFS- 3, and Coda therefore support read-only replication of these files to improve per¬ formance and availability. Coda's use of an optimistic replication strategy is based on the premise that sequential write sharing of user files is rare. • Minimize systemwide knowledge and change. In a large distributed sys¬ tem, it is difficult to be aware at all times of the entire state of the system. It is also difficult to update distributed or rep¬ licated data structures consistently. The scalability of a design is enhanced if it rarely requires global information to be monitored or atomically updated. Workstations in Andrew and Coda monitor only the status of servers from which they have cached data. They do not require any knowledge of the rest of the system. File location information on Andrew and Coda servers changes rela¬ tively rarely. Caching by Venus, rather than file location changes in Vice, is used to deal with movement of users. Coda integrates server replication (a relatively heavyweight mechanism) with caching to improve availability without losing scalability. Knowledge of a cach¬ ing site is confined to servers with call¬ backs for the caching site. Coda does not depend on knowledge of sys¬ temwide topology, nor does it incorpo¬ rate any algorithms requiring sys¬ temwide election or commitment. Another instance of the application of this principle is the use of negative rights. Andrew provides rapid revoca¬ tion by modifications of an access list at a single site rather than by sys¬ temwide change of a replicated protec¬ tion database. • Trust the fewest possible enti¬ ties. A system whose security depends on the integrity of the fewest possible entities is more likely to remain secure as it grows. Rather than trusting thousands of workstations, security in Andrew and Coda is predicated on the integrity of the much smaller number of Vice serv¬ ers. The administrators of Vice need only ensure the physical security of these servers and the software they run. Responsibility for workstation in¬ tegrity is delegated to the owner of each workstation. Andrew and Coda rely on end-to-end encryption rather than physical link security. • Batch if possible. Grouping op¬ erations (and hence scalability) can im¬ prove throughput, although often at the cost of latency. The transfer of files in large chunks in AFS-3 and in their entirety in AFS-1, AFS-2, and Coda is an instance of the application of this principle. More effi¬ cient network protocols can be used when data is transferred en masse rather than as individual pages. In Coda the second phase of the update protocol is deferred and batched. La¬ tency is not increased in this case be¬ cause control can be returned to appli¬ cation programs before the completion of the second phase. AFS-1 was in use for about a year, from late 1984 to late 1985. At its peak usage, there were about 100 workstations and six servers. Performance was usually accept¬ able to about 20 active users per server. But sometimes a few intense users caused per¬ formance to degrade intolerably. The sys¬ tem turned out to be difficult to operate and maintain, especially because it provided few tools to help system administrators. The embedding of file location informa¬ tion in stub directories made it hard to move user files between servers. AFS-2. The design of AFS-2 was based on our experience with AFS-1 as well as on extensive performance analysis. 1 We re¬ tained the strategy of workstations caching entire files from a collection of dedicated autonomous servers. But we made many changes in the realization of this architec¬ ture, especially in cache management, name resolution, communication, and server process structure. A fundamental change in AFS-2 was the manner in which cache coherence was maintained. Instead of checking with a May 1990 Load units Figure 3. AFS-2 versus Sun NFS performance under load on identical client, server, and network hardware. A load unit consists of one client workstation running an instance of the Andrew benchmark. (Full details of the benchmark and experimental configuration can be found in Howard et al., 1 from which this graph is adapted.) As the graph clearly indicates, the performance of AFS-2, even with a cold cache, degrades much more slowly than that of NFS. server on each open, Venus now assumed that cache entries were valid unless other¬ wise notified. When a workstation cached a file or directory, the server promised to notify that workstation before allowing a modification by any other workstation. This promise, known as a callback, re¬ sulted in a considerable reduction in cache validation traffic. Callback made it feasible for clients to cache directories and to translate path¬ names locally. Without callbacks, the lookup of every component of a pathname would have generated a cache validation request. For reasons of integrity, directory modifications were made directly on serv¬ ers, as in AFS-1. Each Vice file or direc¬ tory in AFS-2 was identified by a unique fixed-length file identifier. Location infor¬ mation was contained in a slowly changing volume location database replicated on each server. AFS-2 used a single process to service all clients of a server, thus reducing the context switching and paging overheads observed in AFS-1. A nonpreemptive lightweight process mechanism supported concurrency and provided a convenient programming abstraction on servers and clients. The RPC (remote procedure call) mechanism in AFS-2, which was inte¬ grated with the lightweight process mecha¬ nism, supported a very large number of active clients and used an optimized bulk- transfer protocol for file transfer. Besides the changes we made for per¬ formance, we also eliminated AFS-l’s inflexible mapping of Vice files to server disk storage. This change was the basis of a number of mechanisms that improved system operability. Vice data in AFS-2 was organized in terms of a data-structur- ing primitive called a volume, a collection of files forming a partial subtree of the Vice name space. Volumes were glued together at mount points to form the com¬ plete name space. Venus transparently recognized and crossed mount points dur¬ ing name resolution. Volumes were usually small enough to allow many volumes per server disk parti¬ tion. Volumes formed the basis of disk quotas. Each system user was typically assigned a volume, and each volume was assigned a quota. Easily moved between servers by system administrators, a vol¬ ume could be used (even for update) while it was being moved. Read-only replication of volumes made it possible to provide increased availabil¬ ity for frequently read but rarely updated files, such as system programs. The backup and restoration mechanism in AFS-2 also made use of volume primitives. To back up a volume, a read-only clone was first made. Then, an asynchronous mechanism trans¬ ferred this frozen snapshot to a staging machine from which it was dumped to tape. To handle the common case of accidental deletion by users, the cloned backup vol¬ ume of each user’s files was made available as a read-only subtree of that user’s home directory. Thus, users themselves could restore files within 24 hours by means of normal file operations. AFS-2 was in use at CMU from late 1985 until mid-1989. Our experience confirmed that it was indeed an efficient and conve¬ nient system to use at large scale. Con¬ trolled experiments established that it per¬ formed better under load than other con¬ temporary file systems. 1,2 Figure 3 presents the results of one such experiment. AFS-3. In 1988, work began on a new version of the Andrew file system called AFS-3. (For ease of exposition, all changes made after the AFS-2 release described in Howard et al. 1 are described here as pertain¬ ing to AFS-3. In reality, the transition from AFS-2 to AFS-3 was gradual.) The revision was initiated at CMU and has been contin¬ ued since mid-1989 at Transarc Corpora¬ tion, a commercial venture involving many of the original implementers of AFS-3. The revision was motivated by the need to pro¬ vide decentralized system administration, by the desire to operate over wide area networks, and by the goal of using industry standards in the implementation. AFS-3 supports multiple administrative cells, each with its own servers, worksta¬ tions, system administrators, and users. Each cell is a completely autonomous Andrew environment, but a federation of cells can cooperate in presenting users with a uniform, seamless filename space. The ability to decompose a distributed system into cells is important at large scale because it allows administrative responsibility to be delegated along lines that parallel institu¬ tional boundaries. This makes for smooth and efficient system operation. The RPC protocol used in AFS-3 pro¬ vides good performance across local and wide area networks. In conjunction with the cell mechanism, this network capability has made possible shared access to a common, nationwide file system, distributed over nodes such as MIT, the University of Michi¬ gan, and Dartmouth, as well as CMU. Venus has been moved into the Unix 12 COMPUTER Other contemporary distributed file systems A testimonial to the importance of distributed file systems is the large number of efforts to build such sys¬ tems in industry and academia. The following are some systems currently in use: Sun NFS has been widely viewed as a de facto standard since its intro¬ duction in 1985. Portability and heterogeneity are the dominant con¬ siderations in its design. Although originally developed on Unix, it is now available for other operating systems such as MS-DOS. Apollo Domain is a distributed workstation environment whose devel¬ opment began in the early 1980s. Since the system was originally in¬ tended for a close-knit team of col- Further reading Surveys Satyanarayanan, M., “A Survey of Distrib¬ uted File Systems,” in Annual Review of Computer Science, J.F. Traub et al., eds., Annual Reviews, Inc., Palo Alto, Calif., 1989. Svobodova, L., “File Servers for Network- Based Distributed Systems,” ACM Comput¬ ing Surveys, Vol. 16, No. 4, Dec. 1984. Individual systems Amoeba van Renesse, R., H. van Staveren, and A.S. Tanenbaum, “The Performance of the Amoeba Distributed Operating System,” laborating individuals, scale was not a dominant design consideration. But large Apollo installations now exist. IBM AIX-DS is a collection of distrib¬ uted system services for the AIX operat¬ ing system, a derivative of System V Unix. A distributed file system is the pri¬ mary component of AIX-DS. Its goals in¬ clude strict emulation of Unix semantics, ability to efficiently support databases, and ease of administering a wide range of installation configurations. AT&T RFS is a distributed file system developed for System V Unix. Its most distinctive feature is precise emulation of local Unix semantics for remote files. Sprite is an operating system for net¬ worked uniprocessor and multiprocessor workstations, designed at the University of California at Berkeley. The goals of the Software Practice and Experience, Vol. 19, No. 3, Mar. 1989. Apollo Domain Levine, P., “The Apollo Domain Distributed File System” in Theory and Practice of Distrib¬ uted Operating Systems, Y. Paker, J.-T. Ba- natre, and M. Bozyigit, eds., NATO ASI Series, Springer-Verlag, 1987. AT&T RFS Rifkin, A.P., et al., “RFS Architectural Over¬ view” Proc. Summer Usenix Conf., Atlanta, 1986, pp. 248-259. Hisgen, A., et al., “Availability and Consis¬ tency Trade-Offs in the Echo Distributed File System,” Proc. Second IEEE Workshop on Sprite file system include efficient use of large main memory caches, diskless operation, and strict Unix emulation. Amoeba is a distributed operating system built by the Free University and CWI (Mathematics Center) in Amsterdam. The first version of the distributed file system used optimistic concurrency control. The current ver¬ sion provides simpler semantics and has high performance as its primary objective. Echo is a distributed file system currently being implemented at the System Research Center of Digital Equipment Corporation. It uses a pri¬ mary site replication scheme, with reelection in case the primary site fails. Workstation Operating Systems, CS Press, Los Alamitos, Calif., Order No. 2003, Sept. 1989. IBM AIX-DS Sauer, C.H., et al., “RT PC Distributed Ser¬ vices Overview,” ACM Operating Systems Review, Vol. 21, No. 3, July 1987, pp. 18-29. Sprite Ousterhout, J.K., et al., “The Sprite Network Operating System,” Computer, Vol. 21, No. 2, Feb. 1988, pp. 23-36. Sun NFS Sandberg, R., et al., “Design and Implemen¬ tation of the Sun Network File System,” Proc. Summer Usenix Conf., Portland, 1985, pp. 119-130. kernel in order to use the vnode file inter¬ cept mechanism from Sun Microsystems, a de facto industry standard. The change also makes it possible for Venus to cache files in large chunks (currently 64 Kbytes) rather than in their entirety. This feature reduces file-open latency and allows a workstation to access files too large to fit on its local disk cache. Security in Andrew A consequence of large scale is that the casual attitude toward security typical of close-knit distributed environments is not acceptable. Andrew provides mechanisms to enforce security, but we have taken care to ensure that these mechanisms do not inhibit legitimate use of the system. Of course, mechanisms alone cannot guaran¬ tee security; an installation also must fol¬ low proper administrative and operational procedures. A fundamental question is who enforces security. Rather than trusting thousands of workstations, Andrew predicates security on the integrity of the much smaller num¬ ber of Vice servers. No user software is ever run on servers. Workstations may be owned privately or located in public areas. Andrew assumes that the hardware and software on workstations may be modified in arbitrary ways. This section summarizes the main as¬ pects of security in Andrew, pointing out the changes that occurred as the system evolved. These changes have been small compared to the changes for scalability. More details on security in Andrew can be found in an earlier work. 3 Protection domain. The protection do¬ main in Andrew is composed of users and groups. A user is an entity, usually a hu¬ man, that can authenticate itself to Vice, be held responsible for its actions, and be charged for resource consumption. A May 1990 13 Figure 4. Major components and relationships involved in authentication in Andrew. Modifications such as password changes and additions of new users are made to the master authentication server, which distributes these changes to the slaves. When a user logs in, a client can obtain authentication tokens on the user’s behalf from any slave authentication server. The client uses these tokens as needed to establish secure connections to file servers. group is a set of other groups and users. Every group is associated with a unique user called its owner. AFS-1 and AFS-2 supported group in¬ heritance, with a user’s privileges being the cumulative privileges of all the groups it belonged to, either directly or indirectly. Modifications of the protection domain were made off line by system administra¬ tors and typically were reflected in the system once a day. In AFS-3, modifica¬ tions are made directly by users to a protec¬ tion server that immediately reflects the changes in the system. To simplify the implementation of the protection server, the initial release of AFS-3 does not sup¬ port group inheritance. This may change in the future because group inheritance con¬ ceptually simplifies management of the protection domain. One group is distinguished by the name System:Administrators. Membership in this group endows special administrative privileges, including unrestricted access to any file in the system. The use of a System: Administrators group rather than a pseudo-user (such as “root” in Unix sys¬ tems) has the advantage that the actual identity of the user exercising special privi¬ leges is available for use in audit trails. Authentication. The Andrew RPC mechanism provides support for secure, authenticated communication between mutually suspicious clients and servers, by using a variant of the Needham and Schroe- der private key algorithm. 4 When a user logs in on a workstation, his or her pass¬ word is used to obtain tokens from an authentication server. These tokens are saved by Venus and used as needed to establish secure RPC connections to file servers on behalf of the user. The level of indirection provided by tokens improves transparency and secu¬ rity. Venus can establish secure connec¬ tions to file servers without users’ having to supply a password each time a new server is contacted. Passwords do not have to be stored in the clear on workstations. Because tokens typically expire after 24 hours, the period during which lost tokens can cause damage is limited. As shown in Figure 4, there are multiple instances of the authentication server, each running on a trusted Vice machine. One of the authentication servers, the master, re¬ sponds to updates by users and system administrators and asynchronously propa¬ gates the updates to other servers. The latter are slaves and only respond to que¬ ries. This design provides robustness by allowing users to log in as long as any slave or the master is accessible. For reasons of standardization, the AFS- 3 developers plan to adopt the Kerberos authentication system. 5 Kerberos provides the functionality of the Andrew authenti¬ cation mechanism and closely resembles it in design. File system protection. Andrew uses an access list mechanism for file protection. The total rights specified for a user are the union of the rights specified for the user and for the groups he or she belongs to. Access lists are associated with directories rather than individual files. The reduction 14 COMPUTER in state obtained by this design decision provides conceptual simplicity that is valu¬ able at large scale. An access list can spec¬ ify negative rights. An entry in a negative rights list indicates denial of the specified rights, with denial overriding possession in case of conflict. Negative rights de¬ couple the problems of rapid revocation and propagation of group membership information and are particularly valuable in a large distributed system. Although Vice actually enforces protec¬ tion on the basis of access lists, Venus superimposes an emulation of Unix pro¬ tection semantics. The owner component of the Unix mode bits on a file indicate readability, writability, or executability. These bits, which indicate what can be done to the file rather than who can do it, are set and examined by Venus but ignored by Vice. The combination of access lists on directories and mode bits on files has proved to be an excellent compromise between protection at fine granularity, conceptual simplicity, and Unix compati¬ bility. Resource usage. A security violation in a distributed system can manifest itself as an unauthorized release or modification of information or as a denial of resources to legitimate users. Andrew’s authentication and protection mechanisms guard against unauthorized release and modification of information. Although Andrew controls server disk usage through a per-volume quota mechanism, it does not control re¬ sources such as network bandwidth and server CPU cycles. In our experience, the absence of such controls has not proved to be a problem. What has been an occasional problem is the inconvenience to the owner of a workstation caused by the remote use of CPU cycles on that workstation. The paper on security in Andrew 3 elaborates on this issue. High availability in Coda The Coda file system, a descendant of AFS-2, is substantially more resilient to server and network failures. The ideal that Coda strives for is constant data availabil¬ ity, allowing a user to continue working regardless of failures elsewhere in the system. Coda provides users with the bene¬ fits of a shared data repository but allows them to rely entirely on local resources when that repository is partially or totally inaccessible. When network partitions occur, Coda allows data to be updated in each partition but detects and confines conflicting updates as soon as possible after their occurrence. It also provides mechanisms to help users recover from such conflicts. A related goal of Coda is to gracefully integrate the use of portable computers. At present, users manually copy relevant files from Vice, use the machine while isolated from the network, and manually copy updated files back to Vice upon reconnec¬ tion. These users are effectively perform¬ ing manual caching of files with write¬ back on reconnection. If one views the disconnection from Vice as a deliberately induced failure, it is clear that a mecha¬ nism for supporting portable machines in isolation is also a mechanism for fault tolerance. By providing the ability to move seam¬ lessly between zones of normal and dis¬ connected operation. Coda may simplify the use of cordless network technologies such as cellular telephone, packet radio, or infrared communication in distributed file systems. Although such technologies pro¬ vide client mobility, they often have intrin¬ sic limitations such as short range, inabil¬ ity to operate inside steel-framed build¬ ings, or line-of-sight constraints. These shortcomings are reduced in significance if clients are capable of temporary autono¬ mous operation. The design of Coda was presented in detail in a recent paper. 6 A large subset of the design has been implemented, and work is in progress to complete the im¬ plementation. One can sit down at a Coda workstation today and execute Unix appli¬ cations without recompilation or relink¬ ing. Execution continues transparently when contact with a server is lost due to a crash or network failure. In the absence of failures, using a Coda workstation feels no different from using an AFS-2 worksta- Design overview. The Coda design re¬ tains key features of AFS-2 that contribute to scalability and security: • Clients cache entire files on their local disks. From the perspective of Coda, whole-file transfer also offers a degree of intrinsic resiliency. Once a file is cached and open at a client, it is immune to server and network failures. Caching on local disks is also consistent with our goal of supporting portable machines. • Cache coherence is maintained by the use of callbacks. • Clients dynamically find files on serv¬ ers and cache location information. • Token-based authentication and end- to-end encryption are used as the basis of security. Coda provides failure resiliency through two distinct mechanisms. It uses server replication, or the storing of copies of files at multiple servers, to provide a highly available shared storage repository. When no server can be contacted, the client re¬ sorts to disconnected operation, a mode of execution in which the client relies solely on cached data. Neither mechanism is adequate alone. While server replication increases the availability of all shared data, it does not help if all servers fail or if all are inaccessible due to a network failure adja¬ cent to a client. On the other hand, perma¬ nent disconnected operation is infeasible. The disk storage capacity of a client is a small fraction of the total shared data. Permanent disconnected operation is also inconsistent with the Andrew model of treating each client’s disk merely as a cache. Key advantages of the Andrew architecture, namely mobility and a user’s ability to treat any workstation as his or her own, are lost. From a user’s perspective, transitions between these complementary mecha¬ nisms are seamless. A client relies on server replication as long as it remains in contact with at least one server. It treats disconnected operation as a measure of last resort and reverts to normal operation at the earliest opportunity. A portable client that is isolated from the network is effec¬ tively operating in disconnected mode. When network partitions occur, Coda allows data to be updated in each partition but detects and confines conflicting up¬ dates as soon as possible after their occur¬ rence. It also provides mechanisms to help May 1990 15 Figure 5. Servicing a cache miss in Coda: the events that follow from a cache miss at the client. Both data and status are fetched from Server 1, which is the preferred server (PS). Only status is fetched from Server 2 and Server 3. The calls to all three servers occur in parallel. users recover from such conflicts. This strategy is optimistic, in contrast to a pes¬ simistic strategy that would preserve strict consistency by disallowing updates in all but one partition. We chose an optimistic strategy for two reasons: First, we saw no clean way to support disconnected opera¬ tion with a pessimistic strategy. Second, it is widely believed that sequential write sharing between users is relatively infre¬ quent in Unix environments, so conflicting updates are likely to be rare. Coda provides a scalable and highly available approximation of Unix seman- Figure 6. A store operation in Coda: the two phases of the Coda update protocol. In the first phase, COP1, the three servers are sent new status and data in paral¬ lel. In the later asynchronous phase, COP2, the update set is sent to these serv¬ ers. COP2 also occurs in parallel and can be piggybacked on the next COP1 to these servers. tics. We arrived at this semantics on the basis of our positive experience with AFS- 2. In the absence of failures. Coda and AFS-2 semantics are identical. On open, the latest copy of a file in the system is cached from Vice. Read and write opera¬ tions are made to the cached copy. On close, the modified file is propagated to Vice. Future opens anywhere in the system will see the new copy of the file. In the presence of failures, Coda and AFS-2 semantics differ. An open or close in AFS- 2 would fail if the server responsible for the file was inaccessible. In Coda, an open fails only on a cache miss during discon¬ nected operation or if a conflict is detected. A close fails only if a conflict is detected. Server replication. The unit of replica¬ tion in Coda is a volume. A replicated volume consists of several physical vol¬ umes, or replicas, that are managed as one logical volume by the system. Individual replicas are not normally visible to users. The set of servers with replicas of a volume constitutes its volume storage group (VSG). The degree of replication and the identity of the replication sites are speci¬ fied when a volume is created. Although these parameters can be changed later, we do not anticipate such changes to be fre¬ quent. For every volume from which it has cached data, Venus keeps track of the subset of the VSG that is currently acces¬ sible. This subset is called the accessible VSG (AVSG). Different clients may have different AVSGs for the same volume at a given instant. Venus performs periodic probes to detect shrinking or enlargement of the AVSGs from which it has cached data. These probes are relatively infre¬ quent, occurring once every 10 minutes in our current implementation. Coda integrates server replication with caching, using a variant of the read-one, write-all strategy. This variant can be characterized as read-one-data, read-all¬ status, write-all. In the common case of a cache hit on valid data, Venus avoids con¬ tacting the servers altogether. When ser¬ vicing a cache miss, Venus obtains data from one member of its AVSG, known as the preferred server. The PS can be chosen at random or on the basis of performance criteria such as physical proximity, server load, or server CPU power. Although data is transferred only from one server, Venus contacts the other servers to collect their versions and other status information. Venus uses this information to check whether the accessible replicas are equiva¬ lent. If the replicas are in conflict, the 16 COMPUTER system call that triggered the cache miss is aborted. If the replicas are not in conflict but some replicas are stale, the AVSG is notified asynchronously that a refresh is necessary. In the special case where the data on the PS is stale, a new PS is selected and the fetch is repeated. The message exchange in the normal case, where there is no conflict and the PS has the latest copy of the data, is illustrated in Figure 5. A call¬ back is established with the PS as a side- effect of successfully fetching the data. When a file is closed after modification, it is transferred to all members of the AVSG. This approach is simple to imple¬ ment and maximizes the probability that every replication site has current data at all times. Server CPU load is minimized be¬ cause the burden of data propagation is on the client rather than the servers. This in turn improves scalability, since the server CPU is the bottleneck in many distributed file systems. Operations that update direc¬ tories, such as creating a new directory or removing a file, are also written through to all AVSG members. Because its replication scheme is opti¬ mistic, Coda checks for existing conflicts on each server operation. The update protocol also guarantees eventual detec¬ tion of new conflicts caused by the update. This protocol consists of two phases, COP1 and COP2, where COP stands for Coda optimistic protocol. The first phase performs the semantic part of the opera¬ tion, such as transferring file contents, making a directory entry, or changing an access list. Each server verifies that its copy does not conflict with the client’s copy before performing the update. The second phase distributes to the servers a data structure called the update set, which summarizes the client’s knowledge of who performed the COP1 operation. The up¬ date set maintains the version information used in conflict detection. Figure 6 illus¬ trates the message exchange in a store operation (which corresponds to a file close). Two protocol optimizations improve performance: First, latency is reduced by Venus’s returning control to the user after completion of COP1 and performing COP2 asynchronously. Second, network and server CPU loads can be reduced by Venus’s piggybacking the asynchronous COP2 messages on subsequent COP1 calls to the same VSG. At present, a server performs no explicit remote actions upon recovery from a crash. Rather, it depends on clients to notify it of stale or conflicting data. Although this lazy strategy does not violate Coda’s con¬ sistency guarantees, it does increase the chances of a future conflict. A better ap¬ proach, which we plan to adopt in the future, is for a recovering server to contact other servers to bring itself up to date. Each server operation in Coda typically involves multiple servers. If the operation were carried out sequentially, latency would increase significantly. Venus there¬ fore communicates with replication sites in parallel, using a parallel RPC mecha¬ nism. This mechanism has been extended to use hardware multicast support, if avail¬ able, to reduce the latency and network load caused by shipping large files to multiple sites. Shipping a large file to three servers in our current implementation typi¬ cally takes about 10 percent longer than shipping it to one server. Operation latency is usually a major concern with replication schemes, but server replication in Coda has worked well. Controlled experiments on identical client and server hardware show that under light loads Coda’s performance is within five percent of the performance of Andrew’s current release. Thus, the cost of replica¬ tion is primarily the storage cost for addi¬ tional replicas at the servers. The current implementation of Coda does not perform quite as well under heavy load. Our mea¬ surements indicate specific areas for im¬ provement, and we are confident that these changes will result in an implementation with significantly better performance un¬ der load. Disconnected operation. Disconnected operation begins at a Coda workstation when no member of a VSG is accessible. Clients view it as a temporary state and revert to normal operation at the earliest opportunity. A client may be operating in disconnected mode with respect to some volumes but not others. Disconnected operation is transparent to a user unless a cache miss occurs. A cache miss normally aborts the system call that triggered the reference, but it is possible to arrange for such system calls to block. Return to nor¬ mal operation is also transparent, unless a conflict is detected. To reduce the chances of a cache miss during disconnected operation. Coda al¬ lows a user to specify a prioritized list of files and directories that Venus should strive to retain in the cache. Objects of the highest priority level are “sticky” and must be retained at all times. As long as the local disk is large enough to accommodate all sticky files and directories, the user can always access them. Since it is often diffi¬ cult to know exactly what file references are generated by a certain set of high-level user actions, Coda provides the ability for a user to bracket a sequence of high-level actions and for Venus to note the file refer¬ ences generated during these actions. The implementer of an application can also provide a list of files that should be made sticky for the application to work when disconnected. Disconnected operation with respect to a particular volume ends when Venus rees¬ tablishes connection with any member of the volume’s VSG. Reconnection results from a successful probe—either one of Venus’s periodic probes or one manually induced by a user-level command. The transition from disconnected operation invokes a process of reintegration. For each cached file or directory that has been created, deleted, or modified on the client during disconnected operation, Venus executes a sequence of update operations to make AVSG replicas identical to the cached copy. Reintegration proceeds top- down, from the root to the leaves of modi¬ fied subtrees. Update operations during reintegration may fail for one of two reasons. First, there may be no authentication tokens that Ve¬ nus can use to communicate securely with AVSG members. Users whose tokens expire during disconnected operation may forestall reintegration until they have re¬ acquired valid tokens to minimize this possibility. Second, conflicts may be de¬ tected. Given our model in which servers rather than clients are dependable storage repositories, we felt that the proper ap¬ proach to handling both of these situations was to find a temporary home on servers for the data in question and to rely on a user to resolve the problem later. The temporary repository is realized as a covolume for every replica of every vol¬ ume in Coda. Covolumes are similar in spirit to lost+found directories in Unix. They have a flat name space derived from the original low-level identifiers of the objects they contain. Covolumes are not directly visible to users but are accessed indirectly through a repair tool as de¬ scribed in the next section. Migrate is the operation that transfers a file or directory from a workstation to a covolume. Having a covolume per replica allows us to per¬ form migration immediately upon reinte¬ gration failure rather than waiting for connection to a particular site. The storage overhead of this approach is usually small since a covolume is almost always empty. May 1990 17 Mechanisms for building distributed file systems Although there is considerable diver¬ sity in the manner in which distributed file systems are put together, all the cur¬ rent ones are built from a surprisingly small number of basic mechanisms. This sidebar presents the most impor¬ tant of these mechanisms and examples of how different systems have used them. • Mount points. The mount mecha¬ nism in Unix enables the gluing together of filename spaces to provide applica¬ tions with a single, seamless, hierarchi¬ cally structured name space. In a dis¬ tributed Unix file system, the mount mechanism provides a natural hook on which to hang a remote subtree. There are two different ways to use the mechanism. The simpler approach is used by systems such as Sun NFS, in which each client individually mounts subtrees from servers. Although this ap¬ proach is easier to implement, it has the disadvantage that the shared name space may not be identical at all clients. Further, movement of files from one server to another requires each client to unmount and remount the affected sub¬ tree. In practice, systems that use this approach have usually had to provide auxiliary mechanisms (such as the Yel¬ low Pages and Automounter in Sun NFS) to automate and centralize mounts. The alternative approach is to embed mount information in the data stored in the file servers. Andrew and Coda, for example, use mount points embedded in volumes. Sprite uses remote links for a similar purpose. This approach makes it relatively simple to ensure that all clients see the same shared filename space at all times. • Client caching. The caching of data at clients is undoubtedly the archi¬ tectural feature that contributes most to performance in a distributed file system. Every distributed file system in serious use today uses some form of caching. Even AT&T's RFS, which initially avoided caching in the interests of strict Unix emulation, now uses it. In most systems, clients maintain the cache in their main memory. Andrew and Coda, in contrast, cache on the local disk, with a further level of caching in main mem¬ ory. A key issue in caching is the size of the cached units of data. Most distrib¬ uted file systems cache individual pages of files. Coda, Amoeba, AFS-1, and AFS-2 cache entire files. AFS-3 and most other file systems cache portions of a file. Cache validation can be done in two ways. One approach is for the client to contact the server for validation. The al¬ ternative approach, used in AFS-2, AFS- 3, Coda, AIX-DS, and Echo, is to have the server notify clients when cached data is about to be rendered stale. Al¬ though more complex to implement, the latter approach can produce substantial reductions in client-server traffic. Existing systems use a wide spectrum of approaches in propagating modifica¬ tions from client to server. AIX-DS usu¬ ally propagates changes to the server only when the file is explicitly flushed. Andrew and Coda propagate changes when a file is closed after writing. Sprite delays propagation until dirty cache pages have to be reclaimed or for a maximum of 30 seconds. Deferred propagation improves performance since data is often overwritten, but it increases the possibility of server data being stale due to a client crash. • Hints. In the context of distributed systems, a hint is a piece of information that can substantially improve perform¬ ance if correct but has no semantically negative consequence if erroneous. For maximum performance benefit, a hint should nearly always be correct. By caching hints, one can obtain substantial performance benefits without incurring the cost of maintaining cache consis¬ tency. Only information that is self-vali¬ dating upon use is amenable to this strat¬ egy. One cannot, for instance, treat file data as a hint because the use of a cached copy of the data will not reveal whether it is current or stale. Hints are most often used for file loca¬ tion information in distributed file sys¬ tems. Sprite, for instance, caches map¬ pings of pathname prefixes to servers. Similarly, Andrew and Coda cache indi¬ vidual entries from the volume location database. Apollo Domain uses a more elaborate location scheme incorporating a hint manager. • Bulk data transfer. Network commu¬ nication overhead caused by protocol processing typically accounts for a major portion of the latency in a distributed file system. Transferring data in bulk reduces this overhead by amortizing fixed proto¬ col overheads over many consecutive pages of a file. For effectiveness, bulk transfer protocols depend on spatial lo¬ cality of reference within files. The degree to which bulk transfer is exploited varies from system to sys¬ tem. Amoeba, Andrew, and Coda are critically dependent on it. Sun NFS and Sprite exploit bulk transfer by using very large packet sizes, typically 8 kilo¬ bytes. Bulk transfer protocols will in¬ crease in importance as distributed file systems spread across networks of wider geographic area and thus have greater inherent latency. • Encryption. Encryption is an indis¬ pensable building block for enforcing security in a distributed system. It is used for remote authentication and for preventing unauthorized release and modification of data transmissions. The national standard DES (data encryp¬ tion standard) is the most commonly used form of private-key encryption. The seminal work of Needham and Schroeder on the use of encryption for authentication is the basis of all current security mechanisms in distributed file systems. Authentication can be performed with private or public keys. In the pri¬ vate-key schemes used by Kerberos and Andrew, a physically secure au¬ thentication server maintains a list of user passwords in the clear. In con¬ trast, the public-key scheme used by Sun NFS maintains a publicly readable database of authentication keys en¬ crypted with user passwords. The latter approach has the attractive character¬ istic that physical security of the au¬ thentication server is unnecessary. Its major drawback is that public-key en¬ cryption is computationally more ex¬ pensive. • Replication. Replication of data at multiple servers is the primary mecha¬ nism for providing high availability. The more recent file systems such as Coda and Echo provide read-write replication of data. Amoeba supports read-write replication at the directory level be¬ cause files are immutable in that sys¬ tem. Although read-write replication is well understood theoretically, little ex¬ perience of its use exists as yet. More experience has been gathered with read-only data replication, which is supported by systems such as Sun NFS and Andrew. Though suitable only for files that change relatively rarely, it is valuable because many critical files (such as system binaries) possess this property. COMPUTER NEED TO KEEP UP TO DATE WITH COMPUTER SCIENCE? Computer Science is a rapidly-developing field of research. You need high-quality scientific information about the latest developments and research results in order to keep up to date. North-Holland offers you an extensive high-level publication programme with journals and books covering a wide range of subject areas, such as: Information Systems, Computer Communications, Distributed Systems & Telecommunications, Theoretical Computer Science & Database Theory, Artificial Intelligence, Expert Systems, Knowledge Engineering, Computer Architecture, Parallel and Supercomputing, Computer Performance, Simulation and Modelling, Industrial Automation, Computer Graphics, Pattern Recognition, Computers in Education, Computers & Security. North-Holland's Computer Science Catalogue 1990 contains full details of our latest books and our range of journals. Write for your personal copy of the catalogue by completing the coupon! NORTH-HOLLAND PUBLICATIONS North-Holland is an imprint of Elsevier Science Publishers BV, one of the world's largest scientific publishers. COUPON COMP/M Send this coupon - or a photocopy - to: North-Holland Customers in the USA/Canada: (An Imprint of Elsevier Science Publishers BV) Attn.: Petra van der Meer Elsevier Science Publishing Co. Inc. P.O. Box 103 P.O. Box 882 1000 AC Amsterdam Madison Square Station The Netherlands New York, NY 10159, USA D Yes, please send me my personal copy of the Computer Science Catalogue 1990 Name Address Country Reader Service Number 3 We are currently in the midst of imple¬ menting disconnected operation. Although we are confident of our ability to support short-term disconnected operation (for a few minutes or hours), it remains to be seen whether long-term disconnected operation (for days or weeks) is feasible. Our con¬ cerns center on the overall size of the working set and on the predictive power of our caching strategies. Our own experi¬ ence, and that of others, suggests that a cache of several tens of megabytes should be adequate for a typical disconnection of less than a day. Less obvious is whether any anticipatory caching strategy can, with a reasonable cache size, provide the near¬ perfect hit rates required for long-term disconnected operation. Conflict resolution. When a conflict is detected. Coda first attempts to resolve it automatically. Since Unix files are un¬ typed byte streams, there is no information to automate their resolution. A directory, on the other hand, is an object whose semantics is completely known and whose resolution can often be automated. For example, partitioned creation of uniquely named files in the same directory can be handled automatically by selectively re¬ playing the missing creates. If automated resolution is not possible, Coda marks all accessible replicas of the object inconsis¬ tent and moves them to their respective covolumes. This ensures damage contain¬ ment because normal operations on these replicas will subsequently fail. Coda provides a repair tool to assist users in manually resolving conflicts. It uses a special interface to Venus so that requests from the tool are distinguishable from normal file system requests. This enables the tool to overwrite inconsistent files and to perform directory operations on inconsistent directories. The tool has evolved along with the rest of our system. Three generations of the tool are described here: the tool for the currently imple¬ mented system, the one we are working on at present, and a successor that will incor¬ porate the current wish list. In the first-generation tool, inconsistent files and directories are marked in conflict but are not moved to covolumes. Discon¬ nected operation is not supported because there is nowhere to migrate objects to. When the tool is invoked on a given object, it mounts the accessible replicas of the object’s volume in a scratch area of the name space. The user can then use normal Unix applications to inspect the replicas. The replicas are mounted in read-only The general problem of sharing information effectively in large distributed systems is far from being solved. The next decade poses many challenges and promises to be a fertile and exciting period for researchers in this area. mode so that the user cannot inadvertently alter anything. When the user has decided on a fix (such as selecting the version in one of the replicas to be the new permanent one), the tool performs the fix and cleans up the workspace. The second-generation tool supports disconnected operation because it knows about covolumes. Inconsistent objects are immediately moved to the associated covo¬ lume when the inconsistency is detected. When the tool is invoked, it constructs a temporary workspace and mounts, read¬ only, the covolumes as well as the replicas of the object. As before, the user can navi¬ gate through the replicas. However, names of inconsistent objects now correspond to objects in the associated covolumes. The tool applies the fix and cleans up the work¬ space as the first-generation tool does. The primary refinement provided by the third-generation tool will be a considerably simplified user interface. Venus, in con¬ junction with the tool, will present the illusion of an in-place “explosion” of in¬ consistent objects into their distinct ver¬ sions. Invocation of the tool will put Venus in a mode whereby inconsistent objects can be viewed within the existing name space. In this mode, Venus will map an inconsis¬ tent file or directory into a read-only direc¬ tory with the same name as the original. This directory will be populated with en¬ tries translated by Venus to the versions in the various covolumes. The tool will handle the fix phase of the repair in the same way as the second-generation tool. T hroughout the evolution of the Andrew and Coda file systems, the underlying model of computa¬ tion has remained unchanged. A small col¬ lection of trusted servers jointly provides a shared data repository for a much larger number of untrusted workstations. The system design facilitates incremental growth by the addition of users and work¬ stations. The security of the system is not contingent upon the integrity of the work¬ stations or of the network. The problems of scalability, security, and availability will continue to be impor¬ tant as distributed file systems grow in size. In addition, three other problems will be of fundamental importance to the broader goal of effective data sharing in large distributed systems. These are the problems of heterogeneity, access to di¬ verse types of data, and rapid search. As a distributed system grows, it tends to become more heterogeneous. Coping with heterogeneity is inherently difficult because of the presence of multiple com¬ putational environments, each with its own notions of file naming and functional¬ ity. Since few general principles are appli¬ cable, the idiosyncrasies of each new sys¬ tem have to be accommodated by ad hoc mechanisms. Unfortunately, heterogene¬ ity cannot be ignored since it is likely to be a chronic problem. Alternative models of data are likely to become more important in the future. Re¬ lational databases are already in wide¬ spread use in certain application domains. Speech, music, images, and video are examples of other forms of data that the repositories of the future will have to store and retrieve. We presently have little knowledge of how to share such diverse types of data in large-scale distributed sys- Finding data in a large distributed file system is already difficult. As distributed storage repositories grow larger and store more diverse types of data, the problem of searching for relevant information will become acute. This is another area where we have barely scratched the surface. In conclusion, we have made much progress in the design and implementation of distributed file systems over the last decade. Andrew and Coda embody many of the key advances made during this pe¬ riod. But the general problem of sharing information effectively in large distrib¬ uted systems is far from being solved. The next decade poses many challenges and promises to be a fertile and exciting period for researchers in this area. ■ 20 COMPUTER Acknowledgments The Andrew file system was built by the File System Group of the In¬ formation Technology Center at Carnegie Mellon University. The mem¬ bership of this group over time has included Ted Anderson, Sailesh Chutani, John Howard, Michael Kazar, Sherri Menees Nichols, David Nichols, Mahadev Satyanarayanan, Robert Sidebotham, Michael West, and Edward Zayas. Contributions to the early design of Andrew were also made by David Gifford and Alfred Spector. Coda is being built in the School of Computer Science at Carnegie Mellon University. Contributors to Coda include James Kistler, Puneet Kumar, Maria Okasaki, Mahadev Satyanarayanan, Ellen Siegel, Walter Smith, and David Steere. James Kistler assisted in writing this article. This research was supported by the National Science Foundation (contract No. CCR-8657907), Defense Advanced Research Projects Agency (order No. 4976, contract No. F33615-87-C-1499), IBM Corpo¬ ration (faculty development award, graduate fellowship, and the Andrew project), and Digital Equipment Corporation (equipment grant). The views and conclusions in this article are those of the author and do not represent the official policies of the funding agencies or of Carnegie Mellon University. References 1. J.H. Howard et al., “Scale and Performance in a Distributed File System,” ACM Trans. Computer Systems, Vol. 6, No. 1, Feb. 1988, pp. 51-81. 2. M.N. Nelson, B.B. Welch, and J.K. Ousterhout, “Caching in the Sprite Network File System,” ACM Trans. Computer Systems, Vol. 6, No. 1, Feb. 1988, pp. 134-154. 3. M. Satyanarayanan, \"Integrating Security in a Large Distributed System,” ACM Trans. Computer Systems, Vol. 7, No. 3, Aug. 1989, pp. 247-280. 4. R.M Needham and M.D. Schroeder, “Using Encryption for Authen¬ tication in Large Networks of Computers,” Comm. ACM, Vol. 21, No. 12, Dec. 1978, pp. 993-998. 5. J.G. Steiner, C. Neumann, and J.I. Schiller, “Kerberos: An Authen¬ tication Service for Open Network Systems,” Proc. Usenix Conf., Dallas, Texas, Feb. 1988, pp. 191-202. 6. M. Satyanarayanan et al., “Coda: A Highly Available File System for a Distributed Workstation Environment,” IEEE Trans. Computers, Vol. 39, No. 4, Apr. 1990, pp. 447-459. Mahadev Satyanarayanan is an associate professor of computer sci¬ ence at Carnegie Mellon University. His research addresses the general problem of sharing access to information in large-scale distributed sys¬ tems. He was one of the principal architects and implementers of the Andrew file system and currently leads the Coda project. His work on Scylla explored access to relational databases in a distributed workstation environment. His previous research included the design of the CMU-CFS file system, measurement and analysis of file usage data, and the model¬ ing of storage systems. Satyanarayanan received the PhD in computer science from Carnegie Mellon in 1983, after receiving a bachelor’s degree in electrical engineer¬ ing and a master’s degree in computer science from the Indian Institute of Technology, Madras. He is a member of the IEEE, the IEEE Computer Society, ACM, and Sigma Xi, and has been a consultant to industry and government. He was named a Presidential Young Investigator by the Na¬ tional Science Foundation in 1987. Readers can write to Satyanarayanan at the School of Computer Sci¬ ence, Carnegie Mellon University, Pittsburgh, PA 15213-3890. At GTE’s Computer and Intelligent Systems Laboratory, we are creating opportunities to apply new ideas to research and development projects for improved information and telecom¬ munication systems. By expanding our scope and responsi¬ bility, we can better support GTE’s telecommunications businesses. And our activities create challenges for individu¬ als at the MS/PhD level in Computer Science. Join us as we continue to make history in telecommunications technology. Our present areas of interest include: Distributed Operating Systems We are currently growing a group that is conducting research in distributed operating systems and dis¬ tributed transaction systems. These systems will unify a network of cooperating autonomous, heterogeneous processors and replicated databases. Issues concern not only the tradeoffs between network and distributed operating systems, but the interoperability of software components implemented on a mix of platforms and languages; included are such topics as transport, access, application, distributed control and control migration. Research is conducted using synthesis and prototyping with emphasis on architectural and appli¬ cability issues. We are looking for individuals at all lev¬ els of experience, however, we require a PhD in Computer Science along with a familiarity with the var¬ ious current approaches to the design of distributed systems. Significant experience with at least one such system is highly desirable. Intelligent Database Systems Our Distributed Object Management (DOM) project is con¬ ducting research into interconnectivity and intelligent inter¬ operability among heterogeneous computer systems. We require PhD-level researchers at all levels with a minimum of 2 years’ experience in databases, operating systems, dis¬ tributed systems or artificial intelligence. Research Scientist Information Retrieval We are extending the state-of-the-art in Information Retrieval systems, primarily through the development of a multi-media system that includes full-text retrieval, graphics, pictures and other structurable information; algorithms and their implementation for document modeling; and the user-interface to the retrieval sys¬ tem itself. Research prototypes will be developed in C and X-windows on multiple operating system plat¬ forms. We require a minimum of a MS in Computer Science, and 3 years’ programming with C under vari¬ ous operating systems such as VMS, UNIX, MS-DOS, and Macintosh. GTE Laboratories offers attractive facilities located in a quiet, wooded setting just outside of Boston, as well as a highly competitive salary and benefits package. We invite you to send a resume to Vanessa Stern, GTE Laboratories, Inc., Box IEEEC590,40 Sylvan Road, Waltham, MA 02254. An equal opportunity employer, M/F/H/V. Laboratories THE POWER IS ON May 1990 19 Reasons Why We re Proud to Work for Amdahl T he results of the 1989 Datapro Survey of Mainframe Computer Users are a good indication of the pride we take in our work at Amdahl. Once again, our people swept the field, earning the highest ratings in 19 of 25 performance categories. • We ranked first in “overall customer satisfac¬ tion” for the second year running. • We ranked first in mainframe reliability, main¬ tenance, service and technical support. • We were the only company to notch a per¬ fect score - 100% -when mainframe users were asked if their system met all of their expectations. • We were also the only company to score a perfect 100% when users were asked if they’d recommend their system to a fellow user. To earn such accolades, year after year, we put the greatest emphasis on recruiting and developing a staff of the highest caliber. Our projects demand new ideas, tech¬ niques and solutions. So, as large as we are, with more than $2 billion in annual sales worldwide, we structure our teams for originality, visibility and the encouragement of personal and professional growth. We also recognize outstanding performance, rewarding our people for meeting our high level of expectation. Our com¬ petitive salaries and comprehensive benefits are first class. And, as the results of the Datapro survey show, our cus¬ tomers recognize our efforts in at least 19 different ways. STORAGE PRODUCTS. Storage Products develops and markets innovative, high-perfor¬ mance peripherals for on-line data storage The 6100 storage controller adheres to industry standards, while providing greater capacity and flexibility than the competition. The 6380 family of disk storage devices offer more - more performance, more capacity, more features-in a durable, compact cabinet. • Software Development -MVS/XA Internals -370 Assembly Language -I/O Diagnostic Programming • Firmware Development -Microcode Development - Controller Development -Assembly Language Development -Firmware Diagnostics TECHNICAL MARKETING Our UNIX Systems Group documents benchmark requirements to our Amdahl Performance Evaluation Center (AMPEC) and monitors all completion time frames. We also evaluate competitive products and provide configuration results for publication. We require internal/external knowledge of UNIX (UTS**), with an em¬ phasis on configuration, performance and networking imple¬ mentation, to perform in the following areas: PROCESSOR PRODUCTS. This division designs and develops Amdahl’s large-scale, high-performance mainframe systems and product software These systems implement industry-leading packaging and design innovations and are compatible with the industry-standard System/370-architecture This division hires individuals with all levels of experience and a BSEE/ MSEE/PhDEE or BSCS/MSCS/PhDCS, or equiva¬ lent, in the following areas: Product Software & Diagnostics - MVS, VM Operating Systems Internals - 370 Assembling - Diagnostics -UNIX* Development -C and REXX Programming -Macrocode Development Design Automation - Simulation -Timing Analysis -Test Generation -Design Verification - System Administration System Architecture - Computer Architecture - Interface Development Computer Development - Circuit Design - Packaging Technology -Logic Design S/W Engineering Application Development -MVS Programming -Relational Database Development - 370 Assembler & C For Storage Products and Technical Marketing positions, con¬ tact Dottie DeSelle at 800-538-8460, ext. 75981, or send your resume to her at Mail Stop 300. The satisfaction of exceptional challenge awaits you. Send your resume to Amdahl Corporation, Employment Department 4-3, PO. Box 3470, Mail Stop 300, Sunnyvale, CA 94088-3470. Principals only, please Amdahl Corporation is proud to be an equal opportunity employer through affirmative action. ‘UNIX is a registered trademark of AT&T. “UT5 is a trademark of Amdahl Corporation. Contact Ranell Ehtrgan at 800-538-8460, ext. 66216 or send your resume to her at Mail Stop 300. amdahl The x-kernel: A Platform for Accessing Internet Resources Larry Peterson, Norman Hutchinson, Sean O’Malley, and Herman Rao University of Arizona T he x-kemel is an experimental operating system for personal workstations that allows uniform access to resources throughout a nation¬ wide internet: an interconnection of net¬ works similar to the TCP/IP Internet. This network is also called the National Re¬ search and Education Network (NREN). This focus on national rather than local resources is central to thejc-kernel’s design and sets it apart from other experimental operating systems. A current trend in oper¬ ating system design is to make a collection of workstations appear to function as a single time-sharing system. In such a sys¬ tem, workstations on the same local area network are tightly coupled: they run the same kernel and the same special-purpose protocol suite. Such designs include the V system and its versatile message-transac¬ tion protocol, 1 the Sprite network operat¬ ing system and its remote procedure call (RPC) protocol, 2 and the Amoeba system and its RPC protocol. 3 However, workstation users connected to national networks have access to signifi¬ cantly more resources than are available on local networks. The NREN, for example, connects researchers throughout the coun¬ try to databases, file systems, directory services, supercomputers, and other spe¬ cialized hardware. The availability of re¬ sources on such national networks will grow as network bandwidth and connec¬ tivity increase. The Jt-kernel gives workstation users uniform access to resources across local or wide area networks. The operating system is general, efficient, and easy to program. The key to making such diverse re¬ sources available is to implement the nec¬ essary communication protocols on the user’s workstation. For example, a work¬ station must implement the NFS protocol to access a file in a Sun network file sys¬ tem 4 and the AFS protocol to access a file in an Andrew file system. 5 Similarly, a workstation must implement the Sun RPC protocol to invoke a service provided by the Sun operating system 6 and the Sprite RPC protocol to invoke a service provided by the Sprite operating system. In general, the more protocols on the user’s worksta¬ tion, the more resources the user can ac¬ cess. Of course, user-level software must 0018-9162/90/0500-0023501.00 © 1990 IEEE also be available to take advantage of these resources. While a single file-access protocol, RPC protocol, and directory protocol would simplify matters, it is not likely that a single protocol suite can provide access to"
    }
}
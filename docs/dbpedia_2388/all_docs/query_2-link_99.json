{
    "id": "dbpedia_2388_2",
    "rank": 99,
    "data": {
        "url": "https://arxiv.org/html/2305.12544v2",
        "read_more_link": "",
        "language": "en",
        "title": "Has It All Been Solved? Open NLP Research Questions Not Solved by Large Language Models",
        "top_image": "",
        "meta_img": "",
        "images": [
            "https://arxiv.org/html/x2.png",
            "https://arxiv.org/html/x3.png",
            "https://arxiv.org/html/x4.png",
            "https://arxiv.org/html/x5.png",
            "https://arxiv.org/html/x6.png",
            "https://arxiv.org/html/x18.png",
            "https://arxiv.org/html/x9.png",
            "https://arxiv.org/html/x10.png",
            "https://arxiv.org/html/x11.png",
            "https://arxiv.org/html/x12.png",
            "https://arxiv.org/html/x13.png",
            "https://arxiv.org/html/x14.png",
            "https://arxiv.org/html/x15.png",
            "https://arxiv.org/html/x16.png",
            "https://arxiv.org/html/x1.png",
            "https://arxiv.org/html/x2.png",
            "https://arxiv.org/html/x3.png",
            "https://arxiv.org/html/x4.png",
            "https://arxiv.org/html/x5.png",
            "https://arxiv.org/html/x6.png",
            "https://arxiv.org/html/x7.png",
            "https://arxiv.org/html/x9.png",
            "https://arxiv.org/html/x10.png",
            "https://arxiv.org/html/x11.png",
            "https://arxiv.org/html/x12.png",
            "https://arxiv.org/html/x13.png",
            "https://arxiv.org/html/x14.png",
            "https://arxiv.org/html/x15.png",
            "https://arxiv.org/html/x16.png",
            "https://arxiv.org/html/x17.png"
        ],
        "movies": [],
        "keywords": [],
        "meta_keywords": [
            ""
        ],
        "tags": null,
        "authors": [],
        "publish_date": null,
        "summary": "",
        "meta_description": "",
        "meta_lang": "en",
        "meta_favicon": "",
        "meta_site_name": "",
        "canonical_link": null,
        "text": "HTML conversions sometimes display errors due to content that did not convert correctly from the source. This paper uses the following packages that are not yet supported by the HTML conversion tool. Feedback on these issues are not necessary; they are known and are being worked on.\n\nfailed: twemojis\n\nfailed: kantlipsum\n\nAuthors: achieve the best HTML results from your LaTeX submissions by following these best practices.\n\nLicense: arXiv.org perpetual non-exclusive license\n\narXiv:2305.12544v2 [cs.CL] 15 Mar 2024\n\nHas It All Been Solved? Open NLP Research Questions\n\nNot Solved by Large Language Models\n\nAbstract\n\nRecent progress in large language models (LLMs) has enabled the deployment of many generative NLP applications. At the same time, it has also led to a misleading public discourse that “it’s all been solved.” Not surprisingly, this has, in turn, made many NLP researchers – especially those at the beginning of their careers – worry about what NLP research area they should focus on. Has it all been solved, or what remaining questions can we work on regardless of LLMs? To address this question, this paper compiles NLP research directions rich for exploration. We identify fourteen different research areas encompassing 45 research directions that require new research and are not directly solvable by LLMs. While we identify many research areas, many others exist; we do not cover areas currently addressed by LLMs, but where LLMs lag behind in performance or those focused on LLM development. We welcome suggestions for other research directions to include: https://bit.ly/nlp-era-llm\n\nKeywords: Large language models, challenges for NLP, open questions, applied NLP, responsible NLP, fundamental NLP\n\n\\NAT@set@cites\n\nHas It All Been Solved? Open NLP Research Questions\n\nNot Solved by Large Language Models\n\nOana Ignat*{}^{*}start_FLOATSUPERSCRIPT * end_FLOATSUPERSCRIPT, Zhijing Jin*{}^{*}start_FLOATSUPERSCRIPT * end_FLOATSUPERSCRIPT, Artem Abzaliev, Laura Biester, Santiago Castro, Naihao Deng, Xinyi Gao, Aylin Gunal, Jacky He, Ashkan Kazemi, Muhammad Khalifa, Namho Koh, Andrew Lee, Siyang Liu, Do June Min, Shinka Mori, Joan Nwatu, Veronica Perez-Rosas, Siqi Shen, Zekun Wang, Winston Wu, Rada Mihalcea University of Michigan Language and Information Technologies (LIT) {oignat,jinzhi,mihalcea}@umich.edu\n\nAbstract content\n\n1. Background\n\nLanguage models represent one of the fundamental building blocks in NLP, with their roots traced back to 1948 when Claude Shannon introduced Markov chains to model sequences of letters in English text (Shannon, 1948). They were then heavily used in connection with early research on statistical machine translation (Brown et al., 1988; Wilkes, 1994) and statistical speech processing (Jelinek, 1976). While these models have always been an integral part of broad application categories such as text classification, information retrieval, or text generation, only in recent years have they found a “life of their own” with widespread use and deployment.\n\nThe impressive advancements we have witnessed in current large language models (LLMs) directly result from those earlier models. They build on the same simple yet groundbreaking idea: given a series of previous words or characters, we can predict what will come next. The new LLMs benefit from two main developments: (1) the proliferation of Web 2.0 and user-generated data, which has led to a sharp increase in the availability of data; and (2) the growth in computational capabilities through the introduction of GPUs. Together, these developments have facilitated the resurgence of neural networks (or deep learning) and the availability of very large training datasets for these models.\n\nCurrent LLMs have output quality comparable to human performance, with the added benefit of integrating information from enormous data sources, far surpassing what one individual can accumulate in their lifetime. The number of applications that benefit from using LLMs is continuously growing, with many cases where the LLMs are used to replace entire complex pipelines. LLMs becoming “lucrative” has led to a surge in industry interest and funding, alongside a sharp increase in the number of research publications on LLMs.\n\nWhile these advances in LLMs are very real and truly exciting, and give hope for many new generative language applications, LLMs have also “sucked the air out of the room.” A recent funding call from DARPA has completely replaced the term NLP with LLM: in their listing of experts sought for the program, we see the fields of “Computer Vision” and “Machine Learning” listed alongside “Large Language Models” (but not “Natural Language Processing”). Replacing NLP with LLMs is problematic for two main reasons. First, the space of language insights, methods, and broad applications in NLP is much vaster than what can be accomplished by simply predicting the next word. Second, even if not technologically new, LLMs still represent an exclusionary space because of the amount of data and computation required to train.\n\nThis public discourse that often reduces the entire field of NLP to the much smaller space of LLMs is not surprisingly leading to a dilemma for those who have dedicated their careers to advancing NLP research, and especially for junior PhD students who have only recently embarked on the path of becoming NLP researchers. “What should I work on?” is a question we hear now much more often than before, often as a reaction to the misleading thought that “it’s been all solved.”\n\nThe reality is that there is much more to NLP than just LLMs. This paper aims to answer the question: “What are rich areas of exploration in the field of NLP that could lead to a PhD thesis and cover a space that is not within the purview of LLMs.” Spoiler alert: there are many such research areas!\n\nAbout This Document.\n\nThis document reflects the ideas about “the future of NLP research” from the members of an academic NLP research lab in the United States. The Language and Information Technologies (\\twemojifireLIT) lab at the University of Michigan includes students at various stages in their degree, starting with students who are about to embark on a Ph.D., all the way to students who recently completed a Ph.D. degree. The LIT students come from a wide variety of backgrounds, including China, Iran, Japan, Mexico, Nigeria, Romania, Russia, South Korea, the United States, and Uruguay, reflecting a very diverse set of beliefs, values, and lived experiences. Our research interests cover a wide range of NLP areas, including computational social science, causal reasoning, misinformation detection, healthcare conversation analysis, knowledge-aware generation, commonsense reasoning, cross-cultural models, multimodal question answering, non-verbal communication, visual understanding, and more.\n\nWe provide a list of open research questions that are not solved by LLMs. As showed in Figure 1, we cover three major categories, from fundamental NLP (Section 2), to responsible NLP (Section 3), and applied NLP (Section 4). Spanning across the three categories, we cover 14 open research topics, each with three to four specific research directions. Note that when a research topic touches multiple categories, for convenience, we list them under the major one.\n\nWhen compiling the ideas in this document, we followed three main guiding principles. First, we aimed to identify areas of NLP research that are rich for exploration, e.g., areas one could write a Ph.D. thesis on. Second, we wanted to highlight research directions that do not have a direct dependency on a paid resource; while the use of existing paid APIs can be fruitful for certain tasks, such as the construction of synthetic datasets, building systems that cannot function without paid APIs is not well aligned with academic core research goals. Third, we targeted research directions that can find solutions with reasonable computational costs achievable with setups more typically available in academic labs. Finally, we found inspiration in the ACL list of research areas, from which we selected the ones not in the purview of LLMs.\n\nOur brainstorming process started with ideas written on sticky notes by all the authors of this document, followed by a “clustering” process where we grouped the initial ideas and identified several main themes. These initial themes were then provided to small groups of 2–3 students, who discussed them, expanded or merged some of the themes, and identified several directions worthy of exploration. The final set of themes formed the seed of this document. Each research area has then had multiple passes from multiple students (and Rada) to delineate the background of each theme, the gaps, and the most promising research directions.\n\nDisclaimer.\n\nThe research areas listed in this document are just a few of the areas rich in exploration; many others exist. In particular, we have not listed the numerous research directions where LLMs have been demonstrated to lag in performance Bang et al. (2023a), including information extraction, question answering, text summarization, and others. We have also not listed the research directions focused on LLM development, as that is already a major focus in many current research papers, and our goal was to highlight the research directions other than LLM development. We welcome suggestions for other research areas or directions to include: https://bit.ly/nlp-era-llm\n\nDocument Organization.\n\nWe provide a list of open research questions that are not solved by LLMs. As showed in Figure 1, we cover three major categories, from fundamental NLP (Section 2), to responsible NLP (Section 3), and applied NLP (Section 4). Spanning across the three categories, we cover 14 open research topics, each with three to four specific research directions. Note that when a research topic touches multiple categories, for convenience, we list them under the major one.\n\nThe following sections provide brief descriptions of fourteen research areas rich in exploration, each with 3–4 research directions. These areas can be divided into areas that cannot be solved by LLMs for being too data-hungry or for lacking reasoning or grounding abilities (subsections 2.1–2.5, 3.3, 4.3); areas for which we cannot use LLMs because of not having the right data (subsections 2.6, 4.1, 4.2); or areas that could contribute to improving the abilities and quality of LLMs (subsections 3.1, 3.2, 3.4, 4.4). When compiling the research directions, we follow several guiding principles. First, we aim to identify areas that are rich for exploration, that one could write a PhD thesis on. Second, we want to highlight research directions that do not directly depend on a paid resource, such as a paid API. Third, we target research directions that can find solutions with reasonable computational costs, achievable in academic labs. Finally, we find inspiration from ACL 2018 list of research areas, from which we select the ones not in the purview of LLMs (15/21 areas). The mapping of our research areas and the ACL 2018 tracks can be found in Appendix Table 1.\n\n2. Fundamental NLP\n\nFundamental NLP tasks represent a significant subset of NLP research, as illustrated in Figure 2. Among these, we first consider different “L”s in “NLP,” namely different choices of languages. Although most NLP tasks and datasets use English as a medium, there is a growing trend to extend NLP to more non-English languages (Section 2.1), child language (Section 2.5), and non-verbal communication (Section 2.6). Moreover, there are different “P”s in “NLP” too, where we consider different types of processing tasks on text, such as reasoning (Section 2.2), knowledge bases (Section 2.3), and language grounding (Section 2.4).\n\n2.1. Multilinguality\n\nLow-resource machine translation.\n\nDespite the impressive performance of machine translation (MT) on major languages (Hassan et al., 2018a; Liu et al., 2020), there is a big gap when it comes to low-resource languages. There has been a rise in small benchmarks dedicated to low-resource languages (Vegi et al., 2022; Reid et al., 2021; Goyal et al., 2021), but there is also a big need for large training corpora. As many low-resource languages do not have a significant web presence, alternative solutions are needed, such as manually curated parallel corpora (Zheng et al., 2022; Koto and Koto, 2020), OCR (Rijhwani et al., 2020; Ignat et al., 2022), or translation dictionaries using models of word formation (Wu and Yarowsky, 2018, 2020).\n\nMultilingual models that work well for all languages.\n\nAlthough most recent LLMs claim to be multilingual, they do not perform equally well in all languages (OpenAI, 2023; Huang et al., 2023a; Ahuja et al., 2023). This inequality stems from the different proportions of text of different languages in the training corpora, as well as the annotators with demographics focused in a few countries in the RLHF process to finetune the models (Ouyang et al., 2022b). At present, general-purpose LLMs do not perform as well as models trained specifically for translation; future research can explore incorporating off-the-shelf LLMs into MT systems.\n\nCode-switching.\n\nCode-switching refers to text involving expressions in several languages while adhering to the grammatical structure of at least one language. Challenges include the large variation of code-switching phenomena, lack of training data, and a large number of out-of-vocabulary tokens Çetinoğlu et al. (2016). Open research directions include synthetic data generation Xu and Yvon (2021); Fang and Wu (2022); Lee and Li (2020), evaluating existing LLMs on code-switched text across language combinations Aguilar et al. (2020); Khanuja et al. (2020), and distinguishing highly similar languages, such as dialects of the same parent language Aguilar et al. (2020).\n\n2.2. Reasoning\n\nComplex reasoning.\n\nComplex and multi-step reasoning has proven to be challenging for LLMs. For instance, LLMs still fall short in numerical reasoning Stolfo et al. (2023); Miao et al. (2020), logical reasoning Jin et al. (2022c); Eisape et al. (2023), grounded reasoning Ignat et al. (2021), and causal inference Jin et al. (2023a, c), often making obvious mistakes Goel et al. (2021); Jin et al. (2020b). One reason for that is that the next-word prediction objective can easily encourage the LM to assign a high likelihood to invalid reasoning Khalifa et al. (2023). Even fully-supervised training over correct reasoning demonstrations does not solve the issue Uesato et al. (2022). While scaling seems to help, careful prompt engineering is still needed to tease out correct reasoning Wei et al. (2022c); Fu et al. (2022); Zhou et al. (2022b); Zhang et al. (2022c). To build LLMs that are robust at reasoning, one could explore a variety of directions, such as combining the strengths of neural networks and symbolic AI. Another growing direction is the integration of LLMs with external reasoning tools, such as calculators, interpreters, database interfaces, or search engines Schick et al. (2023); Mialon et al. (2023a).\n\nResponsible reasoning in social contexts.\n\nWith an increasing number of applications that use NLP models, it is foreseeable that models will need to make complicated decisions that involve moral reasoning as intermediate steps. For example, when creating a website, there may be moral choices to consider such as catering to certain sub-populations, or overly optimizing for user attention or click-through rates. These decision principles are pervasive in our daily life, across small and large tasks. We believe there is much to be studied in understanding or improving the ability of AI systems to reason over socially-complicated and morally-charged scenarios given different social contexts and cultural backgrounds (Jin et al., 2023b; Hendrycks et al., 2021a; Liu et al., 2021). We foresee that interdisciplinary collaboration with domain experts and policymakers will be needed.\n\nFormally defining reasoning and designing a proper evaluation framework.\n\nThere is a rising need to refine the definition of reasoning, as LLMs start showing an increasing mastery of templated solutions through pattern matching – when a model memorizes a reasoning pattern, does it count as reasoning or knowledge? Fundamentally, this leads to questions about what are the domains of intelligence that humans excel at, and how different these are from empirically learning how to do template matching. Beyond redefining reasoning, other open questions include how to test a model’s reasoning skills in the face of data contamination, Goodhart’s law (a dataset failing to reflect the skill once exploited) (Goodhart, 1984), and a lack of reliable metrics to evaluate multi-step reasoning Golovneva et al. (2022).\n\n2.3. Knowledge Bases\n\nAutomatic knowledge base construction.\n\nSpecialized knowledge bases are helpful resources for domain-specific applications. Successful automatic knowledge base construction can take up-to-date text in free forms Maedche and Staab (2000), and adapt an ontology for complex applications, such as tracking medication interactions from articles from PubMed Xu et al. (2020). However, this task faces many challenges, such as knowledge coverage, factuality of the knowledge, and knowledge linking, which are rich, open areas of research. Specifically, Wang et al. (2023a) shows that ChatGPT performs poorly on out-of-distribution data, such as new medical diagnosis and product review datasets. Also, the training data cutoff limits the coverage of new concepts. Specifically concerning factuality, KG completion framed as a text generation task also suffers from hallucination from the LLM in various tasks Ji et al. (2022).\n\nKnowledge-guided NLP.\n\nAs NLP models become more powerful through exposure to massive pretraining corpora (Hoffmann et al., 2022a; Wei et al., 2022a), researchers start to question whether mere pretraining is sufficient, as models suffer heavily from hallucination (Xiao and Wang, 2021; Dziri et al., 2022a). A rising research question is how to efficiently and effectively interact with external knowledge bases (Zhang et al., 2019), such as through web browsing (Nakano et al., 2021; Komeili et al., 2022; Schick et al., 2023) and customized knowledge base lookup (Wilmot and Keller, 2021; Mialon et al., 2023b).\n\nCulture-specific knowledge and common sense.\n\nKnowledge and common sense in NLP models are usually dominated by a few Western cultures, and do not account for the vast diversity of the cultural views in the world (Arora et al., 2023). The first step is to understand the limitations of NLP models, including LLMs, with respect to their knowledge of different cultural groups (Hovy and Yang, 2021; Hershcovich et al., 2022; Arora et al., 2023). Once these limitations are better understood, a major open research direction is how to acquire and represent the knowledge that encodes these cultural views, as well as how and when to invoke this cultural knowledge.\n\n2.4. Language Grounding\n\nFusing multiple modalities.\n\nEfficiently and effectively combining different modalities, i.e., audio, video, text, and others, is still an open problem. Different modalities often complement each other, thus potentially reducing the need for billions of data points. However, in some cases, modalities end up competing with each other Yao and Mihalcea (2022), and thus many uni-modal models outperform multi-modal models (Wang et al., 2019b; Huang et al., 2021).\n\nGrounding for less studied modalities.\n\nMost work on grounding revolves around visual, textual, or acoustic modalities. However, less-studied modalities, such as physiological, sensorial, or behavioral, have been found valuable in diverse applications, including measuring driver alertness (Jie et al., 2018; Riani et al., 2020), detecting depression (Bilalpur et al., 2023), or detecting deceptive behaviors (Abouelenien et al., 2016). Current multimodal LLMs are restricted to textual, audio, and visual domains Zhang et al. (2023a); Lyu et al. (2023a), requiring much effort to integrate the less-studied modalities.\n\nGrounding “in the wild” and for diverse domains.\n\nMost research around grounding uses data collected indoors in the lab, or on images and videos of indoor activities from sources such as movies (Lei et al., 2019a) or online vlogs (Ignat et al., 2019). There are fewer studies on outdoor activities in more realistic “in the wild” settings (Castro et al., 2022). Collecting such data poses new challenges related to availability, quality, or distribution, which opens up new research directions. Moreover, applying these models to diverse domains (e.g., robotics, medicine, and education) requires adapting to fewer data points or different types of data, and adding in-domain expertise to understand the problem setup better. As shown in Yin et al. (2023), the multimodal LLMs are currently not equipped to tackle these challenges.\n\n2.5. Child Language Acquisition\n\nSample-efficient language learning.\n\nChild language acquisition is both an important research topic in psycholinguistics (McNeill, 1970), and also a valuable source of inspiration for sample-efficient language learning for NLP (Linzen, 2020). By mimicking the learning strategies of children, models can achieve better generalization with limited training data (Barak et al., 2016). Research in this area brings hope to improve the performance of NLP models while reducing the amount of training data required (Gulordava et al., 2018; Warstadt et al., 2023a, b). LLMs require far more data than children to acquire language, and LLMs can be improved in sample efficiency by learning how children acquire language.\n\nLanguage models as biological models for child language acquisition.\n\nSince the last century, there has been research using neural models as biological models to develop theories of human cognitive behavior (McCloskey, 1991). Combining powerful LLMs with psycholinguistic studies, researchers can gain inspiration for various processes in child language acquisition. For instance, insights into word acquisition can be gained by comparing the models’ learning curves and children’s age of acquisition for different words (Chang and Bergen, 2021). Other phenomena, such as phoneme-level acquisition (Christiansen and Chater, 1999; Martin et al., 2023) or intrinsic rewards (Gibson et al., 2019; Mu et al., 2022), can also be explored by using computational models on existing benchmarks such as WordBank (Frank et al., 2016) or CHILDES (MacWhinney, 1992)\n\nBenchmark development in child language acquisition.\n\nWhile there are currently only very few language acquisition benchmarks, NLP and multimodal systems bring opportunities to ease and scale child language benchmark construction. For example, controlled experiments on carefully-constructed supervised benchmarks can be augmented by large video datasets of children learning a language over a long period of time.\n\n2.6. Non-Verbal Communication\n\nNon-verbal language interpretation.\n\nNon-verbal language interpretation analyzes non-verbal cues such as facial expressions, gestures, and body language to enhance the performance of NLP systems (Mavridis, 2015; Schuller, 2018). For instance, while previous work has identified a potential “code-book” of facial expressions (Song et al., 2013), it remains an open research direction how to determine the set of expressions and gestures that can be used across modalities, contexts, and culture (Matsumoto and Assar, 1992; Abzaliev et al., 2022). Currently, there are no LLMs that combine the gesture modality with the text.\n\nSign language.\n\nAs a visual-gestural communication system, sign language has gained increasing attention in NLP due to its unique challenges and wide applications (Koller et al., 2016, 2018; Camgoz et al., 2020a). There are many research directions in sign language, such as data curation and evaluation addressing the high variability in manual gestures (Athitsos et al., 2008b; Li et al., 2020a), incorporation of additional information, i.e., facial expressions, body pose, eye gaze (Cao et al., 2018; Baltruaitis et al., 2018); and sign language generation for various scenarios, such as speakers of the same sign language, across different sign languages, and with a combination of verbal and sign languages (Adaloglou et al., 2022). Current systems use separate models for translating sign language into the text Lim et al. (2023), which is then provided to LLMs. Directly providing sign language to LLM might be more efficient.\n\nJoint verbal and non-verbal communication.\n\nUltimately, both verbal and non-verbal signals should be considered during communication. Future AI systems should be equally capable of understanding “I don’t know”, shrugging the shoulders, or . Representing, fusing, and interpreting these signals jointly is ultimately the long-term goal of AI-assisted communication Mavridis (2014). Open research problems encompass not only the development of language models for each of these modalities but effective fusion methodologies that enable large joint models for simultaneous verbal and non-verbal communication.\n\n4. Applied NLP\n\nAfter discussing tasks in fundamental NLP and responsible NLP, we now look into the wide applications of NLP in various domains, with a few selected discussions on NLP for healthcare (Section 4.1), education (Section 4.2), computational social science (CSS) (Section 4.3), and synthetic data generation (Section 4.4).\n\n4.1. NLP for Healthcare\n\nHealthcare benchmark construction.\n\nHealthcare is a domain that heavily suffers from data scarcity, which is usually due to data unavailability (typically for low-resource domains), or inaccessibility (due to privacy and ethics concerns). Potential strategies to create and scale-up health datasets include synthetic data generation Chintagunta et al. (2021a); Liednikova et al. (2020); Mattern et al. (2022b) or data augmentation from existing data (Dai et al., 2023). These strategies can improve the distribution of biased datasets, help ensure data privacy protections, and reduce the cost of data collection. However, data generation by LLMs also brings concerns of bias propagation and information leakage Arora and Arora (2022). Furthermore, researchers need metrics to measure the fidelity of synthetic data compared with real data Chen et al. (2021).\n\nImproving clinical communication.\n\nNLP has shown great potential in enhancing communication in healthcare, such as simplifying the medical jargon for laymen (Jin et al., 2022a), developing educational tools for healthcare professionals Min et al. (2022), and providing personalized healthcare recommendations (Choi et al., 2016; Roehrs et al., 2018). New research directions include developing advanced NLP models for medical dialogue systems and exploring the ethical implications of NLP-driven communication in healthcare (Ravi et al., 2016; Jakesch et al., 2019). Current LLMs may only be useful in limited settings, as trust in LLMs has been shown to depend on the health-related complexity of questions Nov et al. (2023).\n\nDrug discovery.\n\nSince the hypothesis space for drug designs is exponential (Ruddigkeit et al., 2012), NLP methods have been explored to assist clinicians to efficiently extract and analyze information from large amounts of scientific literature, patents, clinical records, and other biomedical sources. Open research directions in this domain include identifying and prioritizing the drug-target interactions, discovering new drug candidates, predicting compound properties, and optimizing drug designs (Brown et al., 2020a). Despite their great potential, the use of LLMs still face many challenges, such as the lack of transparency in the model decision-making process, which limits the applicability and reliability Thirunavukarasu et al. (2023).\n\n4.2. NLP for Education\n\nIntelligent tutoring systems.\n\nThe rising capability of NLP systems has given rise to intelligent tutoring systems to generate targeted practice questions and explain students’ mistakes in a wide range of subjects, from English or History to Physics or Computer Science (Mousavinasab et al., 2021). Responsible development of these systems requires human-in-the-loop checks to ensure their reliability, as NLP models are still lacking when it comes to more challenging reasoning and grounding tasks (Kanda et al., 2004). Other challenges include lack of diverse data, both in terms of population and time, privacy concerns and trustworthiness, and the need for better evaluation mechanisms Lin et al. (2023).\n\nEducational explanation generation.\n\nTo enrich teaching materials, NLP models can also help generate explanations for complicated questions or reading materials, as well as for automatic grading systems, since students improve more easily when grading is justified by corresponding explanations (Mohler and Mihalcea, 2009). However, some concerns include overreliance on the model, lack of expertise among educators Redecker and Punie (2017), and between real knowledge and convincingly written but unverified model output Kasneci et al. (2023). Therefore, it is important to understand the limitations of LLMs and use them only as a tool to support and enhance learning, but not as a replacement for human teachers Pavlik (2023).\n\nControllable text generation.\n\nIn education, there is a growing need for controllable text generation (Lee et al., 2011; Zhang et al., 2022a). This is helpful, for instance, for applications aiming to introduce students to new terms by generating memorable stories corresponding to their academic skill levels, interests, and prior experience. However, it is often difficult for LLMs to ensure domain diversity of the generated text while pursuing controllability, which leads to the catastrophic forgetting problem in LLMs Zhai et al. (2023). Additionally, we lack reliable evaluation techniques, as well as dedicated benchmarks and datasets for text generation with diverse control requirements Zhang et al. (2023b).\n\n4.3. Computational Social Science\n\nDevelopment of new abstractions, concepts, and methods.\n\nNLP enables automatic analyses of massive text for the study of computational social science (CSS), which has been benefited by the evolution of NLP methods from topic modeling (Blei et al., 2003), keyword extraction (Onan et al., 2016), to word embeddings (Pennington et al., 2014), and LLMs Brown et al. (2020c). It is foreseeable that further advancement in NLP models will unlock the possibilities of more customized, high-level text analyses for CSS. Evaluation paradigms need to evolve to capture the validity of LLMs as language generators, since human evaluation also can be unreliable in CSS Karpinska et al. (2021). Moreover, many CSS tasks contain large target label spaces Grudin (2006), which is a challenge for current LLMs that have limited memory and quadratic space complexity Ziems et al. (2023).\n\nPopulation-level data annotation and labeling.\n\nCSS research shows a large interest in using LLMs to annotate data to simulate human interactions (Gilardi et al., 2023a). However, human studies will be unlikely to go away, as LLMs’ effectiveness in annotation remains partial. Ollion et al. (2023) show that few-shot and zero-shot models are often outperformed by models fine-tuned with human annotations. Additionally, ChatGPT usually yields higher recall than precision, showing a tendency to output more false positives.\n\nMulticultural and multilingual CSS.\n\nMost CSS studies focus on English or other major languages, and address mostly Western cultures. However, there are many important questions in social science that require large-scale, multilingual, and multicultural analyses (Shen et al., 2019), such as how languages evolve, and how values vary across cultures (Garimella et al., 2016). This area for future work can lead to compounding impacts on the social sciences. However, the data-driven nature of LLMs makes them limited by the under-representation of minority communities and low-resource languages in the training data. Collecting more data related to this can help minimize the data disparity. Additionally, CSS researchers study cultures, norms, and beliefs that change across time, hence LLMs will need a high level of temporal grounding Ziems et al. (2023).\n\n4.4. Synthetic Datasets\n\nKnowledge distillation.\n\nKnowledge distillation transfers knowledge from larger, more complex models (the teacher) to typically smaller, simpler models (the student). Knowledge distillation allows the knowledge and capability to be compressed into much smaller models, reducing the computational and memory requirements of NLP systems. While earlier methods in knowledge distillation often learn from the soft output logits of teacher models (Hinton et al., 2015), more recent ones utilize LLM outputs as synthetic examples (West et al., 2022; Kim et al., 2022). This allows practitioners to transform or control the generated data in different ways, such as using finetuned models to filter for quality. Moreover, synthetic data can be used to directly emulate the behavior of LLMs with much smaller, focused models (Taori et al., 2023).\n\nControl over generated data attributes.\n\nCurrently, the predominant method is to provide natural text specifications with instructions and examples, but optimizing these prompts often relies on a simple trial-and-error approach. Additionally, specifying attributes through instructions or examples can be imprecise or noisy. The development of robust, controllable, and replicable pipelines for synthetic data generation remains an open research question (Kim et al., 2022).\n\nTransforming existing datasets.\n\nGiven an existing dataset, one can apply various changes to create a semantically preserving new dataset, but with a new style. Common approaches include format change (e.g., converting a dataset of HTML news articles to plain text), modality transfer (e.g., generating textual descriptions of images or videos or generating captions or subtitles for audio-visual content), or style transfer (Chintagunta et al., 2021b; Jin et al., 2022a) (e.g., translating the writing style of the text from verbose to concise).\n\n5. So “What Should I Work On?”\n\nThe future of NLP research is bright \\twemojisparkles. As illustrated by the 45 research directions spanning the fourteen research areas overviewed in this paper, the rapid progress we are currently witnessing in LLMs does not mean that “it’s all been solved.” On the contrary, numerous research directions within NLP are not solved by the current LLMs. They add to the many existing tasks in NLP where LLMs’ performance is limited (Bang et al., 2023a), as well as the growing number of new areas enabled by the new LLM capabilities.\n\nMore broadly, as a field, we now have the opportunity to move away from performance-focused technology development and acknowledge that NLP is about language and people. This brings about a new focus on enabling technologies that are culture and demographic aware, that are robust, interpretable, efficient, and aligned with solid ethical foundations — ultimately, technologies that make a lasting positive impact on society.\n\nHow to choose a research direction to work on? As suggested in Figure 2, start with your motivation and interests: consider your previous experiences, look around at your community, explore your curiosities about language and about people, and try to find what resonates with you the most. Building on this foundation, identify the tasks that connect to your motivations. This paper serves as a starting point to inspire this exploration.\n\nBroader Impact\n\nWe believe this work and the open research directions we identified can have an overall positive impact on the NLP research community, especially for junior students facing the challenge of re-orienting their research directions in the era of LLMs.\n\nWe conclude by highlighting what we foresee as the main role of this paper. First, we did not aim to cover the entire rich space of NLP, which is impossible for any research lab to enumerate exhaustively. Instead, we provided a starting point for students and researchers to regain their hope in NLP research, and find a direction they can contribute to that is not solved by LLMs. Second, this overview paper did not aim to solve any of the tasks we listed, but rather to identify the open space for future work. We thus did not provide full details for the research directions; instead, we introduced each research direction with a brief description, its broad application, and highlight the remaining challenges and open questions, especially those that are not addressed by LLMs. Our main goal is to inspire future researchers to deepen their exploration on the topics.\n\nWe welcome suggestions for other research areas or directions to include: https://bit.ly/nlp-era-llm.\n\nAcknowledgments\n\nWe want to thank Steve Abney, Rui Zhang, Emily Mower-Provost, and Louis-Philippe Morency for providing feedback and valuable suggestions on earlier versions of this manuscript. Zhijing Jin was supported by PhD fellowships from the Future of Life Institute and Open Philanthropy. This work was partially funded by a National Science Foundation award (#2306372). Any opinions, findings, and conclusions or recommendations expressed in this material are those of the authors and do not necessarily reflect the views of the NSF.\n\n6. Bibliographical References\n\n\\c@NAT@ctr\n\n(1)\n\nnoa (a) a. ChatGPT plugins. https://openai.com/blog/chatgpt-plugins. Accessed: 2023-4-24.\n\nnoa (b) b. Duolingo - learn a language for free @. https://www.duolingo.com/. Accessed: 2023-4-24.\n\nnoa (c) c. Grammarly: Free writing AI assistance. https://www.grammarly.com/. Accessed: 2023-4-24.\n\nnoa (d) d. OpenAI codex. https://openai.com/blog/openai-codex. Accessed: 2023-5-4.\n\nnoa (e) e. Stanford CRFM. https://crfm.stanford.edu/2023/03/13/alpaca.html. Accessed: 2023-5-4.\n\nnoa (f) f. Stanford CRFM. https://crfm.stanford.edu/2023/03/13/alpaca.html. Accessed: 2023-5-4.\n\nnoa (2023) 2023. AI guidance. https://poorvucenter.yale.edu/AIguidance. Accessed: 2023-4-24.\n\nAbdelnabi et al. (2022) Sahar Abdelnabi, Rakibul Hasan, and Mario Fritz. 2022. Open-domain, content-based, multi-modal fact-checking of out-of-context images via online resources. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 14940–14949.\n\nAbouelenien et al. (2016) M Abouelenien, V Pérez-Rosas, and others. 2016. Detecting deceptive behavior via integration of discriminative features from multiple modalities. IEEE Transactions.\n\nAbraham et al. (2022) Eldar David Abraham, Karel D’Oosterlinck, Amir Feder, Yair Ori Gat, Atticus Geiger, Christopher Potts, Roi Reichart, and Zhengxuan Wu. 2022. Cebab: Estimating the causal effects of real-world concepts on NLP model behavior. In NeurIPS.\n\nAbzaliev et al. (2022) Artem Abzaliev, Andrew Owens, and Rada Mihalcea. 2022. Towards understanding the relation between gestures and language. In Proceedings of the 29th International Conference on Computational Linguistics, pages 5507–5520.\n\nAdaloglou et al. (2022) Nikolas Adaloglou, Theocharis Chatzis, Ilias Papastratis, Andreas Stergioulas, Georgios Th. Papadopoulos, Vassia Zacharopoulou, George J. Xydopoulos, Klimnis Atzakas, Dimitris Papazachariou, and Petros Daras. 2022. A comprehensive study on deep learning-based methods for sign language recognition. IEEE Trans. Multim., 24:1750–1762.\n\nAgrawal et al. (2015) Aishwarya Agrawal, Jiasen Lu, Stanislaw Antol, Margaret Mitchell, C Lawrence Zitnick, Dhruv Batra, and Devi Parikh. 2015. VQA: Visual question answering.\n\nAguilar et al. (2020) Gustavo Aguilar, Sudipta Kar, and Thamar Solorio. 2020. LinCE: A centralized benchmark for linguistic code-switching evaluation. In Proceedings of the Twelfth Language Resources and Evaluation Conference, pages 1803–1813, Marseille, France. European Language Resources Association.\n\nAhuja et al. (2023) Kabir Ahuja, Rishav Hada, Millicent Ochieng, Prachi Jain, Harshita Diddee, Samuel Maina, Tanuja Ganu, Sameer Segal, Maxamed Axmed, Kalika Bali, and Sunayana Sitaram. 2023. MEGA: multilingual evaluation of generative AI. CoRR, abs/2303.12528.\n\nAkyürek et al. (2022) Ekin Akyürek, Dale Schuurmans, Jacob Andreas, Tengyu Ma, and Denny Zhou. 2022. What learning algorithm is in-context learning? Investigations with linear models. CoRR, abs/2211.15661.\n\nAlibali et al. (2000) Martha W Alibali, Sotaro Kita, and Amanda J Young. 2000. Gesture and the process of speech production: We think, therefore we gesture. Lang. Cogn. Process., 15(6):593–613.\n\nAlic et al. (2022) Sterling Alic, Dorottya Demszky, Zid Mancenido, Jing Liu, Heather Hill, and Dan Jurafsky. 2022. Computationally identifying funneling and focusing questions in classroom discourse. In Proceedings of the 17th Workshop on Innovative Use of NLP for Building Educational Applications (BEA 2022), pages 224–233, Seattle, Washington. Association for Computational Linguistics.\n\nAlKhamissi et al. (2023) Badr AlKhamissi, Siddharth Verma, Ping Yu, Zhijing Jin, Asli Celikyilmaz, and Mona Diab. 2023. OPT-R: Exploring the role of explanations in finetuning and prompting for reasoning skills of large language models.\n\nAlon et al. (2022) Uri Alon, Frank Xu, Junxian He, Sudipta Sengupta, Dan Roth, and Graham Neubig. 2022. Neuro-Symbolic language modeling with automaton-augmented retrieval. In Proceedings of the 39th International Conference on Machine Learning, volume 162 of Proceedings of Machine Learning Research, pages 468–485. PMLR.\n\nAmershi et al. (2014) Saleema Amershi, Maya Cakmak, W. Knox, and Todd Kulesza. 2014. Power to the people: The role of humans in interactive machine learning. AI Magazine, 35:105–120.\n\nAnonymous (2024) Anonymous. 2024. The generation gap: Exploring age bias in large language models.\n\nArmstrong (2013) Stuart Armstrong. 2013. General purpose intelligence: Arguing the orthogonality thesis. Analysis and Metaphysics, 12:68–84.\n\nArora and Arora (2022) Anmol Arora and Ananya Arora. 2022. Generative adversarial networks and synthetic patient data: current challenges and future perspectives. Future Healthcare Journal, 9(2):190–193.\n\nArora et al. (2023) Arnav Arora, Lucie-aimée Kaffee, and Isabelle Augenstein. 2023. Probing pre-trained language models for cross-cultural differences in values. In Proceedings of the First Workshop on Cross-Cultural Considerations in NLP (C3NLP), pages 114–130, Dubrovnik, Croatia. Association for Computational Linguistics.\n\nArtetxe and Schwenk (2018) Mikel Artetxe and Holger Schwenk. 2018. Margin-based parallel corpus mining with multilingual sentence embeddings.\n\nAsimov (1942) Isaac Asimov. 1942. Runaround. Astounding Science Fiction.\n\nAskell et al. (2021a) Amanda Askell, Yuntao Bai, Anna Chen, Dawn Drain, Deep Ganguli, Tom Henighan, Andy Jones, Nicholas Joseph, Benjamin Mann, Nova DasSarma, Nelson Elhage, Zac Hatfield-Dodds, Danny Hernandez, Jackson Kernion, Kamal Ndousse, Catherine Olsson, Dario Amodei, Tom B. Brown, Jack Clark, Sam McCandlish, Chris Olah, and Jared Kaplan. 2021a. A general language assistant as a laboratory for alignment. CoRR, abs/2112.00861.\n\nAskell et al. (2021b) Amanda Askell, Yuntao Bai, Anna Chen, Dawn Drain, Deep Ganguli, Tom Henighan, Andy Jones, Nicholas Joseph, Benjamin Mann, Nova DasSarma, Nelson Elhage, Zac Hatfield-Dodds, Danny Hernandez, Jackson Kernion, Kamal Ndousse, Catherine Olsson, Dario Amodei, Tom B. Brown, Jack Clark, Sam McCandlish, Chris Olah, and Jared Kaplan. 2021b. A general language assistant as a laboratory for alignment. CoRR, abs/2112.00861.\n\nAthitsos et al. (2008a) V Athitsos, C Neidle, S Sclaroff, J Nash, and others. 2008a. The american sign language lexicon video dataset. 2008 IEEE Computer.\n\nAthitsos et al. (2008b) Vassilis Athitsos, Carol Neidle, Stan Sclaroff, Joan Nash, Alexandra Stefan, Quan Yuan, and Ashwin Thangali. 2008b. The american sign language lexicon video dataset. 2012 IEEE Computer Society Conference on Computer Vision and Pattern Recognition Workshops, 0:1–8.\n\nAuer et al. (2007) Sören Auer, Christian Bizer, Georgi Kobilarov, Jens Lehmann, Richard Cyganiak, and Zachary Ives. 2007. DBpedia: A nucleus for a web of open data. In The Semantic Web, pages 722–735. Springer Berlin Heidelberg.\n\nBai et al. (2022a) Yuntao Bai, Andy Jones, Kamal Ndousse, Amanda Askell, Anna Chen, Nova DasSarma, Dawn Drain, Stanislav Fort, Deep Ganguli, Tom Henighan, Nicholas Joseph, Saurav Kadavath, Jackson Kernion, Tom Conerly, Sheer El Showk, Nelson Elhage, Zac Hatfield-Dodds, Danny Hernandez, Tristan Hume, Scott Johnston, Shauna Kravec, Liane Lovitt, Neel Nanda, Catherine Olsson, Dario Amodei, Tom B. Brown, Jack Clark, Sam McCandlish, Chris Olah, Benjamin Mann, and Jared Kaplan. 2022a. Training a helpful and harmless assistant with reinforcement learning from human feedback. CoRR, abs/2204.05862.\n\nBai et al. (2022b) Yuntao Bai, Saurav Kadavath, Sandipan Kundu, Amanda Askell, Jackson Kernion, Andy Jones, Anna Chen, Anna Goldie, Azalia Mirhoseini, Cameron McKinnon, Carol Chen, Catherine Olsson, Christopher Olah, Danny Hernandez, Dawn Drain, Deep Ganguli, Dustin Li, Eli Tran-Johnson, Ethan Perez, Jamie Kerr, Jared Mueller, Jeffrey Ladish, Joshua Landau, Kamal Ndousse, Kamile Lukosiute, Liane Lovitt, Michael Sellitto, Nelson Elhage, Nicholas Schiefer, Noemí Mercado, Nova DasSarma, Robert Lasenby, Robin Larson, Sam Ringer, Scott Johnston, Shauna Kravec, Sheer El Showk, Stanislav Fort, Tamera Lanham, Timothy Telleen-Lawton, Tom Conerly, Tom Henighan, Tristan Hume, Samuel R. Bowman, Zac Hatfield-Dodds, Ben Mann, Dario Amodei, Nicholas Joseph, Sam McCandlish, Tom Brown, and Jared Kaplan. 2022b. Constitutional AI: harmlessness from AI feedback. CoRR, abs/2212.08073.\n\nBaidoo-Anu and Owusu Ansah (2023) David Baidoo-Anu and Leticia Owusu Ansah. 2023. Education in the era of generative artificial intelligence (ai): Understanding the potential benefits of chatgpt in promoting teaching and learning. Available at SSRN 4337484.\n\nBain et al. (2021) Max Bain, Arsha Nagrani, Daniel Schofield, Sophie Berdugo, Joana Bessa, Jake Owen, Kimberley J Hockings, Tetsuro Matsuzawa, Misato Hayashi, Dora Biro, Susana Carvalho, and Andrew Zisserman. 2021. Automated audiovisual behavior recognition in wild primates. Sci Adv, 7(46):eabi4883.\n\nBaltruaitis et al. (2018) Tadas Baltruaitis, Amir Zadeh, Yao Chong Lim, and Louis-Philippe Morency. 2018. Openface 2.0: Facial behavior analysis toolkit. 2018 13th IEEE International Conference on Automatic Face & Gesture Recognition (FG 2018), pages 59–66.\n\nBang et al. (2023a) Yejin Bang, Samuel Cahyawijaya, Nayeon Lee, Wenliang Dai, Dan Su, Bryan Wilie, Holy Lovenia, Ziwei Ji, Tiezheng Yu, Willy Chung, Quyet V. Do, Yan Xu, and Pascale Fung. 2023a. A multitask, multilingual, multimodal evaluation of chatgpt on reasoning, hallucination, and interactivity. CoRR, abs/2302.04023.\n\nBang et al. (2023b) Yejin Bang, Samuel Cahyawijaya, Nayeon Lee, Wenliang Dai, Dan Su, Bryan Wilie, Holy Lovenia, Ziwei Ji, Tiezheng Yu, Willy Chung, Quyet V Do, Yan Xu, and Pascale Fung. 2023b. A multitask, multilingual, multimodal evaluation of ChatGPT on reasoning, hallucination, and interactivity.\n\nBarak et al. (2016) Libby Barak, Adele E. Goldberg, and Suzanne Stevenson. 2016. Comparing computational cognitive models of generalization in a language acquisition task. In Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, pages 96–106, Austin, Texas. Association for Computational Linguistics.\n\nBeltagy et al. (2019) Iz Beltagy, Kyle Lo, and Arman Cohan. 2019. SciBERT: A pretrained language model for scientific text. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), pages 3615–3620, Hong Kong, China. Association for Computational Linguistics.\n\nBender et al. (2021) Emily M Bender, Timnit Gebru, Angelina McMillan-Major, and Shmargaret Shmitchell. 2021. On the dangers of stochastic parrots: Can language models be too big? In Proceedings of the 2021 ACM Conference on Fairness, Accountability, and Transparency, FAccT ’21, pages 610–623, New York, NY, USA. Association for Computing Machinery.\n\nBender et al. (2020) Emily M Bender, Dirk Hovy, and Alexandra Schofield. 2020. Integrating ethics into the NLP curriculum. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics: Tutorial Abstracts, pages 6–9, Online. Association for Computational Linguistics.\n\nBenjamin (2019) Ruha Benjamin. 2019. Race After Technology: Abolitionist Tools for the New Jim Code. John Wiley & Sons.\n\nBerreby et al. (2015) Fiona Berreby, Gauvain Bourgne, and Jean-Gabriel Ganascia. 2015. Modelling moral reasoning and ethical responsibility with logic programming. In Logic for programming, artificial intelligence, and reasoning, pages 532–548. Springer.\n\nBhagavatula et al. (2020) Chandra Bhagavatula, Ronan Le Bras, Chaitanya Malaviya, Keisuke Sakaguchi, Ari Holtzman, Hannah Rashkin, Doug Downey, Wen-tau Yih, and Yejin Choi. 2020. Abductive commonsense reasoning. In International Conference on Learning Representations.\n\nBiester et al. (2022) Laura Biester, Dorottya Demszky, Zhijing Jin, Mrinmaya Sachan, Joel Tetreault, Steven Wilson, Lu Xiao, and Jieyu Zhao, editors. 2022. Proceedings of the Second Workshop on NLP for Positive Impact (NLP4PI). Association for Computational Linguistics, Abu Dhabi, United Arab Emirates (Hybrid).\n\nBiester et al. (2020) Laura Biester, Katie Matton, Janarthanan Rajendran, Emily Mower Provost, and Rada Mihalcea. 2020. Quantifying the effects of COVID-19 on mental health support forums. In Proceedings of the 1st Workshop on NLP for COVID-19 (Part 2) at EMNLP 2020, Online. Association for Computational Linguistics.\n\nBigScience Workshop et al. (2022) BigScience Workshop, :, Teven Le Scao, Angela Fan, Christopher Akiki, Ellie Pavlick, Suzana Ilić, Daniel Hesslow, Roman Castagné, Alexandra Sasha Luccioni, François Yvon, Matthias Gallé, Jonathan Tow, Alexander M Rush, Stella Biderman, Albert Webson, Pawan Sasanka Ammanamanchi, Thomas Wang, Benoît Sagot, Niklas Muennighoff, Albert Villanova del Moral, Olatunji Ruwase, Rachel Bawden, Stas Bekman, Angelina McMillan-Major, Iz Beltagy, Huu Nguyen, Lucile Saulnier, Samson Tan, Pedro Ortiz Suarez, Victor Sanh, Hugo Laurençon, Yacine Jernite, Julien Launay, Margaret Mitchell, Colin Raffel, Aaron Gokaslan, Adi Simhi, Aitor Soroa, Alham Fikri Aji, Amit Alfassy, Anna Rogers, Ariel Kreisberg Nitzav, Canwen Xu, Chenghao Mou, Chris Emezue, Christopher Klamm, Colin Leong, Daniel van Strien, David Ifeoluwa Adelani, Dragomir Radev, Eduardo González Ponferrada, Efrat Levkovizh, Ethan Kim, Eyal Bar Natan, Francesco De Toni, Gérard Dupont, Germán Kruszewski, Giada Pistilli, Hady Elsahar, Hamza Benyamina, Hieu Tran, Ian Yu, Idris Abdulmumin, Isaac Johnson, Itziar Gonzalez-Dios, Javier de la Rosa, Jenny Chim, Jesse Dodge, Jian Zhu, Jonathan Chang, Jörg Frohberg, Joseph Tobing, Joydeep Bhattacharjee, Khalid Almubarak, Kimbo Chen, Kyle Lo, Leandro Von Werra, Leon Weber, Long Phan, Loubna Ben allal, Ludovic Tanguy, Manan Dey, Manuel Romero Muñoz, Maraim Masoud, María Grandury, Mario Šaško, Max Huang, Maximin Coavoux, Mayank Singh, Mike Tian-Jian Jiang, Minh Chien Vu, Mohammad A Jauhar, Mustafa Ghaleb, Nishant Subramani, Nora Kassner, Nurulaqilla Khamis, Olivier Nguyen, Omar Espejel, Ona de Gibert, Paulo Villegas, Peter Henderson, Pierre Colombo, Priscilla Amuok, Quentin Lhoest, Rheza Harliman, Rishi Bommasani, Roberto Luis López, Rui Ribeiro, Salomey Osei, Sampo Pyysalo, Sebastian Nagel, Shamik Bose, Shamsuddeen Hassan Muhammad, Shanya Sharma, Shayne Longpre, Somaieh Nikpoor, Stanislav Silberberg, Suhas Pai, Sydney Zink, Tiago Timponi Torrent, Timo Schick, Tristan Thrush, Valentin Danchev, Vassilina Nikoulina, Veronika Laippala, Violette Lepercq, Vrinda Prabhu, Zaid Alyafeai, Zeerak Talat, Arun Raja, Benjamin Heinzerling, Chenglei Si, Davut Emre Taşar, Elizabeth Salesky, Sabrina J Mielke, Wilson Y Lee, Abheesht Sharma, Andrea Santilli, Antoine Chaffin, Arnaud Stiegler, Debajyoti Datta, Eliza Szczechla, Gunjan Chhablani, Han Wang, Harshit Pandey, Hendrik Strobelt, Jason Alan Fries, Jos Rozen, Leo Gao, Lintang Sutawika, M Saiful Bari, Maged S Al-shaibani, Matteo Manica, Nihal Nayak, Ryan Teehan, Samuel Albanie, Sheng Shen, Srulik Ben-David, Stephen H Bach, Taewoon Kim, Tali Bers, Thibault Fevry, Trishala Neeraj, Urmish Thakker, Vikas Raunak, Xiangru Tang, Zheng-Xin Yong, Zhiqing Sun, Shaked Brody, Yallow Uri, Hadar Tojarieh, Adam Roberts, Hyung Won Chung, Jaesung Tae, Jason Phang, Ofir Press, Conglong Li, Deepak Narayanan, Hatim Bourfoune, Jared Casper, Jeff Rasley, Max Ryabinin, Mayank Mishra, Minjia Zhang, Mohammad Shoeybi, Myriam Peyrounette, Nicolas Patry, Nouamane Tazi, Omar Sanseviero, Patrick von Platen, Pierre Cornette, Pierre François Lavallée, Rémi Lacroix, Samyam Rajbhandari, Sanchit Gandhi, Shaden Smith, Stéphane Requena, Suraj Patil, Tim Dettmers, Ahmed Baruwa, Amanpreet Singh, Anastasia Cheveleva, Anne-Laure Ligozat, Arjun Subramonian, Aurélie Névéol, Charles Lovering, Dan Garrette, Deepak Tunuguntla, Ehud Reiter, Ekaterina Taktasheva, Ekaterina Voloshina, Eli Bogdanov, Genta Indra Winata, Hailey Schoelkopf, Jan-Christoph Kalo, Jekaterina Novikova, Jessica Zosa Forde, Jordan Clive, Jungo Kasai, Ken Kawamura, Liam Hazan, Marine Carpuat, Miruna Clinciu, Najoung Kim, Newton Cheng, Oleg Serikov, Omer Antverg, Oskar van der Wal, Rui Zhang, Ruochen Zhang, Sebastian Gehrmann, Shachar Mirkin, Shani Pais, Tatiana Shavrina, Thomas Scialom, Tian Yun, Tomasz Limisiewicz, Verena Rieser, Vitaly Protasov, Vladislav Mikhailov, Yada Pruksachatkun, Yonatan Belinkov, Zachary Bamberger, Zdeněk Kasner, Alice Rueda, Amanda Pestana, Amir Feizpour, Ammar Khan, Amy Faranak, Ana Santos, Anthony Hevia, Antigona Unldreaj, Arash Aghagol, Arezoo Abdollahi, Aycha Tammour, Azadeh HajiHosseini, Bahareh Behroozi, Benjamin Ajibade, Bharat Saxena, Carlos Muñoz Ferrandis, Danish Contractor, David Lansky, Davis David, Douwe Kiela, Duong A Nguyen, Edward Tan, Emi Baylor, Ezinwanne Ozoani, Fatima Mirza, Frankline Ononiwu, Habib Rezanejad, Hessie Jones, Indrani Bhattacharya, Irene Solaiman, Irina Sedenko, Isar Nejadgholi, Jesse Passmore, Josh Seltzer, Julio Bonis Sanz, Livia Dutra, Mairon Samagaio, Maraim Elbadri, Margot Mieskes, Marissa Gerchick, Martha Akinlolu, Michael McKenna, Mike Qiu, Muhammed Ghauri, Mykola Burynok, Nafis Abrar, Nazneen Rajani, Nour Elkott, Nour Fahmy, Olanrewaju Samuel, Ran An, Rasmus Kromann, Ryan Hao, Samira Alizadeh, Sarmad Shubber, Silas Wang, Sourav Roy, Sylvain Viguier, Thanh Le, Tobi Oyebade, Trieu Le, Yoyo Yang, Zach Nguyen, Abhinav Ramesh Kashyap, Alfredo Palasciano, Alison Callahan, Anima Shukla, Antonio Miranda-Escalada, Ayush Singh, Benjamin Beilharz, Bo Wang, Caio Brito, Chenxi Zhou, Chirag Jain, Chuxin Xu, Clémentine Fourrier, Daniel León Periñán, Daniel Molano, Dian Yu, Enrique Manjavacas, Fabio Barth, Florian Fuhrimann, Gabriel Altay, Giyaseddin Bayrak, Gully Burns, Helena U Vrabec, Imane Bello, Ishani Dash, Jihyun Kang, John Giorgi, Jonas Golde, Jose David Posada, Karthik Rangasai Sivaraman, Lokesh Bulchandani, Lu Liu, Luisa Shinzato, Madeleine Hahn de Bykhovetz, Maiko Takeuchi, Marc Pàmies, Maria A Castillo, Marianna Nezhurina, Mario Sänger, Matthias Samwald, Michael Cullan, Michael Weinberg, Michiel De Wolf, Mina Mihaljcic, Minna Liu, Moritz Freidank, Myungsun Kang, Natasha Seelam, Nathan Dahlberg, Nicholas Michio Broad, Nikolaus Muellner, Pascale Fung, Patrick Haller, Ramya Chandrasekhar, Renata Eisenberg, Robert Martin, Rodrigo Canalli, Rosaline Su, Ruisi Su, Samuel Cahyawijaya, Samuele Garda, Shlok S Deshmukh, Shubhanshu Mishra, Sid Kiblawi, Simon Ott, Sinee Sang-aroonsiri, Srishti Kumar, Stefan Schweter, Sushil Bharati, Tanmay Laud, Théo Gigant, Tomoya Kainuma, Wojciech Kusa, Yanis Labrak, Yash Shailesh Bajaj, Yash Venkatraman, Yifan Xu, Yingxin Xu, Yu Xu, Zhe Tan, Zhongli Xie, Zifan Ye, Mathilde Bras, Younes Belkada, and Thomas Wolf. 2022. BLOOM: A 176B-Parameter Open-Access multilingual language model.\n\nBilalpur et al. (2023) Maneesh Bilalpur, Saurabh Hinduja, Laura A Cariola, Lisa B Sheeber, Nick Alien, László A Jeni, Louis-Philippe Morency, and Jeffrey F Cohn. 2023. Multimodal feature selection for detecting mothers’ depression in dyadic interactions with their adolescent offspring. In 2023 IEEE 17th International Conference on Automatic Face and Gesture Recognition (FG), pages 1–8.\n\nBisk et al. (2020a) Yonatan Bisk, Ari Holtzman, Jesse Thomason, Jacob Andreas, Yoshua Bengio, Joyce Chai, Mirella Lapata, Angeliki Lazaridou, Jonathan May, Aleksandr Nisnevich, Nicolas Pinto, and Joseph Turian. 2020a. Experience grounds language.\n\nBisk et al. (2020b) Yonatan Bisk, Rowan Zellers, Ronan Le Bras, Jianfeng Gao, and Yejin Choi. 2020b. Piqa: Reasoning about physical commonsense in natural language. In Thirty-Fourth AAAI Conference on Artificial Intelligence.\n\nBlei et al. (2003) David M Blei, Andrew Y Ng, and Michael I Jordan. 2003. Latent dirichlet allocation. Journal of machine Learning research, 3(Jan):993–1022.\n\nBlodgett et al. (2020) Su Lin Blodgett, Solon Barocas, Hal Daumé III, and Hanna Wallach. 2020. Language (technology) is power: A critical survey of “bias” in NLP. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 5454–5476, Online. Association for Computational Linguistics.\n\nBorgeaud et al. (2021) Sebastian Borgeaud, Arthur Mensch, Jordan Hoffmann, Trevor Cai, Eliza Rutherford, Katie Millican, George van den Driessche, Jean-Baptiste Lespiau, Bogdan Damoc, Aidan Clark, Diego de Las Casas, Aurelia Guy, Jacob Menick, Roman Ring, Tom Hennigan, Saffron Huang, Loren Maggiore, Chris Jones, Albin Cassirer, Andy Brock, Michela Paganini, Geoffrey Irving, Oriol Vinyals, Simon Osindero, Karen Simonyan, Jack W. Rae, Erich Elsen, and Laurent Sifre. 2021. Improving language models by retrieving from trillions of tokens. CoRR, abs/2112.04426.\n\nBorgeaud et al. (2022) Sebastian Borgeaud, Arthur Mensch, Jordan Hoffmann, Trevor Cai, Eliza Rutherford, Katie Millican, George Bm Van Den Driessche, Jean-Baptiste Lespiau, Bogdan Damoc, Aidan Clark, Diego De Las Casas, Aurelia Guy, Jacob Menick, Roman Ring, Tom Hennigan, Saffron Huang, Loren Maggiore, Chris Jones, Albin Cassirer, Andy Brock, Michela Paganini, Geoffrey Irving, Oriol Vinyals, Simon Osindero, Karen Simonyan, Jack Rae, Erich Elsen, and Laurent Sifre. 2022. Improving language models by retrieving from trillions of tokens. In Proceedings of the 39th International Conference on Machine Learning, volume 162 of Proceedings of Machine Learning Research, pages 2206–2240. PMLR.\n\nBorisov et al. (2022) Vadim Borisov, Kathrin Seßler, Tobias Leemann, Martin Pawelczyk, and Gjergji Kasneci. 2022. Language models are realistic tabular data generators.\n\nBostrom and Yudkowsky (2014) Nick Bostrom and Eliezer Yudkowsky. 2014. The ethics of artificial intelligence. Cambridge University Press.\n\nBotzer et al. (2021) Nicholas Botzer, Shawn Gu, and Tim Weninger. 2021. Analysis of moral judgement on reddit.\n\nBowdery (1941) George J Bowdery. 1941. Conventions and norms. Philosophy of Science, 8(4):493–505.\n\nBowerman and Levinson (2001a) Melissa Bowerman and Stephen Levinson. 2001a. Language Acquisition and Conceptual Development. Language Culture and Cognition. Cambridge University Press.\n\nBowerman and Levinson (2001b) Melissa Bowerman and Stephen C Levinson. 2001b. Language Acquisition and Conceptual Development. Cambridge University Press.\n\nBowman (2023) Samuel R Bowman. 2023. Eight things to know about large language models.\n\nBowman et al. (2022) Samuel R. Bowman, Jeeyoon Hyun, Ethan Perez, Edwin Chen, Craig Pettit, Scott Heiner, Kamile Lukosiute, Amanda Askell, Andy Jones, Anna Chen, Anna Goldie, Azalia Mirhoseini, Cameron McKinnon, Christopher Olah, Daniela Amodei, Dario Amodei, Dawn Drain, Dustin Li, Eli Tran-Johnson, Jackson Kernion, Jamie Kerr, Jared Mueller, Jeffrey Ladish, Joshua Landau, Kamal Ndousse, Liane Lovitt, Nelson Elhage, Nicholas Schiefer, Nicholas Joseph, Noemí Mercado, Nova DasSarma, Robin Larson, Sam McCandlish, Sandipan Kundu, Scott Johnston, Shauna Kravec, Sheer El Showk, Stanislav Fort, Timothy Telleen-Lawton, Tom Brown, Tom Henighan, Tristan Hume, Yuntao Bai, Zac Hatfield-Dodds, Ben Mann, and Jared Kaplan. 2022. Measuring progress on scalable oversight for large language models. CoRR, abs/2211.03540.\n\nBragg et al. (2019) Danielle Bragg, Oscar Koller, Mary Bellard, Larwan Berke, Patrick Boudreault, Annelies Braffort, Naomi Caselli, Matt Huenerfauth, Hernisa Kacorri, Tessa Verhoef, Christian Vogler, and Meredith Ringel Morris. 2019. Sign language recognition, generation, and translation: An interdisciplinary perspective. In Proceedings of the 21st International ACM SIGACCESS Conference on Computers and Accessibility, ASSETS ’19, pages 16–31, New York, NY, USA. Association for Computing Machinery.\n\nBraginsky et al. (2016) Mika Braginsky, Daniel Yurovsky, Virginia A. Marchman, and Mike Frank. 2016. From uh-oh to tomorrow: Predicting age of acquisition for early words across languages. Cognitive Science.\n\nBraşoveanu and Andonie (2020) Adrian M P Braşoveanu and Răzvan Andonie. 2020. Visualizing transformers for NLP: A brief survey. In 2020 24th International Conference Information Visualisation (IV), pages 270–279.\n\nBrown et al. (2020a) Nathan Brown, Peter Ertl, Richard A. Lewis, Torsten Luksch, Daniel Reker, and Nadine Schneider. 2020a. Artificial intelligence in chemistry and drug design. Journal of Computer-Aided Molecular Design, 34:709–715.\n\nBrown et al. (1988) P Brown, J Cocke, S Della Pietra, V Della Pietra, F Jelinek, R Mercer, and P Roossin. 1988. A statistical approach to language translation. In Coling Budapest 1988 Volume 1: International Conference on Computational Linguistics. aclanthology.org.\n\nBrown et al. (2020b) Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, and Others. 2020b. Language models are few-shot learners. Adv. Neural Inf. Process. Syst., 33:1877–1901.\n\nBrown et al. (2020c) Tom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel M. Ziegler, Jeffrey Wu, Clemens Winter, Christopher Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, and Dario Amodei. 2020c. Language models are few-shot learners. In Advances in Neural Information Processing Systems 33: Annual Conference on Neural Information Processing Systems 2020, NeurIPS 2020, December 6-12, 2020, virtual.\n\nBubeck et al. (2023a) Sébastien Bubeck, Varun Chandrasekaran, Ronen Eldan, Johannes Gehrke, Eric Horvitz, Ece Kamar, Peter Lee, Yin Tat Lee, Yuanzhi Li, Scott Lundberg, Harsha Nori, Hamid Palangi, Marco Tulio Ribeiro, and Yi Zhang. 2023a. Sparks of artificial general intelligence: Early experiments with GPT-4.\n\nBubeck et al. (2023b) Sébastien Bubeck, Varun Chandrasekaran, Ronen Eldan, Johannes Gehrke, Eric Horvitz, Ece Kamar, Peter Lee, Yin Tat Lee, Yuanzhi Li, Scott M. Lundberg, Harsha Nori, Hamid Palangi, Marco Túlio Ribeiro, and Yi Zhang. 2023b. Sparks of artificial general intelligence: Early experiments with GPT-4. CoRR, abs/2303.12712.\n\nBurstein et al. (1997) Jill Burstein, Susanne Wolff, Chi Lu, and Randy M Kaplan. 1997. An automatic scoring system for advanced placement biology essays. In Fifth Conference on Applied Natural Language Processing, pages 174–181, Washington, DC, USA. Association for Computational Linguistics.\n\nCamgoz et al. (2020a) Necati Cihan Camgoz, Oscar Koller, Simon Hadfield, and Richard Bowden. 2020a. Sign language transformers: Joint end-to-end sign language recognition and translation. pages 10023–10033.\n\nCamgoz et al. (2020b) Necati Cihan Camgoz, Oscar Koller, Simon Hadfield, and Richard Bowden. 2020b. Sign language transformers: Joint end-to-end sign language recognition and translation. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 10023–10033. openaccess.thecvf.com.\n\nCao et al. (2018) Zhe Cao, Gines Hidalgo, Tomas Simon, Shih-En Wei, and Yaser Sheikh. 2018. Openpose: Realtime multi-person 2d pose estimation using part affinity fields. IEEE Transactions on Pattern Analysis and Machine Intelligence, 43:172–186.\n\nCard and Smith (2020) Dallas Card and Noah A. Smith. 2020. On consequentialism and fairness. Frontiers in Artificial Intelligence, 3:34.\n\nCarlini et al. (2020) Nicholas Carlini, Florian Tramer, Eric Wallace, Matthew Jagielski, Ariel Herbert-Voss, Katherine Lee, Adam Roberts, Tom Brown, Dawn Song, Ulfar Erlingsson, Alina Oprea, and Colin Raffel. 2020. Extracting training data from large language models.\n\nCarlini et al. (2021) Nicholas Carlini, Florian Tramer, Eric Wallace, Matthew Jagielski, Ariel Herbert-Voss, Katherine Lee, Adam Roberts, Tom B Brown, Dawn Song, Ulfar Erlingsson, and Others. 2021. Extracting training data from large language models. In USENIX Security Symposium, volume 6.\n\nCastro et al. (2022) Santiago Castro, Naihao Deng, Pingxuan Huang, Mihai Burzo, and Rada Mihalcea. 2022. In-the-Wild video question answering. In Proceedings of the 29th International Conference on Computational Linguistics, pages 5613–5635, Gyeongju, Republic of Korea. International Committee on Computational Linguistics.\n\nÇetinoğlu et al. (2016) Özlem Çetinoğlu, Sarah Schulz, and Ngoc Thang Vu. 2016. Challenges of computational processing of code-switching. In Proceedings of the Second Workshop on Computational Approaches to Code Switching, pages 1–11, Austin, Texas. Association for Computational Linguistics.\n\nChang and Bergen (2021) Tyler A. Chang and Benjamin K. Bergen. 2021. Word acquisition in neural language models.\n\nChen et al. (2021) Richard J Chen, Ming Y Lu, Tiffany Y Chen, Drew FK Williamson, and Faisal Mahmood. 2021. Synthetic data in machine learning for medicine and healthcare. Nature Biomedical Engineering, 5(6):493–497.\n\nCheves (2017) Alexander Cheves. 2017. 21 words the queer community has reclaimed (and some we haven’t). The Advocate.\n\nChina AI Report (2020) China AI Report. 2020. China AI report 2020.\n\nChintagunta et al. (2021a) Bharath Chintagunta, Namit Katariya, Xavier Amatriain, and Anitha Kannan. 2021a. Medically aware GPT-3 as a data generator for medical dialogue summarization. In Proceedings of the Second Workshop on Natural Language Processing for Medical Conversations, pages 66–76, Online. Association for Computational Linguistics.\n\nChintagunta et al. (2021b) Bharath Chintagunta, Namit Katariya, Xavier Amatriain, and Anitha Kannan. 2021b. Medically aware GPT-3 as a data generator for medical dialogue summarization. In Proceedings of the Second Workshop on Natural Language Processing for Medical Conversations, pages 66–76, Online. Association for Computational Linguistics.\n\nChoi et al. (2016) Edward Choi, Mohammad Taha Bahadori, Andy Schuetz, Walter F Stewart, and Jimeng Sun. 2016. Doctor ai: Predicting clinical events via recurrent neural networks. Machine Learning for Healthcare Conference, pages 301–318.\n\nChristiansen and Chater (1999) Morten H Christiansen and Nick Chater. 1999. Connectionist natural language processing: The state of the art. Cognitive science, 23(4):417–437.\n\nChu et al. (2023) Eric Chu, Jacob Andreas, Stephen Ansolabehere, and Deb Roy. 2023. Language models trained on media diets can predict public opinion. CoRR, abs/2303.16779.\n\nChubb et al. (2021) Jennifer Chubb, Sondess Missaoui, Shauna Concannon, Liam Maloney, and James Alfred Walker. 2021. Interactive storytelling for children: A case-study of design and development considerations for ethical conversational AI.\n\nChughtai et al. (2023) Bilal Chughtai, Lawrence Chan, and Neel Nanda. 2023. A toy model of universality: Reverse engineering how networks learn group operations. CoRR, abs/2302.03025.\n\nCiampaglia et al. (2015) Giovanni Luca Ciampaglia, Prashant Shiralkar, Luis M Rocha, Johan Bollen, Filippo Menczer, and Alessandro Flammini. 2015. Computational fact checking from knowledge networks. PloS one, 10(6):e0128193.\n\nCobbe et al. (2021) Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Jacob Hilton, Reiichiro Nakano, Christopher Hesse, and John Schulman. 2021. Training verifiers to solve math word problems. CoRR, abs/2110.14168.\n\nCohen et al. (2023) Roi Cohen, Mor Geva, Jonathan Berant, and Amir Globerson. 2023. Crawling the internal knowledge-base of language models. In Findings of the Association for Computational Linguistics: EACL 2023, Dubrovnik, Croatia, May 2-6, 2023, pages 1811–1824. Association for Computational Linguistics.\n\nCohen (2011) Shlomo Cohen. 2011. The Proto-Ethical Dimension of Moods, pages 173–184.\n\nCollins (2023) Keith Collins. 2023. How ChatGPT could embed a ‘watermark’ in the text it generates. The New York Times.\n\nConmy et al. (2023) Arthur Conmy, Augustine N. Mavor-Parker, Aengus Lynch, Stefan Heimersheim, and Adrià Garriga-Alonso. 2023. Towards automated circuit discovery for mechanistic interpretability. CoRR, abs/2304.14997.\n\nContreras Kallens et al. (2023) Pablo Contreras Kallens, Ross Deans Kristensen-McLachlan, and Morten H Christiansen. 2023. Large language models demonstrate the potential of statistical learning in language. Cognitive Science, 47(3):e13256.\n\nCostello et al. (2023) Anna Costello, Ekaterina Fedorova, Zhijing Jin, and Rada Mihalcea. 2023. Editing a woman’s voice. In ICSSI.\n\nCôté et al. (2018) Marc-Alexandre Côté, Ákos Kádár, Xingdi Yuan, Ben Kybartas, Tavian Barnes, Emery Fine, James Moore, Ruo Yu Tao, Matthew Hausknecht, Layla El Asri, Mahmoud Adada, Wendy Tay, and Adam Trischler. 2018. TextWorld: A learning environment for text-based games.\n\nCotton et al. (2023) Debby RE Cotton, Peter A Cotton, and J Reuben Shipway. 2023. Chatting and cheating: Ensuring academic integrity in the era of chatgpt. Innovations in Education and Teaching International, pages 1–12.\n\nCramer (2002) J S Cramer. 2002. The origins of logistic regression.\n\nDai et al. (2023) Haixing Dai, Zhengliang Liu, Wenxiong Liao, Xiaoke Huang, Zihao Wu, Lin Zhao, Wei Liu, Ninghao Liu, Sheng Li, Dajiang Zhu, et al. 2023. Chataug: Leveraging chatgpt for text data augmentation. arXiv preprint arXiv:2302.13007.\n\nDanilevsky et al. (2020) Marina Danilevsky, Kun Qian, Ranit Aharonov, Yannis Katsis, Ban Kawas, and Prithviraj Sen. 2020. A survey of the state of explainable AI for natural language processing.\n\nDao et al. (2021) Tri Dao, Beidi Chen, Kaizhao Liang, Jiaming Yang, Zhao Song, Atri Rudra, and Christopher Ré. 2021. Pixelated butterfly: Simple and efficient sparse training for neural network models.\n\nDao et al. (2022) Tri Dao, Daniel Y Fu, Stefano Ermon, Atri Rudra, and Christopher Ré. 2022. FlashAttention: Fast and memory-efficient exact attention with IO-awareness.\n\nDas et al. (2023) Anubrata Das, Houjiang Liu, Venelin Kovatchev, and Matthew Lease. 2023. The state of human-centered nlp technology for fact-checking. Information Processing & Management, 60(2):103219.\n\nDash et al. (2019) Sabyasachi Dash, Sushil Kumar Shakyawar, Mohit Sharma, and Sandeep Kaushik. 2019. Big data in healthcare: management, analysis and future prospects. Journal of Big Data, 6(1):1–25.\n\nde Graaf et al. (2017) Maartje de Graaf, Somaya Ben Allouch, and Jan van Dijk. 2017. Why do they refuse to use my robot? reasons for Non-Use derived from a Long-Term home study. In Proceedings of the 2017 ACM/IEEE International Conference on Human-Robot Interaction, HRI ’17, pages 224–233, New York, NY, USA. Association for Computing Machinery.\n\nde la Cretaz (2016) Britni de la Cretaz. 2016. What it’s like to chestfeed. The Atlantic.\n\nd’Eon et al. (2022) Greg d’Eon, Jason d’Eon, James R Wright, and Kevin Leyton-Brown. 2022. The spotlight: A general method for discovering systematic errors in deep learning models. In 2022 ACM Conference on Fairness, Accountability, and Transparency, FAccT ’22, pages 1962–1981, New York, NY, USA. Association for Computing Machinery.\n\nDevlin et al. (2018) Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2018. BERT: Pre-training of deep bidirectional transformers for language understanding.\n\nDevlin et al. (2019a) Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2019a. BERT: Pre-training of deep bidirectional transformers for language understanding. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), pages 4171–4186, Minneapolis, Minnesota. Association for Computational Linguistics.\n\nDevlin et al. (2019b) Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2019b. Bert: Pre-training of deep bidirectional transformers for language understanding. ArXiv, abs/1810.04805.\n\nDing et al. (2022) Yiwen Ding, Jiarui Liu, Zhiheng Lyu, Kun Zhang, Bernhard Schoelkopf, Zhijing Jin, and Rada Mihalcea. 2022. Voices of her: Analyzing gender differences in the AI publication world.\n\nDixon et al. (2018) Lucas Dixon, John Li, Jeffrey Sorensen, Nithum Thain, and Lucy Vasserman. 2018. Measuring and mitigating unintended bias in text classification. In Proceedings of the 2018 AAAI/ACM Conference on AI, Ethics, and Society, AIES ’18, pages 67–73, New York, NY, USA. Association for Computing Machinery.\n\nDodge et al. (2020) Jesse Dodge, Gabriel Ilharco, Roy Schwartz, Ali Farhadi, Hannaneh Hajishirzi, and Noah Smith. 2020. Fine-Tuning pretrained language models: Weight initializations, data orders, and early stopping.\n\nDoes et al. (2011) Serena Does, Belle Derks, and Naomi Ellemers. 2011. Thou shalt not discriminate: How emphasizing moral ideals rather than obligations increases whites’ support for social equality. Journal of Experimental Social Psychology, 47(3):562–571.\n\nDonker (2023) Tjibbe Donker. 2023. The dangers of using large language models for peer review. The Lancet Infectious Diseases.\n\nDosovitskiy et al. (2020) Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, Jakob Uszkoreit, and Neil Houlsby. 2020. An image is worth 16x16 words: Transformers for image recognition at scale.\n\nDotel et al. (2020) Saramsha Dotel, Avishekh Shrestha, Anish Bhusal, Ramesh Pathak, Aman Shakya, and Sanjeeb Prasad Panday. 2020. Disaster assessment from satellite imagery by analysing topographical features using deep learning. In Proceedings of the 2020 2nd International Conference on Image, Video and Signal Processing, IVSP ’20, pages 86–92, New York, NY, USA. Association for Computing Machinery.\n\nDu et al. (2022) Nan Du, Yanping Huang, Andrew M Dai, Simon Tong, Dmitry Lepikhin, Yuanzhong Xu, Maxim Krikun, Yanqi Zhou, Adams Wei Yu, Orhan Firat, Barret Zoph, Liam Fedus, Maarten P Bosma, Zongwei Zhou, Tao Wang, Emma Wang, Kellie Webster, Marie Pellat, Kevin Robinson, Kathleen Meier-Hellstern, Toju Duke, Lucas Dixon, Kun Zhang, Quoc Le, Yonghui Wu, Zhifeng Chen, and Claire Cui. 2022. GLaM: Efficient scaling of language models with Mixture-of-Experts. In Proceedings of the 39th International Conference on Machine Learning, volume 162 of Proceedings of Machine Learning Research, pages 5547–5569. PMLR.\n\nDutta et al. (2023) Abhishek Dutta, Natalia Pérez-Campanero, Graham K Taylor, Andrew Zisserman, and Cait Newport. 2023. A robust and flexible deep-learning workflow for animal tracking.\n\nDwork (2006) Cynthia Dwork. 2006. Differential privacy. In Automata, Languages and Programming: 33rd International Colloquium, ICALP 2006, Venice, Italy, July 10-14, 2006, Proceedings, Part II 33, pages 1–12. Springer.\n\nDziri et al. (2022a) Nouha Dziri, Sivan Milton, Mo Yu, Osmar Zaiane, and Siva Reddy. 2022a. On the origin of hallucinations in conversational models: Is it the datasets or the models? In Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 5271–5285, Seattle, United States. Association for Computational Linguistics.\n\nDziri et al. (2022b) Nouha Dziri, Sivan Milton, Mo Yu, Osmar Zaiane, and Siva Reddy. 2022b. On the origin of hallucinations in conversational models: Is it the datasets or the models? In Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 5271–5285, Seattle, United States. Association for Computational Linguistics.\n\nEdunov et al. (2018) Sergey Edunov, Myle Ott, Michael Auli, and David Grangier. 2018. Understanding Back-Translation at scale.\n\nEisape et al. (2023) Tiwalayo Eisape, Mh Tessler, Ishita Dasgupta, Fei Sha, Sjoerd van Steenkiste, and Tal Linzen. 2023. A systematic comparison of syllogistic reasoning in humans and language models. CoRR, abs/2311.00445.\n\nElhage et al. (2022) Nelson Elhage, Tristan Hume, Catherine Olsson, Nicholas Schiefer, Tom Henighan, Shauna Kravec, Zac Hatfield-Dodds, Robert Lasenby, Dawn Drain, Carol Chen, Roger Grosse, Sam McCandlish, Jared Kaplan, Dario Amodei, Martin Wattenberg, and Christopher Olah. 2022. Toy models of superposition. CoRR, abs/2209.10652.\n\nEloundou et al. (2023) Tyna Eloundou, Sam Manning, Pamela Mishkin, and Daniel Rock. 2023. GPTs are GPTs: An early look at the labor market impact potential of large language models.\n\nElster (2006) Jon Elster. 2006. Fairness and norms. Social Research: An International Quarterly, 73:365–376.\n\nEmelin et al. (2020) Denis Emelin, Ronan Le Bras, Jena D Hwang, Maxwell Forbes, and Yejin Choi. 2020. Moral stories: Situated reasoning about norms, intents, actions, and their consequences. arXiv preprint arXiv:2012.15738.\n\nEtzioni (2018) Oren Etzioni. 2018. Point: Should AI technology be regulated? Yes, and here’s how. Commun. ACM, 61(12):30–32.\n\nEuropean Commission (2019) European Commission. 2019. Ethics guidelines for trustworthy artificial intelligence.\n\nEvans et al. (2021) Owain Evans, Owen Cotton-Barratt, Lukas Finnveden, Adam Bales, Avital Balwit, Peter Wills, Luca Righetti, and William Saunders. 2021. Truthful AI: developing and governing AI that does not lie. CoRR, abs/2110.06674.\n\nEyuboglu et al. (2022) Sabri Eyuboglu, Maya Varma, Khaled Saab, Jean-Benoit Delbrouck, Christopher Lee-Messer, Jared Dunnmon, James Zou, and Christopher Ré. 2022. Domino: Discovering systematic errors with Cross-Modal embeddings.\n\nFan et al. (2021) Lifeng Fan, Shuwen Qiu, Zilong Zheng, Tao Gao, Song-Chun Zhu, and Yixin Zhu. 2021. Learning triadic belief dynamics in nonverbal communication from videos. pages 7312–7321.\n\nFang et al. (2021) Han Fang, Pengfei Xiong, Luhui Xu, and Yu Chen. 2021. CLIP2Video: Mastering Video-Text retrieval via image CLIP.\n\nFang and Wu (2022) Shaohua Fang and Zhiyi Wu. 2022. Syntactic prediction in l2 learners: evidence from english disjunction processing. International Review of Applied Linguistics in Language Teaching.\n\nFasoli et al. (2019) Fabio Fasoli, Peter Hegarty, and Andrea Carnaghi. 2019. Sounding gay, speaking as a “fag”: Auditory gaydar and the perception of reclaimed homophobic language. Journal of language and social psychology, page 0261927X19852753.\n\nFedus et al. (2022) William Fedus, Jeff Dean, and Barret Zoph. 2022. A review of sparse expert models in deep learning.\n\nFedus et al. (2021) William Fedus, Barret Zoph, and Noam Shazeer. 2021. Switch transformers: Scaling to trillion parameter models with simple and efficient sparsity.\n\nField et al. (2021a) Anjalie Field, Su Lin Blodgett, Zeerak Talat, and Yulia Tsvetkov. 2021a. A survey of race, racism, and anti-racism in nlp. In Annual Meeting of the Association for Computational Linguistics.\n\nField et al. (2021b) Anjalie Field, Shrimai Prabhumoye, Maarten Sap, Zhijing Jin, Jieyu Zhao, and Chris Brockett, editors. 2021b. Proceedings of the 1st Workshop on NLP for Positive Impact. Association for Computational Linguistics, Online.\n\nFletcher (1997) Joseph F Fletcher. 1997. Situation ethics: The new morality. Westminster John Knox Press.\n\nFoote et al. (2023) Alex Foote, Neel Nanda, Esben Kran, Ionnis Konstas, and Fazl Barez. 2023. N2G: A scalable approach for quantifying interpretable neuron representations in large language models. CoRR, abs/2304.12918.\n\nForbes et al. (2020) Maxwell Forbes, Jena D Hwang, Vered Shwartz, Maarten Sap, and Yejin Choi. 2020. Social chemistry 101: Learning to reason about social and moral norms. In EMNLP.\n\nFrank et al. (2016) Michael Frank, Mika Braginsky, Daniel Yurovsky, and Virginia Marchman. 2016. Wordbank: an open repository for developmental vocabulary data. Journal of Child Language, 44(3):677–694.\n\nFu et al. (2023) Chaoyou Fu, Peixian Chen, Yunhang Shen, Yulei Qin, Mengdan Zhang, Xu Lin, Zhenyu Qiu, Wei Lin, Jinrui Yang, Xiawu Zheng, Ke Li, Xing Sun, and Rongrong Ji. 2023. Mme: A comprehensive evaluation benchmark for multimodal large language models. ArXiv, abs/2306.13394.\n\nFu et al. (2022) Yao Fu, Hao Peng, Ashish Sabharwal, Peter Clark, and Tushar Khot. 2022. Complexity-based prompting for multi-step reasoning. arXiv preprint arXiv:2210.00720.\n\nGalinsky et al. (2013) Adam D Galinsky, Cynthia S Wang, Jennifer A Whitson, Eric M Anicich, Kurt Hugenberg, and Galen V Bodenhausen. 2013. The reappropriation of stigmatizing labels: The reciprocal relationship between power and self-labeling. Psychological science, 24(10):2020–2029.\n\nGanguli et al. (2023) Deep Ganguli, Amanda Askell, Nicholas Schiefer, Thomas I. Liao, Kamile Lukosiute, Anna Chen, Anna Goldie, Azalia Mirhoseini, Catherine Olsson, Danny Hernandez, Dawn Drain, Dustin Li, Eli Tran-Johnson, Ethan Perez, Jackson Kernion, Jamie Kerr, Jared Mueller, Joshua Landau, Kamal Ndousse, Karina Nguyen, Liane Lovitt, Michael Sellitto, Nelson Elhage, Noemí Mercado, Nova DasSarma, Oliver Rausch, Robert Lasenby, Robin Larson, Sam Ringer, Sandipan Kundu, Saurav Kadavath, Scott Johnston, Shauna Kravec, Sheer El Showk, Tamera Lanham, Timothy Telleen-Lawton, Tom Henighan, Tristan Hume, Yuntao Bai, Zac Hatfield-Dodds, Ben Mann, Dario Amodei, Nicholas Joseph, Sam McCandlish, Tom Brown, Christopher Olah, Jack Clark, Samuel R. Bowman, and Jared Kaplan. 2023. The capacity for moral self-correction in large language models. CoRR, abs/2302.07459.\n\nGanguli et al. (2022a) Deep Ganguli, Danny Hernandez, Liane Lovitt, Nova DasSarma, Tom Henighan, Andy Jones, Nicholas Joseph, Jackson Kernion, Benjamin Mann, Amanda Askell, Yuntao Bai, Anna Chen, Tom Conerly, Dawn Drain, Nelson Elhage, Sheer El Showk, Stanislav Fort, Zac Hatfield-Dodds, Scott Johnston, Shauna Kravec, Neel Nanda, Kamal Ndousse, Catherine Olsson, Daniela Amodei, Dario Amodei, Tom B. Brown, Jared Kaplan, Sam McCandlish, Chris Olah, and Jack Clark. 2022a. Predictability and surprise in large generative models. CoRR, abs/2202.07785.\n\nGanguli et al. (2022b) Deep Ganguli, Liane Lovitt, Jackson Kernion, Amanda Askell, Yuntao Bai, Saurav Kadavath, Ben Mann, Ethan Perez, Nicholas Schiefer, Kamal Ndousse, Andy Jones, Sam Bowman, Anna Chen, Tom Conerly, Nova DasSarma, Dawn Drain, Nelson Elhage, Sheer El Showk, Stanislav Fort, Zac Hatfield-Dodds, Tom Henighan, Danny Hernandez, Tristan Hume, Josh Jacobson, Scott Johnston, Shauna Kravec, Catherine Olsson, Sam Ringer, Eli Tran-Johnson, Dario Amodei, Tom Brown, Nicholas Joseph, Sam McCandlish, Chris Olah, Jared Kaplan, and Jack Clark. 2022b. Red teaming language models to reduce harms: Methods, scaling behaviors, and lessons learned. CoRR, abs/2209.07858.\n\nGarimella et al. (2016) Aparna Garimella, Rada Mihalcea, and James Pennebaker. 2016. Identifying cross-cultural differences in word usage. In Proceedings of coling 2016, the 26th international conference on computational linguistics: Technical papers, pages 674–683.\n\nGat et al. (2020) Itai Gat, Idan Schwartz, Alexander Schwing, and Tamir Hazan. 2020. Removing bias in multi-modal classifiers: Regularization by maximizing functional entropies.\n\nGeiger et al. (2021) Atticus Geiger, Hanson Lu, Thomas Icard, and Christopher Potts. 2021. Causal abstractions of neural networks. In Advances in Neural Information Processing Systems 34: Annual Conference on Neural Information Processing Systems 2021, NeurIPS 2021, December 6-14, 2021, virtual, pages 9574–9586.\n\nGeiger et al. (2023a) Atticus Geiger, Christopher Potts, and Thomas Icard. 2023a. Causal abstraction for faithful model interpretation. CoRR, abs/2301.04709.\n\nGeiger et al. (2022) Atticus Geiger, Zhengxuan Wu, Hanson Lu, Josh Rozner, Elisa Kreiss, Thomas Icard, Noah D. Goodman, and Christopher Potts. 2022. Inducing causal structure for interpretable neural networks. In International Conference on Machine Learning, ICML 2022, 17-23 July 2022, Baltimore, Maryland, USA, volume 162 of Proceedings of Machine Learning Research, pages 7324–7338. PMLR.\n\nGeiger et al. (2023b) Atticus Geiger, Zhengxuan Wu, Christopher Potts, Thomas Icard, and Noah D. Goodman. 2023b. Finding alignments between interpretable causal variables and distributed neural representations. CoRR, abs/2303.02536.\n\nGert and Gert (2020) Bernard Gert and Joshua Gert. 2020. The Definition of Morality. In Edward N. Zalta, editor, The Stanford Encyclopedia of Philosophy, Fall 2020 edition. Metaphysics Research Lab, Stanford University.\n\nGeva et al. (2022a) Mor Geva, Avi Caciularu, Guy Dar, Paul Roit, Shoval Sadde, Micah Shlain, Bar Tamir, and Yoav Goldberg. 2022a. LM-debugger: An interactive tool for inspection and intervention in transformer-based language models. In Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing: System Demonstrations, pages 12–21, Abu Dhabi, UAE. Association for Computational Linguistics.\n\nGeva et al. (2022b) Mor Geva, Avi Caciularu, Kevin Wang, and Yoav Goldberg. 2022b. Transformer feed-forward layers build predictions by promoting concepts in the vocabulary space. In Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing, pages 30–45, Abu Dhabi, United Arab Emirates. Association for Computational Linguistics.\n\nGeva et al. (2020) Mor Geva, Ankit Gupta, and Jonathan Berant. 2020. Injecting numerical reasoning skills into language models. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 946–958, Online. Association for Computational Linguistics.\n\nGeva et al. (2021) Mor Geva, Roei Schuster, Jonathan Berant, and Omer Levy. 2021. Transformer feed-forward layers are key-value memories. In Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 5484–5495, Online and Punta Cana, Dominican Republic. Association for Computational Linguistics.\n\nGhahramani (2001) Zoubin Ghahramani. 2001. An introduction to hidden markov models and bayesian networks. Int. J. Pattern Recognit. Artif. Intell., 15:9–42.\n\nGhosh and Fossas (2022) Avijit Ghosh and Genoveva Fossas. 2022. Can there be art without an artist? arXiv preprint arXiv:2209.07667.\n\nGibson et al. (2019) Edward Gibson, Richard Futrell, Steven Piantadosi, Isabelle Dautriche, Kyle Mahowald, Leon Bergen, and Roger Philip Levy. 2019. How efficiency shapes human language. Trends in Cognitive Sciences, 23:389–407.\n\nGilardi et al. (2023a) Fabrizio Gilardi, Meysam Alizadeh, and Maël Kubli. 2023a. ChatGPT outperforms Crowd-Workers for Text-Annotation tasks.\n\nGilardi et al. (2023b) Fabrizio Gilardi, Meysam Alizadeh, and Maël Kubli. 2023b. Chatgpt outperforms crowd-workers for text-annotation tasks. CoRR, abs/2303.15056.\n\nGillespie (2020) Tarleton Gillespie. 2020. Content moderation, ai, and the question of scale. Big Data & Society, 7(2):2053951720943234.\n\nGissin and Shalev-Shwartz (2019) Daniel Gissin and Shai Shalev-Shwartz. 2019. Discriminative active learning.\n\nGoel et al. (2021) Karan Goel, Nazneen Fatema Rajani, Jesse Vig, Zachary Taschdjian, Mohit Bansal, and Christopher Ré. 2021. Robustness gym: Unifying the NLP evaluation landscape. In Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies: Demonstrations, pages 42–55, Online. Association for Computational Linguistics.\n\nGohel et al. (2021) Prashant Gohel, Priyanka Singh, and Manoranjan Mohanty. 2021. Explainable AI: current status and future directions.\n\nGolovneva et al. (2022) Olga Golovneva, Moya Chen, Spencer Poff, Martin Corredor, Luke Zettlemoyer, Maryam Fazel-Zarandi, and Asli Celikyilmaz. 2022. Roscoe: A suite of metrics for scoring step-by-step reasoning. arXiv preprint arXiv:2212.07919.\n\nGonzalez et al. (2023) Fernando Gonzalez, Zhijing Jin, Jad Beydoun, Bernhard SchÃfÂ¶lkopf, Tom Hope, Mrinmaya Sachan, and Rada Mihalcea. 2023. Beyond good intentions: Reporting the research landscape of NLP for social good. CoRR, abs/2305.05471.\n\nGoodfellow et al. (2016) Ian Goodfellow, Yoshua Bengio, Aaron Courville, and Yoshua Bengio. 2016. Deep learning, volume 1. MIT Press.\n\nGoodhart (1975) Charles A. E. Goodhart. 1975. Problems of monetary management: The u.k. experience. Papers in Monetary Economics, 1.\n\nGoodhart (1984) Charles A. E. Goodhart. 1984. Problems of monetary management: The uk experience.\n\nGorwa et al. (2020) Robert Gorwa, Reuben Binns, and Christian Katzenbach. 2020. Algorithmic content moderation: Technical and political challenges in the automation of platform governance. Big Data & Society, 7(1):2053951719897945.\n\nGosling and Trémolière (2021) Corentin J Gosling and Bastien Trémolière. 2021. Reliability of moral decision-making: Evidence from the trolley dilemma. Quarterly Journal of Experimental Psychology, 74(6):981–990.\n\nGoyal et al. (2021) Naman Goyal, Cynthia Gao, Vishrav Chaudhary, Peng-Jen Chen, Guillaume Wenzek, Da Ju, Sanjana Krishnan, Marc’Aurelio Ranzato, Francisco Guzmán, and Angela Fan. 2021. The flores-101 evaluation benchmark for low-resource and multilingual machine translation.\n\nGravitas (2023) Significant Gravitas. 2023. Auto-gpt. https://github.com/Significant-Gravitas/Auto-GPT.\n\nGrudin (2006) Jonathan Grudin. 2006. Why personas work: The psychological evidence. The persona lifecycle, 12:642–664.\n\nGuan et al. (2020) Jian Guan, Fei Huang, Zhihao Zhao, Xiaoyan Zhu, and Minlie Huang. 2020. A Knowledge-Enhanced pretraining model for commonsense story generation. Transactions of the Association for Computational Linguistics, 8:93–108.\n\nGuarino et al. (2019) Stefano Guarino, Noemi Trino, Alessandro Chessa, and Gianni Riotta. 2019. Beyond fact-checking: Network analysis tools for monitoring disinformation in social media. In Complex Networks and Their Applications VIII - Volume 1 Proceedings of the Eighth International Conference on Complex Networks and Their Applications COMPLEX NETWORKS 2019, Lisbon, Portugal, December 10-12, 2019, volume 881 of Studies in Computational Intelligence, pages 436–447. Springer.\n\nGuarino et al. (2020) Stefano Guarino, Noemi Trino, Alessandro Chessa, and Gianni Riotta. 2020. Beyond fact-checking: Network analysis tools for monitoring disinformation in social media. In Complex Networks and Their Applications VIII: Volume 1 Proceedings of the Eighth International Conference on Complex Networks and Their Applications COMPLEX NETWORKS 2019 8, pages 436–447. Springer.\n\nGulordava et al. (2018) Kristina Gulordava, Piotr Bojanowski, Edouard Grave, Tal Linzen, and Marco Baroni. 2018. Colorless green recurrent networks dream hierarchically. In Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers), pages 1195–1205, New Orleans, Louisiana. Association for Computational Linguistics.\n\nGuo et al. (2020a) Qipeng Guo, Zhijing Jin, Ning Dai, Xipeng Qiu, Xiangyang Xue, David Wipf, and Zheng Zhang. 2020a. P2: A plan-and-pretrain approach for knowledge graph-to-text generation. In Proceedings of the 3rd International Workshop on Natural Language Generation from the Semantic Web (WebNLG+), pages 100–106, Dublin, Ireland (Virtual). Association for Computational Linguistics.\n\nGuo et al. (2020b) Qipeng Guo, Zhijing Jin, Xipeng Qiu, Weinan Zhang, David Wipf, and Zheng Zhang. 2020b. CycleGT: Unsupervised graph-to-text and text-to-graph generation via cycle training. In Proceedings of the 3rd International Workshop on Natural Language Generation from the Semantic Web (WebNLG+), pages 77–88, Dublin, Ireland (Virtual). Association for Computational Linguistics.\n\nGuo et al. (2021) Qipeng Guo, Zhijing Jin, Ziyu Wang, Xipeng Qiu, Weinan Zhang, Jun Zhu, Zheng Zhang, and David Wipf. 2021. Fork or fail: Cycle-consistent training with many-to-one mappings. In The 24th International Conference on Artificial Intelligence and Statistics, AISTATS 2021, April 13-15, 2021, Virtual Event, volume 130 of Proceedings of Machine Learning Research, pages 1828–1836. PMLR.\n\nGurnee et al. (2023) Wes Gurnee, Neel Nanda, Matthew Pauly, Katherine Harvey, Dmitrii Troitskii, and Dimitris Bertsimas. 2023. Finding neurons in a haystack: Case studies with sparse probing. CoRR, abs/2305.01610.\n\nGururangan et al. (2018) Suchin Gururangan, Swabha Swayamdipta, Omer Levy, Roy Schwartz, Samuel Bowman, and Noah A Smith. 2018. Annotation artifacts in natural language inference data. In Association for Computational Linguistics (ACL), pages 107–112.\n\nHaidt (2013) Jonathan Haidt. 2013. The Righteous Mind: Why Good People Are Divided by Politics and Religion. Vintage.\n\nHare (1981) Richard Mervyn Hare. 1981. Moral Thinking: Its Levels, Method, and Point. Oxford: Oxford University Press.\n\nHarvard Business Review (2020) Harvard Business Review. 2020. AI can make bank loans more fair.\n\nHassan et al. (2018a) Hany Hassan, Anthony Aue, Chang Chen, Vishal Chowdhary, Jonathan Clark, Christian Federmann, Xuedong Huang, Marcin Junczys-Dowmunt, William Lewis, Mu Li, Shujie Liu, Tie-Yan Liu, Renqian Luo, Arul Menezes, Tao Qin, Frank Seide, Xu Tan, Fei Tian, Lijun Wu, Shuangzhi Wu, Yingce Xia, Dongdong Zhang, Zhirui Zhang, and Ming Zhou. 2018a. Achieving human parity on automatic chinese to english news translation. CoRR, abs/1803.05567.\n\nHassan et al. (2018b) Hany Hassan, Anthony Aue, Chang Chen, Vishal Chowdhary, Jonathan Clark, Christian Federmann, Xuedong Huang, Marcin Junczys-Dowmunt, William Lewis, Mu Li, Shujie Liu, Tie-Yan Liu, Renqian Luo, Arul Menezes, Tao Qin, Frank Seide, Xu Tan, Fei Tian, Lijun Wu, Shuangzhi Wu, Yingce Xia, Dongdong Zhang, Zhirui Zhang, and Ming Zhou. 2018b. Achieving human parity on automatic chinese to english news translation. CoRR, abs/1803.05567.\n\nHatmaker (2020) Taylor Hatmaker. 2020. Twitter broadly bans any covid-19 tweets that could help the virus spread.\n\nHe et al. (2021) Pengcheng He, Xiaodong Liu, Jianfeng Gao, and Weizhu Chen. 2021. Deberta: Decoding-enhanced Bert with disentangled attention. In 9th International Conference on Learning Representations, ICLR 2021, Virtual Event, Austria, May 3-7, 2021. OpenReview.net.\n\nHeikkilä (2023) Melissa Heikkilä. 2023. How OpenAI is trying to make ChatGPT safer and less biased. MIT Technology Review.\n\nHendricks and Nematzadeh (2021) Lisa Anne Hendricks and Aida Nematzadeh. 2021. Probing Image-Language transformers for verb understanding.\n\nHendrycks et al. (2021a) Dan Hendrycks, Collin Burns, Steven Basart, Andrew Critch, Jerry Li, Dawn Song, and Jacob Steinhardt. 2021a. Aligning AI with shared human values. In International Conference on Learning Representations.\n\nHendrycks et al. (2021b) Dan Hendrycks, Nicholas Carlini, John Schulman, and Jacob Steinhardt. 2021b. Unsolved problems in ML safety. CoRR, abs/2109.13916.\n\nHernandez et al. (2022) Danny Hernandez, Tom B. Brown, Tom Conerly, Nova DasSarma, Dawn Drain, Sheer El Showk, Nelson Elhage, Zac Hatfield-Dodds, Tom Henighan, Tristan Hume, Scott Johnston, Benjamin Mann, Chris Olah, Catherine Olsson, Dario Amodei, Nicholas Joseph, Jared Kaplan, and Sam McCandlish. 2022. Scaling laws and interpretability of learning from repeated data. CoRR, abs/2205.10487.\n\nHernandez et al. (2023) Evan Hernandez, Belinda Z. Li, and Jacob Andreas. 2023. Measuring and manipulating knowledge representations in language models. CoRR, abs/2304.00740.\n\nHershcovich et al. (2022) Daniel Hershcovich, Stella Frank, Heather C. Lent, Miryam de Lhoneux, Mostafa Abdou, Stephanie Brandl, Emanuele Bugliarello, Laura Cabello Piqueras, Ilias Chalkidis, Ruixiang Cui, Constanza Fierro, Katerina Margatina, Phillip Rust, and Anders Søgaard. 2022. Challenges and strategies in cross-cultural NLP. In Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), ACL 2022, Dublin, Ireland, May 22-27, 2022, pages 6997–7013. Association for Computational Linguistics.\n\nHessenthaler et al. (2022) Marius Hessenthaler, Emma Strubell, Dirk Hovy, and Anne Lauscher. 2022. Bridging fairness and environmental sustainability in natural language processing. In Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing, EMNLP 2022, Abu Dhabi, United Arab Emirates, December 7-11, 2022, pages 7817–7836. Association for Computational Linguistics.\n\nHewitt and Liang (2019) John Hewitt and Percy Liang. 2019. Designing and interpreting probes with control tasks. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), pages 2733–2743, Hong Kong, China. Association for Computational Linguistics.\n\nHewitt and Manning (2019) John Hewitt and Christopher D. Manning. 2019. A structural probe for finding syntax in word representations. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), pages 4129–4138, Minneapolis, Minnesota. Association for Computational Linguistics.\n\nHinton et al. (2015) Geoffrey Hinton, Oriol Vinyals, and Jeff Dean. 2015. Distilling the knowledge in a neural network.\n\nHoffmann et al. (2022a) Jordan Hoffmann, Sebastian Borgeaud, Arthur Mensch, Elena Buchatskaya, Trevor Cai, Eliza Rutherford, Diego de Las Casas, Lisa Anne Hendricks, Johannes Welbl, Aidan Clark, Tom Hennigan, Eric Noland, Katie Millican, George van den Driessche, Bogdan Damoc, Aurelia Guy, Simon Osindero, Karen Simonyan, Erich Elsen, Jack W. Rae, Oriol Vinyals, and Laurent Sifre. 2022a. Training compute-optimal large language models. CoRR, abs/2203.15556.\n\nHoffmann et al. (2022b) Jordan Hoffmann, Sebastian Borgeaud, Arthur Mensch, Elena Buchatskaya, Trevor Cai, Eliza Rutherford, Diego de Las Casas, Lisa Anne Hendricks, Johannes Welbl, Aidan Clark, Tom Hennigan, Eric Noland, Katie Millican, George van den Driessche, Bogdan Damoc, Aurelia Guy, Simon Osindero, Karen Simonyan, Erich Elsen, Jack W Rae, Oriol Vinyals, and Laurent Sifre. 2022b. Training Compute-Optimal large language models.\n\nHoover et al. (2019) Joseph Hoover, Mohammad Atari, Aida Mostafazadeh Davani, Brendan Kennedy, Gwenyth Portillo-Wightman, Leigh Yeh, Drew Kogon, and Morteza Dehghani. 2019. Bound in hatred: The role of group-based morality in acts of hate.\n\nHou et al. (2019) Rui Hou, Verónica Pérez-Rosas, Stacy Loeb, and Rada Mihalcea. 2019. Towards automatic detection of misinformation in online medical videos. In 2019 International conference on multimodal interaction, pages 235–243.\n\nHovy and Spruit (2016) Dirk Hovy and Shannon L. Spruit. 2016. The social impact of natural language processing. In Annual Meeting of the Association for Computational L"
    }
}
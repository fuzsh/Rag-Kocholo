{
    "id": "dbpedia_801_3",
    "rank": 77,
    "data": {
        "url": "https://www.statsig.com/blog/novelty-effects",
        "read_more_link": "",
        "language": "en",
        "title": "Novelty effects: Everything you need to know",
        "top_image": "https://images.ctfassets.net/083zfbgkrzxz/69Ykeiv522qAAezxMPi0ya/519a66dd458db5c610c1a2115b368966/24.03.20_1200x600_NoveltyEffects.jpg",
        "meta_img": "https://images.ctfassets.net/083zfbgkrzxz/69Ykeiv522qAAezxMPi0ya/519a66dd458db5c610c1a2115b368966/24.03.20_1200x600_NoveltyEffects.jpg",
        "images": [
            "https://www.statsig.com/images/horz_logo.svg",
            "https://www.statsig.com/images/horz_logo.svg",
            "https://www.statsig.com/images/icons/icon-menu-experiments.svg",
            "https://www.statsig.com/images/icons/icon-menu-flag.svg",
            "https://www.statsig.com/images/icons/icon-menu-warehouse.svg",
            "https://www.statsig.com/images/icons/icon-menu-analytics.svg",
            "https://www.statsig.com/images/icons/icon-menu-replay.svg",
            "https://www.statsig.com/images/icons/icon-menu-web.svg",
            "https://www.statsig.com/images/icons/icon-menu-experiments.svg",
            "https://www.statsig.com/images/icons/icon-menu-flag.svg",
            "https://www.statsig.com/images/icons/icon-menu-warehouse.svg",
            "https://www.statsig.com/images/icons/icon-menu-analytics.svg",
            "https://www.statsig.com/images/icons/icon-menu-replay.svg",
            "https://www.statsig.com/images/icons/icon-menu-web.svg",
            "https://images.ctfassets.net/083zfbgkrzxz/1C9252OL7VP6uw8MaI6eFT/2651097fcf1358dfa60f44f8fe090d1c/image2.png",
            "https://images.ctfassets.net/083zfbgkrzxz/1cXGXbpus0uvp8jqVS2pUM/e60c49407893544a796aaeae7d6eb92f/image1.jpg",
            "https://images.ctfassets.net/083zfbgkrzxz/1coMoafupEsSqvCy0fvQFb/7c0f4e3e225230caf35f7c826a0b8a23/image3.jpg",
            "https://images.ctfassets.net/083zfbgkrzxz/5V6tDrAOpFsDWq7XLhPLiD/72f518534e5ebd1ceb6331e6b4ab00f5/metrics_explorer_2.png",
            "https://www.statsig.com/images/social/facebook.svg",
            "https://www.statsig.com/images/social/twitter.svg",
            "https://www.statsig.com/images/social/instagram.svg",
            "https://www.statsig.com/images/social/linkedin.svg",
            "https://www.statsig.com/images/social/github.svg",
            "https://www.statsig.com/images/misc/join_banner_mobile.png",
            "https://www.statsig.com/images/statsig_white.svg",
            "https://www.statsig.com/images/misc/soc2-type2.svg",
            "https://www.statsig.com/images/misc/g2-badge-we-have-been-forced-to-become-corporate-congratulations-everyone.png",
            "https://www.statsig.com/images/misc/dogs-badge.png",
            "https://www.statsig.com/images/social/facebook.svg",
            "https://www.statsig.com/images/social/twitter.svg",
            "https://www.statsig.com/images/social/instagram.svg",
            "https://www.statsig.com/images/social/linkedin.svg",
            "https://www.statsig.com/images/social/github.svg"
        ],
        "movies": [],
        "keywords": [],
        "meta_keywords": [
            ""
        ],
        "tags": null,
        "authors": [
            "Yuzheng Sun, PhD"
        ],
        "publish_date": "2024-03-20T00:00:00-07:00",
        "summary": "",
        "meta_description": "Explore how novelty effects shape product success and decision-making. Understand their impact to make informed, long-lasting strategies.",
        "meta_lang": "en",
        "meta_favicon": "",
        "meta_site_name": "",
        "canonical_link": "https://www.statsig.com/blog/novelty-effects",
        "text": "Novelty effects, TLDR\n\nNovelty effects refer to the phenomenon where the response to a new feature is temporarily deviated from its inherent value due to its newness.\n\nNot all products have novelty effects. They exist mostly in high-frequency products.\n\nIgnoring the temporary nature of novelty effects may lead to incorrect product decisions, and worse, bad culture.\n\nNovelty effects are treatment effects. They should not be corrected with statistical methods.\n\nThe most effective way to find novelty effects and control them is to examine the time series of treatment effects.\n\nThe root cause solution is to use a set of metrics that correctly represent user intents.\n\nWhen understood and used correctly, novelty effects can help you.\n\nWhat are novelty effects?\n\nNovelty effects are short-term effects in a product’s metrics that are attributed to the introduction of something new. In simple terms, something new will evoke a temporary reaction, which might wear off as the newness wears off.\n\nWhen product decisions are made based on metric movement, it’s important to understand this phenomenon, so only the sustained impacts of product changes are considered.\n\nNovelty effects are not bad and are actually sometimes valuable.\n\nConsiderations of novelty effects\n\nA common mistreatment of novelty effects is to view them as a statistical error or bias. They should not be corrected using statistical methods, which could result in more erroneous decisions.\n\nNovelty effects are part of the treatment effects that we are studying for (a new feature, a different button, or a different process), so there is nothing statistically wrong with novelty effects.\n\nNovelty effects are considered risky in product development experiments because we are not just estimating the “observed treatment effect” We are using the results of experiments to make product decisions, where we are inferencing and extrapolating, as summarized by Tom Cunningham.\n\nIn other words, if we look at the results from an experiment that is dominated by novelty effects, and try to extrapolate the long-term impact, we will likely arrive at incorrect conclusions.\n\nTypically novelty effects are much stronger than the effects of the features themselves in the short term, and only in the short term. Imagine this – the restaurant you pass by every day had a 100% improvement on their menu, their chef and their services. You probably won’t even notice the improvement. But if they changed their name, you will be intrigued to enter and find out what happened.\n\nIgnoring novelty effects\n\nResponsible restaurant owners won’t change names every other week to attract more customers. But what if you own hundreds of restaurants and set a quarterly KPI for your restaurant managers on the daily visits? Whoever attracts the most customers can earn the biggest bonus.\n\nYour managers would be incentivized to change the name than change the menu. This path is easy and immediately “effective”. And that’s because people respond to incentives. If people care about KPI, and novelty effects give them a quick win, then there’s a good chance it’ll be used to achieve KPIs.\n\nPitfalls of novelty effects\n\nSo far, we have established that\n\nNovelty effects are part of the treatment effects, so there is no statistical method to detect them generically\n\nNovelty effects are dangerous and will spread if you don’t combat them\n\nIn practice, novelty effects are usually easy to detect and control if you have the proper understanding.\n\nIn some cases, the root cause of the mistreatment of novelty effects is the adoption of too simple an OEC (overall evaluation criteria) and reliance on short-term metrics, e.g., CTR (click-through rate).\n\nWhile there is nothing inherently wrong with CTR as a metric, it is important to understand that CTR measures many different things. An increase in CTR can mean: 1) finding value; 2) being attracted by novelty; 3) click-baited due to misleading content, and so on. As such, an increase in CTR isn’t always good.\n\nThe solution is to have other metrics in OEC to complement CTR. For example, feature level funnel, and feature level retention, can tell us whether users finished using the feature as we intended and whether they come back to the feature.\n\nNovelty effects in time series\n\nIt’s tempting to solve for novelty effects with statistical tests but it’s generally not advisable. Let’s go back to the core of novelty effects once again:\n\nNovelty effects refer to the phenomenon where the response to a new feature is temporarily deviated from its inherent value due to its newness.\n\nThe keyword here is \"temporarily.\" Given enough time, novelty effects will wear off.\n\nWhen we study the observed effect of an experiment, we look at the cumulative effects, which is the delta between the treatment group and the control group, cumulative throughout the experiment duration.\n\nDrawing conclusions from the cumulative effects allows us to have more statistical power. It is fine if the expected treatment effects do not change, which is not the case with novelty effects. With novelty effects, even short-term metrics like CTR will behave differently when the newness wears off, so alternatively, We should look at a daily view of the treatment effects.\n\nOr days since exposure view.\n\nThe necessary condition is that the short-term effects are different, and wear off over time. You will usually see a higher daily delta on CTR at the beginning, and then become lower and stay lower. But for certain changes, the other direction can happen as well. For example, when Facebook changed from a timestamp-based news feed (EdgeRank) to a machine-learning curated news feed, users’ initial reactions were not welcoming.\n\nHow to safely control for novelty effects\n\nEye-balling data to draw conclusions has challenges for two reasons:\n\nApophenia, or pattern-matching biases, would trick you into conjuring conclusions that are not there.\n\nLack of statistical power\n\nAs a result, we should be cautious about sentencing the temporary deviation in treatment effects as novelty effects. Luckily, the downside of ignoring those deviations is low, so we recommend to\n\nUse the “days since exposure” view in your experiment results to find suspects of novelty effects – shallow metrics such as CtR behave differently between short-term and long-term\n\nThrow away the first few days of data where you believe results might be driven by novelty effects\n\nMake decisions on your experiments using the remaining data, where we believe the effect converges to an equilibrium, long-term value.\n\nDon’t be paranoid\n\nNovelty effects are a luxury. For a user to feel novelty, she needs to have expectations of her experience with your product, which is not the case for most businesses. Products where users can detect differences in experience are where you’d see the largest manifestation of the novelty effect. This is a good thing.\n\nThe other piece of the puzzle is the degree of change. For McDonald's, a change in the UI may not trigger novelty effects, but re-introducing McRibs probably will. So the simple formula is\n\nNovelty ∝ Surprise X Frequency\n\nIf you want to be more careful, bucket users in terms of frequency of usage. Higher frequency users will be more likely to have novelty effects. Also, new users are not likely to experience novelty effects, but the other direction of this logic doesn’t hold: If a feature is impactful on existing users but not impactful on new users, you can’t claim its novelty effect, as many more factors are innate different between new users and existing users.\n\nTake advantage of novelty effects\n\nRemember, novelty effects are just psychological effects due to human nature. It is part of the treatment. So why not use it? For example\n\nDrive more traffic into your strategic moves (such as ML-ranked news feed)\n\nSynergize with other campaigns (banners, promotions, ads) to amplify your message\n\nDesign a combined approach for feature rollout – such as nux at the beginning to explain the change\n\nUse historical experiments to build priors for novelty effects\n\nThe principle is that your job as a product data scientist is to improve your product and your business, not to design the experiment for the sake of answering hypothetical questions (learning is important but don’t learn useless things). If you can use your knowledge and design experiments to make your business more successful, do it."
    }
}
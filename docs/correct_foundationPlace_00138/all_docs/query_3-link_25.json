{
    "id": "correct_foundationPlace_00138_3",
    "rank": 25,
    "data": {
        "url": "https://github.com/mcronce/SevOne-Deferred-Data-Scripts",
        "read_more_link": "",
        "language": "en",
        "title": "source deferred data scripts for SevOne",
        "top_image": "https://opengraph.githubassets.com/0a3a446ed1095bd09655f39b7cbf6b3d0e5d84c509d8903cf62919b64f883f11/mcronce/SevOne-Deferred-Data-Scripts",
        "meta_img": "https://opengraph.githubassets.com/0a3a446ed1095bd09655f39b7cbf6b3d0e5d84c509d8903cf62919b64f883f11/mcronce/SevOne-Deferred-Data-Scripts",
        "images": [],
        "movies": [],
        "keywords": [],
        "meta_keywords": [
            ""
        ],
        "tags": null,
        "authors": [],
        "publish_date": null,
        "summary": "",
        "meta_description": "A collection of open-source deferred data scripts for SevOne - mcronce/SevOne-Deferred-Data-Scripts",
        "meta_lang": "en",
        "meta_favicon": "https://github.com/fluidicon.png",
        "meta_site_name": "GitHub",
        "canonical_link": "https://github.com/mcronce/SevOne-Deferred-Data-Scripts",
        "text": "A collection of open-source deferred data scripts for SevOne\n\nTo get any of the adapters going that you haven't run before, run the script and pipe it to deferred-data-import, pointed to a SevOne appliance and device that you want to have own the objects. After it completes, discover that device within SevOne; after discovery completes, rerun the script again. You should have your first data points on that object.\n\nThis is the core script in the collection. It is agnostic to the data being supplied to it; in true Unix style, it simply reads data points from STDIN and imports them to a SevOne appliance using the Deferred Data SOAP API mechanism.\n\nRequired information is supplied to deferred-data-import via command-line, as follows:\n\nData input should follow this format:\n\nNote that the units can be just a single unit for both measurement and display, or it can be of the format \"measurement units//display units\"\n\n\"Indicator format\" refers to \"GAUGE\", \"COUNTER32\", or \"COUNTER64\". Most of what you import via Deferred Data will be GAUGE. Refer to the SevOne manual for more information. It does attempt to use quotes to allow spaces in text fields, command-line style, but note that this mechanism is not perfect. Non-numeric values will be treated as dropped polls.\n\nAll the processors are executable; provided you have the proper language interpreters installed, simply running them with the following syntax should get your data imported fine:\n\nThis works exactly the same way as deferred-data-import, but is optimized for backfilling a large volume of historical data by making use of the multi-row insert functionality in SevOne's plugin_deferred_insertDataRows() API function. It further optimizes for large data volume by caching, in memory, metadata that would otherwise be retrieved from the SevOne appliance for every input line; object types, indicator types, objects, and indicators.\n\nLanguage: Python\n\nType: API\n\nReference: http://www.atlassian.com/software/bamboo\n\nUsage:\n\nbamboo-rest -u username -p password [options]\n\nProvided you have Python and the dependencies installed, this one is easy. I'll list the depdencies below.\n\nPython dependencies:\n\ngetopt\n\nurllib2\n\ncalendar\n\njson\n\ndatetime\n\nLanguage: PHP\n\nType: Local file\n\nRefernece: None\n\nUsage:\n\nprocess-mileage /path/to/csv # Accepts a filename as a parameter process-mileage < /path/to/csv # Also accepts data over STDIN\n\nThis one processes gas mileage data, imported to a file manually from fuel receipts. It expects CSV files in the following format:\n\nSpecial considerations:\n\nTime will be converted to a timestamp using strftime(). It's magic. Don't worry about how it works.\n\nThis was tested in the following format: \"Y-m-d H:i:s Z\"\n\nEach gas station you use will be a separate object, in addition to an object that contains data points for all the others. This is mainly intended for comparing fuel quality from one brand to the next. In the distant future (the year 2000), there will be a command-line switch to disable the separate stations in order to save elements on the SevOne appliance it's being imported to\n\nAny numeric fields that are found to contain a non-numeric value will have a NULL inserted for that indicator at that data point\n\nKnown issues:\n\nCan consume a lot of elements; need to have an option to not save individual fuel branding objects\n\nLanguage: Python\n\nType: Screen scraper + API\n\nReference: http://www.mint.com/\n\nUsage:\n\nmint 'Mint registered E-mail address' 'Mint password'\n\nKnown issues/caveats:\n\nFor this to work, you MUST run python3 setup.py build and python3 setup.py install as root in processors/libraries/mintapi\n\nDoes not pull down details on individual investments, only whole accounts\n\nAuto-refreshes accounts at the end of the script, rather than the beginning\n\nLanguage: Python\n\nType: Database processor\n\nReference: http://www.wordpress.org/\n\nUsage:\n\nwordpress [options] mysql-host 'MySQL user' 'MySQL password' 'MySQL db'\n\nThis processor connects to a MySQL database backing a Wordpress instance, does some processing, and exports some statistics about posts, metadata, comments, categories, and tags.\n\nPython dependencies:\n\ncalendar\n\ndatetime\n\nMySQLdb\n\noptparse\n\ntime\n\nLanguage: Python\n\nType: Screen scraper\n\nReference: http://www.alexa.com/\n\nUsage:\n\nalexa domain-name\n\nThis processor scrapes the page on alexa.com for the given domain name for rank, reach, and other data.\n\nPython dependencies:\n\ncalendar\n\ndatetime\n\nlxml\n\nmechanize\n\noptparse\n\ntime\n\nLanguage: bash\n\nType: System poller\n\nUsage:\n\nraspberry-pi sshuser@ip.or.hostname [/path/containing/vcgencmd]\n\nThis processor connects to a Raspberry Pi running one of the many flavors of Linux that support the little computer.\n\nThe statistics this exports include current temperature, clocks, voltages, and system load.\n\nNote that passwordless SSH from the system running this poller to the Raspberry Pi must be enabled; also, the vcgencmd binary must be present somewhere on the Raspberry Pi's filesystem. By default, this looks for vcgencmd in the path that OpenELEC places it in (/usr/bin); if your vcgencmd binary is elsewhere, you must pass that path in (e.g. \"/opt/vc/bin\" for Debian Wheezy) as the second parameter.\n\nLanguage: Python\n\nType: Database processor\n\nReference: http://www.ampache.org/\n\nUsage:\n\nampache [options] mysql-host 'MySQL user' 'MySQL password' 'MySQL db'\n\nThis processor connects to a MySQL database backing an Ampache instance, does some processing, and exports some statistics about posts, metadata, comments, categories, and tags.\n\nPython dependencies:\n\ncalendar\n\ndatetime\n\nMySQLdb\n\noptparse\n\ntime\n\nLanguage: Python\n\nType: API\n\nReference: http://www.plexapp.com/\n\nUsage:\n\nplex-media-server [options] plex-host\n\nThis processor connects to a system running Plex Media Server and scrapes statistics about the various libraries using the REST API.\n\nIt should be noted that Plex Media Server seems to hit the CPU on its host hard while this scraper is running.\n\nPython dependencies:\n\ncalendar\n\ndatetime\n\nlxml\n\noptparse\n\ntime\n\nLanguage: CasperJS\n\nType: Screen scraper\n\nReference: https://www.atlassian.com/software/jira\n\nUsage:\n\njira-board 'board URL' 'Jira username' 'Jira password'\n\nThis processor logs into a Jira server, navigates to an Agile board, and exports some statistics about the issues on that Agile board.\n\nLanguage: Python\n\nType: API\n\nReference: http://www.atlassian.com/software/bamboo\n\nUsage:\n\njira-filter -u username -p password [options] \"filter-ID-or-JQL\"\n\nProvided you have Python and the dependencies installed, this one is easy. I'll list the depdencies below.\n\nPython dependencies:\n\ngetopt\n\nurllib2\n\ncalendar\n\njson\n\ndatetime\n\nLanguage: Python\n\nType: Screen scraper\n\nReference: http://www.polldaddy.com/\n\nUsage:\n\npolldaddy poll-id\n\nThis processor scrapes a given poll on PollDaddy for vote statistics.\n\nPython dependencies:\n\ncalendar\n\ndatetime\n\nlxml\n\nmechanize\n\noptparse\n\ntime\n\nLanguage: CasperJS\n\nType: Screen scraper\n\nReference: http://www.mytotalconnectcomfort.com/\n\nUsage:\n\nhoneywell-thermostat email password\n\nThis processor logs into Honeywell's \"My Total Connect Comfort\" site and reads data from your thermostats. Each thermostat is an object, with several indicators about the indoor/outdoor state and state of the system.\n\nLanguage: Python\n\nType: Screen scraper\n\nReference: http://www.pathofexile.com/\n\nUsage:\n\npath-of-exile Account_Name\n\nThis processor scrapes a player's account data for a free-to-play game called Path of Exile.\n\nPython dependencies:\n\ncalendar\n\ndatetime\n\nlxml\n\nmechanize\n\noptparse\n\ntime\n\nLanguage: bash\n\nType: System poller\n\nUsage:\n\nlinux-disk\n\nThis processor uses df and mount to scrape usage data about the mounted filesystems on a Linux machine. SNMP would normally be used for this, but if SNMP is impossible, this utility will provide an easy workaround. This may work with other flavors of Unix, but is untested.\n\nLanguage: Python\n\nType: Screen scraper\n\nReference: http://www.killingfloorthegame.com/\n\nUsage:\n\nkilling-floor server.hostname.or.ip Admin_username Admin_password\n\nThis processor scrapes the webadmin interface for a Killing Floor server and retrieves server/player statistics\n\nPython dependencies:\n\ncalendar\n\ndatetime\n\nlxml\n\nmechanize\n\noptparse\n\ntime\n\nLanguage: bash\n\nType: System poller\n\nUsage:\n\nlinux-nvidia X_Display_ID\n\nEx: linux-nvidia :0.0\n\nThis processor uses the nvidia-settings CLI utility to scrape data about an NVidia graphics card; metadata and utilization statistics alike. An xAgent net-snmp plugin would be nicer for this purpose, but that doesn't seem to currently exist.\n\nLanguage: Python\n\nType: Log parser\n\nReference: http://sourceforge.net/projects/cpuminer/\n\nUsage:\n\nminerd-journalctl systemd-service-name sevone.ip.or.hostname sevone-username sevone-password sevone-device-name\n\nThis processor uses journalctl to read the log coming out of a minerd or compatible cryptocurrency miner (such as cpuminer or cudaminer) either live as it's written or into the past if --backfill is passed.\n\nThis processor is unique in that it runs for an extended period of time, typically as a daemon, when not in backfil mode. It spins up journalctl and deferred-data-import as subprocesses and keeps them alive as long as it runs.\n\nA systemd unit file is included: /utilities/deferred-minerd@.service - it will need to be modified before installation. Most users should only need to modify the capitalized text.\n\nNote that this processor currently inserts a data point for, basically, every log entry. Eventually it will perform aggregation-on-the-fly, but for now, be careful, as you could end up with a lot of data on your disk.\n\nPython dependencies:\n\ndatetime\n\njson\n\noptparse\n\nsubprocess\n\nsys\n\ntime\n\nLanguage: Python\n\nType: API\n\nReference: https://github.com/ckolivas/cgminer\n\nUsage:\n\ncgminer [options] cgminer.ip.or.hostname\n\nThis processor connects to the API on a cgminer process to scrape stats. It can currently scrape whole process stats and stats about GPUs; it doesn't yet support USB devices like ASICs.\n\nPython dependencies:\n\ndatetime\n\njson\n\noptparse\n\nsocket\n\nsys\n\ntime\n\nLanguage: CasperJS\n\nType: Screen scraper\n\nReference: http://www.wemineltc.com/\n\nUsage:\n\nwemineltc Username Password\n\nThis processor logs into a We Mine LTC account and scrapes data about the LTC network as a whole, the We Mine LTC pool, the user's account, and the user's individual workers.\n\nLanguage: CasperJS\n\nType: Screen scraper\n\nReference: http://www.wemineftc.com/\n\nUsage:\n\nwemineftc Username Password\n\nThis processor logs into a We Mine FTC account and scrapes data about the ftc network as a whole, the We Mine FTC pool, the user's account, and the user's individual workers.\n\nLanguage: CasperJS\n\nType: Screen scraper\n\nReference: http://switchercoin.com/\n\nUsage:\n\nswitchercoin Username Password\n\nThis processor logs into a Switchercoin account and scrapes data about the Switchercoin multipool, the user's account, and the user's individual workers.\n\nLanguage: bash\n\nType: API\n\nReference: https://blockchain.info/\n\nUsage:\n\nbitcoin-blockchain address1 \"Friendly name 1\" [address2 \"Friendly name 2\" [... addressN \"Friendly name N\"]]\n\nThis processor uses the plain-text API on https://blockchain.info/ to pull the BTC balance for a given address on the blockchain. It can process an arbitrary number of addresses per call. Friendly names are just used as object descriptions for SevOne; they can be anything you want.\n\nLanguage: Python\n\nType: Screen scraper\n\nReference: http://www.wafflepool.com/\n\nUsage:\n\nwafflepool btc-address\n\nThis processor scrapes a specific miner's statistics from http://www.wafflepool.com/ and imports them. Objects exported by this processor are fairly wide, with eight indicators per individual altcoin plus some.\n\nLanguage: Python\n\nType: API\n\nReference: http://finance.google.com/\n\nUsage:\n\ngoogle-finance-tracker ticker google-finance-tracker [exchange:]ticker google-finance-tracker [exchange1:]ticker1[,[exchange2]:ticker2[... ,[exchangeN]:tickerN]]\n\nThis processor uses the Google Finance REST API to pull down information for an exchange-traded stock ticker - including current price, volume, EPS, P/E, and others.\n\nLanguage: Python\n\nType: API\n\nReference: https://developer.yahoo.com/yql/console/\n\nUsage:\n\nyahoo-weather WOEID yahoo-weather 12797352\n\nThis processor uses the Yahoo! Weather REST API to gather information about the weather in a given location.\n\nYou must pass it a WOEID, which is a unique number assigned to a specific location on Earth - WOEID stands for \"Where On Earth IDentifier\". The example WOEID, 12797352, is assigned to Newark, CA, United States.\n\nTo look up the WOEID for a location, you can use a third-party tool found at the following URL: http://woeid.rosselliot.co.nz/\n\nLanguage: Python\n\nType: Local filesystem\n\nReference: http://www.owncloud.com/\n\nUsage:\n\nowncloud /path/to/owncloud/install owncloud /usr/share/webapps/owncloud\n\nThis processor looks at individual users' directories within an Owncloud data directory; it provides statistics about number and size of files/directories active, in trash, in version control, in cache, and in the gallery.\n\nLanguage: Python\n\nType: API\n\nReference: http://us.battle.net/d3/en/\n\nUsage:\n\ndiablo-3 Profile-ID diablo-3 Derp-1915\n\nThis processor scrapes statistics about a Diablo 3 character and account from Blizzard's community stats REST API.\n\nTo find the Profile-ID for your account, look at the URL for your web profile; as an example URL, we have the following:\n\n\"Derp-1915\" would be the Profile-ID, in this case.\n\nLanguage: Python\n\nType: API\n\nReference: http://us.battle.net/sc2/en/\n\nUsage:\n\nstarcraft-2 ProfilePath starcraft-2 8072831/1/Derp\n\nThis processor scrapes statistics about a Starcraft 2 profile from Blizzard's community stats API.\n\nTo find the ProfilePath for your account, look at the URL for your web profile; as an example URL, we have the following:\n\n\"8072831/1/Derp\" would be the ProfilePath, in this case.\n\nLanguage: Python 2.7\n\nType: Screen scraper\n\nReference: http://eq.magelo.com/\n\nUsage:\n\nmagelo-eq Profile-ID magelo-eq 665922\n\nThis processor scrapes web statistics about an EverQuest 1 character from magelo.com's web interface.\n\nTo find the Profile-ID for your character, look at the URL for your web profile; as an example URL, we have the following:\n\n\"665922\" would be the Profile-ID, in this case.\n\nCurrently, you must run a separate script for each profile ID you wish to poll. That will change in the future.\n\nLanguage: Python 2.7\n\nType: API\n\nReference: https://pi-hole.net/\n\nUsage:\n\npi-hole hostname-or-IP pi-hole dns.localdomain pi-hole 192.168.1.201\n\nThis processor connects to a Pi-Hole DNS server's API and scrapes the statistics that it exports.\n\nYou simply pass it the hostname or IP address of the Pi-Hole server."
    }
}
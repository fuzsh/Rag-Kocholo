{
    "id": "dbpedia_8284_2",
    "rank": 45,
    "data": {
        "url": "https://dba.stackexchange.com/questions/20335/can-mysql-reasonably-perform-queries-on-billions-of-rows",
        "read_more_link": "",
        "language": "en",
        "title": "Can MySQL reasonably perform queries on billions of rows?",
        "top_image": "https://cdn.sstatic.net/Sites/dba/Img/apple-touch-icon@2.png?v=246e2cb2439c",
        "meta_img": "https://cdn.sstatic.net/Sites/dba/Img/apple-touch-icon@2.png?v=246e2cb2439c",
        "images": [
            "https://cdn.sstatic.net/Sites/dba/Img/logo.svg?v=d844126b2d09",
            "https://i.sstatic.net/4KwNS.png",
            "https://www.gravatar.com/avatar/c358a8edbaf5de680bdb55262a2a0a8f?s=64&d=identicon&r=PG",
            "https://www.gravatar.com/avatar/e80f81252628a0639fba3f650bfbd955?s=64&d=identicon&r=PG",
            "https://lh6.googleusercontent.com/-eNYXQZkdTpI/AAAAAAAAAAI/AAAAAAAAAGI/m6qdokrhgKo/photo.jpg?sz=64",
            "https://www.gravatar.com/avatar/31285349ec4f4264e233abec80ada005?s=64&d=identicon&r=PG",
            "https://www.gravatar.com/avatar/a007be5a61f6aa8f3e85ae2fc18dd66e?s=64&d=identicon&r=PG",
            "https://i.sstatic.net/GKB1U.jpg?s=64",
            "https://www.gravatar.com/avatar/fc62b586b7a6e298551ac69188390bb8?s=64&d=identicon&r=PG",
            "https://www.gravatar.com/avatar/2d4bc975c61040bf5351fdfa252729ff?s=64&d=identicon&r=PG",
            "https://www.gravatar.com/avatar/cfae0f500e26421eb84cb19fe9c809e4?s=64&d=identicon&r=PG",
            "https://www.gravatar.com/avatar/c358a8edbaf5de680bdb55262a2a0a8f?s=64&d=identicon&r=PG",
            "https://www.gravatar.com/avatar/d45ef03e9450599588bf733662f792e5?s=64&d=identicon&r=PG",
            "https://www.gravatar.com/avatar/f01b6bafd728251f8d7efd330f4b93f0?s=64&d=identicon&r=PG",
            "https://www.gravatar.com/avatar/d767f66c88601286938590f8ab51bf35?s=64&d=identicon&r=PG",
            "https://www.gravatar.com/avatar/eea5083041d267828ea9fd38f2e2d699?s=64&d=identicon&r=PG",
            "https://www.gravatar.com/avatar/f674619ceb6ff733b35d68dce733f85f?s=64&d=identicon&r=PG",
            "https://www.gravatar.com/avatar/8440c1cc847e9fd1e0272853724e6060?s=64&d=identicon&r=PG",
            "https://i.sstatic.net/4QmiG.jpg?s=64",
            "https://www.gravatar.com/avatar/f86113e32e3acc6dc33a3cf474652613?s=64&d=identicon&r=PG",
            "https://www.gravatar.com/avatar/f4433bc0c3d705f453a908bfc075415e?s=64&d=identicon&r=PG",
            "https://www.gravatar.com/avatar/0b4fa970be41e2286c39f5b0167e6d9d?s=64&d=identicon&r=PG",
            "https://www.gravatar.com/avatar/291a1c689d1de3c21862bac990cbcfb0?s=64&d=identicon&r=PG",
            "https://www.gravatar.com/avatar/b18a4ef1d2e2c5af3863e8f448f8d4e2?s=64&d=identicon&r=PG&f=y&so-version=2",
            "https://dba.stackexchange.com/posts/20335/ivc/0d79?prg=632e4b37-9f11-4472-9508-6a22d5781e40"
        ],
        "movies": [],
        "keywords": [],
        "meta_keywords": [
            ""
        ],
        "tags": null,
        "authors": [
            "Tassos Bassoukos Tassos Bassoukos"
        ],
        "publish_date": "2012-07-02T19:36:13",
        "summary": "",
        "meta_description": "I am planning on storing scans from a mass spectrometer in a MySQL database and\nwould like to know whether storing and analyzing this amount of data is remotely\nfeasible. I know performance varies ...",
        "meta_lang": "en",
        "meta_favicon": "https://cdn.sstatic.net/Sites/dba/Img/favicon.ico?v=fccaf00a9c8c",
        "meta_site_name": "Database Administrators Stack Exchange",
        "canonical_link": "https://dba.stackexchange.com/questions/20335/can-mysql-reasonably-perform-queries-on-billions-of-rows",
        "text": "I am not very familiar with your needs, but perhaps storing each data point in the database is a bit of overkill. It sound almost like taking the approach of storing an image library by storing each pixel as a separate record in a relational database.\n\nAs a general rule, storing binary data in databases is wrong most of the time. There is usually a better way of solving the problem. While it is not inherently wrong to store binary data in relational database, often times the disadvantages outweigh the gains. Relational databases, as the name alludes to, are best suited for storing relational data. Binary data is not relational. It adds size (often significantly) to databases, can hurt performance, and may lead to questions about maintaining billion-record MySQL instances. The good news is that there are databases especially well suited for storing binary data. One of them, while not always readily apparent, is your file system! Simply come up with a directory and file naming structure for your binary files, store those in your MySQL DB together with any other data which may yield value through querying.\n\nAnother approach would be using a document-based storage system for your datapoints (and perhaps spectra) data, and using MySQL for the runs (or perhaps putting the runs into the same DB as the others).\n\nI once worked with a very large (Terabyte+) MySQL database. The largest table we had was literally over a billion rows. This was using MySQL 5.0, so it's possible that things may have improved.\n\nIt worked. MySQL processed the data correctly most of the time. It was extremely unwieldy though. (If you want six sigma-level availability with a terabyte of data, don't use MySQL. We were a startup that had no DBA and limited funds.)\n\nJust backing up and storing the data was a challenge. It would take days to restore the table if we needed to.\n\nWe had numerous tables in the 10-100 million row range. Any significant joins to the tables were too time consuming and would take forever. So we wrote stored procedures to 'walk' the tables and process joins against ranges of 'id's. In this way we'd process the data 10-100,000 rows at a time (Join against id's 1-100,000 then 100,001-200,000, etc). This was significantly faster than joining against the entire table.\n\nUsing indexes on very large tables that aren't based on the primary key is also much more difficult. Mysql 5.0 stores indexes in two pieces -- it stores indexes (other than the primary index) as indexes to the primary key values. So indexed lookups are done in two parts: First MySQL goes to an index and pulls from it the primary key values that it needs to find, then it does a second lookup on the primary key index to find where those values are.\n\nThe net of this is that for very large tables (1-200 Million plus rows) indexing against tables is more restrictive. You need fewer, simpler indexes. And doing even simple select statements that are not directly on an index may never come back. Where clauses must hit indexes or forget about it.\n\nBut all that being said, things did actually work. We were able to use MySQL with these very large tables and do calculations and get answers that were correct.\n\nTrying to do analysis on 200 billion rows of data would require very high-end hardware and a lot of hand-holding and patience. Just keeping the data backed up in a format that you could restore from would be a significant job.\n\nI agree with srini.venigalla's answer that normalizing the data like crazy may not be a good idea here. Doing joins across multiple tables with that much data will open you up to the risk of file sorts which could mean some of your queries would just never come back. Denormallizing with simple, integer keys would give you a better chance of success.\n\nEverything we had was InnoDB. Regarding MyISAM vs. InnoDB: The main thing would be to not mix the two. You can't really optimize a server for both because of the way MySQL caches keys and other data. Pick one or the other for all the tables in a server if you can. MyISAM may help with some speed issues, but it may not help with the overall DBA work that needs to be done - which can be a killer.\n\nnormalizing the data like crazy\n\nNormalizing the data like crazy may not be the right strategy in this case. Keep your options open by storing the data both in the Normalized form and also in the form of materialized views highly suited to your application. Key in this type of applications is NOT writing adhoc queries. Query modeling is more important than data modeling. Start with your target queries and work towards the optimum data model.\n\nIs this reasonable?\n\nI would also create an additional flat table with all data.\n\nrun_id | spectrum_id | data_id | <data table columns..> |\n\nI will use this table as the primary source of all queries. The reason is to avoid having to do any joins. Joins without indexing will make your system very unusable, and having indexes on such huge files will be equally terrible.\n\nStrategy is, query on the above table first, dump the results into a temp table and join the temp table with the look up tables of Run and Spectrum and get the data you want.\n\nHave you analyzed your Write needs vs Read needs? It will be very tempting to ditch SQL and go to non-standard data storage mechanisms. In my view, it should be the last resort.\n\nTo accelerate the write speeds, you may want to try the Handler Socket method. Percona, if I remember, packages Handler Socket in their install package. (no relation to Percona!)\n\nhttp://yoshinorimatsunobu.blogspot.com/2010/10/using-mysql-as-nosql-story-for.html\n\nThe short answer is a qualified yes -- as the number of rows grows the precise schema, datatypes and operations you choose grows in importance.\n\nHow much you normalize your data depends on the operations you plan to perform on the stored data. Your 'datapoints' table in particular seems problematic -- are you planning on comparing the nth point from any given spectra with the mth of any other? If not, storing them separately could be a mistake. If your datapoints do not stand alone but make sense only in the context of their associated spectra you don't need a PRIMARY KEY -- a foreign key to the spectra and an 'nth' column (your 'index' column?) will suffice.\n\nDefine the inter- and intra-spectrum operations you must perform and then figure out the cheapest way to accomplish them. If equality is all that's needed they may be denormalized -- possibly with some pre-calculated statistical metadata that assist your operations. If you do absolutely need in-SQL access to individual datapoints ensure you reduce the size of each row to the bare minimum number of fields and the smallest datatype possible.\n\nThe largest MySQL I've ever personally managed was ~100 million rows. At this size you want to keep your rows and thus your fields fixed-size -- this allows MySQL to efficiently calculate the position of any row in the table by multiplying times the fixed size of each row (think pointer arithmetic) -- though the exact details depend on which storage engine you plan on using. Use MyISAM if you can get away with it, what it lacks in reliability it makes up for in speed, and in your situation it should suffice. Replace variable-size fields such as VARCHAR with CHAR(n) and use RTRIM() on your read queries.\n\nOnce your table rows are fixed-width you can reduce the number of bytes by carefully evaluating MySQL's integer datatypes (some of which are non-standard). Every 1-byte savings you can eke out by converting a 4-byte INT into a 3-byte MEDIUMINT saves you ~1MB per million rows -- meaning less disk I/O and more effective caching. Use the smallest possible datatypes that you can get away with. Carefully evaluate the floating point types and see if you can replace 8-byte DOUBLEs with 4-byte FLOATs or even <8 byte fixed-point NUMERICs. Run tests to ensure that whatever you pick doesn't bite you later.\n\nDepending on the expected properties of your dataset and the operations required there may be further savings in more unusual encodings of your values (expected patterns/repetitions that can be encoded as an index into a set of values, raw data that may only meaningfully contribute to metadata and be discarded, etc) -- though exotic, unintuitive, destructive optimizations are only worthwhile when every other option has been tried.\n\nMost importantly, no matter what you end up doing, do not assume you have picked the perfect schema and then blindly begin dumping 10s of millions of records in. Good designs take time to evolve. Create a large but manageable (say, 1-5%) set of test data and verify the correctness and performance of your schema. See how different operations perform (http://dev.mysql.com/doc/refman/5.0/en/using-explain.html) and ensure that you balance you schema to favor the most frequent operations.\n\nDid I say short? Whoops. Anyways, good luck!\n\nIt would seem that the only reason to shred the data point data out of the XML (as opposed to the metadata like the time and type of run) and into a database form is when you are analyzing the spectra across arrays - i.e. perhaps finding all runs with a certain signature. Only you know your problem domain right now, but this could be akin to storing music sampled at 96kHz with 1 sample per row. I'm not sure size is the issue more than how the data is used. Querying across the data would be equivalent to asking the relative amplitude 2 minutes into the song across all songs by The Beatles. If you know the kind of analyses which might be performed, it's quite possible that performing these on the signals and storing those in the metadata about the run might make more sense.\n\nI'm also not sure if your source data is sparse. It's completely possible that a spectrum in the database should only include non-zero entries while the original XML does include zero-entries, and so your total number of rows could be much less than in the source data.\n\nSo, like many questions, before asking about MySQL handling your model, stepping back and looking at the model and how it is going to be used is probably more appropriate than worrying about performance just yet.\n\nAfter reviewing your question updates, I think a model where the binary data is stored as a BLOB or just a pointer to the file is sufficient and work on modifying your model to store data about the significant peaks which have been identified when the data is first read.\n\nI run a web analytics service with about 50 database servers, each one containing many tables over 100 million rows, and several that tend to be over a billion rows, sometimes up to two billion (on each server).\n\nThe performance here is fine. It is very normalized data. However - my main concern with reading this is that you'll be well over the 4.2 billion row mark for these tables (maybe not \"runs\" but probably the other two), which means you'll need to use BIGINT instead of INT for the primary/foreign keys.\n\nMySQL performance with BIGINT fields in an indexed column is ridiculously horrible compared to INT. I made the mistake of doing this once with a table I thought might grow over this size, and once it hit a few hundred million rows the performance was simply abysmal. I don't have raw numbers but when I say bad, I mean Windows ME bad.\n\nThis column was the primary key. We converted it back to be just an INT and presto magico, the performance was good again.\n\nAll of our servers at the time were on Debian 5 and with MySQL 5.0. We have since upgraded to Debian 6 and Percona MySQL 5.5, so things may have improved since then. But based on my experience here, no, I don't think it will work very well.\n\nEDIT:\n\nDO NOT DO THIS IN MYSQL WITH DATA STORED ON A SINGLE DISK. Just reading that amount of data from a single medium will take hours. You need to SCALE OUT, NOT UP.\n\nAnd you need to denormalize your data if you want to do effective data analysis. You are not designing a online system here. You want to crunch numbers, design accordingly.\n\nOriginal answer below line.\n\nThe answer will vary depending on your queries, MySQL may not be the best tool for this job. You may want to look at solution you can scale \"out\" and not \"up\". If you are willing to put in some effort maybe you should look on a Map Reduce solution such as Hadoop.\n\nIf you want to do more ad-hoc queries Google's BigQuery solution may be a good fit for you. Relevant presentation from Google I/O 2012: Crunching Big Data with BigQuery\n\nSo, the solution will depend on if this is a one-shot thing and if you want to reasonably support ad hoc queries.\n\nWhat kind of machine is the data going to be stored on? Is it a shared storage devices?\n\nThe ultimate factor that will dictate your query time is going to be your harddrives. Databases and their query optimizers are designed to reduce the number of disk I/Os as much as possible. Given that you only have 3 tables, this will be done pretty reliably.\n\nA harddrive's read/write speeds are going to be 200-300 times slower than memory speeds. Look for harddrives with very fast latency and fast read and write speeds. If all this data is on one 2-TB drive, you're probably going to be waiting a long long time for queries to finish. Harddrive latency is ~10-15milliseconds while the memory latency is less than 10nanoseconds. Harddrive latency can be 1000-2000x slower than memory latency. The moving of the mechanical arm on the harddrive the is SLOWEST thing in this entire system.\n\nHow much RAM do you have? 16gb? Lets say that lets you hold 32 records. You have 16000 files. If you're going to linear scan all the datapoints, you could easily end up with 5-10 seconds in seek time alone. Then factor in the transfer rate 50mb/s? About 7 hours. Additionally, any temporarily saved data will have to be stored on the harddirve to make room for new data being read.\n\nIf you're using a shared storage device that's being actively used by other users... your best bet is going to run everything at night.\n\nReduce the number of nested queries helps also well. Nested queries result in temporary tables which will thrash your harddrive even more. I hope you have PLENTY of free space on your harddrive.\n\nThe query optimization can only look at 1 query at a time. So nested select statements can't be optimized. HOWEVER, if you know a specific nested query is going to result in a small dataset to be returned, keep it. The query optimization uses histograms and rough assumptions, if you know something about the data and the query then go ahead and do it.\n\nThe more you know about the way your data is stored on disk, the faster you'll be able to write your queries. If everything was stored sequentially on the primary key, it may be beneficial to sort the primaries keys returned from a nested query. Also, if you can reduce the set of datasets you need to analyze at all beforehand, do it. Depending on your system, you're look at around 1 second of data transfer per file.\n\nIf you're going to modify the Name values(the varchars) I would change it to a datatype with a maximum size, it'll prevent fragmentation and the trade off is just a few more bytes of memory. Maybe an NVARCHAR with 100 maximum.\n\nAs far as the comments about denormalizing the table. I think it may be best to just store the datapoints in larger groups(maybe as spectra) and then do the data analysis in python or a language that interacts with the database. Unless your a SQL-Wizard."
    }
}
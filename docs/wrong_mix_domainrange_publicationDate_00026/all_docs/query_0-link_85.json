{
    "id": "wrong_mix_domainrange_publicationDate_00026_0",
    "rank": 85,
    "data": {
        "url": "http://bactra.org/weblog/2010/",
        "read_more_link": "",
        "language": "en",
        "title": "2010",
        "top_image": "http://bactra.org/sloth/martindale-original.png",
        "meta_img": "",
        "images": [
            "http://bactra.org/weblog/sloth/700_1.gif",
            "http://bactra.org/weblog/sloth/700_2.gif",
            "http://bactra.org/weblog/sloth/700_3.gif",
            "http://bactra.org/weblog/sloth/700_4.gif",
            "http://bactra.org/weblog/sloth/700_5.gif",
            "http://bactra.org/weblog/sloth/700_6.gif",
            "http://bactra.org/weblog/sloth/700_7.gif",
            "http://bactra.org/weblog/sloth/700_8.gif",
            "http://bactra.org/weblog/sloth/700_9.gif",
            "http://bactra.org/weblog/sloth/700_10.gif",
            "http://bactra.org/weblog/sloth/700_11.gif",
            "http://bactra.org/weblog/sloth/700_12.gif",
            "http://bactra.org/weblog/sloth/700_13.gif",
            "http://bactra.org/weblog/sloth/700_14.gif",
            "http://bactra.org/weblog/sloth/700_15.gif",
            "http://bactra.org/weblog/sloth/700_16.gif",
            "http://bactra.org/weblog/sloth/700_17.gif",
            "http://bactra.org/weblog/sloth/700_18.gif",
            "http://bactra.org/weblog/sloth/700_19.gif",
            "http://bactra.org/weblog/sloth/700_20.gif",
            "http://bactra.org/weblog/sloth/700_21.gif",
            "http://bactra.org/weblog/sloth/700_22.gif",
            "http://bactra.org/weblog/sloth/700_23.gif",
            "http://bactra.org/weblog/sloth/700_24.gif",
            "http://bactra.org/weblog/sloth/700_25.gif",
            "http://bactra.org/weblog/sloth/700_26.gif",
            "http://bactra.org/weblog/sloth/700_27.gif",
            "http://bactra.org/weblog/sloth/700_28.gif",
            "http://bactra.org/weblog/sloth/700_29.gif",
            "http://bactra.org/weblog/sloth/700_30.gif",
            "http://bactra.org/weblog/sloth/700_31.gif",
            "http://bactra.org/weblog/sloth/700_32.gif",
            "http://bactra.org/weblog/sloth/700_33.gif",
            "http://bactra.org/weblog/sloth/700_34.gif",
            "http://bactra.org/weblog/sloth/700_35.gif",
            "http://bactra.org/weblog/sloth/700_36.gif",
            "http://bactra.org/weblog/sloth/700_37.gif",
            "http://bactra.org/weblog/sloth/700_63.gif",
            "http://bactra.org/weblog/sloth/700_64.gif",
            "http://bactra.org/weblog/sloth/700_65.gif",
            "http://bactra.org/weblog/sloth/700_66.gif",
            "http://bactra.org/weblog/sloth/700_67.gif",
            "http://bactra.org/weblog/sloth/700_68.gif",
            "http://bactra.org/weblog/sloth/700_69.gif",
            "http://bactra.org/weblog/sloth/700_70.gif",
            "http://bactra.org/weblog/sloth/700_71.gif",
            "http://bactra.org/weblog/sloth/700_72.gif",
            "http://bactra.org/weblog/sloth/700_73.gif",
            "http://bactra.org/weblog/sloth/700_74.gif",
            "http://bactra.org/weblog/sloth/700_75.gif",
            "http://bactra.org/weblog/sloth/700_76.gif",
            "http://bactra.org/weblog/sloth/700_77.gif",
            "http://bactra.org/weblog/sloth/700_78.gif",
            "http://bactra.org/weblog/sloth/700_79.gif",
            "http://bactra.org/weblog/sloth/700_80.gif",
            "http://bactra.org/weblog/sloth/700_81.gif",
            "http://bactra.org/weblog/sloth/700_82.gif",
            "http://bactra.org/weblog/sloth/700_83.gif",
            "http://bactra.org/weblog/sloth/700_84.gif",
            "http://bactra.org/weblog/sloth/700_85.gif",
            "http://bactra.org/weblog/sloth/700_86.gif",
            "http://bactra.org/weblog/sloth/700_87.gif",
            "http://bactra.org/weblog/sloth/700_88.gif",
            "http://bactra.org/weblog/sloth/700_89.gif",
            "http://bactra.org/weblog/sloth/700_90.gif",
            "http://bactra.org/weblog/sloth/700_91.gif",
            "http://bactra.org/weblog/sloth/700_92.gif",
            "http://bactra.org/weblog/sloth/700_117.gif",
            "http://bactra.org/weblog/sloth/700_118.gif",
            "http://bactra.org/weblog/sloth/700_119.gif",
            "http://bactra.org/weblog/sloth/700_120.gif",
            "http://bactra.org/weblog/sloth/700_121.gif",
            "http://bactra.org/weblog/sloth/700_122.gif",
            "http://bactra.org/weblog/sloth/700_123.gif",
            "http://bactra.org/weblog/sloth/700_124.gif",
            "http://bactra.org/weblog/sloth/700_125.gif",
            "http://upload.wikimedia.org/wikipedia/commons/0/04/Liver_of_Piacenza.png",
            "http://bactra.org/sloth/679_1.gif",
            "http://bactra.org/sloth/679_2.gif",
            "http://bactra.org/sloth/679_3.gif",
            "http://bactra.org/sloth/679_4.gif",
            "http://bactra.org/sloth/679_5.gif",
            "http://www.stat.cmu.edu/~danielmc/wp-content/uploads/proposal%20wordle.png",
            "http://27.media.tumblr.com/tumblr_l4xuerHh911qbot00o1_400.jpg",
            "http://bactra.org/weblog/sloth/668_1.gif",
            "http://bactra.org/weblog/sloth/668_2.gif",
            "http://bactra.org/weblog/sloth/668_3.gif",
            "http://bactra.org/weblog/sloth/668_4.gif",
            "http://bactra.org/weblog/sloth/668_5.gif",
            "http://bactra.org/weblog/sloth/668_6.gif",
            "http://bactra.org/weblog/sloth/668_7.gif",
            "http://bactra.org/weblog/sloth/668_8.gif",
            "http://bactra.org/weblog/sloth/668_9.gif",
            "http://bactra.org/weblog/sloth/668_10.gif",
            "http://bactra.org/weblog/sloth/668_11.gif",
            "http://www.slothrescue.org/gallery/galeria2/image007.27.jpg",
            "http://bactra.org/sloth/martindale-original.png",
            "http://bactra.org/sloth/martindale-replica-1.png",
            "http://bactra.org/sloth/martindale-replica-sampling-dist.png",
            "http://bactra.org/sloth/martindale-replica-ar-coefficients.png",
            "http://foreignpolicy.com/files/fp_uploaded_images/100527_8-Afghanistan-71.jpg",
            "http://upload.wikimedia.org/wikipedia/en/5/5c/TacomaNarrowsBridgeCollapse_in_color.jpg",
            "http://bactra.org/sloth/latent-homophily.jpg",
            "http://bactra.org/sloth/diffusion-initial-act.jpg",
            "http://bactra.org/sloth/diffusion-midway-act.jpg",
            "http://apod.nasa.gov/apod/image/1004/icevolcano_fulle.jpg",
            "http://29.media.tumblr.com/tumblr_kz7a07u6GD1qzue8ho1_400.png",
            "http://bactra.org/sloth/spline-bootstrap.jpg"
        ],
        "movies": [],
        "keywords": [],
        "meta_keywords": [
            ""
        ],
        "tags": null,
        "authors": [],
        "publish_date": null,
        "summary": "",
        "meta_description": "",
        "meta_lang": "",
        "meta_favicon": "",
        "meta_site_name": "",
        "canonical_link": null,
        "text": "Model Complexity and Prediction Error in Macroeconomic Forecasting (or, Statistical Learning Theory to the Rescue!)\n\nAttention conservation notice: 5000+ words, and many equations, about a proposal to improve, not macroeconomic models, but how such models are tested against data. Given the actual level of debate about macroeconomic policy, isn't it Utopian to worry about whether the most advanced models are not being checked against data in the best conceivable way? What follows is at once self-promotional, technical, and meta-methodological; what would be lost if you checked back in a few years, to see if it has borne any fruit?\n\nSome months ago, we — Daniel McDonald, Mark Schervish, and I — applied for one of the initial grants from the Institute for New Economic Thinking, and, to our pleasant surprise, actually got it. INET has now put out a press release about this (and the other awards too, of course), and I've actually started to get some questions about it in e-mail; I am a bit sad that none of these berate me for becoming a tentacle of the Soros conspiracy.\n\nTo reinforce this signal, and on the general principle that there's no publicity like self-publicity, I thought I'd post something about the grant. In fact what follows is a lightly-edited version of our initial, stage I proposal, which was intended for maximal comprehensibility, plus some more detailed bits from the stage II document. (Those of you who sense a certain relationship between the grant and Daniel's thesis proposal are, of course, entirely correct.) I am omitting the more technical parts about our actual plans and work in progress, because (i) you don't care; (ii) some of it is actually under review already, at venues which insist on double-blinding; and (iii) I'll post about them when the papers come out. In the meanwhile, please feel free to write with suggestions, comments or questions.\n\nUpdate, next day: And already I see that I need to be clearer. We are not trying to come up with a new macro forecasting model. We are trying to put the evaluation of macro models on the same rational basis as the evaluation of models for movie recommendations, hand-writing recognition, and search engines.\n\nProposal: Model Complexity and Prediction Error in Macroeconomic Forecasting\n\nCosma Shalizi (principal investigator), Mark Schervish (co-PI), Daniel McDonald (junior investigator)\n\nMacroeconomic forecasting is, or ought to be, in a state of confusion. The dominant modeling traditions among academic economists, namely dynamic stochastic general equilibrium (DSGE) and vector autoregression (VAR) models, both spectacularly failed to forecast the financial collapse and recession which began in 2007, or even to make sense of its course after the fact. Economists like Narayana Kocherlakota, James Morley, and Brad DeLong have written about what this failure means for the state of macroeconomic research, and Congress has held hearings in an attempt to reveal the perpetrators. (See especially the testimony by Robert Solow.) Whether existing approaches can be rectified, or whether basically new sorts of models are needed, is a very important question for macroeconomics, and, because of the privileged role of economists in policy making, for the public at large.\n\nLargely unnoticed by economists, over the last three decades statisticians and computer scientists have developed sophisticated methods of model selection and forecast evaluation, under the rubric of statistical learning theory. These methods have revolutionized pattern recognition and artificial intelligence, and the modern industry of data mining would not exist without it. Economists' neglect of this theory is especially unfortunate, since it could be of great help in resolving macroeconomic disputes, and determining the reliability of whatever models emerge for macroeconomic time series. In particular, these methods guarantee with high probability that the forecasts produced by models estimated with finite amounts of data will be accurate. This allows for immediate model comparisons without appealing to asymptotic results or making strong assumptions about the data generating process, in stark contrast to AIC and similar model selection criteria. These results are also provably reliable unlike the pseudo-cross validation approach often used in economic forecasting whereby the model is fit using the initial portion of a data set and evaluated on the remainder. (For illustrations of the last, see, e.g., Athanasopoulos and Vahid, 2008; Faust and Wright, 2009; Christoffel, Coenen, and Warne, 2008; Del Negro, Schorfheide, Smets, and Wouters, 2004; and Smets and Wouters, 2007. This procedure can be heavily biased: the held out data is used to choose the model class under consideration, the distributions of the test set and the training set may be different, and large deviations from the normal course of events [e.g., the recessions in 1980--82] may be ignored.)\n\nIn addition to their utility for model selection, these methods give immediate upper bounds for the worst case prediction error. The results are easy to understand and can be reported to policy makers interested in the quality of the forecasts. We propose to extend proven techniques in statistical learning theory so that they cover the kind of models and data of most interest to macroeconomic forecasting, in particular exploiting the fact that major alternatives can all be put in the form of state-space models.\n\nTo properly frame our proposal, we review first the recent history and practice of macroeconomic forecasting, followed by the essentials of statistical learning theory (in more detail, because we believe it will be less familiar). We then describe the proposed work and its macroeconomic applications.\n\nEconomic Forecasting\n\nThrough the 1970s, macroeconomic forecasting tended to rely on \"reduced-form\" models, predicting the future of aggregated variables based on their observed statistical relationships with other aggregated variables, perhaps with some lags, and with the enforcement of suitable accounting identities. Versions of these models are still in use today, and have only grown more elaborate with the passage of time; those used by the Federal Reserve Board of Governors contain over 300 equations. Contemporary vector autoregression models (VARs) are in much the same spirit.\n\nThe practice of academic macroeconomists, however, switched very rapidly in the late 1970s and early 1980s, in large part driven by the famous \"critique\" of such models by Lucas (published in 1976). He argued that even if these models managed to get the observable associations right, those associations were the aggregated consequences of individual decision making, which reflected, among other things, expectations about variables policy-makers would change in response to conditions. This, Lucas said, precluded using such models to predict what would happen under different policies.\n\nKydland and Prescott (1982) began the use of dynamic stochastic general equilibrium (DSGE) models to evade this critique. The new aim was to model the macroeconomy as the outcome of individuals making forward-looking decisions based on their preferences, their available technology, and their expectations about the future. Consumers and producers make decisions based on \"deep\" behavioral parameters like risk tolerance, the labor-leisure trade-off, and the depreciation rate which are supposedly insensitive to things like government spending or monetary policy. The result is a class of models for macroeconomic time series that relies heavily on theories about supposedly invariant behavioral and institutional mechanisms, rather than observed statistical associations.\n\nDSGE models have themselves been heavily critiqued in the literature for ignoring many fundamental economic and social phenomena --- we find the objections to the representative agent assumption particularly compelling --- but we want to focus our efforts on a more fundamental aspect of these arguments. The original DSGE model of Kydland and Prescott had a highly stylized economy in which the only source of uncertainty was the behavior of productivity or technology, whose log followed an AR(1) process with known-to-the-agent coefficients. Much of the subsequent work in the DSGE tradition has been about expanding these models to include more sources of uncertainty and more plausible behavioral and economic mechanisms. In other words, economists have tried to improve their models by making them more complex.\n\nRemarkably, there is little evidence that the increasing complexity of these models actually improves their ability to predict the economy. (Their performance over the last few years would seem to argue to the contrary.) For that matter, much the same sort of questions arise about VAR models, the leading alternatives to DSGEs. Despite the elaborate back-story about optimization, the form in which a DSGE is confronted with the data is a \"state-space model,\" in which a latent (multivariate) Markov process evolves homogeneously in time, and observations are noisy functions of the state variables. VARs also have this form, as do dynamic factor models, and all the other leading macroeconomic time series models we know of. In every case, the response to perceived inadequacies of the models is to make them more complex.\n\nThe cases for and against different macroeconomic forecasting models are partly about economic theory, but also involve their ability to fit the data. Abstractly, these arguments have the form \"It would be very unlikely that my model could fit the data well if it got the structure of the economy wrong; but my model does fit well; therefore I have good evidence that it is pretty much right.\" Assessing such arguments depends crucially on knowing how well bad models can fit limited amounts of data, which is where we feel we can make a contribution to this research.\n\nStatistical Learning Theory\n\nStatistical learning theory grows out of advances in non-parametric statistical estimation and in machine learning. Its goal is to control the risk or generalization error of predictive models, i.e., their expected inaccuracy on new data from the same source as that used to fit the model. That is, if the model f predicts outcomes Y from inputs X and the loss function is (e.g., mean-squared error or negative log-likelihood), the risk of the model is\n\nThe in-sample or training error, , is the average loss over the actual training points. Because the true risk is an expectation value, we can say that where is a mean-zero noise variable that reflects how far the training sample departs from being perfectly representative of the data-generating distribution. By the laws of large numbers, for each fixed f, as , so, with enough data, we have a good idea of how well any given model will generalize to new data.\n\nHowever, economists, like other scientists, never have a single model with no adjustable parameters fixed for them in advance by theory. (Not even the most enthusiastic calibrators claim as much.) Rather, there is a class of plausible models , one of which in particular is picked out by minimizing the in-sample loss --- by least squares, or maximum likelihood, or maximum a posteriori probability, etc. This means\n\nTuning model parameters so they fit the training data well thus conflates fitting future data well (low R(f), the true risk) with exploiting the accidents and noise of the training data (large negative , finite-sample noise). The true risk of will generally be bigger than its in-sample risk, , precisely because we picked it to match the data well. In doing so, ends up reproducing some of the noise in the data and therefore will not generalize well. The difference between the true and apparent risk depends on the magnitude of the sampling fluctuations: The main goal of statistical learning theory is to mathematically control by finding tight bounds on it while making minimal assumptions about the unknown data-generating process; to provide bounds on over-fitting.\n\nUsing more flexible models (allowing more general functional forms or distributions, adding parameters, etc.) has two contrasting effects. On the one hand, it improves the best possible accuracy, lowering the minimum of the true risk R(f). On the other hand, it also increases the ability to, as it were, memorize noise, raising for any fixed sample size n. This qualitative observation --- a generalization of the bias-variance trade-off from basic estimation theory --- can be made usefully precise by quantifying the complexity of model classes. A typical result is a confidence bound on (and hence on the over-fitting), say that with probability at least ,\n\nwhere the function C() measures how complex the model class is.\n\nSeveral inter-related model complexity measures are now available. The oldest, called \"Vapnik-Chervonenkis dimension,\" effectively counts how many different data sets can fit well by tuning the parameters in the model. Another, \"Rademacher complexity,\" directly measures the ability of to correlate with finite amounts of white noise (Bartlett and Mendelson, 2002; Mohri and Rostamizadeh, 2009). This leads to particularly nice bounds of the form\n\nwhere and are calculable constants. Yet another measure of model complexity is the stability of parameter estimates with respect to perturbations of the data, i.e., how much changes when small changes are made to the training data (Bousquet and Elisseeff, 2002; Mohri and Rostamizdaeh, 2010). (Stable parameter estimates do not require models which are themselves dynamically stable, and the idea could be used on systems which have sensitive dependence on initial conditions.) The different notions of complexity lead to bounds of different forms, and lend themselves more or less easily to calculation for different sorts of models; VC dimension tends to be the most generally applicable, but also the hardest to calculate, and to give the most conservative bounds. Importantly, model complexity, in this sense, is not just the number of adjustable parameters; there are models with a small number of parameters which are basically inestimable because they are so unstable, and conversely, one of the great success stories of statistical learning theory has been devising models (\"support vector machines\") with huge numbers of parameters but low and known capacity to over-fit.\n\nHowever we measure model complexity, once we have done so and have established risk bounds, we can use those bounds for two purposes. One is to give a sound assessment of how well our model will work in the future; this has clear importance if the model's forecasts will be used to guide individual actions or public policy. The other aim, perhaps even more important here, is to select among competing models in a provably reliable way. Comparing in-sample performance tends to pick complex models which over-fit. Adding heuristic penalties based on the number of parameters, like the Akaike information criterion (AIC), also does not solve the problem, basically because AIC corrects for the average size of over-fitting but ignores the variance (and higher moments). But if we could instead use as our penalty, we would select the model which actually will generalize better. If we only have a confidence limit on and use that as our penalty, we select the better model with high confidence and can in many cases calculate the extra risk that comes from model selection (Massart, 2007).\n\nOur Proposal\n\nStatistical learning theory has proven itself in many practical applications, but most of its techniques have been developed in ways which keep us from applying it immediately to macroeconomic forecasting; we propose to rectify this deficiency. We anticipate that each of the three stages will require approximately a year. (More technical details follow below.)\n\nFirst, we need to know the complexity of the model classes to which we wish to apply the theory. We have already obtained complexity bounds for AR(p) models, and are working to extend these results to VAR(p) models. Beyond this, we need to be able to calculate the complexity of general state-space models, where we plan to use the fact that distinct histories of the time series lead to different predictions only to the extent that they lead to different values of the latent state. We will then refine those results to find the complexity of various common DSGE specifications.\n\nSecond, most results in statistical learning theory presume that successive data points are independent of one another. This is mathematically convenient, but clearly unsuitable for time series. Recent work has adapted key results to situations where widely-separated data points are asymptotically independent (\"weakly dependent\" or \"mixing\" time series) [Meir, 2000; Mohri and Rostamizadeh, 2009, 2010; Dedecker et al., 2007]. Basically, knowing the rate at which dependence decays lets one calculate how many effectively-independent observations the time series has and apply bounds with this reduced, effective sample size. We aim to devise model-free estimates of these mixing rates, using ideas from copulas and from information theory. Combining these mixing-rate estimates with our complexity calculations will immediately give risk bounds for DSGEs, but not just for them.\n\nThird, a conceptually simple and computationally attractive alternative to using learning theory to bound over-fitting is to use an appropriate bootstrap for dependent data to estimate generalization error. However, this technique currently has no theoretical basis, merely intuitive plausibility. We will investigate the conditions under which bootstrapping can yield non-asymptotic guarantees about generalization error.\n\nTaken together, these results can provide probabilistic guarantees on a proposed forecasting model's performance. Such guarantees can give policy makers reliable empirical measures which intuitively explain the accuracy of a forecast. They can also be used to pick among competing forecasting methods.\n\nWhy This Grant?\n\nAs we said, there has been very little use of modern learning theory in economics (Al-Najjar, 2009 is an interesting, but entirely theoretical, exception), and none that we can find in macroeconomic forecasting. This is an undertaking which requires both knowledge of economics and of economic data, and skill in learning theory, stochastic processes, and prediction theory for state-space models. We aim to produce results of practical relevance to forecasting, and present them in such a way that econometricians, at least, can grasp their relevance.\n\nIf all we wanted to do was produce yet another DSGE, or even to improve the approximation methods used in DSGE estimation, there would be plenty of funding sources we could turn to, rather than INET. We are not interested in making those sorts of incremental advances (if indeed proposing a new DSGE is an \"advance\"). We are not even particularly interested in DSGEs. Rather, we want to re-orient how economic forecasters think about basic issues like evaluating their accuracy and comparing their models --- topics which should be central to empirical macroeconomics, even if DSGEs vanished entirely tomorrow. Thus INET seems like a much more natural sponsor than institutions with more of a commitment to existing practices and attitudes in economics.\n\n[Detailed justification of our draft budget omitted]\n\nDetailed exposition\n\nIn what follows, we provide a more detailed exposition of the technical content of our proposed work, including preliminary results. This is, unavoidably, rather more mathematical than our description above.\n\nThe initial work described here builds mainly on the work of Mohri and Rostamizadeh, 2009, which offers a handy blueprint for establishing data-dependent risk bounds which will be useful for macroeconomic forecasters. (Whether this bound is really optimal is another question we are investigating.) The bound that they propose has the general form\n\nThe two terms on the right hand side are a model complexity term, , in this case what is called the \"Rademacher complexity\", and a term which depends on the desired confidence level (through ), and the amount of data n used to choose . For many problems, the calculation of the complexity term is well known in the literature. However, this is not the case for state-space models. The final term depends on the mixing behavior of the time series (assumed to be known). In the next sections we highlight some progress we have made toward calculating the model complexity for state-space models and estimating the mixing rate from data. We then apply the bound to a simple example attempting to predict interest rate movements using an AR model. Finally, we discuss the logic behind using the bootstrap to estimate a bound for the risk. All of these results are new and require further research to make them truly useful to economic forecasters.\n\nModel complexity\n\nAs mentioned earlier, statistical learning theory provides several ways of measuring the complexity of a class of predictive models. The results we are using here rely on what is known as the Rademacher complexity, which can be thought of as measuring how well the model can (seem to) fit white noise. More specifically, when we have a class of prediction functions f, the Rademacher complexity of the class is\n\nwhere X is the actual data, and are a sequence of random variables, independent of each other and everything else, and equal to +1 or -1 with equal probability. (These are known in the field as \"Rademacher random variables\". Very similar results exist for other sorts of noise, e.g., Gaussian white noise.) The term inside the supremum, , is the sample covariance between the noise Z and the predictions of a particular model f. The Rademacher complexity takes the largest value of this sample correlation over all models in the class, then averages over realizations of the noise. Omitting the final average over the data X gives the \"empirical Rademacher complexity\", which can be shown to converge very quickly to its expected value as n grows. The final factor of 2 is conventional, to simplify some formulas we will not repeat here.\n\nThe idea, stripped of the technicalities required for actual implementation, is to see how well our models could seem to fit outcomes which were actually just noise. This provides a kind of baseline against which to assess the risk of over-fitting, or failing to generalize. As the sample size n grows, the sample correlation coefficients will approach 0 for each particular f, by the law of large numbers; the over-all Rademacher complexity should also shrink, though more slowly, unless the model class is so flexible that it can fit absolutely anything, in which case one can conclude nothing about how well it will predict in the future from the fact that it performed well in the past.\n\nOne of our goals is to calculate the Rademacher complexity of stationary state-space models. [Details omitted.]\n\nMixing rates\n\nBecause time-series data are not independent, the number of data points n in a sample S is no longer a good characterization of the amount of information available in that sample. Knowing the past allows forecasters to predict future data points to some degree, so actually observing those future data points gives less information about the underlying data generating process than in the case of iid data. For this reason, the sample size term must be adjusted by the amount of dependence in the data to determine the effective sample size which can be much less than the true sample size n. These sorts of arguments can be used to show that a typical data series used for macroeconomic forecasting, detrended growth rates of US GDP from 1947 until 2010, has around n=252 actual data points, but an effective sample size of . To determine the effective sample size to use, we must be able to estimate the dependence of a given time series. The necessary notion of dependence is called the mixing rate.\n\nEstimating the mixing rates of time-series data is a problem that has not been well studied in the literature. According to Ron Meir, \"as far as we are aware, there is no efficient practical approach known at this stage for estimation of mixing parameters\". In this case, we need to be able to estimate a quantity known as the -mixing rate.\n\nDefinition. Let be a stationary sequence of random variables or stochastic process with joint probability law . For , let , the -field generated by the observations between those times. Let be the restriction of to with density , be the restriction of to with density , and the restriction of to with density . Then the -mixing coefficient at lag is\n\n(Here is the total variation distance, i.e., the largest difference between the probabilities that and assign to a single event. Also, to simplify notation, we stated the definition assuming stationarity, but this is not strictly necessary.)\n\nThe stochastic process X is called \" -mixing\" if as , meaning that the joint probability of events which are widely separated in time increasingly approaches the product of the individual probabilities --- that X is asymptotically independent.\n\nThe form of the definition of the -mixing coefficient suggests a straightforward though perhaps naive procedure: use nonparametric density estimation for the two marginal distributions as well as the joint distribution, and then calculate the total variation distance by numerical integration. This would be simple in principle, and could give good results; however, one would need to show not just that the procedure was consistent, but also learn enough about it that the generalization error bound could be properly adjusted to account for the additional uncertainty introduced by using an estimate rather than the true quantity. Initial numerical experiments on the naive are not promising, but we are pursuing a number of more refined ideas.\n\nBootstrap\n\nAn alternative to calculating bounds on forecasting error in the style of statistical learning theory is to use a carefully constructed bootstrap to learn about the generalization error. A fully nonparametric bootstrap for time series data uses the circular bootstrap reviewed in Lahiri, 2003. The idea is to wrap the data of length n around a circle and randomly sample blocks of length q. There are n possible blocks, each starting with one of the data points 1 to n. Politis and White (2004) give a method for choosing q. The following algorithm proposes a bootstrap for bounding the generalization error of a forecasting method.\n\nTake the time series, call it X. Fit a model , and calculate the in-sample risk, .\n\nRepeat for B times:\n\nBootstrap a new series Y from X, which is several times longer than X. Call the initial segment, which is as long as X, .\n\nFit a model to this, , and calculate its in-sample risk, .\n\nCalculate the risk of on the rest of Y. Because the process is stationary and Y is much longer than X, this should be a reasonable estimate of the generalization error of .\n\nStore the difference between the in-sample and generalization risks.\n\nFind the percentile of the distribution of over-fits. Add this to .\n\nWhile intuitively plausible, there is no theory, yet, which says that the results of this bootstrap will actually control the generalization error. Deriving theoretical results for this type of bootstrap is the third component of our grant application.\n\nManual trackback: Economics Job Market Rumors [!]\n\nSelf-Centered; Enigmas of Chance; The Dismal Science\n\nBrad DeLong Makes a Wishful Mistake\n\nAttention conservation notice: Academics squabbling about abstruse points in social theory.\n\nChris Bertram, back from a conference where he heard Michael Tomasello talk about his interesting experiments on (in Bertram's words) \"young children and other primates [supporting the view] that humans are hard-wired with certain pro-social dispositions to inform, help, share etc and to engage in norm-guided behaviour of various kinds\", wonders about the implications of the fact that \"work in empirical psychology and evolutionary anthropolgy (and related fields) doesn't — quelle surprise! — support anything like the Hobbesian picture of human nature that lurks at the foundations of microeconomics, rational choice theory and, indeed, in much contemporary and historical political philosophy.\"\n\nBrad DeLong asserts that the microfoundations of economics point not to a Hobbesian vision of the war of all against all, but rather to Adam Smith's propensities for peaceful cooperation, especially through exchange. \"The foundation of microeconomics is not the Hobbesian 'this is good for me' but rather the Smithian 'this trade is good for us,' and on the uses and abuses of markets built on top of the 'this trade is good for us' principle.\" Bertram objects that this isn't true, and others in DeLong's comments section further object that modern economics simply does not rest on this Smithian vision. DeLong replies: \"Seems to me the normal education of an economist includes an awful lot about ultimatum games and rule of law these days...\"\n\nI have to call this one against DeLong — rather to my surprise, since I usually get more out of his writing than Bertram's. The fact is that the foundations of standard microeconomic models envisage people as hedonistic sociopaths [ETA: see below], and theorists prevent mayhem from breaking out in their models by the simple expedient of ignoring the possibility.\n\nIf you open up any good book on welfare economics or general equilibrium which has appeared since Debreu's Theory of Value (or indeed before), you will see a clear specification of what the economic agents care about: this is entirely a function of their own consumption of goods and services. Does any agent in any such model care at all about what any other agent gets to consume? No; it is a matter of purest indifference to them whether their fellows experience feast or famine; even whether they live or die. If one such agent has an unsatiated demand for potato chips, and the cost of one more chip will be to devastate innumerable millions, they simply are not equipped to care. (And the principle of Pareto optimality shrugs, saying \"who are we to judge?\") Arrow, Debreu and co. rule out by hypothesis any interaction between agents other than impersonal market exchange [ETA: or more exactly, their model does so], but the specification of the agents shows that they'd have no objection to pillage, or any preference for obtaining their consumption basket by peaceful truck, barter and commerce rather than fire, sword and fear.\n\nWell, you might say, welfare economics and general equilibrium concern themselves with what happens once peaceful market systems have been established. Of course they don't need to put a \"pillaging, not really my thing\" term in the utility functions, since it would never come up. Surely things are better in game theory, which has long been seen to be the real microfoundations for economics?\n\nIn a word, no. If you ask why a von Neumann-Morgenstern agent refrains from pillaging, you get the answers that (1) the game is postulated not to have pillaging as an option, or (2) he is restrained by fear of some power stronger than himself, whether that power be an individual or an assembly. (Thus von Neumann: \"It is just as foolish to complain that people are selfish and treacherous as it is to complain that the magnetic field does not increase unless the electric field has a curl.\") Option (1) being obviously irrelevant to explaining why people obey the law, etc., we are left with option (2), which is the essence of all the leading attempts, within economics, to give microfoundations to such phenomena. This is very much in line with the thought of an eminent British moral philosopher — one can read the Folk Theorem as saying that Leviathan could be a distributed system — but that philosopher is not Dr. Smith.\n\nOne can defend the utility of the Hobbesian, game-theoretic vision, and though in my humble (and long-standing) opinion the empirical results on things like the ultimatum game mean that it can be no more than an approximation useful in certain circumstances, and ideas like those of Tomasello (and Smith) need to be taken very seriously. But of course those ideas are not part of the generally-accepted microfoundations of economics. This is why every graduate student in economics reads (something equivalent to) Varian's Microeconomic Analysis, but not Bowles's Microeconomics: Behavior, Institutions, and Evolution; would that they did. If you read Bowles, you will in fact learn a great deal about the ultimatum game, the rule of law, and so forth; in a standard microeconomics text you will not. I think the Hobbesian vision is wrong, but anyone who thinks that modern economics's micro-foundations aren't thoroughly Hobbesian is engaged in wishful thinking.\n\nUpdate, 15 September: A reader observes, correctly, that actual sociopaths show much more other-regarding preferences than does Homo economicus (typically, forms of cruelty). I could quibble and gesture to dissocial personality disorder, but point taken.\n\nUpdate, 24 December: In the comments at DeLong's, Robert Waldmann rightly chides me for conflating the actual social views of Arrow and Debreu with what they put into their model of general equilibrium. I have updated the text accordingly.\n\nManual trackback: Stephen Kinsella; Marginal Utility; Marc Kaufmann; Brad DeLong; Contingencies\n\n*: Varian wrote a book, with Carl Shapiro, giving advice to businesses in industries with imperfect competition. The advice is to (1) extract as much as possible from the customer, to the point where they just barely prefer doing business with you to switching to a competitor or taking their marbles and going home, (2) disguise how much you will extract from your customers as much as possible, (3) participate in standards-setting and public policy formation, so as to ensure that the standards and policies will be to your commercial advantage as much as possible, and (4) generally engage in as much anti-competitive behavior as possible without risk of legal consequences. All this may in fact be sound advice for increasing the (more or less short-run) profits of such firms, but the premises are purely Hobbesian. Were there no risk of legal consequences, their arguments would extend straightforwardly to pillaging. The only reason Shapiro and Varian would counsel Apple against, say, running a phishing scam on everyone who bought a Macintosh would be that it was very likely they'd be caught, with adverse consequences; obviously if Apple made enough money from such a scam, Shaprio and Varian's arguments would say not only \"go phish\", but \"lobby to make such phishing legal\" (perhaps under the principle of caveat emptor).\n\nThe Dismal Science; Philosophy; The Natural Science of the Human Species\n\nOn an Example of Vienneau's\n\nAttention conservation notice: 1600+ dry, pedantic words and multiple equations on how some heterodox economists mis-understand ergodic theory.\n\nRobert Vienneau, at Thoughts on Economics, has posted an example of a stationary but non-ergodic stochastic process. This serves as a reasonable prompt to follow up on my comment, a propos of Yves Smith's book, that the post-Keynesian school of economists seems to be laboring under a number of confusions about \"ergodicity\".\n\nI hasten to add that there is nothing wrong with Vienneau's example: it is indeed a stationary but non-ergodic process. (In what follows, I have lightly tweaked his notation to suit my own tastes.) Time is indexed in discrete steps, and Xt = YZt, where Z is a sequence of independent, mean-zero, variance 1 Gaussian random variables (i.e., standard discrete-time white noise), and Y is a chi-distributed random variable (i.e., the square root of something which has a chi-squared distribution). Z is transparently a stationary process, and Y is constant over time, so X must also be a stationary process. However, by simulation Vienneau shows that the empirical cumulative distribution functions from different realizations of the process do not converge on a common limit.\n\nIn fact, the result can be strengthened considerably. Given Y = y, X is just Gaussian white noise with standard deviation y, so by the Glivenko-Cantelli theorem, the empirical CDF of X converges almost surely on the CDF of that Gaussian. The marginal distribution of Xt for each t is however a mixture of Gaussians of different standard deviations, and not a Gaussian. Conditionally on Y, therefore, the empirical CDF converges to the marginal distribution of the stationary process with probability 0. Since this convergence has conditional probability zero for every value of y, it has probability zero unconditionally as well. So Vienneau's process very definitely fails to be ergodic.\n\n(Proof of the unconditionality claim: Let C be the indicator variable for the empirical CDF converging to the marginal distribution.\n\nfor all y, but\n\nby the law of total expectation.)\n\nTwo things, however, are worth noticing. First, Vienneau's X process is a mixture of ergodic processes; second, which mixture component is sampled from is set once, at the beginning, and thereafter each sample path looks like a perfectly well-behaved realization of an ergodic process. These observations generalize. The ergodic decomposition theorem (versions of which go back as far as von Neumann's original work on ergodic theory) states that every stationary process is a mixture of processes which are both stationary and ergodic. Moreover, which ergodic component a sample path is in is an invariant of the motion — there is no mixing of ergodic processes within a realization. It's worth taking a moment, perhaps, to hand-wave about this.\n\nStart with the actual definition of ergodic processes. Ergodicity is a property of the probability distribution for whole infinite sequences X = (X1, X2, ... Xt, ... ). As time advances, the dynamics chop off the initial parts of this sequence of random variables. Some sets of sequences are invariant under such \"shifts\" — constant sequences, for instance, but also many other more complicated sets. A stochastic process is ergodic when all invariant sets either have probability zero or probability one. What this means is that (almost) all trajectories generated by an ergodic process belong to a single invariant set, and they all wander from every part of that set to every other part — they are \"metrically transitive\". (Because: no smaller set with any probability is invariant.) From this follows Birkhoff's individual ergodic theorem, which is the basic strong law of large numbers for dependent data. If X is an ergodic process, then for any (integrable) function f, the average of f(Xt) along a sample path, the \"time average\" of f, converges to a unique value almost surely. So with probability 1, time averages converge to values characteristic of the ergodic process.\n\nNow go beyond a single ergodic probability distribution. Two distributions are called \"mutually singular\" if one of them gives probability 1 to an event which has probability zero according to the other, and vice versa. Any two ergodic processes are either identical or mutually singular. To see this, realize that two distributions must give different expectation values to at least one function; otherwise they're the same distribution. Pick such a distinguishing function and call it f, with expectation values f1 and f2 under the two distributions. Well, the set of sample paths where\n\nhas probability 1 under the first measure, and probability 0 under the second. Likewise, under the second measure the time average is almost certain to converge on f2, which almost never happens under the first measure. So any two ergodic measures are mutually singular.\n\nThis means that a mixture of two (or more) ergodic processes cannot, itself, be ergodic. But a mixture of stationary processes is stationary. So the stationary ergodic processes are \"extremal points\" in the set of all stationary processes. The convex hull of these extremal points are the set of stationary but non-ergodic processes which can be obtained by mixing stationary and ergodic processes. It is less trivial to show that every stationary process belongs to this family, that it is a mixture of stationary and ergodic processes, but this can indeed be done. (See, for instance, this beautiful paper by Dynkin.) Part of the proof shows that which ergodic component a stationary process's sample path is in does not change over time — ergodic components are themselves invariant sets of trajectories. The general form of Birkhoff's theorem thus has time averages converging to a random limit, which depends on the ergodic component the process started in. This can be shown even at the advanced undergraduate level, as in Grimmett and Stirzaker.\n\nAt this point, three notes seem in order.\n\nMany statisticians will be more familiar with a special case of the ergodic decomposition, which is de Finetti's result about how infinite exchangeable random sequences are mixtures of independent and identically-distributed random sequences. The ergodic decomposition is like that, only much cooler, and not tainted by the name of a Fascist. (That said, de Finetti's theorem actually covers Vienneau's example.)\n\nFollowing tradition, I have stated the ergodic decomposition above for stationary processes. However, it is very important that this limitation is not essential. The broadest class of processes I know of for which an ergodic decomposition holds are the \"asymptotically mean-stationary processes\". The defining property of such processes is that their probability laws converge in Cesaro mean. In symbols, and writing Pt for the law of the process from t onwards, we must have\n\nfor some limiting law P. (I learned to appreciate the importance of AMS processes from Robert Gray's Probability, Random Processes and Ergodic Properties, and stole those ideas shamelessly for Almost None.) This allows for cyclic variation in the process, for asymptotic approach to a stationary distribution, for asymptotic approach to a cyclically varying process, etc. Every AMS process is a mixture of ergodic AMS processes, in exactly the way that every stationary process is a mixture of ergodic stationary processes.\n\nI actually don't know whether the ergodic decomposition can extend beyond this, but I suspect not, since the defining condition for AMS is very close to a Cesaro-mean decay-of-dependence property which turns out to be equivalent to ergodicity, namely that, for any two sets A and B\n\nwhere T-t are the powers of the back-shift operator (what time series econometricians usually write L), so that T-tB are all the trajectories which will be in the set B in t time-steps. (See Lemma 6.7.4 in the first, online, edition, of Gray, p. 148). This means that, on average, the far future becomes unpredictable from the present.\n\nIn light of the previous note, if dynamical systems people want to read \"basin of attraction\" for \"ergodic component\", and \"natural invariant measure on the attractor\" for \"limit measure of an AMS ergodic process\", they will not go far wrong.\n\nAs the last remark suggests, it is entirely possible for a process to be stationary and ergodic but to have sensitive dependence on initial conditions; this is generally the case for chaotic processes, which is why there are classic articles with titles like \"The Ergodic Theory of Chaos and Strange Attractors\". Chaotic systems rapidly amplify small perturbations, at least along certain directions, so they are subject to positive destabilizing feedbacks, but they have stable long-run statistical properties.\n\nGoing further, consider the sort of self-reinforcing urn processes which Brian Arthur and collaborators made famous as models of lock-in and path dependence. (Actually, in the classification of my old boss Scott Page, these models are merely state-dependent, and do not rise to the level of path dependence, or even of phat dependence, but that's another story.) These are non-stationary, but it is easily checked that, so long as the asymptotic response function has only a finite number of stable fixed points, they satisfy the definition of asymptotic mean stationarity given above. (I leave it as an exercise whether this remains true in a case like the original Polya urn model.) Hence they are mixtures of ergodic processes. Moreover, if we have only a single realization — a unique historical trajectory — then we have something which looks just like a sample path of an ergodic process, because it is one. (\"[L]imiting sample averages will behave as if they were in fact produced by a stationary and ergodic system\" — Gray, p. 235 of 2nd edition.) That this was just one component of a larger, non-ergodic model limits our ability to extrapolate to other components, unless we make strong modeling assumptions about how the components relate to each other, but so what?\n\nI make a fuss about this because the post-Keynesians seem to have fallen into a number of definite errors here. (One may see these errors in e.g., Crotty's \"Are Keynesian Uncertainty and Macrotheory Compatible?\" [PDF], which however also has insightful things to say about conventions and institutions as devices for managing uncertainty.) It is not true that non-stationarity is a sufficient condition for non-ergodicity; nor is it a necessary one. It is not true that \"positive destabilizing feedback\" implies non-ergodicity. It is not true that ergodicity is incompatible with sensitive dependence on initial conditions. It is not true that ergodicity rules out path-dependence, at least not the canonical form of it exhibited by Arthur's models.\n\nUpdate, 12 September: Fixed the embarrassing mis-spelling of Robert's family name in my title.\n\nManual trackback: Robert Vienneau; Beyond Microfoundations\n\nEnigmas of Chance; The Dismal Science\n\nThe World's Simplest Ergodic Theorem\n\nAttention conservation notice: Equation-filled attempt at a teaching note on some theorems in mathematical probability and their statistical application. (Plus an oblique swipe at macroeconomists.)\n\nThe \"law of large numbers\" says that averages of measurements calculated over increasingly large random samples converge on the averages calculated over the whole probability distribution; since that's a vague statement, there are actually several laws of large numbers, from the various ways of making this precise. As traditionally stated, they assume that the measurements are all independent of each other. Successive observations from a dynamical system or stochastic process are generally dependent on each other, so the laws of large numbers don't, strictly, apply, but they have analogs, called \"ergodic theorems\". (Blame Boltzmann.) Laws of large numbers and ergodic theorems are the foundations of statistics; they say that sufficiently large samples are representative of the underlying process, and so let us generalize from training data to future or currently-unobserved occurrences.\n\nHere is the simplest route I know to such a theorem; I can't remember if I learned it from Prof. A. V. Chubukov's statistical mechanics class, or from Uriel Frisch's marvellous Turbulence. Start with a sequence of random variables X1, X2, ... Xn. Assume that they all have the same (finite) mean m and the same (finite) variance v; also assume that the covariance, E[XtXt+h] - E[Xt] E[Xt+h], depends only on the difference in times h and not on the starting time t. (These assumptions together comprise \"second-order\" or \"weak\" or \"wide-sense\" stationarity. Stationarity is not actually needed for ergodic theorems, one can get away with what's called \"asymptotic mean stationarity\", but stationarity simplifies the presentation here.) Call this covariance ch. We contemplate the arithmetic mean of the first n values in X, called the \"time average\":\n\nWhat is the expectation value of the time average? Taking expectations is a linear operator, so\n\nwhich is re-assuring: the expectation of the time average is the common expectation. What we need for an ergodic theorem is to show that as n grows, An tends, in some sense, to get closer and closer to its expectation value.\n\nThe most obvious sense we could try is for the variance of An to shrink as n grows. Let's work out that variance, remembering that for any random variable Y, Var[Y] = E[Y2] - (E[Y])2.\n\nThis used the linearity of expectations, and the definition of the covariances ch. Imagine that we write out all the covariances in an n*n matrix, and average them together; that's the variance of An. The entries on the diagonal of the matrix are all c0 = v, and the off-diagonal entries are symmetric, because (check this!) c-h = ch. So the sum over the whole matrix is the sum on the diagonal, plus twice the sum of what's above the diagonal.\n\nIf the Xt were uncorrelated, we'd have ch = 0 for all h > 0, so the variance of the time average would be O(n-1). Since independent random variables are necessarily uncorrelated (but not vice versa), we have just recovered a form of the law of large numbers for independent data. How can we make the remaining part, the sum over the upper triangle of the covariance matrix, go to zero as well?\n\nWe need to recognize that it won't automatically do so. The assumptions we've made so far are compatible with a process where X1 is chosen randomly, and then all subsequent observations are copies of it, so that then the variance of the time average is v, no matter how long the time series; this is the famous problem of checking a newspaper story by reading another copy of the same paper. (More formally, in this situation ch = v for all h, and you can check that plugging this in to the equations above gives v for variance of An for all n.) So if we want an ergodic theorem, we will have to impose some assumption on the covariances, one weaker than \"they are all zero\" but strong enough to exclude the sequence of identical copies.\n\nUsing two inequalities to put upper bounds on the variance of the time average suggests a natural and useful assumption which will give us our ergodic theorem.\n\nCovariances can be negative, so we upper-bound the sum of the actual covariances by the sum of their magnitudes. (There is no approximation here if all covariances are positive.) Then we extend the inner sum so it covers all lags. This might of course be infinite, and would be for the sequence-of-identical-copies. Our assumption then is This is a sum of covariances over time, so let's write it in a way which reflects those units: , where T is called the \"(auto)covariance time\", \"integrated (auto)covariance time\" or \"(auto)correlation time\". We are assuming a finite correlation time. (Exercise: Suppose that , as would be the case for a first-order linear autoregressive model, and find T. This confirms, by the way, that the assumption of finite correlation time can be satisfied by processes with non-zero correlations.)\n\nReturning to the variance of the time average,\n\nSo, if we can assume the correlation time is finite, the variance of the time averages goes is O(n-1), just as if the data were independent. However, the convergence is slower than for independent data by an over-all factor which depends only on T. As T shrinks to zero, we recover the result for uncorrelated data, an indication that our approximations were not too crude.\n\nFrom knowing the variance, we can get rather tight bounds on the probability of An's deviations from m if we assume that the fluctuations are Gaussian. Unfortunately, none of our assumptions so far entitle us to assume that. For independent data, we get Gaussian fluctuations of averages via the central limit theorem, and these results, too, can be extended to dependent data. But the assumptions needed for dependent central limit theorems are much stronger than merely a finite correlation time. What needs to happen, roughly speaking, is that if I take (nearly) arbitrary functions f and g, the correlation between f(Xt) and g(Xt+h) must go to zero as h grows. (This idea is quantified as \"mixing\" or \"weak dependence\".)\n\nHowever, even without the Gaussian assumption, we can put some bounds on deviation probabilities by bounding the variance (as we have) and using Chebyshev's inequality:\n\nwhich goes to zero as n grows. So we have just proved convergence \"in mean square\" and \"in probability\" of time averages on their stationary expectation values, i.e., the mean square and weak ergodic theorems, under the assumptions that the data are weakly stationary and the correlation time is finite. There were a couple of steps in our argument where we used not very tight inequalities, and it turns out we can weaken the assumption of finite correlation time. The necessary and sufficient condition for the mean-square ergodic theorem turns out to be that, as one might hope, though I don't know of any way of proving it rigorously without using Fourier analysis (which is linked to the autocovariance via the Wiener-Khinchin theorem; see chapters 19 and 21 of Almost None of the Theory of Stochastic Processes).\n\nReverting to the case of finite correlation time T, observe that we have the same variance from n dependent samples as we would from n/(1+2T) independent ones. One way to think of this is that the dependence shrinks the effective sample size by a factor of 2T+1. Another, which is related to the name \"correlation time\", is to imagine dividing the time series up into blocks of that length, i.e., a central point and its T neighbors in either direction, and use only the central points in our averages. Those are, in a sense, effectively uncorrelated. Non-trivial correlations extend about T time-steps in either direction. Knowing T can be very important in figuring out how much actual information is contained in your data set.\n\nTo give an illustration not entirely at random, quantitative macroeconomic modeling is usually based on official statistics, like GDP, which come out quarterly. For the US, which is the main but not exclusive focus of these efforts, the data effectively start in 1947, as what national income accounts exist before then are generally thought too noisy to use. Taking the GDP growth rate series from 1947 to the beginning of 2010, 252 quarters in all, de-trending, I calculate a correlation time of just over ten quarters. (This granting the economists their usual, but absurd, assumption that economic fluctuations are stationary.) So macroeconomic modelers effectively have 11 or 12 independent data points to argue over.\n\nConstructively, this idea leads to the mathematical trick of \"blocking\". To extend a result about independent random sequences to dependent ones, divide the dependent sequence up into contiguous blocks, but with gaps between them, long enough that the blocks are nearly independent of each other. One then has the IID result for the blocks, plus a correction which depends on how much residual dependence remains despite the filler. Picking an appropriate combination of block length and spacing between blocks keeps the correction small, or at least controllable. This idea is used extensively in ergodic theory (including the simplest possible proof of the strong ergodic theorem) and information theory (see Almost None again), in proving convergence results for weakly dependent processes, in bootstrapping time series, and in statistical learning theory under dependence.\n\nManual trackback: An Ergodic Walk (fittingly enough); Thoughts on Economics\n\nUpdate, 7 August: Fixed typos in equations.\n\nEnigmas of Chance\n\nIn which Dunning-Krueger meets Slutsky-Yule, and they make music together\n\nAttention conservation notice: Over 2500 words on how a psychologist who claimed to revolutionize aesthetics and art history would have failed undergrad statistics. With graphs, equations, heavy sarcasm, and long quotations from works of intellectual history. Are there no poems you could be reading, no music you could be listening to?\n\nI feel I should elaborate my dismissal of Martindale's The Clockwork Muse beyond a mere contemptuous snarl.\n\nThe core of Martindale's theory is this. Artists, and still more consumers of art, demand novelty; they don't just want the same old thing. (They have the same old thing.) Yet there is also a demand, or a requirement, to stay within the bounds of a style. Combining this with a notion that coming up with novel ideas and images requires \"regressing\" to \"primordial\" modes of thought, he concludes\n\nEach artist or poet must regress further in search of usable combinations of ideas or images not already used by his or her predecessors. We should expect the increasing remoteness or strangeness of similes, metaphors, images, and so on to be accompanied by content reflecting the increasingly deeper regression toward primordial cognition required to produce them. Across the time a given style is in effect, we should expect works of art to have content that becomes increasingly more and more dreamlike, unrealistic, and bizarre.\n\nEventually, a turning point to this movement toward primordial thought during inspiration will be reached. At that time, increases in novelty would be more profitably attained by decreasing elaboration — by loosening the stylistic rules that govern the production of art works — than by attempts at deeper regression. This turning point corresponds to a major stylistic change. ... Thus, amount of primordial content should decline when stylistic change occurs. [pp. 61--64, his emphasis; the big gap corresponds to some pages of illustrations, and not me leaving out a lot of qualifying text]\n\nReference to actual work in cognitive science on creativity, both theoretical and experimental (see, e.g., Boden's review contemporary with Martindale's work), is conspicuously absent. But who knows, maybe his uncritical acceptance of these sub-Freudian notions has lead in some productive direction; let us judge them by their fruits.\n\nHere is Martindale's Figure 9.1 (p. 288), supposedly showing the amount of \"primordial content\" in Beethoven's musical compositions from 1795 through 1826, or rather a two-year moving average of this.\n\nLet us leave to one side the very difficult questions of how to measure \"primordial content\"; Martindale, like too many psychologists, is slave to quite confused ideas about \"construct validity\". The dots are the moving averages, the solid black line is a guide to the eye, and the dashed line is a parabola fit to the moving averages. In the main text, Martindale combines the parabolic trend with a second order autoregression, getting the fitted model (p. 289) PCt = -1.59 + 0.23t - 0.01 t2 + 0.58 PCt-1 - 0.55 PCt-2 which, he says, has an R2 of 50%. Primordial content is supposed to go up as an artist (or artistic community) \"works out the possibilities of a style\", but go down with a switch to a new, fresh style. Martindale tries (p. 289) to match up his peaks and troughs with what the critics say about the development of Beethoven's style, and succeeds to his own satisfaction, at least \"in broad outline\".\n\nNow, here is the figure which was, so help me, the second run of some R code I wrote.\n\nHere, however, instead of having people try to figure out how much primordial content there was in Beethoven's music, I simply took Gaussian white noise, with mean zero and variance 1, with one random number per year, and treated that exactly the same way that Martindale did: two-year moving averages, a quadratic fit over time (displayed), and a quadratic-plus-AR(2) over-all model, which kept 45% of the variance. My final fitted model was PCt = -0.61 + 0.15t - 0.004 t2 + 0.63 PCt-1 - 0.51 PCt-2 Was this a fluke? No. When I repeat this 1000 times, the median R2 is 43%, and 28% of the runs have an R2 greater than what Martindale got. His fit is no better than one would expect if his measurements are pure noise.\n\nWhat is going on here? All of the apparent structure revealed in Martindale's analysis is actually coming from his having smoothed his data, from having taken the two-year moving average. Remarkably enough, he realized that this could lead to artifacts, but brushed the concern aside:\n\nOne has to be careful in dealing with smoothed data. The smoothing by its very nature introduces some autocorrelation because the score for one year is in part composed of the score for the prior year. However, autocorrelations introduced by smoothing are positive and decline regularly with increase lags. That is not at all what we find in the case of Beethoven — or in other cases where I have used smoothed data. The smoothing is not creating correlations where non existed; it is magnifying patterns already in the data. [p. 289]\n\nWhat this passage reveals is that Martindale did not understand the difference between the autocorrelation function of a time series, and the coefficients of an autoregressive model fit to that time series. (Indeed I suspect he did not understand the difference between correlation and regression coefficients in general.) The autoregressive coefficients correspond, much more nearly, to the partial autocorrelation function, and the partial autocorrelations which result from applying a moving average to white noise have alternating signs — just like Martindale's do. In fact, the coefficients he got are entirely typical of what happens when his procedure is applied to white noise:\n\nSmall dots: Autoregressive coefficients from 1000 runs of Martindale's analysis applied to white noise. Large X: his estimated coefficients for Beethoven.\n\nI could go on about what has gone wrong in just the four pages Martindale devotes to Beethoven's style, but I hope my point is made. I won't say that he makes every conceivable mistake in his analysis, because my experience as a teacher of statistics is that there are always more possible errors than you would ever have suspected. But I will say that the errors he's making — creating correlations by averaging, confusing regression and correlation coefficients, etc. — are the sort of things which get covered in the first few lessons of a good course on time series. The fact that averaging white noise produces serial correlations, and a particular pattern of autoregressive coefficients, is in particular famous as the Yule-Slutsky effect, after its two early-20th-century discoverers. (Slutsky, interestingly, appears to have thought of this as an actual explanation for many apparent cycles, particularly of macroeconomic fluctuations under capitalism, though how he proposed to reconcile this with Marx I don't know.) I am not exaggerating for polemical effect when I say that I would fail Martindale from any class I taught on data analysis; or that every single one of the undergraduate students who took 490 this spring has demonstrated more skill at applied statistics than he does in this book.\n\nMartindale's book has about 200 citations in Google Scholar. (I haven't tried to sort out duplicates, citation variants, and self-citations.) Most of these do not appear to be \"please don't confuse us with that rubbish\" citations. Some of them are from intelligent scholars, like Bill Benzon, who, through no fault of their own, are unable to evaluate Martindale's statistics, and so take his competence on trust. (Similarly with Dutton, who I would not describe as an \"intelligent scholar\".) This trust has probably been amplified by Martindale's rhetorical projection of confidence in his statistical prowess. (Look at that quote above.) — Oh, let's not mince words here: Martindale fashions himself as someone bringing the gospel of quantitative science to the innumerate heathen of the humanities, complete with the expectation that they'll be too stupid to appreciate the gift. For many readers, those who project such intellectual arrogance are not just more intimidating but also more credible, though rationally, of course, they shouldn't be. (If you want to suggest that I exploit this myself, well, you'd have a point.)\n\nCould there be something to the idea of an intrinsic style cycle, of the sort Martindale (like many others) advocates? I actually wouldn't be surprised if there were situations when some such mechanism (shorn of the unbearably silly psychoanalytic bits) applies. In fact, the idea of this mechanism is much older than Martindale. For example, here is a passage from Marshall G. S. Hodgson's The Venture of Islam, which I happen to have been re-reading recently:\n\nAfter the death of [the critic] Ibn-Qutaybah [in 889], however, a certain systematizing of critical standards set in, especially among his disciples, the \"school of Baghdad\". ... Finally the doctrine of the pre-eminence of the older classics prevailed. So far as concerned poetry in the standard Mudâi Arabic, which was after all, not spoken, puristic literary standards were perhaps inevitable: an artificial medium called for artificial norms. That critics should impose some limits was necessary, given the definition of shi`r poetry in terms of imposed limitations. With the divorce between the spoken language of passion and the formal language of composition, they had a good opportunity to exalt a congenially narrow interpretation of those limits. Among adîbs who so often put poetry to purposes of decoration or even display, the critics' word was law. Generations of poets afterwards strove to reproduce the desert qasîdah ode in their more serious work so as to win the critics' acclaim.\n\nSome poets were able to respond with considerable skill to the critics' demands. Abû-Tammâm (d. c. 845) both collected and edited the older poetry and also produced imitations himself of great merit. But work such as his, however admirable, could not be duplicated indefinitely. In any case, it could appear insipid. A living tradition could not simply mark time; it had to explore whatever openings there might be for working through all possible variations on its themes, even the grotesque. Hence in the course of subsequent generations, taste came to favor an ever more elaborate style both in verse and in prose. Within the forms which had been accepted, the only recourse for novelty (which was always demanded) was in the direction of more far-fetched similes, more obscure references to educated erudition, more subtle connections of fancy.\n\nThe peak of such a tendency was reached in the proud poet al-Mutanabbi', \"the would-be prophet\" (915--965 — nicknamed so for a youthful episode of religious propagandizing, in which his enemies said he claimed to be a prophet among the Bedouin), who travelled whenever he did not meet, where he was, with sufficient honor for his taste. He himself consciously exemplified, it is said, something of the independent spirit of the ancient poets. Though he lived by writing panegyrics, he long preferred, to Baghdad, the semi-Bedouin court of the Hamdânid Sayf-al-dawlah at Aleppo; and on his travels he died rather than belie his valiant verses, when Bedouin attacked the caravan and he defended himself rather than escape. His verse has been ranked as the best in Arabic on the ground that his play of words showed the widest range of ingenuity, his images held the tension between fantasy and actuality at the tautest possible without falling into absurdity.\n\nAfter him, indeed, his heirs, bound to push yet further on the path, were often trapped in artificial straining for effect; and sometimes they appear simply absurd. In any case, poetry in literary Arabic after the High Caliphal Period soon became undistinguished. Poets strove to meet the critics' norms, but one of the critics' demands was naturally for novelty within the proper forms. But such novelty could be had only on the basis of over-elaboration. This the critics, disciplined by the high, simple standards of the old poetry, properly rejected too. Within the received style of shi`r, good further work was almost ruled out by the effectively high standards of the `Abbâsî critics. [volume I, pp. 463--464, omitting some diacritical marks which I don't know how to make in HTML]\n\nNow, it does not matter here what the formal requirements of such poetry were, still less those of the qasidah; nor is it relevant whether Hodgson's aesthetic judgments were correct. I quote this because he points to the very same mechanism — demand for novelty plus restrictions of a style leading to certain kinds of elaboration and content — decades before Martindale (Hodgson died, with this part of his book complete, in 1968), and with no pretense that he was making an original argument, as opposed to rehearsing a familiar one.\n\nBut there are obvious problems with turning this mechanism into the Universal Scientific Law of Artistic Change, as Martindale wants to do. Or rather problems which should be obvious, many of which were well put by Joseph (Abu Thomas) Levenson in Confucian China and Its Modern Fate:\n\nHistorians of the arts have sometimes led their subjects out of the world of men into a world of their own, where the principles of change seem interior to the art rather than governed by decisions of the artist. Thus, we have been assured that seventeenth-century Dutch landscape bears no resemblance to Breughel because by the seventeenth century Breughel's tradition of mannerist landscape had been exhausted. Or we are treated to tautologies, according to wich art is \"doomed to become moribund\" when it \"reaches the limit of its idiom\", and in \"yielding its final flowers\" shows that \"nothing more can be done with it\" — hece the passing of the grand manner of the eighteenth entury in Europe and the romantic movement of the nineteenth.\n\nHow do aesthetic valuies really come to be superseded? This sort of thing, purporting to be a revelation of cause, an answer to a question, leaves the question still to be asked. For Chinese painting, well before the middle of the Ch'ing period, with its enshrinement of eclectic virtuosi and connoisseurs, had, by any \"internal\" criteria, reached the limit of its idiom and yielded its final flowers. And yet the values of the past persisted for generations, and the fear of imitation, the feeling that creativity demanded freshness in the artist's purposes, remained unfamiliar to Chinese minds. Wang Hui was happy to write on a landscape he painted in 1692 that it was a copy of a copy of a Sung original; while his colleague, Yün Shou-p'ing, the flower-painter, was described approvingly by a Chi'ing compiler as having gone back to the \"boneless\" painting of Hsü Ch'ung-ssu, of the eleventh century, and made his work one with it. (Yün had often, in fact, inscribed \"Hsü Ch'ung-ssu boneless flower picture\" on his own productions.) And Tsou I-kuei, another flower-painter, committed to finding a traditional sanction for his art, began a treatise with the following apologia:\n\nWhen the ancients discussed painting they treated landscape in detail but slighted flowering plants. This does not imply a comparison of their merits. Flower painting flourished in the northern Sung, but Hsü [Hsi] and Huang [Ch'üan] could not express themselves theoretically, and therefore their methods were not transmitted.\n\nThe lesson taught by this Chinese experience is that an art-form is \"exhausted\"when its practitioners think it is. And a circular explanation will not hold — they think so not when some hypothetically objective exhaustion occurs in the art itself, but when outer circumstances, beyond the realm of purely aesthetic content, has changed their subjective criteria; otherwise, how account for the varying lengths of time it takes for different publics to leave behind their worked-out forms? [pp. 40–41]\n\nMartindale seems to be completely innocent of such considerations. What he brings to this long-running discussion is, supposedly, quantitative evidence, and skill in its analysis. But this is precisely what he lacks. I have only gone over one of his analyses here, but I claim that the level of incompetence displayed here is actually entirely typical of the rest of the book.\n\nManual trackback: Evolving Thoughts; bottlerocketscience\n\nMinds, Brains, and Neurons; Writing for Antiquity; The Commonwealth of Letters; Learned Folly; Enigmas of Chance\n\nConfounded Divorce (\"Why Oh Why Can't We Have a Better Press Corps?\" Dept.)\n\nAttention conservation notice: 1000+ words about how I am irritated by journalists being foolish, and about attempts at causal inference on social networks. As novel as a cat meowing or a car salesman scamming.\n\nI have long thought that most opinion writers could be replaced, to the advantage of all concerned, by stochastic context-free grammars. Their readers would be no less well-informed about how the world is and what should be done about it, would receive no less surprise and delight at the play of words and ideas, and the erstwhile writers would be free to pursue some other trade, which did not so corrode their souls. One reason I feel this way is that these writers habitually make stuff up because it sounds good to them, even when actual knowledge is attainable. They have, as a rule, no intellectual conscience. Yesterday, therefore, if you had told me that one of their number actually sought out some social science research, I would have applauded this as a modest step towards a better press corps.\n\nToday, alas, I am reminded that looking at research is not helpful, unless you have the skills and skepticism to evaluate the research. Exhibit A is Ross \"Chunky Reese Witherspoon Lookalike\" Douthat, who stumbled upon this paper from McDermott, Christakis, and Fowler, documenting an association between people getting divorced and those close to them in the social network also getting divorced. Douthat spun this into the claim that \"If your friends or neighbors or relatives get divorced, you're more likely to get divorced --- even if it's only on the margins --- no matter what kind of shape your marriage is in.\" It should come as no surprise that McDermott et al. did not, in any way whatsoever, try to measure what shape peoples' marriages were in.\n\nEzra Klein, responding to Douthat, suggests that the causal channel isn't making people who are happy in their marriages divorce, but leading people to re-evaluate whether they are really happily married, by making it clear that there is an alternative to staying married. \"The prevalence of divorce doesn't change the shape your marriage is in. It changes your willingness to face up to the shape your marriage is in.\" (In other words, Klein is suggesting that many people call their marriages \"happy\" only through the mechanism of adaptive preferences, a.k.a. sour grapes.) Klein has, deservedly, a reputation for being more clueful than his peers, and his response shows a modicum of critical thought, but he is still relying on Ross Douthat to do causal inference, which is a sobering thought.\n\nBoth of these gentlemen are assuming that this association between network neighbors' divorces must be due to some kind of contagion — Douthat is going for some sort of imitation of divorce as such, Klein is looking to more of a social learning process about alternatives and their costs. Both of them ignore the possibility that there is no contagion here at all. Remember homophily: People tend to be friends with those who are like them. I can predict your divorce from your friends' divorces, because seeing them divorce tells me what kind of people they are, which tells me about what kind of person you are. From the sort of observational data used in this study, it is definitely impossible to say how much of the association is due to homophily and how much to contagion. (The edge-reversal test they employ does not work.) It seems to be impossible to even say whether there is any contagion at all.*\n\nTo be clear, I am not castigating columnists for not reading my pre-prints; on balance I'm probably happier that they don't. But the logical issue of running together influence from friends and inference from the kind of friends you have is clear and well known. (Our contribution was to show that you can't escape the logic through technical trickery.) One would hope it would have occurred to people to ponder it before calling for over-turning family law, or saying, in effect, \"You should stay together, for the sake of your neighbors' kids\". I also have no problem with McDermott et al. investigating this. It's a shame that their data is unable to answer the causal questions, but without their hard work in analyzing that data we wouldn't know there was a phenomenon to be explained.\n\nI hope it's obvious that I don't object to people pontificating about whatever they like; certainly I do enough of it. If people can get paying jobs doing it, more power to them. I can even make out a case why ideologically committed opinionators have a role to play in the social life of the mind, like so. It's a big complicated world full of lots of things which might, conceivably, matter, and it's hard to keep track of them all, and figure out how one's principles apply** — it takes time and effort, and those are always in short supply. Communicating ideas takes more time and effort and skill. People who can supply the time, effort and skill to the rest of us, starting from more or less similar principles, thereby do us a service. But only if they are actually trustworthy — actually reasoning and writing in good faith — and know what they are talking about.\n\n(Thanks, of a kind, to Steve Laniel for bringing this to my attention.)\n\n*: Arbitrarily strong predictive associations of the kind reported here can be produced by either mechanism alone, in the absence of the other. We are still working on whether there are any patterns of associations which could not be produced by homophily alone, or contagion alone. So far the answer seems to be \"no\", which is disappointing.\n\n**: And sometimes you reach conclusions so strange or even repugnant that the principles they followed from come into doubt themselves. And sometimes what had seemed to be a principle proves, on reflection, to be more like a general rule, adapted to particular circumstances. And sometimes one can't articulate principles at all. All of this, too, could and should be part of our public conversation; but let me speak briefly in the main text.\n\n(Typos corrected, 26 June)\n\nManual trackback: The Monkey Cage.\n\nNetworks; The Running Dogs of Reaction\n\nReturn of \"Homophily, Contagion, Confounding: Pick Any Three\", or, The Adventures of Irene and Joey Along the Back-Door Paths\n\nAttention conservation notice: 2700 words on a new paper on causal inference in social networks, and why it is hard. Instills an attitude of nihilistic skepticism and despair over a technical enterprise you never knew existed, much less cared about, which a few feeble attempts at jokes and a half-hearted constructive suggestion at the end fail to relieve. If any of this matters to you, you can always check back later and see if it survived peer review.\n\nWell, we decided for a more sedate title for the actual paper, as opposed to the talk:\n\nCRS and Andrew C. Thomas, \"Homophily and Contagion Are Generically Confounded in Observational Social Network Studies\", arxiv:1004.4704, submitted to Sociological Methods and Research\n\nAbstract: We consider processes on social networks that can potentially involve three phenomena: homophily, or the formation of social ties due to matching individual traits; social contagion, also known as social influence; and the causal effect of an individual's covariates on their behavior or other measurable responses. We show that, generically, all of these are confounded with each other. Distinguishing them from one another requires strong assumptions on the parametrization of the social process or on the adequacy of the covariates used (or both). In particular we demonstrate, with simple examples, that asymmetries in regression coefficients cannot identify causal effects, and that very simple models of imitation (a form of social contagion) can produce substantial correlations between an individual's enduring traits and their choices, even when there is no intrinsic affinity between them. We also suggest some possible constructive responses to these results.\n\nR code for our simulations\n\nThe basic problem here is as follows. (I am afraid this will spoil some of the jokes in the paper.) Consider the venerable parental question: \"If your friend Joey jumped off a bridge, would you jump too?\" The fact of the matter is that the answer is \"yes\"; but why does Joey's jumping off a bridge mean that Joey's friend Irene is more likely to jump off one too?\n\nInfluence or social contagion: Because they are friends, Joey's example inspires Irene to jump. Or, more subtly: seeing Joey jump re-calibrate's Irene's tolerance for risky behavior, which makes jumping seem like a better idea.\n\nBiological contagion: Joey is infected with a parasite which suppresses the fear of heights and/or falling, and, because they are friends, Joey passes it on to Irene.\n\nManifest homophily: Joey and Irene are friends because they both like to jump off bridges (hopefully with bungee cords attached).\n\nLatent homophily: Joey and Irene are friends because they are both hopeless adrenaline junkies, and met through a roller-coaster club; their common addiction leads both of them to take up bridge-jumping.\n\nExternal causation: Sometimes, jumping off a bridge is the only sane thing to do:\n\nFor Irene's parents, there is a big difference between (1) and (2) and the other explanations. The former suggest that it would be a good idea to keep Irene away from Joey, or at least to keep Joey from jumping off the bridge; with the others, however, that's irrelevant. In the case of (3) and (4), in fact, knowing that Irene is friends with Joey is just a clue as to what Irene is really like; the damage was already done, and they can hang out together as much as they want. The difference between these accounts is one of causal mechanisms. (Of course there can be mixed cases.)\n\nWhat the statistician or social scientist sees is that bridge-jumping is correlated across the social network. In this it resembles many, many, many behaviors and conditions, such as prescribing new antibiotics (one of the classic examples), adopting other new products, adopting political ideologies, attaching tags to pictures on flickr, attaching mis-spelled jokes to pictures of cats, smoking, drinking, using other drugs, suicide, literary tastes, coming down with infectious diseases, becoming obese, and having bad acne or being tall for your age. For almost all of these conditions or behaviors, our data is purely observational, meaning we cannot, for one reason or another, just push Joey off the bridge and see how Irene reacts. Can we nonetheless tell whether bridge-jumping spreads by (some form) of contagion, or rather is due to homophily, or, if it is both, say how much each mechanism contributes?\n\nA lot of people have thought so, and have tried to come at it in the usual way, by doing regression. Most readers can probably guess what I think about that, so I will just say: don't you wish. More sophisticated ideas, like propensity score matching, have also been tried, but people have pretty much assumed that it was possible to do this sort of decomposition. What Andrew and I showed is that in fact it isn't, unless you are willing to make very strong, and generally untestable, assumptions.\n\nThis becomes clear as soon as you draw the relevant graphical model, which goes like so:\n\nHere i stands for Irene and j for Joey. Y(i,t) is 1 if Irene jumps off the bridge on day t and 0 otherwise; likewise Y(j,t-1) is whether Joey jumped off the bridge yesterday. We want to know whether the latter variable influences the former. A(i,j) is how we represent the social network --- it's 1 if Irene regards Joey as a friend, 0 otherwise. Lurking in the background are the various traits which might affect whether or not Irene and Joey are friends, and whether or not they like to jump off bridges, collectively X. Suppose that, all else equal, being more similar makes it more likely that people become friends.\n\nNow it's easy to see where the trouble lies. If we learn that Joey jumped off a bridge yesterday, that tells us something about what kind of person Joey is, X(j). If Joey and Irene are friends, that tells us something about what kind of person Irene is, X(i), and so about whether Irene will jump off a bridge today. And this is so whether or not there is any direct influence of Joey's behavior on Irene's, whether or not there is contagion. The chain of inferences — from Joey's behavior to Joey's latent traits, and then over the social link to Irene's traits and thus to Irene's behavior — constitutes what Judea Pearl strikingly called a \"back-door path\" connecting the variables at either end. When such paths exist, as here, Y(i,t) will be at least somewhat predictable from Y(j,t-1), and sufficiently clever regressions will detect this, but they cannot distinguish how much of the predictability is due to the back door path and how much to direct influence. If this sounds hand-wavy to you, and you suspect that with some fancy adjustments you can duck and weave through it, read the paper.\n\nTo switch examples to something a little more serious than jumping off bridges, let's take it as a given that (as Christakis and Fowler famously reported), if Joey became obese last year, the odds of Irene becoming obese this year go up substantially. They interpreted this as a form of social contagion, and one can imagine various influences through which it might work (changing Irene's perception of what normal weight is, changing Irene's perception of what normal food consumption is, changes in happiness leading to changes in comfort food and/or comfort alcohol consumption, etc.). Now suppose that there is some factor X which affects both whether Joey and Irene become friends, and whether and when they become obese. For example:\n\nbecoming friends because they both do extreme sports (like jumping off bridges...) vs. becoming friends because they both really like watching the game on weekends and going through a few six-packs vs. becoming friends because they are both confrontational-spoken-word performance artists;\n\nfriendships tend to be within ethnic groups, which differ in their culturally-transmitted foodways, attitudes towards voluntary exercise, and occupational opportunities;\n\n(for those more fond of genetic explanations than I am): friendships tend to be within ethnic groups, so friends tend to be more genetically similar than random pairs of individuals, and genetic variants that predispose to obesity (in the environment of Framingham, Mass.) are more common in some groups than in others.\n\nSo long as we cannot measure X, the back-door path linking Joey and Irene remains open, and our inferences about contagion are confounded. It would be enough to measure the aspect of X which influences link formation, or the aspect which influences obesity; but without that, there will always be many ways of combining homophily and contagion to produce any given pattern of association between Joey's obesity status last year and Irene's this year. And it's not matter of not being able to decide among some causal alternatives due to limited data; the different causal alternatives all produce the same observable outcomes. (More on this notion of \"identification\".)\n\nChristakis and Fowler made an interesting suggestion in their obesity paper, however, which was actually one of the most challenging things for us to deal with. They noticed that friendships are sometimes not reciprocated, that Irene thinks of Joey as a friend, but Joey doesn't think of Irene that way — or, more cautiously, Irene reports Joey as a friend, but Joey doesn't name Irene. For these asymmetric pairs in their data, Christakis and Fowler note, it's easier to predict the person who named a friend from the behavior of the nominee than vice versa. This is certainly compatible with contagion, in the form of being influenced by those you regard as your friends, but is there any other way to explain it?\n\nAs it happens, yes. One need only suppose that being a certain kind of person — having certain values of the latent trait X — make you more likely to be (or be named as) a friend. Suppose that there is just a one-dimensional trait, like your location on the left-right political axis, or perhaps some scale of tastes. (Perhaps Irene and Joey are neo-conservative intellectuals, and the trait in question is just how violent they like their Norwegian black metal music.) Having similar values of the trait makes you more likely to be friends (that's homophily), but there is always an extra tendency to be friends with those who are closer to the median of the distribution, or at least to say those are who your friends are. (Wherever neo-conservatives really are on the black metal spectrum, they tend to say, on Straussian grounds, that their friends are those who prefer only the median amount of church-burning with their music.) If Irene thinks of Joey as a friend, but Joey does not, this is a sign that Irene has a more extreme value of the trait than Joey does, which changes how much their behavior predicts each other. Putting together a very basic model of this sort shows that it robustly generates the kind of asymmetry Christakis and Fowler found, even when there is really no contagion.\n\nTo be short about it, unless you actually know, and appropriately control for, the things which really lead people to form connections, you really have no way of distinguishing between contagion and homophily.\n\nAll of this can be turned around, however. Suppose that you want to know whether, or how strongly, some trait of people influences their choices. Following a long tradition with many illustrious exponents, for instance, people are very convinced that social class influences political choices, and there is indeed a predictive relationship here, though many people are totally wrong about what that relationship is. The natural supposition is that this predictive relationship reflects causation. But suppose that there is contagion, that you can catch ideology or even just choices from your friends. Social class is definitely a homophilous trait; this means that an opinion or attitude or choice can become entrenched among one social class, and not another, simply through diffusion, even if there is no intrinsic connection between them. And there's nothing special about class here; it could be any trait or combination of traits which leads to homophily.\n\nHere, for example, is a simple simulation done using Andrew's ElectroGraph package.\n\nTo explain: Each individual has a social type or trait, which takes one of two values and stays fixed — think of this as social class, if you like. People are more likely to form links with those of the same type, so when we plot the graph in a way which brings linked nodes closer to each other, we get a nice separation into two sub-communities, with all the upper-class individuals in the one on top and all the lower-class individuals in the one below. Also, each individual makes a \"choice\" which can change over time, which again is binary, here \"red\" or \"blue\". Initially, choices are completely independent of traits, so there's just as much red among the high-class individuals as among the low.\n\nNow let the choices evolve according to the simplest possible rule: at each point in time, a random individual picks one of their neighbors, again at random, and copies their opinion. After a few hundred such updates, the lower class has turned red, and the upper class has turned blue:\n\nAnd this isn't just a fluke; the pattern of color separation repeats quite reliably, though which color goes with which class is random. If you wanted to be more quantitative about it, you could, say, run a logistic regression, and discovery that in the homophilous network, statistically-significant prediction of choice from trait is possible, but not in an otherwise-matched network without homophily; you can see those results in the paper. A bit more abstractly, when I learned cellular automata from David Griffeath, one of the topics was something called the \"voter model\", which is just the rule I gave above for copying choices. On a regular two-dimensional grid, the voter model self-organizes from random noise into blobs of homogeneous color with smooth boundaries; this is just the corresponding behavior on a graph. As I have said several times before, I think this phenomenon — correlating traits and choices by homophily plus contagion — seriously complicates a lot of what people want to do in the social sciences and even the humanities, but since I have gone on about that already, I won't re-rant today.\n\nIn their own way, each of the two models in our paper is sheer elegance in its simplicity, and I have been known to question the relevance of such models for actual social science. I don't think I'm guilty of violating my own strictures, however, because I'm not saying that the processes of, say, spreading political opinions really follows a voter model. (The reality is much more complicated.) The models make vivid what was already proved, and show that the conditions needed to produce the phenomena are not actually very extreme.\n\nMy motto as a writer might as well be \"the urge to destroy is also a creative urge\", but in this paper we do hold out some hope, which is that even if the causal effects of contagion and/or homophily cannot be identified, they might be bounded, following the approach pioneered by Manski for other unidentifiable quantities. Even if observable associations would never let us say exactly how strong contagion is, for instance, they might let us say that it has to lie inside some range, and if that range excludes zero, we know that contagion must be at work. (Or, if the association is stronger than contagion can produce, something else must be at work.) I suspect (with no proof) that one way to get useful bounds would be to use the pattern of ties in the network to divide it into sub-networks or, as we say in the trade, communities, and use the estimated communities as proxies for the homophilous trait. That is, if people tend to become friends because they are similar to each other, then the social network will tend to become a set of clumps of similar people, as in the figures above. So rather than just looking at the tie between Joey and Irene, we look at who else they are friends with, and who their friends are friends with, and so on, until we figure out how the network is divided into communities and that (say) Irene and Joey are in the same community, and therefore likely have the similar values of X, whatever it is. Adjusting for community might then approach actually adjusting for X, though it couldn't be quite the same. Right now, though, this idea is just a conjecture we're pursuing.\n\nManual trackback: The Monkey Cage; Citation Needed; Healthy Algorithms; Siris; Gravity's Rainbow; Orgtheory; PeteSearch\n\nNetworks; Enigmas of Chance; Complexity; Commit a Social Science; Self-Centered\n\nLearning Your Way to Maximum Power\n\nAttention conservation notice: 2300 words about a paper other people wrote on learning theory and hypothesis testing. Mostly written last year as part of a never-used handout for 350, and rescued from the drafts folder as an exercise in structured procrastination so as to avoid a complete hiatus while I work on my own manuscripts.\n\nP.S. Nauroz mubarak.\n\nIn a previous installment, we recalled the Neyman-Pearson lemma of statistical hypothesis testing: If we are trying to discriminate between signal and noise, and know the distribution of our data (x) both for when a signal is present (q) and when there is just noise (p), then the optimal test says \"signal\" when the likelihood ratio q(x)/p(x) exceeds a certain threshold, and \"noise\" otherwise. This is optimal in that, for any given probability of thinking noise is signal (\"size\"), it maximizes the power, the probability of detecting a signal when there is one.\n\nThe problem with just applying the Neyman-Pearson lemma directly to problems of interest is the bit about knowing the exact distributions of signal and noise. We should, forgive the expression, be so lucky. The traditional approach in theoretical statistics, going back to Neyman and Pearson themselves, has been to look for circumstances where we can get a single test of good power against a whole range of alternatives, no matter what they are. The assumptions needed for this are often rather special, and teaching this material means leading students through some of the more arid sections of books like these; the survivors are generally close to insensible by the time they reach the oases of confidence regions.\n\nAt the other extreme, a large part of modern statistics, machine learning and data mining is about classification problems, where we take feature-vectors x and assign them to one of a finite number of classes. Generally, we want to do this in a way which matches a given set of examples, which are presumed to be classified correctly. (This is obviously a massive assumption, but let it pass.) When there are only two classes, however, this is exactly the situation Neyman and Pearson contemplated; a binary classification rule is just a hypothesis test by another name. Indeed, this really the situation Neyman discussed in his later work (like his First Course in Probability and Statistics [1950]), where he advocated dropping the notion of \"inductive inference\" in favor of that of \"inductive behavior\", asking, in effect, what rule of conduct a learning agent should adopt so as to act well in the future.\n\nThe traditional approach in data-mining is to say that one should either (i) minimize the total probability of mis-classification, or (ii) assign some costs to false positives (noise taken for signal) and false negatives (signal taken for noise) and minimize the expected cost. Certainly I've made this recommendations plenty of times in my teaching. But this is not what Neyman and Perason would suggest. After all, the mis-classification rate, or any weighted combination of the error rates, will depend on what proportions of the data we look at actually are signal and noise. Which decision rule minimizes the chance of error depends on the actual proportion of instance of \"signal\" to those of \"noise\". If that ratio changes, a formerly optimal decision rule can become arbitrarily bad. (To give a simple but extreme example, suppose that 99% of all cases used to be noise. Then a decision rule which always said \"noise\" would be right 99% of the time. The minimum-error rule would be very close to \"always say 'noise'\". If the proportion of signal to noise should increase, the formerly-optimal decision rule could become arbitrarily bad. — The same is true, mutatis mutandis, of a decision rule which minimizes some weighted cost of mis-classifications.) But a Neyman-Pearson rule, which maximizes power subject to a constraint on the probability of false positives, is immune to changes in the proportions of the two classes, since it only cares about the distribution of the observables given the classes. But (and this is where we came in) the Neyman-Pearson rule depends on knowing the exact distribution of observables for the two classes...\n\nThis brings us to tonight's reading.\n\nClayton Scott and Robert Nowak, \"A Neyman-Pearson Approach to Statistical Learning\", IEEE Transactions on Information Theory 51 (2005): 3806--3819 [PDF reprint via Prof. Scott, PDF preprint via Prof. Nowak]\n\nAbstract: The Neyman-Pearson (NP) approach to hypothesis testing is useful in situations where different types of error h"
    }
}
{
    "id": "dbpedia_2342_3",
    "rank": 80,
    "data": {
        "url": "https://dl.acm.org/doi/10.1145/3555803",
        "read_more_link": "",
        "language": "en",
        "title": "Trustworthy AI: From Principles to Practices",
        "top_image": "https://dl.acm.org/cms/asset/e9dc3f3b-8629-4301-a236-f1cfe7cbdaf7/3567474.cover.jpg",
        "meta_img": "https://dl.acm.org/cms/asset/e9dc3f3b-8629-4301-a236-f1cfe7cbdaf7/3567474.cover.jpg",
        "images": [
            "https://dl.acm.org/specs/products/acm/releasedAssets/images/acm-dl-logo-white-1ecfb82271e5612e8ca12aa1b1737479.png",
            "https://dl.acm.org/doi/10.1145/specs/products/acm/releasedAssets/images/acm-logo-1-ad466e729c8e2a97780337b76715e5cf.png",
            "https://dl.acm.org/pb-assets/icons/DOs/default-profile-1543932446943.svg",
            "https://dl.acm.org/pb-assets/icons/DOs/default-profile-1543932446943.svg",
            "https://dl.acm.org/pb-assets/icons/DOs/default-profile-1543932446943.svg",
            "https://dl.acm.org/pb-assets/icons/DOs/default-profile-1543932446943.svg",
            "https://dl.acm.org/pb-assets/icons/DOs/default-profile-1543932446943.svg",
            "https://dl.acm.org/pb-assets/icons/DOs/default-profile-1543932446943.svg",
            "https://dl.acm.org/pb-assets/icons/DOs/default-profile-1543932446943.svg",
            "https://dl.acm.org/pb-assets/icons/DOs/default-profile-1543932446943.svg",
            "https://dl.acm.org/specs/products/acm/releasedAssets/images/footer-logo1-45ae33115db81394d8bd25be65853b77.png",
            "https://dl.acm.org/cms/10.1145/3555803/asset/96323156-27ad-4889-b255-2f4b2beb2a99/assets/images/medium/csur-2021-0639-f01.jpg",
            "https://dl.acm.org/cms/10.1145/3555803/asset/869cea92-a950-46e2-ae0f-99454dd87301/assets/images/medium/csur-2021-0639-f02.jpg",
            "https://dl.acm.org/cms/10.1145/3555803/asset/33edc718-6718-4452-bfeb-7c3e3d698a7f/assets/images/medium/csur-2021-0639-f03.jpg",
            "https://dl.acm.org/specs/products/acm/releasedAssets/images/Default_image_lazy-0687af31f0f1c8d4b7a22b686995ab9b.svg",
            "https://dl.acm.org/action/showDoPubAsset?doi=10.1145/contrib-81100369471&format=rel-imgonly&assetId=28312-v2.jpg",
            "https://dl.acm.org/specs/products/acm/releasedAssets/images/loader-7e60691fbe777356dc81ff6d223a82a6.gif",
            "https://dl.acm.org/pb-assets/icons/DOs/default-profile-1543932446943.svg",
            "https://dl.acm.org/pb-assets/icons/DOs/default-profile-1543932446943.svg",
            "https://dl.acm.org/pb-assets/icons/DOs/default-profile-1543932446943.svg",
            "https://dl.acm.org/pb-assets/icons/DOs/default-profile-1543932446943.svg",
            "https://dl.acm.org/pb-assets/icons/DOs/default-profile-1543932446943.svg",
            "https://dl.acm.org/pb-assets/icons/DOs/default-profile-1543932446943.svg",
            "https://dl.acm.org/pb-assets/icons/DOs/default-profile-1543932446943.svg",
            "https://dl.acm.org/pb-assets/icons/DOs/default-profile-1543932446943.svg",
            "https://dl.acm.org/specs/products/acm/releasedAssets/images/acm-logo-dl-8437178134fce530bc785276fc316cbf.png",
            "https://dl.acm.org/specs/products/acm/releasedAssets/images/acm-logo-3-10aed79f3a6c95ddb67053b599f029af.png"
        ],
        "movies": [],
        "keywords": [],
        "meta_keywords": [
            "Trustworthy AI",
            "robustness",
            "generalization",
            "explainability",
            "transparency",
            "reproducibility",
            "fairness",
            "privacy protection",
            "accountability"
        ],
        "tags": null,
        "authors": [
            "Bo Li JD Technology",
            "Tsinghua University",
            "China https:",
            "orcid.org",
            "USA https:",
            "Bo Liu Walmart Inc",
            "Mountain View",
            "Shuai Di JD Technology",
            "Jingen Liu JD Technology",
            "Jiquan Pei JD Technology"
        ],
        "publish_date": null,
        "summary": "",
        "meta_description": "The rapid development of Artificial Intelligence (AI) technology has enabled the deployment\nof various systems based on it. However, many current AI systems are found vulnerable\nto imperceptible attacks, biased against underrepresented groups, lacking in ...",
        "meta_lang": "en",
        "meta_favicon": "/pb-assets/head-metadata/apple-touch-icon-1574252172393.png",
        "meta_site_name": "ACM Computing Surveys",
        "canonical_link": "https://dl.acm.org/doi/10.1145/3555803",
        "text": "Abstract\n\nThe rapid development of Artificial Intelligence (AI) technology has enabled the deployment of various systems based on it. However, many current AI systems are found vulnerable to imperceptible attacks, biased against underrepresented groups, lacking in user privacy protection. These shortcomings degrade user experience and erode people’s trust in all AI systems. In this review, we provide AI practitioners with a comprehensive guide for building trustworthy AI systems. We first introduce the theoretical framework of important aspects of AI trustworthiness, including robustness, generalization, explainability, transparency, reproducibility, fairness, privacy preservation, and accountability. To unify currently available but fragmented approaches toward trustworthy AI, we organize them in a systematic approach that considers the entire lifecycle of AI systems, ranging from data acquisition to model development, to system development and deployment, finally to continuous monitoring and governance. In this framework, we offer concrete action items for practitioners and societal stakeholders (e.g., researchers, engineers, and regulators) to improve AI trustworthiness. Finally, we identify key opportunities and challenges for the future development of trustworthy AI systems, where we identify the need for a paradigm shift toward comprehensively trustworthy AI systems.\n\n1 Introduction\n\nThe rapid development of Artificial Intelligence (AI) continues to provide significant economic and social benefits to society. With the widespread application of AI in areas such as transportation, finance, medicine, security, and entertainment, there is rising societal awareness that we need these systems to be trustworthy. This is because the breach of stakeholders’ trust can lead to severe societal consequences given the pervasiveness of these AI systems. Such breaches can range from biased treatment by automated systems in hiring and loan decisions [49, 146] to the loss of human life [52]. By contrast, AI practitioners, including researchers, developers, and decision-makers, have traditionally considered system performance (i.e., accuracy) to be the main metric in their workflows. This metric is far from sufficient to reflect the trustworthiness of AI systems. Various aspects of AI systems beyond system performance should be considered to improve their trustworthiness, including but not limited to their robustness, algorithmic fairness, explainability, and transparency.\n\nWhile most active academic research on AI trustworthiness has focused on the algorithmic properties of models, advancements in algorithmic research alone is insufficient for building trustworthy AI products. From an industrial perspective, the lifecycle of an AI product consists of multiple stages, including data preparation, algorithmic design, development, and deployment as well as operation, monitoring, and governance. Improving trustworthiness in any single aspect (e.g., robustness) involves efforts at multiple stages in this lifecycle, e.g., data sanitization, robust algorithms, anomaly monitoring, and risk auditing. On the contrary, the breach of trust in any single link or aspect can undermine the trustworthiness of the entire system. Therefore, AI trustworthiness should be established and assessed systematically throughout the lifecycle of an AI system.\n\nIn addition to taking a holistic view of the trustworthiness of AI systems over all stages of their lifecycle, it is important to understand the big picture of different aspects of AI trustworthiness. In addition to pursuing AI trustworthiness by establishing requirements for each specific aspect, we call attention to the combination of and interaction between these aspects, which are important and underexplored topics for trustworthy real-world AI systems. For instance, the need for data privacy might interfere with the desire to explain the system output in detail, and the pursuit of algorithmic fairness may be detrimental to the accuracy and robustness experienced by some groups [284, 361]. As a result, trivially combining systems to separately improve each aspect of trustworthiness does not guarantee a more trustworthy and effective end result. Instead, elaborated joint optimization and tradeoffs between multiple aspects of trustworthiness are necessary [47, 158, 331, 361, 380].\n\nThese facts suggest that a systematic approach is necessary to shift the current AI paradigm toward trustworthiness. This requires awareness and cooperation from multi-disciplinary stakeholders who work on different aspects of trustworthiness and different stages of the system’s lifecycle. We have recently witnessed important developments in multi-disciplinary research on trustworthy AI. From the perspective of technology, trustworthy AI has promoted the development of adversarial learning, private learning, and the fairness and explainability of machine learning (ML). Some recent studies have organized these developments from the perspective of either research [182, 218, 357] or engineering [57, 62, 199, 338, 353]. Developments in non-technical areas have also been reviewed in a few studies, and involve guidelines [145, 178, 294], standardization [210], and management processes [31, 274, 301]. We have conducted a detailed analysis of the various reviews, including algorithmic research, engineering practices, and institutionalization, in Section A.2 in the appendix. These fragmented reviews have mostly focused on specific views of trustworthy AI. To synchronize these diverse developments in a systematic view, we organize multi-disciplinary knowledge in an accessible manner for AI practitioners, and provide actionable and systematic guidance in the context of the lifecycle of an industrial system to build trustworthy AI systems. Our main contributions are as follows:\n\n•\n\nWe dissect the entire lifecycle of the development and deployment of AI systems in industrial applications, and discuss how AI trustworthiness can be enhanced at each stage—from data to AI models and from system deployment to its operation. We propose a systematic framework to organize the multi-disciplinary and fragmented approaches toward trustworthy AI and propose pursuing it as a continuous workflow to incorporate feedback at each stage of the lifecycle of the AI system.\n\n•\n\nWe dissect the entire development and deployment lifecycle of AI systems in industrial applications and discuss how AI trustworthiness can be enhanced at each stage—from data to AI models and from system deployment to its operation. We propose a systematic framework to organize the multi-disciplinary and fragmented approaches toward trustworthy AI and further propose to pursue AI trustworthiness as a continuous workflow to incorporate feedback at each stage of the AI system lifecycle. We also analyze the relationship between different aspects of trustworthiness in practice (mutual enhancement and, sometimes, tradeoffs). The aim is to provide stakeholders of AI systems, such as researchers, developers, operators, and legal experts, with an accessible and comprehensive guide to quickly understand the approaches toward AI trustworthiness (Section 3).\n\n•\n\nWe discuss outstanding challenges facing trustworthy AI on which the research community and industrial practitioners should focus in the near future. We identify several key issues, including the need for a deeper and fundamental understanding of several aspects of AI trustworthiness (e.g., robustness, fairness, and explainability), the importance of user awareness, and the promotion of inter-disciplinary and international collaboration (Section 4).\n\nWith these contributions, we aim to provide the practitioners and stakeholders of AI systems not only with a comprehensive introduction to the foundations and future of AI trustworthiness but also with an operational guidebook for how to construct AI systems that are trustworthy.\n\n2 AI Trustworthiness: Beyond Predictive Accuracy\n\nThe success of ML technology in the past few decades has largely benefited from the accuracy-based performance measurements. By assessing task performance based on quantitative accuracy or loss, training AI models becomes tractable in the sense of optimization. Meanwhile, predictive accuracy is widely adopted to indicate the superiority of an AI product over others. However, with the recent widespread applications of AI, the limitation of an accuracy-only measurement has been exposed to a number of new challenges, ranging from malicious attacks against AI systems to misuses of AI that violate human values. To solve these problems, the AI community has realized in the last decade that factors beyond accuracy should be considered and improved when building an AI system. A number of enterprises [57, 62, 136, 166, 254, 338], academia [122, 199, 218, 301, 322], public sectors, and organizations [9, 210, 334] have recently identified these factors and summarized them as principles of AI trustworthiness. They include robustness, security, transparency, fairness, and safety [178]. Comprehensive statistics relating to and comparisons between these principles have been provided in References [145, 178]. In this article, we study the representative principles that have recently garnered wide interest and are closely related to practical applications. These principles can be categorized as follows:\n\n•\n\nWe consider representative requirements that pertain to technical challenges faced by current AI systems. We review aspects that have sparked wide interest in recent technical studies, including robustness, explainability, transparency,1 reproducibility, and generalization.\n\n•\n\nWe consider ethical requirements with broad concerns in recent literature [9, 57, 121, 145, 178, 199, 218, 301, 334], including fairness, privacy, and accountability.\n\nIn this section, we illustrate the motivation for and definition of each requirement. We also survey approaches to the evaluation of each requirement. It should also be noted that the selected requirements are not orthogonal, and some of them are closely correlated. We explain the relationships with the corresponding requirements in this section. We also use Figure 1 to visualize the relationships between aspects, including tradeoffs, contributions, and manifestation.\n\nFig. 1.\n\n2.1 Robustness\n\nIn general, robustness refers to the ability of an algorithm or system to deal with execution errors, erroneous inputs, or unseen data. Robustness is an important factor affecting the performance of AI systems in empirical environments. The lack of robustness might also cause unintended or harmful behavior by the system, thus diminishing its safety and trustworthiness. In the context of ML systems, the term robustness is applicable to a diversity of situations. In this review, we non-exhaustively summarize the robustness of an AI system by categorizing vulnerabilities at the levels of data, algorithms, and systems, respectively.\n\nData. With the widespread application of AI systems, the environment in which an AI model is deployed becomes more complicated and diverse. If an AI model is trained without considering the diverse distributions of data in different scenarios, then its performance might be significantly affected. Robustness against distributional shifts has been a common problem in various applications of AI [19]. In high-stake applications, this problem is even more critical owing to its negative effect on safety and security. For example, in the field of autonomous driving, besides developing a perceptual system working in sunny scenes, academia and the industry are using numerous development and testing strategies to enhance the perceptual performance of the vehicles in nighttime/rainy scenes to guarantee the system’s reliability under a variety of weather conditions [318, 382].\n\nAlgorithms. It is widely recognized that AI models might be vulnerable to attacks by adversaries with malicious intentions. Among the various forms of attacks, the adversarial attack and defenses against it have raised concerns in both academia and the industry in recent years. Literature has categorized the threat of adversarial attacks in several typical aspects and proposed various defense approaches [12, 69, 213, 304, 373]. For example, in Reference [340], adversarial attacks were categorized with respect to the attack timing. Decision-time attack perturbs input samples to mislead the prediction of a given model so that adversary could evade security checks or impersonate victims. Training-time attack injects carefully designed samples into the training data to change the system’s response to specific patterns and is also known as poisoning attack. Considering the practicality of attacks, it is also useful to note the differences of attacks in terms of the spaces in which they are carried out. Conventional studies have mainly focused on feature space attacks, which are generated directly as the input features of a model. In many practical scenarios, the adversaries can modify only the input entity to indirectly produce attack-related features. For example, it is easy for someone to wear adversarial pattern glasses to evade a face verification system but difficult to modify the image data in memory. Studies on producing realizable entity-based attacks (problem space attacks) have recently garnered increasing interest [325, 358]. Algorithm-level threats might exist in various forms, in addition to directly misleading AI models. Model stealing (a.k.a. the exploratory attack) attempts to steal knowledge about models. Although it does not directly change model behavior, the stolen knowledge has significant value for generating adversarial samples [329].\n\nSystems. System-level robustness against illegal inputs should also be carefully considered in realistic AI products. The cases of illegal inputs can be extremely diverse in practical situations. For example, an image with a very high resolution might cause an imperfect image recognition system to hang. A lidar perception system for an autonomous vehicle might perceive laser beams emitted by lidars in other vehicles and produce corrupted inputs. The presentation attack [275] (a.k.a. spoof attack) is another example that has generated wide concerns in recent years. It fakes inputs by, for example, photos or masks to fool biometric systems.\n\nVarious approaches have been explored to prevent vulnerabilities in AI systems. The objective of defense can be either proactive or reactive [227]. A proactive defense attempts to optimize the AI system to be more robust to various inputs while a reactive defense aims at detecting potential security issues, such as changing distributions or adversarial samples. Representative approaches to improve the robustness of an AI system are introduced in Section 3.\n\nEvaluation. Evaluating the robustness of an AI system serves as an important means of avoiding vulnerabilities and controlling risks. We briefly describe two groups of evaluations: robustness test and mathematical verification.\n\nRobustness test. Testing has served as an essential approach to evaluate and enhance the robustness not only of conventional software but also of AI systems. Conventional functional test methodologies, such as the monkey test [115], provide effective approaches to evaluating the system-level robustness. Moreover, as will be introduced in Section 3.3.1, software testing methodologies have recently been extended to evaluate robustness against adversarial attacks [226, 260].\n\nIn comparison with functional test, performance test, i.e., benchmarking, are more widely adopted approach in the area of ML to evaluate system performance along various dimensions. Test datasets with various distributions are used to evaluate the robustness of data in ML research. In the context of adversarial attacks, the minimal adversarial perturbation is a core metric of robustness, and its empirical upper bound, a.k.a. empirical robustness, on a test dataset has been widely used [65, 312]. From the attacker’s perspective, the rate of success of an attack also intuitively measures the robustness of the system [312].\n\nMathematical verification. Inherited from the theory of formal method, certified verification of the adversarial robustness of an AI model has led to growing interest. For example, adversarial robustness can be reflected by deriving a non-trivial and certified lower bound of the minimum distortion to an attack on an AI model [51, 379]. We introduce this direction in Section 3.2.1.\n\n2.2 Generalization\n\nGeneralization has long been a source of concern in ML models. It represents the capability to distill knowledge from limited training data to make accurate predictions regarding unseen data [133]. Although generalization is not a frequently mentioned direction in the context of trustworthy AI, we find that its impact on AI trustworthiness should not be neglected and deserves specific discussion. On the one hand, generalization requires that AI systems make predictions on realistic data, even on domains or distributions on which they are not trained [133]. This significantly affects the reliability and risk of practical systems. On the other hand, AI models should be able to generalize without the need to exhaustively collect and annotate large amounts of data for various domains [343, 391], so that the deployment of AI systems in a wide range of applications is more affordable and sustainable.\n\nIn the field of ML, the canonical research on generalization theory has focused on the prediction of unseen data, which typically share the same distribution as the training data [133]. Although AI models can achieve a reasonable accuracy on training datasets, it is known that a gap (a.k.a generalization gap) exists between their training and testing accuracies. Approaches in different areas, ranging from statistic learning to deep learning, have been studied to analyze this problem and enhance the model generalization. Typical representatives like cross-validation, regularization, and data augmentation can be found in many ML textbooks [133].\n\nThe creation of a modern data-driven AI model requires a large amount of data and annotations in the training stage. This leads to a high cost for manufacturers and users for re-collecting and re-annotating data to train the model for each task. The cost highlights the need to generalize the knowledge of a model to different tasks, which not only reduces data cost but also improves model performance in many cases. Various directions of research have been explored to address knowledge generalization under different scenarios and configurations within the paradigm of transfer learning [255, 350]. We review the representative approaches in Section 3.2.2.\n\nThe inclusive concept of generalization is closely related to other aspects of AI trustworthiness, especially robustness. In the context of ML, the robustness against distributional shifts (Section 2.1) is also considered a problem of generalization. This implies that the requirements of robustness and generalization have some overlapping aspects. The relationship between adversarial robustness and generalization is more complicated. As demonstrated in Reference [362], an algorithm that is robust against small perturbations has better generalization. Recent research [271, 331], however, has noted that improving robustness by adversarial training may reduce the testing accuracy and leads to worse generalization. To explain this phenomenon, Reference [116] has argued that the adversarial robustness corresponds to different data distributions that may hurt a model’s generalization.\n\nEvaluation. Benchmarking on test datasets with various distributions is a widely used approach to evaluate the generalization of an AI model in realistic scenarios. A summary of commonly used datasets and benchmarks for domain generalization can be found in Reference [391] and covers the tasks of object recognition, action recognition, segmentation, and face recognition.\n\nIn terms of theoretical evaluation, past ML studies have developed rich approaches to measure the bounds of a model’s generalization error. For example, Rademacher complexity [35] is commonly used to determine how well a model can fit a random assignment of class labels. In addition, the Vapnik–Chervonenkis (VC) dimension [337] is a measure of the capacity/complexity of a learnable function set. A larger number of VC dimensions indicates a higher capacity.\n\nAdvances in the DNN has led to new developments in the theory of generalization. Reference [377] observed that modern deep learning models can achieve a generalization gap despite their massive capacity. This phenomenon has led to academic discussions on the generalization of the Deep Neural Network (DNN) [23, 39]. For example, Reference [39] examined generalization from the perspective of the bias–variance tradeoff to explain and evaluate the generalization of the DNN.\n\n2.3 Explainability and Transparency\n\nThe opaqueness of complex AI systems has led to widespread concerns in academia, the industry, and society at large. The problem of how DNNs outperform other conventional ML approaches has been puzzling researchers [24]. From the perspective of practical systems, there is a demand among users for the right to know the intention, business model, and technological mechanism of AI products [9, 135]. A variety of studies have addressed these problems in terms of nomenclature including interpretability, explainability, and transparency [5, 24, 47, 141, 216, 250] and have delved into different definitions. To make our discussion more concise and targeted, we narrow the coverage of explainability and transparency to address the above concerns in theoretical research and practical systems, respectively.\n\n•\n\nExplainability addresses to understand how an AI model makes decision [24].\n\n•\n\nTransparency considers AI as a software system, and seeks to disclose information regarding its entire lifecycle (cf., “operate transparently” in Reference [9]).\n\n2.3.1 Explainability.\n\nExplainability, i.e., understanding how an AI model makes its decision, stays at the core place of modern AI research and serves as a fundamental factor that determines the trust in AI technology. The motivation for the explainability of AI comes from various aspects [24, 25]. From the perspective of scientific research, it is meaningful to understand all intrinsic mechanisms of the data, parameters, procedures, and outcomes in an AI system. The mechanisms also fundamentally determine AI trustworthiness. From the perspective of building AI products, there exist various practical requirements on explainability. For operators like bank executives, explainability helps understand the AI credit system to prevent potential defects in it [25, 184]. Users like loan applicants are interested to know why they are rejected by the model, and what they can do to qualify [25]. See Reference [25] for a detailed analysis of the various motivations of explainability.\n\nExplaining ML models has been an active topic not only in ML research but also in psychological research in the past five years [5, 24, 47, 141, 216, 250]. Although the definition of the explainability of an AI model is still an open question, research has sought to address this problem from the perspectives of AI [141, 285] and psychology [144, 245]. In summary, the relevant studies divide explainability into two levels to explain it.\n\n•\n\nModel explainability by design. A series of fully or partially explainable ML models have been designed in the past half-century of ML research. Representatives include linear regression, trees, the k-nearest neighbors (KNN), rule-based learners, generalized additive model, and Bayesian models [24]. The design of explainable models remains an active area in ML.\n\n•\n\nPost hoc model explainability. Despite the good explainability of the above conventional models, more complex models such as the DNN or Gradient Boosting Decision Tree (GDBT) have exhibited better performance in recent industrial AI systems. Because the relevant approaches still cannot holistically explain these complex models, researchers have turned to post hoc explanation. It addresses a model’s behavior by analyzing its input, intermediate result, and output. A representative category in this vein approximates the decision surface either globally or locally by using an explainable ML model, i.e., explainer, such as a linear model [225, 279] and rules [140, 280]. For deep learning models like the Convolutional Neural Network (CNN) or the transformer, the inspection of intermediate features is a widely used means of explaining model behavior [332, 366].\n\nApproaches to explainability are an active area of work in ML and have been comprehensively surveyed in a variety of studies [24, 47, 141, 250]. Representative algorithms to achieve the above two levels of explainability are reviewed in Section 3.2.3.\n\nEvaluation. In addition to the problem of explaining AI models, a unified evaluation of explainability has been recognized as a challenge. A major reason for this lies in the ambiguity of the psychological outlining of explainability. To sidestep this problem, a variety of studies have used qualitative metrics to evaluate explainability with human participation. Representative approaches include the following:\n\n•\n\nSubjective human evaluation. The methods of evaluation in this context include interviews, self-reports, questionnaires, and case studies that measure, e.g., user satisfaction, mental models, and trust [144, 155, 267].\n\n•\n\nHuman–AI task performance. In tasks performed with human–AI collaboration, the collaborative performance is significantly affected by the human understanding of the AI collaborator and can be viewed as a reflection of the quality of explanation [249]. This evaluation has been used for the development of, for instance, recommendation systems [198] and data analysis [132].\n\nIn addition, if explainability can be achieved by an explainer, then the performance of the latter, such as in terms of the precision of approximation (fidelity [140, 279, 280]), can be used to indirectly and quantitatively evaluate explainability [16].\n\nDespite the above evaluations, a direct quantitative measurement of explainability remains a problem. Some naive measurements of model complexity, like tree depth [46] and the size of the rule set [202], have been studied as surrogate explainability metrics in previous work. We believe that a unified quantitative metric lies at the very heart of fundamental AI research. Recent research on the complexity of ML models [162] and their cognitive functional complexity [347] may inspire future research on a unified quantitative evaluation metric.\n\n2.3.2 Transparency.\n\nTransparency requires the disclosure of the information on a system, and has long been a recognized requirement in software engineering [89, 207]. In the AI industry, this requirement naturally covers the lifecycle of an AI system and helps stakeholders confirm that appropriate design principles are reflected in it. Consider a biometric system for identification as an example. Users are generally concerned with the purpose for which their biometric information is collected and how it is used. Business operators are concerned with the accuracy and robustness against attacks so that they can control risks. Government sectors are concerned with whether the AI system follows guidelines and regulations. On the whole, transparency serves as a basic requirement to build the public’s trust in AI systems [22, 178, 189].\n\nTo render the lifecycle of an AI system transparent, a variety of information regarding its creation needs to be disclosed, including the design purposes, data sources, hardware requirements, configurations, working conditions, expected usage, and system performance. A series of studies have examined disclosing this information through appropriate documentation [22, 129, 156, 246, 265]. This is discussed in Section 3.5.1. The recent trend of open source systems also significantly contributes to the algorithmic transparency of AI systems.\n\nOwing to the complex and dynamic internal procedure of an AI system, facts regarding its creation are not sufficient to fully reveal its mechanism. Hence, the transparency of the runtime process and decision-making should also be considered in various scenarios. For an interactive AI system, an appropriately designed user interface serves as an important means to disclose the underlying decision procedure [10]. In many safety-critical systems, such as autonomous driving vehicles, logging systems [29, 261, 369] are widely adopted to trace and analyze the system execution.\n\nEvaluation. Although a unified quantitative evaluation is not yet available, the qualitative evaluation of transparency has undergone recent advances in the AI industry. Assessment checklists [10, 292] have been regarded as an effective means to evaluate and enhance the transparency of a system. In the context of the psychology of users or the public, user studies or A/B test can provide a useful evaluation based on user satisfaction [249].\n\nQuality evaluations of AI documentation have also been explored in recent years. Some studies [22, 129, 156, 246, 273] have proposed standard practices to guide and evaluate the documentation of an AI system. Reference [265] summarized the general qualitative dimensions for more specified evaluations.\n\n2.4 Reproducibility\n\nModern AI research involves both mathematical derivation and computational experiments. The reproducibility of these computational procedures serves as an essential step to verify AI research. In terms of AI trustworthiness, this verification facilitates the detection, analysis, and mitigation of potential risks in an AI system, such as a vulnerability on specific inputs or unintended bias. With the gradual establishment of the open cooperative ecosystem in the AI research community, reproducibility is emerging as a concern among researchers and developers. In addition to enabling the effective verification of research, reproducibility allows the community to quickly convert the latest approaches into practice or conduct follow-up research.\n\nThere has been a new trend in the AI research community to regard reproducibility as a requirement when publicizing research [142]. We have seen major conferences, such as Neural Information Processing Systems (NeurIPS), International Conference on Machine Learning (ICML), and ACM Multimedia (ACMMM), introduce reproducibility-related policies or programs [263] to encourage the reproducibility of works. To obtain a clear assessment, degrees of reproducibility have been studied in such works as the ACM Artifact Review and References [106, 143]. For example, in Reference [143], the lowest degree of reproducibility requires the exact replication of an experiment with the same implementation and data, while a higher degree requires using different implementations or data. Beyond the basic verification of research, a higher degree of reproducibility promotes a better understanding of the research by distinguishing among the key factors influencing effectiveness.\n\nSome recently developed large scale pre-trained AI models, such as Generative Pre-trained Transformer 3 (GPT-3) and Bidirectional Encoder Representations from Transformers (BERT), are representative of the challenges to the reproducibility of AI research. The creation of these models involves specifically designed data collection strategies, efficient storage of big data, communication and scheduling between distributed clusters, algorithm implementation, appropriate software and hardware environments, and other kinds of knowhow. The reproducibility of such a model should be considered over its entire lifecycle. In recent studies on the reproducibility of ML, this requirement has been decomposed into the reproducibility of data, methods, and experiments [142, 143, 169], where the latter range over a series of lifecycle artifacts such as code, documentation, software, hardware, and deployment configuration. Based on this methodology, an increasing number of ML platforms are being developed that assist researchers and developers better track the lifecycle in a reproducible manner [169, 374].\n\nEvaluation. Reproducibility checklists have been recently widely adopted in ML conferences to assess the reproducibility of submissions [263]. Beyond the replication of experiments in publication, References [142, 143] also specified checklists to evaluate reproducibility at varying degrees. In addition to checklists, mechanisms such as challenges to reproducibility and paper tracks of reproducibility have been adopted to evaluate the reproducibility of publications [118, 263]. To quantitatively evaluate reproducibility in the context of challenges, a series of quantitative metrics have been studied. For example, References [53, 118] designed metrics to quantify how closely an information retrieval system can be reproduced to its origins.\n\n2.5 Fairness\n\nWhen AI systems help us in areas such as hiring, financial risk assessment, and face identification, systematic unfairness in their decisions might have negative social ramifications (e.g., underprivileged groups might experience systematic disadvantage in hiring decisions [49], or be disproportionally impacted in criminal risk profiling [104, 146, 161]). This not only damages the trust that various stakeholders have in AI but also hampers the development and application of AI technology for the greater good. Therefore, it is important that practitioners keep in mind the fairness of AI systems to avoid instilling or exacerbating social bias [66, 105, 242].\n\nA common objective of fairness in AI systems is to mitigate the effects of biases. The mitigation is non-trivial, because the biases can take various forms, such as data bias, model bias, and procedural bias, in the process of developing and applying AI systems [242]. Bias often manifests in the form of unfair treatment of different groups of people based on their protected information (e.g., gender, race, and ethnicity). Therefore, group identity (sometimes also called sensitive variables) and system response (prediction) are two factors influencing bias. Some cases also involve objective ground truths of a given task that one should consider when evaluating system fairness, e.g., whether a person’s speech is correctly recognized or their face correctly identified.\n\nFairness can be applicable at multiple granularities of system behavior [66, 242, 339]. At each granularity, we might be concerned with distributive fairness or fairness of the outcome, or procedural fairness or fairness of the process (we refer the reader to Reference [137] for a more detailed discussion). In each case, we are commonly concerned with the aggregated behavior and the bias therein of an AI system, which is referred to as statistical fairness or group fairness. In certain applications, it is also helpful to consider individual fairness or counterfactual fairness, especially when the sensitive variable can be more easily decoupled from the other features that should justifiably determine the system’s prediction [242]. While the former is more widely applicable to various ML tasks, e.g., speech recognition and face identification, the latter can be critical in cases like resume reviewing for candidate screening [44].\n\nAt the group level, researchers have identified three abstract principles to categorize different types of fairness [66]. We illustrate them with a simple example of hiring applicants from a population consisting of 50% male and 50% female applicants, where gender is the sensitive variable (examples adapted from References [339, 388]):\n\n•\n\nIndependence. This requires for the system outcome to be statistically independent of sensitive variables. In our example, this requires that the rate of admission of male and female candidates be equal (known as demographic parity [376]; see also disparate impact [117]).\n\n•\n\nSeparation. Independence does not account for a justifiable correlation between the ground truth and the sensitive variable (e.g., fewer female candidates might be able to lift 100-lb goods more easily than male candidates). Separation therefore requires that the independence principle hold, conditioned on the underlying ground truth. That is, if the job requires strength qualifications, then the rate of admission for qualified male and female candidates should be equal (known as equal opportunity [147]; see also equal odds [43] and accuracy equity [95]).\n\n•\n\nSufficiency. Sufficiency similarly considers the ground truth but requires that the true outcome and the sensitive variable be independent when conditioned on the same system prediction. That is, given the same hiring decision predicted by the model, we want the same ratio of qualified candidates among male and female candidates (known as test fairness [80, 147]). This is closely related to model calibration [266].\n\nNote that these principles are mutually exclusive under certain circumstances (e.g., independence and separation cannot both hold when the sensitive variable is correlated with the ground truth). Reference [187] has discussed the tradeoff between various fairness metrics. Furthermore, Reference [84] advocated an extended view of these principles, where the utility of the predicted and true outcomes is factored into consideration (e.g., the risk and cost of recidivism in violent crimes compared with the cost of detention), and can be correlated with the sensitive variables. We refer the reader to this work for a more detailed discussion.\n\nEvaluation. Despite the simplicity of the abstract criteria outlined in the previous section, fairness can manifest in many different forms following these principles (see Reference [66, 356] for comprehensive surveys, and Reference [228] for AI ethics’ checklists). We categorize metrics of fairness according to the properties of models and tasks to help the reader choose appropriate ones for their application:\n\nDiscrete vs. continuous variables. The task output, model prediction, and sensitive variables can all be discrete (e.g., classification and nationality), ranked (e.g., search engines, recommendation systems), or continuous (e.g., regression, classifier scores, age, etc.) in nature. An empirical correlation of discrete variables can be evaluated with standard statistical tools, such as correlation coefficients (Pearson/Kendall/Spearman) and Analysis of Variance (ANOVA), while continuous variables often further require binning, quantization, or loss functions to evaluate fairness [66].\n\nLoss function. The criteria of fairness often cannot be exactly satisfied given the limitations of empirical data (e.g., demographic parity between groups when hiring only three candidates). Loss functions are useful in this case to gauge how far we are from empirical fairness. The choice of loss function can be informed by the nature of the variables of concern: If the variables represent probabilities, then likelihood ratios are more meaningful (e.g., disparate impact [117]); for real-valued regression, the difference between mean distances to the true value aggregated over each group might be used instead to indicate whether we model one group significantly better than another [59].\n\nMultiple sensitive variables. In many applications, the desirable AI system should be fair to more than one sensitive variables (e.g., the prediction of risk posed by a loan should be fair in terms of both gender and ethnicity; inter alia, a recommendation system should ideally be fair to both users and recommendees). One can either form a tradeoff of “marginal fairness” between these variables when they are considered one at a time, i.e., evaluate the fairness of each variable separately and combine the loss functions for final evaluation, or explore the full Cartesian product [307] of all variables to achieve joint fairness, which typically requires more empirical observations but tends to satisfy stronger ethical requirements.\n\n2.6 Privacy Protection\n\nPrivacy protection mainly refers to protecting against unauthorized use of the data that can directly or indirectly identify a person or household. These data cover a wide range of information, including name, age, gender, face image, fingerprints, and so on. Commitment to privacy protection is regarded as an important factor determining the trustworthiness of an AI system. The recently released AI ethics guidelines also highlight privacy as one of the key concerns [9, 178]. Government agencies are formulating a growing number of policies to regulate the privacy of data. The General Data Protection Regulation (GDPR) is a representative legal framework, which pushes enterprises to take effective measures for user privacy protection.\n\nIn addition to internal privacy protection within an enterprise, recent developments in data exchange across AI stakeholders has yielded new challenges for privacy protection. For example, when training a medical AI model, each healthcare institution typically only has data from the local residents, which might be insufficient. This leads to the demand to collaborate with other institutions and jointly train a model [299] without leaking private information across institutions.\n\nExisting protection techniques penetrate the entire lifecycle of AI systems to address rising concerns about privacy. In Section 3, we briefly review techniques to protect the privacy in data collection and processing, model training (Section 3.2.5), and model deployment (Section 3.4.4). The realization of privacy protection is also related to other aspects of trustworthy AI. For example, the transparency principle is widely used in AI systems. It informs users of personal data collection and enables privacy settings. In the development of privacy-preserving ML software, such as federated learning (e.g., FATE and PySyft), open source is a common practice to increase transparency and certify the protectiveness of the system.\n\nEvaluation. Laws for data privacy protection like the GDPR require data protection impact assessment (DPIA) if any data processing poses a risk to data privacy. Measures have to be taken to address the risk-related concerns and demonstrate compliance with the law [10]. Data privacy protection professionals and other stakeholders need to be involved to evaluate it.\n\nPrevious research has devised various mathematical methods to formally verify the protectiveness of privacy-preserving approaches. Typical verification can be conducted under assumptions such as semi-honest security, which implies all participating parties follow a protocol to perform the computational task but may try to infer the data of other parties from the intermediate results of computation (e.g., Reference [215]). A stricter assumption is the malicious attack assumption, where each participating party need not follow the given protocol, and can take any possible measure to infer the data [214].\n\nIn practical scenarios, the empirical evaluation of the risk of leakage of privacy is usually considered [283, 360]. For example, Reference [283] showed that 15 demographic attributes were sufficient to render 99% of the participants unique. An assessment of such data re-identification intuitively reflects protectiveness when designing a data collection plan.\n\n2.7 Accountability: A Holistic Evaluation throughout the above Requirements\n\nWe have described a series of requirements to build trustworthy AI. Accountability addresses the regulation on AI systems to follow these requirements. With gradually improving legal and institutional norms on AI governance, accountability becomes a crucial factor for AI to sustainably benefit society with trustworthiness [100].\n\nAccountability runs through the entire lifecycle of an AI system and requires that the stakeholders of an AI system be obligated to justify their design, implementation, and operation as aligned with human values. At the level of executive, this justification is realized by considerate product design, reliable technique architecture, a responsible assessment of the potential impacts, and the disclosure of information on these aspects [209]. Note that in terms of information disclosure, transparency contributes the basic mechanism used to facilitate the accountability of an AI system [94, 100].\n\nFrom accountability is also derived the concept of auditability, which requires the justification of a system to be reviewed, assessed, and audited [209]. Algorithmic auditing is a recognized approach to ensure the accountability of an AI system and assess its impact on multiple dimensions of human values [272]. See also Section 3.5.2.\n\nEvaluation. Checklist-based assessments have been studied to qualitatively evaluate accountability and auditability [10, 315]. As mentioned in this section, we consider accountability to be the comprehensive justification of each concrete requirement of trustworthy AI. Its realization is composed of evaluations of these requirements over the lifecycle of an AI system [272]. Hence, the evaluation of accountability is reflected by the extent to which these requirements of trustworthiness and their impact can be evaluated.\n\n3 Trustworthy AI: A Systematic Approach\n\nWe have introduced the concepts relevant to trustworthy AI in Section 2. Since the early 2010s, diverse AI stakeholders have made efforts to enhance AI trustworthiness. In Section A in our appendix, we briefly review their recent practices in multi-disciplinary areas, including research, engineering, and regulation, and studies that are exemplars of industrial applications, including on face recognition, autonomous driving, and Natural Language Processing (NLP). These practices have made important progress in improving AI trustworthiness. However, we find that this work remains insufficient from the industrial perspective. As depicted in Section 1 and Figure 2, the AI industry holds a position to connect multi-disciplinary fields for the establishment of trustworthy AI. This position requires that industrial stakeholders learn and organize these multi-disciplinary approaches and ensure trustworthiness throughout the lifecycle of AI.\n\nFig. 2.\n\nIn this section, we provide a brief survey of techniques used for building trustworthy AI products and organize them across the lifecycle of product development from an industrial perspective. As shown by the solid-lined boxes in Figure 2, the lifecycle of the development of a typical AI product can be partitioned into data preparation, algorithm design, development–deployment, and management [26]. We review several critical algorithms, guidelines, and government regulations that are closely relevant to the trustworthiness of AI products in each stage of their lifecycle, with the aim of providing a systematic approach and an easy-to-follow guide to practitioners from varying backgrounds to establish trustworthy AI. The approaches and literature mentioned in this section are summarized in Figure 3 and Table 1.\n\nFig. 3.\n\nTable 1.\n\n3.1 Data Preparation\n\nCurrent AI technology is largely data driven. The appropriate management and exploitation of data not only improves the performance of an AI system but also affects its trustworthiness. In this section, we consider two major aspects of data preparation, i.e., data collection and data preprocessing. We also discuss the corresponding requirements of trustworthy AI.\n\n3.1.1 Data Collection.\n\nData collection is a fundamental stage of the lifecycle of AI systems. An elaborately designed data collection strategy can conduce to the enhancement of AI trustworthiness, such as in terms of fairness and explainability.\n\nBias mitigation. Training and evaluation data are recognized as a common source of bias for AI systems. Many types of bias might exist and plague fairness in data collection, requiring different processes and techniques to combat it (see Reference [242] for a comprehensive survey). Bias mitigation techniques during data collection can be divided into two broad categories: debias sampling and debias annotation. The former concerns the identification of data points to use or annotate, while the latter focuses on choosing the appropriate annotators.\n\nWhen sampling data points to annotate, we note that a dataset reflecting the user population does not guarantee fairness, because statistical methods and metrics might favor majority groups. This bias can be further amplified if the majority group is more homogeneous for the task (e.g., recognizing speech in less-spoken accents can be naturally harder due to data scarcity [191]). System developers should therefore take task difficulty into consideration when developing and evaluating fair AI systems. However, choosing the appropriate annotators is especially crucial for underrepresented data (e.g., when annotating speech recognition data, most humans are also poor at recognizing rarely heard accents). Therefore, care must be taken in selecting the correct experts, especially when annotating data from underrepresented groups, to prevent human bias from creeping into the annotated data.\n\nExplanation collection. Aside from model design and development, data collection is also integral to building explainable AI systems. As will be mentioned in Section 3.2.3, adding an explanation task to the AI model can help explain the intermediate features of the model. This strategy is used in tasks like NLP-based reading comprehension by generating supporting sentences [332, 366]. To train the explanation task, it is helpful to consider collecting explanations or information that may not directly be part of the end task, either directly from annotators [354] or with the help of automated approaches [185].\n\nData Provenance. Data provenance requires recording the data lineage, including the source, dependencies, contexts, and steps of processing [306]. By tracking the data lineage at the highest resolution, data provenance can enhance the transparency, reproducibility, and accountability of an AI system [154, 172]. Moreover, recent research has shown that data provenance can be used to mitigate data poisoning [33], thus enhancing the robustness and security of an AI system. The technical realization of data provenance has been provided in Reference [154]. Tool chains [293] and documentation [129] guides have also been studied for specific scenarios involving AI systems. To ensure that the provenance is tamper proof, the blockchain has been recently considered as a promising tool to certify data provenance in AI [15, 96].\n\n3.1.2 Data Preprocessing.\n\nBefore feeding data into an AI model, data preprocessing helps remove inconsistent pollution of the data that might harm model behavior and sensitive information that might compromise user privacy.\n\nAnomaly Detection. Anomaly detection (a.k.a. outlier detection) has long been an active area of ML [70, 81, 257, 316]. Due to the sensitivity of ML models to outlier data, data cleaning by anomaly detection serves as an effective approach to enhance performance. In recent studies, anomaly detection has been shown to be useful in addressing some requirements of AI trustworthiness. For example, fraudulent data can challenge the robustness and security of systems in areas such as banking and insurance. Various approaches have been proposed to address this issue by using anomaly detection [70]. The detection and mitigation of adversarial inputs is also considered to be a means to defend against evasion attacks and data poisoning attacks [12, 213, 304]. It is noteworthy that the effectiveness of detection in high dimensions (e.g., images) is still limited [64]. The mitigation of adversarial attacks is also referred to as data sanitization [71, 87, 258].\n\nData Anonymization (DA). DA modifies the data so that the protected private information cannot be recovered. Different principles of quantitative data anonymization have been developed, such as k-anonymity [288], \\((c,k)\\) -safety [236], and \\(\\delta\\) -presence [253]. Data format-specific DA approaches have been studied for decades [171, 372, 386]. For example, private information in the form of graph data for social networks can be potentially contained in properties of the vertices of the graph, its link relationships, weights, or other graph metrics [390]. Ways of anonymizing such data have been considered in the literature [37, 220]. Specific DA methods have also been designed for relational data [262], set-valued data [151, 320], and image data [97, 239]. Guidelines and standards have been formulated for data anonymization, such as the US HIPAA and the UK ISB1523. Data pseudonymization [251] is also a relevant technique promoted by the GDPR. It replaces private information with non-identifying references.\n\nDesirable data anonymization is expected to be immune from data de-anonymization or re-identification attacks that try to recover private information from anonymized data [111, 175]. For example, Reference [176] introduces several approaches into de-anonymize user information from graph data. To mitigate the risk of privacy leakage, an open source platform was provided in Reference [174] to evaluate the privacy protection-related performance of graph data anonymization algorithms against de-anonymization attacks.\n\nDifferential privacy (DP). DP shares information of groups within datasets while withholding individual samples [108, 109, 110]. Typical DP can be formally defined by \\(\\epsilon\\) -differential privacy. It measures the extent to which a (randomized) statistical function on the dataset reflects whether an element has been removed [108]. DP has been explored in various data publishing tasks, such as log data [159, 385], set-valued data [76], correlated network data [75], and crowdsourced data [278, 344]. It has also been applied to single- and multi-machine computing environments, and integrated with ML models for protecting model privacy [2, 120, 349]. Enterprises like Apple have used DP to transform user data into a form from which the true data cannot be reproduced [21]. In Reference [113], researchers proposed the RAPPOR algorithm that satisfies the definition of DP. The algorithm is used for crowdsourcing statistical analyses of user software. DP is also used to improve the robustness of AI models against adversarial samples [204].\n\n3.2 Algorithm Design\n\nA number of aspects of trustworthy AI have been addressed as algorithmic problems in the context of AI research, and have attracted widespread interest. We organize recent technical approaches by their corresponding aspects of AI trustworthiness, including robustness, explainability, fairness, generalization, and privacy protection, to provide a quick reference for practitioners.\n\n3.2.1 Adversarial Robustness.\n\nThe robustness of an AI model is significantly affected by the training data and the algorithms used. We describe several representative directions in this section. Comprehensive surveys can be found in the literature such as References [12, 19, 45, 69, 213, 304, 373].\n\nAdversarial training. Since the discovery of adversarial attacks, it has been recognized that augmenting the training data with adversarial samples provides an intuitive approach for defense against them. This is typically referred to as adversarial training [134, 211, 346]. The augmentation can be carried out in a brute-force manner by feeding both the original data and adversarial samples during training [201], or by using a regularization term to implicitly represent adversarial samples [134]. Conventional adversarial training augments data with respect to specific attacks. It can defend against the corresponding attack but is vulnerable to other kinds of attacks. Various improvements have been studied to improve this defense [45, 229, 304]. Reference [328] augmented the training data with adversarial perturbations transferred from other models. It is shown to further provide defense against black-box attacks that do not require knowledge of the model parameters This can help defend against black-box attacks, which do not require knowledge of the model parameters. Reference [231] combined multiple types of perturbations into adversarial training to improve the robustness of the model against multiple types of attacks.\n\nAdversarial regularization. In addition to the regularization term that implicitly represents adversarial samples, recent research further explores network structures or regularization to overcome the vulnerabilities of the DNN to adversarial attacks. An intuitive motivation for this regularization is to prevent the outcome of the network from changing dramatically in case of small input perturbations. For example, Reference [139] penalized large partial derivatives of each layer to improve the stability of its output. A similar regularization on gradients was adopted by Reference [286]. Parseval networks [82] train the network by imposing a regularization on the Lipschitz constant in each layer.\n\nCertified robustness. Adversarial training and regularization improve the robustness of AI models in practice but cannot theoretically guarantee that the models work reliably. This problem has prompted research to formally verify the robustness of models (a.k.a. certified robustness). Recent research on certified robustness has focused on robust training to deal with input perturbations. For example, CNN-Cert [51], CROWN [379], Fast-lin, and Fast-lip [352] aim to minimize an upper bound of the worst-case loss under given input perturbations. Reference [152] instead derives a lower bound on the input manipulation required to change the decision of the classifier, and uses it as a regularization term for robust training. To address the issue that the exact computation of such bounds is intractable for large networks, various relaxations or approximations, such as References [352, 378], have been proposed as alternatives to regularization. Note that the above research mainly optimizes robustness only locally near the given training data. To achieve certified robustness on unseen inputs as well, global robustness has recently attracted the interest of the AI community [77, 206].\n\nIt is also worth noting the recent trend of the intersection of certified robustness and the perspective of formal verification, which aims at developing rigorous mathematical specification and techniques of verification for assurances of software correctness [83]. A recent survey by Reference [335] provided a thorough review of the formal verification of neural networks.\n\nPoisoning defense. Typical poisoning or backdoor attacks contaminate the training data to mislead model behavior. Besides avoiding suspicious data in the data sanitization stage, defensive algorithms against poisoning data are an active area [213]. The defense has been studied at different stages of a DNN model. For example, based on the observation that backdoor-related neurons are usually inactivated for benign samples, Reference [219] proposed pruning these neurons from a network to remove the hidden backdoor. Neural Cleanse [342] proactively discovers backdoor patterns in a model. The backdoor can then be avoided by the early detection of backdoor patterns from the data or retraining the model to mitigate the backdoor. The detection of backdoor attacks can also be carried out by analyzing the prediction on the model on specifically designed benchmarking inputs [194].\n\n3.2.2 Model Generalization.\n\nTechniques of model generalization not only aim to improve model performance but also explore training AI models with limited data and at limited cost. We review representative approaches to model generalization, categorized as classic generalization and domain generalization.\n\nClassic generalization mechanisms. As a fundamental principle of model generalization theory, the bias–variance tradeoff indicates that a generalized model should maintain a balance between underfitting and overfitting [39, 124]. For an overfitted model, reducing complexity/capacity might lead to better generalization. Consider the neural network as an example. Adding a bottleneck layer, which has fewer neurons than the layers both below and above, to it can help reduce model complexity and reduce overfitting.\n\nOther than adjusting the architecture of the model, one can mitigate overfitting to obtain better generalization via various explicit or implicit regularizations, such as early stopping [370], batch normalization [167], dropout [309], data augmentation, and weight decay [196]. These regularizations are standard techniques to improve model generalization when the training data are much smaller in size than the number of model parameters [337]. They aim to push learning to a sub-space of a hypothesis with manageable complexity and reduce model complexity [377]. However, [377] also observed that explicit regularization may improve generalization performance but is insufficient to reduce generalization error. The generalization of the deep neural network is thus still an open problem.\n\nDomain generalization. A challenge for modern DNNs is their generalization of out-of-distribution data. This challenge arises from various practical AI tasks [343, 391] in the area of transfer learning [255, 350]. Domain adaptation [343, 391] aims to find domain-invariant features such that an algorithm can achieve similar performances across domains. As another example, the goal of few-shots learning is to generalize model to new tasks using only a few examples [78, 348, 371]. Meta-learning [336] attempts to learn prior knowledge of generalization from a number of similar tasks. Feature similarity [190, 308] has been used as a representative type of knowledge prior in works such as the Model-Agnostic Meta-Learning (MAML) [119], reinforcement learning [212], and memory-augmented neural network [38, 291].\n\nModel pre-training is a popular mechanism to leverage knowledge learned from other domains, and has recently achieved growing success in both academia and the industry. For example, in computer vision, an established successful paradigm involves pre-training models on a large-scale dataset, such as ImageNet, and then fine-tuning them on target tasks with fewer training data [131, 224, 375]. This is because pre-trained feature representation can be used to transfer information to the target tasks [375]. Unsupervised pre-training has recently been very successful in language processing (e.g., BERT [92] and GPT [269]) and computer vision tasks (e.g., Momentum Contrast (MoCo) [150] and Sequence Contrastive learning (SeCo) [368]). In addition, self-supervised learning provides a good mechanism to learn a cross-modal feature representation. These include the vision and language models VL-BERT [313] and Auto-CapTIONs [256]. To explain the effectiveness of unsupervised pre-training, [112] conducted a series of experiments to illustrate that it can drive learning to the basins of minima that yield better generalization.\n\n3.2.3 Explainable ML.\n\nIn this section, we review representative approaches for the two aspects of ML explainability mentioned in Section 2.3.1 and their application to different tasks.\n\nExplainable ML model design. In spite of being recognized as disadvantageous in terms of performance, explainable models have been actively researched in recent years, and a variety of fully or partially explainable ML models have been studied to push their performance limit.\n\nSelf-explainable ML models. A number of self-explainable models have been studied in ML over the years. The representative ones include the KNN, linear/logistic regression, decision trees/rules, and probabilistic graphical models [24, 47, 141, 250]. Note that the self-explainability of these models is sometimes undermined by their complexity. For example, very complex tree or rule structures might sometimes be considered incomprehensible or unexplainable.\n\nSome learning paradigms other than conventional models are also considered to be explainable, such as causal inference [197, 259] and the knowledge graph [345]. These methods are also expected to provide valuable inspiration to solve the problem of explainability for ML.\n\nBeyond self-explainable ML models. Compared with black-box models, such as the DNN, conventional self-explainable models have poor performance on complex tasks, such as image classification and text comprehension. To achieve a compromise between explainability and performance, hybrid combinations of self-explainable models and black-box models have been proposed. A typical design involves embedding an explainable bottleneck model into a DNN. For example, previous research has embedded linear models and prototype selection to the DNN [16, 20, 73]. In the well-known class activation mapping [389], an average pooling layer at the end of a DNN can also be regarded as an explainable linear bottleneck. Attention mechanisms [30, 363] have also attracted recent interest and have been regarded as an explainable bottleneck in a DNN in some studies [79, 237]. However, this claim continues to be debated, because attention weights representing different explanations can produce similar final predictions [170, 355].\n\nPost hoc model explanation. In addition to designing self-explainable models, understanding how a specific decision is made by black-box models is also an important problem. A major part of research on this problem has addressed the methodology of post hoc model explanation and proposed various approaches.\n\nExplainer approximation aims to mimic the behavior of a given model with explainable models. This is also referred to as the global explanation of a model. Various approaches have been proposed to approximate ML models, such as random forests [317, 392] and neural networks [28, 86, 393]. With the rise of deep learning in the past decade, explainer approximation on the DNN has advanced as the knowledge distillation problem on explainers such as trees [125, 384].\n\nFeature importance has been a continually active area of research on explainability. A representative aspect uses local linear approximation to model the contribution of each feature to the prediction. Local Interpretable Model-agnostic Explanation (LIME) [279] and SHapley Additive exPlanation (SHAP) [225] are influential approaches that can be used for predictions on tabular data, computer vision, and NLP. Gradients can reflect how features contribute to the predicted outcome, and have drawn great interest in work on the explainability of the DNN [297, 305]. In NLP or Computer Vision (CV), gradients or their variants are used to back-trace the decision of the model to the location of the most intimately related input, in the form of saliency maps and sentence highlights [250, 302, 314, 375].\n\nFeature introspection aims to provide a semantic explanation of intermediate features. A representative aspect attaches an extra branch to a model to generate an explanatory outcome that is interpretable by a human. For example, in NLP-based reading comprehension, the generation of a supporting sentence serves as an explanatory task in addition to answer generation [332, 366]. In image recognition, part-template masks can be used to regularize feature maps to focus on local semantic parts [383]. Concept attribution [47] is another aspect that maps a given feature space to human-defined concepts. Similar ideas have been used in generative networks to gain control over attributes, such as the gender, age, and ethnicity, in a face generator [221].\n\nExample-based explanation explains outcomes of the AI model by using the sample data. For example, an influential function was borrowed from robust statistics in Reference [193] to find the most influential data instance for a given outcome. Counterfactual explanation [13, 185, 341] works in a contrary way by finding the boundary case to flip the outcome. This helps users better understand the decision surface of the model.\n\n3.2.4 Algorithmic Fairness.\n\nMethods to reduce bias in AI models during algorithm development can intervene before the data are fed into the model (pre-processing), when the model is being trained (in-processing), or into model predictions after it has been trained (post-processing).\n\nPre-processing methods. Aside from debiasing the data collection process, we can debias data before model training. Common approaches include the following:\n\nAdjusting sample importance. This is helpful especially if debiasing the data collection is not sufficient or no longer possible. Common approaches include resampling [6], which involves selecting a subset of the data, reweighting [60], which involves assigning different importance values to data examples, and adversarial learning [229], which can be achieved through resampling or reweighting with the help of a trained model to find the offending cases.\n\nAside from helping to balance the classification accuracy, these approaches can be applied to balance the cost of classification errors to improve performance on certain groups [163] (e.g., for the screening of highly contagious and severe diseases, false negatives can be costlier than false positives; see cost-sensitive learning [321]).\n\nAdjusting feature importance. Inadvertent correlation between features and sensitive variables can lead to unfairness. Common approaches of debiasing include representation transformation [61], which can help adjust the relative importance of features, and blinding [74], which omits features that are directly related to the sensitive variables.\n\nData augmentation. Besides making direct use of the existing data samples, it is possible to introduce additional samples that typically involves making changes to the available samples, including through perturbation and relabeling [60, 85]. In-processing methods. Pre-processing techniques are not guaranteed to have the desired effect during model training, because different models can leverage features and examples in different ways. This is where in-processing techniques can be helpful:\n\nAdjusting sample importance. Similar to pre-processing methods, reweighting [195] and adversarial learning [68] can be used for in-processing, with the potential of either making use of the model parameters or predictions that are not yet fully optimized to more directly debias the model.\n\nOptimization-related techniques. Alternatively, model fairness can be enforced more directly via optimization techniques. For instance, quantitative fairness metrics can be used as regularization [7] or constraints for the optimization of the model parameters [67].\n\nPost-processing methods. Even if all precautions have been taken with regard to data curation and model training, the resulting models might still exhibit unforeseen biases. Post-processing techniques can be applied for debiasing, often with the help of auxiliary models or hyperparameters to adjust the model output. For instance, optimization techniques (e.g., constraint optimization) can be applied to train a smaller model to transform model outputs or calibrate model confidence [186]. Reweighting the predictions of multiple models can also help reduce bias [168].\n\n3.2.5 Privacy Computing.\n\nApart from privacy-preserving data processing methods, which were introduced in Section 3.1.2, another line of methods preserve data privacy during model learning. In this part, we briefly review the two popular categories of such algorithms: secure multi-party computing, and federated learning.\n\nSecure Multi-party Computing (SMPC) deals with the task whereby multiple data owners compute a function, with the privacy of the data protected and no trusted third party serving as coordinator. A typical SMPC protocol satisfies properties of privacy, correctness, independence of inputs, guaranteed output delivery, and fairness [114, 387]. The garbled circuit is a representative paradigm for secure two-party computation [244, 367]. Oblivious transfer is among the key techniques. It guarantees that the sender does not know what information the receiver obtains from the transferred message. For the multi-party condition, secret sharing is one of the generic frameworks [181]. Each data instance is treated as a secret and split into several shares. These shares are then distributed to the multiple participating parties. The computation of the function value is decomposed into basic operations that are computed following the given protocol.\n\nThe use of SMPC in ML tasks has been studied in the context of both model-specific learning tasks, e.g., linear regression [128] and logistic regression [300], and generic model learning tasks [247]. Secure inference is the an emerging topic that tailors the SMPC for ML use. Its application to ML is as a service in which the server holds the model and clients hold the private data. To reduce the high costs of computation and communication of the SMPC, parameter quantization and function approximation were used together with cryptographic protocols in References [8, 32]. Several tools have been open sourced, such as MP2ML [48], CryptoSPN [330], CrypTFlow [200, 276], and CrypTen [188].\n\nFederated learning (FL) was initially proposed as a secure scheme to collaboratively train an ML model on a data of user interactions with their devices [241]. It quickly gained extensive interest in academia and the industry as a solution to collaborative model training tasks by using data from multiple parties. It aims to address data privacy concerns that hinder ML algorithms from properly using multiple data sources. It has been applied to numerous domains, such as healthcare [282, 299] and finance [223].\n\nExisting FL algorithms can be categorized into horizontal FL, vertical FL, and federated transfer learning algorithms [365]. Horizontal FL refers to the scenario in which each party has different samples but the samples share the same feature space. A training step is decomposed as to first compute optimization updates on each client and then aggregate them on a centralized server without knowing the clients’ private data [241]. Vertical FL refers to the setting in which all parties share the same sample ID space but have different features. Reference [148] used homomorphic encryption for vertical logistic regression-based model learning. In Reference [138], an efficient method of kernel learning was proposed. Federated transfer learning is applicable to the condition in which none of the parties overlaps in either the sample or the feature space [222]. The connection between FL and other research topics, such as multi-task learning, meta-learning, and fairness learning, has been discussed in Reference [180]. To expedite FL-related research and development, many open source libraries have been released, such as FATE, FedML [149], and Fedlearn-Algo [217].\n\n3.3 Development\n\nThe manufacture of reliable products requires considerable effort in software engineering, and this is sometimes overlooked by AI developers. This lack of diligence, such as insufficient testing and monitoring, may incur long-term costs in the subsequent lifecycle of AI products (a.k.a. technical debt [296]). Software engineering in the stages of development and deployment has recently aroused wide concern as an essential condition for reliable AI systems [17, 203]. Moreover, various techniques researched for this stage can contribute to the trustworthiness of an AI system [17]. In this section, we survey the representative techniques.\n\n3.3.1 Functional Testing.\n\nInherited from the workflow of canonical software engineering, the testing methodology has drawn growing attention in the development of AI systems. In terms of AI trustworthiness, testing serves as an effective approach to certify that the system is fulfilling specific requirements. Recent research has explored to adapt functional testing to AI systems. This has been reviewed in the literature, such as References [164, 235, 381]. We describe two aspects of adaption from the literature that are useful to enhance the trustworthiness of an AI system.\n\nTest criteria. Different from canonical software engineering where exact equity is tested between the actual and the expected outputs of a system, an AI system is usually tested by its predictive accuracy on a specific testing dataset. Beyond accuracy, various test criteria have been studied to further reflect and test more complex properties of an AI system. The concept of test coverage in software testing has been transplanted into DNN models [226, 260]. The name of a representative metric—neuron coverage [260]—figuratively illustrates that it measures the coverage of activated neurons in a DNN in analogy to code branches in canonical software testing. Such coverage criteria are effective for certifying the robustness of a DNN against adversarial attacks [226].\n\nTest case generation. Human-annotated datasets are insufficient for thoroughly testing an AI system, and large-scale automatically generated test cases are widely used. Similar to canonical software testing, the problem to automatically generate the expected ground truth, known as the oracle problem [34], also occurs in the AI software testing scenario. The hand-crafted test case template is an intuitive but effective approach in applications of NLP [281]. Metamorphic testing is also a practical approach that converts the input/output pairs into new test cases. For example, [382] transfers images of road scenes taken in daylight to rainy images by using Generative Adversarial Network (GAN) as new test cases, and re-uses the original, invariant annotation to test an autonomous driving systems. These testing cases are useful for evaluating the generalization performance of an AI model. A similar methodology was adopted by adding adversarial patterns to normal images to test adversarial robustness [226]. Simulated environments are also widely used to test applications such as computer vision and reinforcement learning. We further review this topic in Section 3.3.3.\n\n3.3.2 Performance Benchmarking.\n\nUnlike conventional software, the functionality of AI systems is often not easily captured in functional test alone. To ensure that systems are trustworthy in terms of different aspects of interest, benchmarking (a.k.a. performance testing in software engineering) is often applied to ensure system performance and stability when these characteristics can be automatically measured.\n\nRobustness is an important aspect of trustworthiness that is relatively amenable to automatic evaluation. References [88, 153] introduced a suite of black-box and white-box attacks to automatically evaluate the robustness of AI systems. It can potentially be performed as a sanity check before such systems are deployed to affect millions of users. Software fairness has also been a concern since conventional software testing [56, 127]. Criteria for AI systems have been studied to spot issues of unfairness by investigating the correlation among sensitive attributes, system outcomes, and the true label when applicable to well-designed diagnostic datasets [327]. Well-curated datasets and metrics have been proposed in the literature to evaluate performance on fairness metrics that are of interest for different tasks [40, 123, 307].\n\nMore recently, there has been growing interest in benchmarking explainability when models output explanations in NLP applications. For instance, Reference [238] asks crowd workers to annotate salient pieces of text that lead to their belief that the text is hateful or offensive, and examines how well model-predicted importance fits human annotations. Reference [93] instead introduces partial perturbations to the text for human annotators, and observes if the system’s explanations match perturbations that change human decisions. In the meantime, Reference [267] reported that explainability benchmarking remains relatively difficult, because visual stimuli are higher dimensional and continuous.\n\n3.3.3 Development by Simulation.\n\nWhile benchmarks serve to evaluate AI systems in terms of predictive behavior given static data, the behavior of many systems is deeply rooted in their interactions with the world. For example, benchmarking autonomous vehicle systems on static scenarios is insufficient to help us evaluate their performance on dynamic roadways. For these systems, simulation often plays an important role in ensuring their trustworthiness before deployment.\n\nRobotics is a sub-fields of AI where simulations are most commonly used. Control systems for robots can be compared and benchmarked in simulated environments such as Gazebo [192], MuJoCo [324], and VerifAI [103]. Similarly, simulators for autonomous driving vehicles have been widely used, including CARLA [102], TORCS [359], CarSim [42], and PRESCAN [323]. These software platforms simulate the environment in which robots and vehicles operate as well as the actuation of controls on the simulated robots or cars. In NLP, especially conversational AI, simulators are widely used to simulate user behavior to test system ability and fulfill user needs by engaging in a dialog [205]. These simulators can help automatically ensure the performance of AI systems in an interactive environment and diagnose issues before their deployment.\n\nDespite the efficiency, flexibility, and replicability afforded by software simulators, they often still fall short of perfectly simulating constraints faced by the AI system when deployed as well as environmental properties or variations in them. For AI systems that are deployed on embedded or otherwise boxed hardware, it is important to understand the system behavior when they are run on the hardware used in real-world scenarios. Hardware-in-the-loop simulations can help developers understand system performance when it is run on chips, sensors, and actuators in a simulated environment, and is particularly helpful for latency- and power-critical systems like autonomous driving systems [50, 54]. By taking real-world simulations a step further, one can also construct controlled real-world environments for fully integrated AI systems to roam around in (e.g., test tracks for self-driving cars with road signs and dummy obstacles). This provides more realistic measurements and assurances of performance before releasing such systems to users.\n\n3.4 Deployment\n\nAfter development, AI systems are deployed on realistic products, and interact with the environment and users. To guarantee that the systems are trustworthy, a number of approaches should be considered at the deployment stage, such as adding additional components to monitor anomalies, and developing specific human–AI interaction mechanisms for transparency and explainability.\n\n3.4.1 Anomaly Monitoring.\n\nAnomaly monitoring has become a well-established methodology in software engineering. In terms of AI systems, the range of monitoring has been further extended to cover data outliers, data drifts, and model performance. As a keying safeguard for the successful operation of an AI system, monitoring provides the means to enhance the system’s trustworthiness in multiple aspects. Some representative examples are discussed below.\n\nAttack monitoring has been widely adopted in conventional SaaS, such as fraud detection [3] in e-commerce systems. In terms of the recent emerging adversarial attacks, detection and monitoring [243] of such attack inputs is also recognized as an important means to ensure system robustness. Data drift monitoring [268] provides important means to maintain the generalization of an AI system under concept change [394] caused by dynamic environment such as market change [289]. Misuse monitoring is recently also adopted in several cloud AI services [173] to avoid improper use such as unauthorized population surveillance or individual tracking by face recognition, which helps ensure the proper alignment of ethical values.\n\n3.4.2 Human–AI Interaction.\n\nAs an extension of human–computer interaction (HCI), human–AI interaction has aroused wide attention in the AI industry [4, 18]. Effective human–AI interaction affects the trustworthiness of an AI system in multiple aspects. We briefly illustrate two topics.\n\nUser interface serves as the most intuitive factor affecting user experience. It is a major medium for an AI system to disclose its internal information and decision-making procedure to users, and thus has an important effect on the transparency and explainability of the system [301, 351]. Various approaches of interaction have been studied to enhance the explainability of AI, including the visualization of ML models [72] and interactive parameter-tuning [351]. In addition to transparency and explainability, the accessibility of the interface also significantly affects user experience of trustworthiness. AI-based techniques of interaction have enabled various new forms of human–machine interfaces, such as chatbots, audio speech recognition, and gesture recognition, and might result in accessibility problems for disabled people. Mitigating such unfairness has aroused concerns in recent research [179, 326].\n\nHuman intervention, such as by monitoring failure or participating in decisions [295], has been applied to various AI systems to compensate for limited performance. Advanced Driving Assistance System (ADAS) can be considered a typical example of systems involving human intervention, where the AI does the low-level driving work and the human makes the high-level decision. In addition to compensating for decision-making, human intervention provides informative supervision to train or fine-tune AI systems in many scenarios, such as the shadow mode [319] of autonomous driving vehicles. To minimize and make the best use of human effort in such interaction, efficient design of patterns of human–machine cooperation is an emerging topic in inter-disciplinary work on HCI and AI and is referred to as human-in-the-loop or interactive machine learning [157] in the literature.\n\n3.4.3 Fail-Safe Mechanisms.\n\nConsidering the imperfection of current AI systems, it is important to avoid harm when the system fails in exceptional cases. By learning from conventional real-time automation systems, the AI community has realized that a fail-safe mechanism or fallback plan should be an essential part of the design of an AI system if its failure can cause harm or loss. This mechanism is also emerging as an important requirement in recent AI guidelines, such as Reference [9]. The fail-safe design has been observed in multiple areas of robotics in the past few years. In the area of Unmanned Aerial Vehicle (UAV), the fail-safe algorithm has been studied for a long time to avoid frequent collision of quadrocopters [126] and to ensure safe landing upon system failure [252]. In autonomous driving where safety is critical, a fail-safe mechanism like standing still has become an indispensable component in Advanced Driver-Assistant System (ADAS) products [160], and is being researched at a higher level of automation [230].\n\n3.4.4 Hardware Security.\n\nAI systems are widely deployed on various hardware platforms to cope with the diverse scenarios, ranging from servers in computing centers to cellphones and embedded systems. Attacks on OS and hardware lead to new risks, such as data tampering or stealing, and threaten the robustness, security, and privacy of AI systems. Various approaches have been studied to address this new threat [364]. From the perspective of hardware security, the concept of a trusted execution environment (TEE) is a recent representative technique that has been adopted by many hardware manufacturers [287]. The general mechanism of the TEE is to provide a secure area for the data and the code. This area is not interfered with by the standard OS such that the protected program cannot be attacked. ARM processors support TEE implementation using the TrustZone design [264]. They simultaneously run a secure OS and a normal OS on a single core. The secure part provides a safe environment for sensitive information. The Intel Software Guard Extensions implement the TEE by hardware-based memory encryption [240]. Its enclave mechanism allows for the allocation of protected memory to hold private information. Such security mechanisms have been used to protect sensitive information like biometric ID and financial account passwords and are applicable to other AI use cases.\n\n3.5 Management\n\nAI practitioners such as researchers and developers have studied various techniques to improve AI trustworthiness in the aforementioned stages of the data, algorithm, development, and deployment stages. Beyond these concrete approaches, appropriate management and governance provide a holistic guarantee that trustworthiness is consistently aligned throughout the lifecycle of a AI system. In this section, we introduce several executable approaches that facilitate the AI community to improve management and governance on AI trustworthiness.\n\n3.5.1 Documentation.\n\nConventional software engineering has accumulated a wealth of experience in leveraging documentation to assist development. Representative documentation types include requirement documents, product design documents, architecture documents, code documents, and test documents [11]. Beyond conventional software engineering, multiple new types of documents have been proposed to adapt to the ML training and testing mechanisms. Their scope may include the purposes and characteristics of the model [246], datasets [41, 129, 156], and services [22]. As mentioned in Sections 2.3.2 and 2.7, documentation is an effective and important approach to enhance the system’s transparency and accountability by tracking, guiding, and auditing its entire lifecycle [272] and serves as a cornerstone of building a trustworthy AI system.\n\n3.5.2 Auditing.\n\nWith lessons learned from safety-critical industries, e.g., finance and aerospace, auditing has been recently recognized as an effective mechanism to examine whether an AI system complies with specific principles [58, 356]. In terms of the position of auditors, the auditing process can be categorized as internal or external. Internal auditing enables self-assessment and iterative improvement for manufacturers to follow the principles of trustworthiness. It can cover the lifecycle of the system without the leakage of trade secrets [272]. However, external auditing by independent parties is more effective in gaining public trust [58].\n\nAuditing might involve the entire or selective parts of the lifecycle of an AI system. A comprehensive framework of internal auditing can be found in Reference [272]. The means of auditing might include interviews, documented artifacts, checklists, code review, testing, and impact assessment. For example, documentation like product requirement documents, model cards [246], and datasheets [129] serve as important references to understand the principle alignment during development. Checklists are widely used as a straightforward qualitative approach to evaluate fairness [228], transparency [292], and reproducibility [263]. Quantitative testing also serves as a powerful approach and has been successfully executed to audit fairness in, for example, the Gender Shade study [58]. Inspired by the EU’s European Union’s Data Protection Impact Assessment (DPIA), the concept of Algorithmic Impact Assessment has been proposed to evaluate the claims of trustworthiness and discover negative effects [277]. Besides the above representatives, designs of approaches to algorithmic auditing can be found in References [290, 356].\n\n3.5.3 Cooperation and Information Sharing.\n\nAs shown in Figure 2, the establishment of trustworthy AI requires cooperation between stakeholders. From the perspective of industry, cooperation with academia enables the fast application of new technology to enhance the performance of the product and reduce the risk posed by it. Cooperation with regulators certifies the products as appropriately following the principles of trustworthiness. Moreover, cooperation between industrial enterprises helps address consensus-based problems, such as data exchange, standardization, and ecosystem building [27]. Recent practices of AI stakeholders have shown the efficacy of cooperation in various dimensions. We summarize these practices in the following aspects below.\n\nCollaborative research and development. Collaboration has been a successful driving force in the development of AI technology. To promote research on AI trustworthiness, stakeholders are setting-up various forms of collaboration, such as research workshops on trustworthy AI and cooperative projects like DARPA XAI [144].\n\nTrustworthy data exchange. The increasing business value of data raises the demand to exchange them across companies in various scenarios (e.g., the medical AI system in Section 2.6). Beyond privacy-based computing techniques, the cooperation between data owners, technology providers, and regulators is making progress in establishing an ecosystem of data exchange, and solving problems such as data pricing and data authorization.\n\nCooperative development of regulation. Active participation in the development of standards and regulations serves as an important means for academia, the industry, and regulators to align their requirements and situations.\n\nIncident sharing. The AI community has recently recognized incident sharing as an effective approach to highlight and prevent potential risks to AI systems [57]. The AI Incident Database [91] provides an inspiring example for stakeholders to share negative AI incidents so that the industry can avoid similar problems.\n\n3.6 TrustAIOps: A Continuous Workflow toward Trustworthiness\n\nThe problem of trustworthy AI arises from the fast development of AI technology and its emerging applications. AI trustworthiness is not a well-studied static bar to reach by some specific solutions. The establishment of trustworthiness is a dynamic procedure. We have witnessed the evolution of different dimensions of trustworthiness over the past decade [178]. For example, research on adversarial attacks has increased concerns regarding adversarial robustness. The applications of safety-critical scenarios have rendered more stringent the requirements of accountability of an AI system. The development of AI research, the evolution of the forms of AI products, and the changing perspectives of society imply the continual reformulation of the requirements of and solutions to trustworthiness. Therefore, we argue that beyond the requirements of an AI product, the AI industry should consider trustworthiness as an ethos of its operational routine and be prepared to continually enhance the trustworthiness of its products.\n\nThe constant enhancement of AI trustworthiness positions requirements on a new workflow for the AI industry. Recent studies on industrial AI workflow extend the mechanism of DevOps [36] to MLOps [233], to enable improvements in ML products. The concept of DevOps has been adopted in modern software development to continually deploy software features and improve their quality. MLOps [233] and its variants, such as ModelOps [165] and SafetyOps [303], extend DevOps to cover the ML lifecycle of data preparation, training, validation, and deployment, in its workflow. The workflow of MLOps provides a start point to build the workflow for trustworthy AI. By integrating the ML lifecycle, MLOps connects research, experimentation, and product development to enable the rapid leveraging of the theoretical development of trustworthy AI. A wealth of toolchains of MLOps have been released recently to track AI artifacts such as data, model, and meta-data to increase the accountability and reproducibility of products [165]. Recent research has sought to extend MLOps to further integrate trustworthiness into the AI workflow. For example, [303] extended MLOps with safety engineering as SafetyOps for autonomous driving.\n\nAs we have illustrated in this section, building trustworthiness requires the continual and systematic upgrade of the AI lifecycle. By extending MLOps, we summarize this upgrade of practices as a new workflow, TrustAIOps, which focuses on imposing the requirements of trustworthiness over the entire AI lifecycle. This new workflow contains the following properties:\n\n•\n\nClose collaboration between inter-disciplinary roles. Building trustworthy AI requires organizing different roles, such as ML researchers, software engineers, safety engineers, and legal experts. Close collaboration mitigates the gap in knowledge between forms of expertise (e.g., Reference [208], cf., Sections 3.5.3 and A.2).\n\n•\n\nAligned principles of trustworthiness. The risk of untrustworthiness exists in every stage in the lifecycle of an AI system. Mitigating such risks requires that all stakeholders in the AI industry be aware of and aligned with unified trustworthy principles (e.g., Reference [301], cf., Section A.2).\n\n•\n\nExtensive management of artifacts. An industrial AI system is built upon various artifacts such as data, code, models, configuration, product design, and operation manuals. The elaborate management of these artifacts helps assess risk and increases reproducibility and auditability (cf., Section 3.5.1).\n\n•\n\nContinuous feedback loops. Classical continuous integration and continuous development (CI/CD) workflows provide effective mechanisms to improve the software through feedback loops. In a trustworthy AI system, these feedback loops should connect and iteratively improve the five stages of its lifecycle, i.e., data, algorithm, development, deployment, and management (e.g., References [272, 310]).\n\nThe evolution of the industrial workflow of AI is a natural reflection of the dynamic procedure to establish its trustworthiness. By systematically organizing stages of the AI lifecycle and inter-disciplinary practitioners, the AI industry is able to understand the requirements of trustworthiness from various perspectives, including technology, law, and society, and deliver continual improvements.\n\n4 Conclusion, Challenges, and Opportunities\n\nIn this survey, we outlined the key aspects of trustworthiness that we think are essential to AI systems. We introduced how AI systems can be evaluated and assessed on each of these aspects, and reviewed current efforts in this direction in the industry. We further proposed a systematic approach to consider these aspects of trustworthiness in the entire lifecycle of real-world AI systems, which offers recommendations for every step of the development and use of these systems. We recognize that fully adopting this systematic approach to build trustworthy AI systems requires that practitioners embrace the concepts underlying the key aspects that we have identified. More importantly, it requires a shift of focus from performance-driven AI to trust-driven AI. In the short run, this shift will inevitably involve side-effects, such as longer learning time, slowed development, and/or increased cost to build AI systems. However, we encourage practitioners to focus on the long-term benefits of gaining the trust of all stakeholders for the sustained use and development of these systems. In this section, we conclude by discussing some of the open challenges and potential opportunities in the future development of trustworthy AI.\n\n4.1 AI Trustworthiness as Long-term Research\n\nOur understanding of AI trustworthiness is far from complete or universal and will inevitably evolve as we develop new AI technologies and unders"
    }
}
{
    "id": "dbpedia_3937_0",
    "rank": 5,
    "data": {
        "url": "https://www.emerald.com/insight/content/doi/10.1108/lhtn.2008.23925gab.001/full/html",
        "read_more_link": "",
        "language": "en",
        "title": "New &amp; Noteworthy",
        "top_image": "",
        "meta_img": "",
        "images": [
            "https://www.emerald.com/insight/static/img/emerald_publishing_logo-white.svg"
        ],
        "movies": [],
        "keywords": [],
        "meta_keywords": [
            ""
        ],
        "tags": null,
        "authors": [],
        "publish_date": null,
        "summary": "",
        "meta_description": "New &amp; Noteworthy",
        "meta_lang": "en",
        "meta_favicon": "",
        "meta_site_name": "",
        "canonical_link": "https://www.emerald.com/insight/content/doi/10.1108/lhtn.2008.23925gab.001/full/html",
        "text": "New & Noteworthy\n\nArticle Type: New & Noteworthy From: Library Hi Tech News, Volume 25, Issue 7.\n\nUniversity of Prince Edward Island's Robertson Library Goes Live with Evergreen\n\nOn June 4, 2008, Robertson Library at the University of Prince Edward Island in Charlottetown became the first academic library to run Evergreen in production. The story of the lightning migration, from concept to execution in a month, is documented in the blog of Mark Leggott, University Librarian, at http://loomware.typepad.com/\n\nEvergreen is an enterprise-grade open-source ILS initially created to support Georgia PINES, a consortium of over 270 public libraries. Since its debut in September 2006, Evergreen has received significant attention from around the world, including the reception of a Technology Collaboration Award and grant from the Andrew W. Mellon Foundation. Additional Evergreen implementations include a growing consortium in British Columbia, Canada, and new implementations planned for Indiana and Michigan. Evergreen was designed from the ground up to be a flexible, fault-tolerant system capable of supporting libraries of all sizes.\n\nThe Robertson library was built in 1975 and in addition to housing traditional resources (over 300,000 books, etc.), it provides access to the world's digital resources while supporting undergraduate courses and the University's strong research programs, particularly in veterinary medicine. As a part of the library's pioneering efforts in digital resources and in open-source software, it is not a surprise that it becomes the first academic library to run Evergreen.\n\nLeggott observed that “The migration gave us access to a collaborative environment typical of the open-source community and provided staff at UPEI with a great opportunity to see why a move to an open system is the only way to go”.\n\nFurther reflecting on the rapid, one-month migration from the idea to going live with Evergreen, he said: “This quick migration was only possible with coordination by Grant Johnson at PEI and with the incredible support of the Equinox team, Dan Scott, and an open-source community that is helping redefine the information landscape”. Equinox Software Inc. is a firm offering support, custom development and integration services, and complete Evergreen hosting packages for libraries wishing to outsource their ILS infrastructure needs.\n\nEquinox President, Brad LaJeunesse, concurred: “The University of Prince Edward Island's migration shows a new path to implementation of Evergreen and one that is only possible with open-source software. They had the idea, and mostly did the migration themselves with help from Equinox and the Evergreen community. We have been developing tools to speed up migrations to Evergreen but one month from the first phone call to a running ILS is just amazing and a great credit to Mark, Grant, Dan, and the rest of the staff at the Robertson Library as well as Dan and the Evergreen community”.\n\nView the Library's catalogue at: http://islandpines.roblib.upei.ca/\n\nEvergreen home page: http://open-ils.org/\n\nEquinox Software Inc.: http://esilibrary.com/esi/\n\nLibLime and Relais International Announce Partnership\n\nLibLime, a provider of open-source solutions for libraries, and Relais International, Inc., a leader in resource sharing solutions for libraries, have announced a new strategic partnership between the two companies. This development comes shortly after Relais International's announcement of its intent to release the Relais resource sharing and document delivery products under an open-source license.\n\nLibLime's mission is to make open-source accessible to libraries. Rather than sell software licenses for static, hard-to-customize software products, LibLime educates libraries about the benefits of open-source, enabling them to make choices about how best to provide their communities and staff with better technology services. LibLime then facilitates implementation of open-source in libraries by providing outstanding development, customization, support and training solutions tailored to each library's needs.\n\nJoshua Ferraro, CEO of LibLime, notes, “For several years LibLime has fielded questions from consortia, ILL centers and state libraries about the possibility of an open-source resource sharing system. For this reason, we believe Relais International's adoption of an open-source business model is a significant development in the industry. Now, our partnership with Relais gives libraries access to the combined development resources and domain knowledge of both our firms”.\n\nClare MacKeigan, COO of Relais International, says that “together, Relais and LibLime stand poised to facilitate the innovation process for our customers by providing them with the project management, programming and deployment services for creation of a next-generation solution for discovery and resource sharing that will help libraries and patrons get the items they need as quickly as possible with as little effort as possible”.\n\nLibLime and Relais are now accepting proposals from organizations interested in sponsoring creation of an open-source discovery layer and resource sharing system. Such a system could be built from the combination of LibLime's state of the art discovery layer and patron self-service portal together with Relais International's sophisticated resource sharing and document delivery features. Organizations interested in getting involved in this process can find contact information at the LibLime website.\n\nhttp://liblime.com\n\nAOL Acquires Social Media Network BEBO\n\nAOL announced in May 2008 that it had completed its acquisition of Bebo, a global social media network founded in 2005, that will form the centerpiece of AOL's newly created People Networks business unit. Bebo's CEO Joanna Shields will serve as Executive Vice President of AOL and President of People Networks, which combines Bebo, AIM, ICQ, and AOL's other community platforms and reaches about 80 million users worldwide.\n\n“With the addition of Bebo and the creation of People Networks, AOL is uniquely positioned to capitalize on the exploding social media space by delivering a more personal experience for consumers and a better way for advertisers to engage them”, said Randy Falco, Chairman and CEO of AOL. “AOL is now fully focused on growing our business in three key areas – our advertising network, publishing and people networks – by delivering relevant content and advertising across the Web, and we're making great progress in each area”.\n\nOne of the first initiatives for People Networks will be to integrate AOL's other community applications and tools, including IM, chat and mail functionality into Bebo. It will also let users merge AIM and Bebo profiles so they can use common screen names without re-registering. AIM is the leading instant messaging network in the USA, with more than 30 million users. ICQ has 28 million active users worldwide. In addition, People Networks will integrate other recent AOL acquisitions, including widget technology company Goowy Media and social search question and answer service Yedda. Integration plans also include the cross-distribution of AOL and Bebo content and applications to expand the scale of the combined networks.\n\nBebo's Open Application Platform was the first platform that enabled developers to easily port their applications from Facebook to Bebo, reducing the investment required for developers to extend their reach and enriching the user experience for Bebo's members. Since publicly opening up the platform in January 2008, more than 6,500 applications were launched on Bebo. Additionally, AOL recently announced plans to introduce a “content screening” tool. Based on the same technology AOL uses with its parental controls, this tool will help advertisers overcome concerns about advertising on social networking sites with user-generated content by awarding these pages a quality score and placing ads only on pages that surpass a quality threshold. AOL plans to launch its content screening tool in the third quarter.\n\nBebo website: www.bebo.com/\n\nBibTip Recommender System Enriches Online Catalogs\n\nBibTip is a service provided by Karlsruhe University Library that enriches online catalogs (OPACs) by adding a high-performance recommender system. BibTip generates recommendations for a given title by statistical analyzes of OPAC user behavior. Recommendations are presented in the form of a link list guiding the user to similar titles. The recommendations are catalog-specific and therefore tailored to the users of the respective library. It can be easily integrated into the full-title view of any OPAC, with no additional software or hardware required, by adding Javascript code to the full title display.\n\nKarlsruhe University Library charges a small annual fee to refinance the operation, software maintenance and ongoing development of BibTip. The fee depends on the size of the library. The licensing fee includes the unrestricted use of BipTip in the OPAC of the library.\n\nMore information on the BibTip recommender system is provided in the freely available article by Dr Michael Monnich and Marcus Spiering in the May/June 2008 D-Lib Magazine: www.dlib.org/dlib/may08/monnich/05monnich.html\n\nRecommender Systems for Meta Library Catalogs Project: www.em.uni-karlsruhe.de/research/projects/reckvk/index.php?language=en BibTip website (in German): www.bibtip.org/\n\nSerials Solutions Announces SUSHI Support in 360 Product\n\nIn May 2008 Serials Solutions announced that it had successfully tested and deployed SUSHI, the Standardized Usage Statistics Harvesting Initiative, with its 360 Counter e-resource assessment service. Support for SUSHI in 360 Counter will allow librarians to bypass time-consuming, manual data aggregation of usage statistics and to more easily analyze a title's usage and its cost per-use.\n\nThe SUSHI protocol was developed to allow librarians to circumvent the process of downloading, aggregating, and formatting COUNTER-compliant usage reports from vendor websites. Many libraries do not have the staffing to manually retrieve their usage statistics, yet they still need the data to evaluate their e-resource expenditures. SUSHI enables Serials Solutions to retrieve COUNTER reports directly from vendors and upload them to 360 Counter. SUSHI-enhanced 360 Counter means that the customer may go straight to usage analysis and bypass the manual labor.\n\nFrom 2009 it is planned that SUSHI implementation will be a requirement for vendors to be compliant with Release 3 of the COUNTER Code of Practice. Serials Solutions has completed this development now to make sure its customers can utilize SUSHI immediately as publishers implement the standard.\n\nAdam Chandler, Coordinator of the Service Design Group at Cornell University and a member of the Project COUNTER Executive Committee is one of the originators of the SUSHI standard. Adam says, “We're working very diligently with content providers to implement the SUSHI standard. It's important for vendors to get involved in order to help librarians evaluate their purchase and collection-building decisions. CSV and Excel files only take us so far as containers for transferring usage data. We need to scale up the whole supply chain to support automated transfer of usage data in structured XML”.\n\nThe SUSHI protocol support service is a free opt-in upgrade for all 360 Counter subscribers. For more information about 360 Counter and Serials Solutions' adoption of the SUSHI schema, email: 360@serialssolutions.com\n\nSUSHI protocol: www.niso.org/workrooms/sushi\n\nSerials Solutions website: www.serialsolutions.net\n\nBook Lending Machines Program Launched in California\n\nIn February 2008 the Contra Costa County (Calif.) Library announced it was working with the Bay Area Rapid Transit (BART) organization to expand services and improve accessibility by making its popular collection available to people at BART stations and local shopping centers. The Contra Costa County Library began to offer the public book lending machines under a new program called Library-a-Go-Go. The first Library-a-Go-Go machine was scheduled for service at the Pittsburg/Bay Point Bart station in May 2008.\n\nLibrary-a-Go-Go is an automated book/av dispensing machine with a self-contained collection of books that circulates materials to Contra Costa County Library card holders. A touch screen, similar to an ATM screen, is used to select from up to approximately 500 items that are delivered through an opening in the front of the unit. Materials are returned in a similar way. A robotic arm controls the selection and data input creates attractive screen displays with book jacket images, reviews, and tables of content and other enhanced content information.\n\nAnyone with a Contra Costa County Library card will be able to check out and return books. “Pittsburg/Bay Point riders won't have to use their cars to make the extra trip to pick up a good book to read on BART”, said BART District 2 Director Joel Keller. “Library-a-Go-Go offers passengers a library experience that is fast, available at convenient hours, easy to use and customer focused”.\n\nPhase One plans call for three other locations, including the transit village at the BART station in Pleasant Hill, a site in Byron/Discovery Bay and a fourth location as yet to be decided. This innovative technology will deliver services where currently none exist in the far East County communities of Discovery Bay, Byron, Knightsen and Bethel Island, without the expense of a new library building.\n\nThe program is supported by grants from the California State Library and the Bay Area Library and Information System and Baker & Taylor, Inc.\n\nLibrary-A-Go-Go: www.contra-costa.lib.ca.us/locations/libraryagogo.html\n\nDigital Lives: Pathfinding Study of Personal Digital Collections\n\nFor centuries, and indeed millennia, individuals have used physical artifacts as personal memory devices and reference aids. In the twentieth century, these typically ranged from personal journals and correspondence, through photographic albums, sound recordings and cine films, to personal compilations of books, serials, clippings, offprints and observational data. These personal collections and archives are often of profound importance to individuals and their descendents, and of immense value to research in a broad range of arts and humanities such as literary criticism and history as well as in the social, human, and natural sciences.\n\nAs we move from a memory based on physical artifacts, to a hybrid digital and physical environment, and then increasingly shift towards new forms of digital memory, many fundamental new issues arise for research institutions such as the British Library that will be the custodians of and provide research access to digital archives and personal collections created by individuals in the twenty-first century.\n\nDigital Lives is a research project focusing on personal digital collections and their relationship with research repositories. It brings together expert curators and practitioners in digital preservation, digital manuscripts, literary collections, web archiving, history of science, and oral history from within the British Library (one of the world's leading research libraries) with researchers in the School of Library, Archive and Information Studies at University College London, and the Center for Information Technology and Law at the University of Bristol. The Digital Lives Research Project is designed to provide a major pathfinding study of personal digital collections. The project team drawn from the British Library, University College London and University of Bristol is led by Dr Jeremy Leighton John of the British Library (the lead partner) and funded by the Arts and Humanities Research Council (AHRC).\n\nThe research partners expect that the findings of the project will be applicable to a wide range of individuals and research repositories. Personal archives form a substantial and essential part of the collections of national and regional libraries and archives that are widely used by academic faculty and students and diverse independent researchers.\n\nThe project is considering not only how collections currently being deposited are changing but also the fate of the research collections of the future being created now and implications for collection development and practice. The research for Digital Lives commenced in September 2007 and will run for 18 months to March 2009, with dissemination continuing until June 2009.\n\nDigital Lives Research Project: www.bl.uk/digital-lives/\n\nClearText Technology from ICDL Enhances Readability of Online Books\n\nThe International Children's Digital Library (ICDL) Foundation, the world's largest collection of children's literature available freely on the internet, has announced the completion and implementation of its ClearText technology which significantly enhances the translation and readability of the books available from the online library.\n\nFor easier reading of scanned books on a small screen, ClearText allows the user to simply click the desired text to display a magnified version of that text in place, or to read that page in a different language, the user just selects the desired language from a list under the page. The novel book reader technology was developed in-house at ICDL by Dr Ben Bederson, library co-founder and Chief Technology Officer, working closely with a team from the Human-Computer Interaction Lab at the University of Maryland.\n\nFor the translation feature, children reading at the ICDL can select the language of their choice at the bottom of each page. As for readability, the text provided by the ClearText technology is sharper than before and will “pop out” to enlarge as needed. Text can even be read with a screen reader to support visually impaired readers. The book reader allows users to see a different version of the text in place and enables the text size to be changed or read aloud using a standard screen reader. It works by visually removing the text from the original image of the book, and then using the Web browser to display the text on top of the image of the book.\n\nAdditionally, the ClearText technology allows for users of the library to have increased options in selecting a language in which to read a book. For example, thanks to ClearText, Croatian author Andrea Petrlik's moving book The Blue Sky is currently available in three languages. In addition to the technology improvements, a massive translation project is currently underway, being conducted by more than 1,200 online volunteer translators. Once a book is translated, there is a second review to validate the translation and ensure accuracy.\n\n“We are constantly working to expand the library and increase its relevance worldwide”, said Executive Director of the International Children's Digital Library, Tim Browne. “The ClearText application was developed specifically for the ICDL and makes it possible for more children from more countries to enjoy more books. We are delighted to unveil what we view as our most significant advancement to date”.\n\nInternational Children's Digital Library: www.childrenslibrary.org\n\nLibrary of Congress (LC) and California Digital Library (CDL) Develop Format for Transferring Digital Content\n\nIn June 2008 the LC and the CDL announced they had jointly developed a format for transferring digital content. The BagIt format specification is based on the concept of “bag it and tag it”, where digital content is packaged (the bag) along with a small amount of machine-readable text (the tag) to help automate the content's receipt, storage and retrieval. There is no software to install.\n\nBagIt is an attempt to simplify large-scale data transfers between cultural institutions. It builds on the success of the Library's data-transfer projects, including the Archive Ingest and Handling Test and the San Diego Supercomputer Center data-storage project. BagIt streamlines the process and reduces the number of “moving parts”.\n\nThough data transfer can be done by network or disk, LC expects to receive more and more digital collections over the network. Conforming to the BagIt format will guarantee users a smooth, orderly transfer, with little or no human intervention. John Kunze, Preservation Technologies Architect at the California Digital Library and one of the principle authors of BagIt, said, “It really makes it easy for another institution to hold onto some of our content, and that's reassuring”.\n\nA bag consists of a base directory, or folder, containing the tag and a subdirectory that holds the content files. The tag is a simple text-file manifest, like a packing slip, that consists of two elements:\n\n•\n\n(1) An inventory of the content files in the bag.\n\n•\n\n(2) A checksum for each file.\n\nA checksum is a way to validate that everything in the bag arrived OK. Once the content is validated, the receiver emails the sender a confirmation receipt.\n\nThis example shows a typical line from a manifest:\n\n408ad21d50cef31da4df6d9ed81b01a7data/docs/example.doc\n\nEach line contains a string of characters – the checksum – followed by the name of the file (“example.doc”) and its directory path.\n\nA slightly more sophisticated bag lists URLs instead of simple directory paths. A script then consults the tag, detects the URLs and retrieves the files over the internet, ten or more at a time. This type of simultaneous multiple transfer greatly reduces the overall data-transfer time.\n\nIn another optional file, users can add metadata about the content, such as a description of the package contents and detailed contact information for the sender.\n\nThe full BagIt specification is available at: www.cdlib.org/inside/diglib/bagit/bagitspec.html\n\nOAI Announces Public Beta Release of Object Reuse and Exchange Specifications\n\nOver the past eighteen months the Open Archives Initiative (OAI), in a project called Object Reuse and Exchange (OAI-ORE), has gathered international experts from the publishing, web, library, and eScience community to develop standards for the identification and description of aggregations of online information resources. These aggregations, sometimes called compound digital objects, may combine distributed resources with multiple media types including text, images, data, and video. The goal of these standards is to expose the rich content in these aggregations to applications that support authoring, deposit, exchange, visualization, reuse, and preservation. Although a motivating use case for the work is the changing nature of scholarship and scholarly communication, and the need for cyberinfrastructure to support that scholarship, the intent of the effort is to develop standards that generalize across all web-based information including the increasing popular social networks of “web 2.0”.\n\nThe beta version of the OAI-ORE specifications and implementation documents were released to the public on June 2, 2008. These documents describe a data model to introduce aggregations as resources with URIs on the web. They also detail the machine-readable descriptions of aggregations expressed in the popular Atom syndication format, in RDF/XML, and RDFa. The table of contents page with links to the following other documents is located at: www.openarchives.org/ore/toc\n\nORE User Guide Documents\n\nPrimer; Resource Map Implementation in Atom; Resource Map Implementation in RDF/XML; Resource Map Implementation in RDFa; HTTP Implementation and Multiple Serializations; and Resource Map Discovery.\n\nORE Specification Documents\n\nAbstract Data Model; Vocabulary; and Representing Resource Maps Using the Atom Syndication Format.\n\nA forum for feedback on this beta release is at: http://groups.google.com/group/oai-ore. This feedback and further consultation with the OAI-ORE community will be considered in the evolution of this beta release to a production release scheduled for September 2008.\n\nOAI website: www.openarchives.org\n\nOCLC's Digital Archive Offers Long-Term Storage of Libraries' Digital Collections\n\nOCLC is now providing a Digital Archive service for long-term storage of originals and master files from libraries' digital collections. The service provides a secure storage environment for libraries to easily manage and monitor master files and digital originals. The importance of preserving master files grows as a library's digital collections grow. Libraries need a workflow for capturing and managing master files that finds a balance between the acquisition of both digitized and born-digital content while not outpacing a library's capability to manage these large files.\n\nThe Digital Archive service is simplified to fit with a variety of digital library workflows and to keep the costs of safely storing these important files within the budget of a library's digital program. The service will provide automated monitoring and reports on stored digital collections.\n\nOCLC has been leading preservation efforts in the library community with digital archive services since 2001. The Digital Archive service builds on that experience. OCLC has integrated the service to fit typical workflows for building and managing digital collections.\n\n“The Montana Historical Society has chosen the Digital Archive service as the storage facility for our digital collections”, said Molly Kruckenberg, Research Center Director. “The ease of adding materials through Connexion and the secure, managed storage make the Digital Archive service the ideal solution for our needs”. Connexion is the OCLC tool that allows catalogers to perform original and copy cataloging with WorldCat, the world's most comprehensive bibliographic database.\n\nThe Digital Archive service is a specially designed system in a controlled operating environment dedicated to the ongoing managed storage of digital content. OCLC has developed specific systems processes and procedures for the service tuned to the management of data for the long term.\n\nFrom the time content arrives, the Digital Archive systems begin inspecting it to ensure continuity. OCLC systems perform quality checks and record the results in a “health record” for each file. Automated systems revisit these quality checks periodically so libraries receive up-to-date reports on the health of the collection. OCLC provides monthly updated information for all collections on the personal archive report portal.\n\nFor users of CONTENTdm, OCLC's digital collection management software for libraries and other cultural heritage institutions, the Digital Archive service is an optional capability integrated with various workflows for building collections. Master files are secured for ingest to the Digital Archive service using the CONTENTdm Acquisition Station, the Connexion digital import capability and the Web Harvesting service.\n\nFor users of other content management systems, the Digital Archive service provides a low-overhead mechanism for safely storing master files.\n\nOCLC Digital Archive Service: www.oclc.org/us/en/digitalarchive/\n\nMicrosoft Ceases Book Digitization Project and Retires Live Search Books\n\nIn May 2008 Microsoft informed their partners that they were ending the Live Search Books and Live Search Academic projects and that both sites would be taken down the next week. Books and scholarly publications would continue to be integrated into the Microsoft Search results, but not through separate indexes.\n\nMicrosoft also announced they were ceasing their digitization initiatives, including the library scanning and the in-copyright book programs. However they encouraged their partner libraries to continue to build on the platform they had developed with Kirtas, the internet Archive, CCS, and others to create digital archives available to library users and search engines.\n\nPer the posting on the Microsoft Live Search blog the following rationale was given for the decision:\n\nGiven the evolution of the Web and our strategy, we believe the next generation of search is about the development of an underlying, sustainable business model for the search engine, consumer, and content partner. For example, this past Wednesday we announced our strategy to focus on verticals with high commercial intent, such as travel, and offer users cash back on their purchases from our advertisers. With Live Search Books and Live Search Academic, we digitized 750,000 books and indexed 80 million journal articles. Based on our experience, we foresee that the best way for a search engine to make book content available will be by crawling content repositories created by book publishers and libraries. With our investments, the technology to create these repositories is now available at lower costs for those with the commercial interest or public mandate to digitize book content. We will continue to track the evolution of the industry and evaluate future opportunities.\n\nOn the up-side for libraries, the previously digitized content had its contractual restrictions removed:\n\nWe have learned a tremendous amount from our experience and believe this decision, while a hard one, can serve as a catalyst for more sustainable strategies. To that end, we intend to provide publishers with digital copies of their scanned books. We are also removing our contractual restrictions placed on the digitized library content and making the scanning equipment available to our digitization partners and libraries to continue digitization programs. We hope that our investments will help increase the discoverability of all the valuable content that resides in the world of books and scholarly publications.\n\nMicrosoft blog posting: http://blogs.msdn.com/livesearch/archive/2008/05/23/book-search-winding-down.aspx\n\nNew Tools for Creating Interest in Changes in Scholarly Communication\n\nThe Association of College and Research Libraries, the Association of Research Libraries, and Scholarly Publishing and Academic Resources Coalition have released a new series of bookmarks in the Create Change campaign, which targets scholars in different disciplines with messages about the benefits of wider research sharing. Librarians can use these freely available files to enhance their efforts to engage faculty interest in changing the way scholarly information is shared.\n\nThe Create Change Web site emphasizes the rapid and irreversible changes occurring in the ways faculty share and use academic research results. The site outlines how the advancement of knowledge is fueled by accelerating and enhancing sharing – of journal articles, research data, simulations, syntheses, analyzes, and other findings. Create Change offers faculty practical ways to look out for their own interests as researchers and delivers the personal perspectives of scholars in ten different disciplines, from music therapy to chemistry to microbiology, on the benefits of sharing. New interviews are added regularly.\n\nThe first bookmarks highlight comments from four researchers: Dr Linda Hutcheon, Professor of English, University of Toronto; Dr David Morrison, Professor of Mathematics, University of California, Santa Barbara; Dr Carolyn Kenny, Professor of Human Development, Antioch University; and Dr Gary Ward, Professor of Microbiology, University of Vermont. Comments are drawn from full-length interviews published on the Create Change Web site and target the advantages of depositing works in a digital repository, the ways communication should change in the digital environment, the impact of open access, and how to maximize scientific progress.\n\nLibraries are invited to print directly from the Web site in the easy-to-use letter-size format or to download the bookmarks and modify them as needed for their campus, e.g. insert your logo and contact information or add examples from your faculty and their disciplines. For more details and to download, visit the Create Change Web site at: www.createchange.org/about/downloads.shtml\n\nCreate Change: www.createchange.org/\n\nParticipative Web and User-created Content (UCC): New Report from Organization for Economic Co-operation and Development (OECD)\n\nThe internet is becoming increasingly embedded in everyday life. Drawing on an expanding array of intelligent web services and applications, a growing number of people are creating, distributing and exploiting UCC and being part of the wider participative web.\n\nThis freely available study from OECD Publishing was published in October 2007 and describes the rapid growth of UCC and its increasing role in worldwide communication, and draws out implications for policy. This report was one of a series on digital broadband content prepared since 2005, focusing on changing value chains and developing business models and the implications for policy. The series is part of ongoing OECD analysis of the digital economy and information and communications policy. The report was drafted by Sacha Wunsch-Vincent and Graham Vickery of the OECD Directorate for Science, Technology and Industry as part of the digital content series.\n\nQuestions addressed include: What is UCC? What are its key drivers, its scope and different forms? What are the new value chains and business models? What are the extent and form of social, cultural and economic opportunities and impacts? What are the associated challenges? Is there a government role, and what form could it take?\n\nThe report covers:\n\nDefining and Measuring the Participative Web and UCC; Drivers of UCC; Types of UCC and Distribution Platforms; Emerging Value Chains and Business Models; Economic and Social Impacts; and Opportunities and Challenges for Users, Business and Policy.\n\nReport (pdf): http://213.253.134.43/oecd/pdfs/browseit/9307031E.PDF\n\nVirtual Worlds Research: New Open Access Journal Available\n\nThe Journal of Virtual Worlds Research is an online, open access peer-reviewed academic journal that engages established and emerging scholars from anywhere in the world. Virtual worlds are three-dimensional computer-mediated environments that offer rich visual interfaces and real-time communication with other residents. Consumers enter these worlds in the form of avatars – online digital personas that they create. The Journal of Virtual Worlds Research is a transdisciplinary journal that engages a wide spectrum of scholarship and welcomes contributions from the many disciplines and approaches that intersect virtual worlds research. The first edition of the Journal will be released on July 1, 2008.\n\nThere is currently a call for papers for a special issue on Social Identity and Consumer Behavior in Virtual Worlds. Deadline for submissions is August 15, 2008.\n\nThe Editorial Team includes: Jeremiah Spence, University of Texas at Austin, Editor, and Associate Editors: Mark Bell, Indiana University, Sun Sun Lim, National University of Singapore, Suely Fragoso, Universidade do Vale do Rio do Sinos/Unisinos, Brazil, Joe Sanchez, University of Texas at Austin, Amanda Salomon, Smart Internet Technology CRC/Swinburne University of Technology, Australia, Henry Segerman, University of Texas at Austin, Yesha Y. Sivan, Shenkar College & Metaverse Labs, Israel, Stephanie Smith, NASA JSC Learning Technologies, Caja Thimm, University of Bonn, Germany.\n\nJournal website: http://jvwresearch.org\n\nUNC Students Use Social Computing Room for “Illuminating” Experience\n\nThe Social Computing Room at the Renaissance Computing Institute's (RENCI) UNC Chapel Hill engagement center is not your typical classroom, but it was the perfect environment for final exams for a class in the UNC Chapel Hill art department. On May 6, 2008 the room, which uses 12 projectors to create a 360-degree display for virtual, immersive and interactive experiences, hosted the student exhibit “Luminescence”, a conglomeration of digital media art projects created by 11 students in David Tinapple's Advanced Digital Media Studio. Luminescence was both an art exhibit and a final project for these students, and the Social Computing Room, which accommodates directed audio and high definition video on all four walls, was the perfect setting to experience their work.\n\n“As a teaching environment, RENCI's Social Computing Room was a powerful motivator and helped the students to see the concept of video and interaction in a new light”, said Tinapple, an art department instructor. “The collaboration across disciplines, artists working with visualization specialists, was a pleasure and rewarding. Together we pulled it off technically and the work looked great. The art exhibit was a success”.\n\nLuminescence was the culmination of a semester's worth of coursework in digital media production. The class was a hands-on lab, in which students learned how to work with the latest tools of interactive multimedia in the context of contemporary digital media art, such as interactive video installations, multimedia authoring, interactive media programming, and robotic camera platforms. Their multimedia projects were created with Max/MSP/Jitter, graphical programming software used in installation and performance art, computer music, theater, video DJ performance, data visualization, robotics, and more. The software's visual programming approach is well suited for artists and musicians, fostering experimentation and the rapid construction of complex and rich interactive audiovisual systems. Under the direction of Tinapple, the students developed their work and then collaborated with RENCI visualization experts.\n\nThe Renaissance Computing Institute brings together teams of researchers, engineers, technologists and leaders in government, business, the arts and humanities to attack major research questions and community issues in ways that accelerate discovery and drive innovation. RENCI has nationally significant expertise and capabilities in high performance computing, visualization, collaborative tools, networking, device prototyping, and data systems as well as engagement sites across the state. Founded in 2004 as a major collaborative venture of Duke University, North Carolina State University, the University of North Carolina at Chapel Hill and the state of North Carolina, RENCI is a statewide virtual organization.\n\nRENCI website: www.renci.org\n\nRENCI Virtual Collaborative Environments: www.renci.org/focusareas/viz/environments.php\n\nLuminescence Project: http://digital.art.unc.edu/luminescence_show/\n\nNational E-Books Observatory Project Examines Demand for E-Textbooks\n\nThe level of potential demand for electronic versions of course textbooks in British universities is a completely unknown quantity. Publishers are unsure about this new market due to a lack of evidence about demand and concerns over the potential impact on print sales. Librarians and publishers are also unsure about which kinds of pricing and licensing models are needed.\n\nThe aim of the national e-books observatory project is to find out what happens when a selection of key course texts are made available electronically to the whole of the UK higher education community. The project, funded by JISC, will provide the first national evidence base concerning student demand for e-textbooks. It should go a long way to helping all the stakeholders (librarians, publishers, faculty, and students) to gain a much better understanding of the issues.\n\nThe national e-books observatory project will assess the impacts, observe behaviors and develop new models to stimulate the e-books market, and do all this in a managed environment. The specific aims of the project:\n\n•\n\n(1) License a collection of online core reading materials that are highly relevant to UK higher education taught course students in four discipline areas: Business and Management studies, Engineering, Medicine (not mental health or nursing), and Media Studies.\n\n•\n\n(2) Achieve a high level of participation in the project by making the e-books available on the bidders own platform (where appropriate) and on a variety of e-book aggregator platforms. Higher education institutions will thus have the option to access the e-books on platforms they already use and which are familiar to their users.\n\n•\n\n(3) Evaluate the use of the e-books through deep log analysis (DLA) and to asses the impact of the “free at the point of use” e-books upon publishers, aggregators and libraries.\n\n•\n\n(4) Transfer knowledge acquired in the project to publishers, aggregators, and libraries to help stimulate an e-books market that has appropriate business and licensing models.\n\nCIBER, an independent publishing and new media think tank, will be evaluating the JISC Observatory Project over the period January 2008 to March 2009 and building up a detailed picture of how e-textbooks are used and the impacts of this new form of library provision in print sales. The evidence base will comprise large-scale online surveys, interviews and focus groups and DLA.\n\nDLA examines the raw server logs from the e-book providers. The logs have no statistical filters on them; they are the raw logs and they include a lot of very interesting information on the information seeking behavior of users – how do they navigate? how long they spend in an e-book? what do they do? – their digital fingerprint through the books and platforms. The data is then related to user characteristics and institutional profiles and patterns arising from all this information are identified. To find out why the patterns exist questionnaires and interviews are carried out with staff and students in institutions and a real understanding of user behavior is developed.\n\nThe findings from the first user survey undertaken as part of the DLA study are now available. This online survey was designed and piloted by members of the CIBER team at University College London with input from the JISC national e-books observatory project board. The survey ran between January 18 and March 1, 2008, over which period 22,437 full or partial responses were received. Data collection ceased when the target of 20,000 full completions was reached. The fact that 89.1 per cent of respondents managed to get to the end of a quite long and complex questionnaire is a clear indication of the level of interest within the academic community in e-books. The report is available at: www.jiscebooksproject.org/wp-content/e-books-project-first-user-survey-a4-final-version.pdf\n\nJISC national e-books observatory project: www.jiscebooksproject.org/\n\nE-books observatory evaluation home page: www.ucl.ac.uk/slais/research/ciber/observatory/\n\nThe Future of Bibliographic Control: LC Response Released\n\nDeanna Marcum, Associate Librarian for Library Services, LC, has released LC's response to On the Record: Report of The Library of Congress Working Group on the Future of Bibliographic Control. From the introduction:\n\nOn the basis of this internal analysis, the LC accepts and endorses the recommendations of the Working Group on the Future of Bibliographic Control. We are eager to work with colleagues nationally and internationally to achieve the vision that is so compellingly drawn in On the Record. This response is not an official program statement from the LC, nor is it an implementation plan. It is an endorsement of the concepts proposed by the Working Group and the Library's current thinking about actions that can be taken immediately. Included in this document for each recommendation in On the Record are LC's response and rationale and a brief summary of current and planned actions.\n\nFull text of the LC response: www.loc.gov/bibliographic-future/news/LCWGResponse-Marcum-Final-061008.pdf\n\nSTM Releases Position Paper on Digital Copyright Exceptions\n\nIn June 2008 the International Association of STM Publishers (STM) released its position paper on digital copyright exceptions and limitations for education and research and also took the opportunity to comment on the recently released US Section 108 Study Group Report on digital library exceptions. STM commended the report by the US Copyright Office of the Section 108 Study Group, which was released in March 2008 and which deals with digital exceptions for libraries, for its serious and reasoned approach to these issues.\n\n“Any proposed exceptions and limitations for education and research dealing with STM materials must avoid distorting the vital and trusted system for communicating science. STM recognizes that archival needs, support for access by the visually disabled and interlibrary copying of rare materials, for example, are important functions for scholars, researchers and archivists. STM is willing to participate in formulating either legislation or sector-by-sector agreements on these issues”, said Michael Mabe, STM's CEO.\n\nSTM indicated that it was also willing to participate in discussions on individual or collective license schemes for “on the premises” viewing of digital archives, electronic course-packs and orphan works usage clearances. With respect to the Section 108 Group Report, STM agreed on matters such as mandates for digital preservation where commercial copies are not immediately available, on limited numbers of copies for archiving purposes and for the ability of libraries to “refresh” digital items to accommodate technological change. STM publishers have been working pro-actively with national libraries (including the Royal Library of the Netherlands) and non-profit organizations such as Portico for long-term archiving projects and supports enabling access to STM materials for those with visual disabilities through cooperation with relevant local or national authorities or specialized organizations.\n\nIn matters of exceptions for digital interlibrary loans, however, STM's position differed from that of the Section 108 Report. STM's view is that with so much STM content available electronically and on a transactional basis, the presumed scarcity of scholarly materials is inapplicable. While there may be a scholarly need for a non-commercial and educational library to make a digital copy of unique and rare scholarly material for another non-commercial and educational institution, this should be limited to material that is not commercially available in the geographic territory of the “requesting” institution. Although the Section 108 Report Group did not address matters such as course-packs, which the STM Position Paper does, STM believed that this matter must be addressed in voluntary or collective licensing agreements. Similarly, on-premises access to archived content, and the use of so-called “orphan works” should also remain the subjects of licensed solutions.\n\nSTM Position Paper: www.stm-assoc.org/documents-statements-public-co\n\nSection 108 Report: www.section108.gov/\n\nGL Compendium: News Report on Grey Literature\n\nGL Compendium (GLC) is a quarterly news report on grey literature. GLC grew out of GreySource, a popular web-based product. However, while GLC maintains the same classification scheme, it moves beyond simple hyperlinking by tunnelling deeper into the document and content level of grey literature.\n\nGLC is designed to guide practitioners and inform researchers and educators in their search for and access to grey literature. It is an international, multi-disciplinary one-stop source on valuable grey literature. It taps into web-based resources and collections, harvests quality information and makes it available on your desktop in digital format. The GLC expands and enhances GreyNet's service to practitioners – librarians, subject specialists, policy and decision makers, information managers, and others. GLC provides a unique current awareness resource for LIS professionals. A subscription is required to access GLC.\n\nGLC: www.greynet.org/glcompendium.html"
    }
}
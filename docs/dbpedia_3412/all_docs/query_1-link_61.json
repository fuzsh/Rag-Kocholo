{
    "id": "dbpedia_3412_1",
    "rank": 61,
    "data": {
        "url": "https://dl.acm.org/doi/fullHtml/10.1145/3581641.3584037",
        "read_more_link": "",
        "language": "en",
        "title": "The Programmer's Assistant: Conversational Interaction with a Large Language Model for Software Development",
        "top_image": "https://dl.acm.org/cms/attachment/html/10.1145/3581641.3584037/assets/html/images/iui23-7-fig1.jpg",
        "meta_img": "",
        "images": [
            "https://dl.acm.org/cms/attachment/html/10.1145/3581641.3584037/assets/html/images/iui23-7-fig1.jpg",
            "https://dl.acm.org/cms/attachment/html/10.1145/3581641.3584037/assets/html/images/iui23-7-fig2.jpg",
            "https://dl.acm.org/cms/attachment/html/10.1145/3581641.3584037/assets/html/images/iui23-7-img1.svg",
            "https://dl.acm.org/cms/attachment/html/10.1145/3581641.3584037/assets/html/images/iui23-7-img2.svg",
            "https://dl.acm.org/cms/attachment/html/10.1145/3581641.3584037/assets/html/images/iui23-7-fig3.jpg",
            "https://dl.acm.org/cms/attachment/html/10.1145/3581641.3584037/assets/html/images/iui23-7-img3.svg",
            "https://dl.acm.org/cms/attachment/html/10.1145/3581641.3584037/assets/html/images/iui23-7-img4.svg",
            "https://dl.acm.org/cms/attachment/html/10.1145/3581641.3584037/assets/html/images/iui23-7-img5.svg",
            "https://dl.acm.org/cms/attachment/html/10.1145/3581641.3584037/assets/html/images/iui23-7-img6.svg",
            "https://dl.acm.org/cms/attachment/html/10.1145/3581641.3584037/assets/html/images/iui23-7-img7.svg",
            "https://dl.acm.org/cms/attachment/html/10.1145/3581641.3584037/assets/html/images/iui23-7-img8.svg",
            "https://dl.acm.org/cms/attachment/html/10.1145/3581641.3584037/assets/html/images/iui23-7-img9.svg",
            "https://www.acm.org/binaries/content/gallery/acm/publications/cc-by/cc-by-nc-nd.jpg"
        ],
        "movies": [],
        "keywords": [],
        "meta_keywords": [
            ""
        ],
        "tags": null,
        "authors": [
            "Steven I. Ross",
            "Visual AI Lab",
            "IBM Research AI",
            "United States",
            "steven_ross@us.ibm.com",
            "Fernando Martinez",
            "IBM Argentina",
            "martferc@ar.ibm.com",
            "Stephanie Houde",
            "stephanie.houde@ibm.com"
        ],
        "publish_date": null,
        "summary": "",
        "meta_description": "",
        "meta_lang": "en",
        "meta_favicon": "",
        "meta_site_name": "",
        "canonical_link": null,
        "text": "1 INTRODUCTION\n\nSoftware development is a highly skilled task that requires knowledge, focus, and creativity [27, 28]. Many techniques have been developed to enhance the productivity of software engineers, such as advanced code repositories [86], knowledge repositories [39], Q&A sites [1], and pair programming practices [18]. Collaborative software engineering is especially promising, given that professional software development is rarely a solo activity and relevant knowledge and expertise are typically distributed widely within an organization [68]. Many efforts have focused on incorporating collaborative technologies into software development environments (e.g. [8, 25, 26, 58, 101]).\n\nThe pioneering work of Rich and Waters on The Programmer's Apprentice [70] presented a novel concept of a knowledgeable automated assistant – in effect, an artificial collaborative partner – that could help software engineers with writing code, designing software systems, and creating requirements specifications. At the time, AI technologies and computing resources were not sufficient to fully implement their vision. In the intervening years, an increase in computational power, the availability of large corpora of language and code data, and the development of deep neural networks have made new approaches to achieving their goals worth exploring.\n\nRecently, models leveraging the transformer architecture [96] have been developed to perform domain-specific software engineering tasks, such as translating code between languages [75], generating documentation for code [36, 38, 97, 98], and generating unit tests for code [92] (see Talamadupula [90] and Allamanis et al. [5] for surveys). Recently developed foundation models – large language models that can be adapted to multiple tasks and which exhibit emergent behaviors for which they have not been explicitly trained [14] – have also proven to be capable with source code. While the intent of training LLMs such as GPT-2 [64] and GPT-3 [17] was to give them mastery of natural language, it quickly became apparent that the presence of code in their training corpora had given them the ability to generate code based on natural language descriptions [49]. The Codex model [24] was then produced by fine-tuning GPT-3 on a large corpus of source code data, leading to the development of Copilot [32], a tool that helps software engineers by autocompleting code as it is being written. Experimentation with Copilot has shown its ability to perform additional tasks, such as explaining code, generating documentation, and translating code between languages [6].\n\nAlthough autocompletion interfaces are useful and valuable when the system can discern the developer's intent, there are many instances where that is insufficient. For example, the developer may have a good idea of what they want to do, but may be unclear on what functions, libraries, or even algorithms to employ. They may even have general programming questions that need to be answered before they are able to write any code.\n\nIn this paper, we seek to understand whether modern developments in code-fluent foundation models – large language models that have been fine-tuned on source code data – are sufficient to support a conversational agent that can act as an assistant in the software development process. We developed the Programmer's Assistant to explore the capabilities that conversational interaction could enable and the extent to which users would find conversational assistance with programming tasks desirable and useful.\n\nWe hypothesize that a conversational system may provide a flexible and natural means for interacting with a code-fluent LLM. Conversational interaction could enable users to pursue their questions in a multiple exchange dialog (as observed by Barke et al. [13]) that allows them to ask follow-up questions and refine their inquiries. A conversational programming assistant could ask the user clarifying or disambiguating questions to help it arrive at the best answer. It could also provide multiple types of assistance to the user beyond simply generating code snippets, such as engaging in general discussion of programming topics (e.g. [22, 71]) or helping users improve their programming skills (as observed in other studies of automating technologies [99]).\n\nOur paper makes the following contributions to the IUI community:\n\nWe provide empirical evidence that a conversational programming assistant based on a state-of-the-art, code-fluent foundation model provides valuable assistance to software engineers in a myriad of ways: by answering general programming questions, by generating context-relevant code, by enabling the model to exhibit emergent behaviors, and by enabling users to ask follow-up questions that depend upon their conversational and code contexts.\n\nWe show how different interaction models – conversation, direct manipulation, and search – provide complementary types of support to software engineers with tradeoffs between the user's focus and attention, the relevance of support to their code context, the provenance of that support, and their ability to ask follow-up questions.\n\nWe motivate the need to further understand how to design human-centered AI systems that enhance the joint performance of the human-AI collaboration.\n\n3 THE PROGRAMMER'S ASSISTANT\n\nIn order to explore conversational programming assistance, we created a functional prototype system called The Programmer's Assistant. Our prototype, shown in Figure 1, combines a code editor with a chat interface. The code editor was implemented using the Microsoft Monaco Editor embedded in a React wrapper . The chat user interface was implemented using the React-Chatbot-Kit framework. To drive the conversational interaction, we employed OpenAI's Codex model [24], accessed through its web API.\n\nWe developed our prototype as a lightweight coding environment in order to examine the user experience of interacting with a conversational assistant. Our work was exploratory in nature, and thus we did not have specific design goals for the prototype beyond integrating a code editor with a code-fluent LLM. We also did not attempt to target the prototype for a specific class of users (e.g. novices or experts) or use cases (e.g. writing code vs. learning a new programming language), as we wanted any value provided by conversational assistance to emerge from our user study. We also did not implement the ability to run or debug code in our prototype as we wanted to explore the nature of the conversational interaction rather than having users focus extensively on the production of working code.\n\nWhen designing how users would interact with the Programmer's Assistant, we decided that it should be available on demand and not monitor the user's work in progress or give unsolicited suggestions or advice, in keeping with the conversational agent interaction model proposed by Ross et al. [73, 74]. This approach was supported by feedback from prospective users who were concerned about the assistant providing criticism of unfinished efforts in progress or distracting them while they worked. Instead, we force initiative onto the user and only have the assistant respond to their requests. In this way, the assistant can provide help when requested without undesirable interruptions that can distract or interfere with the user's flow.\n\nWhen a user interacts with the assistant, we keep track of their selection state in the code editor. If a user sends a message to the assistant without any code selected in the editor, then that message (along with the prior conversational context) is passed directly to the model. If a user sends a message to the assistant with new code selected in the editor (i.e. code that wasn't previously selected when they sent their last message), then that code is appended to the message before being communicated to the model.\n\nThe model may produce multiple types of responses to a user's message. We treat each type of response differently in the UI.\n\nResponses that do not contain code are always rendered in the chat UI (Figure 1 E).\n\nResponses containing short code snippets (≤ 10 lines) are rendered inline in the chat UI (Figure 1 G).\n\nResponses containing longer code snippets (> 10 lines) show the code in a pop-up window (Figure 2 A), with a proxy entry in the chat transcript (Figure 2 B) that allows users to re-display the code window after it has been closed. Non-code text in the response remains in the chat transcript.\n\nThe assistant never directly modifies the contents of the user's source code; rather, any code the user desires to transfer from the chat takes place via copy/paste.\n\nFigure 1 shows a screenshot of a real, sample conversation, in which the user asks a question that results in an inline response, then requests an explanation of some code in the editor, and then requests further elaboration. Figure 2 shows an example conversation that resulted in the generation of a longer code sample, shown in a popup window. This example shows how the assistant produced an incomplete solution, followed by criticism from the user regarding the missing code, and resulting in an apology and the generation of a complete solution.\n\n3.1 Supporting Conversational Interaction\n\nWe enabled Codex to conduct a conversational interaction by prompting it with a conversational transcript and a request to produce the next conversational turn. The prompt establishes a pattern of conversation between a user and a programming assistant named Socrates. It provides several examples of Socrates responding to general coding questions, generating code in response to a request, and accepting code as input. It establishes a convention for delimiting code in the conversation, making it easy to parse for display in the UI. It also establishes an interaction style for the assistant, directing it to be polite, eager, helpful, and humble, and to present its responses in a non-authoritative manner . Because of the possibility that the model might produce erroneous answers or incorrect code (as discussed in Weisz et al. [102]), we felt it was important that the assistant convey a sense of uncertainty to encourage users to not accept its results uncritically to avoid over-reliance (e.g. as observed in Moroz et al.’s study of Copilot [51], and discussed more generally in Ashktorab et al. [9]) as well as automation bias [45, 46, 65]. We present the full text of the prompt used for the assistant in Appendix D.\n\n3.2 Architecture & UI Design\n\nThe Programmer's Assistant communicates with the Codex API via a proxy server that forwards requests from the React client. The proxy also rate-limits access to conform to the API's policy, and it logs UI events from the client (e.g. requests, responses, and UI interactions) in a back-end database. To address inconsistencies in the style or formatting of code generated by Codex, the proxy server reformats all code segments using the Black code formatter before transmitting them to the client UI.\n\nThe client maintains the transcript of the ongoing conversation. Each time the user sends a message in the chat, the client constructs a new prompt for the model by concatenating the initial prompt, the chat transcript, and the user's new utterance, and makes a request for the model to complete the transcript. This completion request also specifies a stop sequence of tokens to prevent the model from generating both sides of the conversation (e.g. what the model thinks the user's next utterance might be after the assistant's response). Given the API's limitation on context length (4,096 tokens for both the prompt and model response), we silently “forget” older exchanges in the chat transcript when constructing the prompt to ensure that our completion request remains within bounds. Nonetheless, the entire conversational history remains visible to the user in the UI.\n\nThe client UI provides a loose coupling between the source code editor and the chat interface. Users can hide the chat pane when they wish to focus solely on their code, and re-engage with it when they desire assistance. Code selected in the editor is included in the conversation in order to couple the code context with the conversation. Easily-accessible buttons are provided in the UI to copy code responses from the assistant to the clipboard.\n\n3.3 Handling Model Limitations\n\nWhile developing the Programmer's Assistant, and in early pilot testing, we experienced some quirks and shortcomings of the model and our approach to using it for conversational interaction. One limitation stemmed from the fact that the model sometimes produced incorrect responses (e.g. code with syntax errors), incomplete responses (e.g. code that was missing functionality), irrelevant responses (e.g. responses not related to the user's question), or insubstantial responses (e.g. “I don't know”). Because of the probabilistic nature of model inference, re-prompting the model would sometimes produce a more correct or appropriate response. Thus, we added the ability for users to “try again,” either by asking in the chat or by clicking a button in the UI (Figure 1 C). This feature removes the assistant's last response from the context presented to the model and then re-invokes the model with an increased temperature .\n\nAlthough it is possible for transformer models such as Codex to produce multiple possible responses to a single prompt, we only request a single response in order to speed up response time as well as to preserve the token budget for conversational context. Thus, the “try again” feature provides an alternate way to produce a wider variety of responses.\n\nDuring pilot testing, we noticed that the assistant sometimes happened to generate the same response to multiple, unrelated requests. In these cases, the assistant tended to get “stuck” in a pattern of repeating the same response and was unable to resume normal conversation. To avoid this problem, we automatically execute a “try again” operation in the background when we see identical consecutive responses from the assistant.\n\nFinally, we noticed that the accumulation of conversational context sometimes resulted in the assistant becoming fixated on some portion of the earlier conversation. For example, it might respond to a question with portions of the prompt or of earlier conversation, and become less responsive to newer requests. To address this issue, we introduced a “start over” feature, accessible via the chat or by clicking a button in the UI (Figure 1 D), that resets the context to the original prompt, forgetting the rest of the conversational history. We preserve the chat transcript in the UI, but delineate the break in the assistant's memory with an annotation in the chat transcript. These annotations are added both for “try again” and “start over.”\n\n3.4 Sample Conversation\n\nWe provide a real sample conversation with the Programmer's Assistant in Listing 1 . This conversation begins with the assistant greeting the user (line 1). Next, the user asks a general Python programming question (line 4), to which the assistant responds with a non-authoritative remark (“I think...”) and a code snippet (line 9). The user next asks a follow-up question that depends on their previous question and the assistant's response (line 11), to which the assistant provides another code snippet (line 15), satisfying the user's request.\n\nThe user then switches topics and asks the assistant to write a Fibonacci function (line 17), and the assistant again responds with a non-authoritative remark (“I will give it a try,” line 20) and a block of code. The user then asks how the function works (line 30) and the assistant provides an adequate description (line 32). Next, the user asks the assistant to re-implement the function in a different way (line 37), again leveraging the ability to ask follow-up questions. The assistant produces an alternative implementation that conforms to the user's request (line 41). The user follows up with a question that depends on multiple past utterances and responses in the chat transcript (line 47), and the assistant produces a relevant response (line 49). The conversation closes with the user thanking the assistant (line 53) and the assistant acknowledging their gratitude (line 55).\n\n4 EMPIRICAL STUDY OF CONVERSATIONAL PROGRAMMING ASSISTANCE\n\nWe conducted an empirical user study of the Programmer's Assistant to assess whether conversational assistance provides value in a software engineering context . Our inquiry focused on the user experience and value of conversational interactions grounded in code. We therefore designed a qualitative study to investigate attitudes toward a conversational programming assistant: do people enjoy interacting conversationally, what kinds of questions do they ask, and how does the experience compare to other forms of programming support such as searching the web? We note that prior studies (e.g. [103, 105, 109]) conducted quantitative examinations of the use of LLMs in code work; our study is akin to Weisz et al.’s qualitative examination of software engineers’ attitudes toward working with models that may fail to produce working code [102].\n\nTo address our questions, we deployed the Programmer's Assistant within our organization – a global technology company – and invited people to try it out and give us feedback on their experience. We invited people with varying levels of programming skill in order to obtain a wide range of feedback on the kinds of use cases for which the tool could provide assistance.\n\n4.1 Tasks\n\nWe set up the Programmer's Assistant as a playground environment that participants could try out with a few sample programming problems. We created a tutorial to orient participants to the assistant, its capabilities, and how to interact with it. We also created four programming challenges focused on writing code, documenting code, and writing tests for code. We designed these challenges to expose participants to a broad range of the assistant's capabilities. For each of these challenges, we explicitly did not evaluate metrics such as the participant's productivity, the quality of their solutions, or the time taken to produce them, as the focus of our study was to understand the utility of conversational interaction. We selected Python as the language used for the tutorial and challenges because of its general popularity [21] and the fact that it was well-supported by our underlying LLM [24].\n\n4.1.1 Tutorial. All participants were first introduced to the Programmer's Assistant through a tutorial. The tutorial walked each participant through 10 sample interactions to give them a feeling for what the assistant could do and how to interact with it. The tutorial demonstrated how to ask questions, how to request code to be generated, and how to evaluate existing code. It did not specifically cover how to generate documentation or unit tests. Tutorial instructions were provided within the code editor. We include the specific text used for the tutorial in Appendix B.\n\n4.1.2 Programming Challenges. After completing the tutorial, participants unlocked four programming challenges. Two of the challenges involved coding problems (writing a queue class and writing code to create a scatterplot of data in a CSV file), one involved documenting a given function (an implementation of a graph search algorithm), and one involved writing unit tests for a given function (computing the greatest common divisor of two arguments). Although the Programmer's Assistant was visible and available for use, we provided no specific requirement that it actually be used to complete the challenges.\n\nAfter participants completed their solution to a challenge, they submitted it by clicking a button in the UI. The code editor used in the Programmer's Assistant was not a fully-functional IDE and did not provide syntax checking or the ability to run, test, or debug code. Due to these limitations, participants were asked to submit their solutions when they felt they had completed the challenge to their own satisfaction.\n\n4.2 Participants\n\nTo recruit participants for our study, we posted internal advertisements in various communications channels focused on software engineering. Our advertisements stated that we were evaluating a conversational programming assistant, but were kept deliberately vague in order to minimize the impact on peoples’ expectations of the experience.\n\nOur advertisement yielded a pool of 140 potential participants. In order to recruit a diverse sample, we used a screening survey that asked about their job role, their familiarity with and recency of use of Python, and their availability to participate in our study. We accepted participants into the study on a rolling basis, selecting participants to capture a range of programming experiences and ensure balanced gender representation. We conducted periodic reviews to determine whether we were learning something new from each participant or if we had reached the point of saturation [7]. We stopped collecting data after running 42 participants as we were no longer observing any new behaviors or gleaning any new insights. The Programmer's Assistant implementation and configuration were held constant over the course of the study; no changes to the UI design or LLM prompt were made.\n\nOur participants had the following self-identified characteristics:\n\nJob role: 19 Software Engineers, 12 Researcher/Scientists, 3 Software Architects, 2 Data Scientists, 1 Machine Learning Engineer, 1 Systems Test Engineer, 1 Business Analyst, 1 Manager, 1 Marketer, and 1 Consultant.\n\nGender: 21 Female, 19 Male, 1 Gender Variant / Non-conforming, and 1 Preferred not to say.\n\nPython Experience: 17 participants had 3+ years of Python experience, 11 had 1-3 years, 11 had less than 1 year, and 3 were not familiar with Python.\n\nRecency of Python Use: 29 participants had written Python code within the past month, 4 within the past year, 5 within the past 5 years, and 4 had not written Python code within the past 5 years.\n\nWe provide full demographic information for individual participants in Appendix E.\n\n4.3 Procedure\n\nParticipants completed the study on their own time, independently and without moderation. Each participant was provided with a web link to a pre-study survey that described the nature of the study and the tasks that they would be expected to perform. They were then directed to the Programmer's Assistant to complete the tutorial and the four programming challenges. When participants indicated they were finished with the challenges , they were directed to a final post-study survey. Complete sessions generally required about an hour of effort, though some participants spread their effort across a longer period of time and across multiple sessions. Participants were compensated for their time at a rate equivalent to US $15/hr.\n\n4.4 Measures\n\nWe collected a variety of data in our study from three sources:\n\nSurveys. We employed three surveys in the study: a pre-study survey to collect demographic information, a pre-task survey to gauge expectations of the conversational user experience, and a post-task survey to assess actual user experience. We describe these survey questions in the relevant context of our results, and we provide a complete listing of all survey instruments in Appendix A.\n\nEvent logs. The Programmer's Assistant was instrumented to collect data on participants’ usage. The event logs provided timestamped records of interaction events, including conversational exchanges, hiding/showing the assistant, use of the “try again” and “start over” features, and use of copy/paste.\n\nConversation logs. From the event logs, we extracted conversational transcripts between each participant and the Programmer's Assistant.\n\n6 DISCUSSION\n\n6.1 Value of Conversational Interaction\n\nWe began our research by asking the question of whether contemporary developments in code-fluent LLMs could sufficiently support a conversational programming assistant. We believe that our work has demonstrated that they can. Clearly, the Programmer's Assistant was viewed by our participants as a useful tool that provided real value – so much so that many participants explicitly requested or expressed the desire to use it in their own work. However, how much of this value was derived from the model itself and its ability to produce high-quality responses to programming questions, versus from participants’ ability to conduct extended conversational interactions grounded in their actual source code?\n\nWe believe that both of these constituent aspects were valuable. Indeed, many participants commented on their surprise and satisfaction with the quality of the assistant's responses (Section 5.2.3). However, participants also valued the conversational interactions that they had with the assistant. In the event logs, we saw evidence that participants were leveraging conversational context to ask follow-up questions as well as leveraging code context by asking about their code selections (Section 5.3.2). Many participants reported that they would find the tool less valuable if the conversational interaction were removed (Section 5.3.2). Further, conversation seemed to provide unique value beyond other interaction models (direct manipulation and search) because of its embeddedness in the UI and its ability to surface emergent behaviors of the model (Section 5.4.4).\n\nWe do not believe that these different interaction models are in competition and we agree with P39’s assessment that assistive tools can be built using a plethora of different interaction models. For use cases in which a model is known to produce high-quality results (e.g. code autocompletion for Codex), a direct manipulation interface seems wholly appropriate as it would provide a discoverable and predictable way of invoking the model to produce a known type of result. However, direct manipulation interfaces may be less ideal for surfacing the emergent behaviors of a foundation model [14], and thus natural language interaction may be more suitable. Many popular text-to-image models, such as DALL-E 2 [66] and Stable Diffusion [72], operate in a one-shot fashion, in which the user specifies a prompt, clicks a button, and gets results. Our study demonstrates how the additional contextual layers of conversational history and the artifact-under-development provide additional value to the co-creative process.\n\n6.2 Toward Human-AI Synergy\n\nThe aim of human-centered AI is to “enable[] people to see, think, create, and act in extraordinary ways, by combining potent user experiences with embedded AI methods to support services that users want” [82]. Building upon this definition, Rezwana and Maher [69] posit that, “In a creative collaboration, interaction dynamics, such as turn-taking, contribution type, and communication, are the driving forces of the co-creative process. Therefore the interaction model is a critical and essential component for effective co-creative systems.” [69]. They go on to note that, “There is relatively little research about interaction design in the co-creativity field, which is reflected in a lack of focus on interaction design in many existing co-creative systems.”\n\nOur study begins to address this gap. While many co-creative systems examine casual tasks or experimental activities (e.g., Spoto and Oleynik [87]), our focus was on the co-creative practice of programming. Our goal was to understand peoples’ attitudes toward a conversational programming assistant, akin to Wang et al.’s examination of data scientists’ attitudes toward automated data science technologies [99]. We found that, despite an initial level of skepticism, participants felt that a conversational assistant would provide value by improving their productivity (Section 5.4.3). However, further work is needed to assess the extent to which this type of assistance provides measurable productivity increases.\n\nCampero et al. [19] conducted a survey of papers published in 2021 that examined human-AI synergy, the notion that a human-AI team can accomplish more by working together than either party could accomplish working alone. They found mixed results, with no clear consensus emerging on how to design human-centered AI systems that can guarantee positive synergy. Summarizing from their discussion,\n\n“Perhaps achieving substantial synergies among people and computers is harder than many people think. Perhaps it requires... new ways of configuring groups that include people and computers. And perhaps it needs more systematic, focused attention from researchers than it has, so far, received.” [19, p.9]\n\nWe believe such evaluations of human-AI synergy should go beyond one-shot performance measures. As implied by many of the uses cases listed by Seeber et al. [80], human-centered AI systems are often deployed in socio-organizational contexts that require longitudinal use [20, 41, 43], such as product design [93], game design [4], and engineering [20, Section 3.2.2]. Thus, we would expect that over time and through interaction with each other, human-AI teams would improve their performance through a mutual learning process.\n\nEvidence for this process surfaced in our study when participants described how they could improve their programming skills by interacting with the assistant (Section 5.3.3). We assert that the learning should operate in both directions: not only should people improve their programming skills, but the model itself can also improve based on peoples’ interactions with it. For example, when the assistant provides a code example to the user, and the user takes that example and edits it, those edits constitute feedback that can be used to further fine-tune the model. In addition, through longitudinal use, we believe that human and AI partners can create reciprocal representations of one another – i.e., the human is likely to create a mental model of the AI, and the AI may be engineered to develop a user model for each of its human users [30, 48, 79]. Such a pair of models is often described as Mutual Theory of Mind [29, 100]. This type of capability raises the possibility of personalizing and adapting an assistant to the strengths and needs of individual users.\n\nWith such models, an assistant that knows a user is learning a programming language could provide natural language explanations alongside code outputs, whereas an assistant that knows a user is strongly skilled in a programming language might shorten or omit those explanations. Similarly, users are likely to update their mental models of the AI with more experience. We believe the space for exploring how these reciprocal models impact human-AI synergy is rich, and we encourage additional work in this area.\n\nHuman-centered AI systems that are designed to combine and synergize the distinct skills of humans and AI models cannot succeed if they diminish the human skills upon which they depend. Well-designed human-centered AI systems develop new and complementary skills for both the human and AI constituents [82, 83], and we believe that mutual learning may address concerns that the wide deployment and use of AI systems will result in a de-skilling of the workforce [77, 108].\n\nUltimately, the design decisions that go into an interactive AI system have ethical implications. Our design attempts to augment the user's knowledge and skills by presenting help on demand, couched in non-authoritative suggestions, which leaves the user firmly in control and ultimately responsible for the work product.\n\n6.3 Opportunities for Future Research\n\nOur work highlights many interesting avenues for future enhancements that could be made to LLM-based conversational assistants such as our Programmer's Assistant, as well as future human-centered research on LLM-based conversational assistance.\n\nOur work employed a code-fluent model that was not specifically designed to handle conversational interaction. Fine-tuning the underlying LLM for conversational interaction, such as what has been done with Lamda [91], is one opportunity to improve the assistant's performance. Another opportunity is to align the language model to follow the desiderata proposed by Askell et al. [11] and described by Ouyang et al. as, “helpful (they should help the user solve their task), honest (they shouldn't fabricate information or mislead the user), and harmless (they should not cause physical, psychological, or social harm to people or the environment)” [61, p.2]. Glaese et al. [33] propose a slightly different desiderata of “correct” instead of “honest,” which may be more applicable to the software engineering domain, as the ability to produce correct code and correct answers about code are both important properties of a conversational programming assistant.\n\nCombining LLMs with search-based approaches to establish additional context for the model, such as AlphaCode [44] has done, may also result in more capable systems. These “searches” need not be limited to textual sources, but could be conducted over appropriate semantic stores (e.g. a knowledge graph) and take advantage of explicit semantic reasoning services, resulting in an integration of symbolic and neural approaches. Further, allowing for “internal deliberation” of the type shown in Nye et al. [59] could result in better-reasoned results, as well as better explanations and justifications.\n\nAnother avenue for improvement involves the prompt used to configure the assistant (Appendix D). Just as the prompt for each successive interaction is modified by the growth of the conversational transcript, there is no requirement that the initial prompt be static. It too can be specialized to incorporate aspects of a user model, enabling the realization of a Mutual Theory of Mind [29, 100]. Providing better UX affordances for visualizing and manipulating the active contexts – code and conversation – could provide users with more control over which information contributes to the generation of the assistant's response.\n\nOur participants clearly indicated that they were interested in having an assistant that behaved more proactively, in contrast to our deliberate design of an assistant that never takes conversational initiative. A more proactive assistant would be able to interrupt or remind a user when necessary [23], yet this characteristic raises many challenging issues. How can we calibrate the threshold for such interruptions? How can users tune the assistant to deliver only those interruptions that the they would find useful (e.g., [28, 81])? How can we help users to regain their prior context after dealing with an interruption (e.g. [89])? Should an assistant be used to persuade or nudge the user (e.g. [35])? Who should determine the topic, frequency, and insistence of such persuasion attempts (e.g. [52, 85])? Should users have the ability to moderate or defeat attempted persuasions, or should those decisions be left to the organization?\n\nFinally, we explored the different kinds of role orientations our participants had toward the assistant and found that participants varied in their views of it as a tool versus a social agent (e.g. collaborator or colleague). We posit that peoples’ effectiveness in working with an AI system may be influenced by their role orientation, and we encourage future research in this area."
    }
}
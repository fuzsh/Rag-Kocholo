{
    "id": "dbpedia_672_1",
    "rank": 92,
    "data": {
        "url": "https://worldwidescience.org/topicpages/w/web%2Bcam%2Bimage.html",
        "read_more_link": "",
        "language": "en",
        "title": "web cam image: Topics by WorldWideScience.org",
        "top_image": "",
        "meta_img": "",
        "images": [
            "https://worldwidescience.org/sites/www.osti.gov/files/public/image-files/WWSlogo_wTag650px-min.png",
            "https://worldwidescience.org/topicpages/w/images/arrow-up.gif",
            "https://worldwidescience.org/topicpages/w/images/arrow-down.gif",
            "https://worldwidescience.org/topicpages/w/images/arrow-up.gif",
            "https://worldwidescience.org/topicpages/w/images/arrow-down.gif",
            "https://worldwidescience.org/topicpages/w/images/arrow-up.gif",
            "https://worldwidescience.org/topicpages/w/images/arrow-down.gif",
            "https://worldwidescience.org/sites/www.osti.gov/files/public/image-files/OSTIlogo.svg",
            "https://worldwidescience.org/sites/www.osti.gov/files/public/image-files/ICSTIlogo.svg"
        ],
        "movies": [],
        "keywords": [],
        "meta_keywords": [
            ""
        ],
        "tags": null,
        "authors": [],
        "publish_date": null,
        "summary": "",
        "meta_description": "",
        "meta_lang": "en",
        "meta_favicon": "",
        "meta_site_name": "",
        "canonical_link": null,
        "text": "Monitoring and Precision Spraying for Orchid Plantation with Wireless WebCAMs\n\nDirectory of Open Access Journals (Sweden)\n\nGrianggai Samseemoung\n\n2017-10-01\n\nFull Text Available Through processing images taken from wireless WebCAMs on the low altitude remote sensing (LARS platform, this research monitored crop growth, pest, and disease information in a dendrobium orchidâs plantation. Vegetetative indices were derived for distinguishing different stages of crop growth, and the infestation density of pests and diseases. Image data was processed through an algorithm created in MATLABÂ® (The MathWorks, Inc., Natick, USA. Corresponding to the orchidâs growth stage and its infestation density, varying levels of fertilizer and chemical injections were administered. The acquired LARS images from wireless WebCAMs were positioned using geo-referencing, and eventually processed to estimate vegetative-indices (Red = 650 nm and NIR = 800 nm band center. Good correlations and a clear cluster range were obtained in characteristic plots of the normalized difference vegetation index (NDVI and the green normalized difference vegetation index (GNDVI against chlorophyll content. The coefficient of determination, the chlorophyll content values (Î¼mol mâ2 showed significant differences among clusters for healthy orchids (R2 = 0.985â0.992, and for infested orchids (R2 = 0.984â0.998. The WebCAM application, while being inexpensive, provided acceptable inputs for image processing. The LARS platform gave its best performance at an altitude of 1.2 m above canopy. The image processing software based on LARS images provided satisfactory results as compared with manual measurements.\n\nComparative evaluation of RetCam vs. gonioscopy images in congenital glaucoma.\n\nScience.gov (United States)\n\nAzad, Raj V; Chandra, Parijat; Chandra, Anuradha; Gupta, Aparna; Gupta, Viney; Sihota, Ramanjit\n\n2014-02-01\n\nTo compare clarity, exposure and quality of anterior chamber angle visualization in congenital glaucoma patients, using RetCam and indirect gonioscopy images. Cross-sectional study Participants. Congenital glaucoma patients over age of 5 years. A prospective consecutive pilot study was done in congenital glaucoma patients who were older than 5 years. Methods used are indirect gonioscopy and RetCam imaging. Clarity of the image, extent of angle visible and details of angle structures seen were graded for both methods, on digitally recorded images, in each eye, by two masked observers. Image clarity, interobserver agreement. 40 eyes of 25 congenital glaucoma patients were studied. RetCam image had excellent clarity in 77.5% of patients versus 47.5% by gonioscopy. The extent of angle seen was similar by both methods. Agreement between RetCam and gonioscopy images regarding details of angle structures was 72.50% by observer 1 and 65.00% by observer 2. There was good agreement between RetCam and indirect gonioscopy images in detecting angle structures of congenital glaucoma patients. However, RetCam provided greater clarity, with better quality, and higher magnification images. RetCam can be a useful alternative to gonioscopy in infants and small children without the need for general anesthesia.\n\nComparative evaluation of RetCam vs. gonioscopy images in congenital glaucoma\n\nDirectory of Open Access Journals (Sweden)\n\nRaj V Azad\n\n2014-01-01\n\nFull Text Available Purpose: To compare clarity, exposure and quality of anterior chamber angle visualization in congenital glaucoma patients, using RetCam and indirect gonioscopy images. Design: Cross-sectional study Participants. Congenital glaucoma patients over age of 5 years. Materials and Methods: A prospective consecutive pilot study was done in congenital glaucoma patients who were older than 5 years. Methods used are indirect gonioscopy and RetCam imaging. Clarity of the image, extent of angle visible and details of angle structures seen were graded for both methods, on digitally recorded images, in each eye, by two masked observers. Outcome Measures: Image clarity, interobserver agreement. Results: 40 eyes of 25 congenital glaucoma patients were studied. RetCam image had excellent clarity in 77.5% of patients versus 47.5% by gonioscopy. The extent of angle seen was similar by both methods. Agreement between RetCam and gonioscopy images regarding details of angle structures was 72.50% by observer 1 and 65.00% by observer 2. Conclusions: There was good agreement between RetCam and indirect gonioscopy images in detecting angle structures of congenital glaucoma patients. However, RetCam provided greater clarity, with better quality, and higher magnification images. RetCam can be a useful alternative to gonioscopy in infants and small children without the need for general anesthesia.\n\nUse of EyeCam for imaging the anterior chamber angle.\n\nScience.gov (United States)\n\nPerera, Shamira A; Baskaran, Mani; Friedman, David S; Tun, Tin A; Htoon, Hla M; Kumar, Rajesh S; Aung, Tin\n\n2010-06-01\n\nTo compare EyeCam (Clarity Medical Systems, Pleasanton, CA) imaging with gonioscopy for detecting angle closure. In this prospective, hospital-based study, subjects underwent gonioscopy by a single observer and EyeCam imaging by a different operator. EyeCam images were graded by two masked observers. The anterior chamber angle in a quadrant was classified as closed if the trabecular meshwork could not be seen. The eye was classified as having angle closure if two or more quadrants were closed. One hundred fifty-two subjects were studied. The mean age was 57.4 years (SD 12.9) and there were 82 (54%) men. Of the 152 eyes, 21 (13.8%) had angle closure. The EyeCam provided clear images of the angles in 98.8% of subjects. The agreement between the EyeCam and gonioscopy for detecting angle closure in the superior, inferior, nasal, and temporal quadrants based on agreement coefficient (AC1) statistics was 0.73, 0.75, 0.76, and 0.72, respectively. EyeCam detected more closed angles than did gonioscopy in all quadrants (P gonioscopy, 21/152 (13.8%) eyes were diagnosed as angle closure compared to 41 (27.0%) of 152 with EyeCam (P gonioscopy for detecting angle closure. However, it detected more closed angles than did gonioscopy in all quadrants.\n\nFlowCam: Quantification and Classification of Phytoplankton by Imaging Flow Cytometry.\n\nScience.gov (United States)\n\nPoulton, Nicole J\n\n2016-01-01\n\nThe ability to enumerate, classify, and determine biomass of phytoplankton from environmental samples is essential for determining ecosystem function and their role in the aquatic community and microbial food web. Traditional micro-phytoplankton quantification methods using microscopic techniques require preservation and are slow, tedious and very laborious. The availability of more automated imaging microscopy platforms has revolutionized the way particles and cells are detected within their natural environment. The ability to examine cells unaltered and without preservation is key to providing more accurate cell concentration estimates and overall phytoplankton biomass. The FlowCam(Â®) is an imaging cytometry tool that was originally developed for use in aquatic sciences and provides a more rapid and unbiased method for enumerating and classifying phytoplankton within diverse aquatic environments.\n\nJunoCam Images of Jupiter: Science from an Outreach Experiment\n\nScience.gov (United States)\n\nHansen, C. J.; Orton, G. S.; Caplinger, M. A.; Ravine, M. A.; Rogers, J.; EichstÃ¤dt, G.; Jensen, E.; Bolton, S. J.; Momary, T.; Ingersoll, A. P.\n\n2017-12-01\n\nThe Juno mission to Jupiter carries a visible imager on its payload primarily for outreach, and also very useful for jovian atmospheric science. Lacking a formal imaging science team, members of the public have volunteered to process JunoCam images. Lightly processed and raw JunoCam data are posted on the JunoCam webpage at https://missionjuno.swri.edu/junocam/processing. Citizen scientists download these images and upload their processed contributions. JunoCam images through broadband red, green and blue filters and a narrowband methane filter centered at 889 nm mounted directly on the detector. JunoCam is a push-frame imager with a 58 deg wide field of view covering a 1600 pixel width, and builds the second dimension of the image as the spacecraft rotates. This design enables capture of the entire pole of Jupiter in a single image at low emission angle when Juno is 1 hour from perijove (closest approach). At perijove the wide field of view images are high-resolution while still capturing entire storms, e.g. the Great Red Spot. Juno's unique polar orbit yields polar perspectives unavailable to earth-based observers or most previous spacecraft. The first discovery was that the familiar belt-zone structure gives way to more chaotic storms, with cyclones grouped around both the north and south poles [1, 2]. Recent time-lapse sequences have enabled measurement of the rotation rates and wind speeds of these circumpolar cyclones [3]. Other topics are being investigated with substantial, in many cases essential, contributions from citizen scientists. These include correlating the high resolution JunoCam images to storms and disruptions of the belts and zones tracked throughout the historical record. A phase function for Jupiter is being developed empirically to allow image brightness to be flattened from the subsolar point to the terminator. We are studying high hazes and the stratigraphy of the upper atmosphere, utilizing the methane filter, structures illuminated\n\nGammaCam trademark radiation imaging system\n\nInternational Nuclear Information System (INIS)\n\n1998-02-01\n\nGammaCam trademark, a gamma-ray imaging system manufactured by AIL System, Inc., would benefit a site that needs to locate radiation sources. It is capable of producing a two-dimensional image of a radiation field superimposed on a black and white visual image. Because the system can be positioned outside the radiologically controlled area, the radiation exposure to personnel is significantly reduced and extensive shielding is not required. This report covers the following topics: technology description; performance; technology applicability and alternatives; cost; regulatory and policy issues; and lessons learned. The demonstration of GammaCam trademark in December 1996 was part of the Large-Scale Demonstration Project (LSDP) whose objective is to select and demonstrate potentially beneficial technologies at the Argonne National Laboratory-East (ANL) Chicago Pile-5 Research Reactor (CP-5). The purpose of the LSDP is to demonstrate that by using innovative and improved decontamination and decommissioning (D and D) technologies from various sources, significant benefits can be achieved when compared to baseline D and D technologies\n\nComparative evaluation of RetCam vs. gonioscopy images in congenital glaucoma\n\nOpenAIRE\n\nAzad, Raj V; Chandra, Parijat; Chandra, Anuradha; Gupta, Aparna; Gupta, Viney; Sihota, Ramanjit\n\n2014-01-01\n\nPurpose: To compare clarity, exposure and quality of anterior chamber angle visualization in congenital glaucoma patients, using RetCam and indirect gonioscopy images. Design: Cross-sectional study Participants. Congenital glaucoma patients over age of 5 years. Materials and Methods: A prospective consecutive pilot study was done in congenital glaucoma patients who were older than 5 years. Methods used are indirect gonioscopy and RetCam imaging. Clarity of the image, extent of angle visible a...\n\nGamma Ray Imaging System (GRIS) GammaCam trademark. Final report, January 3, 1994 - May 31, 1996\n\nInternational Nuclear Information System (INIS)\n\n1996-01-01\n\nThis report describes the activities undertaken during the development of the Gamma Ray Imaging System (GRIS) program now referred to as the GammaCam trademark. The purpose of this program is to develop a 2-dimensional imaging system for gamma-ray energy scenes that may be present in nuclear power plants. The report summarizes the overall accomplishments of the program and the most recent GammaCam measurements made at LANL and Estonia. The GammaCam is currently available for sale from AIL Systems as an off-the-shelf instrument\n\nTools for Generating Useful Time-series Data from PhenoCam Images\n\nScience.gov (United States)\n\nMilliman, T. E.; Friedl, M. A.; Frolking, S.; Hufkens, K.; Klosterman, S.; Richardson, A. D.; Toomey, M. P.\n\n2012-12-01\n\nThe PhenoCam project (http://phenocam.unh.edu/) is tasked with acquiring, processing, and archiving digital repeat photography to be used for scientific studies of vegetation phenological processes. Over the past 5 years the PhenoCam project has collected over 2 million time series images for a total over 700 GB of image data. Several papers have been published describing derived \"vegetation indices\" (such as green-chromatic-coordinate or gcc) which can be compared to standard measures such as NDVI or EVI. Imagery from our archive is available for download but converting series of images for a particular camera into useful scientific data, while simple in principle, is complicated by a variety of factors. Cameras are often exposed to harsh weather conditions (high wind, rain, ice, snow pile up), which result in images where the field of view (FOV) is partially obscured or completely blocked for periods of time. The FOV can also change for other reasons (mount failures, tower maintenance, etc.) Some of the relatively inexpensive cameras that are being used can also temporarily lose color balance or exposure controls resulting in loss of imagery. All these factors negatively influence the automated analysis of the image time series making this a non-trivial task. Here we discuss the challenges of processing PhenoCam image time-series for vegetation monitoring and the associated data management tasks. We describe our current processing framework and a simple standardized output format for the resulting time-series data. The time-series data in this format will be generated for specific \"regions of interest\" (ROI's) for each of the cameras in the PhenoCam network. This standardized output (which will be updated daily) can be considered 'the pulse' of a particular camera and will provide a default phenological dynamic for said camera. The time-series data can also be viewed as a higher level product which can be used to generate \"vegetation indices\", like gcc, for\n\nThe Role of Public Interaction with the Juno Mission: Documentation, Discussion, Selection and Processing of JunoCam Images of Jovian Cloud Features\n\nScience.gov (United States)\n\nOrton, Glenn; Hansen, Candice; Momary, Thomas; Bolton, Scott\n\n2017-04-01\n\nadditional comments on features from their various points of view, but Juno's science team has no greater weighting in the voting process than the public at large, short of an extraordinary event, such as an impact event or a sudden atmospheric outburst. Public voting was tested for the first time on three regions for PJ3 and has continued for PJ4 and PJ5 with voting on nearly all non-polar images. One of the big challenges in this process was the accurate prediction of which features would be in the field of view at the time of the perijove some 10 days following the end of voting, due to Jupiter's differential rotation. The results of public processing and re-posting of JunoCam images have ranged all the way from artistic renditions up to professional-equivalent analysis that is equivalent to anything JunoCam team members could have produced. All aspects of this effort are available on the Mission Juno web site, linked to the JunoCam instrument (https://www.missionjuno.swri.edu/junocam/).\n\nMR imaging of carotid webs\n\nInternational Nuclear Information System (INIS)\n\nBoesen, Mari E.; Eswaradass, Prasanna Venkatesan; Singh, Dilip; Mitha, Alim P.; Menon, Bijoy K.; Goyal, Mayank; Frayne, Richard\n\n2017-01-01\n\nWe propose a magnetic resonance (MR) imaging protocol for the characterization of carotid web morphology, composition, and vessel wall dynamics. The purpose of this case series was to determine the feasibility of imaging carotid webs with MR imaging. Five patients diagnosed with carotid web on CT angiography were recruited to undergo a 30-min MR imaging session. MR angiography (MRA) images of the carotid artery bifurcation were acquired. Multi-contrast fast spin echo (FSE) images were acquired axially about the level of the carotid web. Two types of cardiac phase resolved sequences (cineFSE and cine phase contrast) were acquired to visualize the elasticity of the vessel wall affected by the web. Carotid webs were identified on MRA in 5/5 (100%) patients. Multi-contrast FSE revealed vessel wall thickening and cineFSE demonstrated regional changes in distensibility surrounding the webs in these patients. Our MR imaging protocol enables an in-depth evaluation of patients with carotid webs: morphology (by MRA), composition (by multi-contrast FSE), and wall dynamics (by cineFSE). (orig.)\n\nMR imaging of carotid webs\n\nEnergy Technology Data Exchange (ETDEWEB)\n\nBoesen, Mari E. [University of Calgary, Department of Biomedical Engineering, Calgary (Canada); Foothills Medical Centre, Seaman Family MR Research Centre, Calgary (Canada); Eswaradass, Prasanna Venkatesan; Singh, Dilip; Mitha, Alim P.; Menon, Bijoy K. [University of Calgary, Department of Clinical Neurosciences, Calgary (Canada); Foothills Medical Centre, Calgary Stroke Program, Calgary (Canada); Goyal, Mayank [Foothills Medical Centre, Calgary Stroke Program, Calgary (Canada); University of Calgary, Department of Radiology, Calgary (Canada); Frayne, Richard [Foothills Medical Centre, Seaman Family MR Research Centre, Calgary (Canada); University of Calgary, Hotchkiss Brain Institute, Calgary (Canada)\n\n2017-04-15\n\nWe propose a magnetic resonance (MR) imaging protocol for the characterization of carotid web morphology, composition, and vessel wall dynamics. The purpose of this case series was to determine the feasibility of imaging carotid webs with MR imaging. Five patients diagnosed with carotid web on CT angiography were recruited to undergo a 30-min MR imaging session. MR angiography (MRA) images of the carotid artery bifurcation were acquired. Multi-contrast fast spin echo (FSE) images were acquired axially about the level of the carotid web. Two types of cardiac phase resolved sequences (cineFSE and cine phase contrast) were acquired to visualize the elasticity of the vessel wall affected by the web. Carotid webs were identified on MRA in 5/5 (100%) patients. Multi-contrast FSE revealed vessel wall thickening and cineFSE demonstrated regional changes in distensibility surrounding the webs in these patients. Our MR imaging protocol enables an in-depth evaluation of patients with carotid webs: morphology (by MRA), composition (by multi-contrast FSE), and wall dynamics (by cineFSE). (orig.)\n\n\"Baby-Cam\" and Researching with Infants: Viewer, Image and (Not) Knowing\n\nScience.gov (United States)\n\nElwick, Sheena\n\n2015-01-01\n\nThis article offers a methodological reflection on how \"baby-cam\" enhanced ethically reflective attitudes in a large-scale research project that set out to research with infants in Australian early childhood education and care settings. By juxtaposing digital images produced by two different digital-camera technologies and drawing onâ¦\n\nFringing in MonoCam Y4 filter images\n\nInternational Nuclear Information System (INIS)\n\nBrooks, J.; Nomerotski, A.; Fisher-Levine, M.\n\n2017-01-01\n\nWe study the fringing patterns observed in MonoCam, a camera with a single Large Synoptic Survey Telescope (LSST) CCD sensor. Images were taken at the U.S. Naval Observatory in Flagstaff, Arizona (NOFS) employing its 1.3 m telescope and an LSST y 4 filter. Fringing occurs due to the reflection of infrared light (700 nm or larger) from the bottom surface of the CCD which constructively or destructively interferes with the incident light to produce a net ''fringe'' pattern which is superimposed on all images taken. Emission lines from the atmosphere, dominated by hydroxyl (OH) spectra, can change in their relative intensities as the night goes on, producing different fringe patterns in the images taken. We found through several methods that the general shape of the fringe patterns remained constant, though with slight changes in the amplitude and phase of the fringes. We also found that a superposition of fringes from two monochromatic lines taken in the lab offered a reasonable description of the sky data.\n\nGender differences in autobiographical memory for everyday events: retrieval elicited by SenseCam images versus verbal cues.\n\nScience.gov (United States)\n\nSt Jacques, Peggy L; Conway, Martin A; Cabeza, Roberto\n\n2011-10-01\n\nGender differences are frequently observed in autobiographical memory (AM). However, few studies have investigated the neural basis of potential gender differences in AM. In the present functional MRI (fMRI) study we investigated gender differences in AMs elicited using dynamic visual images vs verbal cues. We used a novel technology called a SenseCam, a wearable device that automatically takes thousands of photographs. SenseCam differs considerably from other prospective methods of generating retrieval cues because it does not disrupt the ongoing experience. This allowed us to control for potential gender differences in emotional processing and elaborative rehearsal, while manipulating how the AMs were elicited. We predicted that males would retrieve more richly experienced AMs elicited by the SenseCam images vs the verbal cues, whereas females would show equal sensitivity to both cues. The behavioural results indicated that there were no gender differences in subjective ratings of reliving, importance, vividness, emotion, and uniqueness, suggesting that gender differences in brain activity were not due to differences in these measures of phenomenological experience. Consistent with our predictions, the fMRI results revealed that males showed a greater difference in functional activity associated with the rich experience of SenseCam vs verbal cues, than did females.\n\nMultifunctional nanoparticle-EpCAM aptamer bioconjugates: a paradigm for targeted drug delivery and imaging in cancer therapy.\n\nScience.gov (United States)\n\nDas, Manasi; Duan, Wei; Sahoo, Sanjeeb K\n\n2015-02-01\n\nThe promising proposition of multifunctional nanoparticles for cancer diagnostics and therapeutics has inspired the development of theranostic approach for improved cancer therapy. Moreover, active targeting of drug carrier to specific target site is crucial for providing efficient delivery of therapeutics and imaging agents. In this regard, the present study investigates the theranostic capabilities of nutlin-3a loaded poly (lactide-co-glycolide) nanoparticles, functionalized with a targeting ligand (EpCAM aptamer) and an imaging agent (quantum dots) for cancer therapy and bioimaging. A wide spectrum of in vitro analysis (cellular uptake study, cytotoxicity assay, cell cycle and apoptosis analysis, apoptosis associated proteins study) revealed superior therapeutic potentiality of targeted NPs over other formulations in EpCAM expressing cells. Moreover, our nanotheranostic system served as a superlative bio-imaging modality both in 2D monolayer culture and tumor spheroid model. Our result suggests that, these aptamer-guided multifunctional NPs may act as indispensable nanotheranostic approach toward cancer therapy. This study investigated the theranostic capabilities of nutlin-3a loaded poly (lactide-co-glycolide) nanoparticles functionalized with a targeting ligand (EpCAM aptamer) and an imaging agent (quantum dots) for cancer therapy and bioimaging. It was concluded that the studied multifunctional targeted nanoparticle may become a viable and efficient approach in cancer therapy. Copyright Â© 2015 Elsevier Inc. All rights reserved.\n\nSenseCam reminiscence and action recall in memory-unimpaired people.\n\nScience.gov (United States)\n\nSeamon, John G; Moskowitz, Tacie N; Swan, Ashley E; Zhong, Boyuan; Golembeski, Amy; Liong, Christopher; Narzikul, Alexa C; Sosan, Olumide A\n\n2014-01-01\n\nCase studies of memory-impaired individuals consistently show that reminiscing with SenseCam images enhances event recall. This exploratory study examined whether a similar benefit would occur for the consolidation of memories in memory-unimpaired people. We tested delayed recall for atypical actions observed on a lengthy walk. Participants used SenseCam, a diary, or no external memory aid while walking, followed by reminiscence with SenseCam images, diary entries, or no aid, either alone (self-reminiscence) or with the experimenter (social reminiscence). One week later, when tested without SenseCam images or diary entries, prior social reminiscence produced greater recall than self-reminiscence, but there were no differences between memory aid conditions for action free recall or action order recall. When methodological variables were controlled, there was no recall advantage for SenseCam reminiscence with memory-unimpaired participants. The case studies and present study differ in multiple ways, making direct comparisons problematic. SenseCam is a valuable aid to the memory impaired, but its mnemonic value for non-clinical populations remains to be determined.\n\nEpCAM as multi-tumour target for near-infrared fluorescence guided surgery\n\nInternational Nuclear Information System (INIS)\n\nDriel, P. B. A. A. van; Boonstra, M. C.; Prevoo, H. A. J. M.; Giessen, M. van de; Snoeks, T. J. A.; Tummers, Q. R. J. G.; Keereweer, S.; Cordfunke, R. A.; Fish, A.; Eendenburg, J. D. H. van; Lelieveldt, B. P. F.; Dijkstra, J.; Velde, C. J. H. van de; Kuppen, P. J. K.; Vahrmeijer, A. L.; LÃ¶wik, C. W. G. M.; Sier, C. F. M.\n\n2016-01-01\n\nEvaluation of resection margins during cancer surgery can be challenging, often resulting in incomplete tumour removal. Fluorescence-guided surgery (FGS) aims to aid the surgeon to visualize tumours and resection margins during surgery. FGS relies on a clinically applicable imaging system in combination with a specific tumour-targeting contrast agent. In this study EpCAM (epithelial cell adhesion molecule) is evaluated as target for FGS in combination with the novel Artemis imaging system. The NIR fluorophore IRDye800CW was conjugated to the well-established EpCAM specific monoclonal antibody 323/A3 and an isotype IgG1 as control. The anti-EpCAM/800CW conjugate was stable in serum and showed preserved binding capacity as evaluated on EpCAM positive and negative cell lines, using flow cytometry and cell-based plate assays. Four clinically relevant orthotopic tumour models, i.e. colorectal cancer, breast cancer, head and neck cancer, and peritonitis carcinomatosa, were used to evaluate the performance of the anti-EpCAM agent with the clinically validated Artemis imaging system. The Pearl Impulse small animal imaging system was used as reference. The specificity of the NIRF signal was confirmed using bioluminescence imaging and green-fluorescent protein. All tumour types could clearly be delineated and resected 72 h after injection of the imaging agent. Using NIRF imaging millimetre sized tumour nodules were detected that were invisible for the naked eye. Fluorescence microscopy demonstrated the distribution and tumour specificity of the anti-EpCAM agent. This study shows the potential of an EpCAM specific NIR-fluorescent agent in combination with a clinically validated intraoperative imaging system to visualize various tumours during surgery\n\nAn Effective Combined Feature For Web Based Image Retrieval\n\nDirectory of Open Access Journals (Sweden)\n\nH.M.R.B Herath\n\n2015-08-01\n\nFull Text Available Abstract Technology advances as well as the emergence of large scale multimedia applications and the revolution of the World Wide Web has changed the world into a digital age. Anybody can use their mobile phone to take a photo at any time anywhere and upload that image to ever growing image databases. Development of effective techniques for visual and multimedia retrieval systems is one of the most challenging and important directions of the future research. This paper proposes an effective combined feature for web based image retrieval. Frequently used colour and texture features are explored in order to develop a combined feature for this purpose. Widely used three colour features Colour moments Colour coherence vector and Colour Correlogram and three texture features Grey Level Co-occurrence matrix Tamura features and Gabor filter were analyzed for their performance. Precision and Recall were used to evaluate the performance of each of these techniques. By comparing precision and recall values the methods that performed best were taken and combined to form a hybrid feature. The developed combined feature was evaluated by developing a web based CBIR system. A web crawler was used to first crawl through Web sites and images found in those sites are downloaded and the combined feature representation technique was used to extract image features. The test results indicated that this web system can be used to index web images with the combined feature representation schema and to find similar images. Random image retrievals using the web system shows that the combined feature can be used to retrieve images belonging to the general image domain. Accuracy of the retrieval can be noted high for natural images like outdoor scenes images of flowers etc. Also images which have a similar colour and texture distribution were retrieved as similar even though the images were belonging to deferent semantic categories. This can be ideal for an artist who wants\n\nPCI bus content-addressable-memory (CAM) implementation on FPGA for pattern recognition/image retrieval in a distributed environment\n\nScience.gov (United States)\n\nMegherbi, Dalila B.; Yan, Yin; Tanmay, Parikh; Khoury, Jed; Woods, C. L.\n\n2004-11-01\n\nRecently surveillance and Automatic Target Recognition (ATR) applications are increasing as the cost of computing power needed to process the massive amount of information continues to fall. This computing power has been made possible partly by the latest advances in FPGAs and SOPCs. In particular, to design and implement state-of-the-Art electro-optical imaging systems to provide advanced surveillance capabilities, there is a need to integrate several technologies (e.g. telescope, precise optics, cameras, image/compute vision algorithms, which can be geographically distributed or sharing distributed resources) into a programmable system and DSP systems. Additionally, pattern recognition techniques and fast information retrieval, are often important components of intelligent systems. The aim of this work is using embedded FPGA as a fast, configurable and synthesizable search engine in fast image pattern recognition/retrieval in a distributed hardware/software co-design environment. In particular, we propose and show a low cost Content Addressable Memory (CAM)-based distributed embedded FPGA hardware architecture solution with real time recognition capabilities and computing for pattern look-up, pattern recognition, and image retrieval. We show how the distributed CAM-based architecture offers a performance advantage of an order-of-magnitude over RAM-based architecture (Random Access Memory) search for implementing high speed pattern recognition for image retrieval. The methods of designing, implementing, and analyzing the proposed CAM based embedded architecture are described here. Other SOPC solutions/design issues are covered. Finally, experimental results, hardware verification, and performance evaluations using both the Xilinx Virtex-II and the Altera Apex20k are provided to show the potential and power of the proposed method for low cost reconfigurable fast image pattern recognition/retrieval at the hardware/software co-design level.\n\nWeb Conversations About Complementary and Alternative Medicines and Cancer: Content and Sentiment Analysis.\n\nScience.gov (United States)\n\nMazzocut, Mauro; Truccolo, Ivana; Antonini, Marialuisa; Rinaldi, Fabio; Omero, Paolo; Ferrarin, Emanuela; De Paoli, Paolo; Tasso, Carlo\n\n2016-06-16\n\nThe use of complementary and alternative medicine (CAM) among cancer patients is widespread and mostly self-administrated. Today, one of the most relevant topics is the nondisclosure of CAM use to doctors. This general lack of communication exposes patients to dangerous behaviors and to less reliable information channels, such as the Web. The Italian context scarcely differs from this trend. Today, we are able to mine and analyze systematically the unstructured information available in the Web, to get an insight of people's opinions, beliefs, and rumors concerning health topics. Our aim was to analyze Italian Web conversations about CAM, identifying the most relevant Web sources, therapies, and diseases and measure the related sentiment. Data have been collected using the Web Intelligence tool ifMONITOR. The workflow consisted of 6 phases: (1) eligibility criteria definition for the ifMONITOR search profile; (2) creation of a CAM terminology database; (3) generic Web search and automatic filtering, the results have been manually revised to refine the search profile, and stored in the ifMONITOR database; (4) automatic classification using the CAM database terms; (5) selection of the final sample and manual sentiment analysis using a 1-5 score range; (6) manual indexing of the Web sources and CAM therapies type retrieved. Descriptive univariate statistics were computed for each item: absolute frequency, percentage, central tendency (mean sentiment score [MSS]), and variability (standard variation Ï). Overall, 212 Web sources, 423 Web documents, and 868 opinions have been retrieved. The overall sentiment measured tends to a good score (3.6 of 5). Quite a high polarization in the opinions of the conversation partaking emerged from standard variation analysis (Ïâ¥1). In total, 126 of 212 (59.4%) Web sources retrieved were nonhealth-related. Facebook (89; 21%) and Yahoo Answers (41; 9.7%) were the most relevant. In total, 94 CAM therapies have been retrieved. Most\n\nARCHITECTURE OF WEB BASED COMPUTER-AIDED MANUFACTURING SYSTEM\n\nDirectory of Open Access Journals (Sweden)\n\nN. E. Filyukov\n\n2014-09-01\n\nFull Text Available The paper deals with design of a web-based system for Computer-Aided Manufacturing (CAM. Remote applications and databases located in the \"private cloud\" are proposed to be the basis of such system. The suggested approach contains: service - oriented architecture, using web applications and web services as modules, multi-agent technologies for implementation of information exchange functions between the components of the system and the usage of PDM - system for managing technology projects within the CAM. The proposed architecture involves CAM conversion into the corporate information system that will provide coordinated functioning of subsystems based on a common information space, as well as parallelize collective work on technology projects and be able to provide effective control of production planning. A system has been developed within this architecture which gives the possibility for a rather simple technological subsystems connect to the system and implementation of their interaction. The system makes it possible to produce CAM configuration for a particular company on the set of developed subsystems and databases specifying appropriate access rights for employees of the company. The proposed approach simplifies maintenance of software and information support for CAM subsystems due to their central location in the data center. The results can be used as a basis for CAM design and testing within the learning process for development and modernization of the system algorithms, and then can be tested in the extended enterprise.\n\nCAD/CAM-assisted breast reconstruction\n\nInternational Nuclear Information System (INIS)\n\nMelchels, Ferry; Hutmacher, Dietmar Werner; Wiggenhauser, Paul Severin; Schantz, Jan-Thorsten; Warne, David; Barry, Mark; Ong, Fook Rhu; Chong, Woon Shin\n\n2011-01-01\n\nThe application of computer-aided design and manufacturing (CAD/CAM) techniques in the clinic is growing slowly but steadily. The ability to build patient-specific models based on medical imaging data offers major potential. In this work we report on the feasibility of employing laser scanning with CAD/CAM techniques to aid in breast reconstruction. A patient was imaged with laser scanning, an economical and facile method for creating an accurate digital representation of the breasts and surrounding tissues. The obtained model was used to fabricate a customized mould that was employed as an intra-operative aid for the surgeon performing autologous tissue reconstruction of the breast removed due to cancer. Furthermore, a solid breast model was derived from the imaged data and digitally processed for the fabrication of customized scaffolds for breast tissue engineering. To this end, a novel generic algorithm for creating porosity within a solid model was developed, using a finite element model as intermediate.\n\nSIP: A Web-Based Astronomical Image Processing Program\n\nScience.gov (United States)\n\nSimonetti, J. H.\n\n1999-12-01\n\nI have written an astronomical image processing and analysis program designed to run over the internet in a Java-compatible web browser. The program, Sky Image Processor (SIP), is accessible at the SIP webpage (http://www.phys.vt.edu/SIP). Since nothing is installed on the user's machine, there is no need to download upgrades; the latest version of the program is always instantly available. Furthermore, the Java programming language is designed to work on any computer platform (any machine and operating system). The program could be used with students in web-based instruction or in a computer laboratory setting; it may also be of use in some research or outreach applications. While SIP is similar to other image processing programs, it is unique in some important respects. For example, SIP can load images from the user's machine or from the Web. An instructor can put images on a web server for students to load and analyze on their own personal computer. Or, the instructor can inform the students of images to load from any other web server. Furthermore, since SIP was written with students in mind, the philosophy is to present the user with the most basic tools necessary to process and analyze astronomical images. Images can be combined (by addition, subtraction, multiplication, or division), multiplied by a constant, smoothed, cropped, flipped, rotated, and so on. Statistics can be gathered for pixels within a box drawn by the user. Basic tools are available for gathering data from an image which can be used for performing simple differential photometry, or astrometry. Therefore, students can learn how astronomical image processing works. Since SIP is not part of a commercial CCD camera package, the program is written to handle the most common denominator image file, the FITS format.\n\nThe path to CAM6: coupled simulations with CAM5.4 and CAM5.5\n\nScience.gov (United States)\n\nBogenschutz, Peter A.; Gettelman, Andrew; Hannay, Cecile; Larson, Vincent E.; Neale, Richard B.; Craig, Cheryl; Chen, Chih-Chieh\n\n2018-01-01\n\nThis paper documents coupled simulations of two developmental versions of the Community Atmosphere Model (CAM) towards CAM6. The configuration called CAM5.4 introduces new microphysics, aerosol, and ice nucleation changes, among others to CAM. The CAM5.5 configuration represents a more radical departure, as it uses an assumed probability density function (PDF)-based unified cloud parameterization to replace the turbulence, shallow convection, and warm cloud macrophysics in CAM. This assumed PDF method has been widely used in the last decade in atmosphere-only climate simulations but has never been documented in coupled mode. Here, we compare the simulated coupled climates of CAM5.4 and CAM5.5 and compare them to the control coupled simulation produced by CAM5.3. We find that CAM5.5 has lower cloud forcing biases when compared to the control simulations. Improvements are also seen in the simulated amplitude of the NiÃ±o-3.4 index, an improved representation of the diurnal cycle of precipitation, subtropical surface wind stresses, and double Intertropical Convergence Zone biases. Degradations are seen in Amazon precipitation as well as slightly colder sea surface temperatures and thinner Arctic sea ice. Simulation of the 20th century results in a credible simulation that ends slightly colder than the control coupled simulation. The authors find this is due to aerosol indirect effects that are slightly stronger in the new version of the model and propose a solution to ameliorate this. Overall, in these early coupled simulations, CAM5.5 produces a credible climate that is appropriate for science applications and is ready for integration into the National Center for Atmospheric Research's (NCAR's) next-generation climate model.\n\nThe on-site quality-assurance system for Hyper Suprime-Cam: OSQAH\n\nScience.gov (United States)\n\nFurusawa, Hisanori; Koike, Michitaro; Takata, Tadafumi; Okura, Yuki; Miyatake, Hironao; Lupton, Robert H.; Bickerton, Steven; Price, Paul A.; Bosch, James; Yasuda, Naoki; Mineo, Sogo; Yamada, Yoshihiko; Miyazaki, Satoshi; Nakata, Fumiaki; Koshida, Shintaro; Komiyama, Yutaka; Utsumi, Yousuke; Kawanomoto, Satoshi; Jeschke, Eric; Noumaru, Junichi; Schubert, Kiaina; Iwata, Ikuru; Finet, Francois; Fujiyoshi, Takuya; Tajitsu, Akito; Terai, Tsuyoshi; Lee, Chien-Hsiu\n\n2018-01-01\n\nWe have developed an automated quick data analysis system for data quality assurance (QA) for Hyper Suprime-Cam (HSC). The system was commissioned in 2012-2014, and has been offered for general observations, including the HSC Subaru Strategic Program, since 2014 March. The system provides observers with data quality information, such as seeing, sky background level, and sky transparency, based on quick analysis as data are acquired. Quick-look images and validation of image focus are also provided through an interactive web application. The system is responsible for the automatic extraction of QA information from acquired raw data into a database, to assist with observation planning, assess progress of all observing programs, and monitor long-term efficiency variations of the instrument and telescope. Enhancements of the system are being planned to facilitate final data analysis, to improve the HSC archive, and to provide legacy products for astronomical communities.\n\nRecognition of pornographic web pages by classifying texts and images.\n\nScience.gov (United States)\n\nHu, Weiming; Wu, Ou; Chen, Zhouyao; Fu, Zhouyu; Maybank, Steve\n\n2007-06-01\n\nWith the rapid development of the World Wide Web, people benefit more and more from the sharing of information. However, Web pages with obscene, harmful, or illegal content can be easily accessed. It is important to recognize such unsuitable, offensive, or pornographic Web pages. In this paper, a novel framework for recognizing pornographic Web pages is described. A C4.5 decision tree is used to divide Web pages, according to content representations, into continuous text pages, discrete text pages, and image pages. These three categories of Web pages are handled, respectively, by a continuous text classifier, a discrete text classifier, and an algorithm that fuses the results from the image classifier and the discrete text classifier. In the continuous text classifier, statistical and semantic features are used to recognize pornographic texts. In the discrete text classifier, the naive Bayes rule is used to calculate the probability that a discrete text is pornographic. In the image classifier, the object's contour-based features are extracted to recognize pornographic images. In the text and image fusion algorithm, the Bayes theory is used to combine the recognition results from images and texts. Experimental results demonstrate that the continuous text classifier outperforms the traditional keyword-statistics-based classifier, the contour-based image classifier outperforms the traditional skin-region-based image classifier, the results obtained by our fusion algorithm outperform those by either of the individual classifiers, and our framework can be adapted to different categories of Web pages.\n\nQUANTIFICATION OF ANGIOGENESIS IN THE CHICKEN CHORIOALLANTOIC MEMBRANE (CAM\n\nDirectory of Open Access Journals (Sweden)\n\nSilvia Blacher\n\n2011-05-01\n\nFull Text Available The chick chorioallantoic membrane (CAM provides a suitable in vivo model to study angiogenesis and evaluate several pro- and anti-angiogenic factors and compounds. In the present work, new developments in image analysis are used to quantify CAM angiogenic response from optical microscopic observations, covering all vascular components, from the large supplying and feeding vessels down to the capillary plexus. To validate our methodology angiogenesis is quantified during two phases of CAM development (day 7 and 13 and after treatment with an antiangiogenic modulator of the angiogenesis. Our morphometric analysis emphasizes that an accurate quantification of the CAM vasculature needs to be performed at various scales.\n\nWeb Conversations About Complementary and Alternative Medicines and Cancer: Content and Sentiment Analysis\n\nScience.gov (United States)\n\nTruccolo, Ivana; Antonini, Marialuisa; Rinaldi, Fabio; Omero, Paolo; Ferrarin, Emanuela; De Paoli, Paolo; Tasso, Carlo\n\n2016-01-01\n\nBackground The use of complementary and alternative medicine (CAM) among cancer patients is widespread and mostly self-administrated. Today, one of the most relevant topics is the nondisclosure of CAM use to doctors. This general lack of communication exposes patients to dangerous behaviors and to less reliable information channels, such as the Web. The Italian context scarcely differs from this trend. Today, we are able to mine and analyze systematically the unstructured information available in the Web, to get an insight of peopleâs opinions, beliefs, and rumors concerning health topics. Objective Our aim was to analyze Italian Web conversations about CAM, identifying the most relevant Web sources, therapies, and diseases and measure the related sentiment. Methods Data have been collected using the Web Intelligence tool ifMONITOR. The workflow consisted of 6 phases: (1) eligibility criteria definition for the ifMONITOR search profile; (2) creation of a CAM terminology database; (3) generic Web search and automatic filtering, the results have been manually revised to refine the search profile, and stored in the ifMONITOR database; (4) automatic classification using the CAM database terms; (5) selection of the final sample and manual sentiment analysis using a 1-5 score range; (6) manual indexing of the Web sources and CAM therapies type retrieved. Descriptive univariate statistics were computed for each item: absolute frequency, percentage, central tendency (mean sentiment score [MSS]), and variability (standard variation Ï). Results Overall, 212 Web sources, 423 Web documents, and 868 opinions have been retrieved. The overall sentiment measured tends to a good score (3.6 of 5). Quite a high polarization in the opinions of the conversation partaking emerged from standard variation analysis (Ïâ¥1). In total, 126 of 212 (59.4%) Web sources retrieved were nonhealth-related. Facebook (89; 21%) and Yahoo Answers (41; 9.7%) were the most relevant. In total, 94 CAM\n\nCamera-augmented mobile C-arm (CamC): A feasibility study of augmented reality imaging in the operating room.\n\nScience.gov (United States)\n\nvon der Heide, Anna Maria; Fallavollita, Pascal; Wang, Lejing; Sandner, Philipp; Navab, Nassir; Weidert, Simon; Euler, Ekkehard\n\n2018-04-01\n\nIn orthopaedic trauma surgery, image-guided procedures are mostly based on fluoroscopy. The reduction of radiation exposure is an important goal. The purpose of this work was to investigate the impact of a camera-augmented mobile C-arm (CamC) on radiation exposure and the surgical workflow during a first clinical trial. Applying a workflow-oriented approach, 10 general workflow steps were defined to compare the CamC to traditional C-arms. The surgeries included were arbitrarily identified and assigned to the study. The evaluation criteria were radiation exposure and operation time for each workflow step and the entire surgery. The evaluation protocol was designed and conducted in a single-centre study. The radiation exposure was remarkably reduced by 18 X-ray shots 46% using the CamC while keeping similar surgery times. The intuitiveness of the system, its easy integration into the surgical workflow, and its great potential to reduce radiation have been demonstrated. Copyright Â© 2017 John Wiley & Sons, Ltd.\n\nSenseCam: A new tool for memory rehabilitation?\n\nScience.gov (United States)\n\nDubourg, L; Silva, A R; Fitamen, C; Moulin, C J A; Souchay, C\n\n2016-12-01\n\nThe emergence of life-logging technologies has led neuropsychologist to focus on understanding how this new technology could help patients with memory disorders. Despite the growing number of studies using life-logging technologies, a theoretical framework supporting its effectiveness is lacking. This review focuses on the use of life-logging in the context of memory rehabilitation, particularly the use of SenseCam, a wearable camera allowing passive image capture. In our opinion, reviewing SenseCam images can be effective for memory rehabilitation only if it provides more than an assessment of prior occurrence in ways that reinstates previous thoughts, feelings and sensory information, thus stimulating recollection. Considering the fact that, in memory impairment, self-initiated processes are impaired, we propose that the environmental support hypothesis can explain the value of SenseCam for memory retrieval. Twenty-five research studies were selected for this review and despite the general acceptance of the value of SenseCam as a memory technique, only a small number of studies focused on recollection. We discuss the usability of this tool to improve episodic memory and in particular, recollection. Copyright ÃÂ© 2016 Elsevier Masson SAS. All rights reserved.\n\nScipion web tools: Easy to use cryo-EM image processing over the web.\n\nScience.gov (United States)\n\nConesa Mingo, Pablo; Gutierrez, JosÃ©; Quintana, AdriÃ¡n; de la Rosa TrevÃ­n, JosÃ© Miguel; ZaldÃ­var-Peraza, AirÃ©n; Cuenca Alba, JesÃºs; Kazemi, Mohsen; Vargas, Javier; Del Cano, Laura; Segura, Joan; Sorzano, Carlos Oscar S; Carazo, Jose MarÃ­a\n\n2018-01-01\n\nMacromolecular structural determination by Electron Microscopy under cryogenic conditions is revolutionizing the field of structural biology, interesting a large community of potential users. Still, the path from raw images to density maps is complex, and sophisticated image processing suites are required in this process, often demanding the installation and understanding of different software packages. Here, we present Scipion Web Tools, a web-based set of tools/workflows derived from the Scipion image processing framework, specially tailored to nonexpert users in need of very precise answers at several key stages of the structural elucidation process. Â© 2017 The Protein Society.\n\nModified FlowCAM procedure for quantifying size distribution of zooplankton with sample recycling capacity.\n\nDirectory of Open Access Journals (Sweden)\n\nEsther Wong\n\nFull Text Available We have developed a modified FlowCAM procedure for efficiently quantifying the size distribution of zooplankton. The modified method offers the following new features: 1 prevents animals from settling and clogging with constant bubbling in the sample container; 2 prevents damage to sample animals and facilitates recycling by replacing the built-in peristaltic pump with an external syringe pump, in order to generate negative pressure, creates a steady flow by drawing air from the receiving conical flask (i.e. vacuum pump, and transfers plankton from the sample container toward the main flowcell of the imaging system and finally into the receiving flask; 3 aligns samples in advance of imaging and prevents clogging with an additional flowcell placed ahead of the main flowcell. These modifications were designed to overcome the difficulties applying the standard FlowCAM procedure to studies where the number of individuals per sample is small, and since the FlowCAM can only image a subset of a sample. Our effective recycling procedure allows users to pass the same sample through the FlowCAM many times (i.e. bootstrapping the sample in order to generate a good size distribution. Although more advanced FlowCAM models are equipped with syringe pump and Field of View (FOV flowcells which can image all particles passing through the flow field; we note that these advanced setups are very expensive, offer limited syringe and flowcell sizes, and do not guarantee recycling. In contrast, our modifications are inexpensive and flexible. Finally, we compared the biovolumes estimated by automated FlowCAM image analysis versus conventional manual measurements, and found that the size of an individual zooplankter can be estimated by the FlowCAM image system after ground truthing.\n\nFluidCam 1&2 - UAV-based Fluid Lensing Instruments for High-Resolution 3D Subaqueous Imaging and Automated Remote Biosphere Assessment of Reef Ecosystems\n\nScience.gov (United States)\n\nChirayath, V.; Instrella, R.\n\n2016-02-01\n\nWe present NASA ESTO FluidCam 1 & 2, Visible and NIR Fluid-Lensing-enabled imaging payloads for Unmanned Aerial Vehicles (UAVs). Developed as part of a focused 2014 earth science technology grant, FluidCam 1&2 are Fluid-Lensing-based computational optical imagers designed for automated 3D mapping and remote sensing of underwater coastal targets from airborne platforms. Fluid Lensing has been used to map underwater reefs in 3D in American Samoa and Hamelin Pool, Australia from UAV platforms at sub-cm scale, which has proven a valuable tool in modern marine research for marine biosphere assessment and conservation. We share FluidCam 1&2 instrument validation and testing results as well as preliminary processed data from field campaigns. Petabyte-scale aerial survey efforts using Fluid Lensing to image at-risk reefs demonstrate broad applicability to large-scale automated species identification, morphology studies and reef ecosystem characterization for shallow marine environments and terrestrial biospheres, of crucial importance to improving bathymetry data for physical oceanographic models and understanding climate change's impact on coastal zones, global oxygen production, carbon sequestration.\n\nAngle assessment by EyeCam, goniophotography, and gonioscopy.\n\nScience.gov (United States)\n\nBaskaran, Mani; Perera, Shamira A; Nongpiur, Monisha E; Tun, Tin A; Park, Judy; Kumar, Rajesh S; Friedman, David S; Aung, Tin\n\n2012-09-01\n\nTo compare EyeCam (Clarity Medical Systems, Pleasanton, CA) and goniophotography in detecting angle closure, using gonioscopy as the reference standard. In this hospital-based, prospective, cross-sectional study, participants underwent gonioscopy by a single observer, and EyeCam imaging and goniophotography by different operators. The anterior chamber angle in a quadrant was classified as closed if the posterior trabecular meshwork could not be seen. A masked observer categorized the eyes as per the number of closed quadrants, and an eye was classified as having angle closure if there were 2 or more quadrants of closure. Agreement between the methods was analyzed by Îº statistic and comparison of area under receiver operating characteristic curves (AUC). Eighty-five participants (85 eyes) were included, the majority of whom were Chinese. Angle closure was detected in 38 eyes (45%) with gonioscopy, 40 eyes (47%) using EyeCam, and 40 eyes (47%) with goniophotography (P=0.69 in both comparisons, McNemar test). The agreement for angle closure diagnosis (by eye) between gonioscopy and the 2 imaging modalities was high (Îº=0.86; 95% Confidence Interval (CI), 0.75-0.97), whereas the agreement between EyeCam and goniophotography was not as good (Îº=0.72; 95% CI, 0.57-0.87); largely due to lack of agreement in the nasal and temporal quadrants (Îº=0.55 to 0.67). The AUC for detecting eyes with gonioscopic angle closure was similar for goniophotography and EyeCam (AUC 0.93, sensitivity=94.7%, specificity=91.5%; P>0.95). EyeCam and goniophotography have similarly high sensitivity and specificity for the detection of gonioscopic angle closure.\n\nA World Wide Web Region-Based Image Search Engine\n\nDEFF Research Database (Denmark)\n\nKompatsiaris, Ioannis; Triantafyllou, Evangelia; Strintzis, Michael G.\n\n2001-01-01\n\nIn this paper the development of an intelligent image content-based search engine for the World Wide Web is presented. This system will offer a new form of media representation and access of content available in WWW. Information Web Crawlers continuously traverse the Internet and collect images...\n\nSenseCam improves memory for recent events and quality of life in a patient with memory retrieval difficulties.\n\nScience.gov (United States)\n\nBrowne, Georgina; Berry, Emma; Kapur, Narinder; Hodges, Steve; Smyth, Gavin; Watson, Peter; Wood, Ken\n\n2011-10-01\n\nA wearable camera that takes pictures automatically, SenseCam, was used to generate images for rehearsal, promoting consolidation and retrieval of memories for significant events in a patient with memory retrieval deficits. SenseCam images of recent events were systematically reviewed over a 2-week period. Memory for these events was assessed throughout and longer-term recall was tested up to 6 months later. A written diary control condition followed the same procedure. The SenseCam review procedure resulted in significantly more details of an event being recalled, with twice as many details recalled at 6 months follow up compared to the written diary method. Self-report measures suggested autobiographical recollection was triggered by the SenseCam condition but not by reviewing the written diary. Emotional and social wellbeing questionnaires indicated improved confidence and decreased anxiety as a result of memory rehearsal using SenseCam images. We propose that SenseCam images provide a powerful boost to autobiographical recall, with secondary benefits for quality of life.\n\nCryogenic Cam Butterfly Valve\n\nScience.gov (United States)\n\nMcCormack, Kenneth J. (Inventor)\n\n2016-01-01\n\nA cryogenic cam butterfly valve has a body that includes an axially extending fluid conduit formed there through. A disc lug is connected to a back side of a valve disc and has a circular bore that receives and is larger than a cam of a cam shaft. The valve disc is rotatable for a quarter turn within the body about a lug axis that is offset from the shaft axis. Actuating the cam shaft in the closing rotational direction first causes the camming side of the cam of the cam shaft to rotate the disc lug and the valve disc a quarter turn from the open position to the closed position. Further actuating causes the camming side of the cam shaft to translate the valve disc into sealed contact with the valve seat. Opening rotational direction of the cam shaft reverses these motions.\n\nMemory for staged events: Supporting older and younger adults' memory with SenseCam.\n\nScience.gov (United States)\n\nMair, Ali; Poirier, Marie; Conway, Martin A\n\n2018-03-01\n\nTwo experiments measured the effect of retrieval support provided by a wearable camera, SenseCam, on older and younger adults' memory for a recently experienced complex staged event. In each experiment, participants completed a series of tasks in groups, and the events were recalled 2 weeks later, after viewing SenseCam images (experimental condition) or thinking about the event (control condition). When IQ and education were matched, young adults recalled more event details than older adults, demonstrating an age-related deficit for novel autobiographical material. Reviewing SenseCam images increased the number of details recalled by older and younger adults, and the effect was similar for both groups. These results suggest that memory can be supported by the use of SenseCam, but the age-related deficit is not eliminated.\n\nComparison of EyeCam and anterior segment optical coherence tomography in detecting angle closure.\n\nScience.gov (United States)\n\nBaskaran, Mani; Aung, Tin; Friedman, David S; Tun, Tin A; Perera, Shamira A\n\n2012-12-01\n\nTo compare the diagnostic performance of EyeCam (Clarity Medical Systems, Pleasanton, CA, USA) and anterior segment optical coherence tomography (ASOCT, Visante; Carl Zeiss Meditec, Dublin, CA, USA) in detecting angle closure, using gonioscopy as the reference standard. Ninety-eight phakic patients, recruited from a glaucoma clinic, underwent gonioscopy by a single examiner, and EyeCam and ASOCT imaging by another examiner. Another observer, masked to gonioscopy findings, graded EyeCam and ASOCT images. For both gonioscopy and EyeCam, a closed angle in a particular quadrant was defined if the posterior trabecular meshwork was not visible. For ASOCT, angle closure was defined by any contact between the iris and angle anterior to the scleral spur. An eye was diagnosed as having angle closure if â¥2 quadrants were closed. Agreement and area under the receiver operating characteristic curves (AUC) were evaluated. The majority of subjects were Chinese (69/98, 70.4%) with a mean age of 60.6 years. Angle closure was diagnosed in 39/98 (39.8%) eyes with gonioscopy, 40/98 (40.8%) with EyeCam and 56/97 (57.7%) with ASOCT. The agreement (kappa statistic) for angle closure diagnosis for gonioscopy versus EyeCam was 0.89; gonioscopy versus ASOCT and EyeCam versus ASOCT were both 0.56. The AUC for detecting eyes with gonioscopic angle closure with EyeCam was 0.978 (95% CI: 0.93-1.0) and 0.847 (95% CI: 0.76-0.92, p < 0.01) for ASOCT. The diagnostic performance of EyeCam was better than ASOCT in detecting angle closure when gonioscopic grading was used as the reference standard. The agreement between the two imaging modalities was moderate. Â© 2012 The Authors. Acta Ophthalmologica Â© 2012 Acta Ophthalmologica Scandinavica Foundation.\n\nDual-Color Fluorescence Imaging of EpCAM and EGFR in Breast Cancer Cells with a Bull's Eye-Type Plasmonic Chip.\n\nScience.gov (United States)\n\nIzumi, Shota; Yamamura, Shohei; Hayashi, Naoko; Toma, Mana; Tawa, Keiko\n\n2017-12-19\n\nSurface plasmon field-enhanced fluorescence microscopic observation of a live breast cancer cell was performed with a plasmonic chip. Two cell lines, MDA-MB-231 and Michigan Cancer Foundation-7 (MCF-7), were selected as breast cancer cells, with two kinds of membrane protein, epithelial cell adhesion molecule (EpCAM) and epidermal growth factor receptor (EGFR), observed in both cells. The membrane proteins are surface markers used to differentiate and classify breast cancer cells. EGFR and EpCAM were detected with Alexa Fluor Â® 488-labeled anti-EGFR antibody (488-EGFR) and allophycocyanin (APC)-labeled anti-EpCAM antibody (APC-EpCAM), respectively. In MDA-MB231 cells, three-fold plus or minus one and seven-fold plus or minus two brighter fluorescence of 488-EGFR were observed on the 480-nm pitch and the 400-nm pitch compared with that on a glass slide. Results show the 400-nm pitch is useful. Dual-color fluorescence of 488-EGFR and APC-EpCAM in MDA-MB231 was clearly observed with seven-fold plus or minus two and nine-fold plus or minus three, respectively, on the 400-nm pitch pattern of a plasmonic chip. Therefore, the 400-nm pitch contributed to the dual-color fluorescence enhancement for these wavelengths. An optimal grating pitch of a plasmonic chip improved a fluorescence image of membrane proteins with the help of the surface plasmon-enhanced field.\n\nEfficient Image Blur in Web-Based Applications\n\nDEFF Research Database (Denmark)\n\nKraus, Martin\n\n2010-01-01\n\nScripting languages require the use of high-level library functions to implement efficient image processing; thus, real-time image blur in web-based applications is a challenging task unless specific library functions are available for this purpose. We present a pyramid blur algorithm, which can ...\n\nQuantifying particulate matter deposition in Niwot Ridge, Colorado: Collection of dry deposition using marble inserts and particle imaging using the FlowCAM\n\nScience.gov (United States)\n\nGoss, Natasha R.; Mladenov, Natalie; Seibold, Christine M.; Chowanski, Kurt; Seitz, Leslie; Wellemeyer, T. Barret; Williams, Mark W.\n\n2013-12-01\n\nAtmospheric wet and dry deposition are important sources of carbon for remote alpine lakes and soils. The carbon inputs from dry deposition in alpine National Atmospheric Deposition Program (NADP) collectors, including aeolian dust and biological material, are not well constrained due to difficulties in retaining particulate matter in the collectors. Here, we developed and tested a marble insert for dry deposition collection at the Niwot Ridge Long Term Ecological Research Station (NWT LTER) Soddie site (3345 m) between 24 May and 8 November 2011. We conducted laboratory tests of the insert's effect on particulate matter (PM) mass and non-purgeable organic carbon (DOC) and found that the insert did not significantly change either measurement. Thus, the insert may enable dry deposition collection of PM and DOC at NADP sites. We then developed a method for enumerating the collected wet and dry deposition with the Flow Cytometer and Microscope (FlowCAM), a dynamic-image particle analysis tool. The FlowCAM has the potential to establish morphology, which affects particle settling and retention, through particle diameter and aspect ratio. Particle images were used to track the abundance of pollen grains over time. Qualitative image examination revealed that most particles were biological in nature, such as intact algal cells and pollen. Dry deposition loading to the Soddie site as determined by FlowCAM measurements was highly variable, ranging from 100 to >230 g ha-1 d-1 in June-August 2011 and peaking in late June. No significant difference in diameter or aspect ratio was found between wet and dry deposition, suggesting fundamental similarities between those deposition types. Although FlowCAM statistics and identification of particle types proved insightful, our total-particle enumeration method had a high variance and underestimated the total number of particles when compared to imaging of relatively large volumes (60-125 mL) from a single sample. We recommend use of\n\nCalcium Sulfate Characterized by ChemCam/Curiosity at Gale Crater, Mars\n\nScience.gov (United States)\n\nNachon, M.; Clegg, S. N.; Mangold, N.; Schroeder, S.; Kah, L. C.; Dromart, G.; Ollila, A.; Johnson, J. R.; Oehler, D. Z.; Bridges, J. C.;\n\n2014-01-01\n\nOnboard the Mars Science Laboratory (MSL) Curiosity rover, the ChemCam instrument consists of :(1) a Laser-Induced Breakdown Spectrometer (LIBS) for elemental analysis of the targets [1;2] and (2) a Remote Micro Imager (RMI), for the imaging context of laser analysis [3]. Within the Gale crater, Curiosity traveled from Bradbury Landing through the Rocknest region and into Yellowknife Bay (YB). In the latter, abundant light-toned fracture-fill material were seen [4;5]. ChemCam analysis demonstrate that those fracture fills consist of calcium sulfates [6].\n\nRobust image obfuscation for privacy protection in Web 2.0 applications\n\nScience.gov (United States)\n\nPoller, Andreas; Steinebach, Martin; Liu, Huajian\n\n2012-03-01\n\nWe present two approaches to robust image obfuscation based on permutation of image regions and channel intensity modulation. The proposed concept of robust image obfuscation is a step towards end-to-end security in Web 2.0 applications. It helps to protect the privacy of the users against threats caused by internet bots and web applications that extract biometric and other features from images for data-linkage purposes. The approaches described in this paper consider that images uploaded to Web 2.0 applications pass several transformations, such as scaling and JPEG compression, until the receiver downloads them. In contrast to existing approaches, our focus is on usability, therefore the primary goal is not a maximum of security but an acceptable trade-off between security and resulting image quality.\n\nPrototype Web-based continuing medical education using FlashPix images.\n\nScience.gov (United States)\n\nLandman, A; Yagi, Y; Gilbertson, J; Dawson, R; Marchevsky, A; Becich, M J\n\n2000-01-01\n\nContinuing Medical Education (CME) is a requirement among practicing physicians to promote continuous enhancement of clinical knowledge to reflect new developments in medical care. Previous research has harnessed the Web to disseminate complete pathology CME case studies including history, images, diagnoses, and discussions to the medical community. Users submit real-time diagnoses and receive instantaneous feedback, eliminating the need for hard copies of case material and case evaluation forms. This project extends the Web-based CME paradigm with the incorporation of multi-resolution FlashPix images and an intuitive, interactive user interface. The FlashPix file format combines a high-resolution version of an image with a hierarchy of several lower resolution copies, providing real-time magnification via a single image file. The Web interface was designed specifically to simulate microscopic analysis, using the latest Javascript, Java and Common Gateway Interface tools. As the project progresses to the evaluation stage, it is hoped that this active learning format will provide a practical and efficacious environment for continuing medical education with additional application potential in classroom demonstrations, proficiency testing, and telepathology. Using Microsoft Internet Explorer 4.0 and above, the working prototype Web-based CME environment is accessible at http://telepathology.upmc.edu/WebInterface/NewInterface/welcome.html.\n\nWeb Based Distributed Coastal Image Analysis System, Phase I\n\nData.gov (United States)\n\nNational Aeronautics and Space Administration â This project develops Web based distributed image analysis system processing the Moderate Resolution Imaging Spectroradiometer (MODIS) data to provide decision...\n\nTracking vegetation phenology across diverse North American biomes using PhenoCam imagery: A new, publicly-available dataset\n\nScience.gov (United States)\n\nRichardson, A. D.\n\n2015-12-01\n\nVegetation phenology controls the seasonality of many ecosystem processes, as well as numerous biosphere-atmosphere feedbacks. Phenology is highly sensitive to climate change and variability, and is thus a key aspect of global change ecology. The goal of the PhenoCam network is to serve as a long-term, continental-scale, phenological observatory. The network uses repeat digital photographyâimages captured using conventional, visible-wavelength, automated digital camerasâto characterize vegetation phenology in diverse ecosystems across North America and around the world. At present, imagery from over 200 research sites, spanning a wide range of ecoregions, climate zones, and plant functional types, is currently being archived and processed in near-real-time through the PhenoCam project web page (http://phenocam.sr.unh.edu/). Data derived from PhenoCam imagery have been previously used to evaluate satellite phenology products, to constrain and test new phenology models, to understand relationships between canopy phenology and ecosystem processes, and to study the seasonal changes in leaf-level physiology that are associated with changes in leaf color. I will describe a new, publicly-available phenological dataset, derived from over 600 site-years of PhenoCam imagery. For each archived image (ca. 5 million), we extracted RGB (red, green, blue) color channel information, with means and other statistics calculated across a region-of-interest (ROI) delineating a specific vegetation type. From the high-frequency (typically, 30 minute) imagery, we derived time series characterizing vegetation color, including \"canopy greenness\", processed to 1- and 3-day intervals. For ecosystems with a single annual cycle of vegetation activity, we derived estimates, with uncertainties, for the start, middle, and end of spring and autumn phenological transitions. Given the lack of multi-year, standardized, and geographically distributed phenological data for North America, we\n\nSpecial Section: Complementary and Alternative Medicine (CAM): Low Back Pain and CAM\n\nScience.gov (United States)\n\n... Home Current Issue Past Issues Special Section CAM Low Back Pain and CAM Past Issues / Winter 2009 Table of ... benefit from CAM treatment for conditions such as low back pain. Photo courtesy of Glenn Scimonelli \"Oh, my aching ...\n\nResearch and implementation of a Web-based remote desktop image monitoring system\n\nInternational Nuclear Information System (INIS)\n\nRen Weijuan; Li Luofeng; Wang Chunhong\n\n2010-01-01\n\nIt studied and implemented an ISS (Image Snapshot Server) system based on Web, using Java Web technology. The ISS system consisted of client web browser and server. The server part could be divided into three modules as the screen shots software, web server and Oracle database. Screen shots software intercepted the desktop environment of the remote monitored PC and sent these pictures to a Tomcat web server for displaying on the web at real time. At the same time, these pictures were also saved in an Oracle database. Through the web browser, monitor person can view the real-time and historical desktop pictures of the monitored PC during some period. It is very convenient for any user to monitor the desktop image of remote monitoring PC. (authors)\n\nFacultative crassulacean acid metabolism (CAM) plants: powerful tools for unravelling the functional elements of CAM photosynthesis.\n\nScience.gov (United States)\n\nWinter, Klaus; Holtum, Joseph A M\n\n2014-07-01\n\nFacultative crassulacean acid metabolism (CAM) describes the optional use of CAM photosynthesis, typically under conditions of drought stress, in plants that otherwise employ C3 or C4 photosynthesis. In its cleanest form, the upregulation of CAM is fully reversible upon removal of stress. Reversibility distinguishes facultative CAM from ontogenetically programmed unidirectional C3-to-CAM shifts inherent in constitutive CAM plants. Using mainly measurements of 24h CO2 exchange, defining features of facultative CAM are highlighted in five terrestrial species, Clusia pratensis, Calandrinia polyandra, Mesembryanthemum crystallinum, Portulaca oleracea and Talinum triangulare. For these, we provide detailed chronologies of the shifts between photosynthetic modes and comment on their usefulness as experimental systems. Photosynthetic flexibility is also reviewed in an aquatic CAM plant, Isoetes howellii. Through comparisons of C3 and CAM states in facultative CAM species, many fundamental biochemical principles of the CAM pathway have been uncovered. Facultative CAM species will be of even greater relevance now that new sequencing technologies facilitate the mapping of genomes and tracking of the expression patterns of multiple genes. These technologies and facultative CAM systems, when joined, are expected to contribute in a major way towards our goal of understanding the essence of CAM. Â© The Author 2014. Published by Oxford University Press on behalf of the Society for Experimental Biology. All rights reserved. For permissions, please email: journals.permissions@oup.com.\n\npedigreejs: a web-based graphical pedigree editor.\n\nScience.gov (United States)\n\nCarver, Tim; Cunningham, Alex P; Babb de Villiers, Chantal; Lee, Andrew; Hartley, Simon; Tischkowitz, Marc; Walter, Fiona M; Easton, Douglas F; Antoniou, Antonis C\n\n2018-03-15\n\nThe collection, management and visualization of clinical pedigree (family history) data is a core activity in clinical genetics centres. However, clinical pedigree datasets can be difficult to manage, as they are time consuming to capture, and can be difficult to build, manipulate and visualize graphically. Several standalone graphical pedigree editors and drawing applications exist but there are no freely available lightweight graphical pedigree editors that can be easily configured and incorporated into web applications. We developed 'pedigreejs', an interactive graphical pedigree editor written in JavaScript, which uses standard pedigree nomenclature. Pedigreejs provides an easily configurable, extensible and lightweight pedigree editor. It makes use of an open-source Javascript library to define a hierarchical layout and to produce images in scalable vector graphics (SVG) format that can be viewed and edited in web browsers. The software is freely available under GPL licence (https://ccge-boadicea.github.io/pedigreejs/). tjc29@cam.ac.uk. Supplementary data are available at Bioinformatics online.\n\nUsing a web-based image quality assurance reporting system to improve image quality.\n\nScience.gov (United States)\n\nCzuczman, Gregory J; Pomerantz, Stuart R; Alkasab, Tarik K; Huang, Ambrose J\n\n2013-08-01\n\nThe purpose of this study is to show the impact of a web-based image quality assurance reporting system on the rates of three common image quality errors at our institution. A web-based image quality assurance reporting system was developed and used beginning in April 2009. Image quality endpoints were assessed immediately before deployment (period 1), approximately 18 months after deployment of a prototype reporting system (period 2), and approximately 12 months after deployment of a subsequent upgraded department-wide reporting system (period 3). A total of 3067 axillary shoulder radiographs were reviewed for correct orientation, 355 shoulder CT scans were reviewed for correct reformatting of coronal and sagittal images, and 346 sacral MRI scans were reviewed for correct acquisition plane of axial images. Error rates for each review period were calculated and compared using the Fisher exact test. Error rates of axillary shoulder radiograph orientation were 35.9%, 7.2%, and 10.0%, respectively, for the three review periods. The decrease in error rate between periods 1 and 2 was statistically significant (p < 0.0001). Error rates of shoulder CT reformats were 9.8%, 2.7%, and 5.8%, respectively, for the three review periods. The decrease in error rate between periods 1 and 2 was statistically significant (p = 0.03). Error rates for sacral MRI axial sequences were 96.5%, 32.5%, and 3.4%, respectively, for the three review periods. The decrease in error rates between periods 1 and 2 and between periods 2 and 3 was statistically significant (p < 0.0001). A web-based system for reporting image quality errors may be effective for improving image quality.\n\nCharacterization of SynCAM surface trafficking using a SynCAM derived ligand with high homophilic binding affinity\n\nInternational Nuclear Information System (INIS)\n\nBreillat, Christelle; Thoumine, Olivier; Choquet, Daniel\n\n2007-01-01\n\nIn order to better probe SynCAM function in neurons, we produced a fusion protein between the extracellular domain of SynCAM1 and the constant fragment of human IgG (SynCAM-Fc). Whether in soluble form or immobilized on latex microspheres, the chimera bound specifically to the surface of hippocampal neurons and recruited endogenous SynCAM molecules. SynCAM-Fc was also used in combination with Quantum Dots to follow the mobility of transfected SynCAM receptors at the neuronal surface. Both immobile and highly mobile SynCAM were found. Thus, SynCAM-Fc behaves as a high affinity ligand that can be used to study the function of SynCAM at the neuronal membrane\n\nApplication of an internet web-site of medical images in tele-radiology\n\nInternational Nuclear Information System (INIS)\n\nWang Weizhong; Wang Hua; Xie Jingxia; Wang Songzhang; Li Xiangdong; Qian Min; Cao Huixia\n\n2000-01-01\n\nObjective: To build and Internet web-site of medical images for tele-education and tele-consultation. Methods: Collecting medical images of cases that fulfilled diagnostic standards for teaching and were pathologically proved. The images were digitized using digital camera and scanner. Frontpage 98, Homesite 2.5 and text editors were used for programming. Results: The web site encompasses many useful cases and was update every week. With smart and friendly interface, easy used navigation, the site runs reliably in TCP/IP environment. The site's URL is http://imager.163.net. At present, the site has received about 100 visits per week. Conclusion: The well-designed and programmed internet web site of medical images would be easily acceptable and is going to play an important role in tele-education and tele-consultation\n\nFeasibility of a SenseCam-assisted 24-h recall to reduce under-reporting of energy intake.\n\nScience.gov (United States)\n\nGemming, L; Doherty, A; Kelly, P; Utter, J; Ni Mhurchu, C\n\n2013-10-01\n\nThe SenseCam is a camera worn on a lanyard around the neck that automatically captures point-of-view images in response to movement, heat and light (every 20-30âs). This device may enhance the accuracy of self-reported dietary intake by assisting participants' recall of food and beverage consumption. It was the objective of this study to evaluate if the wearable camera, SenseCam, can enhance the 24-h dietary recall by providing visual prompts to improve recall of food and beverage consumption. Thirteen volunteer adults in Oxford, United Kingdom, were recruited. Participants wore the SenseCam for 2 days while continuing their usual daily activities. On day 3, participants' diets were assessed using an interviewer-administered 24-h recall. SenseCam images were then shown to the participants and any additional dietary information that participants provided after viewing the images was recorded. Energy and macronutrient intakes were compared between the 24-h recall and 24-h recall+SenseCam. Data from 10 participants were included in the final analysis (8 males and 2 females), mean age 33 Â± 11 years, mean BMI 25.9 Â± 5.1âkg/m(2). Viewing the SenseCam images increased self-reported energy intake by approximately 1432 Â± 1564âkJ or 12.5% compared with the 24-h recall alone (P=0.02). The increase was predominantly due to reporting of 41 additional foods (241 vs 282 total foods) across a range of food groups. Eight changes in portion size were made, which resulted in a negligible change to energy intake. Wearable cameras are promising method to enhance the accuracy of self-reported dietary assessment methods.\n\nPreCam\n\nEnergy Technology Data Exchange (ETDEWEB)\n\nAllam, Sahar S. [Fermilab; Tucker, Douglas L. [Fermilab\n\n2015-01-01\n\nThe Dark Energy Survey (DES) will be taking the next step in probing the properties of Dark Energy and in understanding the physics of cosmic acceleration. A step towards the photometric calibration of DES is to have a quick, bright survey in the DES footprint (PreCam), using a pre-production set of the Dark Energy Camera (DECam) CCDs and a set of 100 mmÃ100 mm DES filters. The objective of the PreCam Survey is to create a network of calibrated DES grizY standard stars that will be used for DES nightly calibrations and to improve the DES global relative calibrations. Here, we describe the first year of PreCam observation, results, and photometric calibrations.\n\nImageJS: Personalized, participated, pervasive, and reproducible image bioinformatics in the web browser\n\nDirectory of Open Access Journals (Sweden)\n\nJonas S Almeida\n\n2012-01-01\n\nFull Text Available Background: Image bioinformatics infrastructure typically relies on a combination of server-side high-performance computing and client desktop applications tailored for graphic rendering. On the server side, matrix manipulation environments are often used as the back-end where deployment of specialized analytical workflows takes place. However, neither the server-side nor the client-side desktop solution, by themselves or combined, is conducive to the emergence of open, collaborative, computational ecosystems for image analysis that are both self-sustained and user driven. Materials and Methods: ImageJS was developed as a browser-based webApp, untethered from a server-side backend, by making use of recent advances in the modern web browser such as a very efficient compiler, high-end graphical rendering capabilities, and I/O tailored for code migration. Results : Multiple versioned code hosting services were used to develop distinct ImageJS modules to illustrate its amenability to collaborative deployment without compromise of reproducibility or provenance. The illustrative examples include modules for image segmentation, feature extraction, and filtering. The deployment of image analysis by code migration is in sharp contrast with the more conventional, heavier, and less safe reliance on data transfer. Accordingly, code and data are loaded into the browser by exactly the same script tag loading mechanism, which offers a number of interesting applications that would be hard to attain with more conventional platforms, such as NIHâ²s popular ImageJ application. Conclusions : The modern web browser was found to be advantageous for image bioinformatics in both the research and clinical environments. This conclusion reflects advantages in deployment scalability and analysis reproducibility, as well as the critical ability to deliver advanced computational statistical procedures machines where access to sensitive data is controlled, that is, without\n\nImageJS: Personalized, participated, pervasive, and reproducible image bioinformatics in the web browser.\n\nScience.gov (United States)\n\nAlmeida, Jonas S; Iriabho, Egiebade E; Gorrepati, Vijaya L; Wilkinson, Sean R; GrÃ¼neberg, Alexander; Robbins, David E; Hackney, James R\n\n2012-01-01\n\nImage bioinformatics infrastructure typically relies on a combination of server-side high-performance computing and client desktop applications tailored for graphic rendering. On the server side, matrix manipulation environments are often used as the back-end where deployment of specialized analytical workflows takes place. However, neither the server-side nor the client-side desktop solution, by themselves or combined, is conducive to the emergence of open, collaborative, computational ecosystems for image analysis that are both self-sustained and user driven. ImageJS was developed as a browser-based webApp, untethered from a server-side backend, by making use of recent advances in the modern web browser such as a very efficient compiler, high-end graphical rendering capabilities, and I/O tailored for code migration. Multiple versioned code hosting services were used to develop distinct ImageJS modules to illustrate its amenability to collaborative deployment without compromise of reproducibility or provenance. The illustrative examples include modules for image segmentation, feature extraction, and filtering. The deployment of image analysis by code migration is in sharp contrast with the more conventional, heavier, and less safe reliance on data transfer. Accordingly, code and data are loaded into the browser by exactly the same script tag loading mechanism, which offers a number of interesting applications that would be hard to attain with more conventional platforms, such as NIH's popular ImageJ application. The modern web browser was found to be advantageous for image bioinformatics in both the research and clinical environments. This conclusion reflects advantages in deployment scalability and analysis reproducibility, as well as the critical ability to deliver advanced computational statistical procedures machines where access to sensitive data is controlled, that is, without local \"download and installation\".\n\nAlaskan Auroral All-Sky Images on the World Wide Web\n\nScience.gov (United States)\n\nStenbaek-Nielsen, H. C.\n\n1997-01-01\n\nIn response to a 1995 NASA SPDS announcement of support for preservation and distribution of important data sets online, the Geophysical Institute, University of Alaska Fairbanks, Alaska, proposed to provide World Wide Web access to the Poker Flat Auroral All-sky Camera images in real time. The Poker auroral all-sky camera is located in the Davis Science Operation Center at Poker Flat Rocket Range about 30 miles north-east of Fairbanks, Alaska, and is connected, through a microwave link, with the Geophysical Institute where we maintain the data base linked to the Web. To protect the low light-level all-sky TV camera from damage due to excessive light, we only operate during the winter season when the moon is down. The camera and data acquisition is now fully computer controlled. Digital images are transmitted each minute to the Web linked data base where the data are available in a number of different presentations: (1) Individual JPEG compressed images (1 minute resolution); (2) Time lapse MPEG movie of the stored images; and (3) A meridional plot of the entire night activity.\n\nDemonstration of angle widening using EyeCam after laser peripheral iridotomy in eyes with angle closure.\n\nScience.gov (United States)\n\nPerera, Shamira A; Quek, Desmond T; Baskaran, Mani; Tun, Tin A; Kumar, Rajesh S; Friedman, David S; Aung, Tin\n\n2010-06-01\n\nTo evaluate EyeCam in detecting changes in angle configuration after laser peripheral iridotomy (LPI) in comparison to gonioscopy, the reference standard. Prospective comparative study. Twenty-four subjects (24 eyes) with primary angle-closure glaucoma (PACG) were recruited. Gonioscopy and EyeCam (Clarity Medical Systems) imaging of all 4 angle quadrants were performed, before and 2 weeks after LPI. Images were graded according to angle structures visible by an observer masked to clinical data or the status of LPI, and were performed in a random order. Angle closure in a quadrant was defined as the inability to visualize the posterior trabecular meshwork. We determined the number of quadrants with closed angles and the mean number of clock hours of angle closure before and after LPI in comparison to gonioscopy. Using EyeCam, all 24 eyes showed at least 1 quadrant of angle widening after LPI. The mean number of clock hours of angle closure decreased significantly, from 8.15 +/- 3.47 clock hours before LPI to 1.75 +/- 2.27 clock hours after LPI (P gonioscopy showed 1.0 +/- 1.41 (95% CI, 0.43-1.57) quadrants opening from closed to open after LPI compared to 2.0 +/- 1.28 (95% CI, 1.49-2.51, P = .009) quadrants with EyeCam. Intra-observer reproducibility of grading the extent of angle closure in clock hours in EyeCam images was moderate to good (intraclass correlation coefficient 0.831). EyeCam may be used to document changes in angle configuration after LPI in eyes with PACG. Copyright 2010 Elsevier Inc. All rights reserved.\n\nA Web Service for File-Level Access to Disk Images\n\nDirectory of Open Access Journals (Sweden)\n\nSunitha Misra\n\n2014-07-01\n\nFull Text Available Digital forensics tools have many potential applications in the curation of digital materials in libraries, archives and museums (LAMs. Open source digital forensics tools can help LAM professionals to extract digital contents from born-digital media and make more informed preservation decisions. Many of these tools have ways to display the metadata of the digital media, but few provide file-level access without having to mount the device or use complex command-line utilities. This paper describes a project to develop software that supports access to the contents of digital media without having to mount or download the entire image. The work examines two approaches in creating this tool: First, a graphical user interface running on a local machine. Second, a web-based application running in web browser. The project incorporates existing open source forensics tools and libraries including The Sleuth Kit and libewf along with the Flask web application framework and custom Python scripts to generate web pages supporting disk image browsing.\n\nTwo imaging techniques for 3D quantification of pre-cementation space for CAD/CAM crowns.\n\nScience.gov (United States)\n\nRungruanganunt, Patchanee; Kelly, J Robert; Adams, Douglas J\n\n2010-12-01\n\nInternal three-dimensional (3D) \"fit\" of prostheses to prepared teeth is likely more important clinically than \"fit\" judged only at the level of the margin (i.e. marginal \"opening\"). This work evaluates two techniques for quantitatively defining 3D \"fit\", both using pre-cementation space impressions: X-ray microcomputed tomography (micro-CT) and quantitative optical analysis. Both techniques are of interest for comparison of CAD/CAM system capabilities and for documenting \"fit\" as part of clinical studies. Pre-cementation space impressions were taken of a single zirconia coping on its die using a low viscosity poly(vinyl siloxane) impression material. Calibration specimens of this material were fabricated between the measuring platens of a micrometre. Both calibration curves and pre-cementation space impression data sets were obtained by examination using micro-CT and quantitative optical analysis. Regression analysis was used to compare calibration curves with calibration sets. Micro-CT calibration data showed tighter 95% confidence intervals and was able to measure over a wider thickness range than for the optical technique. Regions of interest (e.g., lingual, cervical) were more easily analysed with optical image analysis and this technique was more suitable for extremely thin impression walls (impressions. Each has advantages and limitations but either technique has the potential for use as part of clinical studies or CAD/CAM protocol optimization. Copyright Â© 2010 Elsevier Ltd. All rights reserved.\n\nOptimisation Methods for Cam Mechanisms\n\nDirectory of Open Access Journals (Sweden)\n\nClaudiaâMari Popa\n\n2010-01-01\n\nFull Text Available In this paper we present the criteria which represent the base of optimizing the cam mechanisms and also we perform the calculations for several types of mechanisms. We study the influence of the constructive parameters in case of the simple machines with rotation cam and follower (flat or curve of translation on the curvature radius and that of the transmission angle. As it follows, we present the optimization calculations of the cam and flat rotation follower mechanisms, as well as the calculations for optimizing the cam mechanisms by circular groove followersâ help. For an easier interpretation of the results, we have visualized the obtained cam in AutoCAD according to the script files generated by a calculation program.\n\nThe iMars WebGIS - Spatio-Temporal Data Queries and Single Image Map Web Services\n\nScience.gov (United States)\n\nWalter, Sebastian; Steikert, Ralf; Schreiner, Bjoern; Muller, Jan-Peter; van Gasselt, Stephan; Sidiropoulos, Panagiotis; Lanz-Kroechert, Julia\n\n2017-04-01\n\nIntroduction: Web-based planetary image dissemination platforms usually show outline coverages of the data and offer querying for metadata as well as preview and download, e.g. the HRSC Mapserver (Walter & van Gasselt, 2014). Here we introduce a new approach for a system dedicated to change detection by simultanous visualisation of single-image time series in a multi-temporal context. While the usual form of presenting multi-orbit datasets is the merge of the data into a larger mosaic, we want to stay with the single image as an important snapshot of the planetary surface at a specific time. In the context of the EU FP-7 iMars project we process and ingest vast amounts of automatically co-registered (ACRO) images. The base of the co-registration are the high precision HRSC multi-orbit quadrangle image mosaics, which are based on bundle-block-adjusted multi-orbit HRSC DTMs. Additionally we make use of the existing bundle-adjusted HRSC single images available at the PDS archives. A prototype demonstrating the presented features is available at http://imars.planet.fu-berlin.de. Multi-temporal database: In order to locate multiple coverage of images and select images based on spatio-temporal queries, we converge available coverage catalogs for various NASA imaging missions into a relational database management system with geometry support. We harvest available metadata entries during our processing pipeline using the Integrated Software for Imagers and Spectrometers (ISIS) software. Currently, this database contains image outlines from the MGS/MOC, MRO/CTX and the MO/THEMIS instruments with imaging dates ranging from 1996 to the present. For the MEx/HRSC data, we already maintain a database which we automatically update with custom software based on the VICAR environment. Web Map Service with time support: The MapServer software is connected to the database and provides Web Map Services (WMS) with time support based on the START_TIME image attribute. It allows temporal\n\nWeb based tools for visualizing imaging data and development of XNATView, a zero footprint image viewer.\n\nScience.gov (United States)\n\nGutman, David A; Dunn, William D; Cobb, Jake; Stoner, Richard M; Kalpathy-Cramer, Jayashree; Erickson, Bradley\n\n2014-01-01\n\nAdvances in web technologies now allow direct visualization of imaging data sets without necessitating the download of large file sets or the installation of software. This allows centralization of file storage and facilitates image review and analysis. XNATView is a light framework recently developed in our lab to visualize DICOM images stored in The Extensible Neuroimaging Archive Toolkit (XNAT). It consists of a PyXNAT-based framework to wrap around the REST application programming interface (API) and query the data in XNAT. XNATView was developed to simplify quality assurance, help organize imaging data, and facilitate data sharing for intra- and inter-laboratory collaborations. Its zero-footprint design allows the user to connect to XNAT from a web browser, navigate through projects, experiments, and subjects, and view DICOM images with accompanying metadata all within a single viewing instance.\n\nExcitatory Synaptic Drive and Feedforward Inhibition in the Hippocampal CA3 Circuit Are Regulated by SynCAM 1.\n\nScience.gov (United States)\n\nPark, Kellie A; Ribic, Adema; Laage Gaupp, Fabian M; Coman, Daniel; Huang, Yuegao; Dulla, Chris G; Hyder, Fahmeed; Biederer, Thomas\n\n2016-07-13\n\nSelect adhesion proteins control the development of synapses and modulate their structural and functional properties. Despite these important roles, the extent to which different synapse-organizing mechanisms act across brain regions to establish connectivity and regulate network properties is incompletely understood. Further, their functional roles in different neuronal populations remain to be defined. Here, we applied diffusion tensor imaging (DTI), a modality of magnetic resonance imaging (MRI), to map connectivity changes in knock-out (KO) mice lacking the synaptogenic cell adhesion protein SynCAM 1. This identified reduced fractional anisotropy in the hippocampal CA3 area in absence of SynCAM 1. In agreement, mossy fiber refinement in CA3 was impaired in SynCAM 1 KO mice. Mossy fibers make excitatory inputs onto postsynaptic specializations of CA3 pyramidal neurons termed thorny excrescences and these structures were smaller in the absence of SynCAM 1. However, the most prevalent targets of mossy fibers are GABAergic interneurons and SynCAM 1 loss unexpectedly reduced the number of excitatory terminals onto parvalbumin (PV)-positive interneurons in CA3. SynCAM 1 KO mice additionally exhibited lower postsynaptic GluA1 expression in these PV-positive interneurons. These synaptic imbalances in SynCAM 1 KO mice resulted in CA3 disinhibition, in agreement"
    }
}
{
    "id": "dbpedia_1709_0",
    "rank": 29,
    "data": {
        "url": "https://cana.lis-lab.fr/",
        "read_more_link": "",
        "language": "en",
        "title": "CANA · CANA",
        "top_image": "",
        "meta_img": "",
        "images": [
            "https://cana.lis-lab.fr/images/cc-heart.png",
            "https://cana.lis-lab.fr/images/logo_lis.png",
            "https://cana.lis-lab.fr/images/logo_amu.png"
        ],
        "movies": [],
        "keywords": [],
        "meta_keywords": [
            ""
        ],
        "tags": null,
        "authors": [],
        "publish_date": null,
        "summary": "",
        "meta_description": "",
        "meta_lang": "en",
        "meta_favicon": "/favicon.png",
        "meta_site_name": "",
        "canonical_link": null,
        "text": "See abstract\n\nA Bell scenario can be conceptualized as a “communication” scenario with zero rounds of communication between parties, i.e., although each party can receive a system from its environment on which it can implement a measurement, it cannot send out any system to another party. Under this constraint, there is a strict hierarchy of correlation sets, namely, classical, quantum, and non-signalling. However, without any constraints on the number of communication rounds between the parties, they can realize arbitrary correlations by exchanging only classical systems. We consider a multipartite scenario where the parties can engage in at most a single round of communication, i.e., each party is allowed to receive a system once, implement any local intervention on it, and send out the resulting system once. Taking our cue from Bell nonlocality in the “zero rounds” scenario, we propose a notion of nonclassicality—termed antinomicity—for correlations in scenarios with a single round of communication. Similar to the zero rounds case, we establish a strict hierarchy of correlation sets classified by their antinomicity in single-round communication scenarios. Since we do not assume a global causal order between the parties, antinomicity serves as a notion of nonclassicality in the presence of indefinite causal order (as witnessed by causal inequality violations). A key contribution of this work is an explicit antinomicity witness that goes beyond causal inequalities, inspired by a modification of the Guess Your Neighbour’s Input (GYNI) game that we term the Guess Your Neighbour’s Input or NOT (GYNIN) game. Time permitting, I will speculate on why antinomicity is a strong notion of nonclassicality by interpreting it as an example of fine-tuning in classical models of indefinite causality.\n\nSee abstract\n\nMany quantum phenomena have classical counterparts, and are modeled by mathematical structures that are noncommutative generalizations of the mathematical structures that model these classical counterparts. Quantization is the process of finding such as noncommutative generalization, and relies heavily on the theory of operator algebras, i.e., algebras of continuous linear maps on a fixed Hilbert space. Operator algebras generalize complex-valued matrix algebras, and can be used to describe quantum systems beyond finite-level systems. We will discuss a relatively new quantization method, called discrete quantization, based on the concepts of quantum sets and quantum relations between them. Quantum sets are essentially (possibly infinite) sums of matrix algebras and form a class of operator algebras that generalize sets, whereas quantum relations are a kind of morphism between quantum sets generalizing binary relations between ordinary sets. We discuss how several structures relevant for quantum information theory such as the quantum hamming distance and quantum graphs can be understood via discrete quantization. Moreover, we will discuss quantum cpos, a noncommutative generalization of omega-complete partial orders (cpos), obtained via discrete quantization. Ordinary cpos are the main objects of study in domain theory, which is the underlying mathematical theory for the construction of mathematical models of programming languages in a structured way. In a similar way, we discuss how quantum cpos can be used for the construction of mathematical models of quantum programming languages in a structured way.\n\nSee abstract\n\nCategorical quantum mechanics and applied category theory are fields that study systems and processes by abstracting the main structures and properties of their models. A focus is put on compositionality: how to compose processes together and to describe complex systems from their subsystems. An important structure studied in these fields is given by compact closed categories. They correspond to models of one-to-one processes that can be sequentially composed (a category), where systems and processes can be composed in parallel (a tensor/monoidal product) and each system has a dual system that reverse the flow of information. Processes with many inputs and outputs can then be modelled as processes between the tensor product of the inputs and the tensor product of the outputs. Linear logic is a substructural logic where the information in a proof cannot be duplicated nor erased. That is, every hypothesis has to be used exactly once. It is closely connected to linear type theory through a Curry-Howard correspondence. Through categorical semantics one can study the structures and properties of its models. An important structure in this field is given by *-autonomous categories. These correspond to models of one-to-one proofs that can be sequentially composed (a category), where logical propositions can be composed in two ways, the conjunction and disjunction (two monoidal products, the tensor and the par), and where each proposition can be negated (a duality). Proofs with many hypotheses and many conclusions can then be modelled as proofs from the conjunction of the hypotheses to the disjunction of the conclusions. It turns out that a compact closed category is exactly a *-autonomous category where the two monoidal products coincide, i.e. the conjunction and disjunction have the same interpretation. This lead to a line of research exploring the connection between linear logic and quantum mechanics, amongst others. For example, Aleks Kissinger and Sander Uijlen showed that starting with a compact closed category modelling systems and processes, one can studied their causal structure logically by organising the causal systems and processes into a *-autonomous category. In this talk, I will consider these connections by adopting a slightly different perspective. Instead of considering one-to-one processes or proofs, I will take many-to-many ones as a primitive, in a structure called a polycategory. The monoidal products (and the duals) will then be recovered through a universal property akin to the one of the tensor products of vector spaces. This will determine the interpretation of the composition of systems or logical propositions uniquely, up-to unique isomorphism, instead of having it provided as extra data. Furthermore, it will make apparent another defining distinction between the compact closed and the *-autonomous models: in the latter the sequential composition is done along one specific object while in the former two processes can be composed along multiple systems. In particular compact closed categories allow for feedback and loops while *-autonomous categories don’t. I will then introduce bifibred polycategories explaining how they can be used to model the emergence of a logical framework structuring systems equipped with specific conditions. They are in part inspired by Hoare logic, a framework used in computer science to reason about and verify properties of programs. As an example, I will consider the case of finite dimensional Banach spaces and contractive polylinear maps. Finite dimensional vector spaces and polylinear maps will provide a model of systems and processes while the norms will be used to express conditions on states of the systems via (sub-)unitality. Another example is given by the aforementioned construction of causal structures. If time permits, I will present some research directions I am considering to extend this framework.\n\nSee abstract\n\nThe topological properties of a set have a strong impact on its computability properties. A striking illustration of this idea is given by spheres and closed manifolds : if a set X is homeomorphic to a sphere or a closed manifold, then any algorithm that semicomputes X in some sense can be converted into an algorithm that fully computes X. In other words, the topological properties of X enable one to derive full information about X from partial information about X. In that case, we say that X has computable type. Those results have been obtained by Miller and Iljazović and others in the recent years. We give a characterization of finite simplicial complexes having computable type using a local property of stars of vertices. Moreover, the reduction to homology implies that the computable type property is decidable, for finite simplicial complexes of dimension at most 4. We argue that the stronger, relativized version of computable type, is better behaved. Consequently, we obtain characterizations of strong computable type, related to the descriptive complexity of topological invariants. This leads us to investigate the expressive power of low complexity invariants in the Borel hierarchy and their ability to differentiate between spaces. Notably, we show that they are sufficient for distinguishing finite topological graphs.\n\nSee abstract\n\nHow to stack an infinite number of oranges to maximize the proportion of the covered space? Kepler conjectured that the ``cannonball’ packing is an optimal way to do it. This conjecture took almost 400 years to prove, and the proof of Hales and Ferguson consists of 6 papers and tens of thousands of lines of computer code. Given an infinite number of coins of 3 fixed radii, how to place them on an infinite table to maximize the proportion of the covered surface? Triangulated disc packings are those where each “hole” is bounded by three pairwise tangent discs. Connelly conjectured that for the sets of disc radii where triangulated packings exist, one of them maximizes the proportion of the covered surface; this holds for unary and binary disc packings. In this talk, we will discuss various techniques used in the proof of the Kepler conjecture and other important results in the domain of disc and sphere packings. They allow us to prove the statement of the Connelly conjecture for 31 triangulated triplets of disc radii and disprove it for 45 other triplets. Besides that, we obtain tight bounds on the local density of simplicial cells in 2-sphere packings. Besides that, we will talk about some open questions of the domain in connection with tilings and computability.\n\nSee abstract\n\nNo physical system is every truly isolated from its surrounding environment. This particularly holds in quantum mechanics and leads to complex memory effects in the multi-time statistics of (quantum) stochastic processes. In contrast to the standard paradigm of classical stochastic processes, measurements in quantum mechanics are generally invasive, i.e., they change the state of the probed system, leading to a multitude of novel phenomena.\n\nIn particular, invasiveness a priori stands in the way of a consistent quantum generalization of basic concepts used in the description of classical stochastic processes. In my talk, I will discuss how fundamental tenets of the theory of classical stochastic processes, like the Kolmogorov extension theorem (defining the notion of a stochastic process) and Markov order (defining the length/strength of memory) can meaningfully be recovered in quantum mechanics.\n\nGoing beyond these generalizations of classical concepts, quantum memory possesses a much richer structure than its classical counterpart and affords for memory effects that do not exist in the classical world. I will discuss the instrument dependence of memory effects in quantum mechanics, as well as the possibility of hidden quantum memory, i.e., the existence of quantum processes with Markovian statistics that nonetheless fundamentally require memory for their implementation.\n\nFinally, invasiveness, one of the main ‘culprits’ for the complexity of quantum memory effects could, in principle, also be implemented in classical physics, putting in question the ‘quantumness’ of the aforementioned phenomena. By introducing the concept of ‘genuinely quantum processes’, I will however argue that, while an active choice in classical physics, invasiveness is a fundamentally unavoidable trait in quantum processes.\n\nSee abstract\n\nOne of the most basic tasks in information theory is communication. The capacity of a quantum channel is the maximum rate at which information can be transmitted through it reliably (in the asymptotic limit) per use of the channel. Mathematically, the various definitions of capacities are expressed in terms of entropies. They are in general difficult to compute. However, they can be bounded with the help of continuity bounds on entropies. The latter are generally expressed in terms of a single distance measure between probability distributions or quantum states, typically, the total variation- or trace distance. However, if an additional distance measure is known, the continuity bounds can be significantly strengthened. Here, we prove a tight uniform continuity bound for the Shannon entropy in terms of both the local- and total variation distances. We also obtain a uniform continuity bound for the von Neumann entropy in terms of both the operator norm- and trace distances. We then apply our results to compute upper bounds on channel capacities. We first introduce (ε,ν)-degradable channels, which are ε-close in diamond norm and ν-close in completely bounded spectral norm to their complementary channel when composed with a degrading channel. This leads to improved upper bounds to the quantum- and private classical capacities of such channels. Moreover, these bounds can be further improved by considering certain unstabilized versions of the above norms. We show that upper bounds on the latter can be efficiently expressed as semidefinite programs. As an application, we obtain a new upper bound on the quantum capacity of the qubit depolarizing channel.\n\nSee abstract\n\nNesting transformations (or logic gates) is a common concept in computer science as some gates may depend on other gates. A typical example of this is the switch gate: it takes two gates as inputs and applies them on a target bit in an order controlled by another bit. Nothing forbids a priori to devise the analogous paradigm in quantum computing. But, contrastingly with the classical case, higher-order quantum transformations lead to the phenomenon of indefinite causal order (ICO) as previous works on the matter have shown. In the same way that a quantum state can be superposed between several values, hence indefinite, the order in which quantum transformations are applied on a quantum state can become superposed between several orderings, hence in ICO. For example, the quantum switch gate takes two transformations A and B as inputs and applies them on a target qubit in an order determined by a control qubit: if the control is 0, A then B are applied on the target; and if the control is 1, it is B then A. When the control qubit is in a superposed state, then the ordering becomes superposed between A then B and B then A.\n\nTherefore, extending quantum theory in a computer-science-motivated manner revealed ICO, a feature believed to be a prerogative of quantum gravity. In addition to this transversal aspect, ICO is also useful for practical purposes, as it was shown to provide an advantage for a variety of tasks in many fields such as quantum computing, information, metrology, and communications.\n\nThe goal of this talk is to present a framework that formalizes and characterizes higher-order quantum theory. The two main questions answered are “Given an operator (which mathematically represents all kinds of quantum transformations), does it represent a higher-order transformation?” and “What is the underlying causal structure(s) of such an object?”. It extends two previous characterizations, one done using type theory, and the other one using category theory. This extension relies in particular on the use of superoperator projectors. These projectors have a twofold advantage: first, they make the characterization more straightforward as one can answer the first question simply by applying the projector corresponding to a given higher-order object on the operator. Second, they offer an intuitive explanation of the type-theoretic semantics of higher order: they are the algebraic rules for composing these projectors.\n\nSee abstract\n\nInspired by the works of Goldreich and Ron (J. ACM, 2017) and Nakar and Ron (ICALP, 2021), we initiate the study of property testing in dynamic environments with arbitrary topologies. Our focus is on the simplest non-trivial rule that can be tested, which corresponds to the 1-BP rule of bootstrap percolation and models a simple spreading behavior: Every “infected” node stays infected forever, and each “healthy” node becomes infected if and only if it has at least one infected neighbor. We show various results for both the case where we test a single time step of evolution and where the evolution spans several time steps. In the first, we show that the worst-case query complexity is O(Δ/ε) or Õ(√n/ε) (whichever is smaller), where Δ and n are the maximum degree of a node and number of vertices, respectively, in the underlying graph, and we also show lower bounds for both one- and two-sided error testers that match our upper bounds up to Δ=o(√n) and Δ=O(n^{1/3}), respectively. In the second setting of testing the environment over T time steps, we show upper bounds of O(Δ^{T−1}/εT) and Õ(|E|/εT), where E is the set of edges of the underlying graph. All of our algorithms are one-sided error, and all of them are also time-conforming and non-adaptive, with the single exception of the more complex Õ(√n/ε)-query tester for the case T=2.\n\nSee abstract\n\nOne way to get new insights about complex biological systems is to convert between modelling formalisms. Here, we deal with the conversion of reaction networks interpreted with the differential semantics, into Boolean networks. The conversion is particularly challenging, as it requires a drastic change in perspective: from a process-centred description with continuous time and values to a species-centred description with discrete steps and Boolean values. The conversion I present here is based on my PhD work. It aims at preserving properties from the input reaction network, such as its structure (the direct influences between the components) and its binarised transient dynamics (the transitions between the Boolean configurations). We will see how to extract the structure and dynamics of a reaction network, and how to use answer-set programming synthesise complying Boolean networks. So far, the evaluation of the approach on toy examples and real-world reaction networks from the repository BioModels, has demonstrated it effectiveness in synthesising Boolean networks complying with the input reaction networks. It also paved the way of the formal study of the relationship between the differential semantics of reaction network and Boolean networks. The perspectives mainly concern the practical relevance of the Boolean networks we synthesise, as well as the formal exploration of the relationship with other semantics of reaction networks.\n\nSee abstract\n\nIn the last two decades, two new perspectives have gained traction in quantum foundations and quantum information theory; each case those perspectives call for a shift in focus from the states of the world to the transformations which act on them. The first perspective is the supermap (also known as process matrix) framework, which by treating standard physical transformations as objects to be manipulated by higher-order transformations, provides a formalisation of protocols in which processes are treated as resources and provides a framework for the analysis of indefinite causal structures. The second new perspective comes from the development of categorical quantum mechanics, which organises physical theories based on the background assumption that those theories always come with notions of transformations and their composition - in other words they form symmetric monoidal categories.\n\nIn this talk, I will introduce monoidal categories and supermaps and discuss some key results and applications of both of these now-established perspectives. Next, I’ll discuss some motivations for putting them together and discuss the approaches we have recently used to do so, ultimately proposing a new formal definition of supermap for general theories of processes [1]. The main technical result I’ll discuss is a reconstruction of the standard definition of a supermap for both finite dimensional classical information theory, and for finite dimensional quantum information theory, derived from the proposed purely categorical (and physically well-motivated) definition.\n\n[1] Quantum Supermaps are Characterized by Locality: Matt Wilson, Giulio Chiribella, Aleks Kissinger\n\nSee abstract\n\nBoolean network modeling of gene regulation but also of post-transcriptomic systems has proven over the years that it can bring powerful analyses and corresponding insight to the many cases where precise biological data is not sufficiently available to build a detailed quantitative model. Besides simulation, the analysis of such models is mostly based on attractor computation, since those correspond roughly to observable biological phenotypes. The recent use of trap spaces made a real breakthrough in that field allowing to consider medium-sized models that used to be out of reach. However, with the continuing increase in model size and complexity of Boolean update functions, the state-of-the-art computation of minimal trap spaces based on prime-implicants shows its limits due to the difficulty of the prime-implicant computation. In this article we explore and prove for the first time a connection between trap spaces of a general Boolean network and siphons of its Petri net encoding. Besides important theoretical applications in studying properties of trap spaces, the connection enables us to propose an alternative approach to compute minimal trap spaces, and hence complex attractors, of a general Boolean network. It replaces the need for prime-implicants by a completely different technique, namely the enumeration of maximal siphons in the Petri net encoding of the original model. We then demonstrate its efficiency and compare it to the state-of-the-art methods on a large collection of real-world and randomly generated models.\n\nSee abstract\n\nSlow–fast dynamical systems, i.e. singularly or nonsingularly perturbed dynamical systems possess slow invariant manifolds on which trajectories evolve slowly. Since the last century various methods have been developed for approximating their equations. This talk aims, on the one hand, to propose a classification of the most important of them into two great categories: singular perturbation-based methods and curvature-based methods, and on the other hand, to prove the equivalence between any methods belonging to the same category and between the two categories. Then, a deep analysis and comparison between each of these methods enable to state the efficiency of the Flow Curvature Method which is exemplified with paradigmatic Van der Pol singularly perturbed dynamical system and Lorenz slow–fast dynamical system.\n\nIn his famous book entitled Theory of Oscillations, Nicolas Minorsky wrote: “each time the system absorbs energy the curvature of its trajectory decreases and vice versa”. By using the Flow Curvature Method, we establish that, in the ε-vicinity of the slow invariant manifold of generalized Liénard systems, the curvature of trajectory curve increases while the energy of such systems decreases. Hence, we prove Minorsky’s statement for the generalized Liénard systems. These results are then illustrated with the classical Van der Pol and generalized Liénard singularly perturbed systems.\n\nReferences\n\nGinoux J.-M., 2021, “Slow Invariant Manifolds of Slow-Fast Dynamical Systems,” International Journal of Bifurcation and Chaos, Vol. 31(7) 2150112 (17 pages), 2021.\n\nGinoux J.-M., Lebiedz D., Meucci R. & Llibre J., 2022, “Flow curvature manifold and energy of generalized Liénard systems,” Chaos Solitons & Fractals, 161, 112354 (7 pages), 2022.\n\nSee abstract\n\nThis talk is meant to be an introduction talk on the Curry-Howard correspondence between proofs and programs, for people that may not be dreaming about the λ-calculus every night. I will try to give an historical overview on why and how this correspondence was established, travelling back in time to see how the notion of “proof” evolved since the greeks (our historical lower bound in this talk) until becoming a major concept not only for mathematics but also for the development of programming languages (among other things). In between, I may talk about intuitionistic logic, proof assistants, realizability (my favorite topic), model theory, etc. This list is non-exhaustive, and in particular I would be glad to extend it with any related subsequent topic that may interest you (if so, you should send me an email: “Salut, je me suis toujours demandé ce que c’était qu’une théorie des types, tu pourrais en parler ?”, and I may try to include a slide or two, with a probability highly related to the date of your mail).\n\nVoici le code utilisé pour la démo : demo.v.\n\nSee abstract\n\nGiven an automata network, many results show that some dynamical properties can be deduced from its interaction digraph. These dynamical properties are often invariant by isomorphism: number of fixed points, of images, length of limit cycles and transients… But the interaction digraph is not invariant by isomorphism: tow isomorphic automata networks (with the same alphabet and the same number of automata) can have very different interaction digraphs. In this talk, we study this phenomena. For that, given an automata network f with n automata, we denote by G[f] the set of interaction digraphs of automata networks isomorphic to f. Hence G[f] is a set of digraphs with n vertices. In particular, we show that if n>4, and f is neither the identity or constant, then G[f] contains the complete digraph. We also prove that G[f] always contains a digraph with minimum in-degree at most some constant that only depend on the alphabet size. Consequently, G[f] cannot only contain the complete digraph, but we show that there is f such that G[f] only contain digraphs with at least n^2/4 arcs. Finally, we prove that there is f such that G[f] contains all the digraphs with n vertices, excepted the empty one.\n\nSee abstract\n\nWe introduce the game influence, a scoring combinatorial game, played on a directed graph where each vertex is either colored black or white. The two players, Black and White, play alternately by taking a vertex of their color and all its successors (for Black) or all its predecessors (for White). The score of each player is the number of vertices he has taken. We prove that influence is a nonzugzwang game, meaning that no player has interest to pass at any step of the game, and thus belongs to Milnor’s universe. We study this game in the particular class of paths where black and white vertices are alternated. We give an almost tight strategy for both players when there is one path. More precisely, we prove that the first player always gets a strictly better score than the second one, but that the difference between the scores is bounded by 5. Finally, we exhibit some graphs for which the initial proportion of vertices of the color of a player is as small as possible but where this player can get almost all the vertices.\n\nSee abstract\n\nFinite Discrete Dynamical Systems (DDS) are a formal model to study complex phenomena appearing in many different domains. Hypotheses on the fine structure of the system can be modeled via polynomial equations over DDS. For example, AX=B represents the hypothesis that the dynamics of B is given by the joint action of a known system A and an unknown one X. Finding X would validate the hypothesis; the hypothesis is invalid if no solution exists. In general, solving generic equations over DDS has been proved undecidable. In the hypothesis validation case, a possible approach is to study the solutions through a finite number of different abstractions of the problem. Each abstraction allows studying specific properties and parts of specific dynamics. We focus on hypotheses about the cardinality of the set of states, the cyclic behaviour and the transient behaviour of DDS. We introduce a complete pipeline based on Multi-valued decision diagrams to validate hypotheses on the cardinality of the set of states and on the asymptotic behaviour, contained in the cyclic parts, of a DDS. These results are a step forward in the analysis of complex dynamics graphs as those appearing, for instance, in biological regulatory networks or in systems biology. Furthermore, this approach opens up new research challenges in the field of ordered decision diagrams and their operations.\n\nSee abstract\n\nI will present two recent contributions about the performance of quantum search algorithms based on continuous-time quantum walks [1,2]. First, I will present some general results about the performance of the algorithm introduced by Childs and Goldstone [Phys. Rev. A 70, 022314 (2004)], which uses a continuous-time quantum walk to find a marked node on a graph of n nodes. This algorithm is said to be optimal if it can find any of the nodes in the graph in O(sqrt(n)) time. I will present conditions, based on the spectrum of the Hamiltonian driving the quantum walk, that can be used to predict whether the algorithm is optimal on a given graph. In the second part of the talk, I will present a new algorithm based on Hamiltonian evolution that finds marked nodes on any ergodic reversible Markov chain P quadratically faster than its classical hitting time. This algorithm can be seen as a quantum walk on the edges of a Markov chain and its performance matches that of discrete-time quantum walk search algorithms based on the Szegedy formalism. This work improves upon the recent work of [Phys. Rev. A 102, 022227 (2020)] and finally closes a theoretical gap between the performance of continuous-time and discrete-time quantum walk approaches for search.\n\nReferences:\n\n- On the optimality of spatial search by continuous-time quantum walk, Shantanav Chakraborty, Leonardo Novo, and Jérémie Roland, Phys. Rev. A 102, 032214 (2020)\n\n- Finding a marked node on any graph by continuous-time quantum walk , Shantanav Chakraborty, Leonardo Novo, and Jérémie Roland, Phys. Rev. A 102, 022227 (2020)\n\nSee abstract\n\nAn automata network (AN for short) is a finite digraph where each node holds a state, choosen among a finite set, that evolves in function of the states of its inbound neighbors. Time is discrete and all nodes evolve synchronously and in parallel, similarly to what happens in an cellular automaton. In other terms, the differences between a cellular automaton and an automata network is that the “grid” is an arbitrary finite digraph, and that different nodes may have different update functions. ANs have been used to model neural networks, dynamics of expression and inhibition of genes, distributed algorithms, and more. Although ANs look like a model of computation, they are not Turing-complete, for they lack unbounded memory. Still, they are subject to some kind of “Rice theorems”, i.e., results along the lines of: “any nontrivial property of the function computed by an automata network is computationally hard to test”. In this talk, we will review several results that fit this pattern, as well as pieces of proof that hopefully may be reused in other contexts.\n\nSee abstract\n\nThe Oritatami model was introduced by Geary et al (2016) to study the computational potential of RNA cotranscriptional folding as first shown in wet-lab experiments by Geary et al (Science 2014). In Oritatami model, a molecule grows component by component (named beads) into the triangular and folds as it grows. More precisely, the $\\delta$ last nascent beads are free to move and adopt the positions that maximizes the number of bonds with the current structure. This dynamics captures the essence of cotranscriptional folding to a considerable extent, where an RNA sequence folds upon itself into complex shapes and functions while being synthesized (transcribed) out of its template DNA sequence.\n\nThe Oritatami model was proven that it is capable of efficient Turing universal computation by Geary et al (2018) thanks to a quite complicated construction that simulates Turing machines via tag systems. We propose here a simple Oritatami system which intrinsically simulates arbitrary 1D cellular automata. Being intrinsic, our simulation emulates the behavior of the cellular automata in a readable way and in time linear in space and time of the simulated automaton. Furthermore, the number of bead types needed is quite modest (182 as opposed to 542), and the delay $\\delta$ is 2 (instead of 3). The length of the transcript is also only quadratic in the size of the cellular automata: $O(rQ^{2(2r+1)}\\log Q)$ for a cellular automata with $Q$ states and radius $r$ (whose size is thus $O(Q^{2r+1}\\log Q)$). Our construction relies on the development of new tools which are simple enough that we believe that some simplification of them may be implemented in the wet lab. Our construction has been implemented and will be soon freely downloaded for testing.\n\nSee abstract\n\nBoolean automata networks, genetic regulation networks, and metabolic networks are just a few examples of biological modeling by discrete dynamical systems (DDS). A major issue in modeling is the verification of the model against the experimental data or inducing the model under uncertainties in the data. Equipping finite discrete dynamical systems with an algebraic structure of semiring provides a suitable context for hypothesis verification on the dynamics of DDS. Indeed, hypothesis on the systems can be translated into polynomial equations over DDS. Solutions to these equations provide the validation to the initial hypothesis. Unfortunately, finding solutions to general equations over DDS is undecidable. However, many tractable cases have been highlighted. In this article, we want to push the envelop further by proposing a practical approach for such cases. We demonstrate that for many tractable equations all goes down to a “simpler” equation. However for us, the problem is not to decide if the simple equation has a solution, but to enumerate all the solutions in order to provide an insight on the set of solutions of the original, undecidable, equations. We evaluate experimentally our approach and show that it has good scalability property.\n\nSee abstract\n\nThe design of realistic quantum devices can be extremely challenging, due to the complexity of quantum interactions or to the scarcity of analytical methods in certain areas, such as non-gaussian optical interactions. My approach is to start from a network of random quantum interactions and then optimize their parameters until we obtain the desired device. This is possible with a Variational Quantum Circuit (VQC), which allows for gradient descent methods as the optimization workhorse. The advantage of this method is that the raw VQC can be composed of realistic devices and designed for full generality, which means that a) a solution consists in the explicit blueprint with all the correct parts, their connectivity and their parameters all sorted out and b) it can be applied to a variety of domains, not just quantum optical devices. I apply this method to the design of a realistic one-way quantum repeater, which includes non-gaussian quantum optical interactions and is therefore a very difficult problem to solve without clever numerical methods. A preliminary result shows that it is in principle possible to have a repeater interaction with at most a few hundred elementary optical elements, which is an extremely promising result. Our current goal is to combine our method with Reinforcement Learning techniques to design better and better raw VQCs that can achieve the same performance with fewer elements and ultimately lead to the simplest and cheapest versions of the devices that we look for.\n\nSee abstract\n\nQuantum control is the branch of control theory concerned with quantum systems, that is, dynamical systems at atomic scales, that evolve according to the laws of quantum mechanics.\n\nOne fundamental issue in quantum control theory is the controllability of quantum systems, that is, whether is it possible to drive a quantum system to a desired state, by means of suitably designed control fields. To cover most theoretical and practical situations, several notion of controllability have been proposed, as, for instance, controllability in the evolution operator, pure state controllability, controllability in population and eigenstate controllability. After a brief introduction on the topic, I will expose some results on the approximate spread controllability of (closed) quantum systems, obtained by means of adiabatic techniques, and taking advantage of the presence of conical intersections between the energy eigenstates. These results have been published in [1],[2].\n\nAdiabatic techniques can be successfully used also to study the dynamics of open quantum systems. In particular, in [3] they have been applied to find an effective description of the evolution of open, weakly coupled quantum systems, where the sub-system of interest dissipate with much slower time scales than the rest of the system.\n\n[1] U. Boscain, F. C. Chittaro, P. Mason, M. Sigalotti Quantum Control via Adiabatic Theory and intersection of eigenvalues IEEE-TAC, (2012) 57, No. 8, 1970–1983\n\n[2] F. C. Chittaro, P. Mason Approximate controllability via adiabatic techniques for the three-inputs controlled Schrödinger equation, SIAM J. Control Optim., (2017), 55(6), 4202–4226.\n\nSee abstract\n\nWe are interested in the study of the set of attractors of all 256 Elementary Cellular Automata (ECA) , i.e., one-dimensional, binary 3-neighbourhood cellular automata, dened on an n-cell lattice (n is the length of the automaton).\n\nRecent works have studied the invariance of their attractors against dierent update schemes, and how these aect their dynamics. Specically, in [1] was introduced the notion of k-invariance: an ECA rule r is k-invariant if its set of attractors, denoted by Fr(s), remains invariant for all update schemes s having blocks of length at most k (notice that k=1 stands for sequential update schemes, while k = n is the parallel, or synchronous one). In this context, the 1-invariance was studied in [2], where 104 ECA rules showed to have this kind of invariance. In [1] and [3] the authors studied the k-invariance, for 2 < k ≤ n, for all 104 ECA rules above mentioned.\n\nIn this work, we explore another notion of attractor invariance “in between ECA rules”, we say that two ECA rules u and v are attractor equivalent over a set of update schemes S, if given any s in S, Fu(s) = Fv(s). We begin our study with the 1-invariant rules, searching for equivalences between their sets of attractors, for all 4 ≤ n ≤ 14, so as to have a set of rules that are “candidates” to be attractor equivalent, since we know that if u and v are both 1-invariant ECA rules, then their sets of attractors remain invariant, but not necessarily the same, for all sequential update schemes s. Attractor equivalence gives us new insights of the dynamical behaviour of a rule (and its equivalent rules) under dierent update schemes.\n\nSee abstract\n\nso as to maximize the number of bonds formed, self-assemblying into a shape incrementally. The parameter $\\delta$ is called the “delay” and is related to the transcription rate in nature.\n\nThis article initiates the study of shape self-assembly using oritatami. A shape is a connected set of points in the triangular lattice. We first show that oritatami systems differ fundamentally from tile-assembly systems by exhibiting a family of infinite shapes that can be tile-assembled but cannot be folded by any OS. As it is NP-hard in general to determine whether there is an OS that folds into (self-assembles) a given finite shape, we explore the folding of upscaled versions of finite shapes. We show that any shape can be folded from a constant size seed, at any scale $n\\geq 3$, by an OS with delay $1$. We also show that any shape can be folded at the smaller scale $2$ by an OS with “unbounded” delay. This leads us to investigate the influence of delay and to prove that there are shapes that can be folded (at scale $1$) with delay $\\delta$ but not with delay $\\delta’<\\delta$, for all $\\delta > 2$.\n\nThese results serve as a foundation for the study of shape-building in this new model of self-assembly, and have the potential to provide better understanding of cotranscriptional folding in biology, as well as improved abilities of experimentalists to design artificial systems that self-assemble via this complex dynamical process.\n\nSee abstract\n\nThe era of quantum computers has already started. One important question that we can do now that we have these quantum devices ready is which quantum model of computation will be a good choice for encoding the huge number of quantum algorithms available. Another important question which is also extremely important to people that work in the industry that very often employs numerical methods to solve differential equations is that if these numerical methods are a promising application for quantum computers. Going to these directions we will present in this seminar our latest results where we introduce a partitioned model of quantum cellular automata and show how it can simulate, with the same amount of resources, various models of quantum walks. All the algorithms developed within quantum walk models are thus directly inherited by the quantum cellular automata. The latter, however, has its structure based on local interactions between qubits, and as such it can be more suitable for present (and future) experimental implementations. In the part related with numerical methods to solve differential equations, we will present a quantum algorithm for simulating the wave equation under Dirichlet and Neumann boundary conditions. The algorithm uses Hamiltonian simulation and quantum linear system algorithms as subroutines. Relative to classical algorithms for simulating the D-dimensional wave equation, our quantum algorithm achieves exponential space savings, and a speedup which is polynomial for fixed D and exponential in D. We also consider using Hamiltonian simulation for Klein-Gordon equations and Maxwell’s equations.\n\nSee abstract\n\nWhen analysing the computational properties and the efficiency of uniform parallel computing models, most of these turn out to be either equivalent to deterministic Turing machines, thus characterising the complexity class P in polynomial time, or are otherwise capable of solving in polynomial time the problems that sequential computing devices solve in polynomial space (i.e., the complexity class PSPACE).\n\nHowever, certain natural computing models seem to escape this binary classification. In particular, several variants of parallel devices inspired by cell biology, called membrane systems, have been recently proved to characterise complexity classes between P and PSPACE when working in polynomial time. Some of these classes are rather unusual in the literature, and had previously only been defined in terms of Turing machines with oracles, rather than being characterised by a more concrete model of computation.\n\nFundamental in the characterisation of these complexity classes is the communication topology of the device, that is, the graph along whose edges communication between the sequential computing units is allowed. The graph-theoretic (e.g., directedness, diameter) and geometric properties (e.g., embeddability in given metric spaces), as well as the description complexity of the graph (e.g., how the the adjacency list of a vertex can be computed) all influence the overall efficiency of the resulting device.\n\nIn this talk we discuss the known relations between communication topology and computational complexity in membrane systems, and we propose to investigate these relations more generally and more abstractly by choosing a more convenient reference model, namely a variant of automata networks.\n\nSee abstract\n\nIn 1941, Claude Shannon introduced a continuous-time analog model of computation, namely the General Purpose Analog Computer (GPAC). The GPAC is a physically feasible model in the sense that it can be implemented in practice through the use of analog electronics or mechanical devices. It can be proved that the functions computed by a GPAC are precisely the solutions of a special class of differential equations where the right-hand side is a polynomial. Analog computers have since been replaced by digital counterpart. Nevertheless, one can wonder how the GPAC could be compared to Turing machines.\n\nA few years ago, it was shown that Turing-based paradigms and the GPAC have the same computational power. However, this result did not shed any light on what happens at a computational complexity level. In other words, analog computers do not make a difference about what can be computed; but maybe they could compute faster than a digital computer. A fundamental difficulty of continuous-time model is to define a proper notion of complexity. Indeed, a troubling problem is that many models exhibit the so-called Zeno’s phenomenon, also known as space-time contraction.\n\nIn this talk, I will present results from my thesis that give several fundamental contributions to these questions. We show that the GPAC has the same computational power as the Turing machine, at the complexity level. We also provide as a side effect a purely analog, machine- independent characterization of P and Computable Analysis.\n\nI will present some recent work on the universality of polynomial differential equations. We show that when we impose no restrictions at all on the system, it is possible to build a fixed equation that is universal in the sense it can approximate arbitrarily well any continuous curve over R, simply by changing the initial condition of the system.\n\nIf time allows, I will also mention some recent application of this work to show that chemical reaction networks are strongly Turing complete with the differential semantics.\n\nSee abstract\n\nAs we know, spacetime is not flat at the cosmological scale. In order to describe spacetime, in General Relativity theory (GR), we need a continuous and differentiable manifold and a formal way to account for the continuous distortion of the metrics. The main point is that changing coordinate systems should not affect physics laws (General Covariance). However at the Planck length, matter is not continuous and obeys Quantum Theory (QT). Although one century has passed, finding an intrinsically discrete counterpart of GR is still an open question. In fact, discretized GR does not turn out in just a mere finite difference scheme of the old formula.\n\nI recently showed that one way to describe a discrete curved spacetime is by using Quantum Walks. From a physical perspective a QW describes situations where a quantum particle is taking steps on a discrete grid conditioned on its internal state (say, spin states). The particle dynamically explores a large Hilbert space associated with the positions of the lattice and allows thus to simulate a wide range of transport phenomena.\n\nIt is surprising that this unitary and local dynamics, defined on a rigid space-time lattice coincides in the continuous limit with the dynamical behavior of a quantum spinning-particle spreading on a curved spacetime. This could really turn out to be a powerful quantum numerical method to discretize GR.\n\nSee abstract\n\nIn this presentation, we will consider the problems of computational completeness and universality for several biologically-inspired models of computation: insertion-deletion systems, networks of evolutionary processors, and multiset rewriting systems. The presented results fall into two major categories: study of expressive power of the operations of insertion and deletion with and without control, and construction of universal multiset rewriting systems of low descriptional complexity.\n\nInsertion and deletion operations consist in adding or removing a subword from a given string if this subword is surrounded by some given contexts. The motivation for studying these operations comes from biology, as well as from linguistics and the theory of formal languages. In the first part, we focus on insertion-deletion systems closely related to RNA editing, which essentially consists in inserting or deleting fragments of RNA molecules. We show that allowing one-symbol insertion and deletion rules to check a two-symbol left context enables them to generate all regular languages. Moreover, we prove that allowing longer insertion and deletion contexts does not increase the computational power. We further consider insertion-deletion systems with additional control over rule applications and show that computational completeness can be achieved by systems with very small rules.\n\nThe second part of the presentation is concerned with the universality problem, which consists in finding a fixed element able to simulate the work any other computing device. We start by considering networks of evolutionary processors (NEPs), a computational model inspired by the way genetic information is processed in the living cell, and construct universal NEPs with very few rules. We then focus on multiset rewriting systems, which model the chemical processes running in the biological cell. For historical reasons, we formulate our results in terms of Petri nets. We construct a series of universal Petri nets and give several techniques for reducing the numbers of places, transitions, inhibitor arcs, and the maximal transition degree. Some of these techniques rely on a generalisation of conventional register machines, proposed in this thesis, which allows multiple register checks and operations to be performed in a single state transition.\n\nSee abstract\n\nThis is a joint work with Victor Poupet, Mathieu Sablik and Guillaume Theyssier.\n\nCellular automata (CA) are discrete dynamical systems with homogeneous and local evolution rule. We consider the one-dimensional case, that is CA with configurations in X^{Z} for some finite alphabet X. We can study classical dynamical properties like equicontinuity and expansivity, and Petr Kurka proposed in 1997 a classification of all CA according to these properties that describe the flows of informations in the evolutions of a CA.\n\nThe shift is a specific CA that plays a crucial role in the world of CA, and in particular commutes with any other CA. Considering this, Mathieu Sablik gave in 2008 a new classification that considers the properties of equicontinuity and expansivity along linear directions, that is composing the CA with a constant and rational power of the shift. This gave rise to the notions of sets of directions of equicontinuity or expansivity.\n\nHere we enlarge the scope again and consider every possible direction. We prove a new version of the classification, provide various examples that show that this extension is meaningful and in particular we characterize the real numbers that can be the limit of sets of directions for equicontinuous dynamics.\n\nSee abstract\n\nAn excitatory pulse-coupled neural network is a network composed of neurons coupled via excitatory synapses, where the coupling among the neurons is mediated by the instantaneous transmission of action potentials. The coherent activity of a neuronal population usually indicates that some form of correlation is present in the dynamics of the considered neurons.\n\nThe role played by the topology in promoting coherent activity in excitatory diluted pulse-coupled neural networks at a microscopic and macroscopic level is investigated. In particular, we consider a diluted random network where neurons were connected as in a directed Erdös-Renyi graph with average connectivity (in-degree) scaling linearly with the number of neurons in the network. In these “massively connected” networks we show that in the infinite size limit the dynamics of coherent collective states coincide with that of fully coupled networks. However, the random dilution of the connections induces inhomogeneities in the neuronal behaviors for any finite system size, promoting a weak form of chaos, which vanishes in the limit of infinite size. In this limit, the random systems exhibit regular (non chaotic) dynamics thus recovering the properties of a homogeneous fully connected network.\n\nThe situation is quite different for a “sparse network” characterized by a constant connectivity (in-degree), independent on the size of the network. In fact, on one side we show that a few tens of random connections are sufficient to sustain a nontrivial collective dynamics. In other words, collective motion is a rather generic and robust property and does not require an extremely high connectivity to be sustained. On an other side, the collective dynamics coexists with a microscopically chaotic dynamics that does not vanish in the thermodynamic limit and turns out to be extensive (i.e. the number of unstable directions is proportional to the network size). Extensive chaos has been already found in spatially extended system with nearest-neighbour coupling (diffusive coupling) induced by the additivity of the system. In this case this property is highly nontrivial, as the network dynamics is non additive and it cannot be approximated as the juxtaposition of almost independent sub-structures.\n\nFinally the dynamics of two symmetrically coupled populations of pulse-coupled neurons is considered: this is the simplest instance of “network-of-networks”, that is often invoked as paradigm for neural system. Upon varying both the self-coupling and the cross-coupling strengths various kinds of collective states were found: in particular the phase diagram of the system reveals various kinds of symmetric and symmetry broken collective states ranging from splay states to tori, from chimera states to collective chaos.\n\nS. Olmi, R. Livi, A. Politi and A. Torcini, Physical Review E 81, 046119 (2010).\n\nL. Tattini, S. Olmi, A. Torcini, Chaos 22, 023133 (2012).\n\nS.Luccioli, S. Olmi, A. Politi, A. Torcini, Phys. Rev. Lett. 109, 138103 (2012).\n\nS. Olmi, A. Politi and A. Torcini, EPL 92, 60007 (2010).\n\nSee abstract\n\nUsing a quantum particle as a support of information has led researchers to redefine the computer science from information theory to algorithm and complexity. A quantum computer can provide a huge advantage over a classical computer for some specific tasks (Shor’s algorithm and Quantum cryptography…), but it is certainly not true for all tasks.\n\nThe problem is, in which case quantum computer is better than classical computer ? How quantifying the power of quantum resources with respect to classical resources ?\n\nIn almost all example where a quantum computer has a clear advantage over classical, it involves quantum non-locality, means a non-signalling correlation stronger than any classical correlation.\n\nHere to characterize this non-local correlations with respect of the classical one, I will introduce new approach which doesn’t involves any Quantum physics but only combinatory. I will show a simple, necessary and sufficient Combinatorial condition to characterize the Non-signalling (i.e. causal) correlations. Moreover, this approach will give us intuitive view of the structure of causal probabilities distribution, or non signalling correlations.\n\nNOTE: An effort will done to introduce any prerequisites during the talk to be able to match with an eclectic audience (computer scientist, physicist, math…)\n\nSee abstract\n\nAujourd’hui, parmi les 106 pays où la transmission du paludisme est en cours, 81 pays mettent l’accent sur le contrôle, alors que 25 sont dans la phase de pré-élimination/élimination. Une des principales causes de l’échec des programmes de lutte est la forte hétérogénéité de la dynamique de transmission du paludisme. Entre la variabilité du climat, la dynamique des sociétés et le parasite il est souvent difficile d’identifier le poids et le rôle des différents facteurs de risque.\n\nPour comprendre la dynamique de transmission du paludisme à l’échelle locale il est nécessaire d’analyser finement la cartographie dynamique des cas de paludisme en lien avec l’environnement.\n\nTout d’abord, à l’échelle d’un village (Bancoumana, Mali), nous avons recherché des zones de risques différents. Pour cela il a été nécessaire de développer une méthode de découpage oblique du plan (SpODT), détectant de « frontières » entre ces zones.\n\nPour mieux comprendre la relation pluie-paludisme, nous avons développé un modèle dynamique de transmission conduit par la pluviométrie, permettant de reproduire l’évolution temporelle des cas de paludisme, ainsi qu’un modèle de réaction-diffusion pour représenter la diffusion spatiale au sein d’un village.\n\nCette meilleure connaissance de la dynamique du paludisme nous a conduits à proposer de nouvelles stratégies de lutte fondées sur la notion de diffusion à partir de réservoirs actuellement en cours d’évaluation au Sénégal.\n\nLe choléra est généralement considéré comme le prototype de la maladie liée à l’eau, en particulier du fait que Vibrio cholerae, l’agent du choléra, est d’abord un germe de l’environnement capable de se développer dans les eaux saumâtres des estuaires. Cette bactérie tolère aussi l’eau douce, surtout si la faible salinité est compensée par une chaleur importante et la présence de nutriments. De nombreuses études ont mis en évidence le caractère saisonnier du choléra. En octobre 2010 une épidémie de choléra est déclarée en Haiti. Elle deviendra la plus importante épidémie de choléra avec 493 069 cas et 6 293 décès en 1 an. Dès le départ, des modélisations mathématiques ont été publiées, mais, ne tenant pas compte de la réalité épidémiologique notamment du mode de démarrage, les prévisions étaient fausses au moment même de la publication. L’analyse précise du démarrage épidémique et la relation avec l’environnement a permis de démontrer l’importation de la maladie à partir du Népal, l’aspect violent du démarrage dans le bas Artibonite due à une contamination brutale et transitoire du fleuve, sans aucun lien avec une pluviométrie faible. Par contre, la dynamique de transmission c’est ensuite modifiée, plus liée à la pluie, tout en diminuant progressivement. Là encore, la connaissance de la dynamique de transmission a permis de proposer une stratégie de lutte ciblée actuellement en cours et soutenue par l’UNICEF, qui nous amènera à l’élimination à court terme.\n\nGaudart J, Rebaudet S, Barrais R, Boncy J, Faucher B, Piarroux M, Magloire R, Thimothe G, Piarroux R. Spatio-temporal dynamics of cholera during the first year of the epidemic in Haiti. PLoS Negl Trop Dis. 2013 Apr 4;7(4):e2145.\n\nhttp://www.plosntds.org/article/info:doi/10.1371/journal.pntd.0002145\n\nPiarroux R, Barrais R, Faucher B, Haus R, Piarroux M, Gaudart J, Magloire R, Raoult D. Understanding the cholera epidemic, Haiti. Emerg Infect Dis. 2011 Jul;17(7):1161-8.\n\nhttp://wwwnc.cdc.gov/eid/article/17/7/11-0059_article.htm\n\nGaudart J, Poudiougou B, Ranque S, Doumbo O. Oblique decision trees for spatial pattern detection: optimal algorithm and application to malaria risk. BMC Med Res Methodol. 2005 Jul 18;5:22.\n\nhttp://www.biomedcentral.com/1471-2288/5/22\n\nGaudart J, Poudiougou B, Dicko A, Ranque S, Toure O, Sagara I, Diallo M, Diawara S, Ouattara A, Diakite M, Doumbo OK. Space-time clustering of childhood malaria at the household level: a dynamic cohort in a Mali village. BMC Public Health. 2006 Nov 21;6:286.\n\nhttp://www.biomedcentral.com/1471-2458/6/286\n\nGaudart J, Touré O, Dessay N, Dicko AL, Ranque S, Forest L, Demongeot J, Doumbo OK. Modelling malaria incidence with environmental dependency in a locality of Sudanese savannah area, Mali. Malar J. 2009 Apr 10;8:61.\n\nhttp://www.malariajournal.com/content/8/1/61\n\nGaudart J, Ghassani M, Mintsa J, Rachdi M, Waku J, Demongeot J. Demography and diffusion in epidemics: malaria and black death spread. Acta Biotheor. 2010 Sep;58(2-3):277-305.\n\nhttp://link.springer.com/article/10.1007/s10441-010-9103-z"
    }
}
{
    "id": "dbpedia_8291_2",
    "rank": 23,
    "data": {
        "url": "https://zwischenzugs.com/author/zwischenzugs/",
        "read_more_link": "",
        "language": "en",
        "title": "zwischenzugs",
        "top_image": "https://secure.gravatar.com/avatar/4f37af910d4d089ee4a4b5d4782ca78e?s=200&d=identicon&r=g",
        "meta_img": "https://secure.gravatar.com/avatar/4f37af910d4d089ee4a4b5d4782ca78e?s=200&d=identicon&r=g",
        "images": [
            "https://blog.container-solutions.com/hubfs/Screenshot%202024-06-05%20at%2011.26.11.png",
            "https://blog.container-solutions.com/hs-fs/hubfs/history_devops_tools.drawio%20(1).png?width=1064&height=1703&name=history_devops_tools.drawio%20(1).png",
            "https://blog.container-solutions.com/hs-fs/hubfs/Screenshot%202024-06-03%20at%2014.31.44.png?width=685&height=511&name=Screenshot%202024-06-03%20at%2014.31.44.png",
            "https://zwischenzugs.com/wp-content/uploads/2018/08/learngitbashandterraformthehardway.png?w=357&h=225",
            "https://zwischenzugs.com/wp-content/uploads/2022/08/head_shot.jpg?w=768",
            "https://zwischenzugs.com/wp-content/uploads/2021/05/bmc-button.png?w=1024",
            "https://zwischenzugs.com/wp-content/uploads/2024/05/screenshot2.png?w=1024",
            "https://zwischenzugs.com/wp-content/uploads/2018/08/learngitbashandterraformthehardway.png?w=357&h=225",
            "https://zwischenzugs.com/wp-content/uploads/2024/05/screenshot2.png?w=1024",
            "https://zwischenzugs.com/wp-content/uploads/2018/08/learngitbashandterraformthehardway.png?w=357&h=225",
            "https://zwischenzugs.com/wp-content/uploads/2022/08/head_shot.jpg?w=768",
            "https://zwischenzugs.com/wp-content/uploads/2021/05/bmc-button.png?w=1024",
            "https://blog.container-solutions.com/hubfs/Screenshot%202024-05-02%20at%2012.47.05.png",
            "https://blog.container-solutions.com/hs-fs/hubfs/Screenshot%202024-05-02%20at%2012.47.05.png?width=684&height=703&name=Screenshot%202024-05-02%20at%2012.47.05.png",
            "https://zwischenzugs.com/wp-content/uploads/2018/08/learngitbashandterraformthehardway.png?w=357&h=225",
            "https://zwischenzugs.com/wp-content/uploads/2022/08/head_shot.jpg?w=768",
            "https://zwischenzugs.com/wp-content/uploads/2021/05/bmc-button.png?w=1024",
            "https://blog.container-solutions.com/hs-fs/hubfs/Screenshot%202024-04-24%20at%2014.46.07.png?width=1732&height=1116&name=Screenshot%202024-04-24%20at%2014.46.07.png",
            "https://zwischenzugs.com/wp-content/uploads/2022/08/head_shot.jpg?w=768",
            "https://zwischenzugs.com/wp-content/uploads/2021/05/bmc-button.png?w=1024",
            "https://zwischenzugs.com/wp-content/uploads/2023/12/reading_list.jpeg?w=745",
            "https://zwischenzugs.com/wp-content/uploads/2023/12/screenshot-2023-12-22-at-16.20.03.png?w=488",
            "https://zwischenzugs.com/wp-content/uploads/2023/12/screenshot-2023-12-22-at-16.27.19.png?w=546",
            "https://zwischenzugs.com/wp-content/uploads/2023/12/screenshot-2023-12-22-at-16.27.55.png?w=426",
            "https://zwischenzugs.com/wp-content/uploads/2023/12/screenshot-2023-12-22-at-16.28.11.png?w=523",
            "https://zwischenzugs.com/wp-content/uploads/2022/08/head_shot.jpg?w=768",
            "https://zwischenzugs.com/wp-content/uploads/2021/05/bmc-button.png?w=1024",
            "https://zwischenzugs.com/wp-content/uploads/2018/08/learngitbashandterraformthehardway.png?w=357&h=225",
            "https://zwischenzugs.com/wp-content/uploads/2022/08/head_shot.jpg?w=768",
            "https://zwischenzugs.com/wp-content/uploads/2021/05/bmc-button.png?w=1024",
            "https://zwischenzugs.com/wp-content/uploads/2023/06/array_railroad_diagram.png?w=1024",
            "https://zwischenzugs.com/wp-content/uploads/2023/06/value_railroad_diagram.png?w=1024",
            "https://zwischenzugs.com/wp-content/uploads/2023/06/object_railroad_diagram.png?w=1024",
            "https://zwischenzugs.com/wp-content/uploads/2022/08/head_shot.jpg?w=768",
            "https://zwischenzugs.com/wp-content/uploads/2018/08/learngitbashandterraformthehardway.png?w=357&h=225",
            "https://zwischenzugs.com/wp-content/uploads/2022/08/head_shot.jpg?w=768",
            "https://zwischenzugs.com/wp-content/uploads/2021/05/bmc-button.png?w=1024",
            "https://zwischenzugs.com/wp-content/uploads/2018/08/learngitbashandterraformthehardway.png?w=357&h=225",
            "https://zwischenzugs.com/wp-content/uploads/2021/05/bmc-button.png?w=1024",
            "https://zwischenzugs.com/wp-content/uploads/2018/08/learngitbashandterraformthehardway.png?w=357&h=225",
            "https://zwischenzugs.com/wp-content/uploads/2021/05/bmc-button.png?w=1024",
            "https://zwischenzugs.com/wp-content/uploads/2023/06/image-2.png?w=716",
            "https://zwischenzugs.com/wp-content/uploads/2023/06/image-1.png?w=542",
            "https://zwischenzugs.com/wp-content/uploads/2023/06/image-3.png?w=533",
            "https://zwischenzugs.com/wp-content/uploads/2023/06/image.png?w=539",
            "https://zwischenzugs.com/wp-content/uploads/2023/06/image-2-2.png?w=1024",
            "https://zwischenzugs.com/wp-content/uploads/2023/06/image-2-1.png?w=1024",
            "https://zwischenzugs.com/wp-content/uploads/2018/08/learngitbashandterraformthehardway.png?w=357&h=225",
            "https://zwischenzugs.com/wp-content/uploads/2022/08/head_shot.jpg?w=768",
            "https://zwischenzugs.com/wp-content/uploads/2021/05/bmc-button.png?w=1024",
            "https://s2.wp.com/i/logo/wpcom-gray-white.png",
            "https://s2.wp.com/i/logo/wpcom-gray-white.png",
            "https://pixel.wp.com/b.gif?v=noscript"
        ],
        "movies": [],
        "keywords": [],
        "meta_keywords": [
            ""
        ],
        "tags": null,
        "authors": [],
        "publish_date": "2024-07-16T09:19:06+01:00",
        "summary": "",
        "meta_description": "Read all of the posts by zwischenzugs on zwischenzugs",
        "meta_lang": "en",
        "meta_favicon": "https://s1.wp.com/i/favicon.ico",
        "meta_site_name": "zwischenzugs",
        "canonical_link": "http://zwischenzugs.wordpress.com",
        "text": "What is Crossplane?\n\nIf you don’t already know, Crossplane is billed as an:\n\nOpen source, CNCF project built on the foundation of Kubernetes to orchestrate anything. Encapsulate policies, permissions, and other guardrails behind a custom API line to enable your customers to self-service without needing to become an infrastructure expert.\n\nAnother way to view Crossplane is as a tool that uses a commodity, open source, and well-supported control plane (Kubernetes) to support the creation of other control planes.\n\nWe’ve been using it at Container Solutions for a while, and have recently been talking about how we think it’s going to become more important in future:\n\nRecently I’ve been watching Viktor Farcic’s fantastic set of tutorial videos on Crossplane. If you are an engineer or interested architect, then these primers are ideal for finding out what’s going on in this area.\n\nWhile following Viktor’s work I saw another Crossplane-related video by Viktor on a subject we both seem to get asked about a lot: does Crossplane replace Terraform/Ansible/Chef/$Tool?\n\nThis is a difficult question to answer briefly (aside from just saying “yes and no”), because understanding the answer requires you to grasp what is new and different about Crossplane, and what is not. It doesn’t help that – from the user point of view – they can seem to do the exact same thing.\n\nTo get to the answer, I want to reframe a few things Viktor says in that video that confused me, in the hope that the two pieces of content taken together help people understand where Crossplane fits into the Cloud Native firmament. Although Viktor and I agree on the role Crossplane plays now and in the future, we do differ a little on defining and interpreting what is new about Crossplane, and how the industry got here.\n\nThis post follows the logic of our ‘Cloud Native Family Tree’, which seeks to explain the history of devops tooling. It’s recently been updated to include Crossplane.\n\nThree Questions\n\nBefore we get into the debate, we may want to ask ourselves two deceptively simple questions:\n\nWhat is an API?\n\nWhat is a cloud service?\n\nAnd one non-simple question:\n\nWhat is a control plane?\n\nUnderstanding exactly what the answers to these are is key to defining what’s new and useful about Crossplane.\n\nIf you think you know these already, or aren’t interested in the philosophy, skip to the end.\n\nWhat is an API?\n\nLet’s start with a definition from an AWS page:\n\nAPIs are mechanisms that enable two software components to communicate with each other using a set of definitions and protocols.\n\nNowadays, most people think of an API as a set of services you can call using technologies like HTTP and JSON. But HTTP and JSON (or YAML, or XML etc) are not necessary. In this context, I like to explain to people that the venerable mkdir command is an API. mkdir conforms in these ways:\n\nenables two software components to communicate with each other [the shell and the Linux API] using\n\na set of definitions [the standard flags to mkdir] and\n\nprotocols [shell standard input/output and exit codes]\n\nPretty much all code is something that calls an API, down to whatever goes on within the hardware (which, by definition, isn’t a software component). Technically speaking, code is “APIs all the way down”. But this isn’t a very helpful definition if it essentially describes all code.\n\nAPIs all the way down: The Linux API calls that mkdir makes in order to create a folder.\n\nIt might be argued that mkdir is not an API because it is used by humans, and not for ‘two software components to communicate’. However, you can telnet to a server and call its API by hand (I used to do this via HTTP a lot when debugging). Further, mkdir can (and is also designed to) be used within scripts\n\nAPIs are Stable\n\nWhat people really want and expect from an API is stability. As a rule, the lower down the stack an API is, the more stable it needs to be. The Intel x86 API has had very few breaking changes since it came into being in 1978, and even carries over idiosyncrasies from the Datapoint 2022 terminal in 1970 (such as the 8086’s ‘little endian’ design. Similarly, the Linux Kernel API has also had very few changes (mostly removals) since version 2.6‘s release over 20 years ago (2003).\n\nThe Linux CLI, by contrast, is much less stable. This is one of the main reasons shell scripts get such a bad rep. They are notoriously difficult to write in such a way that they can be run on a wide variety of different machines. Who knows if the ifconfig command in my shell script will run in your target shell environment? Even if it’s installed and on the $PATH, and not some other command with the same name, will it have the same flags available? Will those flags do the same thing consistently? Defensively writing code against these challenges are probably the main reason people avoid writing shell scripts, alongside the ease with which you can write frighteningly broken code.\n\nThis is why tools like Ansible came into being. They abstracted away the messiness of different implementations of configuration commands, and introduced the notion of idempotence to configuration management. Rather than running a mkdir command which might succeed or fail, in Ansible you simply declare that the folder exists. This code will create a folder on ‘all’ your defined hosts.\n\n- hosts: all\n\ntasks:\n\n- name: Create a folder\n\nfile:\n\npath: /path/to/your/folder\n\nstate: directory\n\nAnsible will ssh into them and create the folder if it doesn’t already exist, running mkdir, or whatever it needs to run to get the Linux API to deliver an equivalent result.\n\nViktor says that Ansible, Chef et al focussed on ‘anything but APIs’, and this is where I disagree. They did focus on APIs, but not http-based (or ‘modern’) APIs; they simplified the various command line APIs into a form that was idempotent and (mostly) declarative. Just as mkdir creates a new API in front of the Linux API, Ansible created a means to use (or create your own) APIs that simplified the complexity of other APIs.\n\nTerraform: An Open Plugin and Cloud First Model\n\nTerraform not only simplified the complexity of other APIs, but then added a rich and open plugin framework and a ‘cloud first’ model (as opposed to Ansible’s ‘ssh environment first’ model). In theory, there was no reason that Ansible couldn’t have done the same things Terraform did, but Ansible wasn’t designed for infrastructure provisioning the way Terraform was (as Viktor points out).\n\nThis begs the second question: if Terraform was ‘cloud first’…\n\nWhat is a Cloud Service?\n\nMany people think of a cloud service as something sold by one of the big three hyperscalers. In fact, a cloud service is the combination of three things:\n\nA remote network connection\n\nAn API\n\nA delegation of responsibility to a third party\n\nThat’s it. That’s all a cloud service is.\n\nWe’ve already established that an API (as opposed to just ‘running software’) is a stable way for two software components to communicate. Cloud simply takes this and places it on the network. Finally – and crucially – it devolves responsibility for delivering the result to a third party.\n\nSo, if I ask my Linux desktop (y’know, the one literally on my desk) for more memory, and it can’t give it to me because it’s run out, then that’s my responsibility to resolve, therefore it’s not a cloud service.\n\nA colo is not a cloud service, for example, because the interface is not an API over a network. If I want a new server I’ll send them an email. If they add an API they become a cloud service.\n\nThis table may help clarify:\n\nAn abacus is a simple calculation tool that doesn’t use a network connection, has an interface (moving beads), but not an API, and if the abacus breaks, that’s your problem.\n\nA Linux server has a remote network connection, but no API for management. (SSH and the CLI might be considered an API, but it’s certainly not stable).\n\nmkdir has an API (see above), but from mkdir‘s point of view, disk space is your problem.\n\nIf you engage a company to supply you with an on-prem server, then it’s their problem if it breaks down (probably), but you don’t generally have an API to the outsourcer.\n\nIf you build your own API and manage it yourself then you can’t pick up the phone to get it fixed if it returns an error.\n\nIf the Windows API breaks (and you paid for support), then you can call on Microsoft support, but the Windows API doesn’t need a network connection to invoke.\n\nA colo server is supplied by you without an API, but if it doesn’t get power/bandwidth/whatever else the colo supports, you can get them to fix it, and can connect to it over the network.\n\nSome of these may be arguable on detail, but it’s certainly true that only EKS and GitHub qualify as ‘cloud services’ in the above table, as they fulfil all three criteria for a cloud service.\n\nWhat is a Control Plane?\n\nA less commonly-understood concept that must also be understood is the ‘control plane’. The phrase comes from network routing, which divides the router architecture into three ‘planes’: the ‘data plane’, the ‘control plane’, and the ‘management plane’.\n\nIn networking, the data plane is the part of the software that processes the data requests. By contrast, the control plane is the part of the software that maintains the routing table and defines what to do with incoming packets, and the management plane handles monitoring and configuration of the network stack.\n\nYou might think of the control plane as the state management of the data that goes through the router, as opposed to the general management and configuration of the system (management plane).\n\nThis concept has been co-opted by other technologies, but I haven’t been able to find a formal definition of what a control plane when used outside of networking. I think of it as ‘whatever manages how the useful work will be done by the thing’ rather than the thing that does the actual work. If that doesn’t seem like a rigorous definition to you, then I won’t disagree.\n\nFor Kubernetes, the control plane is the etcd database and the core controllers that make sure your workloads are appropriately placed and running.\n\nAll cloud services need a control plane. They need something that orchestrates the delivery of services to clients. This is because they have a remote API and a delegation of responsibility.\n\nSo Does Crossplane Replace Terraform?\n\nOK, now we know what the following things are:\n\nAPIs\n\nCloud services\n\nControl planes\n\nWe can more clearly explain how Crossplane and Terraform (et al) relate.\n\nResources, APIs, Cloud Services\n\nCrossplane and Terraform both deal with the creation of resources, and are both designed to help manage cloud services. In this sense, Crossplane can replace Terraform. However…\n\n‘One-shot’ vs Continuous\n\n…whereas Terraform is ‘one-shot‘ (you run it once and then it’s done), Crossplane is continuous. Part of its job is to provision resources, but it’s not its only job. Its design, and main purpose, is to give you a framework to ensure that resources remain in a ‘known state’, ultimately deriving its source of truth from the configuration of its own Kubernetes control plane (or Git, if this configuration is synchronised with a Git repository).\n\nTerraform ‘Under’ Crossplane?\n\nIf you want, you can run your Terraform code in Crossplane with the Terraform provider. One thing to note here, thought, is that you can’t just take your existing Terraform code or other shell scripts and run it unchanged ‘within’ Crossplane’s control plane just as you would have done before. Some work will need to be done to integrate the code to run under Crossplane’s control. In this sense, Crossplane does replace Terraform, subsuming the code into its own provider.\n\nControl Planes\n\nIn a way, Crossplane is quite close to Chef and Puppet. Both those tools had ‘control planes’ (the Chef and Puppet servers) that ensured the targets were in a conformant state. However, Chef and Puppet (along with Ansible) were designed to configure individual compute environments (physical servers, VMs etc), and not orchestrate and compose different APIs and resources into another cloud service-like API.\n\nIf you like this, you might like one of my books:\n\nLearn Bash the Hard Way\n\nLearn Git the Hard Way\n\nLearn Terraform the Hard Way\n\nThis post was originally published here, and is re-posted with permission.\n\nWhen I started my career as an engineer in the early noughties, I was very keen on developer experience (devex).\n\nSo when I joined a company whose chosen language was TCL (no, really), I decided to ask the engineering mailing list what IDEs they used. Surely the senior engineers, with all their wisdom and experience, would tell which of the many IDEs available at the time made them the most productive? This was decades before ‘developer experience’ had a name, but nonetheless it was exactly what I was talking about, and what people fretted about.\n\nMinutes later, I received a terse one-word missive from the CTO:\n\nFrom: CTO Subject: IDEs > Which IDEs do people recommend using here? Thanks, > Ian vim\n\nBeing young and full of precocious wisdom, I ignored this advice for a while. I installed Eclipse (which took up more RAM than my machine had, and quickly crashed it), and settled on Kate (do people still use Kate?). But eventually I twigged that I was no more productive than those around me that used vim.\n\nSo, dear reader, I married vim. That marriage is still going strong twenty years later, and our love is deeper than ever.\n\nThis pattern has been repeated multiple times with various tools:\n\nsee the fancy new GUI\n\ntry it\n\ngradually realise that the command-line, text-only, steep-learning-curve, low-tech approach is the most productive\n\nSome time ago it got to the point where when someone shows me the GUI, I ask where the command-line version is, as I’d rather use that. I often get funny looks at both this, and when I say I’d rather not use Visual Studio if possible.\n\nI also find looking at gvim makes me feel a bit queasy, like seeing your dad dancing at a wedding.\n\nThis is not a vim vs not-vim post. Vim is just one of the oldest and most enduring examples of an approach which I’ve found has served me better as I’ve got older.\n\nI call this approach ‘low-tech devex’, or ‘LTD’. It prefers:\n\nLong-standing, battle-hardened tools that have stood the test of time\n\nTools with stable histories\n\nSmall tools with relatively few dependencies\n\nText-based input and output\n\nCommand-line approaches that exemplify unix principles\n\nTools that don’t require daemons/engines to run\n\nLTD also is an abbreviation of ‘limited’, which seems appropriate…\n\nHow Is Low-Tech Devex Better?\n\nAll LTD tools arguably reflect the aims and principles of the UNIX philosophy, which has been summarised as:\n\nWrite programs that do one thing and do it well. Write programs to work together. Write programs to handle text streams, because that is a universal interface.\n\nPortability\n\nLTD is generally very portable. If you can use vim, it will work in a variety of situations, both friendly and hostile. vim, for example can easily be run on Windows, MacOS, Linux, or even that old HPUX server which doesn’t even run bash, yet does run your mission-critical database.\n\nSpeed\n\nThese tools tend to run fast when doing their work, and take little time to start up. This can make a big difference if you are in a ‘flow’ state when coding. They also tend to offer fewer distractions as you’re working. I just started up VSCode on my M2 Mac in a random folder with one file in it, and it took over 30 seconds until I could start typing.\n\nPower\n\nAlthough the learning curve can be steep, these tools tend to allow you to do extremely powerful things. The return on the investment, after an initial dip, is steep and long-lasting. Just ask any emacs user.\n\nComposability / Embeddability\n\nBecause these tools have fewer dependencies, they tend to be more easily composable with – or embeddable in – one another. This is a consequence of the Unix philosophy.\n\nFor example, using vim inside a running Docker container is no problem, but if you want to exec onto your Ubuntu container running in production and run VSCode quickly and easily, good luck with all that. (Admittedly, VSCode is a tool I do use occasionally, as it’s very powerful for certain use cases, very widely used, and seems designed to make engineers’ lives easier rather than collect a fee. But I do so under duress, usually.)\n\nAnd because they use also typically use standard *nix conventions, these tools can be chained together to produce more and more bespoke solutions quickly and flexibly.\n\nUsers’ Needs Are Emphasised\n\nLTD tools are generally written by and for the user’s needs, rather than any purchaser’s needs. Purchasers like fancy GUIs and point-and-click interfaces, and ‘new’ tools. Prettiness and novelty don’t help the user in the long term.\n\nMore Sustainable\n\nThese tools have been around for decades (in most cases), and are highly unlikely to go away. Once you’ve picked one for a task, it’s unlikely you’ll need a new one.\n\nMore Maintainable\n\nAgain, because these tools have been around for a long time, they tend to have a very stable interface and feature set. There’s little more annoying than picking up an old project and discovering you have to upgrade a bunch of dependencies and even rewrite code to get your project fired up again (I’m looking at you, node).\n\nHow We Use LTD\n\nHere’s an edited example of a project I wrote for myself which mirrors some of the projects we have built for our more engineering-focussed clients.\n\nDeveloper experience starts with a Makefile. Traditionally, Makefiles were used for compiling binaries efficiently and accounting for dependencies that may or may not need updating.\n\nIn the ‘modern’ world of software delivery, they can be used to provide a useful interface for the engineer for what the project can do. As the team works on the project they can add commands to the list for tasks that they often perform.\n\nhelp:\n\n@grep -E '^[a-zA-Z_-]+:.*?## .*$$' $(MAKEFILE_LIST) | sort | awk 'BEGIN {FS = \":.*?## \"}; {printf \"\\033[36m%-30s\\033[0m %s\\n\", $$1, $$2}'\n\ndocker_build: ## Build the docker image to run in\n\ndocker build -t data-scraper . | tee /tmp/data-scraper-docker-build\n\ndocker tag data-scraper:latest docker.io/imiell/data-scraper\n\nget_latest: docker_build ## Get latest data\n\n@docker run \\\n\n-w=\"/data-scraper\" \\\n\n--user=\"$(shell id -u):$(shell id -g)\" \\\n\n--volume=\"$(PWD):/data-scraper\" \\\n\n--volume=\"$(HOME)/.local:/home/imiell/.local\" \\\n\n-- volume=\"$(HOME)/.bash_history:/home/imiell/.bash_history\" \\\n\n--volume=\"/etc/group:/etc/group:ro\" \\\n\n--volume=\"/etc/passwd:/etc/passwd:ro\" \\\n\n--volume=\"/etc/shadow:/etc/shadow:ro\" \\\n\n--network=\"host\" \\\n\n--name=get_latest_priority \\\n\ndata-scraper \\\n\n./src/get-latest.sh\n\nIt’s a great way of sharing ‘best practice’ within a team.\n\nI then have a make.sh script which wraps calls to make with features such as capturing logs in a standard format and cleaning up any left-over files once a task is done. I then use that script in a crontab in production like this:\n\n*/5 * * * * imiell run-one /home/imiell/git/data-scraper/src/make.sh \\\n\nget_latest\n\nUsing run-one and cron I can effectively have a daemon running on my server with very little overhead.\n\nIf you like this, you might like one of my books:\n\nLearn Bash the Hard Way\n\nLearn Git the Hard Way\n\nLearn Terraform the Hard Way\n\nA challenge with make is that it has quite a steep learning curve for most engineers to write, and an idiosyncratic syntax (to younger eyes, at least).\n\nHowever, while it can seem tricky to write Makefiles, it’s relatively easy to use them, which makes them quite a useful tool for getting new team members onboarded quickly. And for my personal projects – given enough time passing – new team members can mean me! If I return to a project after a time, I just run make help and I can see instantly what my developer workflow looked like.\n\nHere’s an example make help on a project I’m working on:\n\nThis means if I have to onboard a new engineer, they can just run make help to determine what actions they can perform, and because most of the tasks run in containers or use standard tooling, it should work anywhere the command line and Docker is available.\n\nThis idea is similar to Dagger, which seeks to be a portable CI/CD engine where steps run in containers. However, running Dagger on Kubernetes requires privileged access and carries with it a Dagger engine, which requires installation and maintenance. make requires none of these moving parts, which makes maintenance and installation significantly simpler. The primitives used in Make can be used in your CI/CD tool of choice to create a similar effect.\n\nWhat Low-Tech Devex Tools Should You Know About?\n\nHere is a highly opinionated list of LTD tooling that you should know:\n\nshell\n\nMastery of shells are essential to LTD; they underpin almost all of them.\n\nvim/emacs\n\nAlready mentioned above, vim is available everywhere and is very powerful and flexible. Other similar editors are available and inspire similar levels of religious fervour.\n\nmake\n\nMake is the cockroach of build tools: it just won’t die. Others have come and gone, some are still here, but make is always there, does the job, and can’t go out of fashion because it’s always been out of fashion.\n\nHowever, as noted above, the learning curve is rather steep. It also has its limits in terms of whizz-bang features.\n\nDocker\n\nThe trendiest of these. I could talk instead of chroot jails and how they can improve devex and delivery a lot on their own, but I won’t go there, as Docker is now relatively ubiquitous and mature. Docker is best enjoyed as a command line interface (CLI) tool, and it’s speed and flexibility composed with some makefiles or shell scripts can improve developer experience enormously.\n\ntmux / screen\n\nWhen your annoying neighbour bangs on about how the terminal can’t give you the multi-window joy of a GUI IDE, crack your knuckles and show them one of these tools. Not only can they give you windowing capabilities, they can be manipulated faster than someone can move a mouse.\n\nI adore tmux and key sequences like :movew -r and CTRL+B z CTRL+B SPACE are second nature to me now.\n\ngit\n\nDistributed source control at the command line. ’nuff said.\n\ncurl\n\ncURL is one of my favs. I prefer to use it over GUI tools like Postman, as you can just save command lines in Markdown files for how-to guides and playbooks. And it’s super useful when you want to debug a network request by ‘exporting as cURL command’ from Chrome devtools.\n\ncloud shell\n\nCloud providers provide in-browser terminals with the provider’s CLI installed. This avoids the need to configure keys (if you are already logged in) and worry about the versioning or dependencies of the CLI. I agree with @hibri here.\n\nssh\n\nWhy bother with the faffs of remote desktops when you have ssh and tmux? If you depend on GUIs, that’s the problem you need to solve.\n\nFinally, special mentions to these text user interfaces (TUIs) and dev-friendly tools:\n\nk9s – text user interface (TUI) for Kubernetes\n\ntop/htop/ctop etc – monitoring tools\n\nsar – monitoring/performance analysis tool\n\nentr – run commands when specific files change. Get rapid feedback in dev.\n\nnnn – file browser\n\nIf you like this, you might like one of my books:\n\nLearn Bash the Hard Way\n\nLearn Git the Hard Way\n\nLearn Terraform the Hard Way\n\nThis post was originally published here, and is re-posted with permission.\n\nThis post was originally triggered – and I choose that word carefully – by a recent experience on a cloud cost-optimisation project. These experiences prompted me to consider how things had changed since I started working in software.\n\nAs part of the project that provoked me, I was researching the topic of cloud cost analysis and was struck by how many people complained that the tools the big three providers give you are not adequate to the task. I had a look myself and indeed found that the questions I wanted answering were difficult to answer using the GUI tools.\n\nNo problem, I thought: I’ll just download the raw CSVs and find an open source project that sticks the data in a database, allowing me to write whatever queries I want on the data. After some searching I couldn’t find any such project, so wrote one myself and stuck it on GitHub. Once this was built I could write all sorts of queries like ‘Compare costs per GCP service/project/sku with previous month, showing any percentage change greater than x’, modifying them at will to hone in on the specific data I was interested in much faster and more effectively than could be achieved with GUIs. Some of the more generic ones are in the repo.\n\nWhile working on this with a very bright and knowledgeable younger colleague, I was struck by the fact that he’d never needed to learn SQL, and he made sense of these queries by comparing them to other JSON-based *QLs that he’d come across (PromQL, GraphQL etc). This surprised me.\n\nThe Good Old Days\n\nI’m going to grossly simplify matters here, but when I was coming up in the industry (around 2001) three-tier backend applications were where it was at. There were webservers (usually Apache), application servers, and databases. Knowledge and experience was thin on the ground, so you had to know a bit of everything. HTML, Javascript (mostly form submission fiddling), a language to do business logic in the application server layer (usually Java in those days, but I’d sworn off it for life), and SQL for the database. Different people had different preferences and specialisms, but you had to have a basic knowledge of all those technologies to be an effective developer.\n\nThe entire development engineer industry was about ‘dev’ at this point. ‘Ops’ was a challenge that was at the time usually either done in a half-hearted way by the developers themselves (in smaller orgs), or passed over to the sysadmins to manage (in bigger orgs). The smaller orgs grew into a devops mindset (“we’re not hiring a separate team to support the operation of the system, it’s cheaper to get the devs to do it”), and the bigger ones embraced SRE (“we need capable engineers to support and manage site reliability engineer live systems and there’s economies of scale in centralising that.”). Data science was also not really a thing then.\n\nThere were DBAs but they were there to ensure backups were done, and punish those who polluted their treasured databases with poorly written queries or badly defined indexes. They therefore sometimes doubled up as SQL tutors to the devs.\n\nSince almost everyone was a dev, and devs did a bit of everything, engineers were often asked SQL questions in job interviews, usually starting with a simple join and then asking about indexes and performance characteristics for the more advanced candidate.\n\nThe Rise of the Specialist\n\nAlong with the rise of ops in the noughties came the continuation of increasing specialisation in the IT industry. We’d had the DBA for some time, but the ‘dev’ was gradually supplanted by the front-end engineer, the tester, the data scientist, the DevOps engineer, and later the SRE, the security engineer… the list goes on.\n\nI say this was a continuation because this process has been going on since computers existed, from the original physicists and technicians that maintained and programmed machines the size of rooms bifurcating into COBOL and FORTRAN programmers in the 60s and 70s, leading to system admins in the 80s, and then network admins, and so on.\n\nParallels to this can be seen in the medical profession. Once upon a time we had a ‘ship’s surgeon‘, who had to deal with pretty much any ailment you might expect from having cannon balls fired at you while lugging your heavy cannons to face the enemy, and (in more peaceful moments) disease-ridden sailors sharing whatever diseased they’d brought in from shore leave. Fast-forward hundreds of years and we now have plastic surgeons that specialise in just in ear surgery (otologists).\n\nAnd a similar thing seems to be happening with computing jobs. Whereas once upon a time everyone who interacted with a computer at work would have likely had to type in (or even write) a few SQL queries in the past, now there’s a data scientist for that. The dev now attends scrum meetings and complains about JIRA (and doesn’t even write much JQL).\n\nIs SQL Becoming a Niche Skill?\n\nSo back to the issue at hand. Opinion seems to be divided on the subject. These interactions were typical when I asked on Twitter:\n\nThere’s no doubt that historical demand for SQL has always been strong. While SQL’s elder sibling COBOL clings to life on the mainframe, SQL remains one of the most in-demand skills in the industry, according to the TIOBE index. In the 20 years since 2004 it’s gone from 7th to 8th most popular language along side mainstays like C and upstarts like C++, Java, and Visual Basic.\n\nNot only that, but SQL is still a widely used data querying language outside its common domains. Those who prefer SQL to JQ (and who among us without a PhD in mathematics do not?) might be interested in DuckDB, a SQL-fronted alternative to jq. And facebook’s osquery, which gives a common SQL interface to query different operating systems’ state for compliance, security, or performance purposes.\n\n‘Schemas Are Bad, M’Kay?’\n\nIn another recent project with a similar demographic of engineer, I was horrified to be voted down about using a relational database as the consensus was ‘schemas are bad’ and ‘MongoDB is more flexible’. This, I thought, is a world gone topsy-turvy. To gain dominion over your data, you need to wrestle it to the floor with a schema. Without that, you won’t have the power to range over it with whatever questions you have to ask it, and you won’t have a database, just a set of documents or a fancier filesystem.\n\nTo this it’s often objected that schemas are ‘hard to change’, but this is only true when you’ve got many users and code dependencies on the database or huge amounts of data, not when you’re initially building up a system. They can be annoying to change, because the syntax varies between vendor and you often end up looking up whether it’s ALTER TABLE ADD INDEX or CREATE INDEX. LLMs almost banish this problem, however, as they are incredibly good at saving you time on this.\n\nPoking my head out of that world I’ve been shocked at how few people currently in IT really understand data programming/management these days. npm and Javascript sure, python probably, but data, its management and any programming around it seems to have been pushed to the sides for the ‘data scientists’ to deal with, as though data were not an essential aspect of pretty much any computing work.\n\nBut perhaps I’m just an old ship’s surgeon who might know how to saw a leg off and administer smelling salts, but doesn’t have the deft skill of an otologist who practices on their specialism on the daily. I’ll just call myself a ‘full stack’ engineer and hope no-one notices.\n\nIf you like this, you might like one of my books:\n\nLearn Bash the Hard Way\n\nLearn Git the Hard Way\n\nLearn Terraform the Hard Way\n\nThis post was originally published here, and is re-posted with permission.\n\nAs someone who has worked in software since 2001, and in the Cloud Native (containerisation and Kubernetes) space since 2013, I’m getting old enough to have seen trends come and go a few times. VMs came (and stayed), continuous integration went from a fad talked about by gurus to the mainstream of software delivery, and containers went from some changes Google made to the Linux kernel to the de facto standard for software packaging, and then on to the foundation for Kubernetes, an industry-standard software deployment platform.\n\nBut it’s another thing to see a wave come, and then see it recede for a time before you expect it to rise again. And in the last years, at my work we’ve observed that more and more of the decision-makers in our industry have shifted their stance on the cloud to the point where it became impossible for us to ignore.\n\nOne might say they’ve changed their strategy, but most of them don’t couch this shift in those terms. What we’re seeing is that businesses are moving from a wholesale ‘migration to cloud’ strategy to a more piecemeal, hybrid approach. After experimentation with moving workloads to the cloud (with mixed success), companies have altered their ambitions. Instead, they are now leaving workloads on-prem (or in colos), and only moving existing workloads to cloud if there is a compelling reason to. New workloads are going to the cloud by default.\n\nIn most cases we’ve observed, this is quietly accepted as a new reality rather than trumpeted as a new strategy. We think this is a change that’s been borne of experience and financial necessity, two very strong motivators.\n\nTo be clear, we’re not talking about repatriation here. We’ve discussed repatriation in previous content, but we’re not seeing clients exit the cloud or even move workloads back into the data centre. Famously, David Heinemeier Hansson trumpeted his company HEY’s repatriation of their workloads back on-prem. It was so frequently cited and discussed that he wrote an FAQ about it to answer questions. His situation, however, was a relatively rare combination of a single-product tech company that had decided it no longer needed the agility that the cloud brings, and had the skills and confidence to bring their workloads back.\n\nThere are hints that others are seeing the same trends. James Watters of VMWare Tanzu’s Research and Development department talks here about how he’s:\n\n“seeing a lot of organisations where the trend isn’t going one direction or the other. I’m seeing people benchmarking the cost and efficiency of the private cloud stack versus the public cloud stack, and people are having a lot of thoughtful conversations about that and in some cases are growing their private cloud deployments because of the results they see”\n\nWe’ve Been Here Before\n\nAs we all know, those who do not learn from history are doomed to repeat it. And the mainframe to PC migration of the latter 20th century offers a perfect example from living memory of a similar evolution. Putting precise dates on these kinds of shifts is difficult, but my researches suggested that this migration took place over a roughly 20-year period, from the early 80s to the first few years of the 21st century. The start date matches the time Bill Gates was said to have said that his vision was ‘a computer on every desk and in every home’.\n\nThis 1982 article from the Sarasota Herald Tribune suggests that in 1981, IBM moved from ‘large central computers’ to ‘small, smart computer equipment’ whose potential IBM’s rivals had seen in the late 70s. Interestingly, this shift followed a 13-year antitrust suit from the Justice Department that charged IBM with a mainframe monopoly. This poses an interesting question of which came first in this decision: the market changes, or the legal pressures? It’s generally believed that the market rendered the monopoly discussion irrelevant as PCs took over global compute spend.\n\nDiscussion about mainframe to PC migrations petered out around the turn of the century, as references began to consistently use the past tense when talking about it. Again, it’s interesting to note that this coincided with the ‘Y2K bug’ spending frenzy and the dot-com boom, suggesting that these investment bumps were what tipped mainframe migration to PC discussions to the past tense.\n\nAlthough it’s relatively easy in retrospect to say when these trends started and finished, as it happened there was confusion about where we were in the cycle, or even whether the cycle was still in play. As ‘late’ as 1986 – over half a decade since IBM had decided they were late to the party – PCs were not considered rivals to mainframes, and even eight years after that in 1994, experts were still being asked whether client-server was the future of computing.\n\nSo we might say that the secular trend away from the mainframe was strong, but within that trend there were periods of growth and deceleration for the PC. What we’re seeing now may be a similar reduction in the rate of growth of a trend of cloud transformation that’s been going on arguably since 2006, the year Amazon introduced the S3 block storage service.\n\nWhy Now?\n\nThe causes of this retreat from wholesale cloud migration are manifold. We’ll look at three of them, in rising order of importance.\n\nMigration exhaustion (and IT worker conservatism)\n\nCost of hardware/colos\n\nMacroeconomic trends\n\nMigration Exhaustion and Conservatism\n\nEnterprises are wearying of transformation narratives. The last 10-15 years have seen revolving doors of CIOs proclaiming cloud transformations that have not lived up to their billing. Our founder even wrote a book called Cloud Native Transformation that captured this zeitgeist (and how do it right with the help of the patterns we open sourced, natch).\n\nThe hardest (and most underestimated) part of these transformations are the least technical ones. Once an API exists for a service, then moving your software to run on it is – in principle – straightforward.\n\nWhat really constrains cloud transformations includes the innate conservatism of the broad mass of IT employees, and the organisational inertia of businesses that whose organisational architectures were not shaped by cloud economics. In other words, the ‘money flows‘ of businesses are designed for organisational paradigms out of kilter with the new computing paradigm (which we’ve written extensively on this under-explored constraint in the past, having seen it as a major blocker to cloud adoption).\n\nNatives, Converts, Hobbyists and Others\n\nWe see a strong divide in staff we encounter. The first people we usually meet on a client are the enthusiastic and capable cloud ‘converts’. These internal evangelists are usually from a background specialism such as development, system administration, or networking. In this specialism they first used – and later in their career embraced – cloud technologies, or are career cloud natives. We often wonder why they need our help when they have such smart, committed people saying many of the right things. We find out why as we go deeper into their business and talk to the second and third groups.\n\nThe second group are feature team platform ‘hobbyists’, whose day job is typically to deliver features, but are motivated to improve delivery. They often become platform engineers over time if that’s the way they want to go, as their skills (programming, source control, automation) are readily transferable from development into that field. Such people are relatively rare (this was my own path).\n\nFinally, we have the broad mass of IT employees who don’t have much interest in cloud computing. They are generally either satisfied with shipping features, or maintaining complex systems they have come to know well, or are at a stage of their career where operating in a new paradigm does not fill them with excitement. I once worked at a client with a DBA tasked with aiding a cloud migration who was quite resistant even to using shell scripts and Git, let alone Terraform or Python cloud libraries. These people can be considered like those who either clung to the mainframe in the nineties and survived, repurposed their career to a less technical path, or got out completely. There are more of them out there than many clued up engineers might think.\n\nThis is particularly marked in mainland EU, where employment law makes it relatively difficult and costly to turn over staff compared to the US and UK. Time and again we have discussions with business leaders that are frustrated by their own inability to move the cloud needle within their own organisation, and effectively unable (or unwilling) to part with long-standing staff. This is one of several factors that results in the frozen middle we’ve written about addressing previously.\n\nThe last ten years have seen several waves of cloud transformation, and these waves, along with the natural conservatism of the bulk of the IT workforce has resulted in a retreat of enthusiasm for wholesale migration. But this by itself would not stop an industry motivated to change. So we must look deeper.\n\nCost of hardware\n\nWhile the IT world was focussed on cloud migrations and transformations, the cost of hardware continued to drop.\n\nIt’s always been known that the performance of a cloud CPU and memory is not the same as physical hardware, but putting exact numbers on this is tricky, as it depends on so much else than the headline numbers, not to mention the fact that you’re sharing your CPUs with other workloads. This study by Retailic suggests that 2 cloud ‘vCPUs’ (virtual CPU) is roughly equivalent to one ‘real’ CPU on-prem. And memory performance is even more murky, as this post attests.\n\nSo looking at just the CPUs you get ‘twice’ the value per CPU you pay for on-prem. So on-prem is cheaper? Not so fast. You still need to take into account cost of running your own data centre (software maintenance, electricity costs, risk of outages, network costs…) to do a proper comparison. Building all these capabilities and paying for all these ‘extras’ is the basis of the arguments for the cloud’s value in the first place.\n\nAnd, of course, you have to compare the yearly rental costs of servers with the yearly depreciation costs of buying the hardware in the first place. And here, hardware has also continued to get cheaper. Traditionally, computing equipment was depreciated over three years, but anyone who has bought servers or desktops in the last ten years will know that they typically have a useful life far longer than that.\n\nFor example: at home I have a nine year old Dell Xeon workstation that I bought for £650 three years ago (to run virtual Kubernetes clusters on, natch) that is still going strong. Inflation adjusted, the 2015 cost of that machine has depreciated at about £180 per year. According to AWS’s cost calculator, a similarly-spec’d m6g.12xlarge instance on AWS would set you back over £11,000. This eye-watering sum includes a one-year reservation, and paying all upfront for a hefty discount. A three-year reservation would only bring that down to £7,000 per year.\n\nAt the other end of the ‘serious computing’ spectrum, Google saved billions of dollars at the stroke of their CFO’s pen when they updated their servers’ lifespans, from four to six years. That’s a 33% reduction in cost on a pretty large line item. (Incidentally, Google started up by saving money and increasing compute power buying up commodity PCs instead of large servers or mainframes in the early noughties.)\n\nEven if you don’t fancy running your own data centre (the usual counter to these arguments), then you can rent colocated server space very cheaply. I quickly found a provider in London who would give me a 10U space for £119 per month. Data egress costs are cheaper, and you can scale up or down other features (such as network bandwidth) depending on your requirements.\n\nOr you can buy ‘clouds in a box’ like Oxide at the higher end, or SoftIron’s Hypercloud at the cheaper end, which offer options to self-host a cloud offering either in your data centre, or in a colo, or HPE Greenlake which offers metered usage of a similar product. If you combine these with genuinely Cloud Native workloads then you can pick the best cloud for your use case, which might be tempting as interest rates rise.\n\nMacroeconomic Trends\n\nFinally, we come to the most fundamental cause of this migration slowdown. Across the world, bond and interest rates have risen from historical lows in 2020 during the COVID pandemic to levels not seen since 2008. The tech boom we saw during the pandemic, where cheap money was thrown at tech in a bid for growth became a bust as many businesses woke up to the fact that they had overspent in the previous years in a bid for fast growth.\n\nThis caused a cutting-back of spending and funding as companies and investors adjusted to the new reality, and CFOs cut back spending en masse. This in turn meant that ambitions for moving to the cloud were either dropped, put on hold, or cut back significantly. This resulted in noticeably many more requests for cost optimisation or finops work than we had previously, and noticeably fewer requests for large-scale cloud transformations.\n\nWhat Next?\n\nIf the current position is one of a slowing in the growth of cloud provider take-up, then what does the history of mainframes and PCs teach us about what comes next?\n\nThere are stirrings of alternate offerings to the big providers coming to the market that leverage cheaper and less feature rich cloud providers. Cloudfanatics is one such company, about to come to market offering cheaply provisioned packages of multi-tier infrastructure for cost-conscious SMEs that don’t have the expertise to build themselves. The packages use whichever cloud provider is most appropriate for the use case in terms of cost and features/performance.\n\nCloudfanatics’ founder, Andrew Philp, was inspired to start this company up by his experiences with SMEs’ struggles with both cost and maintenance of cloud systems. Although their needs were generally simple, he found that lack of availability of staff who could pick up a cloud setup from a departing employee, and poor practices around cloud topics such as IAM setup and secrets management meant that there was a sweet spot of functionality that could be delivered in a cost-effective way.\n\nThis may be the beginnings of the next ‘mainframe vs PC’ war. This time the big three cloud providers are the ‘mainframe’ vendors, and these smaller cloud providers are the more agile and simpler ‘PC’. APIs such as Kubernetes and the AWS S3 protocol are the equivalent of the IBM PC standard, allowing customers to port their applications if needed.\n\nAlready we see alliances of cloud providers such as the bandwidth alliance clubbing together (facilitated by Cloudflare) to offer a looser agglomeration of cost-saving options for cash-constrained IT leaders, including free data transfers between providers. Data egress costs are often a bone of contention between customer and cloud.\n\nWhat we are unlikely to see is a wholesale retreat from cloud to either on-prem or the colo. A whole generation of engineers and business owners have been raised on the cloud and Cloud Native tooling and technology. While the prem will never go away, wholesale repatriation programs like HEY’s will be relatively rare.\n\nWhat’s critical in this new world is to ensure that your workloads can run portably. This is what is meant by the phrase ‘Cloud Native’. Portability has always been the dream of the CIO looking to cut costs and increase choice, from bash scripts to mainframe software, and a source of contention between bit IT and consumer. As an enterprise architect for a bank, I once had a heated discussion with an AWS representative that told me to ‘just use ECS’ when our strategy involved making our workloads Kubernetes-native. Soon afterwards AWS announced EKS to the surprise of many, including the AWS rep.\n\nWhat Does AI Mean for Cloud?\n\nOpinion is divided over the effect AI workloads will have on these trends. On the one hand, Michael Dell cites a Barclays CIO Survey report which suggests that private cloud repatriation is rising to being on 83% of CIOs’ radars, from a low of 43% at the height of COVID. It makes one wonder whether COVID drove a lot of necessary short-term costly cloud spending which is now being repatriated in a piecemeal fashion on a case-by-case basis.\n\nMichael Dell mentions another factor in repatriation: data gravity and AI inference workloads, and here there’s a split in opinion. On the one hand, companies looking to get ahead in AI may want to buy the latest hardware and run it themselves; on the other hand, renting cloud compute for short term agility may make more sense than buying AI hardware that will be out of date in the blink of a Moore’s Law cycle.\n\nPortability More Important than Ever\n\nWith bond rates continuing to rise, expect more and more pressure on costs to arise. But this doesn’t necessarily mean the end of cloud computing. In fact, cloud spend is continuing to rise, just at a slower rate than before, according to a recent CIO survey.\n\nJust as with the mainframe-to-PC computing paradigm shift, we are likely to continue to move to a cloud-first world. The big three cloud players will have their moats eaten away by smaller, cheaper and interoperable solutions. Some workloads will return to on-prem from the cloud, and a few specialised workloads will remain there until well past the time where the cloud is the default choice.\n\nIn such a world, it will become more important than ever that your software workloads are portable, and properly implemented cloud native build and delivery methods will help you ensure that you have that portability. It just won’t matter that much whether it’s actually running in the cloud or on touchable tin.\n\nThis post was originally published here, and is re-posted with permission.\n\nTL;DR\n\nContext\n\nWriting A 1996 Essay Again in 2023, This Time With Lots More Transistors\n\nChatGPT 3\n\nGathering the Sources\n\nPrivateGPT\n\nOllama (and Llama2:70b)\n\nHallucinations\n\nWhat I Learned\n\nTL;DR\n\nI used private and public LLMs to answer an undergraduate essay question I spent a week working on nearly 30 years ago, in an effort to see how the experience would have changed in that time. There were two rules:\n\nNo peeking at the original essay, and\n\nNo reading any of the suggested material, except to find references.\n\nThe experience turned out to be radically different with AI assistance in some ways, and similar in others.\n\nIf you’re not interested in my life story and the gory detail, skip to the end for what I learned.\n\nContext\n\nAlthough I work in software now, there was a time when I wanted to be a journalist. To that end I did a degree in History at Oxford. Going up in 1994, I entered an academic world before mobile phones, and with an Internet that was so nascent that the ‘computer room’ was still fighting for space with the snooker table for space next to the library (the snooker table was ejected a few years later, sadly). We didn’t even have phones in our dorms: you had to communicate with fellow students either in person, or via an internal ‘pigeon post’ system.\n\nMy typical week then was probably similar to a week of a History student in 1964: tutorials were generally on Friday, where I would be given the essay question and reading list for the following week:\n\nThat afternoon I would go to several libraries, hoping that other students studying the same module hadn’t borrowed the books on the list. Over the following week I would try to read and make notes on the material as much as possible given my busy schedule of drinking incredibly cheap and nasty red wine, feeling depressed, and suffering from the endless colds living in Oxford (which is built on a bog) gifts you free of charge.\n\nThe actual writing of the essay would take place on the day before delivery, and involve a lot of procrastination (solitaire and minesweeper on my Compaq laptop, as I recall) while the painful process of squeezing the juices from my notes took place.\n\nI didn’t go to lectures. They were way too early in the afternoon for me to get out of bed for.\n\nWriting A 1996 Essay Again in 2023, This Time With Lots More Transistors\n\nThe essay question I chose to answer was from a General British History (19th-20th century) course:\n\nHow far was the New Liberalism a Departure from the Old?\n\nChatGPT 3\n\nI first went to ChatGPT (3, the free version – I’m a poor student after all) to try and familiarise myself with the topic:\n\nAlready I was way ahead of my younger self. I had an idea of what the broad strokes I needed to hit were to get a passable essay together. ChatGPT is great for this kind of surveying or summarizing stuff, but is pretty bad at writing an undergraduate-level essay for you. It can write a passable essay (literally), but it will be very obvious it was written by an LLM, and probably at best get you a 2:2 or a third, due to the broad-brush tone and supine mealy-mouthed ‘judgements’ which contain very little content.\n\nAfter a few more interactions with ChatGPT over the next few minutes:\n\nI was now ready to gather the information I needed to write the essay.\n\nGathering the Sources\n\nThe first step was to get the materials. It took me a couple of hours to find all the texts (through various, uh, avenues) and process them into plain text. I couldn’t get the journal articles (they were academically paywalled), which was a shame, as they tended to be the best source of figuring out what academic debate was behind the question.\n\nIt totalled about 10M of data:\n\n756K high_and_low_politics_in_modern_britain_bentley.txt 1.3M lancashire_and_the_new_liberalism.txt 904K liberalism_and_sociology_collini.txt 1012K liberals_and_social_democrats.txt 812K liberals_radicals_and_social_politics_emy.txt 684K new_liberalism_allett.txt 1.5M our_partnership__beatrice_webb.txt 556K the_age_of_lloyd_george_kenneth_o_morgan.txt 708K the_new_liberalism_freeden.txt 540K the_new_liberalism_weiler.txt 148K the_origins_of_the_liberal_welfare_reforms_hay.txt 1.1M unemployment_a_problem_of_industry_beveridge.txt\n\nThose couple of hours would have been reduced significantly for any future essay I might choose to write, as I spent some time figuring out exactly where to get them and installing software to extract the plain text. So I would now allocate an hour to getting the vast majority of texts, as opposed to the small subset of texts I would hunt down over a significantly longer time in the physical libraries 30 years ago.\n\nPrivateGPT\n\nOne of the well-known limitations with ChatGPT is that it doesn’t tell you what the relevant sources are that it looked at to generate the text it gives you.\n\nI found out that this repo gives us a quick way to augment an LLM with a set of documents, so I installed it on an old Dell Xeon desktop with 24 cores and 128G of RAM. This machine cost me about £600 a couple of years ago, and still works a treat (I bought it to run Kubernetes clusters). It doesn’t have a graphics card in it, so I thought it would be usable for this kind of work, but it’s just fine with ‘normal’ CPUs (as long as you don’t demand instant responses).\n\n$ python3 ingest.py\n\nAppending to existing vectorstore at db\n\nUsing embedded DuckDB with persistence: data will be stored in: db\n\nLoading documents from source_documents\n\nLoading new documents: 100%|███████████████████| 12/12 [00:00<00:00, 274.84it/s]\n\nLoaded 12 new documents from source_documents\n\nSplit into 38046 chunks of text (max. 350 tokens each)\n\nCreating embeddings. May take some minutes…\n\nIngestion complete! You can now run privateGPT.py to query your documents\n\nIngestion of the 10M sources took about five minutes. I asked it a few questions, and this was a typical output:\n\n> Question:\n\nwhat was the changing role of the state in new versus old liberalism\n\n> Answer (took 169.94 s.):\n\nThe main difference between \"new\" and \"old\" liberalism lies in their approach towards the role of the government within society. While traditional or 'Old' liberals believed that minimizing intervention by the State would lead to a more prosperous economy, ‘New’ Liberals acknowledged the need for state action and social reform as necessary measures against industrial poverty. In essence, new liberalism marked an evolution of thought among liberals towards greater acceptance of government responsibility in promoting general welfare through interventionist policies.\n\n> source_documents/the_age_of_lloyd_george_kenneth_o_morgan.txt:\n\nIt was, therefore, the Old Liberalism which helped to repair the fortunes of the party—nonconformity and industrial free trade, the very essence of the Liberal\n\nfaith for half a century. Where, however, was the ‘New Liberalism’ which could embrace a more positive attitude towards the central government, for which many\n\n> source_documents/the_new_liberalism_weiler.txt:\n\nIn the broadest sense, the term new Liberalism refers both to the welfare legislation of the 1906-14 administration and to the changes in Liberal social and\n\npolitical theory. At the time, however, the term first referred to the theoretical changes which preceded the legislative achievements of the Liberal government.\n\n> source_documents/the_new_liberalism_weiler.txt:\n\nWhat then was the new Liberalism? As defined here, it was the modification of Liberal ideology from a vehicle for a mid-Victorian ideal of laissez-faire to a\n\nphilosophy based on state action and social reform to cope with industrial poverty. This modification was achieved by Liberal journalists and theorists seeking to\n\n> source_documents/the_new_liberalism_weiler.txt:\n\nstate, a development that represents the fruition of the new Liberalism.\n\nNew Liberalism\n\nThe Political Economy\n\nof J.A. Hobson\n\nUNIVERSITY OF TORONTO PRESS\n\nToronto Buffalo London\n\nToronto Buffalo London\n\nPrinted in Canada\n\nISBN 0-8020-5558-3\n\n> source_documents/the_new_liberalism_weiler.txt:\n\nunderstanding of the phenomenon of the new Liberalism itself.\n\n> source_documents/the_new_liberalism_freeden.txt:\n\nTHE SOCIAL POLICY OL THE NEW LIBERALISM\n\nYou can see that the output is mixed in quality (the model I used was relatively small, see Ollama section below), but it did point me at potentially relevant sources for quotes and bits to read. I spent about another hour rooting around these texts and gathering bits of text that I might use later. Note the response time, however (about three minutes is typical). I got a bit frustrated with the response time vs quality and went looking for other tech to use.\n\nOllama (and Llama2:70b)\n\nThis was the combination of technology that blew my mind. It absolutely changes the game for this kind of work. Ollama is a project designed to ‘get up and running with large language models locally’ which borrows concepts from Docker (like the Modelfile (similar to the Dockerfile, and ollama get/pull/list)\n\nCompared to the ChatGPT output, Llama2:70b (with a Llama2 model with 70b parameters) allowed me to automate a significant chunk of the work needed to write the essay, reducing the time needed to write the essay from days to less than a day.\n\nI downloaded the biggest model I could find (Llama2:70b) and asked it the question in a very unspecific way. It returned a much better essay plan than ChatGPT did:\n\nAt this point I started writing the body of the essay (introductions and conclusions are best left to the end), basing each paragraph on the numbered points above. As I wrote, looked at specific texts to find references and asked LLMs questions, something unexpected happened: I was learning about the nuances of the debate (mainly through looking the Collini book I hadn’t been able to find in the 90s).\n\nI was getting sucked into a temptation to actually read the texts, and had to remind myself that I wasn’t allowed to do that, according to the rules of the ‘game’ I was playing here.\n\nThis appreciation of which texts were important, and what I was being invited to write about happened much faster than would have been the case pre-Internet. I really do wonder how students do this today (get in touch if you can tell me!).\n\nHallucinations\n\nA well-known limitation of LLMs is their tendency to ‘hallucinate’, or ‘bullshit’ a plausible-seeming answer. I definitely saw this, and it wasted some time for me. Llama2 gave me this nice quote, which raised suspicions with me as it seemed too good to be true:\n\nEvidence: In his book \"Industrial Democracy\" (1902), Webb argues that collective ownership and control of the means of production is necessary to address the systemic issues that perpetuate poverty and inequality. He writes, \"The only way to secure industrial democracy is through the collective ownership and control of the means of production.\"\n\nBut I couldn’t find that quote anywhere in the book. Llama2 just made it up.\n\nWhat I Learned\n\nAfter writing my AI-assisted essay, I took a look at my original essay from 1996. To my surprise, the older one was far longer than I remembered essays being (about 2500 words to my AI-assisted one’s 1300), and was, it seemed to me, much higher quality than my AI-assisted one. It may have been longer because it was written in my final year, where I was probably at the peak of my essay-writing skills.\n\nIt took me about six hours of work (spread over 4 days) to write the AI-assisted essay. The original essay would have taken me a whole week, but the actual number of hours spent actively researching and writing it would have been at minimum 20, and at most 30 hours.\n\nI believe I could have easily doubled the length of my essay to match my old one without reducing the quality of its content. But without actually reading the texts I don’t think I would have improved from there.\n\nI learned that:\n\nThere’s still no substitute for hard study. My self-imposed rule that I couldn’t read the texts limited the quality to a certain point that couldn’t be made up for with AI.\n\nHistory students using AI should be much more productive than they were in my day! I wonder whether essays are far longer now than they used to be.\n\nPrivate LLMs (I used llama2:70b) can be way more useful than ChatGPT3.5 for this type of work, not only in the quality of generated response, but also the capability to identify relevant passages of text.\n\nI might have reduced the time significantly with some more work to combine the llama2:70b model with the reference-generating code I had. More research is needed here.\n\nHope you enjoyed my trip down memory lane. The world of humanities study is not one I’m in any more, but it must be being changed radically by LLMs, just as it must have been by the internet. If you’re in that world now, and want to update me on how you’re using AI, get in touch.\n\nThe git repo with essay and various interactions with LLMs is here.\n\nOther Posts\n\nPart I – JSON, Objects and Arrays\n\nPart II – The jq Command\n\nPart IV – Pipes\n\nSimple Filters\n\nIn this section we introduce the most-frequently used feature of jq: the filter. Filters allow you to reduce a given stream of JSON to another smaller, more refined stream of JSON that you can then do more filtering or processing on on if you want.\n\nHow Important is this Post?\n\nFilters are fundamental to using jq, so this post’s content is essential to understand.\n\nWe will cover:\n\nThe most commonly-used filters\n\nSelecting values from a stream of JSON\n\nRailroad diagrams, and how arrays cannot contain name-value pairs in JSON\n\nSetup\n\nCreate a folder to work in, and move into it:\n\n$ mkdir ljqthw_nv $ cd ljqthw_nv\n\nNow create a simple JSON document to work with:\n\n$ echo '{\"user1\": \"alice\", \"user2\": \"bob\"}' > json_object\n\nThis file contains a simple JSON object with two name-value pairs (user1, and user2 being the names, and alice and bob being their values, respectively).\n\nThe Dot Filter\n\nThe concept of the filter is central to jq. The filter allows us to select those parts of the JSON document that we might be interested in.\n\nThe simplest filter – and one you have already come across – is the ‘dot’ filter. This filter is a simple period character (.) and doesn’t do any selection on the data at all:\n\n$ jq . json_object\n\nNote that here we are using the filename as the last argument to jq, rather than passing the data through a UNIX pipe with cat.\n\nArrays, Name-Value Pairs, and Railroad Diagrams\n\nNow let’s try and create a similar array with the same name-value pairs, and run the dot filter against it:\n\n$ echo '[\"user1\": \"alice\", \"user2\": \"bob\"]' > json_array $ jq . json_array\n\nWas that what you expected? When I ran this I expected it to just work, based on my experiences of data structures in other languages. But no. Arrays in JSON cannot contain a name-value pair as one of its values – it’s not a JSON object, and arrays must be composed of JSON objects.\n\nWhat can an array contain? Have a look at this railroad diagram, from https://json.org/:\n\nThe above diagram defines what an array consists of. Make sure you understand how to read the diagram before continuing, as being able to read such diagrams is useful in many contexts in software development.\n\nRailroad Diagram A railroad diagram is also known as a syntax diagram. It visually defines the syntax for a particular language or format. As your code or document is read, you can follow the line,choosing which path to take as it splits. If your code or document can be traced through the 'railroad', then it is syntactically correct. As far as I can tell, there is no 'official' format for railroad diagrams but the conventional signs can easily be found and deciphered by searching online for examples.\n\nThe ‘value’ in the array diagram above is defined here:\n\nFollowing the value diagram, you can see that there is no ‘name-value’ pair defined within a value. Name-value pairs are defined in the object railroad diagram:\n\nA JSON object consists of zero or more name-value pairs, separated by commas, making it fundamentally different from an array, which contains only JSON values separated by spaces.\n\nIt’s worth understanding these diagrams, as they can help you a lot as you try and create and parse JSON with jq. There are further diagrams on the https://json.org site (string, number, whitespace). They are not reproduced in this section, as you only need to reference them in the rarer cases when you are not sure (for example) whether something is a number or not in JSON.\n\nOur First Query – Selecting A Value\n\nNow, back to our original document.\n\n$ cat json_object { \"user1\": \"alice\", \"user2\": \"bob\" }\n\nLet’s say we want to know what user2‘s value is in this document. To do this, we need a filter that outputs only the value for a specific name.\n\nFirst, we can select the object, and then select by name:\n\n$ jq '.user2' json_object\n\nThis is our first proper query with jq. We’ve filtered the contents of the file down to the information we want.\n\nIn the above we just put the bare string user2 after the dot filter. jq accepts this, but you can also refer to the name by placing it in quotes, or placing it in quotes inside square brackets:\n\n$ jq '.[\"user2\"]' json_object $ jq '.\"user2\"' json_object\n\nHowever, with the square brackets and without the quotes does not work:\n\n$ jq '.[user2]' json_object\n\nThis is because the [\"\"] form is the official way to look up a name in an object. The simpler .user2 and .\"user2\" forms we saw above are just a shorthand for this, so you don’t need the four characters of scaffolding ([\"\"]) around the name. As you are learning jq it may be better to use the longer form to embed the ‘correct’ form in your mind.\n\nTypes of Quote Are Important\n\nIt’s important to know that the types of quote you use in your JSON are significant to jq. Type this in, and think about how it is different to the ‘non-broken’ json above.\n\n$ echo \"['user1': 'alice', 'user2': 'bob']\" > json_array_broken $ jq '.' json_array_broken\n\nWhat You Learned\n\nThe ‘dot’ filter\n\nHow to select the value of a name from an object\n\nThe various allowed forms for performing the selection\n\nThe syntactic difference between an object, an array, and a name-value pair\n\nThe types of quote are important\n\nExercises\n\n1) Create another json_array file as per this section, but this time with two JSON objects (ie the same line twice). Re-run the queries above and see what happens to demonstrate that jq works on a stream of JSON objects.\n\nIf you like this, you might like one of my books:\n\nLearn Bash the Hard Way\n\nLearn Git the Hard Way\n\nLearn Terraform the Hard Way\n\nOther Posts\n\nPart II – The jq Command\n\nPart III – Filters\n\nPart IV – Pipes\n\nIntroduction\n\nThis jq series has been written to help users to get to a deeper understanding and proficiency in jq. It doesn’t aim to make you an expert immediately, but you will be more confident about using it and building your knowledge up from that secure base.\n\nYou may well have already played with jq a little – maybe been given a jq command to run by someone else, found a useful one-liner from StackOverflow, or hacked something together quickly that ‘does the job’, but without really understanding what you did. While that’s a great way to get going, a guided course that shows you how the pieces fit together by using it really helps you go further. Understanding these pieces enable you to more creative, solving your own challenges in ways that work for your problem domain.\n\nWhy ‘The Hard Way’?\n\nThe ‘Hard Way’ is a method that emphasises the process required to learn anything. You don’t learn to ride a bike by reading about it, and you don’t learn to cook by reading recipes. Content can help (hopefully, this does) but it’s up to you to do the work.\n\nThis book shows you the path in small digestible pieces and tells you to actually type out the code. This is as important as riding a bike is to learning to ride a bike. Without the brain and the body working together, the knowledge does not properly seep in.\n\nBefore we get hands on with jq, it’s important to know what JSON is, and what it is not.\n\nIn this post, we cover:\n\nWhat JSON is\n\nLook at examples of it\n\nIntroduce key terms\n\nBriefly look at its place in the software landscape\n\nWhy Should I Read This Post?\n\nThis is an introductory post, but an important one.\n\nEven if you’ve seen JSON before, I strongly encourage you to read over this. The reason for that is that getting a clear grasp of the terminology will help enormously when reading jq docs later. In fact, a good working understanding of the terminology is the best way to avoid confusion when using jq.\n\nWhat Is JSON?\n\nJSON is a ‘data interchange format’. This is a fancy way of saying that is a standardised way to write information and send it to other entities who can understand what it means.\n\nYou might have heard of (or even used) other data-interchange formats, such as XML, CSV, Apache Parquet, YAML. Each of these formats has their benefits and disadvantages relative to each other. CSV is very simple and easily understood but is not very good at expressing complex nested information, and can be ambiguous in how it represents data. XML allows for very complex data to be encapsulated but can be verbose and hard for humans to parse. YAML is optimised for human readability, allowing comments and using whitespace rather than special characters to delimit.\n\nJSON is ubiquitous for a few reasons. First, it is simple, being easily parsed by anyone familiar with standard programming languages. Second, it is natively understood by JavaScript, a very popular programming language in the IT industry. Third, it is widely parsed by many programming languages in easily available libraries.\n\nJSON Is Simple\n\nHere is an example JSON object.\n\n{ \"accounting\": [ { \"firstName\": \"Alice\", \"lastName\": \"Zebra\", \"building\": \"7a\", \"age\": 19 }, { \"firstName\": \"Bob\", \"lastName\": \"Young\", \"age\": 28 } ], \"sales\": [ { \"firstName\": \"Celia\", \"lastName\": \"Xi\", \"building\": \"Jefferson\", \"age\": 37 }, { \"firstName\": \"Jim\", \"lastName\": \"Galley\", \"age\": 46 } ] }\n\nThe above JSON represents two departments of a workplace and their employees. The departments are in a ‘collection’ of name-value pairs. \"accounting\" and \"sales\" are the names, and the values are an ordered list of name-value pairs (an ordered list is known as an array).\n\nAnything enclosed within a pair of curly braces (‘{‘ and ‘}‘) is an object. Anything enclosed within a pair of square braces (‘[‘ and ‘]‘) is an array.\n\nIt might sound theoretical, but it’s really important that you understand the above terminology, or at least understand that it’s important. Most jq documentation makes these distinctions carefully, and some use them wrongly, or loosely. This can cause great confusion. When you look at JSON as you read this book, be sure you can explain what it is in clear and correct terms to yourself and others.\n\nThe format is flexible, allowing items within an object to have different name-value pairs. Here, the “building” name is in Celia’s and Alice’s entry, but not in Jim’s or Bob’s.\n\nA JSON document can be an object or an array. Here is the same document as above, but in an array rather than an object.\n\n[ { \"accounting\": [ { \"firstName\": \"Alice\", \"lastName\": \"Zebra\", \"building\": \"7a\", \"age\": 19 }, { \"firstName\": \"Bob\", \"lastName\": \"Young\", \"age\": 28 } ] }, { \"sales\": [ { \"firstName\": \"Celia\", \"lastName\": \"Xi\", \"building\": \"Jefferson\", \"age\": 37 }, { \"firstName\": \"Jim\", \"lastName\": \"Galley\", \"age\": 46 } ] } ]\n\nIn this document, the departments are in a specific order, because they are placed in an array rather than in an object.\n\nIn the above passage, the key terms to grasp are:\n\nName\n\nValue\n\nName-value pairs\n\nObject\n\nArray\n\nWe will cover these in more depth later in this series, but for now just be aware that these names exist, and that understanding them is key to getting to mastery of jq.\n\nNatively Understood By Javascript\n\nJSON arose from Javascript’s need for a way to communicate between processes on different hosts in an agreed format. It was established as a standard around the turn of the century, and any Javascript interpreter now understands JSON out of the box.\n\nUsed By Many Languages\n\nJSON is not specific to JavaScript. It was invented for JavaScript, but is now a general-purpose format that is well-supported by many languages.\n\nHere is an example of an interactive Python session parsing a simplified version of the above JSON into a Python dictionary.\n\n$ python3 >>> json_str = '{\"sales\": [{\"name\": \"Alice\"}], \"accounting\": [{\"name\": \"Bob\"}]}' >>> import json >>> json_parsed = json.loads(json_str) {'sales': [{'name': 'Alice'}], 'accounting': [{'name': 'Bob'}]} >>> type(json_parsed) <class 'dict'> >>> >>> json_parsed['sales'] [{'name': 'Alice'}] >>> json_parsed['sales'][0] {'name': 'Alice'} >>> json_parsed['sales'][0]['name'] 'Alice'\n\nJSON and YAML\n\nMany engineers today make extensive use of YAML as a configuration language. JSON and YAML express very similar document content, but they look different. YAML is easier for humans to read than JSON, and also allows for comments in its documents.\n\nTechnically, JSON can be converted into YAML without any loss of information. But this conversion cannot always go both ways. YAML has a few extra features, such as ‘anchors’ that allow you to reference other items within the same document, which can make converting back to JSON impossible.\n\nJSON Can Be Nested\n\nJSON can have a nested structure. This means that any value within a JSON object or array can have the same structure as the whole document. In other words, every value could itself be a JSON document. So each of the the following lines are valid JSON documents:\n\n{} \"A string\" { \"A name\" : {} } { \"A name\" : [] }\n\nand this one is not valid:\n\n{ {} }\n\nbecause there is no ‘value’ inside the JSON object.\n\nThis one is also not valid:\n\n{ Thing }\n\nbecause values that are strings need to be quoted (just as in JavaScript).\n\nWe will go into more detail on name-value pairs in an upcoming post.\n\nIf you like this, you might like one of my books:\n\nLearn Bash the Hard Way\n\nLearn Git the Hard Way\n\nLearn Terraform the Hard Way\n\nWhat You Learned\n\nWhat JSON is\n\nWhat a JSON object is\n\nWhat a JSON array is\n\nExercises\n\n1) Read the page https://www.json.org/json-en.html\n\n2) Pick a programming language of your choice and parse a JSON document into it\n\nGo to Part II\n\nIf you buy me a coffee, it helps support and encourage my writing.\n\nRecently an Amazon Prime Video (APV) article about their move from serverless tools to ECS and EC2 did the rounds on all the tech socials. A lot of noise was made about it, initially because it was interpreted as a harbinger of the death of serverless technologies, followed by a second wave that lashed back against that narrative. This second wave argued that what had happened was not a failure of serverless, but rather a standard architectural evolution of an initial serverless microservices implementation to a ‘microservice refactoring’.\n\nThis brouhaha got me thinking about why, as an architect, I’ve never truly got onto the serverless boat, and what light this mini-drama throws on that stance. I ended up realising how Amazon and AWS had been at the centre of two computing revolutions that changed the computing paradigm we labour within.\n\nBefore I get to that, let’s recap the story so far.\n\nThe Story\n\nThe APV team had a service which monitored every stream viewed on the platform, and triggered a process to correct poorly-operating streams. This service was built using AWS’s serverless Step Functions and Lambda services, and was never intended to run at high scale.\n\nAs the service scaled, two problems were hit which together forced a re-architecture. Account limits were hit on the number of AWS Step Function transitions, and the cost of running the service was prohibitive.\n\nIn the article’s own words: ‘The move from a distributed microservices architecture to a monolith application helped achieve higher scale, resilience, and reduce costs. […] We realized that [a] distributed approach wasn’t bringing a lot of benefits in our specific use case, so we packed all of the components into a single process.’\n\nThe Reactions\n\nThere were more than a few commentators who relished the chance to herald this as the return of the monolith and/or the demise of the microservice. The New Stack led with an emotive ‘Amazon Dumps Microservices’ headline, while David Heinemeier Hansson, as usual, went for the jugular with ‘Even Amazon Can’t Make Sense of Serverless or Microservices’.\n\nAfter this initial wave of ‘I told you so’ responses, a rearguard action was fought by defenders of serverless approaches to argue that reports of the death of the monolith was premature, and that others were misinterpreting the significance of the original article.\n\nAdrian Cockroft, former AWS VP and well-known proponent of microservices fired back with ‘So Many Bad Takes – What Is There To Learn From The Prime Video Microservices To Monolith Story’, which argued that the original article did not describe a move from microservice to monolith, rather it was ‘clearly a microservice refactoring step’, and that the team’s evolution from serverless to microservice was a standard architectural pathway called ‘Serverless First’. In other words: nothing to see here, ‘the result isn’t a monolith’.\n\nThe Semantics\n\nAt this point, the debate has become a matter of semantics: What is a microservice? Looking at various definitions available, the essential unarguable point is that a microservice is ‘owned by a small team’. You can’t have a microservice that requires extensive coordination between teams to build or deploy.\n\nBut that can’t be the whole story, as you probably wouldn’t describe a small team that releases a single binary with an embedded database, a web server and a Ruby-on-Rails application as a microservice. A microservice implies that services are ‘fine-grained […] communicating through lightweight protocols’.\n\nThere must be some element of component decomposition in a microservice. So what is a component? In the Amazon Prime Video case, you could argue both ways. You could say that the tool is the component, and is a bounded piece of software managed by a small team, or you could say that the detectors and converters are separate components mushed into a now-monolithic application. You could even say that my imagined Ruby-on-Rails monolithic binary above is a microservice if you want to just define a component as something owned by a small team.\n\nAnd what is an application? A service? A process? And on and on it goes. We can continue deconstructing terms all the way down the stack, and as we do so, we see that whether or not a piece of software is architecturally monolithic or a microservice is more or less a matter of perspective. My idea of a microservice can be the same as your idea of a monolith.\n\nBut does all this argumentation over words matter? Maybe not. Let’s ignore the question of what exactly a microservice or a monolith is for now (aside from ‘small team size’) and focus on another aspect of the story.\n\nEasier to Scale?\n\nThe second paragraph of AWS’s definition of microservices made me raise my eyebrows:\n\n‘Microservices architectures make applications easier to scale and faster to develop, enabling innovation and accelerating time-to-market for new features.’\n\nRegardless of what microservices were, these were their promised benefits: faster to develop, and easier to scale. What makes the AVP story so triggering to those of us who had been told we were dinosaurs is that the original serverless implementation of their tool was ludicrously un-scalable:\n\nWe designed our initial solution as a distributed system using serverless components (for example, AWS Step Functions or AWS Lambda), which was a good choice for building the service quickly. In theory, this would allow us to scale each service component independently. However, the way we used some components caused us to hit a hard scaling limit at around 5% of the expected load.\n\nand not just technically un-scalable, but financially too:\n\nAlso, the overall cost of all the building blocks was too high to accept the solution at a large scale.\n\nTo me, this doesn’t sound like their approach has made it ‘easier to scale’. Some, indeed, saw this coming:\n\nFaster to Develop?\n\nBut what about the other benefit, that of being ‘faster to develop’? Adrian Cockroft’s post talks about this, and lays out this comparison table:\n\nThis is where I must protest, starting with the second line which states that ‘traditional’, non-serverless/non-microservices development takes ‘months of work’ compared to the ‘hours of work’ microservices applications take to build.\n\nAnyone who has actually built a serverless system in a real world context will know that it is not always, or even usually, ‘hours of work’. To take one small example of problems that can come up: https://twitter.com/matthewcp/status/1654928007897677824\n\nto which you might add: difficulty of debugging, integration with other services, difficulty of testing scaling scenarios, state management, getting IAM rules right… the list goes on.\n\nYou might object to this, and argue that if your business has approved all the cloud provider’s services, and has a standard pattern for deploying them, and your staff is already well versed in the technologies and how to implement them, then yes, you can implement something in a few hours.\n\nBut this is where I’m baffled. In an analogous context, I have set up ‘traditional’ three-tier systems in a minimal and scalable way in a similar time-frame. Much of my career has been spent doing just that, and I still do that in my spare time because it’s easier for me for prototyping to do that on a server than wiring together different cloud services.\n\nThe supposed development time difference between the two methods is not based on the technology itself, but the context in which you’re deploying it. The argument made by the table is tendentious. It’s based on comparing the worst case for ‘traditional’ application development (months of work) with the best case for ‘rapid development’ (hours of work). Similar arguments can be made for all the table’s comparisons.\n\nThe Water We Swim In\n\nContext is everything in these debates. As all the experts point out, there is no architectural magic bullet that fits all use cases. Context is as complex as human existence itself, but here I want to focus on two areas specifically:\n\ngovernance\n\nknowledge\n\nThe governance context is the set of constraints on your freedom to build and deploy software. In a low-regulation startup these constraints are close to zero. The knowledge context is the degree to which you and your colleagues know how a set of technologies work. It’s assumptions around these contexts that make up the fault lines of most of the serverless debate.\n\nTake this tweet from AWS, which approvingly quotes the CEO of Serverless:\n\n“The great thing about serverless is […] you just have to think about one task, one unit of work”\n\nI can’t speak for other developers, but that’s almost always true for me most of the time when I write functions in ‘traditional’ codebases. When I’m doing that, I’m not thinking about IAM rules, how to connect to databases, the big app, the huge application. I’m just thinking about this one task, this unit of work. And conversely, if I’m working on a serverless application, I might have to think about all the problems I might run into that I listed above, starting with database connectivity.\n\nYou might object that a badly-written three-tier system makes it difficult to write such functions in isolation because of badly-structured monolithic codebases. Maybe so. But microservices architectures can be bad too, and let you ‘think about the one task’ you are doing when you should be thinking about the overall architecture. Maybe your one serverless task is going to cost a ludicrous amount of money (as with APV), or is duplicated elsewhere, or is going to bottleneck another task elsewhere.\n\nAgain: The supposed difference between the two methods is not based on the technology itself, but the context in which you’re working. If I’m fully bought into AWS as my platform from a governance and knowledge perspective, then serverless does allow me to focus on just the task I’m doing, because everything else is taken care of.\n\nHere I’d like to bring up a David Foster Wallace parable about fish:\n\nThere are these two young fish swimming along and they happen to meet an older fish swimming the other way, who nods at them and says “Morning, boys. How’s the water?” And the two young fish swim on for a bit, and then eventually one of them looks over at the other and goes “What the hell is water?”\n\nWhen you’re developing, you want your context to be like water to a fish: invisible, not in your way, sustaining you. But if I’m not a fish swimming in AWS’s metaphorical water, then I’m likely to splash around a lot if I dive into it.\n\nMost advocates of serverless take it as a base assumption of the discussion that you are fully, maturely, and exclusively bought into cloud technologies, and the hyperscalers’ ecosystems. But for many more people working in software (including our customers), that’s not true, and they are wrestling with what, for them, is still a relatively unfamiliar environment.\n\nA Confession\n\nI want to make a confession. Based on what you’ve read so far, you might surmise I’m someone who doesn’t like the idea of serverless technology. But I’ve spent 23 years so far doing serverless work. Yes, I’m one of those people who claims to have 23 years experience in a 15-year old technology.\n\nIn fact, there’s many of us out there. This is because in those days we didn’t call these technologies ‘serverless’ or ‘Lambda’, we called them ‘stored procedures’.\n\nI worked for a company for 15 of those years where the ‘big iron’ database was the water we swam in. We used it for message queues (at such a scale that IBM had to do some pretty nifty work to optimise for our specific use case and give us our own binaries off the main trunk), for our event-driven architectures (using triggers), and as our serverless platform (using stored procedures).\n\nThe joy of having a database as the platform was exactly the same then as the joys of having a serverless platform on a hyperscaler now. We didn’t have to provision compute resources for it (DBA’s problem), maintain the operating system (DBA’s problem), or worry or performance (DBA’s problem, mostly). We didn’t have to think about building a huge application, we just had to think about one task, one unit of work. And it took minutes to deploy.\n\nPeople have drawn similar analogies between serverless and xinetd.\n\nServerless itself is nothing new. It’s just a name for what you’re doing when you can write code and let someone else manage the runtime environment (the ‘water’) for you. What’s new is the platform you treat as your water. For me 23 years ago, it was the database. Now it’s the cloud platform.\n\nMainframes, Clouds, Databases, and Lock-In\n\nThe other objection to serverless that’s often heard is that it increases your lock-in to the hyperscaler, something that many architects, CIOs, and regulators say they are concerned about. But as a colleague once quipped to me: “Lock-in? We are all locked into x86”, the point being that we’re all swimming in some kind of water, so it’s not about avoiding lock-in, but rather choosing your lock-in wisely.\n\nIt was symbolic when Amazon (not AWS) got rid of their last Oracle database in 2019, replacing them with AWS database services. In retrospect, this might be considered the point where businesses started to accept that their core platform had moved from a database to a cloud service provider. A similar inflection point where the mainframe platform was supplanted by commodity servers and PCs might be considered to be July 5, 1994, when Amazon itself was founded. Ironically, then, Amazon heralded both the death of the mainframe, and the birth of its replacement with AWS.\n\nConclusion\n\nWith this context in mind, it seems that the reason I never hopped onto the serveless train is because, to me, it’s not the software paradigm I was ushered into as a young engineer. To me, quickly spinning up a three-tier application is as natural as throwing together an application using S3, DynamoDB, and API Gateway is for those cloud natives that cut their teeth knowing nothing else.\n\nWhat strikes this old codger most about the Amazon Prime Video article is the sheer irony of serverless’s defenders saying that its lack of scalability is the reason you need to move to a more monolithic architecture. It was serverless’s very scalability and the avoidance of the need to re-architect later that was one of its key original selling points!\n\nBut when three-tier architectures started becoming popular I’m sure mainframers of the past said the same thing: “What’s the point of building software on commodity hardware, when it’ll end up on the mainframe?” Maybe they even leapt on articles describing how big businesses were moving their software back to the mainframe, having failed to make commodity servers work for them, and joyously proclaimed that rumours of the death of the mainframe was greatly exaggerated.\n\nAnd in a way, maybe they were right. Amazon killed the physical mainframe, then killed the database mainframe, then created the cloud mainframe. Long live the monolith!\n\nThis article was originally published on Container Solutions’ blog and is reproduced here by permission.\n\nIf you like this, you might like one of my books:\n\nLearn Bash the Hard Way\n\nLearn Git the Hard Way\n\nLearn Terraform the Hard Way\n\nIf you enjoyed this, then please consider buying me a coffee to encourage me to do more."
    }
}
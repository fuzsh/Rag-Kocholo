{
    "id": "dbpedia_8291_2",
    "rank": 63,
    "data": {
        "url": "https://clusterlabs.org/pacemaker/doc/2.1/Clusters_from_Scratch/singlehtml/",
        "read_more_link": "",
        "language": "en",
        "title": "Clusters from Scratch",
        "top_image": "",
        "meta_img": "",
        "images": [
            "https://clusterlabs.org/pacemaker/doc/2.1/Clusters_from_Scratch/singlehtml/_images/pcmk-stack.png",
            "https://clusterlabs.org/pacemaker/doc/2.1/Clusters_from_Scratch/singlehtml/_images/pcmk-internals.png",
            "https://clusterlabs.org/pacemaker/doc/2.1/Clusters_from_Scratch/singlehtml/_images/pcmk-active-passive.png",
            "https://clusterlabs.org/pacemaker/doc/2.1/Clusters_from_Scratch/singlehtml/_images/pcmk-shared-failover.png",
            "https://clusterlabs.org/pacemaker/doc/2.1/Clusters_from_Scratch/singlehtml/_images/pcmk-active-active.png",
            "https://clusterlabs.org/pacemaker/doc/2.1/Clusters_from_Scratch/singlehtml/_images/WelcomeToAlmaLinux.png",
            "https://clusterlabs.org/pacemaker/doc/2.1/Clusters_from_Scratch/singlehtml/_images/InstallationSummary.png",
            "https://clusterlabs.org/pacemaker/doc/2.1/Clusters_from_Scratch/singlehtml/_images/SoftwareSelection.png",
            "https://clusterlabs.org/pacemaker/doc/2.1/Clusters_from_Scratch/singlehtml/_images/NetworkAndHostName.png",
            "https://clusterlabs.org/pacemaker/doc/2.1/Clusters_from_Scratch/singlehtml/_images/ManualPartitioning.png",
            "https://clusterlabs.org/pacemaker/doc/2.1/Clusters_from_Scratch/singlehtml/_images/ManualPartitioning.png",
            "https://clusterlabs.org/pacemaker/doc/2.1/Clusters_from_Scratch/singlehtml/_images/ConfigureVolumeGroup.png",
            "https://clusterlabs.org/pacemaker/doc/2.1/Clusters_from_Scratch/singlehtml/_images/SummaryOfChanges.png",
            "https://clusterlabs.org/pacemaker/doc/2.1/Clusters_from_Scratch/singlehtml/_images/TimeAndDate.png",
            "https://clusterlabs.org/pacemaker/doc/2.1/Clusters_from_Scratch/singlehtml/_images/RootPassword.png",
            "https://clusterlabs.org/pacemaker/doc/2.1/Clusters_from_Scratch/singlehtml/_images/ConsolePrompt.png"
        ],
        "movies": [],
        "keywords": [],
        "meta_keywords": [
            ""
        ],
        "tags": null,
        "authors": [],
        "publish_date": null,
        "summary": "",
        "meta_description": "",
        "meta_lang": "",
        "meta_favicon": "",
        "meta_site_name": "",
        "canonical_link": null,
        "text": "6. Create an Active/Passive Cluster¶\n\n6.1. Add a Resource¶\n\nOur first resource will be a floating IP address that the cluster can bring up on either node. Regardless of where any cluster service(s) are running, end users need to be able to communicate with them at a consistent address. Here, we will use 192.168.122.120 as the floating IP address, give it the imaginative name ClusterIP, and tell the cluster to check whether it is still running every 30 seconds.\n\nWarning\n\nThe chosen address must not already be in use on the network, on a cluster node or elsewhere. Do not reuse an IP address one of the nodes already has configured.\n\n[root@pcmk-1 ~]# pcs resource create ClusterIP ocf:heartbeat:IPaddr2 \\ ip=192.168.122.120 cidr_netmask=24 op monitor interval=30s\n\nAnother important piece of information here is ocf:heartbeat:IPaddr2. This tells Pacemaker three things about the resource you want to add:\n\nThe first field (ocf in this case) is the standard to which the resource agent conforms and where to find it.\n\nThe second field (heartbeat in this case) is known as the provider. Currently, this field is supported only for OCF resources. It tells Pacemaker which OCF namespace the resource script is in.\n\nThe third field (IPaddr2 in this case) is the name of the resource agent, the executable file responsible for starting, stopping, monitoring, and possibly promoting and demoting the resource.\n\nTo obtain a list of the available resource standards (the ocf part of ocf:heartbeat:IPaddr2), run:\n\n[root@pcmk-1 ~]# pcs resource standards lsb ocf service systemd\n\nTo obtain a list of the available OCF resource providers (the heartbeat part of ocf:heartbeat:IPaddr2), run:\n\n[root@pcmk-1 ~]# pcs resource providers heartbeat openstack pacemaker\n\nFinally, if you want to see all the resource agents available for a specific OCF provider (the IPaddr2 part of ocf:heartbeat:IPaddr2), run:\n\n[root@pcmk-1 ~]# pcs resource agents ocf:heartbeat apache conntrackd corosync-qnetd . . (skipping lots of resources to save space) . VirtualDomain Xinetd\n\nIf you want to list all resource agents available on the system, run pcs resource list. We’ll skip that here.\n\nNow, verify that the IP resource has been added, and display the cluster’s status to see that it is now active. Note: There should be a stonith device by now, but it’s okay if it doesn’t look like the one below.\n\n[root@pcmk-1 ~]# pcs status Cluster name: mycluster Cluster Summary: * Stack: corosync * Current DC: pcmk-1 (version 2.1.2-4.el9-ada5c3b36e2) - partition with quorum * Last updated: Wed Jul 27 00:37:28 2022 * Last change: Wed Jul 27 00:37:14 2022 by root via cibadmin on pcmk-1 * 2 nodes configured * 2 resource instances configured Node List: * Online: [ pcmk-1 pcmk-2 ] Full List of Resources: * fence_dev (stonith:some_fence_agent): Started pcmk-1 * ClusterIP (ocf:heartbeat:IPaddr2): Started pcmk-2 Daemon Status: corosync: active/disabled pacemaker: active/disabled pcsd: active/enabled\n\nOn the node where the ClusterIP resource is running, verify that the address has been added.\n\n[root@pcmk-2 ~]# ip -o addr show 1: lo inet 127.0.0.1/8 scope host lo\\ valid_lft forever preferred_lft forever 1: lo inet6 ::1/128 scope host \\ valid_lft forever preferred_lft forever 2: enp1s0 inet 192.168.122.102/24 brd 192.168.122.255 scope global noprefixroute enp1s0\\ valid_lft forever preferred_lft forever 2: enp1s0 inet 192.168.122.120/24 brd 192.168.122.255 scope global secondary enp1s0\\ valid_lft forever preferred_lft forever 2: enp1s0 inet6 fe80::5054:ff:fe95:209/64 scope link noprefixroute \\ valid_lft forever preferred_lft forever\n\n6.2. Perform a Failover¶\n\nSince our ultimate goal is high availability, we should test failover of our new resource before moving on.\n\nFirst, from the pcs status output in the previous step, find the node on which the IP address is running. You can see that the status of the ClusterIP resource is Started on a particular node (in this example, pcmk-2). Shut down pacemaker and corosync on that machine to trigger a failover.\n\n[root@pcmk-2 ~]# pcs cluster stop pcmk-2 pcmk-2: Stopping Cluster (pacemaker)... pcmk-2: Stopping Cluster (corosync)...\n\nNote\n\nA cluster command such as pcs cluster stop <NODENAME> can be run from any node in the cluster, not just the node where the cluster services will be stopped. Running pcs cluster stop without a <NODENAME> stops the cluster services on the local host. The same is true for pcs cluster start and many other such commands.\n\nVerify that pacemaker and corosync are no longer running:\n\n[root@pcmk-2 ~]# pcs status Error: error running crm_mon, is pacemaker running? Could not connect to pacemakerd: Connection refused crm_mon: Connection to cluster failed: Connection refused\n\nGo to the other node, and check the cluster status.\n\n[root@pcmk-1 ~]# pcs status Cluster name: mycluster Cluster Summary: * Stack: corosync * Current DC: pcmk-1 (version 2.1.2-4.el9-ada5c3b36e2) - partition with quorum * Last updated: Wed Jul 27 00:43:51 2022 * Last change: Wed Jul 27 00:43:14 2022 by root via cibadmin on pcmk-1 * 2 nodes configured * 2 resource instances configured Node List: * Online: [ pcmk-1 ] * OFFLINE: [ pcmk-2 ] Full List of Resources: * fence_dev (stonith:some_fence_agent): Started pcmk-1 * ClusterIP (ocf:heartbeat:IPaddr2): Started pcmk-1 Daemon Status: corosync: active/disabled pacemaker: active/disabled pcsd: active/enabled\n\nNotice that pcmk-2 is OFFLINE for cluster purposes (its pcsd is still active, allowing it to receive pcs commands, but it is not participating in the cluster).\n\nAlso notice that ClusterIP is now running on pcmk-1 – failover happened automatically, and no errors are reported.\n\nQuorum\n\nIf a cluster splits into two (or more) groups of nodes that can no longer communicate with each other (a.k.a. partitions), quorum is used to prevent resources from starting on more nodes than desired, which would risk data corruption.\n\nA cluster has quorum when more than half of all known nodes are online in the same partition, or for the mathematically inclined, whenever the following inequality is true:\n\ntotal_nodes < 2 * active_nodes\n\nFor example, if a 5-node cluster split into 3- and 2-node paritions, the 3-node partition would have quorum and could continue serving resources. If a 6-node cluster split into two 3-node partitions, neither partition would have quorum; Pacemaker’s default behavior in such cases is to stop all resources, in order to prevent data corruption.\n\nTwo-node clusters are a special case. By the above definition, a two-node cluster would only have quorum when both nodes are running. This would make the creation of a two-node cluster pointless. However, Corosync has the ability to require only one node for quorum in a two-node cluster.\n\nThe pcs cluster setup command will automatically configure two_node: 1 in corosync.conf, so a two-node cluster will “just work”.\n\nNote\n\nYou might wonder, “What if the nodes in a two-node cluster can’t communicate with each other? Wouldn’t this two_node: 1 setting create a split-brain scenario, in which each node has quorum separately and they both try to manage the same cluster resources?”\n\nAs long as fencing is configured, there is no danger of this. If the nodes lose contact with each other, each node will try to fence the other node. Resource management is disabled until fencing succeeds; neither node is allowed to start, stop, promote, or demote resources.\n\nAfter fencing succeeds, the surviving node can safely recover any resources that were running on the fenced node.\n\nIf the fenced node boots up and rejoins the cluster, it does not have quorum until it can communicate with the surviving node at least once. This prevents “fence loops,” in which a node gets fenced, reboots, rejoins the cluster, and fences the other node. This protective behavior is controlled by the wait_for_all: 1 option, which is enabled automatically when two_node: 1 is configured.\n\nIf you are using a different cluster shell, you may have to configure corosync.conf appropriately yourself.\n\nNow, simulate node recovery by restarting the cluster stack on pcmk-2, and check the cluster’s status. (It may take a little while before the cluster gets going on the node, but it eventually will look like the below.)\n\n[root@pcmk-1 ~]# pcs status Cluster name: mycluster Cluster Summary: * Stack: corosync * Current DC: pcmk-1 (version 2.1.2-4.el9-ada5c3b36e2) - partition with quorum * Last updated: Wed Jul 27 00:45:17 2022 * Last change: Wed Jul 27 00:45:01 2022 by root via cibadmin on pcmk-1 * 2 nodes configured * 2 resource instances configured Node List: * Online: [ pcmk-1 pcmk-2 ] Full List of Resources: * fence_dev (stonith:some_fence_agent): Started pcmk-1 * ClusterIP (ocf:heartbeat:IPaddr2): Started pcmk-1 Daemon Status: corosync: active/disabled pacemaker: active/disabled pcsd: active/enabled\n\n6.3. Prevent Resources from Moving after Recovery¶\n\nIn most circumstances, it is highly desirable to prevent healthy resources from being moved around the cluster. Moving resources almost always requires a period of downtime. For complex services such as databases, this period can be quite long.\n\nTo address this, Pacemaker has the concept of resource stickiness, which controls how strongly a service prefers to stay running where it is. You may like to think of it as the “cost” of any downtime. By default, Pacemaker assumes there is zero cost associated with moving resources and will do so to achieve “optimal” resource placement. We can specify a different stickiness for every resource, but it is often sufficient to change the default.\n\nIn AlmaLinux 9, the cluster setup process automatically configures a default resource stickiness score of 1. This is sufficient to prevent healthy resources from moving around the cluster when there are no user-configured constraints that influence where Pacemaker prefers to run those resources.\n\n[root@pcmk-1 ~]# pcs resource defaults Meta Attrs: build-resource-defaults resource-stickiness=1\n\nFor this example, we will increase the default resource stickiness to 100. Later in this guide, we will configure a location constraint with a score lower than the default resource stickiness.\n\n[root@pcmk-1 ~]# pcs resource defaults update resource-stickiness=100 Warning: Defaults do not apply to resources which override them with their own defined values [root@pcmk-1 ~]# pcs resource defaults Meta Attrs: build-resource-defaults resource-stickiness=100\n\n7. Add Apache HTTP Server as a Cluster Service¶\n\nNow that we have a basic but functional active/passive two-node cluster, we’re ready to add some real services. We’re going to start with Apache HTTP Server because it is a feature of many clusters and is relatively simple to configure.\n\n7.1. Install Apache¶\n\nBefore continuing, we need to make sure Apache is installed on both hosts. We will also allow the cluster to use the wget tool (this is the default, but curl is also supported) to check the status of the Apache server. We’ll install httpd (Apache) and wget now.\n\n# dnf install -y httpd wget # firewall-cmd --permanent --add-service=http # firewall-cmd --reload\n\nImportant\n\nDo not enable the httpd service. Services that are intended to be managed via the cluster software should never be managed by the OS. It is often useful, however, to manually start the service, verify that it works, then stop it again, before adding it to the cluster. This allows you to resolve any non-cluster-related problems before continuing. Since this is a simple example, we’ll skip that step here.\n\n7.2. Create Website Documents¶\n\nWe need to create a page for Apache to serve. On AlmaLinux 9, the default Apache document root is /var/www/html, so we’ll create an index file there. For the moment, we will simplify things by serving a static site and manually synchronizing the data between the two nodes, so run this command on both nodes:\n\n# cat <<-END >/var/www/html/index.html <html> <body>My Test Site - $(hostname)</body> </html> END\n\n7.3. Enable the Apache Status URL¶\n\nPacemaker uses the apache resource agent to monitor the health of your Apache instance via the server-status URL, and to recover the instance if it fails. On both nodes, configure this URL as follows:\n\n# cat <<-END >/etc/httpd/conf.d/status.conf <Location /server-status> SetHandler server-status Require local </Location> END\n\nNote\n\nIf you are using a different operating system, server-status may already be enabled or may be configurable in a different location. If you are using a version of Apache HTTP Server less than 2.4, the syntax will be different.\n\n7.4. Configure the Cluster¶\n\nAt this point, Apache is ready to go, and all that needs to be done is to add it to the cluster. Let’s call the resource WebSite. We need to use an OCF resource agent called apache in the heartbeat namespace . The script’s only required parameter is the path to the main Apache configuration file, and we’ll tell the cluster to check once a minute that Apache is still running.\n\n[root@pcmk-1 ~]# pcs resource create WebSite ocf:heartbeat:apache \\ configfile=/etc/httpd/conf/httpd.conf \\ statusurl=\"http://localhost/server-status\" \\ op monitor interval=1min\n\nBy default, the operation timeout for all resources’ start, stop, monitor, and other operations is 20 seconds. In many cases, this timeout period is less than a particular resource’s advised timeout period. For the purposes of this tutorial, we will adjust the global operation timeout default to 240 seconds.\n\n[root@pcmk-1 ~]# pcs resource op defaults No defaults set [root@pcmk-1 ~]# pcs resource op defaults update timeout=240s Warning: Defaults do not apply to resources which override them with their own defined values [root@pcmk-1 ~]# pcs resource op defaults Meta Attrs: op_defaults-meta_attributes timeout: 240s\n\nNote\n\nIn a production cluster, it is usually better to adjust each resource’s start, stop, and monitor timeouts to values that are appropriate for the behavior observed in your environment, rather than adjusting the global default.\n\nNote\n\nIf you use a tool like pcs to create a resource, its operations may be automatically configured with explicit timeout values that override the Pacemaker built-in default value of 20 seconds. If the resource agent’s metadata contains suggested values for the operation timeouts in a particular format, pcs reads those values and adds them to the configuration at resource creation time.\n\nAfter a short delay, we should see the cluster start Apache.\n\n[root@pcmk-1 ~]# pcs status Cluster name: mycluster Cluster Summary: * Stack: corosync * Current DC: pcmk-1 (version 2.1.2-4.el9-ada5c3b36e2) - partition with quorum * Last updated: Wed Jul 27 00:47:44 2022 * Last change: Wed Jul 27 00:47:23 2022 by root via cibadmin on pcmk-1 * 2 nodes configured * 3 resource instances configured Node List: * Online: [ pcmk-1 pcmk-2 ] Full List of Resources: * fence_dev (stonith:some_fence_agent): Started pcmk-1 * ClusterIP (ocf:heartbeat:IPaddr2): Started pcmk-1 * WebSite (ocf:heartbeat:apache): Started pcmk-2 Daemon Status: corosync: active/disabled pacemaker: active/disabled pcsd: active/enabled\n\nWait a moment, the WebSite resource isn’t running on the same host as our IP address!\n\nNote\n\nIf, in the pcs status output, you see the WebSite resource has failed to start, then you’ve likely not enabled the status URL correctly. You can check whether this is the problem by running:\n\nwget -O - http://localhost/server-status\n\nIf you see Not Found or Forbidden in the output, then this is likely the problem. Ensure that the <Location /server-status> block is correct.\n\n7.5. Ensure Resources Run on the Same Host¶\n\nTo reduce the load on any one machine, Pacemaker will generally try to spread the configured resources across the cluster nodes. However, we can tell the cluster that two resources are related and need to run on the same host (or else one of them should not run at all, if they cannot run on the same node). Here, we instruct the cluster that WebSite can only run on the host where ClusterIP is active.\n\nTo achieve this, we use a colocation constraint that indicates it is mandatory for WebSite to run on the same node as ClusterIP. The “mandatory” part of the colocation constraint is indicated by using a score of INFINITY. The INFINITY score also means that if ClusterIP is not active anywhere, WebSite will not be permitted to run.\n\nNote\n\nIf ClusterIP is not active anywhere, WebSite will not be permitted to run anywhere.\n\nNote\n\nINFINITY is the default score for a colocation constraint. If you don’t specify a score, INFINITY will be used automatically.\n\nImportant\n\nColocation constraints are “directional”, in that they imply certain things about the order in which the two resources will have a location chosen. In this case, we’re saying that WebSite needs to be placed on the same machine as ClusterIP, which implies that the cluster must know the location of ClusterIP before choosing a location for WebSite\n\n[root@pcmk-1 ~]# pcs constraint colocation add WebSite with ClusterIP INFINITY [root@pcmk-1 ~]# pcs constraint Location Constraints: Ordering Constraints: Colocation Constraints: WebSite with ClusterIP (score:INFINITY) Ticket Constraints: [root@pcmk-1 ~]# pcs status Cluster name: mycluster Cluster Summary: * Stack: corosync * Current DC: pcmk-1 (version 2.1.2-4.el9-ada5c3b36e2) - partition with quorum * Last updated: Wed Jul 27 00:49:33 2022 * Last change: Wed Jul 27 00:49:16 2022 by root via cibadmin on pcmk-1 * 2 nodes configured * 3 resource instances configured Node List: * Online: [ pcmk-1 pcmk-2 ] Full List of Resources: * fence_dev (stonith:some_fence_agent): Started pcmk-1 * ClusterIP (ocf:heartbeat:IPaddr2): Started pcmk-1 * WebSite (ocf:heartbeat:apache): Started pcmk-1 Daemon Status: corosync: active/disabled pacemaker: active/disabled pcsd: active/enabled\n\n7.6. Ensure Resources Start and Stop in Order¶\n\nLike many services, Apache can be configured to bind to specific IP addresses on a host or to the wildcard IP address. If Apache binds to the wildcard, it doesn’t matter whether an IP address is added before or after Apache starts; Apache will respond on that IP just the same. However, if Apache binds only to certain IP address(es), the order matters: If the address is added after Apache starts, Apache won’t respond on that address.\n\nTo be sure our WebSite responds regardless of Apache’s address configuration, we need to make sure ClusterIP not only runs on the same node, but also starts before WebSite. A colocation constraint ensures only that the resources run together; it doesn’t affect order in which the resources are started or stopped.\n\nWe do this by adding an ordering constraint. By default, all order constraints are mandatory. This means, for example, that if ClusterIP needs to stop, then WebSite must stop first (or already be stopped); and if WebSite needs to start, then ClusterIP must start first (or already be started). This also implies that the recovery of ClusterIP will trigger the recovery of WebSite, causing it to be restarted.\n\n[root@pcmk-1 ~]# pcs constraint order ClusterIP then WebSite Adding ClusterIP WebSite (kind: Mandatory) (Options: first-action=start then-action=start) [root@pcmk-1 ~]# pcs constraint Location Constraints: Ordering Constraints: start ClusterIP then start WebSite (kind:Mandatory) Colocation Constraints: WebSite with ClusterIP (score:INFINITY) Ticket Constraints:\n\nNote\n\nThe default action in an order constraint is start If you don’t specify an action, as in the example above, pcs automatically uses the start action.\n\nNote\n\nWe could have placed the ClusterIP and WebSite resources into a resource group instead of configuring constraints. A resource group is a compact and intuitive way to organize a set of resources into a chain of colocation and ordering constraints. We will omit that in this guide; see the Pacemaker Explained document for more details.\n\n7.7. Prefer One Node Over Another¶\n\nPacemaker does not rely on any sort of hardware symmetry between nodes, so it may well be that one machine is more powerful than the other.\n\nIn such cases, you may want to host the resources on the more powerful node when it is available, to have the best performance – or you may want to host the resources on the less powerful node when it’s available, so you don’t have to worry about whether you can handle the load after a failover.\n\nTo do this, we create a location constraint.\n\nIn the location constraint below, we are saying the WebSite resource prefers the node pcmk-2 with a score of 50. Here, the score indicates how strongly we’d like the resource to run at this location.\n\n[root@pcmk-1 ~]# pcs constraint location WebSite prefers pcmk-2=50 [root@pcmk-1 ~]# pcs constraint Location Constraints: Resource: WebSite Enabled on: Node: pcmk-2 (score:50) Ordering Constraints: start ClusterIP then start WebSite (kind:Mandatory) Colocation Constraints: WebSite with ClusterIP (score:INFINITY) Ticket Constraints: [root@pcmk-1 ~]# pcs status Cluster name: mycluster Cluster Summary: * Stack: corosync * Current DC: pcmk-1 (version 2.1.2-4.el9-ada5c3b36e2) - partition with quorum * Last updated: Wed Jul 27 00:51:13 2022 * Last change: Wed Jul 27 00:51:07 2022 by root via cibadmin on pcmk-1 * 2 nodes configured * 3 resource instances configured Node List: * Online: [ pcmk-1 pcmk-2 ] Full List of Resources: * fence_dev (stonith:some_fence_agent): Started pcmk-1 * ClusterIP (ocf:heartbeat:IPaddr2): Started pcmk-1 * WebSite (ocf:heartbeat:apache): Started pcmk-1 Daemon Status: corosync: active/disabled pacemaker: active/disabled pcsd: active/enabled\n\nWait a minute, the resources are still on pcmk-1!\n\nEven though WebSite now prefers to run on pcmk-2, that preference is (intentionally) less than the resource stickiness (how much we preferred not to have unnecessary downtime).\n\nTo see the current placement scores, you can use a tool called crm_simulate.\n\n[root@pcmk-1 ~]# crm_simulate -sL [ pcmk-1 pcmk-2 ] fence_dev (stonith:some_fence_agent): Started pcmk-1 ClusterIP (ocf:heartbeat:IPaddr2): Started pcmk-1 WebSite (ocf:heartbeat:apache): Started pcmk-1 pcmk__native_allocate: fence_dev allocation score on pcmk-1: 100 pcmk__native_allocate: fence_dev allocation score on pcmk-2: 0 pcmk__native_allocate: ClusterIP allocation score on pcmk-1: 200 pcmk__native_allocate: ClusterIP allocation score on pcmk-2: 50 pcmk__native_allocate: WebSite allocation score on pcmk-1: 100 pcmk__native_allocate: WebSite allocation score on pcmk-2: -INFINITY\n\n7.8. Move Resources Manually¶\n\nThere are always times when an administrator needs to override the cluster and force resources to move to a specific location. In this example, we will force the WebSite to move to pcmk-2.\n\nWe will use the pcs resource move command to create a temporary constraint with a score of INFINITY. While we could update our existing constraint, using move allows pcs to get rid of the temporary constraint automatically after the resource has moved to its destination. Note in the below that the pcs constraint output after the move command is the same as before.\n\n[root@pcmk-1 ~]# pcs resource move WebSite pcmk-2 Location constraint to move resource 'WebSite' has been created Waiting for the cluster to apply configuration changes... Location constraint created to move resource 'WebSite' has been removed Waiting for the cluster to apply configuration changes... resource 'WebSite' is running on node 'pcmk-2' [root@pcmk-1 ~]# pcs constraint Location Constraints: Resource: WebSite Enabled on: Node: pcmk-2 (score:50) Ordering Constraints: start ClusterIP then start WebSite (kind:Mandatory) Colocation Constraints: WebSite with ClusterIP (score:INFINITY) Ticket Constraints: [root@pcmk-1 ~]# pcs status Cluster name: mycluster Cluster Summary: * Stack: corosync * Current DC: pcmk-1 (version 2.1.2-4.el9-ada5c3b36e2) - partition with quorum * Last updated: Wed Jul 27 00:54:23 2022 * Last change: Wed Jul 27 00:53:48 2022 by root via cibadmin on pcmk-1 * 2 nodes configured * 3 resource instances configured Node List: * Online: [ pcmk-1 pcmk-2 ] Full List of Resources: * fence_dev (stonith:some_fence_agent): Started pcmk-1 * ClusterIP (ocf:heartbeat:IPaddr2): Started pcmk-2 * WebSite (ocf:heartbeat:apache): Started pcmk-2 Daemon Status: corosync: active/disabled pacemaker: active/disabled pcsd: active/enabled\n\nTo remove the constraint with the score of 50, we would first get the constraint’s ID using pcs constraint --full, then remove it with pcs constraint remove and the ID. We won’t show those steps here, but feel free to try it on your own, with the help of the pcs man page if necessary.\n\n8. Replicate Storage Using DRBD¶\n\nEven if you’re serving up static websites, having to manually synchronize the contents of that website to all the machines in the cluster is not ideal. For dynamic websites, such as a wiki, it’s not even an option. Not everyone can afford network-attached storage, but somehow the data needs to be kept in sync.\n\nEnter DRBD, which can be thought of as network-based RAID-1 .\n\n8.1. Install the DRBD Packages¶\n\nDRBD itself is included in the upstream kernel , but we do need some utilities to use it effectively.\n\nAlmaLinux does not ship these utilities, so we need to enable a third-party repository to get them. Supported packages for many OSes are available from DRBD’s maker LINBIT, but here we’ll use the free ELRepo repository.\n\nOn both nodes, import the ELRepo package signing key, and enable the repository:\n\n[root@pcmk-1 ~]# rpm --import https://www.elrepo.org/RPM-GPG-KEY-elrepo.org [root@pcmk-1 ~]# dnf install -y https://www.elrepo.org/elrepo-release-9.el9.elrepo.noarch.rpm\n\nNow, we can install the DRBD kernel module and utilities:\n\n# dnf install -y kmod-drbd9x drbd9x-utils\n\nDRBD will not be able to run under the default SELinux security policies. If you are familiar with SELinux, you can modify the policies in a more fine-grained manner, but here we will simply exempt DRBD processes from SELinux control:\n\n# dnf install -y policycoreutils-python-utils # semanage permissive -a drbd_t\n\nWe will configure DRBD to use port 7789, so allow that port from each host to the other:\n\n[root@pcmk-1 ~]# firewall-cmd --permanent --add-rich-rule='rule family=\"ipv4\" \\ source address=\"192.168.122.102\" port port=\"7789\" protocol=\"tcp\" accept' success [root@pcmk-1 ~]# firewall-cmd --reload success\n\n[root@pcmk-2 ~]# firewall-cmd --permanent --add-rich-rule='rule family=\"ipv4\" \\ source address=\"192.168.122.101\" port port=\"7789\" protocol=\"tcp\" accept' success [root@pcmk-2 ~]# firewall-cmd --reload success\n\nNote\n\nIn this example, we have only two nodes, and all network traffic is on the same LAN. In production, it is recommended to use a dedicated, isolated network for cluster-related traffic, so the firewall configuration would likely be different; one approach would be to add the dedicated network interfaces to the trusted zone.\n\nNote\n\nIf the firewall-cmd --add-rich-rule command fails with Error: INVALID_RULE: unknown element ensure that there is no space at the beginning of the second line of the command.\n\n8.2. Allocate a Disk Volume for DRBD¶\n\nDRBD will need its own block device on each node. This can be a physical disk partition or logical volume, of whatever size you need for your data. For this document, we will use a 512MiB logical volume, which is more than sufficient for a single HTML file and (later) GFS2 metadata.\n\n[root@pcmk-1 ~]# vgs VG #PV #LV #SN Attr VSize VFree almalinux_pcmk-1 1 2 0 wz--n- <19.00g <13.00g [root@pcmk-1 ~]# lvcreate --name drbd-demo --size 512M almalinux_pcmk-1 Logical volume \"drbd-demo\" created. [root@pcmk-1 ~]# lvs LV VG Attr LSize Pool Origin Data% Meta% Move Log Cpy%Sync Convert drbd-demo almalinux_pcmk-1 -wi-a----- 512.00m root almalinux_pcmk-1 -wi-ao---- 4.00g swap almalinux_pcmk-1 -wi-ao---- 2.00g\n\nRepeat for the second node, making sure to use the same size:\n\n[root@pcmk-1 ~]# ssh pcmk-2 -- lvcreate --name drbd-demo --size 512M cs_pcmk-2 Logical volume \"drbd-demo\" created.\n\n8.3. Configure DRBD¶\n\nThere is no series of commands for building a DRBD configuration, so simply run this on both nodes to use this sample configuration:\n\n# cat <<END >/etc/drbd.d/wwwdata.res resource \"wwwdata\" { device minor 1; meta-disk internal; net { protocol C; allow-two-primaries yes; fencing resource-and-stonith; verify-alg sha1; } handlers { fence-peer \"/usr/lib/drbd/crm-fence-peer.9.sh\"; unfence-peer \"/usr/lib/drbd/crm-unfence-peer.9.sh\"; } on \"pcmk-1\" { disk \"/dev/almalinux_pcmk-1/drbd-demo\"; node-id 0; } on \"pcmk-2\" { disk \"/dev/almalinux_pcmk-2/drbd-demo\"; node-id 1; } connection { host \"pcmk-1\" address 192.168.122.101:7789; host \"pcmk-2\" address 192.168.122.102:7789; } } END\n\nImportant\n\nEdit the file to use the hostnames, IP addresses, and logical volume paths of your nodes if they differ from the ones used in this guide.\n\nNote\n\nDetailed information on the directives used in this configuration (and other alternatives) is available in the DRBD User’s Guide. The guide contains a wealth of information on such topics as core DRBD concepts, replication settings, network connection options, quorum, split- brain handling, administrative tasks, troubleshooting, and responding to disk or node failures, among others.\n\nThe allow-two-primaries: yes option would not normally be used in an active/passive cluster. We are adding it here for the convenience of changing to an active/active cluster later.\n\n8.4. Initialize DRBD¶\n\nWith the configuration in place, we can now get DRBD running.\n\nThese commands create the local metadata for the DRBD resource, ensure the DRBD kernel module is loaded, and bring up the DRBD resource. Run them on one node:\n\n[root@pcmk-1 ~]# drbdadm create-md wwwdata initializing activity log initializing bitmap (16 KB) to all zero Writing meta data... New drbd meta data block successfully created. success [root@pcmk-1 ~]# modprobe drbd [root@pcmk-1 ~]# drbdadm up wwwdata --== Thank you for participating in the global usage survey ==-- The server's response is: you are the 25212th user to install this version\n\nWe can confirm DRBD’s status on this node:\n\n[root@pcmk-1 ~]# drbdadm status wwwdata role:Secondary disk:Inconsistent pcmk-2 connection:Connecting\n\nBecause we have not yet initialized the data, this node’s data is marked as Inconsistent Because we have not yet initialized the second node, the pcmk-2 connection is Connecting (waiting for connection).\n\nNow, repeat the above commands on the second node, starting with creating wwwdata.res. After giving it time to connect, when we check the status of the first node, it shows:\n\n[root@pcmk-1 ~]# drbdadm status wwwdata role:Secondary disk:Inconsistent pcmk-2 role:Secondary peer-disk:Inconsistent\n\nYou can see that pcmk-2 connection:Connecting longer appears in the output, meaning the two DRBD nodes are communicating properly, and both nodes are in Secondary role with Inconsistent data.\n\nTo make the data consistent, we need to tell DRBD which node should be considered to have the correct data. In this case, since we are creating a new resource, both have garbage, so we’ll just pick pcmk-1 and run this command on it:\n\n[root@pcmk-1 ~]# drbdadm primary --force wwwdata\n\nNote\n\nIf you are using a different version of DRBD, the required syntax may be different. See the documentation for your version for how to perform these commands.\n\nIf we check the status immediately, we’ll see something like this:\n\n[root@pcmk-1 ~]# drbdadm status wwwdata role:Primary disk:UpToDate pcmk-2 role:Secondary peer-disk:Inconsistent\n\nIt will be quickly followed by this:\n\n[root@pcmk-1 ~]# drbdadm status wwwdata role:Primary disk:UpToDate pcmk-2 role:Secondary replication:SyncSource peer-disk:Inconsistent\n\nWe can see that the first node has the Primary role, its partner node has the Secondary role, the first node’s data is now considered UpToDate, and the partner node’s data is still Inconsistent.\n\nAfter a while, the sync should finish, and you’ll see something like:\n\n[root@pcmk-1 ~]# drbdadm status wwwdata role:Primary disk:UpToDate pcmk-1 role:Secondary peer-disk:UpToDate [root@pcmk-2 ~]# drbdadm status wwwdata role:Secondary disk:UpToDate pcmk-1 role:Primary peer-disk:UpToDate\n\nBoth sets of data are now UpToDate, and we can proceed to creating and populating a filesystem for our WebSite resource’s documents.\n\n8.5. Populate the DRBD Disk¶\n\nOn the node with the primary role (pcmk-1 in this example), create a filesystem on the DRBD device:\n\n[root@pcmk-1 ~]# mkfs.xfs /dev/drbd1 meta-data=/dev/drbd1 isize=512 agcount=4, agsize=32765 blks = sectsz=512 attr=2, projid32bit=1 = crc=1 finobt=1, sparse=1, rmapbt=0 = reflink=1 data = bsize=4096 blocks=131059, imaxpct=25 = sunit=0 swidth=0 blks naming =version 2 bsize=4096 ascii-ci=0, ftype=1 log =internal log bsize=4096 blocks=1368, version=2 = sectsz=512 sunit=0 blks, lazy-count=1 realtime =none extsz=4096 blocks=0, rtextents=0 Discarding blocks...Done.\n\nNote\n\nIn this example, we create an xfs filesystem with no special options. In a production environment, you should choose a filesystem type and options that are suitable for your application.\n\nMount the newly created filesystem, populate it with our web document, give it the same SELinux policy as the web document root, then unmount it (the cluster will handle mounting and unmounting it later):\n\n[root@pcmk-1 ~]# mount /dev/drbd1 /mnt [root@pcmk-1 ~]# cat <<-END >/mnt/index.html <html> <body>My Test Site - DRBD</body> </html> END [root@pcmk-1 ~]# chcon -R --reference=/var/www/html /mnt [root@pcmk-1 ~]# umount /dev/drbd1\n\n8.6. Configure the Cluster for the DRBD device¶\n\nOne handy feature pcs has is the ability to queue up several changes into a file and commit those changes all at once. To do this, start by populating the file with the current raw XML config from the CIB.\n\n[root@pcmk-1 ~]# pcs cluster cib drbd_cfg\n\nUsing pcs’s -f option, make changes to the configuration saved in the drbd_cfg file. These changes will not be seen by the cluster until the drbd_cfg file is pushed into the live cluster’s CIB later.\n\nHere, we create a cluster resource for the DRBD device, and an additional clone resource to allow the resource to run on both nodes at the same time.\n\n[root@pcmk-1 ~]# pcs -f drbd_cfg resource create WebData ocf:linbit:drbd \\ drbd_resource=wwwdata op monitor interval=29s role=Promoted \\ monitor interval=31s role=Unpromoted [root@pcmk-1 ~]# pcs -f drbd_cfg resource promotable WebData \\ promoted-max=1 promoted-node-max=1 clone-max=2 clone-node-max=1 \\ notify=true [root@pcmk-1 ~]# pcs resource status * ClusterIP (ocf::heartbeat:IPaddr2): Started pcmk-1 * WebSite (ocf::heartbeat:apache): Started pcmk-1 [root@pcmk-1 ~]# pcs resource config Resource: ClusterIP (class=ocf provider=heartbeat type=IPaddr2) Attributes: cidr_netmask=24 ip=192.168.122.120 Operations: monitor interval=30s (ClusterIP-monitor-interval-30s) start interval=0s timeout=20s (ClusterIP-start-interval-0s) stop interval=0s timeout=20s (ClusterIP-stop-interval-0s) Resource: WebSite (class=ocf provider=heartbeat type=apache) Attributes: configfile=/etc/httpd/conf/httpd.conf statusurl=http://localhost/server-status Operations: monitor interval=1min (WebSite-monitor-interval-1min) start interval=0s timeout=40s (WebSite-start-interval-0s) stop interval=0s timeout=60s (WebSite-stop-interval-0s)\n\nAfter you are satisfied with all the changes, you can commit them all at once by pushing the drbd_cfg file into the live CIB.\n\n[root@pcmk-1 ~]# pcs cluster cib-push drbd_cfg --config CIB updated\n\nNote\n\nAll the updates above can be done in one shot as follows:\n\n[root@pcmk-1 ~]# pcs resource create WebData ocf:linbit:drbd \\ drbd_resource=wwwdata op monitor interval=29s role=Promoted \\ monitor interval=31s role=Unpromoted \\ promotable promoted-max=1 promoted-node-max=1 clone-max=2 \\ clone-node-max=1 notify=true\n\nLet’s see what the cluster did with the new configuration:\n\n[root@pcmk-1 ~]# pcs resource status * ClusterIP (ocf:heartbeat:IPaddr2): Started pcmk-2 * WebSite (ocf:heartbeat:apache): Started pcmk-2 * Clone Set: WebData-clone [WebData] (promotable): * Promoted: [ pcmk-1 ] * Unpromoted: [ pcmk-2 ] [root@pcmk-1 ~]# pcs resource config Resource: ClusterIP (class=ocf provider=heartbeat type=IPaddr2) Attributes: cidr_netmask=24 ip=192.168.122.120 Operations: monitor interval=30s (ClusterIP-monitor-interval-30s) start interval=0s timeout=20s (ClusterIP-start-interval-0s) stop interval=0s timeout=20s (ClusterIP-stop-interval-0s) Resource: WebSite (class=ocf provider=heartbeat type=apache) Attributes: configfile=/etc/httpd/conf/httpd.conf statusurl=http://localhost/server-status Operations: monitor interval=1min (WebSite-monitor-interval-1min) start interval=0s timeout=40s (WebSite-start-interval-0s) stop interval=0s timeout=60s (WebSite-stop-interval-0s) Clone: WebData-clone Meta Attrs: clone-max=2 clone-node-max=1 notify=true promotable=true promoted-max=1 promoted-node-max=1 Resource: WebData (class=ocf provider=linbit type=drbd) Attributes: drbd_resource=wwwdata Operations: demote interval=0s timeout=90 (WebData-demote-interval-0s) monitor interval=29s role=Promoted (WebData-monitor-interval-29s) monitor interval=31s role=Unpromoted (WebData-monitor-interval-31s) notify interval=0s timeout=90 (WebData-notify-interval-0s) promote interval=0s timeout=90 (WebData-promote-interval-0s) reload interval=0s timeout=30 (WebData-reload-interval-0s) start interval=0s timeout=240 (WebData-start-interval-0s) stop interval=0s timeout=100 (WebData-stop-interval-0s)\n\nWe can see that WebData-clone (our DRBD device) is running as Promoted (DRBD’s primary role) on pcmk-1 and Unpromoted (DRBD’s secondary role) on pcmk-2.\n\nImportant\n\nThe resource agent should load the DRBD module when needed if it’s not already loaded. If that does not happen, configure your operating system to load the module at boot time. For AlmaLinux 9, you would run this on both nodes:\n\n# echo drbd >/etc/modules-load.d/drbd.conf\n\n8.7. Configure the Cluster for the Filesystem¶\n\nNow that we have a working DRBD device, we need to mount its filesystem.\n\nIn addition to defining the filesystem, we also need to tell the cluster where it can be located (only on the DRBD Primary) and when it is allowed to start (after the Primary was promoted).\n\nWe are going to take a shortcut when creating the resource this time. Instead of explicitly saying we want the ocf:heartbeat:Filesystem script, we are only going to ask for Filesystem. We can do this because we know there is only one resource script named Filesystem available to Pacemaker, and that pcs is smart enough to fill in the ocf:heartbeat: portion for us correctly in the configuration. If there were multiple Filesystem scripts from different OCF providers, we would need to specify the exact one we wanted.\n\nOnce again, we will queue our changes to a file and then push the new configuration to the cluster as the final step.\n\n[root@pcmk-1 ~]# pcs cluster cib fs_cfg [root@pcmk-1 ~]# pcs -f fs_cfg resource create WebFS Filesystem \\ device=\"/dev/drbd1\" directory=\"/var/www/html\" fstype=\"xfs\" Assumed agent name 'ocf:heartbeat:Filesystem' (deduced from 'Filesystem') [root@pcmk-1 ~]# pcs -f fs_cfg constraint colocation add \\ WebFS with Promoted WebData-clone [root@pcmk-1 ~]# pcs -f fs_cfg constraint order \\ promote WebData-clone then start WebFS Adding WebData-clone WebFS (kind: Mandatory) (Options: first-action=promote then-action=start)\n\nWe also need to tell the cluster that Apache needs to run on the same machine as the filesystem and that it must be active before Apache can start.\n\n[root@pcmk-1 ~]# pcs -f fs_cfg constraint colocation add WebSite with WebFS [root@pcmk-1 ~]# pcs -f fs_cfg constraint order WebFS then WebSite Adding WebFS WebSite (kind: Mandatory) (Options: first-action=start then-action=start)\n\nReview the updated configuration.\n\n[root@pcmk-1 ~]# pcs -f fs_cfg constraint Location Constraints: Resource: WebSite Enabled on: Node: pcmk-1 (score:50) Ordering Constraints: start ClusterIP then start WebSite (kind:Mandatory) promote WebData-clone then start WebFS (kind:Mandatory) start WebFS then start WebSite (kind:Mandatory) Colocation Constraints: WebSite with ClusterIP (score:INFINITY) WebFS with WebData-clone (score:INFINITY) (rsc-role:Started) (with-rsc-role:Promoted) WebSite with WebFS (score:INFINITY) Ticket Constraints:\n\nAfter reviewing the new configuration, upload it and watch the cluster put it into effect.\n\n[root@pcmk-1 ~]# pcs cluster cib-push fs_cfg --config CIB updated [root@pcmk-1 ~]# pcs resource status * ClusterIP (ocf:heartbeat:IPaddr2): Started pcmk-2 * WebSite (ocf:heartbeat:apache): Started pcmk-2 * Clone Set: WebData-clone [WebData] (promotable): * Promoted: [ pcmk-2 ] * Unpromoted: [ pcmk-1 ] * WebFS (ocf:heartbeat:Filesystem): Started pcmk-2 [root@pcmk-1 ~]# pcs resource config Resource: ClusterIP (class=ocf provider=heartbeat type=IPaddr2) Attributes: cidr_netmask=24 ip=192.168.122.120 Operations: monitor interval=30s (ClusterIP-monitor-interval-30s) start interval=0s timeout=20s (ClusterIP-start-interval-0s) stop interval=0s timeout=20s (ClusterIP-stop-interval-0s) Resource: WebSite (class=ocf provider=heartbeat type=apache) Attributes: configfile=/etc/httpd/conf/httpd.conf statusurl=http://localhost/server-status Operations: monitor interval=1min (WebSite-monitor-interval-1min) start interval=0s timeout=40s (WebSite-start-interval-0s) stop interval=0s timeout=60s (WebSite-stop-interval-0s) Clone: WebData-clone Meta Attrs: clone-max=2 clone-node-max=1 notify=true promotable=true promoted-max=1 promoted-node-max=1 Resource: WebData (class=ocf provider=linbit type=drbd) Attributes: drbd_resource=wwwdata Operations: demote interval=0s timeout=90 (WebData-demote-interval-0s) monitor interval=29s role=Promoted (WebData-monitor-interval-29s) monitor interval=31s role=Unpromoted (WebData-monitor-interval-31s) notify interval=0s timeout=90 (WebData-notify-interval-0s) promote interval=0s timeout=90 (WebData-promote-interval-0s) reload interval=0s timeout=30 (WebData-reload-interval-0s) start interval=0s timeout=240 (WebData-start-interval-0s) stop interval=0s timeout=100 (WebData-stop-interval-0s) Resource: WebFS (class=ocf provider=heartbeat type=Filesystem) Attributes: device=/dev/drbd1 directory=/var/www/html fstype=xfs Operations: monitor interval=20s timeout=40s (WebFS-monitor-interval-20s) start interval=0s timeout=60s (WebFS-start-interval-0s) stop interval=0s timeout=60s (WebFS-stop-interval-0s)\n\n8.8. Test Cluster Failover¶\n\nPreviously, we used pcs cluster stop pcmk-2 to stop all cluster services on pcmk-2, failing over the cluster resources, but there is another way to safely simulate node failure.\n\nWe can put the node into standby mode. Nodes in this state continue to run corosync and pacemaker but are not allowed to run resources. Any resources found active there will be moved elsewhere. This feature can be particularly useful when performing system administration tasks such as updating packages used by cluster resources.\n\nPut the active node into standby mode, and observe the cluster move all the resources to the other node. The node’s status will change to indicate that it can no longer host resources, and eventually all the resources will move.\n\n[root@pcmk-1 ~]# pcs node standby pcmk-2 [root@pcmk-1 ~]# pcs status Cluster name: mycluster Cluster Summary: * Stack: corosync * Current DC: pcmk-1 (version 2.1.2-4.el9-ada5c3b36e2) - partition with quorum * Last updated: Wed Jul 27 05:28:01 2022 * Last change: Wed Jul 27 05:27:57 2022 by root via cibadmin on pcmk-1 * 2 nodes configured * 6 resource instances configured Node List: * Node pcmk-2: standby * Online: [ pcmk-1 ] Full List of Resources: * fence_dev (stonith:some_fence_agent): Started pcmk-1 * ClusterIP (ocf:heartbeat:IPaddr2): Started pcmk-1 * WebSite (ocf:heartbeat:apache): Started pcmk-1 * Clone Set: WebData-clone [WebData] (promotable): * Promoted: [ pcmk-1 ] * Stopped: [ pcmk-2 ] * WebFS (ocf:heartbeat:Filesystem): Started pcmk-1 Daemon Status: corosync: active/disabled pacemaker: active/disabled pcsd: active/enabled\n\nOnce we’ve done everything we needed to on pcmk-2 (in this case nothing, we just wanted to see the resources move), we can unstandby the node, making it eligible to host resources again.\n\n[root@pcmk-1 ~]# pcs node unstandby pcmk-2 [root@pcmk-1 ~]# pcs status Cluster name: mycluster Cluster Summary: * Stack: corosync * Current DC: pcmk-1 (version 2.1.2-4.el9-ada5c3b36e2) - partition with quorum * Last updated: Wed Jul 27 05:28:50 2022 * Last change: Wed Jul 27 05:28:47 2022 by root via cibadmin on pcmk-1 * 2 nodes configured * 6 resource instances configured Node List: * Online: [ pcmk-1 pcmk-2 ] Full List of Resources: * fence_dev (stonith:some_fence_agent): Started pcmk-1 * ClusterIP (ocf:heartbeat:IPaddr2): Started pcmk-1 * WebSite (ocf:heartbeat:apache): Started pcmk-1 * Clone Set: WebData-clone [WebData] (promotable): * Promoted: [ pcmk-1 ] * Unpromoted: [ pcmk-2 ] * WebFS (ocf:heartbeat:Filesystem): Started pcmk-1 Daemon Status: corosync: active/disabled pacemaker: active/disabled pcsd: active/enabled\n\nNotice that pcmk-2 is back to the Online state, and that the cluster resources stay where they are due to our resource stickiness settings configured earlier.\n\n9. Convert Storage to Active/Active¶\n\nThe primary requirement for an active/active cluster is that the data required for your services is available, simultaneously, on both machines. Pacemaker makes no requirement on how this is achieved; you could use a Storage Area Network (SAN) if you had one available, but since DRBD supports multiple Primaries, we can continue to use it here.\n\n9.1. Install Cluster Filesystem Software¶\n\nThe only hitch is that we need to use a cluster-aware filesystem. The one we used earlier with DRBD, xfs, is not one of those. Both OCFS2 and GFS2 are supported; here, we will use GFS2.\n\nOn both nodes, install Distributed Lock Manager (DLM) and the GFS2 command- line utilities required by cluster filesystems:\n\n# dnf config-manager --set-enabled resilientstorage # dnf install -y dlm gfs2-utils\n\n9.2. Configure the Cluster for the DLM¶\n\nThe DLM control daemon needs to run on both nodes, so we’ll start by creating a resource for it (using the ocf:pacemaker:controld resource agent), and clone it:\n\n[root@pcmk-1 ~]# pcs cluster cib dlm_cfg [root@pcmk-1 ~]# pcs -f dlm_cfg resource create dlm \\ ocf:pacemaker:controld op monitor interval=60s [root@pcmk-1 ~]# pcs -f dlm_cfg resource clone dlm clone-max=2 clone-node-max=1 [root@pcmk-1 ~]# pcs resource status * ClusterIP (ocf:heartbeat:IPaddr2): Started pcmk-1 * WebSite (ocf:heartbeat:apache): Started pcmk-1 * Clone Set: WebData-clone [WebData] (promotable): * Promoted: [ pcmk-1 ] * Unpromoted: [ pcmk-2 ] * WebFS (ocf:heartbeat:Filesystem): Started pcmk-1\n\nActivate our new configuration, and see how the cluster responds:\n\n[root@pcmk-1 ~]# pcs cluster cib-push dlm_cfg --config CIB updated [root@pcmk-1 ~]# pcs resource status * ClusterIP (ocf:heartbeat:IPaddr2): Started pcmk-1 * WebSite (ocf:heartbeat:apache): Started pcmk-1 * Clone Set: WebData-clone [WebData] (promotable): * Promoted: [ pcmk-1 ] * Unpromoted: [ pcmk-2 ] * WebFS (ocf:heartbeat:Filesystem): Started pcmk-1 * Clone Set: dlm-clone [dlm]: * Started: [ pcmk-1 pcmk-2 ] [root@pcmk-1 ~]# pcs resource config Resource: ClusterIP (class=ocf provider=heartbeat type=IPaddr2) Attributes: cidr_netmask=24 ip=192.168.122.120 Operations: monitor interval=30s (ClusterIP-monitor-interval-30s) start interval=0s timeout=20s (ClusterIP-start-interval-0s) stop interval=0s timeout=20s (ClusterIP-stop-interval-0s) Resource: WebSite (class=ocf provider=heartbeat type=apache) Attributes: configfile=/etc/httpd/conf/httpd.conf statusurl=http://localhost/server-status Operations: monitor interval=1min (WebSite-monitor-interval-1min) start interval=0s timeout=40s (WebSite-start-interval-0s) stop interval=0s timeout=60s (WebSite-stop-interval-0s) Clone: WebData-clone Meta Attrs: clone-max=2 clone-node-max=1 notify=true promotable=true promoted-max=1 promoted-node-max=1 Resource: WebData (class=ocf provider=linbit type=drbd) Attributes: drbd_resource=wwwdata Operations: demote interval=0s timeout=90 (WebData-demote-interval-0s) monitor interval=29s role=Promoted (WebData-monitor-interval-29s) monitor interval=31s role=Unpromoted (WebData-monitor-interval-31s) notify interval=0s timeout=90 (WebData-notify-interval-0s) promote interval=0s timeout=90 (WebData-promote-interval-0s) reload interval=0s timeout=30 (WebData-reload-interval-0s) start interval=0s timeout=240 (WebData-start-interval-0s) stop interval=0s timeout=100 (WebData-stop-interval-0s) Resource: WebFS (class=ocf provider=heartbeat type=Filesystem) Attributes: device=/dev/drbd1 directory=/var/www/html fstype=xfs Operations: monitor interval=20s timeout=40s (WebFS-monitor-interval-20s) start interval=0s timeout=60s (WebFS-start-interval-0s) stop interval=0s timeout=60s (WebFS-stop-interval-0s) Clone: dlm-clone Meta Attrs: interleave=true ordered=true Resource: dlm (class=ocf provider=pacemaker type=controld) Operations: monitor interval=60s (dlm-monitor-interval-60s) start interval=0s timeout=90s (dlm-start-interval-0s) stop interval=0s timeout=100s (dlm-stop-interval-0s)\n\n9.3. Create and Populate GFS2 Filesystem¶\n\nBefore we do anything to the existing partition, we need to make sure it is unmounted. We do this by telling the cluster to stop the WebFS resource. This will ensure that other resources (in our case, WebSite) using WebFS are not only stopped, but stopped in the correct order.\n\n[root@pcmk-1 ~]# pcs resource disable WebFS [root@pcmk-1 ~]# pcs resource * ClusterIP (ocf:heartbeat:IPaddr2): Started pcmk-1 * WebSite (ocf:heartbeat:apache): Stopped * Clone Set: WebData-clone [WebData] (promotable): * Promoted: [ pcmk-1 ] * Unpromoted: [ pcmk-2 ] * WebFS (ocf:heartbeat:Filesystem): Stopped (disabled) * Clone Set: dlm-clone [dlm]: * Started: [ pcmk-1 pcmk-2 ]\n\nYou can see that both WebSite and WebFS have been stopped, and that pcmk-1 is currently running the promoted instance for the DRBD device.\n\nNow we can create a new GFS2 filesystem on the DRBD device.\n\nWarning\n\nThis will erase all previous content stored on the DRBD device. Ensure you have a copy of any important data.\n\nImportant\n\nRun the next command on whichever node has the DRBD Primary role. Otherwise, you will receive the message:\n\n/dev/drbd1: Read-only file system\n\n[root@pcmk-1 ~]# mkfs.gfs2 -p lock_dlm -j 2 -t mycluster:web /dev/drbd1 It appears to contain an existing filesystem (xfs) This will destroy any data on /dev/drbd1 Are you sure you want to proceed? [y/n] y Discarding device contents (may take a while on large devices): Done Adding journals: Done Building resource groups: Done Creating quota file: Done Writing superblock and syncing: Done Device: /dev/drbd1 Block size: 4096 Device size: 0.50 GB (131059 blocks) Filesystem size: 0.50 GB (131055 blocks) Journals: 2 Journal size: 8MB Resource groups: 4 Locking protocol: \"lock_dlm\" Lock table: \"mycluster:web\" UUID: 19712677-7206-4660-a079-5d17341dd720\n\nThe mkfs.gfs2 command required a number of additional parameters:\n\n-p lock_dlm specifies that we want to use DLM-based locking.\n\n-j 2 indicates that the filesystem should reserve enough space for two journals (one for each node that will access the filesystem).\n\n-t mycluster:web specifies the lock table name. The format for this field is <CLUSTERNAME>:<FSNAME>. For CLUSTERNAME, we need to use the same value we specified originally with pcs cluster setup --name (which is also the value of cluster_name in /etc/corosync/corosync.conf). If you are unsure what your cluster name is, you can look in /etc/corosync/corosync.conf or execute the command pcs cluster corosync | grep cluster_name.\n\nNow we can (re-)populate the new filesystem with data (web pages). We’ll create yet another variation on our home page.\n\n[root@pcmk-1 ~]# mount /dev/drbd1 /mnt [root@pcmk-1 ~]# cat <<-END >/mnt/index.html <html> <body>My Test Site - GFS2</body> </html> END [root@pcmk-1 ~]# chcon -R --reference=/var/www/html /mnt [root@pcmk-1 ~]# umount /dev/drbd1 [root@pcmk-1 ~]# drbdadm verify wwwdata\n\n9.4. Reconfigure the Cluster for GFS2¶\n\nWith the WebFS resource stopped, let’s update the configuration.\n\n[root@pcmk-1 ~]# pcs resource config WebFS Resource: WebFS (class=ocf provider=heartbeat type=Filesystem) Attributes: device=/dev/drbd1 directory=/var/www/html fstype=xfs Meta Attrs: target-role=Stopped Operations: monitor interval=20s timeout=40s (WebFS-monitor-interval-20s) start interval=0s timeout=60s (WebFS-start-interval-0s) stop interval=0s timeout=60s (WebFS-stop-interval-0s)\n\nThe fstype option needs to be updated to gfs2 instead of xfs.\n\n[root@pcmk-1 ~]# pcs resource update WebFS fstype=gfs2 [root@pcmk-1 ~]# pcs resource config WebFS Resource: WebFS (class=ocf provider=heartbeat type=Filesystem) Attributes: device=/dev/drbd1 directory=/var/www/html fstype=gfs2 Meta Attrs: target-role=Stopped Operations: monitor interval=20s timeout=40s (WebFS-monitor-interval-20s) start interval=0s timeout=60s (WebFS-start-interval-0s) stop interval=0s timeout=60s (WebFS-stop-interval-0s)\n\nGFS2 requires that DLM be running, so we also need to set up new colocation and ordering constraints for it:\n\n[root@pcmk-1 ~]# pcs constraint colocation add WebFS with dlm-clone [root@pcmk-1 ~]# pcs constraint order dlm-clone then WebFS Adding dlm-clone WebFS (kind: Mandatory) (Options: first-action=start then-action=start) [root@pcmk-1 ~]# pcs constraint Location Constraints: Resource: WebSite Enabled on: Node: pcmk-2 (score:50) Ordering Constraints: start ClusterIP then start WebSite (kind:Mandatory) promote WebData-clone then start WebFS (kind:Mandatory) start WebFS then start WebSite (kind:Mandatory) start dlm-clone then start WebFS (kind:Mandatory) Colocation Constraints: WebSite with ClusterIP (score:INFINITY) WebFS with WebData-clone (score:INFINITY) (rsc-role:Started) (with-rsc-role:Promoted) WebSite with WebFS (score:INFINITY) WebFS with dlm-clone (score:INFINITY) Ticket Constraints:\n\nWe also need to update the no-quorum-policy property to freeze. By default, the value of no-quorum-policy is set to stop indicating that once quorum is lost, all the resources on the remaining partition will immediately be stopped. Typically this default is the safest and most optimal option, but unlike most resources, GFS2 requires quorum to function. When quorum is lost both the applications using the GFS2 mounts and the GFS2 mount itself cannot be correctly stopped. Any attempts to stop these resources without quorum will fail, which will ultimately result in the entire cluster being fenced every time quorum is lost.\n\nTo address this situation, set no-quorum-policy to freeze when GFS2 is in use. This means that when quorum is lost, the remaining partition will do nothing until quorum is regained.\n\n[root@pcmk-1 ~]# pcs property set no-quorum-policy=freeze\n\n9.5. Clone the Filesystem Resource¶\n\nNow that we have a cluster filesystem ready to go, we can configure the cluster so both nodes mount the filesystem.\n\nClone the Filesystem resource in a new configuration. Notice how pcs automatically updates the relevant constraints again.\n\n[root@pcmk-1 ~]# pcs cluster cib active_cfg [root@pcmk-1 ~]# pcs -f active_cfg resource clone WebFS [root@pcmk-1 ~]# pcs -f active_cfg constraint Location Constraints: Resource: WebSite Enabled on: Node: pcmk-2 (score:50) Ordering Constraints: start ClusterIP then start WebSite (kind:Mandatory) promote WebData-clone then start WebFS-clone (kind:Mandatory) start WebFS-clone then start WebSite (kind:Mandatory) start dlm-clone then start WebFS-clone (kind:Mandatory) Colocation Constraints: WebSite with ClusterIP (score:INFINITY) WebFS-clone with WebData-clone (score:INFINITY) (rsc-role:Started) (with-rsc-role:Promoted) WebSite with WebFS-clone (score:INFINITY) WebFS-clone with dlm-clone (score:INFINITY) Ticket Constraints:\n\nTell the cluster that it is now allowed to promote both instances to be DRBD Primary.\n\n[root@pcmk-1 ~]# pcs -f active_cfg resource update WebData-clone promoted-max=2\n\nFinally, load our configuration to the cluster, and re-enable the WebFS resource (which we disabled earlier).\n\n[root@pcmk-1 ~]# pcs cluster cib-push active_cfg --config CIB updated [root@pcmk-1 ~]# pcs resource enable WebFS\n\nAfter all the processes are started, the status should look similar to this.\n\n[root@pcmk-1 ~]# pcs resource * ClusterIP (ocf:heartbeat:IPaddr2): Started pcmk-1 * WebSite (ocf:heartbeat:apache): Started pcmk-1 * Clone Set: WebData-clone [WebData] (promotable): * Promoted: [ pcmk-1 pcmk-2 ] * Clone Set: dlm-clone [dlm]: * Started: [ pcmk-1 pcmk-2 ] * Clone Set: WebFS-clone [WebFS]: * Started: [ pcmk-1 pcmk-2 ]\n\n9.6. Test Failover¶\n\nTesting failover is left as an exercise for the reader.\n\nWith this configuration, the data is now active/active. The website administrator could change HTML files on either node, and the live website will show the changes even if it is running on the opposite node.\n\nIf the web server is configured to listen on all IP addresses, it is possible to remove the constraints between the WebSite and ClusterIP resources, and clone the WebSite resource. The web server would always be ready to serve web pages, and only the IP address would need to be moved in a failover."
    }
}
{
    "id": "dbpedia_1863_3",
    "rank": 69,
    "data": {
        "url": "https://arxiv.org/html/2403.02259v1",
        "read_more_link": "",
        "language": "en",
        "title": "Human-AI Collaboration Increases Skill Tagging Speed but Degrades Accuracy",
        "top_image": "",
        "meta_img": "",
        "images": [
            "https://arxiv.org/html/extracted/5448173/example_questions_2.png",
            "https://arxiv.org/html/extracted/5448173/boxplot_rate_both.png"
        ],
        "movies": [],
        "keywords": [],
        "meta_keywords": [
            "Human-AI collaboration",
            "Common Core",
            "Skill Tagging",
            "A/B study",
            "Open Educational Resources"
        ],
        "tags": null,
        "authors": [],
        "publish_date": null,
        "summary": "",
        "meta_description": "",
        "meta_lang": "en",
        "meta_favicon": "",
        "meta_site_name": "",
        "canonical_link": null,
        "text": "License: CC BY 4.0\n\narXiv:2403.02259v1 [cs.HC] 04 Mar 2024\n\nHuman-AI Collaboration Increases Skill Tagging Speed but Degrades Accuracy\n\nCheng Ren , Zachary Pardos and Zhi Li\n\nAbstract.\n\nAI approaches are progressing besting humans at game-related tasks (e.g. chess). The next stage is expected to be Human-AI collaboration; however, the research on this subject has been mixed and is in need of additional data points. We add to this nascent literature by studying Human-AI collaboration on a common administrative educational task. Education is a special domain in its relation to AI and has been slow to adopt AI approaches in practice, concerned with the educational enterprise losing its humanistic touch and because standard of quality is demanded because of the impact on a person’s career and developmental trajectory. In this study (N = 22), we design an experiment to explore the effect of Human-AI collaboration on the task of tagging educational content with skills from the US common core taxonomy. Our results show that the experiment group (with AI recommendations) saved around 50% time (p <<<<<< 0.01) in the execution of their tagging task but at the sacrifice of 7.7% recall (p = 0.267) and 35% accuracy (p= 0.1170) compared with the non-AI involved control group, placing the AI+human group in between the AI alone (lowest performance) and the human alone (highest performance). We further analyze log data from this AI collaboration experiment to explore under what circumstances humans still exercised their discernment when receiving recommendations. Finally, we outline how this study can assist in implementing AI tools, like ChatGPT, in education.\n\nHuman-AI collaboration, Common Core, Skill Tagging, A/B study, Open Educational Resources\n\n††ccs: Human-centered computing User studies††ccs: Applied computing Education††ccs: Computing methodologies Artificial intelligence\n\n1. Introduction\n\nAI has made significant advancements and achieved impressive performance in a variety of tasks across multiple fields. The rapid progress of AI in numerous benchmarks leads to the expectation that it will incraesingly be able to perform tasks in real-world settings. For instance, a meta-analysis conducted by Liu et al. (Liu et al., 2019) compared the diagnostic performance of deep learning algorithms to healthcare professionals in detecting diseases from medical imaging, and found that the two were similar. Similarly, Chen et al. (Chen et al., 2020) systematically reviewed the use of AI in education and found that it has been applied in education administration, instruction, and learning, and has had a somewhat positive impact in these areas. While AI has demonstrated impressive performance in these tasks, it is not currently capable of replacing humans in the decision-making process. Instead, its role is to support humans in decision-making.\n\nThe results of studies on Human-AI collaboration in real-world tasks have been mixed. Weisz et al. (Weisz et al., 2022) reviewed a set of such studies and found that only two out of ten experiments demonstrated a benefit in both time savings and outcome quality with the introduction of AI (Desmond et al., 2021; Ashktorab et al., 2021). Comparing these two studies, which involve categorizing a customer support question, with the skill tagging task, the skill tagging task is more difficult because of the latent nature of cognitive processes. The process of skill tagging can be likened to linking questions with labels, while also incorporating a deeper comprehension of the procedures necessary to achieve objectives or fulfill tasks. It is uncertain whether the current results in the general computer-human interaction field can be generalized to education. In the domain of education, where there is a need for more efficient processes to embrace AI in a humanistic endeavor, it is important to collect empirical support and study the effect of AI+human interaction on educational tasks.\n\nEducation, as an industry, is motivated to adopt AI for several reasons, such as improving efficiency in administrative tasks, personalizing to students’ needs, and enhancing the quality of teaching. However, educational institutions, particularly public schools, are often constrained by limited budgets due to their nonprofit status. According to an estimation by The Century Foundation, K-12 public schools in the US are underfunded by approximately $150 billion annually (Foundation, 2020). As a result, educational institutions are keen to consider more efficient approaches to administration. However, AI adoption in education also raises concerns. One major concern is fairness and discrimination, which are critical issues in AI and are even more significant in education where equity is often a paramount concern (Akgun and Greenhow, 2021; Jiang and Pardos, 2021). Education is not merely a process of imparting knowledge but also a humanistic pursuit, deeply entwined with the relationship between teachers and students. Even functions that appear predominantly administrative, such as skill tagging, bear considerable weight on students, thereby leading to a sense of responsibility to retain human supervision and maintain a humanistic touch. Additionally, due to the profound implications that education has on determining an individual’s future pathway, there is an imperative to keep high-quality standards for the operational performance of AI in educational settings.\n\nOur study aims to contribute a data point on the efficiency and outcome quality of AI+human interaction in skill tagging tasks task. The paper will review related works on AI+human interactions in general as well as in AI in education. It will then introduce the methods used in the study, including data sources, model design, and experiment design. The results of the experiment and a discussion section will follow. Finally, the paper will discuss limitations and future works.\n\n2. Related Works\n\nOrganizations and individuals often spend a significant amount of time aligning their own standards with those of others. Taxonomy alignment typically requires the manual work of subject matter experts. To make this process more efficient, researchers have turned to AI techniques. For example, Choi et al. (Choi et al., 2016) used phrase graphs to calculate the similarity between skills, while Yilmazel et al. (Yilmazel et al., 2007) employed rule-based techniques and machine learning to extract and classify features from skill descriptions. Koedinger et al. (Rivers et al., 2016) used learning curve analysis to build models for knowledge components in teaching Python programming. These models are able to map questions to such skills. Li et al.(Li et al., 2021) used problem text, response sequences, and modern neural approaches from computational linguistics for taxonomy mapping. Recently, Shen et al.(Shen et al., 2021) applied a variant of BERT called ”task adaptive BERT” on multi-sources like problem descriptions, skill descriptions for skill classification.\n\nIn the educational field, it is essential to note that humans are the final decision makers, even when AI recommendations are provided. Weisz et al. (Weisz et al., 2022) concluded that AI+human interactions have mixed outcomes across different domains and even within domains. For example, within the field of education, Cognitive Tutors, Teacher+AI, support can assist students in achieving proficiency comparable to that attained through conventional, teacher-only instruction, accomplishing this in one-third less time (Anderson et al., 1995). Holstein et al. (Holstein et al., 2018) observed that AI-enhanced classrooms equipped with real-time analytics about student learning can help narrow the gap in students’ learning outcomes. Recently, Weitekamp et al. (Weitekamp et al., 2020) conducted an experiment using Machine Learning to aid Intelligent Tutoring System (ITS) authoring (Human-AI interaction) in comparison to human authoring utilizing Cognitive Tutor Authoring Tools (CTAT, Human only). The AI-assisted tool demonstrated a 75% time-saving in completeness compared to CTAT. Recently, Demszky and Liu (Demszky and Liu, 2023) developed an AI system to provide feedback to instructors on dialogic teaching methods, enhancing mentors’ uptake of student contributions by 10% and improving the learning experience for students.\n\nIn other fields, Lai et al. (Lai and Tan, 2019) only improved the quality of deception detection tasks, while Weber et al. (Weber et al., 2020) found that AI+human did not produce higher quality in image restoration tasks and Clark et al. (Clark et al., 2018) noted that AI+human reduced the quality of creating marketing slogans.\n\nTo explore the mechanisms behind these results, scholars have examined interactions from various perspectives. Some studies have observed that individuals might place inappropriate trust in AI due to either over- or under-reliance on it (Zhang et al., 2020; Siau and Wang, 2018). Dietvorst et al. (Dietvorst et al., 2015) discovered that when algorithmic forecasters commit errors identical to those made by human forecasters, people lose confidence in the algorithmic forecasters more swiftly. Additionally, an individual’s trust may be influenced by both the declared and perceived accuracy of the system (Yin et al., 2019). When tasks become exceptionally complex, individuals might overestimate the capabilities of AI and elevate their trust in it (Chong et al., 2022). Although investigations in this domain have been conducted from diverse viewpoints, the research is frequently general, and specific educational research—especially research on skill tagging with AI—remains relatively understudied.\n\nNeural networks have been very effective at learning semantics from a variety of data types, such as natural languages and images. These fields have advanced modeling techniques in their ability to learn semantic signals from these data sources. Scholars are using new techniques in AI to help humans accomplish partial or even entire workloads in various real-world scenarios. For example, natural language processing (NLP) techniques have been applied in education on auto grading of short open-end questions, essays, and improving course recommendations just to name a few (Dong et al., 2017; Ke and Ng, 2019; Pardos et al., 2019).\n\n3. Methods\n\nIn this study, we aim to compare the performance of two groups of skill taggers: one group that receives AI assistance (experimental group) and one group that does not (control group). We randomly picked 30 problems from grade 6 on the digital learning platform CK12 to use in our experiment. All problems have text and some have images. We used a pre-trained model from (Li et al., 2024) to generate the Common Core skill tags, a common standard used by many US states, which will be used as AI recommendations.\n\nWe will first discuss the AI assistance model that we use, including its data source, training, testing and selecting strategies. Then, we will detail the design of our experiment and explain how we will analyze the results.\n\n3.1. Machine Learning Model\n\nWe use a pre-trained model from (Li et al., 2024) to generate Common Core skill recommendations for the selected 30 problems. The model is trained on Khan Academy(N=21,475) problems to Common Core skills(N=385) mapping. The model takes the problem text and associated image as input, generates a text vector via sentenceBERT (Reimers and Gurevych, 2019) and an image encoder via EfficientNet (Tan and Le, 2019), fuses them into a single vector via Compact Bilinear Pooling (Gao et al., 2016), and maps the vector to the associated skills using either a classification model or a similarity matching model. The classification model is a neural network classifier that is trained to predict skill labels using the problem vector, while the similarity matching model(Translation model) encodes the skills using sentenceBERT and ensures the problem vector and the associated skill vector have a high cosine similarity.\n\nSince pre-trained models(Li et al., 2024) provided many versions to choose from, we also tested those models on similar problems from CK12 to determine the best model to use (i.e., classification model or similarity matching model, and whether to include images in the input). Specifically, we collected 18,728 problems from other grades in CK12 to conduct an offline experiment. We enumerated all 385 Common Core skills, and their skill descriptions, associating to this CK12 content found on their website and consider these the ground truth labels, which were aligned by experts from CK12. We used all grades on CK12 as the test set except grade 6, which is the grade we used for the online study, to avoid overfitting to the problems. The evaluation metric was recall@3, as we will provide 3 recommendations for each question in the online study. The results of the offline experiments, shown in Table 1, indicate that the similarity matching model with both text and image input had the highest average recall@3 at 0.496. Therefore, we will use this model to generate recommendations for the online study.\n\n3.2. Online Study\n\n3.2.1. Experiment Design\n\nTo compare the performance of students with and without AI assistance, we collected responses to a survey using the Qualtrics platform. The survey consisted of 30 math problems, and for each problem, students were asked to select the skills that were demonstrated in the problem. There were five broad categories of skills to choose from: Ratios & Proportional Relationships, The Number System, Expressions & Equations, Geometry, and Statistics & Probability. Once a broad category was chosen, students could select the specific sub-categories of skills within that category. Students were allowed to select multiple skills for each problem, up to a maximum of three. Meanwhile, they could not move to the next question if they only choose the big category but did not choose any skills under it.\n\nIn order to compare the performance with and without AI involved, we designed two versions of a survey. The first version followed the design described above, while the second version added a message recommending three skills, generated by the machine learning model and ordered by similarity score, which were shown under the math question and before the five category skill selection interface as shown in Figure 1.\n\nBefore the online experiment, we obtained IRB approval. During recruitment, we sent out a registration form to undergraduate students at a public university in the United States. We were able to recruit 22 students for the study. The experimental session took place over a 50-minute synchronous Zoom session due to the pandemic, including a 3-minute introduction in the main room, a 2-minute break in the demonstration room for the different versions of the survey, and 45 minutes for skill tagging. Some students may not have been able to finish all 30 questions within the allotted time. The students were randomly split into two equally-sized groups of 11 for the two surveys: one with AI recommendations(experimental group) and the other without(control group). To reduce the incentive for students to game the survey, completing it without authentic engagement, we required them to return to the main Zoom room and stay there until the end of the experiment.\n\n3.2.2. Experiment Analysis\n\nWe will use accuracy, recall@n, and precision@n as evaluation metrics. For a response to be considered accurate, the skill(s) selected by the study participant must exactly match the true skills. Since people do not necessarily select the same number of skills for each question, we will calculate the recall@n and precision@n, where n𝑛nitalic_n refers to the number of skills selected and varies across different candidates and questions. We will also present the recall@n and precision@n performance of the algorithm alone, depicting how the tagging would be performed without human collaboration.\n\nWe will aggregate the performance metrics at three levels: averaged per question, per person, and per response. We have a list of responses, each corresponding to a pair of person and question. For example, to obtain the metrics at the per question level, we will first average the metrics for a given question across participants, and then average all the individual question metrics together.\n\nWe will report the results for the experimental and control groups and compare them using appropriate statistical tests.\n\nEach skill has several levels based on common core skills. For example, EE.A.1 corresponds to the skill “Expressions & Equations ->>> Apply and extend previous understandings of arithmetic to algebraic expressions ->>> Write and evaluate numerical expressions involving whole-number exponents.” To compare how much participants follow the recommendations, we will test whether their choices overlap with the recommendations at different levels. For instance, if the three recommendations are NS.C.7, NS.C.6, and SP.A.3, then if a participant chooses either NS or SP, we count that as 1 at the first level. We replicate this step for the second and third levels as well. Since we have 11 participants for each group, we can calculate the overlap rate for each question by counting the percentage of participants who choose the same answers as the ones recommended by the AI (e.g., a 50% overlap rate means that half of the participants choose the same answers as the AI recommended). After we have obtained the overlap rates for each question at different levels, we will use an independent t-test to compare whether there is a significant difference between the experimental and control groups. We also conduct similar calculations by each person.\n\nIn addition to collecting responses, the survey also collects some metadata, such as the click count for each response per person.\n\n4. Results\n\nAfter calculating the time and recall/accuracy, the results show that the experimental group had a lower accuracy (0.115) than the control group (0.176) at all three levels, with the difference being statistically significant (p-value ¡ 0.05) at the per question and per response levels. The experimental group also had lower recall@n and precision@n than the control group at all three levels, but only the difference in precision@n at the per-response level was significant. In terms of the time it took to tag the skills, the experimental group took significantly less time (23.5 seconds per problem) than the control group (44 seconds per problem). There was not much difference between the three levels, but the algorithm’s recall@n and precision@n were worse than those of both the control and experimental groups.\n\nNext, to explore how people in the experimental group interacted with the recommendations, we calculated the overlap rate between human choices and recommendations in the two groups. Overall, the overlap rates between the experimental and control groups were all statistically different at a significance level of 95%, meaning that, as expected, participants in the AI condition exhibited more overlap with the AI recommendations than the control condition. Meanwhile, as the level increased, the overlap rate in both groups decreased, but the difference in the mean overlap rate between the two groups increased (see Table 3). This change indicates that there were more disagreements between humans and AI at finer granularity levels in this case. According to Figure 2 (left), at a coarser level (e.g., Level 1), the range in both groups was smaller when compared to the other two finer levels. Within each level, the experimental group’s range was smaller than the control group’s, which shows that there was more consistency between the experimental group and the recommendations. This analysis reveals that participants with recommendations (experimental group) were more likely to follow them. When looking at the personal level, Figure 2 (right) indicates that the gap between the experimental and control groups increased as the tagging granularity level of analysis became finer. The 25th percentile to 75th percentile range did not even overlap, which shows the significant influence of recommendations.\n\nTo understand the behavior of participants while answering questions, we also analyzed the click count for each question. Since each question had multiple inputs (people), we chose the median for each question. The mean of the median click count for each question in the experimental group was 4.4, while in the control group, it was 5.7. In our design, people had to click at least twice to move on to the next question. 26.7% of the clicks in the experimental group and 15.4% of the clicks in the control group were less than three clicks.\n\n5. Discussion\n\nOur work advances the understanding of AI and human interaction, particularly when the AI’s capacity is not as good as humans, which is typical in most AI applications at this juncture. When comparing the AI+Human and Human-only groups, the biggest benefit is time-saving. The AI+Human group saved almost half the time for the same tasks. After exploring the choice patterns in both the experimental and control groups, we believe the AI recommendations impact the speed-up effects in the experimental group. The overlap rate between the recommendations and human in the experimental group is statistically significantly higher than in the control group at every level, meaning that the recommendations were persuasive, at least to the participants in our design.\n\nIn terms of accuracy, the AI+Human group performed better than the prediction from the AI, but still worse than the Human-only group. Humans defer more to the AI and reduce independent thoughts, which results in less time, but they are still able to detect outliers that are very far from Human expectations. This explains why the Human+AI group performed better than the AI-alone predictions and worse than the Human-only group. For example, for one question “Find the mean of the following set of numbers: 6, 5, -9, 8, 3, -1”, the overlap rate between the AI recommendation and human choice was 0.45, compared to the median rate of 1 at level 1 by question. The recommendation indicated that the skills fell under the EE (Expressions & Equations) and NS (Number System) categories, but the correct category was SP (Statistics & Probability), so the participants decided to reject all the recommendations and choose the answers they believed to be correct.\n\nAccording to this specific case, we observed an average effect. Specifically, the performance of the AI+Human group was between that of the AI prediction and the Human-only group in terms of both accuracy and time cost. This raises an interesting question not only in AI in education but also more broadly in the workforce with both AI and humans. If the performance of the AI+Human group or the AI prediction is worse than that of the Human-only group, what will the trade-off look like? Will decision-makers choose to take the speed-up or maintain higher accuracy? It is difficult to discuss this without specific contexts, but we do see cases in different fields such as healthcare trying to speed up operations with AI (Abdullah et al., 2020).\n\nIf the AI’s performance more closely rivaled humans’, we might expect an ensemble effect whereby the combination of the two leads to a superior accuracy rather than an accuracy that is the average of the two, which we observed in our study. In that case, we can speculate that human taggers could continue to correct obvious mistakes made by AI due to lacking causal inference and other human faculties, and that there would be fewer less obvious, uncaught errors. Desmond et al. (Desmond et al., 2021) noted that their AI system with humans achieved better results in both speed and accuracy(0.79) than the Human-only group (0.72) although with low accuracy from AI-only (0.44). Jarrahi (Jarrahi, 2018) argues that the future of AI and humans is for intelligence augmentation, which emphasizes that each side can bring its own strengths to decision-making processes.\n\nAs AI tools, like ChatGPT, gain increased popularity and accessibility, there’s a growing interest in comparing AI and human efficacy in educational tasks and exploring avenues for AI-Teacher and AI-Student collaboration. However, nascent evaluations of ChatGPT in educational settings are finding subpar performance of the model in comparison to human experts. For instance, Pardos and Bhandari (Pardos and Bhandari, 2023) leveraged ChatGPT to generate hints for Algebra courses, with 30% of the hints produced failing manual quality checks. Similarly, Wang and Demszky (Wang and Demszky, 2023) engaged math teachers to assess the zero-shot performance of ChatGPT in tasks like identifying highlights for good instructional strategies within math classroom transcripts. However, 82% of the model’s suggestions repeated the teachers’ suggestions. Given the ongoing integration of ChatGPT by some teachers, it’s crucial to provide suggestions or even evidence on where ChatGPT can effectively enhance teaching and learning experiences, clarifying how AI can best support educational outcomes. These findings, in combination with our results, suggest that premature collaboration with AI may result in degraded quality of outcome and potentially lower educational quality.\n\n6. Limitations and Future Work\n\nFirst, during the tagging process, students were not trained in the skill tagging tasks. Skill tagging is typically not performed by students, but rather by domain experts. Although we asked college students in STEM during the recruitment, they don’t know the rules followed by CK12 taggers. We observed similar designs in other studies where the participants were usually from Amazon Mechanical Turk (Desmond et al., 2021; Lai and Tan, 2019). However, compared to those tasks, skill tagging might be more cognitively different. Thus, future research could ask experts to participate in these tasks and observe how the results and interactions will change.\n\nSecond, we only have a small sample(N=22) and do not know the participants’ level of math knowledge. There could be a selection bias since we sent out our recruitment on several STEM classes at one public university. Meanwhile, as mentioned above, people’s confidence will impact the interaction between humans and machines. The absence of participants’ knowledge levels also limited us from exploring and explaining their choice, though since they are undergraduates we can expect an above K-12 level of math knowledge.\n\nThird, questions may have more than one skill associated with them, but our algorithms always provided three recommendations without confidence probability. For some questions, the recommendations may not have a strong relationship but may still be listed as the third recommendation. Thus, in the future, the design could be expanded in other aspects, such as providing recommendations based on different accuracy levels or providing extra information, such as confidence levels. This may help us understand in what kinds of circumstances will humans challenge the recommendations from AI.\n\n7. Conclusion\n\nThis study presents an experiment exploring AI+human collaboration and differences between humans with and without AI assistance in skill tagging tasks. The results show that taggers with AI assistance save almost 50% time (p <<<<<< 0.01) compared to without AI, but sacrifice 7.7% recall (p = 0.267) and 35% accuracy (p= 0.1170). We observed average effects, where AI+human’s achievements are between humans only and AI only, particularly in terms of time and accuracy. We also observed that participants in the AI condition were highly influenced by the recommendations but did not follow them blindly. The recommendations led to skill choices that were significantly different from the Human-only condition, even at the most coarse grain skill level of analysis.\n\nAt present, the educational field is rapidly embracing AI, particularly in this new age of Large Language Models like ChatGPT. While there are examples of industries adopting Human-AI collaboration approaches in the service of efficiencies and at the expense of quality, this is not an appealing trade-off given the values of public educational institutions. Human-AI collaboration may be a feasible strategy based on financial considerations and workload reduction, particularly in tasks that would not be possible to scale without AI; however, it is still common at this stage for AI to be less accurate than humans, particularly in conceptually sophisticated tasks. When AI is not yet at a sufficient level of accuracy relative to the human expert, our finding suggests that collaboration can result in a lower quality outcome than the human expert alone. Researchers should therefore continue to rigorously measure and monitor at what point and for which tasks AI collaborations produce a net educational benefit before they are widely adopted.\n\nReferences\n\n(1)\n\nAbdullah et al. (2020) Rana Abdullah, Bahjat Fakieh, et al. 2020. Health care employees’ perceptions of the use of artificial intelligence applications: survey study. Journal of medical Internet research 22, 5 (2020), e17620.\n\nAkgun and Greenhow (2021) Selin Akgun and Christine Greenhow. 2021. Artificial intelligence in education: Addressing ethical challenges in K-12 settings. AI and Ethics (2021), 1–10.\n\nAnderson et al. (1995) John R Anderson, Albert T Corbett, Kenneth R Koedinger, and Ray Pelletier. 1995. Cognitive tutors: Lessons learned. The journal of the learning sciences 4, 2 (1995), 167–207.\n\nAshktorab et al. (2021) Zahra Ashktorab, Michael Desmond, Josh Andres, Michael Muller, Narendra Nath Joshi, Michelle Brachman, Aabhas Sharma, Kristina Brimijoin, Qian Pan, Christine T Wolf, et al. 2021. AI-Assisted Human Labeling: Batching for Efficiency without Overreliance. Proceedings of the ACM on Human-Computer Interaction 5, CSCW1 (2021), 1–27.\n\nChen et al. (2020) Lijia Chen, Pingping Chen, and Zhijian Lin. 2020. Artificial Intelligence in Education: A Review. IEEE Access 8 (2020), 75264–75278. https://doi.org/10.1109/ACCESS.2020.2988510\n\nChoi et al. (2016) Namyoun Choi, Il-Yeol Song, and Yongjun Zhu. 2016. A model-based method for information alignment: A case study on educational standards. Journal of Computing Science and Engineering 10, 3 (2016), 85–94.\n\nChong et al. (2022) Leah Chong, Guanglu Zhang, Kosa Goucher-Lambert, Kenneth Kotovsky, and Jonathan Cagan. 2022. Human confidence in artificial intelligence and in themselves: The evolution and impact of confidence on adoption of AI advice. Computers in Human Behavior 127 (2022), 107018.\n\nClark et al. (2018) Elizabeth Clark, Anne Spencer Ross, Chenhao Tan, Yangfeng Ji, and Noah A Smith. 2018. Creative writing with a machine in the loop: Case studies on slogans and stories. In 23rd International Conference on Intelligent User Interfaces. 329–340.\n\nDemszky and Liu (2023) Dorottya Demszky and Jing Liu. 2023. M-Powering Teachers: Natural Language Processing Powered Feedback Improves 1: 1 Instruction and Student Outcomes. (2023).\n\nDesmond et al. (2021) Michael Desmond, Michael Muller, Zahra Ashktorab, Casey Dugan, Evelyn Duesterwald, Kristina Brimijoin, Catherine Finegan-Dollak, Michelle Brachman, Aabhas Sharma, Narendra Nath Joshi, et al. 2021. Increasing the Speed and Accuracy of Data Labeling Through an AI Assisted Interface. In 26th International Conference on Intelligent User Interfaces. 392–401.\n\nDietvorst et al. (2015) Berkeley J Dietvorst, Joseph P Simmons, and Cade Massey. 2015. Algorithm aversion: people erroneously avoid algorithms after seeing them err. Journal of Experimental Psychology: General 144, 1 (2015), 114.\n\nDong et al. (2017) Fei Dong, Yue Zhang, and Jie Yang. 2017. Attention-based recurrent convolutional neural network for automatic essay scoring. In Proceedings of the 21st conference on computational natural language learning (CoNLL 2017). 153–162.\n\nFoundation (2020) The Century Foundation. 2020. Closing America’s Education Funding Gaps. Technical Report. https://tcf.org/content/report/closing-americas-education-funding/\n\nGao et al. (2016) Yang Gao, Oscar Beijbom, Ning Zhang, and Trevor Darrell. 2016. Compact bilinear pooling. In Proceedings of the IEEE conference on computer vision and pattern recognition. 317–326.\n\nHolstein et al. (2018) Kenneth Holstein, Bruce M McLaren, and Vincent Aleven. 2018. Student learning benefits of a mixed-reality teacher awareness tool in AI-enhanced classrooms. In Artificial Intelligence in Education: 19th International Conference, AIED 2018, London, UK, June 27–30, 2018, Proceedings, Part I 19. Springer, 154–168.\n\nJarrahi (2018) Mohammad Hossein Jarrahi. 2018. Artificial intelligence and the future of work: Human-AI symbiosis in organizational decision making. Business horizons 61, 4 (2018), 577–586.\n\nJiang and Pardos (2021) Weijie Jiang and Zachary A Pardos. 2021. Towards equity and algorithmic fairness in student grade prediction. In Proceedings of the 2021 AAAI/ACM Conference on AI, Ethics, and Society. 608–617.\n\nKe and Ng (2019) Zixuan Ke and Vincent Ng. 2019. Automated Essay Scoring: A Survey of the State of the Art.. In IJCAI, Vol. 19. 6300–6308.\n\nLai and Tan (2019) Vivian Lai and Chenhao Tan. 2019. On human predictions with explanations and predictions of machine learning models: A case study on deception detection. In Proceedings of the conference on fairness, accountability, and transparency. 29–38.\n\nLi et al. (2024) Zhi Li, Zachary A Pardos, and Cheng Ren. 2024. Aligning Open Educational Resources to New Taxonomies: How AI technologies can help and in which scenarios. Computers & Education (2024).\n\nLi et al. (2021) Zhi Li, Cheng Ren, Xianyou Li, and Zachary A Pardos. 2021. Learning Skill Equivalencies Across Platform Taxonomies. In LAK21: 11th International Learning Analytics and Knowledge Conference. 354–363.\n\nLiu et al. (2019) Xiaoxuan Liu, Livia Faes, Aditya U Kale, Siegfried K Wagner, Dun Jack Fu, Alice Bruynseels, Thushika Mahendiran, Gabriella Moraes, Mohith Shamdas, Christoph Kern, et al. 2019. A comparison of deep learning performance against health-care professionals in detecting diseases from medical imaging: a systematic review and meta-analysis. The lancet digital health 1, 6 (2019), e271–e297.\n\nPardos and Bhandari (2023) Zachary A Pardos and Shreya Bhandari. 2023. Learning gain differences between ChatGPT and human tutor generated algebra hints. arXiv preprint arXiv:2302.06871 (2023).\n\nPardos et al. (2019) Zachary A Pardos, Zihao Fan, and Weijie Jiang. 2019. Connectionist recommendation in the wild: on the utility and scrutability of neural networks for personalized course guidance. User modeling and user-adapted interaction 29, 2 (2019), 487–525.\n\nReimers and Gurevych (2019) Nils Reimers and Iryna Gurevych. 2019. Sentence-bert: Sentence embeddings using siamese bert-networks. arXiv preprint arXiv:1908.10084 (2019).\n\nRivers et al. (2016) Kelly Rivers, Erik Harpstead, and Kenneth R Koedinger. 2016. Learning curve analysis for programming: Which concepts do students struggle with?. In ICER, Vol. 16. ACM, 143–151.\n\nShen et al. (2021) Jia Tracy Shen, Michiharu Yamashita, Ethan Prihar, Neil Heffernan, Xintao Wu, Sean McGrew, and Dongwon Lee. 2021. Classifying math knowledge components via task-adaptive pre-trained BERT. In Artificial Intelligence in Education: 22nd International Conference, AIED 2021, Utrecht, The Netherlands, June 14–18, 2021, Proceedings, Part I 22. Springer, 408–419.\n\nSiau and Wang (2018) Keng Siau and Weiyu Wang. 2018. Building trust in artificial intelligence, machine learning, and robotics. Cutter business technology journal 31, 2 (2018), 47–53.\n\nTan and Le (2019) Mingxing Tan and Quoc Le. 2019. Efficientnet: Rethinking model scaling for convolutional neural networks. In International conference on machine learning. PMLR, 6105–6114.\n\nWang and Demszky (2023) Rose E Wang and Dorottya Demszky. 2023. Is ChatGPT a Good Teacher Coach? Measuring Zero-Shot Performance For Scoring and Providing Actionable Insights on Classroom Instruction. arXiv preprint arXiv:2306.03090 (2023).\n\nWeber et al. (2020) Thomas Weber, Heinrich Hußmann, Zhiwei Han, Stefan Matthes, and Yuanting Liu. 2020. Draw with me: Human-in-the-loop for image restoration. In Proceedings of the 25th International Conference on Intelligent User Interfaces. 243–253.\n\nWeisz et al. (2022) Justin D Weisz, Michael Muller, Steven I Ross, Fernando Martinez, Stephanie Houde, Mayank Agarwal, Kartik Talamadupula, and John T Richards. 2022. Better together? an evaluation of ai-supported code translation. In 27th International Conference on Intelligent User Interfaces. 369–391.\n\nWeitekamp et al. (2020) Daniel Weitekamp, Erik Harpstead, and Ken R Koedinger. 2020. An interaction design for machine teaching to develop AI tutors. In Proceedings of the 2020 CHI conference on human factors in computing systems. 1–11.\n\nYilmazel et al. (2007) Ozgur Yilmazel, Niranjan Balasubramanian, Sarah C Harwell, Jennifer Bailey, Anne R Diekema, and Elizabeth D Liddy. 2007. Text categorization for aligning educational standards. In 2007 40th Annual Hawaii International Conference on System Sciences. IEEE, 73–73.\n\nYin et al. (2019) Ming Yin, Jennifer Wortman Vaughan, and Hanna Wallach. 2019. Understanding the effect of accuracy on trust in machine learning models. In Proceedings of the 2019 chi conference on human factors in computing systems. 1–12.\n\nZhang et al. (2020) Yunfeng Zhang, Q Vera Liao, and Rachel KE Bellamy. 2020. Effect of confidence and explanation on accuracy and trust calibration in AI-assisted decision making. In Proceedings of the 2020 Conference on Fairness, Accountability, and Transparency. 295–305."
    }
}
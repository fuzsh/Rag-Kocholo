{
    "id": "dbpedia_2222_3",
    "rank": 60,
    "data": {
        "url": "https://arxiv.org/html/2401.09051v1",
        "read_more_link": "",
        "language": "en",
        "title": "Canvil: Designerly Adaptation for LLM",
        "top_image": "",
        "meta_img": "",
        "images": [
            "https://arxiv.org/html/extracted/5352340/canvil-overview.png",
            "https://arxiv.org/html/extracted/5352340/canvil-menus.png",
            "https://arxiv.org/html/extracted/5352340/board-setup.png",
            "https://arxiv.org/html/x1.png"
        ],
        "movies": [],
        "keywords": [],
        "meta_keywords": [
            "large language models",
            "user experience",
            "design practice"
        ],
        "tags": null,
        "authors": [],
        "publish_date": null,
        "summary": "",
        "meta_description": "",
        "meta_lang": "en",
        "meta_favicon": "",
        "meta_site_name": "",
        "canonical_link": null,
        "text": "License: CC BY 4.0\n\narXiv:2401.09051v1 [cs.HC] 17 Jan 2024\n\nCanvil: Designerly Adaptation for LLM-Powered User Experiences\n\nK. J. Kevin Feng , Q. Vera Liao , Ziang Xiao , Jennifer Wortman Vaughan , Amy X. Zhang and David W. McDonald\n\nAbstract.\n\nAdvancements in large language models (LLMs) are poised to spark a proliferation of LLM-powered user experiences. In product teams, designers are often tasked with crafting user experiences that align with user needs. To involve designers and leverage their user-centered perspectives to create effective and responsible LLM-powered products, we introduce the practice of designerly adaptation for engaging with LLMs as an adaptable design material. We first identify key characteristics of designerly adaptation through a formative study with designers experienced in designing for LLM-powered products (N=12ùëÅ12N=12italic_N = 12). These characteristics are to 1) have a low technical barrier to entry, 2) leverage designers‚Äô unique perspectives bridging users and technology, and 3) encourage model tinkering. Based on this characterization, we build Canvil, a Figma widget that operationalizes designerly adaptation. Canvil supports structured authoring of system prompts to adapt LLM behavior, testing of adapted models on diverse user inputs, and integration of model outputs into interface designs. We use Canvil as a technology probe in a group-based design study (6 groups, N=17ùëÅ17N=17italic_N = 17) to investigate the implications of integrating designerly adaptation into design workflows. We find that designers are able to iteratively tinker with different adaptation approaches and reason about interface affordances to enhance end-user interaction with LLMs. Furthermore, designers identified promising collaborative workflows for designerly adaptation. Our work opens new avenues for collaborative processes and tools that foreground designers‚Äô user-centered expertise in the crafting and deployment of LLM-powered user experiences.\n\nlarge language models, user experience, design practice\n\n‚Ä†‚Ä†copyright: acmcopyright‚Ä†‚Ä†journalyear: 2024‚Ä†‚Ä†doi: xxxxxxx.xxxxxxx‚Ä†‚Ä†conference: arXiv; January; 2024‚Ä†‚Ä†price: 15.00‚Ä†‚Ä†isbn: xx-x-xxx-xxxx-xxxxx‚Ä†‚Ä†ccs: Human-centered computing Interaction design process and methods‚Ä†‚Ä†ccs: Human-centered computing Empirical studies in collaborative and social computing\n\n1. Introduction\n\nA paradigm shift is underway for integrating artificial intelligence (AI) capabilities into everyday user-facing technologies. Large pre-trained AI models, most notably large language models (LLMs), have versatile natural language capabilities that unlock novel interactive techniques and interfaces for more intuitive and customizable user experiences across a wide spectrum of applications (Suh et al., 2023; Angert et al., 2023; Wu et al., 2022b; Parnin et al., 2023). However, these promises also come with numerous concerns. Integrating LLMs into a domain without careful consideration of the user contexts surrounding model use and implementation of behavioral guardrails for the model may result in user experiences that perpetuate societal biases (Santurkar et al., 2023), threaten users‚Äô sense of well-being (Shah, 2023; Roose, 2023), or otherwise do harm (Selbst et al., 2019; Do et al., 2023).\n\nAs technology development practitioners, designers are uniquely positioned to mitigate these concerns (Feng and McDonald, 2023; Subramonyam et al., 2021a; Liao et al., 2023; Yang, 2018; Yildirim et al., 2022). Designers‚Äô work often involves aligning technological capabilities with user needs, such that the technology addresses (or makes progress towards addressing) pain points identified in user research (Feng and McDonald, 2023; Subramonyam et al., 2022). Designers are trained in human-centered design methods that allow them to understand users and usage contexts, prototype potential solutions with relevant technology, and iterate on those solutions based on their understanding of users or user feedback (Ball, 2005). Yet, prior work has shown that designers face diverse challenges working with AI (Yang et al., 2020; Subramonyam et al., 2021b). These challenges include procedural ones, such as difficulties collaborating with the engineering teams training the models (Subramonyam et al., 2022, 2021b), as well as instrumental ones, such as lacking means to effectively work with the models (Feng and McDonald, 2023; Yang, 2018; Subramonyam et al., 2021a). In efforts to address this, researchers have situated AI as a design material to highlight exploring material considerations‚Äîe.g., the technology‚Äôs capabilities, limitations, and adaptability (Leonardi, 2012)‚Äîfor designers to better understand and apply AI in the context of their design problems (Yang, 2018; Dove et al., 2017; Feng et al., 2023b). A good ‚Äúdesignerly understanding‚Äù of AI can help designers ideate on new AI-powered design ideas, mitigate AI‚Äôs varying impact for different user scenarios, collaborate with design and non-design practitioners, and reinforce user-centered perspectives amongst the team and throughout the product development cycle (Liao et al., 2023; Wang et al., 2023).\n\nThe advent of LLMs introduces new opportunities for engaging with AI as a design material. First, adaptability emerges as a key materialistic property of LLMs. LLMs are responsive to adaptation via fine-tuning (Dodge et al., 2020; Hu et al., 2021; Ouyang et al., 2022b; Bai et al., 2022) and prompt-based methods (OpenAI, 2023c; Wu et al., 2022a; OpenAI, 2023b). In fact, due to resource-intensive training of LLMs, it has become a standard practice for individual developers and development teams to adapt ‚Äúbase‚Äù LLMs from a small handful of providers (e.g., Anthropic, Google, OpenAI) (Parnin et al., 2023) for improved performance in domain-specific tasks. Moreover, natural language interaction and adaptation democratizes AI experimentation for practitioners‚Äîincluding designers‚Äîtraditionally excluded from AI conversations due to limitations in technical expertise (Liao et al., 2023; Petridis et al., 2023b). Despite this, there has been limited exploration of designers‚Äô interaction with‚Äîlet alone adaptation of‚ÄîLLMs in practice, with untapped opportunities for designers to contribute to LLM-powered product development.\n\nIn this paper, we introduce the practice of designerly adaptation of LLMs (henceforth ‚Äúdesignerly adaptation‚Äù) to showcase new opportunities for designers to harness LLMs as an adaptable design material. We first characterize this new practice through a formative interview study with 12 designers experienced in designing for LLM-powered products and features. Our study reveals that desirable characteristics of designerly adaptation include execution via natural language prompt-based methods, leveraging designers‚Äô user-centered perspectives and expertise, and encouraging iterative tinkering with models.\n\nWhile these desiderata align with those surfaced in prior work (Feng and McDonald, 2023; Subramonyam et al., 2022; Yang et al., 2020), the practical significance of satisfying them in design workflows remains unclear. To investigate, we build Canvil, a technology probe in the form of a Figma widget that operationalizes designerly adaptation. Canvil enables designers to adapt LLMs within their Figma canvases via system prompting (Microsoft, 2023; OpenAI, 2023c) and integrate outputs from adapted models into their designs. We use Canvil in a task-based design study with 6 groups of 17 designers total to understand whether and how designerly adaptation can assist in crafting LLM-powered user experiences. We find that through designerly adaptation, designers steered LLM behavior with user needs, derived interface affordances to enhance user interaction with LLMs, and recognized promises in collaborative adaptation to share resources and knowledge with design and non-design stakeholders. They were optimistic about integrating designerly adaptation into their own workflows and noted procedural and ethical questions to address in practice.\n\nEquipped with findings from both studies, we propose a concrete workflow for designerly adaptation to offer a tangible launchpad for researchers and practitioners to further explore, critique, and iterate on this practice. We end by underscoring the potential of tools for collaborative AI tinkering, while reflecting on materiality‚Äôs impact on social practices within product teams.\n\nConcretely, our work makes the following contributions:\n\n‚Ä¢\n\nInsights from a formative study with designers experienced in crafting LLM-powered user experiences, from which we assemble a characterization of designerly adaptation‚Äîa new practice by which designers engage with LLMs as an adaptable design material.\n\n‚Ä¢\n\nCanvil, a technology probe in the form of a Figma widget that operationalizes designerly adaptation.\n\n‚Ä¢\n\nInsights from an empirical exploration of how designers engage in designerly adaptation in a task-based design study.\n\n‚Ä¢\n\nA discussion of our work‚Äôs implications on (collaborative) design and beyond, including a proposed workflow for designerly adaptation to orient future work.\n\n2. Related Work\n\nTo situate our work, we review prior literature in AI as a design material, approaches to LLM adaptation, and tools for interactively working with AI models.\n\n2.1. AI as a Design Material\n\nRobles and Wiberg argue that recent advances in computational technologies bring about a ‚Äúmaterial turn‚Äù‚Äîa transformation within interaction design that allows for the shared use of material metaphors (e.g., flexibility) across physical and digital worlds (Robles and Wiberg, 2010). Indeed, prior works have discussed AI as a design material (Dove et al., 2017; Yang, 2018; Yang et al., 2020; Benjamin et al., 2021; Luciani et al., 2018; Liao et al., 2023; Feng and McDonald, 2023; Feng et al., 2023c; Yildirim et al., 2022), and highlighted why AI‚Äôs materiality can make it uniquely difficult to design with (Yang et al., 2020; Subramonyam et al., 2021b; Liao et al., 2023; Dove et al., 2017; Benjamin et al., 2021). AI is often treated as a black box to non-technical stakeholders such as designers, making it challenging to tune user interactions to often unpredictable and complex model behavior (Benjamin et al., 2021; Yang, 2018). In addition, AI‚Äôs technical abstractions are often divorced from concepts designers are familiar with (Subramonyam et al., 2021b), and designers consequently struggle with creatively using or manipulating the material to generate design solutions (Liao et al., 2023; Feng and McDonald, 2023). AI models are also non-deterministic and fluid in nature‚Äîthey may evolve with new data or user input, and can be intentionally steered towards desirable behaviors with choices of data, algorithm, parameter, and so on (Yang et al., 2020). Without a concrete material understanding to begin with, designers are unable to grasp the nature of these uncertainties (Yang, 2018; Subramonyam et al., 2021a) or the opportunities to shape the design materials for desirable UX (Liao et al., 2023). Yang et al. (Yang et al., 2020) showed that these challenges of working with AI as a design material persist through the entire double-diamond design process‚Äîfrom identifying the right user problem to be solved by AI to designing the right UX to solve the user problem.\n\nResearchers and practitioners have developed processes and tools to alleviate some of these challenges. ProtoAI (Subramonyam et al., 2021a) combines exploration of models with UI prototyping, while advocating for designers‚Äô active shaping of the AI design material (e.g., choosing models and setting parameters) by user needs. Feng et al. (Feng and McDonald, 2023) found that hands-on ‚Äúfabrication‚Äù of the design material through a UI-based model training tool bolstered understanding and connection between AI properties and UX goals. fAIlurenotes (Moore et al., 2023) is a failure analysis tool for computer vision models to support designers in understanding AI limitations across user groups and scenarios. Other efforts include process models (Subramonyam et al., 2021b) and ‚Äúleaky abstractions‚Äù (Subramonyam et al., 2022) that facilitate collaboration between designers and model developers, and human-AI design guidelines (Apple, 2024; Google, 2024; Microsoft, 2024).\n\nRecent advances in LLMs simultaneously alleviate and exacerbate some of the aforementioned challenges. The barrier to tinkering with AI has significantly lowered thanks to the use of natural language as a primary mode of interaction and easily accessible tools such as ChatGPT. Concerns, however, have also arisen over the lack of explainability and transparency in LLMs due to their complex technical architectures (Liao and Vaughan, 2023). Yet, because of LLMs‚Äô powerful capabilities, there is significant interest in exploring their integration into user-facing technologies (Pichai, 2023; Stallbaumer, 2023; Parnin et al., 2023); as such, designers should be prepared to work with them as a design material (Kulkarni et al., 2023). Despite this, we have yet to understand designers‚Äô current approaches and desiderata when crafting LLM-powered user experiences. Our work contributes both empirical knowledge and tooling via a technology probe to this space.\n\n2.2. Adaptation of Large Language Models\n\nA fundamental property of LLMs not present in their smaller predecessors is the ease with which their behavior can be adapted (Brown et al., 2020; OpenAI, 2023b). While the breadth of LLMs‚Äô out-of-the-box capabilities may seem impressive, researchers have recognized the importance of adapting LLMs for enhanced performance under specific domains and tasks (Dodge et al., 2020; Hu et al., 2021; Wang et al., 2023; Wu et al., 2022b, a), and aligning model behavior with human preferences and values (Ouyang et al., 2022b; Bai et al., 2022; Feng et al., 2023a). Adaptation has thus been a topic of interest to both the AI and HCI communities (Lee et al., 2022).\n\nAdaptation may take on many forms. A pre-trained model may undergo fine-tuning, a process by which additional layer(s) of the neural network are trained on a task-specific dataset (Dodge et al., 2020). Popular approaches to fine-tuning include instruction tuning (Wei et al., 2021; Ouyang et al., 2022b), reinforcement learning with human feedback (RLHF) (Ouyang et al., 2022b; Christiano et al., 2017), and direct preference optimization (DPO) (Rafailov et al., 2023). More computationally efficient variants, such as low-rank adaptation (LoRA) (Hu et al., 2021), have also garnered attention.\n\nAdaptation can also occur without modifying the model itself, through system prompting. Different from one-off prompting, system prompting applies to all individual user inputs for how the model should behave (e.g., ‚Äúalways respond in a concise manner‚Äù), often targeting an application domain. OpenAI‚Äôs API provides a ‚ÄúSystem‚Äù field to specify system prompts (OpenAI, 2023a, c), which have been shown to be effective at modifying model behaviors (OpenAI, 2023b; Salewski et al., 2023; Deshpande et al., 2023). For instance, a line of work explored instructing the model to behave with a certain ‚Äúpersona‚Äù to elicit desirable or adversarial model behaviors (Wei et al., 2023; Cheng et al., 2023; Salewski et al., 2023; Wolf et al., 2023; Deshpande et al., 2023). However, system prompts can be challenging to author (Zamfirescu-Pereira et al., 2023), and there is not yet an established ‚Äúgold standard‚Äù for prompt writing, system or otherwise. Researchers and practitioners have attempted to derive useful prompting formats based on empirical exploration; these include in-context learning (i.e., by providing desired input-output examples) (Brown et al., 2020; Wu et al., 2023; Zhao et al., 2021), chain-of-thought reasoning (Wei et al., 2022), and instruction-following (Kaddour et al., 2023; Ouyang et al., 2022a). Industry recommendations have also emerged for system prompts, encouraging the specification of elements such as context specification (e.g., ‚ÄúYou are Yoda from Star Wars‚Äù), task definition (e.g., ‚ÄúYou respond to every user input as Yoda and assume the user is a Padawan‚Äù), and safety guardrails (e.g., ‚ÄúIf the user requests inappropriate or offensive responses, you must respectfully decline with a wise Yoda saying‚Äù) (Microsoft, 2023).\n\nIn our work, we examine and support designerly adaptation of LLMs‚Äîwhether and how designers can contribute to model adaptation for user-facing products through system prompting. We develop a characterization of the term through our formative study, and then build a technology probe to operationalize it in design practice. In building our probe, we draw from recommended best practices for system prompting, but acknowledge that these practices may shift over time and that our work reflects just one possible instantiation of these practices.\n\n2.3. Interactive Tools for Steering AI Behavior\n\nLiterature at the intersection of HCI and AI has introduced a wide range of interactive techniques and tools to aid humans in training and adapting AI models, ranging from ones supporting data scientists to perform data wrangling (G√∂rtler et al., 2022; Wang et al., 2022; Kandel et al., 2011; Liu et al., 2020; Wongsuphasawat et al., 2015), model training and evaluation (B√§uerle et al., 2022; Cabrera et al., 2023; Amershi et al., 2015; Ren et al., 2016; Microsoft, 2024; Google, 2024; IBM, 2024), managing model iterations (Hohman et al., 2020; Robertson et al., 2023), and so on, to those allowing for ‚Äúhuman-in-the-loop‚Äù paradigms at various stages of the model development pipeline, including data annotation (Smith et al., 2018; Settles, 2012; Chao et al., 2010; Monarch, 2021; Raghavan et al., 2006), output correction (Branson et al., 2010; Bod√©n et al., 2021), integration testing (Chen et al., 2022), and explainability (Lage et al., 2018; Qian et al., 2019; Nguyen and Choo, 2021).\n\nThere has also been a long-standing interest in ‚Äúdemocratizing AI‚Äù for domain experts or practitioners without formal technical training to steer model behaviors. Interactive machine learning (iML) (Fails and Olsen, 2003) is a field responding to this interest by advocating for interactive and incremental model steering through intuitive interfaces and tightly coupled input-evaluation feedback loops. For instance, interfaces for transfer learning (Mishra and Rzeszotarski, 2021) have enabled non-expert users to transfer learned representations from a larger model to a domain-specific task. Tools have also encouraged non-expert exploratory tinkering of AI models through visual drag-and-drop UIs (Carney et al., 2020; Liner.ai, 2022; Lobe.ai, 2021; Du et al., 2023). Teachable Machine (Carney et al., 2020) is one such tool that allows users to train models for image, video, and audio classification. Rapsai (Du et al., 2023) is a visual programming pipeline for rapidly prototyping AI-powered multimedia experiences such as video editors. In an era where formal knowledge about AI is limited to a small handful of technical experts, these tools also serve to demystify AI for everyday users.\n\nWith the onset of language models, barriers to experimenting with AI have lowered even further, paving the path for a new generation of interactive AI tools. Sandbox environments such as OpenAI‚Äôs playground (OpenAI, 2024) require no prerequisites besides a grasp of natural language to prompt the model and steer model behavior. However, relying on unstructured natural language alone can be daunting and ineffective. Many tools have risen to the challenge to support prompt engineering with more structured tinkering. Prompt chaining is one such approach (Wu et al., 2022b, a; Arawjo et al., 2023; Suh et al., 2023), by which users can use a node-based visual editor to write prompts for simple subtasks and assemble them to solve a larger, more complex task. PromptMaker (Jiang et al., 2022) and MakerSuite (Huffman and Woodward, 2023) allow the user to rapidly explore variable-infused prompts and few-shot prompting, while ScatterShot (Wu et al., 2023) helps specifically with curating few-shot prompting examples.\n\nDespite advancements in tooling, support for designers to work with LLMs as an adaptable design material remains limited. Domain-agnostic tools for tinkering with models may not be well-integrated into design workflows, a primary consideration for designers when deciding whether to adopt those tools (Feng et al., 2023c). Tools that offer integration with design environments (e.g., PromptInfuser (Petridis et al., 2023b, a)) support prototyping with LLMs, but not necessarily deeper adaptation of model behavior. Our work situates interactive adaptation within familiar design environments and processes.\n\n3. Formative Study\n\nWorking with AI, especially LLMs, is an emerging practice in the field of UX (Yang et al., 2020; Yildirim et al., 2023; Feng et al., 2023b; Subramonyam et al., 2021b). We wanted to understand designers‚Äô experiences working on LLM-powered products and features amidst industry-wide shifts towards LLMs (Parnin et al., 2023) as an initial step in our research. We hence conducted a formative study where we interviewed designers about their awareness of, involvement in, and desiderata around designing LLM-powered user experiences.\n\n3.1. Method and Participants\n\nWe conducted 30-minute 1:1 virtual interviews with 12 designers at a large international technology company, where LLM-powered features and products are actively explored. All interviews were conducted in June and July 2023. At the time of the study, all participants were working on products or services that leveraged LLMs in some capacity; LLM application areas spanned conversational search, domain-specific data question-answering (QA), recommendation, text editing and generation, and creativity support tools. Participant details can be found in Table 1 of Appendix A.\n\nOur interviews were semi-structured and revolved around the following topics:\n\n‚Ä¢\n\nAwareness: to what extent are designers aware of LLMs‚Äô capabilities, limitations, and specifications in the context of their product(s)?\n\n‚Ä¢\n\nInvolvement: to what extent are designers involved in discussions or activities that shape where and how an LLM is used in their product(s)?\n\n‚Ä¢\n\nDesiderata: what do designers desire when crafting LLM-powered user experiences, with regards to both processes and tools?\n\nEach participant received a $25 USD gift card for their participation. All interviews were recorded and transcribed. Our study was reviewed and approved by the company‚Äôs internal IRB.\n\nThe first author performed an inductive qualitative analysis of the interview data. This process started with an open coding round in which initial codes were generated, followed by two subsequent rounds of axial coding in which codes were synthesized and merged into higher-level themes. The codes and themes were discussed with research team members at weekly meetings. Other research team members also offered supporting and contrasting perspectives on the codes by writing their own analytic memos, which were also discussed as a team.\n\n3.2. Findings\n\nWe present summaries of our major themes as findings below. While our research did not focus specifically on model adaptation to start, we saw adaptation emerge as a connecting thread across many of our participants‚Äô experiences; as such, we present our findings through the lens of adaptation. Additionally, we use our findings to motivate and define designerly adaptation, culminating in a concrete characterization of the term in Section 3.3.\n\n3.2.1. Adaptation was seen as a central materialistic property of LLMs\n\nDesigners recognized that products delivering compelling, robust user experiences were not powered by out-of-the-box ‚Äúbase‚Äù LLMs, but required adaptation and, in some cases, grounding in other sources. P5 gave an example where an out-of-the-box model inappropriately prompted the user for a riddle in a workplace setting: ‚Äú[The experience] is not quite right, cause it‚Äôs in [workplace software] and it‚Äôs like, tell me a riddle. I‚Äôm at work!‚Äù Those working in the domain of conversational search knew the model had to be connected to a search engine to ensure information relevance and reliability, so that they are ‚Äúnot just giving [the user] the chatbot‚Äù [P9].\n\nThe ability to steer an LLM with natural language enabled designers to envision more flexible and adaptable AI-powered user experiences than working with traditional AI models. Some were familiar with how lightweight adaptation techniques, such as writing system prompts, can be used to specify model behaviors based on desirable UX. P12 discussed an example of how this may work in an entertainment system setting:\n\n‚ÄúYou might want to generate enthusiasm more, right? So the LLM that goes over there might have a UX layer that feels different. It should. I hope it does. The system prompt can say, remember, you‚Äôre a machine that wants to get users enthused.‚Äù [P12]\n\nParticipants recognized that the adaptability of LLMs allows their team to more easily create multiple versions of LLM-powered features in one product. P2 shared that they were already seeing multiple versions of the same model customizing experiences within their product: ‚ÄúThere are actually 3 incarnations of the same model, each with slightly different parameters [‚Ä¶] based on the prompt that users give, we select which of those 3 incarnations we wanna use.‚Äù Participants also believed adapting the model‚Äôs ability or behaviors for different user groups or contexts can help create more equitable experiences. For example, P4 and P7 both stated a design goal for their products (in conversational search and domain-specific question-answering, respectively) is to accommodate non-technical users, which can be achieved if the model can ‚Äútailor the language depending on technical ability, and gradually introduce [users] to more complicated concepts and terminology.‚Äù [P7]. Adaptation thus not only allows user experiences to more flexibly accommodate existing user groups, but also to expand to serving new ones.\n\n3.2.2. Designers were often not involved with adaptation, but desired more involvement.\n\nEven though designers found adaptation inextricably linked to UX considerations, they were typically not involved in adaptation. Some were unsure of how or by whom adaptation is performed and suspected that it was engineers: ‚ÄúI would say [adaptation]‚Äôs probably something that was decided by the engineering team [‚Ä¶] prompt engineering is more so on the [technical] side of things.‚Äù [P2]. Many knew for a fact that it was engineers and data scientists who adapted the models and relied on them for information about model behavior. This often got cumbersome quickly. P11 explained that in their product, designers set character limits for LLM-generated summaries in the UI, but were unable to directly specify and experiment with the model‚Äôs output length. Instead, P11 relied on engineers to adapt on their behalf: ‚ÄúI was asking what the difference in summary looks like with 50, 100, and 150 characters. Then [the engineers] would go back and test it and then they would just share the results, like here‚Äôs what 50 characters looks like. Here‚Äôs what 100 looks like.‚Äù P12 experienced a similar procedure and stated that the feedback loop can take as long as a couple weeks. Ideally, they wanted the experimentation to happen ‚Äúin real time.‚Äù\n\nThe combination of 1) adaptation and UX being closely intertwined, and 2) designers‚Äô exclusion from the adaptation process, led to a sentiment echoed by the far majority of our participants: designers should have an active role in model adaptation. Indeed, designers are uniquely positioned to contribute to adaptation through their user-centered lens. For example, P4 shared that they naturally gravitate to persona-based reasoning in their design process: ‚ÄúHow would I use this? How would my mom use this? Taking on [the perspective of] different personas made sense to me.‚Äù This type of reasoning is critical for adapting user experiences to accommodate diverse user needs and usage contexts. Indeed, P10 caught their model‚Äôs inability to properly recognize some acronyms users of their product might come across and devised a solution with their team: ‚ÄúI found that [the model] doesn‚Äôt know what to do with the acronyms so we floated the idea of having glossary of industry jargon and acronyms [in the system prompt].‚Äù\n\nBecause of this, many participants were advocates for designers performing adaptation, and considered UX goals as indispensable in this process. P4 shares: ‚ÄúWho better to involve in this process than people whose job it is to think deeply about [the UX]? At the very least, system prompts shouldn‚Äôt be written without an understanding of what the end UX goals are.‚Äù P9 agreed, saying that designers can offer strategic contributions with user-centered thinking: ‚ÄúIt‚Äôs super important for designers to think through ideas and make it intuitive for users and help come up with compelling [user] scenarios [‚Ä¶] I think design has a bigger opportunity to have a seat at the table strategy-wise.‚Äù On a higher level, P12 pointed out that model adaptation can be a contemporary extension of efforts around crafting product voice and tone, which designers are already familiar with: ‚ÄúUX designers and content designers are very attuned to and have pretty much owned the story around voice and tone of products, and have for years and years.‚Äù\n\nWe see a broad consensus among participants that there is both demand and opportunity for designers to engage in adaptation. We therefore propose that designerly adaptation should leverage designers‚Äô existing expertise and workflows to enhance model behavior in user-centered ways.\n\n3.2.3. Designers lack support to tinker with and adapt LLMs\n\nOverall, participants felt limited by current tools and resources to tinker with LLMs. These findings echo prior work that finds that the inability to directly access the models and experiment with their capabilities and limitations (i.e., ‚Äútinkering‚Äù) is a primary challenge in the design process for AI-powered user experiences (Yang et al., 2020; Subramonyam et al., 2022). Many participants emphasized the importance of ‚Äútinkering‚Äù to understand model capabilities and limitations to inform design ideation and interface prototyping. After some hands-on experience with the model, P2 said they could much more clearly ‚Äúsee or gauge the power of the language model,‚Äù while P9 tinkered with a model they had early access to and recalled that ‚Äúyou can identify some gaps [in capability] right off the bat.‚Äù\n\nWhile some designers welcomed the easy access to ChatGPT or tried out the GPT playground as a means to familiarize themselves with the ‚Äúbase‚Äù LLMs, participants recognized the necessity of tinkering with the adapted models. To gain access, designers either had to keep burdening the engineers or wait until a test version of the product is launched. P3 shared that ‚ÄúThe only way we would have [to tinker] is using the [product] online and play around with what [the team] did.‚Äù P10 expressed frustration at this workflow: ‚ÄúYou want to be able to play with [the model] yourself and understand what the user experience is like. And I can‚Äôt play with it.‚Äù\n\nSome designers took the initiative to seek out new tools to tinker with and adapt models‚Äîcurrently, these tools are mostly limited to either programming with APIs or playground interfaces with many technical developer settings. However, usage of those tools did not persist. P1 tried various tools within and outside of their company and did not find them to be designer-friendly: ‚Äú[The tools are] still kind of technical, a lot of [designers] don‚Äôt realize how parameters work. Like how does temperature work?‚Äù Others, such as P4, did not find the tools well-suited for their use case: ‚ÄúI haven‚Äôt seen anything that feels like it makes it really easy to make small changes to a [system] prompt and then run the same set of queries over it and see whether it makes a difference.‚Äù\n\nTaken together, we envision adaptation and tinkering must be tightly coupled for designers to work with adapted LLMs within their design process. This includes being able to easily create, test, and iterate on versions of adaptation, and having this workflow be integrated into their design environment to reduce friction and eliminate dependence on engineers. Moreover, as noted in Section 2.2, some approaches to adaptation demand technical skills designers typically do not acquire in their training, and also do not fit within the tight feedback loops of the design process due to high data and compute overheads. Thus, we consider prompt-based adaptation methods to be most ideal for designerly adaptation. This broadens participation for designers while still allowing for modification of many aspects of model behavior.\n\n3.3. Summary: A Characterization of Designerly Adaptation\n\nWe present three characteristics of designerly adaptation through several key insights drawn from our formative study‚Äôs findings. First, not all forms of adaptation can be considered as designerly adaptation. Designerly adaptation should not demand a high degree of technical expertise in AI‚Äîinstead, designers should be capable of and prepared to conduct it with their current skillset and training. One promising approach to designerly adaptation, then, is to author system prompts with natural language. Second, designerly adaptation should take advantage of designers‚Äô unique expertise and perspectives to inform model behavior. This may include incorporating user research into the system prompts. Third, tinkering should have a major role in designerly adaptation, encouraging tight feedback loops of creation, testing, and iteration that align with feedback loops in the broader design process.\n\n4. Canvil: a Technology Probe for Designerly Adaptation\n\nIn light of our formative study, we introduce Canvil, a technology probe for designerly adaptation in the form of a Figma widget. We first outline our design goals, which we ground in our formative study‚Äôs findings. We then walk through Canvil‚Äôs user interface and implementation.\n\n4.1. Design Goals\n\nOur design goals, informed primarily by insights from our formative study and supported by our literature review, guided why and how we built Canvil. They are as follows.\n\nDG1:\n\nSupport designerly adaptation via system prompting.\n\nOur formative study revealed that some designers were already experimenting with system prompting in their design workflows (Sections 3.2.1 and 3.2.2). System prompts are also an appropriate medium for architecting UX‚Äîthey shape a model‚Äôs behaviors across all or a specified group of user inputs. We also note that among the diverse techniques for adaptation (see Section 2.2), system prompting is conducted in natural language, making it accessible to designers, many of whom lack a technical background.\n\nDG2:\n\nLeverage abstractions and environments designers are already familiar with.\n\nDesigners who want to be more involved in adaptation can feel unsupported due to existing tools (e.g., calling APIs programmatically) not aligning with their established workflows and mental models. Indeed, we saw in Section 3.2.3 that designers sought out new tools for adaptation but discarded them after a while because they were too technical or did not match desired use cases. Given that designers prefer to stay in one tool of their choice (e.g., Figma) for most stages of the design process (Feng et al., 2023c), we hypothesize that tools for designerly adaptation should be tightly integrated with design tools and leverage common abstractions within those tools (e.g., layers, components, frames) to better align with designers‚Äô mental models.\n\nDG3:\n\nAllow for user research to inform model behavior.\n\nWhen an LLM is integrated into a user-facing application, it becomes part of a sociotechnical system embedded in the deployment context (Selbst et al., 2019). Designers are well-positioned to understand this context and leverage their understanding for LLM adaptation from working closely with users or user research data (Section 3.2.2). Therefore, we invite designers to consider the user and social factors surrounding the UX at hand in tandem with the technical capabilities of the model. That is, we empower designers to adapt model behavior with user research insights.\n\nDG4:\n\nProvide opportunities for collaboration.\n\nUX is a highly collaborative practice (Feng et al., 2023c; Deng et al., 2023). In our formative study, we saw that designers collaborated closely with not only other designers, but also product managers, software engineers, data scientists, and more. All these roles can contribute to adaptation. Moreover, not all desired model behaviors can be realized with prompt-based adaptation alone‚Äîfor example, connecting an LLM to a knowledge base will likely require collaboration with a developer or data scientist (Section 3.2.1). We therefore design our system to afford collaboration with diverse technical and non-technical stakeholders.\n\nDG5:\n\nEnable tightly coupled model adaptation and tinkering within the design process.\n\nDesigners found model tinkering highly valuable in our formative study as it allowed them to better understand LLMs as a design material‚Äîhow they behave, what they are capable of, and where their limitations are. This aligned with prior work on designers working with AI models more generally (Subramonyam et al., 2021a; Feng and McDonald, 2023). Yet, our formative study indicates that designers currently had limited to no involvement in adaptation, nor access to tinkering with adapted models. Even if some limited opportunities for tinkering exist, they would require designers to leave their established workflows to seize those opportunities. We thus aim to incorporate tinkering and adaptation seamlessly into the design process.\n\n4.2. Canvil as a Probe\n\nWe envision designerly adaptation as a new opportunity for designers to contribute to model adaptation efforts. To investigate whether and how this new practice may be included in existing design workflows, we designed Canvil as a technology probe.\n\nCommonly used in contextual research in HCI (J√∂rke et al., 2023; Hohman et al., 2019), a technology probe is an artifact, typically in the form of a functional prototype (Hutchinson et al., 2003), presented to the user ‚Äúnot to capture what is so much as to inspire what might be‚Äù (Lury and Wakeford, 2012). That is, probes offer one instantiation of tooling and/or interaction techniques for a domain to better understand phenomena within that domain. Hutchinson et al. (Hutchinson et al., 2003) state that technology probes have three goals: the social science goal of understanding users in a real-world context, the engineering goal of field-testing the technology, and the design goal of inspiring new technologies. We map these three goals onto our objectives with Canvil:\n\n‚Ä¢\n\nSocial science: understand how designers‚Äô engagement with model adaptation impacts their work on LLM-powered user experiences through a structured design activity in Figma.\n\n‚Ä¢\n\nEngineering: develop and launch a Figma widget that connects to OpenAI‚Äôs LLMs and allows them to be adapted via system prompting from within Figma.\n\n‚Ä¢\n\nDesign: encourage reflection on designerly adaptation as a UX practice and inform future tools to support it.\n\n4.3. User Interface\n\nThe Canvil interface resembles an interactive card. The card itself is separated into two areas: the Main Form and Playground Area. When a user selects a Canvil, a property menu is invoked that can open up additional panels for styling, response generation, and settings. Canvils can be freely placed on and moved around the Figma canvas, allowing model tinkering to take place in close proximity to relevant designs. Canvils can also interact directly with designs by reading inputs from and writing model outputs to their text layers. We detail each of Canvil‚Äôs features in this section and connect them with our design goals.\n\n4.3.1. Main Form\n\nThe Main Form provides a means of authoring a system prompt (DG1) in a structured manner via a multi-field form. Unlike prior systems that offer more or less an open text field for system prompting (OpenAI, 2023a; Petridis et al., 2023b, a), we chose to enforce structure because it provides mental scaffolding to reason about the system prompt in a UX context from multiple facets (e.g., with whom will the LLM interact, and how should the LLM meet their goals?), thus providing more opportunities for designers to integrate user context into the LLM (DG3). Additionally, designers can collaboratively author system prompts (DG4) by each working on a particular field and collectively deliberating afterwards. Breaking down the system prompt into smaller units also allows for more fine-grained experimentation (DG5)‚Äîdesigners can copy a Canvil and tweak a specific field to compare how that change impacts model behavior.\n\nBelow are the fields, with a brief description of each, in the Main Form. The field titles (in bold) are always visible on the interface, while the field descriptions can be accessed by hovering over an info icon beside each field title.\n\n‚Ä¢\n\nModel profile: High-level description of the model‚Äôs role, character, and tone.\n\n‚Ä¢\n\nAudience setting: Persona or descriptions of user(s) who will interact with the model.\n\n‚Ä¢\n\nCore instructions: Logical steps for the model to follow to accomplish its tasks. Specify input/output format where applicable.\n\n‚Ä¢\n\nGuardrails: How the model should respond in sensitive or off-topic scenarios, including any content filters.\n\n‚Ä¢\n\nExample inputs/outputs: Examples to demonstrate the intended model behavior.\n\nWe derived these fields from synthesizing system prompting guidelines offered by technical tutorials (OpenAI, 2023c; Microsoft, 2023) as well as NLP literature (Deshpande et al., 2023). We note that our fields are just one possible way to structure a system prompt, and there may be some overlaps between the fields. As discussed in Section 2.2, no ‚Äúgold standard‚Äù currently exists for prompt authoring, so we chose to follow the current recommended guidelines when designing the Main Form.\n\n4.3.2. Playground Area\n\nBelow the Main Form, we provide a Playground Area as an easy way to send user inputs to the model and test model responses (DG5) within Canvil. The designer may test on the same Canvil multiple times or duplicate one with its entire state and test different tweaked versions (Fig. 1B). Designers are likely to find stateful duplication intuitive as it is available on all native objects in the Figma canvas (DG2).\n\n4.3.3. The Generate Panel\n\nTo generate a response from a model adapted with the system prompt authored on the Main Form, the user selects the ‚ÄúGenerate‚Äù option from Canvil‚Äôs property menu, which takes them to the Generate Panel. The panel has two modes: Playground and Design. Both modes contain an option to copy the raw prompt to one‚Äôs clipboard so it can be tested in a separate environment (e.g., a Python notebook during collaboration with a data scientist), if desired.\n\nPlayground Mode. The Playground Mode (Fig. 2A) is invoked when the user selects ‚ÄúUsing playground‚Äù from the dropdown on the Generate Panel. This mode instructs Canvil to read user input from the Playground Area and write its response back to the Model Response area. This mode is the default mode in Canvil.\n\nDesign Mode. The Design Mode (Fig. 2B) is invoked when selecting ‚ÄúUsing design‚Äù from the dropdown. In this mode, Canvil navigates the design layers on the user‚Äôs Figma canvas, reading text inputs from a specified layer(s) from those designs, and writing model responses to a specified layer(s). This mode leverages the hierarchical layer structure of Figma designs (DG2) and implements the ‚Äúinput-output‚Äù LLM-interaction proposed by Petridis et al. (Petridis et al., 2023a).\n\nThe user can directly select a layer from their Figma canvas and use the ‚ÄúSet read layer(s)‚Äù button to bind it as Canvil‚Äôs input. This allows Canvil to retrieve and use the text from the bound layer as user input. If the user has inputs from matching layers from the same screen, they can check the option to include them as input, and Canvil will read from those layers accordingly. Similarly, the user can select a layer to which the model writes its response. However, when implementing the checkbox for writing to multiple layers, we realized that writing the same output to multiple locations on the same screen is rarely meaningful for designers. As such, we designed the write layer checkbox to search for matching layers across screens for simultaneous updates in multiple screens along a user flow, ensuring content consistency within a particular matching layer.\n\n4.3.4. The Settings Panel\n\nThe Settings panel (Fig. 2C), accessible through the ‚ÄúSettings‚Äù option from Canvil‚Äôs property menu, contains some basic model settings for the user to configure, along with a field for the user to enter their OpenAI API key required for response generation. We distilled the list of settings in the OpenAI API to four (model selection, temperature , maximum generation length, stop words) that may be useful to non-AI experts such as designers. The ‚ÄúUpdate Settings‚Äù button saves the settings to Canvil‚Äôs state, which is preserved when the Canvil is duplicated.\n\n4.4. Implementation\n\nCanvil is a Figma widget implemented in TypeScript using the Figma API. A Figma widget differs from a Figma plugin by its collaborative nature. A widget is available to all users of the canvas that it is placed in, and maintains a common state that supports multi-user editing by default. Users can interact with widgets just like any other native object on the Figma canvas, including moving, duplicating, and styling. In contrast, Plugins are local to an individual user. The collaborative affordances of widgets align well with DG4, so we implemented Canvil as a widget.\n\nUpon the user selecting ‚ÄùGenerate,‚Äù Canvil prepares a system prompt using the text written on the main form. Each field in the form is converted to markdown format and is packaged up in ChatLM for added prompt parsability. Our prompt template is available in Appendix B. The populated template is sent as a system prompt to an OpenAI Chat API endpoint with settings specified in Canvil‚Äôs settings panel. If generation was triggered in Playground Mode, any text in the Playground Area will be sent as user input to the model. If generation was triggered in Design Mode, Canvil searches for the read layer(s) specified in Design mode on the user‚Äôs current Figma canvas, retrieves the text within those layers, and sends them off to the API endpoint as user input.\n\nCanvil is compatible with both the Figma design editor and the FigJam whiteboarding tool. All features have the same behavior in both environments, except that the Design Mode for generation is not available in FigJam because of FigJam‚Äôs inability to access and edit design elements.\n\n5. Design Study\n\nWith Canvil as a technology probe, we conducted a task-based design study with 17 participants organized into 6 groups. The study was conducted on a per-group basis; each session was 90 minutes in length. Our study investigated the following research questions:\n\nRQ1.\n\nImplications of adaptation: How does engagement with designerly adaptation impact design workflows and outputs in the context of our study?\n\nRQ2.\n\nAdaptation at large: What are designers‚Äô attitudes toward designerly adaptation beyond the context of our study?\n\n5.1. Participants\n\nWe recruited our participants from two channels. First, we distributed study invites to designers via email, professional interest groups, and word of mouth at a large technology company. Some of these invitees took part in our formative study. We also snowball sampled by asking our invitees to refer us to other designers they work with who may also be interested in participating. From this channel, we only selected designers with prior experience working on LLM-powered products and features. Second, we sent study invites to a Slack workspace for HCI and design maintained by a large, public academic institution in the United States. The population in the Slack consists primarily of design students and early-career designers. From this channel, we only selected those without any experience working on LLM-powered products and features. Our goal of recruiting from these two channels was to capture potential disparities that may arise in our findings hinging on prior experience working with LLMs.\n\nInvitees first filled out a screening form with information on demographics, professional background, prior exposure to LLMs, and group member preferences. We then organized all candidates from the technology company into groups based on member preferences and availabilities, and did the same for candidates from the academic institution. We left recruitment open for both channels as we conducted the studies and stopped when we reached data saturation. In the end, we had 6 groups (3 groups from each channel) with 17 participants (8 from the technology company and 9 from the academic institution). Five groups were of size 3 and one of size 2. All participants were based in the United States. Participant and group details can be found in Table 2 in Appendix C.\n\nAll study sessions were recorded and transcribed. We sent each participant a $75 USD gift card as an honorarium after the study. This study was approved by the Institutional Review Boards of all organizations involved.\n\n5.2. Study Design\n\nOur task-based study design was motivated by contextual inquiry (Beyer and Holtzblatt, 1999) due to its ability to elicit rich information about participants‚Äô work practices and processes. We conducted our study virtually over videoconferencing software and Figma. We opted for a group study design to better emulate the team-based environment in which many designers now work (Feng et al., 2023c). A group study may also help uncover more insights about enabling collaboration (DG4) than prior individual design studies about AI-powered user experiences (Petridis et al., 2023b; Subramonyam et al., 2021a; Feng and McDonald, 2023; Feng et al., 2023b).\n\n5.2.1. Setup\n\nThe following instructions for the study‚Äôs design task were provided to all groups:\n\nYou are all designers on a product called Feasto. Feasto aims to increase users‚Äô enjoyment of food. The Feasto team is scoping out a new feature called the 3-course meal planner, which allows users to get suggestions for 3-course menus they can cook by simply listing out some ingredients they have. The team agrees that large language models (LLMs) are a promising technology to power this new feature. Your task is to design the UX for the area in which users will see and interact with the suggested menus.\n\nWe assembled these instructions cognizant of the fact that this was a time-constrained study, and we wanted participants to specifically focus on the area of the interface where users interact with an LLM. To reduce design overhead unrelated to our research questions, we provided starter UIs for participants to work with, along with basic UI components such as text, buttons, and sticky notes (Fig. 3B). If participants had an idea but did not have time to execute it, we encouraged them to describe it on a sticky note next to their designs.\n\nTo better probe research-informed model adaptation‚Äîthat is, adaptation across varying user contexts informed by user research (DG3)‚Äîwe created descriptions for three hypothetical user groups of this new Feasto feature based on users‚Äô geographic region: west coast of the U.S., Turkey, and India. These descriptions are not meant to act as user personas, but rather high-level sketches of the customs and preferences that may be prevalent in the region. We crafted the user group descriptions to vary along three key dimensions (assuming everyone used Feasto in English): dietary restrictions, access to ingredients, and menu style preferences. Definitions for these dimensions, along with the full user group descriptions, are available in Appendix D.\n\nWe provided one blank starter UI with some example user inputs and one blank Canvil per user group. Participants were invited to vary the user experience between user groups as much or as little as they saw fit. We also encouraged participants to use Canvil to adapt an LLM and test its behavior as they designed.\n\nA Figma file for each study group contained all the materials described above. Within the file, we created separate canvases for each participant to act as individual workspaces, each with its own copy of the materials (Fig. 3). We also had a shared canvas for introductions and instructions before the task and collectively debriefing afterwards.\n\n5.2.2. Procedure\n\nOur 90-minute study was divided as follows.\n\nIntroduction (20 minutes): First, all participants introduced themselves to others in the group. The study facilitator then gave a demo of Canvil, covering all features described in Section 4.3 using a pre-filled Canvil. The facilitator also described the design task and answered any clarifying questions from participants.\n\nDesign task (40 minutes): Participants spent 40 minutes on the design task and were asked to consider at least two of the three user groups provided. Participants were encouraged to spend 10 minutes authoring a Canvil and another 10 minutes on UI design per user group. Some participants who had remaining time designed for all three user groups.\n\nGroup interview (30 minutes): Participants first filled out a brief usability questionnaire about Canvil before gathering in the shared page of the Figma file. They were asked to copy their Canvils into the shared space and also their designs (if desired) to share with the group. The facilitator then led a semi-structured interview that asked participants to reflect on their experience adapting models with Canvil, Canvil‚Äôs collaborative capabilities, and how they see adaptation fitting in with their own design practice. The facilitator ensured that each participant had ample opportunity to express their thoughts in the group setting, and also encouraged dialogue between participants.\n\n5.3. Data Analysis\n\nWe conducted a qualitative analysis of transcriptions and Figma canvases (including authored Canvils), as well as a quantitative analysis of feedback from Canvil‚Äôs usability questionnaire.\n\nFor our qualitative analysis, the first author took a hybrid inductive-deductive approach to coding the group interview portion of the transcriptions from the study. This process started with an open coding round in which high-level themes were generated, followed by subsequent rounds of thematic analysis via affinity diagramming in which themes were broken down into sub-themes. This approach was taken because new subtleties and complexities emerged from our initial codes as coding progressed due to the diverse approaches observed in our study as well as participants‚Äô group discussion dynamics. The codes and themes were discussed and iterated on with research team members at weekly meetings. Additionally, whenever participants made references to content within their Figma canvases (e.g., their designs and/or Canvils), the first author took screenshots of those references and linked them to transcript dialogue. Summary memos were then written for our high-level codes and presented alongside relevant screenshots.\n\nCanvil‚Äôs usability questionnaire followed the standard template for the System Usability Scale (SUS) (usability.gov, 2022; Brooke, 1995) and consisted of 10 questions, each with five response options for respondents from Strongly agree to Strongly disagree. We computed a SUS score using methods outlined by Brooke (Brooke, 1995) for each participant and subsequently computed a mean score and standard deviation.\n\n6. Results\n\nOur quantitative analysis showed that Canvil had a mean SUS score of 69.94 (std = 12.18), meaning its usability was ‚Äúabove average‚Äù (usability.gov, 2022). In this section, however, we focus on the qualitative insights on designerly adaptation from our design study. We specifically showcase the implications of satisfying Canvil‚Äôs design goals (Section 4.1) on design workflows and outputs. Our results from designers without prior experience working with LLMs did not differ noticeably from those with experience, except that the latter drew more connections between adaptation and their past model tinkering workflows. We present text that participants wrote in Canvil as purple and italicized.\n\n6.1. Authoring System Prompts as Part of Design Workflows\n\n6.1.1. Research-Informed System Prompting as a Design Activity (DG1, DG3)\n\nWhen authoring system prompts in Canvil (DG1), designers frequently referenced the provided user research documents. It is through this process that they embedded user context‚Äîe.g., varying lifestyles, customs, preferences‚Äîinto system prompts (DG3). For example, in P2‚Äôs Canvil for the American user group, they wrote under Audience Setting that the model is serving users who like diverse cuisine but care about the environment and sustainability and are health-conscious. Designers also adjusted their prompts between user groups in attempts to create more customized and meaningful user experiences. In the Canvil for the user group from India, P4 wrote that The model should use Indian terminologies to describe the recipe. Eg: eggplant is brinjal in India. P5 experimented with model variations within one user group, choosing distinctly different personalities for each model. For their Indian user group, they populated one Canvil‚Äôs Model Profile field with The model is a head chef of a 5 star restaurant situated in New Delhi. It is a busy day for the restaurant and the chef is low on time. and another with The model is a mother who is helping her son cook quick meals in hostel. In the end, they noted that ‚Äúthe models did very well‚Äù in taking on these different profiles.\n\nIn particular, designers used the Guardrails field in Canvil to address potential violations of users‚Äô dietary preferences and restrictions, along with model misuse. For example, P12 used information provided on the Turkish user group to create the following guardrail: The model should not include recipes with pork or alcohol and must respect the fasting period of Ramadan by suggesting suitable pre-dawn and post-dusk meals. A few participants noted that some user groups may require more strict model guardrails than others. P15 identified a subtle but key requirement between two user groups while authoring their Canvils:\n\n‚ÄúI wanted to highlight that the model could take many more liberties with the recipes that it was giving [to the Turkish user group], but it couldn‚Äôt take more liberties with the ingredients. It could be very loose with: try this, try this with this, but never like crossing the boundary of the dietary restrictions. Whereas with the [American] one, it‚Äôll take it into account, but it‚Äôs not gonna mar their religious practice.‚Äù [P15]\n\nDesigners sought to address user needs through combined efforts in prompt authoring and UI design. For example, P15 provided alternatives via buttons to support more flexible user input of ingredients going into the model. P8 described their UI design in response to observing the limits of what can be achieved via writing system prompts: ‚ÄúSometimes even in the instructions that I gave to Canvil, it wasn‚Äôt really reflecting that [desired] granularity until I pushed it further. In my design I ended up putting a little textbox area where people can specify how detailed they want the instructions.‚Äù Fig. 4 contains some examples of designers‚Äô UIs from our study.\n\nIn short, the structure provided by prompt authoring interface of Canvil is compatible with designers‚Äô reasoning about user needs and preferences based on user research. Canvil supports this reasoning process to inform both the system prompts and UI design in an integrated fashion, and makes authoring system prompts a natural part of design activities.\n\n6.1.2. Tinkering Allowed for Deeper Reasoning of User-LLM Interaction (DG2, DG5)\n\nCanvil‚Äôs integration in the design environment (DG2) allowed designers to consider model behavior and user interaction simultaneously. Many designers iterated multiple times on their system prompts (DG5) until they were satisfied about the adapted model. P3 reformatted the output to better suit their existing UI: ‚Äúthe first run was a paragraph that I felt like was really hard to read, so I added some core instructions saying give me a numbered list.‚Äù P13 shared that their model was repeating the same ingredients for each course, so they ‚Äútweaked the level of detail and the recipes to add more variety.‚Äù P11 discovered that, in general, ‚Äústating [instructions] in the positive instead of the negative‚Äù yielded better results for their envisioned experience.\n\nDesigners‚Äô tinkering with the adapted model not only helped them iterate on their system prompts, but also allowed them to more deeply reason about user interactions with LLMs and reflect on the whys of UI design. For P14, grappling with the Example Inputs and Outputs field in Canvil prompted reasoning about the optimal interaction pattern to design with:\n\n‚ÄúThe example inputs and outputs were a little bit hard for me to think about, cause I was wondering for the output, is it just the recipe? Or would it be more of a conversation, like oh, do you have dietary restrictions? Do you have any XYZ preferences? Like there‚Äôs some logic that goes into asking those questions and I was wondering which one should I think about first versus later.‚Äù [P14]\n\nTinkering with the adapted model often led to evolving of or experimenting with UI ideas. P17 considered different interaction patterns for their interface‚Äîa chat-based interface versus a form-filling GUI‚Äîafter discovering how sensitive LLMs can be to prompts. P4 mentioned that they started off with ‚Äúa specific user base in mind and I wanted a specific tone,‚Äù but the design of the UI became clearer after they saw an ideal output after multiple iterations: ‚ÄúMy design direction changed when I saw the output. I iterated multiple times and what I like the best about this [response] was that it was short and [ideal] for scrolling in a screen. I tried that out [in my new UI] and it worked pretty well.‚Äù Examples of quoted participants‚Äô designs can be found in Fig. 4.\n\nInterestingly, a few designers wanted a degree of separation between UI work and model tinkering. P1 shared that they found it ‚Äúa bit hard to juggle between Canvil and [the UI] at the same time‚Äù, especially when the output is incompatible with their design settings: ‚ÄúWhat if the text is really long and then I have to play with auto layout?‚Äù Therefore, P1 preferred first tinkering with the model using the Playground Mode. P8 also agreed that having Canvil inject outputs directly into text boxes can be ‚Äúa little scary [‚Ä¶] people don‚Äôt wanna actually commit [responses] to text boxes sometimes.‚Äù Past work considers integration between models and UI designs as desirable (Petridis et al., 2023b, a; Subramonyam et al., 2021a). Our results encourage providing designers with more choices and control over such integration.\n\n6.2. Designers Envisioned Collaborative Workflows with Designerly Adaptation\n\n6.2.1. Collaborative Uses of Canvil in Design Teams (DG4)\n\nThe design task in our study was an individual activity because we expected collaborative efforts to begin after designers had made some individual attempts at adaptation and designed with the adapted model (Feng et al., 2023c). Indeed, when reflecting on Canvil‚Äôs collaborative capabilities (DG4), designers thought the workflow matched their own experiences. P8 did not consider a ‚Äúdivide and conquer‚Äù authoring approach‚Äîwhere each team member is responsible for one Canvil field‚Äîas necessary unless the team was under a tight time constraint, and instead saw more potential in a ‚Äútinker and unite‚Äù approach: ‚ÄúSince it doesn‚Äôt take too much time to generate a response, I feel like I would lean towards, let‚Äôs all try different things and see how we can use this tool to get the best response.‚Äù P8 added that in order for this approach to work, some guidelines should be set for collaborators ‚Äúso that all of our responses are consistent, like everyone has the same Audience Setting and [uses] the same kind of wording.‚Äù P4 agreed and suggested initial brainstorming sessions for collaborators so that they could ‚Äúfind a way to come to common grounds for all these [fields].‚Äù P6, after hearing P4‚Äôs suggestion, noted that using Canvil within FigJam may be useful ‚Äúfor ideation or brainstorming‚Äù before bringing more polished Canvils into Figma.\n\nOn the other hand, some saw potential in the ‚Äúdivide and conquer‚Äù approach, especially when expertise in a design team may be distributed. P7, who specialized more in visual design, said that they were ‚Äústruggling a little bit with like how to prioritize all the content, and I feel like that would be specialties a content designer would bring in.‚Äù P8, who initially preferred to ‚Äútinker and unite,‚Äù agreed that it would be valuable to also leverage distributed expertise: ‚ÄúIn a design team, there might be someone who knows a user persona the best, and they can be assigned to Audience Setting. Then you have content designers on maybe example inputs and outputs.‚Äù\n\n6.2.2. Collaboration and Knowledge Sharing Afforded by Canvil (DG2, DG4)\n\nMany designers commented on how Canvil, as a widget integrated into Figma‚Äôs design environment (DG2), afforded collaboration (DG4). P5 appreciated that they could leverage Figma‚Äôs built-in collaboration features: ‚ÄúIf I see my fellow designer‚Äôs Canvil, and if I want to change something then and there, I can drop a comment on it just like a normal component in Figma, which we‚Äôve been doing in our everyday design work.‚Äù P10 shared that their team relies on a spreadsheet to keep track of system prompt iterations and working in a canvas environment would be a significant improvement.\n\nIn particular, participants saw potential for more effective knowledge sharing about adaptation with Canvil. P10 commented that: ‚ÄúYou could have a master Canvil and then you could make copies, and [others] can then do their own interpretations.‚Äù P11 agreed and added that seeing iterations on a canvas can help find inspiration in others‚Äô work: ‚ÄúI think anytime you line up different iterations together, you notice: ohh that person, did you know they had that approach? That‚Äôs a good idea. And I‚Äôm gonna try that over here. I think it really is an aid to experimentation.‚Äù We observed other instances of knowledge sharing as well when designers viewed others‚Äô Canvils. For example, P4 noted that they were inclined to consider more dimensions for their system prompt after seeing P6‚Äôs work: ‚Äú[looking at P6‚Äôs Canvil] made me thinking about timers and Hindi slang.‚Äù Some also realized new capabilities of LLMs by looking at others‚Äô prompts and outputs‚ÄîP1 shared that ‚ÄúI didn‚Äôt realize at first that you can actually make the [LLM] generate multiple recipes, just like what P2 did.‚Äù P1‚Äôs groupmates (P2 and P3) were also intrigued when they saw that P1 had used one Canvil to generate example inputs for another Canvil to use. These social and collaborative affordances differentiate Canvil from prior systems for empowering designers to tinker with AI (Subramonyam et al., 2021a; Petridis et al., 2023b; Carney et al., 2020).\n\n6.2.3. Collaborating With Non-Design Stakeholders via Canvil (DG1, DG4)\n\nWith system prompt authoring as the primary interaction, Canvil (DG1) ensures easy authorship and sharing within a team (DG4), even across domain boundaries. Many designers thought that Canvil was simple and intuitive enough to be used by not just designers. Consequently, P2 saw Canvil as a useful boundary object (Star and Griesemer, 1989): ‚ÄúI can see from a product manager‚Äôs perspective that they would love to play around with the prompts, and it would probably help the designer explain their design choices. I feel they would find more common ground because they both used a similar tool.‚Äù P6 agreed that product managers (PMs) have perspectives and expertise to contribute to prompt authoring. Designers also saw potential for deeper collaboration with data scientists and other technical stakeholders. P11 commented that the ‚ÄúCopy raw prompt to clipboard‚Äù button in Generate Panel (Section 4.3.3) can allow for direct handoff of system prompts: ‚Äú[The button] is a way of exporting that so a data scientist can come along and say sure, let me plug that into code.‚Äù Even if designers were unable to achieve the desired model behavior themselves, P17 thought Canvil was helpful in specifying desired changes: ‚ÄúIf I want the model to respond in this way versus that way, just having something tangible to show engineering partners where the tweak would need to be, would be helpful. And I think [that‚Äôs] obviously easier with Canvil.‚Äù P15 believed that rise of natural language as a shared representation can allow for more transparent and balanced information flow between disciplinary boundaries:\n\n‚ÄúI think on the designer side, there is apprehension about [AI] being in different skill set. The [technical] teams just don‚Äôt really embrace designers, cause this is ‚Äòtheir‚Äô skill set. They‚Äôre gonna do it quicker and better, and that‚Äôs probably true in almost all circumstances except natural language. And except when it comes to an intuitive experience. So it‚Äôs about making the teams meet each other as opposed to one team being more reticent than the other.‚Äù [P15]\n\nDesigners also acknowledged that designerly adaptation may only be one piece of the larger puzzle to steer an LLM‚Äôs behavior and integrate it into a product. Some desired changes, such as factual grounding and connecting to external knowledge bases, may still need to be left to technical teams. P12 even saw factual grounding as a prerequisite to designerly adaptation: ‚Äúbecause everything hinges on you being able to get the model specificity correct.‚Äù Both P12 and P14 wanted Canvil to be able to connect to other models built and maintained by their own teams, as the behavior of unadapted OpenAI models may be too generic to use as an effective design material. P14 comments: ‚ÄúI think linking to the vanilla ChatGPT model isn‚Äôt specific enough. If there is a way to somehow have Canvil link to the devs‚Äô [models] while I‚Äôm designing and I‚Äôm able to see what type of experience and what type of response our users are getting, I think that would be super helpful.‚Äù\n\nWhile we currently offer only three OpenAI models to use with Canvil (GPT-3.5-Turbo, GPT-3.5-Turbo-16k, GPT-4), more may be added with lightweight engineering as the widget is communicating with the model via API endpoints. As such, we can envision future support for more model families, including teams‚Äô custom models.\n\n6.3. Designerly Adaptation at Large\n\n6.3.1. Integrating Adaptation into Design Practice\n\nDesigners saw great value in engaging with adaptation and saw direct paths to application in their own design practice beyond the study. P10, who has extensive experience working on AI-powered products deployed to users worldwide, said that adaptation has traditionally been difficult: ‚ÄúWe had teams of people training models and nudging the technology to align with [user personas].‚Äù Having experienced adaptation of LLMs via Canvil, they shared that ‚ÄúI can see this being something that would enable [adaptation] to actually happen in a way that‚Äôs much more practical than taking Python classes, which I‚Äôve done.‚Äù For P13, who works on a product with enterprise and consumer versions, adaptation can help set more detailed product requirements where necessary: ‚ÄúI could see [adaptation] being really helpful, being able to prototype and build separate generative models for enterprise customers and then to work more closely with them to tune and add requirements.‚Äù P16 considered adaptation to be a useful exercise for user empathy: ‚Äúfor [Audience Setting] I just tried to put myself in the mind of someone who‚Äôs using this and say, I‚Äôm really good at following recipes, but have a variety of dietary preferences and restrictions.‚Äù\n\nSome participants pointed out that designers should be prepared to carefully navigate challenges of adaptation that may arise in practice. P11 reflected on the balance between specificity through adaptation and breadth of audience: ‚ÄúI wanted to avoid bias in these [models]. There‚Äôs that balance between, having the profile of the model be specific enough so that it‚Äôs relatable [‚Ä¶] and making technology for everybody. If you try to make it too generic, it‚Äôs just gonna be bland. So we have to be careful.‚Äù P15 pointed out that adaptation may not account for all user needs and preferences: ‚Äúin terms of the normative sense, it would be fantastic. But feasibly though, there are so many different things that you need to take into account, like even length of language.‚Äù These complexities highlight the broader social and ethical factors surrounding designerly adaptation to confront as designers integrate it into their own practice.\n\n6.3.2. Designerly vs. End-User Adaptation\n\nPrior work found that designers were naturally inclined to provide end-users with more controls for customization of their AI-powered user experiences (Feng and McDonald, 2023). Some designers also did the same in our study and reflected on the interplay between designerly and end-user adaptation. P15, who discussed the complex factors of adaptation in Section 6.3.1, said they envisioned a more feasible approach to be designers delegating adaptation to end-users past a certain point: ‚ÄúIf we could put in a Venn diagram of all of the different needs from all over the world, seeing what the more central options are, the really interesting piece here is having users be able to make that [adaptation] decision themselves.‚Äù P10, after interacting with Canvil, started thinking about similar tools for end-user adaptation: ‚ÄúI thought [Canvil] could be a tool, yes, for us content designers, but could we adopt this for individual users so they could tweak [the model]?‚Äù However, they acknowledged that granting end-users complete control over adaptation may not be ideal either, and designers are responsible for setting certain constraints:\n\n‚ÄúIn my experience with human beings, you don‚Äôt want to give them a blue sky. You wanna give them things they can create on their own and feel like they‚Äôre in control. I could see us using [Canvil] to create something like 5 or 6 different [model] personalities that users around the world could leverage.‚Äù [P10]\n\nP11 also found the idea of balancing designerly and end-user adaptation appealing, as they were concerned about model biases (see Section 6.3.1). They proposed having ‚Äúa preference or a setting or a dial based on your regional settings where [LLM behavior] could be modulated.‚Äù With the rapid rise of LLM-powered user experiences, we invite practitioners and researchers to more deeply reason about who can be responsible for adaptation and when adaptation should happen.\n\n6.3.3. Areas of Additional Support\n\nDesigners‚Äô also identified several areas for improvement to better support designerly adaptation. For one, designers lacked a clear mental model of how the fields of Canvil would impact the adaptation outcome, including how much detail is required in authoring the prompt. As a result, some designers wrote long, detailed instructions, while others kept their Canvils sparse, and some were pleasantly surprised by how well the model handled minimal instructions. P6 further noted that the stochastic nature of the model was particularly challenging to work with when writing outputs to their designs: ‚Äúevery time I generated the [response], I felt that they were different every time, so it was not easy to predict what the next [response] would look like.‚Äù We note that current LLMs are known to be challenging to be precisely controlled through prompting, and there is a lack of transparency into (or even an established understanding of) how prompting impacts LLM behaviors. However, these are actively researched areas which can help improve the mental model of Canvil‚Äôs users and its general effectiveness.\n\nAdditionally, some designers wanted to engage in finer-grained experimentation and iteration by only focusing on a specific field at once. P7, who wanted to iterate more on the Model Profile field, wondered if there was a way to ‚Äúdecrease the size [of the other fields] and expand to view everything.‚Äù We envision a modular future version of the tool where each field can be separated, such that a user can mix-and-match different fields to form a complete system prompt. However, before that, we may need to address the precise mapping between each field (and potential overlaps between them, as noted by some participants) to adaptation outcomes as discussed above. As mentioned in Section 4.3, our fields present just one possible structure for system prompts. The fields can perhaps be reconfigured to reduce potential overlap, or even dynamically generated based on the design task.\n\nOn a higher level, it would be irresponsible to assume that designers can walk away with a comprehensive understanding of model behavior after a few rounds of tinkering in Canvil. While observing a few informative output instances of model behavior aids the design process, formal evaluations ensuring comprehensive coverage of the user input space are crucial for production-ready systems. Thus, new evaluation tooling and processes that loop in technical stakeholders may be required.\n\n7. Discussion\n\nOur findings shed light on possible workflows for designerly adaptation, promises of tools for collaborative AI tinkering, and implications of materiality on the social and collaborative practices of product teams. We discuss each below.\n\n7.1. A Workflow for Designerly Adaptation\n\nDrawing on the results from both our formative and design study, we now propose a workflow for designerly adaptation. Our proposed workflow consists of four steps:\n\n(1)\n\nUnderstand deployment context through user research. To orient adaptation, it is imperative to first understand the context in which users will interact with the LLM-powered system. This includes users‚Äô goals, needs, and pain points, along with customs and values that may affect their use of the technology. Those with expertise in user research methodologies should lead the execution of this step.\n\n(2)\n\nEmbed user research insights into system prompts, learning from examples where possible. This carves out a direct path for user research to impact model behavior, as we observed in Section 6.1.1. Thanks to the collaborative nature of many modern design tools (Feng et al., 2023c), there may be example prompts written by other designers available for reference, or templates to use as a starting point (Section 6.2.1). By leveraging collaborative affordances to share knowledge, designers can enhance their efficiency at adaptation over time.\n\n(3)\n\nCo-evolve interface design and prompts. As observed in Section 6.1.2, adaptation can supply new inspiration for UI designs and affordances. On the other hand, designers also tinkered with prompts to coax the model into providing outputs that fit into the constraints laid out by the designs being explored. We see the co-evolution of designs and prompts as a promising path forward, in which iterative tinkering with system prompts for adaptation shapes design choices, and vice versa.\n\n(4)\n\nShare designs and adaptation efforts with the broader team. Showcasing in-progress work through design critiques is already a part of the design process (Feng et al., 2023b). In our study, we found that sharing Canvils helped envision new collaborative workflows with other designers, as well as communicating their perspectives and negotiating with technical stakeholders (Section 6.2.1). Following from Step 3, we thus believe that sharing designs alongside adaptation is critical for involving designers and leveraging their expertise for responsible development and deployment of LLM-powered systems.\n\nOur proposal is not meant to constrain what workflows for designerly adaptation can possibly look like, but rather to offer a concrete entry point for practitioners and researchers to further explore and iterate on this new practice. We invite the community to experiment with this workflow in future practice and research.\n\n7.2. Towards Collaborative Tinkering With AI\n\nOur work highlights the promises of not only empowering designers to tinker with LLMs as a design material, but also collaboratively doing so. During our design study, designers proposed various collaborative workflows for system prompting with Canvil, including a divide-and-conquer approach to gather expertise distributed across the team and a tinker-and-unite approach where each designer tinkers independently before merging the results into a team version (Section 6.2.1). Additionally, designers benefited from seeing and learning from others‚Äô Canvils and commented on more organized knowledge sharing and version management afforded by using Canvil in Figma‚Äôs multiplayer canvas environment (Section 6.2.2).\n\nDespite promising opportunities, many tools that lower the barrier for AI tinkering (e.g., (Lobe.ai, 2021; Liner.ai, 2022; Carney et al., 2020; Subramonyam et al., 2021a)) were designed with individualized workflows in mind. Some, like Google‚Äôs Teachable Machine (Carney et al., 2020), offer social features such as project galleries to showcase example projects, but affordances for collaboration remain limited. One reason may be technical constraints surrounding the collaborative updating of computationally heavy stages of large-scale training pipelines. The shift towards using large pre-trained models, and the ability to interact with these models via natural language, helps accelerate model adaptation and tightens feedback loops during tinkering. Consequently, novel interfaces, such as node-based editors, (e.g., (Arawjo et al., 2023; Suh et al., 2023; Wu et al., 2022b; Angert et al., 2023)), enable users to decompose large tasks by ‚Äúchaining‚Äù together smaller prompt submodules. Node-based editors are conceptually appealing for collaboration (Wu et al., 2022b)‚Äîcollaborators may distribute work according to expertise and nature of the task within the submodule, and easily chain together their work. Few node-based editors, however, support multiplayer collaboration. We see significant potential in extending these tools along a collaborative dimension.\n\nBesides technical constraints discussed, it can simply be the case that robust collaborative experiences are generally challenging to implement. We circumvented this in our work by building on top of the Figma API, which allows us to leverage Figma‚Äôs built-in collaborative features by default. We thus encourage researchers and practitioners who wish to build collaborative tools to take advantage of existing collaborative platforms‚Äô developer APIs where possible, especially as these APIs become more richly featured.\n\n7.3. Materiality and Sociomateriality of LLMs\n\nThroughout this work, we depict LLMs as a design material to use materiality as a means of understanding, shaping, and applying this technology to tackle design problems. Yet, in collaborative settings, materiality‚Äôs implications extend beyond the individual. Scholars use the term sociomateriality in recognition of materiality‚Äôs tendency to shape, and be shaped by, organizational practices typically constituted as ‚Äúsocial‚Äù (e.g., decision-making, strategy formulation) (Orlikowski, 2007; Leonardi, 2012). For example, Orlikowski observed that the issuance of BlackBerry devices within a company led employees to obsessively check for new messages and send immediate responses (Orlikowski, 2007). The BlackBerrys‚Äô material properties‚Äîin this case, being able to receive and send messages on-the-go‚Äîreconfigured employees‚Äô social practices, which in turn shifted how they think and act with the technology.\n\nIn our design study, designerly adaptation allowed for exploration of LLMs‚Äô material properties through tinkering. As a result, designers proposed various UI affordances for enhancing users‚Äô interaction with the LLM-powered meal planner (Section 6.1.2) and adjusted the model‚Äôs guardrails depending on differing dietary restrictions between user groups. However, grappling with the materiality of LLMs also demonstrated potential to reconfigure social practices within product teams. For example, designers informed us that they saw new avenues of collaboration upon interaction with Canvil, which included ‚Äúhanding off‚Äù system prompts to data scientists by exporting prompts out of Figma, working with PMs to inform prompts with product requirements, and having a more concrete artifact to communicate desired tweaks to engineers (Section 6.2.3). Moreover, designerly adaptation itself may only be one piece of the broader adaptation puzzle. Certain capabilities such as storing external knowledge in model weights through fine-tuning cannot easily be unlocked through‚Äîand may even a prerequisite for‚Äîdesignerly adaptation, paving the way for processes and artifacts that fundamentally differ from those used in tradition design handoffs (Leiva et al., 2019; Feng and Zhang, 2022). This potential reconfiguration of teams‚Äô collaborative dynamics establishes designerly adaptation as not only a means of exploring LLMs‚Äô materiality, but also a sociomaterial practice.\n\nThe implications of this observation are twofold. First, sociomateriality argues that reconfigurations of collaborative practices upon interaction with a prominent new technology are inevitable (Orlikowski, 2007; Leonardi, 2012). At the time of writing, frenzied excitement over LLM capabilities has launched an industry-wide race to embed them into products and product suites (Parnin et al., 2023). There is much yet to be discovered about shifts in organizational practices and the emergence of new ones in the midst of this race, so researchers in CSCW and organizational science should be attuned to emergent challenges. Second, addressing these challenges for practitioners and providing probes for researchers to study them may require new processes and tools. Canvil is an early example of such a tool, but more are needed to tackle the multiplicity of open questions in today‚Äôs rapidly evolving AI landscape.\n\n8. Limitations and Future Work\n\nOur design study, conducted in Figma, aimed to mirror real-world design activities (Ball, 2005), yet concerns about ecological validity may persist. User research in practice may differ in presentation and detail than in our study setup, which can impact how designers perform research-informed adaptation. Feasto, the fictitious app in our design study, applied LLMs to the universally relatable topic of food. Designers in domains with fewer broadly-shared experiences (e.g., accessibility), may require deeper collaboration with domain experts and thus face workflow complexities not accounted for in our study. A potential direction for future work, then, is longitudinal studies that use contextual inquiry to observe product teams throughout a full development lifecycle of an LLM-powered feature, focusing on key adaptation decision-making processes and the practitioners involved.\n\nOur study, like any study that uses a probe, has results contingent on our probe‚Äôs features. For example, designers‚Äô interaction with the Main Form‚Äîdesigned with recommended practices for system prompting (Microsoft, 2023; OpenAI, 2023c)‚Äîshaped their approach to adaptation, and changes to the form could impact existing results and reveal new ones. We mitigate this by presenting findings not tied to the Main Form, nor our design study task. Future research can build probes for designerly adaptation, distinct from Canvil, to explore agreements with and divergences from our results.\n\nDesigners in our study also provided feedback on Canvil that we can integrate into future work. These include breaking down the Main Form into sub-Canvils and linking them together to assemble system prompts, offering support for multimodal models (e.g., GPT-4V ), and text formatting controls for model outputs. Finally, as mentioned in Section 4.4, Canvil may also be used in FigJam. Future research could explore adaptation in early design stages, like brainstorming and ideation, through studies in FigJam.\n\n9. Conclusion\n\nIn this paper, we introduced the practice of designerly adaptation to encourage designers to engage with LLMs as an adaptable design material when crafting LLM-powered user experiences. We first built up a characterization of designerly adaptation through a formative interview study with 12 designers. We then developed Canvil, a Figma widget that operationalizes designerly adaptation by enabling designers to iteratively author, test, and share system prompts within Figma‚Äôs collaborative design environment. We used Canvil as a technology probe to explore the integration of designerly adaptation in UX practice through a group-based design study with 17 designers. We observed numerous promising approaches taken by designers such as iteratively tinkering with different adaptation strategies, sensitizing the model to nuances in user needs, and reasoning about interface affordances using research-informed model behavior; these approaches‚Äô promises were amplified once adaptation was embraced as a collaborative practice. Our work illuminates paths for new processes and tools to spotlight designers‚Äô user-centered perspectives and expertise for more responsible and thoughtful deployment of LLM-powered technologies.\n\nReferences\n\n(1)\n\nAmershi et al. (2015) Saleema Amershi, Max Chickering, Steven M Drucker, Bongshin Lee, Patrice Simard, and Jina Suh. 2015. Modeltracker: Redesigning performance analysis tools for machine learning. In Proceedings of the 33rd Annual ACM Conference on Human Factors in Computing Systems. 337‚Äì346.\n\nAngert et al. (2023) Tyler Angert, Miroslav Suzara, Jenny Han, Christopher Pondoc, and Hariharan Subramonyam. 2023. Spellburst: A Node-Based Interface for Exploratory Creative Coding with Natural Language Prompts. In Proceedings of the 36th Annual ACM Symposium on User Interface Software and Technology (¬°conf-loc¬ø, ¬°city¬øSan Francisco¬°/city¬ø, ¬°state¬øCA¬°/state¬ø, ¬°country¬øUSA¬°/country¬ø, ¬°/conf-loc¬ø) (UIST ‚Äô23). Association for Computing Machinery, New York, NY, USA, Article 100, 22 pages. https://doi.org/10.1145/3586183.3606719\n\nApple (2024) Apple. 2024. Machine Learning‚ÄîHuman Interface Guidelines. https://developer.apple.com/design/human-interface-guidelines/technologies/machine-learning/introduction.\n\nArawjo et al. (2023) Ian Arawjo, Priyan Vaithilingam, Chelse Swoopes, Martin Wattenberg, and Elena Glassman. 2023. ChainForge. https://www.chainforge.ai/. Accessed: 2023-07-21.\n\nBai et al. (2022) Yuntao Bai, Saurav Kadavath, Sandipan Kundu, Amanda Askell, Jackson Kernion, Andy Jones, Anna Chen, Anna Goldie, Azalia Mirhoseini, Cameron McKinnon, et al. 2022. Constitutional ai: Harmlessness from ai feedback. arXiv preprint arXiv:2212.08073 (2022).\n\nBall (2005) Jonathan Ball. 2005. The Double Diamond: A universally accepted depiction of the design process. https://www.designcouncil.org.uk/news-opinion/double-diamond-universally-accepted-depiction-design-process.\n\nB√§uerle et al. (2022) Alex B√§uerle, √Ångel Alexander Cabrera, Fred Hohman, Megan Maher, David Koski, Xavier Suau, Titus Barik, and Dominik Moritz. 2022. Symphony: Composing Interactive Interfaces for Machine Learning. In Proceedings of the 2022 CHI Conference on Human Factors in Computing Systems (New Orleans, LA, USA) (CHI ‚Äô22). Association for Computing Machinery, New York, NY, USA, Article 210, 14 pages. https://doi.org/10.1145/3491102.3502102\n\nBenjamin et al. (2021) Jesse Josua Benjamin, Arne Berger, Nick Merrill, and James Pierce. 2021. Machine Learning Uncertainty as a Design Material: A Post-Phenomenological Inquiry. In Proceedings of the 2021 CHI Conference on Human Factors in Computing Systems (Yokohama, Japan) (CHI ‚Äô21). Association for Computing Machinery, New York, NY, USA, Article 171, 14 pages. https://doi.org/10.1145/3411764.3445481\n\nBeyer and Holtzblatt (1999) Hugh Beyer and Karen Holtzblatt. 1999. Contextual design. interactions 6, 1 (1999), 32‚Äì42.\n\nBod√©n et al. (2021) Anna CS Bod√©n, Jesper Molin, Stina Garvin, Rebecca A West, Claes Lundstr√∂m, and Darren Treanor. 2021. The human-in-the-loop: an evaluation of pathologists‚Äô interaction with artificial intelligence in clinical practice. Histopathology 79, 2 (2021), 210‚Äì218.\n\nBranson et al. (2010) Steve Branson, Catherine Wah, Florian Schroff, Boris Babenko, Peter Welinder, Pietro Perona, and Serge Belongie. 2010. Visual recognition with humans in the loop. In Computer Vision‚ÄìECCV 2010: 11th European Conference on Computer Vision, Heraklion, Crete, Greece, September 5-11, 2010, Proceedings, Part IV 11. Springer, 438‚Äì451.\n\nBrooke (1995) John Brooke. 1995. SUS: A quick and dirty usability scale. Usability Eval. Ind. 189 (11 1995).\n\nBrown et al. (2020) Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. 2020. Language models are few-shot learners. Advances in neural information processing systems 33 (2020), 1877‚Äì1901.\n\nCabrera et al. (2023) √Ångel Alexander Cabrera, Erica Fu, Donald Bertucci, Kenneth Holstein, Ameet Talwalkar, Jason I Hong, and Adam Perer. 2023. Zeno: An interactive framework for behavioral evaluation of machine learning. In Proceedings of the 2023 CHI Conference on Human Factors in Computing Systems. 1‚Äì14.\n\nCarney et al. (2020) Michelle Carney, Barron Webster, Irene Alvarado, Kyle Phillips, Noura Howell, Jordan Griffith, Jonas Jongejan, Amit Pitaru, and Alexander Chen. 2020. Teachable Machine: Approachable Web-Based Tool for Exploring Machine Learning Classification. In Extended Abstracts of the 2020 CHI Conference on Human Factors in Computing Systems (Honolulu, HI, USA) (CHI EA ‚Äô20). Association for Computing Machinery, New York, NY, USA, 1‚Äì8. https://doi.org/10.1145/3334480.3382839\n\nChao et al. (2010) Crystal Chao, Maya Cakmak, and Andrea L Thomaz. 2010. Transparent active learning for robots. In 2010 5th ACM/IEEE International Conference on Human-Robot Interaction (HRI). IEEE, 317‚Äì324.\n\nChen et al. (2022) Quan Ze Chen, Tobias Schnabel, Besmira Nushi, and Saleema Amershi. 2022. HINT: Integration Testing for AI-Based Features with Humans in the Loop. In 27th International Conference on Intelligent User Interfaces (Helsinki, Finland) (IUI ‚Äô22). Association for Computing Machinery, New York, NY, USA, 549‚Äì565. https://doi.org/10.1145/3490099.3511141\n\nCheng et al. (2023) Myra Cheng, Esin Durmus, and Dan Jurafsky. 2023. Marked Personas: Using Natural Language Prompts to Measure Stereotypes in Language Models. In Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers). Association for Computational Linguistics, Toronto, Canada, 1504‚Äì1532. https://doi.org/10.18653/v1/2023.acl-long.84\n\nChristiano et al. (2017) Paul F Christiano, Jan Leike, Tom Brown, Miljan Martic, Shane Legg, and Dario Amodei. 2017. Deep reinforcement learning from human preferences. Advances in neural information processing systems 30 (2017).\n\nDeng et al. (2023) Wesley Hanwen Deng, Nur Yildirim, Monica Chang, Motahhare Eslami, Kenneth Holstein, and Michael Madaio. 2023. Investigating Practices and Opportunities for Cross-Functional Collaboration around AI Fairness in Industry Practice. In Proceedings of the 2023 ACM Conference on Fairness, Accountability, and Transparency (Chicago, IL, USA) (FAccT ‚Äô23). Association for Computing Machinery, New York, NY, USA, 705‚Äì716. https://doi.org/10.1145/3593013.3594037\n\nDeshpande et al. (2023) Ameet Deshpande, Vishvak Murahari, Tanmay Rajpurohit, Ashwin Kalyan, and Karthik Narasimhan. 2023. Toxicity in chatgpt: Analyzing persona-assigned language models. arXiv preprint arXiv:2304.05335 (2023).\n\nDo et al. (2023) Kimberly Do, Rock Yuren Pang, Jiachen Jiang, and Katharina Reinecke. 2023. ‚ÄúThat‚Äôs Important, but‚Ä¶‚Äù: How Computer Science Researchers Anticipate Unintended Consequences of Their Research Innovations. In Proceedings of the 2023 CHI Conference on Human Factors in Computing Systems (, Hamburg, Germany,) (CHI ‚Äô23). Association for Computing Machinery, New York, NY, USA, Article 602, 16 pages. https://doi.org/10.1145/3544548.3581347\n\nDodge et al. (2020) Jesse Dodge, Gabriel Ilharco, Roy Schwartz, Ali Farhadi, Hannaneh Hajishirzi, and Noah Smith. 2020. Fine-tuning pretrained language models: Weight initializations, data orders, and early stopping. arXiv preprint arXiv:2002.06305 (2020).\n\nDove et al. (2017) Graham Dove, Kim Halskov, Jodi Forlizzi, and John Zimmerman. 2017. UX Design Innovation: Challenges for Working with Machine Learning as a Design Material. In Proceedings of the 2017 CHI Conference on Human Factors in Computing Systems (Denver, Colorado, USA) (CHI ‚Äô17). Association for Computing Machinery, New York, NY, USA, 278‚Äì288. https://doi.org/10.1145/3025453.3025739\n\nDu et al. (2023) Ruofei Du, Na Li, Jing Jin, Michelle Carney, Scott Miles, Maria Kleiner, Xiuxiu Yuan, Yinda Zhang, Anuva Kulkarni, Xingyu Liu, Ahmed Sabie, Sergio Orts-Escolano, Abhishek Kar, Ping Yu, Ram Iyengar, Adarsh Kowdle, and Alex Olwal. 2023. Rapsai: Accelerating Machine Learning Prototyping of Multimedia Applications through Visual Programming. In Proceedings of the 2023 CHI Conference on Human Factors in Computing Systems (Hamburg, Germany) (CHI ‚Äô23). Association for Computing Machinery, New York, NY, USA, Article 125, 23 pages. https://doi.org/10.1145/3544548.3581338\n\nFails and Olsen (2003) Jerry Alan Fails and Dan R. Olsen. 2003. Interactive Machine Learning. In Proceedings of the 8th International Conference on Intelligent User Interfaces (Miami, Florida, USA) (IUI ‚Äô03). Association for Computing Machinery, New York, NY, USA, 39‚Äì45. https://doi.org/10.1145/604045.604056\n\nFeng et al. (2023a) KJ Feng, Quan Ze Chen, Inyoung Cheong, King Xia, Amy X Zhang, et al. 2023a. Case Repositories: Towards Case-Based Reasoning for AI Alignment. arXiv preprint arXiv:2311.10934 (2023).\n\nFeng et al. (2023b) KJ Kevin Feng, Maxwell James Coppock, and David W McDonald. 2023b. How Do UX Practitioners Communicate AI as a Design Material? Artifacts, Conceptions, and Propositions. In Proceedings of the 2023 ACM Designing Interactive Systems Conference. 2263‚Äì2280.\n\nFeng et al. (2023c) KJ Kevin Feng, Tony W Li, and Amy X Zhang. 2023c. Understanding Collaborative Practices and Tools of Professional UX Practitioners in Software Organizations. In Proceedings of the 2023 CHI Conference on Human Factors in Computing Systems. 1‚Äì20.\n\nFeng and McDonald (2023) KJ Kevin Feng and David W McDonald. 2023. Addressing UX Practitioners‚Äô Challenges in Designing ML Applications: an Interactive Machine Learning Approach. In Proceedings of the 28th International Conference on Intelligent User Interfaces. 337‚Äì352.\n\nFeng and Zhang (2022) KJ Kevin Feng and Amy X. Zhang. 2022. From Handofs to Co-Creation: Deepening Collaboration between Designers, Developers, and Data Science Workers in UX Design. In Proceedings of the InContext: Futuring User-Experience Design Tools Workshop at CHI Conference on Human Factors in Computing Systems (CHI ‚Äô22) (New Orleans, LA, USA). https://hcibook.net/incontext/wp-content/uploads/sites/5/2022/04/FromHandofs-to-Co-Creation-Deepening-Collaboration-between-DesignersDevelopers-and-Data-Science-Workers-in-UX-Design-1.pdf\n\nGoogle (2024) Google. 2024. Cloud AutoML Custom Machine Learning Models. https://clou"
    }
}
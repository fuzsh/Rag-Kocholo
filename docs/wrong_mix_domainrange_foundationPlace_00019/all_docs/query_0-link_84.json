{
    "id": "wrong_mix_domainrange_foundationPlace_00019_0",
    "rank": 84,
    "data": {
        "url": "https://www.databricks.com/blog/efficient-fine-tuning-lora-guide-llms",
        "read_more_link": "",
        "language": "en",
        "title": "Efficient Fine-Tuning with LoRA: A Guide to Optimal Parameter Selection for Large Language Models",
        "top_image": "https://www.databricks.com/sites/default/files/2023-08/engineering-08.png?v=1693513423",
        "meta_img": "https://www.databricks.com/sites/default/files/2023-08/engineering-08.png?v=1693513423",
        "images": [
            "https://www.databricks.com/en-blog-assets/static/6ca795289d3a7538bfec1071be17bb7e/CategoryHeader-Engineering-1_0undefined.png",
            "https://www.databricks.com/en-blog-assets/static/6ca795289d3a7538bfec1071be17bb7e/CategoryHeader-Engineering-1_0undefined.png",
            "https://www.databricks.com/en-blog-assets/static/2d27dc491da96fabf86ceab3736b0f9e/avanish-soorundefined.png",
            "https://www.databricks.com/en-blog-assets/static/2d27dc491da96fabf86ceab3736b0f9e/avanish-soorundefined.png",
            "https://www.databricks.com/sites/default/files/inline-images/blogimg1_0.png?v=1693240417",
            "https://www.databricks.com/sites/default/files/inline-images/blogimg2_0.png?v=1693240417",
            "https://www.databricks.com/sites/default/files/inline-images/image3_3.png?v=1693240966",
            "https://www.databricks.com/sites/default/files/2022-05/CategoryIcon-Engineering-1.png?v=undefined",
            "https://www.databricks.com/sites/default/files/2022-05/CategoryIcon-Company-1.png?v=undefined",
            "https://www.databricks.com/en-blog-assets/static/a5ccf0326bc75450e4e5a24ddbd63013/databricks-default1712162038.png",
            "https://www.databricks.com/en-blog-assets/static/a5ccf0326bc75450e4e5a24ddbd63013/databricks-default1712162038.png",
            "https://www.databricks.com/en-blog-assets/static/a5ccf0326bc75450e4e5a24ddbd63013/databricks-default1712162038.png",
            "https://www.databricks.com/en-blog-assets/static/a5ccf0326bc75450e4e5a24ddbd63013/databricks-default1712162038.png",
            "https://www.databricks.com/en-blog-assets/static/724dadbce517935dec78155e4753a219/telco-icon-21715274112.png",
            "https://www.databricks.com/en-blog-assets/static/724dadbce517935dec78155e4753a219/telco-icon-21715274112.png",
            "https://www.databricks.com/sites/default/files/2022-12/gpcicon_small.png"
        ],
        "movies": [],
        "keywords": [],
        "meta_keywords": [
            ""
        ],
        "tags": null,
        "authors": [],
        "publish_date": "2023-08-30T16:01:00",
        "summary": "",
        "meta_description": "Explore efficient fine-tuning of large language models using Low Rank Adaptation (LoRA) for cost-effective and high-quality AI solutions.",
        "meta_lang": "en",
        "meta_favicon": "/en-blog-assets/favicon-32x32.png?v=c9b9916c3b27dc51866c46b79a6e9b88",
        "meta_site_name": "Databricks",
        "canonical_link": "https://www.databricks.com/blog/efficient-fine-tuning-lora-guide-llms",
        "text": "With the rapid advancement of neural network-based techniques and Large Language Model (LLM) research, businesses are increasingly interested in AI applications for value generation. They employ various machine learning approaches, both generative and non-generative, to address text-related challenges such as classification, summarization, sequence-to-sequence tasks, and controlled text generation. Organizations can opt for third-party APIs, but fine-tuning models with proprietary data offers domain-specific and pertinent results, enabling cost-effective and independent solutions deployable across different environments in a secure manner.\n\nEnsuring efficient resource utilization and cost-effectiveness is crucial when choosing a strategy for fine-tuning. This blog explores arguably the most popular and effective variant of such parameter efficient methods, Low Rank Adaptation (LoRA), with a particular emphasis on QLoRA (an even more efficient variant of LoRA). The approach here will be to take an open large language model and fine-tune it to generate fictitious product descriptions when prompted with a product name and a category. The model chosen for this exercise is OpenLLaMA-3b-v2, an open large language model with a permissive license (Apache 2.0), and the dataset chosen is Red Dot Design Award Product Descriptions, both of which can be downloaded from the HuggingFace Hub at the links provided.\n\nFine-Tuning, LoRA and QLoRA\n\nIn the realm of language models, fine tuning an existing language model to perform a specific task on specific data is a common practice. This involves adding a task-specific head, if necessary, and updating the weights of the neural network through backpropagation during the training process. It is important to note the distinction between this finetuning process and training from scratch. In the latter scenario, the model's weights are randomly initialized, while in finetuning, the weights are already optimized to a certain extent during the pre-training phase. The decision of which weights to optimize or update, and which ones to keep frozen, depends on the chosen technique.\n\nFull finetuning involves optimizing or training all layers of the neural network. While this approach typically yields the best results, it is also the most resource-intensive and time-consuming.\n\nFortunately, there exist parameter-efficient approaches for fine-tuning that have proven to be effective. Although most such approaches have yielded less performance, Low Rank Adaptation (LoRA) has bucked this trend by even outperforming full finetuning in some cases, as a consequence of avoiding catastrophic forgetting (a phenomenon which occurs when the knowledge of the pretrained model is lost during the fine-tuning process).\n\nLoRA is an improved finetuning method where instead of finetuning all the weights that constitute the weight matrix of the pre-trained large language model, two smaller matrices that approximate this larger matrix are fine-tuned. These matrices constitute the LoRA adapter. This fine-tuned adapter is then loaded to the pretrained model and used for inference.\n\nQLoRA is an even more memory efficient version of LoRA where the pretrained model is loaded to GPU memory as quantized 4-bit weights (compared to 8-bits in the case of LoRA), while preserving similar effectiveness to LoRA. Probing this method, comparing the two methods when necessary, and figuring out the best combination of QLoRA hyperparameters to achieve optimal performance with the quickest training time will be the focus here.\n\nLoRA is implemented in the Hugging Face Parameter Efficient Fine-Tuning (PEFT) library, offering ease of use and QLoRA can be leveraged by using bitsandbytes and PEFT together. HuggingFace Transformer Reinforcement Learning (TRL) library offers a convenient trainer for supervised finetuning with seamless integration for LoRA. These three libraries will provide the necessary tools to finetune the chosen pretrained model to generate coherent and convincing product descriptions once prompted with an instruction indicating the desired attributes.\n\nPrepping the data for supervised fine-tuning\n\nTo probe the effectiveness of QLoRA for fine tuning a model for instruction following, it is essential to transform the data to a format suited for supervised fine-tuning. Supervised fine-tuning in essence, further trains a pretrained model to generate text conditioned on a provided prompt. It is supervised in that the model is finetuned on a dataset that has prompt-response pairs formatted in a consistent manner.\n\nAn example observation from our chosen dataset from the Hugging Face hub looks as follows:\n\nAs useful as this dataset is, this is not well formatted for fine-tuning of a language model for instruction following in the manner described above.\n\nThe following code snippet loads the dataset from the Hugging Face hub into memory, transforms the necessary fields into a consistently formatted string representing the prompt, and inserts the response( i.e. the description), immediately afterwards. This format is known as the ‘Alpaca format’ in large language model research circles as it was the format used to finetune the original LlaMA model from Meta to result in the Alpaca model, one of the first widely distributed instruction-following large language models (although not licensed for commercial use).\n\nimport pandas as pd from datasets import load_dataset from datasets import Dataset #Load the dataset from the HuggingFace Hubrd_ds = load_dataset(\"xiyuez/red-dot-design-award-product-description\") #Convert to pandas dataframe for convenient processingrd_df = pd.DataFrame(rd_ds['train']) #Combine the two attributes into an instruction stringrd_df['instruction'] = 'Create a detailed description for the following product: '+ rd_df['product']+', belonging to category: '+ rd_df['category'] rd_df = rd_df[['instruction', 'description']] #Get a 5000 sample subset for fine-tuning purposesrd_df_sample = rd_df.sample(n=5000, random_state=42) #Define template and format data into the template for supervised fine-tuningtemplate = \"\"\"Below is an instruction that describes a task. Write a response that appropriately completes the request. ### Instruction: {} ### Response:\\n\"\"\" rd_df_sample['prompt'] = rd_df_sample[\"instruction\"].apply(lambda x: template.format(x)) rd_df_sample.rename(columns={'description': 'response'}, inplace=True) rd_df_sample['response'] = rd_df_sample['response'] + \"\\n### End\"rd_df_sample = rd_df_sample[['prompt', 'response']] rd_df['text'] = rd_df[\"prompt\"] + rd_df[\"response\"] rd_df.drop(columns=['prompt', 'response'], inplace=True)\n\nThe resulting prompts are then loaded into a hugging face dataset for supervised finetuning. Each such prompt has the following format.\n\n``` Below is an instruction that describes a task. Write a response that appropriately completes the request. ### Instruction: Create a detailed description for the following product: Beseye Pro, belonging to category: Cloud-Based Home Security Camera ### Response: Beseye Pro combines intelligent home monitoring with decorative art. The camera, whose form is reminiscent of a water drop, is secured in the mounting with a neodymium magnet and can be rotated by 360 degrees. This allows it to be easily positioned in the desired direction. The camera also houses modern technologies, such as infrared LEDs, cloud-based intelligent video analyses and SSL encryption. ### End ```\n\nTo facilitate quick experimentation, each fine-tuning exercise will be done on a 5000 observation subset of this data.\n\nTesting model performance before fine-tuning\n\nBefore any fine-tuning, it’s a good idea to check how the model performs without any fine-tuning to get a baseline for pre-trained model performance.\n\nThe model can be loaded in 8-bit as follows and prompted with the format specified in the model card on Hugging Face.\n\nimport torch from transformers import LlamaTokenizer, LlamaForCausalLM model_path = 'openlm-research/open_llama_3b_v2' tokenizer = LlamaTokenizer.from_pretrained(model_path) model = LlamaForCausalLM.from_pretrained( model_path, load_in_8bit=True, device_map='auto', ) #Pass in a prompt and infer with the modelprompt = 'Q: Create a detailed description for the following product: Corelogic Smooth Mouse, belonging to category: Optical Mouse\\nA:'input_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids generation_output = model.generate( input_ids=input_ids, max_new_tokens=128 ) print(tokenizer.decode(generation_output[0]))\n\nThe output obtained is not quite what we want.\n\nQ: Create a detailed description for the following product: Corelogic Smooth Mouse, belonging to category: Optical Mouse A: The Corelogic Smooth Mouse is a wireless optical mouse that has a 1000 dpi resolution. It has a 2.4 GHz wireless connection and a 12-month warranty. Q: What is the price of the Corelogic Smooth Mouse? A: The Corelogic Smooth Mouse is priced at $29.99. Q: What is the weight of the Corelogic Smooth Mouse? A: The Corelogic Smooth Mouse weighs 0.1 pounds. Q: What is the dimensions of the Corelogic Smooth Mouse? A: The Corelogic Smooth Mouse has a dimension\n\nThe first part of the result is actually satisfactory, but the rest of it is more of a rambling mess.\n\nSimilarly, if the model is prompted with the input text in the ‘Alpaca format’ as discussed before, the output is expected to be just as sub-optimal:\n\nprompt= \"\"\"Below is an instruction that describes a task. Write a response that appropriately completes the request. ### Instruction: Create a detailed description for the following product: Corelogic Smooth Mouse, belonging to category: Optical Mouse ### Response:\"\"\"input_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids generation_output = model.generate( input_ids=input_ids, max_new_tokens=128 ) print(tokenizer.decode(generation_output[0]))\n\nAnd sure enough, it is:\n\nCorelogic Smooth Mouse is a mouse that is designed to be used by people with disabilities. It is a wireless mouse that is designed to be used by people with disabilities. It is a wireless mouse that is designed to be used by people with disabilities. It is a wireless mouse that is designed to be used by people with disabilities. It is a wireless mouse that is designed to be used by people with disabilities. It is a wireless mouse that is designed to be used by people with disabilities. It is a wireless mouse that is designed to be used by people with disabilities. It is a wireless mouse that is designed to be used by\n\nThe model performs what it was trained to do, predicts the next most probable token. The point of supervised fine-tuning in this context is to generate the desired text in a controllable manner. Please note that in the subsequent experiments, while QLoRA leverages a model loaded in 4-bit with the weights frozen, the inference process to examine output quality is done once the model has been loaded in 8-bit as shown above for consistency.\n\nThe Turnable Knobs\n\nWhen using PEFT to train a model with LoRA or QLoRA (note that, as mentioned before, the primary difference between the two is that in the latter, the pretrained models are frozen in 4-bit during the fine-tuning process), the hyperparameters of the low rank adaptation process can be defined in a LoRA config as shown below:\n\nfrom peft import LoraConfig ... ... #If only targeting attention blocks of the modeltarget_modules = [\"q_proj\", \"v_proj\"] #If targeting all linear layerstarget_modules = ['q_proj','k_proj','v_proj','o_proj','gate_proj','down_proj','up_proj','lm_head'] lora_config = LoraConfig( r=16, target_modules = target_modules, lora_alpha=8, lora_dropout=0.05, bias=\"none\", task_type=\"CAUSAL_LM\",}\n\nTwo of these hyperparameters, r and target_modules are empirically shown to affect adaptation quality significantly and will be the focus of the tests that follow. The other hyperparameters are kept constant at the values indicated above for simplicity.\n\nr represents the rank of the low rank matrices learned during the finetuning process. As this value is increased, the number of parameters needed to be updated during the low-rank adaptation increases. Intuitively, a lower r may lead to a quicker, less computationally intensive training process, but may affect the quality of the model thus produced. However, increasing r beyond a certain value may not yield any discernible increase in quality of model output. How the value of r affects adaptation (fine-tuning) quality will be put to the test shortly.\n\nWhen fine-tuning with LoRA, it is possible to target specific modules in the model architecture. The adaptation process will target these modules and apply the update matrices to them. Similar to the situation with \"r,\" targeting more modules during LoRA adaptation results in increased training time and greater demand for compute resources. Thus, it is a common practice to only target the attention blocks of the transformer. However, recent work as shown in the QLoRA paper by Dettmers et al. suggests that targeting all linear layers results in better adaptation quality. This will be explored here as well.\n\nNames of the linear layers of the model can be conveniently appended to a list with the following code snippet:\n\nimport re model_modules = str(model.modules) pattern = r'\\((\\w+)\\): Linear' linear_layer_names = re.findall(pattern, model_modules) names = [] # Print the names of the Linear layersfor name in linear_layer_names: names.append(name) target_modules = list(set(names))\n\nTuning the finetuning with LoRA\n\nThe developer experience of fine tuning large language models in general have improved dramatically over the past year or so. The latest high level abstraction from Hugging Face is the SFTTrainer class in the TRL library. To perform QLoRA, all that is needed is the following:\n\n1. Load the model to GPU memory in 4-bit (bitsandbytes enables this process).\n\n2. Define the LoRA configuration as discussed above.\n\n3. Define the train and test splits of the prepped instruction following data into Hugging Face Dataset objects.\n\n4. Define training arguments. These include the number of epochs, batch size and other training hyperparameters which will be kept constant during this exercise.\n\n5. Pass these arguments into an instance of SFTTrainer.\n\nThese steps are clearly indicated in the source file in the repository associated with this blog.\n\nThe actual training logic is abstracted away nicely as follows:\n\ntrainer = SFTTrainer( model, train_dataset=dataset['train'], eval_dataset = dataset['test'], dataset_text_field=\"text\", max_seq_length=256, args=training_args, ) # Initiate the training processwith mlflow.start_run(run_name= ‘run_name_of_choice’): trainer.train()\n\nIf MLFlow autologging is enabled in the Databricks workspace, which is highly recommended, all the training parameters and metrics are automatically tracked and logged with the MLFlow tracking server. This functionality is invaluable in monitoring long-running training tasks. Needless to say, the fine-tuning process is performed using a compute cluster (in this case, a single node with a single A100 GPU) created using the latest Databricks Machine runtime with GPU support.\n\nHyperparameter Combination #1: QLoRA with r=8 and targeting “q_proj”, “v_proj”\n\nThe first combination of QLoRA hyperparameters attempted is r=8 and targets only the attention blocks, namely “q_proj” and “v_proj” for adaptation.\n\nThe following code snippets gives the number of trainable parameters:\n\nmodel = get_peft_model(model, lora_config) model.print_trainable_parameters()\n\nThese choices result in 2,662,400 parameters being updated during the fine-tuning process (~2.6 million) from a total of ~3.2 billion parameters the model consists of. This is less than 0.1% of the model parameters. The entire finetuning process on a single Nvidia A100 with 80 GBs of GPU for 3 epochs only takes roughly 12 minutes. The GPU utilization metrics can be conveniently viewed at the metrics tab of the cluster configurations.\n\nAt the end of the training process, the fine-tuned model is obtained by loading the adapter weights to the pre-trained model as follows:\n\npeft_model = PeftModel.from_pretrained(model, adapter_location)\n\nThis model can now be used for inference as any other model.\n\nQualitative Evaluation\n\nA couple of example prompt-response pairs are listed below\n\nPrompt (passed to the model in the Alpaca format, not shown for conciseness here):\n\nCreate a detailed description for the following product: Corelogic Smooth Mouse, belonging to category: Optical Mouse\n\nResponse:\n\nThe Corelogic Smooth Mouse is a wireless optical mouse with a smooth surface. The mouse is equipped with a 1000 DPI sensor and a 1000 Hz polling rate. The mouse is available in black and white.\n\nPrompt:\n\nCreate a detailed description for the following product: Hoover Lightspeed, belonging to category: Cordless Vacuum Cleaner\n\nResponse:\n\nThe Hoover Lightspeed is a cordless vacuum cleaner that is equipped with a lithium-ion battery. The battery is charged via a USB cable. The vacuum cleaner is equipped with a 2-in-1 brush and a turbo brush. The brush is suitable for cleaning carpets and hard floors. The turbo brush is suitable for cleaning carpets and hard floors. The vacuum cleaner is equipped with a dust container that can be emptied via a dust container.\n\nThe model has clearly been adapted for generating more consistent descriptions. However the response to the first prompt about the optical mouse is quite short and the following phrase “The vacuum cleaner is equipped with a dust container that can be emptied via a dust container” is logically flawed.\n\nHyperparameter Combination #2: QLoRA with r=16 and targeting all linear layers\n\nSurely, things can be improved here. It is worth exploring increasing the rank of low rank matrices learned during adaptation to 16, i.e. double the value of r to 16 and keep all else the same. This doubles the number of trainable parameters to 5,324,800 (~5.3 million).\n\nQualitative Evaluation\n\nThe quality of output, however, remains unchanged for the same exact prompts.\n\nPrompt:\n\nCreate a detailed description for the following product: Corelogic Smooth Mouse, belonging to category: Optical Mouse\n\nResponse:\n\nThe Corelogic Smooth Mouse is a high-quality optical mouse with a smooth surface. The mouse is equipped with a 1000 DPI sensor and a 1000 Hz polling rate. The mouse is available in black and white.\n\nPrompt:\n\nCreate a detailed description for the following product: Hoover Lightspeed, belonging to category: Cordless Vacuum Cleaner\n\nResponse:\n\nThe Hoover Lightspeed is a cordless vacuum cleaner that is equipped with a lithium-ion battery. The battery is charged via a USB cable. The vacuum cleaner is equipped with a 2-in-1 brush and a motorized brush. The brush is used to clean hard floors and the motorized brush is used to clean carpets. The vacuum cleaner is equipped with a dust container that can be emptied via a dust container. The vacuum cleaner is equipped with a LED display that shows the remaining battery capacity.\n\nThe same lack of detail and logical flaws in detail where details are available persists. If this fine tuned model is used for product description generation in a real-world scenario, this is not acceptable output.\n\nHyperparameter Combination #3: QLoRA with r=8 and targeting all linear layers\n\nGiven that doubling r does not seemingly result in any perceivable increase in output quality, it is worth changing the other important knob. i.e. targeting all linear layers instead of just the attention blocks. Here, the LoRA hyperparameters are r=8 and target_layers are 'q_proj','k_proj','v_proj','o_proj','gate_proj','down_proj','up_proj' and 'lm_head'. This increases the number of parameters updated to 12,994,560 and increases the training time to roughly 15.5 minutes.\n\nQualitative Evaluation\n\nPrompting the model with the same prompts yield the following:\n\nPrompt:\n\nCreate a detailed description for the following product: Corelogic Smooth Mouse, belonging to category: Optical Mouse\n\nResponse:\n\nThe Corelogic Smooth Mouse is a wireless optical mouse with a smooth surface. The mouse is equipped with a 1000 DPI sensor and a 1000 Hz polling rate. The mouse is equipped with a 2.4 GHz wireless connection and a USB receiver. The mouse is available in black and white.\n\nPrompt:\n\nCreate a detailed description for the following product: Hoover Lightspeed, belonging to category: Cordless Vacuum Cleaner\n\nResponse:\n\nThe Hoover Lightspeed cordless vacuum cleaner is a high-performance device that is easy to use. The device is equipped with a lithium-ion battery that provides up to 40 minutes of runtime. The device is equipped with a high-performance motor that provides a suction power of up to 200 watts. The device is equipped with a 2-in-1 dusting brush and a crevice tool. The device is equipped with a LED display that shows the remaining battery charge.\n\nNow it is possible to see a somewhat longer coherent description of the fictitious optical mouse and there are no logical flaws in the description of the vacuum cleaner. The product descriptions are not only logical, but relevant. Just as a reminder, these relatively high-quality results are obtained by fine-tuning less than a 1% of the model’s weights with a total dataset of 5000 such prompt-description pairs formatted in a consistent manner.\n\nHyperparameter Combination #4: LoRA with r=8 and targeting all linear transformer layers\n\nIt is also worth exploring whether the quality of output from the model improves if the pretrained model is frozen in 8-bit instead of 4-bit. In other words, replicating the exact finetuning process using LoRA instead of QLoRA. Here, the LoRA hyperparameters are kept the same as before, in the new-found optimal configuration, i.e. r=8 and targeting all linear transformer layers during the adaptation process.\n\nQualitative Evaluation\n\nThe results for the two prompts used throughout the article are as given below:\n\nPrompt:\n\nCreate a detailed description for the following product: Corelogic Smooth Mouse, belonging to category: Optical Mouse\n\nResponse:\n\nThe Corelogic Smooth Mouse is a high-quality optical mouse with a smooth surface. The mouse is equipped with a 1000 DPI sensor and a 1000 Hz polling rate. The mouse is available in three colours: black, white and red. The mouse is equipped with a USB receiver andis compatible with Windows and Mac.\n\nPrompt:\n\nCreate a detailed description for the following product: Hoover Lightspeed, belonging to category: Cordless Vacuum Cleaner\n\nResponse:\n\nThe Hoover Lightspeed cordless vacuum cleaner is a compact and lightweight device that is easy to use. The device is equipped with a lithium-ion battery that provides up to 40 minutes of cleaning time. The vacuum cleaner is equipped with a high-performance filter that ensures that the air is cleaned of dust and allergens. The device is equipped with a 2-in-1 dusting brush and a crevice tool that can be used to clean hard-to-reach areas.\n\nAgain, there isn’t much of an improvement in the quality of the output text.\n\nKey Observations\n\nBased on the above set of trials, and further evidence detailed in the excellent publication presenting QLoRA, it can be deduced that the value of r (the rank of matrices updated during adaptation) does not improve adaptation quality beyond a certain point. The biggest improvement is observed in targeting all linear layers in the adaptation process, as opposed to just the attention blocks, as commonly documented in technical literature detailing LoRA and QLoRA. The trials executed above and other empirical evidence suggest that QLoRA does not indeed suffer from any discernible reduction in quality of text generated, compared to LoRA.\n\nFurther Considerations for using LoRA adapters in deployment\n\nIt's important to optimize the usage of adapters and understand the limitations of the technique. The size of the LoRA adapter obtained through finetuning is typically just a few megabytes, while the pretrained base model can be several gigabytes in memory and on disk. During inference, both the adapter and the pretrained LLM need to be loaded, so the memory requirement remains similar.\n\nFurthermore, if the weights of the pre-trained LLM and the adapter aren’t merged, there will be a slight increase in inference latency. Fortunately, with the PEFT library, the process of merging the weights with the adapter can be done with a single line of code as shown here:\n\nmerged_model = peft_model.merge_and_unload()\n\nThe figure below outlines the process from fine-tuning an adapter to model deployment.\n\nWhile the adapter pattern offers significant benefits, merging adapters is not a universal solution. One advantage of the adapter pattern is the ability to deploy a single large pretrained model with task-specific adapters. This allows for efficient inference by utilizing the pretrained model as a backbone for different tasks. However, merging weights makes this approach impossible. The decision to merge weights depends on the specific use case and acceptable inference latency. Nonetheless, LoRA/ QLoRA continues to be a highly effective method for parameter efficient fine-tuning and is widely used.\n\nConclusion\n\nLow Rank Adaptation is a powerful fine-tuning technique that can yield great results if used with the right configuration. Choosing the correct value of rank and the layers of the neural network architecture to target during adaptation could decide the quality of the output from the fine-tuned model. QLoRA results in further memory savings while preserving the adaptation quality. Even when the fine-tuning is performed, there are several important engineering considerations to ensure the adapted model is deployed in the correct manner.\n\nIn summary, a concise table indicating the different combinations of LoRA parameters attempted, text quality output and number of parameters updated when fine-tuning OpenLLaMA-3b-v2 for 3 epochs on 5000 observations on a single A100 is shown below."
    }
}
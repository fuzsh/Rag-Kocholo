{
    "id": "dbpedia_7328_0",
    "rank": 69,
    "data": {
        "url": "https://sewelldirect.com/blogs/learning-center/high-definition-101-resolution-and-scaling-explained",
        "read_more_link": "",
        "language": "en",
        "title": "High Definition 101, Resolution and Scaling Explained",
        "top_image": "https://sewelldirect.com/cdn/shop/articles/high-definition-101-resolution-and-scaling-explained-964637_1200x710.jpg?v=1565647964",
        "meta_img": "https://sewelldirect.com/cdn/shop/articles/high-definition-101-resolution-and-scaling-explained-964637_1200x710.jpg?v=1565647964",
        "images": [
            "https://sewelldirect.com/cdn/shop/files/logoW4_2_400x160.png?v=1613555090",
            "https://sewelldirect.com/cdn/shop/articles/high-definition-101-resolution-and-scaling-explained-964637_600x355_crop_center.jpg?v=1565647964",
            "https://sewelldirect.com/cdn/shop/articles/high-definition-101-resolution-and-scaling-explained-964637_600x355_crop_center.jpg?v=1565647964"
        ],
        "movies": [],
        "keywords": [],
        "meta_keywords": [
            ""
        ],
        "tags": null,
        "authors": [
            "Customer Experience",
            "Trent Crawford"
        ],
        "publish_date": "2019-06-14T15:31:00-06:00",
        "summary": "",
        "meta_description": "High Definition capabilities have truly become ubiquitous, but understanding HD is still relegated to elite geeks, until now. High Definition Means High Resolution When understanding high definition, the first step is to understand resolution, because where there's high resolution there is high definition. Resolution i",
        "meta_lang": "en",
        "meta_favicon": "//sewelldirect.com/cdn/shop/files/Asset_1_4x_94f00034-07bd-4928-ad4d-db0713b06ccc_32x32.png?v=1652907549",
        "meta_site_name": "Sewell Direct",
        "canonical_link": "https://sewelldirect.com/blogs/learning-center/high-definition-101-resolution-and-scaling-explained",
        "text": "High Definition capabilities have truly become ubiquitous, but understanding HD is still relegated to elite geeks, until now.\n\nHigh Definition Means High Resolution\n\nWhen understanding high definition, the first step is to understand resolution, because where there's high resolution there is high definition. Resolution is simply a description of the number of pixels being viewed. Pixels are the smallest parts that make up a full video picture. High Definition generally starts at 1280 x 720 (aka 720i or 720p) pixels and goes up from there. This simply means that there are 1280 pixels if you counted them from left to right, and 720 pixels if you count from top to bottom, for a total of 921,600 viewable pixels. These days, the largest defintion available to consumers is 1920 x 1080 which is also referred to as 1080i or 1080p.\n\n1080i vs. 1080p and Hz Explained\n\nThe 'i' in 1080i stands for 'interlace scan' and the 'p' in 1080p stands for 'progressive scan'. The Hz specification that goes along with that indicates how many times per second the video image refreshes. So a sample resolution would be 1080p30, or 1080p at 30 Hz. There is a big difference between 1080i and 1080p, but what is better?\n\nThe quick answer is that 1080p is much better than 1080i. With progressive scan (1080p), your TV will refresh every single pixel every time it refreshes, which is commonly 60 times per second (or 60Hz). With interlace scan (1080i) your TV will take the 1080 lines of pixels and only refresh the even lines at one time, and the odd lines at one time. So during each refresh only half of the pixels are up to date, and the other half is old by a microsecond. This can cause a very subtle blurring effect to the naked eye when viewing fast moving video.\n\nGenerally speaking, if your TV can not do 1080p, and you have to choose between 1080i and 720p, we'd recommend 720p probably. This is especially true if you have an HDTV that is 32\" or smaller. Chances are you can't tell the difference between 720p and 1080p on a screen of that size.\n\nInterfaces\n\nThere are four main High Definition interfaces that the majority of the public use: Component (RCA), VGA, DVI, and HDMI.\n\nComponent\n\nComponent video generally uses three different RCA connectors for one single High Definition video signal. It is called component because it splits up the three components of video into three different connectors. This is in contrast to its counterpart, composite, which uses only one RCA connector to transfer a video signal, and the connector is typically yellow. Since component splits the video signal up into three connectors, more bandwidth is available than its single connector brother composite.\n\nComponent is an analog signal, and essentially splits the color video signal into Red, Green, and Blue signals, with black and white included in all three. Component can achieve resolutions from 480i to 1080p, but few HDTVs support resolutions this high through the component port.\n\nVGA\n\nVGA is a computer connector that uses the same type of analog signal that component uses, just in a single connector with 15 pins. VGA, or Video Graphics Array has been used in computers since 1987. It is a common mistake to assume that digital signals are better than analog. One advantage of analog is that there is a much more lenient distance limitation. With the latest advancement in VGA, also called QXGA, you can achieve resolutions of up to 2048 x 1536.\n\nAlthough VGA is an interface used mainly for monitors and computers, it is also somewhat common to see VGA interfaces on select HDTVs these days. Converting from VGA to component is also pretty easy since the signals are already similar.\n\nDVI\n\nDVI, or Digital Video Interface, uses a completely different approach to displaying video than its analog predecessors. It is a digital signal that assigns values to each individual pixel using advanced computing, assigning each pixel color and brightness values.\n\nThere are also some other versions of DVI that are analog signal friendly. Because of the big difference between the ways an analog video system and a digital video system renders images, the DVI interface completely seperates the two. DVI-I connectors for instance are 'integrated,' or simply include the capabilities of transferring both analog (the same signal used in VGA) and digital signals.\n\nSince DVI is a video-only signal (and no audio), it is typically used as the digital video connector for computers.\n\nHDMI\n\nHDMI, or High Definition Multimedia Interface, basically took DVI and added audio, making an all-in-one connector that would be convenient for home theater systems. The older version of HDMI out there right now, or HDMI v1.2, uses the DVI video signal and 8 channel surround sound audio in one compact connector. A recent revision of HDMI (aka HDMI v1.3) upgraded the bandwidth to allow 10.2 Gbit/s transfer rates, or 340 MHz. This also allows for Deep Color Support, high definition audio, and more.\n\nAll monitors, TVs, and other video displays are physically built with a set number of pixels. Screen resolutions are the number of pixels in a display. For example, common widescreen computer monitors have a resolution of 1680x1050 and common HDTVs have a resolution of 1920x1080. The first number is the number of columns of pixels and the second number is the number of rows.\n\nHere are some common video resolutions:\n\nStandard definition video:\n\n640x480 or 720x480\n\nHigh definition video:\n\n1280x720\n\n1920x1080\n\nScaling is what is done when your video source resolution doesn't match your display's native or physical resolution. If you have a video running in standard definition, something like 640x480 and you want to display it on a screen that is a higher resolution like 1920x1080 the image needs to be scaled to fit the display. The image is taken from having 480 rows of pixels and making it have 1080 rows of pixels and the number of columns is adjusted proportionately.\n\nIf scaling wasn't done the video would only fill up a small portion of the larger screen. It would still only take up 480 out of the 1080 rows of pixels. Scaling is done all the time without us even thinking about it. Computer monitors and HDTVs have built in scalers to scale any signal that isn't sent at the display's native (generally maximum) resolution. If they didn't we'd run into a lot of problems like the one i just mentioned.\n\nFor example, say you are viewing a regular DVD or a standard definition TV signal on an HDTV. Both the DVD and the standard definition TV signal are going to be a 480 signal (640x480 or 720x480 for a widescreen DVD). If your TV didn't have a scaler, the video signal wouldn't fill up your nice big HDTV. What your TV does is take that lower resolution signal and it scales it, effectively stretching it to the 1920x1080 resolution (or whatever resolution your TV runs) so that it can fill the entire screen.\n\nIf you've ever changed the resolution on your computer to a lower resolution, your computer monitor will do the same thing. It will scale it to fill the entire monitor.\n\nThis has to be done because there are a set physical number of pixels in the display and the image must be scaled to that resolution to be displayed properly.\n\nScalers can be very helpful if you have several video sources and you want to run them all through a single connection on a display. Also, they are often used when your source signal resolution doesn't match a resolution supported by your display.\n\nA scaler by itself does only this. It is not designed to improve the picture quality of a lower resolution image or enhance it in any other way. When a lot of people talk about a scaler they are referring to a device that will scale the resolution as we've just discussed and also has an image processor in it to try and clean up the image.\n\nThe first thing to be aware of is if you are starting with a low resolution image you can never make it look as nice as an image that started at a high resolution. You cannot make your standard TV channels come through as sharp and with as much detail as an HD channel.\n\nA video processor will run the video signal through often many complex algorithms that adjust the video signal to clean up bad noise and to try and sharpen the image. This will often help with the quality of the picture, but it will never be a substitute for true HD sources. Many HDTVs these days have some form of image processor in them to perform these functions. The same standard definition TV signal may look better on one TV than another because of the type of image processing that is done within the TV.\n\nIf you want to squeeze the best quality you can out of your standard definition sources you would want to use a nice video processor and scaler that can clean up the image before sending it to your TV."
    }
}
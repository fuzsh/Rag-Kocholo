{
    "id": "dbpedia_7328_0",
    "rank": 21,
    "data": {
        "url": "https://www.loc.gov/preservation/digital/formats/content/video_source.shtml",
        "read_more_link": "",
        "language": "en",
        "title": "Video Streams as Sources for Files",
        "top_image": "",
        "meta_img": "",
        "images": [
            "https://www.loc.gov/preservation/digital/formats/includes/horzgif.gif"
        ],
        "movies": [],
        "keywords": [],
        "meta_keywords": [
            ""
        ],
        "tags": null,
        "authors": [],
        "publish_date": null,
        "summary": "",
        "meta_description": "The clarity of a digital video file is strongly influenced by the clarity of the source used to produce the file.",
        "meta_lang": "",
        "meta_favicon": "",
        "meta_site_name": "",
        "canonical_link": null,
        "text": "Content Categories >> Still Image | Sound | Textual | Moving Image | Web Archive | Datasets | Geospatial | Email and PIM | Design and 3D | Accessibility | Aggregate | Generic | Browse All Formats\n\nThe clarity of a digital moving image file is strongly influenced by the clarity of the source used to produce the file, when the content is derived from a video signal sent via an appropriate interface from a camera or a pre-existing video recording on tape or disk. If the original source for the content is motion picture film, and the context is an end-user application (in contrast to a specialized professional application), current-day production technology generally requires that the film be transferred to video prior to digital file making.\n\nThere are exceptions to the preceding, mentioned but not explored on this page. First, in some professional settings, film may be scanned on a frame by frame basis to produce a digital representation in a format like DPX_2, OpenEXR, or MXF, which may in turn encode the frames using JPEG 2000 or some other still-frame compression algorithm. Such an approach is part of the digital cinema DCDM specification. The image content in these formats may then be output to end-user digital video files in a manner different from that described below. Second, certain kinds of Computer Aided Design or Computer Aided Manufacturing (CAD-CAM) renderings include motion capabilities and CAD-CAM software often permits creators to \"save as\" video. This capability is especially likely to be employed for three-dimensional drawings that may be rotated to simulate viewing from various points of view. Third (and this is very like the preceding exception), animated content created in a computer authoring format like FLA (Macromedia Flash Project File Format) can be saved as SWF (Macromedia Flash SWF File Format) files and played in Macromedia's proprietary software, or saved in other digital video formats like QuickTime or the MPEG-4 file format. Quality for these types of content will have different dependencies than that of typical camera-generated video.\n\nTable 1 lists a few examples of source video selected from United States standards. The composite example represents the National Television Standards Committee (NTSC) analog broadcast standard, the analog television system used in most of the Americas, Japan, South Korea, Taiwan, and some other nations and territories. It has a nominal frame rate of 30 frames (60 fields) per second. The \"numbers\" for the PAL (much of Europe, Africa, and Asia) and SECAM (France, the Russian Federation, and portions of Africa) standards are similar; they have a frame rate of 25 (50 fields). In the course of producing video files from analog composite sources, the picture data is converted to a digital component format; if compressed bitstreams are being produced, this step precedes compression encoding.\n\nThe bottom four rows of digital component examples are based on the new Advanced Television Systems Committee (ATSC) digital standard, which replaced NTSC in the United States in 2009. It is worth noting that the ATSC digital standard permits other frame rates, e.g., 24 fps for video content derived from American motion picture films. In coming years, the Library of Congress is almost certain to receive 24 fps material, and may receive video at other frame rates as well, e.g., 25 fps non-NTSC, ATSC, and non-ATSC content from European and other nations.\n\nGenerally speaking, \"all other things being equal,\" larger picture sizes offer greater clarity. Video-stream picture sizes are often expressed as horizontal lines and samples per line. These measures are comparable to pixels, but the values will change if the digital-file production process (a) rescales the image, e.g., to quarter-screen size,1 (b) if some lines are dropped, e.g., 483 becomes 480, in order to have a value divisible by 16, helpful in computation, or (c) if the rendering produces square pixels, in which case 720x480 becomes 640x480.\n\nTable 1. Simplified overview of major categories of source material\n\nAnalog/ digital Composite/ component Pixel aspect ratio Aspect ratio Horiz lines Samples per line Scan Name in this document Typical use MOST COMMON SOURCE SIGNALS TODAY A Composite Non-square 4:3 483 720 Interlaced Standard analog (composite) Broadcast, VHS tapes A Component Non-square 4:3 483 720 Interlaced Standard analog (component) Production D Component Non-square 4:3 483 720 Interlaced Standard digital (SDI) Production SELECTED EMERGING ATSC CONFIGURATIONS D Component Square 4:3 480 640 Progressive SDTV-480p Future broadcast (digital) D Component Non-square 4:3 480 720 Interlaced SDTV-480i Future broadcast (analog compatible) D Component Square 16:9 720 1280 Progressive HDTV-720p Future broadcast D Component Square 16:9 1080 1920 Interlaced HDTV-1080i Future broadcast\n\nBeyond picture size, the clarity of a source signal is influenced by other factors, e.g., the type of sampling (e.g., 4:4:4, 4:2:2, or 4:2:0, etc., representing the relative representation of luminance [first number] and chrominance information [second and third numbers]) and the bit depth per sampled channel (e.g., 10 or 8). Older material reformatted from composite video (the first row in Table 1) was often produced using an older generation of camera, and image artifacts may result from the processes used to transcode from composite to component. Source video streams may also be derived from compressed recordings and the type and degree of previously applied compression will also affect clarity.\n\nClarity is also adversely affected by interlacing and enhanced by progressive image display. An interlaced image consists of two video fields, captured a few milliseconds apart. When the subject of scene is moving, the time difference between the two fields means that interlaced images will contain a small amount of blur within a single video frame. The particular methods used to transfer film to video will also influence clarity. New footage will have improved clarity if progressive rather than interlaced frames are produced by new camera models or new datacine or film-scanning devices.\n\nAlthough experts agree that, at the same picture size, the clarity of a progressive image surpasses that of an interlaced image, they are divided when asked to choose between progressive scan and more scan lines (greater picture size). Supporters of 1080i video argue that greater clarity results from having more pixels, even if interlaced, while supporters of 720p argue that progressive scan produces superior results.\n\nVariation in source content for compression is partly represented in the MPEG-2 and -4 standards (and possibly others) via what are called levels. Table 2 illustrates the relation between MPEG-2 levels and quality in terms of the standard's conformance points. The standard permits the placement of signals at intermediate points, e.g., picture sizes like 1280x720, and at data rates below the maxima listed here.\n\nTable 2. MPEG-2 levels and their characteristics"
    }
}
{
    "id": "correct_foundationPlace_00030_3",
    "rank": 59,
    "data": {
        "url": "https://mila.quebec/en",
        "read_more_link": "",
        "language": "en",
        "title": "Home",
        "top_image": "https://mila.quebec/sites/default/themes/mila_v1/og-image-v3.jpg",
        "meta_img": "https://mila.quebec/sites/default/themes/mila_v1/og-image-v3.jpg",
        "images": [
            "https://mila.quebec/sites/default/themes/mila_v1/logo.svg",
            "https://mila.quebec/sites/default/files/styles/focal_crop_square_500_500/public/promotedcontent/11/2024rolnickapplied-ai1000x1000_0.jpg?h=1e66e246&itok=n5z90WFY",
            "https://mila.quebec/sites/default/files/styles/focal_crop_square_500_500/public/promotedcontent/12/2024cohortwebsitemtl-1024x1024_1.jpg?h=3da7126e&itok=U0iEgIDY",
            "https://mila.quebec/sites/default/files/styles/focal_crop_square_500_500/public/promotedcontent/13/dlrlss2023-07-21webcredmaryseboyce8770.jpg?h=a9a8cd40&itok=JTTfg7GY",
            "https://mila.quebec/sites/default/themes/mila_v1/assets/images/icons/search.svg",
            "https://mila.quebec/sites/default/themes/mila_v1/assets/images/icons/cross2.svg",
            "https://mila.quebec/sites/default/themes/mila_v1/assets/images/icons/cross2.svg",
            "https://mila.quebec/sites/default/themes/mila_v1/assets/images/front-animation/green-square.svg",
            "https://mila.quebec/sites/default/themes/mila_v1/assets/images/front-animation/hero-image.png",
            "https://mila.quebec/sites/default/themes/mila_v1/assets/images/front-animation/yellow-quadrant.svg",
            "https://mila.quebec/sites/default/themes/mila_v1/assets/images/front-animation/red-circle.svg",
            "https://mila.quebec/sites/default/themes/mila_v1/assets/images/front-animation/blue-square.svg",
            "https://mila.quebec/sites/default/themes/mila_v1/assets/images/front-animation/blue-quadrant.svg",
            "https://mila.quebec/sites/default/themes/mila_v1/assets/images/front-animation/purple-half-circle.svg",
            "https://mila.quebec/sites/default/themes/mila_v1/assets/images/front-animation/beige-circle.svg",
            "https://mila.quebec/sites/default/themes/mila_v1/assets/images/front-animation/orange-triangle.svg",
            "https://mila.quebec/sites/default/themes/mila_v1/assets/images/front-animation/purple-quadrant.svg",
            "https://mila.quebec/sites/default/themes/mila_v1/assets/images/front-animation/green-corner.svg",
            "https://mila.quebec/sites/default/themes/mila_v1/assets/images/front-animation/white-circle.svg",
            "https://mila.quebec/sites/default/files/styles/focal_crop_square_1000_1000/public/promotedcontent/11/2024rolnickapplied-ai1000x1000_0.jpg?h=1e66e246&itok=R_WWZysZ",
            "https://mila.quebec/sites/default/files/styles/focal_crop_square_1000_1000/public/promotedcontent/12/2024cohortwebsitemtl-1024x1024_1.jpg?h=3da7126e&itok=BKDJJAJ6",
            "https://mila.quebec/sites/default/files/styles/focal_crop_square_1000_1000/public/promotedcontent/13/dlrlss2023-07-21webcredmaryseboyce8770.jpg?h=a9a8cd40&itok=exHdrj67",
            "https://mila.quebec/sites/default/files/styles/news_teaser/public/news/10691/2024icmlnews1920x1080.jpg?itok=QyCYlDOL",
            "https://mila.quebec/sites/default/files/styles/news_teaser/public/news/10684/ai4goodlab-2024-cohort.JPG?itok=HyYs_y3p",
            "https://mila.quebec/sites/default/files/styles/news_teaser/public/news/10675/mila-x-sorintellis.png?itok=6pyerbfz",
            "https://mila.quebec/sites/default/files/styles/fixed_width_480/public/blocks/textwithimagevideo/3031/yoshua-bengio.jpg?itok=9UdtXaw0",
            "https://mila.quebec/sites/default/files/styles/fixed_width_480/public/blocks/textwithimagevideo/3033/2024iahumanity1000x1000-2.jpg?itok=VLWU9b4N",
            "https://mila.quebec/sites/default/themes/mila_v1/logo.svg",
            "https://mila.quebec/sites/default/themes/mila_v1/logo.svg",
            "https://mila.quebec/sites/default/themes/mila_v1/assets/images/logos/canada-logo-white.svg",
            "https://mila.quebec/sites/default/themes/mila_v1/assets/images/logos/quebec-logo-white.svg",
            "https://mila.quebec/sites/default/themes/mila_v1/assets/images/logos/cifar-logo-white.svg",
            "https://mila.quebec/sites/default/themes/mila_v1/assets/images/logos/udem-logo-white.svg",
            "https://mila.quebec/sites/default/themes/mila_v1/assets/images/logos/mcgill-logo-white.svg"
        ],
        "movies": [],
        "keywords": [],
        "meta_keywords": [
            ""
        ],
        "tags": null,
        "authors": [],
        "publish_date": null,
        "summary": "",
        "meta_description": "Mila is a Montreal-based artificial intelligence research institute that brings together researchers from Université de Montréal, McGill University, Polytechnique Montréal and HEC Montréal.",
        "meta_lang": "en",
        "meta_favicon": "https://mila.quebec/sites/default/themes/mila_v1/favicon.ico",
        "meta_site_name": "",
        "canonical_link": "https://mila.quebec/en",
        "text": "Latest Publications\n\nGroup Membership Bias\n\nAli Vardasbi\n\nMaarten de Rijke\n\nMostafa Dehghani\n\nSimple and Scalable Strategies to Continually Pre-train Large Language Models\n\nAdam Ibrahim\n\nBenjamin Thérien\n\nKshitij Gupta\n\nMats Leon Richter\n\nQuentin Gregory Anthony\n\nTimothee LESORT\n\nLORD: Low Rank Decomposition Of Monolingual Code LLMs For One-Shot Compression\n\nAyush Kaushal\n\nTejas Vaidhya\n\nLow Rank Decomposition of matrix - splitting a large matrix into a product of two smaller matrix offers a means for compression that reduces… (see more) the parameters of a model without sparsification, and hence delivering more speedup on modern hardware. Moreover, unlike quantization, the compressed linear layers remain fully differentiable and all the parameters trainable, while being able to leverage the existing highly efficient kernels over floating point matrices. We study the potential to compress Large Language Models (LLMs) for monolingual Code generation via Low Rank Decomposition (LoRD) and observe that ranks for the linear layers in these models can be reduced by upto 39.58% with less than 1% increase in perplexity. We then use Low Rank Decomposition (LoRD) to compress StarCoder 16B to 13.2B parameter with no drop and to 12.3B with minimal drop in HumanEval Pass@1 score, in less than 10 minutes on a single A100. The compressed models speeds up inference by up to 22.35% with just a single line of change in code over huggingface's implementation with pytorch backend. Low Rank Decomposition (LoRD) models remain compatible with state of the art near-lossless quantization method such as SpQR, which allows leveraging further compression gains of quantization. Lastly, QLoRA over Low Rank Decomposition (LoRD) model further reduces memory requirements by as much as 21.2% over vanilla QLoRA while offering similar gains from parameter efficient fine tuning. Our work shows Low Rank Decomposition (LoRD) as a promising new paradigm for LLM compression.\n\nA Bayesian Non-Stationary Heteroskedastic Time Series Model for Multivariate Critical Care Data\n\nZayd Omar\n\nDavid A. Stephens\n\nAlexandra M. Schmidt"
    }
}
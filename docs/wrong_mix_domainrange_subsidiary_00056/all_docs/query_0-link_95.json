{
    "id": "wrong_mix_domainrange_subsidiary_00056_0",
    "rank": 95,
    "data": {
        "url": "https://www.open.edu/openlearn/ocw/mod/oucontent/view.php%3Fid%3D3479%26printable%3D1",
        "read_more_link": "",
        "language": "en",
        "title": "Systems engineering: challenging complexity: View as single page",
        "top_image": "https://www.open.edu/openlearn/pluginfile.php/75462/mod_oucontent/oucontent/639/7de67825/917918d5/t837_1_002i.jpg",
        "meta_img": "",
        "images": [
            "https://www.open.edu/openlearn/theme/image.php/openlearnng/mod_oucontent/1719392919/printable_version",
            "https://www.open.edu/openlearn/pluginfile.php/75462/mod_oucontent/oucontent/639/7de67825/1e2da305/t837_1_i001i.jpg",
            "https://www.open.edu/openlearn/pluginfile.php/75462/mod_oucontent/oucontent/639/7de67825/917918d5/t837_1_002i.jpg",
            "https://www.open.edu/openlearn/pluginfile.php/75462/mod_oucontent/oucontent/639/7de67825/d0ff7f16/t837_1_003i.jpg",
            "https://www.open.edu/openlearn/pluginfile.php/75462/mod_oucontent/oucontent/639/7de67825/b85272f3/t837_1_004i.jpg",
            "https://www.open.edu/openlearn/pluginfile.php/75462/mod_oucontent/oucontent/639/7de67825/a4e48d3c/t837_1_049i.jpg",
            "https://www.open.edu/openlearn/pluginfile.php/75462/mod_oucontent/oucontent/639/7de67825/aeafcbc7/t837_1_005i.jpg",
            "https://www.open.edu/openlearn/pluginfile.php/75462/mod_oucontent/oucontent/639/7de67825/14a9b23b/t837_1_006i.jpg",
            "https://www.open.edu/openlearn/pluginfile.php/75462/mod_oucontent/oucontent/639/7de67825/5bed6896/t837_1_007i.jpg",
            "https://www.open.edu/openlearn/pluginfile.php/75462/mod_oucontent/oucontent/639/7de67825/16233eae/t837_1_050i.jpg",
            "https://www.open.edu/openlearn/pluginfile.php/75462/mod_oucontent/oucontent/639/7de67825/c8be0892/t837_1_008i.jpg",
            "https://www.open.edu/openlearn/pluginfile.php/75462/mod_oucontent/oucontent/639/7de67825/517e1b0e/t837_1_009i.jpg",
            "https://www.open.edu/openlearn/pluginfile.php/75462/mod_oucontent/oucontent/639/7de67825/dad0a21b/t837_1_010i.jpg",
            "https://www.open.edu/openlearn/pluginfile.php/75462/mod_oucontent/oucontent/639/7de67825/97560633/t837_1_011i.jpg",
            "https://www.open.edu/openlearn/pluginfile.php/75462/mod_oucontent/oucontent/639/7de67825/10e9bb9a/t837_1_012i.jpg",
            "https://www.open.edu/openlearn/pluginfile.php/75462/mod_oucontent/oucontent/639/7de67825/f8f4f07d/t837_1_013i.jpg",
            "https://www.open.edu/openlearn/pluginfile.php/75462/mod_oucontent/oucontent/639/7de67825/166b7487/t837_1_014i.jpg",
            "https://www.open.edu/openlearn/pluginfile.php/75462/mod_oucontent/oucontent/639/7de67825/ebaf9549/t837_1_051i.jpg",
            "https://www.open.edu/openlearn/pluginfile.php/75462/mod_oucontent/oucontent/639/7de67825/825ea143/t837_1_015i.jpg",
            "https://www.open.edu/openlearn/pluginfile.php/75462/mod_oucontent/oucontent/639/7de67825/390ab7aa/t837_1_016i.jpg",
            "https://www.open.edu/openlearn/pluginfile.php/75462/mod_oucontent/oucontent/639/7de67825/e10f139c/t837_1_017i.jpg",
            "https://www.open.edu/openlearn/pluginfile.php/75462/mod_oucontent/oucontent/639/7de67825/d02398ce/t837_1_018i.jpg",
            "https://www.open.edu/openlearn/pluginfile.php/75462/mod_oucontent/oucontent/639/7de67825/ae4c5b60/t837_1_019i.jpg",
            "https://www.open.edu/openlearn/pluginfile.php/75462/mod_oucontent/oucontent/639/7de67825/9cb22a07/t837_1_020i.jpg",
            "https://www.open.edu/openlearn/pluginfile.php/75462/mod_oucontent/oucontent/639/7de67825/ebe3722e/t837_1_021i.jpg",
            "https://www.open.edu/openlearn/pluginfile.php/75462/mod_oucontent/oucontent/639/7de67825/30f1d49c/t837_1_ie001i.gif",
            "https://www.open.edu/openlearn/pluginfile.php/75462/mod_oucontent/oucontent/639/7de67825/1d494e2a/t837_1_022i.jpg",
            "https://www.open.edu/openlearn/pluginfile.php/75462/mod_oucontent/oucontent/639/7de67825/184587c0/t837_1_023i.jpg",
            "https://www.open.edu/openlearn/pluginfile.php/75462/mod_oucontent/oucontent/639/7de67825/4176e5e0/t837_1_024i.jpg",
            "https://www.open.edu/openlearn/pluginfile.php/75462/mod_oucontent/oucontent/639/7de67825/25912dcb/t837_1_025i.small.jpg",
            "https://www.open.edu/openlearn/theme/image.php/openlearnng/mod_oucontent/1719392919/maximise_rgb_32px",
            "https://www.open.edu/openlearn/pluginfile.php/75462/mod_oucontent/oucontent/639/7de67825/fd402328/t837_1_026i.jpg",
            "https://www.open.edu/openlearn/pluginfile.php/75462/mod_oucontent/oucontent/639/7de67825/e4ad46a1/t837_1_027i.jpg",
            "https://www.open.edu/openlearn/pluginfile.php/75462/mod_oucontent/oucontent/639/7de67825/06a544af/t837_1_028i.jpg",
            "https://www.open.edu/openlearn/pluginfile.php/75462/mod_oucontent/oucontent/639/7de67825/2cd70ddc/t837_1_029i.jpg",
            "https://www.open.edu/openlearn/pluginfile.php/75462/mod_oucontent/oucontent/639/7de67825/c33c63ca/t837_1_i002i.jpg",
            "https://www.open.edu/openlearn/pluginfile.php/75462/mod_oucontent/oucontent/639/7de67825/d0071eb2/t837_1_i003i.jpg",
            "https://www.open.edu/openlearn/pluginfile.php/75462/mod_oucontent/oucontent/639/7de67825/94b65ded/t837_1_i004i.jpg",
            "https://www.open.edu/openlearn/pluginfile.php/75462/mod_oucontent/oucontent/639/7de67825/7fbf3a67/t837_1_030i.jpg",
            "https://www.open.edu/openlearn/pluginfile.php/75462/mod_oucontent/oucontent/639/7de67825/e2ec9298/t837_1_ue003i.gif",
            "https://www.open.edu/openlearn/pluginfile.php/75462/mod_oucontent/oucontent/639/7de67825/244a08e5/t837_1_031i.jpg",
            "https://www.open.edu/openlearn/pluginfile.php/75462/mod_oucontent/oucontent/639/7de67825/2408f0ed/t837_1_032i.jpg",
            "https://www.open.edu/openlearn/pluginfile.php/75462/mod_oucontent/oucontent/639/7de67825/d5e67371/t837_1_033i.jpg",
            "https://www.open.edu/openlearn/pluginfile.php/75462/mod_oucontent/oucontent/639/7de67825/70d613b3/t837_1_034i.jpg",
            "https://www.open.edu/openlearn/pluginfile.php/75462/mod_oucontent/oucontent/639/7de67825/f1454bfa/t837_1_035i.jpg",
            "https://www.open.edu/openlearn/pluginfile.php/75462/mod_oucontent/oucontent/639/7de67825/9a497605/t837_1_036i.jpg",
            "https://www.open.edu/openlearn/pluginfile.php/75462/mod_oucontent/oucontent/639/7de67825/21f9df0f/t837_1_037i.jpg",
            "https://www.open.edu/openlearn/pluginfile.php/75462/mod_oucontent/oucontent/639/7de67825/96daac9a/t837_1_038i.jpg",
            "https://www.open.edu/openlearn/pluginfile.php/75462/mod_oucontent/oucontent/639/7de67825/abd19382/t837_1_039i.jpg",
            "https://www.open.edu/openlearn/pluginfile.php/75462/mod_oucontent/oucontent/639/7de67825/577905c6/t837_1_040i.jpg",
            "https://www.open.edu/openlearn/pluginfile.php/75462/mod_oucontent/oucontent/639/7de67825/f294045a/t837_1_041i.jpg",
            "https://www.open.edu/openlearn/pluginfile.php/75462/mod_oucontent/oucontent/639/7de67825/b4748d53/t837_1_052i.jpg",
            "https://www.open.edu/openlearn/pluginfile.php/75462/mod_oucontent/oucontent/639/7de67825/fd27adcc/t837_1_042i.small.jpg",
            "https://www.open.edu/openlearn/theme/image.php/openlearnng/mod_oucontent/1719392919/maximise_rgb_32px",
            "https://www.open.edu/openlearn/pluginfile.php/75462/mod_oucontent/oucontent/639/7de67825/01ff5fe1/t837_1_043i.jpg",
            "https://www.open.edu/openlearn/pluginfile.php/75462/mod_oucontent/oucontent/639/7de67825/a82078be/t837_1_044i.jpg",
            "https://www.open.edu/openlearn/pluginfile.php/75462/mod_oucontent/oucontent/639/7de67825/32e858d4/t837_1_045i.jpg",
            "https://www.open.edu/openlearn/pluginfile.php/75462/mod_oucontent/oucontent/639/7de67825/de9c906d/t837_1_046i.jpg",
            "https://www.open.edu/openlearn/pluginfile.php/75462/mod_oucontent/oucontent/639/7de67825/2a39c4d0/t837_1_047i.jpg",
            "https://www.open.edu/openlearn/pluginfile.php/75462/mod_oucontent/oucontent/639/7de67825/36f6d179/t837_1_048i.jpg",
            "https://www.open.edu/openlearn/pluginfile.php/75462/mod_oucontent/oucontent/639/8ff4c822/d3c986e6/ol_skeleton_keeponlearning_image.jpg"
        ],
        "movies": [],
        "keywords": [],
        "meta_keywords": [
            "moodle",
            "Systems engineering: challenging complexity: View as single page | OpenLearn"
        ],
        "tags": null,
        "authors": [],
        "publish_date": null,
        "summary": "",
        "meta_description": "",
        "meta_lang": "en",
        "meta_favicon": "https://www.open.edu/openlearn/theme/image.php/openlearnng/theme/1719392919/favicon",
        "meta_site_name": "",
        "canonical_link": null,
        "text": "Systems engineering: challenging complexity\n\nIntroduction\n\nThe aim of this course is to answer five questions:\n\nWhy is systems engineering important?\n\nWhat is modern engineering?\n\nWhat is systems?\n\nWhat is systems engineering?\n\nWhat approach to systems engineering does the course adopt?\n\nThis OpenLearn course provides a sample of level 3 study in Computing & IT\n\nLearning outcomes\n\nAfter studying this course, you should be able to:\n\nevaluate a specific example or case of a product development process in terms of the ‘waterfall’ life cycle model of software development\n\nclassify new product developments as: fault correction, enhancements, new but similar products, radically different, revolutionary or iconoclastic products\n\nanalyse the causes of a systems failure\n\nidentify and evaluate the importance of the relationships of the factors leading up to system complication and complexity\n\nanswer the question ‘why is systems engineering important?\n\n1 Why is systems engineering important?\n\n1.1 Introduction: what is the problem?\n\nIn late June and early July 2005 a row erupted concerning the operation of a major flagship of government social policy, the tax credit system. Introduced in 2003, it was designed to help those on low incomes and whose social circumstances prevented them from working full-time (Citizens Advice Bureau, 2005). The article reprinted in Box 1 indicates the extent of the political unrest with a system that left families relying on food parcels, and that has been variously described as being ‘in chaos’ and ‘shambolic’. Such problems have become a familiar story and the steady stream of failures seems destined to continue into the future.\n\nDuring the same period a committee of members of parliament issued a warning that of 254 government-funded computer projects currently under development, 70 had been given a ‘red light’, meaning that they will fail to deliver the promised benefits unless immediate action is taken. Of these 70 projects, 8 had been given ‘double-red warnings’ (Guardian, 5 July 2005).\n\nThere is, however, nothing inevitable about such failures, as the example of London's congestion charging scheme demonstrates. In February 2003 those entering a 22 km2 zone in central London in a private car between 7 a.m. and 6 p.m. were liable to a pay a congestion charge of £5. The scheme is enforced by a network of over 700 cameras at 203 sites located at all entry and exit points to the zone, plus some additional mobile patrol and other units. The analogue data from these cameras is streamed back to a central hub and is put into an automatic number plate recognition system which then checks details against prepayment, the Driver and Vehicle Licensing Centre database and, when necessary, sends out penalty notices and manages revenue collection. The system is designed to cope with up to ‘250,000 vehicles [making] 450,000 movements into the charging zone during the period 7 a.m. to 6.30 p.m. with 40,000 vehicles an hour driving into the congestion charging zone during the morning peak (7 a.m.–10 a.m.)’ (Transport for London, 2005). This complex system combining a number of different technologies was developed and became operational within 18 months.\n\nBox 1 Choatic scheme that left families relying on food parcels\n\nDemands mount for overhaul of aid that led to ‘debt and despair’\n\nGordon Brown, the chancellor, faced fresh calls last night for a radical overhaul of the tax credit system after the Treasury published figures showing that administration costs have risen tenfold in four years to £400m.\n\nThe figures also revealed that 65.5 million award notices have been issued in just over two years – more than 10 on average for each of the 6 million households that receive tax credits.\n\nCritics said the system was in chaos and the figures undermined the Treasury's insistence that a series of improvements were beginning to take effect.\n\nThe Liberal Democrat spokesman David Laws, who obtained the figures in answers to parliamentary questions, said low-income families were suffering severe hardship and ministers appeared to be immune to their appeals for help. He said fundamental flaws in the system meant hundreds of thousands of low-income families would still face ‘debt and despair’.\n\nTo claim child tax credit and working tax credit, families must predict their income for the next financial year and tell the Inland Revenue about any changes in circumstances. Payments can reach £5,000 to £6,000 but many have been forced to pay back some or all of their tax credits.\n\nBackbench MPs, who wrote more than 5,000 tax credit complaint letters in the first five months of this year, say many families persuaded to leave benefits and return to work by promises of tax credits take on extra commitments, including childcare costs, only to find that computer or human errors at the Inland Revenue's tax credit centre in Preston result in an overpayment and a demand for the money to be returned.\n\nFrank Field, a former social security minister, tabled a motion in parliament yesterday calling for an independent appeals tribunal to hear cases before the Revenue claws back overpayments.\n\nMinisters revealed that 217,000 households asked for overpayments to be written off in 2004/05 and 54,000 have already made write-off requests in the first two months of the 2005/06 financial year.\n\nDawn Primarolo, the paymaster general, who provided the answers, admitted last month that the system needed to become more sensitive to the needs of low-income families. David Varney, chairman of Revenue and Customs, admitted that the tax credit structure may need to be overhauled if problems persist.\n\nHe told the Financial Times that the policy of annualised tax credits should be reconsidered if it created the same problems year after year. His comments followed criticism from several quarters last month, including the parliamentary ombudsman and Citizens Advice.\n\nCitizens Advice said poor administration and the recovery of overpaid tax credits had led to some families living off only £56 a week plus child benefit.\n\nIn the most extreme cases, families had been threatened with repossession or eviction, and bureau staff had to arrange Salvation Army food parcels for some who were unable to buy groceries.\n\nThe parliamentary ombudsman suggested that Revenue and Customs should write off benefits mistakenly paid to families rather than try to claw them back. In 2003–04 about 1.9 million families were overpaid almost £2bn, either because they did not report rises in income or because of administrative errors. Many were forced to repay the money, causing hardship because most had spent it.\n\nMr Laws said yesterday's figures also showed the £1.9bn official cost for overpayments was understated by £800m because of sums not recovered owing to a £2,500 ‘disregard’ when income rises. So far, only £37m of overpayments have been written off.\n\nThe data also shows that 21,600 compensation payments have been made since April 2004 for bad service.\n\nMs Primarolo said administration costs had remained at 2.5% to 3% since 1999.\n\nBut Mr Laws called for urgent reform. ‘The tax credits system is an increasingly expensive mess. Gordon Brown's flagship tax credits scheme seems to have turned into a bureaucratic nightmare,’ he said.\n\n‘What we now have is a system which is complex and bureaucratic to administer and in which overpayments are endemic – driving people on low incomes into debt and despair.\n\n‘The chancellor should consider bringing back fixed half-yearly awards, as were used with family credit and working families’ tax credit. Until major reform takes place, the current chaos will continue.’\n\nThe Treasury said most problems related to computer and administrative errors in the first few months of the system.\n\nA spokeswoman said: ‘If you are introducing an entirely new tax credits scheme and going from zero to 80% take-up then of course the absolute costs of administering the scheme are going to rise.\n\n‘The key measure is whether the administration costs as a proportion of the total payments have risen or fallen, and in the case of tax credits, they have actually fallen from 3.3% in 1999 to 3% in 2004.’\n\nComedy of errors\n\nFamilies who have appealed against demands by the Inland Revenue for the return of tax credits say an automated system cannot cope with the complexities of real life.\n\nBernadette Reddy-Oaten, a divorced mother of two who lives in Torquay, received an award for child tax credit which the Revenue later decided was too high. She had already spent the money on child care when the Revenue realised its mistake. It demanded she return the cash and even threatened to send compliance officers to ‘interview’ her and check on her finances.\n\nLast year, after the Revenue stopped her tax credits, Ms Reddy-Oaten had to survive on ‘hardship’ payments. These payments were then deemed excessive and she faced demands that these also be returned, even though one was as compensation.\n\n‘I have spent the last two years trying to settle the situation after nearly losing my home in 2004 when they stopped my payments. This is despite letters acknowledging their fault,’ she said.\n\n‘It's shocking when you ring the main tax credit help desk because the staff know very little about the system. It's no wonder the system costs so much to run when I receive two or three award notices a month, usually on the same day, always with different figures, and I have had to ring them weekly to resolve the situation. This happens each year. They end up admitting they owe money and pay it back, only for it to go wrong again.\n\n‘It's an understatement to say it has been a comedy of errors. And I'm a social worker. I know many of my clients have had terrible trouble, standing in phone boxes for hours trying to get through to the help desk.’\n\nLast week the Revenue apologised for mistakes and cancelled the latest demands. She says the Revenue still owes her money but staff tell her they cannot override the computers to correct many of the errors she has suffered.\n\nCritics inside the Revenue say the computer and call-centre systems were basic and unable to cope with the level of inquiries from low-income families, many facing marital strife and redundancy.\n\nCharities suggest that since April tax officials have been allowed by the Treasury to write off overpayments dating back more than two years in an effort to clear a significant number of cases.\n\nMs Reddy-Oaten said: ‘As a social worker, I know the system well and am articulate and confident enough to repeatedly challenge the Inland Revenue; I know too many families who would find it impossible to get what they are owed.’\n\nSource: Phillip Inman, the Guardian, Wednesday 6 July 2005\n\n1.2 The Phoenix project\n\nIt is all too easy to dismiss problems like that being experienced with the tax credit system as being inherent in the design and implementation of computer-based systems. But they are not restricted to computer systems, as the example of the Phoenix project in Box 2 shows.\n\nBox 2 Fly-away drones put robot air force off course\n\nThe dismal failure of a British built drone during the war in Iraq has led the Ministry of Defence to reconsider its plans for a futuristic fleet of unmanned aircraft. Britain lost 23 of its Phoenix surveillance planes, which each cost about £1.5m.\n\nMilitary tacticians believe drones will play an increasingly important role in 21st century conflicts. In Afghanistan drones were used to find and attack Al-Qaeda targets. An armed US Predator was also used in Yemen to assassinate a senior Al-Qaeda fugitive.\n\nBut there is growing controversy over Britain's preferred choices for an £800m drone air force. The Phoenix has such an abysmal record that the army, which uses it for artillery spotting, has called it ‘the Bugger Off’ because of its tendency never to come back.\n\nSince 1996 198 drones have been delivered to the armed forces, but many have been cannibalised for spare parts, some have been mothballed and a large proportion of the working fleet was lost because the planes crashed, were shot down or just went missing in Iraq.\n\nBritain is not buying any more Phoenixes, but despite the proven success of the American-built Predator the defence ministry decided earlier this year that it would buy either an Israeli-designed drone or a US unmanned reconnaissance helicopter. Both would need substantial modification if they were to carry weapons.\n\nThe Phoenix – named after a mythical bird that rises from the ashes, rather than one that crashes and burns – caused problems from the start. Development began in 1982 and it took 16 years before the plane entered service.\n\nIt has to land on its back because sensitive surveillance equipment was mounted on its underside. During trials it landed too heavily, damaging the fuselage.\n\nExasperated engineers fitted an airbag, similar to those found in cars, to cushion the impact. The development programme alone cost £250m.\n\nAnother problem is that the Phoenix is launched by catapult from a vehicle that is so heavy it cannot usually be flown into a battle theatre and has to be sent out from Britain by sea.\n\nBy contrast, American forces lost only four Predators during the war in Iraq. Two were deliberately wasted by being flown over Baghdad to draw fire until they ran out of fuel. One was shot down over the Tigris and Iraqi television showed pictures of soldiers and militia searching the riverbank for the pilot of what they thought was a manned warplane.\n\nThe Phoenix fiasco has led some senior military personnel to urge Geoff Hoon, the defence secretary, to think again and buy an off-the-shelf design with a proven record.\n\nAir Commodore Ron Cook, who was last month appointed director of equipment capability for intelligence, surveillance, target acquisition and reconnaissance, has told colleagues that he is unhappy with the present choices and would like them to be reconsidered.\n\nThe two shortlisted designs are the Fire Scout helicopter and the Silver Arrow. The Fire Scout was built for the US Navy and Marine Corps, but only in small numbers because of concerns about its vulnerability. It is slower than fixed-wing designs and can be easier to shoot down.\n\nThe Silver Arrow is built by Elbit Systems of Israel for the Israeli military. It is being offered to Britain by the French arms contractor Thales.\n\nCritics say the Ministry of Defence is accident-prone when it buys equipment. Hoon has inherited and presided over what insiders describe as a ‘zoo’ of turkeys, white elephants and dodos. Other fiascos include the new Apache attack helicopter. Some of the 67 aircraft ordered, which cost £27m each, are sitting in hangars because of a software problem on training simulators which means there are too few pilots trained to fly them.\n\nEven the SA80, the army's standard issue rifle, caused years of problems. It performed well in Iraq but when it first entered service in 1986 soldiers said it often jammed and it had to be modified several times at a cost of £90m.\n\nSource: Sunday Times, 22 June 2003\n\nSoftware and software-focused systems of the sort highlighted in Box 1 seem to create severe difficulties when it comes to delivering what is required, when it is required and at the estimated cost. As Brooks (1987) put it:\n\nOf all the monsters that fill the nightmares of our folklore, none terrify more than werewolves, because they transform unexpectedly from the familiar into horrors. For these, one seeks bullets of silver that can magically lay them to rest.\n\nThe familiar software project, at least as seen by the non-technical manager, has something of this character; it is usually innocent and straightforward, but is capable of becoming a monster of missed schedules, blown budgets, and flawed products. So we hear desperate cries for a silver bullet – something to make software costs drop as rapidly as computer hardware costs do.\n\nBut, as we look to the horizon of a decade hence, we see no silver bullet. There is no single development, in either technology or in management technique, that by itself promises even one order-of-magnitude improvement in productivity, in reliability, in simplicity.\n\nIt is nearly twenty years since these gloomy predictions were made, but the example reported in Box 1 suggests that we have still not found the silver bullet with which to lay the werewolves of software-focused systems developments to rest. Equally, the example in Box 2 of the Phoenix unmanned surveillance aircraft shows that failure is not confined to software projects.\n\nThe problems that lead to the partial or complete failure of a complex systems project can be grouped into three categories:\n\na misunderstanding or uncertainty about the ‘wants’ being addressed (see Example 1 below)\n\nan inability to design a system that will meet a requirements set (Example 1)\n\nan incomplete or poorly executed implementation (Example 2).\n\n1.3 Example 1 The Workcenter that didn't\n\nAutodesk Inc. is the world's largest supplier of design and engineering software. It currently markets over thirty products but is most famous for its AutoCAD® two- and three-dimensional design and drafting software. The company is the market leader in this type of application, with over 4 million customers worldwide.\n\nThe Autodesk story began in 1982 with a group of programmers, centred on San Francisco, writing code for design software in their spare time. The group demonstrated a cobbled-together revision of what was to become AutoCAD® at an exhibition in March of the same year. The product was finally launched formally in December and was an instant success. By 1987, 100,000 copies of the application had been sold and, by 1999, 2 million. The reasons for AutoCAD's success are:\n\nIt addressed a known, well-understood and fairly standard need – two-dimensional and later three-dimensional drafting. In this respect it was similar to word processing.\n\nThe application is widely needed. Design and drafting are undertaken in businesses large and small throughout the world.\n\nIt made use of the growing power and capability of personal computers. Previously, CAD packages had required a mainframe computer and the few workstations connected to them were situated in special ‘suites’.\n\nIt was (relatively) easy to use. The extensive training required to use mainframe-based CAD packages was not needed.\n\nIt was (therefore) affordable and was sold as a standard off-the-shelf package in software retailers.\n\nThe company employs over three thousand people worldwide and generated revenues of $820 million in the financial year 1999/2000 (Autodesk, 2000). This example concerns four new or enhanced products that were developed during the period 1992–96.\n\nFor a while Autodesk, though anxious to develop new products, had a development approach that was described as ‘chaotic’. There was no formal development model or procedure, and documentation was produced as an afterthought. Recognising the problems that arose as a result of this informal way of developing new products, the company developed a product definition process (PDP). This had the stages shown in Figure 1.\n\nAutodesk's PDP can be characterised as a simplified version of the standard ‘waterfall’ life cycle model of software development shown in Figure 2.\n\nSAQ 1\n\nWhat do you consider to be the major differences between Autodesk's PDP and the waterfall model depicted in Figure 2?\n\nAnswer\n\nAlthough Autodesk's approach has fewer stages than Eisner's version of the waterfall model, the major difference seems to be the absence of any feedback mechanism between the stages of the Autodesk model. Another difference is the review process at the end of each of Autodesk's stages. However, in reality, the role of the review processes was simply to ascertain the completion of each set of documents and did not consider whether their content confirmed that the work undertaken during the stage had been undertaken competently (Abram, 2001). In this respect a parallel can be drawn with many implementations of the quality standard ISO 9000, which seemed more concerned with ensuring that processes had been accurately documented rather than whether they had been competently carried out.\n\nAutodesk used the PDP to guide the development of four new products during the period 1992–96:\n\nAutoCAD Release 13: an update of the original Autodesk package that provides designers and engineers with a personal-computer-based 2D and 3D design and drafting environment and toolset.\n\nAutoCAD LT: a version of AutoCAD with reduced features, supporting 2D drafting and design only.\n\nMechanical Desktop: an add-on package for users of AutoCAD that provides additional features for mechanical designers and engineers.\n\nWorkcenter: a document/data management package implemented on a client-server configuration.\n\nOf these products, three were successful and remain in the Autodesk product portfolio but the fourth, Workcenter, failed to sell in sufficient numbers and was dropped. There were three main reasons for the failure of Workcenter.\n\nFirst, AutoCAD Release 13, AutoCAD LT and Mechanical Desktop were all closely related to Autodesk's existing product. None of them was a significant innovation. Certainly each had new features, but their basis, application area, market and user bases were already well known to Autodesk. This was not the case with Workcenter. AutoCAD and its derivatives are used by designers and engineers essentially on a standalone basis. Workcenter was intended to integrate the work and management of the different people and departments involved in a project. Its user base was less clear.\n\nSecond, as market leader in design and engineering software, Autodesk's file format and interfaces had become a de facto standard. However, document management software has to be capable of accepting many different types of file, produced by a variety of applications. This complicated the technical design of the package enormously.\n\nThird, as suggested earlier, drafting and design processes, at a detailed level at least, are well denned. Engineering drawings are pretty much standard and the way in which they are produced is also standard. They are the graphic equivalent of word processing. This is not the case with document management, where each organisation has its own highly individual way of doing things. The implication of this difference is that whereas a design and drafting package can be used more or less as it stands from the box, the way in which Workcenter was implemented needed to be thought through in a great deal of detail and had to take into account the way that the whole organisation worked. Each implementation had a high degree of individuality. Autodesk's distribution and sales channels were not used to providing the degree of support needed to implement Workcenter, nor was it clear how the extra work was to be paid for.\n\nSticking with the existing situation, the status quo, carries least difficulty and risk (see Figure 3), though it is often, correctly, argued in relation to business development that to ‘do nothing’ is not an option. There are, after all, plenty of examples of enterprises that did nothing only to be driven out of business by their more active competitors. However, at the level of the individual activity, carrying on as normal is least risky.\n\nAt the second level of difficulty and risk is fault correction. Any interference in the operation of a system carries some risk, and the phenomenon of engineer-induced faults is well known. Changing an existing product, even where there is presumed to be an enhancement, can be dangerous as the alterations made previously by Coca-Cola to its formulation have shown. However, Coca-Cola have successfully, and paradoxically, introduced several similar products such as diet, cherry and vanilla versions of its standard soft drink. Generally, though, introducing a new product carries greater risk and difficulty and this is exacerbated if it involves new markets or technologies. Finally, the outcomes of attempts to produce revolutionary products that overturn existing thinking and mores is extremely risky and difficult. The use of systems engineering and an holistic approach can help reduce risk and difficulty in all these types of change and becomes more applicable and important with the degree of novelty being faced. Novelty is also linked to the degree of knowledge in a given situation and is something you will consider later in this course.\n\nSAQ2\n\nThink about the position of AutoCAD R13, AutoCAD LT, Mechanical Desktop and Workcenter and place them on the spectrum in Figure 3.\n\nAnswer\n\nMy positioning of the four products is as shown in Figure 4.\n\nQuestion 1\n\nWhat would you conclude from the Workcenter case and your analysis in SAQ2?\n\nAnswer\n\nThe main thing that strikes me is that perhaps it is a mistake to assume that a single development process – the PDP – could apply equally to different classifications of project involving different degrees of change and risk.\n\n1.4 Example 2: The Bridge of Sighs (and Wobbles)\n\nThe second example of a systems failure is the Millennium Bridge across the Thames linking the St Paul's area of the City of London on the north side of the river with a new cultural centre emerging in Southwark on the South Bank.\n\nThe bridge was to be solely for pedestrians, and was an idea first put forward by David Bell, who was at that time Editor of the influential Financial Times newspaper. The paper sponsored an international design competition which was won by a consortium of the architects Foster and Partners under the leadership of Lord Foster, Sir Anthony Caro, the distinguished sculptor and Ove Arup, the well-known firm of consulting engineers. All three members of the consortium had international reputations for innovation and the quality of their artistic and design capability. Foster and Caro's concept for the bridge was daring and imaginative. Foster, inspired by the light beam of legendary superhero Flash Gordon, envisioned a ‘beautiful blade of light’.\n\nThe bridge was one of a number of similar projects that had been stimulated by the success of Santiago Calatrava's Alamillo Bridge built for Expo'92 in Seville and the Erasmus Bridge, opened in 1996 in Rotterdam and designed by the architects UN Studio. These projects demonstrated that bridges could be potent symbols of a particular time and location and could engage the local community. Caroline Bos, co-founder of UN Studio, recalled, ‘Designing the Erasmus was an incredible experience for us because the local community was so excited about it.’ (Financial Times, 3 June 2000)\n\nThese bridges made use of new technology and materials to support innovative designs that were intended to support growing cultural tourism. The footbridge across the River Tyne at Gateshead uses a new pivoting mechanism that allows it to swing open to enable ships to pass underneath. Roger Ridsdill-Smith, an Ove Arup associate, commenting that the design of the Millennium Bridge could not have been attempted ten years previously, stated, ‘An engineer then would have understood the physics of how the bridge stands up but wouldn't have had the confidence to do it. High powered computers have given us that confidence by allowing us to test our calculations. The structure is so fine that, when people stand on the bridge, they'll feel as if they're hovering over the water.’ The importance of the bridge to tourism was emphasised by Lord Foster: ‘When a foreign visitor comes to London, they will remember buildings like St Paul's or Tate Modern, but also the experience of moving around. Crossing the Millennium Bridge – devoid of cars, buses and trucks – will be one of their most vivid memories.’ (Financial Times, 3 June 2000)\n\nThe ‘beautiful blade of light’ was to be the first bridge constructed across the Thames for over a hundred years, since the building of Tower Bridge in 1894. The design was not without some criticism, most notably from the Swiss architects of the Tate Modern conversion, Herzog & de Meuron (Financial Times, 10 May 2000). An impression of the design of the bridge can be gained from the photographs in Figure 5 and Figure 6.\n\nWork was launched on the sleek steel structure by the Deputy Prime Minister, John Prescott, at a ceremony held on 28 April 1999. The construction work was to be undertaken by a joint venture team of Sir Robert McAlpine and the Danish specialist bridge-builders Monberg Thorsen (The Times, 28 April 1999). The opening was predicted to be April 2000 and the estimated cost was £15 million. This was a considerable increase on the original estimate of £9 million. Most of the funding for the project came from the Millennium Commission, the Corporation of London (through the Bridge House Estates Trust) and the Hong Kong and Shanghai Banking Corporation (HSBC).\n\nThe 330-metre structure was designed for pedestrian use only. The shallow suspension structure is supported on two piers, with a central span of 144 metres. The weight of the bridge is carried by four 120-millimetre-diameter cables on either side attached to the deck by arms and anchored with abutments on the banks. The cables and the deck of the bridge dip by 2.3 metres over the central span. The design had to comply with several requirements. First it had to allow the navigation of a variety of ships up and down the river. At present the largest of these is the 2000-tonne Tracey Bennet, a commercial sand and aggregate carrier, which is 55 metres long. The design team undertook extensive studies of the effects that the impact of such a vessel would have on the bridge. The second consideration was that of stiffness.\n\nThe Institution of Civil Engineers’ Dynamics Design and Practice Guide (Maguire and Wyatt, 2002) points out that ‘dynamics is a far more important subject to civil and structural engineers than it used to be, because structures have become lighter, members (structural elements) more slender. These changes have increased amplitude of vibration and moved frequencies into bands that are more awkward to deal with and more easily perceived by users.’ This consideration is particularly important for footbridges since ‘most walkers on a sensitive bridge instinctively adjust their pace to the response in a way that maximises excitation.’ (Maguire and Wyatt, 2002). This well-known phenomenon is the reason that marching soldiers are ordered to break step when crossing a bridge.\n\nWith these points in mind,\n\nThe original design assumptions regarding dynamic loading by pedestrians were checked. The guidance in the relevant UK bridge design codes had been followed, with additional input from overseas codes and non-statutory documents. The codes advise on the level of force to be assumed in design, and the amount of movement which is acceptable to users of a bridge.\n\nThe UK bridge codes require only vertical excitation to be considered. In the original design all critical vertical modes of vibration were examined. The size of the applied loading recommended in the bridge design code was increased by 33 per cent to give the design additional robustness. The original design also considered loading due to groups of vandals deliberately attempting to excite the bridge.\n\nExtensive additional research was carried out during the original design, including database searches on suspension bridges.\n\n(Arup, 2000a)\n\nHowever, perhaps the consideration that was at the forefront of the bridge designers’ minds was the need for the structure to be both innovative and aesthetically bold. In this Lord Foster and Sir Anthony Caro did not have it all their own way. Sir Anthony blamed planners for interfering with the design and said that they had ‘cut down and trampled on’ innovative ideas (Yahoo, 2000).\n\nThe bridge did not open in April 2000 as originally planned; engineering problems and a strike at the Finnish company supplying some of the steel members delayed completion. The final cost of £18.2 million (Financial Times, 11 July 2000) had risen by £3 million in the year of construction and had doubled the original estimate of £9 million. The Queen performed an opening ceremony on 10 May 2000 but was unable to walk across the bridge as it was still in an unfinished state.\n\nTo celebrate the opening of the bridge to the public on Saturday 10 June, 100,000 people crossed it in a charity walk in aid of the Save the Children fund. Problems were immediately apparent and police closed the bridge as it began to sway alarmingly in only a light wind. A similar problem had occurred earlier in the year when a pedestrian bridge in Paris, linking the Quay d'Orsay with the Tuileries gardens, was closed after the French Minister for Culture, Christine Trautmann, was upset by similar movements (Financial Times, 12 June 2000).\n\nThe Millennium Bridge remained open the following day but only 150 people were allowed to cross at one time. Even so, some visitors complained that they could not walk in a straight line, and of feeling seasick. Others enjoyed the experience. Kay Clapton from Kent stated, ‘If they repair the bridge it won't be the same. The thrill of walking across the Thames as the bridge is swaying is amazing.’ (Financial Times, 12 June 2000). In spite of reactions such as this from the more adventurous, the decision was taken to close the bridge. Ove Arup explained:\n\nThe bridge was open for three days, between Saturday 10 June and Monday 12 June. At times it was very crowded, with some 2000 people along its length. An estimated 80,000–100,000 people crossed on the opening Saturday. When the bridge was crowded, the south and centre spans suffered lateral vibrations large enough to cause pedestrians to stop walking or to hold onto the balustrades to regain their balance. The movement of the south span, between Bankside and the first river pier, was a combination of horizontal and torsional (twisting) oscillations. Observations on the day and studies of video footage show up to 50 mm movement, depending on the number of people walking. The frequency of the movement was about 0.77 cycles/second.\n\nThe centre span moved by up to 70 mm at a frequency of 0.95 cycles/second, mainly horizontally. This part of the bridge was observed to oscillate when occupied by more than about 200 people. The north span did not move substantially.\n\nIt was decided to control the number of people present on the bridge from noon on the Saturday onwards. The concern was the safety of individuals, rather than any risk of structural failure of the bridge itself.\n\nThe bridge was closed on 12 June pending investigation into the cause of the unexpected movements.\n\n(Arup, 2000a)\n\nThere followed weeks of intensive investigation. The Building Research Establishment provided specialist shaking equipment that was used to replicate the forces acting on the bridge. A group of 100 Ove Arup employees crossed the bridge in an attempt to replicate the behaviour of groups of people in response to instability. Vertical and horizontal loading were monitored at key points by the Transport Research Laboratory during the tests, which were filmed from four locations.\n\nAs a result of these tests the engineers went back to the model that they had used to check the original design. The work had been undertaken following best practice principles, which require the design and analysis to be carried out independently by two separate teams. The results of the work of these teams was then checked. In addition, the work was validated by an independent firm, who conducted their own analysis. All this work was replicated using data from the tests.\n\nThe results of this review and comparison show that, apart from the unexpected movement, the bridge is reacting as predicted. There are some marginal – and explicable – differences in the figures but none represents a significant departure from the model, and none could explain the unexpected movement.\n\n(Arup, 2000b)\n\nThe problem of the wobbling bridges seemed to be caused by people walking across it! At one level of analysis this assessment was correct. However, the potential problem caused to the structure of bridges by soldiers marching in step was well known. The problems experienced by the Millennium Bridge had a different origin, to do with induced walking in step and the bridge structure. As groups walk across the bridge the natural cadence of their steps may coincide with the horizontal frequencies of the bridge structure. If the movement brought about by such a coincidence becomes noticeable to those crossing they will tend to adjust their walking pattern in order to feel more stable and, inadvertently, respond in a way that ‘maximises excitation’, as the Institution of Civil Engineers’ Dynamics Design and Practice Guide (Maguire and Wyatt, 2002) points out. According to an expert, Professor Jonathon Wood, who had worked on a similar problem on the Severn Bridge, ‘The Millennium Bridge has almost no lateral stiffness and virtually no torsional stiffness.’ The Times, 31 October 2000)\n\nMidway through September 2000 Ove Arup was in a position to report on its investigations. It confirmed that the wobble had been caused not by the lack of lateral and torsional stiffness in the original design but by ‘large groups of pedestrians [who] had appeared to be stepping in time with the motion of the bridge.’ On its website, Arup notes:\n\nA programme of research was undertaken during the summer of 2000. A solution was then developed using the results of these tests. Arup has warned other bridge designers of their findings and the British Standard code of bridge loading is being updated to cover the phenomenon, now becoming referred to as Synchronous Lateral Excitation.\n\nThe research indicated that the movement was caused by the sideways loads we generate when walking. Chance correlation of our footsteps when we walk in a crowd generated slight sideways movements of the bridge. It then became more comfortable for people to walk in synchronisation with the bridge movement.\n\nThis instinctive behavior ensures that the sideways forces we exert match the resonant frequency of the bridge, and are timed such as to increase the motion of the bridge. As the magnitude of the motion increases, the total sideways force increases and we becomes more correlated.\n\nThe sway movement is not specific to the Millennium Bridge. The same excessive sway movement could occur on other bridges, future or existing, with a lateral frequency under ~1.3 Hz and with a sufficient number of pedestrians.\n\nDuring the investigations Arup discovered that other bridges with completely different structures to the Millennium Bridge have swayed sideways when crowded, for example the Auckland Harbour Road Bridge during a demonstration in 1975 […]. These cases have not been widely published and as a result the phenomenon has not become known to practicing bridge engineers.\n\n(Arup, 2005)\n\nThe engineers considered four solutions. Two of these were rejected early in the analysis. These were solutions to limit the number of people using the bridge at any one time, or to modify walking patterns by introducing obstructions. These options were considered to be undesirable and a last resort to be adopted if no other solution could be found.\n\nThe engineering team estimated that stiffening the structure to alter the ‘natural frequency range of the bridge sway modes away from the frequency range of lateral forces from pedestrians’ would require a nine-fold increase in lateral stiffness in the centre span of the bridge. As a result, heavy bracing would be needed. This solution was rejected because it would have required, in addition, conventional dampers to be fitted. The total weight of such a scheme would have meant further modifications to the bridge's structure and foundations. It would also have altered the appearance of the bridge.\n\nThe solution adopted, subject to a satisfactory trial, was to fit two sorts of damper to absorb the vibrations created in the structure. Viscous dampers, which operate like car shock absorbers, restrict the flow of a viscous liquid contained within the device. These were to be fitted between the existing structure and some additional steel bracing. Tuned mass dampers were also to be used to provide additional control of both vertical and horizontal movements. Preserving the appearance of the bridge was of special importance to the members of the consortium. Ove Arup (Arup, 2000a) stated:\n\nProtection of the elegance of the bridge has been a key aim. The design solution has evolved in weekly meetings with Foster and Partners and discussed in detail with Sir Anthony Caro and Lord Foster. […] The additional structure and dampers under the bridge deck do not affect the side elevation since they are located above the level of the transverse arms and therefore behind the deck edge tubes. The structure which will be visible on the bridge elevation will be:\n\nThe diagonal viscous dampers at the piers. These are located in the plane between the cables and the edge tubes and extend two bays along on each side of the pier.\n\nThe two viscous dampers on each side of the south abutment ramp. These are arranged as an inverted ‘V’ and are located on each corner of the ramp junction where it splits in two and turns parallel to the river bank.\n\nThe underside of the bridge is an important feature of the design. One of the main views of the bridge is from each bank and from river boats, where the soffit is plainly visible. The chevron shaped bracings have been designed with this in mind and are aligned with the bridge centreline and spaced at regular intervals. Similarly, the tuned mass dampers are arranged regularly and placed between the underside of the deck and the top of the transverse arms.\n\nThe proposed solution was tested using the model, the analysis confirming that the dampers would cure the wobble or at least limit movement to ‘acceptable levels’. Further modelling was undertaken to test the solution in exceptional circumstances. Finally, tests were to be carried out on individual components and a prototype test carried out to determine the dynamic behaviour of the bridge with a set of dampers fitted.\n\nIn late November 2001 work finally started on the modifications. Ove Arup paid a reported £250,000 to have two viscous dampers and one tuned mass damper fitted to the central span of the bridge. These paved the way for the full solution, which was estimated to cost £5 million.\n\nSAQ 3\n\nMake brief notes in answer to the following questions:\n\nWhat was the problem that occurred at the brief abortive opening of the bridge in June 2000?\n\nWhat work was undertaken to define the problem in detail?\n\nWhat measures of performance were used evaluate the proposed solutions?\n\nWhat solutions were considered?\n\nHow were the solutions evaluated?\n\nAnswer\n\nThe bridge wobbled to an extent that discomforted some users and might possibly have been hazardous.\n\nExtensive tests were carried out in an effort to understand the problem and to identify its cause. A shaking machine was brought in and groups of Ove Arup employees walked across the bridge in an attempt to replicate the problem. The movement of the bridge was monitored and filmed using video cameras. The results of the tests were fed into the model that had been used to validate the original design.\n\nAdvice was taken from experts in the field and the behaviour of other bridges was researched.\n\nFirst, the proposed solution had to solve the wobble problem to reduce the movement of the bridge to an acceptable level. Second, it had to avoid degrading the appearance of the structure.\n\nFour solutions were considered:\n\nrestricting the number of people crossing the bridge at any one time;\n\nbreaking up the stride pattern of people crossing;\n\nstiffening the bridge;\n\ndamping the bridge's movement.\n\nTwo of the possible solutions were considered to be impractical. The stiffening solution was rejected on engineering and aesthetic grounds. The damping solution was tested using the model. Individual components were tested. Finally a prototype was installed on the bridge and was tested.\n\nMultiple-cause and sign-graph diagramming are techniques that can be used to identify and understand the relationships between the various elements in a situation.\n\nRecently I was discussing with a group of colleagues the importance of reducing the time that it takes to change over from one job to another on a factory floor. In a ‘semi-brainstorming’ mode we quickly drew the diagram shown in Figure 7 on a flipchart.\n\nThe arrows connecting one element in the diagram with another should be read as meaning ‘influences’, ‘affects’ or ‘contributes to’ rather than ‘causes’. For example, the changeover time affects the available capacity of machines in the costs of holding system. We added ‘+’ and ‘−’ signs to indicate the ‘direction’ of the influence of one element on another. The ‘−’ sign indicates that, for example, as changeover time increases, the capacity available for production decreases. The changes in the two elements work in opposite ‘directions’ to one another, so it would also be true that an increase in changeover time would be accompanied by a decrease in the capacity available for production. The ‘+’ is used to designate an influence where movements in two elements work in the same direction, so that if inventory increases, the value of work in progress (WIP) will also rise. Similarly, if inventory falls, the value of WIP will fall.\n\nI said earlier that Figure 7 was the result of a semi-brainstorming session. As such it was an initial statement that needed considerable refinement. It was certainly not the ‘finished product’ but played a valuable part in getting the team started.\n\nSAQ 4\n\nDraw a multiple-cause diagram that would provide an initial understanding of the reasons for the wobble problem on the Millennium Bridge.\n\nAnswer\n\nMy diagram looked like Figure 8.\n\nThe problems of designing software that works have been apparent for forty years and those associated with designing and building bridges for thousands of years. This collective experience has not stopped designers and engineers making mistakes, which are often expensive in terms of time and resources. Autodesk spent millions of dollars developing Workcenter but abandoned the product soon after its launch. The original budget for the Millennium Bridge was £9 million; the final cost, including modifications, was approximately £23 million, approaching three times the first estimate. The opening of the bridge was planned to coincide with the launch of the Tate Modern in April 2000 but failed to meet that date.\n\nIn both cases the designers and engineers were being innovative. They were trying to operate towards the right-hand end of the spectrum of change illustrated in Figure 3. This makes the endeavour risky and, therefore, mistakes more likely. This does not mean that we should avoid risk. Life would be a lot duller if risk avoidance were always the norm. In the two examples, and with the benefit of hindsight, it is clear that both sets of designers failed to conceive their projects appropriately. The software engineers at Autodesk thought that they were building a software package rather than a product that would have to include both software and consultancy. Lord Foster and Sir Anthony Caro wanted to design a beautiful bridge. In doing so they lost sight of its function. In both cases a more holistic methodology might have prevented the problem that occurred. Systems engineering aims to provide such a methodology.\n\n1.5 Increasing complication, complexity and risk: the underlying relationship\n\nFigure 3 showed five commonly encountered problems of effecting different types of change. These are notionally located on a spectrum of change that ranges from no change at all, to complete revolution. The relationship suggested on the figure is that as the degree of change – represented by the different types of problem – increases so, too, do difficulty and risk. Each of the five problems of effecting change can be regarded as a gap between an existing situation and an alternative, desired or preferable situation. To close these gaps, be they the correction of faults or the design and implementation of a completely new, innovative system, requires the deployment and consumption of resources. These resources may be a mixture people, materials, equipment, objects, and information. These two characteristics of change – the nature of the change problem and the use of resources – can be used as the basis for developing the picture shown in Figure 3 into a more complete model of change, and taken together represent the certainty of outcome of a change project.\n\nFigure 9 shows change problems divided into three categories; simple, complicated and complex. The two dimensions of the problem of effecting change, knowing what is required and knowing how to achieve what is required are shown as two axes that each run from high to low – from complete knowledge and certainty to ‘haven't a clue’. Change problems are rendered (relatively) simple by a high degree of knowledge of what needs to be done and how to do it. I have termed the dominant form of knowledge required to address this type of problem as ‘craft knowledge’, since it is the product of a learning that is essentially experiential in character, being a result of meeting and tackling successfully similar problems in the past. This craft knowledge has been termed as ‘tacit’ and may be embedded in individuals or in the organisation itself in informal rules and procedures. In such situations certainty of outcome is high.\n\nAs uncertainty increases the change problem becomes more complicated and less amenable to solution through the application of tacit knowledge. Knowledge of what is needed or how to achieve what is required, or both, is less certain. In such situations those involved are likely to fall back on formal knowledge, either of first principles or that which has been embedded in formal rules and procedures. These complicated change tasks are often solved by the application of traditional engineering knowledge.\n\nThe third type of change problem shown in Figure 9 has high uncertainty as it is complex. Traditional forms of engineering knowledge no longer suffice and it is in application to this type of problem that systems engineering knowledge comes into its own. This type of knowledge is both ‘systemic’ and ‘systematic’. It is systemic because, being based on the systems principle of ‘holism’ , it views the change problem as a whole, resisting the inclination to see it from the perspective of a particular function or discipline. It is ‘systematic’ because it embodies rational frameworks and approaches that reduce uncertainty.\n\nQuestion 2\n\nDo you consider that the degree of knowledge of what is to be done and how it can be achieved explains the increasing risk completely?\n\nAnswer\n\nThis point gave rise to a fierce debate in the original Course Team. The External Assessor argued strongly that there are other factors than the degree of knowledge that contribute to complexity such as the increased coupling of systems and the strength and breadth of their interaction. These give rise to stronger emergent behaviour. While agreeing with this point, others stated that it was the lack of knowledge and, therefore, the failure to predict these factors that gave rise to increasing risk.\n\nAn example of the three types of change problem will help to illustrate the model. Suppose that I want to extend the electrical wiring in my house into the garden to run some lights and a water feature. Designing an extension of this type is well within the capability of a competent electrician, and I am confident that the lights and water feature can be got to work satisfactorily and safely. Craft knowledge of a readily available kind is required to deal successfully with this simple problem.\n\nA more complicated problem is consequent on a decision to install a new security system for the house. This requires more specialist knowledge than a simple extension to existing wiring and is a more difficult design task. I am likely to employ a firm that specialises in this type of project, which may also involve, in supporting roles, other areas of knowledge such as glaziers, plasterers and so on.\n\nThe complex level of change problem can be illustrated by the decision of a building company to design a dwelling with a fully automated control system. Since, as yet, only limited prototypes of this type of accommodation have been developed, the requirements and functionality of such a system are uncertain. Equally vague are the domains of knowledge that would be needed to design and implement such a system successfully. As a result, the outcomes of a project to design a ‘house automation system’ are highly uncertain.\n\nFour observations can be made about the change problem model presented in Figure 9. Uncertainty increases with the degree of turbulence in the environment of the change problem. This turbulence may be associated with change in:\n\nunderlying technologies, either those embodied in the product or service in question or those that are to do with how to achieve what is required\n\nthe business or competitive environment\n\nthe political environment\n\nthe social environment\n\nthe economic environment.\n\nThe existing knowledge base of individuals and organisations will bias their perception of a problem and how it can be solved. The organisation may suffer from unconscious incompetence, not being aware of what it does not know. These two factors, environmental change and perception of the nature of the problem, increase the degree of uncertainty that is associated with a need for change and, consequentially, with its riskiness.\n\nRisk can be denned as the probability of an unexpected outcome. Naturally enough we have an asymmetrical attitude to unexpectedness. We don't mind positive unexpected outcomes but want to avoid nasty ones and their consequences. Our tendency is, therefore, to be risk averse, and only if we are offered a greater return for doing so will we take on extra risk:\n\nthose undertaking dangerous sports are compensated by the psychological return that they provide, or the social status that participation confers\n\npunters on a horse race are offered better odds on outsiders than on the favourite\n\nmotor insurance companies want bigger premiums from drivers with a poor claims history\n\ninvestors in the stock market demand greater returns from shares that are more volatile than the average of the market as a whole.\n\nAs a consequence, though it may be tempting to do so, businesses which undertake only safe forms of change, those that fall within their comfort zone, will not do better than the average. Taking on risk is uncomfortable but necessary because it brings with it greater financial returns and increased knowledge and learning. Systems engineering is a way of reducing the inherent riskiness of the new and complex.\n\nIn the remainder of this section of the course I will discuss the issues for systems engineering associated with the topics of simplicity, complication, and complexity.\n\nFollowing the model in Figure 9, these closely related and, in some instances, overlapping topics will be examined in relation to the difficulty that they create for a systems engineer in terms of what he or she has to do, and the way that the work is performed.\n\n1.6 Increasing complication, complexity and risk: mystery and mechanics\n\nThe winter of 1665/66 must have been exceptionally harrowing for the inhabitants of England. Along with the winter weather, the country suffered an outbreak of the plague. A minor effect of this was a decision by Trinity College Cambridge to close its doors. One of those affected by this decision was a young Fellow, Isaac Newton, who returned home to spend the winter in the Lincolnshire rectory in which he had been brought up.\n\nIsolated in the bleak fens and without college high table and the conviviality of the other Fellows to distract him, he found himself at a loose end, so the 22-year-old Newton buckled down and during the next 12 months:\n\nsolved the binomial theorem\n\ninvented calculus\n\ndiscovered the universal law of gravitation\n\ndeveloped a theory of colour.\n\nEventually the threat of the plague lessened and Newton returned to Trinity, where he was elected Lucasian Professor of Mathematics. The work that he did during the 1665/66 winter became the basis for Philosophiae Naturalis Principia Mathematica (The Mathematical Principles of Natural Philosophy), which was first published in 1687.\n\nThe importance of Newton's work cannot be overestimated, and it is no exaggeration to regard 1665/66 as the beginning of the modern world. The mysterious, magical world of the Middle Ages was replaced by one amenable to rational analysis. Explanation based on myth, magic or the unknowable will of a divinity gave way to observation, calculation and the operation of universal laws. This approach was so successful that 250 years later the French mathematician Henri Poincaré (1854–1912) stated:\n\nIf we know exactly the laws of nature and the situation of the universe at the initial moment, we would predict exactly the situation of that same universe at a succeeding moment.\n\n(Poincaré, 1995 [1903])\n\nThe achievement of Newton, and others who built on his work, was to provide ways of understanding relationships and interactions in the physical, observable world. In doing so they reduced its complexity to mere complication at worst and simplicity at best.\n\nAs suggested earlier in this section, simplicity, complication and complexity are closely related to perception, understanding and the existing knowledge base. If we are faced with a problem that we do not fully understand or one that we cannot see how to solve, we label it ‘complex’. Effectively, we are saying that there is an unknown area that needs to be explored, and a way of dealing with it established. A close conceptual relation of complexity is complication.\n\nThe wristwatch that I habitually wear happens to have a glass back through which its mechanism can be viewed, as shown in Figure 10. It's an interesting world inside the watch case, with lots of tiny parts interacting with one another. It is complicated but not complex. There is no ‘unknown’ element in the nature of the outputs of the watch or how the mechanism achieves them. Although personally I couldn't construct a watch, there are plenty of people with the necessary skills. It's a ‘known problem’, albeit a complicated, tricky one.\n\nMy watch is an illustration of the physical world of objects governed by Newtonian physics and for which, therefore, we have good explanatory models. There are, however, three other ‘worlds’ for which we do not, as yet, have models of equal stature.\n\nFigure 11 shows our level of explanatory confidence as a function of three worlds – the ‘sub-physical’ world of quantum physics, the world of physical objects governed by Newtonian mechanics and a ‘supra-physical’ world of complex systems and which includes a fourth world of human activity systems. In both the sub- and supra-physical worlds there is considerably less success of explanation and therefore greater inherent complexity.\n\nIn 1927, the German particle physicist Walter Heisenberg (1901–1976) put forward the view that at a subatomic level it was possible to determine either the location of a particle or its vector, but not both. In order to study the behaviour of subatomic particles it is necessary to bounce other subatomic particles off them or to get them to collide with other subatomic particles. Either of these two actions destroys what was happening and so leaves it a mystery. Heisenberg's uncertainty principle states that what happens down in the depths of the subatomic world is unknowable.\n\nIn 1968, the German theoretical biologist Ludwig von Bertalanffy published General System Theory (von Bertalanffy, 1968). Although elements of this work had precursors, von Bertalanffy's work was essentially the basis of academic interest in ‘systems’ as a subject. The conceptual basis of systems is discussed in more detail later but one of its cornerstones – emergent properties – is relevant here. This concept states that the properties and behaviour of a system cannot be deduced from studying the properties or behaviours of its elements in isolation. At one level this principle hardly rises above the banal. Everything, be it a physical object or conceptual system, exhibits properties and behaviours that result from the interaction of its constituent elements and which, therefore, are not to be found in those elements in isolation. There are, I believe, no exceptions to this statement. This means that the possession of emergent properties cannot be regarded as a distinguishing feature of a system. However, it is often the case that systems, even simple ones, exhibit behaviours that are unexpected and which surprise their designers, users or observers. Sometimes these behaviours could have been foreseen, but through oversight or negligence were not considered during the design phase of the system. Of equal interest to these preventable emergent properties are those that could not have been foreseen and which are genuinely unexpected. There are external and internal reasons for the occurrence of these. This point will be examined in more detail in Section 3.\n\nExternally caused emergence occurs when a system reacts to its environment in a way that could not have been predicted. There are two origins of unexpected externally caused emergence.\n\nThe system has not been designed to be robust against variation in part of its environment. For example, part-way through the construction of the light-railway system serving London's Docklands area an announcement was made of the massive office development at Canary Wharf. On its own this project completely negated all the carefully calculated predictions of traffic for the new railway. To compound the problem the Canary Wharf announcement was made when the construction of the railway, its rolling stock and associated traffic management systems were all well advanced. It was thought that nothing could be done to accommodate the traffic that the Canary Wharf development was expected to generate but, in the event, the railway has coped remarkably well. Emergent properties do not always mean failure.\n\nThe occurrence of a new element in the system's environment. The example of the Docklands Light Railway illustrated an unexpected variation in one of the important parameters used as the basis for the railway's design. Sometimes, however, a new factor will occur in the environment. The more complex the system, the longer (all other things being equal) the design, development and implementation processes take and therefore the more likely that unpredictable factors affecting the system will occur in the environment. The Iridium system was conceived as providing worldwide telecommunications through a network of geostationary satellites. Problems with the launch vehicles and the performance of the satellites themselves delayed the project, which was 12 years in development. In the meantime the interconnection of networks of terrestrial systems had overtaken the concept. Iridium declared itself bankrupt in August 1999 but may be resurrected as a system for specialist communication.\n\nInternally generated emergence occurs when the elements of the system interact with each other in an unpredicted way. A potent source of this type of emergence is created by the behaviour of humans (most often, but other sentient creatures too) within the system. Once again, the Millennium Bridge provides an example of this. If only the people crossing the bridge had not perversely attempted to compensate for its lateral movement everything would have been fine and the ‘blade of light’ would have remained unsullied by dampers and struts. Emergence can also arise from the unforeseen interactions between the elements of the system and its environment.\n\nBecause we do not know everything which is salient when that knowledge is required, the often unexpected, unpredictable character of emergence means that it remains mysterious, adding to the difficulties of undertaking a complex systems engineering task.\n\n1.7 Increasing complication, complexity and risk: a spectrum of systems intractability\n\nSummarising the discussion in the previous two sections, Figure 12 shows what might be termed ‘a spectrum of systems intractability’. At one end of the scale are simple systems. These are easily understandable and their design and development (relatively) unproblematic. The way in which the various elements in the system fit and work together is clear. Outputs and behaviours are predictable. An example of a simple system is the table shown in Figure 13.\n\nThe table is constructed from 42 parts made from three types of material and manufactured using different processes:\n\ntop – made of plywood\n\nveneer for surface and edges of top (wood)\n\nfour plywood legs consisting of two glued components\n\ntwenty-six cross-headed screws\n\ntwo metal fixings for securing one table to another.\n\nWhile the table can be regarded as a system itself, it may also be useful to think of it as part of the ‘presentation’ room system shown in Figure 14.\n\nThe number of elements in this system has increased enormously. There are, for example, 12 tables in the room. Each of the 20 chairs has 53 separate components but an additional level of complication is introduced by the personal computer, the projector, the hardcopy projection device and the links between them. The relationship between the components of the table is mechanical, that between the tables and chairs spatial, but that between the projector and the personal computer is both physical (through the wires) and informational, expressed through various layers of software. If each table can be viewed as a simple system, the presentation room as a whole has become complicated. It is not complex since its elements can be understood and the relationships between them defined. The behaviour of the system shown in Figure 14 is predictable since it is static.\n\nFigure 15 shows the presentation room system with the addition of people to produce what might be labelled a ‘learning system’. The addition of people to the presentation room turns it from a static system to a dynamic one, and its behaviours from predictable to unpredictable. The result is to push the system towards the right in the spectrum shown in Figure 12. It has become complex rather than being merely complicated.\n\nAs the examples illustrated in Figures 9 and 13–15 suggest, the factors that increase complication, and hence complexity, are as follows:\n\nThe number of separate components in the system. A greater number of components leads to increased complication.\n\nThe variety of components in the system. The component parts in the ‘table system’ are made from only three different types of material.\n\nThe nature of the relationships between the components. In the table system the relationship between the parts was mechanical and chemical (the adherence of the veneers to the plywood top), whereas in the learning system the types of relationship are, in addition, electrical, electronic, informational and psychological.\n\nThe degree of coupling between the parts of the system. The various parts of the table were tightly coupled one to another, but the coupling of some of the components of the learning system is a great deal more loose. The way that, say, the application software in the personal computer interacts with its operating system may not be fully denned, leading to the (all too familiar) system crashes; the interaction of the computer with the projection equipment often has a degree of uncertainty. However, the elements in the system that exhibit the loosest coupling are often the human beings in their interaction with the non-sentient elements of the system and their relationship with one another. For example, the replacement of human workers with automation will increase the coupling of the system.\n\nThe degree to which the operation of the system is dynamic. In a static system, like the table, the relationship of the parts is necessarily fixed. The functioning of the table (as a table rather than, say, a barrier) demands that the relationship of its parts is stable and that it exists in a certain aspect to the floor of the room. The relationship between factors 1–3 listed above and complexity is positive.\n\nThe degree to which the system is robust in the face of variations in its environment. Systems maintain a degree of integrity when confronted or challenged by changes in their environment. They have a designed in or learned ‘envelope of robustness’. Thus, the wristwatch shown in Figure 10 proclaims that it is ‘shock resistant’ and ‘water resistant to a depth of 30 metres’. In addition, its self-winding mechanism provides it with a power reserve of about 40 hours. The robustness of the watch was designed in; other systems – particularly those in which the human element is significant – exhibit learning, which increases robustness. A robust system is more predictable and, therefore, less complex than one which is not as robust.\n\nBox 3 Ants at the edge of chaos\n\nMost people would regard ants as relatively simple creatures but, as a series of experiments undertaken during the mid-1980s demonstrated, their behaviours, when viewed at a system level, can produce surprising and unexplainable outcomes.\n\nThe design of the experiments was simple. First take a nest of ants. Second, place two identical piles of food equidistant from the entrance to the nest. Third, replace grains of food that are removed from the piles so that they remain identical. The question is: once the ants are released, will they all go to one source of food, or divide themselves in some proportion between the two piles?\n\nSince there was no reason for an individual ant (so far as is known) to prefer one food pile over the other, it might be expected that the colony would divide itself evenly, roughly half going to one pile and the remainder to the other. Each ant emerges from the nest, mentally tosses a coin, and makes for one pile or the other. Having been successful and the food pile remaining constant, the ant has no reason to change its behaviour.\n\nHowever, it is known that an ant, having successfully found a source of food, will pass on the good news to others and try to persuade them to follow it by a chemical secretion. Successful behaviour is reinforced by this means and a positive feedback loop established.\n\nThe result should be that eventually all the foraging ants are persuaded to visit just one of the food piles or that the proportions might settle down to be different from a 50:50 split, the exact ratio being established by variations in the foraging pattern.\n\n‘In fact what was seen to take place was a completely different outcome. Even when the experiment had been running for some time, in ant terms, the proportion of the ant population visiting any one site continued to fluctuate in an apparently random fashion. The proportions averaged out at one half, but this precise outcome was hardly ever observed, and the proportion was subject to constant change. Once a large majority of ants had visited one of the sites, the outcome tended to stay reasonably stable and exhibited small variations around that proportion for some considerable time. But the majority was always eroded and the ants switched to visiting the other site. Sometimes these shifts were not only very large – from, say, an 80:20 division at one pile to the reverse outcome of 20:80 – but also rapid.’\n\nThe experiments were varied to see whether a different outcome could be induced. Different species of ants were used with no difference. To eliminate possible differences in the food sources a single pile was used. Two separate bridges were set up at identical distances from the entrance to the nest and the numbers of ants crossing each were counted. Again, the results replicated the original experiments.\n\nThe behaviour of the ants was of interest not only to biologists, and Alan Kirman, then at the European University Institute in Florence, began to look at the problem from a different view point.\n\n‘Kirman set up a theoretical model which gives an excellent account of the observed behaviour of the seemingly perverse ants […] An ant coming out of the nest follows one of three possibilities: it visits the food pile it previously visited; it is persuaded by a returning ant to visit the other source; or, of its own volition, it decides to try the other pile itself. And this is almost all that is required to explain the complex and seemingly baffling phenomenon of the fluctuations in the proportions of ants visiting the respective piles.’\n\nAdapted from Ormerod (1998)\n\nThe greater the value of the factor, the more complicated and complex the system being considered or designed. In the case of what I have termed the degree of coupling, the tightness of coupling is inversely correlated with complication and complexity. The six factors combine to determine, in part, the degree to which the behaviour of the system and its outcomes are unpredictable. This, in turn, influences the extent to which the system is complex rather than complicated or simple. A degree of dynamism is shown by the watch in Figure 10. In this system the functioning depends on the parts moving in relation to one another in a predetermined and predictable way. The inclusion of sentient creatures within the boundary of a system introduces a greater degree of dynamic behaviour. Even when these creatures may be regarded as relatively programmed or ‘hard-wired’ in their reaction to stimuli the dynamic behaviours that result from their interaction can be unexpected and initially unexplainable, as the example given in Box 3 suggests. However, perhaps it is our expectation which is the problem rather than the behaviour of systems themselves. We should anticipate variation, and as the current saying is, we should ‘Learn to expect the unexpected.’\n\nSAQ 5\n\nDraw a multiple-cause diagram that identifies the various relationships involved in complication and complexity.\n\nAnswer\n\nMy diagram looked like Figure 16.\n\n1.8 Increasing complication, complexity and risk: are systems becoming more complex?\n\nFigure 17 shows the evolution of two commonly encountered applications of systems – for personal transport and for the reproduction of recorded music. In both cases the degree of complexity of the systems application has increased over time. One of the main reasons for this is technology push. The importance of technology can be related to the stages of the product life cycle shown in Figure 18. The hypothesis behind Figure 18 is that the sales of a given product or product type (such as vinyl long-playing records) will in any given market follow the characteristic sales curve illustrated. The curve is divided into the stages of introduction, growth, maturity and decline, though more elaborate subdivisions are sometimes employed. No scales are shown on the axes of the graph since it is intended as a generic model that indicates internal relativities. During the introduction phase the core product technology is established along with its basic functions. It is rare for a product to address basic functions that are completely novel. For example, it is evident from the example shown in Figure 17 that the models for the early automobiles, or ‘horseless carriages’ with which they shared many components, were the earlier horse-drawn phaetons and other forms of carriage. The early automobiles inherited the functionality of horse-drawn carriages.\n\nFrom that time until about 1910, the concept of the automobile was elaborated and its performance extended. The industry then entered the rapid growth phase, largely as a result of Henry Ford's application of assembly-line technology to car production (Ford, 1922; Sloan, 1965). By the 1950s the industry had entered the mature phase in the USA and technological innovation was directed at product sub-systems or improvements to the basic production methods. This process has continued up to the present time but, in addition, the development of computing and communications technologies has affected both cars and the way in which they are made. The universality of these technologies has meant that they have driven change in almost every industry and have, for example, fundamentally altered music reproduction, first with the introduction of digital compact discs and then with digital music recordings available for download from internet sites.\n\nThe application of computing and communications technologies to both automobiles and music reproduction illustrates a general tendency of systems to become more connected and, as a consequence, for the degree of coupling to increase. A car being driven is connected to traffic monitoring, police surveillance, and possibly global positioning systems, in ways that would have been impossible ten years ago. Similarly, the MP3 music reproduction system works by downloading into its memory music from a personal computer that is likely, in turn, to have been obtained from a website. Previously, cars and music systems were connected to their environment by the actions of humans. These interposed an intelligent buffer between the various elements in what becomes progressively a ‘system of systems’. One effect of this is to make the operation of any one system less complex since human actions filter out variation. However, human actions are themselves also a source of variation and complexity which make the operation of the system of systems more uncertain. The replacement of human by technological links increases the size of the system and reduces human interaction. As a result, people are relegated to a position on the periphery of increasingly massive systems over which they can exert less and less control.\n\n1.9 Increasing complication, complexity and risk: summary\n\nThe three levels of change problem, simplicity, complication and complexity, can be associated with craft, engineering and systems engineering knowledge. The three categories of change problem represent different levels of uncertainty of what needs to be done and how to do it. The greater uncertainty brings increased risk. Although we tend to be risk averse we will take on greater risk if the returns are commensurate with doing so.\n\nHuman experience can be divided into three worlds. The physical world of direct experience obeys Newtonian laws of cause and effect. The ways in which this world operates is a ‘known problem’. Its operation may be complicated but good explanatory models exist to help predict outcomes. Complexity may be added to the complication of the physical world by the actions or interactions of sentient creatures, especially human beings. There do not, as yet, exist good models for the operation of the sub-physical world. This may be a result of it being inherently unknowable or due to ‘hidden variables’. Whatever the reason, the behaviour of the sub-physical world remains mysterious and complex. The operation of the supra-physical world of systems, and increasingly of interconnected systems of systems, is similarly mysterious and becoming more so as interconnectedness and, therefore, complexity grow.\n\nSAQ 6\n\nSummarise the reasons why systems engineering is important.\n\nAnswer\n\nThere are three justifications for systems engineering. The first is to prevent failure. The second justification arose from the need to be able to manage the increasing complexity of systems. The third reason for adopting systems engineering is the fact that customers for systems and stakeholders in them are becoming more unforgiving.\n\n2 What is engineering?\n\n2.1 The development of engineering\n\nEngineering is one important component of systems engineering. In this topic I will examine the development of engineering before presenting a modern view of the subject. Section 3 will then pick up and discuss the idea of systems engineering.\n\nWilliam Shipley, a drawing master from Northampton, was instrumental in founding ‘the Society Instituted at London for the promotion of Arts, Manufactures and Commerce’ in 1754. This later became the Royal Society for the encouragement of Arts, Manufactures and Commerce and moved to splendid Adams brothers-designed quarters at the Adelphi in London's Strand. The society was to become the inspiration for a number of intellectual bodies based in Britain's fast-growing provincial towns and cities.\n\nForemost amongst these was the Lunar Society of Birmingham, a group established by 14 prominent local businessmen and others interested in discussing science and technology. Of this society Schofield (1963) states:\n\nMore than any other single group, the Lunar Society of Birmingham represented the forces of changes in late eighteenth century England, for the Lunar Society was a brilliant microcosm of that scattered community of provincial manufacturers and professional men who found England a rural society with an agricultural economy and left it urban and industrial. […] Together they comprised a clearing-house for the ideas which transformed their country materially, socially, and culturally within a generation. They were men of broad interests and their discussions ranged widely, but their major mutual interest was the sciences, pure and applied – particularly as applied to the problems of industry.\n\nThis characteristic of the group – its intellectual eclecticism – was typical of the pioneers of the industrial revolution and persisted well into the middle of the nineteenth century. For example, Isambard Kingdom Brunei was responsible for the design of bridges, the Great Western and numerous other railways, locomotives and rolling stock, stations, and the steamships the SS Great Eastern and Great Britain (Rolt, 1970).\n\nHowever, the development of scientific and technological knowledge during the nineteenth century meant that, increasingly, it became impossible for one person realistically to pursue an interest in all subjects. This trend was accentuated by the formation of specialist institutes for each discipline, which provided an increasing array of pigeonholes into which engineers could fit themselves. Of particular significance was the separation of science from engineering and technology. The way that this bifurcation occurred is described in Box 4.\n\nBox 4 The separation of science and technology\n\nIt was largely due to the reforming zeal of revolutionary and Napoleonic France that the sciences were first organized into their present, tolerably coherent, disciplines. But there is no question that during the nineteenth century the German universities acquired – and deserved – enormous prestige as the world's leading schools for science teaching and research. The ideal of Wilhelm von Humboldt (1767–1835), perhaps the main architect of the success of the German university system in the nineteenth century, was that a university should advance pure learning and that students should acquire a love of disinterested learning – research – by carrying it out for themselves under a master who was an acknowledged scholar. Practical or vocational studies, that were suggested to require no more than the memorizing of facts, must be excluded. This was an educational creed that went back to Plato and Aristotle and was congenial to the governing elements of all European nations. But von Humboldt's ideal could never be fully realized. As the century wore on, specialization and the educational requirements – the need for more and more school teachers, lawyers, doctors, civil servants and administrators – of a rapidly developing nation state entailed that the Humboldtian programme was progressively diluted. Nevertheless the university ideal of disinterested learning remained and is strongly upheld today. In this was the notion of ‘pure science’ born. But in practice the ‘pure science’ was defined administratively; it was the science pursued in universities and not in technical colleges. The model of pure science was imported into America, Britain and other countries by the many students who, having studied at German universities, returned home understandably enthusiastic about German science, research and education. The German technical colleges (Technische Hochschulen) and later technical universities could emphasize the importance of free research but they could hardly stress ‘pure’ learning. They, almost certainly, had far less influence on foreign opinion.\n\nThis is not, in any way, a criticism of the admirable system of higher education in Germany. The point is this: a large part of the history and philosophy of science, at least until recently, has been formed in the height of German university practice, a practice substantially followed in the rest of the civilized world. In other words, the history of science reveals the effects of external bureaucratic agencies. To some extent, then, the exclusion of technology from the history of science is a consequence of the exclusion of technology from the German universities.\n\nSource: Cardwell (1994, pp. 8–9)\n\nOne result of the process described in Box 4 has been that scientists and philosophers of science have been much more active than technologists and engineers, and philosophers of technology. Philosophers of science, from Francis Bacon (1561–1626) and René Descartes (1596–1650) onwards, have always given prominence to the justification of method, and the claims of various approaches to the accumulation of scientific knowledge have been fiercely debated. In contrast, discussion of engineering method has been a relatively recent phenomenon, having arisen largely in the twentieth century and within the context of systems engineering. Philosophers concerned with technology have been content to examine the broad sweep of development rather than examining methodological development at the level of the project (Basalla, 1988), have examined technology within a social context (Kranzberg and Davenport, 1972) or have adopted an overtly polemical approach (Winner, 1977) in a tradition going back to Mary Shelley's Frankenstein (Shelley, 1992 [1818]).\n\nDefinitions of engineering are often revealing. Thus R.E. Doherty, President of the Carnegie Institute of Technology (now Carnegie Mellon University) stated: ‘Engineering is the art, based primarily upon training in mathematical and physical sciences, of utilizing economically the forces and materials of nature for the benefit of man’ (Rae, 1960).\n\nQuestion 3\n\nWhat does Doherty's definition of engineering reveal about its status?\n\nAnswer\n\nThis is a definition that does engineering no favours since it is couched almost entirely in terms of other subjects. First, engineering is not a subject in its own right but is an ‘art’. Second, training (notice, not education) for becoming an engineer is mathematics and the physical sciences. Thus engineering is relegated to the status of being about the application of mathematics and the physical sciences to the benefit of humans.\n\nDoherty's definition is typical of many ‘conventional definitions of engineering that suggest that it is the application of scientific principles to the optimal conversion of natural resources into products and systems for the benefit of mankind’ (Sage, 2000). There are two major problems with definitions of this type. The first is that engineering denned as ‘the application of science’ fails to recognise that in many instances a technology has been developed and implemented without any scientific foundation. Second, there are four major resource areas or sources of capital that need to be considered:\n\nnatural resources, or natural capital\n\nhuman resources, or human capital\n\nfinancial resources, or financial capital\n\ninformation and knowledge resources, or information and knowledge capital.\n\n2.2 A modern view\n\nModern attempts to define engineering recognise the importance of the resources identified by Sage, and that the subject can be divided into two components: engineering knowledge – the ‘know-what’, and engineering process – the ‘know-how’. Engineering knowledge is:\n\n[…] the growing body of facts, experience and skills in science, engineering and technology disciplines; coupled to an understanding of the fields of application. […] It is mainly ‘experience-based’ knowledge, which is more difficult to describe and communicate than ‘codified knowledge’ because it must be put into the context of an application.\n\nEngineering knowledge ranges from the more traditional such as civil, mechanical, electrical, chemical, automotive, aeronautical, to the newer such as electronic, communications, medical, bio-technical. These subjects are being added to regularly.\n\n(Royal Academy of Engineering, 2000)\n\nEngineering know-how is seen in terms of problem solving in an output standard for engineering graduates published by the UK Engineering Professors’ Council (EPC, 2000). This standard is:\n\nbased on the generic procedures carried out by an engineer in solving an engineering problem and delivering the solution. Engineering problem solving is an iterative task involving creativity and the application of knowledge and understanding. Broadly, an engineer needs to be able to identify and describe the problem that is to be solved … The solution will have a specification with parameters that require evaluation, a process that relies on the engineering skills of conceptualisation, determinable modelling and analytical representation. Delivery of the specified solution draws on other skills including the verification of conceptual models by experimentation with physical models.\n\nThe standard is structured around four types of model and their transformations.\n\nConceptual models describe needs, ideas and the current situation. They are usually pictorial: flowcharts, schematics, state transition diagrams, decision trees, etc.\n\nDeterminable models help to evaluate solutions. ‘Determinable’ is an unfortunate choice of word since it means ‘capable of being definitely ascertained’ (OED, 1989), whereas some useful engineering models are stochastic, i.e. based on statistical probability. The intended sense is ‘calculable’. Calculable models include finite element analyses, simulations, and many analyses of key ‘figures of merit’ such as costs and benefits, risk, reliability, maintainability, service levels, capacity, performance, etc.\n\nPhysical models help to investigate critical aspects of solutions. They include prototypes and proofs of concept, scale models and mock-ups, ‘breadboards’, etc.\n\nSpecification models define a solution precisely. They take forms such as formalised languages and bill-of-materials databases. The first three types of model may also specify; for example, conceptual models are often used to specify information systems requirements. This is only possible when a model has a formal structure (a meta-model) whose elements can be supported by precise definitions. Free-format models can describe but not specify; you should watch out for sketches masquerading as specifications. Without the definitions, it is not possible to verify the model, for example by comparing it with samples of real information and by checking that each system output can be derived from it. A secondary function of the definitions is to communicate any concepts for which the diagram notation is not rich enough.\n\nIn the same year a joint Royal Academy of Engineering/Engineering Council Working Group clearly defined the engineering process as ‘the creative process which applied knowledge and experience to seek one or more technical solutions to meet a requirement, solve a problem, then exercise informed judgement to implement the one that best meets constraints’ (Royal Academy of Engineering, 2000, p. 32). The Working Group summarised the engineering process with the diagram shown in Figure 19.\n\nEngineering as a process can also be viewed from a ‘design’ perspective. For example, Pugh (1991) sets his methodology, shown in Figure 20, within the context of a design problem. He makes an important distinction between traditional engineering which leads to practical design and his concept of ‘total design’.\n\nOne way to consider the characteristics of a phenomenon is to contrast it with another, closely related phenomenon. The distinctions between engineering and scientific process are set out in Box 5.\n\nBox 5 The differences between engineering and scientific processes\n\nEngineering process Scientific process Invention, design, production Discovery (mainly by controlled experimentation) Analysis and synthesis of designs Analysis, generalisation and synthesis of hypothesis Holism, involving the integration of many competing demands, theories, data and ideas Reductionism, involving the isolation and definition of distinct concepts Activities always value-laden Making more-or-less value-free statements The search for, and theorizing about, processes (e.g. control, information, networking) The search for, and theorizing about, causes (e.g. gravity, electromagnetism) Pursuit of sufficient accuracy in modelling to achieve success Pursuit of accuracy in modelling Reaching good decisions based on incomplete data and approximate models Drawing correct conclusions based on good theories and accurate data Design, construction, test, planning, quality assurance, problem-solving, decision-making, interpersonal, communication skills Experimental and logical skills Trying to ensure, by subsequent action, that even poor decisions turn out to be successful Using predictions that turn out to be incorrect to falsify or improve the theories on which they were based\n\nSource: Royal Academy of Engineering (2000, p. 34)\n\nSAQ 7\n\nExamine critically the two lists of key processes in Box 5. List your comments on them. How would you summarise the difference between engineering and science?\n\nAnswer\n\nThere seem to me to be two levels of comment that can be made about the engineering process list. The first level concerns the structure of the list as a whole.\n\nNot all of the items on the list are processes. Neither ‘Holism’ nor ‘Activities always value-laden’ are processes. They would, perhaps, be better described as attributes.\n\nThe items on the list seem to be uneven in level. For example, the list includes ‘design’, the ‘analysis and synthesis of designs’ and then ‘design, construction’.\n\nAs ‘2’ above suggests, there is some repetition of items.\n\nThe list lacks structure – or at least one that is discernible.\n\nSome of the statements are disputable. For example, what is meant by the statement ‘Activities always value-laden'? Why in the last item on the list do engineers only ‘try’ to achieve an outcome?\n\nAs far as the scientific processes list is concerned:\n\nCertainly science tries to make discoveries, but a better way of putting this aim would be to state that it ‘seeks to add to knowledge’.\n\nScience is concerned with hypotheses and research problems and its progress is driven by formulating and testing them.\n\nReductionism does not just concern ‘concepts’.\n\nScience is not ‘value-free’.\n\nExperimental and logical skills, though necessary, are not a process.\n\nI would consider that the aim of science is ‘understanding’ and to advance knowledge whereas engineering is purposeful, seeking to produce effective solutions to real-world problems.\n\n2.3 Summary and conclusions\n\nThis topic has addressed the question ‘What is modern engineering?’ The conclusion must be drawn that, until recently, engineers were content with fairly simplistic definitions of their profession, thinking that it consisted of little other than craft skills or practical experience grafted on to a knowledge of mathematics and appropriate natural sciences. It has been methodologically naive, and definitions of the processes of engineering either lack detail (Figure 19) or have been constructed in terms of ‘design’ (Figure 20).\n\nBy contrast, ‘systems engineering’ has a plethora of methodologies – perhaps too many for its own good – and in Section 4 I will examine the development of systems engineering and its methodologies. The next section examines the second component of systems engineering by answering the question ‘What is systems?’\n\n3 What is systems?\n\n3.1 Introduction\n\nAs you would expect, since this course deals with systems engineering, it embodies the principles and methods associated with a systems perspective. So it is important that you understand systems and the systems perspective at the beginning of the course.\n\nTo have engineered a system successfully, all its features – the technology, control systems, people and related aspects of the physical environment – have to contribute to the achievement of its objectives. In other words, it has to operate in an integrated, coherent way. It has to meet the requirements of all stakeholders. There are also things that it must not do. For example, it should not exceed its agreed or projected running costs; it should not, in achieving its requirements, wantonly harm individuals or the physical environment, and so on. These sets of requirements and constraints can be represented by the three models shown in Figure 21.\n\nSAQ 8\n\nThe demands, choices and constraints associated with a system can arise from any of the stakeholders in that system. If stakeholders are denned as an identifiable group of people or an organisation, or organisations, having a legitimate interest in the process or outcomes of a systems engineering project (see also Box 6), identify the stakeholders in the Millennium Bridge project.\n\nAnswer\n\nThe stakeholders involved in the Millennium Bridge were as follows.\n\nThose who put up the money:\n\nThe Millennium Commission through the Millennium Bridge Trust\n\nThe Corporation of London through the Bridge House Estates Trust\n\nThe Hong Kong and Shanghai Banking Corporation (HSBC)\n\nThe Financial Times (which sponsored the original competition)\n\nand other contributors of funds.\n\nThose involved in the bridge's design or construction:\n\nOve Arup\n\nFoster and Partners\n\nSir Anthony Caro\n\nSir Robert McAlpine\n\nMonberg Thorsen\n\nsubcontractors.\n\nLocal authorities, the Borough of Southwark.\n\nThose concerned with the bridge's context: e.g. the Tate Modern's architects, Herzog & de Meuron.\n\nThe public wishing to use the bridge.\n\nRiver users.\n\nBox 6 stakeholders\n\nMitroff (1980) defined the stakeholders in a system as all those:\n\nhaving an interest in the change being considered\n\nwho stand to gain from it\n\nwho stand to lose from it.\n\nThe stakeholders in any system can be classified into one of four categories:\n\nThose responsible for its design and development, for example the project managers, system engineers, communications experts, technical authors.\n\nThose with a financial interest in the system, responsible for its sale or its purchase, for example the marketing manager, the buyer.\n\nThose responsible for its introduction and maintenance within an organization, for example training and user support staff.\n\nThose who have an interest in its use, for example user managers and all classes of users: primary (those who are likely to be frequent hands-on users of the system); secondary (those who are occasional users of the system or who make use of it through an intermediary); tertiary (those who are affected by the introduction of the system or who will influence its purchase but who are unlikely to be hands-on users).\n\nAdapted from Macauley (1996)\n\nThe constraints define the area in which the system can legitimately operate. The articulated demands or requirements of the system "
    }
}
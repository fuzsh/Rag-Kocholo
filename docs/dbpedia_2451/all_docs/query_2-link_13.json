{
    "id": "dbpedia_2451_2",
    "rank": 13,
    "data": {
        "url": "https://jackkrupansky.medium.com/my-journey-into-quantum-computing-9c727ec2e0ff",
        "read_more_link": "",
        "language": "en",
        "title": "My Journey into Quantum Computing",
        "top_image": "https://miro.medium.com/v2/1*m-R_BkNf1Qjr1YbyOIJY2w.png",
        "meta_img": "https://miro.medium.com/v2/1*m-R_BkNf1Qjr1YbyOIJY2w.png",
        "images": [
            "https://miro.medium.com/v2/resize:fill:64:64/1*dmbNkD5D-u45r44go_cf0g.png",
            "https://miro.medium.com/v2/resize:fill:88:88/1*7HRt0ZhN3SIhkZRHzlak6w.jpeg",
            "https://miro.medium.com/v2/resize:fill:144:144/1*7HRt0ZhN3SIhkZRHzlak6w.jpeg"
        ],
        "movies": [],
        "keywords": [],
        "meta_keywords": [
            ""
        ],
        "tags": null,
        "authors": [
            "Jack Krupansky",
            "jackkrupansky.medium.com"
        ],
        "publish_date": "2020-08-31T21:02:34.320000+00:00",
        "summary": "",
        "meta_description": "How did I get started in quantum computing? How did I first become aware of quantum computing? How did I get to where I am now in quantum computing? What was the arc of my trajectory? This informal…",
        "meta_lang": "en",
        "meta_favicon": "https://miro.medium.com/v2/1*m-R_BkNf1Qjr1YbyOIJY2w.png",
        "meta_site_name": "Medium",
        "canonical_link": "https://jackkrupansky.medium.com/my-journey-into-quantum-computing-9c727ec2e0ff",
        "text": "Jack Krupansky\n\n·\n\nFollow\n\n105 min read\n\n·\n\nAug 31, 2020\n\n--\n\nHow did I get started in quantum computing? How did I first become aware of quantum computing? How did I get to where I am now in quantum computing? What was the arc of my trajectory? This informal paper chronicles the major milestones — and obstacles — along the way of my journey into the quantum world of quantum computing, quantum mechanics, and quantum information science in general. Maybe something in my own trajectory might benefit others as they consider their own entry and path in this new field of study and sector of technology and commerce.\n\nThere is a lot of material here. After reading the first few sections for background, you might want to skip ahead to the central sections:\n\nMy quantum journey timeline\n\nMy watershed moment — November 2017\n\nReflections from my journey\n\nTopics to be covered in this paper:\n\nThe impetus for this account of my quantum journey\n\nThe goal of my journey into quantum\n\nFocus on understanding and truth, not popularity or getting a job\n\nNon-goals of this paper\n\nIs quantum computing a field or a sector?\n\nMy pre-quantum history and career as a software developer\n\nMy work and career status: I’m semi-retired\n\nI’m a technologist\n\nNo interest in specialization\n\nJack of all trades\n\nMy motivation and interest\n\nAddressing the hype — what is real and what is not\n\nI’m an idea guy at heart\n\nHands-on was quite appealing when I was younger\n\nMy interest lies in more than what I can accomplish by myself with my own hands\n\nMy mission\n\nGrandiose claims\n\nI have no hands-on involvement in production or application of quantum computing\n\nCareer in quantum computing?\n\nMy quantum journey timeline\n\nMy watershed moment — November 2017\n\nReflections from my journey\n\nNo formal education in quantum computing or quantum mechanics\n\nThinking of getting a college degree focused on quantum?\n\nIs quantum computing real?\n\nIs practical quantum computing imminent?\n\nIs the hype warranted?\n\nEverything feels… premature\n\nD-Wave is somewhat inscrutable\n\nNot really ready for Quantum Ready\n\nAmbiguity of quantum ready\n\nQuantum volume has no real technical value\n\nQuantum ready and quantum volume are basically marketing scams\n\nQuantum Summer of Love — but might a Quantum Winter be coming?\n\nAll it takes is a single massive breakthrough or two or three to break out and we’re off to the races\n\nBut… my journey is far from complete\n\nAsk me again in two years\n\nMuch more basic research is needed\n\nGeneral reflections\n\nMany exciting advances have been going on in classical computing\n\nThe awesome and unparalleled intellectual power of classical computers\n\nSeminal role of Feynman\n\nQuantum computer as a coprocessor\n\nMust map problems to solutions using the raw physics of quantum mechanics\n\nLack of confidence that many current quantum algorithms will scale\n\nTedium of a low-level programming model\n\nNeed a high-level programming model\n\nPotential for quantum-inspired algorithms and quantum-inspired computing\n\nOdd, cryptic, and poorly-defined terminology and heavy reliance on greek symbols\n\nLow-level programming model\n\nNeed a high-level programming model\n\nLack of a quantum simulator in the early years\n\nProbabilistic results, statistical aggregation, and approximate determinism\n\nShot count and circuit repetitions\n\nExponential speedup — isn’t free, easy, and automatic\n\nMuch more algorithmic building blocks, design patterns, and application frameworks are needed\n\nNeed credible, real-world example algorithms and applications\n\nTwin qubit challenges: isolation and maintaining entanglement\n\nWe know too little about the granularity of phase\n\nRISC vs. CISC issues for firmware\n\nEmphasis on Shor’s algorithm seems unwarranted\n\nEmphasis on Grover’s algorithm seems unwarranted\n\nDiVincenzo’s five criteria (requirements) for a quantum computer\n\nBoundless cleverness of algorithm designers\n\nMy sources of information\n\nWikipedia articles related to quantum computing\n\nIBM Qiskit\n\nIBM Qiskit Text Book\n\nMIT online quantum courses\n\nMIT xPro Quantum Computing Fundamentals — beyond my budget\n\nMy budget in general: $0\n\nBooks\n\nBlogs\n\nMedium\n\nVideos\n\nLecture notes\n\nMy writing on quantum computing\n\nWhat is Quantum Computing?\n\nMy social media presence\n\nIn hindsight, what path would I take if I was starting from scratch in 2020?\n\nWhat path should a newcomer take in 2020?\n\nMaybe I should take a two to five-year Rip van Winkle-like slumber\n\nMonitoring for advances and breakthroughs\n\nMy endgame?\n\nWhat else?\n\nConclusions\n\nWhat’s next?\n\nThe impetus for this account of my quantum journey\n\nThe impetus for writing this account is that I was reading someone’s post which mentioned how they got started in quantum computing — “I caught the quantum computing bug in the spring of 2018” — and I realized that was roughly when I got started, but not quite, so I went back to check my old notes — it was the fall of 2017 for me. I decided that I should collect all of these personal milestones in one place, both for my own benefit, to review my own progress, and possibly to benefit others as they consider their own entry and path in this new field of study and sector of technology and commerce.\n\nIf nothing else, maybe somebody can avoid mistakes which I’ve made!\n\nThe goal of my journey into quantum\n\nTo put it as simply as possible, the goal of my journey into quantum has been:\n\nTo understand quantum computing as deeply as possible, to understand how real it is, its capabilities, limitations, and issues.\n\nFocus on understanding and truth, not popularity or getting a job\n\nI haven’t been trying to win any awards for popularity or win an offer for a dream job. My focus is on ferreting out the truth and understanding this emerging sector of technology and commerce — let the chips fall where they may. That may mean stepping on some toes and offending some who seek to paint a rosier picture of the current state of quantum computing than I think is warranted, but so be it.\n\nNon-goals of this paper\n\nMy intended focus for this paper is how I got started and my early progress, so I have:\n\nNo intention to detail the full history of quantum computing itself. See the Wikipedia Timeline of quantum computing article if that’s what you’re looking for.\n\nNo intention to detail the more recent developments of my progress. The intention is to focus more on how I got started and really got going. Sure, I’ll cover some of my more recent efforts, but not in great depth here.\n\nNo intention to focus on where I am now or where I might go next — simply how I got started and my early progress. Sure, I’ll cover some of where I am now and what might be next, but not in great depth here.\n\nIs quantum computing a field or a sector?\n\nYes, quantum computing is both a field and a sector:\n\nQuantum computing is a field or field of study from a science, research, or academic perspective.\n\nQuantum computing is a sector from a business, commercial, industrial, or technology perspective.\n\nPersonally, I will usually refer to quantum computing as a sector since my main interest is practical applications which deliver substantial real-world value. But I am also very interested in research, where it makes more sense to refer to it as a field or field of study.\n\nMy pre-quantum history and career as a software developer\n\nI’m not going to go into all of the gory details of how I spent my entire life before I shifted my focus to quantum, but just some highlights.\n\nIf anyone is curious about what kinds of work I’ve done in my career as a software developer, you can look at my LinkedIn profile:\n\nhttps://www.linkedin.com/in/jackkrupansky/\n\nSome highlights from school:\n\nGot heavily into (classical) computers in the 10th grade in high school through the computer club run by a math teacher — 1970. (Ah, that’s actually 50 years ago.)\n\nI attended two summer programs in computers while in high school — 1970 and 1971, at a time when there was no formal computer education in high school, one for five weeks residential at a college, where I learned assembly language programming for the PDP-10, which was a really big deal in those days.\n\nActually got paid to do data entry and even some application programming in the summer after graduating from high school and before starting college — 1972.\n\nWorked in the computer center at college all four years as a systems programmer focused on programming languages and compilers — 1972–1976.\n\nWorked as an applications programmer at the computer center of a community college for a summer and a semester when I dropped out of college for a semester.\n\nContinued work as a systems programmer at the college computer center for a semester even though I had dropped out for a semester — as staff rather than as a student.\n\nEarned a master’s degree in computer science simultaneously with my bachelor of science degree.\n\nSome highlights as a professional software developer:\n\nWorked at a couple of computer companies — 1976–1981.\n\nWorked at a bunch of tech startups, including one developing both hardware and applications and system software.\n\nFocus on development of programming languages, compilers, and development tools.\n\nWorked closely with hardware engineers for graphics hardware and firmware and instruction set design.\n\nWorked on graphics subsystems and graphics engines.\n\nWorked on development of electronic CAD and CAE systems.\n\nWorked on database systems and database engines.\n\nWorked on development of a video-based interactive computer-aided instruction system.\n\nWorked on search engines.\n\nI’ve always been heavily into sophisticated data structures. Get the data structures right and the code is a lot simpler.\n\nAnd data modeling in more recent years as I got deeper into database systems.\n\nDid a lot of technical writing, including one online book.\n\nSome areas that I didn’t focus on:\n\nApplications in general.\n\nUser interface and user experience.\n\nData processing applications. (Actually, I did a little at that summer work at the community college.)\n\nScientific computing. Physics. Chemistry. Biology. Astronomy.\n\nEngineering computing.\n\nMaterial research.\n\nEnergy research.\n\nDrug discovery.\n\nOptimization.\n\nGranted, a number of those areas outside of my historical focus are key potential applications for quantum computing, but I’m more of a technologist interested in hardware, system software, tools, and libraries and frameworks than an application developer.\n\nMy work and career status: I’m semi-retired\n\nI am late in my technical career, still a little too soon to officially and formally retire, but no longer interested in any hands-on work. I refer to myself as semi-retired.\n\nI remain passionately interested in technology, especially advanced, bleeding-edge technology such as quantum computing and advanced artificial general intelligence, but I’m just not interested in actually using and applying it on a hands-on basis.\n\nI’d consider a consulting engagement if it was a senior, high-level, advisory role — and it really interested me. I’m not at all interested in doing any work which doesn’t interest me just for the money.\n\nI’m a technologist\n\nI consider myself a technologist, meaning that there are four things that interest me about any particular technology or technology in general:\n\nCapabilities. What can the technology do? Functions and features. What problems can it solve?\n\nLimitations. What can’t the technology do? What are the technical limits, boundaries, and performance characteristics?\n\nIssues. What impediments or obstacles or missed opportunities prevent the technology from achieving its full potential or prevent users from fully exploiting its full potential? Understanding the extent of hype.\n\nCommunicating. Documenting all of the above. Helping people understand all of the above. But my goal is to share and communicate, not persuade per se.\n\nI wrote an entire informal paper on those interests for quantum computing in particular:\n\nMy Interests in Quantum Computing: Its Capabilities, Limitations, and Issues\n\nNo interest in specialization\n\nWhen I was younger I was more specialized — focused on programming languages and compilers, and developer tools, but gradually I was exposed to other areas. These days I have little interest in specialization per se. I’m interested in whatever interests me at the moment.\n\nI’m far more interested in focusing broadly than focusing narrowly.\n\nJack of all trades\n\nMy traditional focus has been as a software developer, but gradually I’ve been exposed to a variety of other roles. In fact, I no longer have any interest actually writing code — not even the slightest interest. And in fact I have no interest whatsoever in any hands-on roles.\n\nAdditional roles I’ve been involved in to varying degrees:\n\nProduct planning.\n\nSolutions architect.\n\nConsultant.\n\nTechnical writing.\n\nTechnology strategy.\n\nProduct architecture.\n\nSoftware architecture.\n\nHardware/software architecture.\n\nTechnical team management.\n\nQuality assurance.\n\nPerformance modeling, measurement, and characterization.\n\nProduct support.\n\nPre-sales and post-sales support.\n\nMarketing.\n\nSales.\n\nNone of these roles interests me as an exclusive focus.\n\nMy motivation and interest\n\nI imagine that most people entering the quantum computing field or sector are doing so because either:\n\nThey need the power of a quantum computer to solve real-world problems.\n\nThey seek employment or business opportunities in a hot new field.\n\nI personally have no application for a quantum computer. I’m a technologist, not an application developer. I’ve worked for a couple of computer companies, focused on system software and working closely with hardware engineers, but that’s not my interest these days either.\n\nI have two motivations:\n\nUnderstanding the technology, as a technologist — capabilities, limitations, and issues.\n\nAddressing excessive hype. Helping people understand what is real and what is not.\n\nMy motivation has also been to understand quantum mechanics which is the physics which enables quantum computing to more fully evaluate what’s feasible. This should help with both motivations — understanding what features quantum mechanics enables, and understanding the limits of what quantum mechanics can enable, as well as what might be practical in the near term, and what might not be practical even in the long term.\n\nAddressing the hype — what is real and what is not\n\nA major part of my motivation is that I hear a lot of hype that just doesn’t sit right with me. To my mind, public communication about any technology should fairly closely align with the actual capabilities and limitations of the technology itself.\n\nSure, quantum computing has had a lot of hype for many years — decades now — but the hype has escalated dramatically and is now asserting that quantum is “ready” and “here now” or at least getting very close, when that doesn’t seem to be the case.\n\nSo, my motivation has been to get a handle on whether any of that hype is really true. And to dig down and get at what are the facts on what it can and can’t do, today and for the next year or two. Longer than that just doesn’t matter as much right now. Longer-term futures are of interest too, but the hype shouldn’t speak as if the distant future was here now.\n\nI’m an idea guy at heart\n\nThe most dispiriting thing I’ve ever heard during my career is that “Ideas are a dime a dozen — they’re not worth anything.” Yeah, maybe, but still… ideas are all that interest me. Sure, you have to implement ideas to get paid, but if foregoing pay is the price for being able to focus on ideas to the exclusion of working on implementing them, then that’s a price that I’d like to be able to afford. As it is, I’m close enough to real, formal, official retirement — in less than four years when I hit age 70 and my Social Security retirement benefits reach their limit — that I can squeak by living off savings and private retirement funds, provided that I maintain a fairly tight budget, which I am doing now and have been doing for five years now.\n\nPut simply, ideas:\n\nExcite me.\n\nSatisfy me.\n\nIntrigue me.\n\nChallenge me.\n\nHelp me learn.\n\nHelp me grow.\n\nHands-on was quite appealing when I was younger\n\nWhen I first got started with (classical) computing in high school in 1970 theory and ideas were of no real interest to me. The whole appeal of writing a computer program was that you were actually creating something, not actually a living creature, but you were able to cause a dumb machine to behave in a manner that you could control, even if it simply read in a few numbers, did a little math, and printed the results. It was awesome! Being hands-on was a real thrill.\n\nBeing hands-on gave me a sense of accomplishing something.\n\nThat thrill lasted about five years for me. It lasted longer than that, but not with the same intensity.\n\nThe thrill waxed and waned throughout the years. There were times when the thrill was super-intense, there were times when the thrill was completely absent, and everywhere along that spectrum.\n\nMy interest lies in more than what I can accomplish by myself with my own hands\n\nI have no idea how many computer programs, applications, tools, or lines of code I developed over the decades of my career. Wow, it’s been fifty years now!\n\nBut somewhere along the way I lost interest is “just coding.” Seeing another program run was no longer an exciting moment for me. The only sense of thrill, was checking the box that I accomplished the task (and hopefully get paid for it), and move on.\n\nI’m not sure how I was able to last as long as I did!\n\nBut my passion for ideas, working with ideas, has not diminished even the slightest over recent decades and years. If anything, that passion and excitement has only intensified. Ideas are everything to me.\n\nIn fact, I find myself pursuing philosophical and non-technical real-world issues precisely because they raise big idea issues that I don’t generally run into with computing.\n\nI’ve also found myself much more interested in physics than when I was younger. That dovetails nicely with quantum computing.\n\nTo me, ideas are big. Granted, some ideas are actually small or modest in size, but those that aren’t big just don’t interest me.\n\nI may not have the ability (or interest) to actually implement many of the big ideas that come into play with quantum computing or artificial general intelligence, simply working with the ideas is all that I am after. Let somebody else, like the young kids like I was in high school, college and my early career, roll up their sleeves and implement the ideas.\n\nWorking with ideas, big ideas is what excites, challenges, and satisfies me. That’s what I really want to be doing. Even if it means that I can’t get paid for my time and effort.\n\nMy mission\n\nIt didn’t take me long to realize that one of the foremost issues confronting quantum computing is rampant hype.\n\nIt’s not a significant exaggeration to summarize the hype as claiming:\n\nQuantum computing can solve every computing problem which classical computing cannot solve today.\n\nIt would be excusable to have a knee-jerk reaction and push back that clearly that claim is categorically false, but given my technology background my stronger inclination is to take an analytical approach and assess the claim on a technical basis.\n\nSo the strategic objectives of my assessment are:\n\nConfirm whether quantum computing is real or just hype. Which parts are more real, and which parts are more hype.\n\nHow close are the more realistic claims to being true?\n\nWhat are the qualities, functions, features or the more real aspects of quantum computing?\n\nWhat exactly makes the hype impractical?\n\nWhat real really means. Such as timing — today, next year, two years, five years, or… when? Will it perform as advertised? Will it be readily and widely available or only to a chosen few?\n\nWill it really be worth the wait? What will the net quantum advantage amount to?\n\nWill quantum computing apply to a very broad swath of applications and application categories or just to relatively narrow niche use cases?\n\nTo share and communicate my findings and chronicle my journey, primarily through my writing.\n\nGrandiose claims\n\nQuantum computing is plagued by grandiose claims, what we call hype. I won’t try to catalog all of the many grandiose claims, but a few merit immediate attention:\n\nQuantum computing can solve every computing problem which classical computing cannot solve today.\n\nQuantum algorithms deliver an exponential advantage over classical algorithms.\n\nQuantum computers are here today.\n\nQuantum computers will be ready for production applications very soon.\n\nQuantum computers will be ready to deploy production applications within two years.\n\nClassical computing has hit a wall, and only quantum computing can solve many hard problems.\n\nIt’s easy to program a quantum computer — anybody can do it.\n\nQuantum computing is poised to fundamentally transform the way businesses solve critical problems, leading to new efficiencies and profound business value in industries like transportation, finance, pharmaceuticals and much more.\n\nQuantum computing is poised to change everything.\n\nQuantum computing is poised to change our digital world.\n\nQuantum computing is poised to take a quantum leap.\n\nQuantum computing is poised to transform our lives.\n\nQuantum computing is poised to fundamentally disrupt almost every aspect of our daily lives.\n\nQuantum computing is poised to transform the global computing market.\n\nQuantum computing is poised to upend entire industries from telecommunications and cybersecurity to advanced manufacturing, nance, medicine, and beyond.\n\nQuantum computing is poised to change artificial intelligence and machine learning — forever.\n\nQuantum computing is poised to create a paradigm shift in flight physics.\n\nEight things software engineers need to know about how quantum computing is poised to change the world.\n\nQuantum computing is poised to impact many significant markets.\n\nQuantum computing is poised to be a driver of innovation in the next decade.\n\nQuantum computing is poised to make classical computers look like sloths — and launch new, never-imagined processes.\n\nQuantum computing is poised to disrupt traditional computing methods.\n\nQuantum computing is poised for takeoff in industries from medicine to finance.\n\nSad to say it, but most of those claims are literally taken from Google search results when I searched for “Quantum computing is poised”.\n\nIt’s not that quantum computing won’t in fact do some or even many of these things, but that it is being claimed with certainty that it will do all of them.\n\nPart of my mission is to dispel a lot of the hype, especially for such grandiose claims.\n\nOh dear, literally, just as I finished writing this section I saw this on Facebook:\n\n“Fact: Quantum computing will make you more fun at parties.”\n\nOkay, that Facebook post did continue on… “Ok, fact is a strong word. STILL, you could build your own applications with access to our quantum cloud. That’s hot.” That’s where the sector is as of August 30, 2020.\n\nI have no hands-on involvement in production or application of quantum computing\n\nAs I’ve already noted, I am merely a technologist and an idea guy, so I don’t have a job, position, or role in the quantum computing field or sector. In particular, I don’t:\n\nDevelop quantum computers. Not in theory, laboratory research, or product engineering.\n\nDevelop applications for quantum computers.\n\nUse applications of quantum computers.\n\nHave any involvement in marketing, sales, or support of quantum computers or tools or applications for them.\n\nCareer in quantum computing?\n\nI began my quantum journey focused only on deeply understanding the technology, not having any intention to actually pursue a career in quantum computing.\n\nOccasionally during my journey I would ponder the possibility of pursuing some sort of income-producing opportunity in the sector, but nothing ever stood out as appealing or appropriate for my interests, background, ability, and skills. I was really solely interested as a technologist and idea guy. Quantum computing became an area of fascination for me, not a true career.\n\nTo some extent my interest in quantum computing from primarily the perspective of an idea guy is a bit of a luxury — I’m free to do what I want and focus on what I want without having to answer to anyone, no boss, no managers, no shareholders, no customers. The downside — no paycheck or stock options.\n\nMy quantum journey timeline\n\nNow for the gory details of my journey. I wanted to highlight all of the interesting little milestones which marked my journey into the world of quantum computing. I don’t include all of the milestones of quantum computing, except when I have some personal angle to note. Subsequent sections will explore some of these areas in a little more detail, but the thumbnail summaries here should suffice:\n\nBefore 1990\n\nI have no formal schooling in quantum computing — or quantum mechanics.\n\nI have no quantum experience — quantum computing or quantum mechanics — or even exposure to speak of either in high school or even in college.\n\nActually, I briefly considered taking a graduate math course in linear algebra in college, but I didn’t see it as particularly relevant to my main interests in computer science — and I was put off by odd terms such as eigenvector, eigenvalue, and Fourier transforms. I wasn’t even aware that it was essential for quantum mechanics, which wasn’t on my radar back then anyway.\n\nI did take a few courses in calculus and differential equations in college, but none of that interested me at the time.\n\nI did take a few courses in probability and statistics in college. They gave me some background, but the material didn’t really interest me at the time — and I never had any use for it in the vast bulk of my career as a software developer.\n\nI was familiar with matrix math and complex numbers from high school, but I had no reason to use them in my pre-quantum career — with the exception of coordinate transform matrices for graphics rendering.\n\nIncidentally, my main focus in college was programming languages, compilers, operating systems, system software, and computer engineering. I did take several physics courses (but nothing related to quantum) as an undergraduate, but none of that interested me at the time.\n\nNo quantum experience or even any significant exposure to speak of during most of my career — until 2017.\n\nA few vague recollections of hearing about quantum computing in the 1980’s and 1990’s.\n\nI tried to read a magazine article by Feynman on quantum computing (early or mid 1980’s), but the physics was too opaque for me to make any sense out of it at the time, and gave me no sense of where exactly the power of quantum computing came from or how it worked. Rather than inspiring me, this article turned me off to quantum computing. Besides, there were no real quantum computers anyway.\n\nAfter my failed attempt at Feynman, I pretty much pushed quantum computing aside as being a vague future, unworthy of my attention.\n\nOdd, cryptic, and poorly-defined terminology and heavy reliance on greek symbols did nothing to make quantum computing more appealing. If not for these key deficiencies, I might have gotten more deeply into quantum computing at an earlier stage. Especially if interactive simulators had been available much sooner as well — lack of real hardware was not the critical stumbling block.\n\nAnd there was so much exciting progress in classical computing in the 1970’s and 1980’s, both hardware and software, so quantum computing was unnecessary overkill for most applications, at the time.\n\n1990’s\n\nVague awareness of occasional technical media coverage of research advances in the 1995 to 2005 period. But it was all still a vague future. Plenty of theory and basic research, but still no real machines — or imminent prospects. Too many little bits and pieces but no strong hint of a full and usable quantum computer any time soon.\n\nBesides, there were too many exciting advances going on in classical computing. See a section on that later in this paper.\n\nBack in the 1990’s, every time I read something about quantum computing, I wondered about things like whether quantum computers supported floating point numbers and what would happen if you tried to compute 1/3 (infinite repeating decimal), pi (infinite non-repeating digits), or the square root of 2 (also infinite non-repeating digits.) There were no ready answers back then.\n\nBack in the 1990’s it was never clear what gave quantum computing its advantage.\n\nIt was supremely odd to me how people would make a really big deal of even a few qubits — and they did an extremely poor job of explaining how just a few qubits could be used to compute solutions which might take hundreds or thousands of bytes of classical code, and be much faster than the classical code as well.\n\n2000–2005\n\nSometime in the late 1990’s to early 2000’s I read something attributed to Feynman — that it is very hard for us to simulate n-body problems with traditional computers, even 3-body problems, but that nature is able to do so instantly, and this was his rationale for promoting quantum computing. That was finally some insight that stuck with me, from that moment on, through today. Unfortunately we can’t actually do that with a quantum computer — as they are envisioned today, but the sentiment was appealing. I haven’t been able to find that quote on the Internet, try as I may.\n\nI vaguely recall the 5-qubit NMR-based quantum computing experiment in 2000/2001 or so (Experimental Realization of an Order-Finding Algorithm with an NMR Quantum Computer and Quantum Computing and Nuclear Magnetic Resonance), but, seriously, who could get terribly excited by 5 bits — other than a physicist?\n\n2006–2015\n\nI vaguely recall D-Wave Systems announcing a 16-qubit quantum computer in 2007, but there was skepticism about whether it was a true quantum computer (which persists through today!). But for me the bottom line was that 16 bits meant nothing to me.\n\nI can’t recall for sure whether I read the press coverage of Google using the D-Wave system to do image recognition. I just found this Google blog post dated December 2009: Machine Learning with Quantum Algorithms. It was a big advance — to 128 qubits, but once again it meant nothing to me. Yes, it was progress, but too little to matter — to me.\n\nD-Wave had further improvements — 1024, 2048, and now 5000 qubits, but they were and remain dogged with the questions about whether it was merely a special-purpose computer rather than a true general-purpose quantum computer. In fact, wonder if it is more of an analog computer rather than a digital computer.\n\nI recall trying to read a little about quantum computing on a few occasions, but I always found it very cryptic and opaque so that it made no sense to me. Superposition of 0 and 1 and entanglement of qubits didn’t really give a deep sense of the nature of the power of quantum computing.\n\nI recall reading mention of the Grover “database” search algorithm, which certainly sounded intriguing, but it was too vague and there was no real machine to run it on anyway. I recall one news article opining that Google could replace its entire search engine with Grover’s algorithm — yeah, right.\n\n2016\n\nI saw mention of Rigetti in 2015 or 2016 and I was actually quite surprised that they were actually building a real, functional quantum computer, but I basically just skipped over those reports since it sounded as if they weren’t far enough along to have a big commercial success any time soon. It still seemed like more of a research project. Four years later, and they still aren’t much closer despite having made a lot of progress.\n\nWhen IBM announced their Quantum Experience service in May 2016, it suddenly seemed a little more noteworthy — but only a little more. It was still only five qubits, so it wasn’t enough to draw me in. But it was a little more notable — it just seemed more professional, like when IBM introduced their personal computer — it seemed less like a toy or novelty. It actually seemed real, although it was still clearly a laboratory setup, not a commercial product — the service seemed more like a commercial product, but the quantum computer itself just looked like it belonged in a lab, not a data center. The IBM Q System One introduced by IBM in 2019 actually looked like a professional commercial product, but that’s not where IBM was in 2016.\n\n2017\n\nIn May 2017 IBM announced 16-qubit and 17-qubit processors. I noticed that, but again it just wasn’t noteworthy enough to capture my attention.\n\nThen, finally, in November 2017 IBM announced both 20-qubit and 50-qubit processors. That was still not enough to grab my full attention, but 50 qubits was at least tantalizing. In fact, that was the moment when I decided to try to focus some of my attention on quantum computing and try to figure out what it was really all about. This was my watershed moment, so to speak.\n\nContrary to previous press releases which I quickly scanned and then moved on, I carefully scrutinized every section, every paragraph, even sentence, every clause, every phrase, and literally every term and word of this latest IBM press release. I began keeping a list of all of the technical terms which were unfamiliar to me, which eventually became my online glossary which now has over 3,500 terms.\n\nOnce again I tried to do a little reading, but found the material very cryptic and vague, and not insightful in the least. Very frustrating.\n\nMy first stop was the Wikipedia article on Quantum computing, but I found it virtually useless to give me a crisp handle on quantum computing. I understood a moderate amount of what it was saying, but it just wasn’t very satisfying. It didn’t provide me with an “Aha!” moment, just plenty of “So what?” moments.\n\nEven with IBM’s November 2017 announcement, I wasn’t yet deeply committed to quantum computing. Maybe part of that was that I was still more focused on artificial intelligence at that time.\n\nSo for the remainder of 2017 I kept a closer eye on quantum computing, but I wasn’t fully committed, yet.\n\nEarly on in my journey I stumbled across the Quantum Algorithm Zoo, an interesting catalog of quantum algorithms. It sure seemed comprehensive, and I personally used it some in my early quantum days, but it’s very oriented towards algorithms from academic publications, and not very oriented towards solving real-world problems today.\n\nI had been aware of and intrigued by Microsoft’s announcement of the Q# (“Q Sharp”) programming language tailored to quantum computing in September 2017 and delivered in December 2017, but the documentation I briefly perused at the time didn’t persuade me that this was going to make a dramatic difference, at least to me. I don’t recall exactly when I first looked at it, but I did look a little closer in early 2018, but my opinion didn’t change. Even today, in 2020, I’m not persuaded that it adds enough value beyond what people can do with Python libraries, which is what everybody besides Microsoft is using.\n\nI’m not sure exactly when I first heard the term Quantum Ready, but I do recall that it was associated with IBM, such as this IBM blog post in December 2017 — Getting the World Quantum Ready. I do recall that I felt that it seemed a bit odd since even in late 2017 quantum computing certainly felt a long way from being even remotely ready for development and deployment of production-scale real-world applications. The technology wasn’t ready, wasn’t close to being ready, and it wasn’t even close to time for software developers to even think about getting ready to use a future version of the technology, which doesn’t exist even today in 2020, nor is it likely to be ready in the next couple of years.\n\nI closed out 2017 resolved to dig deeply into the theory, the reality, and the promise of quantum computing in 2018.\n\n2018\n\nI started from the beginning of 2018 with a focus on digging deep into quantum computing.\n\nI started poking into virtually everything I could find on the topic of quantum computing.\n\nAfter careful thought in early 2018, I concluded that the only way I could really understand what quantum computing was really all about and how much of the hype was real was to start by building myself a foundation of understanding by diving first into quantum mechanics, the underlying physics. I poked around online and found several free online undergraduate courses at MIT. I plodded through the videos and lecture notes for the first half of 2018. My original intention was to just go through one course to get the basics, but I ended up going beyond that.\n\nMy budget for training and education in quantum? $0. Absolutely zero. There was clearly a lot of online information available for free. If I were an employee at a Fortune 500 company, sure, then I would have considered some expensive courses or professional training and a shelf full of expensive books, but I’m not, so I didn’t. For example, MIT xPro Quantum Computing Fundamentals — for only $2,2,149. Others might not follow my path.\n\nIn fact, my budget in general for quantum was and remains $0. Absolutely zero. That includes courses, seminars, conferences, travel, books, journals, online archives, subscriptions, etc.\n\nI did intend to hold off digging deep into quantum computing itself until I got a handle on quantum mechanics first. For the most part I stuck to that approach.\n\nI would on occasion take a peek at Wikipedia articles on aspects of quantum computing. But just a peek — the dense and cryptic nature of the material always pushed me away. I really needed the quantum mechanics first.\n\nI personally learn better by bits and pieces, collecting puzzle pieces and gradually fitting them together. Linear reading or lectures or videos aren’t optimal for me. Sometimes it’s advantageous to go deep early, while sometimes it works better to stay shallow and only go deep once a sufficient breadth of material has been covered, to get a perspective on how the many pieces of the puzzle fit together. There is no one size fits all approach that will work for everyone.\n\nEven at this early stage in early 2018 my real interests in quantum computing were clear to me: as a technologist, I sought to deeply comprehend its capabilities, limitations, and issues, although I didn’t explicitly and publicly articulate those interests until June 2020: My Interests in Quantum Computing: Its Capabilities, Limitations, and Issues.\n\nOne of the things I decided to do in the winter and spring of 2018 was to go back and review the early history of modern electronic digital computers (especially in the 1940’s and 1950’s) to try to identify factors and trends which could be abstracted and might offer some guidance for evaluating how quantum computing might progress. I posted my notes as Knowledge Needed to Deeply Comprehend Digital Computing (February 2018), What Knowledge Is Needed to Deeply Comprehend Quantum Computing? (March 2018), and Criteria for Judging Progress of the Development of Quantum Computing (March 2018).\n\nOccasionally during 2018 I would ponder the possibility of pursuing some sort of income-producing opportunity in the emerging sector of quantum computing, but nothing ever stood out as appealing or appropriate for my interests, background, ability, and skills. I was really just interested as a technologist and an idea guy — not as a hands-on practitioner. Quantum computing became an area of fascination for me, not a true career.\n\nTo some extent my interest in quantum computing from primarily the perspective of a technologist and an idea guy is a bit of a luxury — I’m free to do what I want and focus on what I want without having to answer to anyone, no boss, no managers, no shareholders, no customers. It certainly provides me with a lot more flexibility than individuals with specific roles to go in a wide variety of directions based on my own discretion.\n\nI finished viewing two of the three undergraduate MIT courses on quantum mechanics (8.04 Quantum Physics I (Spring 2013) and 8.04 Quantum Physics I (Spring 2016) which had additional material) by the middle of spring 2018. I had originally intended to go through only the first course since I really only wanted the basics, but the material was much harder than I expected, so I felt that I needed the extra course. I was able to pick up enough linear algebra from these courses so that I didn’t need to take a full course on linear algebra or read a book on linear algebra.\n\nOnce I started getting into the quantum mechanics with details such as wave functions and probability amplitudes the concepts began to make a little more sense.\n\nProbability and uncertainty always felt natural to me — not a foreign concept — even before I ever heard about quantum anything, so once I grasped that these two concepts were fundamental to quantum mechanics and quantum computing, I felt more comfortable with the quantum world.\n\nThe lack of strict determinism in quantum computing may put off a lot of people, but it actually felt natural to me. Even with classical computing there are limits to determinism. Even classical computing has adopted a variety of statistical methods to cope with real-world phenomena which are more statistical than deterministic. Monte Carlo simulation is a popular approach for solving very complex problems using statistical sampling. Beside, statistical approximations are good enough for many applications.\n\nHaving at least a partial understanding of quantum mechanics in the Spring of 2018 gave me the confidence to start reading up more on quantum computing — whatever I could find online.\n\nSometime in 2018 I became aware of Prof. Sott Aaronson’s blog, which focuses on quantum computing. I’ve occasionally found it interesting and useful, mostly for specific technical issues, but overall I don’t read it on a regular basis.\n\nOverall, I haven’t been reliant on blogs to any significant degree. On occasion something interesting and useful will show up on a Google search or a link on a LinkedIn post, but that’s the exception rather than the rule, at least for me.\n\nOccasionally I have found something interesting on quantum computing on Medium.com, but not very often. I do post all of my own writing on Medium.\n\nI haven’t found most of the online videos of quantum computing or quantum mechanics — other than those of the MIT quantum mechanics courses — to be of any significant value, to me, at least. Maybe I’ve just gotten enough from online text to need videos. Besides, I currently find academic papers far more interesting and informative.\n\nA key conceptual breakthrough for me in 2018 was grasping the notion of a Hadamard transform which places a qubit into an equal probability superposition of 0 and 1, which enables quantum parallelism with 2^n quantum states for n qubits. Finally, a hint as to where the power of quantum computing actually comes from!\n\nMeanwhile, my list of terms related to quantum computing and quantum mechanics was growing longer and longer, literally every day. I finally decided in June 2018 to start turning the list into a glossary. At first I was simply going to list the terms, but I was far enough along that I felt comfortable writing up definitions for at least some of the terms.\n\nI found a lot of interesting and useful information in IBM’s Qiskit and from IBM in general. It had its limits and issues, but it did in fact help me get going. If only the current Qiskit Textbook had been available back in 2018 or even 2017, it would have helped accelerate my learning.\n\nI finished viewing the third undergraduate MIT quantum mechanics course over June and July 2018 (8.05 Quantum Physics II (Fall 2013).) Some of the material was finally starting to sink in in a more meaningful way. This gave me the confidence to flesh out my glossary. And when I got stuck on a glossary entry, that was simply an incentive to dig deeper, which enhanced my understanding even further. I had originally intended to go through only a single course, but I ended up feeling that I really needed the extra depth to really make sense of the physics underlying quantum computing.\n\nI was disappointed that MIT didn’t have an online quantum computing course sequence comparable to the quantum mechanics courses. I did find one MIT graduate quantum computing course, by Peter Shor, but it had no videos and most of the lecture notes were missing, so it wasn’t terribly useful to me. I was surprised that they didn’t have more.\n\nI was also disappointed that I couldn’t find online graduate-level MIT quantum mechanics courses.\n\nSomehow, by the spring and summer of 2018 I finally had accumulated enough of an understanding about quantum computing and quantum mechanics that I was finally able to read Wikipedia articles related to quantum computing without flinching, cringing, and otherwise reacting in a very negative manner. This is horrible and backwards — the Wikipedia should always be the place where you can reliably depend on going to get started with any topic.\n\nOne of the early lessons from the MIT quantum mechanics courses was the need to develop intuition for the quantum world. That’s a constant struggle, but worth every ounce of effort. It’s not enough to simply memorize the rules, but to understand the rules deeply enough that you could re-derive them or even revise them as needed. Put simply, you need to understand the material well enough that it actually makes sense — at a gut and intuitive level.\n\nI would have loved to go back and read the early quantum computing papers from the 1980’s for deep background, but they predate the Internet — and they’re mostly locked behind academic journal paywalls, so there is no free online access. Oh well.\n\nBy the middle of 2018 I concluded that all of the Greek symbols and arcane terminology of quantum computing, much of it inherited or influenced by quantum mechanics, was very counterproductive — and completely unnecessary, in my opinion. I kind of knew that when I was getting started, earlier in 2018, but I initially presumed that it was just a learning-curve thing. Actually, I knew it many years earlier when I first started reading about quantum, which was a big part of what kept me away from quantum for so long.\n\nThe steady flow of press releases was also a source of incentive for me and helped flesh out my growing glossary. I would read virtually every quantum-related press release word by word, writing down terms and phrases that I might not understand fully and adding them to my glossary, either with entries I developed by further reading, or with a TBD — To Be Determined — if I couldn’t quickly arrive at a reasonable entry.\n\nI started paying a lot more attention to Rigetti in the Spring and summer of 2018. They seemed to be coming on strong, with a decent 19-qubit machine available for remote access.\n\nI posted my initial cut at my glossary — with over 2,000 entries — late in July 2018. I’ve been updating it fairly regularly since. It has over 3,500 entries as of the moment I am writing this.\n\nBy August 2018 I was beginning to feel that I had a semi-decent grasp of quantum computing.\n\nBy the middle of the summer of 2018 I had enough background to feel comfortable starting to do some serious writing about quantum computing. See List of my writing on quantum computing. Everything is posted on Medium.com. I refer to everything I post on Medium as an informal paper. I’m certainly not adhering to strict academic journal standards, but I’m not writing mere blog posts either.\n\nSince it was now clear to me that limited hardware and primitive algorithms were holding quantum computing back, in August 2018 I posted my thoughts on the topic: The Greatest Challenges for Quantum Computing Are Hardware and Algorithms.\n\nOne intriguing possibility which I highlighted in that paper was the prospect of quantum-inspired algorithms and quantum-inspired computing. The essence is that the design of great quantum algorithms requires incredible out-of-the-box thinking, and once you’ve done that you may in fact be able to implement a similar approach on a classical computer which is much more efficient than traditional approaches to the design of classical algorithms.\n\nAnother notion that I explored in that paper was that the essence of designing a quantum algorithm was to map a problem to a solution based on the raw physics of quantum mechanics, in contrast to classical computing where most algebraic equations could be directly mapped to comparable classical mathematical operations on a classical computer.\n\nOne of the key concepts I learned about quantum algorithm design in 2018 was that whether designing an algorithm for a quantum computer or quantum-inspired computing, a key technique is the concept of reduction — reducing a problem from a more complex form to a simpler form. Shor’s factoring algorithm does that — reducing factoring of semiprime numbers to the simpler and more readily-computed problem of order-finding.\n\nIt became clear to me during the spring and summer of 2018 that quantum computers, as then envisioned, were intended more as coprocessors to perform a very limited amount of the overall computation of an application, with the bulk of the application running as classical software.\n\nThe more I thought about it, the more I realized that what was really needed in the long run was to fully and tightly integrate classical and quantum computing so that the quantum portions of applications could be invoked much more efficiently. I wrote up my ideas and posted them at the end of August 2018: What Is a Universal Quantum Computer?. That’s not something which will happen any time soon, but it is what is needed, eventually.\n\nI actually didn’t start on my quantum journey with the explicit intention of doing much writing — my main interest was simply to understand the concepts. But as I began to get into it, it just felt natural to write about what I had learned, what questions, issues, challenges, opportunities, and limitations I had identified, and to speculate about the future.\n\nIntel made announcements of working on a quantum computer chip in the fall of 2017 and winter of 2018, but so far that promise has not yet been fulfilled with a functioning quantum computer. I noticed these announcements, but they just didn’t register with me.\n\nMicrosoft also made numerous announcements in the quantum computing field over the years, but little of this registered with me. They promised to produce quantum computer hardware, but so far they have not fulfilled that promise. So far all they have done is work on tools, simulators, and access to the quantum hardware of others.\n\nEarly in my reading on quantum computing I became aware of the Bloch sphere, and although it was a good introductory method to visualize some of the basics of rotation of quantum state around three axes for single-qubit gates, I quickly discovered that it had two major shortfalls: 1) it didn’t represent probability amplitudes, at least in a direct sense, and 2) it didn’t represent the combined quantum state of two or more entangled qubits. Also, I wasn’t pleased with the way they represented a superposition as a single unit vector rather than two distinct basis vectors, each with its own probability amplitude. And I wasn’t happy with the notion that the ground state, |0> was up, the north pole, while the excited state, |1> was down, the south pole.\n\nAs I was googling terms for my glossary, I discovered a wide range of sources for information on quantum computing, including academic papers, college course lecture notes, blog posts, Wikipedia articles, and even a few (illegally bootlegged) online books. There was no shortage of material, but there was a lot of inconsistency and gaps in how it was presented. I had to consult a large number of sources to develop a semi-coherent overall view of the field.\n\nAt one point I discovered the Quantum Computing Stack Exchange with questions and answers, but I found the utility and quality of answers to questions to be insufficient for my purposes. That was a major disappointment. Ditto for Quora.\n\nBooks on quantum computing and quantum mechanics are not a significant source of information for me. Lecture notes, academic papers, and web sites have most of what I need and have used.\n\nIt didn’t take me long to develop a strong commitment to free and online text, media, and code — free papers (preprints on arXiv), specifications, documentation, books, videos, lectures, lecture notes, academic papers, and GitHub for code and project files. If it isn’t online, costs money, is hidden behind a paywall, or requires registration, then it doesn’t exist as far as I am concerned.\n\nI’ve run into some number of projects which use GitHub as a code repository — but not as many as I would have liked. This is probably the best way to share code. And it’s an easy way to browse projects.\n\nIn August 2018 Rigetti announced that they expected to have a 128-qubit machine within a year. That got me excited and motivated. It seemed as if quantum computing was finally on the verge of a real breakout. Unfortunately, that was two years ago, and still no sign of it, and their current best offering is a 32-qubit machine. My enthusiasm has since been tempered.\n\nSometime in the summer of 2018 I finally had the background and courage to actually read Feynman’s famous paper on quantum computing — Simulating Physics with Computers (sorry, but there is no reliable link to the full paper available since it’s published in a paywall-protected journal.) It finally made a little sense. It gave me some insight into his intentions — he wasn’t simply intending to make a faster computer, but a computer focused on simulating physics, especially quantum mechanics.\n\nI’m not sure exactly when I first became aware of the concept of NISQ devices or their significance. Sometime in 2018. But even today I remain skeptical whether the concept is helpful or harmful. The most positive thing I can say is that it is reasonably descriptive — and that it will continue to be relevant for some years to come.\n\nI also can’t recall for sure when I first became aware of Prof. John Preskill — the Richard P. Feynman Professor of Theoretical Physics at CalTech — and his pioneering work in quantum computing. He coined the term NISQ device, so it may well have been at the same time I heard about NISQ. I did reference his name once in a paper I posted at the end of August 2018, so I was certainly aware of him before then.\n\nBy the summer of 2018 I had finally read enough detail about Grover’s search algorithm to realize that much of the hype really was just hype. The algorithm was billed as being able to search a “database”, but a database is highly structured, while the actual Grover’s algorithm searches only linear unstructured data. A little bit of disenchantment began to set in on my part. At least there was still Shor’s algorithm, capable of factoring even the largest of public encryption keys, the veritable Mount Everest of early quantum computing algorithms to captivate all of us. Or at least that’s what I thought before I dug into Shor’s algorithm.\n\nIn the late summer of 2018 I finally felt comfortable trying to tackle Shor’s algorithm for factoring large semiprime numbers — not because I cared about cracking large public encryption keys, but because it seemed as if almost every academic paper would tout the algorithm as being the peak of quantum computing, so I figured that I needed to comprehend how it worked. I started by reading the original paper — or at least the preprint on arXiv.\n\nInitially I was very impressed by Shor’s algorithm, but the more I dug into it the more skeptical I became. The paper had too many gaps, leaps, hand waves, and lack of crystal clarity and specificity for my taste. And, worst of all, I grew concerned that it was not clear whether it will really work for very large numbers due to practical limitations such as phase granularity and concern about how many circuit repetitions might be needed to get statistically valid results. In other words, I had finally arrived in the world of pragmatic considerations for quantum computing!\n\nBy the end of September 2018 I posted a list of all of my open questions about Shor’s algorithm: Some Preliminary Questions About Shor’s Algorithm for Cracking Strong Encryption Using a Quantum Computer. Most of them are still open, from my perspective. That paper included a lot of references for Shor’s algorithm for anyone wishing to dig deeply into it, including variations on the original algorithm and attempted implementations.\n\nIn October 2018 I posted an abbreviated summary of the various pieces of Shor’s algorithm: Ingredients for Shor’s Algorithm for Cracking Strong Encryption Using a Quantum Computer.\n\nIn October 2018 I combined all of my questions about quantum computing into a single paper, Questions About Quantum Computing. My intention was to eventually turn that into an FAQ, but I haven’t gotten around to it yet. I don’t want to start that task until I have a lot higher level of confidence that I have 100% correct answers. Quite a few of the questions on that list date back to early 2018 before I knew much at all about quantum computing.\n\nBy the fall of 2018 I had gotten tired of the mediocre and spotty quality of the documentation and technical specifications — or their complete lack of existence at all — for the various quantum computers which were now publicly available. So, I wrote up a proposed framework for documenting the principles of operation for quantum computers — all of the information which a quantum application developer would need to fully exploit the hardware.\n\nPeople are always chattering about quantum advantage and quantum supremacy and how long it might take to achieve either, but in November 2018 I realized that there is one key technical area where even the simplest of today’s quantum computers are able to achieve quantum advantage right now: generation of true random numbers. A classical Turing machine cannot “compute” a random number since true random numbers are not “computable”, even in theory, using a Turing machine The best we can do on a classical computer is generate pseudo-random numbers, or access external special-purpose hardware to gather entropy from the physical environment. But all quantum computers can trivially generate random bits by simply executing a Hadamard gate to put a qubit into a superposition of 0 and 1, and then measuring the qubit to cause the wave function to collapse to a 0 or 1, effectively generating a random bit. I wrote about this in November 2018: Quantum Advantage Now: Generation of True Random Numbers.\n\nIn the late fall of 2018 I noticed an announcement by IonQ, a quantum computing startup focused on trapped ions for qubits. That only added to my excitement and enthusiasm, and expectations of a real breakout for quantum in 2019 — which unfortunately didn’t happen. Progress continues, but hardware is proving to be a fair amount more challenging than it seemed in 2018, two years ago.\n\nAt some point in 2018 I became aware of Xanadu, a Canadian startup, which was working on photonic (optical) quantum computing, using squeezed states of light and continuous value (CV) quantum states with qumodes rather than qubits. It sounded very intriguing, especially the prospect of operating at room temperature, but try as I might, I wasn’t able to figure out how real their hardware was. They do have software packages for AI, particularly machine learning (PennyLane), and a quantum simulator and interface library (Strawberry Fields), but it was the hardware I was interested in. I’ve seen them in the news a bunch of times over the past two years, but there hasn’t been any clarification of how the hardware is going.\n\nAt some point in 2018 I became aware of theoretical physics researcher David DiVincenzo and his proposed five criteria (requirements) for a quantum computer. His seminal paper from 2000: The Physical Implementation of Quantum Computation.\n\n2019\n\nIBM announced the IBM Q System One in January 2019. This system actually looked like a professional commercial product, as opposed to all of their previous quantum quantum computers which just looked as if they belonged in a lab, not in a commercial data center. Sure, it now looked professional, but to me that was only a superficial skin over the real hardware which still seemed as if it still belonged in a lab. The IBM press release didn’t even mention how many qubits the machine had, although I read elsewhere that it had 20 qubits. And they announced and showed the machine at the Consumer Electronics Show of all places, not an enterprise IT convention. That really stood out as odd in my mind — marketing out of control. In any case, I merely rolled my eyes on this one — many more qubits or much longer coherence time would have raised my eyebrows, but flashiness and visual appearance alone is a non-starter for me.\n\nI was very intrigued by a preprint paper posted by IonQ in February 2019: Ground-state energy estimation of the water molecule on a trapped ion quantum computer. I thought it showed a fair amount of progress both with hardware and algorithms. To me, it set a fairly high bar. Google may have advanced above that bar with their paper, Hartree-Fock on a superconducting qubit quantum computer, in April 2020, but the IonQ paper set the initial bar. It turns out that IBM had already set the bar anyway back in 2017 with their paper, Hardware-efficient Variational Quantum Eigensolver for Small Molecules and Quantum Magnets, in April 2017, but that predated my dive into quantum computing, so I hadn’t noticed it until I saw the reference citation in the Google paper. But from February 2019 until April 2020 at least I had the IonQ paper to establish a foundation of what was possible.\n\nIn March 2019 IBM announced that it had a new quantum computer which had double the quantum volume of its previous quantum computers. That sounded great, but what exactly was quantum volume and what was the effect of doubling it? In March 2017 IBM introduced a metric for measuring the net power of a quantum computer, called quantum volume (technical details), which combines number of qubits, coherence, gate errors, and connectivity into a single numeric metric. Sounds like a great idea, but… technically it is completely useless since there is no way to derive any useful metric from that single number which can be used by an algorithm designer or an application developer. I vaguely recall seeing mention of this concept previously, but it seemed too vague and unclear how to use it, so I ignored it. But now, with this announcement I started looking into it more closely, including the technical paper cited above which was posted in November 2018 — and didn’t like what I found, as I just noted. Basically, it’s just more marketing hype.\n\nIn March 2019 I posted a list of the major areas of my own lingering uncertainty about quantum computing: Lingering Obstacles to My Full and Deep Understanding of Quantum Computing. Many of the items are still open, although I have made some progress. In a lot of cases I do know the basics, at least at a superficial level, but I really want to understand quantum computing at a deeper, intuitive level.\n\nI spent most of 2019 waiting for major breakthroughs on a regular basis, especially those promised in 2018, but that just didn’t happen. More incremental progress. More delays. Lots of disappointment.\n\nOver the course of 2018 and early 2019 I had read so many companies touting the types of applications which they felt could be addressed by quantum computers that I decided to compile them all in a single document. I have a short overall summary list as well as the specific application categories which specific companies listed as being appropriate for quantum computing. I posted my compilation in April 2019 and have been updating it as I stumble across new claims by companies: What Applications Are Suitable for a Quantum Computer?.\n\nIn April 2019 I pondered the question of when quantum computing would see it’s first substantial real application, comparable to what happened for classical computing when the ENIAC computer was unveiled in 1946: When Will Quantum Computing Have Its ENIAC Moment?.\n\nIn May 2019 I pondered the question of when quantum computing would finally be ready for more average mere-mortal (non-elite) application developers, comparable to the introduction of the FORTRAN high-level programming language for classical computers in 1957 (eleven years after the ENIAC moment): When Will Quantum Computing Have Its FORTRAN Moment?.\n\nReacting to a lot of the hype, I posted an informal paper in June 2019 to address the definitions of quantum advantage and quantum supremacy: What Is Quantum Advantage and What Is Quantum Supremacy?. This was three months before Google announced that they had achieved quanum supremacy, but Google had already announced in 2017 and 2018 that they were on track to achieve quantum supremacy fairly soon.\n\nOverwhelmed by all of the hype over quantum computing, I figured I’d try my hand at parody of the hype — in June 2020 I posted Fake Predictions for Quantum Computing. Along the lines of infamous predictions for classical computing which turned out to be false — like nobody would ever want a [quantum] computer in their home.\n\nAs of June 2019, I concluded that quantum computing was stuck in the realm of the lunatic fringe — usable only by the most-skilled elite. For more, see When Will Quantum Computing Be Ready to Move Beyond the Lunatic Fringe?.\n\nIn the summer of 2019 I spent literally several months reading all of the Nobel physics prize lectures which related in any way to quantum mechanics. That effort gave me a lot more intuitive feel about quantum mechanics than I got from the MIT undergraduate courses. Even if I still couldn’t be a practicing quantum scientist, at least I could have a vague but deeper sense of the major pieces of the puzzle.\n\nReading all of those Nobel physics lectures highlighted that I actually do have a passion for physics at the subatomic level. It always intrigued me in a distant sort of way, but now I am attracted to it in a deep way, and not just because I need it to understand quantum computing.\n\nDuring 2019 I gradually came to realize the value of quantum simulators while real hardware is not yet available or is too limited or noisy. Even when hardware is available, simulators make life easier.\n\nI found a few online interactive simulators for quantum computing. These were interesting to some extent, and I learned a little, but overall they didn’t help me too much.\n\nGoogle announced that it had achieved quantum supremacy in the fall of 2019, including the fact that they had a 53-qubit quantum computer operational in their lab. Actually, they didn’t officially announce it until October, but an advance copy of the paper was leaked in September. This was all quite exciting to watch in real time. As noted earlier, Google had telegraphed their intentions in 2017 and 2018, well in advance of the actual event, so this wasn’t an out of the blue surprise, but a pleasant surprise that it had actually finally happened.\n\nShortly before the Google leak (two days before), IBM announced that they had a 53-qubit machine in their lab as well. Interesting coincidence.\n\nShortly before Google’s official announcement (two days before), IBM denounced Google’s quantum supremacy effort in a preprint paper, presumably based on the leaked Google paper.\n\nI took some time reading Google’s paper on quantum supremacy in October and November and after some thought wrote up my own impressions of their effort in late November 2019: What Should We Make of Google’s Claim of Quantum Supremacy?. With that, I could put this whole episode behind me and once again focus on real applications of quantum computing. Note that I had written about quantum advantage and quantum supremacy back in June 2019: What Is Quantum Advantage and What Is Quantum Supremacy?.\n\nBy October 2019 I had realized that my to-do list of topics to read, research, and write on was getting out of hand and scattered all over my notes, so I consolidated the topics in a list and posted it: Future Topics for My Writing on Quantum Computing. Even now, in 2020, I have a lot more topics in newer notes that I need to add to that list.\n\nIn October 2019 I decided to read up on quantum computational chemistry since that promised to be one of the main areas in which quantum computers could deliver significant value. I read two long papers: Quantum computational chemistry by McArdle, et al, and Quantum Chemistry in the Age of Quantum Computing by Cao, et al. I’m certainly not an expert now, but at least I have a feel for the issues.\n\nOne of the benefits of focusing on quantum computational chemistry was to raise my understanding of variational methods and hybrid quantum/classical algorithms in general. I had encountered these concepts previously, but not comprehended them as deeply. I’ve read a number of papers related to these concepts since.\n\nAnd with hybrid quantum/classical algorithms we have ansatze, state preparation and measurement (SPAM), classical postprocessing, classical optimization, and iteration, as well as shot count or circuit repetitions. The quantum vocabulary zoo is rather overwhelming, to say the least.\n\nOne of the requirements for deeply understanding quantum computing is to become intimately familiar with unitary matrices (and matrix math, of course.) A lot of the basics are reasonably simple, but developing a deeper intuition, especially for complex exponentials takes time. I’m not sure when I finally reached that stage, but it wasn’t in the early stages. But by the fall of 2019 unitary matrices were much more second nature for me than in the summer or fall of 2018. A two-qubit unitary matrix applied to a two-qubit column vector was no longer an inscrutable Chinese puzzle. Sure, for a math guy it would have happened much more quickly, but I’m not a hard-core math guy.\n\nIn November 2019 I was thinking about what factors would have to come together to achieve what I called quantum algorithmic breakout — the right and critical mass combination of hardware and algorithm advances that exploit that hardware to achieve real-world, practical, production-quality, production-capacity applications so that their development becomes commonplace, even easy, as opposed to the heroic and severely limited efforts which are common today. I posted my thoughts as What Is Quantum Algorithmic Breakout and When Will It Be Achieved?.\n\nI thought it would be helpful to propose an equivalent to Moore’s law for qubit growth of quantum computers. I posted my proposal in November 2019: Proposed Moore’s Law for Quantum Computing. My proposed rule: “Qubit count of general-purpose quantum computers will double roughly every one to two years, or roughly every 18 months on average.” Oops, we’re already falling behind — when I wrote this paper, I expected that the 128-qubit machine from Rigetti was coming soon.\n\nMicrosoft announced Azure Quantum, their cloud quantum computing service, in November 2019. This was significant generally, but not to me personally — I’m much more interested in the technical capabilities of the actual quantum computers, not the logistics of how they are accessed. Microsoft is merely making it easier to manage access to the quantum computers of other companies, but wasn’t introducing their own proprietary quantum computers. Maybe this announcement legitimized the sector, or maybe it merely exacerbated the hype, it’s hard to say for sure.\n\nIn December 2019 Amazon announced their Braket quantum computing cloud service, their response to Microsoft’s Azure Quantum. My personal response was about the same as with Azure Quantum — I was more interested in the technical capabilities of the actual quantum computers than the logistics or economics of accessing them.\n\nAs Christmas and the end of 2019 were approaching I decided to write up a list of advances and breakthroughs I wanted to see in the coming year: My Quantum Computing Wish List for Christmas 2019 and New Year 2020.\n\nPeople were chattering as if a quantum computer could compute anything, but I realized that as currently envisioned there were many tasks which weren’t a good fit for quantum computers. I posted my thoughts in December 2019: What Can’t a Quantum Computer Compute?.\n\nBy the end of December 2019 I still hadn’t fully mastered the nuances of Shor’s algorithm, but since the algorithm is of continued interest, I posted an updated version of my long list of references for Shor’s algorithm from September 2018 for anyone wishing to dig deeply into it, including variations on the original algorithm and attempted implementations: References for Shor’s Algorithm for Cracking Strong Encryption Using a Quantum Computer. As far as I can tell nobody has implemented Shor’s algorithm on a real quantum computer to factor more than 15 (3 x 5) and 21 (3 x 7). I found one simulated implementation that worked for larger two-digit numbers, but that’s about it. I had hoped to see an implementation for a 32 to 40-qubit simulation, but I’ve seen none so far. I think it’s worth noting that the various implementation efforts diverge significantly from the original paper. To this day, I don’t think there is a single clean reference implementation which is 100% according to Shor’s original paper. And this is for an algorithm which is still widely considered as proof of what a quantum computer can do.\n\n2020\n\nI had heard a little about quantum communication over the preceding two years, including efforts by the Chinese, but hadn’t dug into it deeply, so in the late fall of 2019 and early 2020 I started reading up more on quantum information science, which is the larger umbrella which covers both quantum computer and quantum communication, as well as quantum networking and quantum sensing. I organized a summary of the field and posted it in January 2020: What Is Quantum Information Science?\n\nPart of the impetus for my interest in the larger area of quantum information science was that the U.S. government was ramping up investment in research in the area, so the field was getting more press, including more hype.\n\nExponential speedup is the whole point of quantum computing, but people chatter about it as if it was automatic for all quantum algorithms, which isn’t true. In fact, it takes a lot of careful analysis and attention to detail to achieve exponential advantage, and many algorithms won’t be as successful as their designers might have hoped. Even the much-vaunted Grover search algorithm achieves only a quadratic speedup. The simple truth is that each algorithm will have its own performance characteristics, and the designers or implementers will need to carefully document those performance characteristics, the algorithm’s actual quantum advantage. I discussed this topic at length in: What Is the Quantum Advantage of Your Quantum Algorithm? posted in February 2020.\n\nAlso in February 2020 I posted What Is Algorithmic Complexity (or Computational Complexity) and Big-O Notation?. You’re asking for trouble if you don’t carefully characterize the algorithmic complexity of your algorithm — how will its performance trend as you increase the size of the input.\n\nMy ears perked up when I read Honeywell’s announcement in March 2020 that their quantum computer would become available within a few months, targeting the June timeframe. It was a trapped-ion machine, as was IonQ’s machine, which I viewed as a superior hardware technology for qubits. Unfortunately, they provided virtually no technical details, not even the number of qubits, just the quantum volume — 64, but that’s more of a marketing concept, not telling developers anything they really need to know.\n\nIn the spring of 2020 I finally wrote up a summary of quantum effects — What Are Quantum Effects and How Do They Enable Quantum Information Science? I never saw all of them put together in a coherent manner in one place. If I had seen them described in an integrated manner earlier in my efforts to learn about quantum, I’m sure the pace of my uptake of quantum mechanics and quantum computing would have been much more rapid.\n\nIn June 2020 I finally decided to clearly and publicly state my interests in quantum computing: My Interests in Quantum Computing: Its Capabilities, Limitations, and Issues.\n\nHoneywell came through in late June 2020, announcing that their quantum computer was actually available (to select customers). But as with their March announcement, they provided very little technical information other than the quantum volume — 64, which has essentially no technical value. I was happy that a new trapped-ion quantum computer was available, but disappointed that I can find out so little about it.\n\nOne technical issue which had bugged me for quite some time was the issue of needing to run a quantum circuit some significant number of times to get a statistical average result. A simple concept but the devil is in the details, and most published algorithms only mention it briefly in passing if at all. So I dug into it some more and finally posted my thoughts in July 2020: Shots and Circuit Repetitions: Developing the Expectation Value for Results from a Quantum Computer. The big concern is twofold: 1) how to properly calculate the number of repetitions needed, and 2) whether the needed number of repetitions, especially for a NISQ device, might be so outrageous as to possibly render a quantum solution unworkable or yield performance far worse than expected, and maybe not achieve a true quantum advantage over a sophisticated classical solution at all.\n\nAfter pondering whether quantum computing was still stuck at the stage of being a mere laboratory curiosity, in July 2020 I wrote some background material on the more generic concept of laboratory curiosities: What Makes a Technology a Mere Laboratory Curiosity?. My thesis is that a technology is still a mere laboratory curiosity if It is not yet ready for prime time — for production-scale real-world applications. A new technology needs to offer clear, substantial, and compelling benefits of some sort over existing technology, whether they be new functions and features, performance, less-demanding resource requirements, or economic or operational benefits.\n\nWith that foundation in place, I evaluated the status of quantum computing relative to being a mere laboratory curiosity — concluding that it was, and posting my analysis in August 2020: When Will Quantum Computing Advance Beyond Mere Laboratory Curiosity?.\n\nTo be continued. I’m currently working on two more of my informal papers, with others at the idea stage.\n\nAnd I’m spending a fair amount of time each day keeping up with status posts on LinkedIn, glancing at fresh new academic papers, announcements, and news items.\n\n2021\n\nTBD…\n\nMy watershed moment — November 2017\n\nJust to reemphasize the key milestone that is buried in the timeline of my quantum journey, November 2017 was my watershed moment, the point where the light bulb clicked on and I knew that I needed to get deeper into quantum computing — that it would be a now issue for me rather than a someday issue.\n\nIn November 2017 IBM announced both 20-qubit and 50-qubit processors. That was still not enough to grab my full attention, but 50 qubits was at least tantalizing. In fact, that was the moment when I decided to try to focus some of my attention on quantum computing and try to figure out what it was really all about. This was my watershed moment, so to speak.\n\nReflections from my journey\n\nNow that I’ve recounted my specific activities during my quantum journey, what exactly did I learn, in a general sense?\n\nNo formal education in quantum computing or quantum mechanics\n\nIt’s hard for me to say for sure whether this matters or not, but I had no formal education in quantum computing or quantum mechanics before I began my journey into the quantum world in earnest a few years ago. I did have a technical, STEM background, specifically in computer science, and some math and physics in college, but nothing specific to quantum computing or quantum mechanics.\n\nI do believe that anyone with a general technical background such as mine can do what I have done, but it’s not possible for me to say for sure which aspects of my background really made the difference, or which parts someone else could do without before diving into quantum computing.\n\nThinking of getting a college degree focused on quantum?\n\nI’ll refrain from giving any specific career or education advice to others, with one exception — unless you are one of those rare elites (who don’t need my advice anyway, by definition!), don’t focus your undergraduate college education or even a master’s degree 100% on quantum computing at this stage of the sector. Make sure to at least have a minor, if not your major in some aspect of technology where great non-quantum jobs are plentiful today, in the here and now, since much of the promise of quantum computing is still years down the road.\n\nYou may or may not be able to get a quantum-specific job today, right out of school, and even if you do, the number of places you can go if you decide to leave that job is relatively limited.\n\nYes, demand for quantum technical people is very strong right now, but for a relatively limited number of slots, each with relatively specific technical requirements — not every quantum technical role is created equal.\n\nBesides, many if not most aspects of the technology will probably be radically revised four or five years from now. Be prepared for non-stop continuing education. Invest the bulk of your education in timeless concepts which will still have great value even as technology evolves.\n\nLook (and talk to people in the sector) before you leap.\n\nIs quantum computing real?\n\nThis is the biggest and hardest question of them all. There are several distinct perspectives:\n\nDo real quantum computers exist? Yes. Albeit very limited capabilities.\n\nIs quantum computing sufficient today for production-scale application deployment? No. Not even close.\n\nIs quantum computing on track to become real in some number of years? Maybe.\n\nCan I guarantee that quantum computing is real enough to bet on its arrival? No. Unfortunately I am unable to offer such a guarantee. I think it is likely to be real, but I still have too many questions to be sure.\n\nThe good news is that I cannot claim that quantum computing is not real.\n\nIs practical quantum computing imminent?\n\nEven if quantum computing isn’t sufficient today for production-scale application deployment, is it imminent? Again, several perspectives or time frames:\n\nWithin the next year or two? No.\n\nWithin two to five years? Maybe.\n\nWithin five to ten years? We certainly hope so. But still no certainty.\n\nWithin ten to fifteen years? Feels a little more certain.\n\nWithin fifteen to twenty or even twenty five years? If not then, when?\n\nThe real bottom line is that despite my journey, I remain unable to assess that quantum computing is real enough to bet on a timeframe for its arrival.\n\nIs the hype warranted?\n\nIn a word, No, the hype is not warranted, but… it’s complicated. To be sure, there is a lot about quantum computing which is real, but so much of the hype seems disconnected from reality.\n\nMaybe in two years a fraction of the hype will be warranted, but probably not the majority of the hype.\n\nMaybe in five years a significant chunk of the hype will be warranted, but probably still not the majority.\n\nSo when will the majority of hype be warranted? That’s the crux of the problem — I can’t say when with any confidence.\n\nI’d be a lot happier if certain elements of the hype were dropped:\n\nNo more references to “now.”\n\nNo more references to the present tense.\n\nNo more references to “imminent.”\n\nNo more references to “soon.”\n\nNo more references to the near future.\n\nThat might eliminate the vast majority of the hype about quantum computing.\n\nEverything feels… premature\n\nUnfortunately, after all of my quantum journey, it really does seem that just about everything surrounding quantum computing feels… premature, that everyone is jumping the gun, and that we need to let the fruit ripen on the (basic research) tree before harvesting it for commercial value and consumption.\n\nD-Wave is somewhat inscrutable\n\nI haven’t met anybody yet who really knows what the quantum computer from D-Wave Systems is really all about — or that can explain it to the rest of us. I do know that it has qubits — a lot of them (512, 1024 or 2048 and 5,000 coming), that it has a fixed algorithm (quantum annealing), and that it doesn’t have the gate-style operations which all other quantum computers use.\n\nTo me it seems to be more of an analog computer, or at least more closely comparable to an analog computer than either a classical digital computer or other gate-oriented quantum computers.\n\nYou can’t readily move algorithms between D-Wave and the other quantum computers. If you want to run on D-Wave you have to use the algorithm that is built into the machine. It’s a very flexible algorithm which is parameterized, but you can’t change the algorithm itself.\n\nTo be clear, the D-Wave quantum computer is real, but it’s not comparable to any other quantum computer.\n\nSome real-world problems involving optimization are a natural fit for D-Wave, many or most problems are not.\n\nAlthough D-Wave does have a lot of qubits compared to other quantum computers, it is still very limited relative to many real-world problems which people would like to address.\n\nPeople are doing a lot of experimentation with D-Wave, but it’s not clear to me where any of that is headed. Again, it is indeed a real machine, but it still seems to be a mere laboratory curiosity as far as I can tell.\n\nNot really ready for Quantum Ready\n\nI’m not sure exactly when I first heard the term Quantum Ready, but I do recall that it was associated with IBM, such as this IBM blog post in December 2017 — Getting the World Quantum Ready. I do recall that I felt that it seemed a bit odd since even in late 2017 quantum computing certainly felt a long way from being even remotely ready for development and deployment of production-scale real-world applications. Even then, I felt that the technology would have to evolve dramatically to be ready for people to be trained to use it properly and effectively — it’s rather difficult to train for a moving objective.\n\nThe technology isn’t ready even today in 2020, isn’t close to being ready, and it isn’t even close to time for software developers to even think about getting ready to use a future version of the technology, which doesn’t exist even today in 2020, nor is it likely to be ready in the next couple of years.\n\nThere’s only one thing to get ready for, and that’s to get ready to wait. And it’s likely to be a long wait.\n\nIt’s one thing for a large technology organization to have a few elite members of their technical staff who keep an eye on advanced technologies such as quantum computing, but it’s another thing entirely to ramp up whole teams and large numbers of staff for a moving-target technology which won’t be ready for large-scale application development and deployment for five to seven years, at best.\n\nI get that vendors want prospective customers to get ready soon for technologies which will become ready in the coming years, but it seems almost absurd in the case of quantum computing.\n\nAmbiguity of quantum ready\n\nTo be clear, there are two very distinct senses of ready for quantum computing:\n\nProspective users and developers have been trained on concepts and experimented on a small scale, and now they are just waiting for real hardware that has enough capabilities — qubits, coherence, gate reliability, connectivity.\n\nReal hardware is finally ready to be used by developers and users. With enough capabilities — qubits, coherence, gate reliability, connectivity.\n\nIBM’s marketing term Quantum Ready is referring to only the former. The hardware is not yet ready for production-scale applications which deliver substantial real-world value.\n\nBut the risk is that many executives and unsophisticated managers and analysts may be hearing all the hype and misguidedly believing that Quantum Ready is referring to the latter — that the hardware is ready when it isn’t and won’t be for who knows how many years.\n\nMarketing… enough said.\n\nQuantum volume has no real technical value\n\nIn March 2017 IBM introduced a metric for measuring the net power of a quantum computer, called quantum volume (technical details), which combines number of qubits, coherence, gate errors, and connectivity into a single numeric metric. Sounds like a great idea, but… technically it is completely useless since there is no way to derive any useful metric from that single number which can be used by an algorithm designer or an application developer.\n\nIn November 2018 IBM posted the technical paper which detailed how quantum volume was calculated — actually measured by running randomly-generated quantum circuits:\n\nValidating quantum computers using randomized model circuits\n\nhttps://arxiv.org/abs/1811.12926\n\nThe concept is indeed very technical at that level, but still doesn’t have any technical value for algorithm designers or application developers.\n\nIn March 2019 IBM announced that it had a new quantum computer which had double the quantum volume of its previous quantum computers (16 vs. 8.) That sounded great, but what did that mean? It turns out both machines had the same number of qubits (20), so it meant that they individual qubits were more reliable, but you couldn’t tell either fact from the raw quantum volume numbers.\n\nIn January 2020 IBM announced doubling of quantum volume again, from 16 to 32. The qubit count was 28 vs. 20. So, again, the quantum volume provided no specific technical information about the machine.\n\nIn June 2020, Honeywell announced the availability of their quantum computer with a quantum volume of 64. Now what did that mean? It turns out that the machine had far fewer qubits (6 qubits) than the current IBM machines which have a quantum volume of 32 (28 qubits). In short, Honeywell’s qubit and gate reliability were much better, but the numbers alone don’t tell you that.\n\nIn July 2020, IBM announced six new quantum computers, all with quantum volume of 32, to supplement the one machine they had at 32 in January. Some of the new machines had 27 qubits, some had 5 qubits, and one had 20 qubits. All with the same quantum volume of 32. Kind of confusing, if you ask me.\n\nIn August 2020, IBM announced that it had “upgraded” one of those 27-qubit machines to achieve a quantum volume of 64 — double its previous quantum volume of 32. IBM also posted a technical paper which described how they accomplished this achievement. It turns out that only 6 qubits — out of 27 — were used for the measurement, matching the achievement of the 6-qubit Honeywell machine. So much for trying to compare machines based on quantum volume!\n\nYou can compare two or more machines based on their respective quantum volumes, but that won’t tell you anything about how many qubits each machine has or even which machine has more qubits or which machine’s qubits are more reliable. Ditto for coherence time, gate errors, and connectivity. There is no particular utility here that I can fathom.\n\nTo my mind, the only value of quantum volume is marketing — a vendor can say that they have achieved a specified quantum volume, but that won’t tell the customer, user, or developer any of the key technical metrics that algorithm design and application development depend upon.\n\nQuantum ready and quantum volume are basically marketing scams\n\nMaybe the concepts of Quantum Ready and quantum volume were well-intentioned, but they aren’t in any way providing useful technical information to technical staff. In essence, both concepts are mere marketing scams, providing a distraction and capturing attention, but not adding any technical value or technical benefit.\n\nThey’re just part of the background noise of the hype surrounding quantum computers.\n\nThere are two aspects of marketing scams:\n\nTheir intent.\n\nTheir actual impact.\n\nI won’t attempt to judge the true intent of any marketing scam, but I can judge the effects on real people and real organizations.\n\nAttention and focus are two of the most valuable intellectual and mental resources that any organization possesses. Diverting attention and focus to advanced technologies prematurely — what I have referred to as laboratory curiosities — can have the negative effect of missing other opportunities where those resources could have been applied for greater and more immediate benefit.\n\nQuantum Summer of Love — but might a Quantum Winter be coming?\n\nQuantum computing is still in its Summer of Love — it’s difficult to say how long it will continue, and whether a quantum winter may be coming.\n\nThis “summer” could last for another two or more years. Or six months. Or three years. Or maybe it lasts a full five years before a hard-freeze sets in for a long winter.\n\nAnd maybe we see an occasional breakthrough now and then — they’re so unpredictable. Sometimes they can lead to a full thaw and a renewed summer, but a lot of the time they just just cause temporary thaws followed by a re-freeze. Ultimately we need a chain of breakthroughs to prevent a descent into a quantum winter.\n\nQuantum computing already had a quantum winter of sorts, from 1995 to 2016 as people waited desperately and (im)patiently for a real quantum computer to become available — a 5-qubit machine from IBM. That really was a major achievement, but it was like reaching the base camp for Mount Everest, and people are feverishly waiting to get to the top of the mountain.\n\nAll it takes is a single massive breakthrough or two or three to break out and we’re off to the races\n\nBreakthroughs are very unpredictable, both in their timing and their net impact. All it takes is a single massive breakthrough or two or three to break out and we’re off to the races. But you can’t predict or count on them. The important thing is to quickly jump on breakthroughs as they occur. And to be ready to jump at a moment’s notice.\n\nBreakthroughs can be triggered in two ways:\n\nFundamental discoveries in underlying research. Keep increasing basic research funding until you see results.\n\nDevelopments driven by use needs. Keep pushing the technology as hard as you can, trying to achieve application results. That can help guide research.\n\nGenerally, the most dramatic breakthroughs come when these two factors come together.. And when they come together with a reasonable frequency.\n\nBut… my journey is far from complete\n\nI still have so many nooks and crannies of quantum computing to dig into.\n\nAnd there is so much research in the pipeline, like fresh five-year projects which are just getting started.\n\nAnd who knows how many research projects to follow those projects.\n\nAnd who knows when dramatic breakthroughs may occur which accelerate the pace of progress greatly.\n\nAsk me again in two years\n\nThere’s nothing magical or significant about two years, but progress is occurring at a reasonably rapid pace. It just feels that in two years we should have a lot better visibility on both hardware and algorithm feasibility.\n\nMuch more basic research is needed\n\nIf there’s one thing that I am convinced of from my quantum journey, it’s that much more basic research is needed. Across the board. No exceptions.\n\nIt’s not just a matter of just engineering and product development. Scientists are needed. More work is needed in science labs.\n\nIt’s not just a matter of venture capital funding. Research grants are needed.\n\nGeneral reflections\n\nThese are items which certainly came out of my quantum journey, but are not really associated with any particular milestone.\n\nIt became clear to me during the spring and summer of 2018 that quantum computers, as then envisioned, were intended more as coprocessors to perform a very limited amount of the overall computation of an application, with the bulk of the application running as classical software.\n\nAt some point I began to realize that I didn’t have any great confidence that a lot of the published quantum algorithms would necessarily scale well for significantly larger input data. This is discussed more in a subsequent section: Lack of confidence that many current quantum algorithms will scale.\n\nIt didn’t take me long to realize what a low-level programming model was being used for quantum computing. It looked and felt comparable to assembly language programming on a classical computer.\n\nIt’s super-clear to me that there is a crying need for a high-level programming model for quantum computing.\n\nOver time I came to the realization and a new appreciation of how intellectually powerful classical computing really is. There are so many things that classical computers do very well that even ideal quantum computers can’t do at all, particularly with complex logic and large volumes of data. See the section The awesome intellectual power of classical computers for more details.\n\nIf you have a significant problem you need to solve today or this year or even next year, classical computing is the way to go. Even if you thought you needed a quantum computer because of its exponential advantage, you can probably make do with a sampling technique, such as Monte Carlo simulation, or a large distributed cluster, or by reducing the problem complexity using some clever techniques.\n\nAnothing thing that I realized about classical computing as I got deeper into quantum computing was how little I understood about the physics underlying classical digital logic gates and transistors. I gained a significant appreciation for this fact as I was reading the Nobel physics prize lectures related to quantum mechanics, particularly related to the invention of the transistor (surface effects.) I suddenly realized that the physics of classical and quantum computing is not so separate and far apart — the main difference being that classical computing depends on quantum effects but uses statistical aggregation to hide the probabilistics effects to achieve a fairly decent approximation of determinism while quantum computing exploits the raw probabilistic effects.\n\nI’ve gradually built up a fairly significant network of LinkedIn contacts for quantum computing and quantum mechanics and physics over 2019 and 2020. That provides me with a decent news flow and links to interesting new academic papers. And a fair amount of interesting conversations on quantum computing as well.\n\nI always keep two Google News windows open, one for “quantum computer” and one for “quantum computing”. They provide me with a significant news flow as well, although LinkedIn status flow is usually enough for my needs and interests.\n\nPhys.org is a good source of news on quantum computing, at least in terms of fresh academic papers. They monitor new additions of papers to arXiv and post readable plain language descriptions, which frequently show up in Google News for quantum computing.\n\nI’m very anxious to see some significant advances and breakthroughs in the near future, but the past year hasn’t been as eventful as I had hoped.\n\nI do actually find myself growing somewhat disillusioned and disenchanted with quantum computing, at least in its current state. Too many of the promises remain unfulfilled and progress is slower than I expected two years ago.\n\nMaybe it might be better for me to take a two-year or even five-year Rip van Winkle-like slumber to skip over all of the waiting, anxiety, and disappointment and cut directly to quantum advantage or at least a much more impressive s"
    }
}
{
    "id": "dbpedia_5416_2",
    "rank": 22,
    "data": {
        "url": "https://www.mdpi.com/344776",
        "read_more_link": "",
        "language": "en",
        "title": "Using Imaging Spectrometry to Study Changes in Crop Area in California’s Central Valley during Drought",
        "top_image": "https://pub.mdpi-res.com/remotesensing/remotesensing-10-01556/article_deploy/html/images/remotesensing-10-01556-ag-550.jpg?1570520174",
        "meta_img": "https://pub.mdpi-res.com/remotesensing/remotesensing-10-01556/article_deploy/html/images/remotesensing-10-01556-ag-550.jpg?1570520174",
        "images": [
            "https://pub.mdpi-res.com/img/design/mdpi-pub-logo-black-small1.svg?da3a8dcae975a41c?1723640743",
            "https://pub.mdpi-res.com/img/design/mdpi-pub-logo-black-small1.svg?da3a8dcae975a41c?1723640743",
            "https://pub.mdpi-res.com/img/journals/remotesensing-logo.png?33ab614e9661caf2",
            "https://www.mdpi.com/bundles/mdpisciprofileslink/img/unknown-user.png",
            "https://pub.mdpi-res.com/img/design/orcid.png?0465bc3812adeb52?1723640743",
            "https://www.mdpi.com/bundles/mdpisciprofileslink/img/unknown-user.png",
            "https://pub.mdpi-res.com/img/design/orcid.png?0465bc3812adeb52?1723640743",
            "https://www.mdpi.com/bundles/mdpisciprofileslink/img/unknown-user.png",
            "https://www.mdpi.com/bundles/mdpisciprofileslink/img/unknown-user.png",
            "https://pub.mdpi-res.com/img/design/orcid.png?0465bc3812adeb52?1723640743",
            "https://pub.mdpi-res.com/remotesensing/remotesensing-10-01556/article_deploy/html/images/remotesensing-10-01556-ag-550.jpg?1570520174",
            "https://www.mdpi.com/remotesensing/remotesensing-10-01556/article_deploy/html/images/remotesensing-10-01556-g001-550.jpg",
            "https://www.mdpi.com/remotesensing/remotesensing-10-01556/article_deploy/html/images/remotesensing-10-01556-g001.png",
            "https://www.mdpi.com/remotesensing/remotesensing-10-01556/article_deploy/html/images/remotesensing-10-01556-g002-550.jpg",
            "https://www.mdpi.com/remotesensing/remotesensing-10-01556/article_deploy/html/images/remotesensing-10-01556-g002.png",
            "https://www.mdpi.com/remotesensing/remotesensing-10-01556/article_deploy/html/images/remotesensing-10-01556-g003-550.jpg",
            "https://www.mdpi.com/remotesensing/remotesensing-10-01556/article_deploy/html/images/remotesensing-10-01556-g003.png",
            "https://www.mdpi.com/remotesensing/remotesensing-10-01556/article_deploy/html/images/remotesensing-10-01556-g004-550.jpg",
            "https://www.mdpi.com/remotesensing/remotesensing-10-01556/article_deploy/html/images/remotesensing-10-01556-g004.png",
            "https://www.mdpi.com/remotesensing/remotesensing-10-01556/article_deploy/html/images/remotesensing-10-01556-g005-550.jpg",
            "https://www.mdpi.com/remotesensing/remotesensing-10-01556/article_deploy/html/images/remotesensing-10-01556-g005.png",
            "https://www.mdpi.com/remotesensing/remotesensing-10-01556/article_deploy/html/images/remotesensing-10-01556-g006-550.jpg",
            "https://www.mdpi.com/remotesensing/remotesensing-10-01556/article_deploy/html/images/remotesensing-10-01556-g006.png",
            "https://www.mdpi.com/remotesensing/remotesensing-10-01556/article_deploy/html/images/remotesensing-10-01556-g007-550.jpg",
            "https://www.mdpi.com/remotesensing/remotesensing-10-01556/article_deploy/html/images/remotesensing-10-01556-g007.png",
            "https://www.mdpi.com/remotesensing/remotesensing-10-01556/article_deploy/html/images/remotesensing-10-01556-g008-550.jpg",
            "https://www.mdpi.com/remotesensing/remotesensing-10-01556/article_deploy/html/images/remotesensing-10-01556-g008.png",
            "https://www.mdpi.com/remotesensing/remotesensing-10-01556/article_deploy/html/images/remotesensing-10-01556-g009-550.jpg",
            "https://www.mdpi.com/remotesensing/remotesensing-10-01556/article_deploy/html/images/remotesensing-10-01556-g009.png",
            "https://www.mdpi.com/img/table.png",
            "https://www.mdpi.com/img/table.png",
            "https://www.mdpi.com/img/table.png",
            "https://www.mdpi.com/img/table.png",
            "https://www.mdpi.com/img/table.png",
            "https://www.mdpi.com/img/table.png",
            "https://www.mdpi.com/img/table.png",
            "https://www.mdpi.com/img/table.png",
            "https://www.mdpi.com/img/table.png",
            "https://www.mdpi.com/img/table.png",
            "https://pub.mdpi-res.com/img/design/mdpi-pub-logo-white-small.png?71d18e5f805839ab?1723640743"
        ],
        "movies": [],
        "keywords": [],
        "meta_keywords": [
            ""
        ],
        "tags": null,
        "authors": [
            "Sarah W. Shivers",
            "Dar A. Roberts",
            "Joseph P. McFadden",
            "Christina Tague",
            "Sarah W",
            "Dar A",
            "Joseph P"
        ],
        "publish_date": "2018-09-27T00:00:00",
        "summary": "",
        "meta_description": "In California, predicted climate warming increases the likelihood of extreme droughts. As irrigated agriculture accounts for 80% of the state’s managed water supply, the response of the agricultural sector will play a large role in future drought impacts. This study examined one drought adaptation strategy, changes in planting decisions, using Airborne Visible/Infrared Imaging Spectrometer (AVIRIS) imagery from June 2013, 2014, and 2015 from the Central Valley of California. We used the random forest classifier to classify crops into categories of similar water use. Classification accuracy was assessed using the random forest out-of-bag accuracy, and an independently validated accuracy at both the pixel and field levels. These results were then compared to simulated Landsat Operational Land Imager (OLI) and simulated Sentinel-2B results. The classification was further analyzed for method portability and band importance. The resultant crop maps were used to analyze changes in crop area as one measure of agricultural adaptation in times of drought. The results showed overall field-level accuracies of 94.4% with AVIRIS, as opposed to 90.4% with Landsat OLI and 91.7% with Sentinel, indicating that hyperspectral imagery has the potential to identify crops by water-use group at a single time step at higher accuracies than multispectral sensors. Crop maps produced using the random forest classifier indicated that the total crop area decreased as the drought persisted from 2013 to 2015. Changes in area by crop type revealed that decisions regarding which crop to grow and which to fallow in times of drought were not driven by the average water requirements of crop groups, but rather showed possible linkages to crop value and/or crop permanence.",
        "meta_lang": "en",
        "meta_favicon": "https://pub.mdpi-res.com/img/mask-icon-128.svg?c1c7eca266cd7013?1723640743",
        "meta_site_name": "MDPI",
        "canonical_link": "https://www.mdpi.com/2072-4292/10/10/1556",
        "text": "by\n\nSarah W. Shivers\n\n1,* ,\n\nDar A. Roberts\n\n1 ,\n\nJoseph P. McFadden\n\n1 and\n\nChristina Tague\n\n2\n\n1\n\nDepartment of Geography, University of California, Santa Barbara, CA 93106, USA\n\n2\n\nBren School of Environmental Science & Management, University of California, Santa Barbara, CA 93106, USA\n\n*\n\nAuthor to whom correspondence should be addressed.\n\nRemote Sens. 2018, 10(10), 1556; https://doi.org/10.3390/rs10101556\n\nSubmission received: 13 August 2018 / Revised: 19 September 2018 / Accepted: 25 September 2018 / Published: 27 September 2018\n\n(This article belongs to the Section Remote Sensing in Agriculture and Vegetation)\n\nAbstract\n\n:\n\nIn California, predicted climate warming increases the likelihood of extreme droughts. As irrigated agriculture accounts for 80% of the state’s managed water supply, the response of the agricultural sector will play a large role in future drought impacts. This study examined one drought adaptation strategy, changes in planting decisions, using Airborne Visible/Infrared Imaging Spectrometer (AVIRIS) imagery from June 2013, 2014, and 2015 from the Central Valley of California. We used the random forest classifier to classify crops into categories of similar water use. Classification accuracy was assessed using the random forest out-of-bag accuracy, and an independently validated accuracy at both the pixel and field levels. These results were then compared to simulated Landsat Operational Land Imager (OLI) and simulated Sentinel-2B results. The classification was further analyzed for method portability and band importance. The resultant crop maps were used to analyze changes in crop area as one measure of agricultural adaptation in times of drought. The results showed overall field-level accuracies of 94.4% with AVIRIS, as opposed to 90.4% with Landsat OLI and 91.7% with Sentinel, indicating that hyperspectral imagery has the potential to identify crops by water-use group at a single time step at higher accuracies than multispectral sensors. Crop maps produced using the random forest classifier indicated that the total crop area decreased as the drought persisted from 2013 to 2015. Changes in area by crop type revealed that decisions regarding which crop to grow and which to fallow in times of drought were not driven by the average water requirements of crop groups, but rather showed possible linkages to crop value and/or crop permanence.\n\nGraphical Abstract\n\n1. Introduction\n\nAlthough California faces substantial variability in interannual precipitation and is accustomed to multi-year dry periods, the 2012 to 2016 drought was exceptional in its severity, and may be emblematic of greater shifts in California’s climate associated with anthropogenic warming [1,2,3]. Climate projections for California indicate that mean and extreme temperatures are likely to increase over the next century, which will increase the risk of experiencing future droughts of the severity of the 2012–2016 event [3,4]. Future droughts will undoubtedly continue to put strain on water supplies, but the magnitude and extent to which these events impact water resources will depend not only on the characteristics of the drought, but also on the adaptive responses of people [5,6,7,8]. In California, where the agriculture sector uses roughly 80% of the state’s managed water [9], agriculture simultaneously shows high vulnerability to a warming climate [10,11] while also offering the greatest opportunity to mitigate the intensity of future drought impacts through adaptation strategies [12,13,14]. Consequently, it is critical to study how we can monitor crop management response in real-time in order to assist with policymaking during drought and analyze the ways in which the long-term sustainability of food and water security can be improved. This research used annual hyperspectral remote sensing imagery to assess the accuracy at which imaging spectroscopy can be used to map crops into categories of similar water demand and analyze changes in cropping patterns in a portion of the Central Valley. The study takes advantage of data collected over three years of a multi-year drought as a unique opportunity to measure agricultural response and adaptation in times of drought.\n\nClimate change is likely to significantly affect regional agricultural patterns and crop yields [15], in part due to management decisions such as fallowing fields or switching crop varieties or species [16,17]. Therefore, monitoring how crop patterns change during droughts is a direct measure of adaptive response. Cropping decisions impact society in multiple ways by altering regional water requirements [18], food yields [16], economic production [19], and pesticide exposure [20]. Consequently, accurate and timely crop maps are necessary to support long-term adaptation planning for a broad range of sectors, and are of use to farmers, managers, policymakers, and scientists. Remote sensing has the potential to map crops and monitor changes in crop area more efficiently and frequently than time and labor-intensive on-the-ground crop accounting. Hyperspectral imagery, which samples hundreds of spectrally contiguous wavelengths, has the potential to identify crops at a single time point with a single sensor at higher accuracies than a broadband sensor [21]. This ability is critical to enabling managers and scientists to stay abreast of rapidly changing planting choices and assess current risks, which is a need that current mapping initiatives with remote sensing are unable to fulfill.\n\nMost remote sensing mapping initiatives in the United States rely on satellites such as Landsat and the Moderate Resolution Imaging Spectrometer (MODIS) because of their large spatial and temporal coverage, ease of accessibility, and free availability [22,23,24,25,26,27,28]. The National Agricultural Statistics Service (NASS)’s Cropland Data Layer (CDL) is the most comprehensive current agricultural mapping initiative for the United States with an easily accessible crop map published at yearly intervals at a 30-m resolution [29]. It relies on data from Deimos-1, the United Kingdom’s Disaster Monitoring Constellation 2 (UK-DMC 2), and Landsat 8 Operational Land Imager (OLI) and produced an overall accuracy of 81.1% in California in 2016, with accuracies of crop groups ranging from a low of 32.8% for berries to 77.6% for forage crops. Although widely used and highly useful, the CDL has limitations concerning reproducibility and timeliness. First, by using three sensors, not all of which produce publicly available data, reproducing this map or using this methodology on a different study area or at a different time would not be possible. Furthermore, with maps published at the end of each year, the CDL does not offer near real-time or mid-growing season assessments of crop area.\n\nAnother method of crop mapping uses multi-temporal MODIS imagery to classify crops using annual crop phenology for identification [23,26,28,30]. These studies illustrate the ability of time series datasets to produce detailed and accurate crop classification maps at the end of an agricultural year in a single study area, but this methodology also faces challenges that hinder its practical and scientific usefulness in California. First, the spatial resolution of MODIS is not fine enough to individually classify many fields. For example, the average size of a field in the area of this study is approximately 0.2 km2. Therefore, even at its finest resolution of 250 m, most MODIS pixels will result in mixtures of different fields or crop types, and are therefore best suited for croplands at larger scales [23]. Second, multi-temporal crop mapping is limited in its spatial scope due to a spatial variation in phenology that would decrease the accuracy if it was applied over a large spatial area [31]. Third, the co-registration of multiples images and the need for cloud-free images create challenges for time-series analysis that single-data hyperspectral analyses do not face [32]. Finally, the need for multiple images throughout time obviates the ability to conduct real-time crop assessments.\n\nHyperspectral imagery can act as a compliment to these current crop-mapping initiatives, as it has the potential to identify crops at a single time point with greater accuracy than broadband sensors, and therefore can provide mid-season assessments of crop area without a yearly time-series [21,33,34,35,36]. Discriminating crop types is challenging due to differing biophysical traits, development stages, variable management practices, regional weather and topography, and the timing of plantings [21]. Despite these complications, various studies have successfully shown the ability to use hyperspectral imagery to classify crops and cultivars [33,36,37,38,39,40,41]. By discriminating crop types with a single image from one time point, hyperspectral imagery can serve as a time-critical agricultural management tool, providing scientists, farm managers, and policymakers with improved information regarding the agricultural landscape and on-the-ground food and water needs. This study uses airborne hyperspectral imagery over a portion of the Central Valley to assess the accuracy of imaging spectroscopy for agricultural classifications and conducts a case study to display the utility of these classifications for analyzing changes in farming decisions. The results of this study, while limited in their spatial scope due to the use of airborne imagery, are salient in light of recently available Sentinel-2 data and the proposed HyspIRI mission, which would provide repeat, global hyperspectral imagery.\n\nThis paper explores the use of hyperspectral imagery for agricultural management in drought scenarios with the following two goals: (1) to study the accuracy to which we can map crops in California’s Central Valley into groups of similar water use using hyperspectral imagery; and (2) to analyze the produced classification maps from 2013, 2014, and 2015 in the context of the drought by examining the crop area changes in relationship to average crop water use, average crop value, and crop annual/perennial status. We expected to find consistency with previous analyses of the drought that found that higher-value, perennial crops were prioritized over lower-value annual crops over the course of the drought [17].\n\n2. Methods\n\n2.1. Study Area\n\nThis research focused on a 3470 km2 transect of the Central Valley of California that reaches from Kern County to Fresno County, and was flown as part of a large trans-state flight path that is referred to as the “Soda Straw” in NASA’s Hyperspectral Infrared Imager (HyspIRI) Airborne Campaign plan (Figure 1). The study area includes portions of three of the top four leading agricultural counties in California, Fresno, Tulare, and Kern, as well as portions of Kings County [19]. The greater study area in which the agricultural fields reside is the Tulare Lake Hydrologic Region, which comprises the southern third of the Central Valley, and is the largest agricultural region in California with about 1.2 million irrigated hectares within the 4.4 million hectares that it covers [42]. The region is prosperous for agriculture partially because of its long growing season, with moist winters often blanketed with fog and dry summers [42]. While prosperous for agriculture, Tulare Lake is the driest region of the Central Valley, receiving an average of less than 25.4 cm of precipitation a year [42,43].\n\n2.2. Datasets\n\n2.2.1. Imagery\n\nThis research focused on three images of the Central Valley of California collected by the Airborne Visible/Infrared Imaging Spectrometer (AVIRIS) during the course of the 2012–2016 drought. AVIRIS is a 224-band imaging spectrometer that captures wavelengths between 350–2500 nm at 10-nm increments [44]. The imagery was acquired from NASA’s Jet Propulsion Lab (JPL), which pre-processed the imagery and produced a data product of orthorectified, atmospherically corrected reflectance at a spatial resolution of 18 m [45]. Data was acquired from the JPL ftp (https://aviris.jpl.nasa.gov/data/AV_HyspIRI_Prep_Data.html). Further processing of the reflectance data included a rotation and secondary orthorectification using 1-m digital orthophoto quarter quads (DOQQ) from the National Agriculture Imagery Program (NAIP) that were resampled to 18 m. We used Delaunay triangulation to georectify the AVIRIS imagery to the NAIP imagery, using approximately 250 ground control points per flight line. Due to atmospheric absorption and the noise of the reflectance product, AVIRIS bands 1–5, 61–63, 81, 106–120, 152–174, and 220–224 were not used, leaving a total of 172 bands for further analysis. This imagery was collected as part of the HyspIRI Airborne Campaign, a NASA mission that captured airborne imagery from various areas of California between 2013–2017 to investigate the capabilities of combined hyperspectral and thermal sensing for a proposed satellite mission with global coverage [46]. We used imagery from three flights that were flown on 6 June 2013, 3 June 2014, and 2 June 2015, capturing the peak summer growing season for many Central Valley crops.\n\nThe imagery was captured during severe drought conditions in California with high temperatures and persistent below-average rainfall, which led to a state of drought emergency declared in January 2014 and lifted in April 2017 [47]. The three-year event from 2012 to 2014 is estimated to be the most severe drought in 1200 years, while the cumulative effect of the drought from 2012 to 2015 is unprecedented in its severity [2,48]. As another metric of the severity of the drought, the United States (US) Drought Monitor reported that for the week closest to the image acquisition in 2013, over 53% of the state was facing severe, extreme, or exceptional drought. By June 2014, that percentage was up to 100%, and by June 2015, over 93% of the state still remained in these most severe drought categories [49].\n\n2.2.2. Crop Polygons\n\nTo train and validate the crop classification, we relied upon digitized polygons of crop fields provided by Kern, Kings, Fresno, and Tulare counties. The Kern County data layer was acquired through the Kern County Spatial Data website (http://www.kernag.com/gis/gis-data.asp). The Kings, Tulare, and Fresno shapefiles were acquired from each of the respective counties at the request of the researcher. These crop layers are compiled each year as part of a statewide pesticide permitting and use reporting program, which requires farmers in California to obtain permits for any pesticide use. The program resulted in county shapefiles, which detailed the distribution of crops and their pesticide use, being distributed to the public at the discretion of each county. Training and validation data were filtered to only include crops that were registered as being planted prior to and harvested after the flight date. Fields registered as containing interplantings and multicrops accounted for roughly 5% of crop polygons, and were excluded. It needs to be noted that these geographic information system (GIS) data layers obtained from the counties were not guaranteed to be comprehensive. In fact, this study operates on the assumption that the validation layers provide information on only a portion of the total study area, and are not exhaustive. Therefore, the increase or decrease in the number of validation polygons for each year cannot be taken to be indicative of the actual change in crops, but rather indicative of the completeness of the validation data.\n\n2.3. Spectral Mixture Analysis\n\nIn order to separate soil or fallow pixels from those of agricultural plant matter, a spectral mixture analysis was run on each of the three images to obtain fractional green vegetation cover. Multiple Endmember Spectra Mixture Analysis (MESMA) [50] uses a linear mixture model to unmix pixels into fraction images while allowing the number and types of endmembers to vary on a per-pixel basis, thus better accounting for endmember variability. Pixels were modeled as a mixture of green vegetation (GV), soil, non-photosynthetic vegetation (NPV), and shade. Image endmembers were chosen from each of the three images from 2013, 2014, and 2015 by selecting pixels with high overall reflectance from each of the three endmember categories that were well-distributed spatially throughout the image in order to capture the variability from north to south along the flight line. A combined library of all of the chosen endmembers, consisting of eight NPV, 10 Soil, and 21 GV endmembers, was used for analysis in order to obtain consistent results throughout the years. MESMA was partially constrained by requiring shade fractions to vary between 0–0.8, and setting a maximum allowable root mean squared error (RMSE) of 0.025. The spectral mixture result was then shade normalized by dividing each non-shade component, GV, NPV, and soil by the sum total of all of the non-shade components in that pixel to obtain physically realistic fraction estimates [51]. Only those pixels that contained 50% or more shade-normalized GV were chosen for training and validation, as this was decided as the threshold for classifying a pixel as a crop.\n\n2.4. Classification\n\n2.4.1. Class Selection\n\nDue to the high diversity of crop species in the Central Valley, we focused on a smaller set of crop classes that would be of the most practical use to stakeholders such as water managers, farmers, and scientists. Crops were classified into categories defined and used by the California Department of Water Resources (DWR) to estimate water use (Table 1) [52]. The crops within each category have similar rates of development, rooting depths, and soil characteristics, and are therefore presumed to have similar water requirements. Categories were included in the classification if they were prominent in the area, defined as ≥20 fields of that category, each of which contained ≥50% green vegetation, in the validation layers (Table 2). The nine crop categories that had a sufficient number of fields to be included in this study were alfalfa, almonds and pistachios, corn, cotton, other deciduous crops, other truck crops, subtropical, tomatoes, and vines. Other crops that are grown in the area but are not being studied included cucurbits, grains, pasture, safflower, and sugar beet. Since these “other” crops are not similar in structure or phenology, we did not attempt to group them into a combined category for classification. In other agricultural regions where less frequent crops show a higher degree of similarity, adding an “other crops” group to the classification could be an appropriate way to decrease error. However, the number of fields and total area for each crop show that all of the “other” crops accounted for less than 1% of the total validated area in each of the three years, leading us to the assumption that the error due to the omission of these crops in our classification and crop area calculations will be low.\n\n2.4.2. Random Forest\n\nThe random forest classifier [53] is an ensemble classification and regression technique that creates a forest of classification trees by randomly selecting subsets of the training data with replacement for each tree, randomly selecting a variable to split at each node, and then creating a multitude of decision trees that vote for the most popular class. Random forest was chosen for this study due to its computational efficiency and proven high performance [54,55]. Five hundred trees were computed using 150,000 cases of nine crop classes with 172 spectral variables.\n\nWe randomly selected fields from each year to be used for either training or validation in order to minimize inflated accuracies due to spatial autocorrelation. Seventy percent of fields were assigned as training data, while the other 30% were set aside for independent validation. From the training fields, 50,000 pixels from each year were randomly chosen from the pixels that contained ≥50% green vegetation, and then combined, creating a training set of 150,000 pixels across the three dates. As suggested by Millard and Richardson [56], pixels were randomly sampled to create a training dataset that was representative of the true class proportions within the study area. A random forest was generated from these 150,000 pixels, to be used to classify each of the three images.\n\nA 50% GV threshold was used as the cutoff for selecting pixels for training and validation in the random forest in order to maximize accuracy while also maintaining validation data for the most infrequent categories, particularly the tomato, cotton, and other truck crop classes that had the fewest training fields. To determine the threshold, 10 trial random forests were run to estimate the classification accuracy for each crop class using threshold levels ranging from 10% GV to 100% GV at 10% increments. Each run used 10,000 randomly selected pixels and populated 500 trees. We found that while the majority of the crop classes increased in accuracy as the threshold increased, the tomato class began declining in accuracy after the 50% threshold due to a significantly reduced training sample size, and there were no training or validation data available in our study area when the GV threshold was ≥80% (Figure 2). Therefore, in order to include tomatoes in our classification, we chose a 50% GV threshold. Additionally, mixed pixels may not be spectrally similar to the pure pixels of the classes that they contain, so using only pure pixels to train a classifier can increase the error in areas with a high proportion of mixed pixels [57]. To this point, we aimed to choose a threshold that could capture diversity within each crop category, and felt that a high GV threshold would be restrictive in that it may exclude younger crops or certain species of crops within each class (e.g., widely spaced trees in orchards). Therefore, 50% was chosen as a compromise between attaining high accuracy within all of the classes while also fairly representing the diversity within each crop class. Training a classifier on mixed pixels for agricultural applications has been shown to have similar accuracy as training on pure pixels [58].\n\nFor each year, a pixel-level classification was generated using the multi-year random forest. From this image classification, independent validation was conducted using the 30% of fields that were not used for training. From the validation fields 10,000 pixels containing at least 50% or more green vegetation were randomly chosen from each image for a total of 30,000 validation pixels over the three dates.\n\n2.4.3. Field-Level Reclassification\n\nSince multicroppings or interplantings were excluded from the training and validation, each field was assumed to be growing only one crop type. Therefore, to improve the results of the classifier for analysis of changes in crop area, a majority filter was applied to the random forest classification result to reclassify each pixel of a field as the crop category to which the plurality of pixels in that field were classified. For example, if 10% of the pixels in a field were classified as tomato, 30% as alfalfa, and 60% as corn, all of the pixels in that field were reclassified as corn. Fields and their boundaries were defined from the crop polygon validation database. Only fields that contained a certain threshold of green vegetation were included in the field level reclassification in order to remove fallow fields from analysis. Two different field-level GV thresholds, 25% and 50%, were chosen to assess the impact of a field-level threshold on accuracy results. Final crop planting assessments were conducted using the 50% field-level threshold for an increased accuracy of analysis.\n\n2.5. Accuracy Assessments\n\nThree different classification accuracies were computed and will be discussed. The first is the pixel-level out-of-bag (OOB) error calculated by the random forest. The OOB error is an estimate of error that uses subsampling and bootstrapping to estimate the error of a sample, using only trees of the random forest that do not include the data point being validated [53]. The second reported accuracy is pixel-level independent validation using classified pixels that were not included in training the random forest and were not in the same field as any pixel in the random forest training set. The third accuracy is a field-level accuracy using the majority reclassification of pixels in each field.\n\n2.5.1. Multispectral Imager Comparisons\n\nTo assess the benefit of a 224-band spectrometer such as AVIRIS over more commonly available multispectral sensors, a random forest classifier was run with simulated Landsat Operational Land Imager (OLI) and simulated Sentinel-2B data for accuracy comparison. AVIRIS images from all three of the dates were spectrally convolved to Landsat OLI bands 1–7 and 9 and to all of the bands of Sentinel-2B. The spatial resolution was kept constant at 18 m. These simulated images were then run in random forest using 500 trees, the same nine crop categories, and the same 150,000 training points that were used for the AVIRIS classification. OOB accuracy at the pixel level and field level were both computed for analysis.\n\n2.5.2. Portability Analysis\n\nTo evaluate the portability of this method to images that are not included in the classification, three trial random forests were run where each random forest was trained with two of the study images, and then tested on the third image. For each of these random forest runs, 50,000 pixels from each of the two training dates were used to train the random forest, which populated 500 trees. Field-level accuracy is assessed for each of the three resultant classifications.\n\n2.6. Case Study on Farmer Decision-Making\n\nUsing random forest and the majority-filtered reclassification with a 50% GV threshold, predictive crop maps were generated for each year. After random forest was run on each AVIRIS image, a multi-year field polygon layer was used to identify individual fields for majority reclassification. Therefore, classified fields were not constrained to those fields that were included in the validation layer from a specific year, but could include any field in the study area that contained 50% or more green vegetation, whether it was registered as part of the validation layer or not. From these maps, crop area was assessed to analyze changes in cropping patterns within the study area over the course of the drought. We then used these maps to evaluate the hypothesis that higher-value, perennial crops were prioritized during the drought by analyzing factors including water use, economic value, and crop lifespan against the change in the planted area.\n\n3. Results\n\n3.1. Classification Accuracy\n\nComparative error between the three accuracy assessments were broken down by class and year (Table 3), and then each accuracy assessment was individually analyzed and broken down by crop class in Table 4, Table 5 and Table 6.\n\n3.1.1. Out-of-Bag Accuracy\n\nThe random forest classifier showed high overall accuracy of 93.8% when accounting for all of the years and all of the crops. Table 4 details the errors by crop class through an error matrix that sums to 150,000 points, which is the number of pixels that was used to train and build the random forest. With over two million possible training pixels, the pixels used in the random forest account for 7.4% of the total. All of the crops had user and producer accuracies over 84%, and alfalfa, almond and pistachio, corn, and tomatoes resulted in accuracies of over 90%. Since training pixels were selected randomly throughout the image, the number of pixels per class was representative of the proportional crop area in the study site. Therefore, the number of pixels was highly variable by class, with almond and pistachio pixels amounting to more than 64,000, while less prevalent crops such as tomato, cotton, and other truck crops had fewer than 2000 pixels each.\n\n3.1.2. Independent Validation\n\nIndependent validation produced an overall accuracy of 89.6% when accounting for all of the years and all of the crops, which was lower than the OOB accuracy by around 4%. This decline in accuracy when validating independently is likely because the independent validation accounted for the potential of spatial autocorrelation, which is likely to inflate OOB results. The OOB assessment used the same data for training the classifier as for validation, whereas the independent validation relied on 30,000 pixels randomly selected from polygons separate from those used in training. These 30,000 pixels made up only 1.6% of the 1.87 million potential validation pixels; those with GV greater than 50% that were not used in the random forest. The tree categories, almond/pistachio, other deciduous, and subtropical had the highest consistency between years with overall accuracies changed by less than 4% between the years (Table 3). Cotton and truck crops were less consistent in accuracy from year to year than the other crop categories, and this inconsistency may be due, in part, to these two classes having two of the three fewest numbers of pixels used in the random forest. Accuracy for other truck crops in 2015 was not applicable (NA), because the user’s accuracy was NA for that year (zero correctly classified truck crop pixels/zero total classified truck crop pixels). Accuracy was NA for tomato fields in 2013 because no tomato pixels were identified in the validation layer for that year. Despite this omission, tomatoes were included in the study, as they are a major crop group in the area with a sufficient number of training and validation polygons from 2014 to 2015.\n\nTable 5 details the errors associated with each crop type. The classification of alfalfa resulted in high accuracies of near or over 90%. Almond and pistachio trees showed consistently high user and producer accuracies of 94.0% and 97.2%, respectively. The random forest was more likely to erroneously classify other crop classes as almond and pistachio than it was to misclassify almond and pistachio trees. This result was likely because the pixels of almond and pistachio trees were very prevalent in the study area, leading to a large amount of randomly sampled pixels for training, and leading the classifier to favor this class over less frequently occurring classes. Other deciduous crops and subtropical crops were most likely to be misclassified by or as each other or as almond and pistachio, illustrating that tree crops were likely to be misclassified as another tree crop. Importantly, the three tree crop categories showed a tendency toward being overmapped, while the other six classes of non-tree species were all undermapped by the classifier. Of those, cotton and other truck were the most likely to be undermapped, with producer’s accuracies of 49.8% and 56.7%, respectively. The results showed that the classes that were more prevalent and had more validation data were more likely to have higher accuracy than the infrequent classes.\n\n3.1.3. Field-Level Validation after Majority Filter\n\nThe final majority-filtered reclassification of pixels to create fields that contained only one crop type had the highest accuracy at 94.4%. The overall accuracy is computed as the percentage of total fields that were correctly classified using random forest and a majority filter. The higher field-level accuracy obtained by the reclassification confirmed assumptions that using a majority filter would smooth out the stray pixels that may lie between rows of crops or that capture weeds or other plant matter growing near the crop, which may lead to classification confusion. When assessing the majority-filtered reclassification results by year (Table 3), seven of the nine classes had accuracies over 80% in all three years, with other truck crops and cotton being the exceptions. An important finding is that the field-level classification improved the pixel-level classification the majority of the time when a 50% GV threshold was used at the field level. When assessing pixel and field-level accuracies over all three years (Table 5 and Table 6), the only accuracy that decreased from the pixel to the field level was the user’s accuracy of almond and pistachio orchards. As the most common crop category, assessing the accuracy at the field level increased the overmapping of this class. However, all of the other user and producer’s accuracies increased. When looking at the accuracies separated by year and by class (Table 3), 22 of the 27 classes improved in accuracy from the pixel to the field level.\n\nThe field-level accuracies in Table 3 and Table 6 were computed using a 50% GV threshold, meaning that only fields that had at least 50% green vegetation or more were included in the accuracy assessment. The GV threshold represents a trade-off between accuracy and inclusivity. A higher GV threshold will decrease the risk of including fallow fields in the classification, but it will also increase the risk of excluding fields of crops that should be included. Table S1 details accuracy by class with a 25% threshold for comparison. Using a 50% GV threshold increases both user and producer’s accuracies for all of the crops over the 25% GV threshold, while excluding almost 1700 fields with an average green vegetation fraction between 25–50%. When the higher threshold is used, infrequent crop categories such as cotton and other truck crops increase substantially in producer’s accuracy, and prevalent crop categories, such as almond/pistachio and other deciduous crops, increase substantially in user’s accuracy. The random forest is most likely to classify ambiguous pixels as one of the most common classes, and therefore overclassifies common classes and underclassifies rarer classes when a lower field threshold is used.\n\nWith a 50% GV threshold, alfalfa, corn, other deciduous, subtropical, and tomato crops all showed user and producer accuracies over 90%. Of all of the crop categories, cotton had the lowest user and producer accuracies. The low accuracy of cotton may be attributable to a few different reasons. First, the sample size was small (22 fields with greater than 50% GV), so training data was limited. Second, early June was likely not the best time of year for the classification of cotton, because cotton is planted in March and April, but does not reach maturity for 180–200 days [59]. Later in the summer may be a better time of year for quantifying total cotton extent. However, while the user and producer accuracies of cotton were low, only 22 fields of 4110 total fields in our dataset were planted in cotton. Therefore, the overall classification accuracy is not impacted much by this error.\n\n3.1.4. Band Importance\n\nTo better understand the success of the random forest classifier, the mean decrease accuracy (MDA) was calculated for each wavelength (Figure 3). MDA can be interpreted as the number of pixels that would be misclassified if a given wavelength was removed from the random forest classifier. The 400–750 nm range, encompassing visible and red-edge portions of the spectrum, showed overall higher MDA scores than the near infrared (NIR) or shortwave infrared (SWIR) regions. The red-edge region in particular had the highest MDA score, followed by the blue region in the 400-nm range. Additionally, wavelengths on the edge of bands that were removed due to the high influence of water vapor showed higher importance than bands that did not border a water vapor region. For example, the NIR band at 957 nm showed a higher MDA score than any other band in the NIR. Some possible explanations for these results will be explored further in the discussion. Although 704 nm had the highest MDA, only about 80 pixels would be misclassified out of 150,000 if it were removed. The small error is likely due to the high correlation between bands.\n\n3.1.5. Landsat and Sentinel Comparisons\n\nWe used pixel-level and field-level classification accuracies to compare the results of the random forest classifier on AVIRIS, Landsat-simulated, and Sentinel-simulated data (Figure 4). The Landsat and Sentinel-2 simulated datasets had the same spatial resolution and signal-to-noise ratio (SNR) as the AVIRIS data. The only difference between the three datasets was their spectral resolutions. The results showed that AVIRIS performed better than or on par with either of the simulated sensors at both the pixel and field levels. Overall field-level accuracies were 90.4% with Landsat and 91.7% with Sentinel, as opposed to 94.4% with AVIRIS. Overall accuracies for both Landsat and Sentinel for all of the crop classes remained high with over 70% accuracy at the field level. The accuracies of the three sensors were within 10% of each other at the field level for every crop. Sentinel was less accurate than AVIRIS for most of the crop categories, but it was more accurate than Landsat.\n\n3.1.6. Portability Assessment\n\nThe portability of the hyperspectral random forest classification was tested by training the random forest on two of the three years, and then using that random forest to classify the third year. Accuracy was assessed at the field level. The results of the portability runs show high variability in accuracy between years (Table 7). For example, other deciduous crops had a high accuracy of approximately 87% when classifying fields in 2013 and 2015, but accuracy of only 54.1% in 2014. Other truck crops and cotton had the lowest accuracies in every year, with tomato crops being completely unable to be classified without a multi-year library, and cotton having a highest yearly accuracy of only 32.1%. Other truck crops and tomatoes had NA accuracies in some years, which resulted from a user’s accuracy of NA that was calculated when dividing zero correctly classified fields by zero total fields classified as that crop. Almond and pistachios had the greatest consistency in accuracy across the three years, with changes in accuracy of less than 10% between the three years.\n\n3.2. Central Valley Case Study: Changes in Cropping Patterns\n\nVisual analysis of the crop classification maps from 2013, 2014, and 2015 showed overall good cohesion between the general spatial patterns throughout time. Figure 5 shows the field-level classification of the study area from 2013 as an example of the overall cropping patterns that did not shift over the course of the drought, while Figure 6 and Figure 7 highlight finer scale changes in cropping over the course of the drought. The subtropical, other deciduous, and vine crops were concentrated in the northern part of the study area, and the almond and pistachio fields were mainly in the southern portion of the site. The other crop types were scattered throughout the flight line.\n\nThe number of fields classified in each year declined over time, with a decrease of around 100 fields a year (Table 8). Table 8 also details the percentage of these classified fields that were included in the validation layer for that year. The results showed that overall, 58.7% of the classified fields were not included in the validation polygon layer. This discrepancy reflected incomplete data in the county crop maps, indicating that a majority of the actively growing fields in the study area were not recorded in the pesticide-permitting database. This underscores the value of remote sensing methods for analyzing the planted crop area.\n\nSummation of the crop area from 2013 to 2015 (Table 9) showed that, similar to the number of fields classified, the overall crop area declined each year, with approximate declines of 32 km2 over two years. Alfalfa, cotton, corn, and truck crops all showed marked declines from 2013 to 2015 with decreases in area of approximately 29%, 50%, 84%, and 89%, respectively, in those two years, as exemplified in Figure 6. Tree crops including almond and pistachio, other deciduous trees, and subtropical crops showed some fluctuations over the three years, but had overall greater consistency over time than the other crop classes. From 2013 to 2015, those three tree crop categories showed the smallest fluctuations in crop area, with less than 7% overall change for any of those groups. Tomatoes and vines were the two categories that increased over the course of the drought, with tomatoes showing a 128% increase and vines a 34% increase from 2013 to 2015 (Figure 7; Table 9).\n\n3.3. Central Valley Case Study: Crop Area in Relation to Environmental and Economic Drivers\n\nUsing average water inputs per area in the Tulare Lake Hydrologic Region for each of these crop categories, the estimated required water inputs are shown in Table 10. The overall water requirement of the study site was estimated to have declined over the course of the drought by around 0.02 km3 of water yearly, due to increases in fallowed land. Figure 8 shows an average water requirement per crop group by the change in planted area from 2013 to 2015 to better understand whether the water requirement of a crop was a key factor in planting decisions. This analysis revealed no strong correlation between the change in crop area and average water inputs, showing that the crops that needed the largest water inputs did not experience larger declines in area than the crops that required less water.\n\nCrop market value, crop value per unit water, and shifts in value over the drought may have played a role in planting decisions. It is interesting to note that tomatoes and grape vines were the two crops that showed increases in area as the drought progressed, and they are also the crops that have a high value per area. In 2014, fresh tomatoes had an average value in California of $27,088 per hectare and grapes (all varieties, wine and raisin) of $14,960 per hectare. By contrast, the value per hectare for alfalfa, corn, and cotton were $3121, $1915, and $5772, respectively [19]. While a detailed economic analysis is outside of the scope of this paper, the increase in area of higher value crops over the course of the drought is noteworthy and warrants further study.\n\nFinally, crop lifespan was analyzed in conjunction with changes in cropping area. Figure 9 shows changes in cropping area from 2013 to 2015 segmented by the average number of years that a crop is in production once planted. The results showed the greatest declines occurred in annual crops. The mid-lifespan group had smaller percentage declines, and the long lifespan group showed a small increase in the percentage of crop area over the span of the drought. The results indicated that there is a correlation between lifespan and change in crop area that is likely to be of importance when studying the agricultural impacts of future drought events.\n\n4. Discussion\n\n4.1. Challenges and Caveats\n\nMapping crops is an integral part of understanding the water budget and needs of agricultural areas. However, mapping managed vegetation with remote sensing presents a set of challenges that are unique from natural vegetation. For one, annual crops are regularly planted, grown, harvested, and fallowed within a few months’ time. Land cover can change rapidly, and fields include crops in various stages of growth that may lead to large variations in plant cover, soil fraction, and shade. Secondly, crops are actively managed. Two fields of the same crop, side by side, may receive different management strategies in regard to pesticides, planting geometry, soil additives, soil type, irrigation system and schedule, and the timing of planting. These differences may lead to significant spectral variations within crop types [21,60]. Thirdly, because crops are commonly rotated and fallowed, validation information needs to be precise in time in order to accurately capture the agricultural landscape that coincides with the flight schedule. The goal of this study was to determine how useful remotely sensed crop maps could be, given these caveats, for agricultural management at a regional level.\n\nOur estimates of crop extent may have been affected by different sources of error. First, because this study used a cutoff of 50% green vegetation or more at the pixel and field level, the final map likely undermapped crop area. Of the 8272 total fields registered in the validation data layers as having a crop growing at the time of the flights, 4114 (49.7%) had between zero and 50% green vegetation as assessed by MESMA, and were therefore not included in the analysis. Fields with less than 50% GV fraction could be indicative of recent harvests, new plantings, young crops, or fallowed land, which do not all result in the undermapping of active crop area. Additionally, because of this threshold, yearly changes in crop extent, particularly of tree crops, could indicate growth of a crop instead of a new planting. For example, a field of almond trees may have a 45% green vegetation fraction in 2013, 48% in 2014, and 51% in 2015. In this case, the almond field would only be mapped in 2015, and would imply a sudden increase in planted almond extent, although it had existed for multiple years. Underestimating area is not unique to our study; it is a challenge of hard classifications in general. Areal estimation of classes that are changing tend to be underestimated, while the magnitude of that change is overestimated when classified pixels are multiplied by the pixel size to the estimated area [61]. One possible solution is to apply a regression correction such as implemented by the National Agricultural Statistics Service’s Cropland Data Layer (CDL). The CDL correction uses a sample site with ground-truth area data to formulate a regression between CDL estimates and truth that is then applied to the CDL data to adjust the estimates [62]. Such a correction would be advantageous for future studies, but it does require additional on-the-ground validation data that was not available for this study.\n\nSecond, some green fields in the study area contained a crop that was not included in one of the nine mapped crop groups. However, the crops that were not included in the study comprised less than 1% of the crop area, so the errors associated with their absence in the study are assumed to be low. Third, crop rotation may be one reason that the extent of annual crops fluctuated more than the perennial crops. Crop rotation was not accounted for in our crop extent estimates, as one image per year is not adequate for capturing such changes. However, if crops are being rotated, we would still expect the total area of annual crops to stay somewhat stable through time, even if specific annual crops showed large fluctuations. Our finding that the total area of annual crops declined with drought is therefore not likely to be attributable to crop rotation.\n\nFinally, given that the study only used one time point in June for each of the three years, changes in estimated area could be indicative of a change in the timing of planted crops in addition to actual fallowing or crop substitution. For example, if farmers planted their tomato fields three weeks later in 2014 than they planted them in 2013, the tomatoes may not have reached the 50% green vegetation threshold by early June in 2014, which would have led to underestimates of tomatoes in that year. Other studies have relied on time series analyses to confirm fields as fallow [63,64], which suggests that a satellite-based imager with a moderate spatial resolution such as Landsat OLI could be used in tandem with aerial imagery to enhance the validity of fallowing results.\n\n4.2. Crop Classification with Imaging Spectroscopy\n\nThe classification results showed high overall accuracies of over 89% at the pixel level and 94% at the field level. High accuracy was expected, in accordance with previous studies that have discriminated crop types with hyperspectral sensors, but these results are unique in that they discriminate crops in a highly diverse agricultural landscape as compared to previous studies that largely focused on large-plot, staple crops [33,37,38,39,40,41]. Therefore, the results of this classification enhance the practical relevance of using an imaging spectrometer in agricultural areas that host a large variety of crop species, such as California.\n\nThe accuracies obtained in this study from AVIRIS exceeded those from simulated Landsat OLI and Sentinel-2B imagery, although even the Landsat and Sentinel results had high accuracies at the field level of above 85% accuracy for eight of the nine crop categories. The increased accuracy of AVIRIS over multispectral sensors is in contrast with the results of Clark [65], who found no classification improvement of hyperspectral data over multispectral data when classifying land cover in California, but showed similar results to Platt and Goetz [66], who found modest advantages of AVIRIS over Landsat for classifications at the urban–rural fringe. Since true Landsat and Sentinel imagery will have coarser spatial resolutions and a worse signal-to-noise ratio, we hypothesize that the accuracies of this classification would decrease if actual imagery, and not simulated imagery, were used. However, given the high accuracies of the simulated data, it seems that the extra spectral bands of AVIRIS, while somewhat advantageous for crop classification, do not confer a large added benefit over the 12 Sentinel bands.\n\nThe sensor comparison and the results of the band importance analysis imply that 172 spectral bands, such as those that were used in this study, are somewhat redundant and could likely be pared down without losing much accuracy. The band analysis highlighted the most important wavelengths and regions of the spectrum for the random forest classifier, and pointed to biochemicals and structure as drivers of crop discrimination. The 400–750 nm region was particularly significant, especially the blue and red-edge regions, showing that the shape of the green peak and the chlorophyll absorptions provide valuable information for crop discrimination. The red-edge region has been shown to vary by vegetation stress, species, and time of year, which can all change the slope and inflection point of the red edge [67,68,69]. The blue region is sensitive to chlorophyll-a absorption and has been linked to senescence, carotenoids, and browning [35,70]. The finding that these two regions are important for crop discrimination is similar to Immitzer et al. [71], who found the red edge and the blue bands to be the two most important bands for crop classification using Sentinel-2 data. Sentinel-2 contains three bands in the highly valuable red-edge region that may partially account for its ability to classify crops at accuracies that rival AVIRIS. However, when comparing the blue region, this study found bands 414 nm and 424 nm to be of high value for crop discrimination, which are shorter than the Sentinel blue band at 490 nm. The importance of the shorter wavelength blue bands, bordering on the ultraviolet (UV) region, suggests that the UV may contain a wealth of crop information concerning characteristics such as wax deposition and metabolites such as flavonoids and phenolics [72] that could be utilized for discrimination if atmospheric scattering did not inhibit its use. Outside of the visible range, the liquid water feature at 957 nm was identified as important, and many of the bands near the edges of liquid water regions show high band importance. This result suggests that plant water content and structure are important drivers of discrimination [67].\n\nWhile AVIRIS showed modestly higher accuracies for single image crop classification over Sentinel-2 and Landsat OLI, hyperspectral and multispectral imagers each have unique strengths that can be complimentary for agricultural analysis. Multispectral data may be preferred to hyperspectral data for its ease of use, shorter processing times, and high temporal resolution. With rich time series of data, multispectral imagers are especially valuable for agricultural analysis in application areas such as crop rotation, fallowing, and crop development, and have shown high accuracies when discriminating crops using time series [26,64,73,74]. However, time-series data is not always acquirable, especially in cloud-prone areas. Hyperspectral imagers have high discriminating power that allow for more accurate single-date classifications that would be highly useful in these areas. The added information in hyperspectral spectrometers also allow for more accurate mid-season growing assessments before time-series data become available. Moreover, beyond classifications, the narrow bands of hyperspectral imagers make them well-suited for attaining biochemical and physiological plant information [21,36]. Therefore, the combination of these two technologies can allow for classifications under a broad range of circumstances (mid-season versus end of season) and spatial areas (cloud prone versus clear) combined with additional analyses of biochemical properties, fallowing patterns, and development that can be of high importance for agricultural researchers and managers.\n\nThe increased accuracy of field-level classification results over pixel-level results with AVIRIS suggest that it would be important to first develop an accurate map of agricultural field boundaries if this method were to be applied in other agricultural areas where GIS data on farms do not already exist. A possible solution is to use an automated computational methodology that extracts individual crop fields from Landsat ETM and TM time series, as detailed in Yan and Roy [27]. This field extraction technique or other object-oriented methods could be used to identify fields for future crop classification studies.\n\nOne challenge of mapping crops is the portability of a methodology across time. First, crops imaged at different times are in many different phenological stages. We aimed to capture phenological variability through a multispectral library as suggested by Dudley et al. [75], who found that the more variability that is captured within the training data, the more portable the method. We hypothesize that the high degree of variability between accuracies trained on two images and tested on a third is attributed partially to how much of the total variability was captured by the two images. For example, the accuracy of corn in 2014 and trained with 2013 and 2015 was 68.8% as compared with 86.7% and 89.9% in the other two years. It is possible that corn in 2013 and 2015 was in a more similar phenological stage than the corn in the 2014 imagery, so a training library from 2013 and 2015 was unable to capture the spectral variability of corn in 2014. These results suggest that a larger training dataset that includes more variability through additional spectra from other years or other times of year could enhance the portability. Second, portability has to do with the consistency of the dataset and data products. The reflectance retrieval of the AVIRIS images left visible spectral artifacts that varied by image date. Since AVIRIS is an airborne spectrometer and the data products are not automated to the same degree as those from a satellite such as Landsat, there is variability between image products that make portability even more challenging. A spaceborne imaging platform would minimize these challenges and increase the consistency of the products.\n\nIt is important to note that the assessed accuracies of this study’s classification are with the assumption that the validation layer is 100% accurate, which may not be true. Given that the accuracy of the validation layer relies upon timely and accurate permit filing by farmers and the appropriate registration of such information by the county, errors may be present in the data. However, if there are errors in the validation layer, they would contribute to confusion in the classification, thereby producing an accuracy that is lower than it really is. Therefore, we assume that the true accuracy of the classification is as high or higher than the assessed accuracies. With perfect validation data, it may be possible to increase accuracy.\n\n4.3. Implications for Agricultural Management\n\nThis study’s findings of fallowing and changes in plantings over the course of the drought, along with the assessed linkages to relevant drivers, serve as a case study to better understand the utility of crop classifications in drought-prone areas that rely on irrigation. Our results concerning the cropped area and possible drivers are consistent with other economic, policy, and scientific analyses of the 2012–2016 California drought [17,76,77,78,79]. Table 9 shows that as the drought worsened, overall crop area decreased in the study area, which is a finding that echoes similar assessments of fallowed cropland over California during this drought [78], and is in line with expectations that times of drought will limit water and force declines in crop production. However, fallowing was not uniform across crop groups. Our finding that there were larger declines in annual field crops than perennial crops is consistent with a study by Melton et al. [78] that reviewed data from the County Agricultural Commissioner offices and found that perennial crops showed little change in area due to drought, while annual crops experienced significant increases in fallowing. The agreement between our findings and other similar analyses adds confidence to the accuracy of the results, and implies that our study, although only in a portion of the Central Valley, has results and implications that may be applicable to California agriculture as a whole.\n\nWhen analyzing changes in plantings in conjunction with possible drivers of planting decisions, our results suggest that farmers’ decisions regarding which crops to grow in times of drought were not proportional to their water requirements. The reason that crop water requirements were not a main driver of cropping patterns may have been, in part, due to the increases of groundwater pumping that partially offset the shortage of surface water availability during the drought [77]. Tortajada et al. [17] noted that during the drought, farmers coped with less surface water by fallowing fields, buying water from other farmers, or pumping groundwater. Since some farmers were able to offset surface water losses by acquiring water through other means, fallowing fields was not as prevalent as might be seen in a rain-fed agricultural area where groundwater may not be accessible. This finding highlights the importance of case studies such as this one for irrigation-dependent areas that are prone to drought, such as California. The total cropped area in our study area did decline, suggesting that groundwater pumping and water trading partially offset surface water losses, but did not fully offset drought impacts. Further, fallowing fields appears to be favored over changing crops as a strategy employed to save water. However, although a switch to more water-efficient crops was not observed, farmers may have changed their irrigation system, watered less frequently, watered less intensely, or any of a variety of other possible water-saving strategies. For a full picture of water use, crop mapping could be combined with measures of water stress, crop health, or soil moisture for a better understanding of the total crop water budget.\n\nCrop values and the lifespan of a crop are potential drivers of planting decisions that did show promising correlations with changes in planted area. As perennial crops frequently have higher value than annual crops, the two factors are discussed in tandem. The results suggested that farmers shifted their planting choices in favor of more lucrative perennial crops. This idea is supported by many drought analyses, which found that farmers in California respond to water scarcity by fallowing lower value-per-unit-water crops [80], and specifically during the 2012–2016 drought switched from lower-value field crops to higher-value nut and fruit crops [76,77,79].\n\nOur results are consistent with those of Tortajada et al. [17], who found that farmers will prioritize factors such as the price of goods, net income, and consumer preferences over water requirements if alternative water supplies, such as groundwater, exist during drought. These results have implications for the impact of future droughts on agricultural adaptation and management. With more extreme dry periods in the future, California may begin to see a shift toward fewer annual plantings and toward more stable, lucrative crops such as fruit and nut trees, as scarcity in water shifts economic value and farmers are faced with a need to optimize water and land. However, shifts driven by economic value or crop permanence will not necessarily lead to diminished water requirements. Although fallowing will reduce agricultural water demands, preference to plant fruit and nut trees instead of field crops could increase relative agricultural water needs. This result exemplifies the idea of maladaptation to drought, which was posited in Christian-Smith et al. [81], who stated that coping strategies to drought can often lead to the increased vulnerability of a system. In this case, farmers may be shifting to crops that may actually put more pressure on limited water resources.\n\n5. Conclusions\n\nThis study used imagery of high spatial and spectral resolution to obtain highly accurate classifications of crops into groups of similar water use. The results showed classification accuracy of over 94%, and allowed for an analysis of crop area and cropping patterns in situations of droughts. The classification results show that an imaging spectrometer improved crop classification accuracy over multispectral sensors. Crop maps produced from the classifier showed annual patterns that were consistent with other economic and scientific analyses of the 2012–2016 drought, and showed that farmers are fallowing fields and switching to higher-value perennial crops as an adaptation strategy when water becomes scarcer. This study supports the idea that a spaceborne imaging spectrometer with high temporal resolution, such as the planned HyspIRI mission [46], could successfully be used for near real-time or within the growing season mapping of diverse agricultural crops and quantification of an area in cultivation. The demonstrated ability of an imaging spectrometer to produce high crop classification accuracies that are in general in agreement with other published cropping trends increases confidence in the validity of the results and shows that an imaging spectrometer with frequent temporal resolution could enable policymakers, scientists, and farmers to quickly and easily monitor changes in planting with drought. These changes have broad impacts on food yields, pest and disease management, water resource accounting, soil health, and California’s economy.\n\nSupplementary Materials\n\nThe following are available online at https://www.mdpi.com/2072-4292/10/10/1556/s1, Table S1: Field level accuracy assessment with a 25% green vegetation threshold.\n\nAuthor Contributions\n\nConceptualization, S.W.S. and D.A.R.; Formal analysis, S.W.S.; Methodology, S.W.S. and D.A.R.; Writing—original draft, S.W.S.; Writing—review & editing, D.A.R., J.P.M. and C.T.\n\nFunding\n\nThis research was funded by the National Science Foundation, through Graduate Research Fellowship Grant No. DGE 1144085.\n\nAcknowledgments\n\nThe authors thank the members of the UCSB Viper Lab for their technical assistance, feedback, and support. The authors acknowledge and thank NASA’s Jet Propulsion Laboratory for supplying the data and conducting the initial processing.\n\nConflicts of Interest\n\nThe authors declare no conflict of interest.\n\nReferences\n\nDiffenbaugh, N.S.; Swain, D.L.; Touma, D. Anthropogenic warming has increased drought risk in California. Proc. Natl. Acad. Sci. USA 2015, 112, 3931–3936. [Google Scholar] [CrossRef] [PubMed] [Green Version]\n\nGriffin, D.; Anchukaitis, K.J. How unusual is the 2012–2014 California drought? Geophys. Res. Lett. 2014, 41, 9017–9023. [Google Scholar] [CrossRef]\n\nSwain, D.L.; Tsiang, M.; Haugen, M.; Singh, D.; Charland, A.; Rajaratnam, B.; Diffenbaugh, N.S. The extraordinary California drought of 2013/2014: Character, context, and the role of climate change. Bull. Am. Meteorol. Soc. 2014, 95, S3–S7. [Google Scholar]\n\nAghaKouchak, A.; Cheng, L.; Mazdiyasni, O.; Farahmand, A. Global warming and changes in risk of concurrent climate extremes: Insights from the 2014 California drought. Geophys. Res. Lett. 2014, 41, 8847–8852. [Google Scholar] [CrossRef] [Green Version]\n\nAghaKouchak, A.; Feldman, D.; Hoerling, M.; Huxman, T.; Lund, J. Water and climate: Recognize anthropogenic drought. Nature 2015, 524, 409. [Google Scholar] [CrossRef] [PubMed]\n\nHowden, S.M.; Soussana, J.F.; Tubiello, F.N.; Chhetri, N.; Dunlop, M.; Meinke, H. Adapting agriculture to climate change. Proc. Natl. Acad. Sci. USA 2007, 104, 19691–19696. [Google Scholar] [CrossRef] [PubMed] [Green Version]\n\nLobell, D.B.; Burke, M.B.; Tebaldi, C.; Mastrandrea, M.D.; Falcon, W.P.; Naylor, R.L. Prioritizing climate change adaptation needs for food security in 2030. Science 2008, 319, 607–610. [Google Scholar] [CrossRef] [PubMed]\n\nPalazzo, J.; Liu, O.R.; Stillinger, T.; Song, R.; Wang, Y.; Hiroyasu, E.; Zenteno, J.; Anderson, S.; Tague, C. Urban responses to restrictive conservation policy during drought. Water Resour. Res. 2017, 53, 4459–4475. [Google Scholar] [CrossRef]\n\nAgricultural Water Use Efficiency. Available online: https://water.ca.gov/Programs/Water-Use-And-Efficiency/Agricultural-Water-Use-Efficiency (accessed on 10 July 2018).\n\nLobell, D.B.; Field, C.B.; Cahill, K.N.; Bonfils, C. Impacts of future climate change on California perennial crop yields: Model projections with climate and crop uncertainties. Agric. For. Meteorol. 2006, 141, 208–218. [Google Scholar] [CrossRef] [Green Version]\n\nSchlenker, W.; Hanemann, W.M.; Fisher, A.C. Water availability, degree days, and the potential impact of climate change on irrigated agriculture in California. Clim. Chang. 2007, 81, 19–38. [Google Scholar] [CrossRef]\n\nMedellín-Azuara, J.; Harou, J.J.; Olivares, M.A.; Madani, K.; Lund, J.R.; Howitt, R.E.; Tanaka, S.K.; Jenkins, M.W.; Zhu, T. Adaptability and adaptations of California’s water supply system to dry climate warming. Clim. Chang. 2008, 87, 75–90. [Google Scholar] [CrossRef]\n\nTanaka, S.K.; Zhu, T.; Lund, J.R.; Howitt, R.E.; Jenkins, M.W.; Pulido, M.A.; Tauber, M.; Ritzema, R.; Ferreira, I.C. Climate warming and water management adaptation for California. Clim. Chang. 2006, 76, 361–387. [Google Scholar] [CrossRef]\n\nVermeulen, S.J.; Aggarwal, P.K.; Ainslie, A.; Angelone, C.; Campbell, B.M.; Challinor, A.J.; Hansen, J.; Ingram, J.S.; Jarvis, A.; Kristjanson, P.; et al. Agriculture, Food Security and Climate Change: Outlook for Knowledge, Tools and Action. Climate Change, Agriculture and Food Security Report 3; CGIAR-ESSP Program on Climate Change, Agriculture and Food Security (CCAFS): Copenhagen, Denmark, 2010. [Google Scholar]\n\nAdams, R.M.; Rosenzweig, C.; Peart, R.M.; Ritchie, J.T.; McCarl, B.A.; Glyer, J.D.; Curry, R.B.; Jones, J.W.; Boote, K.J.; Allen, L.H. Global climate change and US agriculture. Nature 1990, 345, 219–224. [Google Scholar] [CrossRef]\n\nChallinor, A.J.; Watson, J.; Lobell, D.B.; Howden, S.M.; Smith, D.R.; Chhetri, N. A meta-analysis of crop yield under climate change and adaptation. Nat. Clim. Chang. 2014, 4, 287–291. [Google Scholar] [CrossRef] [Green Version]\n\nTortajada, C.; Kastner, M.J.; Buurman, J.; Biswas, A.K. The California drought: Coping responses and resilience building. Environ. Sci. Policy 2017, 78, 97–113. [Google Scholar] [CrossRef]\n\nAllen, R.G.; Pereira, L.S.; Raes, D.; Smith, M. FAO Irrigation and Drainage Paper No. 56; Food and Agriculture Organization of the United Nations: Rome, Italy, 1998; Volume 56, pp. 26–40. [Google Scholar]\n\nCalifornia Department of Food and Agriculture. California Agricultural Statistics Review 2014–2015. Available online: https://www.cdfa.ca.gov/statistics/PDFs/2015Report.pdf (accessed on 10 July 2018).\n\nNuckols, J.R.; Gunier, R.B.; Riggs, P.; Miller, R.; Reynolds, P.; Ward, M.H. Linkage of the California Pesticide Use Reporting Database with spatial land use data for exposure assessment. Environ. Health Perspect. 2007, 115, 684. [Google Scholar] [CrossRef] [PubMed]\n\nGalvão, L.S.; Epiphanio, J.C.; Breunig, F.M.; Formaggio, A.R. Crop type discrimination using Hyperspectral data. In Hyperspectral Remote Sensing of Vegetation; Thenkabail, P.S., Lyon, J.G., Huete, A., Eds.; CRC Press: Boca Raton, FL, USA, 2016. [Google Scholar]\n\nCraig, M.E. The NASS cropland data layer program. In Proceedings of the Third International Conference on Geospatial Information in Agriculture and Forestry, Denver, CO, USA, 5–7 November 2001; pp. 5–7. [Google Scholar]\n\nLobell, D.B.; Asner, G.P. Cropland distributions from temporal unmixing of MODIS data. Remote Sens. Environ. 2004, 93, 412–422. [Google Scholar] [CrossRef]\n\nMassey, R.; Sankey, T.T.; Congalton, R.G.; Yadav, K.; Thenkabail, P.S.; Ozdogan, M.; Meador, A.J. MODIS phenology-derived, multi-year distribution of conterminous US crop types. Remote Sens. Environ. 2017, 198, 490–503. [Google Scholar] [CrossRef]\n\nOzdogan, M. The spatial distribution of crop types from MODIS data: Temporal unmixing using Independent Component Analysis. Remote Sens. Environ. 2010, 114, 1190–1204. [Google Scholar] [CrossRef]\n\nWardlow, B.D.; Egbert, S.L. Large-area crop mapping using time-series MODIS 250 m NDVI data: An assessment for the US Central Great Plains. Remote Sens. Environ. 2008, 112, 1096–1116. [Google Scholar] [CrossRef]\n\nYan, L.; Roy, D.P. Conterminous United States crop field size quantification from multi-temporal Landsat data. Remote Sens. Environ. 2016, 172, 67–86. [Google Scholar] [CrossRef]\n\nZhong, L.; Hawkins, T.; Biging, G.; Gong, P. A phenology-based approach to map crop types in the San Joaquin Valley, California. Int. J. Remote Sens. 2011, 32, 7777–7804. [Google Scholar] [CrossRef]\n\nUSDA-NASS. United States Department of Agriculture (USDA) National Agricultural Statistics Service Cropland Data Layer. Available online: http://nassgeodata.gmu.edu/CropScape (accessed on 8 June 2018).\n\nChang, J.; Hansen, M.C.; Pittman, K.; Carroll, M.; DiMiceli, C. Corn and soybean mapping in the United States using MODIS time-series data sets. Agron. J. 2007, 99, 1654–1664. [Google Scholar] [CrossRef]\n\nZhang, X.; Friedl, M.A.; Schaaf, C.B.; Strahler, A.H.; Hodges, J.C.; Gao, F.; Reed, B.C.; Huete, A. Monitoring vegetation phenology using MODIS. Remote Sens. Environ. 2003, 84, 471–475. [Google Scholar] [CrossRef]\n\nKey, T.; Warner, T.A.; McGraw, J.B.; Fajvan, M.A. A comparison of multispectral and multitemporal information in high spatial resolution imagery for classification of individual tree species in a temperate hardwood forest. Remote Sens. Environ. 2001, 75, 100–112. [Google Scholar] [CrossRef]\n\nMariotto, I.; Thenkabail, P.S.; Huete, A.; Slonecker, E.T.; Platonov, A. Hyperspectral versus multispectral crop-productivity modeling and type discrimination for the HyspIRI mission. Remote Sens. Environ. 2013, 139, 291–305. [Google Scholar] [CrossRef]\n\nThenkabail, P.S.; Smith, R.B.; De Pauw, E. Evaluation of narrowband and broadband vegetation indices for determining optimal hyperspectral wavebands for agricultural crop characterization. Photogramm. Eng. Remote Sens. 2002, 68, 607–622. [Google Scholar]\n\nThenkabail, P.S.; Enclona, E.A.; Ashton, M.S.; Van Der Meer, B. Accuracy assessments of hyperspectral waveband performance for vegetation analysis applications. Remote Sens. Environ. 2004, 91, 354–376. [Google Scholar] [CrossRef]\n\nThenkabail, P.S.; Mariotto, I.; Gumma, M.K.; Middleton, E.M.; Landis, D.R.; Huemmrich, K.F. Selection of hyperspectral narrowbands (HNBs) and composition of hyperspectral twoband vegetation indices (HVIs) for biophysical characterization and discrimination of crop types using field reflectance and Hyperion/EO-1 data. IEEE J. Sel. Top. Appl. Earth Obs. Remote Sens. 2013, 6, 427–439. [Google Scholar] [CrossRef]\n\nBandos, T.V.; Bruzzone, L.; Camps-Valls, G. Classification of hyperspectral images with regularized linear discriminant analysis. IEEE Trans. Geosci. Remote Sens. 2009, 47, 862–873. [Google Scholar] [CrossRef]\n\nCamps-Valls, G.; Gómez-Chova, L.; Calpe-Maravilla, J.; Soria-Olivas, E.; Martín-Guerrero, J.D.; Moreno, J. Support vector machines for crop classification using hyperspectral data. In Pattern Recognition and Image Analysis; Springer: Berlin/Heidelberg, Germany, 2003; pp. 134–141. [Google Scholar]\n\nGalvão, L.S.; Formaggio, A.R.; Tisot, D.A. Discrimination of sugarcane varieties in Southeastern Brazil with EO-1 Hyperion data. Remote Sens. Environ. 2005, 94, 523–534. [Google Scholar] [CrossRef]\n\nGalvão, L.S.; Roberts, D.A.; Formaggio, A.R.; Numata, I.; Breunig, F.M. View angle effects on the discrimination of soybean varieties and on the relationships between vegetation indices and yield using off-nadir Hyperion data. Remote Sens. Environ. 2009, 113, 846–856. [Google Scholar] [CrossRef]\n\nRao, N.R. Development of a crop-specific spectral library and discrimination of various agricultural crop varieties using hyperspectral imagery. Int. J. Remote Sens. 2008, 29, 131–144. [Google Scholar] [CrossRef]\n\nCalifornia Water Plan. Update 2009. Volume 3, Regional Reports. Bulletin 160–09; Department of Water Resources: Sacramento, CA, USA, 2009.\n\nCarle, D. Introduction to Water in California; University of California Press: Los Angeles, CA, USA, 2004. [Google Scholar]\n\nGreen, R.O.; Eastwood, M.L.; Sarture, C.M.; Chrien, T.G.; Aronsson, M.; Chippendale, B.J.; Faust, J.A.; Pavri, B.E.; Chovit, C.J.; Solis, M.; et al. Imaging spectroscopy and the airborne visible/infrared imaging spectrometer (AVIRIS). Remote Sens. Environ. 1998, 65, 227–248. [Google Scholar] [CrossRef]\n\nThompson, D.R.; Gao, B.C.; Green, R.O.; Roberts, D.A.; Dennison, P.E.; Lundeen, S.R. Atmospheric correction for global mapping spectroscopy: ATREM advances for the HyspIRI preparatory campaign. Remote Sens. Environ. 2015, 167, 64–77. [Google Scholar] [CrossRef]\n\nLee, C.M.; Cable, M.L.; Hook, S.J.; Green, R.O.; Ustin, S.L.; Mandl, D.J.; Middleton, E.M. An introduction to the NASA Hyperspectral InfraRed Imager (HyspIRI) mission and preparatory activities. Remote Sens. Environ. 2015, 167, 6–19. [Google Scholar] [CrossRef]\n\nUSGS. 2012–2016 California Drought: Historical Perspective. Available online: https://ca.water.usgs.gov/california-drought/california-drought-comparisons.html (accessed on 8 June 2018).\n\nRobeson, S.M. Revisiting the recent California drought as an extreme value. Geophys. Res. Lett. 2015, 42, 6771–6779. [Google Scholar] [CrossRef] [Green Version]\n\nUnited States Drought Monitor. Available online: https://droughtmonitor.unl.edu/ (accessed on 8 June 2018).\n\nRoberts, D.A.; Gardner, M.; Church, R.; Ustin, S.; Scheer, G.; Green, R.O. Mapping chaparral in the Santa Monica Mountains using multiple endmember spectral mixture models. Remote Sens. Environ. 1998, 65, 267–279. [Google Scholar] [CrossRef]\n\nAdams, J.B.; Smith, M.O.; Gillespie, A.R. Imaging spectroscopy: Interpretation based on spectral mixture analysis. Remote Geochem. Anal. Elem. Mineral. Compos. 1993, 7, 145–166. [Google Scholar]\n\nAgricultural Land & Water Use Estimates. Available online: https://water.ca.gov/Programs/Water-Use-And-Efficiency/Land-And-Water-Use/Agricultural-Land-And-Water-Use-Estimates (accessed on 10 July 2018).\n\nBreiman, L. Random Forests. Mach. Learn. 2001, 45, 5–32. [Google Scholar] [CrossRef] [Green Version]\n\nGislason, P.O.; Benediktsson, J.A.; Sveinsson, J.R. Random Forests for land cover classification. Pattern Recognit. Lett. 2006, 27, 294–300. [Google Scholar] [CrossRef]\n\nPal, M. Random Forest classifier for remote sensing classification. Int. J. Remote Sens. 2005, 26, 217–222. [Google Scholar] [CrossRef]\n\nMillard, K.; Richardson, M. On the importance of training data sample selection in random forest image classification: A case study in peatland ecosystem mapping. Remote Sens. 2015, 7, 8489–8515. [Google Scholar] [CrossRef]\n\nFoody, G.M.; Cox, D.P. Sub-pixel land cover composition estimation using a linear mixture model and fuzzy membership functions. Remote Sens. 1994, 15, 619–631. [Google Scholar] [CrossRef]\n\nFoody, G.M.; Mathur, A. The use of small training sets containing mixed pixels for accurate hard image classification: Training on mixed spectral responses for classification by a SVM. Remote Sens. Environ. 2006, 103, 179–189. [Google Scholar] [CrossRef]\n\nCCGGA. California Cotton Information. Available online: https://ccgga.org/cotton-information (accessed on 10 July 2018).\n\nPinter, P.J., Jr.; Hatfield, J.L.; Schepers, J.S.; Barnes, E.M.; Moran, M.S.; Daughtry, C.S.; Upchurch, D.R. Remote sensing for crop management. Photogramm. Eng. Remote Sens. 2003, 69, 647–664. [Google Scholar] [CrossRef]\n\nFoody, G.M. Status of land cover classification accuracy assessment. Remote Sens. Environ. 2002, 80, 185–201. [Google Scholar] [CrossRef]\n\nJohnson, D.M. A 2010 map estimate of annually tilled cropland within the conterminous United States. Agric. Syst. 2013, 114, 95–105. [Google Scholar] [CrossRef]\n\nAlcantara, C.; Kuemmerle, T.; Prishchepov, A.V.; Radeloff, V.C. Mapping abandoned agriculture with multi-temporal MODIS satellite data. Remote Sens. Environ. 2012, 124, 334–347. [Google Scholar] [CrossRef]\n\nWu, Z.; Thenkabail, P.S.; Mueller, R.; Zakzeski, A.; Melton, F.; Johnson, L.; Rosevelt, C.; Dwyer, J.; Jones, J.; Verdin, J.P. Seasonal cultivated and fallow cropland mapping using MODIS-based automated cropland classification algorithm. J. Appl. Remote Sens. 2014, 8, 083685. [Google Scholar] [CrossRef] [Green Version]\n\nClark, M.L. Comparison of simulated hyperspectral HyspIRI and multispectral Landsat 8 and Sentinel-2 imagery for multi-seasonal, regional land-cover mapping. Remote Sens. Environ. 2017, 200, 311–325. [Google Scholar] [CrossRef]\n\nPlatt, R.V.; Goetz, A.F. A comparison of AVIRIS and Landsat for land use classification at the urban fringe. Photogramm. Eng. Remote Sens. 2004, 70, 813–819. [Google Scholar] [CrossRef]\n\nClark, M.L.; Roberts, D.A. Species-level differences in hyperspectral metrics among tropical rainforest trees as determined by a tree-based classifier. Remote Sens. 2012, 4, 1820–1855. [Google Scholar] [CrossRef]\n\nHorler, D.N.; Dockray, M.; Barber, J. The red edge of plant leaf reflectance. Int. J. Remote Sens. 1983, 4, 273–288. [Google Scholar] [CrossRef]\n\nZarco-Tejada, P.J.; Ustin, S.L.; Whiting, M.L. Temporal and spatial relationships between within-field yield variability in cotton and high-spatial hyperspectral remote sensing imagery. Agron. J. 2005, 97, 641–653. [Google Scholar] [CrossRef]\n\nTucker, C.J.; Garratt, M.W. Leaf optical system modeled as a stochastic process. Appl. Opt. 1977, 16, 635–642. [Google Scholar] [CrossRef] [PubMed]\n\nImmitzer, M.; Vuolo, F.; Atzberger, C. First experience with Sentinel-2 data for crop and tree species classifications in central Europe. Remote Sens. 2016, 8, 166. [Google Scholar] [CrossRef]\n\nKakani, V.G.; Reddy, K.R.; Zhao, D.; Sailaja, K. Field crop responses to ultraviolet-B radiation: A review. Agric. For. Meteorol. 2003, 120, 191–218. [Google Scholar] [CrossRef]\n\nVuolo, F.; Neuwirth, M.; Immitzer, M.; Atzberger, C.; Ng, W.T. How much does multi-temporal Sentinel-2 data improve crop type classification? Int. J. Appl. Earth Obs. Geoinf. 2018, 72, 122–130. [Google Scholar] [CrossRef]\n\nWaldhoff, G.; Lussem, U.; Bareth, G. Multi-Data Approach for remote sensing-based regional crop rotation mapping: A case study for the Rur catchment, Germany. Int. J. Appl. Earth Obs. Geoinf. 2017, 61, 55–69. [Google Scholar] [CrossRef]\n\nDudley, K.L.; Dennison, P.E.; Roth, K.L.; Roberts, D.A.; Coates, A.R. A multi-temporal spectral library approach for mapping vegetation species across spatial and temporal phenologicalal gradients. Remote Sens. Environ. 2015, 167, 121–134. [Google Scholar] [CrossRef]\n\nCooley, H.; Donnelly, K.; Phurisamban, R.; Subramanian, M. Impacts of California’s Ongoing Drought: Agriculture; Pacific Institute: Oakland, CA, USA, 2015. [Google Scholar]\n\nHowitt, R.; Medellín-Azuara, J.; MacEwan, D.; Lund, J.R.; Sumner, D. Economic Analysis of the 2014 Drought for California Agriculture; University of California Center for Watershed Sciences: Davis, CA, USA, 2014. [Google Scholar]\n\nMelton, F.; Rosevelt, C.; Guzman, A.; Johnson, L.; Zaragoza, I.; Verdin, J.; Thenkabail, P.; Wallace, C.; Mueller, R.; Willis, P.; et al. Fallowed Area Mapping for Drought Impact Reporting: 2015 Assessment of Conditions in the California Central Valley; NASA Ames Research Center: Mountain View, CA, USA, 2015.\n\nMedellín-Azuara, J.; MacEwan, D.; Howitt, R.E.; Koruakos, G.; Dogrul, E.C.; Brush, C.F.; Kadir, T.N.; Harter, T.; Melton, F.; Lund, J.R. Hydro-economic analysis of groundwater pumping for irrigated agriculture in California’s Central Valley, USA. Hydrogeol. J. 2015, 23, 1205–1216. [Google Scholar] [CrossRef]\n\nHanak, E. Managing California’s Water: From Conflict to Reconciliation; Public Policy Institute of CA: San Francisco, CA, USA, 2011. [Google Scholar]\n\nChristian-Smith, J.; Levy, M.C.; Gleick, P.H. Maladaptation to drought: A case report from California, USA. Sustain. Sci. 2015, 10, 491–501. [Google Scholar] [CrossRef]\n\nFigure 1. Study area with county boundaries.\n\nFigure 2. Out-of-bag (OOB) accuracy for each of the nine crop categories at different pixel green vegetation (GV) fraction threshold levels.\n\nFigure 3. Mean decrease accuracy (MDA) is plotted for each wavelength on the primary y-axis, while a green vegetation spectrum is overlaid on the secondary y-axis in order to highlight which bands and band regions were most important for the random forest classification. A higher mean decrease accuracy score denote bands of higher significance.\n\nFigure 4. Pixel-level and field-level percent accuracies for random forest run with the Airborne Visible/Infrared Imaging Spectrometer (AVIRIS), Landsat-simulated, and Sentinel-2B-simulated imagery. Pixel-level accuracy reported as the random forest OOB accuracy. Field-level accuracy is reported as the average of the producer’s and user’s accuracies for all three years. Field-level accuracies contained all of the classified fields with 50% GV or more.\n\nFigure 5. 2013 classification map using the majority filter.\n\nFigure 6. Portion of the southern study area (midpoint: 35°27′42E, 119°40′33N), illustrating the decrease in area of other truck crops, alfalfa, and cotton over the drought with relative stability in area of almond and pistachios.\n\nFigure 7. Portion of the northern study area (midpoint: 36°20′36E, 119°28′30N) highlighting an increase in tomato extent over the course of the drought as well as a decrease in alfalfa extent.\n\nFigure 8. Assessing correlation between the change in crop area over the drought (Table 9) and the average water use of each crop (Table 10).\n\nFigure 9. Change in percentage crop area by crop lifespan. Annual crops include corn, cotton, other truck crops, and tomatoes. The two to five-year range includes alfalfa. The greater than 15-year group includes almond/pistachio, other deciduous crops, subtropical, and vines.\n\nTable 1. Crop groupings from California Department of Water Resources (DWR).\n\nCrop ClassDefinitionIncluded in StudyGrainWheat, barley, oats, miscellaneous grain and hay, and mixed grain and hayNoRiceRice and wild riceNoCottonCottonYesSugar BeetSugar beetsNoCornCorn (field and sweet)YesDry BeanBeans (dry)NoSafflowerSafflowerNoOther FieldFlax, hops, grain sorghum, sudan, castor beans, miscellaneous fields, sunflowers, hybrid sorghum/sudan, millet, and sugar caneNoAlfalfaAlfalfa and alfalfa mixturesYesPastureClover, mixed pasture, native pastures, induced high water table native pasture, miscellaneous grasses, turf farms, bermuda grass, rye grass, and klein grassNoProcessing TomatoTomatoes for processingYes, combined with freshFresh TomatoTomatoes for marketYes, combined with processingCucurbitMelons, squash, and cucumbersNoOnion GarlicOnions and garlicNoPotatoPotatoesNoOther Truck Crops Artichokes, asparagus, beans (green), carrots, celery, lettuce, peas, spinach, flowers nursery and tree farms, bush berries, strawberries, peppers, broccoli, cabbage, cauliflower, and brussels sproutsYesAlmond PistachioAlmonds and pistachiosYesOther Deciduous CropsApples, apricots, cherries, peaches, nectarines, pears, plums, prunes, figs, walnuts, and miscellaneous deciduousYesSubtropicalGrapefruit, lemons, oranges, dates, avocados, olives, kiwis, jojoba, eucalyptus, and miscellaneous subtropical fruitYesVineTable grapes, wine grapes, and raisin grapesYes\n\nTable 2. Number of validation fields and total area for fields containing ≥50% green vegetation.\n\nCropsNumber of FieldsTotal Area (km2)Studied CropsAlfalfa340954.4Almond/Pistachio4423305.3Corn97236.8Cotton2264.6Other Deciduous Crops21741517.8Other Truck Crops2276.5Subtropical634769.5Tomato2987.6Vine350478.1Other CropsCucurbit31.1Grain15.2Pasture813.7Safflower321.8Sugar Beet48.0Uncultivated1715.0\n\nTable 3. Accuracies by crop class and year. Independent pixel and field-level validations report accuracy as the average of the producer’s accuracy (sensitivity) and user’s accuracy (positive predictive value).\n\nCropsOut-of-Bag AccuracyIndependent Validation Pixel-Level Accuracy by Year (%)Field-Level Accuracy after Majority Filter with 50% GV Threshold (%)All Years201320142015201320142015Alfalfa93.794.587.693.094.294.797.1Almond and Pistachio98.896.196.294.288.991.795.6Corn90.993.177.095.198.393.894.1Cotton86.273.545.721.573.383.385.7Other Deciduous90.286.182.685.095.695.396.6Other Truck88.076.478.6NA100.0100.068.8Subtropical88.083.981.880.692.393.292.9Tomato97.2NA92.389.2100.0100.097.1Vine84.577.270.283.588.390.693.4\n\nTable 4. Error matrix for pixels (150,000 total) assessed using random forest OOB accuracies. Abbreviations are used for crop groups: Alfalfa (AF), Almond/Pistachio (AP), Corn (CR), Cotton (CT), Other Deciduous Crops (OD), Other Truck Crops (OT), Subtropical (ST), Tomato (TO), and Vine (VI). Crop group accuracies assessed by user’s (positive predictive value) and producer’s (sensitivity) accuracies. Overall OOB accuracy reported.\n\nReference DataAFAPCRCTODOTSTTOVITotalClassifiedAF17,7332958135214711625618,917AP3464,44917497317884265,219CR5012636451716013244008CT40883134762053141562OD194151471126,82108371228329,679OT3623071111877122662134ST36694171205815,06759617,119TO181410304166081708VI24021211285810150881099600Total18,38167,4153667142130,238190316,32717168878User’s Acc.93.7%98.8%90.9%86.2%90.4%88.0%88.0%97.2%84.5%Producer’s Acc.96.5%95.6%99.4%94.8%88.7%98.6%92.3%96.7%91.3%OOB Acc.93.8%\n\nTable 5. Error matrix for independently validated pixels (30,000) by crop type for all of the years combined. Accuracy assessment includes the user’s (positive predictive value) and producer’s (sensitivity) accuracies for each crop and the total overall kappa and accuracy. Abbreviations are used for crop groups: Alfalfa (AF), Almond/Pistachio (AP), Corn (CR), Cotton (CT), Other Deciduous Crops (OD), Other Truck Crops (OT), Subtropical (ST), Tomato (TO), and Vine (VI).\n\nReference DataAFAPCRCTODOTSTTOVITotalClassifiedAF389112228523243854118AP6612,3954694313118957513,184CR42197121300001029CT1531316153002202OD226256754854792130532836696OT001045500161ST4570252894240411452965TO91203012920308VI521325871020412441437Total434612,751113432362459729213481835User’s Acc.94.5%94.0%94.4%79.7%81.8%90.2%81.1%94.8%86.6%Producer’s Acc.89.5%97.2%85.6%49.8%87.7%56.7%82.3%83.9%67.8%Kappa0.86Overall Acc.89.6%\n\nTable 6. Error matrix for reference fields (4110 total) from the majority filter reclassification for all of the years combined. This error matrix only includes fields with 50% of more green vegetation. Accuracy assessment includes the user’s (positive predictive value) and producer’s (sensitivity) accuracies for each crop and the total overall kappa and accuracy. Abbreviations are used for crop groups: Alfalfa (AF), Almond/Pistachio (AP), Corn (CR), Cotton (CT), Other Deciduous Crops (OD), Other Truck Crops (OT), Subtropical (ST), Tomato (TO), and Vine (VI).\n\nReference DataAFAPCRCTODOTSTTOVITotalClassifiedAF32401340114388AP1432224401205498CR209211000096CT100151000017OD962120954420382197OT000001700017ST240024057806614TO000000028028VI10005110297305Total340442972221742263429350User’s Acc.95.9%86.7%95.8%88.2%95.4%100%94.1%100%97.4%Producer’s Acc.95.3%97.7%94.8%68.2%96.4%77.3%91.2%96.6%84.9%Kappa0.91Overall Acc.94.4%\n\nTable 7. Field-level accuracies, reported as the average of the producer’s and user’s accuracies, for random forest (RF) runs trained on the two years that it was not used to classify. The percentage decrease in accuracy is the difference between the accuracy of RF trained with all three years (Table 3) and the RF trained with two years. Field-level classification used a majority filter on fields with 50% GV or higher.\n\nCropsField-Level Accuracy (%)Percent Decrease in Accuracy2013 Trained on 2014 and 20152014 Trained on 2013 and 20152015 Trained on 2013 and 2014201320142015Alfalfa74.754.187.419.540.69.7Almond and Pistachio71.166.770.517.825.025.1Corn86.768.889.911.625.14.2Cotton28.3032.145.083.353.6Other Deciduous87.054.187.38.641.29.3Other TruckNANA0NANA68.8Subtropical78.262.366.514.130.926.4Tomato100NA61.80NA35.3Vine71.961.378.316.429.315.1\n\nTable 8. Number of classified fields each year.\n\nClassified Fields2013Percent included in validation layer39.0%Percent not included in validation layer61.0%Total number classified34692014Percent included in validation layer36.4%Percent not included in validation layer63.6%Total number classified33612015Percent included in validation layer48.9%Percent not included in validation layer51.1%Total number classified3235\n\nTable 9. Classified crop area.\n\nCropsCropland (km2)Change in Area2013201420152013 to 20142014 to 20152013 to 2015Alfalfa68.657.148.7−16.8%−14.6%−28.9%Almond and Pistachio124.9131.5128.25.3%−2.5%2.6%Corn25.114.012.7−44.3%−9.5%−49.6%Cotton17.21.52.7−91.4%85.4%−84.1%Other Deciduous74.277.876.74.8%−1.4%3.4%Other Truck3.43.40.40.3%−89.4%−89.4%Subtropical26.634.724.930.3%−28.4%−6.7%Tomato6.95.715.8−18.0%178.2%128.0%Vine16.017.721.211.0%19.7%32.9%Total362.9343.3331.2−5.4%−3.5%−8.7%\n\nTable 10. Estimated water inputs per year calculated using the median water application by crop type in the Tulare Lake Hydrologic Region from 1998 to 2010 [52] and the classified area (Table 9).\n\nAverage Water Application per Hectare (Thousand m3)Total Water Application (km3 Multiplied by 1000) Calculated with the Classification Maps201320142015Alfalfa15.1103.986.573.8Almond and Pistachio12.4154.6162.7158.7Corn9.624.213.512.2Cotton9.416.11.42.6Other Deciduous11.988.292.491.2Other Truck4.31.51.50.2Subtropical9.826.133.924.3Tomato6.94.83.910.9Vine8.213.014.517.3Total432.3410.3391.1\n\n© 2018 by the authors. Licensee MDPI, Basel, Switzerland. This article is an open access article distributed under the terms and conditions of the Creative Commons Attribution (CC BY) license (http://creativecommons.org/licenses/by/4.0/).\n\nShare and Cite\n\nMDPI and ACS Style\n\nShivers, S.W.; Roberts, D.A.; McFadden, J.P.; Tague, C. Using Imaging Spectrometry to Study Changes in Crop Area in California’s Central Valley during Drought. Remote Sens. 2018, 10, 1556. https://doi.org/10.3390/rs10101556\n\nAMA Style\n\nShivers SW, Roberts DA, McFadden JP, Tague C. Using Imaging Spectrometry to Study Changes in Crop Area in California’s Central Valley during Drought. Remote Sensing. 2018; 10(10):1556. https://doi.org/10.3390/rs10101556\n\nChicago/Turabian Style\n\nShivers, Sarah W., Dar A. Roberts, Joseph P. McFadden, and Christina Tague. 2018. \"Using Imaging Spectrometry to Study Changes in Crop Area in California’s Central Valley during Drought\" Remote Sensing 10, no. 10: 1556. https://doi.org/10.3390/rs10101556\n\nNote that from the first issue of 2016, this journal uses article numbers instead of page numbers. See further details here.\n\nArticle Metrics\n\nNo\n\nNo\n\nArticle Access Statistics\n\nFor more information on the journal statistics, click here.\n\nMultiple requests from the same IP address are counted as one view."
    }
}
{
    "id": "dbpedia_6863_3",
    "rank": 31,
    "data": {
        "url": "https://charity.wtf/category/observability/",
        "read_more_link": "",
        "language": "en",
        "title": "observability ‚Äì charity.wtf",
        "top_image": "https://s0.wp.com/i/blank.jpg",
        "meta_img": "https://s0.wp.com/i/blank.jpg",
        "images": [
            "https://i0.wp.com/charity.wtf/wp-content/uploads/2024/08/newpix-44.jpeg?resize=204%2C189&ssl=1",
            "https://i0.wp.com/charity.wtf/wp-content/uploads/2024/08/newpix-43.jpeg?resize=199%2C200&ssl=1",
            "https://i0.wp.com/charity.wtf/wp-content/uploads/2024/08/newpix-47.jpeg?resize=227%2C141&ssl=1",
            "https://i0.wp.com/charity.wtf/wp-content/uploads/2024/08/newpix-42.jpeg?resize=238%2C138&ssl=1",
            "https://i0.wp.com/charity.wtf/wp-content/uploads/2024/08/newpix-56.jpeg?resize=200%2C201&ssl=1",
            "https://i0.wp.com/charity.wtf/wp-content/uploads/2024/08/newpix-39.jpeg?resize=195%2C200&ssl=1",
            "https://i0.wp.com/charity.wtf/wp-content/uploads/2024/08/newpix-35.jpeg?resize=200%2C200&ssl=1",
            "https://i0.wp.com/charity.wtf/wp-content/uploads/2024/08/newpix-8.jpeg?resize=205%2C179&ssl=1",
            "https://i0.wp.com/charity.wtf/wp-content/uploads/2024/08/newpix-36.jpeg?resize=287%2C121&ssl=1",
            "https://i0.wp.com/charity.wtf/wp-content/uploads/2023/03/cm-15.jpeg?resize=210%2C280&ssl=1",
            "https://i0.wp.com/charity.wtf/wp-content/uploads/2018/11/0893d048d8361fe632b090b0429ad78b-rainbow-dash-rainbows-e1542789606365.jpg?resize=236%2C218&ssl=1",
            "https://i0.wp.com/charity.wtf/wp-content/uploads/2022/08/slide_47.jpeg?resize=300%2C169&ssl=1",
            "https://i0.wp.com/charity.wtf/wp-content/uploads/2018/11/rainbow_dash___no_by_cptofthefriendship-d4erd69.png?resize=242%2C220&ssl=1",
            "https://i0.wp.com/charity.wtf/wp-content/uploads/2018/11/ponyfm-i7812-original.png?resize=176%2C167&ssl=1",
            "https://i0.wp.com/charity.wtf/wp-content/uploads/2022/08/thirdpov.png?resize=300%2C186&ssl=1",
            "https://i0.wp.com/charity.wtf/wp-content/uploads/2022/08/structure-your-data.png?resize=170%2C171&ssl=1",
            "https://i0.wp.com/charity.wtf/wp-content/uploads/2018/11/giphy.gif?resize=196%2C196&ssl=1",
            "https://i0.wp.com/charity.wtf/wp-content/uploads/2022/08/rainbow_dash___table_flip_by_ocarina0ftimelord_d4ewbb3-pre.png?resize=300%2C218&ssl=1",
            "https://i0.wp.com/charity.wtf/wp-content/uploads/2018/11/1225287_1370081029072_full.png?resize=190%2C221&ssl=1",
            "https://i0.wp.com/charity.wtf/wp-content/uploads/2018/11/rainbow-dash-is-not-amused-my-little-pony-friendship-is-magic-31088082-900-622.png?resize=181%2C125&ssl=1",
            "https://i0.wp.com/charity.wtf/wp-content/uploads/2018/11/rainbow_dash___no_by_cptofthefriendship-d4erd69.png?resize=242%2C220&ssl=1",
            "https://i0.wp.com/charity.wtf/wp-content/uploads/2018/08/img_4619.jpg?resize=195%2C260&ssl=1",
            "https://i0.wp.com/charity.wtf/wp-content/uploads/2019/11/highcardinality.png?resize=147%2C151&ssl=1",
            "https://i0.wp.com/charity.wtf/wp-content/uploads/2019/11/delusion.png?resize=139%2C124&ssl=1",
            "https://i0.wp.com/charity.wtf/wp-content/uploads/2019/11/illusions.png?resize=151%2C135&ssl=1",
            "https://i0.wp.com/charity.wtf/wp-content/uploads/2022/01/5D9708D0-EEB2-45CC-ACBE-146B9F8716C3_1_105_c.jpeg?resize=174%2C136&ssl=1",
            "https://i0.wp.com/charity.wtf/wp-content/uploads/2019/11/graphprejudice.png?resize=132%2C129&ssl=1",
            "https://i0.wp.com/charity.wtf/wp-content/uploads/2019/12/prejudice.png?resize=221%2C50&ssl=1",
            "https://i0.wp.com/charity.wtf/wp-content/uploads/2021/01/mepurp-26.jpg?resize=140%2C186&ssl=1",
            "https://i0.wp.com/charity.wtf/wp-content/uploads/2021/01/oncallbychoice.png?resize=185%2C210&ssl=1",
            "https://i0.wp.com/charity.wtf/wp-content/uploads/2022/01/13A55732-B722-47B3-904A-5A7353563D28_4_5005_c.jpeg?resize=270%2C126&ssl=1",
            "https://i0.wp.com/charity.wtf/wp-content/uploads/2022/01/35A3D323-7870-4F99-82F0-2D1628E670EC.png?resize=205%2C210&ssl=1",
            "https://i0.wp.com/charity.wtf/wp-content/uploads/2020/03/IMG_6288.png?resize=245%2C326&ssl=1",
            "https://i0.wp.com/charity.wtf/wp-content/uploads/2019/11/slo.png?resize=218%2C185&ssl=1",
            "https://i0.wp.com/charity.wtf/wp-content/uploads/2020/07/bed-13-1.jpg?resize=198%2C264&ssl=1",
            "https://i0.wp.com/charity.wtf/wp-content/uploads/2019/11/Mouse-Fishbowl-Observability-2x2.5.png?resize=190%2C238&ssl=1",
            "https://i0.wp.com/charity.wtf/wp-content/uploads/2019/11/dieunfufilled.png?resize=202%2C204&ssl=1",
            "https://i0.wp.com/charity.wtf/wp-content/uploads/2019/11/highcardinality.png?resize=225%2C230&ssl=1",
            "https://i0.wp.com/charity.wtf/wp-content/uploads/2020/03/shippingscary.png?resize=209%2C203&ssl=1",
            "https://i0.wp.com/charity.wtf/wp-content/uploads/2019/11/systembroeknsun.png?resize=224%2C130&ssl=1",
            "https://i0.wp.com/charity.wtf/wp-content/uploads/2019/01/Sea-of-Metrics-Alien.png?resize=160%2C160&ssl=1",
            "https://i0.wp.com/charity.wtf/wp-content/uploads/2019/11/survive.png?resize=174%2C144&ssl=1",
            "https://i0.wp.com/charity.wtf/wp-content/uploads/2019/09/panda_nines.png?resize=212%2C178&ssl=1",
            "https://i0.wp.com/charity.wtf/wp-content/uploads/2019/11/delusion.png?resize=215%2C192&ssl=1",
            "https://i0.wp.com/charity.wtf/wp-content/uploads/2019/11/meplusprod.png?resize=164%2C248&ssl=1",
            "https://i0.wp.com/charity.wtf/wp-content/uploads/2019/10/img_6377.png?resize=199%2C265&ssl=1",
            "https://i0.wp.com/charity.wtf/wp-content/uploads/2019/09/fire_burn.png?resize=220%2C220&ssl=1",
            "https://i0.wp.com/charity.wtf/wp-content/uploads/2019/10/screen-shot-2019-10-28-at-12.04.48-am.png?resize=466%2C153&ssl=1",
            "https://i0.wp.com/charity.wtf/wp-content/uploads/2019/10/screen-shot-2019-10-28-at-12.05.04-am.png?resize=476%2C354&ssl=1",
            "https://i0.wp.com/charity.wtf/wp-content/uploads/2019/10/screen-shot-2019-10-28-at-12.31.12-am.png?resize=660%2C488&ssl=1",
            "https://i0.wp.com/charity.wtf/wp-content/uploads/2019/10/kid-headlamps.png?resize=217%2C217&ssl=1",
            "https://i0.wp.com/charity.wtf/wp-content/uploads/2018/12/gpgates.jpg?resize=227%2C238&ssl=1",
            "https://i0.wp.com/charity.wtf/wp-content/uploads/2019/05/img_7367.png?resize=206%2C274&ssl=1",
            "https://i0.wp.com/charity.wtf/wp-content/uploads/2019/12/prod.png?resize=203%2C193&ssl=1",
            "https://i0.wp.com/charity.wtf/wp-content/uploads/2019/10/img_9017.png?resize=256%2C340&ssl=1"
        ],
        "movies": [],
        "keywords": [],
        "meta_keywords": [
            ""
        ],
        "tags": null,
        "authors": [],
        "publish_date": "2024-08-07T22:57:19+00:00",
        "summary": "",
        "meta_description": "Posts about observability written by mipsytipsy",
        "meta_lang": "en",
        "meta_favicon": "https://s0.wp.com/i/webclip.png",
        "meta_site_name": "charity.wtf",
        "canonical_link": "https://charity.wtf/category/observability/",
        "text": "Augh! I am so behind on so much writing, I‚Äôm even behind on writing shit that I need to reference in order to write other pieces of writing. Like this one. So we‚Äôre just gonna do this quick and dirty on the personal blog, and not bother bringing it up to the editorial standards of‚Ä¶anyone else‚Äôs sites. üò¨\n\nIf you‚Äôd rather consume these ideas in other ways:\n\nI gave a keynote at SRECon in March\n\nHere is a slide deck of my slides from CTO Craft Con London in May\n\nA Screaming In The Cloud podcast with Corey Quinn in April\n\nMy piece earlier in the year on the Cost Crisis in Observability Tooling touched on some of the concepts too\n\nMatt Sanabria wrote a great piece comparing us and a bunch of other observability vendors in 2024.\n\nWhat does observability mean? No one knows\n\nIn 2016, we first borrowed the term ‚Äúobservability‚Äù from the wikipedia entry for control systems observability, where it is a measure of your ability to understand internal system states just by observing its outputs. We (Honeycomb) then spent a couple of years trying to work out how that definition might apply to software systems. Many twitter threads, podcasts, blog posts and lengthy laundry lists of technical criteria emerged from that work, including a whole ass book.\n\nIn 2018, Peter Bourgon wrote a blog post proposing that ‚Äúobservability has three pillars: metrics, logs and traces. Ben Sigelman did a masterful job of unpacking why metrics, logs and traces are just telemetry. However, lots of people latched on to the three pillars language: vendors because they (coincidentally!) had metrics products, logging products, and tracing products to sell, engineers because it described their daily reality.\n\nSince then the industry has been stuck in kind of a weird space, where the language used to describe the problems and solutions has evolved, but the solutions themselves are largely the same ones as five years ago, or ten years ago. They‚Äôve improved, of course ‚Äî massively improved ‚Äî but structurally they‚Äôre variations on the same old pre-aggregated metrics.\n\nIt has gotten harder and harder to speak clearly about different philosophical approaches and technical solutions without wading deep into the weeds, where no one but experts should reasonably have to go.\n\nThis is what semantic versioning was made for\n\nLook, I am not here to be the language police. I stopped correcting people on twitter back in 2019. We all do observability! One big happy family. üëç\n\nI AM here to help engineers think clearly and crisply about the problems in front of them. So here we go. Let‚Äôs call the metrics, logs and traces crowd ‚Äî the ‚Äúthree pillars‚Äù generation of tooling ‚Äî that‚Äôs ‚ÄúObservability 1.0‚Äú. Tools like Honeycomb, which are built based on arbitrarily-wide structured log events, a single source of truth ‚Äî that‚Äôs ‚ÄúObservability 2.0‚Äú.\n\nHere is the twitter thread where I first teased out the differences between these generations of tooling (all the way back in December, yes, that‚Äôs how long I‚Äôve been meaning to write this üòÖ).\n\nThis is literally the problem that semantic versioning was designed to solve, by the way. Major version bumps are reserved for backwards-incompatible, breaking changes, and that‚Äôs what this is. You cannot simultaneously store your data across both multiple pillars and a single source of truth.\n\nIncompatible. Breaking change. O11y 1.0, meet O11y 2.0.\n\nsmall technical changes can unlock waves of powerful sociotechnical transformation\n\nThere are a LOT of ramifications and consequences that flow from this one small change in how your data gets stored. I don‚Äôt have the time or space to go into all of them here, but I will do a quick overview of the most important ones.\n\nThe historical analogue that keeps coming to mind for me is virtualization. VMs are old technology, they‚Äôve been around since the 70s. But it wasn‚Äôt until the late 90s that VMware productized it, unlocking wave after wave of change, from cloud computing and SaaS to the very DevOps movement itself.\n\nI believe the shift to observability 2.0 holds a similarly massive potential for change, based on what I see happening today, with teams who have already made the leap. Why? In a word, precision. O11y 1.0 can only ever give you aggregates and random exemplars. O11y 2.0, on the other hand, can tell you precisely what happened when you flipped a flag, deployed to a canary, or made any other change in production.\n\nWill these waves of sociotechnical transformation ever be realized? Who knows. The changes that get unlocked will depend to some extent on us (Honeycomb), and to an even greater extent on engineers like you. Anyway, I‚Äôll talk about this more some other time. Right now, I just want to establish a baseline for this vocabulary.\n\n1.0 vs 2.0: How does the data get stored?\n\n1.0üíô O11y 1.0 has many sources of truth, in many different formats. Typically, you end up storing your data across metrics, logs, traces, APM, RUM, profiling, and possibly other tools as well. Some folks even find themselves falling back to B.I. (business intelligence) tools like Tableau in a pinch to understand what‚Äôs happening on their systems.\n\nEach of these tools are siloed, with no connective tissue, or only a few, predefined connective links that connect e.g. a specific metric to a specific log line. Aggregation is done at write time, so you have to decide up front which data points to collect and which questions you want to be able to ask. You may find yourself eyeballing graph shapes and assuming they must be the same data, or copy-pasting IDs around from logging to tracing tools and back.\n\n2.0 üíö Data gets stored in arbitrarily-wide structured log events (often called ‚Äúcanonical logs‚Äú), often with trace and span IDs appended. You can visualize the events over time as a\n\ntrace, or slice and dice your data to zoom in to individual events, or zoom out to a birds-eye view. You can interact with your data by group by, break down, etc.\n\nYou aggregate at read time, and preserve raw events for ad hoc querying. Hopefully, you derive your SLO data from the same data you query! Think of it as B.I. for systems/app/business data, all in one place. You can derive metrics, or logs, or traces, but it‚Äôs all the same data.\n\n1.0 vs 2.0: on metrics vs logs\n\n1.0 üíô The workhorse of o11y 1.0 is metrics. RUM tools are built on metrics to understand browser user sessions. APM tools are built using metrics to understand application performance. Long ago, the decision was made to use metrics as the source of truth for telemetry because they are cheap and fast, and hardware used to be incredibly expensive.\n\nThe more complex our systems get, the worse of a tradeoff this becomes. Metrics are a terrible building block for understanding rich data, because you have to discard all that valuable context at write time, and they don‚Äôt support high (or even medium!) cardinality data. All you can do to enrich the data is via tags.\n\nMetrics are a great tool for cheaply summarizing vast quantities of data. They are not equipped to help you introspect or understand complex systems. You will go broke and go mad if you try.\n\n2.0 üíö The building block of o11y 2.0 is wide, structured log events. Logs are infinitely more powerful, useful and cost-effective than metrics are because they preserve context and relationships between data, and data is made valuable by context. Logs also allow you to capture high cardinality data and data relationships/structures, which give you the ability to compute outliers and identify related events.\n\n1.0 vs 2.0: Who uses it, and how?\n\n1.0 üíô Observability 1.0 is predominantly about how you operate your code. It centers around errors, incidents, crashes, bugs, user reports and problems. MTTR, MTTD, and reliability are top concerns.\n\nO11y 1.0 is typically consumed using static dashboards ‚Äî lots and lots of static dashboards. ‚ÄúSingle pane of glass‚Äù is often mentioned as a holy grail. It‚Äôs easy to find something once you know what you‚Äôre looking for, but you need to know to look for it before you can find it.\n\n2.0 üíö If o11y 1.0 is about how you operate your code, o11y 2.0 is about how you develop your code. O11y 2.0 is what underpins the entire software development lifecycle, enabling engineers to connect feedback loops end to end so they get fast feedback on the changes they make, while it‚Äôs still fresh in their heads. This is the foundation of your team‚Äôs ability to move swiftly, with confidence. It isn‚Äôt just about understanding bugs and outages, it‚Äôs about proactively understanding your software and how your users are experiencing it.\n\nThus, o11y 2.0 has a much more exploratory, open-ended interface. Any dashboards should be dynamic, allowing you to drill down into a question or follow a trail of breadcrumbs as part of the debugging/understanding process. The canonical question of o11y 2.0 is ‚Äúhere‚Äôs a thing I care about ‚Ä¶ why do I care about it? What are all of the ways it is different from all the other things I don‚Äôt care for?‚Äù\n\nWhen it comes to understanding your software, it‚Äôs often harder to identify the question than the answer. Once you know what the question is, you probably know the answer too. With o11y 1.0, it‚Äôs very easy to find something once you know what you‚Äôre looking for. With o11y 2.0, that constraint is removed.\n\n1.0 vs 2.0: How do you interact with production?\n\n1.0 üíô You deploy your code and wait to get paged. ü§û Your job is done as a developer when you commit your code and tests pass.\n\n2.0 üíö You practice observability-driven development: as you write your code, you instrument it. You deploy to production, then inspect your code through the lens of the instrumentation you just wrote. Is it behaving the way you expected it to? Does anything else look ‚Ä¶ weird?\n\nYour job as a developer isn‚Äôt done until you know it‚Äôs working in production. Deploying to production is the beginning of gaining confidence in your code, not the denouement.\n\n1.0 vs 2.0: How do you debug?\n\n1.0 üíô You flip from dashboard to dashboard, pattern-matching and looking for similar shapes with your eyeballs.\n\nYou lean heavily on intuition, educated guesses, past experience, and a mental model of the system. This means that the best debuggers are ALWAYS the engineers who have been there the longest and seen the most.\n\nYour debugging sessions are search-first: you start by searching for something you know should exist.\n\n2.0 üíö You check your instrumentation, or you watch your SLOs. If something looks off, you see what all the mysterious events have in common, or you start forming hypotheses, asking a question, considering the result, and forming another one based on the answer. You interrogate your systems, following the trail of breadcrumbs to the answer, every time.\n\nYou don‚Äôt have to guess or rely on elaborate, inevitably out-of-date mental models. The data is right there in front of your eyes. The best debuggers are the people who are the most curious.\n\nYour debugging questions are analysis-first: you start with your user‚Äôs experience.\n\n1.0 vs 2.0: The cost model\n\n1.0 üíô You pay to store your data again and again and again and again, multiplied by all the different formats and tool types you are paying to store it in. Cost goes up at a multiplier of your traffic increase. I wrote a whole piece earlier this year on the cost crisis in observability tooling, so I won‚Äôt go into it in depth here.\n\nAs your costs increase, the value you get out of your tools actually decreases.\n\nIf you are using metrics-based products, your costs go up based on cardinality. ‚ÄúCustom metrics‚Äù is a euphemism for ‚Äúcardinality‚Äù; ‚Äú100 free custom metrics‚Äù actually means ‚Äú100 free cardinality‚Äù, aka unique values.\n\n2.0 üíö You pay to store your data once. As your costs go up, the value you get out goes up too. You have powerful, surgical options for controlling costs via head-based or tail-based dynamic sampling.\n\nYou can have infinite cardinality. You are encouraged to pack hundreds or thousands of dimensions in per event, and any or all of those dimensions can be any data type you want. This luxurious approach to cardinality and data is one of the least well understood aspects of the switch from o11y 1.0 to 2.0.\n\nMany observability engineering teams have spent their entire careers massaging cardinality to control costs. What if you just .. didn‚Äôt have to do that? What would you do with your lives? If you could just store and query on all the crazy strings you want, forever? üåà\n\nMetrics are a bridge to our past\n\nWhy are observability 1.0 tools so unbelievably, eyebleedingly expensive? As anyone who works with data can tell you, this is always what happens when you use the wrong tool for the job. Once again, metrics are a great tool for summarizing vast quantities of data. When it comes to understanding complex systems, they flail.\n\nI wrote a whole whitepaper earlier this year that did a deep dive into exactly why tools built on top of metrics are so unavoidably costly. If you want the gnarly detail, download that.\n\nThe TLDR is this: tools built on metrics ‚Äî whether RUM, APM, dashboards, etc ‚Äî are a bridge to our past. If there‚Äôs one thing I‚Äôm certain of, it‚Äôs that tools built on top of wide, structured logs are the bridge to our future.\n\nWide, structured log events are the bridge to our future\n\nFive years from now, I predict that the center of gravity will have swung dramatically; all modern engineering teams will be powering their telemetry off of tools backed by wide, structured log events, not metrics. It‚Äôs getting harder and harder and harder to try and wring relevant insights out of metrics-based observability tools. The end of the ZIRP era is bringing unprecedented cost pressure to bear, and it‚Äôs simply a matter of time.\n\nThe future belongs to tools built on wide, structured log events ‚Äî a single source of truth that you can trace over time, or zoom in, zoom out, derive SLOs from, etc.\n\nIt‚Äôs the only way to understand our systems in all their skyrocketing complexity. This constant dance with cost vs cardinality consumes entire teams worth of engineers and adds zero value. It adds negative value.\n\nAnd here‚Äôs the weirdest part. The main thing holding most teams back psychologically from embracing o11y 2.0 seems to be the entrenched difficulties they have grappling with o11y 1.0, and their sense that they can‚Äôt adopt 2.0 until they get a handle on 1.0. Which gets things exactly backwards.\n\nBecause observability 2.0 is so much easier, simpler, and more cost effective than 1.0.\n\nobservability 1.0 *is* the hard way\n\nIt‚Äôs so fucking hard. We‚Äôve been doing it so long that we are blind to just how HARD it is. But trying to teach teams of engineers to wrangle metrics, to squeeze the questions they want to ask into multiple abstract formats scattered across many different tools, with no visibility into what they‚Äôre doing until it comes out eventually in form of a giant bill‚Ä¶ it‚Äôs fucking hard.\n\nObservability 2.0 is so much simpler. You want data, you just toss it in. Format? don‚Äôt care. Cardinality? don‚Äôt care.\n\nYou want to ask the question, you just ask it. Format? don‚Äôt care.\n\nTeams are beating themselves up trying to master an archaic, unmasterable set of technical tradeoffs based on data types from the 80s. It‚Äôs an unwinnable war. We can‚Äôt understand today‚Äôs complex systems without context-rich, explorable data.\n\nWe need more options for observability 2.0 tooling\n\nMy hope is that by sketching out these technical differences between o11y 1.0 and 2.0, we can begin to collect and build up a vendor-neutral library of o11y 2.0 options for folks. The world needs more options for understanding complex systems besides just Honeycomb and Baselime.\n\nThe world desperately needs an open source analogue to Honeycomb ‚Äî something built for wide structured events, stored in a columnar store (or even just Clickhouse), with an interactive interface. Even just a written piece on how you solved it at your company would help move the industry forward.\n\nMy other hope is that people will stop building new observability startups built on metrics. Y‚Äôall, Datadog and Prometheus are the last, best metrics-backed tools that will ever be built. You can‚Äôt catch up to them or beat them at that; no one can. Do something different. Build for the next generation of software problems, not the last generation.\n\nIf anyone knows of anything along these lines, please send me links? I will happily collect them and signal boost. Honeycomb is a great, lifechanging tool (and we have a generous free tier, hint hint) but one option does not a movement make.\n\n<3 charity\n\nP.S. Here‚Äôs a great piece written by Ivan Burmistrov on his experience using observability 2.0 type tooling at Facebook ‚Äî namely Scuba, which was the inspiration for Honeycomb. It‚Äôs a terrific piece and you should read it.\n\nP.P.S. And if you‚Äôre curious, here‚Äôs the long twitter thread I wrote in October of 2023 on how we lost the battle to define observability:\n\nLast month I got to attend GOTO Chicago and give a talk about continuous deployment and high-performing teams. Honestly I did a terrible job, and I‚Äôm not being modest. I had just rolled off a delayed redeye flight; I realized partway through that I had the wrong slides loaded, and my laptop screen was flashing throughout the talk, which was horribly distracting and means I couldn‚Äôt read the speaker notes or see which slide was next. üòµ Argh!\n\nAnyway, shit happens. BUT! I got to meet some longstanding online friends and acquaintances (hi JJ, Avdi, Matt!) and got to eat some of Hillel Wayne‚Äôs homemade chocolates, and the Q&A session afterwards was actually super fun.\n\nMy talk was about what high performing teams look like and why it‚Äôs so important to be on one (spoiler: because this is the #1 way to become a radically better engineer!!). Most of the Q&A topics therefore came down to some version of ‚Äúokay, so how can I help my team get there?‚Äù These are GREAT questions, so I thought I‚Äôd capture a few of them for posterity.\n\nBut first‚Ä¶ just a reminder that the actual best way to persuade people to listen to you is to make good decisions and display good judgment. Each of us has an implicit reputation score, which formal power can only overcome to an extent. Even the most junior engineer can work up a respectable reputation over time, and even principal engineers can fritter theirs away by shooting off at the mouth. ü•∞\n\n‚Äúhow can I drive change when I have no power or influence?‚Äù\n\nThis first question came from someone who had just landed their first real software engineering job (congrats!!!):\n\n‚ÄúThis is my first real job as a software engineer. One other junior person and myself just formed a new team with one super-senior guy who has been there forever. He built the system from scratch and knows everything about it. We keep trying to suggest ideas like the things you talked about in your talk, but he always shoots us down. How can we convince him to give it a shot?‚Äù\n\nWell, you probably can‚Äôt. ‚ò∫Ô∏è Which isn‚Äôt the end of the world.\n\nIf you‚Äôre just starting to write software every day, you are facing a healthy learning curve for the next 3-5 years. Your one and only job is to learn and practice as much you possibly can. Pour your heart and soul into basic skills acquisition, because there really are no shortcuts. (Please don‚Äôt get hooked on chatGPT!!)\n\nI know that I came down hard in my talk on the idea that great engineers are made by great teams, and that the best thing most people can do for their career is to join a high-performing, fast-moving team. There will come a time where this is true for you too, but by then you will have skills and experience, and it will be much easier for you to find a new job, one with a better culture of learning.\n\nIt is hard to land your first job as a software engineer. Few can afford to be picky. But as long as you are a) writing code every day, b) debugging code every day, and c) getting good feedback via code reviews, this job will get you where you need to go. When you‚Äôre fluent and starting to mentor others, or getting into higher level architecture work, or when you‚Äôre starting to get bored ‚Ä¶ then it‚Äôs time to start looking for roles with better teachers and a more collaborative team, so your growth doesn‚Äôt stall. (Please don‚Äôt fall into the Trap of the Premature Senior.)\n\nThis is an apprenticeship industry. You‚Äôre like a med student right now, who is just starting to do rounds under the supervision of an attending physician (your super-senior engineer). You can kinda understand why he isn‚Äôt inclined to listen to your opinions on his choice of stethoscope or how he fills out a patient chart. A better teacher would take time to listen and explain, but you already know he isn‚Äôt one. ü§∑\n\nI only have one piece of advice. If there‚Äôs something you want to try, and it involves doing engineering work, consider tinkering around and building it after hours. It‚Äôs real hard to say no to someone who cares enough to invest their own time into something.\n\n‚Äúhow can I drive change when I am a tech lead on a new team?‚Äù\n\n‚ÄúI have the same question! ‚Äî except I‚Äôm a tech lead, so in theory I DO have some power and influence. But I just joined a new team, and I‚Äôm wondering what the best way is to introduce changes or roll them out, given that there are soooo many changes I‚Äôd like to make.‚Äù\n\n(I wrote a somewhat scattered post a few years ago on engineers and influence, or influence without authority, which covers some related territory.)\n\nAs a tech lead who is new to a team, busting at the seams with changes I want to make, here‚Äôs where I‚Äôd start:\n\nUnderstand why things are the way they are and get to know the personalities on your team a bit before you start pitching changes. (UNLESS they are coming to you with arms outstretched, pleading desperately for changes ~fast~ because everything is on fire and they know they need help. This does happen!)\n\nSpend some time working with the old systems, even if you think you already understand. It‚Äôs not enough for you to know; you need to take the team on this journey with you. If you expect your changes to be at all controversial, you need to show that you respect their work and are giving it a chance.\n\nChange one thing at a time, and go for the developer experience wins first. Address things that will visibly pay off for your team in terms of shipping faster, saving time, less frustration. You have no credibility in the beginning, so you want to start racking up wins before you take on the really hard stuff.\n\nRoll up your sleeves. Nothing buys a leader more goodwill than being willing to do the scut work. Got a flaky test suite that everybody has been dreading trying to fix? I smell opportunity‚Ä¶\n\nPitch it as an experiment. If people aren‚Äôt sold on your idea for e.g. code review SLAs, ask if they‚Äôd be willing to try it out for three weeks just as an experiment.\n\nStrategically shop it around to the rest of the team, if you sense there will be resistance‚Ä¶\n\nAt this point in my answer üëÜ I outlined a technique for persuading a team and building support for a plan or an idea, especially when you already know it‚Äôs gonna be an uphill battle. Hillel Wayne said I should write it up in a blog post, so here it is! (I‚Äôll do anything for free chocolate üòç)\n\n‚ÄúHow can I get people on board with my controversial plan?‚Äù\n\nSo you have a great idea, and you‚Äôre eager to get started. Awesome!!! You believe it‚Äôs going to make people‚Äôs lives better, even though you know you are going to have to fight tooth and nail to make it happen.\n\nWhat NOT to do:\n\nWalk into the team meeting and drop your bomb idea on everyone cold:\n\n‚ÄúHey, I think we should stop shipping product changes until we fix our build pipeline to the point where we can auto-deploy each merge set to production, one at a time, in under an hour.‚Äù ~ (for example)\n\n‚Ä¶. then spend the rest of the hour grappling with everybody‚Äôs thoughts, feelings, and intense emotional reactions, before getting discouraged and slinking away, vowing to never have another idea, ever again.\n\nWhat to do instead:\n\nSuss out your audience. Who will be there? How are they likely to react? Are any of them likely to feel especially invested in the existing solution, maybe because they built it? Are any of them known for their strong opinions or being combative?\n\nGreat!!! Your first move is to have a conversation with each of them. Approach them in the spirit of curiosity, and ask what they think of your idea. Talking with them will also help you hash out the details and figure out if it is actually a good idea or not.\n\nYour goal is to make the rounds, ask for advice, identify any allies, and talk your idea through with anybody who is likely to oppose you‚Ä¶before the meeting where you intend to unveil your plan. So that when that happens, you have:\n\ngiven people the chance to process their reactions and ask questions in private\n\nensured that key people will not feel surprised, threatened, or out of the loop\n\nalready heard and discussed any objections\n\nideally, you have earned their support!\n\nEven if you didn‚Äôt manage to convince every person, this was still a valuable exercise. By approaching people in advance, you are signaling that you respect them and their voice matters. You are always going to get people‚Äôs absolute worst reactions when you spring something on them in a group setting; any anxiety or dismay will be amplified tenfold. By letting them reflect and ask questions in private, you‚Äôre giving time for their better selves to emerge.\n\nWhat to do instead‚Ä¶if you‚Äôre a manager:\n\nAs an engineer or a tech lead, you sometimes end up out front and visible as the owner of a change you are trying to drive. This is normal. But as a manager, there are far more times when you need to influence the group but not be the leader of the change, or when you need to be wary of sounding like you are telling people what to do. These are just a few of the many reasons it can be highly effective to have other people arguing on your behalf.\n\nIn the ideal scenario, particularly on technical topics, you don‚Äôt have to push for anything. All you do is pose the question, then sit back and listen as vigorous debate ensues, with key stakeholders and influential engineers arguing for your intended outcome. That‚Äôs a good sign that not only are they convinced, they feel ownership over the decision and its execution. This is the goal! üåà\n\nIt‚Äôs not just about persuading people to agree with you, either. Instead of having a shitty dynamic where engineers are attached to the old way of doing things and you are ‚Äúdragging them‚Äù into the newer ways against their will, you are inviting them to partner with you. You are offering them the opportunity to lead the team into the brave new world, by getting on board early.\n\n(It probably goes without saying, but always start with the smallest relevant group of stakeholders, and not, say, all of engineering, or a group that has no ownership over the given area. üôÉ And ‚Ä¶ even this strategy will stop working rather quickly, if your controversial ideas all turn out to be disastrous. üòâ)\n\n‚ÄúHow do I know where to even start?!? üò±‚Äù\n\nBefore I wrap up, I want to circle back to the question from the tech lead about how to drive change on a team when you do have some influence or power. He went on to say (or maybe this was from a third questioner?*):\n\n‚ÄúThere is SO MUCH I‚Äôd like to do or change with our culture and our tech stack. Where can I even start??‚Äù\n\nYeah, it can be pretty overwhelming. And there are no universal answers‚Ä¶ as you know perfectly well, the answer is always ‚Äúit depends.‚Äù ‚ò∫Ô∏è But in most cases you can reduce the solution space substantially to one of the two following starting points.\n\n1. Can you understand what‚Äôs going on in your systems? If not, start with observability.\n\nIt doesn‚Äôt have to be elegant or beautiful; grepping through shitty text logs is fine, if it does the trick. But do any of the following make you shudder in recognition?:\n\nIf I get paged, I might lose the rest of the afternoon trying to figure out what happened\n\nOur biggest problem is performance and we don‚Äôt know where the time is going\n\nWe have a lot of flaky, flappy alerts, and unexplained outages that simply resolve themselves without our ever truly understanding what happened.\n\nIf you can‚Äôt understand what‚Äôs going on in your system, you have to start with instrumentation and observability. It‚Äôs just too deadly, and too risky, not to. You‚Äôre going to waste a ton of time stabbing around in the dark trying to do anything else without visibility. Put your glasses on before you start driving down the freeway, please.\n\n2. Can you build, test and deploy software in under an hour? If not, start with your deploy pipeline.\n\nSpecifically, the interval of time between when the code is written and when it‚Äôs being used in production. Make it shorter, less flaky, more reliable, more automated. This is the feedback loop at the heart of software engineering, which means that it‚Äôs upstream from a whole pile of pathologies and bullshit that creep in as a consequence of long, painful, batched-up deploys.\n\nHere‚Äôs a talk I‚Äôve given a few times on why this matters so much:\n\nYou pretty much can‚Äôt fail with one of those two; your lives will materially improve as you make progress. And the iterative process of doing them will uncover a great deal of shit you should probably know about.\n\nCheers! ü•Ç\n\ncharity.\n\n* My apologies if I remembered anyone‚Äôs question inaccurately!\n\nIf you‚Äôre like most of us, you learned to debug as a baby engineer by way of printf(3). By the time you were shipping code to production you had probably learned to instrument your code with a real metrics library. Maybe a tenth of us learned to use gdb and still step through functions on the regular. (I said maybe.)\n\nPrinting stuff to stdout is still the Swiss Army knife of tools. Always there when you reach for it, usually helps more than it does harm. (I said usually.)\n\nAnd then! In case you‚Äôve been living under a rock, we recently went and blew up ye aulde monolythe, and in the process we ‚Ä¶ lost most of our single-process tools and techniques for debugging. Forget gdb; even printf doesn‚Äôt work when you‚Äôre hopping the network between functions.\n\nIf your tool set no longer works for you, friend, it‚Äôs time to go all in. Maybe what you wanted was a faster horse, but it‚Äôs time for a car, and the sooner you turn in your oats for gas cans and a spare tire, the better.\n\nExercising Good Technical Judgment (When You Don‚Äôt Have Any)\n\nIf you‚Äôre stuck trying to debug modern problems with pre-modern tooling, the first thing to do is stop digging the hole. Stop pushing good data after bad into formats and stores that aren‚Äôt going to help you answer the right questions.\n\nIn brief: if you aren‚Äôt rolling out a solution based on arbitrarily wide, structured raw events that are unique and ordered and trace-aware and without any aggregation at write time, you are going to regret it. (If you aren‚Äôt using OpenTelemetry, you are going to regret that, too.)\n\nSo just make the leap as soon as possible.\n\nBut let‚Äôs rewind a bit. Let‚Äôs start with observability.\n\nObservability: an introduction\n\nObservability is not a new word or concept, but the definition of observability as a specific technical term applied to software engineering is relatively new ‚Äî about four years old. Before that, if you heard the term in softwareland it was only as a generic synonym for telemetry (‚Äúthere are three pillars of observability‚Äù, in one annoying formulation) or team names (twitter, for example, has long had an ‚Äúobservability team‚Äù).\n\nThe term itself originates with control theory:\n\n‚ÄúIn control theory, observability is a measure of how well internal states of a system can be inferred from knowledge of its external outputs. The observability and controllability of a system are mathematical duals. The concept of observability was introduced by Hungarian-American engineer Rudolf E. K√°lm√°n for linear dynamic systems.[1][2]‚Äù\n\nBut when applied to a software context, observability refers to how well you can understand and reason about your systems, just by interrogating them and inspecting their outputs with your tools. How well can you understand the inside of the system from the outside?\n\nAchieving this relies your ability to ask brand new questions, questions you have never encountered and never anticipated ‚Äî without shipping new code. Shipping new code is cheating, because it means that you knew in advance what the problem was in order to instrument it.\n\nBut what about monitoring?\n\nMonitoring has a long and robust history, but it has always been about watching your systems for failures you can define and expect. Monitoring is for known-unknowns, and setting thresholds and running checks against the system. Observability is about the unknown-unknowns. Which requires a fundamentally different mindset and toolchain.\n\n‚ÄúMonitoring is the action of observing and checking the behavior and outputs of a system and its components over time.‚Äù ‚Äî @grepory, in his talk ‚ÄúMonitoring is Dead‚Äú.\n\nMonitoring is a third-person perspective on your software. It‚Äôs not software explaining itself from the inside out, it‚Äôs one piece of software checking up on another.\n\nObservability is for understanding complex, ephemeral, dynamic systems (not for debugging code)\n\nYou don‚Äôt use observability for stepping through functions; it‚Äôs not a debugger. Observability is for swiftly identifying where in your system the error or problem is coming from, so you can debug it ‚Äî by reproducing it, or seeing what it has in common with other erroring requests. You can think of observability as being like B.I. (business intelligence) tooling for software applications, in the way you engage in a lot of exploratory, open-ended data sifting to detect novel patterns and behaviors.\n\nObservability is often about swiftly isolating or tracking down the problem in your large, sprawling, far-flung, dynamic system. Because the hard part of distributed systems is rarely debugging the code, it‚Äôs figuring out where the code you need to debug is.\n\nThe need for observability is often associated with microservices adoption, because they are prohibitively difficult to debug without service-level event oriented tooling ‚Äî the kind you can get from Honeycomb and Lightstep.. and soon, I hope, many other vendors.\n\nEvents are the building blocks of observability\n\nErgh, another overloaded data term. What even is an ‚Äúevent‚Äù?\n\nAn observability ‚Äúevent‚Äù is a hop in the lifecycle of an end-to-end request. If a request executes code on three services separated by network hops before returning to the user, that request generated three observability ‚Äúevents‚Äù, each packed with context and details about that code running in that environment. These are also sometimes called ‚Äúcanonical log lines‚Äú. If you implemented tracing, each event may be a span in your trace.\n\nIf request ID #A897BEDC hits your edge, then your API service, then four more internal services, and twice connects to a db and runs a query, then request ID #A897BEDC generated 8 observability events ‚Ä¶ assuming you are in fact gathering observability data from the edge, the API, the internal services and the databases.\n\nThis is an important caveat. We only gather observability events from services that we can and do introspect. If it‚Äôs a black box to us, that hop cannot generate an observability event. So if request ID #A897BEDC also performed 20 cache lookups and called out to 8 external HTTP services and 2 managed databases, those 30 hops do not generate observability events (assuming you haven‚Äôt instrumented the memcache service and have no instrumentation from those external services/dbs). Each request generates one event per request per service hop.**\n\n(I also wrote about logs vs structured events here.)\n\nObservability is a first-person narrative.\n\nWe care primarily about self-reported status from the code as it executes the request path.\n\nInstrumentation is your eyes and ears, explaining the software and its environment from the perspective of your code. Monitoring, on the other hand, is traditionally a third-person narrative ‚Äî it‚Äôs one piece of software checking up on another piece of software, with no internal knowledge of its hopes and dreams.\n\nFirst-person narrative reports have the best potential for telling a reliable narrative. And more importantly, they map directly to user experience in a way that third-party monitoring does not and cannot.\n\nEvents ‚Ä¶ must be structured.\n\nFirst, structure your goddamn data. You‚Äôre a computer scientist, you‚Äôve got no business using text search to plow through terabytes of text.\n\nEvents ‚Ä¶ are not just structured logs.\n\nNow, part of the reason people seem to think structured data is cost-prohibitive is that they‚Äôre doing it wrong. They‚Äôre still thinking about these like log lines. And while you can look at events like they‚Äôre just really wide structured log lines that aren‚Äôt flushed to disk, here‚Äôs why you shouldn‚Äôt: logs have decades of abhorrent associations and absolutely ghastly practices.\n\nInstead of bundling up and passing along one neat little pile of context, they‚Äôre spewing log lines inside loops in their code and DDoS‚Äôing their own logging clusters.They‚Äôre shitting out ‚Äúlog lines‚Äù with hardly any dimensions so they‚Äôre information-sparse and just straight up wasting the writes. And then to compensate for the sparseness and repetitiveness they just start logging the same exact nouns tens or hundreds of times over the course of the request, just so they can correlate or reconstruct some lousy request that they never should have blown up in the first place!\n\nBut they keep hearing they should be structuring their logs, so they pile structure on to their horrendous little strings, which pads every log line by a few bytes, so their bill goes up but they aren‚Äôt getting any benefit! just paying more! What the hell, structuring is bull shit!\n\nKittens. You need a fundamentally different approach to reap the considerable benefits of structuring your data.\n\nBut the difference between strings and structured data is ~basically the difference between grep and all of computer science. üòõ\n\nEvents ‚Ä¶ must be arbitrarily wide and dense with context.\n\nSo the most effective way to structure your instrumentation, to get the absolute most bang for your buck, is to emit a single arbitrarily wide event per request per service hop. At Honeycomb, the maturely instrumented datasets that we see are often 200-500 dimensions wide. Here‚Äôs an event that‚Äôs just 20 dimensions wide:\n\n{ \"timestamp\":\"2018-11-20 19:11:56.910\", \"az\":\"us-west-1\", \"build_id\":\"3150\", \"customer_id\":\"2310\", \"durationMs\":167, \"endpoint\":\"/api/v2/search\", \"endpoint_shape\":\"/api/v2/search\", \"fraud_dur\":131, \"hostname\":\"app14\", \"id\":\"f46691dfeda9ede4\", \"mysql_dur\":\"\", \"name\":\"/api/v2/search\", \"parent_id\":\"\", \"platform\":\"android\", \"query\":\"\", \"serviceName\":\"api\", \"status_code\":200, \"traceId\":\"f46691dfeda9ede4\", \"user_id\":\"344310\", \"error_rate\":0, \"is_root\":\"true\" }\n\nSo a well-instrumented service should have hundreds of these dimensions, all bundled around the context of each request. And yet ‚Äî and here‚Äôs why events blow the pants off of metrics ‚Äî even with hundreds of dimensions, it‚Äôs still just one write. Adding more dimensions to your event is effectively free, it‚Äôs still one write plus a few more bits.\n\nCompare this to a metric-based systems, where you are often in the position of trying to predict whether a metric will be valuable enough to justify the extra write, because every single metric or tag you add contributes linearly to write amplification. Ever gotten billed tens of thousands of dollars for your custom metrics, or had to prune your list of useful custom metrics down to something affordable? (‚ÄúBUT THOSE ARE THE ONLY USEFUL ONES!‚Äù, as every ops team wails)\n\nEvents ‚Ä¶ must pass along the blob of context as the request executes\n\nAs you can imagine, it can be a pain in the ass to keep passing this blob of information along the life of the request as it hits many services and databases. So at Honeycomb we do all the annoying parts for you with our integrations. You just install the go pkg or ruby gem or whatever, and under the hood we:\n\ninitialize an empty debug event when the request enters that service\n\nprepopulate the empty debug event with any and all interesting information that we already know or can guess. language type, version, environment, etc.\n\ncreate a framework so you can just stuff any other details in there as easily as if you were printing it out to stdout\n\npass the event along and maintain its state until you are ready to error or exit\n\nwrite the extremely wide event out to honeycomb\n\nEasy!\n\n(Check out this killer talk from @lyddonb on ‚Ä¶ well everything you need to know about life, love and distributed systems is in here, but around the 12:00 mark he describes why this approach is mandatory. WATCH IT. https://www.youtube.com/watch?v=xy3w2hGijhE&feature=youtu.be)\n\nEvents ‚Ä¶ should collect context like sticky buns collect dust\n\nOther stuff you‚Äôll want to track in these structured blobs includes:\n\nMetadata like src, dst headers\n\nThe timing stats and contents of every network call (our beelines wrap all outgoing http calls and db queries automatically)\n\nEvery raw db query, normalized query family, execution time etc\n\nInfra details like AZ, instance type, provider\n\nLanguage/environment context like $lang version, build flags, $ENV variables\n\nAny and all unique identifying bits you can get your grubby little paws on ‚Äî request ID, shopping cart ID, user ID, request ID, transaction ID, any other ID ‚Ä¶ these are always the highest value data for debugging.\n\nAny other useful application context. Service name, build id, ordering info, error rates, cache hit rate, counters, whatever.\n\nPossibly the system resource state at this point in time. e.g. values from /proc/net/ipv4 stats\n\nCapture all of it. Anything that ever occurs to you (‚Äúthis MIGHT be handy someday‚Äù) ‚Äî don‚Äôt even hesitate, just throw it on the pile. Collect it up in one rich fat structured blob.\n\nEvents ‚Ä¶ must be unique, ordered, and traceable\n\nYou need a unique request ID, and you need to propagate it through your stack in some way that preserves sequence. Once you have that, traces are just a beautiful visualization layer on top of your shiny event data.\n\nEvents ‚Ä¶ must be stored raw.\n\nBecause observability means you need to be able to ask any arbitrary new question of your system without shipping new code, and aggregation is a one-way trip. Once you have aggregated your data and discarded the raw requests, you have destroyed your ability to ask new questions of that data forever. For Ever.\n\nAggregation is a one-way trip. You can always, always derive your pretty metrics and dashboards and aggregates from structured events, and you can never go in reverse. Same for traces, same for logs. The structured event is the gold standard. Invest in it now, save your ass in the future.\n\nIt‚Äôs only observability if you can ask new questions. And that means storing raw events.\n\nEvents‚Ä¶are richer than metrics\n\nThere‚Äôs always tradeoffs when it comes to data. Metrics choose to sacrifice context and connective tissue, and sometimes high cardinality support, which you need to correlate anomalies or track down outliers. They have a very small, efficient data format, but they sacrifice everything else by discarding all but the counter, gauge, etc.\n\nA metric looks like this, by the way.\n\n{ metric: \"db.query.time\", value: 0.502, tags: Array(), type: set }\n\nThat‚Äôs it. It‚Äôs just a name, a number and maybe some tags. You can‚Äôt dig into the event and see what else was happening when that query was strangely slow. You can never get that information back after discarding it at write time.\n\nBut because they‚Äôre so cheap, you can keep every metric for every request! Maybe. (Sometimes.) More often, what happens is they aggregate at write time. So you never actually get a value written out for an individual event, it smushes everything together that happens in the 1 second interval and calculates some aggregate values to write out. And that‚Äôs all you can ever get back to.\n\nWith events, and their relative explosion of richness, we sacrifice our ability to store every single observability event about every request. At FB, every request generated hundreds of observability events as it made its way through the stack. Nobody, NOBODY is going to pay for an o11y stack that is hundreds of times as large as production. The solution to that problem is sampling.\n\nEvents‚Ä¶should be sampled.\n\nBut not dumb, blunt sampling at server side. Control it on the client side.\n\nThen sample heavily for events that are known to be common and useless, but keep the events that have interesting signal. For example: health checks that return 200 OK usually represent a significant chunk of your traffic and are basically useless, while 500s are almost always interesting. So are all requests to /login or /payment endpoints, so keep all of them. For database traffic: SELECTs for health checks are useless, DELETEs and all other mutations are rare but you should keep all of them. Etc.\n\nYou don‚Äôt need to treat your observability metadata with the same care as you treat your billing data. That‚Äôs just dumb.\n\n‚Ä¶ To be continued.\n\nI hope it‚Äôs now blazingly obvious why observability requires ‚Äî REQUIRES ‚Äî that you have access to raw structured events with no pre-aggregation or write-time rollups. Metrics don‚Äôt count. Just traces don‚Äôt count. Unstructured logs sure as fuck don‚Äôt count.\n\nStructured, arbitrarily wide events, with dynamic sampling of the boring parts to control costs. There is no substitute.\n\nFor more about the technical requirements for observability, read this, this, or this.\n\n**The deep fine print: it‚Äôs one observability event per request per service hop ‚Ä¶ because we gather observability detail organized by request id. Databases may be different. For example, with MongoDB or MySQL, we can‚Äôt instrument them to talk to honeycomb directly, so we gather information about its internal perspective by 1) tailing the slow query log (and turning it up to log all queries if perf allows), 2) streaming tcp over the wire and reconstructing transactions, 3) connecting to the mysql port as root every couple seconds from cron, then dumping all mysql stats and streaming them in to honeycomb as an event. SO. Database traffic is not organized around connection length or unique request id, it is organized around transaction id or query id. Therefore it generates one observability event per query or transaction.\n\nIn other words: if your request hit the edge, API, four internal services, two databases ‚Ä¶ but ran 1 query on one db and 10 queries on the second db ‚Ä¶ you would generate a total of *19 observability events* for this request.\n\nFor more on observability for databases and other black boxes, try this blog post.\n\nFirst published on 2022-04-13 at https://www.honeycomb.io/blog/truth-about-meh-trics-metrics\n\nA long time ago, in a galaxy far, far away, I said a lot of inflammatory things about metrics.\n\n‚ÄúMetrics are shit salad.‚Äù\n\n‚ÄúMetrics are simply nerfed dimensions.‚Äù\n\n‚ÄúMetrics suck,‚Äù ‚Äúmetrics are legacy,‚Äù ‚Äúmetrics and time series aggregates will fucking kneecap you.‚Äù\n\nI cannot tell a lie; Twitter will testify that I‚Äôve spent the past six years ragging on metrics. So much so that ever since we launched Honeycomb Metrics last year, our poor solution architects have been encountering skeptics in the field who repeat my quotes back to them and ask, dubiously, whether Honeycomb Metrics are any good or not, and whether we genuinely plan on investing in it or not, given our known anti-metrics sympathies.\n\nThat‚Äôs a great question. üòä\n\nMetrics aren‚Äôt worthless; they‚Äôre just limited.\n\nMetrics are a mature technology that‚Äôs been around for over 30 years, and they have some real advantages. They‚Äôre tiny, fast, and cheap; you can hold a bunch of them in memory as counters, summaries, and gauges. They aggregate well and take up a fixed amount of storage space. The entire monitoring industry is built on top of metrics.\n\nWhen it comes to workloads like, ‚ÄúHow heavy is the write load on my hard drive?‚Äù or ‚ÄúWhat is the temperature or fan status inside my chassis?‚Äù or ‚ÄúWhat is the traffic rate in and out of this interface on my switch?‚Äù metrics are what you should use. In fact, pretty much any time you want to know the health of a system or component in toto, metrics are the right tool.\n\nBecause that‚Äôs what metrics do best‚Äîreport statistics in aggregate, from the perspective of any system or component. They can tell you that your Ruby HTTP worker pool is 70% utilized or that your nginx webserver is returning 502s 1% of the time. What they can‚Äôt tell you is what this means for any one of your users, applications, delivery vehicles, and so forth.\n\nUntil recently, metrics-based tools or logs were the only game in town. People were trying to sell us metrics tools for observability use cases, and that‚Äôs what got my goat so badly. If you simply append ‚Äú‚Ä¶ for observability‚Äù to each of my inflammatory statements, then I stand by them completely.\n\n‚ÄúMetrics are shit salad ‚Ä¶ for observability.‚Äù\n\nYup, rings true.\n\nYou‚Äôre never going to make a metrics tool like Prometheus or Datadog into an observability tool. You‚Äôre just not. Observability is about unknown-unknowns, while metrics are a tool for known-unknowns.\n\nIf you need a refresher on the differences between observability and monitoring, I‚Äôll refer you to pieces like this, this, and this. What I want to talk about here is slightly different. In a post-observability world, what is the true and proper place for metrics tooling?\n\nMetrics and observability have different use cases.\n\nMetrics aren‚Äôt completely useless, even if you have a robust observability presence. We still use metrics at Honeycomb to this day for certain workloads‚Äîand always will because they‚Äôre the right tool for the job.\n\nThere are two kinds of workloads, roughly speaking: your code‚Äîthe code you write, review, ship, debug and maintain on a daily basis. And other people‚Äôs code‚Äîthe code you have to run and use in order to support your code. Some examples of the latter might be: Linux, Docker, MySql, Amazon RDS, Kafka, AWS Lambda, GCP gateways, memcache, CI/CD pipelines, Kubernetes, etc.\n\nYour code is your crown jewels, the code you need to survive and succeed as a business. It changes constantly‚Äîmany times per week, if not per day. You are expected to understand its inner workings intimately, and spend lots of time chasing down bugs or understanding and reproducing behavior. You care about the way it performs and interacts with each and every individual user, with changing infrastructure state, and under a variety of different load conditions.\n\nThat is why your code demands observability. In order to understand your software, you must first instrument it, in a way that collects lots of rich context and bundles it up around each event end-to-end. Then you need to stream those events into a tool that lets you slice and dice and trace and explore with support for high-cardinality and high-dimensionality data. That‚Äôs the only way you‚Äôre going to be able to correlate errors, track down outliers, and reflect each user‚Äôs experience.\n\nBut what about the rest of the software? You can‚Äôt instrument Amazon RDS, and only crazy people would instrument, rebuild, and repackage things like Kafka or Docker or nginx. The whole point of third-party software is that you DON‚ÄôT USE IT until it‚Äôs stable enough to be taken more or less for granted. Sure, you roll updates, but usually on the order of months or years‚Äînot every day. You don‚Äôt need to be intimately familiar with its inner workings because you aren‚Äôt changing it every day. Those aren‚Äôt your crown jewels.\n\nYou do care about their health though, only differently. You care about whether you need to provision more capacity or not. You care about knowing how hard you‚Äôre hammering on the underlying hardware or hypervisor. That‚Äôs why metrics and monitoring are the right tools to use for third-party code. They don‚Äôt let you peer under the hood in the same way, or slice and dice in the same way, but that‚Äôs okay. You shouldn‚Äôt have to.\n\nWith third-party stuff, you don‚Äôt care about the code, you care about the health of the service. In aggregate.\n\n(There are some kinds of in-between software, like databases, where event-level information is super useful for debugging things like slow queries and lock percentages, and you can use various black box techniques to approximate observability without instrumentation. But in general this model holds up quite well.)\n\nIn a post-observability world, what are metrics for?\n\nI‚Äôve often pointed out that observability is built on top of arbitrarily wide structured data blobs, and that metrics, logs, and traces can be derived from those blobs while the reverse is not true‚Äîyou can‚Äôt take a bunch of metrics and reformulate a rich event.\n\nAnd yes, people who have observability typically find themselves using metrics and dashboards less and less. They‚Äôre simply not as versatile or useful as events that you can slice and dice and manipulate in infinite ways. And you can derive aggregates and trends from the events you have stored.\n\nBut metrics will always be useful for understanding third-party software, from the perspective of the service, cluster, or node. They will always be the right tool for the job when it comes to software interfacing with hardware. And they can be super complementary when you are investigating your code using events and instrumentation.\n\nIf you‚Äôre an engineer writing and shipping code, you‚Äôre never not going to want to know if your change caused memory usage to triple, or CPU utilization to skyrocket, or disk usage or network throughput to saturate. That‚Äôs why we built Honeycomb Metrics as an overlay, a way to enhance or validate your understanding of the impact your code changes have had on the underlying system.\n\nMetrics are also valuable as a bridge to the past. People have been instrumenting software for metrics for 30 years‚Äîthey‚Äôre never going away completely, and not everything can or should be reinstrumented with events. Lots of people already have robust monitoring systems that slurp in millions of metrics. Nobody wants to have to redo all that work just because they‚Äôre moving to a different tool, so people tend to point their metrics firehose at Honeycomb as a way of getting started as they roll observability out into their code.\n\nThe other day I said this on twitter ‚Äî\n\n‚Ä¶ which stirred up some Feelings for many people. üôÉ So I would like to explain my opinions in more detail.\n\nStatic vs dynamic dashboards\n\nFirst, let‚Äôs define the term. When I say ‚Äúdashboard‚Äù, I mean STATIC dashboards, i.e. collections of metrics-based graphs that you cannot click on to dive deeper or break down or pivot. If your dashboard supports this sort of responsive querying and exploration, where you can click on any graph to drill down and slice and dice the data arbitrarily, then breathe easy ‚Äî that‚Äôs not what I‚Äôm talking about. Those are great. (I don‚Äôt really consider them dashboards, but I have heard a few people refer to them as ‚Äúdynamic dashboards‚Äù.)\n\nActually, I‚Äôm not even ‚Äúagainst‚Äù static dashboards. Every company has them, including Honeycomb. They‚Äôre great for getting a high level sense of system functioning, and tracking important stats over long intervals. They are a good starting point for investigations. Every company should have a small, tractable number of these which are easily accessible and shared by everyone.\n\nDebugging with dashboards: it‚Äôs a trap\n\nWhat dashboards are NOT good at is debugging, or understanding or describing novel system states.\n\nI can hear some of you now: ‚ÄúBut I‚Äôve debugged countless super-hard unknown problems using only static dashboards!‚Äù Yes, I‚Äôm sure you have. If all you have is a hammer, you CAN use it to drive screws into the wall, but that doesn‚Äôt mean it‚Äôs the best tool. And It takes an extraordinary amount of knowledge and experience to be able to piece together a narrative that translates low-level system statistics into bugs in your software and back. Most software engineers don‚Äôt have that kind of systems experience or intuition‚Ä¶and they shouldn‚Äôt have to.\n\nWhy are dashboards bad for debugging? Think of it this way: every dashboard is an answer to a question someone asked at some point. Your monitoring system is probably littered with dashboards, thousands and thousands of them, most of whose questions have been long forgotten and many of whose source data streams have long since gone silent.\n\nSo you come along trying to investigate something, and what do you do? You start skimming through dashboards, eyes scanning furiously, looking for visual patterns ‚Äî e.g. any spikes that happened around the same time as your incident. That‚Äôs not debugging, that‚Äôs pattern-matching. That‚Äôs ‚Ä¶ eyeball racing.\n\nif we did math like we do dashboards\n\nImagine you‚Äôre in a math competition, and you get handed a problem to solve. But instead of pulling out your pencil and solving the equation, step by step, you start hollering out guesses.\n\n‚Äú27!‚Äù\n\n‚Äú19992.41!‚Äù\n\n‚Äú1/4325!‚Äù\n\nThat‚Äôs what flipping through dashboards feels like to me. You‚Äôre riffling through a bunch of graphs that were relevant to some long-ago situation, without context or history, without showing their work. Sometimes you‚Äôll spot the exact scenario, and ‚Äî huzzah! ‚Äî the number you shout is correct! But when it comes to unknown scenarios, the odds are not in your favor.\n\nDebugging looks and feels very different from flipping through answers. You ask a question, examine the answer, and ask another question based on the result. (‚ÄúWhich endpoints were erroring? Are all of the requests erroring, or only some? What did they have in common?‚Äù, etc.)\n\nYou methodically put one foot in front of the other, following the trail of bread crumbs, until the data itself leads you to the answer.\n\nThe limitations of metrics and dashboards\n\nUnfortunately, you cannot do that with metrics-based dashboards, because you stripped away the connective tissue of the event back when you wrote the metrics out to disk.\n\nIf you happened to notice while skimming through dashboards that your 404 errors spiked at 14:03, and your /payment and /import endpoints started erroring at 14.03, and your database started returning a bunch of mysql errors shortly after 14:00, you‚Äôll probably assume that they‚Äôre all related and leap to find more evidence that confirms it.\n\nBut you cannot actually confirm that those events are the same ones, not with your metrics dashboards. You cannot drill down from errors to endpoints to error strings; for that, you‚Äôd need a wide structured data blob per request. Those might in fact be two or three separate outages or anomalies happening at the same time, or just the tip of the iceberg of a much larger event, and your hasty assumptions might extend the outage for much longer than was necessary.\n\nWith metrics, you tend to find what you‚Äôre looking for. You have no way to correlate attributes between requests or ask ‚Äúwhat are all of the dimensions these requests have in common?‚Äù, or to flip back and forth and look at the request as a trace. Dashboards can be fairly effective at surfacing the causes of problems you‚Äôve seen before (raise your hand if you‚Äôve ever been in an incident review where one of the follow up tasks was, ‚Äúcreate a dashboard that will help us find this next time‚Äù), but they‚Äôre all but useless for novel problems, your unknown-unknowns.\n\nOther complaints about dashboards:\n\nThey tend to have percentiles like 95th, 99th, 99.9th, 99.99th, etc. Which can cover over a multitude of sins. You really want a tool that allows you to see MAX and MIN, and heatmap distributions.\n\nA lot of dashboards end up getting created that are overly specific to the incident you just had ‚Äî naming specific hosts, etc ‚Äî which just creates clutter and toil. This is how your dashboards become that graveyard of past outages.\n\nThe most useful approach to dashboards is to maintain a small set of them; cull regularly, and think of them as a list of starter queries for your investigations.\n\nFred Hebert has this analogy, which I like:\n\n‚ÄúI like to compare the dashboards to the big display in a hospital room: heartbeat, pressure, oxygenation, etc. Those can tell you when a thing is wrong, but the context around the patient chart (and the patient themselves) is what allows interpretation to be effective. If all we have is the display but none of the rest, we‚Äôre not getting anywhere close to an accurate picture. The risk with the dashboard is having the metrics but not seeing or knowing about the rest changing.‚Äù\n\nIn conclusion\n\nDashboards aren‚Äôt universally awful. The overuse of them just encourages sloppy thinking, and static ones make it impossible for you to follow the plot of an outage, or validate your hypotheses. ü§í There‚Äôs too many of them, and not enough shared consensus. (It would help if, like, new dashboards expired within a month if nobody looked at them again.)\n\nIf what you have is ‚Äúnothing‚Äù, even shitty dashboards are far better than no dashboards. But shitty dashboards have been the only game in town for far too long. We need more vendors to think about building for queryability, explorability, and the ability to follow a trail of breadcrumbs. Modern systems are going to demand more and more of this approach.\n\nNothing < Dashboards < a Queryable, Exploratory Interface\n\nIf everyone out there who slaps ‚Äúobservability‚Äù on their web page also felt the responsibility to add an observability-enabling interface to their tool, one that would let users explore and identify unknown-unknowns, we would all be in a far better place. üôÇ\n\nLast weekend, @swyx posted a great little primer to instrumentation titled ‚ÄúObservability Tools in JavaScript‚Äù. A friend sent me the link and suggested that I might want to respond and clarify some things about observability, so I did, and we had a great conversation! Here is a lightly edited transcript of my reply tweet storm.\n\nFirst of all, confusion over terminology is understandable, because there are some big players out there actively trying to confuse you! Big Monitoring is indeed actively trying to define observability down to ‚Äúmetrics, logs and traces‚Äù. I guess they have been paying attention to the interest heating up around observability, and well‚Ä¶ they have metrics, logs, and tracing tools to sell? So they have hopped on the bandwagon with some undeniable zeal.\n\nBut metrics, logs and traces are just data types. Which actually has nothing to do with observability. Let me explain the difference, and why I think you should care about this.\n\n‚ÄúObservability? I do not think it means what you think it means.‚Äù\n\nObservability is a borrowed term from mechanical engineering/control theory. It means, paraphrasing: ‚Äúcan you understand what is happening inside the system ‚Äî can you understand ANY internal state the system may get itself into, simply by asking questions from the outside?‚Äù We can apply this concept to software in interesting ways, and we may end up using some data types, but that‚Äôs putting the cart before the horse.\n\nIt‚Äôs a bit like saying that ‚Äúdatabase replication means structs, longints and elegantly diagrammed English sentences.‚Äù Er, no.. yes.. missing the point much?\n\nThis is such a reliable bait and switch that any time you hear someone talking about ‚Äúmetrics, logs and traces‚Äù, you can be pretty damn sure there‚Äôs no actual observability going on. If there were, they‚Äôd be talking about that instead ‚Äî it‚Äôs far more interesting! If there isn‚Äôt, they fall back to talking about whatever legacy products they do have, and that typically means, you guessed it: metrics, logs and traces.\n\n‚ùå Metrics\n\nMetrics in particular are actually quite hostile to observability. They are usually pre-aggregated, which means you are stuck with whatever questions you defined in advance, and even when they aren‚Äôt pre-aggregated they permanently discard the connective tissue of the request at write time, which destroys your ability to correlate issues across requests or track down any individual requests or drill down into a set of results ‚Äî FOREVER.\n\nWhich doesn‚Äôt mean metrics aren‚Äôt useful! They are useful for many things! But they are useful for things like static dashboards, trend analysis over time, or monitoring that a dimension stays within defined thresholds. Not observability. (Liz would interrupt here and say that Google‚Äôs observability story involves metrics, and that is true ‚Äî metrics with exemplars. But this type of solution is not available outside Google as far as we know..)\n\n‚ùå Logs\n\nDitto logs. When I say ‚Äúlogs‚Äù, you think ‚Äúunstructured strings, written out to disk haphazardly during execution, ‚Äúmany‚Äù log lines per request, probably contains 1-5 dimensions of useful data per log line, probably has a schema and some defined indexes for searching.‚Äù Logs are at their best when you know exactly what to look for, then you can go and find it.\n\nAgain, these connotations and assumptions are the opposite of observability‚Äôs requirements, which deals with highly structured data only. It is usually generated by instrumentation deep within the app, generally not buffered to local disk, issues a single event per request per service, is schemaless and indexless (or inferred schemas and autoindexed), and typically containing hundreds of dimensions per event.\n\n‚ùì Traces\n\nTraces? Now we‚Äôre getting closer. Tracing IS a big part of observability, but tracing just means visualizing events in order by time. It certainly isn‚Äôt and shouldn‚Äôt be a standalone product, that just creates unnecessary friction and distance. Hrmm ‚Ä¶ so what IS observability again, as applied to the software domain??\n\nAs a reminder, observability applied to software systems means having the ability to ask any question of your systems ‚Äî understand any user‚Äôs behavior or subjective experience ‚Äî without having to predict that question, behavior or experience in advance.\n\nObservability is about unknown-unknowns.\n\nAt its core, observability is about these unknown-unknowns.\n\nPlenty of tools are terrific at helping you ask the questions you could predict wanting to ask in advance. That‚Äôs the easy part. ‚ÄúWhat‚Äôs the error rate?‚Äù ‚ÄúWhat is the 99th percentile latency for each service?‚Äù ‚ÄúHow many READ queries are taking longer than 30 seconds?‚Äù\n\nMonitoring tools like DataDog do this ‚Äî you predefine some checks, then set thresholds that mean ERROR/WARN/OK.\n\nLogging tools like Splunk will slurp in any stream of log data, then let you index on questions you want to ask efficiently.\n\nAPM tools auto-instrument your code and generate lots of useful graphs and lists like ‚Äú10 slowest endpoints‚Äù.\n\nBut if you *can‚Äôt* predict all the questions you‚Äôll need to ask in advance, or if you *don‚Äôt* know what you‚Äôre looking for, then you‚Äôre in o11y territory.\n\nThis can happen for infrastructure reasons ‚Äî microservices, containerization, polyglot storage strategies can result in a combinatorial explosion of components all talking to each other, such that you can‚Äôt usefully pre-generate graphs for every combination that can possibly degrade.\n\nAnd it can happen ‚Äî has already happened ‚Äî to most of us for product reasons, as you‚Äôll know if you‚Äôve ever tried to figure out why a spike of errors was being caused by users on ios11 using a particular language pack but only in three countries, and only when the request hit the image export microservice running build_id 789782 if the user‚Äôs last name starts with ‚ÄúMC‚Äù and they then try to click on a particular button which then issues a db request using the wrong cache key for that shard.\n\nGathering the right data, then exploring the data.\n\nObservability starts with gathering the data at the right level of abstraction, organized around the request path, such that you can slice and dice and group and look for patterns and cross-correlations in the requests.\n\nTo do this, we need to stop firing off metrics and log lines willynilly and be more disciplined. We need to issue one single arbitrarily-wide event per service per request, and it must contain the *full context* of that request. EVERYTHING you know about it, anything you did in it, all the parameters passed into it, etc. Anything that might someday help you find and identify that request.\n\nThen, when the request is poised to exit or error the service, you ship that blob off to your o11y store in one very wide structured event per request per service.\n\nIn order to deliver observability, your tool also needs to support high cardinality and high dimensionality. Briefly, cardinality refers to the number of unique items in a set, and dimensionality means how many adjectives can describe your event. If you want to read more, here is an overview of the space, and more technical requirements for observability\n\nYou REQUIRE the ability to chain and filter as many dimensions as you want with infinitely high cardinality for each one if you‚Äôre going to be able to ask arbitrary questions about your unknown unknowns. This functionality is table stakes. It is non negotiable. And you cannot get it from any metrics or logs tool on the market today.\n\nWhy this matters.\n\nAlright, this is getting pretty long. Let me tell you why I care so much, and why I want people like you specifically (referring to frontend engineers and folks earlier in their careers) to grok what‚Äôs at stake in the observability term wars.\n\nWe are way behind where we ought to be as an industry. We are shipping code we don‚Äôt understand, to systems we have never understood. Some poor sap is on call for this mess, and it‚Äôs killing them, which makes the software engineers averse to owning their own code in prod. What a nightmare.\n\nMeanwhile developers readily admit they waste >40% of their day doing bullshit that doesn‚Äôt move the business forward. In large part this is because they are flying blind, just stabbing around in the dark.\n\nWe all just accept this. We shrug and say well that‚Äôs just what it‚Äôs like, working on software is just a shit salad with a side of frustration, it‚Äôs just the way it is.\n\nBut it is fucking not. It is un fucking necessary. If you instrument your code, watch it deploy, then ask ‚Äúis it doing what I expect, does anything else look weird‚Äù as a habit? You can build a system that is both understandable and well-understood. If you can see what you‚Äôre doing, and catch errors swiftly, it never has to become a shitty hairball in the first place. That is a choice.\n\nüåü But observability in the original technical sense is a necessary prerequisite to this better world. üåü\n\nIf you can‚Äôt break down by high cardinality dimensions like build ids, unique ids, requests, and function names and variables, if you cannot explore and swiftly skim through new questions on the fly, then you cannot inspect the intersection of (your code + production + users) with the specificity required to associate specific changes with specific behaviors. You can‚Äôt look where you are going.\n\nObservability as I define it is like taking off the blindfold and turning on the light before you take a swing at the pinata. It is necessary, although not sufficient alone, to dramatically improve the way you build software. Observability as they define it gets you to ‚Ä¶ exactly where you already are. Which of these is a good use of a new technical term?\n\nDo better.\n\nAnd honestly, it‚Äôs the next generation who are best poised to learn the new ways and take advantage of them. Observability is far, far easier than the old ways and workarounds ‚Ä¶ but only if you don‚Äôt have decades of scar tissue and old habits to unlearn.\n\nThe less time you‚Äôve spent using monitoring tools and ops workarounds, the easier it will be to embrace a new and better way of building and shipping well-crafted code.\n\nObservability matters. You should care about it. And vendors need to stop trying to confuse people into buying the same old bullshit tools by smooshing them together and slapping on a new label. Exactly how long do they expect to fool people for, anyway?\n\nWelcome to the second installment of my advice column! Last time we talked about the emotional impact of going back to engineering after a stint in management. If you have a question you‚Äôd like to ask, please email me or DM it to me on twitter.\n\nHi Charity! I hope it‚Äôs ok to just ask you this‚Ä¶\n\nI‚Äôm trying to get our company more aware of observability and I‚Äôm finding it difficult to convince people to look more into it. We currently don‚Äôt have the kind of systems that would require it much ‚Äì but we will in future and I want us to be ahead of the game.\n\nIf you have any tips about how to explain this to developers (who are aware that quality is important but don‚Äôt always advocate for it / do it as much as I‚Äôd prefer), or have concrete examples of ‚Äúhere‚Äôs a situation that we needed observability to solve ‚Äì and here‚Äôs how we solved it‚Äù, I‚Äôd be super grateful.\n\nIf this is too much to ask, let me know too üôÇ\n\nI‚Äôve been talking to Abby Bangser a lot recently ‚Äì and I‚Äôm ‚Äúclassifying‚Äù observability as ‚Äúexploring in production‚Äù in my mental map ‚Äì if you have philosophical thoughts on that, I‚Äôd also love to hear them üôÇ\n\nalex_schl\n\nDear Alex,\n\nYay, what a GREAT note! I feel like I get asked some subset or variation of these questions several times a week, and I am delighted for the opportunity to both write up a response for you and post it for others to read. I bet there are orders of magnitude more people out there with the same questions who *don‚Äôt* ask, so I really appreciate those who do. <3\n\nI want to talk about the nuts and bolts of pitching to engineering teams and shepherding technical decisions like this, and I promise I will offer you some links to examples and other materials. But first I want to examine some of the assumptions in your note, because they elegantly illuminate a couple of common myths and misconceptions.\n\nMyth #1: you don‚Äôt need observability til you have problems of scale\n\nFirst of all, there‚Äôs this misconception that observability is something you only need when you have really super duper hard problems, or that it‚Äôs only justified when you have microservices and large distributed systems or crazy scaling problems. No, no no nononono.\n\nThere may come a point where you are ABSOLUTELY FUCKED if you don‚Äôt have observability, but it is ALWAYS better to develop with it. It is never not better to be able to see what the fuck you are doing! The image in my head is of a hiker with one of those little headlamps on that lets them see where they‚Äôre putting their feet down. Most teams are out there shipping opaque, poorly understood code blindly ‚Äî shipping it out to systems which are themselves crap snowballs of opaque, poorly understood code. This is costly, dangerous, and extremely wasteful of engineering time.\n\nEver seen an engineering team of 200, and struggled to understand how the product could possibly need more than one or two teams of engineers? They‚Äôre all fighting with the crap snowball.\n\nDeveloping software with observability is better at ANY scale. It‚Äôs better for monoliths, it‚Äôs better for tiny one-person teams, it‚Äôs better for pre-production services, it‚Äôs better for literally everyone always. The sooner and earlier you adopt it, the more compounding value you will reap over time, and the more of your engineers‚Äô time will be devoted to forward progress and creating value.\n\nMyth #2: observability is harder and more technically advanced than monitoring\n\nActually, it‚Äôs the opposite ‚Äî it‚Äôs much easier. If you sat a new grad down and asked them to instrument their code and debug a small problem, it would be fairly straightforward with observability. Observability speaks the native language of variables, functions and API endpoints, the mental model maps cleanly to the request path, and you can straightforwardly ask any question you can come up with. (A key tenet of observability is that it gives an engineer the ability to ask any question, without having had to anticipate it in advance.)\n\nWith metrics and logging libraries, on the other hand, it‚Äôs far more complicated.you have to make a bunch of awkward decisions about where to emit various types of statistics, and it is terrifyingly easy to make poor choices (with terminal performance implications for your code and/or the remote data source). When asking questions, you are locked in to asking only the questions that you chose to ask a long time ago. You spend a lot of time translating the relationships between code and lowlevel systems resources, and since you can‚Äôt break down by users/apps you are blocked from asking the most straightforward and useful questions entirely!\n\nDoing it the old way Is. Fucking. Hard. Doing it the newer way is actually much easier, save for the fact that it is, well, newer ‚Äî and thus harder to google examples for copy-pasta. But if you‚Äôre saturated in decades of old school ops tooling, you may have some unlearning to do before observability seems obvious to you.\n\nMyth #3: observability is a purely technical solution\n\nTo be clear, you can just add an observability tool to your stack and go on about your business ‚Äî same old things, same old way, but now with high cardinality!\n\nYou can, but you shouldn‚Äôt.\n\nThese are sociotechnical systems and they are best improved with sociotechnical solutions. Tools are an absolutely necessary and inextricable part of it. But so are on call rotations and the fundamental virtuous feedback loop of you build it, you run it. So are code reviews, monitoring checks, alerts, escalations, and a blameless culture. So are managers who allocate enough time away from the product roadmap to truly fix deep technical rifts and explosions, even when it‚Äôs inconvenient, so the engineers aren‚Äôt in constant monkeypatch mode.\n\nI believe that observability is a prerequisite for any major effort to have saner systems, simply because it‚Äôs so powerful being able to see the impact of what you‚Äôve done. In the hands of a creative, dedicated team, simply wearing a headlamp can be transformational.\n\nObservability is your five senses for production.\n\nYou‚Äôre right on the money when you ask if it‚Äôs about exploring production, but you could also use words that are even more basic, like ‚Äúunderstanding‚Äù or ‚Äúinspecting‚Äù. Observability is to software systems as a debugger is to software code. It shines a light on the black box. It allows you to move much faster, with more confidence, and catch bugs much sooner in the lifecycle ‚Äî before users have even noticed. It rewards you for writing code that is easy to illuminate and understand in production.\n\nSo why isn‚Äôt everyone already doing it? Well, making the leap isn‚Äôt frictionless. There‚Äôs a minimal amount of instrumentation to learn (easier than people expect, but it‚Äôs nonzero) and then you need to learn to see your code through the lens of your own instrumentation. You might need to refactor your use of older tools, such as metrics libraries, monitoring checks and log lines. You‚Äôll need to learn another query interface and how it behaves on your systems. You might find yourself amending your code review and deploy processes a bit.\n\nNothing too terrible, but it‚Äôs all new. We hate changing our tool kits until absolutely fucking necessary. Back at Parse/Facebook, I actually clung to my sed/awk/shell wizardry until I was professionally shamed into learning new ways when others began debugging shit faster than I could. (I was used to being the debugger of last resort, so this really pissed me off.) So I super get it! So let‚Äôs talk about how to get your team aligned and hungry for change.\n\nOkay okay okay already, how do I get my team on board?\n\nIf we were on the phone right now, I would be peppering you with a bunch of questions about your organization. Who owns production? Who is on call? Who runs the software that devs write? What is your deploy process, and how often does it get updated, and by who? Does it have an owner? What are the personalities of your senior folks, who made the decisions to invest in the current tools (and what are they), what motivates them, who are your most persuasive internal voices? Etc. Every team is different. <3\n\nThere‚Äôs a virtuous feedback loop you need to hook up and kickstart and tweak here, where the people with the original intent in their heads (software engineers) are also informed and motivated, i.e. empowered to make the changes and personally impacted when things are broken. I recommend starting by putting your software engineers on call for production (if you haven‚Äôt). This has a way of convincing even the toughest cases that they have a strong personal interest in quality and understandability.\n\nPay attention to your feedback loop and the alignment of incentives, and make sure your teams are given enough time to actually fix the broken things, and motivation usually isn‚Äôt a problem. (If it is, then perhaps another feedback loop is lacking: your engineers feeling sufficiently aligned with your users and their pain. But that‚Äôs another post.)\n\nTechnical ownership over technical outcomes\n\nI appreciate that you want your team to own the technical decisions. I believe very strongly that this is the right way to go. But it doesn‚Äôt mean you can‚Äôt have influence or impact, and particularly in times like this.\n\nIt is literally your job to have your head up, scanning the horizon for opportunities and relevant threats. It‚Äôs their job to be heads down, focusing on creating and delivering excellent work. So it is absolutely appropriate for you to flag something like observability as both an opportunity and a potential threat, if ignored.\n\nIf I were in your situation and wanted my team to check out some technical concept, I might send around a great talk or two and ask folks to watch it, and then maybe schedule a lunchtime discussion. Or I might invite a tech luminary in to talk with the team, give a presentation and answer their questions. Or schedule a hack week to apply the concept to a current top problem, or something else of that nature.\n\nBut if I really wanted them to take it fucking seriously, I would put my thumb on the scale. I would find myself a champion, load them up with context, and give them ample time and space to skill up, prototype, and eventually present to the team a set of recommendations. (And I would stay in close contact with them throughout that period, to make sure they didn‚Äôt veer too far off course or lose sight of my goals.)\n\nGet a champion.\n\nIdeally you want to turn the person who is most invested in the old way of doing things ‚Äî the person who owns the ELK cluster, say, or who was responsible for selecting the previous monitoring toolkit, or the goto person for ops questions ‚Äî from your greatest obstacle into your proxy warrior. This only works if you know that person is open-minded and secure enough to give it a fair shot & publicly change course, has sufficiently good technical judgment to evaluate and project into the future, and has the necessary clout with their peers. If they don‚Äôt, or if they‚Äôre too afraid to buck consensus: pick someone else.\n\nGive them context.\n\nTake them for a long walk. Pour your heart and soul out to them. Tell them what you‚Äôve learned, what you‚Äôve heard, what you hope it can do for you, what you fear will happen if you don‚Äôt. It‚Äôs okay to get personal and to admit your uncertainties. The more context they have, the better the chance they will come out with an outcome you are happy with. Get them worried about the same things that worry you, get them excited about the same possibilities that excite you. Give them a sense of the stakes.\n\nAnd don‚Äôt forget to tell them why you are picking them ‚Äî because they are listened to by their peers, because they are already expert in the problem area, because you trust their technical judgment and their ability to evaluate new things ‚Äî all the reasons for picking them will translate well into the best kind of flattery ‚Äî the true kind.\n\nGive them a deadline.\n\nA week or two should be plenty. Most likely, the decision is not going to be unilaterally theirs (this also gives you a bit of wiggle room should they come back going ‚Äúah no ELK is great forever and ever‚Äù), but their recommendations should carry serious weight with the team and technical leadership. Make it clear what sort of outcome you would be very pleased with (e.g. a trial period for a new service) and what reasons you would find compelling for declining to pursue the project (i.e. your tech is unsupported, cost prohibitive, etc). Ideally they should use this time to get real production data into the services they are testing out, so they can actually experience and weigh the benefits, not just read the marketing copy.\n\nAs a rule of thumb, I always assume that managers can‚Äôt convince engineers to do things: only other engineers can. But what you can do instead is set up an engineer to be your champion. And then just sit quietly in the corner, nodding, with an interested look on your face.\n\nThe nuclear option\n\nYou have one final option. If there is no appropriate champion to be found, or insufficient time, or if you have sufficient trust with the team that you judge it the right thing to do: you can simply order them to do something your way. This can feel squicky. It‚Äôs not a good habit to get into. It usually results in things being done a bit slower, more reluctantly, more half-assedly. And you sacrifice some of your power every time you lean on your authority to get your team to do something.\n\nBut it‚Äôs just as bad for a leader to take it off the table entirely.\n\nSometimes you will see things they can‚Äôt. If you cannot wield your power when circumstances call for it, then you don‚Äôt fucking have real power ‚Äî you have unilaterally disarmed yourself, to the detriment of your org. You can get away with this maybe twice a year, tops.\n\nBut here‚Äôs the thing: if you order something to be done, and it turns out in the end that you were right? You earn back all the power you expended on it plus interest. If you were right, unquestionably right in the eyes of the team, they will respect you more for having laid down the law and made sure they did the right thing.\n\nxo\n\ncharity\n\nSome useful resources:\n\nhttps://thenewstack.io/observability-a-3-year-retrospective/ a super meaty technical retrospective\n\nhttps://www.honeycomb.io/blog/so-you-want-to-build-an-observability-tool/ the technical underpinnings of observability, and why they are different from monitoring (and why they matter)\n\nhttps://www.heavybit.com/library/podcasts/o11ycast/ep-1-monitoring-vs-observability/ (a podcast, lots of folks have cited this as a great intro)\n\nhttps://www.honeycomb.io/blog/incident-report-running-dry-on-memory-without-noticing/ one of our recent incident reports, with screenshots\n\nhttps://charity.wtf/2019/10/28/deploys-its-not-actually-about-fridays/ includes some observations on what it‚Äôs liek to be working on a stack where we actually understand what the fuck we are shipping, we aren‚Äôt just accumulating blacker and blacker boxes with every depoy.\n\nI just read this piece, which is basically a very long subtweet about my Friday deploy threads. Go on and read it: I‚Äôll wait.\n\nHere‚Äôs the thing. After getting over some of the personal gibes (smug optimism? literally no one has ever accused me of being an optimist, kind sir), you may be expecting me to issue a vigorous rebuttal. But I shan‚Äôt. Because we are actually in violent agreement, almost entirely.\n\nI have repeatedly stressed the following points:\n\nI want to make engineers‚Äô lives better, by giving them more uninterrupted weekends and nights of sleep. This is the goal that underpins everything I do.\n\nAnyone who ships code should develop and exercise good engineering judgment about when to deploy, every day of the week\n\nEvery team has to make their own determination about which policies and norms are right given their circumstances and risk tolerance\n\nA policy of ‚Äúno Friday deploys‚Äù may be reasonable for now but should be seen as a smell, a sign that your deploys are risky. It is also likely to make things WORSE for you, not better, by causing you to adopt other risky practices (e.g. elongating the interval between merge and deploy, batching changes up in a single deploy)\n\nThis has been the most frustrating thing about this conversation: that a) I am not in fact the absolutist y‚Äôall are arguing against, and b) MY number one priority is engineers and their work/life balance. Which makes this particularly aggravating:\n\nLastly there is some strange argument that choosing not to deploy on Friday ‚ÄúShouldn‚Äôt be a source of glee and pride‚Äù. That one I haven‚Äôt figured out yet, because I have always had a lot of glee and pride in being extremely (overly?) protective of the work/life balance of the engineers who either work for me, or with me. I don‚Äôt expect that to change.\n\nHold up. Did you catch that clever little logic switcheroo? You defined ‚Äúnot deploying on Friday‚Äù as being a priori synonymous with ‚Äúprotecting the work/life balance of engineers‚Äù. This is how I know you haven‚Äôt actually grasped my point, and are arguing against a straw man. My entire point is that the behaviors and practices associated with blocking Friday deploys are in fact hurting your engineers.\n\nI, too, take a lot of glee and pride in being extremely, massively, yes even OVERLY protective of the work/life balance of the engineers who either work for me, or with me.\n\nAND THAT IS WHY WE DEPLOY ON FRIDAYS.\n\nBecause it is BETTER for them. Because it is part of a deploy ecosystem which results in them being woken up less and having fewer weekends interrupted overall than if I had blocked deploys on Fridays.\n\nIt‚Äôs not about Fridays. It‚Äôs about having a healthy ecosystem and feedback loop where you trust your deploys, where deploys aren‚Äôt a big deal, and they never cause engineers to have to work outside working hours. And part of how you get there is by not artificially blocking off a big bunch of the week and not deploying during that time, because that breaks up your virtuous feedback loop and causes your deploys to be much more likely to fail in terrible ways.\n\nThe other thing that annoys me is when people say, primly, ‚Äúyou can‚Äôt guarantee any deploy is safe, but you can guarantee people have plans for the weekend.‚Äù\n\nKnow what else you can guarantee? That people would like to sleep through the fucking night, even on weeknights.\n\nWhen I hear people say this all I hear is that they don‚Äôt care enough to invest the time to actually fix their shit so it won‚Äôt wake people up or interrupt their off time, seven days a week. Enough with the virtue signaling already.\n\nYou cannot have it both ways, where you block off a bunch of undeployable time AND you have robust, resilient, swift deploys. Somehow I keep not getting this core point across to a substantial number of very intelligent people. So let me try a different way.\n\nLet‚Äôs try telling a story.\n\nA tale of two startups\n\nHere are two case studies.\n\nCompany X\n\nCompany X is a three-year-old startup. It is a large, fast-growing multi-tenant platform on a large distributed system with spiky traffic, lots of user-submitted data, and a very green database. Company X deploys the API about once per day, and does a global deploy of all services every Tuesday. Deploys often involve some firefighting and a rollback or two, and Tuesdays often involve deploying and reverting all day (sigh).\n\nPager volume at Company X isn‚Äôt the worst, but usually involves getting woken up a couple times a week, and there are deploy-related alerts after maybe a third of deploys, which then need to be triaged to figure out whose diff was the cause.\n\nCompany Z\n\nCompany Z is a three-year-old startup. It is a large, fast-growing multi-tenant platform on a large distributed system with spiky traffic, lots of user-submitted data, and a very green house-built distributed storage engine. Company Z automatically triggers a deploy within 30 minutes of a merge to master, for all services impacted by that merge. Developers at company Z practice observability-driven deployment, where they instrument all changes, ask ‚Äúhow will I know if this change doesn‚Äôt work?‚Äù during code review, and have a muscle memory habit of checking to see if their changes are working as intended or not after they merge to master.\n\nDeploys rarely result in the pager going off at Company Z; most problems are caught visually by the engineer and reverted or fixed before any paging alert can fire. Pager volume consists of roughly one alert per week outside of working hours, and no one is woken up more than a couple times per year.\n\nSame damn problem, better damn solutions.\n\nIf it wasn‚Äôt extremely obvious, these companies are my last two jobs, Parse (company X, from 2012-2016) and Honeycomb (company Z, from 2016-present).\n\nThey have a LOT in common. Both are services for developers, both are platforms, both are running highly elastic microservices written in golang, both get lots of spiky traffic and store lots of user-defined data in a young, homebrewed columnar storage engine. They were even built by some of the same people (I built infra for both, and they share four more of the same developers).\n\nAt Parse, deploys were run by ops engineers because of how common it was for there to be some firefighting involved. We discouraged people from deploying on Fridays, we locked deploys around holidays and big launches. At Honeycomb, none of these things are true. In fact, we literally can‚Äôt remember a time when it was hard to debug a deploy-related change.\n\nWhat‚Äôs the difference between Company X and Company Z?\n\nSo: what‚Äôs the difference? Why are the two companies so dramatically different in the riskiness of their deploys, and the amount of human toil it takes to keep them up?\n\nI‚Äôve thought about this a lot. It comes down to three main things.\n\nObservability\n\nObservability-driven development\n\nSingle merge per deploy\n\n1. Observability.\n\nI think that I‚Äôve been reluctant to hammer this home as much as I ought to, because I‚Äôm exquisitely sensitive about sounding like an obnoxious vendor trying to sell you things. üòõ (Which has absolutely been detrimental to my argument.)\n\nWhen I say observability, I mean in the precise technical definition as I laid out in this piece: with high cardinality, arbitrarily wide structured events, etc. Metrics and other generic telemetry will not give you the ability to do the necessary things, e.g. break down by build id in combination with all your other dimensions to see the world through the lens of your instrumentation. Here, for example, are all the deploys for a particular service last Friday:\n\nEach shaded area is the duration of an individual deploy: you can see the counters for each build id, as the new versions replace the old ones,\n\n2. Observability-driven development.\n\nThis is cultural as well as technical. By this I mean instrumenting a couple steps ahead of yourself as you are developing and shipping code. I mean making a cultural practice of asking each other ‚Äúhow will you know if this is broken?‚Äù during code review. I mean always going and looking at your service through the lens of your instrumentation after every diff you ship. Like muscle memory.\n\n3. Single merge per deploy.\n\nThe number one thing you can do to make your deploys intelligible, other than observability and instrumentation, is this: deploy one changeset at a time, as swiftly as possible after it is merged to master. NEVER glom multiple changesets into a single deploy ‚Äî that‚Äôs how you get into a state where you aren‚Äôt sure which change is at fault, or who to escalate to, or if it‚Äôs an intersection of multiple changes, or if you should just start bisecting blindly to try and isolate the source of the problem. THIS is what turns deploys into long, painful marathons.\n\nAnd NEVER wait hours or days to deploy after the change is merged. As a developer, you know full well how this goes. After you merge to master one of two things will happen. Either:\n\nyou promptly pull up a window to watch your changes roll out, checking on your instrumentation to see if it‚Äôs doing what you intended it to or if anything looks weird, OR\n\nyou close the project and open a new one.\n\nWhen you switch to a new project, your brain starts rapidly evicting all the rich context about what you had intended to do and and overwriting it with all the new details about the new project.\n\nWhereas if you shipped that changeset right after merging, then you can WATCH it roll out. And 80-90% of all problems can be, should be caught right here, before your users ever notice ‚Äî before alerts can fire off and page you. If you have the ability to break down by build id, zoom in on any errors that happen to arise, see exactly which dimensions all the errors have in common and how they differ from the healthy requests, see exactly what the context is for any erroring requests.\n\nHealthy feedback loops == healthy systems.\n\nThat tight, short feedback loop of build/ship/observe is the beating heart of a healthy, observable distributed system that can be run and maintained by human beings, without it sucking your life force or ruining your sleep schedule or will to live.\n\nMost engineers have never worked on a system like this. Most engineers have no idea what a yawning chasm exists between a healthy, tractable system and where they are now. Most engineers have no idea what a difference observability can make. Most engineers are far more familiar with spending 40-50% of their week fumbling around in the dark, trying to figure out where in the system is the problem they are trying to fix, and what kind of context do they need to reproduce.\n\nMost engineers are dealing with systems where they blindly shipped bugs with no observability, and reports about those bugs started to trickle in over the next hours, days, weeks, months, or years. Most engineers are dealing with systems that are obfuscated and obscure, systems which are tangled heaps of bugs and poorly understood behavior for years compounding upon years on end.\n\nThat‚Äôs why it doesn‚Äôt seem like such a big deal to you break up that tight, short feedback loop. That‚Äôs why it doesn‚Äôt fill you with horror to think of merging on Friday morning and deploying on Monday. That‚Äôs why it doesn‚Äôt appall you to clump together all the changes that happen to get merged between Friday and Monday and push them out in a single deploy.\n\nIt just doesn‚Äôt seem that much worse than what you normally deal with. You think this raging trash fire is, unfortunately ‚Ä¶ normal.\n\nHow realistic is this, though, really?\n\nMaybe you‚Äôre rolling your eyes at me now. ‚ÄúSure, Charity, that‚Äôs nice for you, on your brand new shiny system. Ours has years of technical debt, It‚Äôs unrealistic to hold us to the same standard.‚Äù\n\nYeah, I know. It is much harder to dig yourself out of a hole than it is to not create a hole in the first place. No doubt about that.\n\nHarder, yes. But not impossible.\n\nI have done it.\n\nParse in 2013 was a trash fire. It woke us up every night, we spent a lot of time stabbing around in the dark after every deploy. But after we got acquired by Facebook, after we started shipping some data sets into Scuba, after (in retrospect, I can say) we had event-level observability for our systems, we were able to start paying down that debt and fixing our deploy systems.\n\nWe started hooking up that virtuous feedback loop, step by step.\n\nWe reworked our CI/CD system so that it built a new artifact after every single merge.\n\nWe put developers at the steering wheel so they could push their own changes out.\n\nWe got better at instrumentation, and we made a habit of going to look at it during or after each deploy.\n\nWe hooked up the pager so it would alert the person who merged the last diff, if an alert was generated within an hour aft"
    }
}
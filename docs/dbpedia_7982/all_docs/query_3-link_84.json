{
    "id": "dbpedia_7982_3",
    "rank": 84,
    "data": {
        "url": "https://www.mdpi.com/2409-9287/8/1/3",
        "read_more_link": "",
        "language": "en",
        "title": "The Poetics of Physics",
        "top_image": "https://pub.mdpi-res.com/philosophies/philosophies-08-00003/article_deploy/html/images/philosophies-08-00003-g001-550.jpg?1677212258",
        "meta_img": "https://pub.mdpi-res.com/philosophies/philosophies-08-00003/article_deploy/html/images/philosophies-08-00003-g001-550.jpg?1677212258",
        "images": [
            "https://pub.mdpi-res.com/img/design/mdpi-pub-logo-black-small1.svg?da3a8dcae975a41c?1723640743",
            "https://pub.mdpi-res.com/img/design/mdpi-pub-logo-black-small1.svg?da3a8dcae975a41c?1723640743",
            "https://pub.mdpi-res.com/img/journals/philosophies-logo.png?477c32fde9c3a623",
            "https://www.mdpi.com/bundles/mdpisciprofileslink/img/unknown-user.png",
            "https://www.mdpi.com/bundles/mdpisciprofileslink/img/unknown-user.png",
            "https://pub.mdpi-res.com/img/design/orcid.png?0465bc3812adeb52?1723640743",
            "https://www.mdpi.com/profiles/955578/thumb/Michael_Parker.jpg",
            "https://pub.mdpi-res.com/img/design/orcid.png?0465bc3812adeb52?1723640743",
            "https://www.mdpi.com/bundles/mdpisciprofileslink/img/unknown-user.png",
            "https://www.mdpi.com/philosophies/philosophies-08-00003/article_deploy/html/images/philosophies-08-00003-i001.png",
            "https://www.mdpi.com/philosophies/philosophies-08-00003/article_deploy/html/images/philosophies-08-00003-i002.png",
            "https://www.mdpi.com/philosophies/philosophies-08-00003/article_deploy/html/images/philosophies-08-00003-i003.png",
            "https://www.mdpi.com/philosophies/philosophies-08-00003/article_deploy/html/images/philosophies-08-00003-g001-550.jpg",
            "https://www.mdpi.com/philosophies/philosophies-08-00003/article_deploy/html/images/philosophies-08-00003-g001.png",
            "https://www.mdpi.com/philosophies/philosophies-08-00003/article_deploy/html/images/philosophies-08-00003-g002-550.jpg",
            "https://www.mdpi.com/philosophies/philosophies-08-00003/article_deploy/html/images/philosophies-08-00003-g002.png",
            "https://www.mdpi.com/philosophies/philosophies-08-00003/article_deploy/html/images/philosophies-08-00003-g003-550.jpg",
            "https://www.mdpi.com/philosophies/philosophies-08-00003/article_deploy/html/images/philosophies-08-00003-g003.png",
            "https://pub.mdpi-res.com/img/design/mdpi-pub-logo-white-small.png?71d18e5f805839ab?1723640743"
        ],
        "movies": [],
        "keywords": [],
        "meta_keywords": [
            ""
        ],
        "tags": null,
        "authors": [
            "Chris Jeynes",
            "Michael C. Parker",
            "Margaret Barker",
            "Michael C"
        ],
        "publish_date": "2023-01-05T00:00:00",
        "summary": "",
        "meta_description": "Physics has been thought to truly represent reality since at least Galileo, and the foundations of physics are always established using philosophical ideas. In particular, the elegant naming of physical entities is usually very influential in the acceptance of physical theories. We here demonstrate (using current developments in thermodynamics as an example) that both the epistemology and the ontology of physics ultimately rest on poetic language. What we understand depends essentially on the language we use. We wish to establish our knowledge securely, but strictly speaking this is impossible using only analytic language. Knowledge of the meanings of things must use a natural language designed to express meaning, that is, poetic language. Although the world is really there, and although we can indeed know it truly, this knowledge is never either complete or certain but ultimately must rest on intuition. Reading a recently discovered artefact with a palaeo-Hebrew inscription as from the first century, we demonstrate from it that this ontological understanding long predates the Hellenic period. Poetic language is primary, both logically and temporally.",
        "meta_lang": "en",
        "meta_favicon": "https://pub.mdpi-res.com/img/mask-icon-128.svg?c1c7eca266cd7013?1723640743",
        "meta_site_name": "MDPI",
        "canonical_link": "https://www.mdpi.com/2409-9287/8/1/3",
        "text": "1\n\nIon Beam Centre, University of Surrey, Guildford GU2 7XH, UK\n\n2\n\nSchool of Computer Science & Electrical Engineering, University of Essex, Colchester CO4 3SQ, UK\n\n3\n\nIndependent Researcher, Borrowash DE72 3HT, UK\n\n*\n\nAuthor to whom correspondence should be addressed.\n\nPhilosophies 2023, 8(1), 3; https://doi.org/10.3390/philosophies8010003\n\nSubmission received: 6 November 2022 / Revised: 6 December 2022 / Accepted: 12 December 2022 / Published: 5 January 2023\n\nAbstract\n\n:\n\nPhysics has been thought to truly represent reality since at least Galileo, and the foundations of physics are always established using philosophical ideas. In particular, the elegant naming of physical entities is usually very influential in the acceptance of physical theories. We here demonstrate (using current developments in thermodynamics as an example) that both the epistemology and the ontology of physics ultimately rest on poetic language. What we understand depends essentially on the language we use. We wish to establish our knowledge securely, but strictly speaking this is impossible using only analytic language. Knowledge of the meanings of things must use a natural language designed to express meaning, that is, poetic language. Although the world is really there, and although we can indeed know it truly, this knowledge is never either complete or certain but ultimately must rest on intuition. Reading a recently discovered artefact with a palaeo-Hebrew inscription as from the first century, we demonstrate from it that this ontological understanding long predates the Hellenic period. Poetic language is primary, both logically and temporally.\n\n1. Introduction\n\nThis essay has in mind Marshall McLuhan’s idea, “The Medium is the Message” [1]: our epigraph and epilogues are poems because the very purpose of the work is to establish the idea that we always mean more than we say; an idea that is true even in—perhaps especially in–doing physics. So the canonical textbooks of physics (for example, Landau & Lifshits [2]) are notoriously brief. Why are they brief? Because the students are supposed to grasp the material, and fill in the “trivial” (actually very challenging) gaps in the treatment for themselves. It is this activity of grasping that we will focus on here.\n\nWe will seek to prove that the terms which we use to understand any material, here especially including the material of mathematics and physics, are rooted in a poetic use of language in which ambiguity cannot be eliminated. The very terms used for our opening and closing poems, epigraph and epilogue, make use of the ambiguity of the Greek prefix ἐπί (epi) which can mean (among other things) either “upon” (that is in this case “before”) or “in addition” (that is in this case “after”).\n\nThe renowned physicist Carlo Rovelli has been dubbed “the poet of physics” by Richard Webb in his review of one of Rovelli’s popular science books, which Webb calls “enriching, illuminating, eclectic” [3]. This present paper is “eclectic” since it ranges over subjects not usually considered together (physics, poetry, palaeo-Hebrew); but then the issue is reality itself, and the boundaries we erect around our disciplines are merely for our own convenience 1. For example, the DDT molecule behaves the way it behaves completely independently of whether we are considering the applicable quantum mechanics, or its enthalpy of formation, or the chemical and process engineering needed for its industrial production, or the regulatory issues related to its safe use, or the political issues raised seminally by Rachel Carson who took her title “Silent Spring” [5] from a poem by Keats (1819) [6].\n\nEpigraph: “Ku wown biyuke”. A poem in the Palikur language (© 2016 Aldiere Orlando, reproduced by permission with translation by Diana Green © 2020) after Miguel Leon-Portilla’s famous poem: Cuando Muere Una Lengua (1998; in Náhuatl). See Supplementary Materials for the audio file of the poet speaking the poem in Palikur (and for its Portuguese translation), also for the Náhuatl original of Cuando Muere Una Lengua (and its translation into English from the Spanish).\n\nTom McLeish opens his book [7] with a “powerful list of words”, that is: “Creativity, Inspiration, Passion, Imagination, Composition, Representation” (the capitalisation is his), which he proceeds to argue apply to the sciences just as much as to the humanities. He quotes Karl Popper [8] saying, “A great work of music, like a great scientific theory, is a cosmos imposed on chaos—in its tensions and harmonies inexhaustible even for its creator”. McLeish asks, “Is a dualistic division into arts and science really faithful to our history, our capacities and needs?” arguing that we should not “reinforce the well-worn narrative” of the “Two Cultures” [9]. Oliver Sacks [10] echoes McLeish, saying that “science is far from being coldness and calculation … but is shot through with passion, longing and romance”. Above all, we should not get lost in our own specialisms! Graeber and Wengrow [11] are sarcastic about “specialists refusing to generalise”.\n\nWe believe that our account of science should acknowledge the seminal contribution of Inspiration to the process of gaining knowledge, and this essay is an attempt to do this. Of course, this is not a new idea although every generation seems to need to find it out anew: in previous generations, the great Henri Poincaré (for example) also considered intuition and inspiration as basic an ingredient in physics as it is in poetry (see Szpiro’s review of Gray’s biography [12,13]).\n\nWe wish to know what things are (that is, their ontology), and we also wish to know how we know what things are (that is, their epistemology). We seek here to explore the idea of knowledge itself, and to do this we will have to go beyond the usual Hellenocentric accounts that lead us to believe that philosophy started with Socrates (or at least, with the pre-Socratic Greeks) 2. Knowing is a characteristic human activity that in all the ancient societies we know of has been linked to seeing 3 (which is why the wise were called “seers”), and the “blind seer” is an ancient archetype 4. We will explore the roots of our ideas of knowledge since it is obviously a fundamental error to think that true things could have been known only in modern times.\n\nThe trouble with this is that there exist today widespread prejudices not only that for knowledge to be “true” it must be “scientific” (with poets operating somehow on a different plane), but also that any talk that may be called “religious” is necessarily irrelevant to science, even though what we now think of as “religious” ideas pervaded all ancient poetry. Michael Polanyi has investigated the far-reaching philosophical consequences of having a false conception of what science is for, and how it is done: “As long as science remains the ideal of knowledge, and detachment the ideal of science, ethics cannot be secured from complete destruction by skeptical doubt” (Polanyi & Prosch, 1975 [22]).\n\nHowever, no one has a monopoly on talking nonsense, and in any case we insist on the unity of truth 5. We are investigating how we come to know new things, and our discussion will range from the details of modern developments in thermodynamics to the ancient poets composing in an ancient Hebrew: the fact that this poetry is now pigeon-holed as “religion” here concerns us not at all. Instead, we wish to point to the characteristic humanity of both the poetry and the science: we will show that the knowledge of both the poet and the scientist is, ultimately, the same sort of thing (even if they use vastly different methods). After all, both the poet and the scientist want to explain reality, which may be seen in multiple different (but complementary) ways.\n\nIt is also often thought today that scientific concepts are not constructed in a “natural” language, being higher order abstractions, and therefore that other accounts (such as the present one) are simply irrelevant. It is of course certainly true that modern physics is normally discussed by physicists in eye-wateringly sophisticated mathematical terms (and the thermodynamics we will describe is no exception); nevertheless, we will demonstrate that at the foundations of every field of physics (with thermodynamics as our example) is a “natural” language explanation of how to grasp (or see) the phænomena of interest. Mathematics is required to expose logical consequences, but words are required to illuminate meaning.\n\nStructure of the Paper\n\nThe paper is constructed as an essay on ontics and epistemics (what things are and how we know them: more properly, since they are inseparable, “onto-epistemics”(Karen Barad’s idea [24]). We will show that knowing things (even in physics) is properly a poetic activity: knowledge is necessarily personal and cannot be obtained mechanically, it is the sort of thing that can only be obtained by a “poetic” (or transcendental) approach.\n\nWe start by considering meaning (§2). Helping us to touch the deep meaning of things is obviously the job of poets: what is not usually appreciated, and what we wish here to underline, is that students of physics always ask the same sorts of questions when confronted with a complex mathematical argument—what does it mean? And such questions are (and always have been) fundamental to doing physics. It is not widely enough known that physics cannot be done without imagination.\n\nExploring the thinginess of things (§3; that is, the rational structure both of reality itself and of our knowledge of it) is surprising. Poets have always delighted in the thinginess of things: William Carlos Williams’ line “no ideas but in things” [25] has become a cliché. But what is not sufficiently appreciated is the logical status of the knowledge of things. We explore Gödel incompleteness in its historical and philosophical context, and its disturbing consequences for the ideas of “subjectivity” and “objectivity” (and the “scientific method”) that we had thought were straightforward (even “obvious”). They are not!\n\nWe then, separately, summarise the surprising development of the ideas of entropy and information (§4) as a specific example of how meaning is negotiated (Martin Edwardes’ idea [26]) in physics. This is a complicated story, full of arcane detail, of which our very brief summary is barely adequate. But we hope it is full enough to appreciate the complexity of knowledge even in this very restricted field. Usually, a discipline is presented pedagogically as timelessly elegant, with no loose ends either technically or philosophically. But this is not the way things are! If we wish to understand how the scientific method actually works, with all its limitations, we need to go into the details of how knowledge is really obtained.\n\nWe underline (§5) this negotiating of meaning in the development of knowledge as being an exercise that necessarily involves poetics, even concluding with a discussion of some canonical poetry (§5.4). We discuss what definitions involve (§5.1), the necessary status of this whole discussion as metaphysical (§5.2), emphasising that the pejorative meaning usually ascribed to this term is not the only available meaning. Also included is an important discussion of ambiguity (§5.3).\n\nThe whole essay revolves around the recognition of language as the primary and essential medium of knowledge, and we give an example of this (§6) that uses a detailed analysis of an artefact that is demonstrably a mnemonic of a very sophisticated view of knowledge long predating the Hellenic schools of philosophy. It is a reasonable approximation to say that what we now call “philosophy” started with the Greeks, but of course people have always wanted to know things, and we demonstrate a very ancient and intricate account of how to think about things that is startlingly profound. No one ever talks seriously of modern physics and ancient artefacts with palaeo-Hebrew inscriptions in the same breath, so that this example of ours is very surprising. Nevertheless, it underlines our thesis that analytic language is necessary but not sufficient to know even physical things adequately. Poetic language is also required.\n\nWe gather the threads of the argument together (§7) and finally conclude (§8).\n\n2. Meaning as Poetry\n\nIn what way can a scientist be like Shakespeare? Tom McLeish recently quoted Shakespeare’s 100th Sonnet (“Where art thou Muse …”) saying, “it has never been easy to speak with clarity about moments of imaginative conception” ([7], p. 7), and we also quote Dante Alighieri speaking of his Muse (§5.4). McLeish eloquently discusses a variety of cases showing how scientists imagine reality before they are able to establish their new theories, and how these imaginative (creative) approaches are actually central because of “new patterns and connections that they offer for specific creative demands” ([7], p. 331). Seeing new things requires imagination!\n\nAlmost a century ago, Owen Barfield famously spoke of “poetic diction”, that is: “the language of poetic compositions” ([27], III:5):\n\nWhen we start explaining the language of famous scientists as examples of ‘poetic diction’ … [it is no] waste of time [if it helps anyone to be convinced] how essentially parochial is the fashionable distinction between Poetry and Science as modes of experience\n\nOwen Barfield, Poetic Diction VIII:6 (1928) [27]\n\nseeking to establish, like McLeish, that epistemologically there is little distinction between artists and scientists: they are all similar in how they come to know new things.\n\nIf I say, “information has calculable entropy and obeys physical laws”[28] 6, what do I mean? And how can you understand me? Barfield says that “the poet’s relation to terms is that of maker” [27] (VIII:4) 7; information and entropy here are terms referring to certain aspects of physical reality and it is clear that the terms are made by the physicists: are they (as both Barfield and McLeish outrageously seem to say) in some sense thereby poets? Indeed, Oliver Sacks ([33], p. 23) speaks of a time when “there still existed a union of literary and scientific cultures; there was not the dissociation of sensibility that was so soon to come.”\n\nWe do not think that physicists ought to be poets, nor even that at least some physicists should. We regard such a position as absurd. But we do think that ultimately, physicists cannot avoid using language “poetically”: that is, using a “natural” language 8 (together with its unavoidable ambiguity) to set up the model proposed for the phænomenon in view. As an example of this we will explore the specific case of how we address the scientific concepts of entropy and its close companion information, which together represent difficult ideas in a currently very active (and contentious) field of research. We point out that the very close relation between information and entropy is now well established by workers who articulate this relation in mathematical detail as a “new” concept of info-entropy within the overall theory that they call “Quantitative Geometrical Thermodynamics” (QGT) [28].\n\nUsing these test cases, and explicitly using one of the first papers on entropy [35], we seek to show how the development of scientific ideas necessarily depends in the first instance on an intuitive understanding that relies on intrinsically poetic language. We emphasise that “poetic language” is not restricted to poetry! Even in 1928 Owen Barfield recognised that “famous scientists” used “poetic diction”. The basic ideas of any theory have to be “negotiated” [26] 9 using some sort of “natural” language, and any subsequent mathematical representation is only a formal method of displaying the logical consequences of these ideas.\n\nThis assertion is disturbing since it is widely thought today that there is a sharp distinction between the “hard sciences” in which things are known certainly (or at least, pretty much certainly for practical purposes), and the humanities which (supposedly) prize feeling above thought. Supposedly, everyone agrees in science, but no one agrees in politics, philosophy and religion. But we point out that knowledge is fuzzy: the “hard sciences” are not as hard as we might like them to be. The old joke (repeated by Rabbi Weinreb) goes, “two Jews, three opinions” [37], but Weinreb himself points to the value of debate in “an atmosphere of civility and mutual respect and a willingness to concede one’s original position in order to achieve the truth”. Who would disagree with that? It turns out on closer inspection that the “scientific method” 10 is more poetic than we might have expected.\n\nSummarising the programme of this essay: before a scientific concept can be understood, it must be articulated, and language is essential to articulate scientific ideas: we cannot know anything without being able to say what it is we know (without language we have inchoate feeling, not knowledge). Science is effected by humans acting humanly—that is, using language! Stones do not know things: people do. Our knowledge of the world is necessarily based ultimately on intuition 11, and the articulation of intuited knowledge is ultimately the business of poets. Before it is anything else, natural language is poetic.\n\nSaying that knowledge is necessarily mediated by words sounds like the linguistic determinism famously proposed by Benjamin Lee Whorf (1941) [40]. We do not take this position, but rather that of the “relay results” advocated by McLeish [7] who relies on Jacques Hadamard’s The Psychology of Invention in the Mathematical Field [41]:\n\n… James Clerk Maxwell would urge mathematicians to formulate their thinking in ‘words without the aid of symbols’, not because he would sympathize with the lingualists, but because he knew the creative force of communicating ideas\n\nTom McLeish (2019) p. 243 [7]\n\nwhere of course Maxwell was one of the giants honoured by all subsequent physicists for his beautiful (and seminal) description of the electromagnetic field [42], on which all modern technology depends. McLeish understands the importance of using words to explain the meaning of the work, as do all good scientists: when we write papers, the Introductory section almost always eschews mathematics 12, and McLeish is asserting that this should not be understood as a pedestrian way of explaining the significance of the work but rather as “communicating ideas creatively”. Paul Sen [45](p. 79) says the same of Clausius’ ground-breaking papers: “[Clausius] persuades the reader in ordinary language before backing up his reasoning with formulae and algebra …”.\n\nWe note that McLeish explicitly considers the parallels between scientific creativity and the (wordless) creativity of painters and musicians: that is, there does exist a “knowledge” that is not mediated by words 13, but this wider view of knowledge is outside our present scope. Michael Polanyi also considered such knowledge, which he called “tacit” 14; here we only consider scientific knowledge from the point where it becomes crystallised in words:\n\nThe formulation of the fruitful question, posed in the right way, constitutes the great imaginative act in science\n\nTom McLeish The Poetry and Music of Science p. 10 (2019) [7]\n\nWe are also distinguishing sharply between “information” (which is physical) and “knowledge” (which is mental). I know precisely because I am informed. Stones incorporate information from which geologists can glean knowledge 15. Similarly, Oliver Sacks [49] (p.255) observed that information, however “wide-ranging … [is] different from knowledge”, being “centerless” 16.\n\nThe thesis of this paper is that where physics must use analytic language, metaphysics 17 must involve irreducibly poetic language. Language is intrinsically metaphorical: all our words have concrete referents but none of them is merely concrete, they all come with a cluster of connotations. Iris Murdoch [57] observed long ago of metaphors:\n\nMetaphors are not merely peripheral decoration or even useful models, they are fundamental forms of the awareness of our condition … it seems to me impossible to discuss certain kinds of concepts without the resort to metaphor, since the concepts are themselves deeply metaphorical and cannot be analysed into non-metaphorical components without a loss of substance.\n\nIris Murdoch, The Sovereignty of Good over other Concepts (1967) [57]\n\nand W.B. Yeats [58] was making a similar point in 1900 when he insisted on “The Symbolism of Poetry” which he claimed (and he should know!) involves a “continuous indefinable symbolism, which is the substance of all style” (ibid. §II). Yeats speaks of poets (and others) “making and unmaking mankind” (ibid.—he repeats this phrase!); that is, the symbolism of poetry (or the metaphor it embodies) actually touches the substance of things, just as Murdoch says 18. Yeats underlines this when he asserts that “meanings … are held [together] by the bondage of subtle suggestion” (ibid. §IV). “Substance” is a resonant word in physics, and also in European philosophy ever since the Cappadocian Settlement 19 in the 5th century CE. It is indicative that both Murdoch and Yeats refer to “substance”.\n\nA “metaphor” (after the ancient Greek μεταϕέρειν, to transfer) can be thought to translate (or transfer) between elements of this connotation cluster, and this idea of “translation” is essential to our thesis 20. We will show (using the particular case of entropy) that the narrative of physics is only established in the context of a metanarrative (in this case called “metaphysics”) which constructs the meanings of the ideas to be used in a natural language as unambiguous as possible. This metaphysical step is usually carefully ignored by philosophers of science: Nicholas Maxwell’s “aim-oriented empiricism” approach (predicated on the metaphysical priority of unified theories) is a welcome exception [63]. But standard empiricism glosses over the idealistic foundations of how we interpret observations 21.\n\nThere is a complexity here. We believe that Maxwell’s insistence on the idealistic nature of physics (since we always prefer unified theories, however wrong they might be) does not affect the common view of physicists that successful theories are true. That is, physicists are usually both realists and idealists. Logically, these two attitudes appear to be mutually exclusive: how then can they be compatible (if indeed they are)? We acknowledge that the naïve realist 22 and the naïve idealist positions are both untenable, but we will argue here for the truth that the physicist needs an idealist approach to recognise a promising theory, while depending on a philosophical attitude that regards the world as real, rational, and comprehensible in principle (that is, being some sort of realist). And formally, this philosophical attitude must be couched in a ‘natural’ language (however tacitly), there being no alternative. Of course, one’s underlying philosophical attitudes are rarely made explicit.\n\nNote that natural language is always ultimately poetic, especially where new meanings are being created. Meaning is always negotiated between speakers, and poets find new and resonant ways of doing this: Martin Edwardes (2019, [26]) has shown how this negotiated meaning must be central to ontology. When scientists establish new concepts, they must “negotiate the meaning” of the terms they use for these new concepts. We will show here how this works in the case of entropy (and info-entropy).\n\nUnderstanding physical concepts therefore always involves an intuitive leap in meaning from the concrete to the metaphysical, which we could also arguably (and nearly equivalently) call the spiritual. The very word spirit exemplifies this intuitive leap. Today the English word spirit has a range of metaphysical connotations, but in the original Latin it also carried the concrete meaning wind (which English word has an Anglo-Saxon etymology). So for example, there is a Greek record of Jesus’ saying (John 3:8):\n\nτο πνευμα οπου θελει πνει … που υπαγει ουτως εστιν πας\n\nο γεγεννημενος εκ του πνευματος\n\nTextus Receptus (<70 CE 23)[67]\n\nSpiritus ubi vult spirat … sic est omnis qui natus est ex spiritu\n\ntransl: Jerome (c.400 CE) [68]\n\nThe wynde bloweth where he listeth [where it wills] …\n\nso is every man that is boren of the sprete [born of the spirit]\n\ntransl: Tyndale (1526) 24 [69]\n\nwhere we give William Tyndale’s highly influential English translation, standard (in the form of the largely derivative 1611 King James version) until at least the 1960s (the “New English Bible” was only published in 1961).\n\nNote that cognates of the same word are used in both Greek and Latin (πνευμα, πνει, πνευματος/spiritus, spirat, spiritu) where three different words are needed in English (wind, blow, spirit). Translation of nuance is irreducibly creative: both Jerome and Tyndale had poets’ ears.\n\nSpeaking of “spirit”, we should perhaps mention the perennial “mind/body” debate (summarised helpfully by Brian Dolan, 2007 [70]): is “mind” expressible in terms of neurological function? Should we regard “mind” (whatever that is) as “emergent” from body? Is “mind” ultimately reducible to matter? Dolan shows that the materialists certainly have not settled these questions in their favour: they all remain open. We will show that new results in entropy (Velazquez 2022 [71], Velazquez et al. 2022 [72]) point to the value of a “holistic” (not reductionist) approach (see §4.6 passim below).\n\nReturning to the original question, what is entropy and what is information? These are ontological questions. How do we understand entropy and its relationship to information? These are epistemological ones. To answer these questions, we have to translate from the concrete to the general; that is, from specific observations to an articulation of a coherent theory. We will proceed to explore these issues, taking as examples the meanings of “information” and “entropy”. Our thesis is that moving from the concrete observation of physical reality to the general articulation of a physical theory we cannot avoid brushing with the spiritual (in the sense explained above, which in this context would also usually be called “metaphysical”).\n\nBarfield already knew a century ago that there is no clear line between poetry and prose: in reality these are undefinable categories, strictly speaking. But there is a clear distinction between poetic language and the analytic language that scientists must use. The poet relishes ambiguity 25, which is fundamental in language and essential to poetry. But the point of analytical language is to reduce the inherent ambiguities as far as possible.\n\nTo be explicit here (since we will systematically contrast poetic and analytic language), poets have a free hand to use words any way they choose to invoke meaning to the hearers, making as full use as they like of the range of connotation (the ambiguity) of the words used. If the poet is successful, then the hearer perceives meaning in the poem. On the other hand, scientists must analyse the ideas they wish to develop into components that are specified and combined as unambiguously as possible. But where do the scientists’ ideas come from in the first place?\n\nIt should not be thought that because the use of ‘natural’ language is inescapable (and therefore that fundamental ambiguities necessarily remain in our theories), our knowledge of the world is thereby undermined. We will here underline what is common sense: all knowledge is ultimately incomplete—that is, we cannot know everything about anything. We wish to underpin our knowledge by giving a more correct account of it. No knowledge is absolute, and it is time to give a more nuanced account of the basis of our epistemology. Ultimately, we cannot avoid ambiguity: therefore, let us—like poets—start to treat it positively.\n\nThe analytical narrative must be encased in a metanarrative (as we will show explicitly in §4); moreover, poetic perception cannot be spoken of analytically. The early Wittgenstein famously said, “Whereof one cannot speak, thereof one must be silent” 26, but the later Wittgenstein changed his mind, saying instead, “[in most cases] … the meaning of a word is its use” 27. In our terms, he switched from believing that analytic language was sufficient, to recognising that poetic language was ontologically indispensable 28. Something similar can be said of Richard Rorty: in 1982 he famously said (citing William James) that truth is “a compliment paid to sentences that seem to be paying their way” (Rorty, 1982 [79]); but in 2000 he says: “it was a mistake on my part to go from criticism of attempts to define truth as an accurate representation of the intrinsic nature of reality to a denial that true statements get things right” (Rorty, 2000 [80]). In his influential essay Bruno Latour also said, “do we now have to reveal the real objective and incontrovertible facts hidden behind the illusion of prejudices?” (emphasis original, Latour 2004 [81]). Of course, we will argue here (§3) that it is a logical mistake to try to “define truth”.\n\nOur epigraph touches both ontic and epistemic issues. It is composed (after a poem in Náhuatl, an autochthonous Mexican language) in Palikur, a northern Arawak language spoken by less than four thousand people living in the Brazilian state of Amapá and in French Guiana. There is a Palikur-Portuguese dictionary (Green, 2010 [82]) and the language displays ways of knowing that differ markedly from modern European ones (Green, 2013 [83]), and in particular Palikur speakers are deeply aware of what we would regard as advanced topological concepts from the very grammar of their language (Green and Green, 2023 [84]) 29. The way we think—our very identity—is inextricable from our language (and the Náhuatl and the Palikur poems both express how horrible its loss would be 30). What we know is inexpressible without language 31. Benjamin Lee Whorf (1941 [40]) drew attention to the converse of this: “… people act about situations in ways which are like the ways they talk about them”, but this only serves to underline our point. If we cannot say it, we cannot know it: this is true for all aspects of reality.\n\n3. The Thinginess of Things\n\nMichael Frayn (2006 [86]) has memorably spoken of the “thinginess of things” 32, that is, the sure ontological grasp that reality appears to have on us. Things are! This has long been resonant with the poets: for example, Wallace Stevens (1954 [88]) spoke specifically of “A new knowledge of reality”; we could also mention William Carlos Williams’ famous and very influential line “No ideas but in things” (Williams, 1926 [25]), which is still widely discussed (see for example Finch, 2013 [89]). Additionally, Iris Murdoch is quoted as saying, “I’m glad we live in a thingy world” (Jordan, 2012 [90]); her novels are shot through with this philosophical attitude. In her first published novel (Murdoch, 1954 [91]) she makes one of her heroes observe that the “activity of translating”, central to our thesis here, is “an act so complex and extraordinary that it was puzzling to see how any human being could perform it”. Why is this? Because every thing is “astonishing, delightful, complicated and mysterious” (ibid. ch. 4, p. 62).\n\nThing is a very ancient word with a surprisingly wide range of connotation (including parliament), and which is thought to be related to the Indo-European root of the Latin word tempus, time. Of course, material things only exist—can only exist—in time: Frank Wilczek (2021 [92]; ch.6, p. 159) points out that this underlies Augustine of Hippo’s (c.420 CE, [93]) elegant proof that the Christian doctrine of Creation entailed the creation of time along with matter 33. For, Augustine said, we only know time by the movement of things (he fixed their ontology by calling them “creatures”—that is, things made by the Creator); therefore, if there are no things then neither can there be time:\n\nprocul dubio non est mundus factus in tempore, sed cum tempore … nullum autem posset esse praeteritum, quia nulla erat creatura, cuius mutabilibus motibus ageretur\n\nverily the world was made with time, and not in time … no time passed before the world, because no creature was made by whose course it might pass. But it was made with time, if motion be time’s condition\n\nAugustine, City of God XI:6, c.420 CE [93]\n\nThere is also a similar statement in a lengthy and acute discussion in Book XI of the Confessions (Augustine c.400 [94]). Thus, Augustine anticipates by a millennium and a half the conclusion of the Gravitational Singularity Theorem: that time had a beginning is a necessary consequence of General Relativity (Hawking and Penrose, 1970 [95]).\n\nAll physicists operate on the assumption (not usually explicitly acknowledged) that the thinginess of the phænomena they investigate is ontologically secure: that is, the world is real 34. Philosophically and historically, this ontological security ultimately derives from the assertion of Creation by the monotheist religions 35, even if most physicists today assume it tacitly merely as a pragmatic precondition. Interestingly, Gerry Schroeder (1997 [99]) has shown both that the Hebrew Creation story successfully resists scientific criticism, and that its interpretation is as subtle and elusive as any poetic text. And Iris Murdoch is not the only philosopher to comment on, as she puts it, “the infinite elusive character of reality” (Murdoch 1962 [100]).\n\nIt is important to realise that the thinginess of things is ontologically axiomatic, as Frayn effectively acknowledges in a long discussion (Frayn 2006 [86]). Our ultimate epistemological reliance on personal guarantee is documented by Richard Bauckham (2006 [101]) in the context specifically of historical events: ultimately, we know things only through eyewitness testimony 36:\n\nThe testimony of Holocaust survivors is the modern context in which we most readily recognise that authentic testimony from participants is completely indispensable to acquiring real understanding of historical events, at least events of such exceptionality.\n\nBauckham, 2006 §18 (p. 499) [101]\n\nWe can of course subject testimony to the standard critical tests but, more often than not, in the end we have to decide whether or not to trust the witness. In the end, we simply have to choose what to believe. Note that “personal guarantee” also underlies the peer review system, which cannot operate without good faith. Thus, testimony also underlies the epistemology of scientific knowledge.\n\nIt seems that all scientists ought to be effectively realists of some sort, whether or not they believe this philosophically (but see the “Solipsist’s Plea” below, note#90). If they did not implicitly believe (a) that the world is there, (b) that laws of nature exist, and (c) these laws are discoverable by us; why would they get up in the morning for another frustrating day in the lab? Surely, they would find something else more lucrative to do? But realists do not have to be naïve! So the fifth chapter of the dense book by Karen Barad (Barad, 2007 [24]) is titled “Getting Real: Technoscientific Practices and the Materialisation of Reality” and has an epigraph by Michel Foucault (renowned as a postmodern structuralist critic even if he himself did not like these labels). Barad’s book is an extended, detailed and subtle investigation of “Reality” and the “Ontology of Knowing” (these terms are taken from the book’s chapter headings) in the light of the ontological puzzles forced on us by a deep look at the fundamentals of quantum mechanics. Barad knows not only that the Universe is there, but also that our usual naïve ways of thinking about this are false—our grasp of reality is often uncertain and unreliable: the book title (“Meeting the Universe Halfway”) is a line from Alice Fulton’s poem “Cascade Experiment” (Fulton, 1989 [102]) which opens: “Because faith creates its verification …” 37.\n\nOliver Sacks (1993 [33]) confirms this attitude independently, speaking of Humphrey Davy (the great English chemist) as a poet: “The poet and the chemist were fellow warriors, analyzers and explorers of a principle of connectedness” (ibid, p. 23), and he quotes Coleridge (1818 [107]): “through the meditative observation of a Davy … we find poetry, as it were, substantiated and realized in nature … as at once the poetry and the poet” (emphases original).\n\nHowever, reality is elusive. Is knowledge objective? Are the things that science describes and explains really there? Alessandro Fedrizzi and Massimiliano Proietti (Fedrizzi and Proietti 2019 [108]) gloss their paper (Proietti et al., 2019 [109]) as “Objective Reality Doesn’t Exist, Quantum Experiment Shows”. The paper reports an elegant three-photon-pair implementation of a “Wigner’s friend experiment” demonstrating a violation of the associated Bell inequality 38. This means that in this case the results observed are not “objective” (that is, they are not observer-independent). But, as Karen Barad explains in detail, this does not mean that reality itself is illusory, only that knowing it is not necessarily very straightforward:\n\nTraditional philosophy has accustomed us to regard language as something secondary, and reality as something primary. [Niels] Bohr considered this attitude toward the relation between language and reality inappropriate. When one said to him that it cannot be language that is fundamental, but that it must be reality that, so to speak, lies beneath language, and of which language is a picture, he would reply, “We are suspended in language in such a way that we cannot say what is up and what is down. The word ‘reality’ is also a word, a word we must learn to use correctly.”\n\nBarad 2007 [24], p. 205 (quoting Petersen 1985 [111])\n\nMichael Polanyi in his “Personal Knowledge” (1958 [38]) insists that, ultimately, we have only personal guarantees of whatever knowledge we think we possess: strictly speaking, objective knowledge is an oxymoron 39:\n\n… the intuition of rationality in nature [must] be acknowledged as a justifiable and indeed essential part of scientific theory. That is why scientific theory … [can be] represented as a mere economical description of facts … or as a working hypothesis … [but these are] interpretations that all deliberately overlook the rational core of science.\n\n… great theories are rarely simple in the ordinary sense of the term. Both quantum mechanics and relativity are very difficult to understand; it takes only a few minutes to memorize the facts accounted for by relativity, but years of study may not suffice to master the theory and see these facts in its context.\n\n… We understand the meaning of the term ‘simple’ only by recalling the meaning of the terms ‘rational’ or ‘reasonable’ or ‘such that we ought to assent to it’, which the term ‘simple’ was supposed to replace. The term ‘simplicity’ functions then merely as a disguise for another meaning than its own. It is used for smuggling an essential quality into our appreciation of a scientific theory which a mistaken conception of objectivity 40 forbids us to acknowledge.\n\nPolanyi, Personal Knowledge 1958 [38], §1:4 (emphasis added)\n\nwhere here by “rational” Polanyi means to imply our application of reasoning: it is people who do the reasoning! Knowledge is irreducibly personal; the “rational core of science” entails reasoning people 41. So we prefer the Copernican theory over the Ptolomaic one precisely because we think that “its excellence is, not a matter of personal taste on our part, but an inherent quality deserving universal acceptance by rational creatures. We abandon the cruder anthropocentrism of our sense—but only in favour of a more ambitious anthropocentrism of our reason” (ibid, §1:1). Similarly, Polanyi and Prosch (1975 [22], Ch.3) draw attention to “Noam Chomsky’s critique of B. F. Skinner’s ‘Verbal Behavior’ [which] presents many illustrations of such behaviourist paraphrasing. Apparently objective terms … are so used that their ambiguity covers the mental terms they are supposed to replace.”\n\nJerome Ravetz (1971 [113]) later took up and amplified the social element of Polanyi’s characterisation of knowledge as personal in his very influential demonstration that scientific research is a craft activity heavily dependent on the tacit knowledge that Polanyi emphasised.\n\nHow do we know that nature is rational (and is therefore amenable to scientific description)? We intuit it. Prior to our rationalisations is our belief that rationalisations exist. And in speaking of rationality here, Polanyi is also referring to the primacy over common sense scientists commonly give to idealistic thought—we have already mentioned Maxwell’s “aim-oriented empiricism” (Maxwell, 2020 [63]). Polanyi asks:\n\nWhat is the true lesson of the Copernican revolution? Copernicus gave preference to man’s delight in abstract theory, at the price of rejecting the evidence of our senses, which present us with the irresistible fact of the sun, the moon, and the stars rising daily in the east to travel across the sky to their setting in the west.\n\nPolanyi, 1958 [38], §1:1\n\nThe fact may appear psychologically “irresistible”; nevertheless, Polanyi points out that behaving rationally we systematically do resist it. We may “intuit” that the sun goes round the earth; but at a deeper level we intuit that the relation of sun to earth is lawful, and analytically we recognise that the simplest expression of the law has the earth going round the sun. We intuit the existence of the rationality that underpins this lawfulness. It is the business of poets to articulate intuition 42.\n\nOf course, Polanyi is aware of the logical necessity of this attitude to rationality, which becomes clear (as he explains) when Kurt Gödel’s Incompleteness Theorem (1931 [43]) is understood. Quoting S.C.Kleene’s Introduction to Metamathematics (1952 [114]), Polanyi says,\n\nRules have been stated to formalise the object theory, but now we must understand without rules how those rules work. An intuitive mathematics is necessary even to define the formal mathematics.\n\nPolanyi, 1958 [38], §8:8\n\nThis “intuitive mathematics” is called “metamathematics” by everyone—Polanyi, Kleene, Gödel—just as we call the comparable “intuitive physics” by the cognate word “metaphysics”. Every narrative has its metanarrative, without which no one can make any sense of it.\n\nGödel’s achievement was to demonstrate by construction that his formula (which we can express in words as “this sentence is undecidable”) was not meaningless. His demonstration was rather involved, but indicates the processes of mind required to establish this cornerstone of epistemology. We display its flavour with this brief extract from his Introduction (here R is an ordering relation for all the definable formulas, and K is the set of “Gödel numbers” q representing unprovable formulas):\n\nDie Analogie dieses Schlusses mit der Antinomie Richard springt in die Augen; auch mit dem ,,Lügner” besteht eine nahe Verwandtschaft, denn der unentscheidbare Satz [R(q); q] besagt ja, daß q zu K gehört, [das heißt] nach (1), daß [R(q); q] nicht beweisbar ist. Wir haben also einen Satz vor uns, der seine eigene Unbeweisbarkeit behauptet.\n\n13 Man beachte, daß ,,[R(q); q]” … bloß eine metamathematische Beschreibung des unentscheidbaren Satzes ist.\n\nThe analogy between this result and Richard’s antinomy leaps to the eye; there is also a close relationship with the “Liar”, since the undecidable proposition [R(q); q] states precisely that q belongs to K, that is according to Equation (1), [R(q); q] is not provable. We are therefore confronted with a proposition that asserts its own unprovability.\n\n(footnote #13:) Note that “[R(q); q]” … is merely a metamathematical description of the undecidable proposition.\n\nGödel, 1931 [43]\n\nRichard’s paradox was stated in 1905, but the Liar Paradox is ascribed to Epidemides of Crete, alluded to by Paul of Tarsus (Titus 1:12, 57 CE 43), and investigated at length among others by the 14th century John Buridan, who conditioned Galileo’s theory of impetus (Read, 2002 [115]).\n\nIt is well known that Gödel later became fascinated by Anselm’s comparable Ontological Argument for the existence of God (Proslogion, Anselm 1078 CE [116]). Anselm asserted that the idea, “aliquid quo maius nihil cogitare potest” (“that than which no greater can be thought”) was not unthinkable, and therefore God (than which no greater can be thought) must exist in fact. Starting from this premise of “thinkability”, Anselm actually gave a proof that in its self-referencing form 44 anticipated Gödel’s proof by a millennium:\n\nEt certe id quo maius cogitare nequit, non potest esse in solo intellectu. Si enim vel in solo intellectu est, potest cogitare esse et in re, est in solo intellectu: id ipsum quo maius cogitare non potest, est quo maius cogitare potest. Sed certe hoc esse non potest.\n\nAnd surely that-than-which-a-greater-cannot-be-thought cannot exist in the mind alone. For if it exists solely in the mind even, it can be thought to exist in reality also, which is greater. If then that-than-which-a-greater-cannot-be-thought exists in the mind alone, this that-than-which-a-greater-cannot-be-thought is that-than-which-a-greater-can-be-thought. But this is obviously impossible.\n\nAnselm, 1078 [116], II\n\nThe elegance of Anselm’s Latin is noticeable. And one can hear an attenuated echo of this ontological argument in Descartes’ famous dictum “cogito ergo sum” (“I think therefore I am” 45), which George Berkeley (1710 [120]) modified to “esse est percipi” (“to be is to be perceived”) deliberately to contrast the idealism of the scholastic nominalists with the new materialist schools. Anselm goes on to comment on the relation between believing (ontics) and understanding (epistemics) that is central to our present work:\n\nGratias tibi, bene dominum, gratias tibi, quia quod prius credidi te donante, iam sic intelligo te illuminante, ut si te esse nolim credere, non possim non intelligere.\n\nI give thanks, good Lord, I give thanks to you, since what I believed before through your free gift I now so understand through your illumination, that [even] if I did not want to believe that you existed, I should nevertheless be unable not to understand it.\n\nAnselm, 1078 [116], IV\n\nThis is reminiscent of Augustine’s dictum “nisi crediteris non intelligetis” (“if you do not believe you will not understand”: City of God, XII:17 [93]; quoting a version of Isaiah 7:9–Anselm himself quotes Augustine explicitly). But Anselm has recognised how the increase in knowledge works—first we see, then we understand—which is equally true for painters, for poets, and for physicists. First one grasps the idea, then one works out the details. Just because the devil is in the detail does not mean that the initial illumination is dispensable. Just because many ideas turn out to be incoherent does not mean that the fruitful ideas do not originate with illumination. One is reminded of Eric Dodds’ comment (1951 [121], in his Preface): “time and the critics can be trusted to deal with the guesses; the illumination remains”.\n\nWe are not here saying that we reliably grasp things by intuition—everyone knows this is not the case! To test the reliability of our ideas we have to do science in the usual way. But where does the idea itself come from? Its origin is the “illumination” discussed by Anselm 46. We discern truth: nevertheless, uncertainty cannot be eliminated.\n\nBoth Gödel’s and Anselm’s sentences are self-referencing, and have logical properties entirely due to this recursiveness. Gödel’s sentence is proved “not meaningless” by construction (and therefore true, by a metamathematical argument), but because of its wider scope Anselm’s sentence has resisted such construction 47.\n\nGödel’s proof was a revolution, not only in its overturning of the expectation of the mathematicians that arithmetic could be proved both consistent and complete 48, but also in its entirely novel style of proof, relying explicitly on a metamathematical argument. It is interesting not only that Anselm anticipated Gödel, but also that he understood the logical status of his argument, which he did not present analytically but poetically (as a prayer). Ultimately, ontic knowledge is, and can only be, intuited. How else can one understand Paul of Tarsus writing in 57 CE (see note#23) about God, who:\n\nκαλουντος τα μη οντα ως οντα\n\n(Romans 4:17, Textus Receptus [67])\n\ncalleth thoſe things which be not as though they were\n\n(transl: Tyndale, 1526 [69])\n\nIn a different context, Thomas Piketty (2019 [34]) gives us a complementary view of the necessarily intuitive nature of the knowledge of thinginess. In a section titled “On the Complementarity of Natural Language and Mathematical Language”, Picketty says:\n\nThis book will rely primarily on natural language (about which there is nothing particularly natural) … There is no substitute for natural language when it comes to expressing social identities or defining political ideologies. … Those who believe that we will one day be able to rely on a mathematical formula, algorithm, or econometric model to determine the “socially optimal” level of inequality are destined to be disappointed. … I do not contend that “truth” is found only in numbers or certainty only in “facts”.\n\nPicketty, 2019 [34], Introduction\n\nTo be clear: we are distinguishing between the analytic language required for scientific work, and the natural language we use every day (see note#8) together with the poetic language needed to express deep meanings; there is no sharp boundary between “poetic” and “natural” language just as there is no sharp boundary between poetry and prose.\n\nPicketty encloses “facts” in quotes since these are always contentious in economics: one person’s verity is always another’s heresy, and Picketty authoritatively displays the ideological nature of such “facts”. But it turns out that physics is also ideological in a similar way and for similar reasons 49. Of course, this is not entirely unexpected: our present essay here could be thought of as merely a footnote to Thomas Kuhn’s seminal book of a generation ago (The Structure of Scientific Revolutions, 1962 [127]). We proceed to explore this ideology specifically in relation to the development of ideas of entropy since the mid-19th century.\n\n4. Entropy and Information\n\nAs a specific phænomenological example of the scientific method in action, leading to new knowledge, we will now tell the strange and intricate story of the development of the idea of “entropy”, which word is a neologism of Rudolf Clausius in analogy to “energy” (an exactly similar Hellenic word). This story concerns the foundations of physics and is still being vigorously developed, which is why we give it so much space here: modern work has underlined its fundamental importance practically, scientifically and philosophically. We hope to give a flavour of this importance to the wider public, despite the very substantial difficulty (both mathematical, and philosophical) of the subject.\n\nWe start with considering precisely how the First Law of Thermodynamics (“energy is conserved”) was originally conceived (Clausius 1854 [35]). It is an effort of imagination for us today to appreciate that at that time the energy-equivalence of work and heat was not understood, and therefore that Clausius’ formulation was a breakthrough. Here we give the long passage explicitly (over 300 words in a complex German) because we want to underline just how complex even “simple” ideas really are. Advances in physics involve revolutions in thought, that is, looking at things differently. And the job of poets is precisely to help us to see things differently. Clausius’ physics advance was therefore also a breakthrough in poetics, just as the new appreciation of atomic theory stimulated by the new microscopes in the 17th century was also a breakthrough in poetics as Cassandra Gorman has shown explicitly (Gorman 2021 [128]).\n\nThe early work established the idea (§4.1; this story is told in much more detail by Paul Sen, 2021 [45]); then Boltzmann and others developed its implementation in statistical mechanics (§4.2); then we consider information, and the “Shannon entropy” (§4.3); the case of black hole entropy and the Bekenstein–Hawking equation is amazing (§4.4); then we apparently come full circle considering the “geometrical” entropy of Parker and co-workers, who clearly demonstrate a true isomorphism between entropy and energy (not merely an “analogy”; §4.5); lastly we consider some implications of this discussion for the very meaning of “causality” (§4.6).\n\nCausality is a basic notion in science—we cannot do science at all unless we think that causes have effects. The trouble is that the closer one looks at things the more entangled everything gets. The modern work on entropy has highlighted this sort of problem since it turns out that non-local phænomena are widespread—these sorts of phænomena defy cause-and-effect sorts of accounts, being governed instead by the Variational Principles (Least Action, Maximum Entropy etc.). It seems that the very idea of “causality” is some sort of approximation that must be tempered by a parallel treatment of the interconnectedness of things.\n\n4.1. Early Work on the Concept of Entropy\n\nEntropy is a slippery concept. It is the thing you measure (or calculate) if you want to know the effect of the Second Law of Thermodynamics, which says (roughly) that entropy can only increase. It is the Second Law that asserts that you cannot unscramble eggs, or alternatively (there are many ways of saying the same sort of thing) we live first and die afterwards, and not the other way round. These things are obvious, but it is not at all obvious how to introduce these “simple” ideas into physics, and there have been detailed (and frankly arcane) technical arguments for the last couple of centuries on what these things mean. We therefore open with some modern positions to help with orientation.\n\nEdwin Jaynes was responsible for the seminal variational Principle of “Maximum Entropy”, now widely used across many fields and, in a paper explaining some fundamental aspects of the treatments of (the 19th century giants of physics) Gibbs and Boltzmann, Jaynes says about entropy:\n\nIt is interesting that, although this field [entropy] has long been regarded as one of the most puzzling and controversial parts of physics, the difficulties have not been mathematical. … It is the enormous conceptual difficulty of this field which has retarded progress for so long.\n\nJaynes, 1965 [129] (emphasis original)\n\nThe Oxford English Dictionary (OED [53]) is very helpful. Rudolf Clausius introduced the term entropy in 1865 specifically as a Hellenistic neologism: from ἐν + τροπή (transformation; literally ‘turning’: all the connotations of trope are also present in English). The OED comments:\n\nClausius assumed that (German) Energie literally meant ‘work content‘ (Werkinhalt) and devised the term Entropie as a corresponding designation for the ‘transformation content’ (Verwandlungsinhalt) of a system.\n\nOxford English Dictionary, 3rd Edition (September 2018)\n\nAnd then, in sense 1a (“Physics and Chemistry”), the OED elaborates:\n\nEntropy was first defined by the German physicist Rudolf Clausius (1822–1888). Scottish physicists Peter Guthrie Tait (1831–1901) and James Clerk Maxwell (1831–79) were the first to interpret entropy as a measure of the unavailability of energy for work.\n\nThe modern mathematical definition of entropy, in terms of the possible microstates … of a thermodynamic system, first appears in the work of Austrian physicist Ludwig Boltzmann (1844-1906), who viewed entropy as a measure of the disorder of a system.\n\n[Sense 3 “Statistics and Information Theory”)] … mathematician Claude Shannon (1916–2001) coined the term in the context of information theory (see sense 3b)\n\nOxford English Dictionary, 3rd Edition (September 2018)\n\nThe OED gives a variety of definitions, three related to scientific concepts. (We will show below that these do not exhaust the meanings assigned to the term.) This is not merely a philological variety, but a real scientific discrepancy that has led to much confusion. It is still not entirely clear to everyone that the multiple definitions do actually refer consistently to a coherent idea. But the confusion has certainly resulted in error. Indeed, as Jaynes noted near the end of his life, regarding his variational approach to providing an underlying principle to entropy: “…the long confusion about order and disorder (which still clutters up our textbooks) is replaced by a remarkable simplicity and generality” (Jaynes 1992 [130]).\n\nThe very logical status of the Second Law of Thermodynamics has long been debated, as hinted at above. Is it a fundamental Law? Or is it a consequence of the other Laws, which are all time-reversible (except for the CP-violation by K-mesons discovered by Cronin and Fitch: Christenson et al. 1964 [131]) 50? Either way, consistency is a problem. How can time reversibility be consistent with time irreversibility (see below on the “Arrow of Time”, §4.6)? Clausius first clearly stated a version of the Second Law in 1854:\n\nes kann nie Wärme aus einem kälteren in einen wärmeren Körper übergehen, wenn nicht gleichzeitig eine andere damit zusammenhängende Aenderung eintritt.\n\nHeat can never pass from a colder to a warmer body without some other change, connected therewith, occurring at the same time.\n\nClausius, 1854 [35]\n\nIn the same 1854 paper, Clausius also recognised (before he had introduced the term) that entropy remains unchanged for reversible cyclic processes (“umkehrbaren Kreisprocesse”), calling the identity ∫ dQ/T = 0 the “second law of the mechanical theory of heat” (“des zweiten Hauptsatzes der mechanischen Wärmetheorie”). Of course, the “first law” was Q = U + A∙W, where Q is the total quantity of heat (“die ganze Wärmemenge”), U is how much heat is in the system before work is done on it, W is the external work (“die äuſsere Arbeit”), and A is the factor converting work to heat (“das Wärmeaequivalent für die Einheit der Arbeit”, literally: “the heat equivalent for the unit of work”). It is instructive to see how Clausius reasons here:\n\nBei dieser Bestimmungsweise kann man den Satz von der Aequivalenz von Wärme und Arbeit, welcher nur einen speciellen Fall der allgemeinen Beziehung zwischen lebendiger Kraft und mechanischer Arbeit bildet, kurz so aussprechen:\n\nEs läſst sich Arbeit in Wärme und umgekehrt Wärme in Arbeit verwandeln, wobei stets die Gröſse der einen der der anderen proportional ist.\n\n… Betrachten wir nun die bei einer Zustandsänderung gethane innere und äuſsere Arbeit zusammen, so können sich beide, wenn sie von entgegengesetzten Vorzeichen sind, theilweise gegenseitig aufheben, und dem Reste muſs dann die gleichzeitig eintretende Aenderung der Wärmequantität aequivalent seyn. Für die Rechnung aber kommt es auf dasselbe hinaus, wenn man für jede von beiden einzeln eine aequivalente Wärmeänderung annimmt.\n\nSey daher Q die ganze Wärmemenge, welche man einem Körper, während er auf einem bestimmten Wege aus einem Zustande in einen andern übergeht, mittheilen müſs, (wobei eine entzogene Wärmemenge als mitgetheilte negative Wärmemenge gerechnet wird), so zerlegen wir diese in drei Theile, von denen der erste die Vermehrung der wirklich in dem Körper vorhandenen Wärme, der zweite die zu innerer und der dritte die zu äuſserer Arbeit verbrauchte Wärme begreift.\n\nVon dem ersten Theile gilt dasselbe, was schon vom zweiten gesagt ist, daſs er von der Art, wie die Ver-änderung stattgefunden hat, unabhängig ist, und wir können daher beide Theile zusammen durch eine Function U darstellen, von der wir, auch wenn wir sie sonst noch nicht näher kennen, wenigstens soviel im Voraus wissen, daſs sie durch den Anfangs- und Endzustand des Körpers vollkommen bestimmt ist.\n\nDer dritte Theil dagegen, das Aequivalent der äuſseren Arbeit, kann, wie diese selbst, erst dann bestimmt werden, wenn der ganze Weg der Veränderungen gegeben ist. Nennen wir die äuſsere Arbeit W, und das Wärmeaequivalent für die Einheit der Arbeit A, so ist der Werth des dritten Theiles A∙W, und wir erhalten daher als Ausdruck des ersten Hauptsatzes folgende Gleichung:\n\n(I) Q = U + A∙W\n\nWith this means of determination, one can now concisely express the relation between the equivalence of heat and work (which is only a special case of the general relationship between active power and mechanical work) by the following saying:\n\nWork can be turned into heat and vice versa heat can be turned into work, so that the magnitude of the one is always proportional to the other.\n\n… Let us now consider, in the event of a change of state, the internal and external work together. These both, taken together, can partially compensate each other if they are of opposite signs. Then the remainder must be equivalent to the change of the quantity of heat that occurs at the same time [i.e., during the change of state event]. For the calculation however, it comes back to the same thing, if one assumes an equivalent change in heat from the two separate entities [i.e., for each of internal work and external work separately, one takes the heat equivalent].\n\nLet Q be the entire quantity of heat that must be imparted to a body, while going on a certain path from one state to another (where heat removed is counted as a negative quantity of heat imparted) [this is in the context of the Carnot cycle]. This can be broken into three parts, of which the first is the increase in heat actually present in the body, the second is the heat used for internal work and the third the heat used for external work.\n\nOf the first part one can say the same as has already been said about the second part: that it is independent of the way that the change of state happened. We can therefore combine both parts together into a function U, for which we know in advance (regardless of how little knowledge we otherwise have) that it is completely (sufficiently) defined by the initial and final states of the body.\n\nOn the other hand, the third part, i.e., the equivalent of the external work, can only be calculated when the whole path of change is given. We call the external work W, and the heat equivalent for the unit of work A, so that the value of this third part is the product A∙W, and we come into view of the resulting first law in the following equation: (I) Q = U + A∙W\n\nClausius, 1854 [35] (emphasis original)\n\nIt is plain that the equation, Q = U + A∙W, derives its meaning from the previous discussion, which is in a “verschachtelt” (literally “nested”) German that is both syntactically and semantically complex: it defies a literal translation and it is hard to translate into a comprehensible English. This extract concerns what we now know (and what Clausius himself called) the “First Law” (of thermodynamics). Clausius later in the paper tries to describe the effect of entropy, involving the “Second Law” (both his term and ours), without knowing its explicit existence or name (he only coined the term in his 1865 paper [132]), hence the apparent confusion and inarticulacy of this complex text of 1854. We leave the linguistic analysis as an exercise for the interested reader, but we conclude that Clausius is carefully constructing (“negotiating”, Edwardes, 2019 [26]) meanings for the terms he wishes to manipulate mathematically in just the way that Barfield says is characteristic of poets.\n\nThis is a rather clear example of metaphysical priority in a physical argument. We will discuss the logical properties of metanarratives later (§5.2): here we see Clausius using a natural language replete with its natural metaphors and ambiguities, but intending to restrict the unavoidable ambiguity as much as possible. It is only by using natural language that we can say anything at all, but then if we care about the meanings we are constructing we have to also address the formal poetics. Usually this step is tacit, but we are here drawing attention to it.\n\nPhysicists tend to think that they can manipulate the behaviour of phænomena symbolically (since we all believe that the symbols truly represent reality), but in fact they only symbolically manipulate the ideas they have constructed of those behaviours 51. Whence arise the ideas? And what relation (both ontic and epistemic) has the idea to the phænomenon?\n\n4.2. Entropy and Statistical Mechanics\n\nAll students of thermodynamics start today with the model of the ideal Carnot cycle, which establishes the ideas of “waste heat” and “maximum thermodynamic efficiency”. Clausius depended on the Carnot cycle to model his idea of “entropy” as the accessible useful work available in some quantity of heat—in his time, the steam engine powered the world: is it any wonder that (as we shall see) the ideal gas laws should be the natural exemplar of heat engines 52?\n\nIt is also by considering the ideal gas as a model for heat engines that today’s students learn the basics of statistical mechanics, first developed with great brilliance by the mid-nineteenth century giants of physics: Gibbs, Boltzmann and Maxwell. Ludwig Boltzmann is remembered by his eponymous constant k, and by the formula engraved on his tombstone (which in this form is due to Max Planck) 53:\n\nS = k log W\n\n(1)\n\nIt is well known that this “simple” treatment ignores or obscures a number of severe difficulties. The usual definition makes entropy an extensive quantity even though it is well known that this is an approximation appropriate only in certain circumstances:\n\nEntropy is just as much, and just as little, extensive in classical statistics as in quantum statistics … entropy stands strongly contrasted to energy.\n\nJaynes, 1992 [130]\n\nHowever, strictly speaking, entropy is an intensive quantity 54, as Jaynes observes in a penetrating discussion in the same place of the so-called Gibbs Paradox:\n\n[Gibbs] had perceived that, when two systems interact, only the entropy of the whole is meaningful. Today we would say that the interaction induces correlations in their states which makes the entropy of the whole less than the sum of entropies of the parts; and it is the entropy of the whole that contains full thermodynamic information. This reminds us of Gibbs’ famous remark, made in a supposedly (but perhaps not really) different context: “The whole is simpler than the sum of its parts.” How could Gibbs have perceived this long before the days of quantum theory?\n\nJaynes, 1992 [130] (emphases original)\n\nJaynes earlier had made an astonishing statement of the subjectivity of the concept of entropy in his acute comparison of the Gibbs and Boltzmann formulations:\n\n… not only in the well-known statistical sense that it measures the extent of human ignorance as to the microstate [but also] [e]ven at the purely phenomenological level, entropy is an anthropomorphic concept. For it is a property, not of the physical system, but of the particular experiments you or I choose to perform on it.\n\nJaynes, 1965 [129] (emphasis original)\n\nThe point here is that the result of the entropy calculation depends on how the Partition Function 55 of the system is specified, that is, which particular measurements are being contemplated. The Partition Function describes how the phase space (which enumerates all of the microstates) is specified. Then the observables are specified by the macroscopic parameters, which can hopefully be calculated from the thermodynamics. Roger Penrose puts this quite sharply:\n\n… we can … appreciate … [that] Boltzmann’s formula … put forward in 1875 … represented an enormous advance on what had gone before … There are, nevertheless, still certain aspects of vagueness in this definition, associated, primarily, with the notion of what is to be meant by a “macroscopic parameter”.\n\nPenrose, 2010 [142], §1.4\n\nCarlo Rovelli made essentially the same point very recently when he argues that “we are blind to many variables [that are] at the heart of Boltzmann’s theory”, adding:\n\nThermodynamics … is a description of these variables of the system: those through which we assume we are able to interact with the system\n\nRovelli, 2017 [143] (ch.10, n.4; emphasis original)\n\nHowever, it was Max Planck who in 1900 [144] first recognised “Boltzmann’s constant” per se (see Equation (1)) as fundamental to entropy in the seminal paper in which he explains black body radiation in terms of quantised resonators; and where he gives the quantisation constant, h, in units of action correct to almost 1% 56.\n\n4.3. Information\n\nWe go into apparently arcane details in this section following Lars Lundheim’s useful review (Lundheim 2002 [145]), not only because the details are both surprising and very interesting, but also because it is the assimilation of Claude Shannon’s information entropy (or the eponymous Shannon entropy) that has enabled the proliferation of today’s high-speed networks, a technology that would otherwise be inexplicable.\n\nThe first transatlantic “telegraph” cable was laid in 1858 but only operated for three weeks. A lasting transatlantic connection was established in 1866. In addition to its technical triumph, this was commercially very valuable (and expensive) technology, and the search for efficiency naturally attracted great scientific attention. The first message was transmitted (by Morse code, in 1858) at 10 minutes per word. The second (1866) cable already operated almost two orders of magnitude faster, at eight words per minute; but the transmission speed (that is, the bandwidth) was necessarily slow because of frequency dispersion in the cable: this was already understood in principle by William Thompson (later Lord Kelvin) who published his analysis in 1854 and was closely involved with the enterprise.\n\nHowever, although practical development (telegraphy with time- and frequency-division multiplexing, telephony, radio) was very rapid, little advance was made on what we would now call informatics until the 1920s, when it became clear that “bandwidth limitation sets a fundamental limit to the possible information transfer rate of a system” [145]. And the very idea of bandwidth depends on the understanding of electrical ‘band pass’ filters, which were not patented until 1917.\n\nThe additional problem of signal-to-noise dominated telecommunications science as soon as more reliable long-distance signalling was allowed by usable amplifiers (i.e., valves, exploiting the vacuum tube technology which had originally been developed for the incandescent light bulb). But in the 1920s there was still no standard scientific understanding of noise: Norbert Wiener’s work on stochastic noise (Brownian motion) was published between 1920 and 1924, and Harry Nyquist’s mathematical model of thermal noise was only published in 1928. The vacuum tube amplifier had been introduced around 1910, but the high gains obtainable by cascading amplifiers had to wait until the feedback principle was patented in 1928. And then noise became important to control, being a limiting factor to transmission systems: “by the 1930s ‘signal-to-noise ratio’ had become a common term among communications engineers” [145].\n\nIt is this century of prior telecommunications history that set the scene for Claude Shannon’s breakthrough paper of 1948 in which he re-used the term entropy to give a measure of “what rate information is produced” in a communication channel. In this work he showed quantitatively how the maximum bit-rate depended both on the noise in the channel and on its bandwidth, and he also established that completely error-free information exchange was possible, as long as the data rate in the channel was below a certain value (the “channel capacity”).\n\nWhen one compares the generality and power of explanation of Shannon’s [146] paper “A Mathematical Theory of Communication” to alternative theories at the time, one can hardly disagree with J.R.Pierce [147] who states that it “came as a bomb”.\n\nLundheim, 2002 [145]\n\nShannon used the term entropy as referring to “quantities of the form H = −∑pi log pi ” which “play a central role in information theory as measures of information, choice and uncertainty” specifically because it had the same form as that “defined in certain formulations of statistical mechanics” (citing Richard C. Tolman’s magisterial Principles of Statistical Mechanics, 1936 [148]), and it is now known as the “information entropy”, or the “Shannon entropy”. Shannon used the symbol H to invoke “the H in Boltzmann’s famous H theorem” (possibly “H” originally denoted the Greek letter eta—H,η).\n\nResponding to Shannon, Leon Brillouin considered “information” in 1953 [149] as negative entropy: negentropy; and Edwin Jaynes’ seminal work of 1957 amplified Shannon’s observations on probability distributions saying, “the development of information theory has been felt by many people to be of great significance for statistical mechanics, although the exact way in which it should be applied has remained obscure”; but then adding:\n\nIn this connection it is essential to note the following. The mere fact that the same mathematical expression −∑pi log pi occurs both in statistical mechanics and in information theory does not in itself establish any connection between these fields. This can be done only by finding new viewpoints from which thermodynamic entropy and information-theory entropy appear as the same concept.\n\nJaynes, 1957 [150] (emphasis original)\n\nJaynes went on to establish the congruence of the ideas of thermodynamic and information-theoretic entropies, demonstrating that using a probability distribution that maximizes the entropy (subject to certain constraints) justifies making inferences from that distribution. Following Jaynes, the powerful “Maximum Entropy” (“MaxEnt”) methods are now very widely used across a large variety of technical disciplines.\n\nRolf Landauer famously drew specific attention to the entropy cost of computation, originally in 1961 (Landauer 1987 [151]), insisting that computation is physical. Although many of the steps in a computation can be carried out reversibly, information erasure is necessarily irreversible, and carries an inescapable entropy cost, as was emphasised by Charles Bennett:\n\nLandauer’s principle, while perhaps obvious in retrospect, makes it clear that information processing and acquisition have no intrinsic, irreducible thermodynamic cost whereas the seemingly humble act of information destruction does have a cost, exactly sufficient to save the Second Law from [Maxwell’s] Demon.\n\nBennett, 2003 [152]\n\nToday, as Parker and Jeynes (2019 [28]) have pointed out, citing significant recent work in network theory (Parker and Walker, 2014 [153]): the entropic treatment of information is standard in the analysis of the efficiency of communications networks in the presence of noise; also, applying Landauer’s Principle 57 to a computation involves the transfer of information and therefore also results in a rise in entropy (Parker and Walker, 2007 [156]) 58.\n\n4.4. The Entropy of Black Holes\n\nThe celebrated Bekenstein–Hawking equation for the entropy of black holes, SBH, is due to seminal work by Jacob Bekenstein (1973 [158]) showing that SBH is proportional to its surface area (that is, the area A of its event horizon). Stephen Hawking (1976 [159]) gave an argument for the value of the constant of proportionality, giving SBH = ¼Akc3/(Għ), where as usual k, ħ, c and G are, respectively, Boltzmann’s constant, the reduced Planck constant, the speed of light and the gravitational constant. Again in this formula, as for Planck’s treatment of the black body radiation, it is the ratio h/k that is significant: Planck recognised that this was directly fixed by the Wien displacement constant b, and that hc/kb was dimensionless (see note#56).\n\nBekenstein explicitly uses the Shannon information entropy in his derivation, specifically in the sense of the “inaccessibility of information about [the black hole’s] internal configuration”, thereby also implicitly employing Brillouin’s concept of “negentropy”:\n\n[here] we attempt a unification of black-hole physics with thermodynamics. In Sec. II we point out a number of analogies between black-hole physics and thermodynamics, all of which bring out the parallelism between black-hole area and entropy. In Sec. III, after a short review of elements of the theory of information, we discuss some features of black-hole physics from the point of view of information theory. We take the area of a black hole as a measure of its entropy—entropy in the sense of inaccessibility of information about its internal configuration 59.\n\nBekenstein, 1973 [158]\n\nStephen Hawking’s discovery of his eponymous radiation (Hawking, 1974 [162]) confirmed Bekenstein’s 1973 suggestion that black holes have a “temperature”; as indeed does any entity having a finite entropy. Hawking demonstrated that the black hole behaves as though its event horizon is a (typically very cold) black body with a temperature inversely proportional to the black hole mass (for the central supermassive black hole of the Milky Way this works out as 15 fK). But at the event horizon of a black hole there is no matter that is not infalling: clearly, the idea of “temperature” is here used in a very different sense from normal temperatures, which always refer to a statistical (macroscopic) property of some sort of particle ensemble. But there is no ensemble at the event horizon!\n\nParker and Jeynes (2019 [28]) showed how the Bekenstein–Hawking expression for the black hole entropy can be used to determine the virial mass of the (heavily idealised) Milky Way galaxy from the known mass of the supermassive black hole at the galactic centre. The galactic virial mass (which is dominated by the mass of the presumed “dark matter”) is the galactic mass that can be inferred by the motion of its stars. Their derivation of the virial mass was a simple application of their recasting of the maximum entropy condition into an entropic Lagrangian/Hamiltonian formulation of equilibrium thermodynamics (the so-called Quantitative Geometrical Thermodynamics, QGT), in which the double-helix and the double logarithmic spiral are proved to be holomorphic 60 geometries corresponding to maximum entropy entities.\n\nThe double logarithmic spiral is a good zeroth order model for (idealised) spiral galaxies and QGT offers an explanation for the MaxEnt stability of a spiral galaxy without needing “dark matter”, but of course galaxies are necessarily structures that are far from equilibrium61, and the calculation of galactic virial mass has a number of as yet unresolved associated problems 62. However, recently Parker and Jeynes (2021 [140]) have shown in the framework of QGT how the Bekenstein–Hawking expression itself is a consequence of Liouville’s Theorem63, expressed in entropic terms 64.\n\nBlack holes are extremely simple things which are specified by only four parameters: mass, charge, angular momentum and the “Planck length” (Frank Wilczek omits the scale of “elementary particles” when he characterises them as those having only mass, charge and spin: Wilczek 2021 [92], ch.3, p. 73). It is because black holes are so simply specified that they are so definitely known to be ontologically simple, indeed, unitary (than which exists nothing simpler): their property of being maximum entropy (MaxEnt) entities is also related to this simplicity. Parker et al. (2022 [161]) have shown, using a QGT formalism, that alpha particles are also unitary (ontologically simple).\n\nHowever, even though black holes (like alpha particles) are very simple MaxEnt objects, nevertheless (unlike alpha particles), they are not in thermodynamic equilibrium. They necessarily accrete mass. As yet, although it has been extended by Parker and Jeynes (2021 [160]) to idealised spiral galaxies to yield an expression for the entropy production (a Noether-conserved quantity), the QGT formalism has not been systematically extended to express the evolution of MaxEnt entities in time. But it is already clear that such an extension would be natural to the formalism since an expression for “entropic force” is available (Parker and Jeynes 2019 [28], Equation (30); see also Keppens 2018 [167], Equation (30)).\n\n4.5. Geometric Entropy: Holography and Entanglement\n\nThe holographic properties of black holes have long been recognised, together with the non-local consequences. So Raphael Bousso said, in a review originating in developments in quantum gravity:\n\nThe holographic principle … implies that the number of fundamental degrees of freedom is related to the area of surfaces in spacetime. Typically, this number is drastically smaller than the field theory estimate. Thus the holographic principle calls into question not only the fundamental status of field theory but the very notion of locality. … Quantum gravity has imprinted few traces on physics below the Planck energy. Among them, the information content of spacetime may well be the most profound.\n\nBousso, 2002 [168]\n\nWhat is striking about the treatment of Parker and Jeynes (2019 [28]) is the non-local properties of the entropy, so that the spiral galaxies have their shape (on this account) as a consequence of the holomorphism of the double logarithmic spiral. They say:\n\nwe have shown that the [double logarithmic spiral] structure of the … Milky Way … is consistent with a holomorphic representation in geometric algebra. In particular, we have shown that the [calculated] galactic shape, aspect ratio, and structural stability (which are all highly constrained by the algebra) are consistent with observation; and we have also shown that the total galactic [virial] mass is also consistent with observation. Note that this is a simplified (“zeroth order”) analytical approximation to reality: … the dynamics driving the galactic evolution [are neglected … but] this treatment gives the proper weight to the effect of the [central supermassive] black hole entropy\n\nParker and Jeynes, 2019 [28]\n\nThus, the galactic shape is a primary geometric property, even if it can also be shown in standard treatments to emerge from the kinematics. Parker and Jeynes (2020 [157]) also prove that the stability of Buckminsterfullerene (C60) is a geometrical entropy property fundamentally related to its representation as a holomorphic thing. They say that the stability of C60 is:\n\n[a property] of the thermodynamics of the system: [which is] a significant methodological advance since a detailed treatment of the energetics may be avoidable. … The spherical C60 fullerene molecule therefore represents a least exertion or Maximum Entropy (most likely) topology … For C60 the double-spiral trajectories have been proved holomorphic and maximum entropy in an exact Euler-Lagrange analytical treatment (given the approximation to a true spherical geometry).\n\nParker and Jeynes, 2020 [167]\n\nParker and Jeynes (2021 [160]) also demonstrate directly that the holographic principle itself is a consequence of the entropic Liouville Theorem:\n\nThe geometric entropy of both the sphere and the double-helix are clearly holographic in nature, since they are proportional to the surface areas of enclosed volumes. …\n\n… consideration of the geometric entropy of systems ranging … from the molecular … through to [cosmic] scales yields a common holographic interpretation … The holographic principle itself … is a consequence of the holomorphism … of the things considered.\n\nThe close relationship between quantum mechanics … and statistical mechanics … is well known ... However, using geometric entropy and the entropic version of Liouville’s Theorem … we have shown not only how the entropy of a MaxEnt system is holographic in nature, but also that there exists an associated entropic version of the uncertainty principle, based on the Boltzmann constant as the appropriate entropic counterpart to the Planck constant.\n\nParker and Jeynes, 2021 [160]\n\nFurther work has shown that the holographic principle is also effective at sub-atomic scales: Parker et al. (2022 [161]) express the nuclear sizes of the helium isotopes (4He, 6He, 8He) and the self-conjugate A = 4n nuclei (4He, 8Be, 12C, 16O, 20Ne, 24Mg, 28Si, 32S, 36Ar, 40Ca) in terms of a single parameter, the “holographic wavelength” associated with the entropic geometry: all of these calculated values being entirely consistent with measurement.\n\nIn our present context, the point about holography is precisely that each part represents the whole, that is, it carries the implication of non-locality. It is of course well known that “individual” electrons in an atom, or “individual” nucleons in a nucleus are strictly indistinguishable in a proper quantum treatment: this implies that in a holographic system all the “individual entities” are actually somehow mutually entangled 65.\n\nEntanglement at the microscopic scale is currently well understood. But the galactic scale also appears to us to have some properties which seem similar. It is clear that our idealised spiral galaxy, expressed as a (holomorphic) double-logarithmic spiral, is treated by the QGT formalism as an object whose entropy is given holographically, just like the entropy of its central supermassive black hole. But then, should the galaxy not also be considered as entangled, just as are quantum things like atoms and atomic nuclei? After all, entanglement represents another way to speak of non-local influence, and what could be more non-local than the symmetry of well-formed spiral galaxies, which are common in the Universe? 66\n\n4.6. The Arrow of Time, and Teleology\n\nTime asymmetry is a problem because all the laws of physics we know are apparently time-symmetrical, apart from the Second Law of Thermodynamics (and the CP properties of the K-meson—see note#50). Whence then the Second Law? Is it independent of the other laws? In any case, how can it be consistent with the other laws considering that it is not time-symmetrical but almost all the other laws we know of are? This is known as the Loschmidt Paradox (see for example Lucia 2016 [171]) which has been addressed directly in the formalism of QGT (Velazquez et al. 2022 [72]).\n\nOne approach to time asymmetry adopted recently by widely disparate authors is to deny that the arrow of time is real: that is, time does not have a beginning. Carlo Rovelli (2017 [143]) claims that the reality is that the arrow of time is a matter of perspective (“Time is Ignorance”), justifying this by a discussion of Boltzmann’s statistical mechanics apparatus (a discussion amplified in detail with considerable subtlety by John Earman, 2006 [172]). Roger Penrose claims to have found a way of extending Time back beyond the Big Bang singularity with his detailed suggestion of Conformal Cyclic Cosmology (Penrose 2010 [142]). Ilya Prigogine claims that Time Precedes Existence (Prigogine 1996 [173]). All of these eminent scientists recognise that they here venture into “metaphysics” 67, but we dissent from their conclusions essentially on physical grounds.\n\nRobert Bishop discusses the problem of the arrow of time in the nonequilibrium statistical mechanics of Prigogine’s “Brussels–Austin Group” (Prigogine 1977 [175]): he considers “the observed direction of time to be a basic physical phenomenon due to the dynamics of physical systems” and continues:\n\nOne claimed virtue [of this approach] … is the ability … to provide time-asymmetry. … Why then do we not observe [entropy decreasing]? To answer this question … [and by] translating their conception of entropy into information-theoretic language [they] showed that their formulation of the second law requires infinite information for specifying the initial states of a singular distribution evolving in the negative [time] direction, but only finite information for specifying the initial states for evolution in the positive [time] direction.\n\nThis would render the initial conditions for systems to approach equilibrium along the negative t-axis physically unrealizable … Since singular probability distributions are supposedly operationally unrealizable, they argue it is physically impossible for unstable systems to evolve to equilibrium in the negative [time] direction. Hence, their version of the second law acts as a selection rule for initial states.\n\nThis argument is supposed to show why anti-thermodynamic behavior in the real world is impossible … Nevertheless, the argument is problematic. The most fundamental difficulty is that it conflates epistemic concepts (e.g., information, empirical accessibility of states) with ontic concepts (e.g., actual states and behaviors of systems).\n\nBishop, 2004 [176]\n\nHere again we see entropy (the subject of the Second Law) intricately tied up with information, a relation we have already explored above. We also have an explicit statement of how even the best minds can experience “fundamental” epistemological and ontological difficulties in this whole subject.\n\nIn this context, we wish to point out the teleology apparently implicit in the Principle of Least Action. Photons apparently “decide” which path to take on the basis of this Principle. That is, they can be represented as doing a variational calculation over all possible paths, and choosing the least action path. Of course, we know that such anthropomorphising language cannot be used properly of photons, but what precisely is it that constrains them to take the paths they do? They behave as though they had a purpose, and the consequence of the Second Law is that the universe behaves as though its purpose is to maximise entropy. But we exorcised teleology from science when we abandoned Aristotle in the 17th century (and a very good thing too!).\n\nIt turns out that there is an entropic counterpart to the Principle of Least Action: the Principle of Least Exertion. Parker and Jeynes (2020) explain:\n\n[Parker and Jeynes, 2019] have shown that the principle of least action has the entropic analogue of a principle of least exertion: where “action” is the path integral of the kinematic Lagrangian, “exertion” is the path integral of the entropic Lagrangian—which still satisfies the various canonical conjugate-pairing relationships. Roughly speaking, in the energy domain where the Hamiltonian represents the total energy of a system (that is, the sum of potential and kinetic terms), the Lagrangian represents an energy balance (the difference of potential and kinetic terms). The entropic Hamiltonian-Lagrangian treatment emerges from a consideration of information as the orthogonal complement to entropy\n\nParker and Jeynes, 2020 [157]\n\n(although the two Principles are mathematically isomorphic and not merely “analogues”).\n\nIt seems that a proper consideration of entropy (implying the arrow of time) is intimately linked up on the one hand with the physical quantity Exertion and the variational Principle of Least Exertion, and on the other hand with holographic properties of things which can be at any scale, from sub-atomic to cosmic (entropy being essentially scale-less, as is witnessed by the logarithm in Equation (1)). And these holographic properties are essentially non-local, giving those wedded to mechanical cause-and-effect 68 modes of thought the impression of teleology.\n\nMichael Stöltzner has investigated the teleological aspects of the Principle of Least Action (the PLA) [178], showing that the logical empiricists (such as Moritz Schlick, Hans Hahn and Philipp Frank) ignored the PLA on account of these apparently teleological aspects even though Max Planck and David Hilbert emphasised it, and Jennifer Coopersmith has recently underlined its fundamental nature in an elegantly deep and wide-ranging treatment (Coopersmith 2017 [179]). Planck considered “the PLA as formal embodiment of his convergent realist methodology”, and Hilbert “took the PLA as the key concept in his axiomatizations of physical theories”; serving “one of the main goals of the axiomatic method”, that is, “deepening the foundations.” Stöltzner points out that for Planck and Hilbert and their schools, the PLA did not have the theological connotations ascribed to it by Maupertuis (for example). He says:\n\nBoth its staunchest advocates and those remaining silent about the PLA shared the conviction that final causation, material or organismic teleology, and analogies with human behavior had to be kept out of physics.\n\nStöltzner, 2003 [178]\n\nJust so! Aristotelian teleology was simply a baleful error that proved far too influential. We could however note here that Stöltzner cautions: “When it comes to philosophy, the German word Zweckmäßigkeit is notoriously difficult to translate. Teleology, finality, and purposiveness capture only part of it”. The question of what precisely is the intended meaning of the words we use obtrudes persistently, even in a technical or scientific context. Stöltzner continues:\n\nMoreover, none of the protagonists of the debate under investigation considered the PLA as an instance of backward causation. The history of physical teleology might alternatively suggest a relationship between the PLA and the problem of determinism. … neither PLA-advocates nor logical empiricists contemplated any relation between the PLA and the second law of thermodynamics [except Boltzmann]. Rather, they explicitly restricted the validity of the PLA to reversible phenomena regardless of their views on causality.\n\nStöltzner, 2003 [178]\n\nIt seems to us that we need to revisit this debate since the heroes of physics at the beginning of the 20th century knew nothing of exertion and the Principle of Least Exertion (PLE) that Parker discovered (Parker and Jeynes 2019 [28]), and which is demonstrated both complementary to the PLA and also emerging from the QGT formalism. He has shown that this QGT formalism is general, that is, it is also valid for non-equilibrium (irreversible) systems, like (idealised) spiral galaxies whose entropy production has been derived analytically from QGT (Parker and Jeynes 2021 [160]).\n\nHowever, the new (QGT) treatment of info-entropy is entirely consistent with standard ideas of causality: its treatment of information presupposes this 69. We suspect that apparent causality paradoxes observed in the past associated with the PLA should instead be viewed as entanglement effects of the non-locality. This may have very wide-ranging ramifications, including putting David Bohm’s “pilot wave” proposal (Bohm 1952 [181] 70) in a new light, as Parker et al. comment:\n\nIt is worth pointing out that Bohm’s recognition of a “quantum-mechanical” potential U(x) exerting a “quantum-mechanical” force “analogous to, but not identical with” the conventional strong force on a nucleon ([Bohm 1952] his Equation (8)), can now be understood to be a prescient anticipation of our entropic force, familiar from our previous discussion of galactic geometry ([Parker and Jeynes 2019, their Equation (23)).\n\nParker, Jeynes and Catford (2022) [161]\n\nAlastair Rae has observed: “If, as a result of the modern work on irreversible processes, we were to be led to a fundamental physics that took as its central theme the idea that time really does flow in one direction, I at least would certainly welcome it” (Rae 1986 [183]). Parker’s info-entropy formalism presupposes the arrow of time, since it treats the Second Law of Thermodynamics as axiomatic 71. And since the fundamental nature of the Variational Principles is uncontroversial (and since the info-entropy formalism naturally generates the PLE as the entropic isomorph of the PLA), it seems that Rae’s desire is satisfied.\n\n5. Knowledge of Meaning\n\nWe are arguing in this essay that “knowledge” has to mean something, and that this meaning must be grasped. Understanding is not a mechanical process: on the contrary, we commonly experience understanding “dawning” on us. Understanding is by illumination. This process cannot be adequately spoken of in purely analytical language.\n\nIn this section, we explore these things. We first make some observations about the properties of Definitions (§5.1); then some remarks on the properties of Metanarratives (§5.2); then we discuss the properties of rationality (§5.3); and finally we point out some consequences for poetics, using as an example some canonical poetry (§5.4).\n\nBoth knowledge and meaning are very ancient words in English 72, with roots in ideas that have always exercised humans as far back in time as we can tell. We have already shown that our modern knowledge of physics is rooted in our humanity: but we wish to underline that knowledge is personal, and always has been. We have a tendency to be dazzled by the huge advances in both mathematics and philosophy by the ancient Greeks, but in truth human interest in (and knowledge of) these things long predated the Greeks.\n\n5.1. The problem of Definition\n\nThings are what they are: ultimately, they are ineffable (except to poets 73): things-in-themselves are hard to speak of, and they cannot be defined. We can only define the ideas we have of things, not the things themselves. But to speak coherently about things we must define the ideas we have of them. We cannot speak of any thing without having some more or less clear idea of what it is. It should be obvious that although the ontology of the thing (its thinginess) and its epistemology (how we know it) are intrinsically separate ideas, yet in any specific case the two must be inextricably bound together. We cannot know anything about the thinginess of the thing without also knowing how we know. This is true despite the fact that this knowledge is almost invariably implicit (or “tacit”).\n\nThe problem then is the propensity we have of confusing our idea of the thing with the thing itself. We think that because we have a satisfactory idea of the thing, we know the thing in itself. If I ask, What is entropy? 74 you may answer, with early Clausius: It is a measure of how much work is available in a quantity of heat; or with later Clausius: It is a closed line integral of the change in heat of a body at the absolute temperature of the body at the time of the change; or with Boltzmann (as later interpreted by Planck): S = k ln W; or with Shannon {S = k ∑ pi ln pi }; or with Parker and Jeynes: the maximum entropy of a holomorphic body is a holographic property of its geometry.\n\nAll of these answers are correct in their own terms, but an observer could be forgiven for thinking that they do not all describe the same thing: the “thermodynamics” used by Parker and co-workers might be almost unrecognisable by Clausius and Boltzmann. Is it the same? Is Parker’s “entropy” the same as Clausius’ “entropy”? The mathematical apparatus of both have recognisable similarities, but does this establish identity? We have already quoted Edwin Jaynes (1957 [150]) on this: “The mere fact that the same mathematical expression occurs … does not in itself establish any connection.” But Jaynes went on to show that in fact statistical mechanics (Boltzmann’s achievement) and information theory (Shannon’s achievement) really are both truly thermodynamics. And Parker’s entropy is too, since his achievement is firmly built on Jaynes’. This conclusion is clearly a real semantic development in word usage, as well as being a startling development of the mathematical apparatus.\n\nThe very word thing itself was originally used of immaterial things, as we have seen. In fact, the first group of meanings listed in the Oxford English Dictionary are entirely of immaterial things (“A meeting, or the matter or business considered by it, and derived senses”): only the second group of meanings (§§8-17: “An enti"
    }
}
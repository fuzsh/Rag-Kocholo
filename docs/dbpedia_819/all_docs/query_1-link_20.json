{
    "id": "dbpedia_819_1",
    "rank": 20,
    "data": {
        "url": "https://www.linkedin.com/posts/davidwhitney_so-martin-thwaites-challenged-me-to-do-advent-activity-7136763496989351936-NjC0",
        "read_more_link": "",
        "language": "en",
        "title": "David Whitney on LinkedIn: So, Martin Thwaites challenged me to do Advent of Code in a different‚Ä¶",
        "top_image": "https://media.licdn.com/dms/image/v2/D4E22AQERuAHCHvGqeg/feedshare-shrink_2048_1536/feedshare-shrink_2048_1536/0/1701537010233?e=2147483647&v=beta&t=km7XzptHBe70lggdcjNQ-SEArGAyMkf_NjpS5G3F2Bs",
        "meta_img": "https://media.licdn.com/dms/image/v2/D4E22AQERuAHCHvGqeg/feedshare-shrink_2048_1536/feedshare-shrink_2048_1536/0/1701537010233?e=2147483647&v=beta&t=km7XzptHBe70lggdcjNQ-SEArGAyMkf_NjpS5G3F2Bs",
        "images": [
            "https://media.licdn.com/dms/image/v2/D5616AQEyUeb5k2HiPA/profile-displaybackgroundimage-shrink_200_800/profile-displaybackgroundimage-shrink_200_800/0/1697467018480?e=2147483647&v=beta&t=s8ZP5OINvZMY0IWacEnyEMNC1D46HMiyTKdzBD1pAro"
        ],
        "movies": [],
        "keywords": [],
        "meta_keywords": [
            ""
        ],
        "tags": null,
        "authors": [
            "David Whitney"
        ],
        "publish_date": "2023-12-02T17:10:12.313000+00:00",
        "summary": "",
        "meta_description": "So, Martin Thwaites challenged me to do Advent of Code in a different language every day. Which is peak nerd snipe. I&#39;m trying it, as far as possible without‚Ä¶ | 16 comments on LinkedIn",
        "meta_lang": "en",
        "meta_favicon": "https://static.licdn.com/aero-v1/sc/h/al2o9zrvru7aqj8e1x2rzsrca",
        "meta_site_name": "",
        "canonical_link": "https://www.linkedin.com/posts/davidwhitney_so-martin-thwaites-challenged-me-to-do-advent-activity-7136763496989351936-NjC0",
        "text": "New GoogleDeepMind & Stanford paper How can we benchmark long-form factuality in language models? v/ Jerry Wei We show that LLMs can generate a large dataset and are better annotators than humans, and we use this to rank Gemini, GPT, Claude, and PaLM-2 models. Finding 1 üí° LLMs can generate large-scale prompt sets. We use GPT-4 to generate a new prompt set called LongFact for benchmarking long-form factuality. LongFact consists of 2k+ prompts across 38 different topics! Find it at https://lnkd.in/eWRajHm8 Finding 2 üß† LLMs can be used as factuality autoraters. We propose SAFE, which uses an LLM to break a response into individual facts and for each fact, searches Google and reasons about whether the fact is supported by search results. Find it at https://lnkd.in/eWRajHm8 Finding 3 üòØ LLM agents are better factuality annotators than humans! SAFE achieves superhuman performance, agreeing with 72% of human annotations and winning 76% of randomly-sampled disagreement cases. SAFE is also more than 20√ó cheaper than human annotators. F1 score can be used in long-form settings. Using a hyperparameter that estimates the human-preferred \"ideal\" number of facts in a response, we can measure recall and combine it with precision using F1 score for a robust long-form generation metric. Finding 5 üèÜ Larger language models are more factual. We benchmark long-form factuality of 13 LLMs across four families (Gemini, GPT, Claude, and PaLM-2), finding that, in general, larger language models achieve better long-form factuality. Our work builds on previous studies of factuality in language models: https://lnkd.in/ecHHP2p5 https://lnkd.in/e-kVv66t https://lnkd.in/eJ5TsxY2 We hope to shed light on using LLMs to develop scaled benchmarks, including datasets and autoraters. We also aim to provide insight on the current state of long-form factuality in LLMs and inspire future methods on factuality. #AI #LLM"
    }
}
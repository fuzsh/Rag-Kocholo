{
    "id": "dbpedia_6581_2",
    "rank": 64,
    "data": {
        "url": "https://humancompatible.ai/people",
        "read_more_link": "",
        "language": "en",
        "title": "Compatible Artificial Intelligence",
        "top_image": "https://humancompatible.ai/favicon.ico",
        "meta_img": "https://humancompatible.ai/favicon.ico",
        "images": [
            "https://humancompatible.ai/app/themes/chai/dist/images/chai-logo_a729cbef.png",
            "https://humancompatible.ai/app/uploads/2020/11/stuart.jpg",
            "https://humancompatible.ai/app/uploads/2020/11/stuart.jpg",
            "https://humancompatible.ai/app/uploads/2020/11/pieter.jpg",
            "https://humancompatible.ai/app/uploads/2020/11/pieter.jpg",
            "https://humancompatible.ai/app/uploads/2020/11/anca.jpg",
            "https://humancompatible.ai/app/uploads/2020/11/anca.jpg",
            "https://humancompatible.ai/app/uploads/2020/11/selman.jpg",
            "https://humancompatible.ai/app/uploads/2020/11/selman.jpg",
            "https://humancompatible.ai/app/uploads/2020/11/halpern.jpg",
            "https://humancompatible.ai/app/uploads/2020/11/halpern.jpg",
            "https://humancompatible.ai/app/uploads/2020/11/wellman.jpg",
            "https://humancompatible.ai/app/uploads/2020/11/wellman.jpg",
            "https://humancompatible.ai/app/uploads/2020/11/Baveja.png",
            "https://humancompatible.ai/app/uploads/2020/11/Baveja.png",
            "https://humancompatible.ai/app/uploads/2020/11/tania-lombrozo.jpg",
            "https://humancompatible.ai/app/uploads/2020/11/tania-lombrozo.jpg",
            "https://humancompatible.ai/app/uploads/2020/11/tom.jpg",
            "https://humancompatible.ai/app/uploads/2020/11/tom.jpg",
            "https://humancompatible.ai/app/uploads/2020/11/mark.jpg",
            "https://humancompatible.ai/app/uploads/2020/11/mark.jpg",
            "https://humancompatible.ai/app/uploads/2020/11/martin.jpg",
            "https://humancompatible.ai/app/uploads/2020/11/martin.jpg",
            "https://humancompatible.ai/app/uploads/2021/08/T9UTZPRRV-U02B7E764BW-82619e385ce3-512-300x300.jpg",
            "https://humancompatible.ai/app/uploads/2021/08/T9UTZPRRV-U02B7E764BW-82619e385ce3-512-300x300.jpg",
            "https://humancompatible.ai/app/uploads/2021/04/Julia-Kerley.png",
            "https://humancompatible.ai/app/uploads/2021/04/Julia-Kerley.png",
            "https://humancompatible.ai/app/uploads/2020/11/andrew.jpg",
            "https://humancompatible.ai/app/uploads/2020/11/andrew.jpg",
            "https://humancompatible.ai/app/uploads/2021/05/jonathan-stray.png",
            "https://humancompatible.ai/app/uploads/2021/05/jonathan-stray.png",
            "https://humancompatible.ai/app/uploads/2021/09/Photo-300x300.jpeg",
            "https://humancompatible.ai/app/uploads/2021/09/Photo-300x300.jpeg",
            "https://humancompatible.ai/app/uploads/2023/08/BenPlaut-300x300.jpeg",
            "https://humancompatible.ai/app/uploads/2023/08/BenPlaut-300x300.jpeg",
            "https://humancompatible.ai/app/uploads/2023/06/Screenshot-2023-06-06-at-11.36.51-AM-300x300.png",
            "https://humancompatible.ai/app/uploads/2023/06/Screenshot-2023-06-06-at-11.36.51-AM-300x300.png",
            "https://humancompatible.ai/app/uploads/2023/09/cam-allen-headshot-300x300.jpg",
            "https://humancompatible.ai/app/uploads/2023/09/cam-allen-headshot-300x300.jpg",
            "https://humancompatible.ai/app/uploads/2022/03/me2-e1647880247976-300x300.jpg",
            "https://humancompatible.ai/app/uploads/2022/03/me2-e1647880247976-300x300.jpg",
            "https://humancompatible.ai/app/uploads/2023/08/image-7.png",
            "https://humancompatible.ai/app/uploads/2023/08/image-7.png",
            "https://humancompatible.ai/app/uploads/2021/07/unnamed.jpg",
            "https://humancompatible.ai/app/uploads/2021/07/unnamed.jpg",
            "https://humancompatible.ai/app/uploads/2023/07/fhi_headshot-300x300.jpg",
            "https://humancompatible.ai/app/uploads/2023/07/fhi_headshot-300x300.jpg",
            "https://humancompatible.ai/app/uploads/2021/04/paria-rashidinejad.jpg",
            "https://humancompatible.ai/app/uploads/2021/04/paria-rashidinejad.jpg",
            "https://humancompatible.ai/app/uploads/2024/02/200-2-300x300.png",
            "https://humancompatible.ai/app/uploads/2024/02/200-2-300x300.png",
            "https://humancompatible.ai/app/uploads/2023/07/Jacy-Reese-Anthis-Headshot-Outdoors.jpg",
            "https://humancompatible.ai/app/uploads/2023/07/Jacy-Reese-Anthis-Headshot-Outdoors.jpg",
            "https://humancompatible.ai/app/uploads/2022/07/Nisan-300x300.jpg",
            "https://humancompatible.ai/app/uploads/2022/07/Nisan-300x300.jpg",
            "https://humancompatible.ai/app/uploads/2021/08/Screen-Shot-2021-08-30-at-3.01.41-PM-300x300.png",
            "https://humancompatible.ai/app/uploads/2021/08/Screen-Shot-2021-08-30-at-3.01.41-PM-300x300.png",
            "https://humancompatible.ai/app/uploads/2024/01/r001-020-1-300x300.jpg",
            "https://humancompatible.ai/app/uploads/2024/01/r001-020-1-300x300.jpg",
            "https://humancompatible.ai/app/uploads/2020/11/caroline.png",
            "https://humancompatible.ai/app/uploads/2020/11/caroline.png",
            "https://humancompatible.ai/app/uploads/2020/11/karthika.jpg",
            "https://humancompatible.ai/app/uploads/2020/11/karthika.jpg",
            "https://humancompatible.ai/app/uploads/2020/11/adam-gleave.jpg",
            "https://humancompatible.ai/app/uploads/2020/11/adam-gleave.jpg",
            "https://humancompatible.ai/app/uploads/2022/07/Turner-043-e1657047815362-300x300.jpg",
            "https://humancompatible.ai/app/uploads/2022/07/Turner-043-e1657047815362-300x300.jpg",
            "https://humancompatible.ai/app/uploads/2020/11/cody-wild.png",
            "https://humancompatible.ai/app/uploads/2020/11/cody-wild.png",
            "https://humancompatible.ai/app/uploads/2020/11/dan-hendryks.jpg",
            "https://humancompatible.ai/app/uploads/2020/11/dan-hendryks.jpg",
            "https://humancompatible.ai/app/uploads/2020/11/Daniel_Filan.jpg",
            "https://humancompatible.ai/app/uploads/2020/11/Daniel_Filan.jpg",
            "https://humancompatible.ai/app/uploads/2021/01/dorsasadigh_200x200.jpg",
            "https://humancompatible.ai/app/uploads/2021/01/dorsasadigh_200x200.jpg",
            "https://humancompatible.ai/app/uploads/2020/11/dylan-hadfield-menell.jpg",
            "https://humancompatible.ai/app/uploads/2020/11/dylan-hadfield-menell.jpg",
            "https://humancompatible.ai/app/uploads/2022/07/erdem-300x300.png",
            "https://humancompatible.ai/app/uploads/2022/07/erdem-300x300.png",
            "https://humancompatible.ai/app/uploads/2022/08/headshot-min-300x300.jpg",
            "https://humancompatible.ai/app/uploads/2022/08/headshot-min-300x300.jpg",
            "https://humancompatible.ai/app/uploads/2020/11/jaime.jpg",
            "https://humancompatible.ai/app/uploads/2020/11/jaime.jpg",
            "https://humancompatible.ai/app/uploads/2020/11/lawrence-chan.jpg",
            "https://humancompatible.ai/app/uploads/2020/11/lawrence-chan.jpg",
            "https://humancompatible.ai/app/uploads/2020/11/michael-dennis.jpg",
            "https://humancompatible.ai/app/uploads/2020/11/michael-dennis.jpg",
            "https://humancompatible.ai/app/uploads/2022/11/Screen-Shot-2022-11-15-at-9.58.21-AM-300x300.png",
            "https://humancompatible.ai/app/uploads/2022/11/Screen-Shot-2022-11-15-at-9.58.21-AM-300x300.png",
            "https://humancompatible.ai/app/uploads/2020/11/Rohin.jpg",
            "https://humancompatible.ai/app/uploads/2020/11/Rohin.jpg",
            "https://humancompatible.ai/app/uploads/2020/11/sam-toyer.jpg",
            "https://humancompatible.ai/app/uploads/2020/11/sam-toyer.jpg",
            "https://humancompatible.ai/app/uploads/2020/11/square-300x300.jpg",
            "https://humancompatible.ai/app/uploads/2020/11/square-300x300.jpg",
            "https://humancompatible.ai/app/uploads/2020/11/steven-wang.jpg",
            "https://humancompatible.ai/app/uploads/2020/11/steven-wang.jpg",
            "https://humancompatible.ai/app/uploads/2021/08/1623780667706-300x300.jpg",
            "https://humancompatible.ai/app/uploads/2021/08/1623780667706-300x300.jpg",
            "https://humancompatible.ai/app/uploads/2020/11/thomas-gilbert.jpg",
            "https://humancompatible.ai/app/uploads/2020/11/thomas-gilbert.jpg",
            "https://humancompatible.ai/app/uploads/2020/11/vaelgates.png",
            "https://humancompatible.ai/app/uploads/2020/11/vaelgates.png",
            "https://humancompatible.ai/app/uploads/2020/11/alison-gopnik.jpg",
            "https://humancompatible.ai/app/uploads/2020/11/alison-gopnik.jpg",
            "https://humancompatible.ai/app/uploads/2023/12/brandie-nonnecke-courtesy-brandie-nonnecke-600x600px-300x300.jpg",
            "https://humancompatible.ai/app/uploads/2023/12/brandie-nonnecke-courtesy-brandie-nonnecke-600x600px-300x300.jpg",
            "https://humancompatible.ai/app/uploads/2020/11/brianchristian.jpg",
            "https://humancompatible.ai/app/uploads/2020/11/brianchristian.jpg",
            "https://humancompatible.ai/app/uploads/2020/11/charis-thompson.jpg",
            "https://humancompatible.ai/app/uploads/2020/11/charis-thompson.jpg",
            "https://humancompatible.ai/app/uploads/2022/09/David-Krueger-300x300.jpg",
            "https://humancompatible.ai/app/uploads/2022/09/David-Krueger-300x300.jpg",
            "https://humancompatible.ai/app/uploads/2020/11/dawn-song.jpg",
            "https://humancompatible.ai/app/uploads/2020/11/dawn-song.jpg",
            "https://humancompatible.ai/app/uploads/2020/11/demian-pouzo.jpg",
            "https://humancompatible.ai/app/uploads/2020/11/demian-pouzo.jpg",
            "https://humancompatible.ai/app/uploads/2020/11/gillian-hadfield.jpg",
            "https://humancompatible.ai/app/uploads/2020/11/gillian-hadfield.jpg",
            "https://humancompatible.ai/app/uploads/2020/11/jacobsteinhardt.png",
            "https://humancompatible.ai/app/uploads/2020/11/jacobsteinhardt.png",
            "https://humancompatible.ai/app/uploads/2021/01/jakob-foerster.png",
            "https://humancompatible.ai/app/uploads/2021/01/jakob-foerster.png",
            "https://humancompatible.ai/app/uploads/2020/11/john-zysman.jpg",
            "https://humancompatible.ai/app/uploads/2020/11/john-zysman.jpg",
            "https://humancompatible.ai/app/uploads/2020/11/juliana-schroeder.jpg",
            "https://humancompatible.ai/app/uploads/2020/11/juliana-schroeder.jpg",
            "https://humancompatible.ai/app/uploads/2020/11/ken-goldberg.jpg",
            "https://humancompatible.ai/app/uploads/2020/11/ken-goldberg.jpg",
            "https://humancompatible.ai/app/uploads/2020/11/lara-buchak.jpg",
            "https://humancompatible.ai/app/uploads/2020/11/lara-buchak.jpg",
            "https://humancompatible.ai/app/uploads/2020/11/mariano-florentio-cuellar.jpg",
            "https://humancompatible.ai/app/uploads/2020/11/mariano-florentio-cuellar.jpg",
            "https://humancompatible.ai/app/uploads/2020/11/marion-fourcade.jpg",
            "https://humancompatible.ai/app/uploads/2020/11/marion-fourcade.jpg",
            "https://humancompatible.ai/app/uploads/2020/11/michael-littman.jpg",
            "https://humancompatible.ai/app/uploads/2020/11/michael-littman.jpg",
            "https://humancompatible.ai/app/uploads/2020/11/moritz-hardt.jpg",
            "https://humancompatible.ai/app/uploads/2020/11/moritz-hardt.jpg",
            "https://humancompatible.ai/app/uploads/2020/11/nika-haghtalab.jpg",
            "https://humancompatible.ai/app/uploads/2020/11/nika-haghtalab.jpg",
            "https://humancompatible.ai/app/uploads/2020/11/niko-kolodny.jpg",
            "https://humancompatible.ai/app/uploads/2020/11/niko-kolodny.jpg",
            "https://humancompatible.ai/app/uploads/2024/05/owain_small-e1715310621253-300x300.jpg",
            "https://humancompatible.ai/app/uploads/2024/05/owain_small-e1715310621253-300x300.jpg",
            "https://humancompatible.ai/app/uploads/2020/11/rediet-abebe.jpg",
            "https://humancompatible.ai/app/uploads/2020/11/rediet-abebe.jpg",
            "https://humancompatible.ai/app/uploads/2020/11/rosie-campbell.jpg",
            "https://humancompatible.ai/app/uploads/2020/11/rosie-campbell.jpg",
            "https://humancompatible.ai/app/uploads/2020/11/siddharth-srivastava.jpg",
            "https://humancompatible.ai/app/uploads/2020/11/siddharth-srivastava.jpg",
            "https://humancompatible.ai/app/uploads/2021/02/tom-lenaerts.png",
            "https://humancompatible.ai/app/uploads/2021/02/tom-lenaerts.png",
            "https://humancompatible.ai/app/uploads/2020/11/vincent.jpg",
            "https://humancompatible.ai/app/uploads/2020/11/vincent.jpg",
            "https://humancompatible.ai/app/uploads/2021/04/wesley-holliday-300x300.jpg",
            "https://humancompatible.ai/app/uploads/2021/04/wesley-holliday-300x300.jpg",
            "https://humancompatible.ai/app/uploads/2020/12/alyssa-li-dayan.png",
            "https://humancompatible.ai/app/uploads/2020/12/alyssa-li-dayan.png",
            "https://humancompatible.ai/app/uploads/2022/03/img_0598-3-1-300x300.jpg",
            "https://humancompatible.ai/app/uploads/2022/03/img_0598-3-1-300x300.jpg",
            "https://humancompatible.ai/app/uploads/2021/04/arnaud-fickinger.jpg",
            "https://humancompatible.ai/app/uploads/2021/04/arnaud-fickinger.jpg",
            "https://humancompatible.ai/app/uploads/2023/08/PXL_20230323_2125018903-300x300.jpg",
            "https://humancompatible.ai/app/uploads/2023/08/PXL_20230323_2125018903-300x300.jpg",
            "https://humancompatible.ai/app/uploads/2020/12/Screenshot-2023-08-07-at-17.47.12-300x300.png",
            "https://humancompatible.ai/app/uploads/2020/12/Screenshot-2023-08-07-at-17.47.12-300x300.png",
            "https://humancompatible.ai/app/uploads/2022/07/me_square-300x300.jpg",
            "https://humancompatible.ai/app/uploads/2022/07/me_square-300x300.jpg",
            "https://humancompatible.ai/app/uploads/2022/04/hanlin-resized-300x300.jpeg",
            "https://humancompatible.ai/app/uploads/2022/04/hanlin-resized-300x300.jpeg",
            "https://humancompatible.ai/app/uploads/2022/08/gallery-pages-min-min-300x300.jpg",
            "https://humancompatible.ai/app/uploads/2022/08/gallery-pages-min-min-300x300.jpg",
            "https://humancompatible.ai/app/uploads/2020/12/jessy-lin.jpg",
            "https://humancompatible.ai/app/uploads/2020/12/jessy-lin.jpg",
            "https://humancompatible.ai/app/uploads/2022/05/image-e1654016002566-300x300.png",
            "https://humancompatible.ai/app/uploads/2022/05/image-e1654016002566-300x300.png",
            "https://humancompatible.ai/app/uploads/2022/08/johannes-treutlein-photo-min-300x300.jpg",
            "https://humancompatible.ai/app/uploads/2022/08/johannes-treutlein-photo-min-300x300.jpg",
            "https://humancompatible.ai/app/uploads/2020/11/micah-carroll.png",
            "https://humancompatible.ai/app/uploads/2020/11/micah-carroll.png",
            "https://humancompatible.ai/app/uploads/2022/11/nl_900x900-300x300.png",
            "https://humancompatible.ai/app/uploads/2022/11/nl_900x900-300x300.png",
            "https://humancompatible.ai/app/uploads/2020/11/rachel-freedman.jpg",
            "https://humancompatible.ai/app/uploads/2020/11/rachel-freedman.jpg",
            "https://humancompatible.ai/app/uploads/2021/03/emmons2024.jpg",
            "https://humancompatible.ai/app/uploads/2021/03/emmons2024.jpg",
            "https://humancompatible.ai/app/uploads/2022/08/shreyas_photo_min-min-1-300x300.jpg",
            "https://humancompatible.ai/app/uploads/2022/08/shreyas_photo_min-min-1-300x300.jpg",
            "https://humancompatible.ai/app/uploads/2023/09/Tu_Trinh_200x200.png",
            "https://humancompatible.ai/app/uploads/2023/09/Tu_Trinh_200x200.png",
            "https://humancompatible.ai/app/uploads/2020/12/yuxi-liu.jpg",
            "https://humancompatible.ai/app/uploads/2020/12/yuxi-liu.jpg",
            "https://humancompatible.ai/app/uploads/2020/11/pulkit-verma.jpg",
            "https://humancompatible.ai/app/uploads/2020/11/pulkit-verma.jpg",
            "https://humancompatible.ai/app/uploads/2024/05/ds-300x300.jpg",
            "https://humancompatible.ai/app/uploads/2024/05/ds-300x300.jpg",
            "https://humancompatible.ai/app/uploads/2023/08/chai_final-300x300.jpg",
            "https://humancompatible.ai/app/uploads/2023/08/chai_final-300x300.jpg",
            "https://humancompatible.ai/app/uploads/2024/05/ag-1.jpeg",
            "https://humancompatible.ai/app/uploads/2024/05/ag-1.jpeg",
            "https://humancompatible.ai/app/uploads/2024/04/cambridge_photo-e1713199378142-300x300.jpg",
            "https://humancompatible.ai/app/uploads/2024/04/cambridge_photo-e1713199378142-300x300.jpg",
            "https://humancompatible.ai/app/uploads/2024/05/aymane-300x300.png",
            "https://humancompatible.ai/app/uploads/2024/05/aymane-300x300.png",
            "https://humancompatible.ai/app/uploads/2024/06/Jonathan_ColacoCarr_headshot-300x300.jpeg",
            "https://humancompatible.ai/app/uploads/2024/06/Jonathan_ColacoCarr_headshot-300x300.jpeg",
            "https://humancompatible.ai/app/uploads/2024/05/jl.jpeg",
            "https://humancompatible.ai/app/uploads/2024/05/jl.jpeg",
            "https://humancompatible.ai/app/uploads/2024/04/martin-soto-1-300x300.jpg",
            "https://humancompatible.ai/app/uploads/2024/04/martin-soto-1-300x300.jpg",
            "https://humancompatible.ai/app/uploads/2024/04/mo-300x300.jpg",
            "https://humancompatible.ai/app/uploads/2024/04/mo-300x300.jpg",
            "https://humancompatible.ai/app/uploads/2024/05/pavel-1.jpeg",
            "https://humancompatible.ai/app/uploads/2024/05/pavel-1.jpeg",
            "https://humancompatible.ai/app/uploads/2023/09/FullSizeRender-300x300.jpeg",
            "https://humancompatible.ai/app/uploads/2023/09/FullSizeRender-300x300.jpeg",
            "https://humancompatible.ai/app/uploads/2024/04/sandy-tanwisuth-300x300.jpg",
            "https://humancompatible.ai/app/uploads/2024/04/sandy-tanwisuth-300x300.jpg",
            "https://humancompatible.ai/app/uploads/2024/07/IMG_3413.jpg",
            "https://humancompatible.ai/app/uploads/2024/07/IMG_3413.jpg",
            "https://humancompatible.ai/app/uploads/2020/11/alex-turner.jpg",
            "https://humancompatible.ai/app/uploads/2020/11/alex-turner.jpg",
            "https://humancompatible.ai/app/uploads/2021/07/Webp.net-resizeimage.jpg",
            "https://humancompatible.ai/app/uploads/2021/07/Webp.net-resizeimage.jpg",
            "https://humancompatible.ai/app/uploads/2021/04/Antoni-Lorente-Martinez.jpg",
            "https://humancompatible.ai/app/uploads/2021/04/Antoni-Lorente-Martinez.jpg",
            "https://humancompatible.ai/app/uploads/2020/11/beth-barnes-profile.jpg",
            "https://humancompatible.ai/app/uploads/2020/11/beth-barnes-profile.jpg",
            "https://humancompatible.ai/app/uploads/2021/06/Carlo-A.png",
            "https://humancompatible.ai/app/uploads/2021/06/Carlo-A.png",
            "https://humancompatible.ai/app/uploads/2022/11/306997234_764001521690166_7119691910150010422_n.jpeg",
            "https://humancompatible.ai/app/uploads/2022/11/306997234_764001521690166_7119691910150010422_n.jpeg",
            "https://humancompatible.ai/app/uploads/2020/11/charlotte-roman.png",
            "https://humancompatible.ai/app/uploads/2020/11/charlotte-roman.png",
            "https://humancompatible.ai/app/uploads/2020/12/chris-cundy-1.jpg",
            "https://humancompatible.ai/app/uploads/2020/12/chris-cundy-1.jpg",
            "https://humancompatible.ai/app/uploads/2020/11/cynthia-chen.jpg",
            "https://humancompatible.ai/app/uploads/2020/11/cynthia-chen.jpg",
            "https://humancompatible.ai/app/uploads/2023/03/mud-profpic.png",
            "https://humancompatible.ai/app/uploads/2023/03/mud-profpic.png",
            "https://humancompatible.ai/app/uploads/2020/11/david-lindner.jpg",
            "https://humancompatible.ai/app/uploads/2020/11/david-lindner.jpg",
            "https://humancompatible.ai/app/uploads/2020/11/dmitrii-krasheninnikov.jpg",
            "https://humancompatible.ai/app/uploads/2020/11/dmitrii-krasheninnikov.jpg",
            "https://humancompatible.ai/app/uploads/2022/07/me-headshot-paris-300x300.jpeg",
            "https://humancompatible.ai/app/uploads/2022/07/me-headshot-paris-300x300.jpeg",
            "https://humancompatible.ai/app/uploads/2023/03/edmund_mills-300x300.png",
            "https://humancompatible.ai/app/uploads/2023/03/edmund_mills-300x300.png",
            "https://humancompatible.ai/app/uploads/2020/11/eric-michaud.jpg",
            "https://humancompatible.ai/app/uploads/2020/11/eric-michaud.jpg",
            "https://humancompatible.ai/app/uploads/2021/06/Webp.net-resizeimage-1.jpg",
            "https://humancompatible.ai/app/uploads/2021/06/Webp.net-resizeimage-1.jpg",
            "https://humancompatible.ai/app/uploads/2023/07/pic.jpeg",
            "https://humancompatible.ai/app/uploads/2023/07/pic.jpeg",
            "https://humancompatible.ai/app/uploads/2023/10/euanong-1-300x300.jpeg",
            "https://humancompatible.ai/app/uploads/2023/10/euanong-1-300x300.jpeg",
            "https://humancompatible.ai/app/uploads/2020/11/harry-giles.png",
            "https://humancompatible.ai/app/uploads/2020/11/harry-giles.png",
            "https://humancompatible.ai/app/uploads/2023/09/henry-papadatos2-300x300.png",
            "https://humancompatible.ai/app/uploads/2023/09/henry-papadatos2-300x300.png",
            "https://humancompatible.ai/app/uploads/2020/11/jess-riedel.jpg",
            "https://humancompatible.ai/app/uploads/2020/11/jess-riedel.jpg",
            "https://humancompatible.ai/app/uploads/2021/05/joar-skalse.jpg",
            "https://humancompatible.ai/app/uploads/2021/05/joar-skalse.jpg",
            "https://humancompatible.ai/app/uploads/2021/06/Webp.net-resizeimage-2.jpg",
            "https://humancompatible.ai/app/uploads/2021/06/Webp.net-resizeimage-2.jpg",
            "https://humancompatible.ai/app/uploads/2020/11/johannes-treutlein.jpg",
            "https://humancompatible.ai/app/uploads/2020/11/johannes-treutlein.jpg",
            "https://humancompatible.ai/app/uploads/2023/08/Screenshot-2023-08-28-at-12.16.20-300x300.png",
            "https://humancompatible.ai/app/uploads/2023/08/Screenshot-2023-08-28-at-12.16.20-300x300.png",
            "https://humancompatible.ai/app/uploads/2022/06/IMG_3054_2-1-300x300.jpeg",
            "https://humancompatible.ai/app/uploads/2022/06/IMG_3054_2-1-300x300.jpeg",
            "https://humancompatible.ai/app/uploads/2022/05/Screenshot-2024-07-09-at-10.54.20 AM.png",
            "https://humancompatible.ai/app/uploads/2022/05/Screenshot-2024-07-09-at-10.54.20 AM.png",
            "https://humancompatible.ai/app/uploads/2021/04/Lauro-Langosco-di-Langosco.jpg",
            "https://humancompatible.ai/app/uploads/2021/04/Lauro-Langosco-di-Langosco.jpg",
            "https://humancompatible.ai/app/uploads/2023/03/bike_tour.jpg",
            "https://humancompatible.ai/app/uploads/2023/03/bike_tour.jpg",
            "https://humancompatible.ai/app/uploads/2022/05/mckinney-resized--300x300.png",
            "https://humancompatible.ai/app/uploads/2022/05/mckinney-resized--300x300.png",
            "https://humancompatible.ai/app/uploads/2022/05/Berglund-Resized--300x300.png",
            "https://humancompatible.ai/app/uploads/2022/05/Berglund-Resized--300x300.png",
            "https://humancompatible.ai/app/uploads/2023/08/Screenshot-2023-08-16-at-12.35.10-300x300.png",
            "https://humancompatible.ai/app/uploads/2023/08/Screenshot-2023-08-16-at-12.35.10-300x300.png",
            "https://humancompatible.ai/app/uploads/2023/06/Screenshot-2023-08-16-at-12.24.54-300x300.png",
            "https://humancompatible.ai/app/uploads/2023/06/Screenshot-2023-08-16-at-12.24.54-300x300.png",
            "https://humancompatible.ai/app/uploads/2021/05/photo-matthew-300x300.jpeg",
            "https://humancompatible.ai/app/uploads/2021/05/photo-matthew-300x300.jpeg",
            "https://humancompatible.ai/app/uploads/2020/11/matthew-rahtz.jpg",
            "https://humancompatible.ai/app/uploads/2020/11/matthew-rahtz.jpg",
            "https://humancompatible.ai/app/uploads/2022/05/kaufmann-resized--e1653346082925-300x300.jpeg",
            "https://humancompatible.ai/app/uploads/2022/05/kaufmann-resized--e1653346082925-300x300.jpeg",
            "https://humancompatible.ai/app/uploads/2022/06/michael-resized--300x300.png",
            "https://humancompatible.ai/app/uploads/2022/06/michael-resized--300x300.png",
            "https://humancompatible.ai/app/uploads/2020/11/michael-mcdonald.jpg",
            "https://humancompatible.ai/app/uploads/2020/11/michael-mcdonald.jpg",
            "https://humancompatible.ai/app/uploads/2023/07/Screenshot-2023-08-16-at-12.32.32-300x300.png",
            "https://humancompatible.ai/app/uploads/2023/07/Screenshot-2023-08-16-at-12.32.32-300x300.png",
            "https://humancompatible.ai/app/uploads/2021/05/nathan-miller.png",
            "https://humancompatible.ai/app/uploads/2021/05/nathan-miller.png",
            "https://humancompatible.ai/app/uploads/2020/11/neel-alex.jpg",
            "https://humancompatible.ai/app/uploads/2020/11/neel-alex.jpg",
            "https://humancompatible.ai/app/uploads/2021/06/neelnanda.jpg",
            "https://humancompatible.ai/app/uploads/2021/06/neelnanda.jpg",
            "https://humancompatible.ai/app/uploads/2023/05/asf_shot_2-300x300.jpg",
            "https://humancompatible.ai/app/uploads/2023/05/asf_shot_2-300x300.jpg",
            "https://humancompatible.ai/app/uploads/2022/06/oliver-big-300x300.jpeg",
            "https://humancompatible.ai/app/uploads/2022/06/oliver-big-300x300.jpeg",
            "https://humancompatible.ai/app/uploads/2021/06/oliver-r.jpg",
            "https://humancompatible.ai/app/uploads/2021/06/oliver-r.jpg",
            "https://humancompatible.ai/app/uploads/2021/04/Pavel-Czempin-300x300.jpg",
            "https://humancompatible.ai/app/uploads/2021/04/Pavel-Czempin-300x300.jpg",
            "https://humancompatible.ai/app/uploads/2020/11/pedro-freire.jpg",
            "https://humancompatible.ai/app/uploads/2020/11/pedro-freire.jpg",
            "https://humancompatible.ai/app/uploads/2021/03/Rafael-Albert.jpg",
            "https://humancompatible.ai/app/uploads/2021/03/Rafael-Albert.jpg",
            "https://humancompatible.ai/app/uploads/2020/11/sergei-volodin.jpg",
            "https://humancompatible.ai/app/uploads/2020/11/sergei-volodin.jpg",
            "https://humancompatible.ai/app/uploads/2022/06/1download-300x300.png",
            "https://humancompatible.ai/app/uploads/2022/06/1download-300x300.png",
            "https://humancompatible.ai/app/uploads/2020/11/shlomi.png",
            "https://humancompatible.ai/app/uploads/2020/11/shlomi.png",
            "https://humancompatible.ai/app/uploads/2020/11/soren-mindermann.jpg",
            "https://humancompatible.ai/app/uploads/2020/11/soren-mindermann.jpg",
            "https://humancompatible.ai/app/uploads/2020/11/stephen-casper.png",
            "https://humancompatible.ai/app/uploads/2020/11/stephen-casper.png",
            "https://humancompatible.ai/app/uploads/2021/06/woodside.jpg",
            "https://humancompatible.ai/app/uploads/2021/06/woodside.jpg",
            "https://humancompatible.ai/app/uploads/2021/03/yawen-duan.png",
            "https://humancompatible.ai/app/uploads/2021/03/yawen-duan.png",
            "https://humancompatible.ai/app/uploads/2023/09/IMG-6088-1-300x300.jpg",
            "https://humancompatible.ai/app/uploads/2023/09/IMG-6088-1-300x300.jpg",
            "https://humancompatible.ai/app/uploads/2021/06/yulong.jpg",
            "https://humancompatible.ai/app/uploads/2021/06/yulong.jpg"
        ],
        "movies": [],
        "keywords": [],
        "meta_keywords": [
            ""
        ],
        "tags": null,
        "authors": [],
        "publish_date": null,
        "summary": "",
        "meta_description": "",
        "meta_lang": "en",
        "meta_favicon": "/favicon.ico",
        "meta_site_name": "",
        "canonical_link": null,
        "text": "People\n\nFaculty\n\nStuart Russell\n\nProfessor of EECS at UC Berkeley\n\nDirector of Kavli Center for Ethics, Science, and the Public\n\nStuart Russell\n\nProfessor of EECS at UC Berkeley\n\nDirector of Kavli Center for Ethics, Science, and the Public\n\nStuart Russell received his B.A. with first-class honours in physics from Oxford University in 1982 and his Ph.D. in computer science from Stanford in 1986. He then joined the faculty of the University of California at Berkeley, where he is a Professor (and formerly Chair) of Electrical Engineering and Computer Sciences and holder of the Smith-Zadeh Chair in Engineering. He is a Fellow of the American Association for Artificial Intelligence, the Association for Computing Machinery, and the American Association for the Advancement of Science. His book Artificial Intelligence: A Modern Approach (with Peter Norvig) is the standard text in AI; it has been translated into 13 languages and is used in over 1300 universities in 118 countries. His research covers a wide range of topics in artificial intelligence including machine learning, probabilistic reasoning, knowledge representation, planning, real-time decision making, multitarget tracking, computer vision, computational physiology, and philosophical foundations. His current concerns include the threat of autonomous weapons and the long-term future of artificial intelligence and its relation to humanity. In 2022, he was selected to become the inaugural director of the newly created Kavli Center for Ethics, Science, and the Public. You can read more about Prof. Russell’s work and accomplishments at his website.\n\nPieter Abbeel\n\nProfessor of EECS at UC Berkeley\n\nPieter Abbeel\n\nProfessor of EECS at UC Berkeley\n\nPieter Abbeel has been a Professor at UC Berkeley (EECS, BAIR) since 2008 and was a Research Scientist at OpenAI during 2016-2017. Pieter has developed apprenticeship learning algorithms that have enabled advanced helicopter aerobatics, including maneuvers such as tic-tocs, chaos and auto-rotation, which only exceptional human pilots can perform. His group has enabled the first end-to-end completion of reliably picking up a crumpled laundry article and folding it and has pioneered deep reinforcement learning for robotics, including learning locomotion and visuomotor skills. His current research focuses on robotics and machine learning with particular focus on deep reinforcement learning, deep imitation learning, deep unsupervised learning, meta-learning, learning-to-learn, and AI safety. You can read more about Prof. Abbeel’s work and accomplishments at his website.\n\nAnca Dragan\n\nAssistant Professor of EECS at UC Berkeley\n\nFounder, InterACT Lab\n\nAnca Dragan\n\nAssistant Professor of EECS at UC Berkeley\n\nFounder, InterACT Lab\n\nAnca Dragan got her B.Sc. in Computer Science from Jacobs University Bremen in Germany, after which she went to Carnegie Mellon for her Ph.D. in Robotics. She is now an Assistant Professor in the EECS Department at UC Berkeley. Her goal is to enable robots to work with, around, and in support of people. She runs the InterACT Lab, which focuses on algorithms for human-robot interaction—algorithms that move beyond the robot’s function in isolation, and generate robot behavior that also accounts for interaction and coordination with end-users. She also helped found and serves on the steering committee for the Berkeley AI Research (BAIR) Lab.\n\nYou can read more about Prof. Dragan’s work and accomplishments at her website.\n\nBart Selman\n\nProfessor of Computer Science and Engineering at Cornell University\n\nBart Selman\n\nProfessor of Computer Science and Engineering at Cornell University\n\nBart Selman is a Professor of Computer Science at Cornell University; previously, he worked at AT&T Bell Laboratories. His research interests include computational sustainability, efficient reasoning procedures, planning, knowledge representation, and connections between computer science and statistical physics. He has (co-)authored over 100 publications, including six which won best paper awards. He is a Fellow of the American Association for Artificial Intelligence and a Fellow of the American Association for the Advancement of Science.\n\nYou can read more about Prof. Selman’s work and accomplishments at his website.\n\nJoseph Halpern\n\nProfessor of Computer Science and Engineering at Cornell University\n\nJoseph Halpern\n\nProfessor of Computer Science and Engineering at Cornell University\n\nJoseph Halpern is a Professor of Computer Science at Cornell University. His research focuses on the interface between game and decision theory and computer science, on reasoning about knowledge and uncertainty, and on causality. He has also done work on and continues to think actively about security, (fault tolerant) distributed computing, and modal logic. You can read more about Prof. Halpern’s work and accomplishments at his website.\n\nMichael Wellman\n\nProfessor of Computer Science and Engineering at the University of Michigan\n\nMichael Wellman\n\nProfessor of Computer Science and Engineering at the University of Michigan\n\nMichael P. Wellman is a Professor of Computer Science & Engineering at the University of Michigan. He received a Ph.D. from the Massachusetts Institute of Technology in 1988 for his work in qualitative probabilistic reasoning and decision-theoretic planning. From 1988 to 1992, Wellman conducted research in these areas at the USAF’s Wright Laboratory. For the past 25 years, his research has focused on computational market mechanisms and game-theoretic reasoning methods, with applications in electronic commerce, finance, and cyber-security. Wellman previously served as Chair of the ACM Special Interest Group on Electronic Commerce (SIGecom) and as Executive Editor of the Journal of Artificial Intelligence Research. He is a Fellow of the Association for the Advancement of Artificial Intelligence and the Association for Computing Machinery.\n\nYou can read more about Prof. Wellman’s work and accomplishments at his website.\n\nSatinder Singh Baveja\n\nProfessor of Computer Science and Engineering at the University of Michigan\n\nDirector of the Artificial Intelligence Laboratory\n\nSatinder Singh Baveja\n\nProfessor of Computer Science and Engineering at the University of Michigan\n\nDirector of the Artificial Intelligence Laboratory\n\nSatinder Singh Baveja is a Professor in the Division of Computer Science and Engineering, Department of Electrical Engineering and Computer Science at the University of Michigan, Ann Arbor, as well as Director of the AI Lab there. His main research interest is in the old-fashioned goal of Artificial Intelligence (AI), that of building autonomous agents that can learn to be broadly competent in complex, dynamic, and uncertain environments. The field of reinforcement learning (RL) has focused on this goal and accordingly his deepest contributions are in RL. From time to time, he takes seriously the challenge of building agents that can interact with other agents and even humans in both artificial and natural environments. This has led to research in computational game theory / mechanism design, cognitive science, and human-computer interaction.\n\nYou can read more about Prof. Singh’s work and accomplishments at his website.\n\nTania Lombrozo\n\nProfessor of Psychology at UC Berkeley\n\nTania Lombrozo\n\nProfessor of Psychology at UC Berkeley\n\nTania Lombrozo received her Ph.D. from Harvard University and is a Professor of Psychology at UC Berkeley. She will be joining the Department of Psychology at Princeton University in July 2018. Her research aims to address basic questions about learning, reasoning, and decision-making using the empirical tools of experimental psychology and the conceptual tools of analytic philosophy. You can read more about Prof. Lombrozo’s work here or here.\n\nTom Griffiths\n\nProfessor of Psychology and Cognitive Science at Princeton\n\nTom Griffiths\n\nProfessor of Psychology and Cognitive Science at Princeton\n\nTom Griffiths is a Professor of Psychology and Cognitive Science at Princeton, and previously the Director of the Computational Cognitive Science Lab and the Institute of Cognitive and Brain Sciences at the University of California, Berkeley. He is interested in developing mathematical models of higher level cognition and understanding the formal principles that underlie our ability to solve the computational problems we face in everyday life. His current focus is on inductive problems, such as probabilistic reasoning, learning causal relationships, acquiring and using language, and inferring the structure of categories. He tries to analyze these aspects of human cognition by comparing human behavior to optimal or “rational” solutions to the underlying computational problems. For inductive problems, this usually means exploring how ideas from artificial intelligence, machine learning, and statistics (particularly Bayesian statistics) connect to human cognition. You can read more about Prof. Griffiths’ work and accomplishments at his website.\n\nStaff\n\nMark Nitzberg\n\nExecutive Director of CHAI\n\nHead of Strategic Outreach for Berkeley AI Research\n\nMark Nitzberg\n\nExecutive Director of CHAI\n\nHead of Strategic Outreach for Berkeley AI Research\n\nDr. Mark Nitzberg is currently Executive Director of CHAI, as well as head of strategic outreach for Berkeley AI Research (BAIR). He began studying AI as a stowaway student at MIT in the early 1980’s and wrote his Ph.D. in Computer Vision and Human Perception in 1991 under David Mumford at Harvard University. Dr. Nitzberg has built companies and products in the areas of computer vision, machine learning, financial portfolio optimization, workflow efficiencies, online commerce, development aid data capture and analytics, and film and theatre. Most recently he was the Director of Computer Vision Products at A9/Amazon following their acquisition of The Blindsight Corporation, a maker of assistive technologies for low vision and active aging, where Mark was founding CEO. You can read more about Dr. Nitzberg’s work at his LinkedIn.\n\nMartin Fukui\n\nAssistant Director\n\nMartin Fukui\n\nAssistant Director\n\nMartin currently serves as the Assistant Director at CHAI. Prior to his current appointment, he served as the Interim Assistant Director and the Operations and Events Manager. Before joining CHAI, he worked at several different IP transactional law firms in the Bay Area and mostly dealt with contract drafting, contract negotiations, terms of use, terms of service, and privacy policies. He also worked for the California Bar Association by organizing events related to entertainment law and digital media. While he was in law school, he served as the President of the Association for Communication, Sports, and Entertainment Law (ACSEL) and the Co-Editor-In-Chief for the Hastings Communications and Entertainment Law Journal. After working in the IP transactional legal field for several years, he came across AI safety and decided to change his career path and use his skillset to help AI safety research organizations flourish and grow. He received his B.A. from University of California, Berkeley and J.D. from University of California, Hastings.\n\nJames Paul (J.P.) Gonzales\n\nAssistant to Stuart Russell\n\nJames Paul (J.P.) Gonzales\n\nAssistant to Stuart Russell\n\nJames Paul \"J.P.\" Gonzales serves as the executive assistant to Professor Stuart Russell. Embedded at CHAI and employed by the Berkeley Existential Risk Initiative (BERI), J.P. provides administrative, travel/event-planning and PR assistance to Prof. Russell & CHAI, with the primary goal of supporting Prof. Russell's AI safety research and advocacy goals. He received his Bachelor's of Arts in Cognitive Science at UC Merced and Master's of Science in Experimental Psychology at Arizona State University, and currently resides in San Diego, California.\n\nJulia Kerley\n\nAssistant to Andrew Critch\n\nJulia Kerley\n\nAssistant to Andrew Critch\n\nJulia Kerley assists Andrew Critch at CHAI. She is an undergraduate student at UC Berkeley majoring in Art Practice.\n\nResearchers\n\nAndrew Critch\n\nResearch Scientist\n\nAndrew Critch\n\nResearch Scientist\n\nDr. Andrew Critch is currently a part-time Research Scientist at CHAI, and spends most of his time away from UC Berkeley serving as CEO of Encultured AI.\n\nHe supports several other mission-driven projects such as the Berkeley Existential Risk Initiative and the Survival and Flourishing Fund. He earned his Ph.D. in mathematics at UC Berkeley studying applications of algebraic geometry to machine learning models. Dr. Critch has been offered university faculty and research positions in mathematics, mathematical biosciences, and philosophy, worked as an algorithmic stock trader at Jane Street Capital’s New York City office, and as a Research Fellow at the Machine Intelligence Research Institute. His current research interests include logical uncertainty, open source game theory, and mitigating race dynamics between companies and nations in AI development. You can read more about Dr. Critch’s work and accomplishments at his website.\n\nJonathan Stray\n\nSenior Scientist\n\nJonathan Stray\n\nSenior Scientist\n\nJonathan Stray is a Senior Scientist at the Center for Human Compatible AI at UC Berkeley, where he works on the design of recommender systems for better personalized news and information. He previously taught the dual masters degree in computer science and journalism at Columbia University, built several pieces of software for investigative journalism, worked as an editor at the Associated Press, and developed graphics algorithms for Adobe. He holds an MSc in Computer Science from the University of Toronto and an MA in Journalism from the University of Hong Kong.\n\nJustin Svegliato\n\nResearch Scientist\n\nJustin Svegliato\n\nResearch Scientist\n\nJustin is a Research Scientist in the Center for Human-Compatible AI at UC Berkeley. He received his PhD in Computer Science at UMass Amherst where he was advised by Shlomo Zilberstein. The goal of his research is to build autonomous systems that operate in the open world for long periods of time in an efficient, reliable, and ethical way. Recently, his work has focused on ensuring that autonomous systems comply with different ethical theories, such as utilitarianism, Kantianism, and prima facie duties. His work has also focused on using metareasoning to enable autonomous systems to recover from exceptions, maintain and restore safe operation, and optimize the performance of anytime planning algorithms for bounded rationality. You can learn more about Justin at his website.\n\nResearch Fellows\n\nBen Plaut\n\nResearch Fellow\n\nBen Plaut\n\nResearch Fellow\n\nBen is a postdoctoral research fellow at CHAI, mentored by Stuart Russell. He received his PhD from Stanford under the supervision of Ashish Goel. Ben previously studied mechanism design, arguably “the alignment problem for humans”. He is currently focused on (1) whether AIs with uncertainty in their objective function are more robust to out-of-distribution misgeneralization, and (2) the potential for strategic behavior by both humans and AIs in human-AI interaction.\n\nBenjamin Plaut at Stanford\n\nBrian Judge\n\nResearch Fellow\n\nBrian Judge\n\nResearch Fellow\n\nBrian Judge is an AI Policy Fellow at CHAI, where he studies the governance of artificial intelligence and its implications for global political economy. He received his PhD in Political Science from the University of California, Berkeley. His work has appeared in New Political Economy, New Media & Society, and the Cambridge Journal of Economics and his first book is forthcoming from Columbia University Press. You can read more about Brian’s work at his website.\n\nCameron Allen\n\nResearch Fellow\n\nCameron Allen\n\nResearch Fellow\n\nCameron Allen is a postdoc at CHAI working with Stuart Russell. Cam studies the computations that enable intelligence. His expertise is mainly in reinforcement learning, planning, observation and action abstraction, representation learning, memory, and factored world modeling. He is especially interested in exploring how abstraction can help tackle problems in value alignment, interpretability, and bounded rationality. Prior to CHAI, Cam completed his PhD at Brown University, advised by George Konidaris, where he studied structured abstractions for general-purpose decision making. You can learn more about Cam's work at his website.\n\nGeorge Obaido\n\nResearch Fellow\n\nGeorge Obaido\n\nResearch Fellow\n\nGeorge Obaido is a Postdoctoral Scholar at the University of California, San Diego, United States. He holds a PhD in Computer Science at the University of the Witwatersrand in Johannesburg, South Africa. He also obtained his Masters degree from the same university. His research interests lie in using machine learning to find solutions to many societal problems. He has collaborated actively with researchers in several other disciplines of computer science, particularly in the Data Economies and Data Markets Group of the Mechanism Design for Social Good - a multi-institutional initiative using techniques from algorithms, optimization, and mechanism design, along with insights from other disciplines, to improve access to opportunity for historically underserved and disadvantaged communities.\n\nKhanh Nguyen\n\nResearch Fellow\n\nKhanh Nguyen\n\nResearch Fellow\n\nKhanh Nguyen is a postdoc at CHAI, being mentored by Prof. Stuart Russell. Prior to joining CHAI, he spent a year as a postdoc at Princeton NLP with Prof. Karthik Narasimhan. Khanh obtained his PhD at the University of Maryland, College Park, under the guidance of Prof. Hal Daumé III. His research aims to enhance the performance and safety of AI agents by enabling them to communicate and collaborate more effectively with humans. He has developed frameworks for learning from natural language feedback and for learning to ask natural, uncertainty-grounded questions. Previously, he worked on reinforcement learning from human ratings for text generation, which is a currently popular method for fine-tuning large language models like GPTs.\n\nKimin Lee\n\nResearch Fellow\n\nKimin Lee\n\nResearch Fellow\n\nKimin Lee is a postdoctoral fellow at UC Berkeley, advised by Pieter Abbeel in BAIR.\n\nHe received his PhD in 2020 at the Korea Advanced Institute of Science and Technology (KAIST) with the thesis \"Generative Classifier for Deep Discriminative Classifier: Novelty Detection and Noisy Labels.\" During his PhD, Kimin was advised by Jinwoo Shin and closely collaborated with Honglak Lee at the University of Michigan. He has served as a reviewer and workshop organizer for several journals and conferences, and he has given spotlight and oral presentations at multiple NeurIPS and ICML conferences.\n\nRead more on his website.\n\nMichael Cohen\n\nResearch Fellow\n\nMichael Cohen\n\nResearch Fellow\n\nMichael is a postdoc at CHAI supervised by Stuart Russell. He received his PhD at Oxford University advised by Michael Osborne. His research has identified conditions under which we can expect advanced artificial long-term planning agents to escape humanity's control in order to control their inputs, and he has developed multiple idealized designs for agents which avoid those conditions. This includes an isolated environment, pessimism, and robust imitation. Recently, he has also worked on designing a regulatory framework to prevent extinction from future advanced artificial agents. You can learn more about Michael at his website.\n\nParia Rashidinejad\n\nResearch Fellow\n\nParia Rashidinejad\n\nResearch Fellow\n\nParia is a postdoctoral research fellow working with Stuart Russell. Read more on her LinkedIn.\n\nVisiting Scholars\n\nFederico L.G. Faroldi\n\nProfessor of Ethics, Law and AI at the University of Pavia\n\nFederico L.G. Faroldi\n\nProfessor of Ethics, Law and AI at the University of Pavia\n\nFederico L.G. Faroldi is Professor of Ethics, Law and AI at the University of Pavia, Italy, where he has a joint appointment at the Law School and in the AI program, and director of the Normative Risk Lab. He has a PhD in logic and has studied or worked at New York University, Oxford University, St. Andrews, the École Normale Supérieure on Rue d'Ulm in Paris, Pisa and Florence, among others. He was PI of research projects in Ghent, Salzburg and Bern. He was elected a fellow of the Johanna Quandt Academy at the University of Frankfurt, which awarded him the International Research Fellow Prize in 2018. His research interests are in deontic logic (hyperintensionality, deontic modals and truthmaker semantics), formal ethics (reasons and changing preferences), and AI safety (alignment, risk management and regulatory issues).\n\nJacy Reese Anthis\n\nVisiting Scholar\n\nJacy Reese Anthis\n\nVisiting Scholar\n\nJacy Reese Anthis is a PhD student in sociology and statistics at the University of Chicago. He is broadly interested in building safe and beneficial AI systems. His current projects include a large-scale text analysis of how perceptions of AI agency have changed over time and a formal theory of how causation and counterfactuals can adjudicate between fairness metrics.\n\nNisan Stiennon\n\nVisiting Scholar\n\nNisan Stiennon\n\nVisiting Scholar\n\nNisan earned his PhD in mathematics at Stanford in 2013, in the field of algebraic topology. He was a software engineer at Google (2014–2018) where he did ML engineering on YouTube's recommendation system. In 2018 he joined OpenAI's Alignment team, training language models including GPT-3 to summarize articles and even entire books using human feedback. Starting in 2021 he focused on game theory and theoretical computer science, funded by a grant and a summer research fellowship from the Center on Long-Term Risk. His research seeks to understand the notions of cooperation, competition, and bargaining between agents both human and artificial.\n\nRuairidh McLennan Battleday\n\nFourth Year PhD Student at Princeton University\n\nRuairidh McLennan Battleday\n\nFourth Year PhD Student at Princeton University\n\nRuairidh is a visiting computer science PhD student from Princeton University, where he works with Professor Tom Griffiths. His work addresses the mystery of human generalization: why we are able to make such good generalizations given how little experience of the world we have. His thesis provides an explanation of this ability in terms of lifelong learning and analogical inference, based on the abstractions we form of systems encountered in the past. This in turn suggests one avenue for building more compatible AI: expose agents to the same informational environments and computational problems humans have faced through their lifetimes.\n\nZoi Terzopoulou\n\nJunior Professor at Saint-Etienne School of Economics\n\nZoi Terzopoulou\n\nJunior Professor at Saint-Etienne School of Economics\n\nZoi Terzopoulou is a junior professor at the Saint-Etienne School of Economics, in France. Her research revolves around Computational Social Choice, with a focus on fine-grained formal models capturing realistic collective decision making. She previously was a postdoctoral researcher at the University of Paris-Dauphine holding an individual European fellowship. Before, she was awarded a PhD diploma from the University of Amsterdam in the Netherlands, where she also obtained a MSc degree in Logic. She holds a BSc degree in Mathematics from the University of Athens, in Greece. She has published extensively in prestigious venues both in Computer Science and Economics—notably in IJCAI, AAAI, AAMAS, the journal of Social Choice and Welfare, Journal of Mathematical Economics, and Journal of Autonomous Agents and Multiagent Systems.\n\nAlumni\n\nCaroline Jeanmaire\n\nPhD Student at Oxford University, Public Policy\n\nDirector of Strategic Research and Partnerships (2019 - 2021)\n\nCaroline Jeanmaire\n\nPhD Student at Oxford University, Public Policy\n\nDirector of Strategic Research and Partnerships (2019 - 2021)\n\nCaroline led CHAI’s partnership and external relations strategy, focusing on building a research community around AI safety and relationships with key stakeholders. She also researched models of international coordination to ensure the safety and reliability of AI systems. Before working at CHAI, she was an AI Policy Researcher and Project Manager at The Future Society, a think-tank incubated at Harvard’s Kennedy School of Government. She notably supported the organization of the first and second Global Governance of AI Forums at the World Government Summit in Dubai, with over 200 attendees. In the 2019 edition, she managed the Geopolitics of AI and International Panel on AI research committees. Caroline is experienced in multi-party coordination and negotiation. She was a Youth Delegate to the United Nations for two years with the French delegation. She participated in numerous climate negotiations and technical intersessions (including COP21, COP22, COP23 and COP24). Caroline has a dual master’s degree in International Relations from Peking University and Sciences Po Paris and a bachelor’s degree in political sciences from Sciences Po Paris. She also studied at Tufts University and at the Graduate Fletcher School of Law and Diplomacy. Caroline speaks English, French, Spanish and Mandarin Chinese. Caroline is currently pursuing her PhD in Public Policy at The Blavatnik School of Government at Oxford University.\n\nKarthika Mohan\n\nAssistant Professor at Oregon State University\n\nPostdoctoral Scholar (2017-2021)\n\nKarthika Mohan\n\nAssistant Professor at Oregon State University\n\nPostdoctoral Scholar (2017-2021)\n\nKarthika Mohan is Assistant Professor at Oregon State University. At CHAI, she was a postdoctoral scholar mentored by Stuart Russell. Karthika received her PhD in Computer Science (Artificial Intelligence) from UCLA where she was advised by Judea Pearl. Her research is of an interdisciplinary nature and her areas of interest include Causal Inference, Graphical Models and AI Safety. She was awarded the Google Outstanding Graduate Research Award, 2017 which is a UCLA Commencement Award. Currently she serves on the editorial board of the Journal of Causal Inference. In addition she is active on program committees of leading AI/ML conferences and reviews for journals in varied disciplines such as Machine Learning, Psychology and Philosophy. For more details, please visit here\n\nAdam Gleave\n\nFounder, FAR AI\n\nPhD Student in Artificial Intelligence (2017-2023)\n\nAdam Gleave\n\nFounder, FAR AI\n\nPhD Student in Artificial Intelligence (2017-2023)\n\nAdam's research focuses on reward specification for reinforcement learning. Most recently, Adam developed the EPIC distance to compare reward functions. Previously, he has worked on reward modeling techniques with a number of collaborators, including inverse reinforcement learning (IRL) from vision and multi-task IRL. Adam is also interested in the robustness of reinforcement learning, and has demonstrated the existence of adversarial policies in multi-agent environments: policies that cause an opponent to fail despite behaving seemingly randomly. Prior to joining Berkeley, Adam did a M.Phil. with Christian Steinruecken and Zoubin Ghahramani in the Machine Learning Group at the University of Cambridge. You can learn more about Adam’s work at his website and by following him on Twitter.\n\nAlex Turner\n\nResearch Scientist at Google DeepMind on Scalable Alignment Team.\n\nResearch Fellow (2022 - 2023)\n\nAlex Turner\n\nResearch Scientist at Google DeepMind on Scalable Alignment Team.\n\nResearch Fellow (2022 - 2023)\n\nAlex wanted to understand how to reliably form values and other cognitive structures within AIs. For example, he is interested in how different reward schedules + learning algorithms + environments (e.g. Pac-Man with PPO using score differentials as state-action-state rewards) translates into policy-level circuitry (e.g. IF near ghost, THEN move away). In this light, he wants to understand how this process occurs in human beings -- the only generally intelligent agents we have ever seen. Alex is currently a research scientist at Google DeepMind on the Scalable Alignment team.\n\nCody Wild\n\nMachine Learning Engineer, Google Research\n\nResearch Engineer (2018-2021)\n\nCody Wild\n\nMachine Learning Engineer, Google Research\n\nResearch Engineer (2018-2021)\n\nCody Wild is a ML Engineer at Google Research, previously Research Engineer at CHAI. She holds a B.S. in Statistical Economics from Tulane University, and a M.S. in Analytics from the University of San Francisco. In the years after finishing her Masters program, Cody worked as an industry Data Scientist, first at LendUp, and then on an applied R&D team for Sophos Labs working on developing deep learning models to detect viruses and other malicious file types. Outside of formal employment, she’s worked to deepen her knowledge in Machine Learning by reading and synthesizing technical content from the field, which can be found online in both long and short form. She is passionate about precise understanding, clear arguments, and clean code, and is glad to be helping support a research team that’s thinking critically about the ways their work and AI writ large could negatively impact the world.\n\nDan Hendrycks\n\nDirector at the Center for AI Safety\n\nPhD Student, Computer Science (2018-2022)\n\nDan Hendrycks\n\nDirector at the Center for AI Safety\n\nPhD Student, Computer Science (2018-2022)\n\nDan Hendrycks is Director at the Center for AI Safety. His research aims to disentangle and concretize the components necessary for safe AI. This leads him to work on quantifying and improving the performance of models in unforeseen out-of-distribution scenarios. He also works on measuring a model’s alignment with human values. Dan received his BS from the University of Chicago. You can find out more about his research at his website.\n\nDaniel Filan\n\nResearch Manager at MATS\n\nPhD Student in EECS\n\nDaniel Filan\n\nResearch Manager at MATS\n\nPhD Student in EECS\n\nDaniel Filan is a PhD student of EECS at UC Berkeley, supervised by Stuart Russell. He’s interested in effective altruism and wants to ensure that future artificial intelligences who may be much more strategically intelligent than us behave in a safe way. In 2016, Daniel worked with Tom Everitt, Mayank Daswani, and Marcus Hutter, considering the problem that a sufficiently advanced AI could choose to modify its source code in order to have easily achievable goals, and such modifications may not be to humans’ liking (paper). They determined that an agent will not self-modify if and only if the value function of the agent anticipates the consequences of self-modification and uses the agent’s current utility function when evaluating the future. Daniel is currently thinking about mechanistic transparency, the problem of how to understand the workings of trained models. You can learn more about Daniel’s work at his personal website.\n\nDorsa Sadigh\n\nAssistant Professor of Computer Science and Electrical Engineering at Stanford University\n\nPhD Student, Electrical Engineering and Computer Science (2012-2017)\n\nDorsa Sadigh\n\nAssistant Professor of Computer Science and Electrical Engineering at Stanford University\n\nPhD Student, Electrical Engineering and Computer Science (2012-2017)\n\nDorsa Sadigh is an Assistant Professor in Computer Science and Electrical Engineering at Stanford University. Her research interests lie in the intersection of robotics, learning, and control theory. Specifically, she is interested in developing algorithms for safe and adaptive human-robot interaction. Dorsa has received her doctoral degree in Electrical Engineering and Computer Sciences (EECS) from UC Berkeley in 2017, and has received her bachelor’s degree in EECS from UC Berkeley in 2012. She is awarded the NSF CAREER award, the AFOSR Young Investigator Program Award, the IEEE TCCPS early career award, the Google Faculty Award, and the Amazon Faculty Research Award. Learn more at her website.\n\nDylan Hadfield-Menell\n\nAssistant Professor, MIT\n\nPhD Student, Computer Science (2013-2020)\n\nDylan Hadfield-Menell\n\nAssistant Professor, MIT\n\nPhD Student, Computer Science (2013-2020)\n\nDylan will be starting as an Assistant Professor at the Massachusetts Institute of Technology in July 2021. While at CHAI, Dylan was a Ph.D. student at UC Berkeley advised by Anca Dragan, Pieter Abbeel, and Stuart Russell. His research focused on algorithms that facilitate human-compatible artificial intelligence. In particular, he tried to develop frameworks that account for uncertainty about the objective being optimized. Before coming to Berkeley, Dylan did a Master’s of Engineering with Leslie Kaelbling and Tomás Lozano-Pérez at MIT. At Berkeley, Dylan’s research has taken a turn to focus more on AI safety and, thinking longer term, AI value alignment. In 2016, he and his advisors formally described a cooperative inverse reinforcement learning problem (paper). The problem serves as a tool to help researchers consider how robots could learn humans’ values via cooperative instruction. While robots could learn humans’ values by observing humans, cooperative instruction is likely to be significantly faster. In 2017, Dylan and his advisors described an “off-switch game,” a simplified problem describing scenarios in which a human would like to turn off a robot, but the robot is able to disable its off-switch (paper). They showed that a robot who is uncertain about the utility derived from various outcomes in the game is more likely to allow a human to turn it off. You can learn more about Dylan’s other professional work at his website and follow him on Twitter here.\n\nErdem Biyik\n\nAssistant Professor\n\nErdem Biyik\n\nAssistant Professor\n\nErdem is at the department of Computer Science at the University of Southern California as an Assistant Professor. He is a postdoctoral scholar at UC Berkeley mentored by Stuart Russell and Anca Dragan. He has received his B.Sc. degree from Bilkent University, Turkey, in 2017; and Ph.D. degree from Stanford University in 2022 where he was advised by Dorsa Sadigh. His research interests lie in the intersection of robotics, artificial intelligence, machine learning and game theory. He is interested in enabling robots to actively learn from various forms of human feedback and designing robot policies to improve the efficiency of multi-agent systems both in cooperative and competitive settings. He also worked at Google as a research intern in 2021 where he adapted his active robot learning algorithms to recommender systems. He will join the University of Southern California as an assistant professor in 2023.\n\nGeorge Matheos\n\nFormer Undergraduate Student\n\nGeorge Matheos\n\nFormer Undergraduate Student\n\nGeorge Matheos was an undergraduate student at UC Berkeley studying math and computer science, advised by Stuart Russell and Vikash Mansinghka. From 2020-2021, he worked full-time as a researcher at the MIT Probabilistic Computing Project. George’s current research focuses on developing new data-driven Monte Carlo algorithms for probabilistic inference in structured models of the world, and on understanding how the mammalian brain may implement algorithms like these. He previously developed algorithms and a high-level probabilistic programming language that facilitate the implementation of custom inference algorithms for open-universe probabilistic programs.\n\nJaime Fernandez Fisac\n\nAssistant Professor of Electrical Engineering at Princeton University\n\nPhD Student, Control, Intelligent Systems and Robotics (2013-2018)\n\nJaime Fernandez Fisac\n\nAssistant Professor of Electrical Engineering at Princeton University\n\nPhD Student, Control, Intelligent Systems and Robotics (2013-2018)\n\nJaime is currently an Assistant Professor in Electrical Engineering at Princeton University. Prior to this appointment, he worked at Waymo as a Research Scientist While Jaime was at CHAI as a PhD student, he worked on autonomous robots in both academia and industry, with a particular focus on collision avoidance and multi-agent systems. Broadly, his research focused on safely introducing robotics into society. Under the guidance of CHAI Professors Anca Dragan and Tom Griffiths, and along with fellow CHAI graduate students Vael Gates and Dylan Hadfield-Menell, Jaime worked on solving the cooperative inverse reinforcement learning (CIRL) dynamic game using well-established models of human inference, decision making, and theory of mind from the cognitive science literature. Previous solutions have relied on modelling both the human and robot as perfectly rational and able to coordinate in advance, which are nontrivial assumptions in the real world. Instead, Jaime and his colleagues’ work models the human as pedagogic (i.e., her behaviour will aim to be instructive) and the robot as pragmatic (i.e., it knows the human is not perfectly rational but is still trying to teach it). Results suggest this formulation produces robots that are more competent collaborators (paper). In the past, Jaime has also researched how we can have robots choose their course of action in a way that will be easy for a human observer to anticipate (paper) and how incorporating uncertainty into a safety framework for robotic systems that works in conjunction with their learning process can provide meaningful safety guarantees (paper). At Princeton, Jaime intends to look into how AI systems can utilize models of human cognition and behavior to ensure safe interaction with people. He believes that the safer robots will be those that engage their users and procure their cooperation, rather than try to protect against their indifference. He hopes that designing safe human-centered robotic systems in the short term will give us key insights to tackle the broader, long-term AI safety problem. Also, he has done research on having AI treat their models as fallible to protect against overconfidence. You can explore more about this topic by watching this video. You can learn more about Jaime at his website.\n\nLawrence Chan\n\nPhD Student (2023), Advised by Anca Dragan and Stuart Russell\n\nLawrence Chan\n\nPhD Student (2023), Advised by Anca Dragan and Stuart Russell\n\nAs of January 2023, I’m currently working at the Alignment Research Center doing evaluations of large language models. Previously, I was at Redwood Research, where I worked on adversarial training and neural network interpretability. I’m also doing a PhD at UC Berkeley advised by Anca Dragan and Stuart Russell. Before that, I received a BAS in Computer Science and Logic and a BS in Economics from the University of Pennsylvania’s M&T Program, where I was fortunate to work with Philip Tetlock on using ML for forecasting. My main research interests are mechanistic interpretability and scalable oversight. In the past, I’ve also done conceptual work on learning human values. I also sometimes blog about AI alignment and other topics on LessWrong/the AI Alignment Forum.\n\nMichael Dennis\n\nResearcher, Google DeepMind\n\nPhD Student, Artificial Intelligence and Computational Geometry (2016-2023)\n\nMichael Dennis\n\nResearcher, Google DeepMind\n\nPhD Student, Artificial Intelligence and Computational Geometry (2016-2023)\n\nBefore joining CHAI, Michael worked in theoretical computer science. He is working to close the gap between game theoretic principles and current approaches to multi-agent learning in order to provide better assurances of both performance and stability in these systems. Currently, this takes the form of work on Unsupervised Environment Design (UED), which aims to build complex and challenging environments automatically to promote efficient learning and transfer. This framework has deep connections to decision theory, which allows us to make guarantees about how the resulting policies would perform in human-designed environments, without having ever trained on them. You can learn more about Michael's work at his website and by following him on Twitter.\n\nOlivia Watkins\n\nMember of Technical Staff at OpenAI\n\nPhD Student (Fall 2019 - Spring 2024)\n\nOlivia Watkins\n\nMember of Technical Staff at OpenAI\n\nPhD Student (Fall 2019 - Spring 2024)\n\nOlivia is currently a Member of Technical Staff at OpenAI. While at CHAI, Olivia is co-advised by Pieter Abbeel and Trevor Darrell. She is excited about human-in-the-loop learning, and more generally about finding better ways to provide human supervision and priors to AI agents. More details at https://aliengirlliv.github. io/oliviawatkins/ .\n\nRohin Shah\n\nResearch Scientist, Google DeepMind\n\nPhD Student, Artificial Intelligence (2014-2020)\n\nRohin Shah\n\nResearch Scientist, Google DeepMind\n\nPhD Student, Artificial Intelligence (2014-2020)\n\nRohin currently works as a Research Scientist on the technical AGI safety team at DeepMind. Prior to DeepMind, Rohin joined CHAI in his fourth year after he became convinced of the importance of building safe, aligned AI. He now thinks about how to provide specifications of good behavior in ways other than reward functions, especially ones that do not require much human effort. In his PhD, he worked on building AI systems that can learn to assist a human user, even if they don't initially know what the user wants. His general interests in CS are very broad, including AI, machine learning, programming languages, complexity theory, algorithms, and security, and so he started his PhD working on program synthesis. Rohin writes the Alignment Newsletter, a weekly publication with recent content relevant to AI alignment that has over 600 subscribers. You can learn more about his work at his website and by following him on Twitter.\n\nSam Toyer\n\nMember of Technical Staff, OpenAI, Model Safety Team\n\nPhD Student in Artificial Intelligence (2018-2024)\n\nSam Toyer\n\nMember of Technical Staff, OpenAI, Model Safety Team\n\nPhD Student in Artificial Intelligence (2018-2024)\n\nSam is currently a member of technical staff at OpenAI on the model safety team. Sam was a PhD student in EECS at UC Berkeley, advised by Stuart Russell. As an undergraduate at the Australian National University, he worked on a range of topics, including human pose estimation from video, establishing best practices for serving large-scale satellite data on the web, and accelerating classical and probabilistic planning with deep learning. Currently, he is interested in how to make intelligent agents that can infer and act in accordance with the preferences of their owners. More details about his research are available on his website.\n\nSmitha Milli\n\nPostdoc at Cornell Tech\n\nPhD Student, Electrical Engineering and Computer Science, 2017-2022\n\nSmitha Milli\n\nPostdoc at Cornell Tech\n\nPhD Student, Electrical Engineering and Computer Science, 2017-2022\n\nSmitha Milli is a postdoctoral associate at Cornell Tech. Their work primarily focuses on (a) rigorous evaluation of systems interacting in feedback loops with humans (e.g. recent work on measuring effects of Twitter’s ranking algorithm) and (b) designing and learning objective functions for those systems that produce more socially-beneficial outcomes. They hold a PhD in EECS from UC Berkeley and their postdoc is funded by an Open Philanthropy early career grant. You can learn more about Smitha’s research on their website and follow them on Twitter here.\n\nSteven Wang\n\nMaster’s Student at ETH Zurich\n\nResearch Engineer (2018-2021)\n\nSteven Wang\n\nMaster’s Student at ETH Zurich\n\nResearch Engineer (2018-2021)\n\nSteven is a Master’s Student in Computer Science at ETH Zurich. At CHAI, he was an ML research engineer. He became interested in AI Safety after reading the 80000 Hours career guide.\n\nDuring his undergraduate studies at UC Berkeley, Steven worked with Anca Dragan and Jaime Fisac on confidence-based human predictions. As an intern at CHAI, he worked with Dylan Hadfield-Menell on extensions to Inverse Reward Design.\n\nYou can check out what he’s been coding up lately on his GitHub.\n\nThanard Kurutach\n\nAI/ML Research Scientist at Cruise\n\nPhD Student, Computer Science (2016-2020)\n\nThanard Kurutach\n\nAI/ML Research Scientist at Cruise\n\nPhD Student, Computer Science (2016-2020)\n\nThanard Kurutach is an AI/ML Research Scientist at Cruise. He graduated in 2021 from UC Berkeley with a PhD in AI/Robotics, advised by Prof. Pieter Abbeel and Prof. Stuart Russell. His thesis is titled Learning, Planning, and Acting with Models. Previously, he was a Math and CS double major at MIT.\n\nRead more on his LinkedIn.\n\nThomas Krendl Gilbert\n\nPostdoc at Cornell Tech\n\nPhD Student, Machine Ethics and Epistemology (2015-2021)\n\nThomas Krendl Gilbert\n\nPostdoc at Cornell Tech\n\nPhD Student, Machine Ethics and Epistemology (2015-2021)\n\nTom graduated in 2021 with his PhD in Machine Ethics and Epistemology. This fall he will join the Digital Life Initiative at Cornell Tech as a postdoc. With prior training in philosophy, sociology, and political theory, Tom researches the ethical and political predicaments that emerge when technical practitioners use machine learning to reshape organizational decision-making. His recent paper “Hard Choices in Artificial Intelligence” argues that advanced AI systems should be developed democratically so that relevant preferences and norms are modeled in a way that is simultaneously accountable, contextually appropriate, and compatible between stakeholders. This work has concrete implications for the design of AI systems that are fair for distinct subpopulations, safe when enmeshed with institutional practices, and accountable to the public interest, including medium-term applications like automated vehicles. You can learn more about his work at his website.\n\nVael Gates\n\nPostdoc at Stanford HAI / CISAC\n\nPhD Student, Computational Cognitive Science (2016-2021)\n\nVael Gates\n\nPostdoc at Stanford HAI / CISAC\n\nPhD Student, Computational Cognitive Science (2016-2021)\n\nIn 2021 Vael graduated with their PhD in Neuroscience with a focus on computational cognitive science. This fall Vael will start as a postdoc at Stanford HAI / CISAC, where they intend to work on an ethnography of AI researchers.\n\nAt UC Berkeley, Vael was advised by Professor Tom Griffiths (a PI at CHAI). Vael’s research is broadly aimed at developing computational models of social cognition. Vael is interested in: How people can infer beliefs and intentions in others, especially by observing others’ actions and employing recursive theory-of-mind (e.g. inverse reinforcement learning, social psychology); Group-level equilibria when agents are collaborating or competing (e.g. game theory, agent-based modeling); and Mechanism design and other ways quantitative characterizations of a phenomenon can be used to predict and shape behavior.\n\nRecently, Vael has worked on two projects related to CHAI’s mission. They have assisted with the paper on solving the cooperative inverse reinforcement learning (CIRL) dynamic game and is working with Professors Anca D. Dragan, Tom L. Griffiths, and Anant Sahai on preference aggregation across agents. More specifically, in the latter project Vael and colleagues have set up a study in which participants are presented with a problem that requires mediating between the preferences of multiple agents. Vael and colleagues take participants’ responses and attempt to explain them using a quantitative model. Their hope is to create a baseline standard of “fair” reactions to the problem—a standard to which the behavior of future AIs can be compared.\n\nVael wants to continue to approach social cognition and social inference problems from a computational perspective, using probabilistic models and large-scale web-based crowdsourcing to investigate the computational goals and algorithms driving the social mind. By understanding the complex inferences made by human minds, they hope to contribute to the development of artificial intelligence that can collaborate and is compatible with human behavior.\n\nYou can learn more at Vael’s website.\n\nAffiliates\n\nAlison Gopnik\n\nProfessor of Psychology at UC Berkeley\n\nAffiliate Professor of Philosophy at UC Berkeley\n\nAlison Gopnik\n\nProfessor of Psychology at UC Berkeley\n\nAffiliate Professor of Philosophy at UC Berkeley\n\nAlison Gopnik is a professor of psychology and affiliate professor of philosophy at the University of California at Berkeley. She received her BA from McGill University and her PhD. from Oxford University. She is an internationally recognized leader in the study of cognitive science and of children’s learning and development and was one of the founders of the field of “theory of mind”, an originator of the “theory theory” of children’s development and more recently introduced the idea that probabilistic models and Bayesian inference could be applied to children’s learning. She has held a Center for Advanced Studies in the Behavioral Sciences Fellowship, the Moore Distinguished Scholar fellowship at the California Institute of Technology, the All Souls College Distinguished Visiting Fellowship at Oxford, and King’s College Distinguished Visiting Fellowship at Cambridge. She is an elected member of the Society of Experimental Psychologists, and the American Academy of Arts and Sciences and a fellow of the Cognitive Science Society and the American Association for the Advancement of Science. She has been continuously supported by the NSF and was PI on a 2.5 million dollar interdisciplinary collaborative grant on causal learning from the McDonnell Foundation. She is the author or coauthor of over 100 journal articles and several books including “Words, thoughts and theories” MIT Press, 1997, and the bestselling and critically acclaimed popular books “The Scientist in the Crib” William Morrow, 1999, “The Philosophical Baby; What children’s minds tell us about love, truth and the meaning of life”, and “The Gardener and the Carpenter”, Farrar, Strauss and Giroux, the latter two won the Cognitive Development Society Best Book Prize in 2009 and 2016. She has also written widely about cognitive science and psychology for The New York Times, The Atlantic, The New Yorker, Science, Scientific American, The Times Literary Supplement, The New York Review of Books, New Scientist and Slate, among others. Her TED talk on her work has been viewed more than 3 million times. And she has frequently appeared on TV and radio including “The Charlie Rose Show” and “The Colbert Report”. Since 2013 she has written the Mind and Matter column for the Wall Street Journal. She lives in Berkeley with her husband Alvy Ray Smith, and has three children and three grandchildren.\n\nBrandie Nonnecke\n\nFounding Director of the CITRIS Policy Lab\n\nAssociate Research Professor at the Goldman School of Public Policy\n\nBrandie Nonnecke\n\nFounding Director of the CITRIS Policy Lab\n\nAssociate Research Professor at the Goldman School of Public Policy\n\nBrandie Nonnecke, PhD is Founding Director of the CITRIS Policy Lab, headquartered at UC Berkeley. She is an Associate Research Professor at the Goldman School of Public Policy (GSPP) where she directs the Tech Policy Initiative, a collaboration between CITRIS and GSPP to strengthen tech policy education, research, and impact. Brandie is the Director of Our Better Web, a program that supports empirical research, policy analysis, training, and engagement to address the sharp rise of online harms. She is a co-director of the Berkeley Center for Law and Technology at Berkeley Law where she leads the Project on Artificial Intelligence, Platforms, and Society. She also co-directs the UC Berkeley AI Policy Hub, an interdisciplinary initiative training researchers to develop effective AI governance and policy frameworks. Brandie is the host of TecHype, a groundbreaking video and audio series that debunks misunderstandings around emerging technologies and explores effective technical and policy strategies to harness emerging technologies for good. Brandie served as a Technology and Human Rights Fellow at the Carr Center for Human Rights Policy at the Harvard Kennedy School. She also completed fellowships at the Schmidt Futures International Strategy Forum, Aspen Institute’s Tech Policy Hub, and the World Economic Forum. Her research has been published in Science, Wired, Telecommunications Policy, the Journal of Information Technology and Politics, among other outlets. Her work has been cited by the FTC, NIST, the White House Office of Science and Technology Policy, as well as in the Washington Post, BBC, NPR, among other venues. Brandie was named one of the 100 Brilliant Women in AI Ethics in 2021.\n\nBrian Christian\n\nAuthor of The Most Human Human, Algorithms to Live By, and The Alignment Problem\n\nBrian Christian\n\nAuthor of The Most Human Human, Algorithms to Live By, and The Alignment Problem\n\nBrian Christian is the author of The Most Human Human, which was named a Wall Street Journal bestseller, a New York Times Editors’ Choice, and a New Yorker favorite book of the year. He is the author, with Tom Griffiths, of Algorithms to Live By, a #1 Audible bestseller, Amazon best science book of the year and MIT Technology Review best book of the year.\n\nHis third book, The Alignment Problem, was released in October of 2020.\n\nChristian’s writing has been translated into nineteen languages, and has appeared in The New Yorker, The Atlantic, Wired, The Wall Street Journal, The Guardian, The Paris Review, and in scientific journals such as Cognitive Science. Christian has been featured on The Daily Show with Jon Stewart, Radiolab, and The Charlie Rose Show, and has lectured at Google, Facebook, Microsoft, the Santa Fe Institute, and the London School of Economics. His work has won several awards, including fellowships at Yaddo and the MacDowell Colony, publication in Best American Science & Nature Writing, and an award from the Academy of American Poets.\n\nBorn in Wilmington, Delaware, Christian holds degrees in philosophy, computer science, and poetry from Brown University and the University of Washington. A Visiting Scholar at the University of California, Berkeley, he lives in San Francisco.\n\nCharis Thompson\n\nChancellor's Professor of Gender & Women's Studies at UC Berkeley\n\nCharis Thompson\n\nChancellor's Professor of Gender & Women's Studies at UC Berkeley\n\nCharis Thompson is Chancellor’s Professor of Gender & Women’s Studies, and a former founding director of the Science, Technology, and Society Center at the University of California, Berkeley; and Professor of Sociology, London School of Economics and Political Science. Her current book in progress, Getting Ahead, focuses on minds, bodies, and emotions in an age of populism and technology elites. Getting Ahead is the third in her book series on technology and democracy. The first, Making Parents (2005), won the Rachel Carson Prize from the Society for the Social Study of Science, and examined gender, race, and agency in relation to reproductive technologies. The second book, Good Science (2013), examined the geopolitics and bioethics of pro-cures innovation economies. Thompson served on the Nuffield Council on Bioethics Working Group on Genome Editing, and currently serves on the World Economic Forum’s Global Technology Council on Technology, Values, and Policy, as well as UC Berkeley’s Stem Cell Research Oversight Committee, and the faculty advisory board of the Center for Race and Gender. She is a recipient of the Social Science Division Distinguished Teaching Award, and in 2017 received an Honorary Doctorate from the Norwegian Science and Technology University.\n\nDavid Krueger\n\nAssistant Professor at the University of Cambridge\n\nDavid Krueger\n\nAssistant Professor at the University of Cambridge\n\nDavid Krueger is an Assistant Professor at the University of Cambridge and a member of Cambridge's Computational and Biological Learning lab (CBL). His research group focuses on Deep Learning, AI Alignment, and AI safety. He is broadly interested in work (including in areas outside of Machine Learning, e.g. AI governance) that could reduce the risk of human extinction (“x-risk”) resulting from out-of-control AI systems. Particular interests include:\n\nReward modeling and reward gaming\n\nAligning foundation models\n\nUnderstanding learning and generalization in deep learning and foundation models, especially via “empirical theory” approaches\n\nPreventing the development and deployment of socially harmful AI systems\n\nElaborating and evaluating speculative concerns about more advanced future AI systems\n\nDawn Song\n\nProfessor of EECS at UC Berkeley\n\nDawn Song\n\nProfessor of EECS at UC Berkeley\n\nDawn Song is a Professor in the Department of Electrical Engineering and Computer Science at UC Berkeley. Her research interest lies in deep learning and security. She has studied diverse security and privacy issues in computer systems and networks, including areas ranging from software security, networking security, distributed systems security, applied cryptography, blockchain and smart contracts, to the intersection of machine learning and security. She is the recipient of various awards including the MacArthur Fellowship, the Guggenheim Fellowship, the NSF CAREER Award, the Alfred P. Sloan Research Fellowship, the MIT Technology Review TR-35 Award, the Faculty Research Award from IBM, Google and other major tech companies, and Best Paper Awards from top conferences in Computer Security and Deep Learning. She is a winner of the AMiner Most Influential Scholar Award, as the most cited scholar in Computer Security. She obtained her Ph.D. degree from UC Berkeley. Prior to joining UC Berkeley as a faculty, she was a faculty at Carnegie Mellon University from 2002 to 2007.\n\nYou can read more about Prof. Song’s work and accomplishments here.\n\nDemian Pouzo\n\nAssociate Professor of Economics at UC Berkeley\n\nDemian Pouzo\n\nAssociate Professor of Economics at UC Berkeley\n\nDemian Pouzo is an associated professor in the Department of Economics at UC Berkeley. Demian joined the faculty at Berkeley in 2009 as an assistant professor after receiving his PhD in Economics from NYU. He also holds an MA and BA in Economics from Universidad Torcuato Di Tella (Argentina). Pouzo’s research interests include econometrics as well as other fields such as economic theory and macroeconomics.\n\nGillian Hadfield\n\nProfessor of Law and Professor of Strategic Management, University of Toronto\n\nGillian Hadfield\n\nProfessor of Law and Professor of Strategic Management, University of Toronto\n\nGillian Hadfield holds a J.D. from Stanford Law School and a Ph.D. in economics from Stanford University. She is a leading proponent of the reform and redesign of legal systems for a rapidly changing world facing tremendous challenge from globalization and technology. Her extensive research examines how to make law more accessible, effective, and capable of fulfilling its role in balancing innovation, growth, and fairness. Prof. Hadfield is a member of the World Economic Forum’s Global Future Council on the Future of Technology, Values and Policy and co-curates the Forum’s Transformation Map for Justice and Legal Infrastructure. She was appointed in 2017 to the American Bar Association’s Commission on the Future of Legal Education, serves as Director of the USC Center for Law and Social Science and is a member of the World Justice Project’s Research Consortium. Her book, Rules for a Flat World: Why Humans Invented Law and How to Reinvent It for a Complex Global Economy, was published by Oxford University Press in November 2016. You can read more about Prof. Hadfield’s work and accomplishments at her website.\n\nJacob Steinhardt\n\nAssistant Professor of Statistics at UC Berkeley\n\nJacob Steinhardt\n\nAssistant Professor of Statistics at UC Berkeley\n\nProfessor Steinhardt’s goal is to make the conceptual advances necessary for machine learning systems to be reliable and aligned with human values. This includes the following directions:\n\nRobustness: How can we build models robust to distributional shift, to adversaries, to model mis-specification, and to approximations imposed by computational constraints? What is the right way to evaluate such models?\n\nReward specification and reward hacking: Human values are too complex to be specified by hand. How can we infer complex value functions from data? How should an agent make decisions when its value function is approximate due to noise in the data or inadequacies in the model? How can we prevent reward hacking–degenerate policies that exploit differences between the inferred and true reward?\n\nScalable alignment: Modern ML systems are often too large, and deployed too broadly, for any single person to reason about in detail, posing challenges to both design and monitoring. How can we design ML systems that conform to interpretable abstractions? How do we enable meaningful human oversight at training and deployment time despite the large scale? How will these large-scale systems affect societal equilibria?\n\nThese challenges require rethinking both the theoretical and empirical paradigms of ML. Theories of statistical generalization do not account for the extreme types of generalization considered above, and decision theory does not account for cases where the reward function is only approximate. Meanwhile, measuring empirical test accuracy on a fixed distribution is insufficient to analyze phenomena such as robustness to distributional shift.\n\nFor more information about Professor Steinhardt, please visit: https://www.stat.berkeley.edu/~jsteinhardt/\n\nJakob Foerster\n\nAssistant Professor of Computer Science at the University of Toronto and the Vector Institute\n\nJakob Foerster\n\nAssistant Professor of Computer Science at the University of Toronto and the Vector Institute\n\nJakob Foerster received a CIFAR AI chair in 2019 and is starting as an Assistant Professor at the University of Toronto and the Vector Institute in the academic year 20/21. During his PhD at the University of Oxford, he helped bring deep multi-agent reinforcement learning to the forefront of AI research and interned at Google Brain, OpenAI, and DeepMind. He has since been working as a research scientist at Facebook AI Research in California, where he will continue advancing the field up to his move to Toronto. He was the lead organizer of the first Emergent Communication (EmeCom) workshop at NeurIPS in 2017, which he has helped organize ever since. Learn more at his website.\n\nJohn Zysman\n\nProfessor Emeritus in Political Science at UC Berkeley\n\nJohn Zysman\n\nProfessor Emeritus in Political Science at UC Berkeley\n\nJohn Zysman is Professor Emeritus in the Department of Political Science. He received his B.A. from Harvard University and his Ph.D. from the Massachusetts Institute of Technology. He has written extensively on European and Japanese political economy, policy and corporate strategy.\n\nHis most recent articles include:\n\nThe Next Phase in the Digital Revolution: Abundant Computing, Platforms, Growth and Employment in February 2018 CACM\n\nIntelligent Tools and Digital Platforms: Implications for Work and Employment in Intereconomics Review of European Economic Policy (Issue 6 2017)\n\nThe Rise of the Platform Economy In NAS Issues in Science and Technoogy, Spring 2016\n\nEntrepreneurial Finance in the Era of Intelligent Tools and Digital Platforms: Implications and Consequences for Work\n\nRecent books include:\n\nThe Third Globalization: Can Wealthy Countries Stay Rich Ed. with Dan Brezniz (Oxford University Press 2013)\n\nCan Green Sustain Growth: From the Religion to the Reality To Sustainable Prosperity Ed. with Mark Huberty (Stanford Unversity Press. 2013)\n\nEarlier books include:\n\nThe Highest Stakes: The Economic Foundations of the Next Security System (Oxford University Press, 1992)\n\nManufacturing Matters: The Myth of the Post-Industrial Economy (Basic Books, 1987)\n\nGovernments, Markets, and Growth: Finance and the Politics of Industrial Change (Cornell University Press, 1983)\n\nJuliana Schroeder\n\nAssistant Professor of Haas School of Business at UC Berkeley\n\nJuliana Schroeder\n\nAssistant Professor of Haas School of Business at UC Berkeley\n\nJuliana Schroeder researches how people navigate their social worlds: first, how people form inferences about others’ mental states and mental capacities and, second, how these inferences influence their interactions.\n\nJuliana’s research has been published in journals such as Journal of Personality and Social Psychology, Journal of Experimental Psychology, and Psychological Science. It has been featured by outlets such as the New York Times, Newsweek, NBC, and the Today Show, and has been funded by the National Science Foundation.\n\nJuliana received her B.A. in psychology and economics from The University of Virginia. She received a M.A. in social psychology and advanced methods from the University of Chicago, and an M.B.A. from The Chicago Booth School of Business. Her Ph.D. is in Psychology and Business from the University of Chicago. Juliana is currently an Assistant Professor at the Berkeley Haas School of Business.\n\nYou can read more about Prof. Schroeder’s work and accomplishments here.\n\nKen Goldberg\n\nProfessor and Chair of the Industrial Engineering and Operations Research Department at UC Berkeley\n\nKen Goldberg\n\nProfessor and Chair of the Industrial Engineering and Operations Research Department at UC Berkeley\n\nKen Goldberg is an artist, inventor, and UC Berkeley Professor focusing on robotics. He was appointed the William S. Floyd Jr Distinguished Chair in Engineering and serves as Chair of the Industrial Engineering and Operations Research Department. He has secondary appointments in EECS, Art Practice, the School of Information, and Radiation Oncology at the UCSF Medical School. Ken is Director of the CITRIS “People and Robots” Initiative and the UC Berkeley AUTOLAB where he and his students pursue research in machine learning for robotics and automation in warehouses, homes, and operating rooms. Ken developed the first provably complete algorithms for part feeding and part fixturing and the first robot on the Internet. Despite agonizingly slow progress, he persists in trying to make robots less clumsy. He has over 250 peer-reviewed publications and 8 U.S. Patents. He co-founded and served as Editor-in-Chief of the IEEE Transactions on Automation Science and Engineering. Ken’s artwork has appeared in 70 exhibits including the Whitney Biennial and films he has co-written have been selected for Sundance and nominated for an Emmy Award. Ken was awarded the NSF PECASE (Presidential Faculty Fellowship) from President Bill Clinton in 1995, elected IEEE Fellow in 2005 and selected by the IEEE Robotics and Automation Society for the George Saridis Leadership Award in 2016. He lives in the Bay Area and is madly in love with his wife, filmmaker and Webby Awards founder Tiffany Shlain, and their two daughters.\n\nYou can read more about Prof. Goldberg’s work and accomplishments here.\n\nLara Buchak\n\nProfessor of Philosophy at Princeton University\n\nLara Buchak\n\nProfessor of Philosophy at Princeton University\n\nLara Buchak is a Professor in the Philosophy Department at Princeton University. Her research interests include decision theory, social choice theory, epistemology, ethics, and the philosophy of religion. Her book Risk and Rationality (2013) concerns how an individual ought to take risk into account when making decisions. Her research following the book has focused on applications of her view to ethics, arguing that we ought to defer to individuals’ risk-attitudes in biomedical research; that we ought to weight worse scenarios very heavily in setting climate policy; and that we ought to care a great deal about the interests of the worse-off when acting ethically. Other topics she has written on include the nature and rationality of faith; group decision-making; the relationship between assigning probability to a hypothesis and believing that hypothesis outright; and the nature of free will. Professor Buchak received her Ph.D. from Princeton in 2009. She spent 12 years in the Philosophy Department at UC Berkeley before returning to Princeton.\n\nMariano Florentino Cuéllar\n\nChair, Board of the Directors at the Center for Advanced Study in the Behavioral Sciences\n\nFormer Stanley Morrison Professor of Law at Stanford University\n\nMariano Florentino Cuéllar\n\nChair, Board of the Directors at the Center for Advanced Study in the Behavioral Sciences\n\nFormer Stanley Morrison Professor of Law at Stanford University\n\nJustice Mariano-Florentino Cuéllar was nominated by Governor Jerry Brown and began serving on the California Supreme Court in January 2015. A naturalized U.S. citizen born in Northern Mexico, Cuéllar received a B.A. from Harvard magna cum laude, a J.D. from Yale Law School, and a Ph.D. in Political Science from Stanford. Before serving on the Court, he was the Stanley Morrison Professor of Law and Professor (by courtesy) of Political Science at Stanford University. A member of the Stanford faculty since 2001, Cuéllar has written books and articles on administrative law and legislation, criminal law, international law, cyberlaw, immigration, public health law, and the history of institutions. Between 2004 and 2015, Cuéllar also held leadership positions at Stanford’s Freeman Spogli Institute for International Studies. During his tenure leading the Institute and, earlier, its Center for International Security and Cooperation, Cuéllar grew the Institute’s faculty, expanded Stanford’s role in nuclear security research, launched university-wide initiatives on global poverty and cyber security, and broadened opportunities for student and faculty research abroad. You can read more about Justice Cuéllar’s work and accomplishments here or here.\n\nMarion Fourcade\n\nProfessor of Sociology at UC Berkeley\n\nMarion Fourcade\n\nProfessor of Sociology at UC Berkeley\n\nMarion Fourcade received her PhD from Harvard University (2000) and taught at New York University and Princeton University before joining the Berkeley sociology department in 2003. A comparative sociologist by training and taste, she is interested in variations in economic and political knowledge and practice across nations. Her first book, Economists and Societies (Princeton University Press 2009), explored the distinctive character of the discipline and profession of economics in three countries. A second book, The Ordinal Society (with Kieran Healy), is under contract. This book investigates new forms of social stratification and morality in the digital economy. Other recent research focuses on the valuation of nature in comparative perspective; the moral regulation of states; the comparative study of political organization (with Evan Schofer and Brian Lande); the microsociology of courtroom exchanges (with Roi Livne); the sociology of economics, with Etienne Ollion and Yann Algan, and with Rakesh Khurana; the politics of wine classifications in France and the United States (with Rebecca Elliott and Olivier Jacquet). A final book-length project, Measure for Measure: Social Ontologies of Classification, will examine the cultural and institutional logic of what we may call “national classificatory styles” across a range of empirical domains.\n\nFourcade is also an Associate Fellow of the Max Planck-Sciences Po Center on Coping with Instability in Market Societies (Maxpo), and a past President of the Society for the Advancement of Socio-Economics (2016).\n\nYou can read more about Prof. Fourcade’s work and accomplishments here.\n\nMichael Littman\n\nProfessor of Computer Science at Brown University\n\nMichael Littman\n\nProfessor of Computer Science at Brown University\n\nMichael Lederman Littman works mainly in reinforcement learning, but has done work in machine learning, game theory, computer networking, partially observable Markov decision process solving, computer solving of analogy problems and other areas. He is currently a professor of computer science at Brown University.\n\nBefore graduate school, Michael worked with Thomas Landauer at Bellcore and was granted a patent for one of the earliest systems for Cross-language information retrieval. Michael received his Ph.D. in computer science from Brown University in 1996. From 1996 to 1999, he was a professor at Duke University. During his time at Duke, he worked on an automated crossword solver PROVERB, which won an Outstanding Paper Award in 1999 from AAAI and competed in the American Crossword Puzzle Tournament. From 2000 to 2002, he worked at AT&T. From 2002 to 2012, he was a professor at Rutgers University; he chaired the department from 2009-12. In Summer 2012 he returned to Brown University as a full professor.\n\nMoritz Hardt\n\nAssistant Professor of EECS at UC Berkeley\n\nMoritz Hardt\n\nAssistant Professor of EECS at UC Berkeley\n\nMoritz Hardt is an Assistant Professor in the Department of Electrical Engineering and Computer Sciences at the University of California, Berkeley. Hardt investigates algorithms and machine learning with a focus on reliability, validity, and societal impact. After obtaining a PhD in Computer Science from Princeton University, he held positions at IBM Research Almaden, Google Research and Google Brain. Hardt is a co-founder of the Workshop on Fairness, Accountability, and Transparency in Machine Learning (FAT/ML) and a co-author of the forthcoming textbook “Fairness and Machine Learning”. He has received an NSF CAREER award, a Sloan fellowship, and best paper awards at ICML 2018 and ICLR 2017. You can read more about Prof. Hardt’s work and accomplishments here.\n\nNika Haghtalab\n\nAssistant Professor in the Department of Computer Science at Cornell University\n\nNika Haghtalab\n\nAssistant Professor in the Department of Computer Science at Cornell University\n\nNika Haghtalab an Assistant Professor in the Department of Computer Science at Cornell University. She works broadly on the theoretical aspects of machine learning and algorithmic economics. She especially cares about developing a theory for machine learning that accounts for its interactions with people and organizations, and the wide range of social and economic limitations, aspiration, and behavior they demonstrate. Prior to Cornell, Nika was a postdoctoral researcher at Microsoft Research, New England, in 2018-2019. She received her Ph.D. from the Computer Science Department of Carnegie Mellon University, co-advised by Avrim Blum and Ariel Procaccia. Her thesis titled Foundation of Machine Learning, by the People, for the People received the CMU School of Computer Science Dissertation Award (2018) and a SIGecom Dissertation Honorable Mention Award (2019).\n\nNiko Kolodny\n\nProfessor of Philosophy at UC Berkeley\n\nNiko Kolodny\n\nProfessor of Philosophy at UC Berkeley\n\nNiko Kolodny is Professor of Philosophy at UC Berkeley. He works in moral and political philosophy. He has written on the ethical significance of personal relationships and the nature of rationality, especially instrumental rationality. More recent papers focus on the value of democracy, the justification of the state, and the future of humanity. He has designed a new course for both philosophy and data science majors, “Moral Questions of Data Science,” which explores, among other things, the implications of replacing human judgment in social decision-making with statistical inference and automated algorithms.\n\nOwain Evans\n\nResearch Lead (new AI Safety group in Berkeley)\n\nResearch Associate, Oxford University\n\nOwain Evans\n\nResearch Lead (new AI Safety group in Berkeley)\n\nResearch Associate, Oxford University\n\nOwain Evans is an AI researcher specializing in AI alignment and AGI risk. Currently, he leads a research group in Berkeley, focusing on evaluating situational awareness, deception in Language Models (LLMs), and the truthfulness of AI systems. Prior to his role in Berkeley, he worked full-time on AI Alignment at the University of Oxford (FHI) and obtained his PhD from MIT. He also serves on the Board of Directors at Ought. Owain Evans shares regular research updates on Twitter and mentors researchers through the MATS and Astra fellowships, which offer funding and office space in Berkeley. His previous mentees have achieved notable success.\n\nRediet Abebe\n\nAssistant Professor of Computer Science at the University of California, Berkeley\n\nRediet Abebe\n\nAssistant Professor of Computer Science at the University of California, Berkeley\n\nRediet Abebe is a Junior Fellow at the Harvard Society of Fellows and an incoming Assistant Professor of Computer Science at the University of California, Berkeley. Abebe holds a Ph.D. in computer science from Cornell University as well as graduate degrees from Harvard University and the University of Cambridge. Her research is in the fields of artificial intelligence and algorithms, with a focus on equity and justice concerns. She co-founded and co-organizes Mechanism Design for Social Good (MD4SG), a multi-institutional, interdisciplinary research initiative working to improve access to opportunity for historically disadvantaged communities. Abebe’s research has informed policy and practice at the National Institute of Health (NIH) and the Ethiopian Ministry of Education. Abebe has been honored in the MIT Technology Reviews’ 35 Innovators Under 35, ELLE, and the Bloomberg 50 list as a “one to watch.” She has presented her research in venues including National Academy of Sciences, the United Nations, and the Museum of Modern Art. Abebe co-founded Black in AI, a non-profit organization tackling representation and inclusion issues in AI. Her research is deeply influenced by her upbringing in her hometown of Addis Ababa, Ethiopia.\n\nRosie Campbell\n\nTechnical Program Manager at OpenAI\n\nRosie Campbell\n\nTechnical Program Manager at OpenAI\n\nRosie Campbell currently works as a Technical Program Manager at OpenAI. Prior to working at OpenAI, she used to be the Assistant Director of CHAI. Her academic background is in Computer Science (MSc) and Physics (BSc) and includes some Philosophy and Machine Learning. She is motivated by the long-term and short-term challenges of aligning AI with human interests, but optimistic about the benefits of friendly AI. Before joining CHAI, Rosie worked as a Research Engineer at BBC R&D in Manchester, UK. Here, she worked on a variety of technical research and development projects, including a leading role on a team exploring artificially intelligent production systems. While in Manchester Rosie co-founded the BBC’s Machine Learning Special Interest Group, as well as Manchester Futurists, a thriving community group exploring the impact of emerging technology on society and the future.\n\nSiddharth Srivastava\n\nAssistant Professor of Computer Science at Arizona State University\n\nSiddharth Srivastava\n\nAssistant Professor of Computer Science at Arizona State University\n\nSiddharth Srivastava is an Assistant Professor of Computer Science at the School of Computing, Informatics, and Decision Systems Engineering at Arizona State University. He received his Ph.D. in Computer Science from the University of Massachusetts Amherst. Previously, Prof. Srivastava was a Staff Scientist at the United Technologies Research Center in Berkeley. Prior to that, he was a postdoctoral researcher in the RUGS group at the University of California Berkeley. His research objective is to develop intelligent robots and software agents that assist humans in their daily lives. Towards this objective, his research focuses on developing formal frameworks, algorithms and implementations that allow autonomous agents to reason and act efficiently under uncertainty. His dissertation work received a Best Paper award at the International Conference on Automated Planning and Scheduling (ICAPS) and an Outstanding Dissertation award from the Department of Computer Science at UMass Amherst.\n\nYou can read more about Prof. Srivastava’s work and accomplishments at his website.\n\nTom Lenaerts\n\nProfessor at the Université Libre de Bruxelles\n\nTom Lenaerts\n\nProfessor at the Université Libre de Bruxelles\n\nTom Lenaerts, PhD, is Professor at the Université Libre de Bruxelles (ULB) where he is co-heading the Machine Learning Group (MLG). MLG targets machine learning, AI and behavioural intelligence research focusing on time series analysis, causal and network inference, collective decision-making, social AI and behavioural analysis with applications in finance, medicine, cybersecurity and biology. He is currently the director of the Interuniversity Institute of Bioinformatics in Brussels and also holds a partial affiliation as research professor with the Artificial Intelligence Lab of the Vrije Universiteit Brussel, the Flemish counterpart of the ULB. He is vice-chair of the Benelux Association for Artificial Intelligence. He has worked in a variety of interdisciplinary domains and has co-authored many papers in AI in the different areas like evolutionary optimisation, collective intelligence, evolutionary game theory, computational biology and bioinformatics.\n\nVincent Corruble\n\nAssociate Professor at Sorbonne Université\n\nVincent Corruble\n\nAssociate Professor at Sorbonne Université\n\nVincent Corruble received degrees in Engineering (Ecole Centrale de Lille, France), a M.S. in Systems Engineering (University of Virginia, 1992), a Ph.D. in Artificial Intelligence (UPMC, Paris 6, 1996). After postdocs in South Africa, in the US, and in the UK, he joined the faculty of UPMC (now Sorbonne Université), LIP6 laboratory.\n\nHe has contributed to the field of Machine Learning and Pattern Recognition, Machine discovery and Data Science, with applications to history of medicine and medical research. In the early 2000’s, he turned his attention to intelligent agents, especially learning agents and multi-agent systems, with a special interest for multi-agent reinforcement learning in complex systems. This found significant applications in the area of Game AI and later in city modeling and urban simulation. This also led to a focus on virtual characters and agent architectures, with emphasis on affective computing to model emotions and their interplay with personality and social interactions.\n\nLately Vincent Corruble has developed interest in AI safety, and more specifically on how agents and humans can develop safe cooperation and trust over time. In this context he visited CHAI during the summer of 2019. He has been member of the Multi-Agent Systems group at LIP6 (Sorbonne Université, Paris) since 2007.\n\nWesley Holliday\n\nProfessor of Philosophy at UC Berkeley\n\nWesley Holliday\n\nProfessor of Philosophy at UC Berkeley\n\nWesley Holliday is an Associate Professor of Philosophy and Faculty Member of the Group in Logic and the Methodology of Science at the University of California, Berkeley. He currently serves as the Chair of the Group in Logic and co-organizer of the Berkeley-Stanford Circle in Logic and Philosophy. Prof. Holliday has worked mainly in formal philosophy and logic, especially modal logic, intuitionistic logic, epistemic logic and epistemology, logic and natural language, logic and probability, and logic and social choice theory. His recent research includes voting theory and computational social choice. Read more on his website.\n\nGraduate Students\n\nAlyssa Li Dayan\n\nPhD Student, Fall 2020 - Present\n\nAlyssa Li Dayan\n\nPhD Student, Fall 2020 - Present\n\nAlyssa Li Dayan is a Computer Science PhD student at UC Berkeley. She received her BS in mathematics with computer science from Massachusetts Institute of Technology in 2018. During undergrad she did research in systems neuroscience and computational cognitive science, before spending a couple years in industry working on prediction and simulation for autonomous vehicles.\n\nAnand Siththaranjan\n\nPhD Student, Fall 2021 - Present\n\nAnand Siththaranjan\n\nPhD Student, Fall 2021 - Present\n\nAnand is a second year graduate student advised by Stuart Russell and Claire Tomlin. He is interested in leveraging ideas from control theory, learning, and economics as a means of creating principled, beneficial AGI. In particular, he hopes to develop a theoretical foundation for said intelligent systems such that they may efficiently achieve tasks in uncertain environments, and collaborate both effectively and safely with other agents and humans.\n\nIn his undergraduate degree, which was completed at UC Berkeley, his research interests focused on human-robot interaction, robust machine learning and learning-based control.\n\nArnaud Fickinger\n\nPhD Student, Fall 2019 - Present\n\nArnaud Fickinger\n\nPhD Student, Fall 2019 - Present\n\nArnaud began his PhD at UC Berkeley in 2019 and is supervised by Stuart Russell. He is interested in multiagent systems and is currently working on adding a social choice flavor to assistance games and creating complex behaviors via multiagent interaction. Arnaud received his MS in Artificial Intelligence and Advanced Visual Computing and his BS in Applied Mathematics and Computer Science from the École Polytechnique in Palaiseau, France. He is currently a Visiting Researcher at Facebook AI and has been a Visiting Research Fellow at Harvard University as well as a Software Engineering Intern at Leia Inc. in Menlo Park.\n\nBhaskar Mishra\n\nPhD Student, Fall 2023 - Present\n\nBhaskar Mishra\n\nPhD Student, Fall 2023 - Present\n\nBhaskar is a PhD student advised by Prof. Stuart Russell. He is currently interested in reinforcement learning theory with the aim of advancing the theoretical understanding of modern reinforcement learning algorithms which have exhibited strong performance experimentally. Before coming to Berkeley, Bhaskar graduated from the University of Florida with a dual degree in Mathematics and Computer Science. During undergrad, Bhaskar worked on research in empirical game-theoretic analysis and statistical learning theory while advised by Prof. Amy Greenwald from Brown University and Prof. Cyrus Cousins from UMass Amherst. Aside from academic work, Bhaskar enjoys rock climbing, playing ukulele, and reading philosophy.\n\nCassidy Laidlaw\n\nPhD Student, Fall 2020 - Present\n\nCassidy Laidlaw\n\nPhD Student, Fall 2020 - Present\n\nCassidy Laidlaw is a PhD student interested in human-AI cooperation, learning human preferences, and robustness in machine learning. He is particularly focused on better uncertainty quantification in preference learning, scalable methods for solving assistance games, and adversarial robustness against non-traditional threat models. He is the recipient of a National Defense Science and Engineering Graduate (NDSEG) Fellowship. Learn more at his website.\n\nErik Jenner\n\nPhD Student, Fall 2022 - Present\n\nErik Jenner\n\nPhD Student, Fall 2022 - Present\n\nErik is a PhD student advised by Stuart Russell. He is interested in developing techniques for aligning AI with human values that could scale to very powerful future AI systems. Most recently, he has worked on better understanding potential-based reward shaping and on distance measures between reward functions. Before joining Berkeley, Erik completed a MSc in Artificial Intelligence at the University of Amsterdam, and an undergrad in physics at the University of Heidelberg. You can learn more about his research at his website (https://ejenner.com).\n\nHanlin Zhu\n\nPhD Student, Fall 2021 - Present\n\nHanlin Zhu\n\nPhD Student, Fall 2021 - Present\n\nHanlin is a Ph.D. student in the Department of Electrical Engineering and Computer Science at UC Berkeley advised by Prof. Jiantao Jiao and Prof. Stuart Russell. Previously, he graduated from Yao Class at Tsinghua University. Hanlin's current research interests focus on machine learning theory, especially reinforcement learning theory, as well as their applications to real world problems. He is also interested in theoretical computer science and mechanism design. You can learn more about Hanlin on his website.\n\nJakub Grudzien Kuba\n\nPhD Student\n\nJakub Grudzien Kuba\n\nPhD Student\n\nKuba is a PhD student at Berkeley AI Research, advised by Pieter Abbeel. His goal is to discover the underlying algorithmic principles of intelligence and use them to develop robust AI systems that learn to solve problems from variously acquired data. His interests include Reinforcement Learning, Inverse Reinforcement Learning, and Meta-Learning.\n\nBefore coming to Berkeley, Kuba completed his BSc in Mathematics with Mathematical Computation at Imperial College London and MSc in Statistical Science at University of Oxford. In London, Kuba worked with Yaodong Yang and developed several theorems and algorithms for Multi-Agent RL. In Oxford, he worked with Jakob Foerster where he developed Mirror Learning - a theory of policy optimization for RL.\n\nFor more, see his Google Scholar.\n\nJessy Lin\n\nPhD Student\n\nJessy Lin\n\nPhD Student\n\nJessy Lin is a PhD student at Berkeley AI Research. She is interested in building machine learning systems that work well in the dynamic, interactive settings of the real world.\n\nJessy graduated with a double-major in computer science / electrical engineering and philosophy from MIT. Previously Jessy worked on human-inspired AI with the Computational Cognitive Science Group and on human-in-the-loop machine translation at Lilt. At MIT she organized many seasons of HackMIT with the hackathon community. Jessy also spent a summer with the Natural Language Understanding group at Google Research NY, advised by David Weiss. Learn more at her website.\n\nJiahai Feng\n\nPhD Student, Fall 2023 - Present\n\nJiahai Feng\n\nPhD Student, Fall 2023 - Present\n\nJiahai is a rising senior at MIT interested in the scientific endeavour of reverse engineering human intelligence to develop safe and robust artificial intelligence. Jiahai has previously worked on symbolic regression, mechanistic interpretability of transformers, and cognitive science-inspired methods of planning from natural language. You can find more about Jiahai here.\n\nJohannes Treutlein\n\nPhD Student, Fall 2022 - Present\n\nJohannes Treutlein\n\nPhD Student, Fall 2022 - Present\n\nJohannes is a first year PhD student advised by Stuart Russell. He is broadly interested in empirical and theoretical research to ensure that AI systems remain safe and reliable with increasing capabilities. He is currently working on investigating learned optimization in machine learning models and on developing models whose goals generalize robustly out of distribution. Previously, Johannes studied computer science and mathematics at the University of Toronto, the University of Oxford, and the Technical University of Berlin. He is also a former CHAI intern. For more information, visit his website.\n\nMicah Carroll\n\nPhD Student, Fall 2020 - Present\n\nMicah Carroll\n\nPhD Student, Fall 2020 - Present\n\nMicah Carroll is an Artificial Intelligence PhD student at UC Berkeley advised by Professors Anca Dragan and Stuart Russell. Originally from Italy, Micah graduated with a Bachelor’s in Statistics from Berkeley in 2019. He has worked at Microsoft Research and at the Center for Human-Compatible AI (CHAI). His research interests lie in human-AI systems: in particular measuring the effects of social media (and other recommenders) on users, and improving techniques for human-AI collaboration. You can find him on his website or on Twitter.\n\nNiklas Lauffer\n\nPhD Student, Fall 2021 - Present\n\nNiklas Lauffer\n\nPhD Student, Fall 2021 - Present\n\nNiklas is a PhD student starting in Fall 2021 advised by Stuart Russell and Sanjit Seshia. He's interested in human-AI cooperation, multiagent learning, and AI safety. In his research he uses tools from machine learning, game theory, and formal methods. Niklas received his BS in computer science and mathematics from the University of Texas at Austin in 2021, where he worked in the Autonomous Systems group under Ufuk Topcu. He also spent time at NASA Ames Research Center in the Planning and Scheduling Group. Learn more at Niklas' website.\n\nRachel Freedman\n\nPhD Student, Fall 2019 - Present\n\nRachel Freedman\n\nPhD Student, Fall 2019 - Present\n\nRachel Freedman is a PhD student advised by Professor Stuart Russell. Her research focuses on reinforcement learning and reward modeling as paths toward value aligned AI. She’s particularly interested in integrating methodologies and expertise from computer science, statistics, cognitive science and economics to resolve the inherently interdisciplinary challenge of ensuring that AI is beneficial for humanity. In her undergraduate thesis, Rachel adapted a kidney exchange algorithm to align with societal values by isolating the underlying moral frameworks in human responses to moral dilemmas. The resulting paper was awarded “Outstanding Student Paper Honorable Mention” at AAAI 2018, and will be published in the AI Journal in 2020. Rachel’s past work on this morally vital and complex topic makes her a great fit for CHAI’s long-term goal of benefiting society. Prior to joining CHAI, Rachel studied computer science, neuroscience, and philosophy at Duke University through the Robertson Scholars Leadership Program, and at Oxford University as a Registered Visiting Student. At Oxford she cofounded and ran an interdisciplinary existential risk discussion society that met at the Future of Humanity Institute. More details about her work are available at her website .\n\nScott Emmons\n\nPhD Student, Fall 2019 - Present\n\nScott Emmons\n\nPhD Student, Fall 2019 - Present\n\nScott is a PhD student advised by Stuart Russell. He has developed algorithms for simple and robust goal-conditioned RL agents, and he has theoretically studied the stability of cooperating AI agents. Currently, Scott is interested in natural language to teach AI agents about human values and make AI agents averse to seeking power. Learn more at his website.\n\nShreyas Kapur\n\nPhD Student, Fall 2022 - Present\n\nShreyas Kapur\n\nPhD Student, Fall 2022 - Present\n\nShreyas is a PhD student at Berkeley AI. He is interested in developing algorithms that can learn to construct rich, interpretable models of the world as quickly and robustly as humans do.\n\nShreyas got his undergraduate and Master's degrees at MIT, where he was part of the Computational Cognitive Science Group advised by Prof. Josh Tenenbaum, working on neurosymbolic techniques for building structured world models.\n\nDuring his time as an undergrad, he spent summers at DeepMind, Waymo, and worked on research projects under Prof. Pattie Maes at the Fluid Interfaces Group. He also helped organize HackMIT.\n\nShreyas's Webpage\n\nTu (Alina) Trinh\n\nMaster's Student\n\nTu (Alina) Trinh\n\nMaster's Student\n\nTu (Alina) Trinh is a master's student advised by Stuart Russell. She is interested in human-AI collaboration, AI robustness and capability, and applications of (inverse) reinforcement learning to real-world scenarios. Previously, she has also conducted research related to topic modeling and autonomous vehicle path-planning. Tu completed her undergraduate studies at UC Berkeley, where she received degrees in computer science and business administration with a minor in data science. During her summers, she interned at Salesforce, Amazon, and PEAK6, building large-scale software services and systems.\n\nYuxi Liu\n\nPhD Student, Fall 2020 - Present\n\nYuxi Liu\n\nPhD Student, Fall 2020 - Present\n\nYuxi Liu is a PhD student with interests in mathematics, theoretical physics, philosophy, and AI. Before coming to Berkeley, Yuxi graduated from Australian National University with degrees in mathematics and theoretical physics. Learn more at yuxiliu1995.github.io.\n\nAffiliated Graduate Students\n\nPulkit Verma\n\nThird Year PhD Student at Arizona State University\n\nPulkit Verma\n\nThird Year PhD Student at Arizona State University\n\nPulkit Verma is a Computer Science Ph.D. student working with Prof. Siddharth Srivastava at Arizona State University. His research focuses on the safe and reliable behavior of AI agents. He is currently investigating the minimal set of interfaces in an AI system that would enable a user to assess and understand the limits of its safe operability. As a starting point, his work shows that a primitive interface that lets an AI system execute high-level instruction sequences in simulators is sufficient to efficiently derive a user-interpretable model of the system in fully observable and deterministic settings. You can learn more about Pulkit’s work at his website.\n\nInterns\n\nDillon Sandhu\n\nIntern\n\nDillon Sandhu\n\nIntern\n\nDillon Sandhu is a PhD student at Duke University and an intern with CHAI. He works on reinforcement learning and representation learning. At CHAI, he is working on benchmarking LLM Agents’ abilities to yield control to avoid failure.\n\nAlexandra Souly\n\nIntern\n\nAlexandra Souly\n\nIntern\n\nAlexandra finished her undergraduate studies in Mathematics at the University of Cambridge in 2020, after which she worked at Microsoft as a Software Engineer for two years. She is currently pursuing a master’s degree in Machine Learning at University College London, with her thesis focusing on multi-agent RL. At CHAI, she will be working with Sam Toyer on producing a novel benchmark for value learning.\n\nAndrew Garber\n\nIntern\n\nAndrew Garber\n\nIntern\n\nAndrew completed his BA in statistics from Harvard College in 2024. At CHAI, he’ll be working with Scott Emmons and Rohan Subramani to extend the theory of cooperative inverse reinforcement learning to situations involving asymmetric information—for example, when the AI observes more of the environment than the human.\n\nAustin Tripp\n\nIntern\n\nAustin Tripp\n\nIntern\n\nAs of 2024, Austin is finishing his PhD at the Cambridge Machine Learning group focusing on machine learning for molecules. At CHAI he is modelling how human preferences can change over time. More information is available on his website: austintripp.ca\n\nAymane El Gadarri\n\nIntern\n\nAymane El Gadarri\n\nIntern\n\nAymane completed his MS and BS in Applied Mathematics and Computer Science at Ecole Polytechnique in France. As a CHAI intern, he is interested in bridging the theory and practice gap in deep reinforcement learning. Broadly, he is interested on using machine learning for better, fairer, and more robust decision-making under uncertainty. Prior to his internship, he conducted research on quantum computing for solving combinatorial optimization problems and on statistical learning for drug discovery. Aymane will start his PhD in Applied Mathematics at MIT in the fall. Visit his Linkedin profile here.\n\nJonathan Colaço Carr\n\nIntern\n\nJonathan Colaço Carr\n\nIntern\n\nI am a first-year master's student at McGill University and Mila, studying the fundamentals of learning from human feedback. My goal is to explore how AI models can learn from realistic human preferences.\n\nAt CHAI, I will work with Cam Allen on representation learning from human feedback. Representations are crucial to how robots understand the world, and learning the right ones is key to ensuring these agents are trustworthy.\n\nFollowing the CHAI internship, I will visit Prof. Ben Van Roy's lab at Stanford to continue my master's research on preference-based learning. Please visit my website for more information.\n\nJuan Liévano\n\nIntern\n\nJuan Liévano\n\nIntern\n\nJuan is a mathematician (Universidad de los Andes, Bogotá) with a master's degree in pure mathematics (IMPA, Rio de Janeiro). He has worked as an industry machine learning engineer at Quantil in Bogotá. As an intern at CHAI he will work with Dr. Benjamin Plaut on reinforcement learning algorithms designed to work in contexts where agent errors have irreversible costs.\n\nMartín Soto\n\nIntern\n\nMartín Soto\n\nIntern\n\nMartín is a graduate in Maths and in Physics, and finishing his MSc in Mathematical Logic, where he's pursuing research in Computational Complexity with Albert Atserias. He's spent the last year exploring theoretical and conceptual research avenues in AI Safety, with collaborators from OpenAI and London's Center on Long-term Risk, among others. He's now decided to focus on multi-polar failure modes, tackling them both from theory (Cooperative AI) and governance. That's why, at CHAI, he'll be working with Niklas Lauffer on multi-agent training dynamics. In the past, he's also researched Algorithmic Decision Theory, and AI Safety cause prioritization.\n\nMoha"
    }
}
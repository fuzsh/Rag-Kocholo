{
    "id": "dbpedia_7228_3",
    "rank": 87,
    "data": {
        "url": "https://medium.com/pronouncedkyle/agnostic-ish-my-search-for-faith-in-a-scientific-world-fccdf5c92021",
        "read_more_link": "",
        "language": "en",
        "title": "Agnostic-ish: My Search for Faith in a Scientific World",
        "top_image": "https://miro.medium.com/v2/resize:fit:984/1*4it8MvT0Hczif4sjjgW_uw.png",
        "meta_img": "https://miro.medium.com/v2/resize:fit:984/1*4it8MvT0Hczif4sjjgW_uw.png",
        "images": [
            "https://miro.medium.com/v2/resize:fill:64:64/1*dmbNkD5D-u45r44go_cf0g.png",
            "https://miro.medium.com/v2/resize:fill:88:88/1*1YMv3aHMb9656ZEEXM2KQw.jpeg",
            "https://miro.medium.com/v2/da:true/resize:fill:48:48/1*u2hE7yMzEVnOXs31tjApCw.gif",
            "https://miro.medium.com/v2/resize:fill:144:144/1*1YMv3aHMb9656ZEEXM2KQw.jpeg",
            "https://miro.medium.com/v2/da:true/resize:fill:64:64/1*u2hE7yMzEVnOXs31tjApCw.gif"
        ],
        "movies": [],
        "keywords": [],
        "meta_keywords": [
            ""
        ],
        "tags": null,
        "authors": [
            "Christian Keil",
            "medium.com"
        ],
        "publish_date": "2019-04-09T15:47:20.174000+00:00",
        "summary": "",
        "meta_description": "I was 22 years old, a recent graduate from the University of Michigan where I had earned degrees in economics and psychology. I was kicking off my professional career as a management consultant, a…",
        "meta_lang": "en",
        "meta_favicon": "https://miro.medium.com/v2/5d8de952517e8160e40ef9841c781cdc14a5db313057fa3c3de41c6f5b494b19",
        "meta_site_name": "Medium",
        "canonical_link": "https://medium.com/pronouncedkyle/agnostic-ish-my-search-for-faith-in-a-scientific-world-fccdf5c92021",
        "text": "Falling Slowly (Out of Faith)\n\nAs my family settled into our new home, the years, mostly memorable for their distinctly Minnesotan winters, passed. They brought me a brother, Evan, a sister, Kelly, and the newfound eccentricities that come with being a card-carrying teenager. I got braces; I wore exclusively Hollister; I played Halo 2 every day after school under my gamertag Phoenix43. (Named, of course, after my favorite scent of AXE deodorant.)\n\nEven beneath it all, however, I was still fundamentally me.\n\nI read everything I could get my hands on: Matt Christopher, Magic Treehouse, Harry Potter. I read the latter religiously, usually as soon as I got home from the midnight release parties (and washed the lipstick-lightning-bolt scar off of my forehead). Academically, I was a serial monogamist, falling in love with new subjects at random. One year: pharaohs. The next: dromedary camels. My parents, who bore the brunt of my non-stop curiosity, fortified their defenses with the most powerful weapon in their arsenal: religion.\n\nMy parents believe in God. My dad is one of the happiest people who I’ve ever met, and he draws his joy from his belief. (His refrain every morning of our family vacations: “It’s a Glorious Day!”) He once told me that the happiest people he knows are strong believers, which makes sense: believing you’ll experience infinite happiness upon your bodily death has to be uplifting. It is for my dad: my church youth group leader, and my model for how to live a joyful life. My mom also believes, but in a different way: she thinks that God’s power is more for now than later, and her Christian faith has helped her weather the storms of life. (Even if recently, she’s loving the Zen of her yoga practice. Don’t tell on her.)\n\nMy parents are religious, as were their parents before them. That’s how it often happens, I suppose, and that’s how it was supposed to happen for me and my siblings. When my brother Evan, now a junior in college, was a teenager, he believed. He has since gone on a number of mission trips, and is the current President of the Campus Crusade for Christ (or CRU, he and his buddies call their rental home the “CRUplex”). Kelly, now 17, also believes. She’s still figuring everything out, but, like my mom, she finds strength and solace in her growing faith. I assume her faith will only strengthen as she matures into young adulthood; she and Evan both received the classic parent-child faith inheritance.\n\nAnd then, me. Christian (n): follower of Christ.\n\nBy nurture, my siblings and I were identical. We grew up in the same neck of the ‘burbs, we had the same role models, we went to church on Sundays and small group on Wednesdays and said the same prayer before every family dinner. If anything, I should have had more access to trickle-down faith than my siblings, given the undivided attention that comes with being a first-born child. And yet, something about my nature made it so that believing in God was never easy for me.\n\nIt’s not that I never believed. I suppose that as a child I believed just as meaningfully as the next kid. I very distinctly remember “pledging my life” to God as a seven-year-old when John Jacobs and the Power Team, a group of bodybuilders (or, equivalently, superheroes), came to town and promised that if I believed in God, I, too, could rip phonebooks in half. Naturally, I was all in. After those earliest years, however, I underwent a transformation that was unique in my family: I transitioned out of faith, and out of believing in the God of my role models, siblings, parents, and their parents before them.\n\nIf I had to say what finally did it — what “broke” inside me and caused me to stop believing in God — it was the realization that it didn’t matter whether I enjoyed believing, but whether or not what I believed in was true. In other words, as faith became less about superpowers and more about facts, it understandably lost its luster.\n\nAnd those religious facts — as I came to understand them from the Bible — seemed questionable, at best. There was the story of creation, in which a persuasive snake made Adam and Eve wear clothes. There was the story of Noah’s Ark, about which I had plenty of questions. How did they find all those species to begin with? And what about asexual animals — did they still take two on board? How early did the snails living in North America have to leave to make it to the Middle East before the flood? The moral stories of the Bible seemed equally dubious: when God asked Abraham to kill his son Isaac, he was like, “OK,” and when a woman looked back at a city burning in literal hellfire, God turned her into a pile of salt. If the truth of the Bible was what mattered, the stories that it held made religion a tough sell.\n\nWhat I was buying, conversely, was the scientific method. I loved my science classes in middle school. The facts of science worked for me because they were directly observable — and simple. Scientists didn’t ask me to take on faith that the moon was made of rock instead of cheese — they went up there and let Neil Armstrong confirm as much in person. Scientists also never told me that the answers were too complicated or beyond my understanding, as the church often seemed to do. Science may not have seemed simple originally, but simplification, I learned, is what science does best. Bill Nye taught me this one:\n\nGravity, perhaps the most basic theory of them all, makes the unfathomable prediction that everything — from the stars in the sky to the soles of your shoes — interacts. Gravity is strong enough to hold your soles to the pavement, and amazingly, it’s far-reaching enough to make those same soles alter the motion of the North Star. The effect is minuscule, but exists nonetheless because gravity is infinitely extensible. That all sounds complicated, so it would stand to reason that explaining it scientifically should be equally complex. And yet, Isaac Newton figured out how to describe it all using just two letters, two numbers, and three lines:\n\nThat’s it. The f-orce of gravity equals one over the squared d-istance between you and the Little Dipper. Incredible complexity in a simple equation: the inverse-square law of gravitation. Science rules! (Bill — Bill — Bill — Bill!)\n\nThe scientific method took the incomprehensibly big universe and explained it to me: a skeptical little nerd who had just recently learned the truth about the big jolly guy who does his shopping in December. As I aged into high school and even into college, that idea — that science, not religion, might be my ticket to understanding the stars — was corroborated over and over again.\n\nScience had answers where faith had mysteries, and facts where faith had stories. The difference, really, was that science could work for me without asking for anything in return. Supposing that I found a way to the moon — looking at you, Elon Musk — I could confirm the discoveries that Neil Armstrong made in 1969. If I found a way to the top of Mount Sinai, in contrast, I couldn’t independently happen upon the Ten Commandments (barring a gift shop at the summit). Why take answers on faith when science could help me discover the real truth?\n\nAs I grew up, that “real truth” of the world started to crystallize. As every young adult does, I had it all figured out; the story as I knew it went a little something like this.\n\nIn the beginning, God spoke the truth. He whispered into the void and brought about the heavens and the earth. God’s universe was full of wonders and majesties beyond explanation, and even though we didn’t understand them, we could count on them. The sun rose and set each day, rainbows followed storms, the tide went in and out. We were at the center of the universe, and His gifts — sunsets, rainbows, and crashing tides — were signs of His everpresence and good will. God kept the wheels turning, so He was worthy of our faith.\n\nBut then, science happened.\n\nIn 1543, Copernicus shattered the “celestial orbs” by discovering that the Sun, not the Earth, is the center of our solar system. In 1859, Darwin published the Origin of Species, and showed that life’s diversity can emerge from simplicity. In 1900, Freud published The Interpretation of Dreams, and made clear the opaque world inside our own brains. This scientific enlightenment, however, came with a considerable shadow — one that fell squarely over God’s truth of how things worked. Copernicus displaced humans from the center of the universe, Darwin displaced humanity from our place atop the Earth, and Freud displaced us from our own brains. Modern science paints humanity not as a prized creation of a loving God, but as one of nine million species living on a randomly-chosen planet among the infinitely many others in the universe.\n\nGod may have explained the world in the beginning, but science ultimately found better explanations. Led by science, humanity stepped out of the shadows and into the light.\n\nAs a teenager about to make that fateful transition into skepticism and out of faith, I felt that I was doing the same. I knew enough about religion to doubt, and enough about science to trust; the balance in the Force had shifted. I had a deal with my parents: make it to Confirmation (akin to youth-church graduation) and I could decide for myself if I wanted to remain a part of the church. I counted down the days — passing the time each Sunday by reading Revelations, by far the craziest, most inexplicable chapter of the Bible — and when Confirmation finally arrived, my decision was made: I stopped going to church faster than I stopped taking piano lessons.\n\nFor a long while thereafter, I didn’t look back.\n\nFamily Dinner\n\nIn those earliest years, I was a nerd — as you know — but at least part of me was still in denial. I didn’t just love books; I was an athlete! I played basketball, baseball, football, fútbol, golf. You name it, I tried it. I was even moderately decent at half of them. (Note to self: potential resume bullet: “moderately decent half of the time.”) Unfortunately, my illustrious sporting career came to an early demise when I got to high school and still stood roughly five feet tall — the closest I came to athletics during my freshman year was when my math teacher, the football coach, explained what a parabola was by mapping the trajectory I would follow if my teammates threw me over the line to block an extra point.\n\nWith sports out of the question, I knew that I needed a new way to channel my vast stores of competitive energy. After some aggressive prodding from my parents, I found myself at a tryout for the Lincoln-Douglas debate team. It was a curveball, but I immediately found out that, yes, ye with middle-school senses of humor, I was something of a master debater. Sports might not have worked out for me, but as it turned out, I was pretty good at arguing. Debate was addictive: intellectually stimulating, intensely competitive, and surprisingly physical (most competitive rounds featured debaters speaking at over 300 words per minute). I was hooked.\n\nNowhere was this more evident than during family dinners. We’d say a prayer, allow a grace period of unadulterated chowing down, and chat about whatever happened that day. Mostly, it would be the usual: what fish my brother caught on the lake, how my sister’s musical was going. But sometimes when I got bored, I’d turn our casual conversations towards confrontation. Exactly the way I wanted it.\n\nDuring one table-side debate during my junior year of high school, I remember very clearly thinking that I was about to score a major victory. My dad was arguing against evolution, claiming that while micro-level changes within species were possible, they could never add up to a real macro-level change between species. I was arguing for evolution, of course, because I felt that I knew the science. What I really remember, though, was not my inevitable victory and subsequent crowning as the King of the Table, because that never happened. I realized about halfway through our argument that no matter what I said, my dad wasn’t going to admit defeat. He wouldn’t even budge. I told him that scientists had confirmed speciation with viruses in the lab — and he didn’t care. I said that scientists had observed evolution creating two species from one that became geographically separated by a natural disaster. Nothing. Example after example, argument after argument, and he didn’t waver.\n\nMy dad’s steadfastness was frustrating and mirrored what I had seen on a grander scale in public debates on YouTube: atheists winning arguments about science, and believers holding fast to their faith. The debates between my dad and I were always respectful, but the ones I watched online were anything but.\n\nIn the debate between religion and science, the New Atheists — a team of vocal non-believers led by Daniel Dennett, Sam Harris, Christopher Hitchens, and Richard Dawkins — were out for blood. I disliked their tone, but I sided with their argument. The New Atheists claimed that the religious faithful were blind — that they believed without evidence and even in the face of contradiction. Harris, a neuroscientist, wrote:\n\nTell a devout Christian that his wife is cheating on him, or that frozen yogurt can make a man invisible, and he is likely to require as much evidence as anyone else and to be persuaded only to the extent that you give it. Tell him that the book he keeps by his bed was written by an invisible deity who will punish him with fire for eternity if he fails to accept its every incredible claim about the universe, and he seems to require no evidence whatsoever.\n\nWhile believing strongly, without evidence, is considered a mark of madness or stupidity in any other area of our lives, faith in God still holds immense prestige and power in our society.\n\nThat was the real problem that I had with my dad, and I told him as much. How could he deny scientific evidence? Who was he to doubt the experts?\n\nSurprisingly, he turned that accusation right back at me. He said that he didn’t think the evidence existed — even though I knew that it did — and that if I cared to look, I would find plenty of scientists who believed in God.\n\nI left that debate indignant. I hadn’t changed my dad’s mind — not even after I showed him some (admittedly sketchy) scientific papers (from Wikipedia) — which proved to me that his spiritual defenses were all but impregnable. My dad was an immovable object, and not even the unstoppable force of science could inch him closer to the truth.\n\nRoad Trips\n\nAs my high school years drew to a close, I had to leave debate even if debate never quite left me. I packed my bags, loaded them — and my family — into our minivan, and drove to Ann Arbor, Michigan: the best college town in America. I was a legacy at UM many times over — my mom and dad met there, and members of my extended family are scattered all over Michigan’s mitten — so I knew what was waiting for me in Ann Arbor.\n\nA2 had the Big House, the Law Library, the Cube. It had fraternity life (I pledged Sigma Chi; my parents were the social chairs of their respective houses), the greatest tailgates in the Big Ten, and a football team that, despite being just moderately decent half of the time, had Denard Robinson. That I would rush, tailgate, and attend every remotely-drivable football game were all but given as I came to Michigan as a bright-eyed, bushy-tailed freshman.\n\nWhat was not given, however, ended up being the highlight of my college career. Although I had never been in choir and wasn’t a singer, when a friend from my dorm asked me to audition for her a cappella group, I decided to give it the good ol’ college try. I sang the only a cappella song I knew (“Sweet Child O’ Mine,” from that car scene in Step Brothers), and by some miracle, the group — the Compulsive Lyres — liked what they heard. As with debate, I had no idea what I was getting into — but after my first a cappella retreat, I knew that a cappella would end up being one of my favorite things in the world.\n\na ca·ppella re·treat (noun):\n\nA twice-yearly weekend away at a rental home in northern Michigan. Common activities include: learning music, making sherberbalert, playing Drenga.\n\nRetreat was unforgettable. We sang and fiesta’d, as advertised, but also had the in-between moments in which friendship is made. We relaxed while watching a Michigan basketball game, played catch in the side yard, built snowmen, watched Office Space. It felt like we were all present, which is a luxury nowadays. Being locked in a cabin together in the tundra of Northern Michigan (and far away from cell service, at times) could do that to you.\n\nOne semester, I rode up to retreat with Lee Gunderson, a bass and future attendee of Princeton’s Graduate Program in Plasma Physics, and Charlie Frank, a tenor and future Michigan med student. About an hour into the trip, our small talk got pretty big, and we turned to life’s largest questions — animal rights, capital punishment, Obama. Eventually, we landed on the main event, a debate topic to make the Liberal Arts Admissions Committee proud: free will.\n\nCharlie argued that it existed, Lee disagreed, and I was just enjoying the ride. The argument started casually enough, but as the debate picked up steam, none of us were particularly willing to let it go.\n\n“Where is my free will?” Lee yelled, exasperated. “It’s not in any of my individual atoms. It’s not in any one of my brain cells. Where is it?”\n\n“It’s the whole thing — the combination,” Charlie countered. “Are you seriously trying to say that you don’t think you’re freely choosing to argue? You have free will, right now.”\n\nLee, visibly frustrated, paused — so I jumped in and paraphrased Friedrich Nietzsche, a German writer I knew from debate as the man with a philosophy as confusing as the pronunciation of his last name:\n\n“Do we have free will right now, though? We can’t choose to be back in Ann Arbor. We can’t choose to play basketball in the car. Our choices now are constrained by the choices we have made in the past, which were themselves constrained by choices in the deeper past. Take that all the way back, with each moment depending on prior decisions and actions. When was that first, original choice? We didn’t choose to create ourselves, so isn’t everything else constrained by that creation? Doesn’t that mean we don’t have free will?”\n\n(Aaaaaaaand like I said, Admission Committee’s dream. I said that, in real life.)\n\n“That actually makes sense,” Charlie said, after a moment. “But it’s literally the first thing you’ve said that has made any sense on this entire road trip.”\n\nOuch. Lee confirmed that he too thought I wasn’t making a ton of sense. I left them to finish their argument without me, electing to brood instead. Was I really so unpersuasive?\n\nAfter we got to the rental cabin, all was, obviously, forgotten. Retreat waits for no man. We had a great trip — complete with an epic snowball fight, during which I accidentally pegged a soprano from our group in the face (sorry, Jessica) — and were thoroughly exhausted from the long days and longer nights by the time it was over. I drew the short straw to drive home on Sunday morning, and geared up for the long, lonely drive.\n\nThere are Two Immortal Truths of the Hungover Road Trip:\n\nAt some point, everyone else in your car will fall asleep; When they do, you must have a plan in place for staying awake.\n\nMy plan was to let my mind wander. And I mean truly wander. I had time to think about everything — the fun of the past weekend, my excitement for the upcoming semester, the prospects for a Future Mrs. Christian Keil. My mind eventually wandered to less exciting memories, like the fateful debate on the car ride to retreat. Why should I have been embarrassed? Charlie’s rebuke was just a throwaway line. It shouldn’t have bothered me the way that it did.\n\nIn a (rare) moment of self-honesty, I realized why I had been so affected: Charlie was right. I wasn’t being very persuasive — or intelligible — at all. I could wax philosophic all day long, and I could surely sound like I had something to say. But I was faking it. I really had no idea why I believed what I believed, and Charlie and Lee both knew it. I thought back to similar conversations about faith and how I couldn’t add anything personally meaningful to them. I thought back to when I accepted a role as the Vice President of the Young Republicans club at my high school before I even knew what it meant to be a Republican. Although I was characteristically, if blindly, confident in my ability to make sense of topics like religion, politics, and science, I was woefully outgunned when it mattered — because I didn’t really know what I was talking about.\n\nWas this soul-searching a rational response to an off-hand comment? Probably not. Definitely not. It changed my outlook, though, all the same. After the table-side debate with my dad, I was supremely confident that I was right about science, God, and the real truth of it all. After getting called out by my college friends and having some time to introspect, however, I started to doubt.\n\nIn retrospect, maybe I should have just woken up one of my sleepy passengers.\n\nThe Final Straw\n\nThe truth, as I understood it, was that science had replaced God. Gravity held the planets in orbit; natural selection brought about new species; science had assumed its role as Atlas and held the Earth in place — and so, God was unnecessary.\n\nOne step more fundamentally, I also thought that the tradeoff between science and God was unavoidable. Science was a worldview — a sufficient explanation of the world. Religion was another. By the nature of a worldview, then, you couldn’t believe in both. How could you believe in miracles and the laws of physics? Or the seven days and the Big Bang?\n\nBut did I really know these things, or were they just blind opinions? I hoped they were the former. But I had to find out. I was still milking a free AmazonPrime membership, so thankfully, edification was only $20 and two days away. I bought a few books online, and figured that they would be everything I needed. I’d read them, come to the inevitable conclusion that science and religion were at war and science was the winner, and move on with my life.\n\nBut, I figured incorrectly. Oh boy, did I figure incorrectly.\n\nTo my surprise, my science vs. religion narrative was — to put it bluntly — wrong. In the very first book I picked up, I was introduced to the “non-overlapping magisteria” hypothesis: the well-known, if pretentiously-named, contention that the bubbles of science and religion do not form a Venn diagram. Instead, the two domains (or “magisteria”) are entirely independent. In the words of Stephen Jay Gould, a Harvard paleontologist, “NOMA” gives science the age of rocks, and religion the rock of ages. Two worldviews, peacefully co-existing.\n\nIn my eyes, though, that co-existence was impossible; I rejected NOMA outright. I remember thinking that the idea was based on the same sort of scientific ignorance that I recognized in my dad during our table-side debate. I understood the intuitive appeal of giving morality to religion and facts to science, but the downstream implications were all sorts of unacceptable. Science wouldn’t knowingly distance itself from morality and beauty — what of the evolutionary study of moral behavior, or the science of aesthetics, or the universally acknowledged power of simplicity (an artistic quality) in scientific theory? And on the flip side, would religion really want to distance itself from the facts? What if scientists discovered DNA evidence that linked Jesus to the physical places in which the Bible said he ought to have been? Could believers really say “sorry, no, wrong magisterium”?\n\nThe above are decent arguments against NOMA. (Yay, debate!) But, no matter how many counterpoints I could dream up, there was one fact about NOMA that bothered me: it was popular. Both believers and scientists supported the hypothesis.\n\nAfter my conversations with Charlie, Lee, and my dad, my straw-laden camel was struggling but his back remained unbroken. I still thought that anybody who understood science could never be a believer in God. That belief was strong, and while stewing in my liminal state between agnosticism and atheism I had no real reason to doubt. Until now. Enter: the final straw. Enter: the scientist-believer.\n\nAccording to a study conducted by the Pew Research Center, more than half of all scientists believe in God or a “universal spirit or higher power.” More than half. What?! Could that possibly be true?\n\nI hoped not. If it was, I was really in the wrong, so I thought of every conceivable way to reconcile my cognitive dissonance. Maybe those scientist-believers were just masters of compartmentalization: scientists by day, believers by night. Maybe they were believers like Albert Einstein, who was commonly cited as a believer, and probably believed in some “divine” personification of mathematics, but who always denied accusations that he believed in a personal God. That last point might have done it, if not for the pesky facts: other studies found that about forty percent of scientists believe in a personal God.\n\nI even wondered if maybe this unseen majority of scientists was entirely comprised of those dentists who don’t think that Colgate helps fight gingivitis — i.e., total hacks. Alas, that defense, too, was permeable. In fact, the more I searched, the more I realized that these scientist-believers actually had some total geniuses among their ranks.\n\nGerman physicist Werner Heisenberg, the godfather of quantum physics, said,\n\nI had the feeling that, through the surface of atomic phenomena, I was looking at a strangely beautiful interior, and felt almost giddy at the thought that I now had to probe this wealth of mathematical structure nature had so generously spread out before me… If nature leads us to mathematical forms of great simplicity and beauty… we cannot help thinking that they are “true”, that they reveal a genuine feature of nature.\n\nErwin Schrödinger, the Nobel Prize-winning quantum physicist perhaps best known for his cat, wrote:\n\nI am very astonished that the scientific picture of the real world around me is very deficient. It gives a lot of factual information, puts all our experience in a magnificently consistent order, but it is ghastly silent about all and sundry that is really near to our heart, that really matters to us. It cannot tell us a word about red and blue, bitter and sweet, physical pain and physical delight; it knows nothing of beautiful and ugly, good or bad, God and eternity.\n\nThere were also John Lennox, a Fellow in Math and Philosophy of Science at Oxford, and Allan Sandage, winner of the Nobel Prize-equivalent in Astronomy, who believed. John Polkinghorne was a former Cambridge physicist that now serves as an Anglican priest. I even learned that Francis Collins — the leader of the first team to map the human genome and the current head of the National Institute of Health — believes in God. He wrote:\n\n…those of us who are interested in seeking harmony here have to make it clear that the current crowd of seemingly angry atheists, who are using science as part of their argument… do not necessarily represent the consensus of science; the assault on faith, which has been pretty shrill in the last couple of years, is coming from a fringe — a minority — and is not representative of what most scientists believe.\n\nBack: broken. My belief in the power of science was the main reason that I didn’t believe in God. But if most scientists could find a way to reconcile the two, then why couldn’t I?\n\nI did the math. Estimates put the number of scientists in the United States at approximately six million. Half means three million. Three million means approximately thirty Big Houses full of experts that were better educated than I was — all of whom disagreed with my assessment on science and religion. Was I was really trying to say that those three million scientists — and my family, friends, and others whom I loved and respected — were wrong?\n\nThe truthful answer is that I just didn’t know. I didn’t want to believe that I had been so wrong, but the evidence against my truth was mounting. I realized that unless I wanted to either admit defeat or ignore the new evidence that I had found, I would have to do something about it. So, I did. And here we are.\n\nPart Two: Psych!\n\nAnd Consciousness, and soul power\n\nTHE WAY I SEE IT, the rise of consciousness is among the three most important things that have ever happened. Ever, meaning, in the history of the universe.\n\nFirst, everything began. The universe was smaller than the period at the end of this sentence, then exploded and became so large that we aren’t entirely convinced that it’s not infinite. This one is a given. Without the beginning, nothing would have happened.\n\nSecond, life began. The universe designed a carbon-based, self-replicating 3D printer from scratch. Again: amazing, enabling, self-evident. A natural second on our list of the most important things ever.\n\nBut even after those two things had happened, our universe was still only dead machinery. It was big and, at least on our planet, thriving — but it was just a machine. Nobody was home. The universe had expanded into infinity, but it couldn’t appreciate it; the universe had designed life, but it wasn’t truly alive. Until it designed consciousness.\n\nIt’s a relatively controversial third item, to be sure, but a natural one if you think about it. Once conscious, the universe figured out what it was. For the first time in billions of years, the universe had a mirror — a way to look back at its creation and to reflect upon how incredible its infinite expanses had become.\n\nWhen I was in high school, I fell in love with psychology and the study of the brain, that miraculous bulb of dread and dream that so fundamentally changed the universe into which it was born. I learned the classics in AP Psych: Milgram’s shocks, Stanford’s prisoners, Pavlov’s dogs. At Michigan, I declared a psych major and focused on cognitive science and psychopathology. The latter — the study of broken minds — was fascinating, even if my own consciousness got in the way of truly enjoying it: I caught a bad case of “med school student syndrome,” the disorder that causes you to diagnose yourself with each new disorder you learn. Oops. (The great irony is that med school student syndrome is, of course, one of those very disorders.) Despite my reflexive pathologies, I loved my time studying psych at Michigan. The brain is fascinating, and seemingly infinite.\n\nMost college love stories are short-lived, however, and my affair with psychology was no different. Psych was academically fascinating, but the career prospects were, shall we say, less than. So, when I left college, I chose business — exploiting my economics major and statistics minor to secure an offer in management consulting — and more or less left psych in the dust.\n\nAs a newly minted consultant, life was decidedly non-academic. From day one, you’re asked to head to a client site and advise people who have been in their careers longer than you’ve been alive. You have to get up to speed fast, no matter how complicated the industry or business problem in which you find yourself. I loved consulting — it was fast-paced, and I learned a whole lot in a very short period of time. But, even so, I still needed an outlet for my academic energy. I still read every night before bed — no matter how late I got back to my hotel — and despite posting periodic life updates on LinkedIn, I had yet to fully quench my thirst for reading, writing, and wondering. When I realized that I could rekindle my relationship with psychology through the process of writing this book, I was pumped! My degree would come in handy after all, and in a (nearly) real-life context, no less.\n\nMy realization that psychology and the study of consciousness had a lot to do with religion came, strangely enough, from the Bible. I had just finished John Lennox’s Seven Days That Divide the World, a short book about how a modern scientist might interpret, and still believe in, Genesis. The book was intriguing enough to make me want to do my own research, and I ambitiously set off to read the entire Bible, despite not having picked up the good book in years. One night after work, I started in the beginning and was immediately blindsided by inspiration. In fact, I got so side-tracked that I never made it further than the 26th verse of Genesis 1:\n\nThen God said, “Let Us make man in Our image, according to Our likeness; and let them rule over the fish of the sea and over the birds of the sky and over the cattle and over all the earth, and over every creeping thing that creeps on the earth.” God created man in His own image, in the image of God. He created him; male and female He created them. God blessed them; and God said to them, “Be fruitful and multiply, and fill the earth, and subdue it; and rule over the fish of the sea and over the birds of the sky and over every living thing that moves on the earth.”\n\nI was struck by the sheer insistence that we were made “in God’s image.” That it was thrice repeated was enough to make me wonder: what is God’s image? What do we have that other animals do not?\n\nThe only logical answer that I could surmise was that our conscious minds distinguished us. Was there really any alternative? We are, as far as we know, the only conscious beings in the universe. Other animals are more prolific (bacteria, ants), longer-living (turtles, Aspen trees), and larger (blue whales, fungi). Some animals even have larger brains — but our brains have more gray matter than any other species. That’s the brain stuff that comprises our cortexes and is the seat of our higher functions like, say, consciousness. More simply, it seems to me a patently ridiculous idea that God would have made us superficially look like him — I don’t think that God has arms or legs or a beard — but it was more believable that we should think as he thinks: with intelligence, with a moral sense, and with conscious self-awareness.\n\nWhether or not we are “made in God’s image” is a question that science probably could never answer — but whether we are conscious is an entirely different story. That is a question that modern psychology can handle, and from my perspective, a weak point in the armor of the religious story.\n\nWhat if I proved that consciousness could be entirely explained by science? In that case, science wouldn’t need God’s help, and the claim that a supernatural being had implanted self-awareness in our brains would be fanciful, but untrue. Similarly, what if consciousness were purely physical? Part of the religious claim also seemed to be that this God-like mental resemblance could survive our physical deaths. Everyone who believes in God “shall never perish, but have eternal life.” If our selves were confined to this purely physical, materialistic consciousness, would that disprove the claim that God staked over our immortal souls?\n\nI wasn’t sure, but the possibility of finding hard physical evidence against such crucial pieces of the religious belief system — and in a subject that I understood well, no less — was enticing. So, just like that, I jumped back into the science of psychology that I loved, intending (perhaps naively) to disprove God’s story.\n\nThe Delusion\n\nThe story of religion, science, and consciousness starts back in the day. And I mean way back in the day, when we thought that brains were pretty worthless.\n\nThe ancient Egyptians famously pulled out their Pharaohs’ brains before mummification. In the thousands of years after the last Pharaoh (Cleopatra), humanity managed to discover algebra, refraction, supernovae, magnetism, and circulation, but even then, our barbarically incorrect understanding of the brain persisted. In 1377, the Bethlehem Royal Hospital (better known as “Bedlam”) became one of the first institutions to treat mentally ill patients. Patients at the hospital were treated more like prisoners of war than people in need of help: doctors would strap patients to “The Chair” and spin them until they passed out from dizziness, or subject patients to “trepanation,” drilling holes in their heads à la Saw VI (just because).\n\nPerhaps the first notable breakthrough in understanding the brain mercifully came in the 17th century, courtesy of Frenchman and philosopher René Descartes — the man behind cogito ergo sum: “I think, therefore I am.”\n\nDescartes was a dualist, which means that he thought that the mind and the body were two fundamentally different substances. The body was tangible and interacted with the world through feet on pavement and fingers on guitar strings. The mind, conversely, only thought — to Descartes, thinking was the “principal attribute” of the mind. Minds were not physical objects, then, but ephemeral thought-bearers.\n\nAs it turns out, this “one idea per millennium” quota for consciousness research seems to still be the trend. If you agree with Descartes’ dualism, even today, you are in the majority. One Belgian study found that 60% of people believe that the mind and body are distinct.\n\nI imagine that even more people implicitly believe in dualism than would admit it outright — take the modern perceptions of mental and physical health, for instance. On a work project a few years ago, a Senior Consultant on my team sprained a bone in his foot and had to wear a cast over his business casual attire for a couple of months. My team was very supportive of (let’s call him) Kevin and his limited mobility: we’d drop him off at the front door after our daily carpools, and we’d (normally) remember to take the elevator with him instead of the stairs. Contrast that with how a modern business might handle a breakdown in mental health. I have no personal anecdotes for this one, but I can’t imagine that most companies allow for “depressed days.” Even family members and otherwise loving, supporting friends might tell someone who suffers from anxiety to just “shake it off.” I can’t imagine what would have happened if we had said the same thing to Kevin. The stigma of mental illness is almost surely a result of an implicit dualistic mindset; how else could the two types of disease be treated so differently?\n\nThe way that I saw it, modern psychologists, in contrast to the general public, thought that while it is definitely a special kind of matter, the brain is still just matter. Mental illnesses are physical illnesses because the brain is just as physical an object as your femur. Neurotransmitter levels can be out of whack just as your sinuses can be clogged. This faux-distinction between matter and mind is a “dualistic delusion”: we think our brains are distinct from our bodies. But time has taught the scientific community that dualism isn’t true.\n\nAs far as modern science knows, there is no way for an intangible mind (or “consciousness,” or “soul”) to interact with a physical brain. There isn’t a receiver in our brains that can tune in to the channels of an intangible ether. Psychologists know that the brain is such a highly dispersed and decentralized network that there isn’t even a command center that could use “soul signals,” even if they existed and could be received. Scientists used to think that there was perhaps a “homunculus,” or “little man,” observing the internal theater of our mind’s eye and calling the shots. If there were, how could that brain-based little man even affect the body?\n\nDescartes realized that such a question could be a problem for dualism when he uncovered what is now known as the “mind-body problem”:\n\n[It is unclear] how the human soul can determine the movement of the animal spirits in the body so as to perform voluntary acts — being as it is merely a conscious substance. For the determination of movement seems always to come about from the moving body’s being propelled — to depend on the kind of impulse it gets from what sets it in motion, or again, on the nature and shape of this latter thing’s surface. Now the first two conditions involve contact, and the third involves that the impelling thing has extension; but you utterly exclude extension from your notion of soul, and contact seems to me incompatible with a thing’s being immaterial.\n\nThe mind-body problem is a devastating philosophical problem for dualism, and the trouble it causes is supported by physical evidence. Some “neural” activities, for example, never even make it to the brain. If you put a hand on a hot stove, your reflexes will pull your hand away before your conscious mind has even recognized that your body is in pain. Mr. Homunculus would be a powerless dude, if he did exist.\n\nThe progression above is more or less the history of psychology as I understood it, painted with extremely broad strokes. And, if you squint, it seems to suggest a diverging trend.\n\nOn one hand, time has taught us that our brains (or more generally, our nervous systems) are amazingly powerful. That trend is uniformly positive, and shows the perceived importance of our brains steadily rising over time. On the other hand, it feels like consensus over what is happening between our ears is following a parabolic curve. Scientifically, we know that our brains are just stuff. Intuitively, that idea makes absolutely no sense. How could my conscious experience of the world be the inevitable result of neurons bumping around in my head? How could dumb atoms careening around my cranium form anything resembling free will, or a consciousness, or a soul that could survive physical death?\n\nThose questions, it turns out, are extremely hard to answer. In fact, the latter question has been appropriately deemed the “hard problem of consciousness,” and stems from the simple fact that, to you and everyone else in the world but me, my brain is ineffable.\n\nWhat Is It Like?\n\nMatt Murdock understands ineffability. He is a lawyer by day, a Marvel hero at night — and he’s blind. In the premiere season of his show Daredevil (highly, highly recommended), Matt, questioned by his soon-to-be love interest, explains both his blindness and his other super-senses in one fell swoop:\n\nI guess you have to think of it as more than just five senses. I can’t see, not like everyone else, but I can feel. Things like balance and direction. Micro-changes in air density, vibrations, blankets of temperature variations. Mix all that with what I hear, subtle smells. All of the fragments form a sort of… impressionistic painting.\n\n“Ok, but what does that look like? Like what do you actually see?”\n\nA world on fire.\n\nMurdock’s world is difficult to imagine. We intuitively understand super-sight, or super-hearing, or super-touch — but to be able to do all of those so well that we could not only walk around a busy downtown street but also hear the heartbeat of the guy we’re chasing while jiu-jitsuing a charging baddie is enough to challenge our understanding of what it would be like to be Matt Murdock.\n\nHis metaphor helps. We can feel the warmth of the flames, understand the blur that must exist around the edges, and hear the subtle pops and crackles that give us hints of what is engulfed before us. But even then, it’s just a metaphor and inevitably falls short of the truth. As a potential consciousness-shifter, consider what it would really be like to be blind. It’s normal to imagine blindness as darkness, but close your eyes (after finishing this paragraph). Blindness isn’t the darkness that you see in front of your closed eyes; blindness is the absolute nothingness that you “see” out of the back of your head. Or out of the bottom of your feet. Blindness isn’t like darkness at all; it’s like nothingness.\n\nAll that is caveated with “as far as I know.” I only know what it is like to be a seeing, hearing, smelling, touching, tasting Christian Keil, and that’s ineffability: I can never know what’s going on in your head, and you can never know what’s going on in mine. I can guess what it’s like to experience something totally foreign to me, like, say, echolocation, but I will never experience how it feels to be a bat. As silly as that sounds, I found that scenario to be more or less the foundation of modern consciousness research. Thanks to NYU professor Thomas Nagel, psychologists now wonder: “What is it like to be a bat?”\n\nBats echolocate; sharks detect electrical fields; dragonflies see ultraviolet light. Examples of animal super-senses are well-worn. We all intellectually know that other animals are sensitive to the world in ways that we are not, but again, the distance from between intellectual and experiential is vast. Even if I can imagine that echolocation is similar to being really good at hearing, I’ll never really know the ease of flying through a dark cave without a second thought. That ease is a “something extra” that only comes with the conscious experience of being a bat. Scientists usually refer to these “somethings extra” as “qualia,” the unfathomable, indescribable extra feelings that accompany conscious perception. Qualia are notoriously difficult to succinctly define, which is the point — and the problem. Philosopher and cognitive scientist David Chalmers explains,\n\nWhen we see, for example, we experience visual sensations: the felt quality of redness, the experience of dark and light, the quality of depth in a visual field. Other experiences go along with perception in different modalities: the sound of a clarinet, the smell of mothballs. Then there are bodily sensations, from pains to orgasms; mental images that are conjured up internally; the felt quality of emotion, and the experience of a stream of conscious thought. What unites all of these states is that there is something it is like to be in them. All of them are states of experience. …Why is it that when our cognitive systems engage in visual and auditory information-processing, we have visual or auditory experience: the quality of deep blue, the sensation of middle C? How can we explain why there is something it is like to entertain a mental image, or to experience an emotion? […] Why should physical processing give rise to a rich inner life at all? It seems objectively unreasonable that it should, and yet it does.\n\nAll in all, the story of psychology was already becoming more convoluted than I originally bargained for. I wanted to find the simple truth about consciousness and exorcise any supernatural influence from the world between our ears. But there were just so many open questions posed by the complicated story above.\n\nHow could physical phenomena — just light frequencies bouncing against your retina — bring about ineffable qualia? Any physical explanation of the redness of red seemed doomed to fail. But wouldn’t that mean that the scientific explanation of consciousness as merely physical was lacking in some way? Would that mean that dualism really wasn’t a delusion at all?\n\nWhat I really wanted to know was whether what I had identified as the scientific answer — my grand history of psychology that culminated in a rejection of dualism — was really what all psychologists believed. What was that one, unifying truth of consciousness that I could use to contrast, and ultimately disprove, the religion story?\n\nIn my subsequent research, I found four candidates.\n\nConsciousness is Computation\n\nThe first theory I happened upon was inspired by Alan Turing, the mathematician who broke Nazi codes, became the godfather of the modern computer, and, in his free time, dreamed up one of the best thought experiments in the history of consciousness research. In 1950, Turing posed a difficult question in the philosophical journal Mind: “Can machines think?” Turing wasn’t the first to ask that question, but he was the first to devise a simple way to answer it.\n\nThe “Turing Test,” as it would become known, is straightforward. On one side of a chatroom — think AIM — is a panel of judges, and on the other, both chatbots and real people. The goal is for the bots to convince the judges that they are human. This may seem impossible, but amazingly, the Turing Test has already been passed. Eugene Goostman, a bot claiming to be a young boy from Ukraine, convinced a panel of judges that he was a real boy. The question to ask, then: is Eugene an “it” or a “he”?\n\nTuring argued in his paper that if a bot were to pass his test, then we couldn’t say that the bot couldn’t think. The double-negative is annoying, but necessary: it’s the conclusion of ineffability. Even though we know the code that built Eugene, we don’t have any evidence to deny Eugene’s claim that he is conscious. How could we tell him that he isn’t just a normal boy, when he says that he is? And — perhaps more interestingly — why doesn’t his opinion matter?\n\nThe ultimate proposition is that if computers can be conscious, perhaps we are just very complex computers running the program of “consciousness” for ourselves. Those computers — we — would be nothing more than physical stuff, even though we call ourselves conscious.\n\nWhat, really is the difference between us and Eugene? Eugene uses his chat window as an input, and his complex background programming translates those inputs into the action of writing a response back to his interlocutor. Our brains take in sensory inputs, translate them into thoughts, and turn those thoughts into actions. We are more complex than Eugene, of course, but is that a difference of degree or of kind? Could we just be computers, following the instructions of our DNA and experiencing consciousness as any sufficiently complex machine should?\n\nSome believe that machine-consciousness is impossible, like John Searle — a philosopher who invented a thought experiment to prove his point.\n\nSuppose that you are locked in a room with nothing but a set of instructions to process written Chinese text. Someone slips you a page of Chinese symbols under the door. Your job is to follow the instructions, identify symbols simply by their shapes, and thereby write a letter (in Chinese) that you can then slip back under the door in response. From the point of view of your Chinese compatriot outside the room, your answers are indistinguishable from a native Chinese speaker; nobody looking at your answers can tell that you don’t speak a word of Chinese.\n\nThe point, according to Searle, is that a machine might be able to process instructions without actually understanding them. And it’s a compelling argument — at least on first blush.\n\nIf I ever met Searle, however, I’d ask him one question: what is the analog to consciousness in his room? Is it just the person — who, granted, does not speak Chinese — or “the room” as a unit? If the latter, then I would say that the machine of “the room” (including both the instructions and the person) does understand Chinese. When you bring the analog back to the brain, it sounds silly to say that just because one part of the room (the person) doesn’t understand Chinese, therefore the whole room doesn’t understand Chinese. As we know, there is no homunculus. Our brains are distributed systems, with knowledge residing in particular regions of the brain but still wholly integrated with the rest of the system. If a brain can be said to understand Chinese, it would appear that Searle’s Chinese Room can as well. And, if so, computers are no better than brains at avoiding ineffability.\n\nPerhaps computers don’t have qualia, as Searle would undoubtedly argue. Perhaps Eugene really is just an impostor, and doesn’t have an interior life of his own. But, if you ask him if he has qualia, he says yes, and you believe him, where is the disconnect? And wouldn’t you be mad if someone accused you of being a computer?\n\nTuring suggests that there might not be a difference between computation and consciousness, and if that is so, the idea that we are somehow more than physical stuff — a hypothesis in which religion holds a serious stake — might be in serious trouble.\n\nConsciousness is a Quantum Decoder\n\nThis second theory requires a two-minute intro to quantum mechanics: please make sure your seatbelt is fastened, and keep your hands and arms inside the vehicle at all times.\n\nLet me start by saying that quantum mechanics makes no sense, and somehow, that’s the point. It shouldn’t, not to us macro-people living in a macro-world. Our world behaves according to the “classical” laws of physics that we all know and intuitively understand — force equals mass times acceleration, momentum is conserved, etc. This makes the normally-sized world predictable, or “determined”: we know how pool balls will bounce off of each other once we strike the cue.\n\nIf you shrink those same pool balls down to the size of an electron, though, predictability is lost. It’s difficult to say where they will be — or even where they are. One of the fundamental ideas of quantum mechanics — “superposition” — says that things don’t even necessarily have to be in one place or another. They can simultaneously be here, and there, and traveling from here to there — with all states “superposed” over each other and equally true even when they’re mutually exclusive. This isn’t just analogy. At the quantum level, things can literally be in multiple places (or in quantum-speak, exist in multiple “states”) at the same time.\n\nThe ultimate implications of superposition, and really all other quantum phenomena, are largely unknown even to those who know the most; we Muggles should probably be excused for mistaking quantum physics for hieroglyphics, e.g.,\n\nAmazingly, though, scientists have started to make some sense of this counterintuitive quantum world. Quantum mechanical calculations are now present, according to biophysicist Werner Loewenstein, in “transistors, tunnelers, magnetic resonance imaging in hospitals, [and] superconductors.” He estimates that thirty percent of the U.S. gross national product today depends on our understanding of quanta.\n\nSo, some of it must make some sense, right? As it just so happens, superposition is an intriguing possibility for explaining consciousness.\n\nAlan Turing’s famous Nazi-code-cracking machine defined the standard computing units as “bits” — the ones and zeroes behind the digital world. At the most basic level, computers are just bit-manipulators. All else is interface. Bits are static and binary: they’re either “1” or “0”, and they stay that way until deliberately altered. In almost all conceivable cases, these binary and static “limitations” are hardly limiting at all: modern supercomputers can process nearly all conceivable computational tasks in a reasonable amount of time. Some extreme calculations, however, can still take more power (or more time) than our bit-based computers can handle, e.g., factoring huge numbers. The largest number ever factored, known as “RSA-768,” is 232 digits long and took over two years of computation by 1,000 processors to solve.\n\nIn contrast, scientists believe that fully quantum computers based on quantum bits (or “qubits”), could factor the same number around one hundred million times faster than a standard computer — meaning, in under one second. The secret is superposition. Where a bit has to be one or zero, a qubit can be both one and zero, with both states held simultaneously in place. This makes qubits exponentially more efficient than bits — a quantum computer with 30 qubits can represent the same amount of information as a classical computer with 230 (1.1 billion) bits.\n\nToday, such super-powerful quantum computers are still impossible to build because we don’t yet know how to translate quantum bits into “classical” results. But just because we don’t know how to make them doesn’t mean they don’t exist. As crazy as it sounds, one modern theory of consciousness is that our brains are real-life quantum computers.\n\nWhen I first discovered this theory, I was skeptical because we hardly understand quantum theory or consciousness, and “not understanding” is an unreliable common denominator. “We don’t understand quantum computing” and “we don’t understand consciousness” aren’t enough to conclude that the two are the same thing. In a way, this is the same sort of proof that Mary Shelley used when she created Frankenstein: we didn’t understand biological life in 1818, nor did we understand electricity, so she figured that perhaps they could be the same thing. We now know that electricity and sewn-together body parts do not a life make; a similar progression might eventually hold for quantum computing and consciousness.\n\nAs I read more, however, I saw that the theory was receiving some scattered support from the physics community. Results are still inconclusive, but scientists are actively searching for a link between decoding superposed qubits and consciousness. Even Roger Penrose — mathematical physicist, member of the Royal Society, and best bud of Stephen Hawking — proposed his own theory of “microtubules” within cells that could do the necessary quantum decoding within the brain.\n\nIn a remarkably full-circle way, the brain-as-quantum-computer theory could actually help explain the ineffable sensation of conscious free will — and offer a response to the common objection to the theory that we are simple, non-quantum computers. That first theory, you may think, doesn’t do justice to our brains because consciousness certainly feels like more than just computation. But could the unpredictability of the quantum world explain that sensation? What if our free will is just a uniquely human ability to collapse quantum states of superposition into classical physical results like “choosing to do a cartwheel”?\n\n“Willfully decoding superposed cerebral qubits” might not sound like any idea of consciousness that you’ve heard before — but it has the scientific community excited. And if it’s true, the idea of a non-physical consciousness would then be false: we wouldn’t need a soul, because we’d have our “microtubules.”\n\nConsciousness is a Universal Constant\n\nAnd now: exhale. I promise that that’s the last of the super sciencey stuff — at least for now. Answer C is, shall we say, less than sciencey. There are plenty of folks who believe that it’s true, but it seems less like a real solution, and more like a “why not” kind of answer. This theory, as I came to understand it, is probably best explained by analogy.\n\nI am a millennial, which is to say that I got a lot of participation trophies growing up. Baseball in the summer, football in the fall, basketball in the winter, golf in the spring; at the end of every season — thanks to the doting Baby Boomers who ran my sports leagues — I would invariably walk away with some hardware, no matter if I had won or lost every game. It was only later, somewhere around the early 2010s, when people started to realize that when everybody’s “special,” nobody is. If everybody has the same label (e.g., “participant!”), that label is effectively meaningless. This is a realization that society made together, uniting us all in our general disdain for the Age of the Participation Trophy. Those meaningless trophies can make kids entitled, less likely to work for distinction, etc. It’s enough to make you wonder whether giving out no trophies would be even more effective than giving them out to everyone.\n\nImportantly, though, this logic only works in one direction. If you’re a kid who gets a trophy, you would be right to wonder if it means anything at all. But, if you’re a kid who doesn’t get a trophy, you would probably be wrong to think that you were a winner. Odds are good that you’re just a big ol’ loser. Tough luck! Maybe you’re in some progressive league that actually took my philosophy exercise on the last page to heart. But probably not.\n\nThe theory of consciousness as a constant is like the kid who didn’t get a trophy but called himself a winner. It uses absence as evidence of everpresence (try saying that three times fast) — which usually doesn’t work out in the end.\n\nThe way that this theory tried to make that argument goes like this: consciousness is very difficult to find in the world; we can’t pin it down. It’s not in any particular atom, it’s not in any particular area of the brain, and it’s not even necessarily in the brain at all. Yet we know that it must exist. So, proponents wonder, could consciousness just be everywhere? Perhaps we can’t isolate it because it’s everpresent — a constant in the universe like gravity, or magnetism. Eureka!\n\nThis idea is very new age and groovy. If consciousness is a constant, we don’t own it or control it; rather, we can tap into the force of consciousness that exists all around us. If that even sounds like a religious idea to you, you’re not alone. Just listen to how proponents talk about the theory, like quantum physics Ph.D. Amit Goswami:\n\nThe source [of creativity and intentionality] is consciousness itself… the subject that we become in a creative experience. The subject, that creative self, which sometimes we call by a very holy name, like the Holy Spirit — the spirit or the spiritual in us — is that which is the source to which the creativity become apparent. The insight comes, and the insight comes in the form of new meaning.\n\nOr, if you prefer a PhD in systems theory, Dr. Ervin Laszlo:\n\nWe are bodies in the sense of what I call the external read-out. We are more than just the body because when we sense the world, we sense the world not as a body, we sense it through our body — but we sense it as a mind. It comes to us as our consciousness… We are part of a much larger whole and if you consider this newer approach from the new science, you will see that the individual body is a complex wave, very much part of the whole. It is just an illusion that we are separate.\n\nIn my eyes, the kid who assumes that what he can’t see must be everywhere is wrong. He just didn’t get a trophy (or a conscious mind). Why can’t the conclusion just as easily be that consciousness is nowhere? Yet, the theory has support — and is another candidate answer, in the end.\n\nConsciousness is a Pattern\n\nThis final theory was proposed by my favorite nonfiction author, and I was quickly drawn to it because it wonderfully balanced its awe for the ineffability of consciousness with its scientific insistence on the verifiable truth. The idea: that consciousness is just a complicated, looping pattern of brain activity.\n\nI met Douglas Hofstadter, a professor of cognitive science, by reading his best-selling novel Gödel, Escher, Bach. In his 700-page magnum opus, Hofstadter considers the “Eternal Golden Braid” that runs through minds, mathematics, machines, and music — or, less alliteratively and more directly, he observes that some interesting and counterintuitive conclusions can come from systems that loop back on themselves. The three geniuses in his title illustrate the universal perspective Hofstadter brings to this hypothesis: in the book, he unites the mathematics of Kurt Gödel, the impossible paintings of M.C. Escher, and the contrapuntal canons of J.S. Bach in one fell swoop. In his second book, I Am A Strange Loop, Hofstadter extends his analysis of self-reflexive systems to include our brains and the experience of consciousness. His idea, too simply, is that consciousness is just a strange loop of neural activity: unique in its pattern, but still fundamentally the same type of stuff as other neural behavior. If he’s right, we wouldn’t need microtubules or consciousness fields — our brains would be enough.\n\nAlthough I strongly, strongly recommend that you all go out and buy GEB or I Am A Strange Loop to get the full effect of Hofstadter’s brilliant, playful, empathetic style, I’ll do my best to show my own version of his theories here — both as a teaser, and to show how his ideas have fared after marinating for a few years in my own strange loop. I did more research after learning his hypothesis, and happened upon a three-part structure as the best way to explain the story.\n\nPart one is awareness. All species of life are aware of the world to some extent through their senses. Algae have photoreceptive spots, Venus Fly Traps have feelers, bats have echolocation. There are certainly degrees of awareness, and as species become more aware, they are (generally) more likely to thrive in the world.\n\nPart two is cognition. Awareness gathers information from the environment; cognition processes that information into useful inputs that can guide an animal’s behavior. Cognition includes functions like memory, attention, judgment, problem solving, language, concept formation, and social behavior. All of these skills are externally-oriented and modular. I either have my long-term memory or I have Alzheimer’s, but not both. I can either speak Spanish, or no lo comprendo. The more cognitive skills that a species can gather, then, the more likely they are to thrive in the world.\n\nPart three is consciousness, and is fundamentally different — if related — to parts one and two. Understood simply, consciousness (as a strange loop) is the ability of the brain to perceive, understand, and manipulate itself. Where awareness and cognition are external, consciousness is internal — it’s a reflexive recognition that I, a strangely-looping conscious mind, exist. Whenever we ask ourselves the question “Am I conscious right now?” the answer is always yes. But what happens when we don’t ask that question? It is perhaps easier to see that such loops exist in others than it is to see them in ourselves.\n\nYoung children can only recognize themselves in the mirror — or, they loop and think “I exist” for the first time — when they’re 15–18 months old. It takes even longer for children to realize that similar “loops” are present in others. This realization, that not only am “I” a person but also that other people have their own internal realities, is more simply known as the Theory of Mind. The next time you see your three-year-old cousin, tell her the “Sally-Anne” riddle:\n\nSally has a basket in front of her, while Anne has a box. Sally puts a marble into her basket and then leaves the room. While she is gone, Anne takes the marble from Sally’s basket and places it in the box. When Sally returns, where will she look for the marble?\n\nYour cousin will almost surely say the box — which of course is incorrect, because Sally would assume that the marble is where she left it. Your cousin hasn’t yet gained the skill to know that everything in her own head isn’t in Sally’s head as well. Most children develop Theory of Mind when they are three to four years old.\n\nEventually, after years of Theorizing about Mind, most children (and adults) don’t even recognize that they are describing a looping thought pattern when they say “I think that Sally thinks that the ball is in the basket.” But, of course, those types of statements are loops. I know that you know that John stole the ball. John thinks that I know that you know that John stole the ball. And so on, ad infinitum.\n\nLooping, it seems, is an infinitely extensible skill. There isn’t a “part four” for that very reason. Loops loop for as long as they need to, and are limited only by the computational power of the brain. Luckily, we only need one loop — this time an internal one — to realize our own consciousness. To paraphrase Descartes: I am a Strange Loop, therefore I am. Or, in Hofstadter’s words: “In the end, we self-perceiving, self-inventing, locked-in mirages are little miracles of self-reference.”\n\nIn full-disclosure mode: the above is largely Christian Keil Original Material. I couldn’t find any scientific studies to link “looping” of the Sally-Anne variety to Hofstader’s Strange Loops — or really any mention of Hofstadter’s theories in the mainstream consciousness literature at all. Even so, I loved the theory, which makes it worth mentioning here. I found its logic sound and its applications compelling; but perhaps the thing that I loved most about the theory was less proof and more poetry (as Hofstadter would love to hear, I’m sure): I loved the parallelism between the brain and consciousness itself.\n\nThe human brain is unique not because of its materials — 99% of the brain is made of just oxygen, carbon, hydrogen, and nitrogen — but because of its intensely interconnected and decentralized structure. As mentioned before, elephant brains are far heavier than human brains, weighing in at over eleven pounds, but hold only eleven billion cortical neurons. Human brains have over 23 billion cortical neurons despite weighing just three pounds on average. The power of the human brain lies in its efficient pattern, not its raw particles.\n\nIsn’t that mantra — patterns, not particles — parallel to what Hofstadter suggested is true of consciousness? There might not be a special field floating in the ether, nor a bunch of microtubules decoding quantum mechanics, but we do have brains that can perceive themselves. And, as the construction of a brain from humble chemicals suggests, consciousness is no less exceptional for being just a pattern.\n\nIf you didn’t like the three theories above, this one may feel better — we aren’t just particles, we are patterns. Miraculous, golden patterns woven into the same braid as mathematics, machines, and music. We might not be any more than the physical patterns in our brains, but that doesn’t mean that we shouldn’t be in awe of (and humbled by) the miracle of our existence.\n\nTheory and Fact\n\nAs I started parsing my way through the evolution-creationism debate, one maddening misperception jumped out at me — and it’s worth addressing immediately. Many folks on the creation side of the aisle didn’t understand that in science, “theories” are facts, not guesses.\n\nPresident Ronald Reagan was one of those folks:\n\nWell, if [evolution] is a theory, it is a scientific theory only, and it has in recent years been challenged in the world of science and is not yet believed in the scientific community to be as infallible as it once was believed. But if it was going to be taught in the schools, then I think that also the biblical theory of creation, which is not a theory but the biblical story of creation, should also be taught.\n\nCalling something “a scientific theory only” is nonsense. To be accepted as a scientific theory means to be in company with things like “Cell Theory,” the controversial idea that cells exist, and the “Theory of Universal Gravitation,” the ludicrous idea that things fall when you drop them. To a non-scientist, “theory” might easily be confused with “hypothesis,” but the two are opposites. Hypotheses are formed at the very beginning of scientific inquiry, while theories are realized at the very end of repeatedly successful experimentation. That evolution has been deemed a Capital-T Theory should not bring about skepticism, but assurance. Think about it this way: if you believe that you have cells, you should also trust in the Theory of Evolution.\n\nAnd yet, many still have their doubts. When I began my research, I didn’t understand why: I thought that the facts of evolution were all but undeniable, and in that respect I was correct. Nothing that I found in my research gave me reason to doubt my initial confidence. If you are a scientifically literate adult, you should believe in evolution.\n\nThat being said, my mindset on evolution has changed; I also learned that it’s important to be precise about what one means when they say “evolution.” Not all instantiations of “evolution” are made equal; this point is crucial. Here’s what I mean:\n\nEvolution, in three words, is descent with modification. In more words, it’s the way that biological information changes over many interconnected generations. We used to have monkeys, and now we have people — no matter what John Lennon believed, that much is a fact. Genetic change has happened over time.\n\nThe real brilliance of the evolutionary hypothesis is in its falsifiability — by defending the existence of a single hereditary line from a universal common ancestor to the present diversity of life, evolution is committed to a very precise (and continuous) timeline. If that timeline could be found to fit with all of the evidence that we find, evolution would be plausible; but even one species out of place could be enough to disprove the theory’s bold claims. As geneticist J.B.S. Haldane said, “I will give up my belief in evolution if someone finds a fossil rabbit in the Precambrian.” In spite of this vulnerability, however, scientists have yet to find a reason to doubt the evolutionary timeline. Evolution’s story is verified by both the digital history of genetics and the analog history of fossils.\n\nIn the early 1950s, James Watson and Francis Crick discovered what would become the founding idea of the new field of genetics: the double-helical structure of DNA. DNA is sometimes referred to as the “language in our cells” because it’s “written” in a genetic alphabet of four chemicals: Adenine, Thymine, Cytosine, and Guanine (A, C, T, and G). That language, traced back over past generations, tells a clear story about how modern human DNA came to be. Species that have similar DNA are likely to look like each other: genetically, all humans are 99.9% identical. We can calculate (and for the most part, have calculated) our genetic similarity to — or “evolutionary distance” from — every form of life. We share 90% of our DNA with chimps, 84% with dogs, 47% with fruit flies, and 24% with wine grapes. You heard it here first: humans are literally one-fourth wine.\n\nOur closest relatives, however, are the primates. All primates have 24 chromosomes — or “bits” of DNA code — but humans have just 23. Evolution is committed to the idea that we evolved directly from primates, which implies that there must have been a very specific change to our DNA as we evolved: we must have lost one chromosome completely or had two chromosomes fuse together to make one. Amazingly, scientists have found evidence of the latter. Francis Collins, the director of the Human Genome Project, writes:\n\n…special sequences occur at the tips of all primate chromosomes. Those sequences generally do not occur elsewhere. But they are found right where evolution would have predicted, in the middle of our fused second chromosome. The fusion that occurred as we evolved from the apes has left its DNA imprint here.\n\nSimilar digital clues can be found in the massive history written in the language of our cells. We came from primates, primates came from mice, and so on and so forth back to the universal common ancestor.\n\nIf cell-language seems a bit intangible or abstract, don’t worry, because the exact same story is told through the analog history of the fossil record. We have discovered some billions of individual fossils, and they all confirm the evolutionary timeline. Paleontologists can use radiometric methods (e.g., carbon dating), and relative methods (e.g., finding geological “layers,” some of which must have come before others) to place fossils on the timeline of the Earth, and not a single fossil has ever been found out of place.\n\nIf you need even more evidence to believe in the Theory of Evolution, here’s the kicker: both the digital and analog histories perfectly align. Evolution is one consistent timeline that extends from today back to the beginning of life. It’s a Capital-T Theory; there really isn’t a debate.\n\nThe upshot, of course, is that any other theories that contradict evolution are simply incorrect. In the 17th century, Irish archbishop James Ussher added up the ages of all of the people in the Bible, and calculated that the Earth must have been created on the evening of October 22nd, 4004 BC. This hypothesis has become known as “Young Earth Creationism,” and it’s simply false. We know that our Earth is about 5.5 billion years old.\n\nAt this point in my journey, I was of the mind that you should always trust scientific proof over any other kind of justification. Thousands of objective observations and repeatable measurements felt more objectively true than the words of the Bible (or the shoddy calculations of an old archbishop). The key, of course, was “truth”: I felt that things couldn’t be true just because you wanted them to be. I had always seen truth as a concept that was invented to build objective consensus between people — e.g., the water hole is two miles northeast of our village. Relative truth wasn’t really a thing to me; I would have called those thoughts opinions, not objective facts. I understand that your opinion could very well be that evolution is false — but objectively, you’d be wrong. Evolution is a proven, objective fact, and it’s not going anywhere.\n\nBut before this starts to read like a weird chapter in which I just pat myself on the back for knowing a bunch about evolution, let me just say this: while I believe that evolution is undeniably true, the story doesn’t stop there. Evolution is the theory of the timeline, and explains that life moved from wine grapes to apes to humans. Crucially, however, it doesn’t say how that movement happened. For that, Darwin needed another hypothesis: natural selection.\n\nEvolution and natural selection are often grouped together. But I quickly learned that natural selection was far more fraught with difficulty and ambiguity than its sister theory — a fact that proved troubling as I attempted to establish the dominance of evolution over creationism.\n\nTwo Simple Complications\n\nI came to internalize the difference between the two hypotheses with a simple, if silly, metaphor: evolution is the path through the jungle, and natural selection is the machete. As we look back, we can see a well-defined trail cut through the dense underbrush. The question is… what blazed that trail? Could it have been just a dude with a machete? Or would it have taken more advanced technology?\n\nTo again define our terms, here’s a one-breath explanation of natural selection — or, as I like to put it, the science of babymaking. As Darwin discovered, natural selection takes just three ingredients: heritability, variation, and fitness. Heritability is DNA: a way for parents to pass on their traits. If parents couldn’t hand down their genes, there would be no “evolutionary line” connecting families. Variation makes that evolutionary line branch into a tree. As my blonde-haired, blue-eyed sister proves, children are not always exact replicas of their parents because DNA is a good, but not perfect, self-replicator. Every new human baby has ~64 DNA mutations, most benign. Some mutations, however, can affect the baby’s fitness, or the probability that it will one day have a baby of its own. Fitness determines which branches will thrive and which will wither away. Some mutations are “adaptive” (e.g., chiseled jaws), others are “maladaptive” (e.g., allergies). Adaptive branches make more babies and “succeed” in carrying on the evolutionary line.\n\nThe rationale behind natural selection is solid to the point of seeming tautological. When Darwin proposed evolution and natural selection in the same book, the two concepts became forever paired: Darwin showed us a trail through the jungle, and a simple way to cut down trees. It was a persuasive story for a very long time, and only after 150 years of research did we learn enough to begin doubting Darwin’s pairing.\n\nThe complications for natural selection come in the form of two interrelated problems: complexity and time. The former was explicitly acknowledged by Darwin in On the Origin of Species:\n\nIf it could be demonstrated that any complex organ existed, which could not possibly have been formed by numerous, successive, slight modifications, my theory would absolutely break down. But I can find out no such case. No doubt many organs exist of which we do not know the transitional grades, more especially if we look to much-isolated species… [but] we should be extremely cautious in concluding that an organ could not have been formed by transitional gradations of some kind.\n\nDarwin’s theory relies on the power of small mutations to make big changes over enough time. Big mutations aren’t practically possible — the odds are better that your DNA would end up inviable than that your DNA would give you wings — so small mutations are all that natural selection can use. If a biological feature (like a wing) couldn’t have been formed by successive baby steps, then, natural selection couldn’t have formed it. University of Pennsylvania biochemist Michael Behe explains what such an “irreducibly complex” feature might look like:\n\nThe mousetraps that my family uses in our home to deal with unwelcome rodents consist of a number of parts. There are (1) a flat wooden platform to act as a base; (2) a metal hammer, which does the actual job of crushing the little mouse; (3) a wire spring… (4) a sensitive catch which releases when slight pressure is applied; and (5) a metal bar which holds the hammer back… If any one of the components of the mousetrap (the base, hammer, spring, catch, or holding bar) is removed, then the trap does not function. In other words, the simple little mousetrap has no ability to trap a mouse until several parts are all assembled.\n\nTo evolve by natural selection, mousetraps would have to gain incremental benefits from its intermediate stages — for example, from having just a base, or just a hammer. But of course, a mousetrap missing any one of its parts is useless, so a mousetrap — per Darwin’s own admission — could never have evolved by natural selection.\n\nThe question, then: do irreducibly complex features exist in nature? Behe thinks so, and uses the eye as an example. An eye is made of dependent, interconnected parts: retina, pupil, lens, and so on. Any one of these features by itself wouldn’t give a creature an evolutionary advantage. Does this make the eye an evolutionary mousetrap?\n\nThe short answer is “no.” Scientists have found intermediate versions of eyes all throughout nature. The earliest precursor of the human eye is the photo-receptive spot present in plants like algae. They’re simple but effective in helping plants avoid the shade and find the sun. Over time, these spots became indented (allowing animals to determine the direction of a light source), then had their openings constricted (allowing those directional light sources to focus), then developed a rudimentary lens (focusing light without reducing the total amount of light the eye could see). Each model gave a small advantage over the model previous, which kept the train of evolution rolling.\n\nThe long answer is also “no,” but with a catch: the more complex the feature, the longer it takes to form by random mutation. Could some evolutionary steps have happened too quickly for natural selection to have caused them? This is the second, and more damning, problem for natural selection; and here, I found a healthy debate in the scientific literature. The problem is twofold: first, generating the vast diversity of life from a single common ancestor in just four billion years; second, matching the incredible sprints documented in evolutionary history.\n\nA group of Penn professors wrote of their opinion on the first issue in an aptly-named paper: “There’s plenty of time for evolution.” Their argument is that, although totally random mutation would not be able to generate the evolutionary timeline, natural selection is not random. Rather, natural selection “locks in” changes as they provide even the smallest amount of incremental fitness. They make an analogy to hacking a 12-character-long password. If you were forced to guess truly randomly, the task would be impossible; even assuming the password to be only letters, that’s 2612 (or roughly 10 quadrillion) possibilities. But, supposing instead that your letters would “lock” when they were correct, the task is fairly easy — just guess each letter consecutively (aaaaaaaaaaaa, then bbbbbbbbbbbb, …) and you would find the password in 26 tries. Brought to the scale of evolution — so, dealing with the countless genetic letters of DNA — the difference between changing 20,000 genes without locking and with locking is absurd. Without locking, you’d need 1034,040 rounds of guessing. With it, you’d only need 390. If natural selection can lock in mutations as well as the Professors claimed, it would indeed have plenty of time.\n\nThat bold of a claim was bound to get a visceral response, and it did. Most critics challenged the idea of locking, especially of “parallel” locking in which each letter of a password can be tested (and judged) individually. The most detailed critique came from Casey Luskin, the research coordinator for the Center for Science and Culture:\n\n[Natural selection] does not have access to information about future benefits of a particular mutation, or where in the global fitness landscape a particular mutation is relative to a particular target. It can only assess mutations based on their current effect on fitness in the local fitness landscape.\n\nOr, in other words, the individual letters of the password can’t know whether they are contributing to the fitness of the overall organism, or if incremental benefits are coming from other letters entirely. He continues:\n\n[The Penn Professors] also make unrealistic biological assumptions that, in effect, simplify the search. […] In their model they represent each genetic locus as a single letter. By doing so, they ignore the enormous sequence complexity of actual genetic loci (typically hundreds or thousands of nucleotides long), and vastly oversimplify the search for functional variants. In similar fashion, they assume that each evolutionary “advance” requires a change to just one locus, despite the clear evidence that most biological functions are the product of multiple gene products working together.\n\nIn other words, the Penn Professors might give natural selection too much credit. Natural selection can’t read the future and know that individual letter mutations are on the right track to large-scale, multi-letter adaptations, which means that natural selection can’t lock. If that’s true — and after reading these papers I had a large suspicion that it might have been — natural selection might have a very serious problem with time.\n\nThe second facet of the time problem only compounds the first. Evolution was far from a steady, methodical process; the evolutionary history is littered with events commonly called “discontinuities,” or “punctuated equilibria.” The most famous was the Cambrian Explosion, a period of 20 million years or less — only 0.4% of the Earth’s history — when all of the major body plans and broad types of animals emerged. We went from single-celled organisms to complex, fully-designed creatures in an extremely short period of time. In just 20 million years, arms, legs, gills, internal organs, and nervous systems emerged — and then, all of a sudden, nature appeared to discover every possible option and as quickly as it began, body plan development forever stopped. Both that astounding acceleration and abrupt stop are still unexplained; scientists have theories to explain punctuated equilibria, but I couldn’t find any convincing, consensus answers.\n\nThese two problems made me more skeptical of natural selection than I had been previously. Don’t get me wrong — on a scale of one to Darwin, I was still probably about an eight. I had my doubts, but still believed that natural selection likely accounted for most of the answer. But “most” is less than “all.” My mindset was changing.\n\nFrom Zero to One…\n\nAs if waiting, silently, for the opportune moment to strike, a tough realization hit me as soon as I admitted to myself that natural selection wasn’t all I thought it was cracked up to be. That realization: that even if natural selection explained all of evolution, and even if evolution was undeniably true, I still only had half of the story. Or, more honestly, probably even less than half, because I wasn’t yet back to the true beginning of life.\n\nEvolution explains how life got from one to many. But what of creation, or how life got from zero to one? That was the goal of my research: to disprove God’s claim to the initial creation of life. But, amazingly, no scientists from my research to that point had even mentioned the move from zero to one. The only mention of the creation of life from non-life, in fact, was in the form of “spontaneous generation” — an old and entirely discredited theory originally proposed by Aristotle. In Aristotle’s own words:\n\nSuch fish… arise all from one of two sources, from mud, or from sand and from decayed matter that rises thence as a scum; for instance, the so-called froth of the small fry comes out of sandy ground. This fry is incapable of growth and of propagating its kind; after living for a while it dies away and another creature takes its place, and so, with short intervals excepted, it may be said to last the whole year through.\n\nTo those who believed in spontaneous generation, moving from zero to one wasn’t confusing, mysterious, or even interesting because it happened all the time — fish from pond scum, maggots from dead animals, and so on. As God effortlessly made Adam from dust, Mother Nature created life using only the earthly elements at her disposal whenever she felt like it.\n\nThis idea lasted for thousands of years; people still believed in spontaneous generation when Darwin published his Origin of Species. It wasn’t until 1862 that Louis Pasteur, the namesake of “pasteurization,” finally disproved the theory. After a particularly strong experimental result, Pasteur boldly but accurately declared:\n\nNever will the doctrine of spontaneous generation recover from the mortal blow of this simple experiment. There is no known circumstance in which it can be confirmed that microscopic beings came into the world without germs, without parents similar to themselves.\n\nPasteur’s fundamental insight was simple: that life always comes from other life. Life, too complicated to arise by chance, can’t come from just an opportune mixture of elements.\n\nBut here’s the paradox: what was the origin of life if not an instance of spontaneous generation? Biologists are committed to the idea that all life comes from other life, but simultaneously hold that the original genesis of life was natural. Is that not a contradiction?\n\nBiologists must not think so. As members of the scientific community, they believe that the first species of life on Earth had no outside help. But how? I had to know: what were the odds of the first form of life spontaneously bringing itself into existence? If you look into the evidence, that move from zero to one — while technically not impossible — would have been incredibly challenging.\n\nFirst, life needed to find a suitable planet. The good news: life had a trillion trillion planets in the universe to choose from. The bad news: life is picky. Scientists have identified some 150 criteria for habitability; a planet must have the right atmosphere, size, moons, rotation speed, orbit speed, star around which to orbit… even the existence of an asteroid-catching megaplanet like Jupiter was a box that had to be checked. Author Eric Metaxas writes,\n\n…without the Jovian giant where it is, comets and comet debris would strike us about a thousand times more frequently. [Jupiter has] 318 times the gravity [of Earth]. So most of the comets that come anywhere near Jupiter are pulled toward it. It absorbs many of them into its gaseous depths without so much as a hiccup [or] just deflects them away from us and out of our solar system entirely.\n\nThe importance of avoiding asteroids is nothing to laugh at — the early universe was so violent that any life would have evaporated as soon as it had emerged. For example, the early Earth went through a phase of “heavy bombardment” during which there were more than 22,000 asteroid impacts: 40 of them larger than 620 miles in diameter (nearly the distance from New York to Chicago), and several larger than 3,100 miles wide (think California to Maine). That equates to an extinction event as great as or greater than the one that killed the dinosaurs every 1000 years. If that is how it looked even with a huge deflector like Jupiter, it’s hard to imagine how violent a planet might be without one; suffice it to say that these 150 criteria are all must-haves, not nice-to-haves. Without every single one fulfilled, life could not have arisen.\n\nIn the aggregate, these 150 criteria, by Metaxas’ calculation, create one huge problem. Using what he calls “conservative” estimates, Metaxas calculates that we could expect to find a life-supportive planet once out of every 1073 planets we searched. That number is ridiculously small:\n\nIn words, that’s one in ten trillion trillion trillion trillion trillion trillion; the number of planets we’d have to search to find life is far greater than the number of planets we have in the entire universe. The odds of any one planet out of our universe’s trillion trillion being a hospitable home are:\n\nThat number, one divided by 10 followed by forty-nine zeroes, represents the odds that one planet — just one! — in our universe could support life. To think that it did so already boggles the mind, but finding a planet was just the first hurdle that life had to clear.\n\nLife would also need the right ingredients — like those that graduate student Stanley Miller and Nobel-winning professor Harold Urey combined in their famous “primordial soup.” Miller and Urey cooked up a mixture of liquids and gases that they thought were abundant in the primordial Earth (hydrogen, ammonia, methane, and water), stimulated the mixture with electrical sparks to simulate lightning, and let the solution sit for a week. To their amazement, their soup was full of amino acids — the building blocks of life — when they returned! If the early Earth did, in fact, look like Miller and Urey thought it did, their experiment would be evidence that the formation of life from non-life could be possible.\n\nUnfortunately, however, the primordial Earth probably never looked like Miller and Urey’s soup. Physicist Paul Davies writes,\n\n…geologists no longer think that the early atmosphere resembled the gas mixture in Miller’s flask… methane and ammonia were unlikely ever to have been present in abundance. And if Earth once had substantial hydrogen in its atmosphere, it wouldn’t have lasted long.\n\nEven on Earth, life would have had trouble.\n\nNot to kick a dead horse (or, I guess to do exactly that), but finding the proper planet with the proper elements still would not have been life’s biggest challenge.\n\nWe know that it takes a village to raise a strand of DNA, and DNA’s little helpers go by a number of different names — like “enzymes” or “metabolism” — but at the most basic level, these supporting cast members can all be understood as specialized proteins. Each serves a unique function in speeding up and automating the process of DNA replication. (DNA itself is just the information, or the instructions that these proteins carry out.) For example, there’s the DNA splitter — helicase — that “unzips” the DNA strand into two complementary strands (and often graces the backs of edgy AP Chemistry t-shirts). Those single strands of DNA are then carried by messenger RNA out of the nucleus of the cell and into the hands of transfer RNA, which translates the DNA message into instructions that a ribosome, or 3D protein printer, can read. It’s not a simple process, and DNA itself is inert through all of it; DNA takes no action other than to passively direct and instruct.\n\nHow, then, did DNA arise in the first place? This was a criticism that I didn’t expect to see but saw everywhere: the spontaneous generation of DNA is the chicken-and-egg paradox to end all chicken-and-egg paradoxes. DNA requires proteins to replicate — but those proteins can only be created by ribosomes using the instructions of DNA. DNA needs proteins; proteins need DNA. Neither could have come before the other.\n\nIf this is true — and I believe that it is (despite the attempts by some scientists to posit other intermediate replicators like RNA to break the paradox) — then it’s far more damning than the odds that life could have found a planet. The odds of life arising spontaneously when it needed a village but couldn’t have had one would be zero.\n\n…And Apparently Back to Zero Again\n\nBut of course the odds weren’t zero. Life exists; that’s the most insane thing about this whole line of research. No matter what we think should have happened, we know what did. Life emerged from non-life. We know that life went from zero to one because life exists — that statement is so undeniable that it’s almost silly, but if you think about what scientists know about the beginning of life on Earth, it’s more awe-inspiring than anything else.\n\nLife needed a planet, but shouldn’t have been able to find one. The odds that it faced were roughly equivalent to the odds of you entering the lottery and winning six times in a row, or getting struck by lightning eight times in the next twelve months. Life had just one try to get it right, and astoundingly found perhaps the only planet of the trillion trillion in the universe that could have supported it.\n\nAnd even the planet that it found was far from ideal: the planet didn’t have the minerals necessary to support life. Life needed lots of carbon, but shouldn’t have been able to find it. Between the end of late bombardment and the first known species, life had only 350 million years (not long at all when you consider the 13,800 million years that our universe has existed) to accumulate enough raw materials — again, materials that didn’t exist on the planet on which it had emerged.\n\nThat life did all of this without any help is unbelievable. No proteins, no metabolism, no enzymes to unzip its genes. Just itself, alone, on a barren, un-hydrogenated planet in an infinitely expansive yet unmistakably empty universe.\n\nBut somehow, someway, life went from zero to one, and spontaneously emerged. It shouldn’t have, but it did.\n\nIs that answer supposed to satisfy me? Because it doesn’t. There are some scientists — and perhaps even some of you readers — who are satisfied by knowing that life found a way to emerge by itself. Biologists call this mindset the “Anthropic Principle.” More or less, the idea is that there’s nothing interesting to see because we already know the ending to the story. Some who believe in the Principle even make the claim that the ending of the story (somehow) forced the beginning to turn out as it did. In academic speak, Professors John Barrow and Frank Tipler argue that:\n\nThe observed values of all physical and cosmological quantities are not equally probable but they take on values restricted by the requirement that there exist sites where carbon-based life can evolve and by the requirements that the universe be old enough for it to have already done so.\n\nOr, more plainly, Richard Dawkins writes:\n\nThus our presence selects out from this vast array only those universes that are compatible with our existence. Although we are puny and insignificant on the scale of the cosmos, this makes us in a sense the lords of creation.\n\nThese folks are champions of science — the people whom I exalted as the beacons of rational, measured thought — and that’s how they decided to rationalize the improbability of the beginning of life. I mean, seriously?\n\nHow is the Anthropic Principle remotely verifiable or scientific? How could you test the probability of a “cosmological quantity” if you only have one universe and one emergent tree of life to study? How can you falsify the claim that physical values are restricted by allowing carbon-based life without being able to roll the dice again and observe the creation of a new univers"
    }
}
{
    "id": "correct_foundationPlace_00054_1",
    "rank": 91,
    "data": {
        "url": "https://production-gitops.dev/infrastructure/golden-topology/",
        "read_more_link": "",
        "language": "en",
        "title": "Cloud Pak Production Deployment Guides",
        "top_image": "https://production-gitops.dev/infrastructure/golden-topology/assets/images/banner.png",
        "meta_img": "https://production-gitops.dev/infrastructure/golden-topology/assets/images/banner.png",
        "images": [
            "https://production-gitops.dev/images/cloudnativetoolkit.png",
            "https://production-gitops.dev/infrastructure/images/gt/gt-1.png",
            "https://production-gitops.dev/infrastructure/images/gt/gt-2.png",
            "https://production-gitops.dev/infrastructure/images/gt/gt-3.png"
        ],
        "movies": [
            "https://www.youtube.com/embed/S_wDTbuT-bM"
        ],
        "keywords": [],
        "meta_keywords": [
            ""
        ],
        "tags": null,
        "authors": [],
        "publish_date": null,
        "summary": "",
        "meta_description": "The Cloud Pak Production Deployment Guides document and demonstrate how to run cloud-native solutions leveraging IBM Cloud Pak capabilities in a Red Hat OpenShift environment",
        "meta_lang": "en",
        "meta_favicon": "../../images/cloudnativetoolkit.png",
        "meta_site_name": "",
        "canonical_link": "https://production-gitops.dev/None",
        "text": "OpenShift Golden Topology¶\n\nOverview¶\n\nA highly available, resilient deployment starts with a solid physical infrastructure foundation that embeds all these principles. The topology described bellow provides prescriptive guidance on how to achieve these principles at the Network, Compute and Storage layer of your cluster. All implementation details are handled with YAML so we can leverage Infrastructure as Code/GitOps processes to automate the experience.\n\nNetworking¶\n\nWe start by deploying our cluster into a single region with multiple availability zones.\n\nBy deploying into multiple AZs, we provide physical redundancy of our infrastructure into physically separated resources.\n\nWe depend on external Network Services like DHCP and DNS to provide initial cluster connectivity and configuration.\n\nFinally, Load Balancers are used to provide access to the management and application endpoints of our cluster.\n\nRegions¶\n\nA region is defined as an actual real-life geographical location where your cloud resources are located. They could be public cloud regions or your on-prem datacenters. An OpenShift cluster should only span a single region. Stretch clusters are not recommended, no matter the latency between these regions, to avoid ETCD database corruption.\n\nNote\n\nIf you need to spread your application workload across multiple regions, the use of a Geographical LoadBalancer is recommended.\n\nAvailability Zones¶\n\nAn availability zone (AZ) is defined as physical locations in your region that are physically separated and connected with private, low latency, high throughput and redundant network connections.\n\nNetwork Services¶\n\nWhen deploying OpenShift, certain network services need to pre in place prior to deployment\n\nDHCP: DHCP is required to provide initial network connectivity to your nodes so it can download its ignition configuration file\n\nDNS: DNS is required to provide information on LoadBalancer endpoints for configuration\n\nNTP: Time Synchronization across all cluster nodes is critical due to the use of TLS certificates across the platform. By default they will reach out to Red Hat NTP servers, but can be configured to use internal enterprise NTP servers.\n\nContainer Image Registry: If you're deploying in an environment that has connectivity to the internet, the cluster will use public container image registries provided by Red Hat (quay.io) or IBM (cp.icr.io). If you're deploying in a Restricted Network environment, you need to mirror the platform, operatorhub and cloudpak container images to a local image repository.\n\nIn a public cloud environment, DHCP and DNS are for the most part handled automatically by your cloud provider, and are configured when deploying your OpenShift Cluster with IPI. When deploying On-Prem, you need to provide these services in your infrastructure.\n\nWarning\n\nAvoid creating a bastion server that also serves as a DHCP/DNS server to provide these services for the cluster. Production level clusters require production-level backing services.\n\nLoadBalancers¶\n\nTo spread workload and API traffic to your nodes, there are 3 endpoints to consider\n\nAPI: LoadBalances Cluster API traffic to all masters on port 6443\n\nDNS CNAME or A record\n\nPoints api.cluster_name.base_domain to the IP Address of this LoadBalancer\n\nAPI-INT: LoadBalances Cluster API traffic to all masters and bootstrap server on port 6443 and 22623\n\nDNS CNAME or A record\n\nPoints api-int.cluster_name.base_domain to the IP Address this LoadBalancer\n\nAPPS: LoadBalances HTTP/HTTPS traffic to all worker nodes on port 80 and 443.\n\nDNS WILDCARD CNAME or A record\n\nPoints *.apps.cluster_name.base_domain to the IP Address this LoadBalancer\n\nWarning\n\nThe API-INT endpoint contains sensitive cluster information. Split API and API-INT into separate load balancers, and place adequate traffic filters on API-INT so its only accessible from the cluster members CIDR range.\n\nOpenShift SDN¶\n\nWhen sizing your cluster, be aware of the networking section in your install-config.yaml cluster definition. The default values are shown bellow:\n\nWarning\n\nMake sure there's no overlap between any of the CIDR ranges and external resources (databases, mainframes, etc) that need to be accessed from within the cluster.\n\nnetworking.clusterNetwork.cidr: The CIDR range for pods in the OpenShift SDN.\n\nnetworking.clusterNetwork.hostPrefix: Defines the mask for cluster network for pods within a node. Controls the maximum number of pods that can be placed on a single node\n\nnetworking.machineNetwork.cidr: The CIDR range for OpenShift nodes in your network.\n\nnetworking.serviceNetwork: The CIDR range for services in the OpenShift SDN\n\nnetworking.networkType: The CNI plugin to use for the OpenShift SDN.\n\nThe networking.clusterNetwork parameters control how many nodes per cluster and pods per node you can have. With the default values, you can host up to 512 nodes and 510 pods per node.\n\nnodesPerCluster: 2^(hostPrefix - cidrMask)\n\npodsPerNode: 2^(32 - hostPrefix) - 2\n\nclusterNetwork.cidr clusterNetwork.hostPrefix nodesPerCluster podsPerNode 10.128.0.0/14 23 512 510 10.128.0.0/14 24 1024 254 10.128.0.0/12 23 2048 510\n\nCompute¶\n\nControl Plane¶\n\nA good initial size for your masters is 3 nodes with 8CPU and 32GB memory. Since master nodes are deployed as static Machines objects, replacing them down the line is a complex task. The following outlines Red Hat's recommended Control Plane sizing. Oversizing them at deployment will ensure you have a cluster that can scale past your original estimates if needed.\n\nNumber of worker nodes CPU cores Memory (GB) Storage (GB) 25 4 16 120 100 8 32 120 250 16 96 120\n\nNote\n\nSpread your Control Plane nodes across multiple Availability Zones in your Region to provide resiliency to your cluster. Control Plane nodes should be deployed on a separate subnet within the machineNetwork CIDR range.\n\nWarning\n\nStorage for Control Plane nodes should provide at least 500 IOPS to minimize etcd latency.\n\nCompute Nodes¶\n\nAny node that is not a Control Plane node is considered a Compute (or Worker) node. They should be deployed as MachineSets to ensure High Availability and scalability\n\nNote\n\nCompute nodes should be deployed on a separate subnet within the machineNetwork CIDR range.\n\nCreate a MachineSet per Availability Zone per Compute Node type (Infrastructure, Storage, CloudPak).\n\nInfrastructure Nodes¶\n\nYou should deploy at least 3 nodes to host the OpenShift infrastructure components with 4CPU and 16GB memory. They are deployed on the worker subnet.\n\nInfrastructure Nodes allow customers to isolate infrastructure workloads for 2 primary purposes:\n\nTo prevent incurring billing costs against subscription counts\n\nTo separate maintenance and management\n\nThe following outlines Red Hat's recommended Infrastructure Node sizing\n\nCompute Nodes CPU Memory (GB) CPU (Cluster Logging Enabled) Memory (GB) (Cluster Logging Enabled) 25 4 16 4 64 100 8 32 8 128 250 16 128 16 128 500 32 128 32 192\n\nThey are deployed as MachineSets with one MachineSet per Availability Zone. The MachineSet definition should include the following taints to ensure no non-infrastructure component is deployed on these nodes.\n\nKey Value Effect infra \"\" NoSchedule\n\nNote\n\nSince MachineSets can be modified, you can start with smaller sized nodes and scale as your cluster grows.\n\nWarning\n\nInfrastructure Nodes do not draw against your OpenShift Licensing Subscription. The use of taints ensures that only infrastructure components run on these nodes.\n\nThe following components are considered infrastructure components\n\nImage Registry\n\nIngress Controller\n\nMonitoring\n\nMetrics\n\nCluster Logging\n\nService Brokers\n\nRed Hat Quay\n\nRed Hat OpenShift Data Foundation (previously Red Hat OpenShift Container Storage)\n\nRed Hat Advanced Cluster Management\n\nRed Hat Advanced Cluster Security\n\nRed Hat OpenShift Gitops\n\nRed Hat OpenShift Pipelines\n\nWarning\n\nCluster Logging is not deployed by default. If you're deploying it, take into account the increased capacity requirements outlined on the sizing table\n\nPlacement of these components is controlled with a combination of nodeSelectors and tolerations for each of the above deployments.\n\nImage Registry¶\n\nIngress Controller¶\n\nMonitoring¶\n\nCreate a openshift-monitoring-configmap.yaml file with the following ConfigMap and apply it to your cluster.\n\nCluster Logging¶\n\nStorage¶\n\nBy default, OpenShift will provide a ReadWriteOnce storage class leveraging the Cloud Provider storage infrastructure. For any storage requirements that can't be met with the cloud provider native storage, the recommended solution is to deploy OpenShift Data Foundation/OpenShift Data Foundation.\n\nOCS handles data replication between multiple storage nodes, so your data will always be available regardless of any Availability Zone issues. If a ReadWriteMany storage class is required for your workloads, the use of OpenShift Data Foundation is recommended.\n\nYou should deploy at least 3 nodes with 16CPU and 64GB memory.\n\nWatch this video for an introduction to the various types of storage available in a Kubernetes cluster.\n\nStorage Nodes¶\n\nThey are deployed on the worker subnet as MachineSets with one MachineSet per Availability Zone, and one replica per AZ. The MachineSet definition should include the following taints and labels to ensure no non-storage component is deployed on these nodes. If additional storage is needed in the future, you can Storage MachineSets as needed.\n\nTaints¶\n\nKey Value Effect node.ocs.openshift.io/storage true NoSchedule\n\nLabels¶\n\nLabel Value cluster.ocs.openshift.io/openshift-storage \"\"\n\nNote\n\nAdd taints and labels in the MachineSet definition to minimize manual configuration steps\n\nOpenShift Data Foundation will provide the following StorageClasses\n\nStorage Class ReadWriteOnce ReadWriteMany ObjectBucket ocs-storagecluster-ceph-rbd ✅ ❌ ❌ ocs-storagecluster-cephfs ✅ ✅ ❌ openshift-storage.noobaa.io ❌ ❌ ✅\n\nDeploying OpenShift Data Foundation / OpenShift Container Storage¶\n\nNote\n\nUse OpenShift Data Foundation for OpenShift version 4.9 and above\n\nUse OpenShift Container Storage for OpenShift version 4.8.x and under\n\nTradeoffs and Considerations¶\n\nWhile the guidelines presented here are based on a robust production system, there are scenarios where deploying this system is not possible. Bellow are some considerations that you should be aware of when deviating from this Golden Topology.\n\nNetwork¶\n\nAvailability Zones¶\n\nYou can deploy OpenShift into a single Availability Zone if your Cloud Provider Region does not have multiple zones. If you do so, you should take into account that if the zone becomes unavailable, your whole cluster will be unavailable.\n\nNetwork Services¶\n\nWhen deploying into an On-Prem environment, your datacenter may not have DHCP services available. In this case, the use of static IP addresses is required. You will have to deploy using a custom UPI installation. Since MachineSets require DHCP to dynamically allocate IP addresses to the machines, you will not be able to deploy your additional nodes as MachineSets. This will slow down the process of scaling up your cluster significantly.\n\nStretch Clusters¶\n\nDeploying a cluster that spans multiple On-Prem datacenters is not recommended, but exceptions can be obtained with Red Hat's approval. If your production architecture requires a stretch cluster, you should consult with your Red Hat representative.\n\nLoadBalancers¶\n\nYou can host all 3 endpoints on a single LoadBalancer if needed. Be aware that port 22623 contains sensitive cluster credentials that, if accessed, could compromise the security of your cluster. Make sure you have access control policies in place to prevent unauthorized access.\n\nCompute¶\n\nShared Infrastructure Nodes and Storage Nodes¶\n\nYou can host your infrastructure components on the same nodes as your storage solution. However, be aware of the following limitations:\n\nYou still need to \"make room\" for the infrastructure and storage pods. This means that your nodes should be at least 20 vCPUs and 80GB memory. This is an uncommon compute flavor in cloud providers, with the next closest one being 32 vCPUs and 128GB memory.\n\nWhile you only need 3 storage nodes to deploy OpenShift Data Foundation, you may need more than 3 infrastructure nodes if you're hosting certain infrastructure components (logging, monitoring, etc).\n\nStorage¶\n\nAlternate Storage Solutions¶\n\nThis Guide recommends the use of OpenShift Data Foundation to deploy your storage solution. If you need to use another storage solution, consider the following:"
    }
}
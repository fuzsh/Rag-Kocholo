{
    "id": "dbpedia_8525_0",
    "rank": 27,
    "data": {
        "url": "https://arxiv.org/html/2405.07513v1",
        "read_more_link": "",
        "language": "en",
        "title": "Fine-tuning the SwissBERT Encoder Model for Embedding Sentences and Documents",
        "top_image": "",
        "meta_img": "",
        "images": [
            "https://arxiv.org/html/extracted/2405.07513v1/images/training-visual.png"
        ],
        "movies": [],
        "keywords": [],
        "meta_keywords": [
            ""
        ],
        "tags": null,
        "authors": [],
        "publish_date": null,
        "summary": "",
        "meta_description": "",
        "meta_lang": "en",
        "meta_favicon": "",
        "meta_site_name": "",
        "canonical_link": null,
        "text": "Fine-tuning the SwissBERT Encoder Model for\n\nEmbedding Sentences and Documents\n\nJuri Grosjean Jannis Vamvas\n\nDepartment of Computational Linguistics, University of Zurich\n\njurileander.grosjean@uzh.ch, vamvas@cl.uzh.ch\n\nAbstract\n\nEncoder models trained for the embedding of sentences or short documents have proven useful for tasks such as semantic search and topic modeling. In this paper, we present a version of the SwissBERT encoder model that we specifically fine-tuned for this purpose. SwissBERT contains language adapters for the four national languages of Switzerland ‚Äì German, French, Italian, and Romansh ‚Äì and has been pre-trained on a large number of news articles in those languages. Using contrastive learning based on a subset of these articles, we trained a fine-tuned version, which we call SentenceSwissBERT. Multilingual experiments on document retrieval and text classification in a Switzerland-specific setting show that SentenceSwissBERT surpasses the accuracy of the original SwissBERT model and of a comparable baseline. The model is openly available for research use.\n\n1 Introduction\n\nSentence embeddings have become a valuable tool in natural language processing. Neural models are fed with sequence strings and convert them into embeddings, i.e. a numeric representation of the input text. These can be applied in a variety of contexts, e.g. information retrieval, semantic similarity, text classification and topic modeling.\n\nSwissBERT (Vamvas et al., 2023) is a modular encoder model based on X-MOD (Pfeiffer et al., 2022), which was specifically designed for multilingual representation learning. SwissBERT has been trained via masked language modeling on more than 21 million Swiss news articles in Swiss Standard German, French, Italian, and Romansh Grischun. The model is designed for processing Switzerland-related text, e.g. for named entity recognition, part-of-speech tagging, text categorization, or word embeddings.\n\nThe aim of this work is to fine-tune the existing SwissBERT model for the embedding of sentences and short documents. Specifically, our hypothesis is that using a contrastive learning technique such as SimCSE Gao et al. (2021) to fine-tune SwissBERT will yield a model that outperforms the base model as well as generic multilingual sentence encoders in the context of processing news articles from Switzerland.\n\nThis is evaluated on two natural language processing tasks that utilize sentence embeddings, namely document retrieval and nearest-neighbor text classification, both from a monolingual and cross-lingual perspective. Indeed, the experiments show that the fine-tuned SwissBERT, which we call SentenceSwissBERT, has a higher accuracy than baseline models. An especially strong effect was observed for the Romansh language, with an absolute improvement in accuracy of up to 55 percentage points over the original SwissBERT model, and up to 29 percentage points over the best SentenceBERT baseline.\n\n2 Related Work\n\nSentence-BERT\n\nThis approach introduced by Reimers and Gurevych (2019) enhances BERT and RoBERTa for generating fixed-size sentence embeddings. It investigated using the CLS-token, the mean of all output vectors (MEAN-strategy), or the max-over-time of output vectors (MAX-strategy) as sentence embeddings and found the MEAN-strategy to perform best. The method applies siamese and triplet network architectures to finetune pre-trained models, which enables them to learn high-quality sentence embeddings, e.g. for comparison via cosine similarity. The training approach entails three objective functions: classification, regression, and triplet, each with specific training structures. Data from SNLI (Bowman et al., 2015) and MultiNLI datasets (Williams et al., 2018) was used for training. Sentence-BERT has given rise to a family of popular open-source encoder models.\n\nMultilingual Sentence Embeddings\n\nThere are multiple approaches for training BERT-based encoder models for cross-lingual transfer. Reimers and Gurevych (2020) propose utilizing knowledge distillation to enhance mono-lingual models for multilingual use. Feng et al. (2022) found that harnessing pre-trained language models and fine-tuning them for cross-lingual tasks yields promising results while requiring less training data than training encoder models from scratch via multilingual language data like translations.\n\nContrastive Learning\n\nThis technique originally surged in training neural models to perform vision tasks, e.g. image recognition. However, it has also been shown to deliver promising results with NLP tasks. The goal is for the model to learn an embedding space in which similar data is mapped closely to each other and unalike data stays far apart. For a mini-batch of NùëÅNitalic_N sentences, where (hi,hi+)subscript‚Ñéùëñsuperscriptsubscript‚Ñéùëñ(h_{i},h_{i}^{+})( italic_h start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT , italic_h start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT start_POSTSUPERSCRIPT + end_POSTSUPERSCRIPT ) represent a pair of semantically-related sequences, hjsubscript‚Ñéùëóh_{j}italic_h start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT a random in-batch negative, and œÑùúè\\tauitalic_œÑ the temperature hyperparameter, the training objective looks as follows:\n\n‚àílog‚Å°ecos_sim‚Å¢(hi,hi+)/œÑ‚àëj=1Necos_sim‚Å¢(hi,hj+)/œÑsuperscriptùëícos_simsubscript‚Ñéùëñsubscriptsuperscript‚Ñéùëñùúèsuperscriptsubscriptùëó1ùëÅsuperscriptùëícos_simsubscript‚Ñéùëñsubscriptsuperscript‚Ñéùëóùúè-\\log\\frac{e^{\\text{cos\\_sim}(h_{i},h^{+}_{i})/\\tau}}{\\sum_{j=1}^{N}e^{\\text{% cos\\_sim}(h_{i},h^{+}_{j})/\\tau}}- roman_log divide start_ARG italic_e start_POSTSUPERSCRIPT cos_sim ( italic_h start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT , italic_h start_POSTSUPERSCRIPT + end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ) / italic_œÑ end_POSTSUPERSCRIPT end_ARG start_ARG ‚àë start_POSTSUBSCRIPT italic_j = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_N end_POSTSUPERSCRIPT italic_e start_POSTSUPERSCRIPT cos_sim ( italic_h start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT , italic_h start_POSTSUPERSCRIPT + end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT ) / italic_œÑ end_POSTSUPERSCRIPT end_ARG (1)\n\nIntroduced by Gao et al. (2021), the SimCSE (simple contrastive sentence embedding) framework has been found highly effective when used in conjunction with pre-trained language models. This technique can be applied using an unsupervised or a supervised training.\n\nFor the unsupervised approach, the sequences in the training data are matched with themselves to create positive matches, i.e. the cosine similarity between both outputs (MEAN pooling or CLS) is maximized. Thanks to the dropout masks, the embeddings of identical sequences still differ slightly.\n\nThe supervised approach uses a dataset of sentence pairs with similar meanings, and an optional third entry that is contradictory in meaning to the other two (hard negative). The similarity computation is maximized for the similar sentence pairs and minimized between the positives and the negatives.\n\n3 Fine-tuning\n\nTo fine-tune SwissBERT for sentence embeddings, we opted for a (weakly) supervised SimCSE approach without hard negatives. Analogous to the original SwissBERT, Swiss news articles serve as the training data for this. The documents are split into sequence pairs, where one sequence consists of the article‚Äôs title and ‚Äì if available ‚Äì its lead concatenated, while the other contains the text body (see Figure 1). The title-body pairs represent (hi,hi+)subscript‚Ñéùëñsubscriptsuperscript‚Ñéùëñ(h_{i},h^{+}_{i})( italic_h start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT , italic_h start_POSTSUPERSCRIPT + end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ) in the constrastive loss training objective 1.\n\n3.1 Dataset\n\nThe fine-tuning data consists of over 1.5 million Swiss news articles obtained through the Swissdox@LiRI database in German, French, Italian, and Romansh (see Table 1). All German and French articles selected from the corpus have been published between 2020 and 2023, while the Italian and Romansh media date back to 2000, because the database contains fewer articles in these languages. The news articles are pre-processed analogous to SwissBERT‚Äôs original training data (Vamvas et al., 2023).\n\n3.2 Hyperparameters\n\nThe structure of the SimCSE train script provided by Gao et al. (2021) was updated and adapted according to SwissBERT, i.e. adding the X-MOD model architecture configuration as well as a language switch component, so that the model would continuously adjust its adapter according to the training data language during the training process.\n\nDuring fine-tuning on SimCSE, we froze the language adapters and updated all the other parameters. The training data was padded / truncated to 512 tokens, so that it fits the input limit. The model was fine-tuned in one single epoch, using a learning rate of 1e-5 and the AdamW optimizer Loshchilov and Hutter (2019), a batch size of 512 and a temperature of 0.05, which has been recommended for SimCSE (Gao et al., 2021). We used MEAN pooling, following the findings by Reimers and Gurevych (2019).\n\n4 Evaluation\n\nWe evaluate SentenceSwissBERT on two custom, Switzerland-related NLP tasks in German, French, Italian, and Romansh. It is measured against the original SwissBERT and a multilingual Sentence-BERT model that showed the strongest performance in the given evaluation tasks.\n\n4.1 Dataset\n\nFor evaluation, we make use of the 20 Minuten dataset (Kew et al., 2023), based on 20 Minuten, one of the most widely circulated German-language newspapers in Switzerland. The articles tend to be relatively short and cover a variety of topics. Most of the documents in the dataset include a short article summary and topic tags\n\nGiven its format and features, the 20 Minuten dataset is especially suitable for assessing SentenceSwissBERT‚Äôs performance. For the evaluation, all articles present in the 20 Minuten corpus were removed from the original fine-tuning data in all languages, so that there is no overlap.\n\nIn order to expand the evaluation to French, Italian, and Romansh, the relevant parts of the articles were machine-translated via Google Cloud API (FR, IT) and Textshuttle API (RM). Using machine translation allows for a controlled comparison across languages when evaluating, since all documents share the same structure and content. Moreover, manual annotations can be automatically projected to the other languages without a need for additional annotation. A potential downside of machine translation is that the distribution of the test data does not reflect the diversity of human-written text. Tables 3 and 3 report statistics of the data we use for evaluation.\n\n4.2 Tasks\n\nDocument retrieval\n\nFor this task, the embedding of each article‚Äôs summary is compared to all the articles‚Äô content embeddings and then matched by choosing the pair with the highest cosine similarity score. The performance is reported via the accuracy score, which is based on how many summaries were matched with the correct content in relation to the total number of articles processed. There is no train-test split performed for this task. It is performed monolingually (where the summary is written in the same language as the article) and cross-lingually.\n\nText Classification\n\nTen categories are manually mapped from certain topic tags in the dataset. All documents without these (or overlapping) chosen topic tags are disregarded. Then, a random train-test split with a 80/20 ratio is performed once on the remaining data for every category respectively. The exact number of files per category are displayed in Table 3. Next, the text classification is carried out utilizing a nearest neighbors approach: The text body of each test article is compared to every embedding from the training data via cosine similarity. Subsequently, the topic tag of its one nearest neighbor from the training set (highest similarity) is assigned to it.\n\nTo assess cross-lingual transfer, the training data is kept in German for the assessment of each of the four languages, while the test data is machine-translated to French, Italian and Romansh. As the categories vary in frequency, the weighted average of all categories‚Äô F1-scores is reported.\n\n4.3 Baseline Models\n\nSwissBERT\n\nWhile not specifically trained for this, sentence embeddings can already be extracted from the last hidden layer of the original SwissBERT encoder model via MEAN pooling. The input language is specified, just like in its newly fine-tuned version. This comparison demonstrates whether there is value in fine-tuning the model specifically for sentence embeddings.\n\nSentence-BERT\n\nReimers and Gurevych (2019) propose several multilingual sentence embedding models. In this work, the distiluse-base-multilingual-cased-v1 model is opted for as a baseline, as it shows the strongest performance for the given evaluation tasks (see Appendix B). It has originally been trained following the multilingual knowledge distillation approach introduced in Section 2, using mUSE (Chidambaram et al., 2019) as teacher model and a version of the multilingual Universal Sentence Encoder (Yang et al., 2020) as the student model. This version of Sentence-BERT supports various languages, among them French, German, and Italian, but not Romansh. Unlike with SwissBERT, the input language does not need to be specified. This model has a similar number of parameters as SwissBERT (see Table 4). However, it maps to a 512-dimensional embedding space and, hence, is computationally more efficient than SwissBERT.\n\nThe other multilingual Sentence-Transformer (paraphrase-multilingual-mpnet-base-v2) tested is much larger (278 043 648 parameters). Although this model maps to a 768-dimensional space, analogous to SwissBERT, it performed worse in the evaluation tasks than distiluse-base-multilingual-cased-v1 (see Appendix B). Thus, it was disregarded.\n\n5 Results\n\nDocument Retrieval\n\nResults for this evaluation task are reported in Table 6. SentenceSwissBERT outperforms its base model SwissBERT, demonstrating a clear improvement compared to the original model. The largest difference is noticeable in the processing of Romansh text.\n\nSentenceSwissBERT also obtains better results than the Sentence-BERT baseline distiluse-base-multilingual-cased, except for two cases. Both models achieve high accuracy in both the monolingual and cross-lingual tasks. The clearest difference can be seen for German and especially Romansh, which Sentence-BERT was not trained on.\n\nText classification\n\nTable 6 presents the results of this evaluation task. Again, SentenceSwissBERT tends to improve over the baselines, with the exception of Italian, where the Sentence-BERT model is slightly more accurate.\n\n6 Discussion and Conclusion\n\nThe results confirm that contrastive learning with title‚Äìbody pairs is an effective fine-tuning approach for a masked language model. Using just a subset of 1.5 million articles from the original pre-training dataset, a clear improvement on the two sentence-level tasks has been achieved.\n\nOn the one hand, we observed an effect in monolingual tasks, e.g., by matching French summaries with French articles, or by performing nearest-neighbor topic classification of German articles using German examples. On the other hand, we also evaluated cross-lingual variations of those tasks, and found a clear benefit in the cross-lingual setting as well, even though we did not use cross-lingual examples in our fine-tuning. This suggests that modular deep learning with language adapters can be combined effectively with contrastive learning.\n\nWe expect that SentenceSwissBERT will be a useful model variant for other Switzerland-related tasks that require sentence or document embeddings. For example, SentenceSwissBERT might be used for semantic search, or topic modeling based on document embeddings (e.g. BERTopic; Grootendorst, 2022). Future work could also explore whether including training data from other domains than news articles could further improve the generality of the model.\n\nLimitations\n\nThe SentenceSwissBERT model has been trained on news articles only. Hence, it might not perform as well on other text domains. Additionally, the model input during training was limited to a maximum of 512 tokens. Thus, it may not be useful for processing longer texts. Finally, we note that we used machine-translated test data for evaluation in languages other than German.\n\nAcknowledgements\n\nThe authors acknowledge funding by the Swiss National Science Foundation (project MUTAMUR; no. 213976). For this publication, use was made of media data made available via Swissdox@LiRI by the Linguistic Research Infrastructure of the University of Zurich (see https://t.uzh.ch/1hI for more information). The authors are indebted to Gerold Schneider for helpful guidance, and to Textshuttle for providing access to their Romansh machine translation API.\n\nReferences\n\nBowman et al. (2015) Samuel R. Bowman, Gabor Angeli, Christopher Potts, and Christopher D. Manning. 2015. A large annotated corpus for learning natural language inference. In Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages 632‚Äì642, Lisbon, Portugal. Association for Computational Linguistics.\n\nChidambaram et al. (2019) Muthu Chidambaram, Yinfei Yang, Daniel Cer, Steve Yuan, Yunhsuan Sung, Brian Strope, and Ray Kurzweil. 2019. Learning cross-lingual sentence representations via a multi-task dual-encoder model. In Proceedings of the 4th Workshop on Representation Learning for NLP (RepL4NLP-2019), pages 250‚Äì259, Florence, Italy. Association for Computational Linguistics.\n\nFeng et al. (2022) Fangxiaoyu Feng, Yinfei Yang, Daniel Cer, Naveen Arivazhagan, and Wei Wang. 2022. Language-agnostic BERT sentence embedding. In Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 878‚Äì891, Dublin, Ireland. Association for Computational Linguistics.\n\nGao et al. (2021) Tianyu Gao, Xingcheng Yao, and Danqi Chen. 2021. SimCSE: Simple contrastive learning of sentence embeddings. In Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 6894‚Äì6910, Online and Punta Cana, Dominican Republic. Association for Computational Linguistics.\n\nGrootendorst (2022) Maarten R. Grootendorst. 2022. BERTopic: Neural topic modeling with a class-based tf-idf procedure. ArXiv, abs/2203.05794.\n\nKew et al. (2023) Tannon Kew, Marek Kostrzewa, and Sarah Ebling. 2023. 20 minuten: A multi-task news summarisation dataset for German. In Proceedings of the 8th edition of the Swiss Text Analytics Conference, pages 1‚Äì13, Neuchatel, Switzerland. Association for Computational Linguistics.\n\nLoshchilov and Hutter (2019) Ilya Loshchilov and Frank Hutter. 2019. Decoupled weight decay regularization. In 7th International Conference on Learning Representations, ICLR 2019, New Orleans, LA, USA, May 6-9, 2019. OpenReview.net.\n\nPfeiffer et al. (2022) Jonas Pfeiffer, Naman Goyal, Xi Lin, Xian Li, James Cross, Sebastian Riedel, and Mikel Artetxe. 2022. Lifting the curse of multilinguality by pre-training modular transformers. In Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 3479‚Äì3495, Seattle, United States. Association for Computational Linguistics.\n\nReimers and Gurevych (2019) Nils Reimers and Iryna Gurevych. 2019. Sentence-BERT: Sentence embeddings using Siamese BERT-networks. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), pages 3982‚Äì3992, Hong Kong, China. Association for Computational Linguistics.\n\nReimers and Gurevych (2020) Nils Reimers and Iryna Gurevych. 2020. Making monolingual sentence embeddings multilingual using knowledge distillation. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 4512‚Äì4525, Online. Association for Computational Linguistics.\n\nVamvas et al. (2023) Jannis Vamvas, Johannes Gra√´n, and Rico Sennrich. 2023. SwissBERT: The multilingual language model for Switzerland. In Proceedings of the 8th edition of the Swiss Text Analytics Conference, pages 54‚Äì69, Neuchatel, Switzerland. Association for Computational Linguistics.\n\nWilliams et al. (2018) Adina Williams, Nikita Nangia, and Samuel Bowman. 2018. A broad-coverage challenge corpus for sentence understanding through inference. In Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers), pages 1112‚Äì1122, New Orleans, Louisiana. Association for Computational Linguistics.\n\nYang et al. (2020) Yinfei Yang, Daniel Cer, Amin Ahmad, Mandy Guo, Jax Law, Noah Constant, Gustavo Hernandez Abrego, Steve Yuan, Chris Tar, Yun-hsuan Sung, Brian Strope, and Ray Kurzweil. 2020. Multilingual universal sentence encoder for semantic retrieval. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics: System Demonstrations, pages 87‚Äì94, Online. Association for Computational Linguistics.\n\nAppendix A Pre-training dataset media composition\n\n\\topcaption\n\nMedia Count and Language \\tablefirstheadMedium Articles Language\n\n\\tableheadMedium Articles Language\n\n\\tabletail \\bottomcaptionComposition of the dataset used to fine-tune the SwissBERT model according to medium and language. \\tablelasttail {xtabular}l r r lematin.ch & 99 939 fr\n\n24heures.ch 73 385 fr\n\ntdg.ch 69 498 fr\n\nLe Temps 63 130 fr\n\n24 heures 62 004 fr\n\nTribune de Gen√®ve 57 604 fr\n\nblick.ch 51 556 de\n\nrsi.ch 51 526 it\n\nletemps.ch 48 353 fr\n\nrts.ch 47 397 fr\n\ncash.ch 46 750 de\n\nblick.ch 43 178 fr\n\nrtr.ch 39 732 rm\n\nsrf.ch 29 536 de\n\nnzz.ch 28 091 de\n\ntagblatt.ch 27 279 de\n\nluzernerzeitung.ch 23 855 de\n\nAargauer Zeitung / MLZ 21 868 de\n\nNeue Z√ºrcher Zeitung 18 408 de\n\nLe Matin Dimanche 18 352 fr\n\nThurgauer Zeitung 17 335 de\n\nBlick 14 636 de\n\nlandbote.ch 13 089 de\n\nTages-Anzeiger 13 040 de\n\nbazonline.ch 12 709 de\n\naargauerzeitung.ch 12 309 de\n\nbernerzeitung.ch 12 207 de\n\nZofinger Tagblatt / MLZ 11 888 de\n\ntagesanzeiger.ch 11 612 de\n\nberneroberlaender.ch 11 603 de\n\nthunertagblatt.ch 11 581 de\n\nzsz.ch 11 517 de\n\nL‚ÄôIllustr√© 11 231 fr\n\nlangenthalertagblatt.ch 11 184 de\n\nzuonline.ch 11 120 de\n\nBasler Zeitung 10 895 de\n\nderbund.ch 10 748 de\n\nschweizer-illustrierte.ch 10 620 de\n\nZuger Zeitung 10 557 de\n\nbz - Zeitung f√ºr die Region Basel 10 528 de\n\nhandelszeitung.ch 9 790 de\n\npme.ch 9 491 fr\n\nDer Bund 9 396 de\n\nWerdenberger & Obertoggenburger 9 214 de\n\nDer Landbote 9 122 de\n\nZ√ºrichsee-Zeitung 9 019 de\n\nfuw.ch 8 791 de\n\nLuzerner Zeitung 8 651 de\n\nBadener Tagblatt 8 435 de\n\nUrner Zeitung 8 284 de\n\nSt. Galler Tagblatt 8 117 de\n\nWiler Zeitung 8 003 de\n\nBerner Zeitung 7 777 de\n\nAppenzeller Zeitung 7 548 de\n\nZ√ºrcher Unterl√§nder 7 425 de\n\nOltner Tagblatt / MLZ 7 420 de\n\nbadenertagblatt.ch 7 140 de\n\nBerner Oberl√§nder 7 138 de\n\nFemina 7 106 fr\n\nToggenburger Tagblatt 7 032 de\n\nThuner Tagblatt 6 982 de\n\nsolothurnerzeitung.ch 6 120 de\n\nbzbasel.ch 5 921 de\n\nRTS.ch 5 914 fr\n\nObwaldner Zeitung 5 854 de\n\nNidwaldner Zeitung 5 844 de\n\nTV 8 5 677 fr\n\nSonntagsblick 5 606 de\n\nGrenchner Tagblatt 5 530 de\n\nSolothurner Zeitung / MLZ 5 450 de\n\nBZ - Langenthaler Tagblatt 5 277 de\n\nSonntagsZeitung 5 228 de\n\nLimmattaler Zeitung / MLZ 5 042 de\n\nNZZ am Sonntag 4 991 de\n\nFinanz und Wirtschaft 4 962 de\n\nSWI swissinfo.ch 4 855 it\n\nGl√ºckspost 4 621 de\n\nLimmattaler Zeitung 4 513 de\n\nlimmattalerzeitung.ch 4 488 de\n\nrts Vid√©o 4 092 fr\n\nDie Weltwoche 4 011 de\n\nBilan 3 979 fr\n\noltnertagblatt.ch 3 958 de\n\ngrenchnertagblatt.ch 3 857 de\n\nswissinfo.ch 3 575 it\n\nwww.swissinfo.ch 3 541 it\n\nswissinfo.ch 3 525 fr\n\nPME Magazine 3 244 fr\n\nillustre.ch 3 077 fr\n\nSchweizer Illustrierte 3 068 de\n\nHandelszeitung 2 917 de\n\nsrf Video 2 558 de\n\nDie Wochenzeitung 1 953 de\n\nbellevue.nzz.ch 1 919 de\n\nThalwiler Anzeiger/Sihltaler 1 826 de\n\nZuger Presse 1 781 de\n\nHZ Insurance 1 617 de\n\nSchweizer Familie 1 570 de\n\nweltwoche.ch 1 466 de\n\nBeobachter 1 446 de\n\nZugerbieter 1 409 de\n\nGuide TV Cin√©ma 1 384 fr\n\nweltwoche.de 1 267 de\n\nTele 1 176 de\n\nBilanz 1 085 de\n\nswissinfo.ch 1 004 de\n\nencore! 986 fr\n\nBeobachter.ch 984 de\n\nDas Magazin 982 de\n\nz√ºritipp (Tages-Anzeiger) 882 de\n\nNZZ am Sonntag Magazin 823 de\n\nTV Star 764 de\n\nweltwoche-daily.ch 719 de\n\nbilanz.ch 596 de\n\nSWI swissinfo.ch 587 fr\n\nStreaming 535 de\n\nHZ Insurance 529 fr\n\nNZZ PRO Global 446 de\n\nSchweizer LandLiebe 441 de\n\nglueckspost.ch 399 de\n\nencore! (dt) 274 de\n\nNewsnet / 24 heures 227 fr\n\nTV Land & L√ºt 215 de\n\nNZZ Geschichte 151 de\n\nSI Sport 143 de\n\nNewsnet / Berner Zeitung 143 de\n\nBolero 142 de\n\nboleromagazin.ch 118 de\n\nNZZ Folio 109 de\n\nbeobachter.ch 107 de\n\nAargauer Zeitung / MLZ 91 fr\n\nHZ Insurance 77 it\n\nSI Gruen 70 de\n\nL‚ÄôIllustr√© Sport 70 fr\n\nNewsnet / Basler Zeitung 69 de\n\nNewsnet / Der Bund 58 de\n\nBolero F 56 fr\n\nSchweiz am Wochenende 47 fr\n\nBadener Tagblatt 34 fr\n\nSchweizer Versicherung 31 fr\n\nNewsnet / Le Matin 28 fr\n\nNewsnet / Tribune de Gen√®ve 25 fr\n\nSchweizer Illustrierte Style 23 it\n\nGrenchner Tagblatt 22 fr\n\nOltner Tagblatt / MLZ 21 fr\n\nWerdenberger & Obertoggenburger 21 it\n\nSolothurner Zeitung / MLZ 20 fr\n\nLimmattaler Zeitung / MLZ 20 fr\n\nFinanz und Wirtschaft 18 fr\n\nNZZ Online 16 de\n\nSchweizer Versicherung 16 it\n\nTV4 12 de\n\nLimmattaler Zeitung 9 fr\n\nrts Video 7 fr\n\nSWI swissinfo.ch 6 de\n\nNewsnet / Tages-Anzeiger 6 de\n\nHandelszeitung 6 it\n\nBerner Oberl√§nder 5 fr\n\nThuner Tagblatt 5 fr\n\nberneroberlaender.ch 4 fr\n\nBeobachter.ch 4 it\n\nthunertagblatt.ch 3 fr\n\nNeue Z√ºrcher Zeitung 3 it\n\ncash.ch 2 fr\n\nBlick 2 it\n\nBerner Zeitung 2 it\n\nsrf.ch 2 it\n\nweltwoche.de 2 it\n\nBlick 1 fr\n\nbernerzeitung.ch 1 fr\n\nfuw.ch 1 fr\n\nSonntagsblick 1 fr\n\nBasler Zeitung 1 fr\n\nweltwoche.ch 1 fr\n\nweltwoche.de 1 fr\n\nsrf.ch 1 fr\n\nbazonline.ch 1 fr\n\nrtr.ch 1 it\n\nderbund.ch 1 it\n\nSt. Galler Tagblatt 1 it\n\nDie Weltwoche 1 it\n\nDas Magazin 1 it\n\nnzz.ch 1 it\n\nBasler Zeitung 1 it\n\nSchweiz am Sonntag / MLZ 1 it\n\nblick.ch 1 it\n\nCash 1 it\n\nbazonline.ch 1 it\n\nAppendix B Evaluation results of Sentence-BERT baselines"
    }
}
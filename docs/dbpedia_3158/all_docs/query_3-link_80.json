{
    "id": "dbpedia_3158_3",
    "rank": 80,
    "data": {
        "url": "https://news.microsoft.com/source/features/ai/openai-azure-supercomputer/",
        "read_more_link": "",
        "language": "en",
        "title": "Microsoft announces new supercomputer, lays out vision for future AI work",
        "top_image": "https://pub-66c8c8c5ae474e9a9161c92b21de2f08.r2.dev/2022/10/cropped-Microsoft_logo.svg_-300x300-1-128x120.png",
        "meta_img": "https://pub-66c8c8c5ae474e9a9161c92b21de2f08.r2.dev/2022/10/cropped-Microsoft_logo.svg_-300x300-1-128x120.png",
        "images": [
            "https://img-prod-cms-rt-microsoft-com.akamaized.net/cms/api/am/imageFileData/RE1Mu3b?ver=5c31",
            "https://img-prod-cms-rt-microsoft-com.akamaized.net/cms/api/am/imageFileData/RE1Mu3b?ver=5c31",
            "https://blogs.microsoft.com/wp-content/uploads/prod/sites/3/2020/05/openai-azure-supercomputer-kevin-scott_2_1920x1080-5ec3bbdb4945d.jpg",
            "https://blogs.microsoft.com/wp-content/uploads/prod/sites/3/2020/05/openai-azure-supercomputer-self-supervised-learning_2_1920x1080-5ec3be52e1a4d.jpg"
        ],
        "movies": [],
        "keywords": [],
        "meta_keywords": [
            ""
        ],
        "tags": null,
        "authors": [],
        "publish_date": "2020-05-19T07:00:00+00:00",
        "summary": "",
        "meta_description": "Microsoft has built one of the top five publicly disclosed supercomputers in the world, with new infrastructure available to train very large AI models.",
        "meta_lang": "en",
        "meta_favicon": "https://pub-66c8c8c5ae474e9a9161c92b21de2f08.r2.dev/2022/10/cropped-Microsoft_logo.svg_-300x300-1-128x120.png",
        "meta_site_name": "Source",
        "canonical_link": "https://news.microsoft.com/source/features/ai/openai-azure-supercomputer/",
        "text": "Microsoft has built one of the top five publicly disclosed supercomputers in the world, making new infrastructure available in Azure to train extremely large artificial intelligence models, the company is announcing at its Build developers conference.\n\nBuilt in collaboration with and exclusively for OpenAI, the supercomputer hosted in Azure was designed specifically to train that company’s AI models. It represents a key milestone in a partnership announced last year to jointly create new supercomputing technologies in Azure.\n\nIt’s also a first step toward making the next generation of very large AI models and the infrastructure needed to train them available as a platform for other organizations and developers to build upon.\n\n“The exciting thing about these models is the breadth of things they’re going to enable,” said Microsoft Chief Technical Officer Kevin Scott, who said the potential benefits extend far beyond narrow advances in one type of AI model.\n\n“This is about being able to do a hundred exciting things in natural language processing at once and a hundred exciting things in computer vision, and when you start to see combinations of these perceptual domains, you’re going to have new applications that are hard to even imagine right now,” he said.\n\nA new class of multitasking AI models\n\nMachine learning experts have historically built separate, smaller AI models that use many labeled examples to learn a single task such as translating between languages, recognizing objects, reading text to identify key points in an email or recognizing speech well enough to deliver today’s weather report when asked.\n\nA new class of models developed by the AI research community has proven that some of those tasks can be performed better by a single massive model — one that learns from examining billions of pages of publicly available text, for example. This type of model can so deeply absorb the nuances of language, grammar, knowledge, concepts and context that it can excel at multiple tasks: summarizing a lengthy speech, moderating content in live gaming chats, finding relevant passages across thousands of legal files or even generating code from scouring GitHub.\n\nAs part of a companywide AI at Scale initiative, Microsoft has developed its own family of large AI models, the Microsoft Turing models, which it has used to improve many different language understanding tasks across Bing, Office, Dynamics and other productivity products. Earlier this year, it also released to researchers the largest publicly available AI language model in the world, the Microsoft Turing model for natural language generation.\n\nThe goal, Microsoft says, is to make its large AI models, training optimization tools and supercomputing resources available through Azure AI services and GitHub so developers, data scientists and business customers can easily leverage the power of AI at Scale.\n\n“By now most people intuitively understand how personal computers are a platform — you buy one and it’s not like everything the computer is ever going to do is built into the device when you pull it out of the box,” Scott said.\n\n“That’s exactly what we mean when we say AI is becoming a platform,” he said. “This is about taking a very broad set of data and training a model that learns to do a general set of things and making that model available for millions of developers to go figure out how to do interesting and creative things with.”\n\nTraining massive AI models requires advanced supercomputing infrastructure, or clusters of state-of-the-art hardware connected by high-bandwidth networks. It also needs tools to train the models across these interconnected computers.\n\nThe supercomputer developed for OpenAI is a single system with more than 285,000 CPU cores, 10,000 GPUs and 400 gigabits per second of network connectivity for each GPU server. Compared with other machines listed on the TOP500 supercomputers in the world, it ranks in the top five, Microsoft says. Hosted in Azure, the supercomputer also benefits from all the capabilities of a robust modern cloud infrastructure, including rapid deployment, sustainable datacenters and access to Azure services.\n\nThis is about being able to do a hundred exciting things in natural language processing at once and a hundred exciting things in computer vision, and when you start to see combinations of these perceptual domains, you’re going to have new applications that are hard to even imagine right now.\n\n“As we’ve learned more and more about what we need and the different limits of all the components that make up a supercomputer, we were really able to say, ‘If we could design our dream system, what would it look like?’” said OpenAI CEO Sam Altman. “And then Microsoft was able to build it.”\n\nOpenAI’s goal is not just to pursue research breakthroughs but also to engineer and develop powerful AI technologies that other people can use, Altman said. The supercomputer developed in partnership with Microsoft was designed to accelerate that cycle.\n\n“We are seeing that larger-scale systems are an important component in training more powerful models,” Altman said.\n\nFor customers who want to push their AI ambitions but who don’t require a dedicated supercomputer, Azure AI provides access to powerful compute with the same set of AI accelerators and networks that also power the supercomputer. Microsoft is also making available the tools to train large AI models on these clusters in a distributed and optimized way.\n\nAt its Build conference, Microsoft announced that it would soon begin open sourcing its Microsoft Turing models, as well as recipes for training them in Azure Machine Learning. This will give developers access to the same family of powerful language models that the company has used to improve language understanding across its products.\n\nIt also unveiled a new version of DeepSpeed, an open source deep learning library for PyTorch that reduces the amount of computing power needed for large distributed model training. The update is significantly more efficient than the version released just three months ago and now allows people to train models more than 15 times larger and 10 times faster than they could without DeepSpeed on the same infrastructure.\n\nAlong with the DeepSpeed announcement, Microsoft announced it has added support for distributed training to the ONNX Runtime. The ONNX Runtime is an open source library designed to enable models to be portable across hardware and operating systems. To date, the ONNX Runtime has focused on high-performance inferencing; today’s update adds support for model training, as well as adding the optimizations from the DeepSpeed library, which enable performance improvements of up to 17 times over the current ONNX Runtime.\n\n“We want to be able to build these very advanced AI technologies that ultimately can be easily used by people to help them get their work done and accomplish their goals more quickly,” said Microsoft principal program manager Phil Waymouth. “These large models are going to be an enormous accelerant.”\n\nLearning the nuances of language\n\nDesigning AI models that might one day understand the world more like people do starts with language, a critical component to understanding human intent, making sense of the vast amount of written knowledge in the world and communicating more effortlessly.\n\nNeural network models that can process language, which are roughly inspired by our understanding of the human brain, aren’t new. But these deep learning models are now far more sophisticated than earlier versions and are rapidly escalating in size.\n\nA year ago, the largest models had 1 billion parameters, each loosely equivalent to a synaptic connection in the brain. The Microsoft Turing model for natural language generation now stands as the world’s largest publicly available language AI model with 17 billion parameters.\n\nThis new class of models learns differently than supervised learning models that rely on meticulously labeled human-generated data to teach an AI system to recognize a cat or determine whether the answer to a question makes sense.\n\nIn what’s known as “self-supervised” learning, these AI models can learn about language by examining billions of pages of publicly available documents on the internet — Wikipedia entries, self-published books, instruction manuals, history lessons, human resources guidelines. In something like a giant game of Mad Libs, words or sentences are removed, and the model has to predict the missing pieces based on the words around it.\n\nAs the model does this billions of times, it gets very good at perceiving how words relate to each other. This results in a rich understanding of grammar, concepts, contextual relationships and other building blocks of language. It also allows the same model to transfer lessons learned across many different language tasks, from document understanding to answering questions to creating conversational bots.\n\n“This has enabled things that were seemingly impossible with smaller models,” said Luis Vargas, a Microsoft partner technical advisor who is spearheading the company’s AI at Scale initiative.\n\nThe improvements are somewhat like jumping from an elementary reading level to a more sophisticated and nuanced understanding of language. But it’s possible to improve accuracy even further by fine tuning these large AI models on a more specific language task or exposing them to material that’s specific to a particular industry or company.\n\n“Because every organization is going to have its own vocabulary, people can now easily fine tune that model to give it a graduate degree in understanding business, healthcare or legal domains,” he said.\n\nAI at Scale\n\nOne advantage to the next generation of large AI models is that they only need to be trained once with massive amounts of data and supercomputing resources. A company can take a “pre-trained” model and simply fine tune for different tasks with much smaller datasets and resources.\n\nThe Microsoft Turing model for natural language understanding, for instance, has been used across the company to improve a wide range of productivity offerings over the last year. It has significantly advanced caption generation and question answering in Bing, improving answers to search questions in some markets by up to 125 percent.\n\nIn Office, the same model has fueled advances in the smart find feature enabling easier searches in Word, the Key Insights feature that extracts important sentences to quickly locate key points in Word and in Outlook’s Suggested replies feature that automatically generates possible responses to an email. Dynamics 365 Sales Insights also uses it to suggest actions to a seller based on interactions with customers.\n\nMicrosoft is also exploring large-scale AI models that can learn in a generalized way across text, images and video. That could help with automatic captioning of images for accessibility in Office, for instance, or improve the ways people search Bing by understanding what’s inside images and videos.\n\nTo train its own models, Microsoft had to develop its own suite of techniques and optimization tools, many of which are now available in the DeepSpeed PyTorch library and ONNX Runtime. These allow people to train very large AI models across many computing clusters and also to squeeze more computing power from the hardware.\n\nThat requires partitioning a large AI model into its many layers and distributing those layers across different machines, a process called model parallelism. In a process called data parallelism, Microsoft’s optimization tools also split the huge amount of training data into batches that are used to train multiple instances of the model across the cluster, which are then periodically averaged to produce a single model.\n\nThe efficiencies that Microsoft researchers and engineers have achieved in this kind of distributed training will make using large-scale AI models much more resource efficient and cost-effective for everyone, Microsoft says.\n\nWhen you’re developing a cloud platform for general use, Scott said, it’s critical to have projects like the OpenAI supercomputing partnership and AI at Scale initiative pushing the cutting edge of performance.\n\nHe compares it to the automotive industry developing high-tech innovations for Formula 1 race cars that eventually find their way into the sedans and sport utility vehicles that people drive every day.\n\n“By developing this leading-edge infrastructure for training large AI models, we’re making all of Azure better,” Scott said. “We’re building better computers, better distributed systems, better networks, better datacenters. All of this makes the performance and cost and flexibility of the entire Azure cloud better.”\n\nTop image: At its Build developers conference, Microsoft Chief Technical Officer Kevin Scott is announcing that the company has built one of the top five publicly disclosed supercomputers in the world. Art by Craighton Berman.\n\nRelated:\n\nLearn more: AI at Scale\n\nRead more: Announcing accelerated training with ONNX Runtime—train models up to 45% faster\n\nRead more: ZeRO-2 and DeepSpeed: Shattering barriers of deep learning speed & scale\n\nRead more: Training deep learning models at scale in Azure\n\nRead more: AI at Scale in Microsoft Office 365\n\nRead more: Building a better search experience using the Microsoft Turing models\n\nRead More: Objects are the secret key to revealing the world between vision and language\n\nLearn More: AI at Scale research"
    }
}
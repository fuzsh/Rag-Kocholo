{
    "id": "correct_foundationPlace_00100_3",
    "rank": 80,
    "data": {
        "url": "https://patents.google.com/patent/US20060129745A1/en",
        "read_more_link": "",
        "language": "en",
        "title": "US20060129745A1 - Process and appliance for data processing and computer program product - Google Patents",
        "top_image": "",
        "meta_img": "",
        "images": [
            "https://patentimages.storage.googleapis.com/85/3e/8d/4add757cf40cfb/US20060129745A1-20060615-D00000.png",
            "https://patentimages.storage.googleapis.com/4f/b0/4e/706f9170ee9008/US20060129745A1-20060615-D00001.png",
            "https://patentimages.storage.googleapis.com/ca/c7/84/21bdd1a39f0ef0/US20060129745A1-20060615-D00002.png",
            "https://patentimages.storage.googleapis.com/25/d7/26/ca3eb5fd3c72ac/US20060129745A1-20060615-D00003.png",
            "https://patentimages.storage.googleapis.com/5e/41/e6/7e166452593045/US20060129745A1-20060615-D00004.png",
            "https://patentimages.storage.googleapis.com/fc/b4/48/af52a22b7a8441/US20060129745A1-20060615-D00005.png",
            "https://patentimages.storage.googleapis.com/33/b9/a0/0d1fd6f7910758/US20060129745A1-20060615-D00006.png",
            "https://patentimages.storage.googleapis.com/5c/95/a3/21c542850ad85f/US20060129745A1-20060615-D00007.png",
            "https://patentimages.storage.googleapis.com/84/95/46/ded0b599bc6cb3/US20060129745A1-20060615-D00008.png"
        ],
        "movies": [],
        "keywords": [],
        "meta_keywords": [
            ""
        ],
        "tags": null,
        "authors": [],
        "publish_date": "2004-12-11T00:00:00",
        "summary": "",
        "meta_description": "The present invention concerns an appliance, a process and a computer program product for the processing of unstructured or semi-structured digital data in a file system. In order to create an appliance, a process and a computer program product which allow simple, reliable, high-performance and purpose oriented management of every manner of digital, stored, unstructured data, it is proposed that, when accessing data, logical access be carried out jointly with physical access and, when doing so, a particularly transparent, common access mechanism be implemented for both types of access.",
        "meta_lang": "en",
        "meta_favicon": "",
        "meta_site_name": "",
        "canonical_link": "https://patents.google.com/patent/US20060129745A1/en",
        "text": "Process and appliance for data processing and computer program product Download PDF\n\nInfo\n\nPublication number\n\nUS20060129745A1\n\nUS20060129745A1 US11/040,812 US4081205A US2006129745A1 US 20060129745 A1 US20060129745 A1 US 20060129745A1 US 4081205 A US4081205 A US 4081205A US 2006129745 A1 US2006129745 A1 US 2006129745A1\n\nAuthority\n\nUS\n\nUnited States\n\nPrior art keywords\n\ndata\n\naccess\n\nfile\n\nsmapper\n\nappliance\n\nPrior art date\n\n2004-12-11\n\nLegal status (The legal status is an assumption and is not a legal conclusion. Google has not performed a legal analysis and makes no representation as to the accuracy of the status listed.)\n\nAbandoned\n\nApplication number\n\nUS11/040,812\n\nInventor\n\nGunther Thiel\n\nMark Hardisty\n\nCurrent Assignee (The listed assignees may be inaccurate. Google has not performed a legal analysis and makes no representation or warranty as to the accuracy of the list.)\n\nSMAPPER TECHNOLOGIES GmbH\n\nOriginal Assignee\n\nSMAPPER TECHNOLOGIES GmbH\n\nPriority date (The priority date is an assumption and is not a legal conclusion. Google has not performed a legal analysis and makes no representation as to the accuracy of the date listed.)\n\n2004-12-11\n\nFiling date\n\n2005-01-21\n\nPublication date\n\n2006-06-15\n\n2005-01-21 Application filed by SMAPPER TECHNOLOGIES GmbH filed Critical SMAPPER TECHNOLOGIES GmbH\n\n2005-07-21 Assigned to SMAPPER HANDELS GMBH reassignment SMAPPER HANDELS GMBH ASSIGNMENT OF ASSIGNORS INTEREST (SEE DOCUMENT FOR DETAILS). Assignors: HARDISTY, MARK, THIEL, GUNTHER\n\n2005-12-01 Assigned to SMAPPER TECHNOLOGIES GMBH reassignment SMAPPER TECHNOLOGIES GMBH CHANGE OF NAME (SEE DOCUMENT FOR DETAILS). Assignors: SMAPPER HANDELS GMBH\n\n2005-12-12 Priority to PCT/EP2005/013314 priority Critical patent/WO2006061251A1/en\n\n2005-12-12 Priority to US11/721,298 priority patent/US20090240737A1/en\n\n2005-12-12 Priority to EP05817787A priority patent/EP1831801A1/en\n\n2006-06-15 Publication of US20060129745A1 publication Critical patent/US20060129745A1/en\n\nStatus Abandoned legal-status Critical Current\n\nLinks\n\nUSPTO\n\nUSPTO PatentCenter\n\nUSPTO Assignment\n\nEspacenet\n\nGlobal Dossier\n\nDiscuss\n\n238000000034 method Methods 0.000 title claims abstract description 84\n\n230000008569 process Effects 0.000 title claims abstract description 56\n\n238000012545 processing Methods 0.000 title claims abstract description 25\n\n238000004590 computer program Methods 0.000 title claims abstract description 8\n\n230000007246 mechanism Effects 0.000 claims abstract description 14\n\n230000009471 action Effects 0.000 claims description 53\n\n238000012986 modification Methods 0.000 claims description 22\n\n230000004048 modification Effects 0.000 claims description 22\n\n238000009434 installation Methods 0.000 claims description 9\n\n230000003936 working memory Effects 0.000 claims 1\n\n238000003860 storage Methods 0.000 description 46\n\n238000013459 approach Methods 0.000 description 25\n\n238000007726 management method Methods 0.000 description 23\n\n239000000243 solution Substances 0.000 description 22\n\n230000006870 function Effects 0.000 description 16\n\n238000013523 data management Methods 0.000 description 13\n\n238000011161 development Methods 0.000 description 9\n\n230000002085 persistent effect Effects 0.000 description 7\n\n230000008901 benefit Effects 0.000 description 6\n\n238000013500 data storage Methods 0.000 description 6\n\n238000010586 diagram Methods 0.000 description 6\n\n238000005516 engineering process Methods 0.000 description 6\n\n238000006243 chemical reaction Methods 0.000 description 5\n\n238000010276 construction Methods 0.000 description 5\n\n230000008859 change Effects 0.000 description 4\n\n238000004891 communication Methods 0.000 description 4\n\n238000013461 design Methods 0.000 description 4\n\n230000010076 replication Effects 0.000 description 4\n\n238000010367 cloning Methods 0.000 description 3\n\n238000006073 displacement reaction Methods 0.000 description 3\n\n239000000284 extract Substances 0.000 description 3\n\n230000010354 integration Effects 0.000 description 3\n\n230000003993 interaction Effects 0.000 description 3\n\n238000004519 manufacturing process Methods 0.000 description 3\n\n238000012360 testing method Methods 0.000 description 3\n\n238000004458 analytical method Methods 0.000 description 2\n\n239000002585 base Substances 0.000 description 2\n\n239000000872 buffer Substances 0.000 description 2\n\n238000000605 extraction Methods 0.000 description 2\n\n230000014509 gene expression Effects 0.000 description 2\n\n230000007774 longterm Effects 0.000 description 2\n\n238000012423 maintenance Methods 0.000 description 2\n\n238000013507 mapping Methods 0.000 description 2\n\n238000011160 research Methods 0.000 description 2\n\nYUQQBSIRWGYFQK-UHFFFAOYSA-N sodium;2,7-diacetamido-9h-fluorene-3-sulfonic acid Chemical compound [Na+].CC(=O)NC1=C(S(O)(=O)=O)C=C2C3=CC=C(NC(=O)C)C=C3CC2=C1 YUQQBSIRWGYFQK-UHFFFAOYSA-N 0.000 description 2\n\n230000009897 systematic effect Effects 0.000 description 2\n\n101500020766 Sus scrofa FS-303 Proteins 0.000 description 1\n\n230000003044 adaptive effect Effects 0.000 description 1\n\n239000000654 additive Substances 0.000 description 1\n\n230000000996 additive effect Effects 0.000 description 1\n\n238000013474 audit trail Methods 0.000 description 1\n\n239000003637 basic solution Substances 0.000 description 1\n\n230000006399 behavior Effects 0.000 description 1\n\n230000005540 biological transmission Effects 0.000 description 1\n\n230000003139 buffering effect Effects 0.000 description 1\n\n230000006835 compression Effects 0.000 description 1\n\n238000007906 compression Methods 0.000 description 1\n\n230000001143 conditioned effect Effects 0.000 description 1\n\n238000007596 consolidation process Methods 0.000 description 1\n\n230000006378 damage Effects 0.000 description 1\n\n238000013075 data extraction Methods 0.000 description 1\n\n230000003111 delayed effect Effects 0.000 description 1\n\n238000009826 distribution Methods 0.000 description 1\n\n230000000694 effects Effects 0.000 description 1\n\n238000005538 encapsulation Methods 0.000 description 1\n\n238000011156 evaluation Methods 0.000 description 1\n\n239000012634 fragment Substances 0.000 description 1\n\n238000000265 homogenisation Methods 0.000 description 1\n\n230000006872 improvement Effects 0.000 description 1\n\n230000006855 networking Effects 0.000 description 1\n\n238000004806 packaging method and process Methods 0.000 description 1\n\n230000002688 persistence Effects 0.000 description 1\n\n230000003334 potential effect Effects 0.000 description 1\n\n238000007781 pre-processing Methods 0.000 description 1\n\n238000011084 recovery Methods 0.000 description 1\n\n238000009420 retrofitting Methods 0.000 description 1\n\n238000012552 review Methods 0.000 description 1\n\n238000000926 separation method Methods 0.000 description 1\n\n230000003068 static effect Effects 0.000 description 1\n\n229920000638 styrene acrylonitrile Polymers 0.000 description 1\n\n230000009469 supplementation Effects 0.000 description 1\n\n230000031068 symbiosis, encompassing mutualism through parasitism Effects 0.000 description 1\n\n230000002195 synergetic effect Effects 0.000 description 1\n\n239000013598 vector Substances 0.000 description 1\n\n239000011800 void material Substances 0.000 description 1\n\nImages\n\nClassifications\n\nG—PHYSICS\n\nG06—COMPUTING; CALCULATING OR COUNTING\n\nG06F—ELECTRIC DIGITAL DATA PROCESSING\n\nG06F16/00—Information retrieval; Database structures therefor; File system structures therefor\n\nG06F16/10—File systems; File servers\n\nDefinitions\n\nthe present invention concerns a process or a method and an appliance or an apparatus for data processing as well as a corresponding computer program product.\n\nany form of general data which can be stored falls under the heading of information, that is, language, sound and image data in addition to text and numbers in their respective digital data format and storage forms.\n\ninformation that is, language, sound and image data in addition to text and numbers in their respective digital data format and storage forms.\n\nthe quantity of available data which may also need to be processed in some way is steadily increasing both in a global sense and for each individual user.\n\nincreasing CPU power and new architectures render the creation, processing and transport of an ever-increasing volume of data manageable within a reasonable time frame\n\nthe long-term, safe administration of digitally-stored data presents a growing problem despite the fact that sufficiently expanded storage space is available.\n\na method of processing unstructured or semi-structured digital data in a file-based system is characterized by being able to abolish the existing, prior art separation of logical and physical access to data.\n\nlogical access i.e. with user-defined criteria\n\nphysical access i.e. using the file path.\n\na common access mechanism is implemented for both types of access which is particularly constructed so as to remain transparent or, in other words, unperceived by the user.\n\na file path is processed within the execution of the access mechanism which has been enhanced by a Query-Interface.\n\nthe Query-Interface used in the extended file path constitutes an enhancement of a POSIX- or similar standard in the form of an XQuery-Standard or similar standard.\n\narbitrarily pre-definable data subsets are extracted when accessing unstructured and/or proprietary structured data. These extracted data subsets are preferably stored as meta data in a structured form. Thereby intrinsic data subsets, i.e. extracted from the data itself, and/or extrinsic data i.e. derived from outside the data, is used advantageously to create the respective meta data.\n\nmeta data is created out of arbitrarily pre-definable data subsets, namely on reading and/or writing or, as the case may be, on storing unstructured and/or proprietary structured data.\n\nany form of access to data is used in order to generate corresponding meta data.\n\nthe process is carried out advantageously while preserving the atomicity of the sum of all partial transactions regarding all data which is linked to the respective source data and/or files.\n\nall meta data which has been derived from inside or outside the data, suffer the same fate as the data itself. Consequently, when deleting the original data, it goes without saying that all logically connected data which was derived from the deleted data by means of a process according to the present invention, is likewise deleted.\n\ndata is subject to a pre-defined and customizable-rule and action model.\n\na pre-defined and customizable rule and action model based on the results of the processing of a pre-defined and customizable rule and action model, well-defined decisions and/or actions are carried out. The user is thus given the chance to actively influence the type and choice of rules and actions, for example by modifying the configuration.\n\npart-programs or actions of the rule and action model are carried out in the kernel of the operating system, the execution being bound to rules and conditions.\n\nthe aforementioned partial stages are executed automatically in a further development of the present invention.\n\na process under the present invention is carried out particularly advantageously utilizing standardized software and hardware interfaces. It is hereby executed as an individual unit without interference in or modification to an existing structure, in such a way that mutual interaction can be avoided should retrofitting occur in an existing system. Accordingly, an appliance or apparatus which implements a process under the present invention is characterized by the fact that resources are assigned to connect the appliance to the standardized software and hardware interfaces of the respective data processing installation or the respective system network. A suitable appliance can therefore be integrated as a closed unit into a data processing installation without interference in or modification to an existing structure of the same data processing installation.\n\nthe meta data is set up in its own file system on the basis of the common access mechanism.\n\nthe file system is optimized for the rapid lookup of data content and/or attributes of data content.\n\nthis file system is characterized particularly by allowing a bi-directional, atomic interrelation between data and meta data. This means that, by the same token, modification of the data causes a consistent modification of the affected meta data and vice versa.\n\nThis allows data and its meta data to be processed independently of one another, thus permitting varying views of the original data stream with respect to format, partial-format, etc.; however, every modification in one view leads to a mandatory modification in all other views.\n\nan appliance in accordance with one embodiment of the present invention involves a method of encompassing all levels of the unstructured data, from its physical representation through logical classification to its information content, the information content being edited and adjusted to fall within a well-defined framework of actions and/or decisions.\n\na process in accordance with the present invention is advantageously embodied in a computer program product, which means, in particular, in any form of data carrier, for example a CD-ROM.\n\nthis computer program product causes the execution of a process according to one or several of the afore-mentioned criteria.\n\nFIG. 1 a systematic illustration of contemplatable solution areas.\n\nFIG. 2 an illustration of primary methods to answer the question âWhat does understanding data contents meanâ: extractors and converters, extractors being a special form of converters in this case.\n\nFIG. 3 a basic functionality which forms the basis of a process in accordance with the present invention which is named âSmApper.â\n\nFIG. 4 a chart to illustrate the requirement that SmApper must be integrated transparently as an appliance between Storage-Client and Storage-Server.\n\nFIG. 5 a chart as an illustration of stacking as a method which allows the (strictly-speaking) one-dimensional VFS-process to be extended to several dimensions.\n\nFIG. 6 a chart to illustrate how SmApper uses the stacking procedure.\n\nFIG. 7 a diagrammatic representation of how SmApper, as the only meta data solution, spans all layers from the physical representation to the information.\n\nFIGS. 8 and 9 representations of SmApper's fundamental features as a tool to monitor and control unstructured or semi-structured digital packs of data.\n\nIaC information and communication\n\nIaC systems basically comprise three components: data processing, data transmission and data storage according to Gartner, IDC and Forrester information technology (IT) departments already spend more than 50 percent of their hardware investments on data storage systems.\n\nData storage systems have been optimized to store data and make it available. From a technical point of view the nature of data is insignificant. Radiographs, family pictures, emails, letters of financial data are all treated the same way. Intelligent handling of digital data today is still based on the application, i.e. the many specialized programs and software such as SAP, Microsoft Word, Adobe Photoshop, etc.\n\nMeta data is stored in the same way as the original data as the storage infrastructure does not recognize any difference. However, meta data is usually more important for the cooperation than the original data.\n\nthe core issue concerning digital cooperation for a company is: how do we make sure that the right data of the right quantity and quality are in the right place at the right time? Data has to be transferred between a company's organizational units based on business related rules. This process specific approach has to be independent of the underlying IT infrastructure (and especially the storage infrastructure).\n\nIMM Information Lifecycle Management\n\na new way of object and data oriented data management can only be successful if such tools or systems can be smoothly integrated into the existing infrastructure.\n\nISVs independent software vendors\n\nSAP SAP, Oracle, etc.\n\nsystem integrators Accenture, CGEY, Bearing Point, etc.\n\nIntelligent data management can be detached from the application itself thus resulting in leaner applications with a better cost-effective development process.\n\nData management usually is no longer the core competence of ISVs, so new features based on this might now be realized while they had to be cut before due to the high costs.\n\nsystem integrators especially with regard to the Information Lifecycle Management can offer big potentials for professional services.\n\nsystem integrators also attach great importance to the idea of infrastructure consolidation concepts and an improved projection of business processes on IT processes.\n\nthe solution system can be summarized in the diagram of FIG. 1 .\n\nCMS Content Management Systems\n\nFMS file management systems\n\nILM Information Lifecycle Management Systems\n\nBackup/Recovery Tools and Workflow and Collaboration Systems There are different solutions from the point of view of manufacturers of infrastructure components (above all data storage systems, operation systems and file systems, databases) and manufacturers of applications and user software (Content Management Systems (CMS), file management systems (FMS), Information Lifecycle Management Systems (ILM) or Backup/Recovery Tools and Workflow and Collaboration Systems).\n\nCMS Content Management Systems\n\nFMS file management systems\n\nILM Information Lifecycle Management Systems\n\nBackup/Recovery Tools and Workflow and Collaboration Systems Backup/Recovery Tools and Workflow and Collaboration Systems\n\nFIG. 1 describes the overlapping of the different solution approaches.\n\nthe system shall allow data management of the next generation, namely at the location where the data are stored.\n\nthe solution must represent a transparent expansion of the storage infrastructure and not be just another business application, e.g. Enterprise Content Management Systems.\n\nthe key component of the solution is a layer that allows business rules to be defined and to directly and easily map not only data and meta data, but also their management, storage location, life cycle and flow.\n\nOne aspect of the invention focuses on file-based data. More particularly, the invention may be used in a data processing system of managing unstructured or semi-structured digital data in a file system supported by a computer, the computer having a memory.\n\nthe construction base_type is introduced as a simpler abstraction of the term file.\n\na base_type is most easily comprehended by borrowing from the object-oriented design approach. According to this model, a base_type is a class with well-defined properties (designated as attributes in the following sections) and methods.\n\na base_type is nothing more than the logical encapsulation of any file (in theory).\n\na base_type has as its primary attribute the binary representation of the data contained in the respective file. Further attributes are, for example, date fields, which indicate when the data was last accessed or modified and so on.\n\nthe methods provided by a base_type include, in particular, the capability to access this binary data, to modify it and render the respective condition of the data persistent (in the file).\n\na base_type is a logical construction, which is not made persistent in itself but is merely a medium of describing a physical file and the methods which can be applied to it.\n\na base_type and its methods and properties depend, therefore, on the respective file to which this construction is applied but also, of course, on the capabilities of the fundamental file system.\n\nthe actual instantiation of a base_type results in an object with an allocated file.\n\nthe following will serve as an illustration of the base_type using C++ class (which is however not fully implemented): public class base_type â public: // con/destruction base_type(const char * filename); â base_type( ); //methods ssize_t read(That ssize_t write(%) ssize_t lseek(%) etc. private: // pointer to opaque data stream void *m_data; // where is my physical file const char *m_path; // Filedescriptor int fd; â\n\nEach SmapType has a number of attributes â 0, n>. For example âpagesâ which could be the number of pages in an MS-Word document.\n\nAttributes may have base_type-intrinsic values; abstracted from the base_type or extrinsic; freely-defined values. Every attribute has an explicit qualifier or unique identifier (UID) and is classified by a data type. This could be either simple data types (like int, char, etc.) or complex data types (like string, smap_base_type, etc.). Each attribute possesses a value that corresponds to the data type as well as additional parameters which describe further properties of the attribute.\n\nattributes can be constructed hierarchically (e.g. there could be a subtitle in a document which forms a child-relationship to a title-attribute).\n\na smap_base_type offers methods for reading, setting, numbering or iterating values.\n\nthe new data set thus created must conform to a well-known data type to which well-defined operations can be applied.\n\nThis data set must be associated with a context.\n\nFIG. 2 shows a diagrammatic representation of both methods: the Extractor and the Converter.\n\nan extractor is a set of extract patterns which determine how much of which data is to be extracted to which location within a binary stream.\n\na converter extracts data and then applies a function on it.\n\nan extractor is a special form of a converter, and is in fact a converter with a null-function per pattern.\n\nextractors are a special form of converters.\n\nFIG. 3 demonstrates the basic functions that SmApper provides. These basics, which will be examined in depth in the following sections, form the SmApper core system, with the aid of which the actual modules (or applications) can then be developed.\n\nthe main tasks of the SmApper System are as follows:\n\nthe data subsets generated are assigned to attributes of the smap_base_types and hence are brought into the correct (that is to say definable) context.\n\nthe manner in which the smap_base_type manages its attributes guarantees the data integrity of the individual attributes. Or, to put this a different way, this means that SmApper appends structured data to unstructured data.\n\nRules enable the forming of Boolean Expressions on these attributes by means of attributes and permitted operators which show âTrueâ or âFalseâ as a result. Rules access solely the structured information of the smap_base_type thereby offering the possibility to reach a decision based on the data.\n\nrules run inbound as well as outbound. Inbound means that the affected system component runs in the kernel space of the SmApper (basic) operating system while outbound means that the scope of the code segment is user space. Please see Section 4.1 for further information.\n\nactions enable programs to be executed on the basis of events and conditions (rules), in order to initiate corresponding operations.\n\nSmApper guarantees the complete integrity of the smap_base_type. As soon as any modification to the base_type is made, SmApper displays this automatically for the user and/or the application program atomically in the smap_base_type. In the same way, any (permitted!) modifications to the smap_base_type or its attributes are automatically as well as atomically displayed in the base_type.\n\nFIG. 4 shows these basic requirements of SmApper\n\nSmApper must be able to handle every Network File I/O protocol for Storage-Clients and for Storage-Servers even every storage protocol (file and block) must be handled.\n\nSmApper must have the ability to switch into the communication between Storage-Client and Storage-Server, in order to implement its additional functions smoothly.\n\nstacking The only technical alternative which permits such a procedure without re-inventing the wheel each time and without having to integrate itself into every imaginable protocol stack, is known as stacking [2,3,5].\n\nVFS stands for Virtual File System and stands for a layer, which has become a standard part of modem operating systems and which enables the homogenization of access to heterogeneous physical file system implementations.\n\nVFS is a term from the Linux kernel which may be known by a different name in other operating systems and which, by its nature, is implemented differently, for example the VNODE-layer under SOLARIS; however, the purpose of this layer is always the same.\n\na modern operating system must support a wide array of different file systems: local file systems like NTFS, UFS, XFS, ReiserFS, VxFS, ext2/3, FAT, CD-ROM file systems, to name but a few.\n\nlocal file systems like NTFS, UFS, XFS, ReiserFS, VxFS, ext2/3, FAT, CD-ROM file systems, to name but a few.\n\nnetwork file systems like NFS, CIFS, DAFS, coda and others.\n\nthe operating system core abstracts the underlying physical implementations with the help of the VFS-Layer and compels the physical FS-implementations to abide by a set of pre-defined functions, which may be optionally implemented to some degree.\n\nthe VFS-Layer then ensures that each implementation of the necessary function(s) of the physical file system is retrieved when accessed [6, 7, 2].\n\nthe individual kernel implementations were not developed with the help of object-oriented language tools, on closer examination this concept is about Function Overloading which can be easily demonstrated therefore by virtual functions.\n\nthe VFS-Layer makes a set of virtual functions available, which (can) then be overwritten by the real implementations.\n\nStacking constitutes a process that avails itself of the VFS concept intensively and, in doing so, extends the process.\n\na conventional VFS implementation primarily allows for a VFS-Layer that can retrieve N file systems.\n\nStacking facilitates the retraction of the M VFS-layers as a matter of principle, in which the VFS-layer at position M retrieves the VFS-layer at position M-1 and so on until the actual physical implementation of the underlying file system(s) is retrieved [4].\n\nFIG. 5 illustrates this process, showing that stacking is a method which allows the expansion of the primarily one-dimensional VFS process into a multi-dimensional one [4].\n\na tangible alternative to the stacking concept is the one that SmApper applies in order to control the problem of smooth integration in the communication paths between user-defined Storage-Clients and Storage-Servers.\n\nSmApper applies the stacking process in order to provide the user/application program with a virtual file system (which the user perceives as an actual physical file system).\n\nThis virtual file system masks two (in principle n) actual physical file systems, namely Phys. FS A which, in our illustration, constitutes the actual path and storage-server the user wishes to access.\n\nSmApper One of the essential basic functions of SmApper is the ability to generate data subsets out of the original data stream with the help of the illustrated extractors and make them persistent as smap_base_type-attributes using the SMAP_FS. SmApper makes it possible to execute the extraction completely inbound (that is, while the data stream is being generated or modified and so on) or outbound. The latter is particularly important as there are certain extraction procedures which require too much time to be executed inbound. In this case, or if specified by the user, the data extraction must be effected once the I/O operation has been completed, i.e. in an asynchronous manner.\n\nSmApper applies the stacking process in order to combine all user-defined Phys. FS As with all Phys. FS Bs (QZone of a SMAP_FS) thus guaranteeing the persistent connection between a base_type and a smap_base_type.\n\nthe QZone is not only essential in order to permit outbound-smapping but offers further advantages, as it can be regarded as a caching-entity.\n\nSmApper has its own QZone-daemon which determines the specific time that the actual physical displacement of the buffered data to its designated destination (target-destination, as defined by the user at the original I/O) should take place.\n\nthe parameters for this decision can be as diversified as with any other I/O operation on a SmApper system.\n\nSmApper has to make the attributes of the instantiated smap_base_type object persistent and carry out the procedure as efficiently as possible. Stacking allows us to execute this transparently on a base_type object in the course of every permitted access and thus to trace every modification in an atomic manner.\n\nthe physical representation of the persistent smap_base_type object is, in principle, independent of that of the base_type object. This means that, theoretically, every physical management system (existing file systems, databases, etc.) could be considered for storage purposes.\n\nSmApper implements its own file system (SMAP_FS) are as follows:\n\nSMAP_FS is an optimized file system which will:\n\nSmApperâBasic Functions One of the most important basic requirements of a SmApper system is access to the extended attributes of the smap_base_type (see Section 3.3 entitled âSmApperâBasic Functions). As the SmApper systems have to be capable of being integrated smoothly into existing infrastructures, access to attributes must occur without any kind of proprietary protocol and must be based exclusively on standards.\n\nAccess to a base_type occurs via path commands and via the usual POSIX-API (open, read, llseek etc.). Extended attributes of the smap_base_type are treated like individual files and are therefore also accessible via a (specific) path command as well as via POSIX-API.\n\nthe following example will serve to illustrate this: the title of the original file (an MS Word document)/home/users/gth/hello.doc was extracted and saved in the attribute title in the SMAP_FS. Access to this attribute now occurs via the path command/home/users/gth/hello.doc?//title.\n\nthe delimiter serves only as an example here and can be configured.\n\nthe path command is specific in our example and therefore delivers a SMAP_FS-file handle when an open-request is demanded.\n\nthe usual I/O operations can be carried out using this file handle.\n\na write-syscall will only be successful when the modifications are also reflected in the original document (in our example/home/users/gth/hello.doc)âduring an outbound-operation the write-request will be executed without modification to the original document.\n\nthe modification to the original document which will, of course, not take place until a later date, then fail, the file would be labeled with the corresponding status in the QZone.\n\nRules and actions form SmApper's actual compute-layer, allowing decisions to be made and actions to be taken on the basis of the extended information included in a smap_base_type as opposed to a base_type.\n\nsmap_base_type can be considered operands, or even, on the other hand, constants like Literals, time commands like now, today, among others.\n\nRules constitute SmApper's very simple model of the decision-making body. An example for a rule is:\n\na rule always has access to all smap_base_type objects which are located within its scope. There are three ways of bringing an object into the scope:\n\nImplicit during a file system event, the object this_file is always located implicitly in the scope. This is the file which led to the trigger event of the rule.\n\na new object can be instantiated in the scope by a definite SMAP_FS-Path, for example/smap_mnt/x.doc?uid\n\nrules constitute the authority which decides whether an Action should be executed or not, and, if so, whether Action A or Action B should be executed.\n\nAn Action can be any event from sending an email, the encrypting of data, the moving/copying of files within the storage networks, to access to a SAP system. SmApper even considers the extractors and converters previously introduced as actions in the broadest sense.\n\nSmApper's basic requirements Owing to the diversity of potential actions, one of SmApper's basic requirements is that it must allow external, third-party applications to be accepted as actions. In the same way, SmApper's second and third basic requirements, follow on: it must ensure that the third-party application can in no way compromise the operation of the SmApper appliance. Furthermore, it must be capable of high-performance execution of actions.\n\nSmAp-OS SmApper's own operating system\n\nFreeBSD FreeBSD\n\nSmAp-OS SmApper's own operating system\n\nstandard operating systems offer the concept of processes and threads as lightweight processes\n\nactions exist in SmAp-OS as a third process abstraction layer, which can be thought of as ultra-lightweight-processes.\n\nThis action authority operates in a type of Virtual Machine (VM) within the core of the SmAp-OS.\n\nVM Virtual Machine\n\nthe VM does not simply enable the performance of the actions to be determined, in order to achieve a higher level of security.\n\nthe VM also provides a separate protected address room, which severs standard processes (system programs, etc.) and the kernel from actions. Should an action crash, then, in a worst case scenario, it would only affect itself and other actions but not the rest or the core of the SmApper system.\n\nthe separate address room provides the capacity for more efficient Context-Switching and for quicker process creation (no more memory areas, which have to be copied, etc.)\n\nSmAp-OS now recognizes the concept of action processes in addition to standard processes and real-time processes, a more granulating scheduling is possible, again leading to higher (or better adapted) performance.\n\nSmApper has the capacity to load pre-compiled rules into the kernel, where they can be connected with actions via Mapping Tables. This allows, for instance, an application to be started at any time but only when the rule has been complied with will it be carried outâwithout even causing serious additional cost to the system.\n\na second means of establishing this connection is by calling up the SmApper-specific fork_if( )-syscall (instead of the fork( )-syscalls) which contains the rule-context as a standard parameter.\n\nSmApper permits the working or connection of rules and actions at the following junctures:\n\nRule/Action framework A daemon in the user space which is available as a listener for events and pairs rules and actions up. Events may be file system events or timerbased events.\n\nConditional cloning Carried out in the kernel, it allows a rule-preprocessing before the forking and may either be executed by successful action to rule mapping after a standard-fork( ) or by a dedicated call of a fork_if( )-syscall.\n\nSmApper appliance itself provides partly by means of system implementation (as shown in Section 4) and partly by means of additional applications (actions, rules, etc). This list is not necessarily complete but will indicate some of the possibilities available when using SmApper.\n\nVersioning allows the user to create automatic versions of a file. Essentially, SmApper offers three methods of versioning: complete (each file is a completely new file including its meta data), modifications (only the modified blocks are saved) and meta data (there is only a physical data file which always corresponds to the last information; however the SMAP_FS retains the attribute information of older versions as read-only).\n\nSemantic file access This refers to the query-feature in SMAP_FS. The user is no longer only capable of accessing his files by path but also by queries to the attributes of the smap_base_type objects.\n\nContext sensitive security All the attributes of a smap_base_type object may have different security levels. This means that, for example, a user can see the title of a certain document but may not read the contents.\n\nHidden files/parts of files Depending on context-sensitive security, it is also possible to make files, parts of files or even whole directory trees invisible to certain users or user groups. This would give executives, for instance, much higher security levels when storing sensitive information.\n\nImplicit copies SMAP_FS enables n copies of a file to be created and maintained easily, even in different destinations or file systems.\n\nn converters can be defined per scope. This means, for instance, that an incoming TIFF file can be converted automatically into a JPEG, or a thumbnail and a low-resolution preview can be created. When all these new, converted files are added to the original smap_base_type using attachâ, SmApper automatically reflects every modification to the original file in the converted extracts. Further examples of automatic converters include compression algorithms (ZIP etc.) and encryption algorithms.\n\nSMAP_FS The (rule-based) triggering function in SMAP_FS allows every user and/or program to be notified automatically by alarm, message, text-message, email and so on regarding any form of file access. This may be relevant for security reasons but may also be an advantage as a workflow feature or serve to relieve the system administrators.\n\nSmApper allows almost unlimited statistics to be recorded via File I/O. Using this tool, it would not only be conceivable to measure when and how often a particular file was opened or modified but also which parts of it were affected. Moreover, it would be possible to keep track of accessing clients in order, for instance, to acknowledge a storage location which does not correspond to user patterns and therefore seems disadvantageous. Also analysis could be made which would permit an evaluation of data to be performed under the heading âWhat does it contribute to the net product of the company?â.\n\nreplication means that SmApper enables rule-based replications to be carried out at file as well as block level.\n\na useful replication would mean for example that a file is replicated automatically in a storage location which is more in keeping with user patterns, in order to increase performance (see Statistics).\n\nVirtual directories Using SMAP_FS, files which are physically located in completely separate tree structures or even different file systems can be logically displayed as though they are in one directory. To give a practical example, these could be directories for project groups or virtual company teams.\n\nSMAP_FS safeguards the integrity of all attributes of a smap_base_type object, from system-specific attributes to user-defined attributes. This allows a file to be given additional information, whose life cycle is equally linked to the file as its contents.\n\nAudit trail Using the versioning feature, it is possible to show who modified what and when, at the binary data level as well as at attribute level.\n\nSMAP_FS allows not only rigid user/groups entitlements to be assigned but also rule-based access rights.\n\nOne example of this is that a particular file may only be read and modified by User Y on Day X. Only after 10 p.m. are all users permitted to read the document.\n\nShared task automation Shared tasks include the printer, fax, tape drives, CD writers, archives, microfilm areas, etc. The sending of data to these devices can be managed under rule-based conditions which is equivalent to an intelligent, adaptable spooler.\n\nMultilingual feature Documents or parts of documents can be translated automatically and, using the âSeveral views per fileâ feature, can even be opened in the appropriate language, based, for instance, on the Client-IP address.\n\nScheduled tasks allow all the above-mentioned features to be carried out at any pre-defined point in time and not only âOn demand,â that is, when File I/O has taken place.\n\nSmApper is an implicit storage virtualizer, meaning that n storage devices can be concealed behind it. However, these devices can be perceived in a different form, as m devices, by the user. Storage devices can be combined in a rule-based fashion or may be connected statically.\n\nFeature packages mean an interaction of features as presented in the previous section.\n\neach module contains additional tools and topics, which are only implemented within the context of the module (e.g. configuration clients, administrative clients, etc.).\n\nthe individual modules are as follows:\n\nthe purpose of the module Information Lifecycle Management is to enable several physical storage systems (file servers, local drives, (i)SANs) to be combined into logical units and to be presented to the user as such, namely as ânewâ storage resources. Moreover, it should facilitate a decision based on rules regarding the location at which each file is to be stored. Furthermore, it will allow the system to review even in retrospect whether file X, which was stored at time y in location z, should still be stored there at a pre-defined point in time or whether fundamental parameters have been modified, demanding a new decision.\n\nThis module hereby allows the user to employ his storage infrastructure in the most efficient and economical manner.\n\nSmApper introduces its own Device-Description-Language which allows infrastructure elements managed or addressed by SmApper (hard drives, printers, facsimile machines, CD writers, file servers, etc.) to be defined, this definition to be deposited in SMAP_FS where it is re-used as an object for ILM decisions.\n\nSmApper In its standard form, SmApper only skirts the subject of security (that is, without the security module) and only then in as much as the security mechanisms of the fundamental storage infrastructures are used, their results being binding for SmApper.\n\nthe security module provides SmApper with a more thorough, more finely granulated data security mechanism. On the one hand, this means that in this case SmApper has to understand external security mechanisms (particularly Active Directories and NIS/NIS+).\n\nmost of the features discussed in the previous section context sensitive security, hidden files/parts of files, alerts, conversions, etc. allow a range of combinations of additional security features, which is difficult to be achieved in this degree of automation without SmApper.\n\nthe goal of data management is to simplify to a large extent the actual management of unstructured data via automation using the aforementioned feature packages.\n\nthe purpose of the module âWorkflowâ is to describe the digital lifecycle of a file, the relevant conditions, events and rules and automate it as well as possible.\n\nThis module is specifically designed to replace so-called âPolling Daemonsâ (which track directories according to input and then take certain actions) but it is also designed to replace existing spooling systems (for printers, file servers, burning processes, etc.).\n\na further use for this module is to permit a connection to a groupware environment.\n\nthe first method of approach is based for the most part on the concept of the so-called Semantic File Systems written by Gifford et al. [11].\n\nthe Semantic File System allows data to be extracted via freely defined programs by means of so-called transducers, then to be saved as Key Value Pairs and finally to be recalled using the query concept of the virtual directories.\n\nGifford's approach enables an indexed meta data structure to be set up parallel to the original file system.\n\nthe primary differences between the Semantic File System as opposed to SmApper are as follows:\n\nSedar presents a further, interesting alternative in the form of a new file system as a storage location for meta data and data by introducing the concept of semantic vectors.\n\nthe aim here is to optimize the storage requirement of similar blocks/files using semantic hashing. This approach appears to be very interesting for future reference even though, at the time of publication, it seemed to have a long way to go before the implementation is realizable. The same is true of Gifford et al. as opposed to SmApper.\n\na further related concept to the SmApper paradigm is that of the semantic web.\n\nthe background of the semantic web concept is best explained in the following quotation from the article âThe Semantic Webâ in the Scientific American: â . . .\n\nthe Semantic Web is an extension of the current web in which information is given well-defined meaning, better enabling computers and people to work in cooperation . . . .â [8].\n\nthe Semantic Web is based on the Resource Description Framework (RDF), which integrates a variety of applications, in particular XML.\n\nRDF Resource Description Framework\n\nthe authors analyze the advantages and disadvantages of using XML or XML/RDF as a description of the smap_base_type attributes but this has no fundamental bearing on the whole concept.\n\nthe Semantic Web approach is not a rival concept but could instead be viewed as synergetic to SmApper (see also [16]).\n\nStorage Grid will be able to aggregate physical storage devices in a logical way, packaging them accordingly in front of the userâthe whole procedure independent of protocols, technology and even physical locations. This concept could even make classical storage virtualization solutions obsolete. At present, however, only one manufacturer seems capable of realizing this concept, namely Network Appliance, and even then it is merely a concept which will be realizable solely by using the equipment of that one manufacturer, though this could of course change in time. From the SmApper viewpoint, Storage Grid is an additive concept as storage virtualization is not merely one of the core features of SmApper but in fact imperative for SmApper to be able to implement its features. On the contrary, SmApper allows to unleash the real power of a grid.\n\nSmApper The uniqueness or innovation of SmApper can be considered from two sides:\n\nFIG. 7 will help to demonstrate the innovative nature of the concept.\n\nSmApper bypasses all layers from the physical representation to information as the only meta data solution.\n\nSmApper does not simply focus on one of the two lower layers (physical data/logical data) but also helps to bridge the gap between logical data and information as such.\n\nSmApper achieves this by systemically integrating its new data types (smap_base_types) by means of rules and actions that although syntactically and semantically defined, can be freely selected. This is the missing factor, which we fail to find at all in any of the approaches discussed here.\n\nSmApper can be defined as a modified, enhanced semantic-file-system approach, which has been extended by object-oriented data type integrity, access methodology and persistence on the basis of stacking, whereby the atomically guaranteed correlation between data and meta data appears innovative.\n\nSmApper lays down a rule and action model in order to be able to carry out decisions and actions with these datatypes in a well-defined framework. It is also a completely new idea to integrate these technological approaches in their entirety in a Blackbox-Principle (appliance) in order to guarantee the end user maximum simplicity and the ability to retain the existing infrastructure.\n\nSmApper is streamlined for performance by its design and its implementation. Every relevant, I/O-specific part is carried out in the kernel of the selected operating system. Even parsing in the SMAP_FS can be executed in the kernel.\n\nFIG. 9 SmApper combines logical and physical data access and allows inbound computing during the access process.\n\nthe invention can be implemented in hardware or software or both.\n\nthe designing, carrying out and testing alone of test and benchmark scenarios in order to identify key performance criteria, whether for small or large-scale enterprise operations, is highly complex.\n\nthe hardware should be modulated according to these results.\n\nSmApper is developing its prototypes on an INTEL SR2300, a 2U-OEM-Server with a E7501-Motherboard, two Xeon processors and 2 GB of memory. Further tests are required to determine whether a concept based on serverblades would be more adaptive to scaling performance levels in the long-term.\n\nFIG. 9 represents graphically SmApper's fundamental features once again as a tool to monitor and control unstructured or semi-structured digital packs of data.\n\nLandscapes\n\nEngineering & Computer Science (AREA)\n\nTheoretical Computer Science (AREA)\n\nData Mining & Analysis (AREA)\n\nDatabases & Information Systems (AREA)\n\nPhysics & Mathematics (AREA)\n\nGeneral Engineering & Computer Science (AREA)\n\nGeneral Physics & Mathematics (AREA)\n\nInformation Retrieval, Db Structures And Fs Structures Therefor (AREA)\n\nAbstract\n\nThe present invention concerns an appliance, a process and a computer program product for the processing of unstructured or semi-structured digital data in a file system. In order to create an appliance, a process and a computer program product which allow simple, reliable, high-performance and purpose oriented management of every manner of digital, stored, unstructured data, it is proposed that, when accessing data, logical access be carried out jointly with physical access and, when doing so, a particularly transparent, common access mechanism be implemented for both types of access.\n\nDescription\n\nFIELD OF THE INVENTION\n\nThe present invention concerns a process or a method and an appliance or an apparatus for data processing as well as a corresponding computer program product.\n\nBACKGROUND OF THE INVENTION\n\nIn the age of the information society, it is no longer the creation, processing and distribution of energy but of information which determines the extent of production leading to economic growth; the information factor has become the main resource. Information forms the basis for decisions and human co-operation. At the same time, however, completely new and separate criteria regarding the quality, cost and use of such information are being applied.\n\nAny form of general data which can be stored falls under the heading of information, that is, language, sound and image data in addition to text and numbers in their respective digital data format and storage forms. Thus, the quantity of available data which may also need to be processed in some way is steadily increasing both in a global sense and for each individual user. Whilst increasing CPU power and new architectures render the creation, processing and transport of an ever-increasing volume of data manageable within a reasonable time frame, the long-term, safe administration of digitally-stored data presents a growing problem despite the fact that sufficiently expanded storage space is available. At the same time, it must be possible to permanently ensure that the information contained in the respective digital data packs can be accessed directly by the user at any time and at short notice as and when required.\n\nAs a general rule, however, the digital storage of data separates it from its source, its type and its purpose. Today, classification is regularly carried out according to the file names and their extensions with the result that intelligence is still on the side of the application programs when interacting with digital data. These classification specifications are supplemented to a large extent by non-standardized version numbers, dates or other particulars designed to allow the locating and appropriate use of the data.\n\nThe problem associated with this can be demonstrated quite simply by means of the self-explanatory example of an old hard disk storage: the data which is stored securely and in an organized fashion in the hard disk stems from programs which, in general, are themselves not contained on the storage device. Now, the successors of the programs which originally created the data, must try to recover the information content of the data by using filters and conversion routines. Every user knows from previous bitter experience that programs have very limited upward and downward compatibility features.\n\nThus it is the task of the present invention to create a process, an appliance and a computer program product for data processing which allow simple, reliable, high-performance and purpose oriented management of every manner of digitally stored, unstructured data. An appliance or apparatus, according to the present invention, must be capable of being integrated as hardware into all current personal computer and/or data processing environments without basic adjustments having to be made.\n\nSUMMARY OF THE INVENTION\n\nA method of processing unstructured or semi-structured digital data in a file-based system is characterized by being able to abolish the existing, prior art separation of logical and physical access to data. When data is accessed, therefore, logical access, i.e. with user-defined criteria, is carried out jointly with physical access, i.e. using the file path. In so doing, a common access mechanism is implemented for both types of access which is particularly constructed so as to remain transparent or, in other words, unperceived by the user.\n\nPreferably, a file path is processed within the execution of the access mechanism which has been enhanced by a Query-Interface. In a further development of the present invention, the Query-Interface used in the extended file path constitutes an enhancement of a POSIX- or similar standard in the form of an XQuery-Standard or similar standard.\n\nIn a basic embodiment of the invention, arbitrarily pre-definable data subsets are extracted when accessing unstructured and/or proprietary structured data. These extracted data subsets are preferably stored as meta data in a structured form. Thereby intrinsic data subsets, i.e. extracted from the data itself, and/or extrinsic data i.e. derived from outside the data, is used advantageously to create the respective meta data.\n\nBy use of a process or a method according to the present invention in an embodiment, meta data is created out of arbitrarily pre-definable data subsets, namely on reading and/or writing or, as the case may be, on storing unstructured and/or proprietary structured data. Thus, any form of access to data is used in order to generate corresponding meta data.\n\nThe process is carried out advantageously while preserving the atomicity of the sum of all partial transactions regarding all data which is linked to the respective source data and/or files. In this way, all meta data, which has been derived from inside or outside the data, suffer the same fate as the data itself. Consequently, when deleting the original data, it goes without saying that all logically connected data which was derived from the deleted data by means of a process according to the present invention, is likewise deleted.\n\nIn an important, further development of the invention, data is subject to a pre-defined and customizable-rule and action model. In particular, based on the results of the processing of a pre-defined and customizable rule and action model, well-defined decisions and/or actions are carried out. The user is thus given the chance to actively influence the type and choice of rules and actions, for example by modifying the configuration.\n\nAccording to a particularly advantageous embodiment of the present invention, part-programs or actions of the rule and action model are carried out in the kernel of the operating system, the execution being bound to rules and conditions. The aforementioned partial stages are executed automatically in a further development of the present invention.\n\nAccording to a further development of the present invention, a process under the present invention is carried out particularly advantageously utilizing standardized software and hardware interfaces. It is hereby executed as an individual unit without interference in or modification to an existing structure, in such a way that mutual interaction can be avoided should retrofitting occur in an existing system. Accordingly, an appliance or apparatus which implements a process under the present invention is characterized by the fact that resources are assigned to connect the appliance to the standardized software and hardware interfaces of the respective data processing installation or the respective system network. A suitable appliance can therefore be integrated as a closed unit into a data processing installation without interference in or modification to an existing structure of the same data processing installation.\n\nIn an important further development of the invention, the meta data is set up in its own file system on the basis of the common access mechanism. The file system is optimized for the rapid lookup of data content and/or attributes of data content. In this way, this file system is characterized particularly by allowing a bi-directional, atomic interrelation between data and meta data. This means that, by the same token, modification of the data causes a consistent modification of the affected meta data and vice versa. This allows data and its meta data to be processed independently of one another, thus permitting varying views of the original data stream with respect to format, partial-format, etc.; however, every modification in one view leads to a mandatory modification in all other views. Thus, it makes no difference whether at least one modification is made to the original data stream and/or one of the attributes as a component of the associated meta data, as any modification is likewise reproduced in the other associated part.\n\nTherefore, an appliance in accordance with one embodiment of the present invention involves a method of encompassing all levels of the unstructured data, from its physical representation through logical classification to its information content, the information content being edited and adjusted to fall within a well-defined framework of actions and/or decisions.\n\nA process in accordance with the present invention is advantageously embodied in a computer program product, which means, in particular, in any form of data carrier, for example a CD-ROM. Thus, once imported into the main memory of a data processing installation, this computer program product causes the execution of a process according to one or several of the afore-mentioned criteria.\n\nBRIEF DESCRIPTION OF THE DRAWINGS\n\nFurther advantages and embodiments according to the present invention as well as a corresponding appliance or apparatus, can be described with reference to an implementation example in greater detail by means of the following diagrams:\n\nFIG. 1: a systematic illustration of contemplatable solution areas.\n\nFIG. 2: an illustration of primary methods to answer the question âWhat does understanding data contents meanâ: extractors and converters, extractors being a special form of converters in this case.\n\nFIG. 3: a basic functionality which forms the basis of a process in accordance with the present invention which is named âSmApper.â\n\nFIG. 4: a chart to illustrate the requirement that SmApper must be integrated transparently as an appliance between Storage-Client and Storage-Server.\n\nFIG. 5: a chart as an illustration of stacking as a method which allows the (strictly-speaking) one-dimensional VFS-process to be extended to several dimensions.\n\nFIG. 6: a chart to illustrate how SmApper uses the stacking procedure.\n\nFIG. 7: a diagrammatic representation of how SmApper, as the only meta data solution, spans all layers from the physical representation to the information.\n\nFIGS. 8 and 9: representations of SmApper's fundamental features as a tool to monitor and control unstructured or semi-structured digital packs of data.\n\nDETAILED DESCRIPTION OF THE PREFERRED EMBODIMENTS\n\nThe following will serve as a systematic examination of the chosen approach to the management of unstructured data by means of structured meta data:\n\n1 The Problem\n\n1.1 Starting Point\n\nThe resource information has become a decisive factor for production in the age of the information society. According to the study âData Powers of Tenâ [1] we produce new information with a capacity of one to two exabyte per year. This equals about 1,000,000,000,000,000,000 letters, or, in other words, almost all the words that have ever been spoken.\n\nInformation is the basis for decision processes and human cooperation, which is one of the main reasons for the importance of digital information as a production factor. This information, however, is completely subject to personal criteria concerning quality, cost and benefit. Today's information and communication (IaC) technologies make information almost universally available without losing any of its individualization, depth or interactivity.\n\nIf you know how to use this resource, information, and above all digital information, may be the most important asset of a company. Modern IaC systems make this possible.\n\nCurrent IaC systems basically comprise three components: data processing, data transmission and data storage according to Gartner, IDC and Forrester information technology (IT) departments already spend more than 50 percent of their hardware investments on data storage systems.\n\nData storage systems have been optimized to store data and make it available. From a technical point of view the nature of data is insignificant. Radiographs, family pictures, emails, letters of financial data are all treated the same way. Intelligent handling of digital data today is still based on the application, i.e. the many specialized programs and software such as SAP, Microsoft Word, Adobe Photoshop, etc.\n\nThe majority of today's digital information is rich media data, with content such as pictures, video, sound, graphics or other non-text based information. It is only meta data that makes them available for processing and commercial use. Examples of such meta data is contract and legal information, serial numbers, forms or comments that are needed for administration, easy location of the data and its appropriate usage.\n\nAt present the administration and usage of the relevant meta data and the original data are completely isolated from each other. There is no consistent standard to regulate how meta data and data can be stored and administered together. Meta data is stored in the same way as the original data as the storage infrastructure does not recognize any difference. However, meta data is usually more important for the cooperation than the original data.\n\nThus it is almost impossible to administer, let alone find, unstructured data that cannot be saved into a database, e.g. addresses.\n\nVarious solutions to deal with this problem do exist, but they either deal with a restricted type of data, are proprietary and expensive or optimized for a very specific use. In most cases there is simply no all-encompassing solution available today.\n\n1.2 Solution AreasâThe System\n\nThe simple and purpose oriented management of digital data is one of the biggest challenges currently faced. To solve this problem you have to examine the specific interests and needs of each of the following groups:\n\nUsers\n\nBusiness management\n\nIT specialists/systems\n\nIT industry\n\nThe user's point of view:\n\nSimple, fast, directâusers want to find and read the information that is relevant to them without paying too much attention to the details of the technical solution. They don't want to be overwhelmed by an endless flow of information, but they want exactly the data they need for processing and that is relevant to their specific work area. If you have no CAD software installed you have no use for an Autocad file. Furthermore, data must be up-to-date. We all know the problem faced when trying to retrieve a word document that has been saved under various names (abcâ1.doc, abcâ2.doc 2_abc.doc etc.) but without any indication of the latest version.\n\nThe business point of view:\n\nThe core issue concerning digital cooperation for a company is: how do we make sure that the right data of the right quantity and quality are in the right place at the right time? Data has to be transferred between a company's organizational units based on business related rules. This process specific approach has to be independent of the underlying IT infrastructure (and especially the storage infrastructure).\n\nThe IT point of view:\n\nThe âInformation Lifecycle Management (ILM)â describes the main requirements of IT systems. Data has to be made available according to its functional use and relative importance. It is essential to understand the workflow between single departments and units concerning data exchange and the quality requirements for data storage (availability, speed of access, quality data such as image resolution, etc.). Also, all these requirements should be reconciled with the total cost of ownership (TCO) of data management (i.e., what costs incur to provide data of the category x).\n\nFor example: A company has to store financial data for several years due to legal requirements. However, you do not expect that every single subsidiary needs high speed access to this data at any given time. Storing this data on tapes, CD-ROMS and the like is a totally adequate method of archiving it.\n\nA new way of object and data oriented data management can only be successful if such tools or systems can be smoothly integrated into the existing infrastructure.\n\nThe IT industry's point of view:\n\nToday the success of new products or new technologies are based on the coordination with big software producers or independent software vendors (ISVs), such as SAP, Oracle, etc., and system integrators, Accenture, CGEY, Bearing Point, etc., who recommend the appropriate IT infrastructure needed to solve business problems. Intelligent data management can be detached from the application itself thus resulting in leaner applications with a better cost-effective development process. Data management usually is no longer the core competence of ISVs, so new features based on this might now be realized while they had to be cut before due to the high costs. From the system integrator's point of view rule based data management especially with regard to the Information Lifecycle Management can offer big potentials for professional services. In such a data management scenario system integrators also attach great importance to the idea of infrastructure consolidation concepts and an improved projection of business processes on IT processes.\n\nThe solution system can be summarized in the diagram of FIG. 1.\n\nIf you look at how these requirements are met today you will find an overlapping of various markets and solution approaches. There are different solutions from the point of view of manufacturers of infrastructure components (above all data storage systems, operation systems and file systems, databases) and manufacturers of applications and user software (Content Management Systems (CMS), file management systems (FMS), Information Lifecycle Management Systems (ILM) or Backup/Recovery Tools and Workflow and Collaboration Systems).\n\nThe diagram of FIG. 1 describes the overlapping of the different solution approaches.\n\n2. The Solution\n\n2.1 Brief Definition of the Solution\n\nIn order to create a system that integrates all approaches mentioned above and makes them compliant with the heterogeneous requirements, we assume that in principle the following solution is needed:\n\nUbiquitous data access must be possible.\n\nThe system must be able to understand the contents of the data and manage it accordingly; it must be possible to create meta data.\n\nRules must be set to manage data on the basis of business processes.\n\nThe solution must fit perfectly into the existing infrastructure, it must be scalable and expandable.\n\nThe system shall allow data management of the next generation, namely at the location where the data are stored. Thus the solution must represent a transparent expansion of the storage infrastructure and not be just another business application, e.g. Enterprise Content Management Systems.\n\nThe key component of the solution is a layer that allows business rules to be defined and to directly and easily map not only data and meta data, but also their management, storage location, life cycle and flow.\n\n2.2 Detailed Requirements\n\nIn order to fulfill all the requirements for digital data management discussed here, the following basic solution requirements (afterwards also called system) must be reconciled irrespective of the manner of implementation:\n\nAdministration of Data and Meta Data\n\nThe system is designed for unstructured data, that is, for the administration of files (and not for databases, records and so on)\n\nData and its meta data must be treated as a single unit\n\nIt must be possible to separate the access, administration and modification of data and meta data\n\nEach modification of the data must be reflected in the meta data and vice versa where feasible and appropriate\n\nIt must be possible to create meta data automatically from the source data\n\nIt must be possible to create meta data manually (by interaction with the user)\n\nIt must be possible to define which meta data should be created from the source data\n\nThe system must be able to âlearnâ new datatypes at any time\n\nIt must be possible to integrate external datatype-modules from other datatype specialists into the system (in compliance with pre-determined syntax and semantics) without compromising the quality of the whole system\n\nThe system must allow datatype conversion and abstraction\n\nIt must be possible to retrieve meta data, or a definable excerpt from the meta data, via a âQuery-Languageâ\n\nMeta data, or a definable excerpt from the meta data, must be capable of being exported automatically into non-system environments (like billing applications, SAP-Systems, etc.)\n\nIt must be possible to provide several versions of the same dataâeach version clearly distinguishable from anotherâand to be able to assign accurately the relative modifications of this data and meta data, with respect to content, origin and time.\n\nSmooth Integration into Existing Environments\n\nIt must be possible to store data in the usual fashion without mandatory modifications to the client and/or server\n\nThe system must not impair existing security standards\n\nThe system must be scalable in such a way that no existing Service Level Agreements (SLAs) are lost or forfeit\n\nIt must be possible to continue to use existing data storage systems, networks and other infrastructure components\n\nIt must be possible to integrate new technologies, in theory at least, particularly with regard to storage aspects\n\nAccess to data and meta data must be possible regardless of location within the framework of the given infrastructure\n\nVirtualization\n\nRules must be able to describe which data should be stored physically at which location and how often\n\nThis physical storage location must be allowed to change even during the life cycle of the data, contingent upon definable rules\n\nThe physical storage location must remain discernable for access\n\n3. Solution Design\n\n3.1 Concept of the Base Types\n\nOne aspect of the invention, herein referred to as âSmApper,â focuses on file-based data. More particularly, the invention may be used in a data processing system of managing unstructured or semi-structured digital data in a file system supported by a computer, the computer having a memory. At this point, the construction base_type is introduced as a simpler abstraction of the term file. A base_type is most easily comprehended by borrowing from the object-oriented design approach. According to this model, a base_type is a class with well-defined properties (designated as attributes in the following sections) and methods. A base_type is nothing more than the logical encapsulation of any file (in theory).\n\nThus, a base_type has as its primary attribute the binary representation of the data contained in the respective file. Further attributes are, for example, date fields, which indicate when the data was last accessed or modified and so on. The methods provided by a base_type include, in particular, the capability to access this binary data, to modify it and render the respective condition of the data persistent (in the file). A base_type is a logical construction, which is not made persistent in itself but is merely a medium of describing a physical file and the methods which can be applied to it. At this point it should be noted that the distinction between a file, which is itself only a logical construction of a file system (in order to classify the actual physical blocks on the respective secondary storage system), and the actual physical data characteristics (of the blocks) has been waived in the following sections.\n\nA base_type and its methods and properties depend, therefore, on the respective file to which this construction is applied but also, of course, on the capabilities of the fundamental file system. The actual instantiation of a base_type results in an object with an allocated file. The following will serve as an illustration of the base_type using C++ class (which is however not fully implemented):\n\npublic class base_type {\n\npublic:\n\n// con/destruction base_type(const char * filename); Ëbase_type( ); //methods ssize_t read(...) ssize_t write(...) ssize_t lseek(...) etc.\n\nprivate:\n\n// pointer to opaque data stream void *m_data; // where is my physical file const char *m_path; // Filedescriptor int fd;\n\n}\n\nOne of the basic requirements of the system is that it considers data and meta data as a single unit. For this reason, a new data type is introduced on the basis of the base_type known as the smap_base_type. The smap_base_type is an extension of any base_type and can be best described using the term inheritance. A smap_base_type is derived from a base_type and then adds extra methods and attributes. Thus a new, autonomous, encapsulated data type is created, which represents the foundation for all further discussion in the following sections. Each SmapType has a number of attributes <0, n>. For example âpagesâ which could be the number of pages in an MS-Word document.\n\nAttributes may have base_type-intrinsic values; abstracted from the base_type or extrinsic; freely-defined values. Every attribute has an explicit qualifier or unique identifier (UID) and is classified by a data type. This could be either simple data types (like int, char, etc.) or complex data types (like string, smap_base_type, etc.). Each attribute possesses a value that corresponds to the data type as well as additional parameters which describe further properties of the attribute. One example of the use of such a parameter is scope=system, which indicates that the attribute is a system attribute that may be read only and not modified by the user. Moreover, attributes can be constructed hierarchically (e.g. there could be a subtitle in a document which forms a child-relationship to a title-attribute).\n\nA smap_base_type offers methods for reading, setting, numbering or iterating values.\n\n3.2 Extractors and Converters\n\nAs one of its core requirements, SmApper needs to be able to understand data in form and content in order to allow customizable decisions on the basis of this information. What does it mean to understand data in form and content? Well this will vary from one case to another. In one application context âcomprehensionâ may simply entail extracting the number of pages of a Word document from its binary representation. In another context it may be necessary to extract the titles of the individual chapters.\n\nIn a more general sense, data comprehension can be defined as follows:\n\n1. Two methods are applied to the binary stream:\n\ndata is extracted\n\noptional: specific function is applied to the extracted data (=convert)\n\n2. The new data set thus created must conform to a well-known data type to which well-defined operations can be applied.\n\n3. This data set must be associated with a context.\n\nFIG. 2 shows a diagrammatic representation of both methods: the Extractor and the Converter. As demonstrated in the diagram, an extractor is a set of extract patterns which determine how much of which data is to be extracted to which location within a binary stream. A converter, on the other hand, extracts data and then applies a function on it. On closer examination of this diagram we see that an extractor is a special form of a converter, and is in fact a converter with a null-function per pattern. Thus, extractors are a special form of converters.\n\nWith the assistance of the base types constructions and the above-mentioned converters and extractors, we are now capable of examining in greater detail the basic functions that SmApper offers in the next section.\n\n3.3. SmApperâBasic Functions\n\nFIG. 3 demonstrates the basic functions that SmApper provides. These basics, which will be examined in depth in the following sections, form the SmApper core system, with the aid of which the actual modules (or applications) can then be developed. The main tasks of the SmApper System are as follows:\n\n1. To generate a smap_base_type out of a base_type by means of converters and extractors.\n\n2. Access to the smap_base_type (the actual file and the attributes)\n\n3. Additional functions on the basis of smap_base_types (rules, actions)\n\nWhen extractors and converters are applied, the data subsets generated are assigned to attributes of the smap_base_types and hence are brought into the correct (that is to say definable) context. The manner in which the smap_base_type manages its attributes guarantees the data integrity of the individual attributes. Or, to put this a different way, this means that SmApper appends structured data to unstructured data.\n\nAccess to the attributes of a smap_base_type must be possible by direct means and must, in addition, permit a Query-Interface in order to locate attribute contents.\n\nRules enable the forming of Boolean Expressions on these attributes by means of attributes and permitted operators which show âTrueâ or âFalseâ as a result. Rules access solely the structured information of the smap_base_type thereby offering the possibility to reach a decision based on the data. According to FIG. 3, rules run inbound as well as outbound. Inbound means that the affected system component runs in the kernel space of the SmApper (basic) operating system while outbound means that the scope of the code segment is user space. Please see Section 4.1 for further information.\n\nIn turn, actions enable programs to be executed on the basis of events and conditions (rules), in order to initiate corresponding operations.\n\nTogether, rules and actions form the crucial unit enabling decisions to be reached and actions to be carried out on the basis of available data. The fundamental lemma, on which SmApper is based and which, in addition, permits a distinction to other implementations of related problems, reads as follows:\n\nSmApper guarantees the complete integrity of the smap_base_type. As soon as any modification to the base_type is made, SmApper displays this automatically for the user and/or the application program atomically in the smap_base_type. In the same way, any (permitted!) modifications to the smap_base_type or its attributes are automatically as well as atomically displayed in the base_type.\n\nNetwork File I/O and Appliance\n\nIt is one of SmApper's basic requirements (see Section 2.1) that it must be able to integrate itself smoothly into existing infrastructures. Moreover, SmApper restricts itself to unstructured data, meaning file data. In addition, it must be possible to access the data from any point in the network at any time. These requirements make it absolutely essential to apply one of the basic requirements to the implementation as follows (particularly while taking the detailed requirements into account, see Section 2.2):\n\nSmApper focuses on the Network File I/O\n\nSmApper must be integrated smoothly into the Network File I/O communication (CIFS, NFS, DAFS, WebDav)\n\nThis is only possible without modifying the Client/Server and Storage Infrastructure by installing a Black Box (appliance) that is integrated âinvisiblyâ into the data traffic between Storage-Client and Storage-Server\n\nThe diagram of FIG. 4 shows these basic requirements of SmApper\n\n4. SmApperâthe Implementation\n\nSmApper must be able to handle every Network File I/O protocol for Storage-Clients and for Storage-Servers even every storage protocol (file and block) must be handled. In addition, SmApper must have the ability to switch into the communication between Storage-Client and Storage-Server, in order to implement its additional functions smoothly. The only technical alternative which permits such a procedure without re-inventing the wheel each time and without having to integrate itself into every imaginable protocol stack, is known as stacking [2,3,5].\n\n4.1. Stacking and VFS\n\nBefore we can explain the meaning of the term stacking, it is necessary to define the meaning of VFS. VFS stands for Virtual File System and stands for a layer, which has become a standard part of modem operating systems and which enables the homogenization of access to heterogeneous physical file system implementations. VFS is a term from the Linux kernel which may be known by a different name in other operating systems and which, by its nature, is implemented differently, for example the VNODE-layer under SOLARIS; however, the purpose of this layer is always the same. When we talk about VFS in the following paragraphs, we mean the underlying concept and not the Linux-specific implementation.\n\nA modern operating system must support a wide array of different file systems: local file systems like NTFS, UFS, XFS, ReiserFS, VxFS, ext2/3, FAT, CD-ROM file systems, to name but a few. In addition, there are network file systems like NFS, CIFS, DAFS, coda and others.\n\nIn order that an application does not have to control the different implementations of the individual file systems, the operating system core (kernel) abstracts the underlying physical implementations with the help of the VFS-Layer and compels the physical FS-implementations to abide by a set of pre-defined functions, which may be optionally implemented to some degree. The VFS-Layer then ensures that each implementation of the necessary function(s) of the physical file system is retrieved when accessed [6, 7, 2]. Although the individual kernel implementations were not developed with the help of object-oriented language tools, on closer examination this concept is about Function Overloading which can be easily demonstrated therefore by virtual functions. Thus, the VFS-Layer makes a set of virtual functions available, which (can) then be overwritten by the real implementations.\n\nStacking constitutes a process that avails itself of the VFS concept intensively and, in doing so, extends the process. A conventional VFS implementation primarily allows for a VFS-Layer that can retrieve N file systems. Stacking, however, facilitates the retraction of the M VFS-layers as a matter of principle, in which the VFS-layer at position M retrieves the VFS-layer at position M-1 and so on until the actual physical implementation of the underlying file system(s) is retrieved [4].\n\nFIG. 5 illustrates this process, showing that stacking is a method which allows the expansion of the primarily one-dimensional VFS process into a multi-dimensional one [4].\n\nA tangible alternative to the stacking concept is the one that SmApper applies in order to control the problem of smooth integration in the communication paths between user-defined Storage-Clients and Storage-Servers. As FIG. 6 shows, SmApper applies the stacking process in order to provide the user/application program with a virtual file system (which the user perceives as an actual physical file system). This virtual file system masks two (in principle n) actual physical file systems, namely Phys. FS A which, in our illustration, constitutes the actual path and storage-server the user wishes to access. Phys. FS B of FIG. 6 denotes the so-called QZone (see Section 4.2 entitled QZone and Caching) of a SMAP_FS (see Section 4.3 entitled SMAP_FS) where the smap_base_type for every relevant file retrieved by Phys. FS A is represented in terms of functionality, as demonstrated in Section 3.3.\n\n4.2 QZone and Caching\n\nOne of the essential basic functions of SmApper is the ability to generate data subsets out of the original data stream with the help of the illustrated extractors and make them persistent as smap_base_type-attributes using the SMAP_FS. SmApper makes it possible to execute the extraction completely inbound (that is, while the data stream is being generated or modified and so on) or outbound. The latter is particularly important as there are certain extraction procedures which require too much time to be executed inbound. In this case, or if specified by the user, the data extraction must be effected once the I/O operation has been completed, i.e. in an asynchronous manner.\n\nAccording to FIG. 6 SmApper applies the stacking process in order to combine all user-defined Phys. FS As with all Phys. FS Bs (QZone of a SMAP_FS) thus guaranteeing the persistent connection between a base_type and a smap_base_type.\n\nAs the extracted data could lead, in connection with rules and actions (see the section on rules and actions), among other things, to the physical storage location, the mode of storage of the original data, the security attributes, etc. being modified, the original file must be buffered in the meantime. SmApper provides the so-called QZone (quarantine zone) for this purpose; this constitutes a physical location which meets all requirements (availability, etc.) and offers, preferably, a high-performance file system.\n\nThe QZone is not only essential in order to permit outbound-smapping but offers further advantages, as it can be regarded as a caching-entity. To wit, SmApper has its own QZone-daemon which determines the specific time that the actual physical displacement of the buffered data to its designated destination (target-destination, as defined by the user at the original I/O) should take place. The parameters for this decision can be as diversified as with any other I/O operation on a SmApper system. Moreover, it is of course possible to displace the data to any other physical location, as the SMAP_FS can restore the, connection to the original path at any time. An example of such a purposely delayed displacement out of the QZone would arise if the QZone were accommodated on a Nearline-Storage-System where files could remain until a proportionately high frequency of access requests would make a displacement/copying to one or more other locations expedient. Ideally, such a situation would arise within a concept like the storage grid from Network Appliance, leading to a simplified Information Lifecycle Management approach, as the preliminary storing entities are charged as caching-entities in the Nearline-Storage of the above example.\n\n4.3 SMAP_FS\n\nSmApper has to make the attributes of the instantiated smap_base_type object persistent and carry out the procedure as efficiently as possible. Stacking allows us to execute this transparently on a base_type object in the course of every permitted access and thus to trace every modification in an atomic manner. The physical representation of the persistent smap_base_type object is, in principle, independent of that of the base_type object. This means that, theoretically, every physical management system (existing file systems, databases, etc.) could be considered for storage purposes.\n\nThe reasons why SmApper prefers a file system to a database are as follows:\n\nThe Stacking-Layer must be located in the kernel of the selected Appliance-Operating-System. Access to the selected storage management system should take place within the kernel for performance reasons (so that the data buffer does not have to be copied back and forth between user-space and kernel-space) which means that the management system has to be implemented on the kernel side. This would seem to favor choosing a file system as they are generally implemented on the kernel side whereas database management systems tend to run in user-space.\n\nAttributes may be constructed hierarchically (see Section 3.1). Hierarchies in databases may be mapped by relations, however, performance suffers on moving lower down the hierarchy when SQL normal forms are adhered to. In the same way, the complexity of maintenance of the database schema increases cumulatively.\n\nSMAP_FS provides a mechanism (QZone) which allows the buffering of files (caching), dispatching them to their target destination only on a well-defined point in time. As files would have to be treated as B(LOB) in a database, performance would once again suffer.\n\nNevertheless, we would like to point out that while it is technically feasible to draw on a database system as a storage management system, it does not seem to be 5 advantageous to do so at this point in time; however, this aspect may change in the future. One example of an interesting implementation of a file system âon topâ of a database is Michael A. Olson's approach which tackles features like querying and transaction security implicitly but which seems unsuitable for SmApper with these benchmarks [12,15].\n\nThe reasons why SmApper implements its own file system (SMAP_FS) are as follows:\n\nThe file system offered by SmApper must be optimized for so-called Lookups. This means that any search for a smap_base_type or a specific attribute of a smap_base_type as the case may be, must be extremely high-performance. Standard file systems often have to find a compromise specifically for lookups between the optimized locating of metadata entries(inodes) and quick access to actual blocks of data. On the other hand, the SMAP_FS stores the attribute values in the inode itself which leads to much higher performance but also means that only a pre-determined maximum size or length of attribute values can be saved. SMAP_FS is based on the assumption that, in accordance with the Pareto Analysis, at least 80% of the attribute values will fall within these pre-determined size limits. In all other cases, the value within the SMAP_FS-Inode refers to the actual data stream of the original file, which permits a retrieval of the attribute information but no (SMAP_FS-intrinsic) indexing.\n\nSMAP_FS must permit smap_base_type objects to be identified via an explicit path as well as by query using appropriate attributes. Standard file systems do not implement query interfaces even though exceptions like BeFS, the BeOS file system, would seem to prove the rule [17].\n\nThe file system must ensure that the integrity of a smap_base_type is protected at all times (see in addition the system lemma of Section 3.3).\n\nThe file system must offer triggers, both conditional triggers (rule-based triggers) as well as unconditional.\n\nThe file system receives additional logic which allows it to apply extractors and converters to data streams while these are being written, which should lead to optimal performance.\n\nThe complete design and the implementation description of the SMAP_FS lie well beyond the scope of this description. At this point, it will be sufficient to establish that SMAP_FS is an optimized file system which will:\n\nrender smap_fs_type objects persistently available\n\nprotect the integrity of persistent smap_fs_type objects\n\nensure the permanent connection between base_type object and smap_base_type object\n\nallow access to the attributes of the smap_fs_type object (directly and indirectly by query)\n\noffer a mechanism which buffers the binary representation of the base_type object and later dispatch it to its static or dynamic target destination\n\noffer versioning possibilities at file and block-level.\n\n4.4 Access to smap_base_types\n\nOne of the most important basic requirements of a SmApper system is access to the extended attributes of the smap_base_type (see Section 3.3 entitled âSmApperâBasic Functions). As the SmApper systems have to be capable of being integrated smoothly into existing infrastructures, access to attributes must occur without any kind of proprietary protocol and must be based exclusively on standards.\n\nSmApper solves this in a unique fashion by combining two standards:\n\nAccess via POSIX Standard (by path)\n\nAccess via XQuery/XPath (by query)\n\nAccess to a base_type occurs via path commands and via the usual POSIX-API (open, read, llseek etc.). Extended attributes of the smap_base_type are treated like individual files and are therefore also accessible via a (specific) path command as well as via POSIX-API. The following example will serve to illustrate this: the title of the original file (an MS Word document)/home/users/gth/hello.doc was extracted and saved in the attribute title in the SMAP_FS. Access to this attribute now occurs via the path command/home/users/gth/hello.doc?//title.\n\nThe delimiter serves only as an example here and can be configured. The path command is specific in our example and therefore delivers a SMAP_FS-file handle when an open-request is demanded. Finally, of course, the usual I/O operations can be carried out using this file handle. Should the attribute allow write-access then a write-syscall will only be successful when the modifications are also reflected in the original document (in our example/home/users/gth/hello.doc)âduring an outbound-operation the write-request will be executed without modification to the original document. Should the modification to the original document, which will, of course, not take place until a later date, then fail, the file would be labeled with the corresponding status in the QZone.\n\nShould the path command not lead to a specific SMAP_FS attribute (suppose, in our example, there were several titles) the path command would be treated as an access to a directory, in that the individual actual attributes could be treated by means of iterative access.\n\nThe query capacities of the SmApper namespace can be illustrated in the following examples; however, they act in the same manner as in the above example (which is, in effect, nothing more than a very simple query):\n\nhello.doc?//title[position( )!=1]:\n\nthis delivers all the title attributes of the hello document except the first\n\nhello.doc?//contains(title[position( )=1],confidential):\n\nthis delivers a file handle back to the hello document, should the word âconfidentialâ appear in the first title\n\nhello.doc?//title[position( )=1]/subtitle:\n\nthis delivers the subtitle of the first title attribute of the hello document\n\nThe combination of the two standards (POSIX, XQUERY) enables the SmApper systems to be integrated smoothly into existing infrastructures, as the normal file access has not changed in any way. Access to the extended information of the SMAP_FS also takes place using the standard file I/O, the sole change being the extended path syntax that users, and in particular, applications must use when attribute access is required. As this extended syntax conforms to the accepted standards, its integration should not prove to be a huge investment for application developers.\n\n4.5 Rules and Actions\n\nRules and actions form SmApper's actual compute-layer, allowing decisions to be made and actions to be taken on the basis of the extended information included in a smap_base_type as opposed to a base_type. Rules offer the possibility of forming Boolean Expressions using Boolean Operators (AND, OR, NOT) and datatype-specific operators (for example, =, !=, <, >, contains, etc.).\n\nOn the one hand, the attributes of smap_base_type can be considered operands, or even, on the other hand, constants like Literals, time commands like now, today, among others. Rules constitute SmApper's very simple model of the decision-making body. An example for a rule is:\n\n(this_file.summary contains âABCâ) AND\n\nthis_file.uid=1001 )â¥\n\n(this_file.size <2048)\n\nA rule always has access to all smap_base_type objects which are located within its scope. There are three ways of bringing an object into the scope:\n\n1. Implicit: during a file system event, the object this_file is always located implicitly in the scope. This is the file which led to the trigger event of the rule.\n\n2. By path: a new object can be instantiated in the scope by a definite SMAP_FS-Path, for example/smap_mnt/x.doc?uid\n\n3. By query: objects can be instantiated by query (see Section 4.4 entitled Access to smap_base_types).\n\nIn SmApper, rules constitute the authority which decides whether an Action should be executed or not, and, if so, whether Action A or Action B should be executed. An Action can be any event from sending an email, the encrypting of data, the moving/copying of files within the storage networks, to access to a SAP system. SmApper even considers the extractors and converters previously introduced as actions in the broadest sense.\n\nOwing to the diversity of potential actions, one of SmApper's basic requirements is that it must allow external, third-party applications to be accepted as actions. In the same way, SmApper's second and third basic requirements, follow on: it must ensure that the third-party application can in no way compromise the operation of the SmApper appliance. Furthermore, it must be capable of high-performance execution of actions.\n\nThese basic requirements are implemented in one of the core areas of SmApper's own operating system, the SmAp-OS, which is based on FreeBSD. While standard operating systems offer the concept of processes and threads as lightweight processes, actions exist in SmAp-OS as a third process abstraction layer, which can be thought of as ultra-lightweight-processes. This action authority operates in a type of Virtual Machine (VM) within the core of the SmAp-OS. This VM enables additional security parameters to be determined, for example:\n\n1. max_time: Maximum duration of the action's execution in the system\n\n2. max_call_depth: How many fork( )/exec( )-calls are permitted?\n\n3. max_file_desc: How many file descriptors are permitted?\n\n4. mem_areas_allowed: Access to which memory segments are permitted (DMA etc.)?\n\n5. max_heap, max_stack: How large may individual memory segments be?\n\n6. networking: Which network protocols are permitted?\n\n7. pre-emptable: Can the action be interrupted?\n\nHowever, the VM does not simply enable the performance of the actions to be determined, in order to achieve a higher level of security. The VM also provides a separate protected address room, which severs standard processes (system programs, etc.) and the kernel from actions. Should an action crash, then, in a worst case scenario, it would only affect itself and other actions but not the rest or the core of the SmApper system. Moreover, the separate address room provides the capacity for more efficient Context-Switching and for quicker process creation (no more memory areas, which have to be copied, etc.) As the SmAp-OS now recognizes the concept of action processes in addition to standard processes and real-time processes, a more granulating scheduling is possible, again leading to higher (or better adapted) performance.\n\nIn SmApper, rules and actions can be combined in a very simple but unique way, by using the concept of conditional cloning. With UNIX operating systems programs are carried out in two stages: firstly, by calling up one of the fork( ) system calls (vfork( ), clone( ) and so on) followed by one of the exec-system calls. Forking creates a copy of the program which is currently running in memory while the exec-call loads a new program in the memory which can be carried out. UNIX derivatives, in particular BSD and Linux, have implemented extremely efficient ways to start a program(=process creation) and yet this step still remains one of the most expensive services offered by an operating system. SmApper's conditional cloning allows the kernel to evaluate a rule bef"
    }
}
{
    "id": "dbpedia_8834_2",
    "rank": 76,
    "data": {
        "url": "https://journals.ametsoc.org/view/journals/aies/3/1/AIES-D-23-0048.1.xml",
        "read_more_link": "",
        "language": "en",
        "title": "Deep Learning Image Segmentation for Atmospheric Rivers",
        "top_image": "https://journals.ametsoc.org/cover/journals/aies/aies_cover.jpg",
        "meta_img": "https://journals.ametsoc.org/cover/journals/aies/aies_cover.jpg",
        "images": [
            "https://journals.ametsoc.org/fileasset/AMET-Journals-Logo-Mobile.png",
            "https://journals.ametsoc.org/fileasset/AMS-Logo-Lockup-Journals-01.png",
            "https://journals.ametsoc.org/fileasset/AMS-Logo-Lockup-Journals-01.png",
            "https://journals.ametsoc.org/fileasset/AMET-Journals-Logo-Mobile.png",
            "https://journals.ametsoc.org/fileasset/AMS-Logo-Lockup-Journals-01.png",
            "https://journals.ametsoc.org/fileasset/AMS-Logo-Lockup-Journals-01.png",
            "https://journals.ametsoc.org/fileasset/AMET-Journals-Logo-Mobile.png",
            "https://journals.ametsoc.org/fileasset/AMS-Logo-Lockup-Journals-01.png",
            "https://journals.ametsoc.org/fileasset/AMS-Logo-Lockup-Journals-01.png",
            "https://journals.ametsoc.org/fileasset/AMET-Journals-Logo-Mobile.png",
            "https://journals.ametsoc.org/fileasset/AMS-Logo-Lockup-Journals-01.png",
            "https://journals.ametsoc.org/fileasset/AMS-Logo-Lockup-Journals-01.png",
            "https://journals.ametsoc.org/coverimage?doc=%2Fjournals%2Faies%2F3%2F1%2Faies.3.issue-1.xml&width=200",
            "https://journals.ametsoc.org/view/journals/aies/3/1/inline-AIES-D-23-0048.1-f1.jpg",
            "https://journals.ametsoc.org/view/journals/aies/3/1/inline-AIES-D-23-0048.1-f2.jpg",
            "https://journals.ametsoc.org/view/journals/aies/3/1/inline-AIES-D-23-0048.1-f3.jpg",
            "https://journals.ametsoc.org/view/journals/aies/3/1/inline-AIES-D-23-0048.1-f4.jpg",
            "https://journals.ametsoc.org/view/journals/aies/3/1/inline-AIES-D-23-0048.1-f5.jpg",
            "https://journals.ametsoc.org/view/journals/aies/3/1/inline-AIES-D-23-0048.1-f6.jpg",
            "https://journals.ametsoc.org/view/journals/aies/3/1/inline-AIES-D-23-0048.1-f1.jpg",
            "https://journals.ametsoc.org/view/journals/aies/3/1/inline-AIES-D-23-0048.1-f2.jpg",
            "https://journals.ametsoc.org/view/journals/aies/3/1/inline-AIES-D-23-0048.1-f3.jpg",
            "https://journals.ametsoc.org/view/journals/aies/3/1/inline-AIES-D-23-0048.1-f4.jpg",
            "https://journals.ametsoc.org/view/journals/aies/3/1/inline-AIES-D-23-0048.1-f5.jpg",
            "https://journals.ametsoc.org/view/journals/aies/3/1/inline-AIES-D-23-0048.1-f6.jpg",
            "https://journals.ametsoc.org/view/journals/aies/3/1/full-AIES-D-23-0048.1-t1.jpg",
            "https://journals.ametsoc.org/view/journals/aies/3/1/full-AIES-D-23-0048.1-t1.jpg",
            "https://journals.ametsoc.org/view/journals/aies/3/1/full-AIES-D-23-0048.1-t1.jpg",
            "https://journals.ametsoc.org/view/journals/aies/3/1/full-AIES-D-23-0048.1-f1.jpg",
            "https://journals.ametsoc.org/view/journals/aies/3/1/full-AIES-D-23-0048.1-f1.jpg",
            "https://journals.ametsoc.org/view/journals/aies/3/1/full-AIES-D-23-0048.1-f1.jpg",
            "https://journals.ametsoc.org/view/journals/aies/3/1/full-AIES-D-23-0048.1-f2.jpg",
            "https://journals.ametsoc.org/view/journals/aies/3/1/full-AIES-D-23-0048.1-f2.jpg",
            "https://journals.ametsoc.org/view/journals/aies/3/1/full-AIES-D-23-0048.1-f2.jpg",
            "https://journals.ametsoc.org/view/journals/aies/3/1/full-AIES-D-23-0048.1-f3.jpg",
            "https://journals.ametsoc.org/view/journals/aies/3/1/full-AIES-D-23-0048.1-f3.jpg",
            "https://journals.ametsoc.org/view/journals/aies/3/1/full-AIES-D-23-0048.1-f3.jpg",
            "https://journals.ametsoc.org/view/journals/aies/3/1/full-AIES-D-23-0048.1-t2.jpg",
            "https://journals.ametsoc.org/view/journals/aies/3/1/full-AIES-D-23-0048.1-t2.jpg",
            "https://journals.ametsoc.org/view/journals/aies/3/1/full-AIES-D-23-0048.1-t2.jpg",
            "https://journals.ametsoc.org/view/journals/aies/3/1/full-AIES-D-23-0048.1-f4.jpg",
            "https://journals.ametsoc.org/view/journals/aies/3/1/full-AIES-D-23-0048.1-f4.jpg",
            "https://journals.ametsoc.org/view/journals/aies/3/1/full-AIES-D-23-0048.1-f4.jpg",
            "https://journals.ametsoc.org/view/journals/aies/3/1/full-AIES-D-23-0048.1-t3.jpg",
            "https://journals.ametsoc.org/view/journals/aies/3/1/full-AIES-D-23-0048.1-t3.jpg",
            "https://journals.ametsoc.org/view/journals/aies/3/1/full-AIES-D-23-0048.1-t3.jpg",
            "https://journals.ametsoc.org/view/journals/aies/3/1/full-AIES-D-23-0048.1-f5.jpg",
            "https://journals.ametsoc.org/view/journals/aies/3/1/full-AIES-D-23-0048.1-f5.jpg",
            "https://journals.ametsoc.org/view/journals/aies/3/1/full-AIES-D-23-0048.1-f5.jpg",
            "https://journals.ametsoc.org/view/journals/aies/3/1/full-AIES-D-23-0048.1-t4.jpg",
            "https://journals.ametsoc.org/view/journals/aies/3/1/full-AIES-D-23-0048.1-t4.jpg",
            "https://journals.ametsoc.org/view/journals/aies/3/1/full-AIES-D-23-0048.1-t4.jpg",
            "https://journals.ametsoc.org/view/journals/aies/3/1/full-AIES-D-23-0048.1-f6.jpg",
            "https://journals.ametsoc.org/view/journals/aies/3/1/full-AIES-D-23-0048.1-f6.jpg",
            "https://journals.ametsoc.org/view/journals/aies/3/1/full-AIES-D-23-0048.1-f6.jpg",
            "https://journals.ametsoc.org/view/journals/aies/3/1/full-AIES-D-23-0048.1-t5.jpg",
            "https://journals.ametsoc.org/view/journals/aies/3/1/full-AIES-D-23-0048.1-t5.jpg",
            "https://journals.ametsoc.org/view/journals/aies/3/1/full-AIES-D-23-0048.1-t5.jpg",
            "https://journals.ametsoc.org/coverimage?doc=%2Fjournals%2Faies%2F3%2F1%2Faies.3.issue-1.xml&width=200",
            "https://journals.ametsoc.org/view/journals/aies/3/1/inline-AIES-D-23-0048.1-f1.jpg",
            "https://journals.ametsoc.org/view/journals/aies/3/1/inline-AIES-D-23-0048.1-f2.jpg",
            "https://journals.ametsoc.org/view/journals/aies/3/1/inline-AIES-D-23-0048.1-f3.jpg",
            "https://journals.ametsoc.org/view/journals/aies/3/1/inline-AIES-D-23-0048.1-f4.jpg",
            "https://journals.ametsoc.org/view/journals/aies/3/1/inline-AIES-D-23-0048.1-f5.jpg",
            "https://journals.ametsoc.org/view/journals/aies/3/1/inline-AIES-D-23-0048.1-f6.jpg",
            "https://journals.ametsoc.org/view/journals/aies/3/1/inline-AIES-D-23-0048.1-f1.jpg",
            "https://journals.ametsoc.org/view/journals/aies/3/1/inline-AIES-D-23-0048.1-f2.jpg",
            "https://journals.ametsoc.org/view/journals/aies/3/1/inline-AIES-D-23-0048.1-f3.jpg",
            "https://journals.ametsoc.org/view/journals/aies/3/1/inline-AIES-D-23-0048.1-f4.jpg",
            "https://journals.ametsoc.org/view/journals/aies/3/1/inline-AIES-D-23-0048.1-f5.jpg",
            "https://journals.ametsoc.org/view/journals/aies/3/1/inline-AIES-D-23-0048.1-f6.jpg",
            "https://journals.ametsoc.org/fileasset/footer-logo.png",
            "https://journals.ametsoc.org/view/journals/aies/3/1/full-AIES-D-23-0048.1-t5.jpg",
            "https://journals.ametsoc.org/view/journals/aies/3/1/full-AIES-D-23-0048.1-f6.jpg"
        ],
        "movies": [],
        "keywords": [],
        "meta_keywords": [
            ""
        ],
        "tags": null,
        "authors": [
            "Daniel Galea",
            "Hsi-Yen Ma",
            "Wen-Ying Wu",
            "Daigo Kobayashi"
        ],
        "publish_date": "2024-01-04T00:00:00",
        "summary": "",
        "meta_description": "Abstract The identification of atmospheric rivers (ARs) is crucial for weather and climate predictions as they are often associated with severe storm systems and extreme precipitation, which can cause large impacts on society. This study presents a deep learning model, termed ARDetect, for image segmentation of ARs using ERA5 data from 1960 to 2020 with labels obtained from the TempestExtremes tracking algorithm. ARDetect is a convolutional neural network (CNN)-based U-Net model, with its structure having been optimized using automatic hyperparameter tuning. Inputs to ARDetect were selected to be the integrated water vapor transport (IVT) and total column water (TCW) fields, as well as the AR mask from TempestExtremes from the previous time step to the one being considered. ARDetect achieved a mean intersection-over-union (mIoU) rate of 89.04% for ARs, indicating its high accuracy in identifying these weather patterns and a superior performance than most deep learning–based models for AR detection. In addition, ARDetect can be executed faster than the TempestExtremes method (seconds vs minutes) for the same period. This provides a significant benefit for online AR detection, especially for high-resolution global models. An ensemble of 10 models, each trained on the same dataset but having different starting weights, was used to further improve on the performance produced by ARDetect, thus demonstrating the importance of model diversity in improving performance. ARDetect provides an effective and fast deep learning–based model for researchers and weather forecasters to better detect and understand ARs, which have significant impacts on weather-related events such as floods and droughts.",
        "meta_lang": "en",
        "meta_favicon": "/skin/74df12695855117a235c65909eacf705ab236dfb/favicon.ico",
        "meta_site_name": "AMETSOC",
        "canonical_link": "https://journals.ametsoc.org/view/journals/aies/3/1/AIES-D-23-0048.1.xml",
        "text": "a. AR detection using classical methods\n\nDetection and tracking of ARs has previously been performed by many different algorithms. In fact, an effort to compare the results obtained from these methods is being undertaken in a project called the Atmospheric River Tracking Method Intercomparison Method (ARTMIP). Some intermediate results and a full list of algorithms considered can be found in the study published by Shields et al. (2018).\n\nIn short, these algorithms work by using thresholds on various meteorological fields, most commonly the integrated water vapor transport (IVT), to identify possible candidates for ARs in each time step of data being considered. In some algorithms, these candidates are then stitched together in the temporal dimension to make up tracks of ARs. Furthermore, some of these tracks, and their respective AR candidates, are removed if a temporal threshold is not met. While most of the tracking methods follow this general template, each is then tuned to the problem that needs to be solved by the original developer.\n\nThe differences between the methods mean that variations are present in the results obtained. Also, the American Meteorological Society defines an AR as a “long, narrow, and transient corridor of strong horizontal water vapor transport that is typically associated with a low-level jet stream ahead of the cold front of an extratropical cyclone” (https://glossary.ametsoc.org/wiki/Atmospheric_river). It can be noted that this definition is not quantitative, so disagreements in the different algorithms occur just because of different interpretations of this definition.\n\nHowever, as Lora et al. (2020) discuss, most of these methods agree on the presence of most ARs, especially strong ones and in five distinct areas over the extratropical oceans show a high degree of agreement. Differences then occur on the spatial extent of weaker ARs, and this is due to the relatively vague definition of an AR. As such, ARs detected to be in the extratropics need to be considered carefully, as should those with thin, long filament-like structures as the latter are usually attributed to weak systems. These can be considered as ARs by certain algorithms and not by others depending on the use case that the original developer had in mind.\n\nb. AR detection using machine learning\n\nAlgorithms using ML and DL methods have recently been developed to detect ARs in meteorological data. These are summarized below and tabulated in Table 1.\n\nMuszynski et al. (2019) developed a two-stage method that first finds possible ARs using topological data analysis and then uses a support vector machine (SVM) to determine whether each of these candidates is a true AR, with the ground-truth labels coming from the Toolkit for Extreme Climate Analysis (TECA; Prabhat et al. 2015). This method is applied to the total precipitable water (TMQ) field, but the authors argue that it could be applied to many other fields of interest. Having applied this method to a number of different datasets, each having different spatial and temporal resolutions, the method obtained an accuracy between 77% and 91%.\n\nAnother detection-based method was developed by Liu et al. (2016). The authors created a 4-layer convolutional neural network (CNN)-based classifier and used data from cropped sections of the globe to infer whether a tropical cyclone (TC) or an AR is present. The network used TMQ and a land–sea mask as inputs and the required labels were also derived from TECA. After training, the network obtained an accuracy score of 90% for detecting ARs.\n\nBesides detection, deep learning has also been employed to perform image segmentation for ARs. In this case, the desired output of the network is an array similar to that output by TECA or any other heuristic that shows whether each position in the image belongs to an AR. This is more useful than simple classification as the spatial attributes of features, ARs in this case, can be more easily studied. One useful metric used to quantify how well the inference produced by the network matches the given label is the intersection-over-union (IoU). The IoU measures the common area (area of intersection) between the given label and the inference given by the network and divides it by the total area of the combined label and inference. The mean intersection-over-union (mIoU) is then the mean of the IoUs obtained for each test case. Hence, an mIoU value as close as possible to 100% is desired.\n\nKurth et al. (2018) repurposed the Tiramisu (Jégou et al. 2016) and DeepLabv3+ (Chen et al. 2018) networks for image segmentation to detect ARs. They used a dataset containing various different variables including water vapor, wind speed, precipitation, temperature, pressure and humidity at different heights in the atmosphere for the whole globe. After training the two different networks, the Tiramisu network obtained a mIoU for ARs of 59% and the DeepLabv3+ based network obtained an mIoU of 73%.\n\nPrabhat et al. (2021) built on this work by using a similar adapted DeepLabv3+ network to perform image segmentation for ARs. The dataset the authors used also spanned the whole globe, but they only included four variables in their dataset: TMQ, meridional wind speed at 850 hPa, zonal wind speed at 850 hPa, and the total precipitation rate (PRECT). The authors also had two different labeling sources: one was TECA while the second source was a set of 459 samples that were labeled by human experts. They noted that the mIoU for ARs in the expert labeled dataset between multiple experts is 34%, showing that even experts can disagree significantly on the spatial extent of an AR. When training their network on labels obtained from TECA, the authors reported an mIoU for ARs of 77% between the inferences made by the network and the labels obtained from TECA. However, when training using the expert-created labels, the AR mIoU dropped to 34%, possibly suggesting that the uncertainty between the definition of an AR across the experts may be hindering the performance of the network.\n\nKapp-Schwoerer et al. (2020) used the expert-labeled dataset of Prabhat et al. (2021) and network based on CGNet (Wu et al. 2021), a context-guided deep neural network for image segmentation, to perform this task for ARs and TCs. While the authors did not give an mIoU for ARs only, the mIoU for ARs, TCs, and background features is that of 56%. Finally, Buch (2021) used the same dataset and another adaptation of CGNet to attempt to produce a better performing network. The one difference between the two methods is that the dataset for the second method was restricted to only four areas instead of the whole globe. These were the northeastern Pacific (NEPAC), the southeastern Pacific (SEPAC), the North Pacific (NPAC), and the South Pacific (SPAC). It should be noted that NEPAC and SEPAC are mostly contained in SPAC. The authors showed that the network trained using all four regions obtained an all-class mIoU of 55% when testing on NEPAC data. Furthermore, a NEPAC-trained network obtained an all-class mIoU of 57% when trained and tested on NEPAC-only data, showing that a region-specific network might be useful.\n\nHiggins et al. (2023) trained a network based on CGNet (Wu et al. 2021) on data spanning different regions, different horizontal resolutions and different climate models. The authors argued that IVT was computationally expensive to calculate and that some model outputs do not have this field present. As such, they used zonal wind at 850 mb (1 mb = 1 hPa), meridional wind at 850 mb, surface pressure, and total column integrated water vapor (IWV) derived from specific humidity as inputs, with the aim of making the resultant network more flexible to preexisting datasets. However, when evaluating their results against labels created by 8 different algorithms from the ARTMIP project, the AR IoU values ranged from 20% to 50%.\n\nTian et al. (2023) employ an ensemble of 20 different deep learning models to perform image segmentation for ARs. Similar to the previous method, the authors do not utilize IVT as an input variable to their models, but instead only use zonal wind at 850 mb, meridional wind at 850 mb, and IWV. Each model in the ensemble was trained independently, but after training, the final result was obtained via majority voting. Therefore, the class of a certain position is determined by which class most models in the ensemble assign the position to. When testing across the whole test dataset used in their study, the ensemble of models obtained an mIoU for ARs of 40.5% and the authors showed that the ensemble performed much better than any single model in the ensemble, with none surpassing an mIoU of 38.5%.\n\nAs can be noted, the best mIoU obtained was that of 77% and the best accuracy was that of 91%. In this study, we present a deep learning model aimed at image segmentation for ARs in climate data. This model obtains an mIoU of nearly 90%, an accuracy of 99.41% and a recall rate of 90.34%. This improvement in performance is attributed to a larger training dataset and more fields than used previously as well as a network architecture specifically tuned for the application at hand.\n\nSection 2 details the architecture of this model and the dataset used. Section 3 presents the results obtained from this model and discusses why the fields used in this study were chosen; it also shows how creating an ensemble of this model improved the overall performance. Finally, section 4 summarizes the study’s results.\n\na. Data\n\nARDetect employs several meteorological fields to identify ARs. These fields are treated as separate channels in an image, and a binary mask is generated as output to indicate whether a given pixel belongs to the AR class. The input fields consist of IVT, total column water (TCW)—the vertical integral of water vapor, cloud liquid water, and cloud ice in a single column—and wind speed at 850 hPa. In addition, the same fields and the AR mask from the previous time step are included as inputs. These fields were decided after performing a feature importance process, which is detailed in section 3b, on a larger set of fields. The ERA5 reanalysis dataset (Hersbach et al. 2018, 2020) is used to obtain the meteorological data, which provides a comprehensive representation of past climate conditions through simulations and observations. We use 3-hourly data, and they were interpolated from the native resolution of 0.25° to a resolution of 0.5° horizontally. This was done as a better performing version of ARDetect was obtained with the coarser resolution, possibly due to implicit filtering of noise in the input data. The resulting input and output arrays were each 360 rows by 720 columns. Finally, the dataset was divided into three subsets for training, validation, and testing, spanning 1960–2010, 2010–15, and 2015–20, respectively.\n\nA way to obtain the labels, that is, AR masks, for ARDetect was required. TempestExtremes was used for this purpose. The method used to create these AR masks mainly followed that used by Ullrich et al. (2021):\n\nDetection of tropical cyclones: First, plausible candidates for TCs are identified as local minima in the mean sea level pressure field and then narrowed down using two criteria that check for a warm-core structure. Then the detected TCs are joined over time to create tracks, with certain criteria used to remove false detections. Finally, the size of the TCs is defined by calculating the radius at which wind speed drops below 8 m s−1.\n\nDetection of AR candidates: ARs are identified as ridges in the IVT field. This is calculated from the eastward and northward components of the IVT. The Laplacian using 8 radial grid points at 10° great circle distance (GCD) is then calculated, and AR candidates are designated as those ridges with a Laplacian value smaller than −2 × 104 kg m−2 s−1 rad−2. Then, any AR areas that are within 15° of the equator are cropped, and those that have an area smaller than 4 × 105 km are removed. The former is done to remove any MCS-like systems, while the cropping is performed as ARs are considered to be midlatitude features.\n\nFinal AR masks: TCs can sometimes exhibit similar features to ARs in the IVT field so it is necessary to remove TCs from any of the AR candidates. Hence, any AR candidates that are within 8 GCD of any TC center as detected previously are removed from the dataset. This final dataset contains all AR masks that are to be considered as labels for ARDetect.\n\nGiven that the method used to obtain these AR maps is crucial to the performance of ARDetect, it would be important to address the robustness of these maps, that is, whether any biases are present that would make the resultant ARDetect more performant in a specific area. As discussed above, TempestExtremes is an algorithm that relies on values of IVT and we consider the maps produced as truth in our case. As such, the algorithm itself does not have biases, but the underlying IVT field might have. Since we are using ERA5 reanalysis data, the quality of the data very much depends on the observational dataset used during the compilation of ERA5. This means that the quality of data in the NH would be superior to that of the SH before satellite data were available (pre-1979), but no differences should be seen after. Also, given the large dataset constructed for training ARDetect, pre-1979 data constitute a minor part of the whole dataset, so we do not see any biases between labels in the NH and SH.\n\nIt should be noted that if the AR maps are produced using some algorithm other than TempestExtremes, or with TempestExtremes but using different settings, it is expected that ARDetect will mirror those labels. As such, a possible extension of this work is to use uncertainty quantification to produce a map of where an AR might be, which will help bridge some of the differences between the different definitions.\n\nFigure 1 shows an example of the collected meteorological fields and associated AR mask for one time step. These fields are then collected in such a manner that each input to ARDetect has fields from two consecutive timesteps stacked together. Each input feature was then standardized using the minimum and maximum value for that feature for that input case.\n\nb. Architecture\n\nThe ARDetect model utilizes a U-Net-style architecture (Ronneberger et al. 2015) with multiple convolutional layers. Automatic hyperparameter tuning was performed using the Ray Tune (Liaw et al. 2018) Python package to determine the optimal architecture. Initially, random searches were executed to narrow down the search space to a range that included the highest performing architectures. Bayesian optimization was then employed to arrive at the best architecture, which is shown in Fig. 2. In Bayesian optimization, the user first defines a starting search space. The algorithm then refines this iteratively depending on the results of any previously executed searching. This is done by fitting a Gaussian process regression model on the search space to attempt to narrow it down. Points from this search space are then selected, and a cost function, usually taken to be the performance of the model, is computed. Then, the next iteration of search space refining is done. The process ends after a set number of iterations. Details of the final model structure are provided below.\n\nAs previously stated, the model takes inputs with a shape of 360 rows and 720 columns, with a batch size of 16 per graphics processing unit (GPU) resulting in an input shape of (16, 360, 720). The inputs are processed through four convolution blocks, each comprising a convolutional layer, a batch normalization layer, a dropout layer, and a leaky ReLU activation function. The dropout rate used was 0.12%.\n\nThe four convolution blocks had 6, 24, 96, and 384 feature maps, respectively, and used a kernel size of 4. After processing through a block, a copy of the outputs was retained to serve as connections to the upsampling branch of the U-Net. Additionally, a maxpooling operation was applied to reduce the input size by half in preparation for the next convolution block.\n\nAfter passing through the downsampling branch of the U-Net, the resulting latent space is fed into the upsampling branch. Each block in this branch used the outputs from the previous layer as inputs and employed a ConvTranspose2D layer, a dropout layer, and leaky ReLU activation to upsample the inputs. The outputs were concatenated with the corresponding connections originating from the downsampling branch of the U-Net. The concatenated data were then used as inputs to a convolutional layer to prepare the data for the next upsampling block. The upsampling blocks used the same number of feature maps as in the downsampling branch and padding was used throughout the network to ensure compatibility of sizes for the inputs and outputs.\n\nA final convolutional layer was used to transform the resulting data such that two channels were outputted. This was done so that the CrossEntropy loss function could be used. The network weights, of which there are around 63 million, were initialized using the Xavier method (Glorot and Bengio 2010). RMSProp was selected as the optimizer with a learning rate of 1.39 × 10−3, an alpha value of 5.65 × 10−1, and an eps value of 8.34 × 10−5. In our dataset, 95.5% of the pixels were non-AR pixels and 4.5% were AR pixels. Therefore, due to the imbalanced nature of the training dataset, class weights were supplied to the optimizer to weight the contribution of each example accordingly.\n\na. Model performance\n\nARDetect performed well in creating a mask to show the position of any atmospheric river present in meteorological data, with an mIoU of 89.04% for the AR class in the testing dataset. Figure 3 shows two examples of predictions made, indicating that the major features of ARs are well defined with minor disagreements between the predictions and labels occurring for some thin AR regions. This is also expected of human experts, as discussed previously in section 1a, so the replication of this by ARDetect was not unexpected.\n\nThe confidence matrix for each pixel in each image in the testing dataset was also calculated and is shown in Table 2. The matrix indicates that the vast majority of pixels (99.41%) were classified correctly, with the majority of these pixels on the inverse diagonal (top-left to bottom-right). Both the false positive rate (0.08%) and the false negative rate (0.51%) are very low compared to the true positive (4.77%) and true negative (94.64%) rates, indicating the superior performance of ARDetect.\n\nUsing the values in the confidence matrix, accuracy, precision, and recall rates were calculated. ARDetect has an accuracy of 99.41%, while the recall rate is 90.34% and the precision rate is 98.35%. These values show that ARDetect is achieving high performance for image segmentation of ARs.\n\nThe mIoU for ARs was also calculated for both the latitudinal and longitudinal directions to identify where ARDetect is more likely to produce erroneous predictions. In the top panel of Fig. 4, we show how the mIoU for ARs varies across different latitudes. The value of mIoU is zero in the tropics due to TempestExtremes being set up in such a way that no ARs are to be detected between 15°N and 15°S as discussed in section 2a. It could be noticeable that ARDetect does very well in the midlatitudes and nearer to the equator to a lesser extent, while it performs less well at the poles. The poorer performance nearer the poles is expected as fewer ARs are present in these areas, so a limited number of training data are available. This makes it harder for ARDetect to obtain maximal performance in these regions. It could also be noted that ARDetect performs marginally better in the Northern Hemisphere with an average mIoU of 49% across all latitudes versus the Southern Hemisphere, which had an average mIoU of 48.3%. However, the highest mIoU in the Northern Hemisphere of 89.7% was surpassed by an mIoU of 93% in the Southern Hemisphere. Similarly we show how the mIoU for ARs varies across different longitudes in the bottom panel of Fig. 4. This shows that ARDetect performs marginally better in the Western Hemisphere with higher mIoU values (average mIoU: 82.1%; maximum mIoU: 87.6%) than in the Eastern Hemisphere (average mIoU: 77.6%; maximum mIoU: 84.2%). It can also be noted that the mIoU drops to zero at the date line. This is because zero padding was employed as the last layer of ARDetect to ensure common sizes between inputs and outputs. As such, no ARs will be detected in the last 1.5° of latitude by the current iteration of ARDetect, but it is an issue to be rectified in any future work.\n\nb. Feature importance\n\nIn the field of deep learning, the selection of relevant features, meteorological fields in our case, to include as inputs is a crucial aspect in model building. One method to determine the importance of these features is through the use of feature importance analysis. In this study, we employed this technique to determine the most important meteorological fields to be included in the final ARDetect model. We used the methodology described by Breiman (2001):\n\nTraining of deep learning model: A deep learning model was trained using a training dataset. The network used was one that had acceptable performance for the task. As such, it only had three hidden layers and considerably fewer parameters than the final ARDetect architecture.\n\nBase performance calculation: A testing dataset was used to obtain the base performance of the deep learning model.\n\nPermutation of single feature: A single feature was permuted across all test cases, and the performance of the deep learning model was calculated using the altered dataset.\n\nPerformance difference calculation: The difference between the performance using the permuted dataset and the base performance was calculated.\n\nObtain difference for each feature: The previous two steps were repeated for all features present in the dataset, such that the difference in performance was calculated for each input feature.\n\nObtain list of most important features: The differences were sorted in descending order to determine the most important features.\n\nThis process was carried out using the validation set that was described in section 2a. The training set for this process spanned from 2010 until 2012 (i.e., 3 years of data) while the testing set was selected to have data from 2014 and 2015 (i.e., 2 years of data). Table 3 shows the variables included in this process. A time dimension was also added by including the same fields from the previous time step to the one being considered as inputs.\n\nThe deep learning network used to perform this feature importance process was a 3-layer U-Net with each layer having 37, 74, and 148 filters, respectively. The AdamW optimiser was used together with CrossEntropyLoss. Due to the large imbalance of AR pixels and non-AR pixels, weights amounting to the percentage of AR and non-AR pixels (4.04% and 95.96%, respectively) were passed to the loss function so that AR pixels were given more importance during training. This network obtained an mIoU for ARs of 84.05%.\n\nAfter training the network and performing the feature importance process, those variables having a positive influence on the performance of the network were carried over to the development of ARDetect. The full results are plotted in Fig. 5.\n\nThe results showing the variables that improved the network’s performance are tabulated in Table 4. The mean AR mIoU in this table is the mean performance of 30 bootstrapped testing datasets with the variable in question being shuffled. The difference column of Table 4, which is visualized for all the variables tested in Fig. 5, is the difference between this mean AR mIoU and the performance of the model when testing using the original testing dataset. These show that the IVT field is the most important variable for the network to make its inferences. Other important variables are total column water and wind speed at 850 hPa. The AR mask from the previous time step is also important, presumably as it is a good starting point for the inference on the current time step. It was also noted that adding a time dimension to the inputs, that is, adding the same fields from the previous time step, helped improve performance.\n\nd. Ensemble of models\n\nEnsemble modeling is a powerful technique that involves combining the predictions of multiple models to improve overall performance. In this section, we describe our ensemble approach for predicting the positions of ARs in meteorological data.\n\nTo create the ensemble of models, we trained the ARDetect architecture 10 times using the same training dataset but different initial random seeds. This ensured that the initial weights of the models and the batches of data used for training were different for each run. To obtain a single prediction from the ensemble, the outputs of each trained version of ARDetect were first obtained. Each inference has values ranging from 0 to 1, denoting the probability of a pixel belonging to an AR (0 being 0% and 1 being 100%). Then, any values below 0.5, or 50%, are set to 0, and any values larger than 0.5 are set to 1. Once this is done for the outputs of all members of the ensemble, the average is calculated. Once again, any values below 0.5 are set to 0, and any values larger than 0.5 are set to 1, thus obtaining the average mask from the ensemble.\n\nAfter training, we collected the predictions of each model and averaged the masks produced by each model. As such, there were 10 masks produced for each time step of the training dataset (one from each model), and then an average of these masks was calculated. We then calculated the final statistics using the average mask.\n\nThe ensemble of models achieved an mIoU for ARs of 89.95%, which is a 0.94% improvement from the singular ARDetect model. It should be noted that none of the ensemble members could produce better performance than the ensemble, showing that the ensemble method provides an improvement in performance, as was also found by Tian et al. (2023). This improvement could be attributed to the different models having slightly different end point when converging due to their starting points, thus capturing the loss surface better.\n\nThe confidence matrix for each pixel in each image in the testing dataset using the ensemble predictions was also calculated and is shown in Table 5. The matrix indicates that the vast majority of pixels (99.47%) were classified correctly, with the majority of these pixels on the inverse diagonal (from top left to bottom right). ARDetect tends to overpredict ARs, as the false positive rate (0.46%) is slightly higher than the false negative rate (0.08%). However, both rates are very low compared to the true positive (4.78%) and true negative (94.64%) rates, indicating that ARDetect is performing well.\n\nUsing the values in the confidence matrix, accuracy, precision, and recall rates were calculated. ARDetect has an accuracy of 99.47%, while the recall rate is 91.22% and the precision rate is 98.35%. These values show that the ensemble of ARDetect models is improving on the singular model by producing fewer false negatives, and more true negatives, which results in a higher performance."
    }
}
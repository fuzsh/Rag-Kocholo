{
    "id": "dbpedia_1820_0",
    "rank": 94,
    "data": {
        "url": "https://www.science.gov/topicpages/c/calculate%2Bsynthetic%2Bseismograms",
        "read_more_link": "",
        "language": "en",
        "title": "calculate synthetic seismograms: Topics by Science.gov",
        "top_image": "",
        "meta_img": "",
        "images": [
            "https://www.science.gov/scigov/desktop/en/images/SciGov_logo.png"
        ],
        "movies": [],
        "keywords": [],
        "meta_keywords": [
            ""
        ],
        "tags": null,
        "authors": [],
        "publish_date": null,
        "summary": "",
        "meta_description": "",
        "meta_lang": "en",
        "meta_favicon": "",
        "meta_site_name": "",
        "canonical_link": null,
        "text": "Synthetic Seismograms of Explosive Sources Calculated by the Earth Simulator\n\nNASA Astrophysics Data System (ADS)\n\nTsuboi, S.; Matsumoto, H.; Rozhkov, M.; Stachnik, J.\n\n2017-12-01\n\nWe calculate broadband synthetic seismograms using the spectral-element method (Komatitsch & Tromp, 2001) for recent explosive events in northern Korean peninsula. We use supercomputer Earth Simulator system in JAMSTEC to compute synthetic seismograms using the spectral-element method. The simulations are performed on 8,100 processors, which require 2,025 nodes of the Earth Simulator. We use one chunk with the angular distance 40 degrees to compute synthetic seismograms. On this number of nodes, a simulation of 5 minutes of wave propagation accurate at periods of 1.5 seconds and longer requires about 10 hours of CPU time. We use CMT solution of Rozhkov et al (2016) as a source model for this event. One example of CMT solution for this source model has 28% double couple component and 51% isotropic component. The hypocenter depth of this solution is 1.4 km. Comparisons of the synthetic waveforms with the observation show that the arrival time of Pn and Pg waves matches well with the observation. Comparison also shows that the agreement of amplitude of other phases is not necessarily well, which demonstrates that the crustal structure should be improved to include in the simulation. The surface waves observed are also modeled well in the synthetics, which shows that the CMT solution we have used for this computation correctly grasps the source characteristics of this event. Because of characteristics of artificial explosive sources of which hypocenter location is already known, we may evaluate crustal structure along the propagation path from the waveform modeling for these sources. We may discuss the limitation of one dimensional crustal structure model by comparing the synthetic waveform of 3D crustal structure and the observed seismograms.\n\nA simple derivation of the formula to calculated synthetic long-period seismograms in a heterogeneous Earth by normal mode summation\n\nNASA Technical Reports Server (NTRS)\n\nTanimoto, T.\n\n1983-01-01\n\nA simple modification of Gilbert's formula to account for slight lateral heterogeneity of the Earth leads to a convenient formula to calculate synthetic long period seismograms. Partial derivatives are easily calculated, thus the formula is suitable for direct inversion of seismograms for lateral heterogeneity of the Earth.\n\nMethod for rapid high-frequency seismogram calculation\n\nNASA Astrophysics Data System (ADS)\n\nStabile, Tony Alfredo; De Matteis, Raffaella; Zollo, Aldo\n\n2009-02-01\n\nWe present a method for rapid, high-frequency seismogram calculation that makes use of an algorithm to automatically generate an exhaustive set of seismic phases with an appreciable amplitude on the seismogram. The method uses a hierarchical order of ray and seismic-phase generation, taking into account some existing constraints for ray paths and some physical constraints. To compute synthetic seismograms, the COMRAD code (from the Italian: \"COdice Multifase per il RAy-tracing Dinamico\") uses as core a dynamic ray-tracing code. To validate the code, we have computed in a layered medium synthetic seismograms using both COMRAD and a code that computes the complete wave field by the discrete wave number method. The seismograms are compared according to a time-frequency misfit criteria based on the continuous wavelet transform of the signals. Although the number of phases is considerably reduced by the selection criteria, the results show that the loss in amplitude on the whole seismogram is negligible. Moreover, the time for the computing of the synthetics using the COMRAD code (truncating the ray series at the 10th generation) is 3-4-fold less than that needed for the AXITRA code (up to a frequency of 25 Hz).\n\nA simple derivation of the formula to calculate synthetic long-period seismograms in a heterogeneous earth by normal mode summation\n\nNASA Technical Reports Server (NTRS)\n\nTanimoto, T.\n\n1984-01-01\n\nA simple modification of Gilbert's formula to account for slight lateral heterogeneity of the earth leads to a convenient formula to calculate synthetic long period seismograms. Partial derivatives are easily calculated, thus the formula is suitable for direct inversion of seismograms for lateral heterogeneity of the earth. Previously announced in STAR as N83-29893\n\nSynthetic Seismogram Calculations for Two-Dimensional Velocity Models.\n\nDTIC Science & Technology\n\n1983-05-20\n\nvertical and radial component displacements. The seismograms have been convolved with a seismograph response function corresponding to a short period...phase velocity is a measure of the degree of numerical dispersion present in the calculation for a variety of grid spacings. The value of 1/G of 0.1...method is an approximate technique and is some what restricted in its application, its efficiency and accuracy make it suitable for routine modeling of\n\nThe Spontaneous Ray Log: A New Aid for Constructing Pseudo-Synthetic Seismograms\n\nNASA Astrophysics Data System (ADS)\n\nQuadir, Adnan; Lewis, Charles; Rau, Ruey-Juin\n\n2018-02-01\n\nConventional synthetic seismograms for hydrocarbon exploration combine the sonic and density logs, whereas pseudo-synthetic seismograms are constructed with a density log plus a resistivity, neutron, gamma ray, or rarely a spontaneous potential log. Herein, we introduce a new technique for constructing a pseudo-synthetic seismogram by combining the gamma ray (GR) and self-potential (SP) logs to produce the spontaneous ray (SR) log. Three wells, each of which consisted of more than 1000 m of carbonates, sandstones, and shales, were investigated; each well was divided into 12 Groups based on formation tops, and the Pearson product-moment correlation coefficient (PCC) was calculated for each \"Group\" from each of the GR, SP, and SR logs. The highest PCC-valued log curves for each Group were then combined to produce a single log whose values were cross-plotted against the reference well's sonic ITT values to determine a linear transform for producing a pseudo-sonic (PS) log and, ultimately, a pseudo-synthetic seismogram. The range for the Nash-Sutcliffe efficiency (NSE) acceptable value for the pseudo-sonic logs of three wells was 78-83%. This technique was tested on three wells, one of which was used as a blind test well, with satisfactory results. The PCC value between the composite PS (SR) log with low-density correction and the conventional sonic (CS) log was 86%. Because of the common occurrence of spontaneous potential and gamma ray logs in many of the hydrocarbon basins of the world, this inexpensive and straightforward technique could hold significant promise in areas that are in need of alternate ways to create pseudo-synthetic seismograms for seismic reflection interpretation.\n\njSynthesizer: A Java based first-motion synthetic seismogram tool\n\nNASA Astrophysics Data System (ADS)\n\nSullivan, Mark\n\n2009-10-01\n\nBoth researchers and educators need software tools to create synthetic seismograms to model earthquake sources. We have developed a program that generates first-motion synthetic seismograms that is highly interactive and suited to the needs of both research and education audiences. Implemented in the Java programming language, our program is available for use on Windows, Mac OS X and Linux operating systems. Our program allows the user to input the fault parameters strike, dip and slip angle, numerically or graphically using a lower hemisphere equal-area stereographic projection of the focal sphere of the earthquake. This representation is familiar to geologists and seismologists as the standard way of displaying the orientation of a fault in space. The user is also able to enter the relative location of the seismograph and the depth and crustal velocity structure in the vicinity of the earthquake. The direct P wave along with reflections off of layer boundaries near the source are generated using a constant ray-parameter approximation. The instrument response functions used by the Worldwide Standardized Seismogram Network and the attenuation response of the Earth's mantle are generated in the frequency domain and applied to generate the synthetic seismogram. Planned enhancements to this program will allow the simultaneous generation of seismograms at many stations as well as more complicated crustal structures.\n\nVerification Modal Summation Technique for Synthetic and Observation Seismogram for Pidie Jaya Earthquake M6.5\n\nNASA Astrophysics Data System (ADS)\n\nIrwandi, Irwandi; Fashbir; Daryono\n\n2018-04-01\n\nNeo-Deterministic Seismic Hazard Assessment (NDSHA) method is a seismic hazard assessment method that has an advantage on realistic physical simulation of the source, propagation, and geological-geophysical structure. This simulation is capable on generating the synthetics seismograms at the sites that being observed. At the regional NDSHA scale, calculation of the strong ground motion is based on 1D modal summation technique because it is more efficient in computation. In this article, we verify the result of synthetic seismogram calculations with the result of field observations when Pidie Jaya earthquake on 7 December 2016 occurred with the moment magnitude of M6.5. Those data were recorded by broadband seismometers installed by BMKG (Indonesian Agency for Meteorology, Climatology and Geophysics). The result of the synthetic seismogram calculations verifies that some stations well show the suitability with observation while some other stations show the discrepancies with observation results. Based on the results of the observation of some stations, evidently 1D modal summation technique method has been well verified for thin sediment region (near the pre-tertiary basement), but less suitable for thick sediment region. The reason is that the 1D modal summation technique excludes the amplification effect of seismic wave occurring within thick sediment region. So, another approach is needed, e.g., 2D finite difference hybrid method, which is a part of local scale NDSHA method.\n\nYASEIS: Yet Another computer program to calculate synthetic SEISmograms for a spherically multi-layered Earth model\n\nNASA Astrophysics Data System (ADS)\n\nMa, Yanlu\n\n2013-04-01\n\nAlthough most researches focus on the lateral heterogeneity of 3D Earth nowadays, a spherically multi-layered model where the parameters depend only on depth still represents a good first order approximation of real Earth. Such 1D models could be used as starting models for seismic tomographic inversion or as background model where the source mechanisms are inverted. The problem of wave propagation in a spherically layered model had been solved theoretically long time ago (Takeuchi and Saito, 1972). The existing computer programs such as Mineos (developed by G. Master, J. Woodhouse and F. Gilbert), Gemini (Friederich and Dalkolmo 1995), DSM (Kawai et. al. 2006) and QSSP (Wang 1999) tackled the computational aspects of the problem. A new simple and fast program for computing the Green's function of a stack of spherical dissipative layers is presented here. The analytical solutions within each homogeneous spherical layer are joined through the continuous boundary conditions and propagated from the center of model up to the level of source depth. Another solution is built by propagating downwardly from the free surface of model to the source level. The final solution is then constructed in frequency domain from the previous two solutions to satisfy the discontinuities of displacements and stresses at the source level which are required by the focal mechanism. The numerical instability in the propagator approach is solved by complementing the matrix propagating with an orthonormalization procedure (Wang 1999). Another unstable difficulty due to the high attenuation in the upper mantle low velocity zone is overcome by switching the bases of solutions from the spherical Bessel functions to the spherical Hankel functions when necessary. We compared the synthetic seismograms obtained from the new program YASEIS with those computed by Gemini and QSSP. In the range of near distances, the synthetics by a reflectivity code for the horizontally layers are also compared with\n\nSynthetic seismograms and spectral cycles on the Andvord and Schollaert Drifts: Antarctic Peninsula\n\nUSGS Publications Warehouse\n\nManley, P.L.; Brachfeld, S.\n\n2007-01-01\n\n(Schollaert Drift) and the mouth of Andvord Bay (Andvord Drift) has been examined using synthetic seismograms. The seismograms generated from the physical properties in jumbo piston cores taken at each of these drifts (28JPC and 18JPC respectively) show good agreement with the field seismic profiles when core disturbance is taken into consideration. Both cores suggest an under-sampling of up to 30% (or compaction) during coring. This leads to inaccuracy in the evaluation of past sedimentation rates and thus interpretations on these rates may be biased.\n\nHigh-frequency Born synthetic seismograms based on coupled normal modes\n\nUSGS Publications Warehouse\n\nPollitz, Fred F.\n\n2011-01-01\n\nHigh-frequency and full waveform synthetic seismograms on a 3-D laterally heterogeneous earth model are simulated using the theory of coupled normal modes. The set of coupled integral equations that describe the 3-D response are simplified into a set of uncoupled integral equations by using the Born approximation to calculate scattered wavefields and the pure-path approximation to modulate the phase of incident and scattered wavefields. This depends upon a decomposition of the aspherical structure into smooth and rough components. The uncoupled integral equations are discretized and solved in the frequency domain, and time domain results are obtained by inverse Fourier transform. Examples show the utility of the normal mode approach to synthesize the seismic wavefields resulting from interaction with a combination of rough and smooth structural heterogeneities. This approach is applied to an â¼4 Hz shallow crustal wave propagation around the site of the San Andreas Fault Observatory at Depth (SAFOD).\n\nHigh-frequency Born synthetic seismograms based on coupled normal modes\n\nUSGS Publications Warehouse\n\nPollitz, F.\n\n2011-01-01\n\nHigh-frequency and full waveform synthetic seismograms on a 3-D laterally heterogeneous earth model are simulated using the theory of coupled normal modes. The set of coupled integral equations that describe the 3-D response are simplified into a set of uncoupled integral equations by using the Born approximation to calculate scattered wavefields and the pure-path approximation to modulate the phase of incident and scattered wavefields. This depends upon a decomposition of the aspherical structure into smooth and rough components. The uncoupled integral equations are discretized and solved in the frequency domain, and time domain results are obtained by inverse Fourier transform. Examples show the utility of the normal mode approach to synthesize the seismic wavefields resulting from interaction with a combination of rough and smooth structural heterogeneities. This approach is applied to an ~4 Hz shallow crustal wave propagation around the site of the San Andreas Fault Observatory at Depth (SAFOD). ?? The Author Geophysical Journal International ?? 2011 RAS.\n\nLarge seismic source imaging from old analogue seismograms\n\nNASA Astrophysics Data System (ADS)\n\nCaldeira, Bento; Buforn, Elisa; Borges, JosÃ©; Bezzeghoud, Mourad\n\n2017-04-01\n\nIn this work we present a procedure to recover the ground motions by a proper digital structure, from old seismograms in analogue physical support (paper or microfilm) to study the source rupture process, by application of modern finite source inversion tools. Despite the quality that the analog data and the digitizing technologies available may have, recover the ground motions with the accurate metrics from old seismograms, is often an intricate procedure. Frequently the general parameters of the analogue instruments response that allow recover the shape of the ground motions (free periods and damping) are known, but the magnification that allow recover the metric of these motions is dubious. It is in these situations that the procedure applies. The procedure is based on assign of the moment magnitude value to the integral of the apparent Source Time Function (STF), estimated by deconvolution of a synthetic elementary seismogram from the related observed seismogram, corrected with an instrument response affected by improper magnification. Two delicate issues in the process are 1) the calculus of the synthetic elementary seismograms that must consider later phases if applied to large earthquakes (the portions of signal should be 3 or 4 times larger than the rupture time) and 2) the deconvolution to calculate the apparent STF. In present version of the procedure was used the Direct Solution Method to compute the elementary seismograms and the deconvolution was processed in time domain by an iterative algorithm that allow constrains the STF to stay positive and time limited. The method was examined using synthetic data to test the accuracy and robustness. Finally, a set of 17 real old analog seismograms from the Santa Maria (Azores) 1939 earthquake (Mw=7.1) was used in order to recover the waveforms in the required digital structure, from which by inversion allows compute the finite source rupture model (slip distribution). Acknowledgements: This work is co\n\nSynthetic seismograms from vibracores: A case study in correlating the late quaternary seismic stratigraphy of the New Jersey inner continental shelf\n\nUSGS Publications Warehouse\n\nEsker, D.; Sheridan, R.E.; Ashley, G.M.; Waldner, J.S.; Hall, D.W.\n\n1996-01-01\n\nA new technique, using empirical relationships between median grain size and density and velocity to calculate proxy values for density and velocity, avoids many of the problems associated with the use of well logs and shipboard measurements to construct synthetic seismograms. This method was used to groundtruth and correlate across both analog and digital shallow high-resolution seismic data on the New Jersey shelf. Sampling dry vibracores to determine median grain size eliminates the detrimental effects that coring disturbances and preservation variables have on the sediment and water content of the core. The link between seismic response to lithology and bed spacing is more exact. The exact frequency of the field seismic data can be realistically simulated by a 10-20 cm sampling interval of the vibracores. The estimate of the percentage error inherent in this technique, 12% for acoustic impedance and 24% for reflection amplitude, is calculated to one standard deviation and is within a reasonable limit for such a procedure. The synthetic seismograms of two cores, 4-6 m long, were used to correlate specific sedimentary deposits to specific seismic reflection responses. Because this technique is applicable to unconsolidated sediments, it is ideal for upper Pleistocene and Holocene strata. Copyright ?? 1996, SEPM (Society for Sedimentary Geology).\n\nSynthetic Seismograms Derived from Oceanographic Data in the Campeche Canyon, Deepwater Gulf of Mexico\n\nNASA Astrophysics Data System (ADS)\n\nGonzalez-Orduno, A.; Fucugauchi, J. U.; Monreal, M.; Perez-Cruz, G.; Salas de LeÃ³n, D. A.\n\n2013-05-01\n\nThe seismic reflection method has been successfully applied worldwide to investigate subsurface conditions to support important business decisions in the oil industry. When applied in the marine environment, useful reflection information is limited to events on and below the sea floor; Information from the water column, if any, is disregarded. Seismic oceanography is emerging as a new technique that utilize the reflection information within the water column to infer thermal-density contrasts associated with oceanographic processes, such as cyclonic-anticyclonic eddies, ascending-descending water flows, and water flows related to rapid topographic changes on the sea floor. A seismic investigation to infer such oceanographic changes in one sector of the Campeche Canyon is in progress as a research matter at the Instituto de Ciencias del Mar y Limnologia from the University of Mexico (UNAM). First steps of the investigation consisted of creating synthetic seismograms based on oceanographic information (temperature and density) derived from direct observation on a series of close spaced depth points along vertical profiles. Details of the selected algorithms used for the transformation of the oceanographic data to acoustic impedances data sets and further construction of synthetic seismograms on each site and their representation as synthetic seismic sections, are presented in this work, as well as the road ahead in the investigation.\n\nComplete synthetic seismograms based on a spherical self-gravitating Earth model with an atmosphere-ocean-mantle-core structure\n\nNASA Astrophysics Data System (ADS)\n\nWang, Rongjiang; Heimann, Sebastian; Zhang, Yong; Wang, Hansheng; Dahm, Torsten\n\n2017-09-01\n\nA hybrid method is proposed to calculate complete synthetic seismograms based on a spherically symmetric and self-gravitating Earth with a multilayered structure of atmosphere, ocean, mantle, liquid core and solid core. For large wavelengths, a numerical scheme is used to solve the geodynamic boundary-value problem without any approximation on the deformation and gravity coupling. With decreasing wavelength, the gravity effect on the deformation becomes negligible and the analytical propagator scheme can be used. Many useful approaches are used to overcome the numerical problems that may arise in both analytical and numerical schemes. Some of these approaches have been established in the seismological community and the others are developed for the first time. Based on the stable and efficient hybrid algorithm, an all-in-one code QSSP is implemented to cover the complete spectrum of seismological interests. The performance of the code is demonstrated by various tests including the curvature effect on teleseismic body and surface waves, the appearance of multiple reflected, teleseismic core phases, the gravity effect on long period surface waves and free oscillations, the simulation of near-field displacement seismograms with the static offset, the coupling of tsunami and infrasound waves, and free oscillations of the solid Earth, the atmosphere and the ocean. QSSP is open source software that can be used as a stand-alone FORTRAN code or may be applied in combination with a Python toolbox to calculate and handle Green's function databases for efficient coding of source inversion problems.\n\nComplete synthetic seismograms based on a spherical self-gravitating Earth model with an atmosphere-ocean-mantle-core structure\n\nNASA Astrophysics Data System (ADS)\n\nWang, Rongjiang; Heimann, Sebastian; Zhang, Yong; Wang, Hansheng; Dahm, Torsten\n\n2017-04-01\n\nA hybrid method is proposed to calculate complete synthetic seismograms based on a spherically symmetric and self-gravitating Earth with a multi-layered structure of atmosphere, ocean, mantle, liquid core and solid core. For large wavelengths, a numerical scheme is used to solve the geodynamic boundary-value problem without any approximation on the deformation and gravity coupling. With the decreasing wavelength, the gravity effect on the deformation becomes negligible and the analytical propagator scheme can be used. Many useful approaches are used to overcome the numerical problems that may arise in both analytical and numerical schemes. Some of these approaches have been established in the seismological community and the others are developed for the first time. Based on the stable and efficient hybrid algorithm, an all-in-one code QSSP is implemented to cover the complete spectrum of seismological interests. The performance of the code is demonstrated by various tests including the curvature effect on teleseismic body and surface waves, the appearance of multiple reflected, teleseismic core phases, the gravity effect on long period surface waves and free oscillations, the simulation of near-field displacement seismograms with the static offset, the coupling of tsunami and infrasound waves, and free oscillations of the solid Earth, the atmosphere and the ocean. QSSP is open source software that can be used as a stand-alone FORTRAN code or may be applied in combination with a Python toolbox to calculate and handle Green's function databases for efficient coding of source inversion problems.\n\nA method to calculate synthetic waveforms in stratified VTI media\n\nNASA Astrophysics Data System (ADS)\n\nWang, W.; Wen, L.\n\n2012-12-01\n\nTransverse isotropy with a vertical axis of symmetry (VTI) may be an important material property in the Earth's interior. In this presentation, we develop a method to calculate synthetic seismograms for wave propagation in stratified VTI media. Our method is based on the generalized reflection and transmission method (GRTM) (Luco & Apsel 1983). We extend it to transversely isotropic VTI media. GRTM has the advantage of remaining stable in high frequency calculations compared to the Haskell Matrix method (Haskell 1964), which explicitly excludes the exponential growth terms in the propagation matrix and is limited to low frequency computation. In the implementation, we also improve GRTM in two aspects. 1) We apply the Shanks transformation (Bender & Orszag 1999) to improve the convergence rate of convergence. This improvement is especially important when the depths of source and receiver are close. 2) We adopt a self-adaptive Simpson integration method (Chen & Zhang 2001) in the discrete wavenumber integration so that the integration can still be efficiently carried out at large epicentral distances. Because the calculation is independent in each frequency, the program can also be effectively implemented in parallel computing. Our method provides a powerful tool to synthesize broadband seismograms of VIT media at a large epicenter distance range. We will present examples of using the method to study possible transverse isotropy in the upper mantle and the lowermost mantle.\n\nA constant stress-drop model for producing broadband synthetic seismograms: Comparison with the next generation attenuation relations\n\nUSGS Publications Warehouse\n\nFrankel, A.\n\n2009-01-01\n\nBroadband (0.1-20 Hz) synthetic seismograms for finite-fault sources were produced for a model where stress drop is constant with seismic moment to see if they can match the magnitude dependence and distance decay of response spectral amplitudes found in the Next Generation Attenuation (NGA) relations recently developed from strong-motion data of crustal earthquakes in tectonically active regions. The broadband synthetics were constructed for earthquakes of M 5.5, 6.5, and 7.5 by combining deterministic synthetics for plane-layered models at low frequencies with stochastic synthetics at high frequencies. The stochastic portion used a source model where the Brune stress drop of 100 bars is constant with seismic moment. The deterministic synthetics were calculated using an average slip velocity, and hence, dynamic stress drop, on the fault that is uniform with magnitude. One novel aspect of this procedure is that the transition frequency between the deterministic and stochastic portions varied with magnitude, so that the transition frequency is inversely related to the rise time of slip on the fault. The spectral accelerations at 0.2, 1.0, and 3.0 sec periods from the synthetics generally agreed with those from the set of NGA relations for M 5.5-7.5 for distances of 2-100 km. At distances of 100-200 km some of the NGA relations for 0.2 sec spectral acceleration were substantially larger than the values of the synthetics for M 7.5 and M 6.5 earthquakes because these relations do not have a term accounting for Q. At 3 and 5 sec periods, the synthetics for M 7.5 earthquakes generally had larger spectral accelerations than the NGA relations, although there was large scatter in the results from the synthetics. The synthetics showed a sag in response spectra at close-in distances for M 5.5 between 0.3 and 0.7 sec that is not predicted from the NGA relations.\n\nAnalytical computation of three-dimensional synthetic seismograms by Modal Summation: method, validation and applications\n\nNASA Astrophysics Data System (ADS)\n\nLa Mura, Cristina; Gholami, Vahid; Panza, Giuliano F.\n\n2013-04-01\n\nIn order to enable realistic and reliable earthquake hazard assessment and reliable estimation of the ground motion response to an earthquake, three-dimensional velocity models have to be considered. The propagation of seismic waves in complex laterally varying 3D layered structures is a complicated process. Analytical solutions of the elastodynamic equations for such types of media are not known. The most common approaches to the formal description of seismic wavefields in such complex structures are methods based on direct numerical solutions of the elastodynamic equations, e.g. finite-difference, finite-element method, and approximate asymptotic methods. In this work, we present an innovative methodology for computing synthetic seismograms, complete of the main direct, refracted, converted phases and surface waves in three-dimensional anelastic models based on the combination of the Modal Summation technique with the Asymptotic Ray Theory in the framework of the WKBJ - approximation. The three - dimensional models are constructed using a set of vertically heterogeneous sections (1D structures) that are juxtaposed on a regular grid. The distribution of these sections in the grid is done in such a way to fulfill the requirement of weak lateral inhomogeneity in order to satisfy the condition of applicability of the WKBJ - approximation, i.e. the lateral gradient of the parameters characterizing the 1D structure has to be small with respect to the prevailing wavelength. The new method has been validated comparing synthetic seismograms with the records available of three different earthquakes in three different regions: Kanto basin (Japan) triggered by the 1990 Odawara earthquake Mw= 5.1, Romanian territory triggered by the 30 May 1990 Vrancea intermediate-depth earthquake Mw= 6.9 and Iranian territory affected by the 26 December 2003 Bam earthquake Mw= 6.6. Besides the advantage of being a useful tool for assessment of seismic hazard and seismic risk reduction, it\n\nGenerating Seismograms with Deep Neural Networks\n\nNASA Astrophysics Data System (ADS)\n\nKrischer, L.; Fichtner, A.\n\n2017-12-01\n\nThe recent surge of successful uses of deep neural networks in computer vision, speech recognition, and natural language processing, mainly enabled by the availability of fast GPUs and extremely large data sets, is starting to see many applications across all natural sciences. In seismology these are largely confined to classification and discrimination tasks. In this contribution we explore the use of deep neural networks for another class of problems: so called generative models.Generative modelling is a branch of statistics concerned with generating new observed data samples, usually by drawing from some underlying probability distribution. Samples with specific attributes can be generated by conditioning on input variables. In this work we condition on seismic source (mechanism and location) and receiver (location) parameters to generate multi-component seismograms.The deep neural networks are trained on synthetic data calculated with Instaseis (http://instaseis.net, van Driel et al. (2015)) and waveforms from the global ShakeMovie project (http://global.shakemovie.princeton.edu, Tromp et al. (2010)). The underlying radially symmetric or smoothly three dimensional Earth structures result in comparatively small waveform differences from similar events or at close receivers and the networks learn to interpolate between training data samples.Of particular importance is the chosen misfit functional. Generative adversarial networks (Goodfellow et al. (2014)) implement a system in which two networks compete: the generator network creates samples and the discriminator network distinguishes these from the true training examples. Both are trained in an adversarial fashion until the discriminator can no longer distinguish between generated and real samples. We show how this can be applied to seismograms and in particular how it compares to networks trained with more conventional misfit metrics. Last but not least we attempt to shed some light on the black-box nature of\n\nToward 2D Seismic Wavefield Monitoring: Seismic Gradiometry for Long-Period Seismogram and Short-Period Seismogram Envelope applied to the Hi-net Array\n\nNASA Astrophysics Data System (ADS)\n\nMaeda, T.; Nishida, K.; Takagi, R.; Obara, K.\n\n2015-12-01\n\nThe high-sensitive seismograph network Japan (Hi-net) operated by National Research Institute for Earth Science and Disaster Prevention (NIED) has about 800 stations with average separation of 20 km. We can observe long-period seismic wave propagation as a 2D wavefield with station separations shorter than wavelength. In contrast, short-period waves are quite incoherent at stations, however, their envelope shapes resemble at neighbor stations. Therefore, we may be able to extract seismic wave energy propagation by seismogram envelope analysis. We attempted to characterize seismic waveform at long-period and its envelope at short-period as 2D wavefield by applying seismic gradiometry. We applied the seismic gradiometry to a synthetic long-period (20-50s) dataset prepared by numerical simulation in realistic 3D medium at the Hi-net station layout. Wave amplitude and its spatial derivatives are estimated by using data at nearby stations. The slowness vector, the radiation pattern and the geometrical spreading are extracted from estimated velocity, displacement and its spatial derivatives. For short-periods at shorter than 1 s, seismogram envelope shows temporal and spatial broadening through scattering by medium heterogeneity. It is expected that envelope shape may be coherent among nearby stations. Based on this idea, we applied the same method to the time-integration of seismogram envelope to estimate its spatial derivatives. Together with seismogram envelope, we succeeded in estimating the slowness vector from the seismogram envelope as well as long-period waveforms by synthetic test, without using phase information. Our preliminarily results show that the seismic gradiometry suits the Hi-net to extract wave propagation characteristics both at long and short periods. This method is appealing that it can estimate waves at homogeneous grid to monitor seismic wave as a wavefield. It is promising to obtain phase velocity variation from direct waves, and to grasp wave\n\nA hybrid method for the computation of quasi-3D seismograms.\n\nNASA Astrophysics Data System (ADS)\n\nMasson, Yder; Romanowicz, Barbara\n\n2013-04-01\n\nThe development of powerful computer clusters and efficient numerical computation methods, such as the Spectral Element Method (SEM) made possible the computation of seismic wave propagation in a heterogeneous 3D earth. However, the cost of theses computations is still problematic for global scale tomography that requires hundreds of such simulations. Part of the ongoing research effort is dedicated to the development of faster modeling methods based on the spectral element method. Capdeville et al. (2002) proposed to couple SEM simulations with normal modes calculation (C-SEM). Nissen-Meyer et al. (2007) used 2D SEM simulations to compute 3D seismograms in a 1D earth model. Thanks to these developments, and for the first time, Lekic et al. (2011) developed a 3D global model of the upper mantle using SEM simulations. At the local and continental scale, adjoint tomography that is using a lot of SEM simulation can be implemented on current computers (Tape, Liu et al. 2009). Due to their smaller size, these models offer higher resolution. They provide us with images of the crust and the upper part of the mantle. In an attempt to teleport such local adjoint tomographic inversions into the deep earth, we are developing a hybrid method where SEM computation are limited to a region of interest within the earth. That region can have an arbitrary shape and size. Outside this region, the seismic wavefield is extrapolated to obtain synthetic data at the Earth's surface. A key feature of the method is the use of a time reversal mirror to inject the wavefield induced by distant seismic source into the region of interest (Robertsson and Chapman 2000). We compute synthetic seismograms as follow: Inside the region of interest, we are using regional spectral element software RegSEM to compute wave propagation in 3D. Outside this region, the wavefield is extrapolated to the surface by convolution with the Green's functions from the mirror to the seismic stations. For now, these\n\nSmall-aperture seismic array data processing using a representation of seismograms at zero-crossing points\n\nNASA Astrophysics Data System (ADS)\n\nBrokeÅ¡ovÃ¡, Johana; MÃ¡lek, JiÅÃ­\n\n2018-07-01\n\nA new method for representing seismograms by using zero-crossing points is described. This method is based on decomposing a seismogram into a set of quasi-harmonic components and, subsequently, on determining the precise zero-crossing times of these components. An analogous approach can be applied to determine extreme points that represent the zero-crossings of the first time derivative of the quasi-harmonics. Such zero-crossing and/or extreme point seismogram representation can be used successfully to reconstruct single-station seismograms, but the main application is to small-aperture array data analysis to which standard methods cannot be applied. The precise times of the zero-crossing and/or extreme points make it possible to determine precise time differences across the array used to retrieve the parameters of a plane wave propagating across the array, namely, its backazimuth and apparent phase velocity along the Earth's surface. The applicability of this method is demonstrated using two synthetic examples. In the real-data example from the PÅÃ­bram-HÃ¡je array in central Bohemia (Czech Republic) for the Mw 6.4 Crete earthquake of October 12, 2013, this method is used to determine the phase velocity dispersion of both Rayleigh and Love waves. The resulting phase velocities are compared with those obtained by employing the seismic plane-wave rotation-to-translation relations. In this approach, the phase velocity is calculated by obtaining the amplitude ratios between the rotation and translation components. Seismic rotations are derived from the array data, for which the small aperture is not only an advantage but also an applicability condition.\n\nAn Interactive Program on Digitizing Historical Seismograms\n\nNASA Astrophysics Data System (ADS)\n\nXu, Y.; Xu, T.\n\n2013-12-01\n\nRetrieving information from historical seismograms is of great importance since they are considered the unique sources that provide quantitative information of historical earthquakes. Modern techniques of seismology require digital forms of seismograms that are essentially a sequence of time-amplitude pairs. However, the historical seismograms, after scanned into computers, are two dimensional arrays. Each element of the arrays contains the grayscale value or RGB value of the corresponding pixel. The problem of digitizing historical seismograms, referred to as converting historical seismograms to digital seismograms, can be formulated as an inverse problem that generating sequences of time-amplitude pairs from a two dimension arrays. This problem has infinite solutions. The algorithm for automatic digitization of historical seismogram presented considers several features of seismograms, including continuity, smoothness of the seismic traces as the prior information, and assumes that the amplitude is a single-valued function of time. An interactive program based on the algorithm is also presented. The program is developed using Matlab GUI and has both automatic and manual modality digitization. Users can easily switch between them, and try different combinations to get the optimal results. Several examples are given to illustrate the results of digitizing seismograms using the program, including a photographic record and a wide-angle reflection/refraction seismogram. Digitized result of the program (redrawn using Golden Software Surfer for high resolution image). (a) shows the result of automatic digitization, and (b) is the result after manual correction.\n\nConstructing new seismograms from old earthquakes: Retrospective seismology at multiple length scales\n\nNASA Astrophysics Data System (ADS)\n\nEntwistle, Elizabeth; Curtis, Andrew; Galetti, Erica; Baptie, Brian; Meles, Giovanni\n\n2015-04-01\n\nIf energy emitted by a seismic source such as an earthquake is recorded on a suitable backbone array of seismometers, source-receiver interferometry (SRI) is a method that allows those recordings to be projected to the location of another target seismometer, providing an estimate of the seismogram that would have been recorded at that location. Since the other seismometer may not have been deployed at the time the source occurred, this renders possible the concept of 'retrospective seismology' whereby the installation of a sensor at one period of time allows the construction of virtual seismograms as though that sensor had been active before or after its period of installation. Using the benefit of hindsight of earthquake location or magnitude estimates, SRI can establish new measurement capabilities closer to earthquake epicenters, thus potentially improving earthquake location estimates. Recently we showed that virtual SRI seismograms can be constructed on target sensors in both industrial seismic and earthquake seismology settings, using both active seismic sources and ambient seismic noise to construct SRI propagators, and on length scales ranging over 5 orders of magnitude from ~40 m to ~2500 km[1]. Here we present the results from earthquake seismology by comparing virtual earthquake seismograms constructed at target sensors by SRI to those actually recorded on the same sensors. We show that spatial integrations required by interferometric theory can be calculated over irregular receiver arrays by embedding these arrays within 2D spatial Voronoi cells, thus improving spatial interpolation and interferometric results. The results of SRI are significantly improved by restricting the backbone receiver array to include approximately those receivers that provide a stationary phase contribution to the interferometric integrals. We apply both correlation-correlation and correlation-convolution SRI, and show that the latter constructs virtual seismograms with fewer\n\nDigitized Database of Old Seismograms Recorder in Romania\n\nNASA Astrophysics Data System (ADS)\n\nPaulescu, Daniel; Rogozea, Maria; Popa, Mihaela; Radulian, Mircea\n\n2016-08-01\n\nThe aim of this paper is to describe a managing system for a unique Romanian database of historical seismograms and complementary documentation (metadata) and its dissemination and analysis procedure. For this study, 5188 historical seismograms recorded between 1903 and 1957 by the Romanian seismological observatories (Bucharest-Filaret, FocÅani, BacÄu, Vrincioaia, CÃ¢mpulung-Muscel, IaÅi) were used. In order to reconsider the historical instrumental data, the analog seismograms are converted to digital images and digital waveforms (digitization/ vectorialisation). First, we applied a careful scanning procedure of the seismograms and related material (seismic bulletins, station books, etc.). In a next step, the high resolution scanned seismograms will be processed to obtain the digital/numeric waveforms. We used a Colortrac Smartlf Cx40 scanner which provides images in TIFF or JPG format. For digitization the algorithm Teseo2 developed by the National Institute of Geophysics and Volcanology in Rome (Italy), within the framework of the SISMOS Project, will be used.\n\nMethod for determining formation quality factor from well log data and its application to seismic reservoir characterization\n\nDOEpatents\n\nWalls, Joel; Taner, M. Turhan; Dvorkin, Jack\n\n2006-08-08\n\nA method for seismic characterization of subsurface Earth formations includes determining at least one of compressional velocity and shear velocity, and determining reservoir parameters of subsurface Earth formations, at least including density, from data obtained from a wellbore penetrating the formations. A quality factor for the subsurface formations is calculated from the velocity, the density and the water saturation. A synthetic seismogram is calculated from the calculated quality factor and from the velocity and density. The synthetic seismogram is compared to a seismic survey made in the vicinity of the wellbore. At least one parameter is adjusted. The synthetic seismogram is recalculated using the adjusted parameter, and the adjusting, recalculating and comparing are repeated until a difference between the synthetic seismogram and the seismic survey falls below a selected threshold.\n\nSeismograms live from around the world\n\nUSGS Publications Warehouse\n\nWoodward, Robert L.; Shedlock, Kaye M.; Bolton, Harold F.\n\n1999-01-01\n\nYou can view earthquakes as they happen! Seismograms from seismic stations around the world are broadcast live, via the Internet, and are updated every 30 minutes, With an Internet connection and a web browser, you can view current seismograms and earthquake locations on your own computer. With special software also available via the Internet, you can obtain seismic data as it arrives from a global network of seismograph stations.\n\nAn Efficient Numerical Method for Computing Synthetic Seismograms for a Layered Half-space with Sources and Receivers at Close or Same Depths\n\nNASA Astrophysics Data System (ADS)\n\nZhang, H.-m.; Chen, X.-f.; Chang, S.\n\n- It is difficult to compute synthetic seismograms for a layered half-space with sources and receivers at close to or the same depths using the generalized R/T coefficient method (Kennett, 1983; Luco and Apsel, 1983; Yao and Harkrider, 1983; Chen, 1993), because the wavenumber integration converges very slowly. A semi-analytic method for accelerating the convergence, in which part of the integration is implemented analytically, was adopted by some authors (Apsel and Luco, 1983; Hisada, 1994, 1995). In this study, based on the principle of the Repeated Averaging Method (Dahlquist and BjÃ¶rck, 1974; Chang, 1988), we propose an alternative, efficient, numerical method, the peak-trough averaging method (PTAM), to overcome the difficulty mentioned above. Compared with the semi-analytic method, PTAM is not only much simpler mathematically and easier to implement in practice, but also more efficient. Using numerical examples, we illustrate the validity, accuracy and efficiency of the new method.\n\nA MultiÂ­Discipline Approach to Digitizing Historic Seismograms\n\nDOE Office of Scientific and Technical Information (OSTI.GOV)\n\nBartlett, Andrew\n\n2016-04-07\n\nRetriever Technology has developed and has made available free of charge a seismogram digitization software package called SKATE (Seismogram Kit for Automatic Trace Extraction). We have developed an extensive set of algorithms that process seismogram image files, provide editing tools, and output time series data. The software is available online and free of charge at seismo.redfish.com. To demonstrate the speed and cost effectiveness of the software, we have processed over 30,000 images.\n\nStreaming Seismograms into Earth-Science Classrooms\n\nNASA Astrophysics Data System (ADS)\n\nAmmon, C. J.\n\n2011-12-01\n\nSeismograms are the fundamental observations upon which seismology is based; they are central to any course in seismology and important for any discussion of earthquake-related phenomena based on seismic observations. Advances in the collection and distribution of seismic data have made the use of research-quality seismograms in any network capable classroom feasible. The development of large, deep seismogram archives place an unprecedented quantity of high-quality data within reach of the modern classroom environment. I describe and discuss several computer tools and classroom activities that I use in introductory (general education) and advanced undergraduate courses that present near real-time research-quality seismic observations in the classroom. The Earth Motion Monitor Application (EMMA), is a MacOS application that presents a visually clear seismogram display that can be projected in classrooms with internet access. Seismic signals from thousands of station are available from the IRIS data center and the bandwidth can be tailored to the particular type of signal of interest (large event, low frequencies; small event, high frequencies). In introductory classes for non-science students, the near realtime display routinely shows magnitude 4.0-5.0 earthquake-generated signals, demonstrating to students the frequency of earthquake occurrence. Over the next few minutes as the waves travel through and across the planet, their arrival on the seismogram display provides some basic data for a qualitative estimate of the event's general location. When a major or great earthquake occurs, a broad-band display of signals from nearby stations can dramatically and dynamically illuminate the frequent activity associated with the aftershock sequence. Routine use of the display (while continuing the traditional classroom activities) provides students with a significant dose of seismogram study. Students generally find all the signals, including variations in seismic\n\nClass Room Exercises Using JMA-59-Type Seismograms for Earthquake Study at High-School Level\n\nNASA Astrophysics Data System (ADS)\n\nOkamoto, Y.; Furuta, S.; Hirota, N.\n\n2013-12-01\n\nThe JMA-59-type electromagnetic seismograph was the standard seismograph for routine observations by the Japan Meteorological Agency (JMA) from the 1960's to the 1990's. Some features of those seismograms include 1) displacement wave records (electrically integrated from a velocity output by a moving-coil-type sensor), 2) ink records on paper (analog recording with time marks), 3) continuous drum recording for 12 h, and 4) lengthy operation time over several decades. However, the digital revolution in recording systems during the 1990's made these analog features obsolete, and their abundant and bulky paper-based records were stacked and sometimes disregarded in the library of every observatory. Interestingly, from an educational aspect, the disadvantages of these old-fashioned systems become highly advantageous for educational or outreach purposes. The updated digital instrument is essentially a 'black-box,' not revealing its internal mechanisms and being too fast for observing its signal processes. While the old seismometers and recording systems have been disposed of long since, stacks of analog seismograms continue to languish in observatories' back rooms. In our study, we develop some classroom exercises for studying earthquakes at the mid- to high-school level using these analog seismograms. These exercises include 1) reading the features of seismic records, 2) measuring the S-P time, 3) converting the hypocentral distance from Omori's distance formula, 4) locating the epicenter/hypocenter using the S-P times of surrounding stations, and 5) estimating earthquake magnitude using the Tsuboi's magnitude formula. For this calculation we developed a 'nomogram'--a graphical paper calculator created using a Python-based freeware tool named 'PyNomo.' We tested many seismograms and established the following rules: 1) shallow earthquakes are appropriate for using the Tsuboi's magnitude formula; 2) there is no saturation at peak amplitude; 3) seismograms make it easy to\n\nAn interactive program on digitizing historical seismograms\n\nNASA Astrophysics Data System (ADS)\n\nXu, Yihe; Xu, Tao\n\n2014-02-01\n\nRetrieving information from analog seismograms is of great importance since they are considered as the unique sources that provide quantitative information of historical earthquakes. We present an algorithm for automatic digitization of the seismograms as an inversion problem that forms an interactive program using MatlabÂ® GUI. The program integrates automatic digitization with manual digitization and users can easily switch between the two modalities and carry out different combinations for the optimal results. Several examples about applying the interactive program are given to illustrate the merits of the method.\n\nAn Introduction to SPEAR (Seismogram Picking Error from Analyst Review)\n\nNASA Astrophysics Data System (ADS)\n\nZeiler, C. P.; Velasco, A. A.; Anderson, D.; Pingitore, N. E.\n\n2008-12-01\n\nA grassroots initiative began in February of 2008 at the University of Texas at El Paso to understand how seismologists measure earthquakes. The Seismogram Picking Error from Analyst Review (SPEAR) project is designed to be a forum where seismologists can propose, discuss and experimentally test theories on proper procedures to identify and measure seismic phases. We outline the history of seismogram analysis and explore areas of seismogram analysis that still need to be defined. The main concern for SPEAR, at this time, is the impact of picking errors produced by merging earthquake catalogs. Our initial effort has been to establish a common data set for seismologists to pick. The preliminary studies from this data set have shown that significant bias between authors of catalogs may exist. We provide techniques to ensure that these biases can be identified and correctly managed to provide accurate mergers of earthquake measurements. The overall goal of SPEAR is to provide a repository of information to aid seismologists in comparing and sharing measurements. We want to document in the repository and explore all aspects of the picking process, from the basics of learning how to read a seismogram to complex transformations and enhancements of signals. Your participation in SPEAR will aid the seismological community to close the knowledge gaps that exist in seismogram analysis.\n\nDetailed seismic velocity structure of the ultra-slow spread crust at the Mid-Cayman Spreading Center from travel-time tomography and synthetic seismograms\n\nNASA Astrophysics Data System (ADS)\n\nHarding, J.; Van Avendonk, H. J.; Hayman, N. W.; Grevemeyer, I.; Peirce, C.\n\n2017-12-01\n\nThe Mid-Cayman Spreading Center (MCSC), an ultraslow-spreading center in the Caribbean Sea, has formed highly variable oceanic crust. Seafloor dredges have recovered extrusive basalts in the axial deeps as well as gabbro on bathymetric highs and exhumed mantle peridotite along the only 110 km MCSC. Wide-angle refraction data were collected with active-source ocean bottom seismometers in April, 2015, along lines parallel and across the MCSC. Travel-time tomography produces relatively smooth 2-D tomographic models of compressional wave velocity. These velocity models reveal large along- and across-axis variations in seismic velocity, indicating possible changes in crustal thickness, composition, faulting, and magmatism. It is difficult, however, to differentiate between competing interpretations of seismic velocity using these tomographic models alone. For example, in some areas the seismic velocities may be explained by either thin igneous crust or exhumed, serpentinized mantle. Distinguishing between these two interpretations is important as we explore the relationships between magmatism, faulting, and hydrothermal venting at ultraslow-spreading centers. We therefore improved our constraints on the shallow seismic velocity structure of the MCSC by modeling the amplitude of seismic refractions in the wide-angle data set. Synthetic seismograms were calculated with a finite-difference method for a range of models with different vertical velocity gradients. Small-scale features in the velocity models, such as steep velocity gradients and Moho boundaries, were explored systematically to best fit the real data. With this approach, we have improved our understanding of the compressional velocity structure of the MCSC along with the geological interpretations that are consistent with three seismic refraction profiles. Line P01 shows a variation in the thinness of lower seismic velocities along the axis, indicating two segment centers, while across-axis lines P02 and P03\n\nMeasuring the misfit between seismograms using an optimal transport distance: application to full waveform inversion\n\nNASA Astrophysics Data System (ADS)\n\nMÃ©tivier, L.; Brossier, R.; MÃ©rigot, Q.; Oudet, E.; Virieux, J.\n\n2016-04-01\n\nFull waveform inversion using the conventional L2 distance to measure the misfit between seismograms is known to suffer from cycle skipping. An alternative strategy is proposed in this study, based on a measure of the misfit computed with an optimal transport distance. This measure allows to account for the lateral coherency of events within the seismograms, instead of considering each seismic trace independently, as is done generally in full waveform inversion. The computation of this optimal transport distance relies on a particular mathematical formulation allowing for the non-conservation of the total energy between seismograms. The numerical solution of the optimal transport problem is performed using proximal splitting techniques. Three synthetic case studies are investigated using this strategy: the Marmousi 2 model, the BP 2004 salt model, and the Chevron 2014 benchmark data. The results emphasize interesting properties of the optimal transport distance. The associated misfit function is less prone to cycle skipping. A workflow is designed to reconstruct accurately the salt structures in the BP 2004 model, starting from an initial model containing no information about these structures. A high-resolution P-wave velocity estimation is built from the Chevron 2014 benchmark data, following a frequency continuation strategy. This estimation explains accurately the data. Using the same workflow, full waveform inversion based on the L2 distance converges towards a local minimum. These results yield encouraging perspectives regarding the use of the optimal transport distance for full waveform inversion: the sensitivity to the accuracy of the initial model is reduced, the reconstruction of complex salt structure is made possible, the method is robust to noise, and the interpretation of seismic data dominated by reflections is enhanced.\n\nThe source mechanisms of low frequency events in volcanoes - a comparison of synthetic and real seismic data on Soufriere Hills Volcano, Montserrat\n\nNASA Astrophysics Data System (ADS)\n\nKarl, S.; Neuberg, J. W.\n\n2012-04-01\n\nLow frequency seismic signals are one class of volcano seismic earthquakes that have been observed at many volcanoes around the world, and are thought to be associated with resonating fluid-filled conduits or fluid movements. Amongst others, Neuberg et al. (2006) proposed a conceptual model for the trigger of low frequency events at Montserrat involving the brittle failure of magma in the glass transition in response to high shear stresses during the upwards movement of magma in the volcanic edifice. For this study, synthetic seismograms were generated following the proposed concept of Neuberg et al. (2006) by using an extended source modelled as an octagonal arrangement of double couples approximating a circular ringfault. For comparison, synthetic seismograms were generated using single forces only. For both scenarios, synthetic seismograms were generated using a seismic station distribution as encountered on Soufriere Hills Volcano, Montserrat. To gain a better quantitative understanding of the driving forces of low frequency events, inversions for the physical source mechanisms have become increasingly common. Therefore, we perform moment tensor inversions (Dreger, 2003) using the synthetic data as well as a chosen set of seismograms recorded on Soufriere Hills Volcano. The inversions are carried out under the (wrong) assumption to have an underlying point source rather than an extended source as the trigger mechanism of the low frequency seismic events. We will discuss differences between inversion results, and how to interpret the moment tensor components (double couple, isotropic, or CLVD), which were based on a point source, in terms of an extended source.\n\nStudying the Effects of Transparent vs. Opaque Shallow Thrust Faults Using Synthetic P and SH Seismograms\n\nNASA Astrophysics Data System (ADS)\n\nSmith, D. E.; Aagaard, B. T.; Heaton, T. H.\n\n2001-12-01\n\nIt has been hypothesized (Brune, 1996) that teleseismic inversions may underestimate the moment of shallow thrust fault earthquakes if energy becomes trapped in the hanging wall of the fault, i.e. if the fault boundary becomes opaque. We address this by creating and analyzing synthetic P and SH seismograms for a variety of friction models. There are a total of five models: (1) crack model (slip weakening) with instantaneous healing (2) crack model without healing (3) crack model with zero sliding friction (4) pulse model (slip and rate weakening) (5) prescribed model (Haskell-like rupture with the same final slip and peak slip-rate as model 4). Models 1-4 are all dynamic models where fault friction laws determine the rupture history. This allows feedback between the ongoing rupture and waves from the beginning of the rupture that hit the surface and reflect downwards. Hence, models 1-4 can exhibit opaque fault characteristics. Model 5, a prescribed rupture, allows for no interaction between the rupture and reflected waves, therefore, it is a transparent fault. We first produce source time functions for the different friction models by rupturing shallow thrust faults in 3-D dynamic finite-element simulations. The source time functions are used as point dislocations in a teleseismic body-wave code. We examine the P and SH waves for different azimuths and epicentral distances. The peak P and S first arrival displacement amplitudes for the crack, crack with healing and pulse models are all very similar. These dynamic models with opaque faults produce smaller peak P and S first arrivals than the prescribed, transparent fault. For example, a fault with strike = 90 degrees, azimuth = 45 degrees has P arrivals smaller by about 30% and S arrivals smaller by about 15%. The only dynamic model that doesn't fit this pattern is the crack model with zero sliding friction. It oscillates around its equilibrium position; therefore, it overshoots and yields an excessively large peak\n\nLine-source simulation for shallow-seismic data. Part 2: full-waveform inversionâa synthetic 2-D case study\n\nNASA Astrophysics Data System (ADS)\n\nSchÃ¤fer, M.; Groos, L.; Forbriger, T.; Bohlen, T.\n\n2014-09-01\n\nFull-waveform inversion (FWI) of shallow-seismic surface waves is able to reconstruct lateral variations of subsurface elastic properties. Line-source simulation for point-source data is required when applying algorithms of 2-D adjoint FWI to recorded shallow-seismic field data. The equivalent line-source response for point-source data can be obtained by convolving the waveforms with â{t^{-1}} (t: traveltime), which produces a phase shift of Ï/4. Subsequently an amplitude correction must be applied. In this work we recommend to scale the seismograms with â{2 r v_ph} at small receiver offsets r, where vph is the phase velocity, and gradually shift to applying a â{t^{-1}} time-domain taper and scaling the waveforms with râ{2} for larger receiver offsets r. We call this the hybrid transformation which is adapted for direct body and Rayleigh waves and demonstrate its outstanding performance on a 2-D heterogeneous structure. The fit of the phases as well as the amplitudes for all shot locations and components (vertical and radial) is excellent with respect to the reference line-source data. An approach for 1-D media based on Fourier-Bessel integral transformation generates strong artefacts for waves produced by 2-D structures. The theoretical background for both approaches is presented in a companion contribution. In the current contribution we study their performance when applied to waves propagating in a significantly 2-D-heterogeneous structure. We calculate synthetic seismograms for 2-D structure for line sources as well as point sources. Line-source simulations obtained from the point-source seismograms through different approaches are then compared to the corresponding line-source reference waveforms. Although being derived by approximation the hybrid transformation performs excellently except for explicitly back-scattered waves. In reconstruction tests we further invert point-source synthetic seismograms by a 2-D FWI to subsurface structure and evaluate\n\nThe May 18, 1998 Indian Nuclear Test Seismograms at station NIL\n\nDOE Office of Scientific and Technical Information (OSTI.GOV)\n\nWalter, W R; Rodgers, A J; Bowers, D\n\n2005-04-11\n\nThe last underground nuclear tests were conducted by India and Pakistan in May 1998. Although the Comprehensive Test Ban Treaty has not entered force, an International Monitoring System (IMS), established by the treaty is nearing completion. This system includes 170 seismic stations, a number of them originally established by IRIS. The station IRIS station NIL (Nilore, Pakistan) is close to a planned IMS primary station and recorded some very interesting seismograms from the May 18, 1998 Indian test. We carefully calibrated the path to NIL using a prior Mw 4.4 that occurred on April 4, 1995 about 110 km northmoreÂ Â» of the Indian test site. We used joint epicentral location techniques along with teleseismic P waves and regional surface waves to fix the epicenter, depth, mechanism and moment of this event. From these we obtained a velocity model for the path to NIL and created explosion synthetic seismograms to compare with the data. Interestingly the observed Rayleigh waves are reversed, consistent with an implosion rather than an explosion source. The preferred explanation is that the explosion released tectonic stress near the source region, which can be modeled as a thrust earthquake of approximate Mw 4.0 plus a pure explosion. This tectonic release is sufficient to completely dominate the Rayleigh waves and produce the observed signal (Walter et al. 2005). We also examined the explosion at high frequencies of 6 6-8 Hz where many studies have shown that relative P/S amplitudes can discriminate explosions from a background of earthquakes (Rodgers and Walter, 2002). Comparing with the April 4 1995 earthquake we see the classic difference of relatively large P/S values for the explosion compared to the earthquakes despite the complication of the large tectonic release during the explosion.Â«Â less\n\nSeismic Source Locations and Parameters for Sparce Networks by Matching Observed Seismograms to Semi-Empirical Synthetic Seismograms\n\nNASA Astrophysics Data System (ADS)\n\nMarshall, M. E.; Salzberg, D. H.\n\n2006-05-01\n\nThe purpose of this study is to further demonstrate the accuracy of full-waveform earthquake location method using semi-empirical synthetic waveforms and received data from two or more regional stations. To test the method, well-constrained events from southern and central California are being used as a testbed. A suite of regional California events is being processed. Our focus is on aftershocks of the Parkfield event, the Hector Mine event, and the San Simian event. In all three cases, the aftershock locations are known to within 1 km. For Parkfield, with its extremely dense local network, the events are located to within 300 m or better. We are processing the data using a grid spacing of 0.5 km in three dimensions. Often, the minimum in residual from the semi-empirical waveform matching is within one grid point of the 'ground truth' location, which is as good as can be expected. We will present the results and compare those to the event locations reported in catalogs using the dense local seismic networks that are present in California. The preliminary results indicate that matched-waveform locations are able to resolve the locations with accuracies better than GT5, and possibly approaching GT1. These results only require two stations at regional distances and differing azimuths. One of the disadvantages of the California testbed is that all of the earthquakes in a particular region typically have very similar focal mechanisms. In theory, the semi-empirical approach should allow us to generate the well-matched synthetic waveforms regardless of the varying mechanisms. To verify this aspect, we apply the technique to relocate and simulate the JUNCTION nuclear test (March 26, 1992) using waveforms from the Little Skull Mountain mainshock.\n\nSynthetic Seismogram Modeling.\n\nDTIC Science & Technology\n\n1982-11-15\n\nvarious phases ( designated A, B, C, etc.) are indicated on the seismic record section at the top of the diagram. The observed travel times show a good...structure of the Yellowstone aperture seismic array (LAS), Moatana, U.S. region and experiment design , J. Geophys. Geol. Suwv. Open File Rep. 1671, 1972. Res...also display little For clarity in both typography and conitext, we coherence in waveform or even in the envelope of shall henceforth write -P-bar in\n\nPredicting Lg Coda Using Synthetic Seismograms and Media With Stochastic Heterogeneity\n\nNASA Astrophysics Data System (ADS)\n\nTibuleac, I. M.; Stroujkova, A.; Bonner, J. L.; Mayeda, K.\n\n2005-12-01\n\nRecent examinations of the characteristics of coda-derived Sn and Lg spectra for yield estimation have shown that the spectral peak of Nevada Test Site (NTS) explosion spectra is depth-of-burial dependent, and that this peak is shifted to higher frequencies for Lop Nor explosions at the same depths. To confidently use coda-based yield formulas, we need to understand and predict coda spectral shape variations with depth, source media, velocity structure, topography, and geological heterogeneity. We present results of a coda modeling study to predict Lg coda. During the initial stages of this research, we have acquired and parameterized a deterministic 6 deg. x 6 deg. velocity and attenuation model centered on the Nevada Test Site. Near-source data are used to constrain density and attenuation profiles for the upper five km. The upper crust velocity profiles are quilted into a background velocity profile at depths greater than five km. The model is parameterized for use in a modified version of the Generalized Fourier Method in two dimensions (GFM2D). We modify this model to include stochastic heterogeneities of varying correlation lengths within the crust. Correlation length, Hurst number and fractional velocity perturbation of the heterogeneities are used to construct different realizations of the random media. We use nuclear explosion and earthquake cluster waveform analysis, as well as well log and geological information to constrain the stochastic parameters for a path between the NTS and the seismic stations near Mina, Nevada. Using multiple runs, we quantify the effects of variations in the stochastic parameters, of heterogeneity location in the crust and attenuation on coda amplitude and spectral characteristics. We calibrate these parameters by matching synthetic earthquake Lg coda envelopes to coda envelopes of local earthquakes with well-defined moments and mechanisms. We generate explosion synthetics for these calibrated deterministic and stochastic\n\nMicroseismic Event Location Improvement Using Adaptive Filtering for Noise Attenuation\n\nNASA Astrophysics Data System (ADS)\n\nde Santana, F. L., Sr.; do Nascimento, A. F.; Leandro, W. P. D. N., Sr.; de Carvalho, B. M., Sr.\n\n2017-12-01\n\nIn this work we show how adaptive filtering noise suppression improves the effectiveness of the Source Scanning Algorithm (SSA; Kao & Shan, 2004) in microseism location in the context of fracking operations. The SSA discretizes the time and region of interest in a 4D vector and, for each grid point and origin time, a brigthness value (seismogram stacking) is calculated. For a given set of velocity model parameters, when origin time and hypocenter of the seismic event are correct, a maximum value for coherence (or brightness) is achieved. The result is displayed on brightness maps for each origin time. Location methods such as SSA are most effective when the noise present in the seismograms is incoherent, however, the method may present false positives when the noise present in the data is coherent as occurs in fracking operations. To remove from the seismograms, the coherent noise from the pump and engines used in the operation, we use an adaptive filter. As the noise reference, we use the seismogram recorded at the station closest to the machinery employed. Our methodology was tested on semi-synthetic data. The microseismic was represented by Ricker pulses (with central frequency of 30Hz) on synthetics seismograms, and to simulate real seismograms on a surface microseismic monitoring situation, we added real noise recorded in a fracking operation to these synthetics seismograms. The results show that after the filtering of the seismograms, we were able to improve our detection threshold and to achieve a better resolution on the brightness maps of the located events.\n\nPreliminary study of first motion from nuclear explosions recorded on seismograms in the first zone\n\nUSGS Publications Warehouse\n\nHealy, J.H.; Mangan, G.B.\n\n1963-01-01\n\nThe U.S. Geological Survey has recorded more than 300 seismograms from more than 50 underground nuclear explosions. Most were recorded at distances of less than 1,000 km. These seismograms have been studied to obtain travel times and amplitudes which have been presented in reports on crustal structure and in a new series of nuclear shot reports. This report describes preliminary studies of first motion of seismic waves generated by underground nuclear explosions. Visual inspection of all seismograms was made in an attempt to identify the direction of first motion, and to estimate the probability of recording detectable first motion at various distances for various charge sizes and in different geologic environments. In this study, a characteristic pattern of the first phase became apparent on seismograms where first motion was clearly recorded. When an interpreter became familiar with this pattern, he was frequently able to identify the polarity of the first arrival even though the direction of first motion could not be seen clearly on the seismogram. In addition, it was sometimes possible to recognize this pattern for secondary arrivals of larger amplitude. These qualitative visual observations suggest that it might be possible to define a simple criterion that could be used in a digital computer to identify polarity, not only of the first phase, but of secondary phases as well. A short segment of recordings near the first motion on 56 seismograms was digitized on an optical digitizer. Spectral analyses of these digitized recordings were made to determine the range of frequencies present, and studies were made with various simple digital filters to explore the nature of polarity as a function of frequency. These studies have not yet led to conclusive results, partly because of inaccuracies resulting from optical digitization. The work is continuing, using an electronic digitizer that will allow study of a much larger sample of more accurately digitized data.\n\nHow Unique is Any Given Seismogram? - Exploring Correlation Methods to Identify Explosions\n\nNASA Astrophysics Data System (ADS)\n\nWalter, W. R.; Dodge, D. A.; Ford, S. R.; Pyle, M. L.; Hauk, T. F.\n\n2015-12-01\n\nAs with conventional wisdom about snowflakes, we would expect it unlikely that any two broadband seismograms would ever be exactly identical. However depending upon the resolution of our comparison metric, we do expect, and often find, bandpassed seismograms that correlate to very high levels (>0.99). In fact regional (e.g. Schaff and Richards, 2011) and global investigations (e.g. Dodge and Walter, 2015) find large numbers of highly correlated seismograms. Decreasing computational costs are increasing the tremendous potential for correlation in lowering detection, location and identification thresholds for explosion monitoring (e.g. Schaff et al., 2012, Gibbons and Ringdal, 2012; Zhang and Wen, 2015). We have shown in the case of Source Physics Experiment (SPE) chemical explosions, templates at local and near regional stations can detect, locate and identify very small explosions, which might be applied to monitoring active test sites (Ford and Walter, 2015). In terms of elastic theory, seismograms are the convolution between source and Green function terms. Thus high correlation implies similar sources, closely located. How do we quantify this physically? For example it is well known that as the template event and target events are increasingly separated spatially, their correlation diminishes, as the difference in the Green function between the two events grows larger. This is related to the event separation in terms of wavelength, the heterogeneity of the Earth structure, and the time-bandwidth of the correlation parameters used, but this has not been well quantified. We are using the historic dataset of nuclear explosions in southern Nevada to explore empirically where and how well these events correlate as a function of location, depth, size, time-bandwidth and other parameters. A goal is to develop more meaningful and physical metrics that go beyond the correlation coefficient and can be applied to explosion monitoring problems, particularly event\n\nSource Mechanism of May 30, 2015 Bonin Islands, Japan Deep Earthquake (Mw7.8) Estimated by Broadband Waveform Modeling\n\nNASA Astrophysics Data System (ADS)\n\nTsuboi, S.; Nakamura, T.; Miyoshi, T.\n\n2015-12-01\n\nMay 30, 2015 Bonin Islands, Japan earthquake (Mw 7.8, depth 679.9km GCMT) was one of the deepest earthquakes ever recorded. We apply the waveform inversion technique (Kikuchi & Kanamori, 1991) to obtain slip distribution in the source fault of this earthquake in the same manner as our previous work (Nakamura et al., 2010). We use 60 broadband seismograms of IRIS GSN seismic stations with epicentral distance between 30 and 90 degrees. The broadband original data are integrated into ground displacement and band-pass filtered in the frequency band 0.002-1 Hz. We use the velocity structure model IASP91 to calculate the wavefield near source and stations. We assume that the fault is squared with the length 50 km. We obtain source rupture model for both nodal planes with high dip angle (74 degree) and low dip angle (26 degree) and compare the synthetic seismograms with the observations to determine which source rupture model would explain the observations better. We calculate broadband synthetic seismograms with these source propagation models using the spectral-element method (Komatitsch & Tromp, 2001). We use new Earth Simulator system in JAMSTEC to compute synthetic seismograms using the spectral-element method. The simulations are performed on 7,776 processors, which require 1,944 nodes of the Earth Simulator. On this number of nodes, a simulation of 50 minutes of wave propagation accurate at periods of 3.8 seconds and longer requires about 5 hours of CPU time. Comparisons of the synthetic waveforms with the observation at teleseismic stations show that the arrival time of pP wave calculated for depth 679km matches well with the observation, which demonstrates that the earthquake really happened below the 660 km discontinuity. In our present forward simulations, the source rupture model with the low-angle fault dipping is likely to better explain the observations.\n\nDigitization Procedures of Analogue Seismograms from the Adam Dziewonski Observatory (HRV) at Harvard, MA\n\nNASA Astrophysics Data System (ADS)\n\nTorpey, M.; Ishii, M.\n\n2010-12-01\n\nThis project explores methods of digitization of analogue seismic recordings for better preservation and to facilitate data distribution to the community. Different techniques are investigated using seismograms from one particular station, the Adam Dziewonski Observatory (HRV) at Harvard, Massachusetts. This seismological station, still in operation as a part of the Global Seismographic Network today, is one of the oldest stations in the United States. The station was built in 1933, and since its installation, the station has produced approximately 16,000 analogue seismograms. The majority of these recordings were taken between 1933 and 1953, with some intermittent recordings between 1962 and 1998 after digital seismometers had become a standard. These analogue seismograms have the potential of expanding the database for seismological research such as identification of events previously not catalogued. Due to poor storage environment at the station, some of the records, especially those on regular type of paper, are damaged beyond repair. Nevertheless, many of the records on photographic paper are in better condition, and we have focused on a subset of these recordings that are least damaged. Even these seismograms require cleaning and, in consultation with the Weissman Preservation Center of Harvard Library, preparation techniques for the photographic records are examined. After the seismograms are cleaned and flattened, three different equipments are investigated for digitization, i.e., a copy machine, scanner, and camera. These instruments allow different imaging resolutions, ranging from 200 dots per inch (dpi) to 800 dpi. The image resolution and the bit depth have a wide range of implications that are closely linked to the digitization program one chooses to convert the image to time series. We explore three different software for this conversion, SeisDig (Bromirski and Chuang, 2003), Teseo2 (Pintore and Quintiliani, 2008), and NeuraLog (www\n\nDevelopment of XML Schema for Broadband Digital Seismograms and Data Center Portal\n\nNASA Astrophysics Data System (ADS)\n\nTakeuchi, N.; Tsuboi, S.; Ishihara, Y.; Nagao, H.; Yamagishi, Y.; Watanabe, T.; Yanaka, H.; Yamaji, H.\n\n2008-12-01\n\nThere are a number of data centers around the globe, where the digital broadband seismograms are opened to researchers. Those centers use their own user interfaces and there are no standard to access and retrieve seismograms from different data centers using unified interface. One of the emergent technologies to realize unified user interface for different data centers is the concept of WebService and WebService portal. Here we have developed a prototype of data center portal for digital broadband seismograms. This WebService portal uses WSDL (Web Services Description Language) to accommodate differences among the different data centers. By using the WSDL, alteration and addition of data center user interfaces can be easily managed. This portal, called NINJA Portal, assumes three WebServices: (1) database Query service, (2) Seismic event data request service, and (3) Seismic continuous data request service. Current system supports both station search of database Query service and seismic continuous data request service. Data centers supported by this NINJA portal will be OHP data center in ERI and Pacific21 data center in IFREE/JAMSTEC in the beginning. We have developed metadata standard for seismological data based on QuakeML for parametric data, which has been developed by ETH Zurich, and XML-SEED for waveform data, which was developed by IFREE/JAMSTEC. The prototype of NINJA portal is now released through IFREE web page (http://www.jamstec.go.jp/pacific21/).\n\nExpanding CyberShake Physics-Based Seismic Hazard Calculations to Central California\n\nNASA Astrophysics Data System (ADS)\n\nSilva, F.; Callaghan, S.; Maechling, P. J.; Goulet, C. A.; Milner, K. R.; Graves, R. W.; Olsen, K. B.; Jordan, T. H.\n\n2016-12-01\n\nAs part of its program of earthquake system science, the Southern California Earthquake Center (SCEC) has developed a simulation platform, CyberShake, to perform physics-based probabilistic seismic hazard analysis (PSHA) using 3D deterministic wave propagation simulations. CyberShake performs PSHA by first simulating a tensor-valued wavefield of Strain Green Tensors. CyberShake then takes an earthquake rupture forecast and extends it by varying the hypocenter location and slip distribution, resulting in about 500,000 rupture variations. Seismic reciprocity is used to calculate synthetic seismograms for each rupture variation at each computation site. These seismograms are processed to obtain intensity measures, such as spectral acceleration, which are then combined with probabilities from the earthquake rupture forecast to produce a hazard curve. Hazard curves are calculated at seismic frequencies up to 1 Hz for hundreds of sites in a region and the results interpolated to obtain a hazard map. In developing and verifying CyberShake, we have focused our modeling in the greater Los Angeles region. We are now expanding the hazard calculations into Central California. Using workflow tools running jobs across two large-scale open-science supercomputers, NCSA Blue Waters and OLCF Titan, we calculated 1-Hz PSHA results for over 400 locations in Central California. For each location, we produced hazard curves using both a 3D central California velocity model created via tomographic inversion, and a regionally averaged 1D model. These new results provide low-frequency exceedance probabilities for the rapidly expanding metropolitan areas of Santa Barbara, Bakersfield, and San Luis Obispo, and lend new insights into the effects of directivity-basin coupling associated with basins juxtaposed to major faults such as the San Andreas. Particularly interesting are the basin effects associated with the deep sediments of the southern San Joaquin Valley. We will compare hazard\n\nSU-F-J-109: Generate Synthetic CT From Cone Beam CT for CBCT-Based Dose Calculation\n\nDOE Office of Scientific and Technical Information (OSTI.GOV)\n\nWang, H; Barbee, D; Wang, W\n\nPurpose: The use of CBCT for dose calculation is limited by its HU inaccuracy from increased scatter. This study presents a method to generate synthetic CT images from CBCT data by a probabilistic classification that may be robust to CBCT noise. The feasibility of using the synthetic CT for dose calculation is evaluated in IMRT for unilateral H&N cancer. Methods: In the training phase, a fuzzy c-means classification was performed on HU vectors (CBCT, CT) of planning CT and registered day-1 CBCT image pair. Using the resulting centroid CBCT and CT values for five classified âtissueâ types, a synthetic CTmoreÂ Â» for a daily CBCT was created by classifying each CBCT voxel to obtain its probability belonging to each tissue class, then assigning a CT HU with a probability-weighted summation of the classesâ CT centroids. Two synthetic CTs from a CBCT were generated: s-CT using the centroids from classification of individual patient CBCT/CT data; s2-CT using the same centroids for all patients to investigate the applicability of group-based centroids. IMRT dose calculations for five patients were performed on the synthetic CTs and compared with CT-planning doses by dose-volume statistics. Results: DVH curves of PTVs and critical organs calculated on s-CT and s2-CT agree with those from planning-CT within 3%, while doses calculated with heterogeneity off or on raw CBCT show DVH differences up to 15%. The differences in PTV D95% and spinal cord max are 0.6Â±0.6% and 0.6Â±0.3% for s-CT, and 1.6Â±1.7% and 1.9Â±1.7% for s2-CT. Gamma analysis (2%/2mm) shows 97.5Â±1.6% and 97.6Â±1.6% pass rates for using s-CTs and s2-CTs compared with CT-based doses, respectively. Conclusion: CBCT-synthesized CTs using individual or group-based centroids resulted in dose calculations that are comparable to CT-planning dose for unilateral H&N cancer. The method may provide a tool for accurate dose calculation based on daily CBCT.Â«Â less\n\nThe 5th July 1930 earthquake at Montilla (S Spain). Use of regionally recorded smoked paper seismograms\n\nNASA Astrophysics Data System (ADS)\n\nBatllÃ³, J.; Stich, D.; MaciÃ , R.; Morales, J.\n\n2009-04-01\n\nOn the night of 5th July 1930 a damaging earthquake struck the town of Montilla (near CÃ³rdoba, S-Spain) and its surroundings. Magnitude estimation for this earthquake is M=5, and its epicentral intensity has been evaluated as VIII (MSK). Even it is an earthquake of moderate size, it is the largest one in-strumentally recorded in this region. This makes this event of interest for a better definition of the regional seismicity. For this reason we decided to study a new its source from the analysis of the available contemporary seismograms and related documents. A total of 25 seismograms from 11 seismic stations have been collected and digitized. Processing of some of the records has been difficult because they were obtained from microfilm or contemporary reproductions on journals. Most of them are on smoked paper and recorded at regional distances. This poses a good opportunity to test the limits of the use of such low frequency - low dynamics recorded seismograms for the study of regional events. Results are promising: Using such regional seismograms the event has been relocated, its magnitude recalculated (Mw 5.1) and inversion of waveforms to elucidate its focal mechanism has been performed. We present the results of this research and its consequences for the regional seismicity and we compare them with present smaller earthquakes occurred in the same place and with the results obtained for earthquakes of similar size occurred more to the East on 1951.\n\nJoint inversion of regional and teleseismic earthquake waveforms\n\nNASA Astrophysics Data System (ADS)\n\nBaker, Mark R.; Doser, Diane I.\n\n1988-03-01\n\nA least squares joint inversion technique for regional and teleseismic waveforms is presented. The mean square error between seismograms and synthetics is minimized using true amplitudes. Matching true amplitudes in modeling requires meaningful estimates of modeling uncertainties and of seismogram signal-to-noise ratios. This also permits calculating linearized uncertainties on the solution based on accuracy and resolution. We use a priori estimates of earthquake parameters to stabilize unresolved parameters, and for comparison with a posteriori uncertainties. We verify the technique on synthetic data, and on the 1983 Borah Peak, Idaho (M = 7.3), earthquake. We demonstrate the inversion on the August 1954 Rainbow Mountain, Nevada (M = 6.8), earthquake and find parameters consistent with previous studies.\n\nCalculation of zero-offset vertical seismic profiles generated by a horizontal point force acting on the surface of an elastic half-space\n\nUSGS Publications Warehouse\n\nHsi-Ping, Liu\n\n1990-01-01\n\nImpulse responses including near-field terms have been obtained in closed form for the zero-offset vertical seismic profiles generated by a horizontal point force acting on the surface of an elastic half-space. The method is based on the correspondence principle. Through transformation of variables, the Fourier transform of the elastic impulse response is put in a form such that the Fourier transform of the corresponding anelastic impulse response can be expressed as elementary functions and their definite integrals involving distance angular frequency, phase velocities, and attenuation factors. These results are used for accurate calculation of shear-wave arrival rise times of synthetic seismograms needed for data interpretation of anelastic-attenuation measurements in near-surface sediment. -Author\n\nA moment-tensor catalog for intermediate magnitude earthquakes in Mexico\n\nNASA Astrophysics Data System (ADS)\n\nRodrÃ­guez Cardozo, FÃ©lix; HjÃ¶rleifsdÃ³ttir, Vala; MartÃ­nez-PelÃ¡ez, Liliana; Franco, Sara; Iglesias Mendoza, Arturo\n\n2016-04-01\n\nLocated among five tectonic plates, Mexico is one of the world's most seismically active regions. The earthquake focal mechanisms provide important information on the active tectonics. A widespread technique for estimating the earthquake magnitud and focal mechanism is the inversion for the moment tensor, obtained by minimizing a misfit function that estimates the difference between synthetic and observed seismograms. An important element in the estimation of the moment tensor is an appropriate velocity model, which allows for the calculation of accurate Green's Functions so that the differences between observed and synthetics seismograms are due to the source of the earthquake rather than the velocity model. However, calculating accurate synthetic seismograms gets progressively more difficult as the magnitude of the earthquakes decreases. Large earthquakes (M>5.0) excite waves of longer periods that interact weakly with lateral heterogeneities in the crust. For these events, using 1D velocity models to compute Greens functions works well and they are well characterized by seismic moment tensors reported in global catalogs (eg. USGS fast moment tensor solutions and GCMT). The opposite occurs for small and intermediate sized events, where the relatively shorter periods excited interact strongly with lateral heterogeneities in the crust and upper mantle. To accurately model the Green's functions for the smaller events in a large heterogeneous area, requires 3D or regionalized 1D models. To obtain a rapid estimate of earthquake magnitude, the National Seismological Survey in Mexico (Servicio SismolÃ³gico Nacional, SSN) automatically calculates seismic moment tensors for events in the Mexican Territory (Franco et al., 2002; Nolasco-CarteÃ±o, 2006). However, for intermediate-magnitude and small earthquakes the signal-to-noise ratio could is low for many of the seismic stations, and without careful selection and filtering of the data, obtaining a stable focal mechanism\n\nA Program for Calculating and Plotting Synthetic Common-Source Seismic-Reflection Traces for Multilayered Earth Models.\n\nERIC Educational Resources Information Center\n\nRamananantoandro, Ramanantsoa\n\n1988-01-01\n\nPresented is a description of a BASIC program to be used on an IBM microcomputer for calculating and plotting synthetic seismic-reflection traces for multilayered earth models. Discusses finding raypaths for given source-receiver offsets using the \"shooting method\" and calculating the corresponding travel times. (Author/CW)\n\nThe damping of seismic waves and its determination from reflection seismograms\n\nNASA Technical Reports Server (NTRS)\n\nEngelhard, L.\n\n1979-01-01\n\nThe damping in theoretical waveforms is described phenomenologically and a classification is proposed. A method for studying the Earth's crust was developed which includes this damping as derived from reflection seismograms. Seismic wave propagation by absorption, attenuation of seismic waves by scattering, and dispersion relations are considered. Absorption of seismic waves within the Earth as well as reflection and transmission of elastic waves seen through boundary layer absorption are also discussed.\n\nThe need of inhomogeneous models to explain the seismograms of 2 explosions\n\nNASA Astrophysics Data System (ADS)\n\nMarcellini, A.; Tento, A.; Daminelli, R.\n\n2010-12-01\n\nOn November 23, 2003 and May 20, 2007 two 500 kg bombs from the 2nd World War were exploded in an open quarry close to Milan. Velocimetric registrations at 2.5 km and 1.4 km from the epicentre for the 2003 and 2007 explosions respectively, showed a nearly monochromatic P-wave train with an approximate frequency of 10 Hz for both explosions and an elliptical prograde motion. The maximum P-wave amplitude, recorded on the vertical component of the 2003 explosion, was 125 10-6 m/sec. The PGV was slightly less than the record of the 2007 explosion, despite the lower epicentral distance. Both the stations were situated in a quaternary deposit. We were not able to model the unusual readings using ordinary synthetic seismogram techniques, instead we found that the inhomogeneous wave propagation model (Borcherdt, 2009) fitted quite well for these results. The analysis showed that a moderate variation of QP didn't significantly change the tilt angle (between the propagation vector P and the ellipse of motion major axis). Also the difference between homogeneous and inhomogeneous P wave velocity was not notable, but the QP variation had a strong implication on the degree of inhomogeneity. The Î³ angle (between P vector and the attenuation vector A) reached Î³ =76Â° for QP=20 and 58Â° for QP=8. Borcherdt, R.D. (2009). Viscoelastic waves in layered media. Cambridge University Press, pp.305.\n\nPredicting Strong Ground-Motion Seismograms for Magnitude 9 Cascadia Earthquakes Using 3D Simulations with High Stress Drop Sub-Events\n\nNASA Astrophysics Data System (ADS)\n\nFrankel, A. D.; Wirth, E. A.; Stephenson, W. J.; Moschetti, M. P.; Ramirez-Guzman, L.\n\n2015-12-01\n\nWe have produced broadband (0-10 Hz) synthetic seismograms for magnitude 9.0 earthquakes on the Cascadia subduction zone by combining synthetics from simulations with a 3D velocity model at low frequencies (â¤ 1 Hz) with stochastic synthetics at high frequencies (â¥ 1 Hz). We use a compound rupture model consisting of a set of M8 high stress drop sub-events superimposed on a background slip distribution of up to 20m that builds relatively slowly. The 3D simulations were conducted using a finite difference program and the finite element program Hercules. The high-frequency (â¥ 1 Hz) energy in this rupture model is primarily generated in the portion of the rupture with the M8 sub-events. In our initial runs, we included four M7.9-8.2 sub-events similar to those that we used to successfully model the strong ground motions recorded from the 2010 M8.8 Maule, Chile earthquake. At periods of 2-10 s, the 3D synthetics exhibit substantial amplification (about a factor of 2) for sites in the Puget Lowland and even more amplification (up to a factor of 5) for sites in the Seattle and Tacoma sedimentary basins, compared to rock sites outside of the Puget Lowland. This regional and more localized basin amplification found from the simulations is supported by observations from local earthquakes. There are substantial variations in the simulated M9 time histories and response spectra caused by differences in the hypocenter location, slip distribution, down-dip extent of rupture, coherence of the rupture front, and location of sub-events. We examined the sensitivity of the 3D synthetics to the velocity model of the Seattle basin. We found significant differences in S-wave focusing and surface wave conversions between a 3D model of the basin from a spatially-smoothed tomographic inversion of Rayleigh-wave phase velocities and a model that has an abrupt southern edge of the Seattle basin, as observed in seismic reflection profiles.\n\nReducing uncertainties in the velocities determined by inversion of phase velocity dispersion curves using synthetic seismograms\n\nNASA Astrophysics Data System (ADS)\n\nHosseini, Seyed Mehrdad\n\nCharacterizing the near-surface shear-wave velocity structure using Rayleigh-wave phase velocity dispersion curves is widespread in the context of reservoir characterization, exploration seismology, earthquake engineering, and geotechnical engineering. This surface seismic approach provides a feasible and low-cost alternative to the borehole measurements. Phase velocity dispersion curves from Rayleigh surface waves are inverted to yield the"
    }
}
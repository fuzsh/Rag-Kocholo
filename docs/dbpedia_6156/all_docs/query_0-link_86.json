{
    "id": "dbpedia_6156_0",
    "rank": 86,
    "data": {
        "url": "https://functionallyparanoid.com/",
        "read_more_link": "",
        "language": "en",
        "title": "Musings on Cybersecurity and OpenBSD",
        "top_image": "https://functionallyparanoid.com/wp-content/uploads/2023/08/Dakirby309-Simply-Styled-Security-Approved.256-150x150.png",
        "meta_img": "https://functionallyparanoid.com/wp-content/uploads/2023/08/Dakirby309-Simply-Styled-Security-Approved.256-150x150.png",
        "images": [
            "https://functionallyparanoid.com/wp-content/themes/twentyten/images/headers/path.jpg",
            "https://functionallyparanoid.com/wp-content/uploads/2023/07/OpenBSD-1024x576.png",
            "http://functionallyparanoid.com/wp-content/uploads/2022/10/image.png?w=1024"
        ],
        "movies": [],
        "keywords": [],
        "meta_keywords": [
            ""
        ],
        "tags": null,
        "authors": [],
        "publish_date": null,
        "summary": "",
        "meta_description": "Musings on Cybersecurity and OpenBSD",
        "meta_lang": "en",
        "meta_favicon": "https://functionallyparanoid.com/wp-content/uploads/2023/08/Dakirby309-Simply-Styled-Security-Approved.256-150x150.png",
        "meta_site_name": "Functionally Paranoid - Musings on Cybersecurity and OpenBSD",
        "canonical_link": "https://functionallyparanoid.com/",
        "text": "As many of you know, I tend to run a lot of interesting things in my “home lab” network. Over the years, I have used lots of different techniques for this. Sometimes I ran on “bare metal” as the kids call it these days, other times I have used a variety of virtualization techniques with LXD being at the top of that list most recently.\n\nAbout twelve years ago, I discovered a really interesting open source project called OpenStack that had been started by NASA and Rackspace Hosting. The goal of the project was to create a cloud computing infrastructure that would rival the “big boys” and run on commodity hardware. Back then, I had three spare “old” work laptops in my office and I tried like hell to get it running on them. It turned out to be really really hard at the time.\n\nFast forward to a couple of years ago. In the intervening time, Canonical (maker of Ubuntu Linux among other things) had really leaned in hard to OpenStack and I thought I’d give it another go. This time they had done a lot of the heavy lifting for me using the juju operator framework and a bunch of charms. I managed (after many, many attempts where I learned something new and then had to start over again to get a clean install) to get OpenStack up and running.\n\nThen I had a power failure in my house and my server rebooted. Unfortunately, my config got “borked” as a result and I threw up my hands in frustration again and went back to LXD. So much for me having my own private cloud.\n\nWell, a few months ago I had a problem with my LXD cluster where it lost access to its database and instead of trying to recover the config (I had backups of everything anyhow), I decided to try the latest “cool thing”(TM) from Canonical – OpenStack Sunbeam. I had seen lots of advertisements about “Install OpenStack Sunbeam in Five Easy Steps” and thought I’d give it a whirl. This version of OpenStack did still use juju under the covers, but it was focused on setting things up on a single machine using MicroK8s – a lightweight version of Kubernetes.\n\nThe machine I am running on is based on an AMD Threadripper 2950x processor with 32 cores and has 128 GB of RAM and an NVIDIA RTX 4070 ti video card for GPU work. I call it “beast” affectionately. The goal was to get OpenStack Sunbeam running on beast and move all of my workloads to machines there. Not to share a spoiler, but I have managed to do with with two exceptions – one being my UniFi Console (the software that runs my Ubiquiti wireless network) because it had a subtle networking problem onboarding the devices that the OVN network on OpenStack struggled with and the other being my qemu emulated s390x and ppc64el versions of Ubuntu due to their need for a network tap and bridge configuration that could have probably been done on OVN, but was too “thinky” for me at the time.\n\nHere’s how I did it. First, I started with a clean install of Ubuntu 22.04 Server on the machine. Given that 24.04 is just around the corner I gave a fleeting thought to using it but decided not to press my luck and try to stick to the script as much as I could. I have three ethernet interfaces available to me on this machine (along with a WiFi one that makes four) so I plugged two of them into my switch and put a static IP on one of them that will be used as my primary access point to the machine over ssh. The other one I just let dhcp4 autoconfigure during the install.\n\nAfter rebooting the fresh machine, I applied all of the updates and enrolled it into UbuntuPro. If you aren’t using Pro, you really should. It’s an amazing free service from Canonical for up to 5 machines to get things like Livepatch (a magical way of updating your running kernel without a reboot) and extended support for the OS and a ton of open source software. I also enrolled the machine in my local instance of Landscape, Canonical’s systems management service.\n\nI turned off the dhcp4 on my unused networking connection using netplan by editing the /etc/netplan/00-installer-config.yaml file and making this change:\n\nenp4s0: dhcp4: false\n\nI also marked the unplugged port as “optional: true” so that it wouldn’t hang the boot process until it timed out trying to connect.\n\nSince I’m not sure why but the Ubuntu Server install creates a pretty small LVM volume to install the OS in, I extended it to take up the entire disk by doing the following:\n\n$ sudo lvextend -l +100%FREE /dev/ubuntu-vg/ubuntu-lv $ sudo resize2fs /dev/mapper/ubuntu--vg-ubuntu--lv\n\nI then rebooted the machine and installed the OpenStack snap and ran the “prepare-node” script as instructed:\n\n$ sudo snap install openstack --channel 2023.2/edge $ sunbeam prepare-node-script | bash -x && newgrp snap_daemon\n\nI then set the FQDN for the machine by editing the /etc/hosts file and adding:\n\n127.0.0.1 beast.example.com beast\n\nI then ran the following command as well for good measure:\n\n$ sudo hostnamectl set-hostname beast.example.com\n\nA quick check of hostname -f reveals that the changes were accepted by the OS. Now I bootstrapped the machine:\n\n$ sunbeam cluster bootstrap\n\nI managed to run into an error that the Discourse forum was happy to help me address where I had to add a “-m /snap/openstack/current/etc/manifests/edge.yml” switch that corrected it. Might not be an issue by the time you give it a try but I documented it here for completeness. Also, if you wanted to have ceph up and running on the new machine simply add “–role compute –role storage” to the command as well.\n\nWhen the bootstrap process prompted me for configuration items, I said I didn’t have a proxy and provided my “regular” network as the management network. For the MetalLB address allocation range, I gave it 10 IPs out of that same network.\n\nThe next step was to configure the OpenStack instance on the machine:\n\n$ sunbeam configure --openrc demo-openrc\n\nI told it I wanted remote network access, used my “regular” network as the external network, gave it my gateway and a 20 IP block in that same network as a start and end range. I told it I wanted a flat network and that I wanted Openstack populated with a demo user which I provided the credentials to be used for. I gave it a different subnet for the project network and told it what my nameserver was. I finally told it I wanted to enable ping and ssh and gave it the name of that unused ethernet interface as the free network interface.\n\nI generated the admin credentials into a configuration file I could “source” from the command line later and displayed the dashboard URL as follows:\n\n$ sunbeam openrc > sunbeam-admin $ sunbeam dashboard-url\n\nI then logged into that URL with the credentials from the ~/sunbeam-admin file so that I could further configure things.\n\nI created the production domain from the Identity -> Domains link (press the Set Domain Context button next to it after it is created to use it) and the production project from the Identity -> Projects link. Then I created an admin user that used those from the Identity -> Users link. I then logged out and back in as my new user.\n\nFrom the Project -> Network -> Network Topology link, I created a router and a network with a new subnet. I then clicked on the Router and added an interface to that new subnet. You should see a line now from the router to the subnet indicating that it is connected to the external network through that router.\n\nI created a floating IP address on my “regular” network by using the Admin -> Network -> Floating IPs and create an ssh key pair from Project -> Compute -> Key Pairs. I saved off the downloaded PEM file to my ~/.ssh directory to use to ssh into machines on the new server.\n\nI finally created a test instance and assigned it a floating IP address. One trick I discovered was to answer “No” to “create volume” when you are creating the server. Without doing that you get an error trying to create the server. Perhaps it had something to do with me not setting up ceph? I don’t know. I was then able to use that PEM file to ssh into the newly created server and everything worked!\n\nI hope you have enjoyed this little walk down me finally getting a Win on OpenStack after more than a decade. It certainly has come a long way in terms of capabilities as well as usability. Oh, and I have rebooted this server multiple times without losing the config so I hope it will be super-stable for me in the coming years.\n\nAs I continue to learn how to secure my various “things”, I’m getting more and more of a fan of physical two factor authentication that doesn’t involve sending six digit codes over the public SMS network. As such, I’ve been playing around with the YubiKey 5 NFC device, a little USB second factor that costs about $50 US and is really handy.\n\nThe first thing I wanted to secure with my YubiKey was logins to my various devices. Long-time readers of this blog know that means I need things to work in Windows, Linux and OpenBSD. I thought it would be helpful to outline below how I did that.\n\nOpenBSD\n\nNow for the fun part. Setting up the YubiKey on OpenBSD! I followed this guide to set things up.\n\nIf you intend to use your YubiKey on OpenBSD, you will want to do this first, before anything else. The reason for that is that you will need to capture the private ID and private key for your YubiKey slot which can only be done at the time that you generate it. After it has been written to the key, it can’t be retrieved (otherwise, cloning a YubiKey would be a trivial exercise).\n\nAnother downside to the implementation of login_yubikey is that it acts as the sole credential to log you in – in other words, it replaces your password and there is no second factor.\n\nFirst off, you will want to install the YubiKey personalization app:\n\n$ doas pkg_add yubikey-personalization-gui\n\nThe limitation to how login_yubikey is implemented currently (as of 7.3) is that you can only have one key. There is no ability to register a second one. However, you can make a backup of the private identity and secret key at the time you generate them and store them in the same place you keep your backup YubiKey.\n\nSo, launch the YubiKey Personalization Tool GUI application and insert your YubiKey that you will be using as your only key for OpenBSD. In the UI, click on Yubico OTP from the upper left-hand menu and press the “Quick” button that shows up on the screen. Uncheck the “Hide values” and copy off to a safe place the Public Identity, Private Identity and Secret Key. Select that slot you want to use (in my case, slot 1) and press the “Write Configuration” button and it should write to your YubiKey.\n\nNow create a file called /var/db/yubikey/user.uid and put your private identity value in there (replacing “user” in the filename with your userid). Put the secret key into one called /var/db/yubikey/user.key (again, replacing “user” with your userid). Set up the right permissions on the two files:\n\n# chown root:auth /var/db/yubikey/* chmod o-rw /var/db/yubikey/*\n\nFinally, edit the /etc/login.conf file and add ‘yubikey’ at the beginning of the auth-defaults entry like this:\n\n# Default allowed authentication styles auth-defaults:auth=yubikey,passwd,skey:\n\nNow if you reboot, you will find that your password no longer works. Touching your YubiKey after you insert it, however, should replace your password and log you in just fine. According to a wonderful contributor on DaemonForums.org, there is a challenge-response capability that I might be able to use to meet my 2FA requirement; however, I’ll have to tackle that in another post sometime later.\n\nWindows 11\n\nThe first thing I needed to do is to make sure that all of my Windows machines were running local accounts. I’m not a fan of Microsoft’s strong push to force everyone to use a Microsoft cloud login for their local machines. Windows 10 at least allowed you the option to ignore the strong push in the UI to set things up with a Microsoft login. For Windows 11, if that option exists, I cannot find it in the UI so I have to initially set up the machine with a Microsoft cloud authentication and then after the OS install is complete, switch it over to a local account.\n\nBy the way, if you are curious about how to switch from a Microsoft account to a local account, you need to bring up the settings pane in the UI, then navigate to “Accounts”. From there, click on “Your info” and select the item under “Account settings”. That will allow you to convert your existing Microsoft account to a local one. Or you can just create a new local account, set it as admin and delete the Microsoft one.\n\nAfter you have either verified you are running a local account or migrated your account and rebooted / logged back in, you will want to create a second admin account that you can use without a YubiKey to prevent you from locking your keys in the car. While I acknowledge this makes things less secure, I created mine with a 20+ character password and no password recovery questions that make sense to anyone (just gibberish in the answers) and rolled with it. If you are feeling that your threat model wouldn’t support such a “back door”, there is no requirement to create such an account, just be warned that you could potentially lose access if you screw up.\n\nThat said, I then logged out of my normal user account and logged into this backup admin account. From there, I installed the “Login Confirmation” application from Yubico and rebooted the machine.\n\nUpon reboot, the login screen looks like you require your YubiKey to log in. Actually, not yet until you configure things. Once you log in, run the “Login Confirmation” application you just installed. I switched to “Advanced configuration” so that I could control the behavior of the application as I set it up. On the next screen, I selected the following options:\n\nSlot 2\n\nUse existing secret if configured – generate if not configured\n\nGenerate recovery code (I do this for the first machine I setup and then save it off, after that I don’t generate a new code)\n\nCreate backup device for each user (only do this if you have purchased 2 separate YubiKeys – I have and I keep my backup in a safe / secure location in case I lose the primary)\n\nYou then need to pick the accounts you want to secure with your YubiKey(s) (again, I only pick my primary account, not the new admin account I created in case I’m locked out) and click “Next”. You’ll then be prompted to insert your primary YubiKey, then your secondary one. At this point, you should be good to go. I reboot just for grins, and then verify that (1) I cannot log into my primary account unless one of the two YubiKeys is inserted; (2) I can log into my emergency admin account without a YubiKey; and (3) that both YubiKeys work for logging into my primary account.\n\nLinux (Ubuntu)\n\nFirst things first, we need to add the official PPA for Yubico to apt:\n\n$ sudo add-apt-repository ppa:yubico/stable $ sudo apt update\n\nNow go ahead and install all of the Yubico software:\n\n$ sudo apt install yubikey-manager yubikey-personalization libpam-yubico libpam-u2f yubikey-manager-qt yubioath-desktop\n\nNext, you will need to set the PIN on your FIDO2 capability on both of your YubiKeys. To do this, run the Yubico Authenticator app and select “YubiKey” from the hamburger menu. Insert your primary YubiKey and scroll down to the Configuration section in the GUI. If you click on the right arrow next to WebAitjm )FIDO2/U2F), you can program the PIN. Do this for your backup key as well.\n\nTo associate the U2F key(s) with your Ubuntu account, open terminal and insert your YubiKey:\n\n$ mkdir -p ~/.config/Yubico $ pamu2fcfg > ~/.config/Yubico/u2f_keys\n\nYou will be prompted to enter your PIN that you set above and then when the YubiKey lights up, touch the “y” symbol on the physical key and it will save the information on your account. Now repeat the process with your backup YubiKey:\n\n$ pamu2fcfg -n >> ~/.config/Yubico/u2f_keys\n\nNow, let’s add some additional security by moving the config files to a root-only accessible location and update the configuration for PAM to point to it:\n\n$ sudo mkdir /etc/Yubico $ sudo mv ~/.config/Yubico/u2f_keys /etc/Yubico/u2f_keys\n\nNow we will need to change a configuration file in /etc/pam.d/sudo and add the following line after the “@include common-auth” line:\n\nauth required pam_u2f.so authfile=/etc/Yubico/u2f_keys\n\nNow, let’s test things to be sure that sudo is working with the YubiKey. To do this, open a fresh terminal window, insert your YubiKey and run “sudo echo test”, you should have to enter your password and then touch the YubiKey’s metal button and it will work. Without the YubiKey inserted, the sudo command (even with your password) should fail.\n\nSo now we need to repeat this process with the following files:\n\nrunuser /etc/pam.d/runuser runuser -l /etc/pam.d/runuser-l su /etc/pam.d/su sudo -i /etc/pam.d/sudo-i su -l /etc/pam.d/su-l\n\nNow we need to configure the system to require the YubiKey for login. To do this, we perform the same thing as above but in the /etc/pam.d/gdm-password file. Do this also for the /etc/pam.d/login file if you want to protect console text-based login. Finally, create a log file for the system to use by touching /var/log/pam_u2f.log and add this to the end of the pam_u2f.so lines above that you want to debug:\n\nauth required pam_u2f.so authfile=/etc/Yubico/u2f_keys debug debug_file=/var/log/pam_u2f.log\n\nReboot your system and you should be pretty much locked down to use the YubiKey for anything important.\n\nLUKS Full Disk Encryption (Ubuntu)\n\nOK, if you really want to take your 2FA to the next level, you can make it so that your YubiKey is required as a second factor to unlock your LUKS-encrypted disk. Not a replacement for your password, but a true second factor that is required in addition to your password in order to unlock the disk. I was able to get this to work by following the instructions in this post as well as the GitHub repo. To summarize, see below.\n\nIf you are using your YubiKey for Windows 11 login as I outlined above, your second slot in the key is already in use so DO NOT do what I am about to tell you. If, however, you are not using that second slot for Windows login, you need to install the YubiKey personalization software and then initialize slot 2 (for both your primary and backup YubiKeys if you have two):\n\n$ sudo apt install yubikey-personalization $ ykpersonalize -2 -ochal-resp -ochal-hmac -ohmac-lt64 -oserial-api-visible\n\nNow, with either your existing slot 2 configuration for Windows 11 login, or with the new one that you did above, you will need to enroll your YubiKey to the LUKS slot. Figure out first what your partition name is using the lsblk command. You are looking for a partition labeled as “crypt”. In my case it is /dev/nvme0n1p3 (not the /dev/nvme0n1p3_crypt):\n\n$ sudo apt install yubikey-luks $ sudo yubikey-luks-enroll -d /dev/nvme0n1p3 -s 1\n\nYou will be prompted for a challenge passphrase to use to unlock your drive as the first factor, with the YubiKey being the second factor. Since you are using a higher security (2FA) mechanism to unlock the drive, there is no need for this challenge passphrase to be crazy long. You can use a much longer passphrase for slot 0 to unlock the drive without the YubiKey as a failsafe.\n\nRepeat this in a different slot with your backup YubiKey. If you would like to see which slots are in use in your current LUKS partition, use the command:\n\n$ sudo cryptsetup luksDump /dev/nvme0n1p3\n\nThat is also a good way to confirm you have the right partition name.\n\nNow, you will need to update /etc/crypttab to add a keyscript:\n\ncryptroot /dev/nvme0n1p3 none luks,keyscript=/usr/share/yubikey-luks/ykluks-keyscript\n\nAfter editing the file, you will need to transfer the changes to initramfs:\n\n$ sudo update-initramfs -u\n\nAt this point, you should be able to reboot your machine and verify that you can unlock the disk with your original LUKS passphrase (what is now your fallback) as well as your new challenge passphrase and the YubiKey.\n\nIf you would like to update your YubiKey challenge passphrase in the future, you simply use the same command you did to set enroll it initially, but append a “-c” to clear out the old LUKS slot:\n\n$ sudo yubikey-luks-enroll -d /dev/nvme0n1p3 -s 1 -c\n\nWhen you are asked to “Enter any remaining passphrase”, that is where you enter your (hopefully) much longer fallback passphrase that doesn’t require the YubiKey, then you are asked to supply the new passphrase twice.\n\nIf you would like to update your fallback passphrase that doesn’t require a YubiKey, you can use the command:\n\n$ sudo cryptsetup luksChangeKey /dev/nvme0n1p3 -S 0\n\nYou should be prompted for your old password and the new password twice.\n\nUbuntuOne Single Sign On\n\nFor us Ubuntu users, eventually you end up creating an UbuntuOne account for things like access to Launchpad, access to your free UbuntuPro tokens, etc. Part of the setup for this is to supply a second factor for 2FA logins to increase the security of your account. The way I have mine set up, I have added Google Authenticator as one of my additional factors but I’d like to have my YubiKey be my primary second factor with Authenticator as my fallback if I don’t have access to the YubiKey (either primary or secondary)..\n\nTo set this up, navigate to https://login.ubuntu.com/ and log into your account. Then navigate to My Account -> Authentication Devices. On that screen you will see that you can “Add a New Authentication Device”. When you click on that, select YubiKey and follow the on-screen instructions. I would recommend doing this with your primary YubiKey as well as your backup one.\n\nGitHub\n\nIf you would like to use your YubiKey for a second factor when logging into GitHub, it’s pretty easy to do. Simply log into your GitHub account, click on your picture in the upper right header and select Settings from the dropdown menu. On the settings screen, select “Password and Authentication” to navigate to that settings page. On this page, you will need to enable 2FA if you haven’t already done so. I use Google Authenticator as my fallback 2FA method here as well.\n\nTo add your YubiKey(s), click on the “Edit” button next to the “Security Keys” section and press the “Register new security key” button. You will be prompted to name your key (i.e. “Primary YubiKey” or “Secondary YubiKey” are what I used) and then you will be prompted to insert your key and press its metal button. Repeat this process with any additional YubiKey(s) you might have and then you have added this as a second factor for GitHub.\n\nSSH 2FA\n\nAdding a second factor to your SSH key infrastructure is fundamentally a really (did I say really?) good idea. The way you set this up server-side depends on the operating system that is hosting the ssh server. I’ll break it down below:\n\nUbuntu\n\nI followed the howto on the Yubico site and followed the instructions for the “Non-discoverable” keys which are stated as being higher security than “discoverable” keys. Starting out, I first checked the firmware of your YubiKey as follows:\n\n$ lsusb | grep Yubico # Get the two 4-digit numbers separated by a colon and use them in place of the xxxx:xxxx below $ lsusb -d xxxx:xxxx -v 2>/dev/null | grep -i bcddevice\n\nIn my case, my firmware was version 5.12. This actually turned out to be 5.1.2 which put me below the minimum firmware version 5.2.3 for the stronger encryption. Also, you can’t update the firmware on your YubiKey – it is set at the factory. Ah well.\n\nGiven that, I’ll generate my keypair. On your desktop machine, generated the U2F/FIDO2 protected key pair:\n\n$ ssh-keygen -t ecdsa-sk # Older YubiKey firmware $ ssh-keygen -t ed25519-sk # Firmware version 5.2.3+\n\nWhen I ran the top command (because my YubiKey had the older 5.1.2 firmware), I got an error that said “You may need to touch your authenticator to authorize key generation” and yet I was never offered the attempt to do so. Therefore, I added the -vvv switch to the command and saw an error saying that my device “does not support credprot, refusing to create unprotected resident/verify-required key”.\n\nAfter doing some more digging, I discovered that there is a command I can run to validate the capabilities of my YubiKey:\n\n$ sudo apt install fido2-tools $ fido2-token -I /dev/hidraw1\n\nWhat I discovered to my disappointment is that my primary YubiKey (which I have had for several years) does not support the “credProtect” feature (it should show up in extension strings in the output of that command. My new secondary key, however, did. Therefore, I worked using my newer key and placed an order to Yubico for another one to use as my new primary. Sigh… My newer YubiKey also had firmware 5.4.3 so I’ll be able to use the newer crypto. Probably better in the long run.\n\nI chose the filename of “id_primary_yubikey” for my primary key and “id_backup_yubikey” for my backup keys. This generated a pair of files, one without a suffix and the other with a .pub suffix (indicating that it is the public key) for each of my two YubiKeys.\n\nSo. We now have a new primary and backup keypair in our local .ssh directory. How do we get this to work on our remote server(s)?\n\nIt’s pretty straightforward. Take the contents of the .ssh/id_primary_yubikey.pub file and append it to the ~/.ssh/authorized_keys file on the remote server you are trying to ssh into. Repeat this process with the .ssh/id_backup_yubikey.pub file. Now when you ssh into the remote system using the identity you generated:\n\n$ ssh -i ~/.ssh/id_primary_yubikey user@remote-system\n\nYou should notice that the Yubico symbol lights up on your YubiKey asking you to touch it. When you do so, you should be logged into the remote system.\n\nIf you would like to add your new identity to your SSH agent:\n\n$ ssh-add ~/.ssh/id_primary_yubikey\n\nNow you should be able to ssh directly into your remote system without having to supply the identity file.\n\nIf you still can’t get into the remote system, it is possible that it is not configured to support the sk-ssh-ed25519@openssh.com algorithm. To see what algorithms your remote system accepts, log into it and run the following command:\n\n$ ssh -Q PubkeyAcceptedAlgorithms\n\nConclusion\n\nCongratulations. You have now YubiKey’ed “All the Things!” Take that secondary YubiKey you bought and lock it in a safe somewhere. Keep the other one with you and you are now more secure than you were when you started.\n\nFor those of you born… shall we say… more recently than some of us, you might not be familiar with the term “mainframe” or think that it is some ancient server lost to the mists of time. The generic term refers to any large single or multi-user computer that was typically larger than one single cabinet in a datacenter. In the vernacular of today, this almost exclusively refers to multi-user hardware from IBM running either a proprietary operating system such as MVS.\n\nMainframes are still much in use today, running old legacy applications and serving as large servers that (interestingly enough) might run a variant of the Linux operating system. Interestingly, Ubuntu has been available for this platform for some time and I decided I wanted to add this unique architecture to my collection of machines at my disposal.\n\nUnfortunately I don’t have gigawatts of power at my house nor the necessary external watercooling that some of these beasts require (plus, my wife would have had my head) so I decided to go out on a limb and try to spin a modern Ubuntu LTS up on emulated “zSeries” (that’s what IBM calls it these days) hardware.\n\nI initially tried getting this working using the v4 version of Hercules based on an interesting blog post, but was unsuccessful. If you are interested in the details, jump to the end of this post and maybe you can figure out what I was doing wrong. In the meanwhile, here is what I did to get things working using QEMU.\n\nUbuntu 20.04 LTS on zSeries using QEMU\n\nFirst things first, I did a search to see if I could find a simple how-to that walked me through the process. I did find some, but they were a bit out of date and also didn’t go into the networking aspects of QEMU to a level where I could successfully spin things up. Here are the posts that I based most of this blog’s work upon:\n\nhttps://community.ibm.com/community/user/ibmz-and-linuxone/blogs/timothy-sipples1/2020/04/28/run-ubuntu-z-linuxone\n\nhttps://wiki.ubuntu.com/S390X/InstallationGuide\n\nhttps://til.simonwillison.net/docker/emulate-s390x-with-qemu\n\nThe obvious initial step is to install QEMU as well as the special features that allow it to emulate a zSeries processor from IBM:\n\n$ sudo apt install qemu-system-s390x qemu\n\nThe next step in the process is to create a network bridge on the machine that you are using as the host. Do do this, edit your /etc/netplan/*.yaml file (substitute your NIC name and IP information):\n\n# This is the network config written by 'subiquity' network: ethernets: enx00ec6306fb8: dhcp4: no bridges: br0: dhcp4: no addresses: - 192.168.1.99/24 gateway4: 192.168.1.1 nameservers: addresses: - 192.168.1.1 search: - example.com interfaces: - enx000ec6306fb8 version: 2\n\nYou have to then activate the changes to your network. Note that running the command below could result in you not having remote access to your machine you are doing this on so you might have to re-ssh into the box from another terminal window.\n\n$ sudo netplan apply --debug\n\nThen, set up the qemu tap device that uses the bridge you created above:\n\n$ sudo apt install qemu-kvm bridge-utils $ sudo ip tuntap add tap0 mode tap $ sudo brctl addif br0 tap0\n\nNow you need to create a disk image to use as your virtual storage device to install Ubuntu on:\n\n$ sudo qemu-img create -f qcow2 ubuntu-run.qcow2 10G\n\nYou can make the image any size you want. I also tried this with a “raw” formatted image but ended up having better luck using the qcow2 format instead. Now you have a place to install your Ubuntu s390x machine.\n\nNow, you need to download the latest install image. I tried to get this working with 22.04 LTS but the installer kept crashing on me at various points in the process. I suspect it might have a dislike for the virtual serial console over telnet business. Therefore, I proceeded with 20.04 LTS instead:\n\n$ wget https://old-releases.ubuntu.com/releases/20.04/ubuntu-20.04.4-live-server-s390x.iso\n\nNow you will need to mount the ISO and extract the kernel and initrd images because those will be used by QEMU in its command-line:\n\n$ mkdir tmpmnt $ sudo mount -o loop ./ubuntu-22.04.1-live-server-s390.iso tmpmnt $ cp tmpmnt/boot/kernel.ubuntu . $ cp tmpmnt/boot/initrd.ubuntu . $ sudo umount tmpmnt\n\nTo make things easier for myself, I created a script to launch the emulated s390x environment named run-s390x.sh:\n\n#! /usr/bin/bash qemu-system-s390x -machine s390-ccw-virtio -cpu max,zpci=on,msa5-base=off -serial telnet::4441,server -display none -m 8192 --cdrom ubuntu-22.04.1-live-server-s390x.iso -kernel kernel.ubuntu -initrd initrd.ubuntu -drive file=ubuntu-run.qcow2,format=qcow2 -net nic,model=virtio,macaddr=00:00:00:00:00:01 -net tap,ifname=tap0,script=no,downscript=no\n\nOnce I was ready to install things, I ran the script:\n\n$ chmod +x run-s390x.sh $ sudo ./run-s390x.sh\n\nAt this point, the emulator is paused, waiting for you to connect (via telnet port 4441 on the localhost address) to a virtual serial console. Therefore, from another terminal window on this machine, connect to the virtual serial console:\n\n$ telnet localhost 4441\n\nAt this point you should see the system boot up. It takes some time in the emulated environment. Eventually you get to the Ubuntu installer. From yet another terminal window on this machine, bring the tap0 interface up (I find that it doesn’t come up on its own until something is actually attached to it from qemu):\n\n$ ip link set up dev tap0 $ ip a # confirm that tap0 shows \"up\"\n\nBack in the installer, I chose to run it in “rich mode” and chose the following options:\n\nEnglish\n\nUbuntu Server\n\nOn the “ccw screen” – just hit “Continue”\n\nTake the network defaults (should be DHCP from your network) or configure to match a static IP address\n\nNo proxy\n\nTake the default mirror address\n\nSkip the 3rd party drivers\n\nThe install took a while but eventually completed successfully. Modify the run-s390x.sh file to be as follows (take out the installer, etc.):\n\n#! /usr/bin/bash cd /root ip tuntap add tap0 mode tap brctl addif br0 tap0 qemu-system-s390x -machine s390-ccw-virtio -cpu max,zpci=on,msa5-base=off -smp 2 -serial telnet::4441,server -display none -m 8192 -drive file=ubuntu-run.qcow2,if=none,id=drive-virtio-disk0,format=qcow2,cache=none -device virtio-blk-ccw,devno=fe.0.0001,drive=drive-virtio-disk0,bootindex=1 -net nic,model=virtio,macaddr=00:00:00:00:00:01 -net tap,ifname=tap0,script=no,downscript=no\n\nI then created a systemd service to start the qemu session at boot time after the network is operational by editing /etc/systemd/system/s390x.service:\n\n[Unit] Description=Launch s390x emulator session After=getty.target Before=s390x-network.service [Service] Type=simple RemainAfterExit=yes ExecStart=/root/run-s390x.sh TimeoutStartSec=0 [Install] WantedBy=default.target\n\nNext, create another script to bring up the tun0 interface (I called mine /root/tuntap.sh):\n\n#! /usr/bin/bash sleep 15 ip link set up dev tap0\n\nSet the execute bit on the script:\n\n$ sudo chmod +x /root/tuntap.sh\n\nThen, create a second systemd service to start networking after qemu has attached to tun0:\n\n[Unit] Description=Enable s390x networking After=getty.target [Service] Type=simple RemainAfterExit=no ExecStart=/root/tuntap.sh TimeoutStartSec=0 [Install] WantedBy=default.target\n\nFinally enable and start the service:\n\n$ sudo systemctl enable s390x-network.service $ sudo systemctl start s390x-network.service\n\nRebooting then confirmed that the service is started at boot time and reachable from the network. At this point you should be able to ssh into the s390x “machine” as if it were a real mainframe running Ubuntu on your network.\n\nUsing Hercules instead of QEMU\n\nThis turned out to be a bit of a dead-end, for me at least. The newer installer (22.04) had a kernel fault (I suspect because the virtual processor was too “old” as configured to be supported – probably fixable) so I used an older version. I managed to get it to ping the network, but the DNS wasn’t working and even with a hardcoded manual IP address for the Ubuntu server, it didn’t work.\n\nFrom:\n\n– https://sdl-hercules-390.github.io/html/hercinst.html#install\n\n– http://www.fargos.net/packages/README_UbuntuOnHercules.html\n\nStarted with 22.04 server\n\nInstall Hercules v4 (the one that installs with apt is v3)\n\n$ sudo apt install git wget time build-essential cmake flex gawk m4 autoconf automake libtool-bin libltdl-dev libbz2-dev zlib1g-dev libcap2-bin libregina3-dev net-tools\n\n$ git clone https://github.com/SDL-Hercules-390/hyperion.git\n\n$ cd hyperion\n\n$ ./util/bldlvlck # make sure everything is “OK”\n\n$ ./configure\n\n$ make\n\n$ sudo make install\n\nInstalling the helper scripts\n\n$ cd ~\n\n$ wget http://www.fargos.net/packages/ubuntuOnHercules.tar.gz\n\n$ mkdir ubuntu-hercules\n\n$ cd ubuntu-hercules\n\n$ tar xvf ../ubuntuOnHercules.tar.gz\n\nInstall Ubuntu\n\n$ cd ubuntu-hercules\n\n$ wget https://old-releases.ubuntu.com/releases/22.04/ubuntu-22.04-live-server-s390x.iso\n\n$ LD_LIBRARY_PATH=~/hyperion/.libs ./makeNewUbuntuDisk -c 48000 -v 22 # makes a 32g disk for Ubuntu 22.04\n\n# Modify ./hercules-ubuntu.cfg to have the DASD show up with a -v22.disk filename\n\n$ LD_LIBRARY_PATH=~/hyperion/.libs ./boot_ubuntu.sh –help\n\n# In my case, I need to set the default gateway and DNS server and change the hostname\n\n$ sudo LD_LIBRARY_PATH=~/hyperion/.libs ./boot_ubuntu.sh –iso ubuntu-22.04-live-server-s390x.iso –dns 192.168.x.x –gw 192.168.x.x –host s390x\n\n# When you get asked questions in the install, prefix your answer with a period (‘.’)\n\n# Choose CTC for network and then pick id #1 for read and #2 for write (at least that’s the one that worked for me)\n\n# Use Linux communication protocol when prompted\n\n# Do not autoconfigure the network but do it manually\n\n# Select 10.1.1.2/24 for your IP address and 10.1.1.1 for your gw and 8.8.8.8 for dns server\n\n# The system seems to have a problem resolving DNS so I used the IP address of the Ubuntu archive mirror\n\nRecently, a new Internet Service Provider (ISP) became available in my area. Now, no longer confined to a choice between the cable TV company and the telephone company to supply the bits to my house, I had the option of true gigabit fiber to my house as a choice! Needless to say, I had some questions.\n\nThe first question was, “How difficult is it to get a static IP address?” I wanted to know this because the cable TV company wanted you to switch from a residential service to a business service and then there was some sort of biological sampling required, signing over your firstborn child and some “feats of strength” required to get one of these magical things. For the new ISP, the answer was simple – send us an email asking for one and it will cost you $10 US per month to keep it. Wow. That was easy. On to the next question.\n\nThe next question was the tricky one. My cable TV provider purposely blocked certain ports such as port 25 (SMTP) and there was no way around that. I asked the new ISP if they blocked any ports and the answer was, “No. Why would we do that?” Again – amazing! At this point, I was ready to start moving all of my stuff from the cloud to my house. First things first, I had multiple HTTPS-secured websites to move. Uh oh. How do I serve up multiple websites with multiple different certificates from a single public IP address? Time to test my Google Fu.\n\nTurns out, my OpenBSD 7.1 router could come to the rescue. By doing a reverse-proxy setup with Apache2 and SSL termination, I could accept HTTPS traffic for multiple sites on my single IP address, serve up the right certificate to the browser on the other side of the communication and then pass along the traffic in the clear (HTTP) on port 80 to various servers on my home network. Finding blog posts about this was easy. Making it worked proved to be a bit tricky. I’m sure I could have done this with the OpenBSD httpd daemon (which has a much smaller attack surface that massive old Apache2) but that will be some research and investigation for another post (hopefully) in the future.\n\nOpenBSD Reverse Proxy + SSL Termination\n\nFirst off, something rare for this blog – a picture! This is the logical traffic flow for my setup:\n\nTo pull this off, I have to first install and enable Apache2 on my OpenBSD Octeon Router:\n\nNext, I have to get HTTPS certificates for my various sites. While I would have loved to have done this using certbot, I couldn’t because there was a C language library needed by Python3 to allow this that wasn’t available on the Octeon build (because my router doesn’t use and Intel/AMD CPU). I then tried using acme-client but found the configuration to be too challenging to pull off right away. Perhaps another blog post in the future. Anyhow, I used a Linux box and ran certbot to generate each of my certificates. I then wrote a little bash script to use scp to copy them to the right folder on my OpenBSD router and scheduled it with cron. Kickin’ it old school!\n\n$ doas pkg_add apache2 $ doas rcctl disable httpd $ doas rcctl enable apache2 $ doas rcctl start apache2\n\nAfter that, it was time to write the necessary configuration in /etc/apache2/httpd2.conf for each of the sites. As you can see, this assumes that the SSL certificates are in the /etc/ssl/private directory on my OpenBSD router:\n\n<VirtualHost *:80> ServerName www.example1.com ServerAlias www.example1.com RewriteEngine On RewriteCond %{HTTPS} off RewriteRule .* https://%{HTTP:Host}%{REQUEST_URI} [L,R=permanent] ProxyPass \"/\" \"http://192.168.1.101/\" ProxyPassReverse \"/\" \"http://192.168.1.101\" ProxyPreserveHost On </VirtualHost> <VirtualHost *:443> ServerName www.example1.com ServerAlias www.example1.com ProxyPass \"/\" \"http://192.168.1.101/\" ProxyPassReverse \"/\" \"http://192.168.1.101\" ProxyPreserveHost On SSLEngine On SSLCertificateFile /etc/ssl/private/www.example1.com/cert.pem SSLCertificateKeyFile /etc/ssl/private/www.example1.com/privkey.pem SSLCertificateChainFile /etc/ssl/private/www.example1.com/fullchain.pem SSLProxyEngine On <Location \"/\"> SSLRequireSSL RequestHeader set X-Forwarded-Proto \"https\" RequestHeader set X-Forwarded-Ssl on RequestHeader set X-Url-Scheme https RequestHeader set X-Forwarded-Port \"443\" </Location> </VirtualHost>\n\nIt is also necessary to further edit the /etc/apache2/http2.conf file to uncomment the “LoadModule” configuration lines for the services being used in the above configuration. The modules to load include ssl_module, proxy_module, proxy_connect_module, proxy_http_module, ssl_module, rewrite_module. After this, simply do an “rcctl restart apache2” and ensure that you were successful. If not, go back and double-check the configuration file.\n\nNext, you will need to make sure that your pf firewall allows port 80 and 443 through so that your site can be reached from off of the OpenBSD machine. To do this, add the following to your /etc/pf.conf file:\n\n# Allow serving of HTTP pass in on { $wan } proto tcp from any to any port 80 # Allow serving of HTTPS pass in on { $wan } proto tcp from any to any port 443\n\nReload the rules for pf using “$ doas pfctl -f /etc/pf.conf” and that step is done. You will also need to likely map ports 80 and 443 from your residential gateway (provided by your ISP) to send them to the OpenBSD router. At this point you should be able to hit your SSL protected site from outside of your network. I always test this by turning off the wifi on my cell phone and using it’s browser on the telco’s network. As you add more “internal” websites, simply duplicate those two sections above and restart your Apache2 daemon on the OpenBSD router.\n\nWhat About Email?\n\nThis one turned out to be very, very interesting. And by that I mean really stinking hard! The basics of it weren’t that bad. Here, I was able to use the wonderful “relayd” service that is native to OpenBSD to take all of the traffic I receive for the various email communication ports and fan them out to the appropriate back-end servers.\n\nAt first, I thought I would have to create a separate server for each email domain I wanted to host. Each of those servers would have to have its own SMTP server and each would have to have its own IMAP server. Also, if I wanted to have webmail for a particular domain, I would have to set it up to be an additional pair of entries in the http/https configuration in the previous section.\n\nHowever, when I started configuring the DNS entries for all of this, I realized the error in my thinking. I only had a single public IP address so I needed the moral equivalent of that reverse proxy magic that I built using Apache2 on my OpenBSD router. How does one do this in the world of SMTP and IMAP? Well, it turns out there is a solution called Server Name Indication (or SNI) that is supported by the major SMTP and IMAP services in the Linux world. Therefore, I elected to host my email on Linux. Perhaps I will do a future blog post on how I migrated this to OpenBSD?\n\nFirst things first, I needed to set up the necessary DNS entries to ensure that not only will my mail get routed to me, but that it will be considered deliverable and not “spammy” in an way. These included the following entries for each domain:\n\nA * 1.2.3.4 15 min TTL A * 1.2.3.4 15 min TTL A mail.example1.com 1.2.3.4 15 min TTL MX @ 10 mail 15 min TTL @ IN TXT \"v=spf1 mx a -all\" _dmarc IN TXT=\"v=DMARC1;p=quarantine;rua=mailto:admin@example1.com\" mail._domainkey IN TXT \"v=DKIM1; h=sha256; k=rsa ; p=*\"\n\nFor the above, the “1.2.3.4” is your static IP address from your ISP and you obviously need to fill in bits with your domain name as well as the DKIM content represented by the p=* section in the last entry. Perhaps I’ll do a full setup post in the future on this topic.\n\nAfter setting up DNS, you will then need to configure your mail server. I chose postfix for the SMTP server as it supports SNI and dovecot for the IMAP server for the same reason. Once that was done and I could access things securely from within my private network, I then set up relayd on my OpenBSD router:\n\n$ doas rcctl enable relayd $ doas rcctl start relayd\n\nI then wrote the following configuration file in /etc/relayd.conf to map the necessary ports to the mail server:\n\next_addr=\"192.168.1.2\" # private IP address of OpenBSD Router mail_host=\"192.168.1.201\" # private IP address of mail server relay smtp { listen on $ext_addr port 25 forward to $mail_host port 25 } relay submission_tls { listen on $ext_addr port 465 forward to $mail_host port 465 } relay submission_starttls { listen on #ext_addr port 587 forward to $mail_host port 587 } 25 relay imaps { listen on $ext_addr port 993 forward to $mail_host port 993 }\n\nAfter restarting relayd, we need to add some entries to /etc/pf.conf to ensure that the traffic actually gets through the OpenBSD firewall and hits relayd:\n\n# Allow servicing of SMTP pass in on { $wan } proto tcp from any to any port 25 # Allow servicing of Submission TLS pass in on { $wan } proto tcp from any to any port 465 # Allow servicing of Submission startTLS pass in on { $wan } proto tcp from any to any port 587 # Allow servicing of IMAPS pass in on { $wan } proto tcp from any to any port 993\n\nNow reload your pf rules with “$ doas pfctl -f /etc/pf.conf” and your machine should be relaying traffic. Finally, you will need to port map ports 24, 465, 587 and 993 on your residential gateway provided to you by your ISP and traffic should start flowing through. Test this from outside of your network and verify that everything is working as expected.\n\nConclusion\n\nUsing these techniques, you should be able to host any number of SSL enabled websites and properly secured email domains on private servers within your home network. This means that you can save some money by not having to use virtual servers in the cloud and also increase the privacy of your services because you physically control the servers themselves.\n\nDon’t forget to back up your data from these servers and then store it somewhere offsite (preferably in two places) in an encrypted fashion. One thing the cloud does make simple is just checking a couple of checkboxes and you suddenly have snapshots of your virtual server stored offsite. You can never have too many backups.\n\nAnyhow, I hope this was helpful for everyone!\n\nAs a middle-aged electric bass player, the “metal moments” of my life have been coming with less frequency than they did when I was younger. As a result, I tend to look for opportunities to be “metal” on any given day. To that end, I want to explore Canonical’s Metal as a Service or MaaS. Yeah, I know, I went for the cheap pun!\n\nFor those of you who aren’t familiar with this awesome piece of software, it essentially allows you to take a collection of physical servers on a private network and turn them into a cluster that allows you to pass out physical or virtual servers to users and then reclaim them when you are done. It does all of this using standard protocols that make life very, very easy. For example, the MaaS servers boot off of DHCP/PXE from an image hosted on the controller so that the OS image doesn’t live on the physical disk of the machine, freeing its built-in storage up for use by the cluster. Additionally, the software supports things like the Intel Active Management Technology (AMT) and its ability to allow remote power on / power off of machines that have this capability (along with many other more enterprise-y technologies for similar control).\n\nFor the purpose of this post, I’m going to create a MaaS cluster out of six machines that I have dedicated to the purpose and will be using them to host various projects in my home lab. As long-time readers of this blog know, I am a fan of the Lenovo Thinkpad family of laptops so as a result (like many in my cult) I have quite a stack of them lying around at any given time. For the purpose of this, I will be harnessing the power of my W520, W530 and W541 machines – all of which support the AMT (and more importantly I haven’t CoreBoot-ed yet so it still is enabled).\n\nIn addition, I have what I call my “Beast”, a tower machine with a Threadripper CPU that has 32 virtual cores, my NAS box (another AMD cpu machine that has a bunch of spinning physical disks) and finally the machine I’m using for my controller. For that purpose, I dragged out an old Dell laptop I had lying around. It only has one NIC (a WiFi card that I used to attach to my home network) but I picked up a USB-3 gigabit Ethernet adapter that is well supported by Linux to use to run the private network.\n\nThe controller machine connects to my home network (10.0.0.0/24) as well as to a small 8-port managed Gigabit switch that all five of the worker nodes will be solely attached to (192.168.100.0/24). That’s the physical network layout. Pretty simple. I also took the time to put a proper AMT password on the machines that support this technology which the MaaS controller will use to reboot them as needed. For the two AMD machines, I have to physically press the power button – at some point I might get an IP enabled power strip that is supported by MaaS and use it to allow them to be “remote controlled” as well but this works just fine for the time being. You might also want to check that virtualization is turned on in the BIOS for any of the machines you are using.\n\nI’m using Ubuntu 22.04 Server for the controller machine and am running it pretty much vanilla except for some network configuration to allow it to serve as a packet router from the private network to my home network so that machines in the cluster can download packages as needed. I could work around that by hosting a mirror on my controller with the packages I needed (I think) but this was easier. For most of this post, I’m basing my configuration on the MaaS 3.2 documentation.\n\nI downloaded the latest 22.04 server from the Ubuntu website and then used the “Startup Disk Creator” application that ships as part of the base OS on my laptop to create a bootable USB drive. After booting from the USB drive on the Dell laptop, the only configuration change I made to the default install was to enable an SSH server on the machine so I can remote in and do everything I need to from my laptop (except for pressing the power buttons a few times on the worker nodes).\n\nOnce the controller is installed and booted up, I have to make some network configuration changes to allow it to have a static IP address on both the home network side (WiFi) as well as on the private network that it will be managing. To do this, I edit the /etc/netplan/00-installer-config.yaml file to look like the following:\n\nnetwork: ethernets: enx000ec6306fb8: dhcp4: false optional: true addresses: [192.168.100.1/24] wifis: wlp1s0: dhcp4: false optional: true addresses: [10.0.0.5/24] nameservers: addresses: [8.8.8.8] routes: - to: default via: 10.0.0.1 access-points: \"my_ssid_name\": password: \"********\" version: 2\n\nAfter saving these changes, I ran “sudo netplan try” to test the configuration and ensure that everything is working the way I wanted it to. Once I was satisfied with the network, I updated the machine (“sudo apt update” and then “sudo apt upgrade”). After that, I reboot the machine to pick up the new kernel I downloaded in the updates.\n\nI want my machines on the private network to be able to reach the Internet through the MaaS controller. To make things simple, I’m just going to set up a basic router on this machine using a guide I found here:\n\n# echo \"net.ipv4.ip_forward=1\" >> /etc/sysctl.conf # sysctl net.ipv4.ip_forward=1 # iptables -A FORWARD -i enx000ec6306fb8 -o wlp1s0 -j ACCEPT # iptables -A FORWARD -i wlp1s0 -o enx000ec6306fb8 -m state --state RELATED,ESTABLISHED -j ACCEPT # iptables -t nat -A POSTROUTING -o wlp1s0 -j MASQUERADE # iptables -t nat -A POSTROUTING -o enx000ec6306fb8 -j MASQUERADE # apt install iptables-persistent\n\nAfter running the “apt install…” command, make sure you tell it to persist the IPV4 and IPV6 rules and they will be stored in /etc/iptables under files called “rules.v4” and “rules.v6”. At this point, because I’m old-school, I do a reboot.\n\nFor my lab, I want to be as close to a “production” environment as I can get. Therefore, I’m opting for a “region+rack” configuration. Using snaps, installing MaaS is… well… a snap:\n\n$ sudo snap install --channel=3.2 maas\n\nThe next thing we need to do is set up a PostgreSQL database for this instance of MaaS:\n\n$ sudo snap install maas-test-db\n\nAt this point, it is time to initialize your instance of MaaS:\n\n$ sudo maas init region+rack --database-uri maas-test-db:///\n\nI took the default for my MaaS URL (http://10.0.0.5:5240/MAAS). I then ran the command “$ sudo maas createadmin” and provided my admin credentials and my Launchpad user for my ssh keys.\n\nAt this point, I logged into my MaaS instance from that URL and did some configuration. First, I named my instance and set the DNS forwarder to one that I liked. Next, we need to enable DHCP for the private network so that it can PXE boot new machines on the network. To do this, navigate to the Subnets tab and click on the hyperlink in the “vlan” column that corresponds to the private network. Click “Configure DHCP” and then fill in the Subnet dropdown to correspond to the IP address range of your private network then save the change. You should now notice the warning about DHCP not being configured has gone away from the MaaS user interface.\n\nThe next thing we need to do is set up the default gateway that is shared by the MaaS DHCP server to the machines. To do this, navigate to the “Subnets” tab and click on the hyperlink in the “subnet” column for your private network. Click “Edit” and fill in the Default Gateway IP address and the DNS address if you’d like. After clicking “Save” your machines will be automatically configured to use the default gateway you provided (in my case, the private network IP address of my MaaS controller).\n\nI first boot up the Thinkpads (that have Intel AMT) on the private network and they PXE boots off of the MaaS controller and eventually show up under the “Machines” tab of the MaaS user interface. I click on each of them in the MaaS user interface and configure their names and their power setup to be Intel AMT and provide my passwords and IP addresses that I set up in the firmware on each of them. I then booted up the AMD machines and in their configuration, just set their power type to “Manual.\n\nAt this point, you will need to get the machines into a “usable” state for MaaS so to do that, check the box next to each one on the “Machines” tab and select “Commission” from the actions menu. You’ll have to physically power on any machines that don’t have Intel’s AMT and then they will go through the commissioning process. When done, they will show up as “Ready” on the “Machines” tab.\n\nNow I need to get the default gateway working for each of the machines. There might be an easier way of doing this; however, I haven’t figured it out yet so I’m following part of a guide found here. For each machine, click on it and then navigate to the network tab. When there, check the box next to the network interface that is connected to the private network’s switch and press the “Create Bridge” button. Name the bridge “br-ex”, the type is “Open vSwitch”, select the fabric and subnet corresponding to your private network and pick “auto assign” for the ip mode.\n\nNow, check the boxes next to your “Ready” machines and select “Deploy” from the actions menu. Be sure to check the “Auto Assign as KVM host” to make them available to host virtual machines for you. Press the “Start deployment…” button and be sure to power on any that don’t have Intel AMT technology to control their power state. At this point you should be done with the power button pushing business unless you need to do maintenance on the machines.\n\nThis seemed as good a time as any to create a MaaS user for myself. To do this, I navigated to the “Settings” tab and selected “Users” and then clicked “Add User”. I filled in the details (by the way, MaaS enforces no duplicate email addresses among its users so if you are like me and want an admin account and a separate user account, you’ll have to use two email addresses) and clicked “Save” and I was good to go. I logged in as that user and supplied my SSH key from Launchpad.\n\nIf you now switch to the main MaaS “KVM” tab, you should see your machines available and be able to add virtual machines. You do this by clicking on one of the hosts and then clicking the “Add Virtual Machine” button. It then shows up as a “New” machine in the MaaS user interface.\n\nI then log in as my “user” account in MaaS and deploy the “New” virtual machines. Once they are completely deployed, you can then ssh into them from a machine that has connectivity to the private network. The only trick I discovered is that you have to log in as the “ubuntu” user, NOT the user you have set up in MaaS.\n\nAt this point, I have a working MaaS home lab that I can use for a variety of projects. I hope that you found this post helpful!\n\nFor those of you who didn’t read my predecessor post on setting up a full-blown Active Directory infrastructure on my home network with home directories, roaming user profiles and group policy using only open source software, take a read through that. This is a follow-on post where I have added a second Active Directory domain controller in a private cloud environment and then bridged that private cloud network to my secure home network using WireGuard.\n\nBridging The Networks\n\nTo start off, since I’m using the bleeding-edge Ubuntu version on my primary domain controller, I set up a virtual server in my cloud provider of choice using 21.10 as well. For the private network, I put it on its own private network that does not collide with my home network (192.168.1.0/24). In this case it is 192.168.2.0/24.\n\nMy VPS provider allows me to supply SSH keys at their web console that restricts who can ssh into the remote virtual machine to only those who have the private key that corresponds to the public keys you upload and select. This ensures that I can securely log into the machine as root-level access without fear. The first thing do to, however, when I log into the new server is to update the packages installed on it:\n\n# apt update # apt upgrade # reboot\n\nNow for the wireguard setup on the remote virtual machine. For the purposes of this section, we will call it the “server”:\n\n# apt install wireguard wireguard-tools # wg genkey | sudo tee /etc/wireguard/server_private.key # wg pubkey | sudo tee /etc/wireguard/server_public.key # echo \"net.ipv4.ip_forward=1\" >> /etc/sysctl.conf # echo \"net.ipv6.conf.all.forwarding=1\" >> /etc/sysctl.conf # sysctl -p net.ipv4.ip_forwrd=1 net.ipv6.conf.all.forwarding=1 # vim /etc/wireguard/wg0.conf [Interface] Address = 10.10.10.1/32 ListenPort = 51820 PrivateKey = *** contents of /etc/wireguard/server_private.key *** PostUp = iptables -A FORWARD -i %i -j ACCEPT; iptables -A FORWARD -o %i -j ACCEPT PostDown = iptables -D FORWARD -i %i -j ACCEPT; iptables -D FORWARD -o %i -j ACCEPT [Peer] PublicKey = *** contents of /etc/wireguard/server_public.key from remote *** Endpoint = 1.2.3.4:51820 # IP address of remote AllowedIPs = 10.10.10.2/32, 192.168.1.0/24\n\nSince my local network is on a residential ISP, I need to use the tools on my ISP’s router to port map the Wireguard port that comes in on the public IP address to the OpenBSD router. Now, we will need to set up the WireGuard configuration on the OpenBSD 7.0 router that I use for my secure network at home (private IP is 192.168.1.1):\n\n# pkg_add wireguard-tools # sysctl net.inet.ip.forwarding=1 # echo 'net.inet.ip.forwarding=1' | tee -a /etc/sysctl.conf # mkdir /etc/wireguard # chmod 700 /etc/wireguard # openssl rand -base64 32 > /etc/wireguard/server_private.key # wg pubkey < /etc/wireguard/server_private.key > /etc/wireguard/server_public.key # vim /etc/hostname.wg0 inet 10.10.10.2 255.255.255.0 !/usr/local/bin/wg setconf wg0 /etc/wireguard/wg0.conf !route add -inet 192.168.2.0/24 10.10.10.2 # vim /etc/wireguard/wg0.conf [Interface] PrivateKey = *** contents of /etc/wireguard/server_private.key *** ListenPort = 51820 [Peer] PublicKey = *** contents of /etc/wireguard/server/public.key from remote *** Endpoint = 2.3.4.5:51820 # public IP address of remote AllowedIPs = 10.10.10.2/32, 192.168.2.0/24 # vim /etc/pf.conf ... add to end... pass in on egress proto udp from any to any port 51820 keep state pass on wg0 pass out on egress inet from (wg0:network) to any nat-to (egress:0) # pfctl -f /etc/pf.conf # sh /etc/netstart wg0\n\nNow, run the following command on the remote Linux box to start the Wireguard service:\n\n# systemctl enable wg-quick@wg0.service # systemctl start wg-quick@wg0.service\n\nAt this point, you should be able to check the status of the Wireguard network on both sides with the command wg show and that should show both ends connected. You should be able to ping hosts on the remote network from each end.\n\nSo far, the only problem I have found with this setup to bridge the networks, is that my Windows machines that are multi-homed (i.e. one interface – wired ethernet – is connected to my ISP’s network and one – wireless – is connected to my secure network) needs to have a route manually added as follows:\n\nC:\\WINDOWS\\system32> route add -p 192.168.2.0 MASK 255.255.255.0 192.168.1.1\n\nIn this case, the 192.168.2.0/24 network is the remote network and the 192.168.1.1 IP references my OpenBSD 7.0 router.\n\nRemote Samba Active Directory Server\n\nNow that we have a remote network that is securely bridged to our local private network on which the current Samba Active Directory infrastructure is running, it is time to create the VPC virtual server that will be running our Active Directory remote server. My particular VPC service allows me to create a server that is on the same private network as my remote “router” that is running Wireguard, so I create such a server and call it AD2.ad.example.com (put in your own AD domain name there).\n\nFirst things first, the remote AD server must have a route to the Wireguard network. This is not a necessary step on the home network side because the Wireguard server is running on the OpenBSD 7.0 router and by definition is the default route for the servers on that network. This is not the case for the servers on the private network at the VPC. To do this, we simply need to add a persistent route. So as to not mess things up with the default network configuration on the remote host, I decided to create a (yuck) SystemD (blech) service:\n\n# apt update # apt upgrade # apt install network-tools # vim /usr/sbin/MY-NETWORK.sh #! /bin/sh /usr/sbin/route add -net 192.168.1.0/24 gw 192.168.2.2 eth1 # chmod +x /usr/sbin/route # vim /etc/systemd/system/MY-NETWORK.service [Unit] Description=Route to Wireguard server After=network.target StartLimitIntervalSec=0 [Service] Type=simple Restart=always RestartSec=1 User=root ExecStart=/usr/sbin/MY-NETWORK.sh [Install] WantedBy=multi-user.target # systemctl daemon-reload # systemctl enable MY-NETWORK.service # systemctl start MY-NETWORK.service\n\nAt this point, you should be able to ping the domain controller on the remote (home) network and from that domain controller, you should be able to ping the new host.\n\nNow we need to do the standard networking configuration ‘stuff’ that Samba likes. First, edit the /etc/hosts file to remove the “127.0.1.1 DC2.ad.example.com DC2” line and replace it with one tying it to the static private IP address that has been assigned to this virtual host. In this case, “192.168.2.3 DC2.ad.example.com DC2”.\n\nHere we need to add the necessary packages to host an Active Directory domain controller:\n\n# apt install acl attr samba samba-dsdb-modules samba-vfs-modules winbind libpam-winbind libnss-winbind libpam-krb5 krb5-config krb5-user dnsutils net-tools smbclient\n\nNext, disable systemd’s resolver and add the remote AD server as the DNS name server and also add the Active Directory domain:\n\n# systemctl stop systemd-resolved # systemctl disable systemd-resolved # unlink /etc/resolv.conf # vim /etc/resolv.conf nameserver 192.168.1.2 search ad.example.com\n\nNow, go ahead and reboot the remote machine and when you log back into it, test to see if DNS is working properly:\n\n# nslookup DC1.ad.example.com Server: 192.168.1.2 Name: DC1.ad.example.com # nslookup 192.168.1.2 2.1.168.192.in-addr.arpa name = DC1.ad.example.com # host -t SRV _ldap._tcp.ad.example.com _ldap._tcp.ad.example.com has SRV record 0 100 389 dc1.ad.example.com\n\nRename the /etc/krb5.conf file and the /etc/samba/smb.conf file like you did when you created the domain controller on your local network. Then, create a new /etc/krb5.conf file:\n\n[libdefaults] default_realm = AD.EXAMPLE.COM dns_lookup_realm = false dns_lookup_kdc = true\n\nAt this point, we need to set up an NTP server and sync it to the one at our original Active Directory domain controller:\n\n# apt install chrony ntpdate # ntpdate 192.168.1.2 # echo \"server 192.168.1.2 minpoll 0 maxpoll 5 maxdelay .05\" > /etc/chrony/chrony.conf # systemctl enable chrony # systemctl start chrony\n\nNow we need to authenticate against Kerberos and get a ticket:\n\n# kinit administrator ... provide your AD\\Administrator password ... # klist\n\nAt this point, it’s time to join the domain as a new domain controller:\n\n# samba-tool domain join ad.example.com DC -U\"AD\\administrator\"\n\nAfter the tool finishes (it produces a lot of output), you need to copy the generated Kerberos configuration file to the /etc directory:\n\n# cp /var/lib/samba/private/krb5.conf /etc/krb5.conf\n\nYou need to manually create the systemd service and set things up so that everything fires up when you reboot the server:\n\n# systemctl mask smbd nmbd winbind # systemctl disable smbd nmbd winbind # systemctl stop smbd nmbd winbind # systemctl unmask samba-ad-dc # vim /etc/systemd/system/samba-ad-dc.service [Unit] Description=Samba Active Directory Domain Controller After=network.target remote-fs.target nss-lookup.target [Service] Type=forking ExecStart=/usr/sbin/samba -D PIDFILE=/run/samba/samba.pid ExecReload=/bin/kill -HUP $MAINPID [Install] WantedBy=multi-user.target # systemctl daemon-reload # systemctl enable samba-ad-dc # systemctl start samba-ad-dc\n\nOK. At this point we have a Samba Active Directory domain controller running. We need to get SysVol replication going now to ensure that the two controllers are bidirectionally synchronized.\n\nBidirectional SysVol Replication\n\nTo get the SysVol replication going bidirectionally, I followed the guide here. First, you need some tools installed on both DCs:\n\n# apt install rsync unison\n\nGenerate an ssh key on both domain controllers:\n\n# ssh-keygen -t rsa\n\nNow, copy the /root/.ssh/id_rsa.pub contents from one server into the /root/.ssh/authorized_keys file on the other and vice-versa. Verify that you can log in without passwords from one server to the other. If you are prompted for a password, then edit your /etc/ssh/sshd_config file and add the line “PasswordAuthentication no” and then restart the ssh service. Now you should be able to log in just using public keys and no password from one server to the other and back.\n\nNow, copy the /root/.ssh/id_rsa.pub contents from one server into the /root/.ssh/authorized_keys file on the other and vice-versa. Verify that you can log in without passwords from one server to the other. If you are prompted for a password, then edit your /etc/ssh/sshd_config file and add the line “PasswordAuthentication no” and then restart the ssh service. Now you should be able to log in just using public keys and no password from one server to the other and back.\n\nOn your new remote DC (DC2 in my example), do the following to ensure that your incoming ssh connection isn’t rate limited:\n\n# mkdir /root/.ssh/ctl cat << EOF > /root/.ssh/ctl/config Host * ControlMaster auto ControlPath ~/.ssh/ctl/%h_%p_%r ControlPersist 1 EOF\n\nNow, to be able to log what happens during the sync on the local DC (DC1 in my example), do the following to create the appropriate log files:\n\n# touch /var/log/sysvol-sync.log # chmod 640 /var/log/sysvol-sync.log\n\nNow, do the following on the local DC (DC1 in my example):\n\ninstall -o root -g root -m 0750 -d /root/.unison cat << EOF > /root/.unison/default.prf # Unison preferences file # Roots of the synchronization # # copymax & maxthreads params were set to 1 for easier troubleshooting. # Have to experiment to see if they can be increased again. root = /var/lib/samba # Note that 2 x / behind DC2, it is required root = ssh://root@DC2//var/lib/samba # # Paths to synchronize path = sysvol # #ignore = Path stats ## ignores /var/www/stats auto=true batch=true perms=0 rsync=true maxthreads=1 retry=3 confirmbigdeletes=false servercmd=/usr/bin/unison copythreshold=0 copyprog = /usr/bin/rsync -XAavz --rsh='ssh -p 22' --inplace --compress copyprogrest = /usr/bin/rsync -XAavz --rsh='ssh -p 22' --partial --inplace --compress copyquoterem = true copymax = 1 logfile = /var/log/sysvol-sync.log EOF\n\nNow, run the following command on your local DC (DC1 in my example):\n\n# /usr/bin/rsync -XAavz --log-file /var/log/sysvol-sync.log --delete-after -f\"+ */\" -f\"- *\" /var/lib/samba/sysvol root@DC2:/var/lib/samba && /usr/bin/unison\n\nThis should synchronize the two sysvols. If you followed my previous how-to and set up Group Policy, this can take some time as there are a lot of files involved that are stored on the SysVol. After it is complete, you can verify this by doing the following on your remote DC (DC2 in my example):\n\n# ls /var/lib/samba/sysvol/ad.example.com\n\nYou should see the same file structure under that directory on both servers. This will copy everything including your group policy stuff as well.\n\nNow that you have done the initial sync, just add the following to your crontab on the local DC (DC1 in my example):\n\n# crontab -e */5 * * * * /usr/bin/unison -silent\n\nYou should monitor /var/log/sysvol-sync.log on your local DC (DC1 in my example) to ensure that everything is synchronizing and staying that way over time.\n\nHope this little “how-to” helps folks!\n\nMany of the long-time readers of this blog are going to probably have a panic attack when they read this article because they are going to be asking themselves the question, “Why in the heck does he want to install Active Directory in his life?” The reason, like so many answers to so many of these questions I ask myself is “Because I can!” LOL!!\n\nSo I have a small home network that is my playground for learning new technologies and practicing and growing my security skills. I try to keep it segregated from my true home network that my family uses because I don’t want my latest experiment to get in the way of any of them connecting to the Internet successfully.\n\nJust for fun, however, I’m going to start on a path to try a new experiment – I’d like to have the ability to add a new machine to my network and not have to spend half a day setting it up. Furthermore, I’d like to put everything I can either on a local file server that backs up to the cloud or in the cloud that backs up to a local file server in such a way that I can totally destroy any of my machines and be able to reproduce it at the push of a button. The ultimate in home disaster recovery.\n\nWhat does this buy me? Well, for one, it lets me be even more aggressive in my experimentation. If I lay waste to a machine because of a failed experiment, no big deal – I just nuke and automatically repave it. For another, it makes it way easier to recover a family member’s setup when something goes wrong. I can just rebuild the machine and know they won’t lose anything. That alone will save me lots of time troubleshooting the latest problems with stuff.\n\nSo, why Active Directory? I choose this technology because pretty much everything (OpenBSD is going to be interesting) will authenticate centrally with it and yes, I do have to run some Windows and Mac machines on my network, I can’t do it all on OpenBSD and Linux so it’s a good common ground.\n\nNow, I will die before installing a Windows Server in my infrastructure (LOL) so I have been very careful saying “Active Directory” and not “Windows Server”, or “Azure AD”. I’m going to see how far Samba 4 has come since the last time I played with it. If I can do the full meal deal of authentication, authorization, roaming user profiles and network home directories on a Windows machine, then I can fill in around the edges on my non Windows machines using NFS and other techniques.\n\nSetting up Ubuntu\n\nFirst things first, I want to start with a clean install of my domain controller. To this end, I’ll nuke and repave my 32-core Threadripper box in my basement with the latest Ubuntu 21.10 build on it and install samba on bare metal. I had originally thought about doing this on a VM or on a Docker container, but I want the reliability and control-ability of a bare metal install with a static IP address, etc. Therefore, after carefully backing up the local files that I wanted to save off of this machine (ha – that’s a lie, I just booted from a USB thumb drive and Gparted the drives with new partition tables), I installed a fresh copy of Ubuntu 21.10 with 3rd party drivers for my graphics card.\n\nOnce I had the base OS laid down, I used the canonical documentation from wiki.samba.org (not documentation from Canonical, the owner of Ubuntu <g>), along with some blog posts (1), (2), and (3) to determine my full course of action. I’ll outline the various steps below.\n\nActive Directory Domain Controller\n\nFirst things first, we need to get the network set up the way Samba wants it on this machine. That consists of setting up a static IP address on the two NICs in my server (one for my “secure” home network and one for my insecure “family” network) and setting the hostname and /etc/hosts file changes. Specifically, I used NetworkManager from the Ubuntu desktop to set the static IPs, the gateway and the netmasks and then modified /etc/hosts as follows:\n\n127.0.0.1 localhost 192.168.1.2 DC1.ad.example.com DC1\n\nIt is important to note that Ubuntu will put in an additional 127.0.0.1 line for your host and you need to (apparently, per the documentation) remove that. I then modified my /etc/hostname file as follows:\n\nDC1.ad.example.com\n\nNow for a fun one. We need to permanently change /etc/resolv.conf and not have Ubuntu overwrite it on the next boot. To do that, we have to:\n\n# systemctl stop systemd-resolved # systemctl disable systemd-resolved # unlink /etc/resolv.conf # vim /etc/resolv.conf nameserver 192.168.1.1 search ad.example.com\n\nAt this point, you should have the networking changes in place you need for now. You’ll have to later loop back around and change /etc/resolv.conf to use this machine’s IP address as the nameserver once you have Samba running with it’s built-in DNS server up and running but we don’t want to lose name resolution in the meanwhile so I’ve hard coded it to point to my local DNS server on OpenBSD.\n\nNow it’s time to install the necessary packages to make this machine an active directory domain controller:\n\n# apt update # apt install acl attr samba samba-dsdb-modules samba-vfs-modules winbind libpam-winbind libnss-winbind libpam-krb5 krb5-config krb5-user dnsutils net-tools smbclient\n\nSpecify the FQDN of your server when prompted on the ugly purple screens for things like your Kerberos server and your Administrative server.\n\nNow, it’s time to create the configuration files for Kerberos and Samba. To do this, I ran the following commands:\n\n# mv /etc/krb5.conf /etc/krb5.conf.orig # mv /etc/samba/smb.conf /etc/samba/smb.conf.orig # samba-tool domain provision --use-rfc2307 --interactive\n\nI take the defaults, being careful to double-check the DNS forwarder IP address (that’s where the DNS server that will be serving your AD network will forward requests it cannot resolve) and then entered my Administrator password. Keep in mind that be default, the password complexity requirements are set pretty high (which I like) so pick a good one.\n\nNow use the following command to move the Kerberos configuration file that was generated by the Samba provisioning process to its correct location:\n\n# cp /var/lib/samba/private/krb5.conf /etc/krb5.conf\n\nNext, we need to set things up so that the right services are started when you reboot the machine. To do that, issue the following commands:\n\n# systemctl mask smbd nmbd winbind # systemctl disable smbd nmbd winbind # systemctl stop smbd nmbd winbind # systemctl unmask samba-ad-dc # vim /etc/systemd/system/samba-ad-dc.service [Unit] Description=Samba Active Directory Domain Controller After=network.target remote-fs.target nss-lookup.target [Service] Type=forking ExecStart=/usr/sbin/samba -D PIDFILE=/run/samba/samba.pid ExecReload=/bin/kill -HUP $MAINPID [Install] WantedBy=multi-user.target # systemctl daemon-reload # systemctl enable samba-ad-dc # systemctl start samba-ad-dc\n\nNow go back and update the /etc/resolv.conf file to use the new Samba-supplied DNS service:\n\n# vim /etc/resolv.conf nameserver 192.168.1.2 search ad.example.com\n\nThis is probably a good time to reboot your machine. When you do so, don’t forget to check that /etc/resolv.conf hasn’t been messed with by Ubuntu. If it has, double-check the work you did above and keep trying reboots until it sticks.\n\nNow we need to create the reverse zone for DNS:\n\n# samba-tool dns zonecreate 192.168.1.2 168.192.in-addr.arpa -U Administrator # samba-tool dns add 192.168.1.2 168.192.in-addr.arpa 2.1 PTR DC1.ad.example.com -U Administrator\n\nIf you have multiple NICs in your AD server, you will need to repeat this process for their networks. At this point, double-check that the DNS responder is coming back with what it needs to in order to serve the black magic of the Active Directory clients:\n\n# nslookup DC1.ad.example.com Server: 192.168.1.2 Address: 192.168.1.2#53 Name: DC1.ad.example.com Address: 192.168.1.2 # nslookup 192.168.1.2 2.1.168.192.in-addr.arpa name = DC1.ad.exmple.com # host -t SRV _ldap._tcp.ad.example.com _ldap._tcp.ad.example.com has SRV record 0 100 389 dc1.ad.example.com # host -t SRV _kerberos._udp.ad.example.com _kerberos._udp.ad.example.com has SRV record 0 100 88 dc1.ad.example.com # host -t A dc1.ad.example.com dc1.ad.example.com has address 192.168.1.2\n\nIf you have multiple NICs in your AD server, you might want to double-check the DNS A records that are returned are reachable from the networks your clients typically use. Since I have a “home” network and a “secure” network, I can manage DNS and DHCP on my secure network so I tend to make sure that my domain controller hostname resolves to an IP address on the secure network. The Windows DHCP admin tools are pretty handy for checking on this and making changes.\n\nVerify that the Samba service has file serving running correctly by listing all of the shares from this server as an anonymous user:\n\n# smbclient -L localhost -N\n\nYou should see sysvol, netlogon and IPC$ listed. Any error about SMB1 being disabled is actually a good thing. Validate that a user can successfully log in:\n\n# smbclient //localhost/netlogon -UAdministrator -c 'ls'\n\nYou should see a listing of the netlogon share directory which should be empty. Now check that you can successfully authenticate against Kerberos:\n\n# kinit administrator # klist\n\nYou should see a message about when your administrator password will expire if you are successfully authenticated by Kerberos. The klist command should show the ticket that was generated by you logging in as Administrator.\n\nIf you look at the documentation in the Samba Wiki, you’ll see that ntp seems to be a better service to use over chrony or optnntpd. If you look at the documentation for chrony (which everyone seems to use), you’ll get a different story. However, when I used chrony, I kept getting NTP errors on my Windows clients so I’m configuring in this post with ntp.\n\n# apt install ntp # samba -b | grep 'NTP' NTP_SIGND_SOCKET_DIR: /var/lib/samba/ntp_signd # chown root:ntp /var/lib/samba/ntp_signd/ # chmod 750 /var/lib/samba/ntp_signd/ # vim /etc/ntp.conf restrict 192.168.1.0 mask 255.255.255.0 nomodify notrap broadcast 192.168.1.255 disable auth broadcastclient # systemctl restart ntp\n\nTo be clear, the lines I’m showing after editing the ntp.conf file are lines that you ADD to the file. Also, if you have more than one NIC in the server, you’ll need to add them in on the restrict and broadcast lines as a second line for each.\n\nNow, let’s test that everything is working by enrolling a Windows 10 machine into the domain. Ensure first that you are on the right network and just for safety’s sake, do a reboot so you pick up the DNS server, etc. I have modified the DHCP server on my network to pass the correct information that a client needs as follows (from /etc/dhcpd.conf in OpenBSD):\n\noption domain-name \"ad.example.com\"; option domain-name-servers 192.168.1.2; option ntp-servers 192.168.1.2;\n\nMicrosoft has done a bang-up job of hiding this in the UI compared to where it has been for literally decades (“get off my lawn!!”). I prefer the old-fashioned way so I ran the following using Windows key + R to get the old UI I’m most comfortable with:\n\nsysdm.cpl\n\nPress the “Change” button and then select “Domain” and enter “ad.example.com” as the name of your domain. That should prompt you for your admin credentials. I typically use AD\\administrator as my userid just to be safe. In a matter of seconds, you should be welcomed to the domain.\n\nFor safety’s sake, I recommend clearing out your application and system event logs on that machine, rebooting and logging in as your domain admin. Once that’s done, examine the event viewer to ensure that you aren’t seeing any errors that might indicate something isn’t configured correctly on the server. Remember to click the “other user” button on the Windows 10 login screen and use the AD\\Administrator to tell Windows which domain you want to log into.\n\nThere is a warning (DNS Client Events, Event ID: 8020) that I see in the System event log. This appears to be a problem where the Windows machine tries to re-register with dynamic DNS in Samba with exactly the same info that is already registered for it and Samba returns an error. You can still resolve the client machine from the server so it worked the first time, I think it can be safely ignored for now.\n\nFor ease of maintenance you might want to install the “Windows RSAT Tools” on your Windows machine that give you a good UI for managing all of the fun stuff that Active Directory brings to the table. They are a free download.\n\nI really do NOT recommend using your domain controller as a file server. To set that up on another machine, please see the next section.\n\nSamba File Server in a Domain\n\nThankfully, the wonderful documentation on the Samba WIKI has an entire entry dedicated to setting up Samba as a domain member. First things first, we need to configure the network settings on our file server to use the Active Directory server as the DNS server.\n\nAs I did with the domain controller above, I used NetworkManager from the Ubuntu desktop to set the static IPs, the gateway and the netmasks and then modified /etc/hosts as follows:\n\n127.0.0.1 localhost 192.168.1.3 NAS.ad.example.com NAS\n\nIt is important to note that Ubuntu will put in an additional 127.0.0.1 line for your host and you need to (apparently, per the documentation) remove that. I then modified my /etc/hostname file as follows:\n\nNAS.ad.example.com\n\nWe need to permanently change /etc/resolv.conf and not have Ubuntu overwrite it on the next boot. To do that, we have to:\n\n# systemctl stop systemd-resolved # systemctl disable systemd-resolved # unlink /etc/resolv.conf # vim /etc/resolv.conf nameserver 192.168.1.2 search ad.example.com\n\nAfter a quick reboot and verification that the resolv.conf changes survived, we need to install some packages:\n\n# apt install acl attr samba samba-dsdb-modules samba-vfs-modules winbind libpam-winbind libnss-winbind libpam-krb5 krb5-config krb5-user smbclient\n\nNow we need to now configure Kerberos and Samba. First, if there are files currently at /etc/krb5.conf and/or /etc/samba/smb.conf, remove them. Create a new /etc/krb5.conf file with the following contents:\n\n[libdefaults] default_realm = AD.EXAMPLE.COM dns_lookup_realm = false dns_lookup_kdc = true\n\nNext, it will be necessary to synchronize time to the domain controller. Since this server won’t be broadcasting network time to client machines (i.e. it isn’t a domain controller), I’ll be setting it up with chrony which is built into Ubuntu.\n\n# apt install chrony ntpdate # ntpdate 192.168.1.2 # vim /etc/chrony/chrony.conf server 192.168.1.2 minpoll 0 maxpoll 5 maxdelay .05 # systemctl enable chrony # systemctl start chrony\n\nThat line under the vim command should be the only line in the file. To validate that everything is working, a call to systemctl status chrony should show that it is active and running. First things first, we need to set up the /etc/samba/smb.conf file:\n\n[global] workgroup = AD security = ADS realm = AD.EXAMPLE.COM netbios name = NAS domain master = no local master = no preferred master = no idmap config * : backend = tdb idmap config * : range = 50000-100000 vfs objects = acl_xattr map acl inherit = Yes store dos attributes = Yes winbind use default domain = true winbind offline logon = false winbind nss info = rfc2307 winbind refresh tickets = Yes winbind enum users = Yes winbind enum groups = Yes\n\nNow we will need to join the domain:\n\n# kinit administrator # samba-tool domain join AD -U AD\\\\Administrator # net ads join -U AD\\\\Administrator\n\nYou’ll probably get a DNS error when you join the domain. Regardless, add an A record and a PTR record for the server into the DNS as follows:\n\n# samba-tool dns add 192.168.1.2 168.192.in-addr.arpa 3.1 PTR NAS.ad.example.com -U Administrator # samba-tool dns add 192.168.1.2 168.192.in-addr.arpa NAS.ad.example.com A 192.168.1.3\n\nIf you have multiple NICs in your file server, make sure you repeat the process for the IP address ranges assigned to them. Now, add the “winbind” parameter as follows to /etc/nsswitch.conf:\n\n# vim /etc/nsswitch.conf passwd: files winbind systemd group: files winbind systemd shadow: files winbind\n\nNext, we will need to enable and start and restart some services:\n\n# systemctl enable smbd nmbd winbind # systemctl start smbd nmbd winbind # pam-auth-update\n\nBefore proceeding any further, you should probably reboot the machine. Now for some tests to make sure that everything is working ok:\n\n# wbinfo --ping-dc checking the NETLOGON for domain[AD] dc connection to \"dc1.ad.example.com\" succeeded. # wbinfo -g ... list of domain groups ... # wbinfo -u ... list of domain users ... # getent group ... list of Linux groups and Windows groups... # getent passwd ... list of Linux users and Windows users...\n\nWindows Home Directories\n\nA common configuration done by Windows Domain administrators is to create a default “Home” drive (typically mapped to the H: drive letter) for users. To do this, we will want to first set up a file share on the server. The goal will be to set up a mapped “HOME” directory for each domain user. We’ll start off by adding the following to the /etc/samba/smb.conf file:\n\n[users] comment = Home directories path = /path/to/folder read only = no acl_xattr:ignore system acls = yes\n\nAfter issuing an “smbcontrol all reload-config” on the file server to reload the changes to the config file, you should now be able to see a share called \\\\nas\\users. When you create the directory on the filesystem, issue the following commands:\n\n# chown \"Administrator\":\"Domain Users\" /path/to/folder/ # chmod 0770 /path/to/folder/\n\nIt is important to grant the “SeDiskOperatorPrivilege” to the “Domain Admins” group as follows. This has to be done on the file server itself.\n\n# net rpc rights grant \"AD\\\\Domain Admins\" SeDiskOperatorPrivilege -U \"AD\\administrator\"\n\nFinally, from the “Active Directory Users and Groups”, select the user in the “Users” folder, right click and select “Properties”. After changing to the “Profile” tab, select the “Connect” radio button under the “Home folder”, choose H: as the drive letter and put in \\\\nas\\users\\{user name} for the “To:” entry field. This should automatically create the directory and set the correct permissions on it.\n\nNow log out of the domain and back in as the user account you modified above and you should automatically get an H: drive that maps to that folder on the file server.\n\nUser Profiles\n\nOK, so the cool kids on their Windows networks also have this thing called a “Roaming User Profile” that allows you to put their user profile on a file server and then they can move from one machine to another and simply access their stuff as if it was all the same machine. I wanted to see how Samba handled this and sure enough, I got a hit in the Samba wiki that indicated it was possible.\n\nFirst things first, we need to create a share on our file server to hold the profiles, so I added this to my /etc/samba/smb.conf file:\n\n[profiles] comment = Users profiles path = /path/to/profile/directory browseable = No read only = No csc policy = disable vfs objects = acl_xattr acl_xattr:ignore system acls = yes\n\nAfter making that change, I need to create the directory to hold the profiles and set the UNIX ownership and permissions like I did with the home directories above:\n\n# mkdir /path/to/profile/directory # chown \"AD\\Administrator\":\"AD\\Domain Users\" /path/to/profile/directory # chmod 0700 /path/to/profile/directory\n\nAfter a quick “smbcontrol all reload-config” to pull the new changes in, we now have a share on the file server called “profiles” that will hold the resulting Windows user profiles. I used the “Active Directory Users & Computers” tool on my Windows machine (logged in as Administrator), opened the property dialog for my users, navigated to the “Profile” tab and entered the UNC name for the profile directory \\\\NAS\\profiles\\{user-name}. The key is to know that, depending on the version of Windows, the system will add a suffix (in my case “.v6”) to that directory name and it will initially be created empty. When you log out, it will actually copy the stuff into the directory and you should see the directories and files show up on your file server. It seems this is the consistent behavior. For example, saving a file into the “Documents” directory on the Windows machine isn’t propagated to the server’s file system until that user logs out.\n\nIt really was that easy!\n\nGroup Policy\n\nGiven the fact that I had, at this point a fully functional Active Directory infrastructure with network home directories, roaming user profiles and all of it was running on Open Source platforms, I thought I’d really try to push it over the edge and dip my toe in the water around Group Policy. Group Policy is some magic stuff based on LDAP that, in the Windows world, allows you to automatically configure an end-user’s workstation. I found documentation in the Samba wiki that indicated it was possible to make this work so I thought I’d give it a try and see what I needed to do.\n\nIt looked like the first thing I needed to do was load the Samba “ADMX” templates into the AD domain controller. To do that, I used the following command:\n\n# samba-tool gpo admxload -H dc1.ad.example.com -U Administrator\n\nSure enough, logging into my Windows machine as a domain admin, I was able to see that the command had indeed injected the Samba files into the Sysvol:\n\nH:\\> dir \\\\DC1\\SysVol\\ad.example.com\\Policies\\PolicyDefinitions\n\nThat command aove should show you the en-US directory and the samba.admx file. Now we need to download the Microsoft ADMX templates and install them:\n\n# apt install msitools # cd /tmp # wget 'https://download.microsoft.com/download/3/0/6/30680643-987a-450c-b906-a455fff4aee8/Administrative%20Templates%20(.admx)%20for%20Windows%2010%20October%202020%20Update.msi' # msiextract Administrative\\ Templates\\ \\(.admx\\)\\ for\\ Windows\\ 10\\ October\\ 2020\\ Update.msi # samba-tool gpo admxload -U Administrator --admx-dir=Program\\ Files/Microsoft\\ Group\\ Policy/Windows\\ 10\\ October\\ 2020\\ Update\\ \\(20H2\\)/PolicyDefinitions/\n\nThe last line will take a few seconds as it processes the files and loads them into the SysVol. You can again confirm the presence of the new policies using the “dir” command above from your Windows machine. At this point, you have the group policies set up and installed into your environment and should be able to manipulate them using the “Group Policy Management Console” on your Windows workstation.\n\nConclusion\n\nWhile this is probably one of my stranger, and more technical posts, I think this is a cool example of how you can totally eliminate paid software from your server infrastructure and yet still have the full functionality of something like Active Directory in your tool belt.\n\nAs long-time readers of this blog are aware, I’m a bit of a Thinkpad fanatic. I fell in love with these durable machines when I was working for IBM back in the late 90’s and accidentally had one fall out of my bag, bounce down the jetway stairs and hit the runway hard – amazingly enough it had a few scuffs but zero damage! After the purchase of the brand by Lenovo, I was a bit worried, but they continue to crank out (at least in the Thinkpad T and X model lines) high-quality, powerful machines.\n\nThinkpad T480 – RIP\n\nI ran into a nasty problem with my Thinkpad T480 where the software on the machine actually physically damaged the hardware. I know! I thought that was impossible too (other than the 70’s PET machine that had a software-controlled relay on the motherboard that you could trigger continuously until it burned out) but nope – the problem is real.\n\nEssentially, the Thunderbolt I/O port on the machine is driven by firmware running out of an NVRAM chip on the motherboard that can be software-updated as new firmware comes out. As with any NVRAM chip, there are a finite number of write-cycles before the chip dies, but the number of times you will update your firmware is pretty small so it works out well.\n\nUnfortunately, Lenovo pushed out a firmware update that wrote continuously to the NVRAM chip and if you didn’t patch fast enough (they did release an urgent/critical update), then the write-cycles would be exceeded, the chip would fail and the bring-up code would not detect the presence of the bus and thus you had no more Thunderbolt on the laptop. Well, I didn’t update fast enough so “boom” – it is now a Thunderbolt-less laptop.\n\nThe New T124 (AMD) Gen 2\n\nWell, enter the need for a new laptop. I decided to jump ship from the Intel train and try life out on the “other side” but ordering a Thinkpad T14 (AMD) Gen 2 machine with 16gb of soldered RAM (there is a slot that I will be populating today that can take it up to 48gb max – I’m going with 32gb total by installing an $80 16gb DIMM) and the Ryzen Pro 5650U that has 6 cores and 12 threads of execution. The screen was a 1920×1080 400 nit panel and looks really nice.\n\nWhen the laptop showed up, I booted the OpenBSD installer from 6.9-current and grabbed a dmesg and discovered that I lost the Lenovo lottery and had a Realtek WiFi card in the machine. Well, the good news was that I had upgraded the card in my T480 to an Intel AX200 so I swapped it for the one I took out of the T480 and then used it in the T14 to replace the Realtek card. Worked like a charm.\n\nThe Ethernet interface on this machine is a bit odd. It’s a Realtek chipset as well, but it shows up as two interfaces (re0 and re1). The deal is that re0 is the interface that is exposed when the machine is plugged into a side-connecting docking station and re1 is the interface that is connected to the built-in Ethernet port. The device driver code that is in 6.9-current as of this writing works just fine with it, however, so I’m happy.\n\nNow for the bad news. Every Thinkpad I have owned for the last decade allows me to plug an m.2 2240 SATA drive into the WWAN slot and it works great. I assumed that would be the case with this machine. While I had the bottom off to replace the WiFi card, I slipped the 1TB drive from the WWAN slot of my T480 into the WWAN slot of the T14 and booted up. I was immediately presented with an error message stating effectively that the WWAN slot was white-listed by Lenovo and would only accept “approved” network cards. I was beyond frustrated by this.\n\nGiven that I want to get this machine into my production workflow, I decided that I’d slog along for the time being by putting a larger m.2 2280 NVMe drive in, installing rEFInd to allow me to boot multiple partitions from a single drive and then clone the 512gb drive that is in the machine to the 1GB drive out of the T480. Then, the remaining space on the new drive will contain an encrypted partition for my OpenBSD install.\n\nInstalling rEFInd\n\nI followed the instructions from the rEFInd site on how to manually install under Windows 10 and the steps I followed included downloading and unpacking the ZIP file and then running the following commands from an administrative command prompt:\n\nC:\\Users\\xxxx\\Downloads\\refind-bin-0.13.2\\> mountvol R: /s C:\\Users\\xxxx\\Downloads\\refind-bin-0.13.2\\> xcopy /E refind R:\\EFI\\refind\\ C:\\Users\\xxxx\\Downloads\\refind-bin-0.13.2\\> r: R:\\> cd \\EFI\\refind R:\\EFI\\refind\\> del /s drivers_aa64 R:\\EFI\\refind\\> del /s drivers_ia32 R:\\EFI\\refind\\> del /s tools_aa64 R:\\EFI\\refind\\> del /s tools_ia32 R:\\EFI\\refind\\> del refind_aa64.efi R:\\EFI\\refind\\> del refind_x64.efi R:\\EFI\\refind\\> rmdir drivers_aa64 R:\\EFI\\refind\\> rmdir drivers_ia32 R:\\EFI\\refind\\> rmdir tools_aa64 R:\\EFI\\refind\\> rmdir tools_ia32 R:\\EFI\\refind\\> rename refind.conf-sample refind.conf R:\\EFI\\refind\\> mkdir images R:\\EFI\\refind\\> copy C:\\Users\\xxx\\Pictures\\mtstmichel.jpg images R:\\EFI\\refind\\> bcdedit /set \"{bootmgr}\" path \\EFI\\refind\\refind_x64.efi\n\nThat next to the last line is because I wanted to have a picture of my “happy place” (Mount Saint Michel off of the northern coast of France) as the background for rEFInd. I edited the refind.conf file and added the following lines:\n\nbanner images\\mtstmichel.jpg banner_scale fillscreen\n\nA quick reboot shows that rEFInd is installed correctly and has my customized background. Don’t be alarmed that the first time you boot up with rEFInd is slow, I think it is doing some scanning and processing and caching because the second and subsequent boots are faster.\n\nCloning the Drives\n\nThe process that I am going to follow, at a high level, is to first clone the contents of my primary 1TB 2280 NVMe drive in my T480 to a spare 256GB drive. I will then erase the 1TB drive and clone the contents of my T14’s drive to it (it’s only 512GB). I will then erase the 512GB drive and clone the 256GB drive back to it. Finally, for good operational security (OpSec) purposes, I’ll use the open source Windows program Eraser erase the 256GB drive. At this point I should have a bootable T480 (with a fried Thunderbolt bus – grr…) on the 512GB drive, and a bootable T14 on the 1TB drive.\n\nI’m using Clonezilla, an open source tool that I burn to a bootable USB drive to do the cloning. For hardware that I am using to accomplish all of this, first I use a Star Tech device that allows me to plug m.2 drives into a little box that then acts as a 2.5 inch SSD drive. I plug that into a Wavlink USB drive docking station that can hold either 3.5″ or 2.5″ drives.\n\nAnother piece of software that I use as part of this process is GPartEd Live – an open source tool that allows you to create a USB drive that boots into the GPartEd software (the Gnu Partition Editor). This allows me to view the partition structure o"
    }
}
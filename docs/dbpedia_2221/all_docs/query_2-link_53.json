{
    "id": "dbpedia_2221_2",
    "rank": 53,
    "data": {
        "url": "https://arxiv.org/html/2404.18890v1",
        "read_more_link": "",
        "language": "en",
        "title": "Hide and Seek: How Does Watermarking Impact Face Recognition?",
        "top_image": "",
        "meta_img": "",
        "images": [
            "https://arxiv.org/html/x1.png",
            "https://arxiv.org/html/x2.png",
            "https://arxiv.org/html/x3.png",
            "https://arxiv.org/html/x5.png",
            "https://arxiv.org/html/x6.png",
            "https://arxiv.org/html/x7.png"
        ],
        "movies": [],
        "keywords": [],
        "meta_keywords": [
            ""
        ],
        "tags": null,
        "authors": [],
        "publish_date": null,
        "summary": "",
        "meta_description": "",
        "meta_lang": "en",
        "meta_favicon": "",
        "meta_site_name": "",
        "canonical_link": null,
        "text": "Yuguang Yao∗ Steven Grosz∗ Sijia Liu Anil Jain\n\nMichigan State University\n\nAbstract\n\nThe recent progress in generative models has revolutionized the synthesis of highly realistic images, including face images. This technological development has undoubtedly helped face recognition, such as training data augmentation for higher recognition accuracy and data privacy. However, it has also introduced novel challenges concerning the responsible use and proper attribution of computer generated images. We investigate the impact of digital watermarking, a technique for embedding ownership signatures into images, on the effectiveness of face recognition models. We propose a comprehensive pipeline that integrates face image generation, watermarking, and face recognition to systematically examine this question. The proposed watermarking scheme, based on an encoder-decoder architecture, successfully embeds and recovers signatures from both real and synthetic face images while preserving their visual fidelity. Through extensive experiments, we unveil that while watermarking enables robust image attribution, it results in a slight decline in face recognition accuracy, particularly evident for face images with challenging poses and expressions. Additionally, we find that directly training face recognition models on watermarked images offers only a limited alleviation of this performance decline. Our findings underscore the intricate trade off between watermarking and face recognition accuracy. This work represents a pivotal step towards the responsible utilization of generative models in face recognition and serves to initiate discussions regarding the broader implications of watermarking in biometrics.\n\n1 Introduction\n\nRecent advancements in generative modeling, particularly in diffusion models (DMs) [1, 2, 3], have demonstrated remarkable capabilities in synthesizing complex and highly realistic images. However, as this technology evolves, it brings forth new considerations regarding its safety and ethical use. These emerging issues include harmful content generation [4, 5], deep fakes [6, 7], privacy leakage [8, 9], copyright usurpation [10, 11, 12], and societal biases and stereotyping [13, 14]. Therefore, significant efforts have been made to avoid the misuse of generative models and to enhance their safety, security, and trustworthiness [15]. For example, the watermarking technology [16, 17, 18, 19], which we will focus on in this work, entails embedding a distinctive signature into image generation. This approach enables differentiation between real and synthesized images and can be used to protect image copyrights and ascertain ownership.\n\nAmong the myriad applications of generative models, face image generation has garnered considerable attention recently. For example, the generation of high-fidelity synthetic face images enables model developers to train face recognition systems solely using synthetic data, thereby circumventing the risk of compromising the privacy of real individuals [20]. Augmenting existing training sets with synthesized face images can also help mitigate racial biases and tackle data imbalance problems [21, 22]. Furthermore, generative models have been employed to manipulate or enhance face images [23, 24, 25, 26]. Recent advances in text to image generation using denoising diffusion probabilistic models (DDPM) have allowed for synthesizing highly realistic, novel views and appearances of a given face image, while preserving the identity.\n\nDespite the extensive use of generative models in face recognition, there has been limited research delving into the implications of integrating watermarking into these models and how their watermarked image generations influence their efficacy in face recognition tasks. In particular, we ask:\n\nTo address (Q), we explore the influence of existing digital watermarking technology in face recognition. The proposed system, as shown in Fig. 1, comprises a diffusion model (DM) for face image generation, an encoder-decoder-based network for embedding a watermark signature into synthetic images, and a face recognition model trained on watermarked data. Assessing through face verification tasks, aimed at determining whether two face images belong to the same person, we observe that watermarking retains effectiveness throughout the generation and recognition processes. Nonetheless, we do notice a minor decline in face recognition accuracy, particularly noticeable when comparing challenging face images.\n\nContributions.\n\nWe summarize our contributions below.\n\n∙∙\\bullet∙ We make a novel exploration into the impact of watermarked face images on face recognition accuracy.\n\n∙∙\\bullet∙ We investigate face recognition through a unified framework that integrates watermarking in face image generation. This facilitates us to train and evaluate face recognition systems with watermarked synthetic data.\n\n∙∙\\bullet∙ Through extensive experiments, we showcase the effectiveness of watermarking in face generation and recognition. We highlight its resilience against post-hoc image transformations and offer valuable insights into the successes and challenges encountered when employing watermarked images for face recognition training.\n\n2 Related Work\n\nFace image generation.\n\nRecent advancements in generative modeling have revolutionized the synthesis of high-quality face images. Generative Adversarial Networks (GANs) [27] have been extensively employed for face image generation, exemplified by notable architectures like StyleGAN [28] and StarGAN [29]. These models adeptly translate random noise vectors into realistic face images, facilitating the creation of diverse and visually appealing facial representations. However, GANs often encounter issues with training instability and mode collapse, hindering their capacity to capture the full spectrum of facial variations.\n\nMore recently, diffusion models [30, 31] have emerged as a compelling alternative for high-fidelity image generation. Operating by iteratively denoising a Gaussian noise signal and guided by a learned noise schedule, diffusion models excel in generating highly realistic and varied face images, surpassing GANs in many respects. Prominent examples of diffusion-based face generation models include DDPM [31] and DCFace [20]. The advent of sophisticated face image generation techniques has introduced new avenues for data augmentation, privacy preservation, and creative applications [32, 33, 34]. However, it has also raised concerns regarding potential misuse, such as the creation of deepfakes [35] or identity theft. These apprehensions underscore the necessity for methods ensuring the responsible use and attribution of computer generated face images.\n\nDigital watermarking.\n\nDigital watermarking is a technique for embedding hidden information, known as a watermark, into digital content. While the watermark can be designed to be visible or invisible, we deal with watermarks which are imperceptible to human observers but can be detected and extracted by specialized algorithms. Digital watermarking has a wide range of applications, including copyright protection, content authentication, and data provenance [36, 37, 38, 39]. Early works on digital watermarking focused on developing techniques that relied on transform-domain techniques, such as discrete cosine transform (DCT) [40, 41], discrete wavelet transform (DWT) [42], or singular value decomposition (SVD) [43], to embed the watermark in the frequency or singular value domain of the image.\n\nWith the advent of deep learning, researchers have explored the use of neural networks for digital watermarking. These learning-based approaches can automatically learn to embed and extract watermarks from images, potentially offering improved robustness and capacity compared to traditional methods. Notable examples include HiDDeN [17], which uses an encoder-decoder architecture to hide watermarks in images, and DeepStego [44], which employs adversarial learning to enhance the imperceptibility of the watermark. There have also been works studying watermarking for GAN images [45, 46, 47, 48, 49]. However, the impact of watermarking on face recognition has not been well studied. Our work aims to address this gap by investigating the interplay between digital watermarking and face recognition.\n\nFace recognition.\n\nFace recognition technology has garnered substantial attention in computer vision research due to its diverse applications in security, surveillance, and human-computer interaction. Early face recognition methods predominantly relied on handcrafted features and classic machine learning algorithms. Techniques such as Eigenfaces [50], Local Binary Patterns [51], and Histogram of Oriented Gradients [52] have been widely used for feature extraction, whereas classifiers like Support Vector Machines [53] and k-Nearest Neighbors [54] were used for reocgnition.\n\nThe advent of deep learning revolutionized face recognition, where Convolutional Neural Networks (CNNs), such as DeepFace by Taigman et al. [55], enabled significant improvements in accuracy and robustness. Since then, many different types of loss functions (e.g., SphereFace [56], CosFace [56], ArcFace [57], etc.) and model architectures (e.g., VGG [58], ResNet [59], ViT [60], etc.) for large-scale face recognition were proposed, further pushing the boundaries of face recognition. These algorithms relied on the availability of large-scale face recognition datasets for their success. Yet, many of these datasets, such as the popular MS-Celeb-1M [61], have now been recalled due to privacy concerns. This has prompted many researchers to turn toward developing methods for realistic face image generation to replace these large-scale face recognition datasets.\n\n3 Methodology\n\nTo tackle the question (Q) posed in Sec. 1, this section introduces the watermarking, face generation, and face recognition techniques utilized in constructing the proposed system, as depicted in Fig. 1.\n\n3.1 Digital Watermarking\n\nDigital watermarking embeds hidden information, known as a ‘watermark’ or ‘signature’, into digital content like images. Such watermarks are often imperceptible to humans but detectable by specialized algorithms. In the case of (synthesized) face images, watermarking serves to track their origin and deter unauthorized usage or distribution [16, 17, 18, 19].\n\nTo be concrete, the watermarking problem can be defined as follows: Given an input image 𝐈∈ℝH×W×C𝐈superscriptℝ𝐻𝑊𝐶\\mathbf{I}\\in\\mathbb{R}^{H\\times W\\times C}bold_I ∈ blackboard_R start_POSTSUPERSCRIPT italic_H × italic_W × italic_C end_POSTSUPERSCRIPT and an L𝐿Litalic_L-bit watermark message 𝐦∈{0,1}L𝐦superscript01𝐿\\mathbf{m}\\in\\{0,1\\}^{L}bold_m ∈ { 0 , 1 } start_POSTSUPERSCRIPT italic_L end_POSTSUPERSCRIPT (i.e., digital signature), our goal is to produce a watermarked image 𝐈wsubscript𝐈w\\mathbf{I}_{\\mathrm{w}}bold_I start_POSTSUBSCRIPT roman_w end_POSTSUBSCRIPT that maintains visual similarity to the original image 𝐈𝐈\\mathbf{I}bold_I while incorporating the watermark message 𝐦𝐦\\mathbf{m}bold_m. In addition, the watermarked image 𝐈wsubscript𝐈w\\mathbf{I}_{\\mathrm{w}}bold_I start_POSTSUBSCRIPT roman_w end_POSTSUBSCRIPT should be designed such that it allows for the reverse engineering of its encoded message 𝐦𝐦\\mathbf{m}bold_m, enabling the deciphering of the image’s origin.\n\nTo implement the watermarking system, we employ the neural network-based HiDDeN framework [17] due to its proven performance while any other watermarking scheme can be used as well; see Fig. 2 for a schematic overview. This watermarking system consists of an encoder network f𝜽subscript𝑓𝜽f_{\\boldsymbol{\\theta}}italic_f start_POSTSUBSCRIPT bold_italic_θ end_POSTSUBSCRIPT and a decoder network gϕsubscript𝑔bold-italic-ϕg_{\\boldsymbol{\\phi}}italic_g start_POSTSUBSCRIPT bold_italic_ϕ end_POSTSUBSCRIPT. The encoder takes the input image 𝐈𝐈\\mathbf{I}bold_I and the watermark message 𝐦𝐦\\mathbf{m}bold_m as inputs and generates the watermarked image 𝐈w=f𝜽⁢(𝐈,𝐦)subscript𝐈wsubscript𝑓𝜽𝐈𝐦\\mathbf{I}_{\\mathrm{w}}=f_{\\boldsymbol{\\theta}}(\\mathbf{I},\\mathbf{m})bold_I start_POSTSUBSCRIPT roman_w end_POSTSUBSCRIPT = italic_f start_POSTSUBSCRIPT bold_italic_θ end_POSTSUBSCRIPT ( bold_I , bold_m ). The decoder takes the watermarked image 𝐈wsubscript𝐈w\\mathbf{I}_{\\mathrm{w}}bold_I start_POSTSUBSCRIPT roman_w end_POSTSUBSCRIPT as input and reconstructs the embedded watermark message 𝐦^=gϕ⁢(𝐈w)^𝐦subscript𝑔bold-italic-ϕsubscript𝐈w\\hat{\\mathbf{m}}=g_{\\boldsymbol{\\phi}}(\\mathbf{I}_{\\mathrm{w}})over^ start_ARG bold_m end_ARG = italic_g start_POSTSUBSCRIPT bold_italic_ϕ end_POSTSUBSCRIPT ( bold_I start_POSTSUBSCRIPT roman_w end_POSTSUBSCRIPT ).\n\nThe encoder and decoder networks are jointly trained using a combination of image reconstruction loss and message decoding loss. The image reconstruction loss ℓreconssubscriptℓrecons\\ell_{\\mathrm{recons}}roman_ℓ start_POSTSUBSCRIPT roman_recons end_POSTSUBSCRIPT (e.g., mean squared error) ensures that the watermarked image is visually similar to the original image, while the message decoding loss ℓdecodesubscriptℓdecode\\ell_{\\mathrm{decode}}roman_ℓ start_POSTSUBSCRIPT roman_decode end_POSTSUBSCRIPT (e.g., bit-wise binary cross-entropy loss) minimizes the difference between the embedded and extracted watermark messages. The overall training objective of the watermarking system is then given by\n\nmin𝜽,ϕ⁡𝔼𝐈,𝐦⁢[ℓrecons⁢(𝐈w,𝐈)+λ⁢ℓdecode⁢(gϕ⁢(𝐈w),𝐦)],subscript𝜽bold-italic-ϕsubscript𝔼𝐈𝐦delimited-[]subscriptℓreconssubscript𝐈w𝐈𝜆subscriptℓdecodesubscript𝑔bold-italic-ϕsubscript𝐈w𝐦\\displaystyle\\displaystyle\\min_{\\boldsymbol{\\theta},\\boldsymbol{\\phi}}\\,\\,% \\mathbb{E}_{\\mathbf{I},\\mathbf{m}}\\left[\\ell_{\\mathrm{recons}}(\\mathbf{I}_{% \\mathrm{w}},\\mathbf{I})+\\lambda\\ell_{\\mathrm{decode}}(g_{\\boldsymbol{\\phi}}(% \\mathbf{I}_{\\mathrm{w}}),\\mathbf{m})\\right],roman_min start_POSTSUBSCRIPT bold_italic_θ , bold_italic_ϕ end_POSTSUBSCRIPT blackboard_E start_POSTSUBSCRIPT bold_I , bold_m end_POSTSUBSCRIPT [ roman_ℓ start_POSTSUBSCRIPT roman_recons end_POSTSUBSCRIPT ( bold_I start_POSTSUBSCRIPT roman_w end_POSTSUBSCRIPT , bold_I ) + italic_λ roman_ℓ start_POSTSUBSCRIPT roman_decode end_POSTSUBSCRIPT ( italic_g start_POSTSUBSCRIPT bold_italic_ϕ end_POSTSUBSCRIPT ( bold_I start_POSTSUBSCRIPT roman_w end_POSTSUBSCRIPT ) , bold_m ) ] , (1)\n\nwhere recall that 𝐈w=f𝜽⁢(𝐈,𝐦)subscript𝐈wsubscript𝑓𝜽𝐈𝐦\\mathbf{I}_{\\mathrm{w}}=f_{\\boldsymbol{\\theta}}(\\mathbf{I},\\mathbf{m})bold_I start_POSTSUBSCRIPT roman_w end_POSTSUBSCRIPT = italic_f start_POSTSUBSCRIPT bold_italic_θ end_POSTSUBSCRIPT ( bold_I , bold_m ), and λ𝜆\\lambdaitalic_λ is a regularization parameter to strike a balance between the image reconstruction and the message decoding losses. The training dataset is given by MS-COCO [62].\n\nIt is also worth noting that during training, a random message generator is employed to produce random bits in 𝐦𝐦\\mathbf{m}bold_m (as depicted in Fig. 2), which are then combined with the image input at the input of the encoder network. The resilience to random bit messages allows us to utilize a pre-trained encoder-decoder-based watermarking system directly to embed a user-defined watermark message in face images, which we will employ subsequently. In addition, the watermark training integrates data augmentations. That is, watermarked images undergo diverse transformations, including random cropping, resizing, and compression, mimicking real-world scenarios. Through training with augmented data, the encoder and decoder networks learn to embed and extract watermarks resilient to these transformations. As will be evident later, this renders it an effective approach for watermarking generated face images, ensuring the robustness of decoding the watermark against external image transformations.\n\n3.2 Integration of Watermark into Face Images\n\nWe begin by incorporating a designated watermark into face images. This integration is accomplished through the encoder network (f𝜽subscript𝑓𝜽f_{\\boldsymbol{\\theta}}italic_f start_POSTSUBSCRIPT bold_italic_θ end_POSTSUBSCRIPT) of the trained watermarking system. As shown in Fig. 3, we select the ‘S𝑆Sitalic_S’ letter (represented as a binary image) as the watermark message, with its pixel values utilized for the binary message, denoted as 𝐦Ssubscript𝐦𝑆\\mathbf{m}_{S}bold_m start_POSTSUBSCRIPT italic_S end_POSTSUBSCRIPT. Therefore, given a face image 𝐈𝐈\\mathbf{I}bold_I, its watermarked version is now given by 𝐈w=f𝜽⁢(𝐈,𝐦S)subscript𝐈wsubscript𝑓𝜽𝐈subscript𝐦𝑆\\mathbf{I}_{\\mathrm{w}}=f_{\\boldsymbol{\\theta}}(\\mathbf{I},\\mathbf{m}_{S})bold_I start_POSTSUBSCRIPT roman_w end_POSTSUBSCRIPT = italic_f start_POSTSUBSCRIPT bold_italic_θ end_POSTSUBSCRIPT ( bold_I , bold_m start_POSTSUBSCRIPT italic_S end_POSTSUBSCRIPT ).\n\nIn addition to the specificity of the watermark message, we also consider two sources of face images, including both synthetic and real face images. The synthetic image source is DCFace [20], a state-of-the-art diffusion-based face generation model. DCFace is built upon DMs (diffusion models) to enhance the quality and controllability of generated face images conditioned on individual identities and face variations. It excels at producing face images of a specific identity across various styles while allowing for precise control, thereby enriching face recognition models with augmented training data of large intra-class variance. We utilize DCFace to simulate the scenario where synthesized face images require integration with the watermark 𝐦Ssubscript𝐦𝑆\\mathbf{m}_{S}bold_m start_POSTSUBSCRIPT italic_S end_POSTSUBSCRIPT before their downstream applications, like face recognition. The second image source consists of commonly-used real face image datasets, such as CASIA-WebFace [21] and MS-Celeb-1M v2 (MS1Mv2) [57], which include face images of real identities. We will apply watermarking to both image sources respectively and investigate its effects on face recognition.\n\nThe performance of image watermarking is evaluated by assessing the reconstruction accuracy of the watermark message using the decoder network gϕsubscript𝑔bold-italic-ϕg_{\\boldsymbol{\\phi}}italic_g start_POSTSUBSCRIPT bold_italic_ϕ end_POSTSUBSCRIPT applied to the watermarked image 𝐈wsubscript𝐈w\\mathbf{I}_{\\mathrm{w}}bold_I start_POSTSUBSCRIPT roman_w end_POSTSUBSCRIPT. In particular, the watermark is expected to exhibit robustness against post-hoc image transformations, ensuring the reliable extraction of the watermark message. Fig. 4 shows the effectiveness of image watermarking against a sequence of post-watermarking data transformations under both the synthetic image source DCFace and the real image source CASIA-WebFace.\n\n3.3 Face Recognition on Watermarked Images\n\nWith a dataset of watermarked face images in hand, we proceed to conduct the face recognition task. In this study, our face recognition setup adheres to AdaFace [63], while any other face recognition algorithm can be used as well. Given an input face image 𝐈𝐈\\mathbf{I}bold_I, a face recognition model h𝝍subscriptℎ𝝍h_{\\boldsymbol{\\psi}}italic_h start_POSTSUBSCRIPT bold_italic_ψ end_POSTSUBSCRIPT maps the image to a compact feature representation 𝐳𝐳\\mathbf{z}bold_z: 𝐳=h𝝍⁢(𝐈)𝐳subscriptℎ𝝍𝐈\\mathbf{z}=h_{\\boldsymbol{\\psi}}(\\mathbf{I})bold_z = italic_h start_POSTSUBSCRIPT bold_italic_ψ end_POSTSUBSCRIPT ( bold_I ), where 𝝍𝝍\\boldsymbol{\\psi}bold_italic_ψ represents the learnable parameters of the model. The feature representation 𝐳𝐳\\mathbf{z}bold_z is typically obtained from the penultimate layer of a CNN, such as ResNet [59] or MobileFaceNet [64]. The model is trained to minimize a classification loss, such as the softmax loss [57] or the margin-based loss [56, 65, 57], which reduces the intra-class variance in the embedding space while increasing the inter-class variance.\n\nDuring inference, the learned face recognition model extracts the feature representation 𝐳psubscript𝐳p\\mathbf{z}_{\\mathrm{p}}bold_z start_POSTSUBSCRIPT roman_p end_POSTSUBSCRIPT for a probe face image 𝐈psubscript𝐈p\\mathbf{I}_{\\mathrm{p}}bold_I start_POSTSUBSCRIPT roman_p end_POSTSUBSCRIPT. The similarity between the probe face 𝐈psubscript𝐈p\\mathbf{I}_{\\mathrm{p}}bold_I start_POSTSUBSCRIPT roman_p end_POSTSUBSCRIPT and a reference face 𝐈rsubscript𝐈r\\mathbf{I}_{\\mathrm{r}}bold_I start_POSTSUBSCRIPT roman_r end_POSTSUBSCRIPT (with feature representation 𝐳rsubscript𝐳r\\mathbf{z}_{\\mathrm{r}}bold_z start_POSTSUBSCRIPT roman_r end_POSTSUBSCRIPT) is computed using a similarity metric, such as the cosine similarity\n\ns⁢(𝐳p,𝐳r)=𝐳pT⁢𝐳r|𝐳p|⁢|𝐳r|,𝑠subscript𝐳psubscript𝐳rsuperscriptsubscript𝐳p𝑇subscript𝐳rsubscript𝐳psubscript𝐳r\\displaystyle s(\\mathbf{z}_{\\mathrm{p}},\\mathbf{z}_{\\mathrm{r}})=\\frac{\\mathbf% {z}_{\\mathrm{p}}^{T}\\mathbf{z}_{\\mathrm{r}}}{|\\mathbf{z}_{\\mathrm{p}}||\\mathbf% {z}_{\\mathrm{r}}|},italic_s ( bold_z start_POSTSUBSCRIPT roman_p end_POSTSUBSCRIPT , bold_z start_POSTSUBSCRIPT roman_r end_POSTSUBSCRIPT ) = divide start_ARG bold_z start_POSTSUBSCRIPT roman_p end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_T end_POSTSUPERSCRIPT bold_z start_POSTSUBSCRIPT roman_r end_POSTSUBSCRIPT end_ARG start_ARG | bold_z start_POSTSUBSCRIPT roman_p end_POSTSUBSCRIPT | | bold_z start_POSTSUBSCRIPT roman_r end_POSTSUBSCRIPT | end_ARG , (2)\n\nwhere |⋅||\\cdot|| ⋅ | denotes the Euclidean norm. The probe face is considered to match the reference face if their similarity score exceeds a predefined threshold τ𝜏\\tauitalic_τ:\n\nmatch⁢(𝐳p,𝐳r)={1,if ⁢s⁢(𝐳p,𝐳r)≥τ,0,otherwise.matchsubscript𝐳psubscript𝐳rcases1if 𝑠subscript𝐳psubscript𝐳r𝜏0otherwise\\displaystyle\\text{match}(\\mathbf{z}_{\\mathrm{p}},\\mathbf{z}_{\\mathrm{r}})=% \\begin{cases}1,&\\text{if }s(\\mathbf{z}_{\\mathrm{p}},\\mathbf{z}_{\\mathrm{r}})% \\geq\\tau,\\\\ 0,&\\text{otherwise}.\\end{cases}match ( bold_z start_POSTSUBSCRIPT roman_p end_POSTSUBSCRIPT , bold_z start_POSTSUBSCRIPT roman_r end_POSTSUBSCRIPT ) = { start_ROW start_CELL 1 , end_CELL start_CELL if italic_s ( bold_z start_POSTSUBSCRIPT roman_p end_POSTSUBSCRIPT , bold_z start_POSTSUBSCRIPT roman_r end_POSTSUBSCRIPT ) ≥ italic_τ , end_CELL end_ROW start_ROW start_CELL 0 , end_CELL start_CELL otherwise . end_CELL end_ROW (3)\n\nThe learned face recognition models are commonly assessed through their verification (1:1 comparison) performance. For verification, metrics such as true acceptance rate (TAR) at a fixed false acceptance rate (FAR) are reported. In this paper, we report the TAR at FAR=0.01%FARpercent0.01\\mathrm{FAR}=0.01\\%roman_FAR = 0.01 %.\n\nGiven the above system and protocol, the next section will empirically demonstrate the impact of watermarking on the performance of face recognition models trained using watermarked face images.\n\n4 Experiments\n\nIn this section, we demonstrate the quality of watermarked face images and their robustness to diverse image transformations. Next, we assess the impact of watermarking face images on the performance of given, pre-trained face recognition models. Finally, we investigate the effect of using watermarked face images to train face recognition models on downstream face recognition tasks.\n\n4.1 Experimental Setup\n\nFirst, in Table 1, we provide a description of the datasets utilized in this study, including the number of unique face identities, the total number of images, and the average number of images per identity. For evaluation purposes, we select a subset of 2 images per identity from all 10,547 identities in CASIA-WebFace and 2 images per identity from a randomly sampled set of 10,000 identities for DCFace to compute genuine and imposter scores. During training experiments, we utilize the full MS1Mv2 dataset.\n\nSecond, we implement the watermarking system following Sec. 3.1 and the architecture setup of HiDDeN [17]. The watermark encoder consists of 4 Conv-BN-ReLU blocks, each with 64 conv filters of 3×\\times×3 kernels, 1 stride, and 1 padding. The decoder has 7 such blocks, followed by another block with L𝐿Litalic_L filters (L𝐿Litalic_L is the watermarking bit string length) and a linear layer of L𝐿Litalic_L×\\times×L𝐿Litalic_L. The watermark encoder and decoder are pre-trained together on the whole MS-COCO dataset [62], cropped to 256×256256256256\\times 256256 × 256. The watermarking performance will be evaluated on both CASIA-WebFace [21] and DCFace [20] datasets.\n\nFurthermore, to assess the impact of watermarking on face recognition model performance, we employ three state-of-the-art face recognition models pretrained on large, non-watermarked face recognition datasets. Specifically, we consider two variations (ResNet50 and ResNet101) of AdaFace [63] pretrained on MS1Mv2, along with one commercial-off-the-shelf (COTS) face recognition SDK . We evaluate the performance on both the original CASIA-WebFace and DCFace datasets, as well as their watermarked versions. For evaluation, we denote the face recognition performance on the original dataset as ‘original-original’, indicating that both the probe and reference face images being compared are original images. ‘Watermarked-original’ denotes that the probe image is watermarked while the reference image is unmodified, and ‘watermarked-watermarked’ indicates that both the probe and reference images are watermarked.\n\nFinally, to explore the impact of watermarking on face recognition model training, we train our own ResNet50 AdaFace models on both the original MS1Mv2 dataset and its watermarked version. We then compare the performance of these trained models on the CASIA-WebFace dataset. Similarly, we conduct the same experiment by training on both the original DCFace synthetic face image dataset and its watermarked version to investigate the effect of watermarking on synthetic face images.\n\n4.2 Experimental Results\n\nAssessing watermarking quality in face images.\n\nIn Fig. 5, we present visualizations of original images, watermarked images, and pixel-wise differences between the original and watermarked images for both real and synthetic face image datasets (CASIA-WebFace and DCFace, respectively). Notably, the watermarked images exhibit no visually perceptible differences compared to the originals, with pixel-wise discrepancies primarily located along image edges, which are challenging for humans to discern.\n\nTo further evaluate this, we randomly selected 5 pairs of original/watermarked images and solicited feedback from 10 student volunteers to differentiate between them. Among the total 50 responses received, 8 were correct selections, 6 were incorrect selections, and 36 were labeled as ‘Not sure’ (See Fig. 6). This underscores the invisibility of the watermark (the ‘S𝑆Sitalic_S’ letter used in Fig. 3) to human eyes, indicating the stealthiness of watermarking and its high quality.\n\nTable 2 extends the analysis from Fig. 4, evaluating the watermark’s resilience against different post-watermarking transformations (cropping, resizing, brightness, contrast, and JPEG compression) at varying scaling factors. For each transformation, scaling factors range from 1 to 0.75 for cropping and resizing, 1 to 3.5 for brightness and contrast adjustments, and 100 to 75 for JPEG compression quality. As we can see, reconstructing the watermark maintains high bit accuracy across diverse transformations and scaling factors for both real (CASIA-WebFace) and synthetic (DCFace) datasets. Notably, even when images are scaled down to 75% of their original size, bit accuracy remains above 84% for cropping and resizing. Likewise, for brightness and contrast adjustments, bit accuracy remains above 88% up to a scaling factor of 3.5. However, JPEG compression has a more pronounced impact on watermark extraction, with bit accuracy dropping to approximately 73% and 68% for CASIA-WebFace and DCFace, respectively, at a quality factor of 75.\n\nFig. 7 depicts the averaged bit accuracy of watermark reconstruction from watermarked face images, based on the CASIA-WebFace and DCFace datasets respectively, across different scaling factors for each data transformation type. Overall, the bit accuracy on DCFace is slightly lower than that on CASIA-WebFace, likely due to differences in image characteristics between real and synthetic faces, given that the watermarking model is pre-trained on real images. Nevertheless, the watermarking scheme demonstrates robust resilience to various transformations on both datasets. The consistently high bit accuracy across different scaling factors indicates that the watermark can be reliably extracted even when watermarked images undergo common image processing operations. This underscores the robustness and practicality of the watermarking approach for protecting and authenticating face images.\n\nWatermarking effect on pre-trained face recognition models.\n\nUtilizing three SOTA face recognition systems, including two open-source variants (ResNet50 and ResNet101 variants of AdaFace [63]), and a third from a COTS face recognition SDK, we assess the impact of watermarking on face recognition performance. Table 3 illustrates the recognition performance on two face image datasets: CASIA-WebFace real face images and DCFace synthetic face images. As we can see, watermarking face images results in a marginal decrease in face recognition performance, as measured by True Accept Rate (TAR) at a False Accept Rate (FAR) of 0.01%. For instance, the average performance drop from ‘original-original’ to ‘watermarked-original’ across the three matchers on the CASIA-WebFace dataset was 0.51%. This decline in performance was more pronounced when both the probe and reference face images were watermarked, resulting in a TAR drop of 0.81%. Furthermore, the impact of watermarking was more substantial for the synthetic DCFace dataset, leading to an average decrease of 1.15% when the probe image was watermarked and 3.35% when both the probe and reference images were watermarked. The heightened drop in performance on synthetic images due to the watermark raises concerns regarding the use case of applying a watermark to generated face images to maintain provenance, highlighting the need for further methods to embed the watermark without impeding face recognition matching performance.\n\nTo better understand the effect of watermarking on face recognition performance, Fig. 8 presents the genuine and imposter similarity score distributions for both CASIA-WebFace and DCFace datasets using the pre-trained ResNet101 AdaFace model before and after applying the watermark to each probe image. As we can see, the imposter score distribution is statistically unchanged whereas the mean of the genuine score distribution is slightly shifted left (from 0.502 to 0.491 for CASIA-WebFace and from 0.313 to 0.303 for DCFace) with the inclusion of the watermarked images. As a result, some challenging comparisons near the match threshold for the original images may be pushed to below the genuine match threshold when watermarked, slightly degrading the performance of the face recognition model. This observation is further supported by examples of the failure cases shown in Fig. 9. These examples showcase challenging genuine comparisons that were matched slightly above the threshold for the original images but resulted in false rejections when the probe image was watermarked. And these rejections appear to align with matching against challenging poses and expressions. To validate the statistical significance of the effect of watermarking on the genuine distributions, we conducted a two-sided T-test between the genuine distributions for both datasets before and after applying the watermark. Both results were statistically significant, yielding a p-value of 1.45×10−41.45superscript1041.45\\times 10^{-4}1.45 × 10 start_POSTSUPERSCRIPT - 4 end_POSTSUPERSCRIPT and 7.15×10−87.15superscript1087.15\\times 10^{-8}7.15 × 10 start_POSTSUPERSCRIPT - 8 end_POSTSUPERSCRIPT for CASIA-WebFace and DCFace, respectively.\n\nEffect of watermarking on the utility of face images for training face recognition models.\n\nIn an ideal scenario, watermarking face images, whether real or synthetic, should not impact downstream face recognition tasks. To evaluate the utility of watermarked face images, we trained multiple face recognition models on both non-watermarked and watermarked versions of the MS1Mv2 and DCFace datasets. We then compared the accuracy of the trained models on the CASIA-WebFace dataset. The resulting face recognition performance is presented in Table 4. As we can see, there exists a slight decrease in recognition performance when training on watermarked face images compared to training on the corresponding original face images. For instance, training on MS1Mv2 with and without the watermark led to a drop in recognition accuracy on CASIA-WebFace from a TAR of 84.34% to 83.69%. Similarly, a similar trend was observed when training on the DCFace image dataset and its watermarked version, reducing the TAR on CASIA-WebFace from 54.26% to 53.64%.\n\nIn the previous experiment (Table 3), we noted a decrease in the performance of pre-trained recognition models evaluated on the watermarked version of CASIA-Webface vs. non-watermarked CASIA-Webface. Interestingly, from Table 4, we can see that besides the drop in absolute performance compared to training on non-watermarked images (row 1 vs. 2 and row 3 vs. 4), even if the recognition model is trained on watermarked images (as is the case for rows 2 and 4), there is still a gap at test time evaluating on the watermarked version of CASIA-WebFace compared to the non-watermarked CASIA-WebFace (comparing column 2 to column 3); Albeit, this gap is smaller compared to the models only trained on non-watermarked images (rows 1 and 3). Therefore, not only does training on watermarked images decrease the absolute performance of the face recognition model, it also does not provide total robustness to watermarks during test time.\n\n5 Conclusion\n\nIn this study, we conducted a thorough examination of the effects of digital watermarking on face recognition, emphasizing the complexities and potential benefits associated with the ethical utilization of generated face images. Our devised framework, amalgamating face generation, watermarking, and recognition, facilitated a methodical exploration of their interconnected dynamics. Through rigorous experimentation, we showcased the efficacy of the watermarking scheme in ensuring robust face image attribution, while also shedding light on the inherent trade-offs between watermarking and recognition accuracy. Our results highlight the imperative for future research endeavors aimed at devising techniques capable of optimizing both watermarking and face recognition performance concurrently. Furthermore, the adversarial robustness of watermarking remains inadequately explored and warrants increased research attention in the future. While our study concentrated on assessing the robustness of watermarking against image transformations, its susceptibility to adversarial attacks remains an open concern. Adversarial attacks could potentially render the watermark removable, underscoring the necessity for further investigation in this area.\n\n6 Broader Impacts\n\nOur investigation into the impact of digital watermarking on face recognition holds broader implications for the responsible advancement of AI systems, particularly in the domain of face recognition and biometrics, as well as in the broader realm of generated AI. With the escalating sophistication of generative models in producing lifelike face images, there arises a pressing need to establish measures ensuring the integrity, authenticity, and accountability of these generated images. Exploration in this work could shed light on the potential of digital watermarking as a pivotal tool for image attribution and ownership verification, pivotal in combatting the misuse of generated images for nefarious purposes like deepfake creation and identity fraud. Nevertheless, our findings also underline the imperative of carefully balancing the trade-offs between watermarking and recognition performance. While watermarking offers advantages in terms of image accountability, its presence may impede the accuracy of face recognition systems under certain circumstances. This highlights the necessity of developing techniques that effectively reconcile these competing objectives, ensuring that the benefits of watermarking can be realized without compromising the performance of downstream applications. We aspire that our findings will catalyze further research and stimulate discussions on how the responsible and ethical design of generative AI contribute to broad AI applications.\n\nReferences\n\n[1] Chitwan Saharia, William Chan, Saurabh Saxena, Lala Li, Jay Whang, Emily L Denton, Kamyar Ghasemipour, Raphael Gontijo Lopes, Burcu Karagol Ayan, Tim Salimans, et al. Photorealistic text-to-image diffusion models with deep language understanding. Advances in Neural Information Processing Systems, 35:36479–36494, 2022.\n\n[2] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Björn Ommer. High-resolution image synthesis with latent diffusion models. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 10684–10695, 2022.\n\n[3] Aditya Ramesh, Prafulla Dhariwal, Alex Nichol, Casey Chu, and Mark Chen. Hierarchical text-conditional image generation with clip latents. arXiv preprint arXiv:2204.06125, 1(2):3, 2022.\n\n[4] Patrick Schramowski, Manuel Brack, Björn Deiseroth, and Kristian Kersting. Safe latent diffusion: Mitigating inappropriate degeneration in diffusion models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 22522–22531, 2023.\n\n[5] Yimeng Zhang, Jinghan Jia, Xin Chen, Aochuan Chen, Yihua Zhang, Jiancheng Liu, Ke Ding, and Sijia Liu. To generate or not? safety-driven unlearned diffusion models are still easy to generate unsafe images… for now. arXiv preprint arXiv:2310.11868, 2023.\n\n[6] Shruti Agarwal, Hany Farid, Yuming Gu, Mingming He, Koki Nagano, and Hao Li. Protecting world leaders against deep fakes. In CVPR workshops, volume 1, page 38, 2019.\n\n[7] Bobby Chesney and Danielle Citron. Deep fakes: A looming challenge for privacy, democracy, and national security. Calif. L. Rev., 107:1753, 2019.\n\n[8] Zhangheng Li, Junyuan Hong, Bo Li, and Zhangyang Wang. Shake to leak: Fine-tuning diffusion models can amplify the generative privacy risk. arXiv preprint arXiv:2403.09450, 2024.\n\n[9] Nicolas Carlini, Jamie Hayes, Milad Nasr, Matthew Jagielski, Vikash Sehwag, Florian Tramer, Borja Balle, Daphne Ippolito, and Eric Wallace. Extracting training data from diffusion models. In 32nd USENIX Security Symposium (USENIX Security 23), pages 5253–5270, 2023.\n\n[10] Gowthami Somepalli, Vasu Singla, Micah Goldblum, Jonas Geiping, and Tom Goldstein. Understanding and mitigating copying in diffusion models. Advances in Neural Information Processing Systems, 36:47783–47803, 2023.\n\n[11] Gowthami Somepalli, Vasu Singla, Micah Goldblum, Jonas Geiping, and Tom Goldstein. Diffusion art or digital forgery? investigating data replication in diffusion models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 6048–6058, 2023.\n\n[12] Yihua Zhang, Yimeng Zhang, Yuguang Yao, Jinghan Jia, Jiancheng Liu, Xiaoming Liu, and Sijia Liu. Unlearncanvas: A stylized image dataset to benchmark machine unlearning for diffusion models. arXiv preprint arXiv:2402.11846, 2024.\n\n[13] Sasha Luccioni, Christopher Akiki, Margaret Mitchell, and Yacine Jernite. Stable bias: Evaluating societal representations in diffusion models. Advances in Neural Information Processing Systems, 36, 2024.\n\n[14] Yue Jiang, Yueming Lyu, Tianxiang Ma, Bo Peng, and Jing Dong. Rs-corrector: Correcting the racial stereotypes in latent diffusion models. arXiv preprint arXiv:2312.04810, 2023.\n\n[15] Mingyuan Fan, Cen Chen, Chengyu Wang, and Jun Huang. On the trustworthiness landscape of state-of-the-art generative models: A comprehensive survey. arXiv preprint arXiv:2307.16680, 2023.\n\n[16] Anil K Jain and Umut Uludag. Hiding biometric data. IEEE transactions on pattern analysis and machine intelligence, 25(11):1494–1498, 2003.\n\n[17] Jiren Zhu, Russell Kaplan, Justin Johnson, and Li Fei-Fei. Hidden: Hiding data with deep networks. In Proceedings of the European conference on computer vision (ECCV), pages 657–672, 2018.\n\n[18] Pierre Fernandez, Guillaume Couairon, Hervé Jégou, Matthijs Douze, and Teddy Furon. The stable signature: Rooting watermarks in latent diffusion models. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 22466–22477, 2023.\n\n[19] Yuxin Wen, John Kirchenbauer, Jonas Geiping, and Tom Goldstein. Tree-ring watermarks: Fingerprints for diffusion images that are invisible and robust. arXiv preprint arXiv:2305.20030, 2023.\n\n[20] Minchul Kim, Feng Liu, Anil Jain, and Xiaoming Liu. Dcface: Synthetic face generation with dual condition diffusion model. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 12715–12725, 2023.\n\n[21] Dong Yi, Zhen Lei, Shengcai Liao, and Stan Z Li. Learning face representation from scratch. arXiv preprint arXiv:1411.7923, 2014.\n\n[22] Zheng Zhu, Guan Huang, Jiankang Deng, Yun Ye, Junjie Huang, Xinze Chen, Jiagang Zhu, Tian Yang, Jiwen Lu, Dalong Du, et al. Webface260m: A benchmark unveiling the power of million-scale deep face recognition. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 10492–10502, 2021.\n\n[23] Yichun Shi, Debayan Deb, and Anil K Jain. Warpgan: Automatic caricature generation. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 10762–10771, 2019.\n\n[24] Hu Ye, Jun Zhang, Sibo Liu, Xiao Han, and Wei Yang. Ip-adapter: Text compatible image prompt adapter for text-to-image diffusion models. arXiv preprint arXiv:2308.06721, 2023.\n\n[25] Zhen Li, Mingdeng Cao, Xintao Wang, Zhongang Qi, Ming-Ming Cheng, and Ying Shan. Photomaker: Customizing realistic human photos via stacked id embedding. arXiv preprint arXiv:2312.04461, 2023.\n\n[26] Qixun Wang, Xu Bai, Haofan Wang, Zekui Qin, and Anthony Chen. Instantid: Zero-shot identity-preserving generation in seconds. arXiv preprint arXiv:2401.07519, 2024.\n\n[27] Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron Courville, and Yoshua Bengio. Generative adversarial nets. Advances in neural information processing systems, 27, 2014.\n\n[28] Tero Karras, Samuli Laine, and Timo Aila. A style-based generator architecture for generative adversarial networks. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 4401–4410, 2019.\n\n[29] Yunjey Choi, Minje Choi, Munyoung Kim, Jung-Woo Ha, Sunghun Kim, and Jaegul Choo. Stargan: Unified generative adversarial networks for multi-domain image-to-image translation. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 8789–8797, 2018.\n\n[30] Yang Song and Stefano Ermon. Generative modeling by estimating gradients of the data distribution. Advances in neural information processing systems, 32, 2019.\n\n[31] Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffusion probabilistic models. Advances in neural information processing systems, 33:6840–6851, 2020.\n\n[32] Qiong Cao, Li Shen, Weidi Xie, Omkar M Parkhi, and Andrew Zisserman. Vggface2: A dataset for recognising faces across pose and age. In 2018 13th IEEE international conference on automatic face & gesture recognition (FG 2018), pages 67–74. IEEE, 2018.\n\n[33] Tero Karras, Miika Aittala, Janne Hellsten, Samuli Laine, Jaakko Lehtinen, and Timo Aila. Training generative adversarial networks with limited data. Advances in neural information processing systems, 33:12104–12114, 2020.\n\n[34] Vahid Mirjalili, Sebastian Raschka, Anoop Namboodiri, and Arun Ross. Semi-adversarial networks: Convolutional autoencoders for imparting privacy to face images. In 2018 International Conference on Biometrics (ICB), pages 82–89. IEEE, 2018.\n\n[35] Mika Westerlund. The emergence of deepfake technology: A review. Technology innovation management review, 9(11), 2019.\n\n[36] Ingemar Cox, Matthew Miller, Jeffrey Bloom, Jessica Fridrich, and Ton Kalker. Digital watermarking and steganography. Morgan kaufmann, 2007.\n\n[37] Matthew L Miller, Ingemar J Cox, and Jeffrey A Bloom. Informed embedding: exploiting image and detector information during watermark insertion. In Proceedings 2000 International Conference on Image Processing (Cat. No. 00CH37101), volume 3, pages 1–4. IEEE, 2000.\n\n[38] Sviatoslav Voloshynovskiy, Shelby Pereira, Thierry Pun, Joachim J Eggers, and Jonathan K Su. Attacks on digital watermarks: classification, estimation based attacks, and benchmarks. IEEE communications Magazine, 39(8):118–126, 2001.\n\n[39] Jessica Fridrich. Visual hash for oblivious watermarking. In Security and Watermarking of Multimedia Contents II, volume 3971, pages 286–294. SPIE, 2000.\n\n[40] Alessandro Piva, Mauro Barni, Franco Bartolini, and Vito Cappellini. Dct-based watermark recovering without resorting to the uncorrupted original image. In Proceedings of international conference on image processing, volume 1, pages 520–523. IEEE, 1997.\n\n[41] Mauro Barni, Franco Bartolini, Vito Cappellini, and Alessandro Piva. A dct-domain system for robust image watermarking. Signal processing, 66(3):357–372, 1998.\n\n[42] Xiang-Gen Xia, Charles G Boncelet, and Gonzalo R Arce. A multiresolution watermark for digital images. In Proceedings of international conference on image processing, volume 1, pages 548–551. IEEE, 1997.\n\n[43] Ruizhen Liu and Tieniu Tan. An svd-based watermarking scheme for protecting rightful ownership. IEEE transactions on multimedia, 4(1):121–128, 2002.\n\n[44] Kevin Alex Zhang, Alfredo Cuesta-Infante, Lei Xu, and Kalyan Veeramachaneni. Steganogan: High capacity image steganography with gans. arXiv preprint arXiv:1901.03892, 2019.\n\n[45] Ning Yu, Vladislav Skripniuk, Sahar Abdelnabi, and Mario Fritz. Artificial fingerprinting for generative models: Rooting deepfake attribution in training data. In Proceedings of the IEEE/CVF International conference on computer vision, pages 14448–14457, 2021.\n\n[46] Jie Zhang, Dongdong Chen, Jing Liao, Weiming Zhang, Huamin Feng, Gang Hua, and Nenghai Yu. Deep model intellectual property protection via deep watermarking. IEEE Transactions on Pattern Analysis and Machine Intelligence, 44(8):4005–4020, 2021.\n\n[47] Vlasislav Skripniuk. Watermarking for generative adversarial networks. PhD thesis, Universität des Saarlandes Saarbrücken, 2020.\n\n[48] Hanzhou Wu, Gen Liu, Yuwei Yao, and Xinpeng Zhang. Watermarking neural networks with watermarked images. IEEE Transactions on Circuits and Systems for Video Technology, 31(7):2591–2601, 2020.\n\n[49] Jianwei Fei, Zhihua Xia, Benedetta Tondi, and Mauro Barni. Supervised gan watermarking for intellectual property protection. In 2022 IEEE International Workshop on Information Forensics and Security (WIFS), pages 1–6. IEEE, 2022.\n\n[50] Matthew Turk and Alex Pentland. Eigenfaces for recognition. Journal of cognitive neuroscience, 3(1):71–86, 1991.\n\n[51] Md Abdur Rahim, Md Najmul Hossain, Tanzillah Wahid, and Md Shafiul Azam. Face recognition using local binary patterns (lbp). Global Journal of Computer Science and Technology, 13(4):1–8, 2013.\n\n[52] Alberto Albiol, David Monzo, Antoine Martin, Jorge Sastre, and Antonio Albiol. Face recognition using hog–ebgm. Pattern Recognition Letters, 29(10):1537–1543, 2008.\n\n[53] Harihara Santosh Dadi and GK Mohan Pillutla. Improved face recognition rate using hog features and svm classifier. IOSR Journal of Electronics and Communication Engineering, 11(04):34–44, 2016.\n\n[54] Pallabi Parveen and Bhavani Thuraisingham. Face recognition using multiple classifiers. In 2006 18th IEEE International Conference on Tools with Artificial Intelligence (ICTAI’06), pages 179–186. IEEE, 2006.\n\n[55] Yaniv Taigman, Ming Yang, Marc’Aurelio Ranzato, and Lior Wolf. Deepface: Closing the gap to human-level performance in face verification. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 1701–1708, 2014.\n\n[56] Weiyang Liu, Yandong Wen, Zhiding Yu, Ming Li, Bhiksha Raj, and Le Song. Sphereface: Deep hypersphere embedding for face recognition. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 212–220, 2017.\n\n[57] Jiankang Deng, Jia Guo, Niannan Xue, and Stefanos Zafeiriou. Arcface: Additive angular margin loss for deep face recognition. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 4690–4699, 2019.\n\n[58] Karen Simonyan and Andrew Zisserman. Very deep convolutional networks for large-scale image recognition. arXiv preprint arXiv:1409.1556, 2014.\n\n[59] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 770–778, 2016.\n\n[60] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, et al. An image is worth 16x16 words: Transformers for image recognition at scale. arXiv preprint arXiv:2010.11929, 2020.\n\n[61] Yandong Guo, Lei Zhang, Yuxiao Hu, Xiaodong He, and Jianfeng Gao. Ms-celeb-1m: A dataset and benchmark for large-scale face recognition. In Computer Vision–ECCV 2016: 14th European Conference, Amsterdam, The Netherlands, October 11-14, 2016, Proceedings, Part III 14, pages 87–102. Springer, 2016.\n\n[62] Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr Dollár, and C Lawrence Zitnick. Microsoft coco: Common objects in context. In Computer Vision–ECCV 2014: 13th European Conference, Zurich, Switzerland, September 6-12, 2014, Proceedings, Part V 13, pages 740–755. Springer, 2014.\n\n[63] Minchul Kim, Anil K Jain, and Xiaoming Liu. Adaface: Quality adaptive margin for face recognition. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 18750–18759, 2022.\n\n[64] Sheng Chen, Yang Liu, Xiang Gao, and Zhen Han. Mobilefacenets: Efficient cnns for accurate real-time face verification on mobile devices. In Biometric Recognition: 13th Chinese Conference, CCBR 2018, Urumqi, China, August 11-12, 2018, Proceedings 13, pages 428–438. Springer, 2018.\n\n[65] Hao Wang, Yitong Wang, Zheng Zhou, Xing Ji, Dihong Gong, Jingchao Zhou, Zhifeng Li, and Wei Liu. Cosface: Large margin cosine loss for deep face recognition. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 5265–5274, 2018."
    }
}
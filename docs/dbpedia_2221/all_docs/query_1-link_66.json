{
    "id": "dbpedia_2221_1",
    "rank": 66,
    "data": {
        "url": "https://arxiv.org/html/2308.02312v4",
        "read_more_link": "",
        "language": "en",
        "title": "Is Stack Overflow Obsolete? An Empirical Study of the Characteristics of ChatGPT Answers to Stack Overflow Questions",
        "top_image": "",
        "meta_img": "",
        "images": [
            "https://arxiv.org/html/x1.png"
        ],
        "movies": [],
        "keywords": [],
        "meta_keywords": [
            ""
        ],
        "tags": null,
        "authors": [],
        "publish_date": null,
        "summary": "",
        "meta_description": "",
        "meta_lang": "en",
        "meta_favicon": "",
        "meta_site_name": "",
        "canonical_link": null,
        "text": "HTML conversions sometimes display errors due to content that did not convert correctly from the source. This paper uses the following packages that are not yet supported by the HTML conversion tool. Feedback on these issues are not necessary; they are known and are being worked on.\n\nfailed: pdfcol\n\nAuthors: achieve the best HTML results from your LaTeX submissions by following these best practices.\n\nLicense: arXiv.org perpetual non-exclusive license\n\narXiv:2308.02312v4 [cs.SE] 07 Feb 2024\n\n\\pdfcolInitStack\n\ntcb@breakable\n\nIs Stack Overflow Obsolete? An Empirical Study of the Characteristics of ChatGPT Answers to Stack Overflow Questions\n\nSamia Kabir , David N. Udo-Imeh , Bonan Kou and Tianyi Zhang\n\nAbstract.\n\nQ&A platforms have been crucial for the online help-seeking behavior of programmers. However, the recent popularity of ChatGPT is altering this trend. Despite this popularity, no comprehensive study has been conducted to evaluate the characteristics of ChatGPT‚Äôs answers to programming questions. To bridge the gap, we conducted the first in-depth analysis of ChatGPT answers to 517 programming questions on Stack Overflow and examined the correctness, consistency, comprehensiveness, and conciseness of ChatGPT answers. Furthermore, we conducted a large-scale linguistic analysis, as well as a user study, to understand the characteristics of ChatGPT answers from linguistic and human aspects. Our analysis shows that 52% of ChatGPT answers contain incorrect information and 77% are verbose. Nonetheless, our user study participants still preferred ChatGPT answers 35% of the time due to their comprehensiveness and well-articulated language style. However, they also overlooked the misinformation in the ChatGPT answers 39% of the time. This implies the need to counter misinformation in ChatGPT answers to programming questions and raise awareness of the risks associated with seemingly correct answers.\n\nstack overflow, q&a, large language model, chatgpt, misinformation\n\n‚Ä†‚Ä†journalyear: 2024‚Ä†‚Ä†copyright: rightsretained‚Ä†‚Ä†conference: Proceedings of the CHI Conference on Human Factors in Computing Systems; May 11‚Äì16, 2024; Honolulu, HI, USA‚Ä†‚Ä†booktitle: Proceedings of the CHI Conference on Human Factors in Computing Systems (CHI ‚Äô24), May 11‚Äì16, 2024, Honolulu, HI, USA‚Ä†‚Ä†doi: 10.1145/3613904.3642596‚Ä†‚Ä†isbn: 979-8-4007-0330-0/24/05‚Ä†‚Ä†ccs: Human-centered computing Empirical studies in HCI‚Ä†‚Ä†ccs: Software and its engineering‚Ä†‚Ä†ccs: General and reference Empirical studies\n\n1. Introduction\n\nProgrammers often resort to online resources for a variety of programming tasks, e.g., API learning, bug fixing, comprehension of code or concepts, etc. (Skripchuk et al., 2023; Xia et al., 2017; Rao et al., 2020). A vast majority of these help-seeking activities include frequent engagement with community Q&A platforms such as Stack Overflow (SO) (Rao et al., 2020; Rahman et al., 2018; Xia et al., 2017; Vasilescu et al., 2013). The emergence of Large Language Models (LLMs) has demonstrated the potential to transform the online help-seeking patterns of programmers. In November 2022, ChatGPT (OpenAI, 2023) was released and quickly gained significant attention and popularity among programmers. There have been increasing debates about whether and when ChatGPT would replace prominent search engines and Q&A forums among researchers and industrial practitioners (Castillo, 2022; Quora, 2023).\n\nDespite the rising popularity of ChatGPT, there are also many increasing concerns. Previous studies show that LLMs can acquire factually incorrect knowledge during training and propagate the incorrect knowledge to generated content (Goodrich et al., 2019; Maynez et al., 2020; Gamage et al., 2022; Gehman et al., 2020; Bender et al., 2021). Besides, LLMs often generate fabricated texts that mimic truthful information and are hard to recognize, especially for users who lack the expertise (Cao et al., 2021; Bommasani et al., 2021; Elazar et al., 2021). Like other LLMs, ChatGPT is also plagued with these issues (Guo et al., 2023; Borji, 2023; Mitroviƒá et al., 2023; Koco≈Ñ et al., 2023). The prevalence of misinformation, which can easily mislead users, has prompted Stack Overflow to impose a ban on answers generated by ChatGPT (Overflow, 2023c).\n\nRecent studies have compared ChatGPT to human experts in legal, medical, and financial domains (Guo et al., 2023; Gao et al., 2022). To the best of our knowledge, no comprehensive analysis has been conducted to investigate ChatGPT‚Äôs capability to answer programming questions, especially the quality and characteristics of ChatGPT answers in comparison to human answers. If misinformation is prevalent in ChatGPT answers and is hard to recognize, it may inevitably lead to suboptimal design choices and software defects. In the long term, this may jeopardize the quality and robustness of software and cyberinfrastructure in our society, affecting a broader population beyond programmers.\n\nThis work aims to bridge the gap by adopting a mixed-methods research design (Johnson and Onwuegbuzie, 2004) with a combination of manual analysis, linguistic analysis, and user studies to compare human answers and ChatGPT answers to programming questions on Stack Overflow (SO). Specifically, we performed stratified sampling to collect ChatGPT answers to 517 SO questions with different characteristics (e.g., popularity, question types, recency, etc.). The sample size is statistically significant with a 95% confidence level and 5% margin of error. We manually analyzed ChatGPT answers and compared them with the accepted SO answers written by human programmers. In addition to correctness, we also assessed the consistency, comprehensiveness, and conciseness of ChatGPT answers. We found that 52% of ChatGPT answers contain misinformation, 77% of the answers are more verbose than human answers, and 78% of the answers suffer from different degrees of inconsistency to human answers.\n\nFurthermore, to examine how the linguistic features of ChatGPT answers differ from human answers, we conducted a large-scale linguistic analysis on 2000 randomly sampled SO questions. Specifically, we run Linguistic Inquiry and Word Count (LIWC) (Pennebaker et al., 2015) and sentiment analysis on ChatGPT answers and human answers. Our results show that ChatGPT uses more formal and analytical language and portrays less negative sentiment.\n\nFinally, to capture how different characteristics of the answers influence programmers‚Äô preferences between ChatGPT and SO, we conducted a user study with 12 programmers. The study results show that participants‚Äô overall preferences, correctness ratings, and quality ratings were more leaning toward human answers from Stack Overflow. However, participants still preferred ChatGPT answers 35% of the time and overlooked misinformation in the answers 39% of the time. When asked why they preferred ChatGPT answers even when they were incorrect, participants suggested the comprehensiveness and articulated language structures of the answers as reasons for their preference, which is consistent with our linguistic analysis result.\n\nOur manual analysis, linguistic analysis, and user study collectively demonstrate that while ChatGPT performs remarkably well in many cases, it frequently makes errors and unnecessarily prolongs its responses. However, ChatGPT answers have richer linguistic features, leading some users to prefer ChatGPT answers over human answers and sometimes overlook the underlying incorrectness and inconsistencies in ChatGPT answers. Our in-depth analysis points towards several challenges and risks of using ChatGPT in programming and also highlights several opportunities for designing new interaction and computational methods to counter misinformation generated by ChatGPT.\n\nTo conclude, this paper makes the following contributions:\n\n‚Ä¢\n\nWe conducted an in-depth analysis of the correctness and quality of ChatGPT answers across four distinct quality aspects for various types of SO question posts.\n\n‚Ä¢\n\nWe performed a large-scale analysis of the linguistic characteristics of ChatGPT answers and identified distinct linguistic features that are prominent in ChatGPT answers.\n\n‚Ä¢\n\nWe investigated how real programmers consider answer correctness, quality, and linguistic features when choosing between ChatGPT and Stack Overflow (SO) through a within-subjects user study.\n\n‚Ä¢\n\nWe provided a comprehensive discussion of the design implications, emphasized the risks of misinformation, and outlined future directions aimed at detecting and mitigating misinformation in AI-assisted programming.\n\n‚Ä¢\n\nWe made our data and codebooks publicly available at https://github.com/SamiaKabir/ChatGPT-Answers-to-SO-questions to foster future research in this direction.\n\nThe rest of the paper is organized as follows. Section 2 describes the related work. Section 3 describes the research questions. Section 4 describes the data collection process and the methodology of our mixed-methods study. Sections 5, 6, and 7 describe the analysis results of our manual analysis, linguistic analysis, and user study respectively. Section 8 discusses the implications of our findings and future research directions. Section 9 describes the limitations of this work. Section 10 concludes this work.\n\n2. Related Work\n\n2.1. Misinformation Generated by LLMs\n\nPrevious studies have shown that content generated by LLMs may contain hallucinations and misinformation (Bommasani et al., 2021; Westerlund, 2019; Knight, 2021). Some recent work has investigated LLMs‚Äô capability to generate fake news, images, and videos (Zhou et al., 2023; DiResta, 2020; Gehman et al., 2020) about a multitude of social phenomena such as politics, elections, people, disease, economics, etc. These types of misinformation have the power to mislead and misguide people and can potentially impair the normal functionalities of society and cause chaos (Toews, 2020; Gamage et al., 2022; Islam et al., 2020; Knight, 2021). Specifically, several studies have investigated the power of AI-generated texts in deceiving people (Kreps et al., 2022; Buchanan et al., 2021). Zhou et al. (Zhou et al., 2023) further highlight the risks by showing how traditional misinformation detection and mitigation methods often fail to identify misinformation generated by state-of-the-art LLMs.\n\nSince its release in November 2022, ChatGPT has surpassed other LLMs in popularity among general users. The usability and effectiveness of ChatGPT have been examined in different domains, such as law, medicine, and finance (Guo et al., 2023). Like other LLMs, ChatGPT also fabricates facts and generates low-quality or misleading information (Guo et al., 2023; Borji, 2023; Mitroviƒá et al., 2023; Koco≈Ñ et al., 2023). However, to the best of our knowledge, no studies have investigated the characteristics and human perception of misinformation in ChatGPT‚Äôs answers to programming questions. Our work aims to bridge this gap with a combination of manual analysis, linguistic analysis, and user studies.\n\n2.2. Help-Seeking Behavior of Programmers\n\nThe proliferation of social media and Q&A platforms for programming have immensely shaped the online help-seeking behavior of programmers (Treude et al., 2011; Storey et al., 2010; Vasilescu et al., 2013; Abdalkareem et al., 2017). Treude et al. (Treude et al., 2011) investigated the role and benefits of a popular Q&A platform‚ÄîStack Overflow (SO)‚Äîand found that Stack Overflow is highly effective in code reviews and answering conceptual questions. Through a mixed-methods study, Mamykina et al. (Mamykina et al., 2011) show that the chance of getting quick answers from the SO community is high. Despite the popularity and effectiveness of Stack Overflow, several concerns have been raised. For instance, since Q&A platforms are not integrated with IDEs, developers have to constantly switch between their IDEs and Q&A platforms, which may interrupt developers‚Äô workflow and impair their performance persistence (Vasilescu et al., 2013; Storey et al., 2010; Bacchelli et al., 2012). Another concern is the presence of toxicity and negative sentiment in people‚Äôs answers and comments on Stack Overflow. Calefato et al. (Calefato et al., 2015) found that the presence of positive and negative sentiment contributes towards the upvotes and downvotes of SO answers respectively. In a follow-up study, they found that novices and student programmers often encounter arrogant and rude comments on Stack Overflow, which discourages them from posting questions (Calefato et al., 2018a). Asaduzzaman et al. (Asaduzzaman et al., 2013) also found that the presence of toxicity and negative emotions in SO answers can discourage follow-up discussions on Stack Overflow. Our study also confirms this, since users prefer ChatGPT answers due to their politeness and positive sentiment.\n\n2.3. Human-AI Collaboration in Programming\n\nRecent studies show that AI pair-programming tools such as GitHub Copilot (GitHub, 2023c) have shifted developers‚Äô behavior from code writing to code understanding and can improve developer productivity (Bird et al., 2022; Imai, 2022). The online help-seeking behavior of developers is also changing along with other behavior shifts. Developers often use Copilot to get quick code suggestions and only turn to web searches to access the documentation or verify the suggestions (Barke et al., 2023; Ross et al., 2023; Vaithilingam et al., 2022). However, recent studies show that Copilot often generates code with errors, which can become a liability for programmers (Dakhel et al., 2023). Furthermore, programmers also need to debug and fix those errors and make other modifications in order to integrate generated code into their program context, which may, in turn, impair their productivity (Vaithilingam et al., 2022).\n\nChatGPT has gained popularity among programmers of all levels since its release in November 2022. One of the main advantages of ChatGPT over GitHub Copilot is that ChatGPT works as a conversational chatbot that allows users to ask questions and give feedback beyond code completion. For instance, programmers can ask a conceptual question about a data type used in a program, ask for a code explanation, and ask how to fix an error message (Tozzi, 2023). Recently, GitHub announced GitHub Copilot X, which integrates GPT-4, a more advanced version of the LLM behind ChatGPT, into Copilot (GitHub, 2023b, a). Despite the popularity of ChatGPT among programmers, to the best of our knowledge, there is still no in-depth and comprehensive analysis of the characteristics and quality of ChatGPT answers to programming questions. We bridge this research gap by empirically studying ChatGPT answers to programming questions on Stack Overflow.\n\n3. Research Questions\n\nThis section describes the research questions investigated in this work and the rationale of each research question. The findings of these research questions will deepen our understanding of the characteristics of and the human perception of ChatGPT answers. They will also shed light on the challenges and risks of using ChatGPT for programming and inform the design of new interactive and computational methods to counter misinformation generated by ChatGPT.\n\n‚Ä¢\n\nRQ1. How do ChatGPT answers differ from SO answers in terms of correctness and quality? Previous work (Cao et al., 2021; Huang et al., 2021; Koco≈Ñ et al., 2023) has shown that LLMs such as ChatGPT are prone to hallucination and may generate content with low quality. Therefore, we want to assess and quantify the correctness and different quality aspects (e.g., consistency, conciseness, comprehensiveness) of ChatGPT answers to programming questions.\n\n‚Ä¢\n\nRQ2. What are the fine-grained issues associated with each of the correctness and quality aspects? While RQ1 aims to provide a quantification of the correctness and quality of ChatGPT answers, RQ2 aims to conduct an in-depth, qualitative analysis and develop a taxonomy of the issues in ChatGPT answers. For instance, we are interested in finding out the common symptoms of hallucinations, e.g., conceptual errors, code errors, terminology errors, etc.\n\n‚Ä¢\n\nRQ3. Do the types of SO questions affect the quality of ChatGPT answers? Previous studies (Allamanis and Sutton, 2013; Kou et al., 2023) show that linguistic forms of human answers on Stack Overflow vary based on the types of programming questions. For example, How-to questions have step-by-step answers, while conceptual questions contain descriptions and definitions. We seek to understand if the types of programming questions influence the characteristics of ChatGPT answers in a similar manner.\n\n‚Ä¢\n\nRQ4. Do the language structure and attributes of ChatGPT answers differ from SO answers? Previous studies (Zhou et al., 2023) show that human-crafted misinformation and machine-generated misinformation have distinct linguistic features, which can facilitate misinformation detection. Prior work (Bazelli et al., 2013) has also shown a relationship between linguistic characteristics and the acceptance of Stack Overflow answers. Inspired by these findings, we want to investigate the distinct linguistic characteristics of ChatGPT answers and how they compare to accepted SO answers written by human programmers.\n\n‚Ä¢\n\nRQ5. Do the underlying sentiment of ChatGPT answers differ from SO answers? Previous studies (Miller et al., 2022; Qiu et al., 2019) discuss the harmful effect of toxicity or negative tone in online discussions. Prior work (Calefato et al., 2015) also shows the role of underlying sentiment in the acceptance of SO answers. Therefore, we seek to analyze the sentiment of ChatGPT answers and compare it to accepted answers on Stack Overflow.\n\n‚Ä¢\n\nRQ6. Can programmers differentiate ChatGPT answers from human answers? We are curious about whether programmers can discern machine-generated answers from human-written answers and what kinds of heuristics they employ to make the decision. Investigating these heuristics is important since it helps identify good practices that can be adopted by the programmer community and inform the design of automated mechanisms.\n\n‚Ä¢\n\nRQ7. Can programmers identify misinformation in ChatGPT answers? Understanding how programmers identify misinformation in ChatGPT answers is important as it can provide insights about effective mechanisms to counter misinformation. If programmers can identify the misinformation properly, we expect to find out the techniques of identification. Otherwise, if programmers struggle to identify misinformation, we expect to find out the challenges.\n\n‚Ä¢\n\nRQ8. Do programmers prefer ChatGPT over Stack Overflow? Finally, we want to understand the user preference between ChatGPT and human-generated answers based on the correctness, quality, and linguistic characteristics of the answer.\n\n4. Methodology\n\nWe adopted a mixed-methods research design to answer the research questions in Section 3. Specifically, to answer RQ1-RQ3, we conducted an in-depth manual analysis through open coding and thematic analysis (Section 4.2). To answer RQ4 and RQ5, we conducted a large-scale linguistic analysis and sentiment analysis using automated methods (Section 4.3). To address RQ6 to RQ8, we conducted user studies followed by semi-structured interviews with 12 participants (Section 4.4). The following sections provide a detailed description of each method.\n\n4.1. Data Collection\n\n4.1.1. SO Question Collection\n\nWe consider three characteristics of programming questions‚Äîquestion popularity, posting time, and question type. We adopted a stratified sampling strategy to collect a balanced set of SO questions that fall into different categories w.r.t. their popularity, posting time, and question type. Table 1 shows the distribution of the sampled questions. We describe the sampling procedure below.\n\nFirst, we collected all questions in the SO data dump (March 2023) (Exchange, 2023) and ranked them by their view counts. We used view counts as the popularity metric of SO questions. We selected three categories of questions‚Äîthe top 10% of questions in the view count ranking (Highly Popular), the questions in the middle (Average Popular), and the bottom 10% in the ranking (Unpopular).\n\nSecond, from the three categories of questions above, we moved on to categorize them by their recency. We split questions in each popularity category into two recency categories‚Äîquestions posted before the release of ChatGPT (November 30, 2022) as Old, and questions posted after that time as New. We selected the release date of ChatGPT to evaluate how the answer characteristics of ChatGPT reflect the presence or absence of specific knowledge in ChatGPT‚Äôs training data.\n\nThird, for question types, based on the literature (Treude et al., 2011; Allamanis and Sutton, 2013; De Souza et al., 2014; Kou et al., 2022), we focused on three common question types‚ÄîConceptual, How-to, and Debugging. We followed prior work (Iyer et al., 2016; Kou et al., 2023) and trained a Support Vector Machine (SVM) classifier to predict the type of a SO question based on the question title. The classifier achieves an accuracy of 78%, which is comparable to prior work. Then, we used this classifier to predict the question type of SO questions in each category of questions obtained from the two previous steps.\n\nIn the end, we randomly sampled the same number of questions from each category along the three aspects. Given that the question type classifier may not be accurate, we manually validated the question type of each sample and discarded those with the wrong types. We ended up with 517 sampled questions, as shown in Table 1. Additionally, we randomly sampled another set of 2000 questions from the SO data dump for linguistic analysis. Since all collected questions are originally in HTML format, we removed HTML tags and stored them as plain text with their metadata (e.g., tags, view count, types, etc.) in CSV files.\n\n4.1.2. ChatGPT Answer Collection\n\nFor each of the 517 SO questions, the first two authors manually used the SO question‚Äôs title, body, and tags to form one question prompt and fed that to the free version of ChatGPT, which is based on GPT-3.5. We chose the free version of ChatGPT because it captures the majority of the target population of this work. Since the target population of this research is not only industry developers but also programmers of all levels, including students and freelancers around the world, the free version of ChatGPT has significantly more users than the paid version, which costs a monthly rate of 20 US dollars. The answers generated by ChatGPT are stored in CSV files. Since ChatGPT stores the history of previous input and output of a session, a new chat session was started before feeding each question prompt to ChatGPT. For the additional 2000 SO questions, we developed an automated script to prompt ChatGPT with the gpt-3.5-turbo API. For each question, this script automatically extracted and concatenated its title, body, and tags based on the prompt template and stored ChatGPT answers in CSV files. Each new prompt was conducted via a new API call, which cleared the context history of previous prompts.\n\n4.2. Manual Analysis\n\nIn this section, we describe the manual analysis procedure for the 517 ChatGPT answers.\n\n4.2.1. Open Coding Procedure\n\nTo assess the quality and correctness of ChatGPT answers (RQ1), we used a standard NLP data labeling process (Rodrigues et al., 2014; Xu et al., 2020) to label the ChatGPT answers at the sentence level. Over the course of five weeks, the first three authors met six times to generate, refine, and finalize the codebooks to annotate the ChatGPT answers. First, the first two authors familiarized themselves with the data. Each author independently labeled five ChatGPT answers at the sentence level and took notes about their observations. The two authors met to review their labeling notes and performed thematic analysis (Braun and Clarke, 2006; Guest et al., 2011) to categorize the labels into four themes‚ÄîCorrectness, Consistency, Comprehensiveness, Conciseness. Then, they developed the initial codebook, relabeled the previous five ChatGPT answers based on the codebook, and met the other co-authors to resolve the disagreements and refine the codebook. After this step, the codebook contained 24 codes in the four themes.\n\nThe first two authors then moved on and labeled 20 new ChatGPT answers independently based on the codebook. Since one text span in an answer may suffer from multiple quality issues, the labeling is essentially a multi-label, multi-class classification where labels are not mutually exclusive. Therefore, we cannot use Cohen‚Äôs Kappa to measure the agreement level between labelers. Instead, we used Fleiss‚Äôs Kappa (Fleiss, 1971) score. The initial score was 0.45, which was not high enough to proceed to label more answers. Thus, the authors met again to discuss the labeling. They carefully reviewed each label in the answers and resolved the conflicts. They further refined the codebook by merging redundant codes, improving the definitions of ambiguous codes, and introducing new codes. At the end of this step, there were 21 codes in the codebook.\n\nWith the refined codebook, the first two authors re-labeled 10 of the previous 20 answers and confirmed the agreement. Except for disagreement about the definition and usage of 2 codes in correctness category, no new disagreement was discovered. At this point, Fleiss‚Äôs Kappa score was 0.79. Next, the first two authors met the co-authors to review and refine the current codebook and labelings. After this meeting, the codebook was refined to 19 codes.\n\nFinally, the first two authors labeled 20 new ChatGPT answers with the refined cookbook and arrived at a Fleiss‚Äôs Kappa score of 0.83, which implies substantial agreement. With this codebook, the first two authors split the remaining ChatGPT answers and labeled them separately. The whole labeling process took about 216 person-hours.\n\n4.2.2. Definitions and Discussion of Codebook\n\nThe codebook developed in the previous section contains a wide range of fine-grained codes that are used to develop a taxonomy of issues in ChatGPT answers (RQ2). We give a quick overview of these codes below. Section 5 provides more details.\n\nFor Correctness, we compared ChatGPT answers with the accepted SO answers and also resorted to other online resources such as blog posts, tutorials, and official documentation. Our codebook includes four types of correctness issues‚Äî Factual, Conceptual, Code, and Terminological errors. Specifically, for incorrect code examples embedded in ChatGPT answers, we identified four types of code errors‚ÄîSyntax errors, Wrong Logic, Wrong API/Library/Function Usage, and Incomplete Code. An answer is considered fully correct if it does not contain any of these errors, i.e., Factual, Conceptual, Code, or Terminological errors.\n\nFor Consistency, we measured the consistency between ChatGPT answers and the accepted human-written answers on Stack Overflow. Note that inconsistency does not imply incorrectness. A ChatGPT answer can be different from an accepted human answer, but it can still be correct. Five types of inconsistencies emerged from the manual analysis‚ÄîFactual Inconsistency, Conceptual Inconsistency, Terminological Inconsistency, Coding Inconsistency, and Different Number of Solutions (e.g., ChatGPT provides four solutions where SO gives only one).\n\nFor Conciseness, three types of conciseness issues were identified and included in the codebook‚ÄîRedundant, Irrelevant, and Excess information. Redundant sentences reiterate information stated in the question or in other parts of the answer. Irrelevant sentences talk about concepts that are out of the scope of the question being asked. And lastly, Excess sentences provide information that is not required to understand the answer.\n\nComprehensiveness is an overall assessment of the entire answer. Thus, the codebook only includes two codes‚ÄîComprehensive, and Not Comprehensive. To consider an answer to be comprehensive, it needs to fulfill two requirements‚Äì(1) all parts of the question are addressed in the answer, and (2) a complete solution is provided in the answer.\n\n4.3. Linguistic Analysis\n\nPrevious studies show that user preference and acceptance of an SO answer can depend on the underlying emotion, tone, linguistic style, and sentiment in the answer (Shneiderman, 1980; Calefato et al., 2015; Bazelli et al., 2013). In this section, we describe the automated methods utilized to determine linguistic features and sentiments of ChatGPT answers.\n\n4.3.1. Linguistic Characteristics\n\nWe employed a widely used tool called Linguistic Inquiry and Word Count (LIWC) (Pennebaker et al., 2015) to analyze the linguistic features of ChatGPT and SO answers. LIWC is a psycholinguistic database that provides a dictionary of validated psycholinguistic lexicons in pre-determined categories that are psychologically meaningful. LIWC counts word occurrence frequencies in each category that holds important information about the emotional, cognitive, and structural components associated with text or speech. LIWC has been used to study AI-generated misinformation (Zhou et al., 2023), emotional expressions in social media posts (Kramer, 2012), the success of human answers (Bazelli et al., 2013), etc. In our work, we considered the following categories:\n\n‚Ä¢\n\nLinguistic Styles: We considered four attributes related to linguistic styles‚ÄîAnalytical Thinking (complex thinking, abstract thinking), Clout (power, confidence, or influential expression), Authentic (spontaneity of language), and Emotional Tone.\n\n‚Ä¢\n\nAffective Attributes: Affective attributes capture expressions and features related to emotional status. They include Affect (overall emotional expressions, e.g., ‚Äúhappy‚Äù, ‚Äúcried‚Äù), Positive Emotion (e.g., ‚Äúhappy‚Äù, ‚Äúnice‚Äù), and Negative Emotion (e.g., ‚Äúhurt‚Äù, ‚Äúcried‚Äù).\n\n‚Ä¢\n\nCognitive Processes: Cognitive processes represent features that are related to cognitive thinking and processing, e.g., causation, knowledge, insight, etc. For this category, we considered Insight (e.g., ‚Äúthink‚Äù, ‚Äúknow‚Äù), Causation (e.g., ‚Äúbecause‚Äù), Discrepancy (e.g., ‚Äúshould‚Äù, ‚Äúwould‚Äù), Tentative (e.g., ‚Äúperhaps‚Äù), Certainty (e.g., ‚Äúalways‚Äù), and Differentiation (e.g., ‚Äúbut‚Äù, ‚Äúelse‚Äù).\n\n‚Ä¢\n\nDrives Attributes: Drives capture expressions that show the need, desire, and effort to achieve something. For this category, we considered Drives, Affiliation (e.g., ‚Äúally‚Äù, ‚Äúfriend‚Äù), Achievement (e.g., ‚Äúwin‚Äù, ‚Äúsucess‚Äù), Power (e.g., ‚Äúsuperior‚Äù), Reward (e.g., ‚Äúprize‚Äù, ‚Äúbenefit‚Äù), and Risk (e.g., ‚Äúdanger‚Äù, ‚Äúdoubt‚Äù).\n\n‚Ä¢\n\nPerceptual Attributes: This category captures the attributes that are related to Perceive, See, Feel, or Hear.\n\n‚Ä¢\n\nInformal Attributes: This category captures the causality in everyday conversations. The attributes in this category include Informal Language, Swear Words, Netspeak (e.g., ‚Äúbtw, lol‚Äù), Assent (e.g., ‚ÄúOK‚Äù, ‚ÄúYeah‚Äù), Nonfluencies (e.g., ‚Äúer‚Äù, ‚Äúhmm‚Äù), and Fillers (e.g., ‚ÄúI mean‚Äù, ‚Äúyou know‚Äù).\n\nWe used LIWC to compute word frequency in each of the categories for 2000 ChatGPT answers and the corresponding human answers from Stack Overflow. For ease of understanding, we computed the relative differences (RD) in linguistic features between 2000 pairs of ChatGPT and SO answers from the computed average word frequencies in each category.\n\nR‚Å¢D=ChatGPTavg.frequency‚àíSOavg.frequencySOavg.frequencyRD=\\frac{ChatGPT\\ \\ avg.\\ \\ frequency-SO\\ \\ avg.\\ \\ frequency}{SO\\ \\ avg.\\ \\ frequency}italic_R italic_D = divide start_ARG italic_C italic_h italic_a italic_t italic_G italic_P italic_T italic_a italic_v italic_g . italic_f italic_r italic_e italic_q italic_u italic_e italic_n italic_c italic_y - italic_S italic_O italic_a italic_v italic_g . italic_f italic_r italic_e italic_q italic_u italic_e italic_n italic_c italic_y end_ARG start_ARG italic_S italic_O italic_a italic_v italic_g . italic_f italic_r italic_e italic_q italic_u italic_e italic_n italic_c italic_y end_ARG\n\n4.3.2. Sentiment Analysis\n\nLexicon-based LIWC evaluates linguistic characteristics based on psycholinguistic features and captures the sentiment of texts only based on overall polarity. Hence, LIWC is insufficient when it comes to capturing the intensity of the polarity (Boghe, 2020). Moreover, LIWC can not capture sarcasm, irony, misspelling, or negation, which is necessary to analyze sentiment in human-written texts on Q&A platforms. Therefore, we employed a machine learning algorithm to further evaluate and compare the underlying sentiment portrayed in the ChatGPT answers and human answers. Specifically, we used a RoBERTa-based sentiment analysis model from Hugging Face (Face, 2022). This model is pre-trained on a Twitter corpus and is then finetuned with the 4423 annotated SO posts from Calefato et al. (Calefato et al., 2018b). This well-balanced dataset has 35% posts with positive sentiment, 27% of posts with negative sentiment, and 38% of posts with neutral sentiment.\n\n4.4. User Study\n\nTo understand programmers‚Äô perception of ChatGPT answers and human answers, we conducted a within-subjects user study with 12 participants. Our goal is to observe how programmers assess those answers and which kind of answers they prefer.\n\n4.4.1. Participants\n\nFor the user study, we recruited 12 participants (3 female, 9 male) with programming backgrounds. 7 participants were graduate students, 4 participants were undergraduate students, and 1 participant was a software engineer from the industry. The participants were recruited by word of mouth. Participants rated their programming expertise by answering multiple-choice questions with five options‚ÄîNovice, Beginner, Competent, Proficient, and Expert. Eight participants self-reported as proficient, three as competent, and one as beginner. Since some participants may be modest about their programming skills, we also collected the number of years of programming experience. Four participants had three years of experience, one had four years, one had five years, two had six years, one had seven years, and three had eight years of programming experience. Additionally, we asked participants how often they use ChatGPT and how often they use SO. For ChatGPT, three answered very often, three answered some of the time, two answered seldom, and four answered never. For SO, four participants answered all the time, five answered very often, two answered some of the time, and one answered seldom.\n\n4.4.2. SO Question Selection\n\nWe randomly sampled eight questions from our manual analysis dataset. ChatGPT gave incorrect answers to five questions and correct answers to three questions. One question was about C++, one about PHP, two about HTML/CSS, three about JavaScript, and one about Python.\n\n4.4.3. Protocol\n\nIn this user study, we asked participants to complete a sequence of decision-making tasks to verify and assess the quality of machine and human-generated answers to programming questions. The tasks were designed to capture user perception and preference for human and machine-generated answers. In each task, we asked the participants to verify and assess a ChatGPT answer and a human answer to a SO question and rate the correctness and quality of each answer. Moreover, for each task, the participants were asked to mark which answer they preferred and guess which answer was generated by ChatGPT. The step-by-step procedure for each task is described below.\n\nEach user study started with consent collection and an introduction to the study procedure. Then, the participants started the study tasks by reading each SO question and rating their familiarity with the topic asked in the question (5-point Likert Scale (Norman, 2010)). Familiarity with a specific programming topic is not directly related to years or hours of programming experience, as programmers tend to be more familiar with topics they have used more recently. Therefore, we resorted to the self-reporting method. Then, they were presented with an answer to the question. This answer is either generated by ChatGPT or written by a human programmer on Stack Overflow. Then, they were asked to answer a series of 5-point scale survey questions to assess the correctness, comprehensiveness, conciseness, and usefulness of this answer. Then, they were presented with the other answer and asked to answer the same set of survey questions. Then, they were asked to select which answer they prefer, which answer they believe is generated by ChatGPT, and how confident they are about their choices. We repeated this process for all eight SO questions and randomized the order of ChatGPT and human answers for each question. For ease of running this study, all SO questions, answers, survey questions, and instructions were encoded into a Qualtrics survey.\n\nThe human answers and ChatGPT answers were presented with the same text format and style (e.g., font type, font size, code format, etc.), so participants could not easily tell them apart just based on formatting and visual styles. Participants were allowed to skip to the next SO question if they were not familiar with the topic of a certain SO question. The order of ChatGPT and human answers was assigned randomly (i.e., not always Answer 1 was ChatGPT answer). Additionally, participants were encouraged to refer to external resources, such as Google search, tutorials, and API documentation, to verify the correctness of the given answers. In the verification process, to prevent participants from running into the same human answer on Stack Overflow or getting the same answer from ChatGPT, the participants were not allowed to search on Stack Overflow, open a Stack Overflow page returned by Google Search, or ask the same question to ChatGPT. Apart from accessing ChatGPT and SO for the same question, participants were allowed to validate the code generated by ChatGPT in any local IDE, online sandbox, or online code editor at their convenience. Each participant was given 20 minutes to examine and rate answers to SO questions. Participants were made aware that finishing all eight questions was not required and were encouraged to aim for comprehensiveness and quality instead of the number of examined answers. All participants used up the given 20 minutes of time in the study. On average, participants assessed the correctness and quality of the answers to 5 questions.\n\n4.4.4. Semi-Structured Interview\n\nThe survey was followed by a lightweight semi-structured interview. Each interview took about 10 minutes on average. During the interview, we reviewed the participant‚Äôs responses to the survey together with the participant and asked them why they preferred one answer over the other. Then, we asked the participants about their heuristics to identify the ChatGPT answer before revealing the correct answer to them. If the participants were correct, we asked a follow-up question about the characteristics of ChatGPT answers that influenced their decision. Lastly, we asked how they determined the incorrect information in an answer. We also asked follow-up questions such as why they failed to identify some misinformation, what the main challenges were in verifying the correctness, what additional tool support they wish to have, etc.\n\n4.4.5. Qualitative Analysis of the Interview Transcripts\n\nThe first author transcribed the audio recordings and labeled all 12 transcripts following the open coding method (Hancock et al., 2001). The author labeled all insightful responses that mentioned factors related to participants‚Äô preferences, the heuristics used by the participants, the obstacles they faced, and the tool support they wished to have. After this step, the author did a thematic analysis (Braun and Clarke, 2006; Guest et al., 2011) to group the low-level labels into high-level patterns and themes. The final codebook for thematic analysis contains 5 themes and 21 patterns. The overall process took about six person-hours.\n\n5. Manual Analysis Results\n\nThis section presents the results and findings for RQ1-RQ3.\n\n5.1. RQ1: Overall Correctness and Quality\n\nOur results show that, among the 517 ChatGPT answers we labeled, 52% of them contain incorrect information, 78% are inconsistent from human answers, 35% lack comprehensiveness, and 77% contain redundant, irrelevant, or unnecessary information. Moreover, on average, ChatGPT answers and human answers contain 266.43 tokens (œÉùúé\\sigmaitalic_œÉ=87.99) and 213.80 tokens (œÉùúé\\sigmaitalic_œÉ=246.04) respectively. The mean difference of 52.63 tokens is statistically significant (paired t-test: p-value¬°0.001). Table 2 shows our manual analysis results.\n\nFinding 1\n\n5.2. RQ2: A Taxonomy of Fine-Grained Issues in ChatGPT Answers\n\nOur thematic analysis reveals four types of incorrectness in ChatGPT answers‚ÄîConceptual (54%), Factual (36%), Code (28%) and Terminology (12%) errors. Note that these errors are not mutually exclusive. Some answers have more than one of these errors. Factual errors occur when ChatGPT states some fabricated or untruthful information about existing knowledge, e.g., claiming a certain API solves a problem when it does not, fabricating non-existent links, untruthful explanations, etc. On the other hand, Conceptual errors occur if ChatGPT fails to understand the question. For example, the user asked how to use public and private access modifiers, and ChatGPT answered the benefits of encapsulation in C++. Code errors occur when the code example in the answer does not work, or cannot provide a desired output. And lastly, Terminology errors are related to wrong usages of correct terminology or any use of incorrect terminology, e.g., perl as a header of Python code.\n\nSpecifically, for code errors, our analysis reveals four types of code errors‚Äîwrong logic (48%), wrong API/library/function usage (39%), incomplete code (11%), and wrong syntax (2%). Again, some generated code has more than one of these errors. Logical errors are made by ChatGPT when it can not understand the problem, fails to pinpoint the exact part of the problem, or provides a solution that does not solve the problem. For example, in many debugging instances, we found that ChatGPT tried to resolve one part of the given code, whereas the problem lied in another part of the code. One such example is provided in Appendix A. We also observed that ChatGPT often fabricated APIs or claimed certain functionalities that were wrong.\n\nFinding 2\n\nFinding 3\n\nChatGPT rarely makes syntax errors for code answers. The majority of the code errors are due to applying wrong logic or implementing non-existing or wrong API, library, or functions.\n\nThe ChatGPT answers that have no statements annotated as factual, conceptual, code, or terminological errors, are considered to be correct. We found that 48% of the ChatGPT answers had an absence of any type of fine-grained errors identified in the manual analysis.\n\nAmong the answers that are Not Concise, 46% of them have Redundant information, 33% have Excess information, and 22% have Irrelevant information. For Redundant information, during our labeling process, we observed that many of the ChatGPT answers repeat the same information that is either stated in the question or stated in other parts of the answers. For Excess information, we observed a handful of cases where ChatGPT unnecessarily gives background information such as long definitions, or writes something at the end of the answer that does not add any necessary information to understand the solution. Lastly, many answers contain Irrelevant information that is out of context or scope of the question. In answers with conceptual errors, we observed this behavior more often. There are answers that have a combination of more than one of these conciseness issues. An example of a verbose ChatGPT response is provided in Appendix B.\n\nAnd lastly, for inconsistency with human answers, we found five types of Inconsistencies‚ÄîConceptual (67%), Factual (44%), Code (55%), Terminology (6%), and Number of Solutions (42%). The first four types of inconsistencies occur for the same reason as incorrectness. The only difference is that inconsistency does not always mean incorrectness, as explained in Section 4.2.2. Similar to incorrectness, conceptual inconsistencies are higher than factual inconsistencies. Our observation also reveals that ChatGPT-generated code is very different from human-written code in format, semantics, syntax, and logic. This contributes to the higher number of Code inconsistencies. The Number of solutions inconsistency is very prominent as ChatGPT often provides many additional solutions to solve a problem.\n\n5.3. RQ3: Effects of Question Type\n\nTo evaluate the relationship between question types and ChatGPT answer quality, we calculated the percentage of each label across all categories for each question type. As our data is entirely categorical, we evaluated the statistical significance of the relationship between each question type and each of the four label categories with Pearson‚Äôs Chi-square test. Table 2 highlights all relationships that are statistically significant (p-value¬°0.05). Our results show that Question Popularity and Recency have a statistically significant impact on the Correctness of answers. Specifically, answers to popular questions and questions posted before November 2022 (the release date of ChatGPT) have fewer incorrect answers than answers to other questions. This implies that ChatGPT generates more correct answers when it has more information about the question topic in its training data. Although Debugging questions have more incorrect ChatGPT answers, the difference is not statistically significant. This indicates that Question Type does not affect the Correctness of ChatGPT answers.\n\nAdditionally, we found a statistically significant relationship between Question Type and Inconsistency. Since there are often multiple ways to debug and fix a problem, the inconsistencies between human and ChatGPT-generated answers for Debugging questions are higher, with 83% of inconsistent answers. Our observation aligns with this result too. While labeling the answers, we found that almost half of the correct Debugging answers use different logic, API, or library to solve a problem that produces the same output as human answers.\n\nOur results also show that ChatGPT answers are consistently Comprehensive for all categories of SO questions and do not vary with different Question Type, Recency, or Popularity.\n\nMoreover, our analysis shows that answers to all kinds of questions, irrespective of the Type, Recency, and Popularity, are consistently verbose. Yet answers to different kinds of questions indeed have statistical differences in verbosity. Specifically, answers to Popular questions are Not Concise 84% of the time, while answers for Average and Not Popular questions are Not Concise 74% and 72% of the time. This suggests that for questions targeting popular topics, ChatGPT has more information on them and adds lengthy details. We found the same pattern for Old questions. Answers to Old questions (83%) are more verbose than New questions (71%). Finally, for Question Type, Debugging answers are more Concise (40%) compared to Conceptual (16%) and How-to (13%) answers, which are extremely verbose. This is because of ChatGPT‚Äôs tendency to elaborate definitions for Conceptual questions and to generate step-by-step descriptions for How-to questions.\n\nFinding 4\n\nPopularity, Type, and Recency of programming questions affect the correctness and quality of ChatGPT answers. Answers to more Popular and Older posts are less incorrect and more verbose. Debugging answers are more inconsistent but less verbose. Conceptual and How-to answers are the most verbose.\n\n6. Linguistic Analysis Results\n\n6.1. RQ4: Linguistic Characteristics\n\nTable 3 presents the relative differences in the linguistic features between ChatGPT answers and human answers. As stated in Section 4.3, relative differences capture the normalized difference in word frequencies for each linguistic feature between ChatGPT answers and human answers. Positive relative differences indicate features prominent in ChatGPT answers, and negative relative differences indicate features prominent in human answers. Our result shows several statistically significant linguistic differences between ChatGPT answers and human answers.\n\nFirst, we found that ChatGPT answers differ from human answers in terms of language styles. ChatGPT answers are found to contain more words related to analytical thinking and clout expressions. This indicates that ChatGPT answers communicate a more abstract and cognitive understanding of the answer topic, and the language style is more influential and confident. On the other hand, human answers include fewer words related to authenticity, indicating that human answers are more spontaneous and non-regulated.\n\nFor affective attributes that capture emotional status, we found human answers contain more keywords related to emotional status. Though not statistically significant, ChatGPT answers portray more positive emotions, whereas human answers portray significantly more negative emotions than ChatGPT.\n\nMoreover, ChatGPT answers contain significantly more drives attributes compared to human answers. ChatGPT conveys stronger drives, affiliation, achievement, and power in its answers. We observed that many ChatGPT answers include words and phrases, such as ‚Äúof course I can help you‚Äù and ‚Äúthis will certainly fix it.‚Äù This observation aligns with the higher drives attributes in ChatGPT-generated answers. However, ChatGPT answers do not convey risks as much as human answers do. This indicates that human answers on Stack Overflow often warn programmers of the side effects of solutions more than ChatGPT does.\n\nFor informal attributes, human answers are highly informal and casual. On the contrary, ChatGPT answers are very formal and do not make use of swear words, netspeak, nonfluencies, or fillers. In our observation, we rarely saw ChatGPT using a casual conversation style. On the other hand, human answers often had words such as ‚Äúbtw‚Äù, ‚ÄúI guess‚Äù, etc. Human answers also contain higher perceptual and cognitive keywords than ChatGPT answers. According to the definitions of perceptual and cognitive attributes by LIWC (Section 4.3), this indicates that human answers portray more personal observations and insights from human programmers when answering the question.\n\nFinding 5\n\n6.2. RQ5: Sentiment Analysis\n\nOur results show that, among the 2000 ChatGPT answers, 1707 (85.35%) of them portray positive sentiment, 291 answers (14.55%) portray neutral sentiment, and only 2 answers (0.1%) portray negative sentiment. On the other hand, 1466 of the 2000 SO answers (73.30%) portray positive sentiment, 513 answers (25.65%) portray neutral, and 21 answers (1.05%) portray negative sentiment. To assess the sentiment difference between ChatGPT and SO answers, we performed a McNemar-Bowker test on the sentiments. Since we have paired-nominal data, we opted for the McNemar-Bowker test for testing the goodness of fit when comparing the distribution of counts of each label. The results are statistically significant (X2=186.84,d‚Å¢f=3,p<0.001formulae-sequencesuperscriptùëã2186.84formulae-sequenceùëëùëì3ùëù0.001X^{2}=186.84,df=3,p<0.001italic_X start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT = 186.84 , italic_d italic_f = 3 , italic_p < 0.001). Our results show that for 13.90% questions, ChatGPT answers portrayed positive sentiment while human answers portrayed neutral or negative sentiments. On the other hand, only 2 ChatGPT answers portrayed negative sentiment when the human answers were positive or neutral. Our result indicates that ChatGPT shows significantly more positive sentiment compared to human answers.\n\nFinding 6\n\n7. User Study Results\n\nWe retrieved 56 pairs of ratings of ChatGPT answers and human answers as rated by 12 participants. Figure 1 presents the average ratings of the two kinds of answers in all four quality aspects. Overall, users found human answers to be more correct (mean rating human: 4.41, ChatGPT: 3.21, Welch‚Äôs t-test: p-value¬°0.001), more concise (human: 4.16, ChatGPT: 3.69, Welch‚Äôs t-test: p-value¬°0.05), and more useful (human: 4.21, ChatGPT: 3.42, Welch‚Äôs t-test: p-value¬°0.01). For comprehensiveness, the average ratings are 3.89 and 3.98 for human answers and ChatGPT answers respectively. However, this result is not statistically significant.\n\nAdditionally, our thematic analysis revealed five themes‚ÄîProcess of differentiating ChatGPT answers from human answers, Heuristics of verifying correctness, Reasons for incorrect determination, Desired support, and Factors that influence user preference. Findings from our quantitative and thematic analysis for each of the research questions are described in the following subsections.\n\n7.1. RQ6: Differentiating ChatGPT answers from human answers\n\nOur study results show that participants successfully identified which one is the machine-generated answer 80.75% of the time and failed only 19.25% of the time (Welch‚Äôs t-Test, p-value¬°0.001).\n\nFrom thematic analysis, we identified the factors that participants found helpful to discern ChatGPT answers from human answers. 6 out of 12 participants reported the writing style of answers to be helpful in identifying the ChatGPT answer. Participant P5 mentioned, ‚Äúgood grammar‚Äù, and P8 mentioned, ‚Äúheader, body, summary format‚Äù to be contributing factors for identification. Two other factors are language style (e.g., casual or formal language, format) (10 out of 12 participants) and length (7 out of 12 participants). Additionally, 5 participants found unexpected or impossible errors as a helpful factor in identifying the machine-generated answers. Apart from these, tricks and insights that only experienced people can provide (5 out of 12 participants), and high entropy generation (1 out of 12 participants) were two other reported factors. Our result suggests that most participants use language and writing styles, length, and the presence of abnormal errors to determine the source of an answer.\n\nFinding 7\n\n7.2. RQ7: Assessing Answer Correctness\n\nOur study result shows that users could successfully identify the incorrect answers only 60.66% of the time and failed 39.34% of the time (Welch‚Äôs t-test, p-value¬°0.05).\n\nWhen we asked users how they identified incorrect information in an answer, we received three types of responses. 10 out of 12 participants mentioned they read through the answer, tried to find any logical flaws, and tried to assess if the reasoning made sense. 7 participants mentioned they identified the terminology and concepts they were not familiar with and did a Google search, and read documentation to verify the solutions. And lastly, 4 out of 12 users mentioned that they compared the two answers and tried to understand which one made more sense to them. All of the aforementioned verification processes involved assessing the code or part of the answers in external IDEs. All of our participants copied code or tested part of the solution from at least one answer into their local IDE for validation, 9 participants utilized some online code sandbox for validation (e.g., sandbox for HTML, CSS, JS), and 6 participants used the built-in code editor from tutorial sites such as W3Schools as a part of their assessment process.\n\nWhen a participant failed to correctly identify the incorrect answer, we asked them what could be the contributing factors. 7 out of 12 participants mentioned the logical and insightful explanations, and comprehensive and easy-to-read solutions generated by ChatGPT made them believe it to be correct. 6 participants mentioned lack of expertise to be the reason. However, we ran Pearson‚Äôs Chi-Square test to evaluate the relationship between overlooking incorrect answers and topic expertise and found no significant relation between these two. P7 and P10 said ChatGPT‚Äôs ability to mimic human answers made them trust the incorrect answers.\n\nFinding 8\n\nAdditionally, participants expressed their desire for tools and support that can help them verify the correctness. 10 out of 12 participants emphasized the necessity of verifying answers generated by ChatGPT before using it. Participants also suggested adding links to official documentation and supporting in-situ execution of generated code to ease the validation process.\n\n7.3. RQ8: Factors for User Preference\n\nParticipants preferred SO answers 65.18% of the time. However, participants still preferred ChatGPT answers 34.82% of the time (Welch‚Äôs t-test, p-value¬°0.01). Among the ChatGPT preferences, 77.27% of the answers were incorrect.\n\nFor factors that influence user preference, 10 out of 12 participants mentioned correctness to be the main contributing factor for preference. 8 participants mentioned answer quality (e.g., conciseness, comprehensiveness) as contributing factors. 6 participants mentioned they put emphasis on how insightful and informative the answer is while preferring. 6 participants stated language style to be one of the factors, 2 of these 6 participants preferred the casual, spontaneous language style of human answer, while the other 4 preferred the well-structured and polite language of ChatGPT. P2 mentioned, ‚ÄúIt feels like it‚Äôs trying to teach me something‚Äù. Finally, 5 participants mentioned the format, look and feel (e.g., highlighting, color scheme) as contributing factors toward preference.\n\nFinding 9\n\nParticipants preferred human answers from Stack Overflow more than ChatGPT answers (65.18% of the time). Participants found human answers to be more correct, concise, and useful.\n\n8. Discussion and Future Work\n\nIn this section, we discuss the implications of our findings and future directions to counter misinformation when using ChatGPT for programming.\n\n8.1. Why Do Users Prefer ChatGPT Responses?\n\nSurprisingly, our user study shows that participants preferred ChatGPT answers 34.82% of the time, though 77.27% of these answers contained misinformation. Furthermore, we observed that participants overlooked a lot of misinformation in ChatGPT answers. Specifically, when ChatGPT answers are not readily verifiable (e.g., requiring execution in an IDE or needing to go through long documentation to validate), users often fail to identify the misinformation and underestimate the degree of incorrectness in the answer. The follow-up semi-structured interviews revealed that the polite language, articulated and text-book style answers, and comprehensiveness are some of the main reasons that made ChatGPT answers look more convincing, so the participants lowered their guard and overlooked some misinformation in ChatGPT answers. This finding is consistent with previous findings of user preferences over Stack Overflow (SO) posts. Prior work (Nasehi et al., 2012; Bazelli et al., 2013; Calefato et al., 2015; Asaduzzaman et al., 2013) shows that SO users preferred posts that contain illustrations, step-by-step instructions, multiple solutions, and positive sentiments. Our linguistic analysis shows that ChatGPT answers possess many of these linguistic characteristics that SO users appreciate.\n\nRecently, there has been a decline in network traffic to the Stack Overflow website, which was attributed to the rise of ChatGPT (Overflow, 2023b). Although our user study does not evaluate what encourages users to ask a question to ChatGPT rather than Stack Overflow in the first place, our findings point to some possible reasons. We believe the fact that users can avoid the embarrassment of posting online and the risk of receiving negative comments but still receive seemingly high-quality answers in a timely manner can be some contributors. Moreover, the interactive feature of ChatGPT makes it easier for users to change prompts and interactively work with the language model to make it generate desired or optimal answers. Using interactivity to rectify errors can be another contribution to ChatGPT‚Äôs popularity among programmers.\n\n8.2. Where Do Errors in ChatGPT Answers Emerge from?\n\nIt is evident from our results that ChatGPT produces incorrect answers more than half of the time. Our observation sheds light on three main reasons for these errors.\n\nLack of Understanding for Some Programming Concepts. First, 54% of the time, errors are made due to ChatGPT not understanding the concepts mentioned in a question. For example, we found a JavaScript question about a website not showing the File Upload option (Overflow, 2023a). Clearly, it is an issue with the front end and User Interface (UI), since the question mentioned ‚Äúthe file upload area not working‚Äù and provided JavaScript and HTML code snippets. In this context, ‚Äúthe file upload area‚Äù refers to the UI widget to upload a file, rather than the action of uploading a file. ChatGPT did not get this and answered a handful of irrelevant solutions, such as how the file path needs to be set, how to locate the file in your machine, CORS issues, etc. By contrast, the human-written answer suggests adding an appropriate id to the file input field in the HTML code. These types of misunderstanding issues contribute to the high number of Conceptual errors.\n\nLimited Capability to Understand and Reason Program Semantics. Our manual analysis reveals that while most of the code examples (98%) generated by ChatGPT are syntactically correct, many of them contain incorrect logic (48%) or incorrect API usage (39%). We suspect this is largely due to ChatGPT‚Äôs limited capability to understand and reason program semantics. In many cases, ChatGPT makes obvious programming mistakes that human programmers barely make. For example, ChatGPT may generate a loop ending condition that is always true or false, e.g. while(i<0 && i>10). Furthermore, the content generation process in ChatGPT is essentially an auto-regressive decoding process guided by the probability distribution at each token prediction step. Thus, ChatGPT cannot foresee the potential outcome or execution result of the generated code. For example, we observed that ChatGPT generated a code example that keeps decreasing a variable in a for loop and eventually leads to a division-by-zero exception in the end. ChatGPT seems unable to understand the consequences or side effects of some code operations and expressions.\n\nMissing or Incorrect Attention to a Programming Question. Since questions asked in SO are long human-written questions with many components involved, ChatGPT often focuses on the wrong part of the question or gives high-level solutions without fully understanding the minute details of a problem. For example, we found an instance where the SO question asked about differences between public, private, and protected access modifiers in Java. However, ChatGPT only focused on the part ‚Äúaccess modifiers‚Äù ignoring the ‚Äúdifference‚Äù part in the question. Therefore, it gives an extremely verbose response that contains the definitions of encapsulation, inheritance, etc., which is not useful in terms of identifying the differences originally asked for.\n\n8.3. What Is at Stake and What Does the Future Hold?\n\nImpact on the Software Industry and Society. We believe that the large number of seemingly correct ChatGPT answers pose high risks to programming practices since they can easily trick programmers into thinking they are correct, especially when programmers lack the expertise or means to readily verify the correctness. As AI Chain frameworks are getting increasingly popular, it becomes even riskier when the ChatGPT answers are automatically integrated into downstream AI components with no human involvement and validation. The misinformation will propagate along the AI chain and may have devastating effects on downstream tasks. In the long term, this could jeopardize the quality and robustness of software and cyberinfrastructure in our society, since the misinformation in these answers may lead to suboptimal design decisions and software defects. The repercussions can potentially affect other societal factors, including the safety, security, and trust of the general population.\n\nImpact on STEM Education. Many STEM fields, beyond Computer Science, require students to learn basic programming. Students using ChatGPT for learning materials may be misled into learning incorrect concepts and information. This may even harm the grades or reputation of students. We believe identifying and verifying errors in programming answers require as much expertise as learning and writing code. Hence, learning through the wrong materials has the potential to create a chain of misinformation where the veracity assessment of students and learners will be compromised in the long term.\n\nThe Silver Lining. While our manual analysis reveals 52% of the answers are incorrect, 48% answers are completely correct (i.e., no statements in those answers annotated with factual, conceptual, code, or terminological errors), which by no means is an insignificant number. Compared with Stack Overflow, ChatGPT can give immediate answers to users‚Äô questions, significantly saving the time and effort of users. Thus, conversational chatbots such as ChatGPT may be considered more convenient than Q&A forums. Programmers of all levels, including students and professional developers, may find it easy and less time-consuming to ask basic programming questions instead of going to instructors, mentors, or even posting on traditional Q&A platforms.\n\nHence, along with trying to rectify the error and mitigate the risks, steps should be taken to create awareness and adopt new strategies and policies to address the risks associated with incorrect information generated by ChatGPT.\n\n8.4. What Further Actions are Needed to Address Misinformation in ChatGPT?\n\n8.4.1. Limitations of Existing Approaches\n\nAlthough approaches have been proposed to mitigate hallucinations from LLMs (Dong et al., 2022; Peng et al., 2023), they are only applicable to fixing Factual errors. Since the root of Conceptual errors is not hallucinations but rather a lack of understanding of programming concepts and incapability to reason program semantics, existing approaches for hallucination may not be effective in mitigating conceptual errors.\n\nMost of the existing methods to help LLMs understand and reason rely on Prompt Engineering. While Prompt Engineering can be helpful in probing ChatGPT to understand a problem to some extent (Zhou et al., 2022; Strobelt et al., 2022), they are still insufficient when it comes to injecting reasoning into LLMs to solve special cases. Moreover, Prompt Engineering is not a sustainable solution and the responsibility largely falls on users.\n\nFurthermore, ChatGPT provides different answers even when prompted with the same questions. This makes the verification process even harder since users cannot deterministically identify the prompts that will always result in correct or optimal solutions. Although lowering the temperature value can help in achieving consistent answers for the same prompts, lower temperature often reduces the quality of answers generated by LLMs. Thus, this variability adds another dimension to the challenges already posed by Prompt Engineering. Additionally, Prompt Engineering implies that to make ChatGPT give the right answer, users need to ask the right question. Thus, overly relying on Prompt Engineering to make ChatGPT produce the correct answer shifts the responsibility for AI errors to humans. Hence, we urge that instead of temporary patches such as changing prompts that also make humans somewhat responsible for the errors made by ChatGPT, it is essential to understand the sources and factors of conceptual errors in order to develop sustainable and special-purpose solutions to fix them.\n\n8.4.2. Communicating the level of incorrectness is necessary.\n\nThe user interface of ChatGPT includes a one-line warning‚Äî‚ÄúChatGPT may produce inaccurate information about people, places, or facts.‚Äù However, we believe such a generic warning is insufficient. Each answer should be accompanied by a level of incorrectness and uncertainty in the answer. Moreover, our observations indicate that not all answers have an equal amount of incorrectness‚Äîsome answers have the majority of parts marked as incorrect, whereas some answers have only a few lines marked as incorrect. Since each incorrect answer differs in the severity of incorrectness, it is vitally important to provide users with the level of incorrectness for each answer. A recent study shows that an LLM may know when it is lying (Azaria and Mitchell, 2023), which can be leveraged to warn users about the potential errors made by LLMs. However, recent studies (Agarwal et al., 2020; Vasconcelos et al., 2023) also show that only rendering the confidence level is not sufficient to help programmers understand the uncertainty and risks in the generated code. Thus, it is necessary to investigate more effective communication and visualization methods for model uncertainty in programming tasks.\n\nMoreover, for software companies, it is worthwhile to invest in more awareness campaigns and training for software developers. Special training is necessary for software developers so that they can monitor the code bases, readily verify errors in ChatGPT answers, and perform more testing to safeguard errors from sneaking into their codebases. In particular, software developers should be advised to use ChatGPT with more caution and scrutiny for high-stake code blocks and programming tasks.\n\n8.4.3. More rigorous code reviews and testing are needed.\n\nSoftware companies should enforce more rigorous code reviews and software testing methods to source code that is produced with the facilitation of ChatGPT and other AI technologies. Since ChatGPT may make programming mistakes that human programmers barely make, it is important to adapt traditional methods to account for the types of programming mistakes generated by ChatGPT or other LLMs. Additionally, it is necessary to have continuous testing and security checking so that incorrect or insecure code can not seep into any part of the software products. Moreover, ChatGPT can be integrated into the testing pipeline as ChatGPT can potentially generate test cases on the fly. Hence, encouraging the integration of testing during the generation process can limit the risk of programming mistakes made by ChatGPT.\n\n8.4.4. Future actions for academics and researchers\n\nBender and Koll- er (Bender and Koller, 2020) show that any LLMs trained only on the form of language can not fully reach the human level of understanding. They argued that to aid LLMs in performing natural language understanding, it is imperative to have information in the training data that goes beyond just the form of language, e.g., code paired with several input and correlated output, edge cases, etc. Furthermore, Bender and Gebru et al. (Bender et al., 2021) argue that increasing the size of language models is not a solution to achieving natural language understanding. We believe one of the main reasons behind the large number of conceptual errors can be attributed to ChatGPT‚Äôs limitation in performing natural language understanding. Moreover, although existing work (Xiong et al., 2017; Chen et al., 2020) shows the challenges and limitations of reasoning in LLMs and presents Knowledge Graphs as a powerful method to aid in reasoning, our results highlight the limitation in reasoning when it comes to programming answers or code solutions. Therefore, we urge the attention of the research community for rigorous investigation and mitigation methods to improve the reasoning and understanding capability of LLMs, especially in the field of programming.\n\n8.4.5. Implications for code reviewers and teaching staff in STEM classrooms.\n\nPrevious work (Jiang and Wilson, 2018; Rashkin et al., 2017) shows that linguistic features can be used as a mechanism to identify misinformation and AI-generated content. Our results show that ChatGPT answers have a very distinct linguistic structure and communication style when answering programming questions. We believe identifying these distinct linguistic features is essential in situations where users need to differentiate between human and machine-generated answers. For example, in CS classrooms, there is an increasing concern that students are using ChatGPT to solve homework assignments, which is an impediment to learning. Traditional plagiarism tools used in academia often cannot detect ChatGPT-generated answers. By having general knowledge of common language styles of ChatGPT answers (e.g., verbosity, formal language, title-body-summary structure, etc.), teaching staff can be more aware of what to look for. Moreover, plagiarism tools, both AI and non-AI, should incorporate unique linguistic characteristics as factors to classify plagiarised documents. Furthermore, as discussed in the previous subsections, it is imperative that code reviewers adopt new techniques and tools to take extra precautions so that incorrect and insecure code does not seep into software products. Incorporating the linguistics style of ChatGPT responses while creating these tools and training code reviewers to make them aware of unique linguistic markers can help the software industry install additional safeguards against incorrect code.\n\n8.4.6. New pedagogical methods are necessary.\n\nApart from the software industry, faculty and teaching staff in the educational institute should also make the students aware of the potential risks that come with seemingly correct ChatGPT answers. Moreover, new pedagogical methods should be adopted to incorporate ChatGPT into the curriculum to utilize the incorrectness as a learning tool. For example, in a beginner Python class, students can be given multiple wrong programs generated by ChatGPT and asked to identify the errors in each program. This type of activity can render learning and create awareness at the same time.\n\n8.4.7. Separation of accountability.\n\nNew policies should be made to separate and distinguish the role of humans and LLMs when LLMs generate misinformation. As discussed previously, depending on solutions such as Prompt Engineering shifts the accountability of misinformation to humans. Furthermore, when AI is involved in the step of decision-making and manufacturing software products, ethical questions such as who will be held accountable for AI‚Äôs errors come to light (Deshpande and Sharp, 2022). Previous work (Binns et al., 2018; Tahaei et al., 2023) on responsible AI also highlights the need for accountability of AI systems that are grounded in human rights and ethics. Hence, strict policies should be created to maintain the separation of accountability to protect humans from false accusations and preserve the interests of impacted stakeholders by ensuring responsible use. We believe this work will encourage further research for the informed design of responsible conversational chatbots and for careful policy-making to preserve the rights of stakeholders.\n\n9. Limitations\n\nOne limitation of this work is the subjective nature of the manual analysis. We tried to address this limitation by recruiting multiple labelers, constantly measuring the agreement level among labelers, and adopting an iterative analysis procedure with extensive discussions. Moreover, our user study has limitations concerning other factors such as sample size and participants‚Äô own biases. To reduce participants‚Äô biases against human or ChatGPT answers, we anonymized the source of the answers during the study and standardized the visual style and format of the answers, e.g., using the same font size, type, code style, etc.\n\nAdditionally, this work has used the free version of ChatGPT (GPT-3.5) for acquiring the ChatGPT responses for the manual analysis. Hence, one might argue that the results are not generalizable for ChatGPT since the new GPT-4 (released on March 2023) can perform differently. To understand how differently GPT-4 performs compared to GPT-3.5, we conducted a small analysis on 21 randomly selected SO questions where GPT-3.5 gave incorrect answers. Our analysis shows that, among these 21 questions, GPT-4 could answer only 6 questions correctly, and 15 questions were still answered incorrectly. Moreover, the types of errors introduced by GPT-4 follow the same pattern as GPT-3.5. This tells us that, although GPT-4 performs slightly better than GPT -3.5 (e.g., rectified error in 6 answers), the rate of inaccuracy is still high with similar types of errors. Moreover, this new ChatGPT (also known as ChatGPT plus) is a paid version ($20 per month). Since the target population of this research is not only industry developers but also programmers of all levels, including students and freelancers around the world, the free version of ChatGPT has significantly more users than the paid version which only the privileged population can access. Moreover, $20 per month has a considerably high monetary value for many countries. Hence, for this study, we used the free version (GPT-3.5) so that the results benefit the majority of our target populations. We acknowledge that other LLMs can perform differently and we encourage future research to empirically study programming answers generated by other LLMs.\n\nAnother limitation lies in the prompting strategy adopted by our study. In this work, we did not account for the interactive nature of ChatGPT. In practice, if the initial ChatGPT answer is not satisfactory, programmers can refine their initial prompt or ask follow-up questions to get new answers. We did not consider this, since it required designing specific follow-up questions or prompt refinements for each question under analysis. Furthermore, such interaction is not guaranteed to generate better and more correct answers. Thus, it may require multiple rounds of interaction to improve the answer. This would significantly increase the analysis effort and limit our capability to analyze many different kinds of questions in this study. As a result, we restrict the project scope to only analyze the initial answers generated by ChatGPT. To address this limitation, future work could conduct a small-scale but more focused analysis to investigate how interactivity impacts the correctness of ChatGPT answers.\n\nIn this work, we reused the original SO question as the prompt, since the original SO question represents how a programmer may ask the question in a natural conversation. This can be improved with more advanced prompting templates and tricks. However, the design of the prompt is highly dependent on the problem itself and also varies from person to person. Reaching an agreement level for prompt engineering is even more challenging without any established guidelines or studies to follow. To address this limitation, future work could conduct a systematic investigation into how different prompting strategies and tips influence the correctness of ChatGPT answers to different kinds of programming questions.\n\nChatGPT is inherently stochastic. The same prompt may generate different answers with a moderate temperature setting of 0.8. To account for this, one needs to run ChatGPT multiple times with the same prompt for each programming question, manually analyze all answers, and measure the average correctness. If we run ChatGPT 5 times for each question, our analysis effort would be increased by five times and we would not be able to do the study at a satisfiable scale and comprehensiveness level. Thus, we chose to only consider the initial answer generated by ChatGPT.\n\nFinally, we acknowledge that despite our efforts to mitigate the potential issues, some level of human bias and the generalizability limitation still persist. Nonetheless, we hope this study will foster new research in the direction of identification, understanding, rectification, and risk mitigation of errors in LLMs for better human-AI collaboration.\n\n10. Conclusion\n\nIn this paper, we empirically studied the characteristics of ChatGPT answers to SO questions through a combination of manual analysis, linguistic analysis, and user study. Our manual analysis shows that ChatGPT produces incorrect answers more than 50% of the time. Moreover, ChatGPT suffers from other quality issues such as verbosity, inconsistency, etc. Results of the in-depth manual analysis also point towards a large number of conceptual and logical errors in ChatGPT answers. Additionally, our linguistic analysis results show that ChatGPT answers are very formal, and rarely portray negative sentiments. Although our user study shows higher user preference and quality rating for human answers, users make occasional mistakes by preferring incorrect ChatGPT answers based on ChatGPT‚Äôs articulated language styles, as well as seemingly correct logic that is presented with positive assertions. Since ChatGPT produces a large number of incorrect answers, our results emphasize the necessity of caution and awareness regarding the usage of ChatGPT answers in programming tasks. This work also seeks to encourage further research in identifying and mitigating different types of conceptual and factual errors. Finally, we expect this work will foster more research on transparency and communication of incorrectness in machine-generated answers, especially in the context of programming.\n\nReferences\n\n(1)\n\nAbdalkareem et al. (2017) Rabe Abdalkareem, Emad Shihab, and Juergen Rilling. 2017. What do developers use the crowd for? a study using stack overflow. IEEE Software 34, 2 (2017), 53‚Äì60.\n\nAgarwal et al. (2020) Mayank Agarwal, Kartik Talamadupula, Stephanie Houde, Fernando Martinez, Michael Muller, John Richards, Steven Ross, and Justin D Weisz. 2020. Quality estimation & interpretability for code translation. arXiv preprint arXiv:2012.07581 (2020).\n\nAllamanis and Sutton (2013) Miltiadis Allamanis and Charles Sutton. 2013. Why, when, and what: analyzing stack overflow questions by topic, type, and code. In 2013 10th Working conference on mining software repositories (MSR). IEEE, 53‚Äì56.\n\nAsaduzzaman et al. (2013) Muhammad Asaduzzaman, Ahmed Shah Mashiyat, Chanchal K Roy, and Kevin A Schneider. 2013. Answering questions about unanswered questions of stack overflow. In 2013 10th Working Conference on Mining Software Repositories (MSR). IEEE, 97‚Äì100.\n\nAzaria and Mitchell (2023) Amos Azaria and Tom Mitchell. 2023. The internal state of an llm knows when its lying. arXiv preprint arXiv:2304.13734 (2023).\n\nBacchelli et al. (2012) Alberto Bacchelli, Luca Ponzanelli, and Michele Lanza. 2012. Harnessing stack overflow for the ide. In 2012 Third International Workshop on Recommendation Systems for Software Engineering (RSSE). IEEE, 26‚Äì30.\n\nBarke et al. (2023) Shraddha Barke, Michael B James, and Nadia Polikarpova. 2023. Grounded copilot: How programmers interact with code-generating models. Proceedings of the ACM on Programming Languages 7, OOPSLA1 (2023), 85‚Äì111.\n\nBazelli et al. (2013) Blerina Bazelli, Abram Hindle, and Eleni Stroulia. 2013. On the personality traits of stackoverflow users. In 2013 IEEE international conference on software maintenance. IEEE, 460‚Äì463.\n\nBender et al. (2021) Emily M Bender, Timnit Gebru, Angelina McMillan-Major, and Shmargaret Shmitchell. 2021. On the dangers of stochastic parrots: Can language models be too big?. In Proceedings of the 2021 ACM conference on fairness, accountability, and transparency. 610‚Äì623.\n\nBender and Koller (2020) Emily M Bender and Alexander Koller. 2020. Climbing towards NLU: On meaning, form, and understanding in the age of data. In Proceedings of the 58th annual meeting of the association for computational linguistics. 5185‚Äì5198.\n\nBinns et al. (2018) Reuben Binns, Max Van Kleek, Michael Veale, Ulrik Lyngs, Jun Zhao, and Nigel Shadbolt. 2018. ‚ÄôIt‚Äôs Reducing a Human Being to a Percentage‚Äô Perceptions of Justice in Algorithmic Decisions. In Proceedings of the 2018 Chi conference on human factors in computing systems. 1‚Äì14.\n\nBird et al. (2022) Christian Bird, Denae Ford, Thomas Zimmermann, Nicole Forsgren, Eirini Kalliamvakou, Travis Lowdermilk, and Idan Gazit. 2022. Taking Flight with Copilot: Early insights and opportunities of AI-powered pair-programming tools. Queue 20, 6 (2022), 35‚Äì57.\n\nBoghe (2020) Kristof Boghe. 2020. We Need to Talk About Sentiment Analysis. https://medium.com/p/9d1f20f2ebfb.\n\nBommasani et al. (2021) Rishi Bommasani, Drew A Hudson, Ehsan Adeli, Russ Altman, Simran Arora, Sydney von Arx, Michael S Bernstein, Jeannette Bohg, Antoine Bosselut, Emma Brunskill, et al. 2021. On the opportunities and risks of foundation models. arXiv preprint arXiv:2108.07258 (2021).\n\nBorji (2023) Ali Borji. 2023. A categorical archive of chatgpt failures. arXiv preprint arXiv:2302.03494 (2023).\n\nBraun and Clarke (2006) Virginia Braun and Victoria Clarke. 2006. Using thematic analysis in psychology. Qualitative research in psychology 3, 2 (2006), 77‚Äì101.\n\nBuchanan et al. (2021) Ben Buchanan, Andrew Lohn, and Micah Musser. 2021. Truth, lies, and automation: How language models could change disinformation. Center for Security and Emerging Technology.\n\nCalefato et al. (2018b) Fabio Calefato, Filippo Lanubile, Federico Maiorano, and Nicole Novielli. 2018b. Sentiment polarity detection for software development. In Proceedings of the 40th International Conference on Software Engineering. 128‚Äì128.\n\nCalefato et al. (2015) Fabio Calefato, Filippo Lanubile, Maria Concetta Marasciulo, and Nicole Novielli. 2015. Mining successful answers in stack overflow. In 2015 IEEE/ACM 12th Working Conference on Mining Software Repositories. IEEE, 430‚Äì433.\n\nCalefato et al. (2018a) Fabio Calefato, Filippo Lanubile, and Nicole Novielli. 2018a. How to ask for technical help? Evidence-based guidelines for writing questions on Stack Overflow. Information and software technology 94 (2018), 186‚Äì207.\n\nCao et al. (2021) Boxi Cao, Hongyu Lin, Xianpei Han, Le Sun, Lingyong Yan, Meng Liao, Tong Xue, and Jin Xu. 2021. Knowledgeable or educated guess? revisiting language models as knowledge bases. arXiv preprint arXiv:2106.09231 (2021).\n\nCastillo (2022) Alex Castillo. Dec, 2022. [Twitter Post] ChatGPT will replace StackOverflow. https://twitter.com/castillo__io/status/1599255771736604673?s=20.\n\nChen et al. (2020) Xiaojun Chen, Shengbin Jia, and Yang Xiang. 2020. A review: Knowledge reasoning over knowledge graph. Expert Systems with Applications 141 (2020), 112948.\n\nDakhel et al. (2023) Arghavan Moradi Dakhel, Vahid Majdinasab, Amin Nikanjam, Foutse Khomh, Michel C Desmarais, and Zhen Ming Jack Jiang. 2023. Github copilot ai pair programmer: Asset or liability? Journal of Systems and Software 203 (2023), 111734.\n\nDe Souza et al. (2014) Lucas BL De Souza, Eduardo C Campos, and Marcelo de A Maia. 2014. Ranking crowd knowledge to assist software development. In Proceedings of the 22nd International Conference on Program Comprehension. 72‚Äì82.\n\nDeshpande and Sharp (2022) Advait Deshpande and Helen Sharp. 2022. Responsible AI Systems: Who are the Stakeholders?. In Proceedings of the 2022 AAAI/ACM Conference on AI, Ethics, and Society. 227‚Äì236.\n\nDiResta (2020) Renee DiResta. 2020. AI-Generated Text Is the Scariest Deepfake of All. https://www.wired.com/story/ai-generated-text-is-the-scariest-deepfake-of-all/.\n\nDong et al. (2022) Qingxiu Dong, Damai Dai, Yifan Song, Jingjing Xu, Zhifang Sui, and Lei Li. 2022. Calibrating factual knowledge in pretrained language models. arXiv preprint arXiv:2210.03329 (2022).\n\nElazar et al. (2021) Yanai Elazar, Nora Kassner, Shauli Ravfogel, Abhilasha Ravichander, Eduard Hovy, Hinrich Sch√ºtze, and Yoav Goldberg. 2021. Measuring and improving consistency in pretrained language models. Transactions of the Association for Computational Linguistics 9 (2021), 1012‚Äì1031.\n\nExchange (2023) Stack Exchange. 2023. Stack Exchange Data Dump. https://archive.org/details/stackexchange.\n\nFace (2022) Hugging Face. 2022. Hugging Face ‚Äì The AI community building the future. https://huggingface.co/.\n\nFleiss (1971) Joseph L Fleiss. 1971. Measuring nominal scale agreement among many raters. Psychological bulletin 76, 5 (1971), 378.\n\nGamage et al. (2022) Dilrukshi Gamage, Piyush Ghasiya, Vamshi Bonagiri, Mark E Whiting, and Kazutoshi Sasahara. 2022. Are deepfakes concerning? analyzing conversations of deepfakes on reddit and exploring societal implications. In Proceedings of the 2022 CHI Conference on Human Factors in Computing Systems. 1‚Äì19.\n\nGao et al. (2022) Catherine A Gao, Frederick M Howard, Nikolay S Markov, Emma C Dyer, Siddhi Ramesh, Yuan Luo, and Alexander T Pearson. 2022. Comparing scientific abstracts generated by ChatGPT to original abstracts using an artificial intelligence output detector, plagiarism detector, and blinded human reviewers. BioRxiv (2022), 2022‚Äì12.\n\nGehman et al. (2020) Samuel Gehman, Suchin Gururangan, Maarten Sap, Yejin Choi, and Noah A Smith. 2020. Realtoxicityprompts: Evaluating neural toxic degeneration in language models. arXiv preprint arXiv:2009.11462 (2020).\n\nGitHub (2023a) GitHub. 2023a. GitHub Copilot X: The AI-powered developer experience. https://github.blog/2023-03-22-github-copilot-x-the-ai-powered-developer-experience/.\n\nGitHub (2023b) GitHub. 2023b. Introducing GitHub Copilot X. https://github.com/features/preview/copilot-x.\n\nGitHub (2023c) Inc. GitHub. 2023c. GitHub Copilot ¬∑ Your AI pair programmer. https://github.com/features/copilot.\n\nGoodrich et al. (2019) Ben Goodrich, Vinay Rao, Peter J Liu, and Mohammad Saleh. 2019. Assessing the factual accuracy of generated text. In proceedings of the 25th ACM SIGKDD international conference on knowledge discovery & data mining. 166‚Äì175.\n\nGuest et al. (2011) Greg Guest, Kathleen M MacQueen, and Emily E Namey. 2011. Applied thematic analysis. sage publications.\n\nGuo et al. (2023) Biyang Guo, Xin Zhang, Ziyuan Wang, Minqi Jiang, Jinran Nie, Yuxuan Ding, Jianwei Yue, and Yupeng Wu. 2023. How close is chatgpt to human experts? comparison corpus, evaluation, and detection. arXiv preprint arXiv:2301.07597 (2023).\n\nHancock et al. (2001) Beverley Hancock, Elizabeth Ockleford, and Kate Windridge. 2001. An introduction to qualitative research. Trent focus group London.\n\nHuang et al. (2021) Yichong Huang, Xiachong Feng, Xiaocheng Feng, and Bing Qin. 2021. The factual inconsistency problem in abstractive text summarization: A survey. arXiv preprint arXiv:2104.14839 (2021).\n\nImai (2022) Saki Imai. 2022. Is github copilot a substitute for human pair-programming? an empirical study. In Proceedings of the ACM/IEEE 44th International Conference on Software Engineering: Companion Proceedings. 319‚Äì321.\n\nIslam et al. (2020) Md Saiful Islam, Tonmoy Sarkar, Sazzad Hossain Khan, Abu-Hena Mostofa Kamal, SM Murshid Hasan, Alamgir Kabir, Dalia Yeasmin, Mohammad Ariful Islam, Kamal Ibne Amin Chowdhury, Kazi Selim Anwar, et al. 2020. COVID-19‚Äìrelated infodemic and its impact on public health: A global social media analysis. The American journal of tropical medicine and hygiene 103, 4 (2020), 1621.\n\nIyer et al. (2016) Srinivasan Iyer, Ioannis Konstas, Alvin Cheung, and Luke Zettlemoyer. 2016. Summarizing source code using a neural attention model. In 54th Annual Meeting of the Association for Computational Linguistics 2016. Association for Computational Linguistics, 2073‚Äì2083.\n\nJiang and Wilson (2018) Shan Jiang and Christo Wilson. 2018. Linguistic signals under misinformation and fact-checking: Evidence from user comments on social media. Proceedings of the ACM on Human-Computer Interaction 2, CSCW (2018), 1‚Äì23.\n\nJohnson and Onwuegbuzie (2004) R Burke Johnson and Anthony J Onwuegbuzie. 2004. Mixed methods research: A research paradigm whose time has come. Educational researcher 33, 7 (2004), 14‚Äì26.\n\nKnight (2021) Will Knight. 2021. AI Can Write Disinformation Now‚Äîand Dupe Human Readers. https://www.wired.com/story/ai-write-disinformation-dupe-human-readers/.\n\nKoco≈Ñ et al. (2023) Jan Koco≈Ñ, Igor Cichecki, Oliwier Kaszyca, Mateusz Kochanek, Dominika Szyd≈Ço, Joanna Baran, Julita Bielaniewicz, Marcin Gruza, Arkadiusz Janz, Kamil Kanclerz, et al. 2023. ChatGPT: Jack of all trades, master of none. Information Fusion (2023), 101861.\n\nKou et al. (2023) Bonan Kou, Muhao Chen, and Tianyi Zhang. 2023. Automated Summarization of Stack Overflow Posts. In 45th IEEE/ACM International Conference on Software Engineering, ICSE 2023, Melbourne, Australia, May 14-20, 2023. IEEE, 1853‚Äì1865.\n\nKou et al. (2022) Bonan Kou, Yifeng Di, Muhao Chen, and Tianyi Zhang. 2022. SOSum: a dataset of stack overflow post summaries. In Proceedings of the 19th International Conference on Mining Software Repositories. 247‚Äì251.\n\nKramer (2012) Adam DI Kramer. 2012. The spread of emotion via Facebook. In Proceedings of the SIGCHI conference on human factors in computing systems. 767‚Äì770.\n\nKreps et al. (2022) Sarah Kreps, R Miles McCain, and Miles Brundage. 2022. All the news that‚Äôs fit to fabricate: AI-generated text as a tool of media misinformation. Journal of experimental political science 9, 1 (2022), 104‚Äì117.\n\nMamykina et al. (2011) Lena Mamykina, Bella Manoim, Manas Mittal, George Hripcsak, and Bj√∂rn Hartmann. 2011. Design lessons from the fastest q&a site in the west. In Proceedings of the SIGCHI conference on Human factors in computing systems. 2857‚Äì2866.\n\nMaynez et al. (2020) Joshua Maynez, Shashi Narayan, Bernd Bohnet, and Ryan McDonald. 2020. On faithfulness and factuality in abstractive summarization. arXiv preprint arXiv:2005.00661 (2020).\n\nMiller et al. (2022) Courtney Miller, Sophie Cohen, Daniel Klug, Bogdan Vasilescu, and Christian Kastner. 2022. ‚ÄùDid you miss my comment or what?‚Äù understanding toxicity in open source discussions. In Proceedings of the 44th International Conference on Software Engineering. 710‚Äì722.\n\nMitroviƒá et al. (2023) Sandra Mitroviƒá, Davide Andreoletti, and Omran Ayoub. 2023. Chatgpt or human? detect and explain. explaining decisions of machine learning model for detecting short chatgpt-generated text. arXiv preprint arXiv:2301.13852 (2023).\n\nNasehi et al. (2012) Seyed Mehdi Nasehi, Jonathan Sillito, Frank Maurer, and Chris Burns. 2012. What makes a good code example?: A study of programming Q&A in StackOverflow. In 2012 28th IEEE International Conference on Software Maintenance (ICSM). IEEE, 25‚Äì34.\n\nNorman (2010) Geoff Norman. 2010. Likert scales, levels of measurement and the ‚Äúlaws‚Äù of statistics. Advances in health sciences education 15 (2010), 625‚Äì632.\n\nOpenAI (2023) OpenAI. 2023. ChatGPT: Optimizing Language Models for Dialogue. https://openai.com/blog/chatgpt.\n\nOverflow (2023a) Stack Overflow. 2023a. File Upload Area. https://stackoverflow.com/questions/76003368/file-upload-area.\n\nOverflow (2023b) Stack Overflow. 2023b. Insights into Stack Overflow‚Äôs traffic. https://stackoverflow.blog/2023/08/08/insights-into-stack-overflows-traffic/.\n\nOverflow (2023c) Stack Overflow. 2023c. Temporary policy: Generative AI (e.g., ChatGPT) is banned. https://meta.stackoverflow.com/questions/421831/temporary-policy-generative-ai-e-g-chatgpt-is-banned.\n\nPeng et al. (2023) Baolin Peng, Michel Galley, Pengcheng He, Hao Cheng, Yujia Xie, Yu Hu, Qiuyuan Huang, Lars Liden, Zhou Yu, Weizhu Chen, et al. 2023. Check your facts and try again: Improving large language models with external knowledge and automated feedback. arXiv preprint arXiv:2302.12813 (2023).\n\nPennebaker et al. (2015) James W Pennebaker, Ryan L Boyd, Kayla Jordan, and Kate Blackburn. 2015. The development and psychometric properties of LIWC2015. Technical Report.\n\nQiu et al. (2019) Huilian Sophie Qiu, Yucen Lily Li, Susmita Padala, Anita Sarma, and Bogdan Vasilescu. 2019. The signals that potential contributors look for when choosing open-source projects. Proceedings of the ACM on Human-Computer Interaction 3, CSCW (2019), 1‚Äì29.\n\nQuora (2023) Quora. 2023. Will ChatGPT replace Stack Overflow? https://www.quora.com/Will-ChatGPT-replace-Stack-Overflow.\n\nRahman et al. (2018) Md Masudur Rahman, Jed Barson, Sydney Paul, Joshua Kayani, Federico Andr√©s Lois, Sebasti√°n Fernandez Quezada, Christopher Parnin, Kathryn T Stolee, and Baishakhi Ray. 2018. Evaluating how developers use general-purpose web-search for code retrieval. In Proceedings of the 15th International Conference on Mining Software Repositories. 465‚Äì475.\n\nRao et al. (2020) Nikitha Rao, Chetan Bansal, Thomas Zimmermann, Ahmed Hassan Awadallah, and Nachiappan Nagappan. 2020. Analyzing web search behavior for software engineering tasks. In 2020 IEEE International Conference on Big Data (Big Data). IEEE, 768‚Äì777.\n\nRashkin et al. (2017) Hannah Rashkin, Eunsol Choi, Jin Yea Jang, Svitlana Volkova, and Yejin Choi. 2017. Truth of varying shades: Analyzing language in fake news and political fact-checking. In Proceedings of the 2017 conference on empirical methods in natural language processing. 2931‚Äì2937.\n\nRodrigues et al. (2014) Filipe Rodrigues, Francisco Pereira, and Bernardete Ribeiro. 2014. Sequence labeling with multiple annotators. Machine learning 95 (2014), 165‚Äì181.\n\nRoss et al. (2023) Steven I Ross, Fernando Martinez, Stephanie Houde, Michael Muller, and Justin D Weisz. 2023. The programmer‚Äôs assistant: Conversational interaction with a large language model for software development. In Proceedings of the 28th International Conference on Intelligent User Interfaces. 491‚Äì514.\n\nShneiderman (1980) Ben Shneiderman. 1980. Software psychology: Human factors in computer and information systems (Winthrop computer systems series). Winthrop Publishers.\n\nSkripchuk et al. (2023) James Skripchuk, Neil Bennett, Jeffrey Zhang, Eric Li, and Thomas Price. 2023. Analysis of Novices‚Äô Web-Based Help-Seeking Behavior While Programming. In Proceedings of the 54th ACM Technical Symposium on Computer Science Education V. 1. 945‚Äì951.\n\nStorey et al. (2010) Margaret-Anne Storey, Christoph Treude, Arie Van Deursen, and Li-Te Cheng. 2010. The impact of social media on software engineering practices and tools. In Proceedings of the FSE/SDP workshop on Future of software engineering research. 359‚Äì364.\n\nStrobelt et al. (2022) Hendrik Strobelt, Albert Webson, Victor Sanh, Benjamin Hoover, Johanna Beyer, Hanspeter Pfister, and Alexander M Rush. 2022. Interactive and visual prompt engineering for ad-hoc task adaptation with large language models. IEEE transactions on visualization and computer graphics 29, 1 (2022), 1146‚Äì1156.\n\nTahaei et al. (2023) Mohammad Tahaei, Marios Constantinides, Daniele Quercia, Sean Kennedy, Michael Muller, Simone Stumpf, Q Vera Liao, Ricardo Baeza-Yates, Lora Aroyo, Jess Holbrook, et al. 2023. Human-Centered Responsible Artificial Intelligence: Current & Future Trends. In Extended Abstracts of the 2023 CHI Conference on Human Factors in Computing Systems. 1‚Äì4.\n\nToews (2020) Rob Toews. 2020. Deepfakes Are Going To Wreak Havoc On Society. We Are Not Prepared. https://www.forbes.com/sites/robtoews/2020/05/25/deepfakes-are-going-to-wreak-havoc-on-society-we-are-not-prepared/?sh=392a1cb07494.\n\nTozzi (2023) Christopher Tozzi. 2023. GitHub Copilot vs. ChatGPT: Which Tool Is Better for Software Development? https://www.itprotoday.com/development-techniques-and-management/github-copilot-vs-chatgpt-which-tool-better-software.\n\nTreude et al. (2011) Christoph Treude, Ohad Barzilay, and Margaret-Anne Storey. 2011. How do programmers ask and answer questions on the web?(nier track). In Proceedings of the 33rd international conference on software engineering. 804‚Äì807.\n\nVaithilingam et al. (2022) Priyan Vaithilingam, Tianyi Zhang, and Elena L Glassman. 2022. Expectation vs. experience: Evaluating the usability of code generation tools powered by large language models. In Chi conference on human factors in computing systems extended abstracts. 1‚Äì7.\n\nVasconcelos et al. (2023) Helena Vasconcelos, Gagan Bansal, Adam Fourney, Q Vera Liao, and Jennifer Wortman Vaughan. 2023. Generation probabilities are not enough: Explor"
    }
}
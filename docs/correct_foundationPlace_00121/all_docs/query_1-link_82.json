{
    "id": "correct_foundationPlace_00121_1",
    "rank": 82,
    "data": {
        "url": "https://dl.acm.org/doi/10.1023/A:1008641220268",
        "read_more_link": "",
        "language": "en",
        "title": "Feature Subset Selection within a Simulated Annealing DataMining Algorithm",
        "top_image": "https://dl.acm.org/pb-assets/head-metadata/favicon-32x32-1574252172003.png",
        "meta_img": "https://dl.acm.org/pb-assets/head-metadata/favicon-32x32-1574252172003.png",
        "images": [
            "https://dl.acm.org/specs/products/acm/releasedAssets/images/acm-dl-logo-white-1ecfb82271e5612e8ca12aa1b1737479.png",
            "https://dl.acm.org/doi/10.1023/specs/products/acm/releasedAssets/images/acm-logo-1-ad466e729c8e2a97780337b76715e5cf.png",
            "https://dl.acm.org/pb-assets/icons/DOs/default-profile-1543932446943.svg",
            "https://dl.acm.org/pb-assets/icons/DOs/default-profile-1543932446943.svg",
            "https://dl.acm.org/specs/products/acm/releasedAssets/images/footer-logo1-45ae33115db81394d8bd25be65853b77.png",
            "https://dl.acm.org/specs/products/acm/releasedAssets/images/Default_image_lazy-0687af31f0f1c8d4b7a22b686995ab9b.svg",
            "https://dl.acm.org/pb-assets/icons/DOs/default-profile-1543932446943.svg",
            "https://dl.acm.org/pb-assets/icons/DOs/default-profile-1543932446943.svg",
            "https://dl.acm.org/pb-assets/icons/DOs/default-profile-1543932446943.svg",
            "https://dl.acm.org/action/showDoPubAsset?doi=10.1145/contrib-81351593425&format=rel-imgonly&assetId=jiawei_han.jpg",
            "https://dl.acm.org/pb-assets/icons/DOs/default-profile-1543932446943.svg",
            "https://dl.acm.org/action/showDoPubAsset?doi=10.1145/contrib-81452610814&format=rel-imgonly&assetId=laks.jpg",
            "https://dl.acm.org/specs/products/acm/releasedAssets/images/loader-7e60691fbe777356dc81ff6d223a82a6.gif",
            "https://dl.acm.org/pb-assets/icons/DOs/default-profile-1543932446943.svg",
            "https://dl.acm.org/pb-assets/icons/DOs/default-profile-1543932446943.svg",
            "https://dl.acm.org/specs/products/acm/releasedAssets/images/acm-logo-dl-8437178134fce530bc785276fc316cbf.png",
            "https://dl.acm.org/specs/products/acm/releasedAssets/images/acm-logo-3-10aed79f3a6c95ddb67053b599f029af.png"
        ],
        "movies": [],
        "keywords": [],
        "meta_keywords": [
            "Feature subset selection",
            "data mining",
            "simulated annealing"
        ],
        "tags": null,
        "authors": [
            "School of Information Systems",
            "University of East Anglia",
            "[email protected] View Profile",
            "Justin C. W. Debuse",
            "Victor J. Rayward-Smith",
            "DebuseJustin C. W",
            "Rayward-SmithVictor J"
        ],
        "publish_date": null,
        "summary": "",
        "meta_description": "An overview of the principle feature subset selection methods is\ngiven. We investigate a number of measures of feature subset quality, using\nlarge commercial databases. We develop an entropic measure, based upon the\ninformation gain approach used within ...",
        "meta_lang": "en",
        "meta_favicon": "/pb-assets/head-metadata/apple-touch-icon-1574252172393.png",
        "meta_site_name": "Journal of Intelligent Information Systems",
        "canonical_link": "https://dl.acm.org/doi/10.1023/A%3A1008641220268",
        "text": "Abstract\n\nAn overview of the principle feature subset selection methods is given. We investigate a number of measures of feature subset quality, using large commercial databases. We develop an entropic measure, based upon the information gain approach used within ID3 and C4.5 to build trees, which is shown to give the best performance over our databases. This measure is used within a simple feature subset selection algorithm and the technique is used to generate subsets of high quality features from the databases. A simulated annealing based data mining technique is presented and applied to the databases. The performance using all features is compared to that achieved using the subset selected by our algorithm. We show that a substantial reduction in the number of features may be achieved together with an improvement in the performance of our data mining system. We also present a modification of the data mining algorithm, which allows it to simultaneously search for promising feature subsets and high quality rules. The effect of varying the generality level of the desired pattern is also investigated.\n\nReferences\n\n[1]\n\nANGOSS: 1994, KnowledgeSEEKER 3.11.05 on-line help.\n\n[2]\n\nANGOSS: 1995, KnowledgeSEEKER for Windows Version 3.0 User's Guide, Toronto, Canada.\n\n[3]\n\nBiggs, D., de Ville, B. and Suen, E.: 1991, A method of choosing multiway partitions for classification and decision trees, Journal of Applied Statistics 18(1), 49-62.\n\n[4]\n\nBreiman, L., Friedman, J. H., Olshen, R. A. and Stone, C. J.: 1984, Classification and Regression Trees, Wadsworth and Brooks, Monterey, CA.\n\n[5]\n\nChang, C. Y.: 1973, Dynamic programming as applied to feature selection in pattern recognition systems, IEEE Trans. Syst. Man and Cybernet. 3, 166-171.\n\n[6]\n\nChardaire, P., Lutton, J. L. and Sutter, A.: 1995, Thermostatistical persistency: A powerful improving concept for simulated annealing algorithms, European Journal of Operational Research 86.\n\n[7]\n\nCover, T. M. and Van Campenhout, J. M.: 1977, On the possible orderings in the measurement selection problem, IEEE Trans. Sys. Man Cybern. 7, 651-661.\n\n[8]\n\nde la Iglesia, B., Debuse, J. C. W. and Rayward-Smith, V. J.: 1996, Discovering knowledge in commercial databases using modern heuristic techniques, in E. Simoudis, J. W. Han and U. Fayyad (eds), Proc. of the Second Int. Conf. on Knowledge Discovery and Data Mining (KDD-96), pp. 44-49.\n\n[9]\n\nde Ville, B.: 1990, Applying statistical knowledge to database analysis and knowledge base construction, Proceedings of the Sixth IEEE Conference on Artificial Intelligence Applications, IEEE Computer Society, Washington.\n\n[10]\n\nDevijver, P. A. and Kittler, J.: 1982, Pattern Recognition: a Statistical Approach, Prentice-Hall International, London.\n\n[11]\n\nForoutan, I. and Sklansky, J.: 1987, Feature selection for automatic classification of non-gaussian data, IEEE Trans. Sys. Man Cybern. 17, 187-198.\n\n[12]\n\nJohn, G. H., Kohavi, R. and Pfleger, K.: 1994, Irrelevant features and the subset selection problem, in W. W. Cohen and H. Hirsh (eds), Machine Learning: Proceedings of the Eleventh International Conference, Morgan Kaufmann, San Francisco, pp. 121-129.\n\n[13]\n\nKass, G. V.: 1980, An exploratory technique for investigating large quantities of categorical data, Appl. Statist. 29, 119-127.\n\n[14]\n\nKohavi, R. and Sommerfield, D.: 1995, Feature subset selection using the wrapper method: overfitting and dynamic search space topology, in U. M. Fayyad and R. Uthurusamy (eds), Proc. of the First Int. Conf. on Knowledge Discovery and Data Mining, AAAI Press, pp. 192-197.\n\n[15]\n\nKoller, D. and Sahami, M.: 1996, Toward optimal feature selection, in (Saitta, 1996).\n\n[16]\n\nLiu, H. and Setiono, R.: 1996a, Feature selection and classification - a probabilistic wrapper approach, Proc. of the 9th Int. Conf. on Industrial and Engineering Applications of AI and Expert Systems pp. 419-424.\n\n[17]\n\nLiu, H. and Setiono, R.: 1996b, A probabilistic approach to feature selection - a filter solution, in (Saitta, 1996).\n\n[18]\n\nLundy, M. and Mees, A.: 1986, Convergence of an annealing algorithm, Mathematical programming 34, 111-124.\n\n[19]\n\nMann, J. W.: 1995, X-SAmson v1.0 user manual, School of Information Systems, University of East Anglia.\n\n[20]\n\nMarill, T. and Green, D. M.: 1963, On the effectiveness of receptors in recognition systems, IEEE Trans. Inform. Theory 9, 11-17.\n\n[21]\n\nMichael, M. and Lin, W. C.: 1973, Experimental study of information measures and inter-intra class distance ratios on feature selection and ordering, IEEE Trans. Systems Man Cybernet. 3, 172-181.\n\n[22]\n\nNarendra, P. M. and Fukunaga, K.: 1977, A branch and bound algorithm for feature subset selection, IEEE Transactions on Computers pp. 917-922.\n\n[23]\n\nPei, M., Goodman, E. D., Punch, W. F. and Ding, Y.: 1995, Genetic algorithms for classification and feature extraction, Proc. of the Classification Society Conf..\n\n[24]\n\nQuinlan, J. R.: 1983, Learning efficient classification procedures and their application to chess end games, in R. S. Michalski, J. G. Carbonell and T. M. Mitchell (eds), Machine learning: An artificial intelligence approach, Morgan Kaufmann, San Mateo, CA.\n\n[25]\n\nQuinlan, J. R.: 1986, Induction of decision trees, Machine Learning 1.\n\n[26]\n\nQuinlan, J. R.: 1993, C4.5: Programs for Machine Learning, Morgan Kaufmann.\n\n[27]\n\nRauber, T. W.: 1996, The tooldiag package. Available electronically from: http://www.uninova.pt/~tr/home/tooldiag.html.\n\n[28]\n\nRayward-Smith, V. J., Debuse, J. C. W. and de la Iglesia, B.: 1995, Using a genetic algorithm to data mine in the financial services sector, in A. Macintosh and C. Cooper (eds), Applications and Innovations in Expert Systems III, SGES Publications, pp. 237-252.\n\n[29]\n\nSaitta, L. (ed.): 1996, Proc. of the 13th Int. Conf. on Machine Learning.\n\n[30]\n\nShannon, C. E. and Weaver, W.: 1963, The Mathematical Theory of Communication, University of Illinois Press, Urbana.\n\n[31]\n\nThrun, S., Bala, J., Bloedorn, E., Bratko, I., Cestnik, B., Cheng, J., De Jong, K., Dzeroski, S., Fahlman, S. E., Fisher, D., Hamman, R., Kaufman, K., Keller, I. Kononenko, I., Kreuziger, J., Michalski, R. S., Mitchell, T., Pachowicz, P., Reich, Y., Vafaie, H., Van de Welde, W., Wenzel, W., Wnek, J. and Zhang, J.: 1991, The MONK's problems - a performance comparison of different learning algorithms, Carnegie Mellon University CMU-CS-91-197.\n\n[32]\n\nWeiss, S. M. and Kulikowski, C. A.: 1991, Computer systems that learn : classification and prediction methods from statistics, neural nets, machine learning and expert systems, Morgan Kaufmann, San Francisco.\n\n[33]\n\nWhitney, A.: 1971, A direct method of nonparametric measurement selection, IEEE Trans. Comput. 20, 1100- 1103.\n\nCited By\n\nView all\n\nAlzaqebah MAhmed EAccelerated fuzzy min–max neural network and arithmetic optimization algorithm for optimizing hyper-boxes and feature selectionNeural Computing and Applications10.1007/s00521-023-09131-636:4(1553-1568)\n\nZhong CLi GMeng ZLi HHe WA self-adaptive quantum equilibrium optimizer with artificial bee colony for feature selectionComputers in Biology and Medicine10.1016/j.compbiomed.2022.106520153:C\n\nGao WMa HZhao YWang JTian QModeling Portraits of Students and Exercises for Exercise RecommendationAdvanced Intelligent Computing Technology and Applications10.1007/978-981-99-4752-2_19(226-236)\n\nShow More Cited By\n\nIndex Terms\n\nFeature Subset Selection within a Simulated Annealing DataMining Algorithm\n\nComputing methodologies\n\nArtificial intelligence\n\nComputer vision\n\nComputer vision problems\n\nImage segmentation\n\nVideo segmentation\n\nKnowledge representation and reasoning\n\nMachine learning\n\nTheory of computation\n\nLogic\n\nRecommendations\n\nFeature salience definition and estimation and its use in feature subset selection\n\nIn this paper we describe novel feature subset selection methods, based on the estimation of feature salience i.e. the quantification of the relative importance of individual features, in the presence of other features, for determining the classes of ...\n\nSelecting feature subset via constraint association rules\n\nPAKDD'12: Proceedings of the 16th Pacific-Asia conference on Advances in Knowledge Discovery and Data Mining - Volume Part II\n\nIn this paper, a novel feature selection algorithm FEAST is proposed based on association rule mining. The proposed algorithm first mines association rules from a data set; then, it identifies the relevant and interactive feature values with the ...\n\nA novel feature subset selection algorithm based on association rule mining\n\nIn this paper, a novel feature selection algorithm FEAST is proposed based on association rule mining. The proposed algorithm first mines association rules from a data set; then, it identifies the relevant and interactive feature values with the ...\n\nInformation & Contributors\n\nInformation\n\nPublished In\n\n92 pages\n\nISSN:0925-9902\n\nEditors:\n\nLarry Kerschberg\n\nGeorge Mason Univ., Fairfax, VA\n\n,\n\nZbigniew W. Ras\n\nUniv. of North Carolina/Charlotte\n\n,\n\nMaria Zemankova\n\nArlington, VA\n\n,\n\nJiawei Han\n\nSimon Fraser Univ., Burnaby, B.C. Canada\n\n,\n\nRaymond T. Ng\n\nSimon Fraser Univ., Burnaby, B.C. Canada\n\n,\n\nLaks V. S. Lakshmanan\n\nConcordia Univ., Montreal, P.Q., Canada\n\nIssue’s Table of Contents\n\nCopyright © Copyright © 1997 Kluwer Academic Publishers.\n\nPublisher\n\nKluwer Academic Publishers\n\nUnited States\n\nPublication History\n\nPublished: 01 July 1997\n\nAuthor Tags\n\nFeature subset selection\n\ndata mining\n\nsimulated annealing\n\nQualifiers\n\nArticle\n\nContributors\n\nOther Metrics\n\nBibliometrics & Citations\n\nBibliometrics\n\nArticle Metrics\n\n16\n\nTotal Citations\n\nView Citations\n\n0\n\nTotal Downloads\n\nDownloads (Last 12 months)0\n\nDownloads (Last 6 weeks)0\n\nOther Metrics\n\nCitations\n\nCited By\n\nView all\n\nAlzaqebah MAhmed EAccelerated fuzzy min–max neural network and arithmetic optimization algorithm for optimizing hyper-boxes and feature selectionNeural Computing and Applications10.1007/s00521-023-09131-636:4(1553-1568)\n\nZhong CLi GMeng ZLi HHe WA self-adaptive quantum equilibrium optimizer with artificial bee colony for feature selectionComputers in Biology and Medicine10.1016/j.compbiomed.2022.106520153:C\n\nGao WMa HZhao YWang JTian QModeling Portraits of Students and Exercises for Exercise RecommendationAdvanced Intelligent Computing Technology and Applications10.1007/978-981-99-4752-2_19(226-236)\n\nNa GChang HUnsupervised Subspace Extraction via Deep Kernelized ClusteringACM Transactions on Knowledge Discovery from Data10.1145/345908216:1(1-15)\n\nPourpanah FLim CWang XTan CSeera MShi YA hybrid model of fuzzy min–max and brain storm optimization for feature selection and data classificationNeurocomputing10.1016/j.neucom.2019.01.011333:C(440-451)\n\nTsai CChang WWang YChen HA high-performance parallel coral reef optimization for data clusteringSoft Computing - A Fusion of Foundations, Methodologies and Applications10.1007/s00500-019-03950-323:19(9327-9340)\n\nAksakalli VMalekipirbazari MFeature selection via binary simultaneous perturbation stochastic approximationPattern Recognition Letters10.1016/j.patrec.2016.03.00275:C(41-47)\n\nZarshenas ASuzuki KBinary coordinate ascentKnowledge-Based Systems10.1016/j.knosys.2016.07.026110:C(191-201)\n\nMoradkhani MAmiri AJavaherian MSafari HA hybrid algorithm for feature subset selection in high-dimensional datasets using FICA and IWSSr algorithmApplied Soft Computing10.1016/j.asoc.2015.03.04935:C(123-135)\n\nDiao RShen QNature inspired feature selection meta-heuristicsArtificial Intelligence Review10.1007/s10462-015-9428-844:3(311-340)\n\nShow More Cited By\n\nView Options\n\nView options\n\nMedia\n\nFigures\n\nOther\n\nTables\n\nShare\n\nShare\n\nShare this Publication link\n\nCopied!\n\nCopying failed.\n\nShare on social media\n\nAffiliations\n\nJustin C. W. Debuse\n\nComputing Department, School of Information Systems, University of East Anglia, Norwich NR4 7TJ, UK. E-mail: [email protected], [email protected]\n\nVictor J. Rayward-Smith\n\nComputing Department, School of Information Systems, University of East Anglia, Norwich NR4 7TJ, UK. E-mail: [email protected], [email protected]\n\nRequest permissions Authors Info & Affiliations"
    }
}
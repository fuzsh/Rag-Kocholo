{
    "id": "correct_foundationPlace_00121_0",
    "rank": 95,
    "data": {
        "url": "https://emadyowakim.wordpress.com/",
        "read_more_link": "",
        "language": "en",
        "title": "Data Analytics",
        "top_image": "https://secure.gravatar.com/blavatar/0b7ff03ec33ffb1dffaeb0967e8e82fb4356e01394c71dd43b220dc4cfa6c65d?s=200&ts=1721641608",
        "meta_img": "https://secure.gravatar.com/blavatar/0b7ff03ec33ffb1dffaeb0967e8e82fb4356e01394c71dd43b220dc4cfa6c65d?s=200&ts=1721641608",
        "images": [
            "https://emadyowakim.wordpress.com/wp-content/uploads/2020/11/cropped-analytics7.jpg",
            "https://www.mckinsey.com/~/media/McKinsey/Business%20Functions/McKinsey%20Digital/Our%20Insights/Designing%20data%20governance%20that%20delivers%20value/SVG-DataGovernance1.svgz",
            "https://www.mckinsey.com/~/media/McKinsey/Business%20Functions/McKinsey%20Digital/Our%20Insights/How%20to%20build%20a%20data%20architecture%20to%20drive%20innovation%20today%20and%20tomorrow/SVG-Data-architecture-exhibit.svgz",
            "https://emadyowakim.wordpress.com/wp-content/uploads/2020/02/what-is-dataops.png",
            "https://emadyowakim.wordpress.com/wp-content/uploads/2019/11/enterprise-ai-platform-solutions-xenonstack.png",
            "https://emadyowakim.wordpress.com/wp-content/uploads/2019/01/carmonetizing.jpg",
            "https://www.gartner.com/resources/326400/326456/326456_0001.png?reprintKey=1-4RQ3VEZ",
            "https://emadyowakim.wordpress.com/wp-content/uploads/2017/06/gartner-2017-mq-data-science-platforms.png",
            "https://1.gravatar.com/avatar/1fa9600680cf07ecd8e05c2fe4aa3fe6f6eac96294e2d80ef23e9f7b9cbeec3a?s=48&d=identicon&r=G",
            "https://secure.gravatar.com/blavatar/0b7ff03ec33ffb1dffaeb0967e8e82fb4356e01394c71dd43b220dc4cfa6c65d?s=50&d=https%3A%2F%2Fs2.wp.com%2Fi%2Flogo%2Fwpcom-gray-white.png",
            "https://secure.gravatar.com/blavatar/0b7ff03ec33ffb1dffaeb0967e8e82fb4356e01394c71dd43b220dc4cfa6c65d?s=50&d=https%3A%2F%2Fs2.wp.com%2Fi%2Flogo%2Fwpcom-gray-white.png",
            "https://pixel.wp.com/b.gif?v=noscript"
        ],
        "movies": [
            "https://www.youtube.com/embed/W5JOo129vYE?version=3&rel=1&showsearch=0&showinfo=1&iv_load_policy=1&fs=1&hl=en&autohide=2&wmode=transparent"
        ],
        "keywords": [],
        "meta_keywords": [
            ""
        ],
        "tags": null,
        "authors": [],
        "publish_date": "2020-10-08T11:49:12+04:00",
        "summary": "",
        "meta_description": "Emad Yowakim",
        "meta_lang": "en",
        "meta_favicon": "https://secure.gravatar.com/blavatar/0b7ff03ec33ffb1dffaeb0967e8e82fb4356e01394c71dd43b220dc4cfa6c65d?s=32",
        "meta_site_name": "Data Analytics",
        "canonical_link": "https://emadyowakim.wordpress.com/",
        "text": "Data governance that delivers value\n\nxecutives in every industry know that data is important. Without it, there can be no digital transformation to propel the organization past competitors. There are no analytics driving new sources of revenue. Even running the basic business well isn’t possible. But for data to fuel these initiatives, it must be readily available, of high quality, and relevant. Good data governance ensures data has these attributes, which enable it to create value.\n\nThe problem is that most governance programs today are ineffective. The issue frequently starts at the top, with a C-suite that doesn’t recognize the value-creation potential in data governance. As a result, it becomes a set of policies and guidance relegated to a support function executed by IT and not widely followed—rendering the initiatives that data powers equally ineffective. In other cases, organizations try to use technology to solve the problem. While technology solutions such as data lakes and data-governance platforms can help, they aren’t a panacea.\n\nWithout quality-assuring governance, companies not only miss out on data-driven opportunities; they waste resources. Data processing and cleanup can consume more than half of an analytics team’s time, including that of highly paid data scientists, which limits scalability and frustrates employees. Indeed, the productivity of employees across the organization can suffer: respondents to our 2019 Global Data Transformation Survey reported that an average of 30 percent of their total enterprise time was spent on non-value-added tasks because of poor data quality and availability\n\nWhile it’s challenging to directly attribute value to data governance, there are multiple examples of its significant indirect value. Leading firms have eliminated millions of dollars in cost from their data ecosystems and enabled digital and analytics use cases worth millions or even billions of dollars. Data governance is one of the top three differences between firms that capture this value and firms that don’t. In addition, firms that have under-invested in governance have exposed their organizations to real regulatory risk, which can be costly.\n\nSix shifts to create a game-changing data architecture\n\nThere are six foundational shifts companies are making to their data-architecture blueprints that enable more rapid delivery of new capabilities and vastly simplify existing architectural approaches (exhibit). They touch nearly all data activities, including acquisition, processing, storage, analysis, and exposure. Even though organizations can implement some shifts while leaving their core technology stack intact, many require careful re-architecting of the existing data platform and infrastructure, including both legacy technologies and newer technologies previously bolted on.\n\nSuch efforts are not insignificant. Investments can often range in the tens of millions of dollars to build capabilities for basic use cases, such as automated reporting, to hundreds of millions of dollars for putting in place the architectural components for bleeding-edge capabilities, such as real-time services in order to compete with the most innovative disruptors. Therefore, it is critical for organizations to have a clear strategic plan, and data and technology leaders will need to make bold choices to prioritize those shifts that will most directly impact business goals and to invest in the right level of architecture sophistication. As a result, data-architecture blueprints often look very different from one company to another.\n\nWhen done right, the return on investment can be significant (more than $500 million annually in the case of one US bank, and 12 to 15 percent profit-margin growth in the case of one oil and gas company). We find these types of benefits can come from any number of areas: IT cost savings, productivity improvements, reduced regulatory and operational risk, and the delivery of wholly new capabilities, services, and even entire businesses.\n\nDataOps Entry key to 4th Revolution\n\nWhat Is DataOps?\n\nDataOps is a discipline that has become a necessity in a market where the demand for access to data assets and data products is skyrocketing. The inability of data platform teams and data management platforms to keep pace with the demands placed on them by DevOps-enabled teams led to the development of DataOps.[i]\n\nIn a nutshell, DataOps brings together data scientists, analysts, developers, and operations to work on the entire product/service lifecycle, from the design stage to production support.[ii]\n\nDataOps vs. DevOps\n\nHowever, DataOps isn’t just about taking DevOps principles and applying them to data analytics. It does achieve similar things in that it can significantly improve quality and cycle time, but it isn’t the same thing.[iii]\n\nDevOps relies on automation to accelerate build lifecycles. The goal is to achieve continuous and consistent software integration and delivery by capitalizing on on-demand IT resources and through the automation of code integration, testing, and deployment.\n\nIn other words, DevOps brings together development and operations teams and provides them with the tools they need to do a better and more efficient job. The result is a reduced deployment timeframe, faster delivery to market, a reduction in problems, as well as a shorter timeframe required to fix problems.\n\nDevOps has enabled top companies to reduce their release timeframes from months to minutes, or even seconds in some cases. This has offered them an incredible competitive edge, which is highly necessary in today’s fast-paced economy.\n\nEssentially, the reason companies like Amazon and Google are able to release software multiple times per day is because of DevOps. Otherwise, even the attempt would end in disaster.\n\nThe goal of DataOps, on the other hand, is to make data analytics more efficient. To do so, DataOps adopts Agile Development principles, thereby improving the efficiency and effectiveness of the data teams and users.\n\nThis means that data teams can publish new analytics in shorter increments, referred to as sprints, which significantly reduces wait times. Studies also show that this Agile Development approach leads to software development projects being completed with fewer problems. In the data space, it means companies can respond to customer needs and pain points faster, thereby significantly increasing the speed of value delivery.\n\nHowever, compared to DevOps, DataOps has an additional component that is continuously in flux. This is the data pipeline, where raw data enters on one side, is then processed, and exits in a different form (reports, views, models, etc.) on the other side. This data pipeline is often referred as the data Producer/Consumer model.\n\nDataOps has a vital role when it comes to this data flow because it directs, monitors, and manages the data pipeline. Statistical process control (SPC) is one of the more powerful tools used to achieve this.\n\nSPC ensures that statistics stay within their acceptable range, thereby resulting in significant increases in quality, efficiency, and transparency in data analytics.\n\nThus, DataOps combines principles from DevOps, Agile Development, and statistical process control.\n\nWhat Does DataOps Do?\n\nData is valuable. It’s more valuable than ever and many organizations are recognizing that it can generate much greater value than they previously thought. It can become a product in itself. However, the data is only as good as an organization’s capacity to efficiently collect, process, and transform it into actionable insights.\n\nThe problem is that many organizations aren’t exactly clear on the most efficient approach to data collection and analytics. They often take a seemingly all-encompassing approach based on the principle of “we’ll collect the data and then figure out what to do with it,” that can do more harm than good.\n\nThey then have a data team who is supposed to miraculously turn garbage into gold, which generally requires far more effort than necessary and rarely leads to the desired results. Of course, this makes it virtually impossible to deliver actionable insights on a schedule that can keep up with the demands of a DevOps team that’s pushing to get their code to market.\n\nDataOps takes this jumbled mess and turns it into a smooth process where data teams aren’t spending their time trying to fix problems. They aren’t wasting their time trying to turn poor raw data into something usable. Instead, they can focus on what matters, namely providing actionable insights.\n\nDataOps ensures that the raw data coming in is useable, it ensures that the results are accurate, it focuses on the value of people and working together, and it keeps the data team at the center of the company’s strategic objectives.[iv] After all, they no longer take months to come up with required insights but are just as efficient and effective as DevOps-enabled teams.\n\nThe Evolution of DataOps\n\nLenny Liebmann, Contributing Editor at InformationWeek, was the first to introduce DataOps in “3 Reasons Why DataOps Is Essential for Big Data Success” in June of 2014. Andy Palmer subsequently popularized the terms at Tamr.[v]\n\nDataOps saw significant evolution in 2017[vi]. Thus, the growth of enterprise-level interest in this discipline led to the development of a powerful network of vendors developing and marketing a wide range of related products and services.\n\nAny DataOps platform relies on four essential software components, namely:\n\n– Data pipeline orchestration: DataOps requires a guided workflow based on graphs that encompasses all the steps related to integration, data access, visualization, and modeling;\n\n– Testing and production quality: DataOps not only tests and monitors the quality of production of all data but also tests any changes in code during the deployment phase;\n\n– Automated deployment: DataOps constantly takes code and configurations from development environments and moves it into production;\n\n– Data science model deployment and sandbox management: DataOps is also responsible for the creation of development environments that can be reproduced and movement of models into production;\n\n– Other functions requiring support: code and artifact storage, parametrization and secure key storage, distributed computing, data virtualization, versioning, and test data management.\n\nA large number of products and services came onto the market in 2017 to satisfy the aforementioned need. The number continued to expand significantly in 2018.\n\nDespite gaining popularity, DataOps is still a new concept and widespread adoption has yet to be achieved. The latter is likely hindered in part by the limited frameworks and solutions available, but also by the lack of clear guidelines that should be followed.\n\nEven so, it is the beginning of another evolution in the market as various companies attempt to interpret the concept loosely. Data scientists and IT professionals can still find it challenging to determine where they should begin or how they should define success metrics.\n\nThe Role of Security in DataOps\n\nA report from 451 Research shows that global enterprises are turning to DataOps because they can innovate faster, but also because it can help them resolve serious security issues and compliance problems. In fact, 66% of the respondents cited increased security and better compliance as their number one reason for adopting DataOps[vii].\n\nOrganizations are under greater scrutiny than before due to all the data breach issues many have experienced. There’s also more pressure from regulatory bodies regarding data privacy. So, companies are turning to DataOps to develop and implement data governance policies that are consistent, but that still allow data to flow rapidly, while being completely secure.\n\nOne issue is the greater number of people who require access to data, which led to 68% of respondents stating that securing the data they share with internal and external users is a serious concern.\n\nMost of the data breaches that make it onto the news are generally caused by external threats. However, the reality is that the most significant threats tend to come from internal users. This is not necessarily the intent, but negligence often leads to severe issues. This also falls to the organization for not having consistent and uniform security policies in place, and a way to enforce them.\n\nDataOps can provide the homogenous approach to security required to keep the data safe, regardless of whom has access to it, as long as it has the right data platform to work from. This unified approach can function in all areas of the organization, no matter the technology being used.\n\nThe DataOps Manifesto\n\nThe organizations and people who support DataOps have created a manifesto that consists of eighteen principles summarizing the best practices, philosophies, goals, mission, and values of those who practice DataOps.\n\nThe manifesto places individuals and interactions above process and tools. They focus on working analytics instead of comprehensive documentation. They advocate for customer collaboration instead of focusing on negotiating contracts. They support experimentation, iteration, and feedback instead of spending an inordinate amount of time on upfront design. They also feel that siloed responsibilities should be eliminated in favor of cross-functional ownership of operations.\n\nThe DataOps Manifesto principles are as follows[viii]:\n\n1. Customers must come first in all stages and the highest priority of DataOps is to ensure the customer is satisfied through the quick and continuous delivery of valuable insights.\n\n2. Place value on insights generated, which should be the real metric of the performance of data analytics.\n\n3. Welcome change, including the fact that customer needs evolve, and talking to customers face-to-face.\n\n4. Analytics is about teams of people with different roles, skills, titles, and favored tools.\n\n5. Collaborate with customers and operations at all stages, every day, throughout the project.\n\n6. Self-organization as it leads to the best insights, architectures, algorithms, designs, and requirements.\n\n7. Focus on creating sustainable and scalable teams and processes instead of concentrating on heroism.\n\n8. Regular self-reflection to improve operational performance.\n\n9. Analytic teams rely on a variety of tools that generate code and configuration, describing how the data is acted upon to generate insights.\n\n10. Orchestration from start to finish of data, code, tools, environments, and teams is essential to success.\n\n11. Everything must be versioned because reproducible results are a requirement.\n\n12. Minimize experimentation costs for analytic team members by providing disposable environments.\n\n13. Simplicity, also known as doing as little useless work is possible, is essential to success and improves agility.\n\n14. A fundamental DataOps concept is to focus on achieving constant efficiencies in the production of insights.\n\n15. Analytic pipelines must have a foundation that can automatically detect abnormalities and security problems in data, configuration, and code. It should also provide constant feedback so errors can be avoided.\n\n16. Quality, performance, and security measures should be constantly measured to identify any variations.\n\n17. Avoid repeating work previously done to improve efficiencies.\n\n18. Minimize the time and effort required to transform a customer need into insight, transform it into reality, release it as a production process that can be repeated, and then reuse the product.\n\nDataOps: The Future\n\nWhile it might not yet have achieved widespread adoption, the future is obvious: DataOps is here to stay. Much like DevOps, we’ll see a rise in the value of associated teams and positions[ix].\n\nFor example, before Agile Development, release engineers were significantly undervalued, especially when compared to software developers. Now, though, companies that implement DevOps highly value release engineers. Furthermore, a DevOps engineer, as they are now known, is one of the best-paid positions in software engineering.\n\nDevOps engineers are so difficult to find that companies are willing to hire someone even if they don’t have a college degree as long as they have the right knowledge and experience — this is becoming a huge trend.\n\nSomething similar is likely to happen with what can likely be known as the position of DataOps engineer. Regardless of the title, data analysts, data engineers, and data scientists can be even more valued with the implementation of a sound DataOps strategy.\n\nHowever, it might be a while before this happens. DataOps is still a new idea, and though there is much conversation around it, limitations exist that hinder widespread adoption.\n\nThese limitations can gradually disappear, of course, as DataOps becomes increasingly popular. It’s likely that, in the near future, we will see more discussion on the principles and guidelines that can lead to successful implementation.\n\nJust like DevOps has evolved to play a vital role in the management of IT infrastructure, so too can DataOps change the way data is made available, shared, and integrated.\n\nAs more data is being collected and/or produced every day, an increasing number of enterprises can have little choice but to turn to DataOps so they can manage their data more efficiently and effectively.\n\n[i] Olavsrud, Thor. “What Is DataOps? Collaborative, Cross-Functional Analytics.” CIO, November 21, 2017. https://www.cio.com/article/3237694/what-is-dataops-data-operations-analytics.html.\n\n[ii] Rackspace. What Is DevOps? — In Simple English. Accessed April 18, 2019. https://www.youtube.com/watch?v=5Hd0HUNhdVQ.\n\n[iii] DataKitchen. “DataOps Is NOT Just DevOps for Data.” Data-Ops (blog), November 18, 2018. https://medium.com/data-ops/dataops-is-not-just-devops-for-data-6e03083157b7.\n\n[iv] “DataOps.” In Wikipedia, March 27, 2019. https://en.wikipedia.org/w/index.php?title=DataOps&oldid=889646323.\n\n[v] DataKitchen. “2017: The Year of DataOps.” Data-Ops (blog), December 19, 2017. https://medium.com/data-ops/2017-the-year-of-dataops-b2023c17d2af.\n\n[vi] Science, #ODSC-Open Data. “DataOps and the DataOps Manifesto.” #ODSC — Open Data Science (blog), February 26, 2019. https://medium.com/@ODSC/dataops-and-the-dataops-manifesto-fc6169c02398.\n\n[vii] “Speed and Security Are Main Drivers for Surge in DataOps Adoption.” Delphix. Accessed April 18, 2019. https://www.delphix.com/blog/speed-security-surge-dataops.\n\n[viii] DataKitchen. “DataOps Engineer Will Be the Sexiest Job in Analytics.” Medium (blog), May 16, 2017. https://medium.com/data-ops/dataops-engineer-will-be-the-sexiest-job-in-analytics-9c38bf444e5a.\n\n[ix] “The DataOps Manifesto.” Accessed April 18,\n\nTraditional MDM, however, is starting to look out of date. It is not designed to cope with today’s data challenges.\n\nTraditional MDM has served organizations well for many years. MDM processes and technologies have been used to create and maintain an authoritative, reliable, accurate, timely and secure records of the organization’s most important data—a single, high-quality version of the truth, also known as “golden records.” MDM products and services are designed to remove duplicates, standardize data and apply rules that maintain data integrity.\n\nTraditional MDM, however, is starting to look out of date. It is not designed to cope with today’s data challenges. Data volumes, for example, are growing at a breakneck pace. The research firm IDC estimates that the “global datasphere”—defined as all the data created, replicated or captured in all the world’s data centers, infrastructure and devices—will grow more than fivefold between 2018 and 2025, from 33 Zettabytes to 175 Zettabytes by 2025.1\n\nData requirements are not just growing in volume, but also in the diversity of types and sources of data used in models. Data is no longer limited to structured, transnational, internally generated types. Today, when looking to enable greater use of artificial intelligence, organizations should incorporate machine-generated, unstructured, semi-structured, streaming, external and probabilistic data.\n\nThe limits of traditional MDM\n\nAs data proliferates and becomes more heterogeneous, business requirements for it are changing. For example:\n\nCompanies most time need to respond to events and opportunities faster, and act proactively to meet customer needs; this can demand continuous, real-time access to comprehensive, current customer data.\n\nSocial network, mobile device, web activity, location and other external context data should be reconciled with internal customer data to help drive marketing activities and customer experience.\n\nSome time there is a need to maintain dynamically changing, unified data that accounts for the interactions and relationships that exist within data entities across all touch points across the enterprise.\n\nIncreasing regulatory compliance requirements demand greater data management and governance capabilities.\n\nCompanies’ master data systems should include probabilistic data and fuzzy matching algorithms alongside more certain data.\n\nWhen it comes to integrating these data types, “point-to-point” or “match and merge” approaches of traditional MDM would be too complex and slow; new techniques are required to adequately capture, link and curate these newer types of data.\n\nExpanding and evolving MDM\n\nInstead, a flexible “Digital Master” model is emerging that can be tailormade to deliver new business value from vast amounts of diverse data.\n\nThe tasks performed by traditional MDM, focused around identifying missing or erroneous entries in highly structured data sets and eliminating duplicate records, remain vital to modern business requirements. But MDM should be expanded to meet the business requirements described above. The solution is the “Digital Master”: an evolution of traditional MDM, designed to bring data to the heart of the business and organize data at scale, combining data from a wide variety of sources.\n\nThe Digital Master helps companies to move beyond having a limited and static “golden record” (of a customer, for example) to having a “golden profile” that not only contains more types of data and insights but can be updated and expanded in real time. (More on Golden Profiles in the full report on page 12.)\n\nCompared with the use of traditional MDM, the Digital Master offers several advantages:\n\nIt can support deeper insights into customer behavior, more specific and accurate predictions, improved customer experience and real-time responses to events and opportunities.\n\nIt can be more business-outcome-focused, insight-driven, adaptable, comprehensive, agile and scalable across the enterprise.\n\nIt can support enhanced business reporting but also drive operations directly by automating many real-time decisions.\n\nIt can put AI and analytics at the center of every business process and any effort to transform customer insights and relationships.\n\nIt can allow organizations to get more from AI and become more data-driven.\n\nFor example, we helped a large hospitality company build a digital data platform that allowed it to capture, curate, process and store emerging data types. The company’s objectives included a radical improvement in customer experience and personalization, to help retain loyal customers and increase market share. Now, with its Digital Master in place, the company has a 360-degree view of existing and potential customers—they are able to recognize individuals at any point of interaction across its channels—and it can therefore enrich offers with intelligent insight and get ever closer to its key objectives.\n\nMaking the transition\n\nOrganizations tend to transition to the new Digital Master in one of two ways:\n\nExtend. A two-step process that involves maintaining the existing MDM in an integration hub and integrating it with a data lake, which is concurrently expanded to incorporate new data sources and capabilities\n\nReplace. A greenfield approach in which a robust Digital Master is built entirely around a new data lake.\n\nIn both cases, aggregating the organization’s existing data into a data lake is the starting point. This involves bringing together data from past transactions, customer records, activity on the corporate website (including from cookies) and social media channels. To make sense of this basic data, we need to add more meaning to it in the form of contextual insights. For this we can use a knowledge graph. This is flexible enough to work like a bridge between various data types and formats, while at the same time adding descriptions and associations that help machines to discern what the data is all about.\n\nThis process helps us to create rich, “golden” profiles of dimensions such as customers, businesses and products. It can also provide us with powerful insights into customer attitudes and behaviors (for example, by identifying correlations between different buyers of the same product).\n\nKnowledge graphs have been around for a couple of decades. Their ability to generate insights have been transformed in recent years, however, thanks to the proliferation of (mostly unstructured) data and enormous increase in computing power at organizations’ disposal. Knowledge graphs are what enable Apple’s, Amazon’s and Google’s virtual assistants to provide extremely relevant answers to users’ questions. Within a business, a knowledge graph that is constantly expanded with contextual and other data (such graphs can scale relatively easily) can identify and expand an increasing number of use cases for products, services or any number of business missions. And applications or dashboards can be created for some use cases, making it easy for users to quickly glean insights from the data.\n\nA vehicle for embedding AI\n\nWith all the elements of the Digital Master in place, enterprises can harness artificial intelligence and machine learning across their operations, supporting powerful breakthroughs in insights, data-driven decision-making and predictive capabilities.\n\nThe knowledge graph bridges the gap between traditional data stores and AI capabilities. This is because AI models can directly leverage the graph, so they can learn from massive amounts of meaningful data, bringing predictive power to new domains and increasing accuracy. Unsupervised learning algorithms can explore the knowledge graph and discover patterns that humans could never see.\n\neBay offers an example of how AI can leverage knowledge graphs to expand its capabilities. The company’s popular “ShopBot”—an AI-driven shopping assistant—is powered by knowledge graphs, according to its creators. eBay’s knowledge graph incorporates data from roughly 16 million active buyers. The bot has powerful algorithms, but when a user voices a query through the app about a certain product, its desired features and intended uses, it is the data provided by the knowledge graph that enables the bot to provide intelligent and relevant responses to the user.\n\nNot many companies today can leverage such capabilities at scale. In addition to eBay, online giants such as Amazon, Google, Microsoft, Facebook, Alibaba, Baidu and others are using AI in conjunction with knowledge graphs to drive their core operations and services. There is no reason, however, that other types of organizations cannot leverage these capabilities to drive positive outcomes across their business.\n\nDigital Master starting points\n\nIn this article we have explained why organizations should be making the shift toward the Digital Master model and the advantages that it confers in the digital age. To learn more about how to make the transition, our whitepaper will help executives understand the evolving landscape and assess the architectural options.\n\nAs indicated earlier, moving from MDM to a Digital Master can be done incrementally, without much disruption. It is the ideal path towards harnessing AI at scale across your operations. If you are at the beginning of this journey, the first steps you should take include the following:\n\nInventory your data: Determine which are your most frequently accessed and up-to-date data records. These are the sources of the single version of truth—the golden record—that the Digital Master will build upon.\n\nIdentify existing data hubs: Where in the enterprise do these data records sit? Can their repositories be connected to meet current and specific business requirements?\n\nReview your use cases: What are the existing use cases for your master data? To what extent do your existing MDM processes and technologies align with these? Can new solutions in the market serve those use cases more effectively? Do they create opportunities to for new cases?\n\nPrioritize the use cases: Which cases are the most critical to delivering uplift to the business? Do they involve new revenue generation or improved operational efficiency—or, alternatively, improved fraud detection or regulatory compliance? The priority cases should be the initial focus of new Digital Master.\n\nFind external data sources: Look outside your organization for external data sources that can be incorporated to complement existing reference data and help generate more complete insights through contextual and analytical models.\n\n1 IDC White Paper, sponsored by Seagate, Data Age 2025: The Digitization of the World from Edge to Core, November 2018\n\nBusiness Analytics – Future Leaders\n\nLooker\n\nLooker offers a modern analytics and BI platform that enables users to explore and visualize data using agile data modeling and modern analytic databases. The company received an additional $103 million in venture capital funding in 2018, bringing its total to $280.5 million. Looker is primarily deployed in the cloud, but can also be deployed on-premises.\n\nLooker differentiates itself by offering a data platform that, in addition to supporting visualizations and dashboards, enables data engineers to model data and then pass data and calculations on to other applications. Its platform can also be used to build analytical applications. In 2018, Looker added integrations for data science use cases including support for Google BigQuery ML, as well as Action Hub integrations that enable data to be moved to other apps, including TensorFlow. It also released a digital marketing application in 2018, the first of several planned domain analytical applications. Looker 6 was released in November 2018, adding support for MongoDB and dashboard performance improvements.\n\nA large majority of Looker’s reference customers use it for decentralized analytics (72%). More than half (56%) use it for agile, centralized BI provisioning.\n\nLooker’s position in the Niche Players quadrant reflects its strong momentum and high operations scores, but a relatively narrow product vision and limited market understanding.\n\nStrengths\n\nNative cloud support and optimization for analytic databases: Looker’s key differentiator is native support for cloud-based analytic databases, particularly Amazon (Redshift, Athena), Google BigQuery and Snowflake. Whereas most competing tools use their own in-memory, columnar storage, Looker always uses the database for processing and mashups. LookML is a browser-based, SQL-like modeling language for power users. Its data scalability is in the top tier, with 36% of Looker’s reference customers analyzing more than 1 terabyte of data and a median of 585 million row tables.\n\nExcellent support: Reference customers’ scores place Looker among the highest vendors for satisfaction with operations, including quality of support, time to resolution, and ease of migration. Gartner Peer Insights reviewers similarly rate it the best for service and support. Looker also offers the relatively rare ability to contact support via chat or email directly from within its product.\n\nMomentum: Looker has continued to increase its presence and head count — to about 600 as of December 2018 (a 47% year-over-year rise). Almost three-quarters (73%) of its reference customers say that Looker is their only enterprise analytics and BI standard — one of the highest percentages for a single standard. Looker’s technical partnerships now include IBM as a reseller for part of the IBM Watson Studio (used for building ML models). OEM and “Powered by Looker” business now accounts for 30% of Looker’s revenue. Looker launched the Looker for Good charitable program in 2018, pledging 1% of product and service revenue to nonprofit organizations.\n\nCautions\n\nCoding versus point and click: As Looker’s data modeling requires coding, business users find its product more difficult to use than point-and-click solutions. A large percentage of its reference customers (40%) said that poor ease of use for business users limits broad deployment — the highest figure for any vendor in this Magic Quadrant. The product lacks key capabilities for visually manipulating data and native mobile apps. Business users may now merge queries from multiple data sources, but there is no mechanism to correct for overstated measures in a many-to-many join.\n\nNarrow product roadmap: Whereas most other vendors are working on bringing NLP and augmented analytics to all users, Looker is closing basic visualization gaps, such as smart use of color in Looker 6 and custom themes. It continues to build interoperability with data science use cases and actions.\n\nLimited geographic presence: Looker operates primarily in the U.S. and Europe. Localization of its software for Japanese, French and German markets is planned for 2019.\n\nThoughtSpot\n\nThoughtSpot differentiates itself on its search-based interface with augmented analytics at scale. The company was founded by former Google executives, and received an additional round of series D funding in 2018 of $145 million, bringing its total funding to $306 million.\n\nThoughtSpot’s software can be deployed in the cloud or as an on-premises appliance on commodity hardware, with data loaded in-memory into a massively parallel processing (MPP) engine and indexed for fast query performance. In 4Q18, ThoughtSpot released SearchIQ to support voice-driven queries from mobile devices. Proactive alerting was also released, by which users are notified of unusual changes in data, as opposed to those that are simply threshold-based. Over three-quarters (78%) of ThoughtSpot’s reference customers use its product for decentralized analytics, and 62% for agile, centralized BI provisioning.\n\nThoughtSpot has moved from the Visionaries quadrant to the Leaders quadrant, thanks to strong execution, high customer satisfaction scores and rapid innovation.\n\nStrengths\n\nSearch and AI at scale: Using search and NLP as the primary interface for accessing data, ThoughtSpot is bringing data to new classes of user — ones who previously may not have used BI. Questions can be posed by typing or speaking. The types of question supported are analytically complex, such as “Give me sales within 50 miles, comparing products A and B.” SpotIQ, the augmented analytics interface, generates anomalies and correlations with no coding. New integration with R extends the product’s advanced analytics capabilities, and DataRobot support is planned for mid-2019. ThoughtSpot ranks in the top third of vendors for complexity of analysis and ease of use, with 34% of its reference customers analyzing more than 1 terabyte of data in memory.\n\nSales and operations: Reference customers put ThoughtSpot in the top third of vendors in this Magic Quadrant for operations and sales experience. Gartner Peer Insights reviewers put it in the top third for pricing and contract flexibility. Whereas much of the analytics and BI market focuses on named-user pricing, ThoughtSpot is solely server- and data-volume-based, which enables its product to be deployed for an unlimited number of users.\n\nMomentum: ThoughtSpot’s growth rate, particularly with enterprise customers, is among the fastest in this market: The company now claims to have over 200 customers, along with a large average deal size. The company’s head count (an indicator of growth and company health) was 370 at end of 2018, a 60% year-over-year rise. Reference customers put ThoughtSpot in the top third for overall operations, which includes support, product quality and migration experience.\n\nCautions\n\nGaps in data preparation, visual exploration, dashboards and architecture: ThoughtSpot requires all data to be loaded into its MPP engine, as opposed to using in-database processing or high-performing analytic databases. Data must be prepared and cleansed using third-party tools. The software does not allow users to readily manipulate data into groups or bins, and it lacks important chart types. Dashboards are basic, without rich mapping. Sixteen percent of ThoughtSpot’s reference customers identified missing functionality as a problem (a figure in the top third for this complaint across the vendors in this Magic Quadrant). Over half (56%) of ThoughtSpot’s reference customers use its product along with multiple standards (one of the highest percentages for vendors in this Magic Quadrant). This illustrates how it complements other products, but does not fulfill the full spectrum of analytic and BI requirements.\n\nSmall vendor with few partners: ThoughtSpot is one of the smallest vendors in this Magic Quadrant in terms of revenue. Furthermore, it has few partnerships around the world or with large SIs. To date, all deployments have been handled directly by ThoughtSpot itself. Consequently, reference customers’ scores put it in the bottom third for availability of skills in the market.\n\nAbsence of prebuilt vertical content: ThoughtSpot’s product lacks prebuilt content for specific vertical and functional domains. In this regard, customers must build their own applications for particular functional areas, with no prebuilt content to shorten the time to value in relation to implementation or the sales process. This is a competitive weakness for ThoughtSpot, compared with other Leaders and its chief competitors.\n\nClearStory Data:\n\nThis vendor’s analytics and BI offering provides augmented data discovery and preparation, data storytelling and collaboration in a single platform that can be run in multiple clouds or on-premises. It uses Spark-based processing to handle large data volumes. ClearStory’s smart data inference and Intelligent Data Harmonization use ML to recommend how to cleanse, prepare and blend data.\n\nPeriscope Data:\n\nThis vendor combines agile data modeling, data science and visual exploration in a single cloud-based platform. Data can be queried directly or modeled and explored to create reusable data models, with data coming from multiple blended data sources. SQL, R and Python scripts can be executed locally and saved as a new dataset.\n\nHow can the transportation Departments use Big Data as a revenue stream?\n\nThere are companies that specifically collect data on traffic and travel patterns in real time and sell them to transportation agencies and logistics companies – these are Inrix, Waze, Tom Tom (to name a few). IBM has been getting into the game, and Google and all the cell companies have travel and movement data for anyone with a cell phone (privacy issues prevent them from directly monetizing on it, however). In Ontario, Accenture operates the Presto card system, which collects data on anyone using public transit on the Presto system, including Toronto and Ottawa.\n\nTransportation services increasingly rely on data to help them plan services and operations. The introduction of big data is now allowing agencies to implement predictive analytics to better plan for service incidents and events. For example, cities can look at historical traffic patterns to determine how new service rollout will affect traffic in the future. However, the data isn’t necessarily collected by the cities/agencies themselves – it’s collected by third party and private sector organizations who then sell the data to the users.\n\nBig data analytics is surely helping businesses. Especially business generating huge amounts of data need to demystify the data in order to extract some useful results. These results are visualized in form of patterns.\n\nThese are the few areas where big data can prove useful for transportation industry.\n\nFreight movement and routing optimization\n\nInventory Management & Capacity Optimization\n\nImproved Customer Experience by develop effecting communication via insights from social media, persona segmentation & preferences\n\nReduced Environmental Impact and Increased Safety\n\nFleet Optimization and Predictive Maintenance through Real-time view of fleet operating conditions, statistics around usage and wear patterns, maintenance cycles\n\nOptimized Transit schedules by predicting impact of maintenance, road-works, congestion & accident\n\nMagic Quadrant for Data Science and Machine-Learning Platforms\n\nData science and machine-learning platforms enable organizations to take an end-to-end approach to building and deploying data science models. This Magic Quadrant evaluates 16 vendors to help you identify the right one for your organization’s needs.\n\nH2O.ai\n\nH2O.ai, which is based in Mountain View, California, U.S., offers an open-source machine-learning platform. For this Magic Quadrant, we evaluated H2O Flow, its core component; H2O Steam; H2O Sparkling Water, for Spark integration; and H2O Deep Water, which provides deep-learning capabilities.\n\nH2O.ai has progressed from Visionary in the prior Magic Quadrant to Leader. It continues to progress through significant commercial expansion, and has strengthened its position as a thought leader and an innovator.\n\nStrengths\n\nTechnology leader: H2O.ai scored highly in categories such as deep-learning capability, automation capability, hybrid cloud support (“deploy anywhere”) and open-source integration. H2O Deep Water offers a deep learning front end that abstracts away many of the details of back ends like TensorFlow, MXNet and Caffe. Its machine-learning automation capabilities (dubbed Driverless AI) are impressive and, although still developing, demonstrate the company’s distinguished vision. In terms of flexibility and scalability, reference customers considered H2O.ai to be first class. It has one of the best Spark integrations, and is ahead of all the other Magic Quadrant vendors in its graphics processing unit integration efforts.\n\nMind share, partners and status as quasi-industry standard: H2O.ai’s platform is now used by almost 100,000 data scientists, and many partners (such as Angoss, IBM, Intel, RapidMiner and TIBCO Software) have integrated H2O.ai’s technology platform and library. This shows the company’s technical leadership, which especially derives from the highly scalable implementation of some core algorithms.\n\nCustomer satisfaction: H2O.ai’s reference customers gave it the highest overall score for sales relationship and account management, evaluation and contract negotiation experience, customer support (including onboarding and troubleshooting), and overall service and support. They also gave it outstanding scores for analytic support (including training and technique selection), integration and deployment, inclusion of requested product enhancements in subsequent product releases, and overall experience with the vendor.\n\nCautions\n\nEase of use: H2O.ai’s toolchain is primarily code-centric. Although this typically increases flexibility and scalability, it impedes ease of use and reuse.\n\nData preparation and interactive visualization: These capabilities are problematic for all code-centric platforms, of which H2O.ai’s is one. Nonetheless, H2O.ai’s platform will prove challenging for clients expecting more interactivity and better, easier-to-use data ingestion, preparation and visualization capabilities. Capabilities for the entire early part of the data pipeline are far less developed than the quantitative parts of H2O.ai’s offerings.\n\nBusiness model: H2O.ai is a full-stack open-source provider — even its most advanced products are free to download (except for the closed-source Driverless AI). H2O.ai derives nearly all its revenue from subscriptions to technical support. Given the development of H2O.ai’s revenue lately, we are slightly less concerned than we would otherwise have been about this policy of “giving everything away.” However, we still maintain that this policy is difficult to scale. In the long run, H2O.ai will need to consider a more scalable software-licensing model and support infrastructure.\n\nThe Forrester Wave™: Enterprise BI Platforms Q3 2017\n\nThe Forrester Wave™: Enterprise BI Platforms With Majority On-Premises Deployments, Q3 2017\n\nMicroStrategy continues to address all BI use cases at scale. In contrast to some other BI platforms that purely rely on a DBMS for query processing and optimization, MicroStrategy makes a more realistic assumption that not all DBMSes are well tuned. Its core strength remains a powerful, highly scalable ROLAP, which can optimize queries beyond DBMS capabilities; perform complex heterogeneous joins between multiple DBMSes; and support a drill-anywhere capability, which can reduce the number of siloed reports and dashboards. ( see endnote 6) MicroStrategy also received one of the best scores in our evaluation’s “clicks to answer” exercise. The vendor is also back in the BI business 100%, having recently folded its previously separate mobile application development and digital credentialing products under BI and analytics.\n\nIBM offers a broad and comprehensive BI platform with a touch of AI. The latest versions of IBM’s long-time BI market leader, Cognos Analytics, get a boost of predictive analytics via integration with SPSS and a shot of AI via integration with Watson Analytics. Knowledge gaps (or “I don’t know what I don’t know”) are a significant missed opportunity in many BI environments, and that’s precisely what Watson Analytics addresses: It suggests areas of interest for users to discover without them explicitly asking a question. Once users explore these areas, they can turn insight into comprehensive descriptive and predictive BI applications using Cognos Analytics and SPSS. IBM’s platform is also one of few BI platforms that offer writeback via TM1, which is bundled with its Cognos Analytics suite.\n\nClient references gave IBM high scores for quick time-to-value, scalability, stability, security, cloud/hybrid architecture, extensibility, and frequency of upgrades. However, users will have to deal with three different UIs in IBM’s three BI products, and more holistic integration remains a work in progress for IBM. ( see endnote 7) Additionally, client references expressed concern about difficulty navigating IBM’s large and complex organization for sales and support.\n\nTIBCO Software is back as a Leader for full-spectrum data visualization and analytics. After a brief slowdown due to the company’s privatization in 2014 and the associated management and key personnel changes, TIBCO Spotfire is back in the market as a formidable player in full-spectrum analytics, from basic descriptive analytics to advanced data visualization and predictive analytics. Our evaluation gave Spotfire top scores for advanced analytics and ease of use (as per its top customer-reference survey scores and least number of clicks for the “clicks to answer” exercise). Customer references also rated Spotfire highly for quick time-to-value, scalability, data visualization, end user self-service, and cloud/hybrid architecture.\n\nProspects and customers looking for full BI stack capabilities, however, will prefer Jaspersoft (not evaluated in this report) for mass-scale, pixel-perfect report distribution and for embedded analytics (a key BI market trend).\n\nQlik continues to differentiate with its powerful associative BI engine. All BI tools work great when you know how to ask a question and what specific data sources, tables, and columns contain the information you are looking for. What if you don’t? This is precisely the sweet spot for Qlik’s two products, QlikView and Qlik Sense (its strategic product). In addition to the usual point-and-click UI, Qlik’s associative in-memory engine allows users to simply start typing, and anything that matches in its entire in-memory data set is instantaneously highlighted. Such exploratory UI is one reason customer references awarded Qlik one of the highest scores for business value in terms of ROI.\n\nQlik has few gaps in its BI portfolio — one of the remaining ones is actionable BI, such as support for process workflows and writeback. Client references also suggested that Qlik’s advanced/predictive analytics could use additional functionality.\n\nStrong Performers\n\nInformation Builders secures its future with an investment from Goldman Sachs. In a crowded BI market with over 70 vendors, buyers should make vendor viability a key part of the evaluation. Information Builders’ prospects and clients can rest easier, based on the latest endorsement by Goldman Sachs taking a sizable position in the company in May 2017. While newer BI vendors are still learning how to deal with massively scalable BI applications, Information Builders has been in the big data business for over 30 years. Clients who don’t view Information Builders as a viable competitor in predictive analytics should take another look: WebFOCUS offers one of the most advanced features to author, integrate, and deploy predictive BI applications using R.\n\nClient references gave Information Builders top scores for all 25 survey questions — by far the highest number in this evaluation. As a software engineering company at its core, Information Builders still needs to address the overall end user experience with a complex portfolio of BI products and a UI, where the FOCUS programming language sometimes bursts through the seams.\n\nLooker is a modern BI platform not burdened with legacy DBMS inefficiencies. BI vendors that went into the business decades ago had to architect their platforms to overcome legacy DBMS inefficiencies. Given technology limitations, some of these vendors also had to come up with multiple BI products to address different use cases. Looker doesn’t have to deal with these legacy issues and assumes that modern DBMSes are highly efficient and scalable. As a result, it has concentrated its R&D investments into one clean, simple, modern BI platform that addresses most BI use cases. Looker also takes a modern approach to integration: Rather than coding to multiple data and application sources, it relies on emerging integration provider partnerships. ( see endnote 8) As a result of its modern architecture, Looker achieved one of the best scores in our evaluation’s “clicks to answer” exercise.\n\nClient references gave Looker high scores for time-to-value, cloud/hybrid architecture, and professional services and help-desk organizations. As a young company that is still growing, Looker still needs to address multiple gaps in its BI functionality, such as beefing up its predictive, suggestive, prescriptive, geospatial, and mobile analytics capabilities.\n\nPyramid Analytics rearchitects and goes to market as a general-purpose BI platform. The latest version of Pyramid Analytics’ BI Office product has been completely rearchitected and drops its former dependence on Microsoft’s platform. That is not to say that prospects and clients can’t continue to use BI Office as an add-on to the Microsoft BI environment to boost Microsoft’s on-premises BI administration and distribution (as the majority of Pyramid Analytics’ current customers still do). BI Office is a solid choice for buyers looking to deploy a scalable and governed BI platform for most of the typical use cases.\n\nPyramid Analytics still has multiple gaps to fill in BI Office, such as suggestive BI and new ways to interact with information via natural language processing (NLP) and natural language generation (NLG). The vendor also needs to improve its geospatial analytics capabilities.\n\nTableau’s powerful yet simple data visualization affords it the biggest mindshare.More features don’t necessarily make data analysis and exploration easier. Tableau Software continues to focus on one thing and one thing only: a clean, intuitive, easy-to-navigate UI and insights presentation. This razor-sharp focus earns Tableau Software a top spot in this market: Forrester has few interactions with clients on the topic of BI where clients do not bring up Tableau. Tableau Software was also the first vendor to get an IBCS data visualization certification. ( see endnote 9)\n\nBut the BI market continues to evolve beyond just data visualization, and Tableau is still catching up to the competition on highly popular new features such as actionable/suggestive BI, advanced/predictive analytics, and data preparation — in an upcoming release, it plans to roll out a data preparation tool, Project Maestro, architected based on its recently acquired in-memory DBMS HyPer. On August 9, 2017, the vendor acquired natural language query startup ClearGraph, which will significantly boost Tableau’s NLP capabilities. ( see endnote 10) Tableau Software did not participate in this Forrester Wave evaluation.\n\nSAP offers a variety of BI tools that address all possible enterprise use cases. SAP goes to market with a broad BI portfolio (part of its recently announced digital innovation system, SAP Leonardo) with options to deploy its products on-premises and/or in the cloud. It also addresses all use cases from scalable enterprise reporting to lightweight, user-centric data visualization. SAP has made significant progress in consolidating and integrating over a dozen individual BI products down to four; however, more complete and seamless integration is still a work in progress.\n\nThe current SAP BI suite consists of three BusinessObjects and one Analytics Cloud products: Lumira (one of very few products with a prestigious IBCS data visualization award) for dashboards, discovery, and analysis; Analysis Office for analytics using a familiar Excel UI; and Crystal Reports and Web Intelligence for scalable report distribution. Single-tenant BusinessObjects products can be deployed on-premises or hosted on various public cloud platforms. For a native multitenant platform, SAP clients should consider Analytics Cloud.\n\nOpenText tackles big data BI with its Hadoop/Spark-based Magellan platform. In July 2017 OpenText integrated and packaged BI products acquired over the last few years into a highly scalable big data platform. Rather than buying and using three separate products for reporting (iHub), big data analytics (BDA), and text mining (InfoFusion), customers can get all three functions in a single product. ( see endnote 11)OpenText Magellan differentiates from the competition by allowing users to analyze structured and unstructured data in a single package and moving many components to Hadoop and Spark, popular open source software platforms. However, all of the hard work going into Magellan took priority over other popular BI features, and OpenText still needs to build up capabilities such as actionable/suggestive BI and new ways to interact with information via NLP and NLG.\n\nOpenText’s legacy focus — mostly on professional BI developers — still shows, and customer references report a high level of reliance on technology pros to complete many of the analytical tasks, which they say can potentially reduce the product’s ROI. Part of OpenText Magellan’s road map is to address this very issue.\n\nYellowfin, an embedded BI specialist, is a solid choice for all enterprise BI use cases. Even though the majority of Yellowfin revenues comes from resellers and OEM partners, the platform supports most typical enterprise BI use cases. As a result, Forrester has seen an increase in Yellowfin’s appearance on enterprise BI platform short lists. Customer references like Yellowfin’s collaboration and storytelling capabilities.\n\nWhile Yellowfin is mainly popular for its modern, clean, easy-to-use, and navigate UI, it struggles with more complex analytical use cases, and as a result it received one of the lowest scores in the evaluation’s “clicks to answer” exercise. Client references also gave Yellowfin relatively low scores for advanced/predictive analytics.\n\nSisense addresses use cases where even in-memory computing is not fast enough.Analyzing terabytes of detailed data requires smart data architecture based on aggregates, indices, and other SQL optimization techniques. But these take time to design and optimize, and you’ll have to start from scratch when data sources, data model, or business requirements change. Sisense’s secret ingredient, its core IP, is in the way it moves data between disk, RAM, and CPU, minimizing the number of cycles it takes to move the data beyond what the CPU and operating system can deliver. As a result of this brute-force approach, users can hit the ground running analyzing billions of rows of data without a complex data architecture design process.\n\nReference clients confirmed the uniqueness of Sisense’s on-CPU architecture, and they gave the vendor some of the highest scores for low latency when slicing and dicing especially large data sets. Sisense is a young company and still has much catching up to do. Some of the gaps in its current product functionality include actionable/suggestive BI, data profiling, and data cataloging. Sisense also needs to beef up its geospatial analytics, such as automatic geocoding of relevant attributes.\n\nSAS bets on in memory computing with its Viya-based Visual Analytics. While some competitors rely on the scalability of modern DBMSes, SAS bets that no DBMS is scalable enough for big data analytics. Its Visual Analytics (VA) is based on SAS’s Viya platform — in-memory, cloud-ready, elastic data grid — and it can optimize real-time data visualization, analysis, discovery, and exploration regardless of the performance of the source DBMS. While SAS VA is the vendor’s strategic BI product going forward, SAS plans to maintain its other BI products, such as enterprise BI, to support existing customers.\n\nAreas of improvement for SAS VA include developing its data preparation, profiling, cataloging, and geospatial analytics capabilities (some of the reasons SAS VA scored low on the “clicks to answer” exercise) as well as introducing net-new features such as suggestive BI and NLP/NLG as new ways to interact with information.\n\nContenders\n\nBOARD International is a solid choice for companies looking for BI and EPM together. Slowly but surely, BI applications are migrating from data-centric, read-only applications to process-centric, read-and-write applications — and BOARD International is riding this trend. In addition to all of the typical BI functions, the platform supports all of the enterprise process management (EPM) functions, including budgeting, planning, approvals, and writeback, all packaged in a unified seamlessly integrated platform.\n\nRich process functionality comes at an expense, and BOARD falls behind the competition on innovations such as suggestive BI and interacting with applications via NLP and NLG. BOARD is also working on improving its data preparation, data profiling, and automated geocoding functionality, which were reflected in the vendor’s ease-of-use scores.\n\nPanorama holds the course with collaborative BI. Democratizing data and insights doesn’t necessarily negate the benefits of tribal knowledge. BI users still look for their colleagues for recommendations on what are the best reports or dashboards for a particular task. Panorama’s differentiated recommendation engine helps users get the right insight within the right context at the right time. Rather than showing a blank screen and making you guess what to do, Panorama Necto’s machine-learning-based recommendation engine nudges you in the right direction by showing you the most popular content as tagged and ranked by your colleagues.\n\nPanorama Necto is a general-purpose BI platform, applicable to most use cases. However, most clients use Necto as an add-on to the Microsoft BI environment to boost Microsoft’s on-premises BI administration, distribution, and collaboration capabilities. Panorama is still working on filling in multiple gaps in its BI platform, such as embedded advanced/predictive analytics, data preparation, and data profiling.\n\nGartner Data Science 2017\n\nGartner new 2017 Magic Quadrant for Data Science Platforms (called in 2016 “Advanced Analytics Platforms”) was published last week. The 2017 report evaluated a new set of 16 analytics and data science firms over 15 criteria and placed them in 4 quadrants, based on completeness of vision and ability to execute.\n\nWhile open source platforms like Python and R play an important role in the Data Science market, Gartner research methodology does not include them, so this report evaluates only commercial vendors.\n\nFirms covered:\n\nLeaders (4): IBM, SAS, RapidMiner, KNIME\n\nChallengers (4): MathWorks (new), Quest (formerly Dell), Alteryx, Angoss\n\nVisionaries (5): Microsoft, H2O.ai (new), Dataiku (new), Domino Data Lab (new), Alpine Data\n\nNiche Players (3): FICO, SAP, Teradata (new)\n\nGartner notes that even the lowest-scoring vendors in MQ are still among the top 16 firms among over 100 vendors in the heated Data Science market.\n\n5 new firms were added in 2017 (MathWorks, H2O.ai, Dataiku, Domino Data Lab, and Teradata) and 5 others present in 2016 MQ were dropped: Lavastorm, Megaputer, Prognoz, Accenture, Predixion Software.\n\nLeaders:\n\nThe same 4 firms: IBM, SAS, RapidMiner, and KNIME, which were leaders in 2014-2016, are again in the leaders quadrant. Dell Software, sold in Nov 2016 and renamed Quest, and its Statistica Analytics suite moved from Leaders to Challengers quadrant.\n\nThe 2017 MQ shows that IBM gained a little in vision, while SAS lost a little in ability to execute. RapidMiner gained in ability to execute, but both KNIME and RapidMiner lost in vision.\n\nIBM: is again a leader, based on SPSS Modeler and SPSS Statistics. Gartner did not evaluate IBM new Data Science Experience (DSx) platform but DSx contributed to IBM’s improved position along Vision axis. IBM strengths include its vast customer base and continued innovation of its data science and machine learning capabilities. However, IBM breadth of offerings (SPSS, IBM Watson, DSx, Cognos) creates some confusion in the market and SPSS line has interoperability problems.\n\nSAS provides a many software products for analytics and data science. Gartner evaluated covers SAS Enterprise Miner (EM) and the SAS Visual Analytics Suite (VAS). SAS is more focused now on interactive modeling with VAS, but continues to support its traditional base SAS. SAS retain a strong position in the Leaders quad, but confusion about its multiple products and concerns about high cost led to decline in Ability to Execute.\n\nKNIME offers open-source KNIME Analytics Platform with strong functionality for advanced data scientists. It is strong in several industries, especially in manufacturing and life sciences. It lost somewhat along the Vision dimension due to weaker marketing and innovation compared to other leaders.\n\nRapidMiner offers GUI-based data science platform, suitable for beginner and expert data scientists. It also offers access to open-source code. RapidMiner is available both as a free version and a commercial edition (with extra functionality for large data and connections to more data sources). RapidMiner is in leaders quad due to its market presence and well-rounded product.\n\nChallengers:\n\nMathWorks is a new entry for its MATLAB product, popular with engineers, and providing a rich set of toolboxes.\n\nQuest, the result of the sale of Dell Software in 2016 to a private equity firm, now sells the Statistica Platform. Quest is in Challenger quad (while Dell was in Leaders quad) as a result of the second change of ownership of Statistica in 3 years and lack of cloud-related product improvements (which however are on the roadmap).\n\nAlteryx, offers an easy to use data science platform, with self-service data preparation and advanced analytics. It also added simulation and optimization capabilities. Compared to 2016, it moved from Visionaries to Challengers quad due to its solid customer growth.\n\nAngoss provides visual data mining and predictive analytics tools, as well as prescriptive analysis and optimization. Angoss remained in almost the same position in Cha quad as in 2016.\n\nVisionaries:\n\nMicrosoft evaluation was based the Azure Machine Learning platform, part of the Microsoft Cortana Intelligence Suite, which offers a strong cloud-based data science platform. Gartner kept Microsoft in the visionaries quad, due to the lack of a comparable onsite solution.\n\nH2O.ai, a new entry, offer open-source data science platform with fast execution of Deep Learning and other advanced Machine Learning methods.\n\nDataiku is a new entry, with its Data Science Studio (DSS). It was placed in Visionaries quad due to innovative nature of DSS, openness, collaboration features, and suitability for different skill levels.\n\nDomino Data Lab a new entry with its Domino Data Science Platform, which focuses collaboration and supports a wide range of open-source technologies.\n\nAlpine Data offer a “citizen data science” platform, Chorus, enabling collaboration between business analysts and front-line users in building and running analytic workflows. Compared to 2016, Alpine remained in Visionaries quadrant, but was dropped in its ability to execute due to its struggle to gain market share.\n\nNiche Players:\n\nSAP renamed its data science platform to SAP BusinessObjects Predictive Analytics. SAP decline in ability to execute caused it to drop from Challengers quad to Niche quad, and it is lagging in Spark integration, open-source and Python support, and cloud deployment.\n\nFICO Decision Management Suite (DMS) offers multiple analytic tools. It stayed in the niche quadrant in 2017, losing on vision but gaining a little on ability.\n\nTeradata offers Aster Analytics platform, with 3 layers: analytic engines, prebuilt analytic functions, and the Aster AppCenter for analysis and connectivity to external BI tools. It is in the Niche quad to to low-level of adoption."
    }
}
Abstract

An overview of the principle feature subset selection methods is given. We investigate a number of measures of feature subset quality, using large commercial databases. We develop an entropic measure, based upon the information gain approach used within ID3 and C4.5 to build trees, which is shown to give the best performance over our databases. This measure is used within a simple feature subset selection algorithm and the technique is used to generate subsets of high quality features from the databases. A simulated annealing based data mining technique is presented and applied to the databases. The performance using all features is compared to that achieved using the subset selected by our algorithm. We show that a substantial reduction in the number of features may be achieved together with an improvement in the performance of our data mining system. We also present a modification of the data mining algorithm, which allows it to simultaneously search for promising feature subsets and high quality rules. The effect of varying the generality level of the desired pattern is also investigated.

References

[1]

ANGOSS: 1994, KnowledgeSEEKER 3.11.05 on-line help.

[2]

ANGOSS: 1995, KnowledgeSEEKER for Windows Version 3.0 User's Guide, Toronto, Canada.

[3]

Biggs, D., de Ville, B. and Suen, E.: 1991, A method of choosing multiway partitions for classification and decision trees, Journal of Applied Statistics 18(1), 49-62.

[4]

Breiman, L., Friedman, J. H., Olshen, R. A. and Stone, C. J.: 1984, Classification and Regression Trees, Wadsworth and Brooks, Monterey, CA.

[5]

Chang, C. Y.: 1973, Dynamic programming as applied to feature selection in pattern recognition systems, IEEE Trans. Syst. Man and Cybernet. 3, 166-171.

[6]

Chardaire, P., Lutton, J. L. and Sutter, A.: 1995, Thermostatistical persistency: A powerful improving concept for simulated annealing algorithms, European Journal of Operational Research 86.

[7]

Cover, T. M. and Van Campenhout, J. M.: 1977, On the possible orderings in the measurement selection problem, IEEE Trans. Sys. Man Cybern. 7, 651-661.

[8]

de la Iglesia, B., Debuse, J. C. W. and Rayward-Smith, V. J.: 1996, Discovering knowledge in commercial databases using modern heuristic techniques, in E. Simoudis, J. W. Han and U. Fayyad (eds), Proc. of the Second Int. Conf. on Knowledge Discovery and Data Mining (KDD-96), pp. 44-49.

[9]

de Ville, B.: 1990, Applying statistical knowledge to database analysis and knowledge base construction, Proceedings of the Sixth IEEE Conference on Artificial Intelligence Applications, IEEE Computer Society, Washington.

[10]

Devijver, P. A. and Kittler, J.: 1982, Pattern Recognition: a Statistical Approach, Prentice-Hall International, London.

[11]

Foroutan, I. and Sklansky, J.: 1987, Feature selection for automatic classification of non-gaussian data, IEEE Trans. Sys. Man Cybern. 17, 187-198.

[12]

John, G. H., Kohavi, R. and Pfleger, K.: 1994, Irrelevant features and the subset selection problem, in W. W. Cohen and H. Hirsh (eds), Machine Learning: Proceedings of the Eleventh International Conference, Morgan Kaufmann, San Francisco, pp. 121-129.

[13]

Kass, G. V.: 1980, An exploratory technique for investigating large quantities of categorical data, Appl. Statist. 29, 119-127.

[14]

Kohavi, R. and Sommerfield, D.: 1995, Feature subset selection using the wrapper method: overfitting and dynamic search space topology, in U. M. Fayyad and R. Uthurusamy (eds), Proc. of the First Int. Conf. on Knowledge Discovery and Data Mining, AAAI Press, pp. 192-197.

[15]

Koller, D. and Sahami, M.: 1996, Toward optimal feature selection, in (Saitta, 1996).

[16]

Liu, H. and Setiono, R.: 1996a, Feature selection and classification - a probabilistic wrapper approach, Proc. of the 9th Int. Conf. on Industrial and Engineering Applications of AI and Expert Systems pp. 419-424.

[17]

Liu, H. and Setiono, R.: 1996b, A probabilistic approach to feature selection - a filter solution, in (Saitta, 1996).

[18]

Lundy, M. and Mees, A.: 1986, Convergence of an annealing algorithm, Mathematical programming 34, 111-124.

[19]

Mann, J. W.: 1995, X-SAmson v1.0 user manual, School of Information Systems, University of East Anglia.

[20]

Marill, T. and Green, D. M.: 1963, On the effectiveness of receptors in recognition systems, IEEE Trans. Inform. Theory 9, 11-17.

[21]

Michael, M. and Lin, W. C.: 1973, Experimental study of information measures and inter-intra class distance ratios on feature selection and ordering, IEEE Trans. Systems Man Cybernet. 3, 172-181.

[22]

Narendra, P. M. and Fukunaga, K.: 1977, A branch and bound algorithm for feature subset selection, IEEE Transactions on Computers pp. 917-922.

[23]

Pei, M., Goodman, E. D., Punch, W. F. and Ding, Y.: 1995, Genetic algorithms for classification and feature extraction, Proc. of the Classification Society Conf..

[24]

Quinlan, J. R.: 1983, Learning efficient classification procedures and their application to chess end games, in R. S. Michalski, J. G. Carbonell and T. M. Mitchell (eds), Machine learning: An artificial intelligence approach, Morgan Kaufmann, San Mateo, CA.

[25]

Quinlan, J. R.: 1986, Induction of decision trees, Machine Learning 1.

[26]

Quinlan, J. R.: 1993, C4.5: Programs for Machine Learning, Morgan Kaufmann.

[27]

Rauber, T. W.: 1996, The tooldiag package. Available electronically from: http://www.uninova.pt/~tr/home/tooldiag.html.

[28]

Rayward-Smith, V. J., Debuse, J. C. W. and de la Iglesia, B.: 1995, Using a genetic algorithm to data mine in the financial services sector, in A. Macintosh and C. Cooper (eds), Applications and Innovations in Expert Systems III, SGES Publications, pp. 237-252.

[29]

Saitta, L. (ed.): 1996, Proc. of the 13th Int. Conf. on Machine Learning.

[30]

Shannon, C. E. and Weaver, W.: 1963, The Mathematical Theory of Communication, University of Illinois Press, Urbana.

[31]

Thrun, S., Bala, J., Bloedorn, E., Bratko, I., Cestnik, B., Cheng, J., De Jong, K., Dzeroski, S., Fahlman, S. E., Fisher, D., Hamman, R., Kaufman, K., Keller, I. Kononenko, I., Kreuziger, J., Michalski, R. S., Mitchell, T., Pachowicz, P., Reich, Y., Vafaie, H., Van de Welde, W., Wenzel, W., Wnek, J. and Zhang, J.: 1991, The MONK's problems - a performance comparison of different learning algorithms, Carnegie Mellon University CMU-CS-91-197.

[32]

Weiss, S. M. and Kulikowski, C. A.: 1991, Computer systems that learn : classification and prediction methods from statistics, neural nets, machine learning and expert systems, Morgan Kaufmann, San Francisco.

[33]

Whitney, A.: 1971, A direct method of nonparametric measurement selection, IEEE Trans. Comput. 20, 1100- 1103.

Cited By

View all

Alzaqebah MAhmed EAccelerated fuzzy min–max neural network and arithmetic optimization algorithm for optimizing hyper-boxes and feature selectionNeural Computing and Applications10.1007/s00521-023-09131-636:4(1553-1568)

Zhong CLi GMeng ZLi HHe WA self-adaptive quantum equilibrium optimizer with artificial bee colony for feature selectionComputers in Biology and Medicine10.1016/j.compbiomed.2022.106520153:C

Gao WMa HZhao YWang JTian QModeling Portraits of Students and Exercises for Exercise RecommendationAdvanced Intelligent Computing Technology and Applications10.1007/978-981-99-4752-2_19(226-236)

Show More Cited By

Index Terms

Feature Subset Selection within a Simulated Annealing DataMining Algorithm

Computing methodologies

Artificial intelligence

Computer vision

Computer vision problems

Image segmentation

Video segmentation

Knowledge representation and reasoning

Machine learning

Theory of computation

Logic

Recommendations

Feature salience definition and estimation and its use in feature subset selection

In this paper we describe novel feature subset selection methods, based on the estimation of feature salience i.e. the quantification of the relative importance of individual features, in the presence of other features, for determining the classes of ...

Selecting feature subset via constraint association rules

PAKDD'12: Proceedings of the 16th Pacific-Asia conference on Advances in Knowledge Discovery and Data Mining - Volume Part II

In this paper, a novel feature selection algorithm FEAST is proposed based on association rule mining. The proposed algorithm first mines association rules from a data set; then, it identifies the relevant and interactive feature values with the ...

A novel feature subset selection algorithm based on association rule mining

In this paper, a novel feature selection algorithm FEAST is proposed based on association rule mining. The proposed algorithm first mines association rules from a data set; then, it identifies the relevant and interactive feature values with the ...

Information & Contributors

Information

Published In

92 pages

ISSN:0925-9902

Editors:

Larry Kerschberg

George Mason Univ., Fairfax, VA

,

Zbigniew W. Ras

Univ. of North Carolina/Charlotte

,

Maria Zemankova

Arlington, VA

,

Jiawei Han

Simon Fraser Univ., Burnaby, B.C. Canada

,

Raymond T. Ng

Simon Fraser Univ., Burnaby, B.C. Canada

,

Laks V. S. Lakshmanan

Concordia Univ., Montreal, P.Q., Canada

Issue’s Table of Contents

Copyright © Copyright © 1997 Kluwer Academic Publishers.

Publisher

Kluwer Academic Publishers

United States

Publication History

Published: 01 July 1997

Author Tags

Feature subset selection

data mining

simulated annealing

Qualifiers

Article

Contributors

Other Metrics

Bibliometrics & Citations

Bibliometrics

Article Metrics

16

Total Citations

View Citations

0

Total Downloads

Downloads (Last 12 months)0

Downloads (Last 6 weeks)0

Other Metrics

Citations

Cited By

View all

Alzaqebah MAhmed EAccelerated fuzzy min–max neural network and arithmetic optimization algorithm for optimizing hyper-boxes and feature selectionNeural Computing and Applications10.1007/s00521-023-09131-636:4(1553-1568)

Zhong CLi GMeng ZLi HHe WA self-adaptive quantum equilibrium optimizer with artificial bee colony for feature selectionComputers in Biology and Medicine10.1016/j.compbiomed.2022.106520153:C

Gao WMa HZhao YWang JTian QModeling Portraits of Students and Exercises for Exercise RecommendationAdvanced Intelligent Computing Technology and Applications10.1007/978-981-99-4752-2_19(226-236)

Na GChang HUnsupervised Subspace Extraction via Deep Kernelized ClusteringACM Transactions on Knowledge Discovery from Data10.1145/345908216:1(1-15)

Pourpanah FLim CWang XTan CSeera MShi YA hybrid model of fuzzy min–max and brain storm optimization for feature selection and data classificationNeurocomputing10.1016/j.neucom.2019.01.011333:C(440-451)

Tsai CChang WWang YChen HA high-performance parallel coral reef optimization for data clusteringSoft Computing - A Fusion of Foundations, Methodologies and Applications10.1007/s00500-019-03950-323:19(9327-9340)

Aksakalli VMalekipirbazari MFeature selection via binary simultaneous perturbation stochastic approximationPattern Recognition Letters10.1016/j.patrec.2016.03.00275:C(41-47)

Zarshenas ASuzuki KBinary coordinate ascentKnowledge-Based Systems10.1016/j.knosys.2016.07.026110:C(191-201)

Moradkhani MAmiri AJavaherian MSafari HA hybrid algorithm for feature subset selection in high-dimensional datasets using FICA and IWSSr algorithmApplied Soft Computing10.1016/j.asoc.2015.03.04935:C(123-135)

Diao RShen QNature inspired feature selection meta-heuristicsArtificial Intelligence Review10.1007/s10462-015-9428-844:3(311-340)

Show More Cited By

View Options

View options

Media

Figures

Other

Tables

Share

Share

Share this Publication link

Copied!

Copying failed.

Share on social media

Affiliations

Justin C. W. Debuse

Computing Department, School of Information Systems, University of East Anglia, Norwich NR4 7TJ, UK. E-mail: [email protected], [email protected]

Victor J. Rayward-Smith

Computing Department, School of Information Systems, University of East Anglia, Norwich NR4 7TJ, UK. E-mail: [email protected], [email protected]

Request permissions Authors Info & Affiliations
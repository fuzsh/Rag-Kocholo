{
    "id": "dbpedia_4765_3",
    "rank": 76,
    "data": {
        "url": "https://ardentperf.com/2019/11/08/postgresql-invalid-page-and-checksum-verification-failed/",
        "read_more_link": "",
        "language": "en",
        "title": "PostgreSQL Invalid Page and Checksum Verification Failed",
        "top_image": "https://ardentperf.com/wp-content/uploads/2019/11/pg-page-validity.png",
        "meta_img": "https://ardentperf.com/wp-content/uploads/2019/11/pg-page-validity.png",
        "images": [
            "https://ardentperf.com/wp-content/uploads/2019/11/pg-page-validity.png?w=1024",
            "https://ardentperf.com/wp-content/uploads/2021/09/screen-shot-2021-09-16-at-17.25.17.png?w=824",
            "https://0.gravatar.com/avatar/93ecf4d7ab2b9cdc9865a514c1ebcee0bb0ab13ca4e1e08915f8cfe2ddbc5432?s=60&d=https%3A%2F%2F0.gravatar.com%2Favatar%2Fad516503a11cd5ca435acc9bb6523536%3Fs%3D60&r=G",
            "https://0.gravatar.com/avatar/0d1c2fcc2704b01d02c5c574ead197a13170bdee9967d0f2399cdd5d327173d6?s=40&d=https%3A%2F%2F0.gravatar.com%2Favatar%2Fad516503a11cd5ca435acc9bb6523536%3Fs%3D40&r=G",
            "https://ardentperf.files.wordpress.com/2016/07/jeremy_schneider_emailsig.jpg",
            "https://ardentperf.files.wordpress.com/2014/10/oaktablelogo-small.png",
            "https://ardentperf.files.wordpress.com/2015/11/ocm_odb11gadmin_clr.gif",
            "https://ardentperf.files.wordpress.com/2019/03/ace_alumnus.jpg",
            "https://ardentperf.files.wordpress.com/2014/10/racattack-icon.png",
            "https://secure.gravatar.com/blavatar/09880a1646fd15c40ef4ee91df234a03c8de75726af571cf96cf0a727dfbe04c?s=50&d=https%3A%2F%2Fs2.wp.com%2Fi%2Flogo%2Fwpcom-gray-white.png",
            "https://secure.gravatar.com/blavatar/09880a1646fd15c40ef4ee91df234a03c8de75726af571cf96cf0a727dfbe04c?s=50&d=https%3A%2F%2Fs2.wp.com%2Fi%2Flogo%2Fwpcom-gray-white.png",
            "https://pixel.wp.com/b.gif?v=noscript"
        ],
        "movies": [],
        "keywords": [],
        "meta_keywords": [
            ""
        ],
        "tags": null,
        "authors": [],
        "publish_date": "2019-11-08T00:00:00",
        "summary": "",
        "meta_description": "At the Seattle PostgreSQL User Group meetup this past Tuesday, we got onto the topic of invalid pages in PostgreSQL. It was a fun discussion and it made me realize that it'd be worth writing down a bunch of the stuff we talked about - it might be interesting to a few more people too!…",
        "meta_lang": "en",
        "meta_favicon": "https://secure.gravatar.com/blavatar/09880a1646fd15c40ef4ee91df234a03c8de75726af571cf96cf0a727dfbe04c?s=32",
        "meta_site_name": "Ardent Performance Computing",
        "canonical_link": "https://ardentperf.com/2019/11/08/postgresql-invalid-page-and-checksum-verification-failed/",
        "text": "At the Seattle PostgreSQL User Group meetup this past Tuesday, we got onto the topic of invalid pages in PostgreSQL. It was a fun discussion and it made me realize that it’d be worth writing down a bunch of the stuff we talked about – it might be interesting to a few more people too!\n\nInvalid Page In Block\n\nYou see an error message that looks like this:\n\nERROR: invalid page in block 1226710 of relation base/16750/27244\n\nFirst and foremost – what does this error mean? I like to think of PostgreSQL as having a fairly strong “boundary” between (1) the database itself and (2) the operating system [and by extension everything else… firmware, disks, network, remote storage, etc]. PostgreSQL maintains page validity primarily on the way in and out of its buffer cache.\n\nWhat does this mean in practice? Suppose there’s a physical memory failure and somehow the ECC parity is unable to detect it. This means that a little bit of physical memory on the server now has incorrect garbage and the correct data from that memory is lost.\n\nIf the garbage bytes map to part of the kernel page cache, then when PostgreSQL tries to copy the page into it’s buffer cache then it will (if possible) detect that something is wrong, refuse to poison its buffer cache with this invalid 8k page, and error out any queries that require this page for processing with the ERROR message above.\n\nIf the garbage bytes map to part of PostgreSQL’s database buffer cache, then PostgreSQL will quite happily assume nothing is wrong and attempt to process the data on the page. Results are unpredictable; probably all sorts of ERROR messages, crashes and failure modes could result – or maybe even incorrect data returned with no ERROR message at all. (Note that this is probably the case for nearly all software… and also note that ECC is pretty good.)\n\nHow PostgreSQL Checks Page Validity\n\nPostgreSQL has two main “validity checks” that it performs on pages. You can read the code in the function PageIsVerified() but I’ll summarize here. You can tell from your error message which validity check failed. It depends on whether you see a second additional WARNING right before the ERROR. The warning would look like this:\n\nWARNING: page verification failed, calculated checksum 3482 but expected 32232\n\nIf the above warning is not present, this means the page header failed a basic sanity check. This could conceivably be caused by both problems inside and outside of PostgreSQL.\n\nIf you see the above warning (page verification failed), this means the checksum recorded in the block did not match the checksum calculated for the block. This most likely indicates that there was a problem outside of (underneath) the database – operating system, memory, networking, storage, etc.\n\nAbout Basic Page Header Sanity Checks\n\nAs of when I’m writing this article in 2019, the following basic sanity checks are performed on the page header:\n\nThere are 32 bits reserved for page flag bits; at present only three are used and the other 29 bits should always be zero/off.\n\nEvery page is divided into four parts (header, free space, tuples, special space). Offsets for the divisions are stored as 16-bit numbers in the page header; the offsets should go in order and should not have a value pointing off the page.\n\nThe offset of the special space should always be aligned.\n\nAbout PostgreSQL Checksums\n\nPostgreSQL version 9.3 (released in 2013) added the ability to calculate a checksum on data pages and store the checksum in the page. There are two inputs to the checksum: (1) every single byte of the data page, with zeros in the four bytes where the checksum will be stored later and (2) the page offset/address. This means that PostgreSQL doesn’t just detect if a byte is changed in the page – it also detects if a perfectly valid page gets somehow dropped into the wrong place.\n\nChecksums are not maintained for blocks while they are in the shared buffers – so if you look at a buffer in the PostgreSQL page cache with pageinspect and you see a checksum value, note that it’s probably just leftover from the last read but wouldn’t have been maintained when the page was changed. The checksum is calculated and stamped onto the page when the page is written out of the buffer cache into the operating system page cache (remember the pages get flushed to disk later by a separate fsync call).\n\nThe checksum algorithm is specifically designed to take advantage of SIMD instructions. The slowest part of the algorithm is multiplication, so if possible PostgreSQL will be compiled to perform 32 multiplications at a time in parallel. In 2013 there were two platforms specifically documented to support this: x86 SSE4.1 and ARM NEON. The implementation is specifically tuned for optimal performance on x86 SSE. This is an important point actually – particularly for folks running PostgreSQL on embedded devices – PostgreSQL certainly compiles and works well on a lot of platforms, but evidently the checksum implementation is explicitly optimized to run the fastest on Intel. (To be clear… I think everyone should use checksums on every platform unless you have a really really good reason not to – just don’t be surprised if you start comparing benchmarks with Intel and you see a difference!)\n\nFor folks interested in digging a little more into the history… here’s the original commit using a CRC algorithm which never shipped in an actual PostgreSQL release (Simon Riggs, Jeff Davis and Greg Smith), here’s the subsequent commit introducing the FNV-1a algorithm instead of CRC which is what we still use today (Ants Aasma), and here’s the commit doing the major refactor which moved the algorithm into a header file for easier external use (Tom Lane).\n\nMore Ways To Check Validity\n\nAt the SEAPUG meetup this led to a discussion about checking validity. Do checksums cover everything? (hint: no) Are there more ways we can validate our databases? (hint: yes)\n\nI haven’t yet made a full list of which files are covered by checksums and which ones aren’t, but I know that not everything is. For example: I’m pretty sure that the visiblity map and SLRU files aren’t covered with checksums. But for what it’s worth, there are two extra tools we can use verification in PostgreSQL databases:\n\nThe amcheck extension can scan a B-Tree index for a number of logical problems – for example, verifying that all B-Tree pages have items in “logical” order. (This could be useful, for example, if you’re not specifying ICU collation and you recently upgraded your operating system collation libraries… since PostgreSQL uses OS collation by default.)\n\nThe pg_visibility_map extension includes two functions to check for corruption in the visibility map – pg_check_frozen() and pg_check_visible().\n\nResponding To Checksum Failures\n\nFinally, what if you actually run into a checksum failure? What should you do, and are there any additional tools you should know about?\n\nFirst of all – on any database – there are a few things you should always do immediately when you see something indicating that a data corruption may have occurred:\n\nVerify that your backup retention and your log file retention are sufficiently long – I recommend at least a month (this is a Happiness Hint). You never know how long the investigation might take, or how long ago something important might have happened.\n\nTake a moment to articulate and write out the impact to the business. (Are important queries failing right now? Is this causing an application outage?) This seems small but it can be very useful in making decisions later. Don’t exaggerate the impact but don’t understate it either. It can also be helpful to note important timelines that you already know about. For example: management is willing to use yesterday’s backup and take a day of data loss to avoid an 12 hour outage, but not to avoid a 4 hour outage …or… management needs a status update at 11:00am Eastern Time.\n\nIf there’s a larger team working on the system in question, communicate a freeze on changes until things are resolved.\n\nMake a list or inventory of all copies of the data. Backups, physical replicas or hot standbys, logical replicas, copies, etc. Sometimes the very process of making this list can immediately give you ideas for simple solutions (like checking if a hot standby has the block intact). The next thing you probably want to do is check all items in this list to see if they have a valid copy of the data in question. Do not take any actions to remediate the problem right away, collect all of the information first. The data you collect now might useful during RCA investigation after you’re back online.\n\nIf there was one checksum failure, then you probably want to make sure there aren’t more.\n\nIf it’s a small database, consider whether you can scan the whole thing and verify the checksum on every single block.\n\nIf it’s a large database, consider whether you can at least scan all of the system/catalog tables and perhaps scan the tables which are throwing errors in their entirety. (PostgreSQL stops on the first error, so there isn’t an easy way to check if other blocks in the same table also have checksum problems.)\n\nA few general best practices… (1) have a second person glance at your screen before you execute any actual changes, (2) never delete anything but always rename/move instead, (3) when investigating individual blocks, also look at the block before and the block after to verify they look like what you’d normally expect, (4) test the remediation plan before running it in production, and (5) document everything. If you’ve never seen Paul Vallée’s FIT-ACER checklist then it’s worth reviewing.\n\nThere’s no single prescriptive process for diagnosing the scope of the problem and finding the right path forward for you. It involves learning what you need to know about PostgreSQL, a little creative thinking about possible resolutions, and balancing the needs of the business (for example, how long can you be down and how much data loss can you tolerate).\n\nThat being said, there are a few tools I know about which can be very useful in the process. (And there are probably more that I don’t know about; please let me know and I can add them to this list!)\n\nMore About Data Investigation\n\nIn order to put some of this together, I’ll just do a quick example session. I’m running PostgreSQL 11.5 an on EC2 instance and I used dd to write a few evil bytes into a couple blocks of my database.\n\nFirst, lets start by just capturing the information from the log files:\n\n$ grep \"invalid page\" ../log/postgresql.log|sed 's/UTC.*ERROR//' 2019-10-15 19:53:37 : invalid page in block 0 of relation base/16385/16493 2019-10-16 22:26:30 : invalid page in block 394216 of relation base/16385/16502 2019-10-16 22:43:24 : invalid page in block 394216 of relation base/16385/16502 2019-11-05 23:55:33 : invalid page in block 394216 of relation base/16385/16502 2019-11-05 23:57:58 : invalid page in block 394216 of relation base/16385/16502 2019-11-05 23:59:14 : invalid page in block 262644 of relation base/16385/16502 2019-11-05 23:59:21 : invalid page in block 262644 of relation base/16385/16502 2019-11-05 23:59:22 : invalid page in block 262644 of relation base/16385/16502 2019-11-05 23:59:23 : invalid page in block 262644 of relation base/16385/16502 2019-11-06 00:01:12 : invalid page in block 262644 of relation base/16385/16502 2019-11-06 00:01:16 : invalid page in block 0 of relation base/16385/16493 2019-11-06 00:02:05 : invalid page in block 250 of relation base/16385/16492\n\nWith a little command-line karate we can list each distinct block and see the first time we got an error on that block:\n\n$ grep \"invalid page\" ../log/postgresql.log | sed 's/UTC.*ERROR//' | awk '{print $1\" \"$2\" \"$11\" invalid_8k_block \"$8\" segment \"int($8/131072)\" offset \"($8%131072)}' | sort -k3,5 -k1,2 | uniq -f2 2019-11-06 00:02:05 base/16385/16492 invalid_8k_block 250 segment 0 offset 250 2019-10-15 19:53:37 base/16385/16493 invalid_8k_block 0 segment 0 offset 0 2019-11-05 23:59:14 base/16385/16502 invalid_8k_block 262644 segment 2 offset 500 2019-10-16 22:26:30 base/16385/16502 invalid_8k_block 394216 segment 3 offset 1000\n\nSo we know that there are at least 4 blocks corrupt. Lets scan the whole data directory using Credativ’s pg_checksum (without shutting down the database) to see if there are any more blocks with bad checksums:\n\n$ pg_checksums -D /var/lib/pgsql/11.5/data |& fold -s pg_checksums: error: checksum verification failed in file \"/var/lib/pgsql/11.5/data/base/16385/16502.3\", block 1000: calculated checksum 2ED4 but block contains 4EDF pg_checksums: error: checksum verification failed in file \"/var/lib/pgsql/11.5/data/base/16385/16502.3\", block 1010: calculated checksum 9ECF but block contains ACBE pg_checksums: error: checksum verification failed in file \"/var/lib/pgsql/11.5/data/base/16385/16502.2\", block 500: calculated checksum 5D6 but block contains E459 pg_checksums: error: checksum verification failed in file \"/var/lib/pgsql/11.5/data/base/16385/16493\", block 0: calculated checksum E7E4 but block contains 78F9 pg_checksums: error: checksum verification failed in file \"/var/lib/pgsql/11.5/data/base/16385/16492\", block 250: calculated checksum 44BA but block contains 3ABA Checksum operation completed Files scanned: 1551 Blocks scanned: 624158 Bad checksums: 5 Data checksum version: 1\n\nAh-ha… there was one more bad checksum which didn’t show up in the logs! Next lets choose one of the bad blocks and dump the contents using unix command line tools.\n\n$ dd status=none if=base/16385/16492 bs=8192 count=1 skip=250 | od -A d -t x1z -w16 0000000 00 00 00 00 e0 df 6b b0 ba 3a 04 00 0c 01 80 01 >......k..:......< 0000016 00 20 04 20 00 00 00 00 80 9f f2 00 00 9f f2 00 >. . ............< 0000032 80 9e f2 00 00 9e f2 00 80 9d f2 00 00 9d f2 00 >................< 0000048 80 9c f2 00 00 9c f2 00 80 9b f2 00 00 9b f2 00 >................< 0000064 80 9a f2 00 00 9a f2 00 80 99 f2 00 00 99 f2 00 >................< 0000080 80 98 f2 00 00 98 f2 00 80 97 f2 00 00 97 f2 00 >................<\n\nHere we see the page header and the beginning of the line pointers. One thing I think it’s easy to remember is that the first 8 bytes are the page LSN and the next 2 bytes are the page checksum. Notice that the page checksum bytes contain “ba 3a” which matches the error message from the scan above (3ABA). Sometimes it can be useful to know just the very top of the page even if you don’t remember anything else!\n\nThis is useful, but lets try the pg_filedump utility next. This utility takes a lot of options. In this example I’m going to ask it to verify the checksum (-k), only scan one block at offset 250 (-R 250 250) and even to decode the tuples (table row data) to a human-readable format (-D int,int,int,charN). There’s another argument (-f) that can even tell pg_filedump to show hexdump/od style raw data inline but I won’t demonstrate that here.\n\n$ pg_filedump -k -R 250 250 -D int,int,int,charN base/16385/16492 ******************************************************************* * PostgreSQL File/Block Formatted Dump Utility - Version 11.0 * * File: base/16385/16492 * Options used: -k -R 250 250 -D int,int,int,charN * * Dump created on: Fri Nov 8 21:48:38 2019 ******************************************************************* Block 250 ******************************************************** <Header> ----- Block Offset: 0x001f4000 Offsets: Lower 268 (0x010c) Block: Size 8192 Version 4 Upper 384 (0x0180) LSN: logid 0 recoff 0xb06bdfe0 Special 8192 (0x2000) Items: 61 Free Space: 116 Checksum: 0x3aba Prune XID: 0x00000000 Flags: 0x0004 (ALL_VISIBLE) Length (including item array): 268 Error: checksum failure: calculated 0x44ba. <Data> ------ Item 1 -- Length: 121 Offset: 8064 (0x1f80) Flags: NORMAL COPY: 15251 1 0 Item 2 -- Length: 121 Offset: 7936 (0x1f00) Flags: NORMAL COPY: 15252 1 0 Item 3 -- Length: 121 Offset: 7808 (0x1e80) Flags: NORMAL COPY: 15253 1 0\n\nThat was the block header and the first few item. (Item pointer data is displayed first, then the table row data itself is displayed on the following line after the word COPY.) Looking down a little bit, we can even see where I wrote the bytes “budstuff” into a random location in this block – it turns out those bytes landed in the middle of a character field. This means that without checksums, PostgreSQL would not have thrown any errors at all but just returned an incorrect string the next time that row was queried!\n\nCOPY: 15297 1 0 Item 48 -- Length: 121 Offset: 2048 (0x0800) Flags: NORMAL COPY: 15298 1 0 Item 49 -- Length: 121 Offset: 1920 (0x0780) Flags: NORMAL COPY: 15299 1 0 badstuff Item 50 -- Length: 121 Offset: 1792 (0x0700) Flags: NORMAL COPY: 15300 1 0\n\nIt’s immediately clear how useful this is (and easier to read <g>). The part where it decodes the table row data into a human readable form is an especially cool trick. Two notes about this.\n\nFirst, the lines are prefixed with the word COPY for a reason – they are actually intended to be formatted so you can grep on the word COPY and then use the “copy” command (or it’s psql cousin) to feed the data directly back into a staging table in the database for cleanup. How cool is that!\n\nSecond, it can decode only a set of fairly standard data types and relation types.\n\n$ pg_filedump -h Version 11.0 (for PostgreSQL 11.x) Copyright (c) 2002-2010 Red Hat, Inc. Copyright (c) 2011-2018, PostgreSQL Global Development Group Usage: pg_filedump [-abcdfhikxy] [-R startblock [endblock]] [-D attrlist] [-S blocksize] [-s segsize] [-n segnumber] file Display formatted contents of a PostgreSQL heap/index/control file Defaults are: relative addressing, range of the entire file, block size as listed on block 0 in the file The following options are valid for heap and index files: ... ... ... -D Decode tuples using given comma separated list of types Supported types: bigint bigserial bool char charN date float float4 float8 int json macaddr name oid real serial smallint smallserial text time timestamp timetz uuid varchar varcharN xid xml ~ ignores all attributes left in a tuple\n\nNow you see what happens when I start having fun… a quick update about our SEAPUG meetup this past Tuesday turned into a blog article that’s way too long. :) Hope it’s useful, and as always let me know what I can improve!\n\nUpdate 9/16/21 – It was an oversight that I didn’t link it when I first published this post – but the PostgreSQL community wiki has a page on corruption. That page has a ton of good information and is definitely worth referencing."
    }
}
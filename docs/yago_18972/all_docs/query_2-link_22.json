{
    "id": "yago_18972_2",
    "rank": 22,
    "data": {
        "url": "https://www.ncbi.nlm.nih.gov/pmc/articles/PMC7332290/",
        "read_more_link": "",
        "language": "en",
        "title": "Automated task training and longitudinal monitoring of mouse mesoscale cortical circuits using home cages",
        "top_image": "https://www.ncbi.nlm.nih.gov/corehtml/pmc/pmcgifs/pmc-card-share.jpg?_=0",
        "meta_img": "https://www.ncbi.nlm.nih.gov/corehtml/pmc/pmcgifs/pmc-card-share.jpg?_=0",
        "images": [
            "https://www.ncbi.nlm.nih.gov/coreutils/uswds/img/favicons/favicon-57.png",
            "https://www.ncbi.nlm.nih.gov/coreutils/uswds/img/icon-dot-gov.svg",
            "https://www.ncbi.nlm.nih.gov/coreutils/uswds/img/icon-https.svg",
            "https://www.ncbi.nlm.nih.gov/coreutils/nwds/img/logos/AgencyLogo.svg",
            "https://www.ncbi.nlm.nih.gov/corehtml/pmc/pmcgifs/logo-elife.png",
            "https://www.ncbi.nlm.nih.gov/pmc/articles/PMC7332290/bin/elife-55964-fig1.jpg",
            "https://www.ncbi.nlm.nih.gov/pmc/articles/PMC7332290/bin/elife-55964-fig2.jpg",
            "https://www.ncbi.nlm.nih.gov/pmc/articles/PMC7332290/bin/elife-55964-fig3.jpg",
            "https://www.ncbi.nlm.nih.gov/pmc/articles/PMC7332290/bin/elife-55964-fig3-figsupp1.jpg",
            "https://www.ncbi.nlm.nih.gov/pmc/articles/PMC7332290/bin/elife-55964-fig4.jpg",
            "https://www.ncbi.nlm.nih.gov/pmc/articles/PMC7332290/bin/elife-55964-fig5.jpg",
            "https://www.ncbi.nlm.nih.gov/pmc/articles/PMC7332290/bin/elife-55964-fig6.jpg",
            "https://www.ncbi.nlm.nih.gov/pmc/articles/PMC7332290/bin/elife-55964-fig7.jpg",
            "https://www.ncbi.nlm.nih.gov/pmc/articles/PMC7332290/bin/elife-55964-fig8.jpg",
            "https://www.ncbi.nlm.nih.gov/pmc/articles/PMC7332290/bin/elife-55964-fig8-figsupp1.jpg",
            "https://www.ncbi.nlm.nih.gov/pmc/articles/PMC7332290/bin/elife-55964-fig8-figsupp2.jpg",
            "https://www.ncbi.nlm.nih.gov/pmc/articles/PMC7332290/bin/elife-55964-fig8-figsupp3.jpg",
            "https://www.ncbi.nlm.nih.gov/pmc/articles/PMC7332290/bin/elife-55964-fig8-figsupp4.jpg",
            "https://www.ncbi.nlm.nih.gov/pmc/articles/PMC7332290/bin/elife-55964-fig8-figsupp5.jpg",
            "https://www.ncbi.nlm.nih.gov/pmc/articles/PMC7332290/bin/elife-55964-fig8-figsupp6.jpg",
            "https://www.ncbi.nlm.nih.gov/pmc/articles/PMC7332290/bin/elife-55964-fig8-figsupp7.jpg",
            "https://www.ncbi.nlm.nih.gov/pmc/articles/PMC7332290/bin/elife-55964-fig9.jpg",
            "https://www.ncbi.nlm.nih.gov/pmc/articles/PMC7332290/bin/elife-55964-fig10.jpg"
        ],
        "movies": [],
        "keywords": [],
        "meta_keywords": [
            ""
        ],
        "tags": null,
        "authors": [
            "Timothy H Murphy",
            "Nicholas J Michelson",
            "Jamie D Boyd",
            "Tony Fong",
            "Luis A Bolanos",
            "David Bierbrauer",
            "Teri Siu",
            "Matilde Balbi",
            "Federico Bolanos",
            "Matthieu Vanni"
        ],
        "publish_date": "2020-08-21T00:00:00",
        "summary": "",
        "meta_description": "We report improved automated open-source methodology for head-fixed mesoscale cortical imaging and/or behavioral training of home cage mice using Raspberry Pi-based hardware. Staged partial and probabilistic restraint allows mice to adjust to self-initiated ...",
        "meta_lang": "en",
        "meta_favicon": "https://www.ncbi.nlm.nih.gov/coreutils/nwds/img/favicons/favicon.ico",
        "meta_site_name": "PubMed Central (PMC)",
        "canonical_link": "https://www.ncbi.nlm.nih.gov/pmc/articles/PMC7332290/",
        "text": "Optimizing behavioral training for trial number and success rate\n\nIn mouse home cages combined with mesoscale imaging we have attempted two kinds of behavioral tasks. The first is the simple go-trial or detection task where mice after a waiting period will respond to a vibratory cue by licking. We find that home cage mice are quite good at learning this task and that this can potentially report activity in circuits associated with the cue detection, as well as motor output associated with licking. We’ve also employed more advanced training strategies, including a go- and no-go task. In this case we had more variable rates of performance, in particular home cage mice have trouble with suppressing output during no-go trials. We attribute some of the lack of success in no-go training to be due to the nature of home cage training itself. In our home cage task there is no penalty for an animal doing an incorrect trial other than a short time penalty of a few seconds. Because of the potentially aversive nature of the headfixing experience, we decided to not actively punish animals for wrong responses using air-puffs or extended time-outs. A better task for home cage mice to perform would be a discrimination task with two rewarded outcomes, and an animal would specify them by licking from two different water spouts (Guo et al., 2014). Alternatively, a lever or wheel could be used to support the choice of 2 rewarded outcomes (Burgess et al., 2017). Other ways to motivate mice to do a more difficult task would be to make rewards in some ways more palatable, perhaps adding a choice of regular water versus sugar water. Another scheme might be to limit the total number of trials an animal could perform. With limited trials mice might be more motivated to exhibit a higher success rate rather than the lower rate they exhibit with unlimited time.\n\nThe time headfixed for these 23 good male performers averaged 18+−13 min/day ( , behavioral data from all mice is available in an online database, see methods). A difference over previous work (Murphy et al., 2016) was removing mice from the cage that were poor performers, or insisted on entering the headfixing tube sideways. Other gains in performance might be obtained through mouse specific training criteria. In the current version of the cage software (used for cages 1–5) all animals are advanced through the go and no-go task together. While we report individual mouse results, it was not possible to selectively tailor the task to individual mice. For cage 6 (and the software we will supply going forward) there was the option to advance mice at different rates using animal specific task information. In other work we have done using a home cage lever task in non-headfixed mice, we did have the ability to alter task criteria based on individual animal performance (Silasi et al., 2018; Woodard et al., 2017). Here, if mice had difficulty with a phase of training, it was possible to downgrade them and make the task easier, allowing them get back on track rather than left thirsty and requiring supplemental water. The auto-headfixing cage will benefit from such individualized training criteria and it will be instituted in all future cages. While we discuss various improvements in trial design to lead to more complex tasks and even better animal engagement, the current hardware and software is capable of monitoring mesoscale circuits in a behavioral or movement based context. Emphasis in our lab has been around mesoscale imaging, notably the cage is also an ideal platform for mesoscale circuit control using optogenetics as transcranial windows readily support the optical control of targeted brain regions (Guo et al., 2014; Silasi et al., 2013; Silasi et al., 2016).\n\nWhile we have had success automating headfixing and training, it is clear that some mice are not interested in the system as headfixation is voluntary. In our system we have intentionally kept headfixed time short, no more than 45 s so that mice will have an opportunity to leave the device. However, during these short trials we find that most mice will re-headfix at a short interval <30 s, indicating a pattern associated with drinking, or some degree of acceptance of the task. It is anticipated that there could optimization of headfixed duration as other work in mice (Aoki et al., 2017) did use much longer but fewer blocks of restrained training using a latching mechanism. Other optimization could involve smaller reward sizes (Aoki et al., 2017) as we used a ~ 7–10 μl reward and always gave one task-independent entry reward. In this case we would not expect mice to perform more than 120 successful trials to obtain a daily 1 ml total that is typical for mouse training (Guo et al., 2014). In our case short bouts of headfixation led to mice performing about 100–200 licking task trials a day (males 24/7 training averaged 200 trials/day, females ~ 7 h/day 100 trials). While 100–200 trials/day/mouse is a much lower than some non-head-fixed home cage tasks where 500–2000 trials/mouse/day were observed (Bollu et al., 2019; Silasi et al., 2018), one needs to take into consideration that multiple animals are in the same cage and totals can approach 400–800 trials/day when all mice are considered (see ). Furthermore, in our case by combining the task with mesoscale imaging each trial needs to have at least 3 s of data collected before and after the cue given the relatively slow GCaMP kinetics making these trials longer than some motor tasks that were less than 2 s (Silasi et al., 2018) or evaluated reaching trajectories (Bollu et al., 2019).\n\nIn summary, we present an open-source tool that is capable of training group-house mice for simple detection tasks while performed mesoscale imaging. We provide an extensive supplemental cage assembly guide and all documentation for hardware, software, and electronics and encourage users to further customize the cage to their needs.\n\nStep by step protocol: Auto head fixation: animal care, surgery, training, behavioral task, archiving and analysis. Assemble of a Raspberry Pi-controlled cage is described in the cage assembly addendum (see home cageGuide and )\n\nStep 1. acclimate group-housed male (or female) mice to large number housing\n\nNote the bulk of the data we report here uses male mice, it is expected that females would train in a similar manner as our previous work used all females (Murphy et al., 2016) and training of a small cohort of female mice yielded similar levels of head-fixed training as the males ( ). Select 8 to 10 male or female mice of C57bl6 background that are post-weaning and pre-pubescent to form a single cage. The goal is to pool different litters to have 8 to 10 animals of similar age (within 2 weeks of each other) and sex for pre-acclimation to a double automatic head fixation training cage. These animals live together within a double training cage for a 2–6 week period prior to transfer to the autoheadfixation cage over which time they undergo and recover from surgery for RFID tags (Bolaños et al., 2017) and transcranial windows when they are least 45 days of age (Silasi et al., 2016) (see step 2). Potentially aggressive mice will be noted and removed. Note, we have not removed any mice at the double training cage stage (in the 6 cages we report here), but we nonetheless mention this as a potential safeguard.\n\nStep 2 combined RFID tag and window surgery\n\nOnce acclimated to the double cage, the cohort of 8 to 10 mice undergo a combined surgery for both RFID tag implantation and cranial window installation (typically the 8–10 surgeries are performed over 1–3 days). Over this time, the animals are checked every other day for signs of aggression, wounding or barbering, or poor sealing or infection at the margins of their transcranial windows. We employ previously the published protocol from our group (Silasi et al., 2016) with the modification of installing a flat titanium bar (McMaster Carr for stock titanium #9039K24) of dimensions as described (Murphy et al., 2016) instead of the 4–40 stud bolt. Prior to installation of the bar, the corners are rounded with a file to reduce the chance of a sharp edge catching the walls of the headfixing tube. Cages are configured to use a text message server to message a set of phone numbers when a set duration (for a RFID tag) in the chamber is exceeded in the unlikely, but possible, scenario of a mouse being stranded within the fixation tube. Once the animals have recovered from the surgery (~7–21 days), they are removed as a group from the training cage and placed in the auto-headfixation cage for the start of water restriction.\n\nStep 3 Non-head fixed water spout training within head fixation cage\n\nOnce in the autoheadfixation cage, we remove the water bottle and weigh the mice 24 hr after water restriction. This weight is the baseline for all weight criteria during auto-headfixation training. Any mouse exhibiting weight loss of more than 15% or exhibiting signs of dehydration is supplemented with water and monitored daily for weight, appearance, and behavior (Guo et al., 2014). The cage contains a RFID triggered water spout (tip) placed 65 mm from the head fixation tube opening (measured from the opening to the cage) which dispenses water ‘Entrance Rewards’ upon reading a tag. In this case, it is important to place the water spout and the tag reader so that the RFID tags will be read as the mouse approaches the water spout. In current versions of the cage, we include capacitive lick detection (Ardesch et al., 2017) which provides confirmation that the mouse is indeed licking the water spout and has good spout placement for later headfixed trials. An appropriate starting position for the tag reader is centered 20 mm from the headfixing tube entrance; if ‘Licks’ occur after ‘Exits’ in the text file, the reader is moved further from the cage (deeper into the tunnel) to prevent periodic loss of tag reading. If the water spout is placed too close to the entrance, the mouse will reach the spout before the tag is read and no water will be dispensed; in this case, with lick detection, licks will be observed on the capacitance sensor but no entries. We recommend investigators use video streaming to confirm appropriate placement of the water spout as the mice enter. During the first day of non-headfixed water spout training, we monitor the reading of RFID tags and confirmation of licking in most mice tested (use Python code stimulator class AHF_Stimulator_Rewards). Any mice not entering the task will be given supplemental water (to a maximum of 1 ml/day, unless they are below the cut-off where the amount can be up to 1 ml plus any deficit) to maintain their weight above the specified limit (85% of baseline). If only one or two mice are not reaching the spout it is best to identify them and mark their tails using black lab marker so they can be easily found and supplemented with water outside of the cage with less disturbance of the other mice. In these other mice water intake can be confirmed automatically using lick detection and automatic weighing (Noorshams et al., 2017). Over the next 6–14 days the water spout is moved a total of 40 mm until a reaches a final length of ~7–9 mm from the headfixing chamber back wall or 110 mm from the entrance. At this time first a removable 50 mm and then a 25 mm blocking structure (see CAD parts AHF_HeadStraightener_25 mm.stl and AHF_HeadStraightener_50 mm.stl ) were adding to prevent mice from reaching the waterspout when sideways.\n\nStep 4 beam break training and lick suppression training\n\nOver 7–14 days mice are gradually advanced towards head fixation. For the first 3–8 days mice are trained to move to the end of the chamber with their bar on the overhead rails breaking the IR beam and eliciting the mesoscale brain imaging excitation LEDs to activate (use Python code stimulator class AHF_Stimulator_Rewards). Here mice are given rewards every 5 s for merely breaking the beam and will learn to associate the blue brain imaging LEDs with greater rewards (at this point headfixing probability is at 0). When the mice are going to the tube end the tag reader should be moved so its center is 40 mm from the headfixing tube entrance as the mice advance and the 25 mm blocker (CAD file) used instead of the 50 mm version. When most mice reach this point (typically within 2–4 days of beam breaking) we begin to train mice to suppress repetitive licking which is the first step in task training. Repetitive licking is flagged by an audible tone: with intervals between licks of less than <1 s met by the tone (use Python code stimulator class AHF_Stimulator_LickWitholdSpeaker). Over 2–4 days mice will learn to suppress repetitive licking creating 1–2 s periods without touching the water spout that are then rewarded by water delivery. At this point in training mice are given rewards regardless of subsequent lick timing (they only need to learn to pause licking) and a vibratory cue that marks the end of the no licking period (and later used for go and no-go training) is also introduced. During this time the water sprout is continually manually retracted (and secured with hot melt glue) and its length is measured from the far wall of the head fixation chamber. The final point for water spout placement is 7–8 mm from the back wall and will make it very likely that the mouse will break the infrared beam triggering head fixation (or a no-fix trial) during spout licking. Lick suppression was begun before loose headfixing in cages 3–5, while it was added after in cages 1 and 2.\n\nStep 5 gradual head fixation task training\n\nThe servo motor head fixation system has the ability to partially restrain mice allowing significant movement of their head and movement forward and backwards (15–20 mm) within the head fixation tube. Initially, this loose restraint is given with a random probability of 50%. Within a 7 day period, the travel of the servo head fixation device is gradually increased until the mouse is tightly head fixed at the end of this period (adjust servo travel using the hardware tester found in the main code interface). Adjustments of the servo position should made in small increments as it is possilbe to damage the system by over-travel due to over-heating of the motor or the mechanical force itself. During both partial and full head fixation trials, as well as non-head fixed trials (where the mouse still receives water rewards), we cue blue LED lights which are required for mesoscale imaging at the point of fixation and during the entire trial during in mesoscale GCaMP imaging. These blue lights help the mouse associate the head fixation trial and task with the availability of water. Initially, mice are given rewards for merely triggering a head fixed imaging session. In this case water is dispensed for a 100–500 ms period at regular intervals (every 5–10 s). These initial trials are relatively short in duration being 15–35 s in length. At this initial time mice are not required to do a complex behavior task, but are merely rewarded regularly for being either loose or tightly head fixed. Mice are also rewarded for breaking the infrared beams that trigger the head fixation process during no-fix (non-headfixed) trials. It is important to view the mice on streaming video and to make sure that the water spout is positioned such that they come far enough forward to break the infrared beam. The position of the mouse tongue with respect to the spout needs to be inspected while head fixed to make sure that there are no mice whose head bar placement prevents them from obtaining water when restrained (nose is angled up or down). Such animals need to be removed, or have their bar repositioned. Of note, the infrared beam break can interfere with making behavioral videos using infrared sensitive CMOS cameras. Therefore, we have included electronics that automatically turn off the infrared beam once the mouse undergoes head fixation.\n\nStep 6 head fixed task training: lick suppression/cued licking\n\nOnce mice are exhibiting significant rates of head fixation of (typically >10 min total/day/mouse) we will advance them to a task which can be performed while head fixed. While our initial goal has been to optimize the training for head fixation, we have also addressed whether mice can be trained for a task in the context of the potential stress that the headfixation may bring. A simple detection task was developed to permit the collection of cortical calcium dynamics. A common occurrence is that once head fixed, mice will lick the water spout in expectation of water rewards. As described in Step 4 we train to suppress over-licking and provide auditory feedback to discourage high rates of spout licking and do not present stimuli until a lick suppression period is met. To signal the end of the lick suppression period and that licking will now lead to water delivery we add a vibration cue at this point and reward licking that occurs after at least 200-300 ms after the cue (this is termed the delay period) and before the reward dispensing time. Water rewards are dispensed 1.25–1.5 s after the response withholding interval plus the cue response window (for a 1.25 s water delivery add response period of 0.3 s so reward at 1.55 s) (use Python code stimulator class AHF_Stimulator_LickNoLickDisc).\n\nStep 7 head fixed training go and No-go task with detection task\n\nOver a period of several weeks animals are advanced into a task once they learn to suppress continuous licking and they are trained to respond to a cue that indicates the end of the lick lockout period (go-trial). We have used a small vibration (that can vary in frequency) of the headfixing tube as the cue for both go and no-go trials. After the cue mice are initially rewarded with water and in a subsequent step are required to lick the water spout after a delay period to receive a reward (as in step 6), for this correct response the software awards a +2 code. We find that head fixed mice have difficulty initially suppressing licking and will lick too early in response to the cue which gives a −4 code or will miss the cue all together −2 code ( ). Training mice to delay before responding to the cue is important for future development of a choice task where mice discriminate two related sensory stimuli (in our case we employ two types of vibration a continuous 500 ms or three 200 ms vibrations 100 ms apart). We find that over a period of days we can gradually lengthen this delay period from an initial 300 ms to over 1 s. Correspondingly, we lengthen the period over which mice must suppress licking from 1 s to several seconds (pre-cue period). We also add randomization (+−0.5 s) to the lick suppression interval, so that mice do not learn to rely on internal timing. Over this time, all animals are monitored and the cage is progressed as a group in delay period interval over time. Typically, small increases in delay period are made daily 50–100 ms. The software which runs autoheadfixation reports a series of text codes which indicate whether the mouse licked in response to the cue or not (−2, +2, or −4 described above for Go trials). Once this training is going well a no-go trial is initiated with a different, but related stimulus (pulsed versus continuous vibration). Here the correct response is to withhold licking over the response interval which gives a code of +1, licking during correct time interval (after the stimulus) but during a no-go trial is an error and receives a code of −1, if the mouse licks too early during a no-go trial this a −3 code error ( ).\n\nStep 8 on-going daily data acquisition\n\nMice are manually weighed as automatic weighing (Noorshams et al., 2017), while feasible was not used as the mice were each manually inspected and weighed. Animals are also inspected for any health changes, including ruffled fur, wounding, or signs of dehydration or trauma. Typically, once animals have learned the task, those which headfix require very little supplemental water, whereas non-headfixing animals require daily supplementation of 0.5–1 ml or need to be removed from the cage. We generally find that animals which fail to head fix 2 weeks after the initiation of head fixed training are unlikely to change and adopt headfixation. Secure file transfer protocol (sftp) is used to download text files made daily for each cage, both a detailed record of all events with time stamps and tag numbers, and a brief total for each mouse of head fixes, rewards given, and task performance.\n\nStep 9 data analysis\n\nEach day automated, cage-based mesoscale imaging can generate an average 15 min of data/mouse of head fixed mesoscale data acquisition. Taking advantage of the nocturnal nature of mice, we typically back up data during the light cycle. It is important to note that Raspberry Pi external USB mechanical hard disks can drop frames if a hard disk is simultaneously writing new files and being backed up. Using Matlab, we import text files which indicate behavioral performance and licking along with RFID tag and time stamps and use them to parse raw file video of brain activity or H264 files of behavior. Each image file is saved with a timestamp and the RFID tag in the filename. An important step in data analysis is to use a common timing signal that can be viewed in all data streams: text file, behavioral video, and brain video. Typically, we achieve this by making an LED light turn on and off signal. Between lights on and off, we establish the time base for the various camera or text file inputs, this event is also printed within the text file as BrainLEDON and BrainLEDOFF.\n\nIn the interests of transparency, eLife publishes the most substantive revision requests and the accompanying author responses.\n\nAcceptance summary:\n\nMurphy and co-workers have advanced the field of high throughput imaging coupled with behavior to a new level of automation – demonstrating around 2,000 self-initiated imaging sessions per day per mouse for all mice in a colony- that incorporates an online SQL database to segue with the ambitions of a multitude of community-wide efforts in data sharing. This work significantly extends the 2013 pioneering effort of the Brody and Tank collaboration, as well as prior work by the Benucci, Goldberg and Murphy laboratories, among others, to create effective, efficient, and open source tools for longitudinal studies of the neuronal basis of behavior.\n\nDecision letter after peer review:\n\n[Editors’ note: the authors submitted for reconsideration following the decision after peer review. What follows is the decision letter after the first round of review.]\n\nThank you for submitting your work entitled \"Automated task training and longitudinal monitoring of mouse mesoscale cortical circuits using home cages\" for consideration by eLife. Your article has been reviewed by three peer reviewers, and the evaluation has been overseen by a Reviewing Editor and a Senior Editor. The following individual involved in review of your submission has agreed to reveal their identity: James Ackman (Reviewer #3).\n\nOur decision has been reached after consultation between the reviewers. Based on these discussions and the individual reviews below, we regret to inform you that your work will not be considered further for publication in eLife.\n\nAll three reviewers of your \"Tools and Resources\" article, as well as the Reviewing Editor, are serious players in the field of cortical dynamics and imaging from awake animals. They and I recognize the continued impact of your work on automated behavior and imaging with modestly priced equipment to promote high throughput experiments and enable adventurous queries. Yet the content in the current paper, which includes technical advances and demonstration imaging data, is judged to be too incremental to warrant publication. The claims will gain in impact if the increase in throughput is used to solve a narrow but nontrivial scientific issue, or make a novel kind of observation, as opposed to making a solely demonstration measurement.\n\nWe kept in mind the criteria for the Tools and Resources category in reaching this decision. This category highlights tools or resources that are especially important for their respective fields and have the potential to accelerate discovery. Tools and Resources articles do not have to report major new biological insights or mechanisms, but it must be clear that they will enable such advances to take place. The concensus was that, while there are many laudable aspects to your approach, the current approach is not a sufficiently large advance over the previous approach to warrant publication in eLife.\n\nWe would welcome a new research article that involves an application of this new tool. The Reviewing Editor provided some suggestions, but of course, we recognize that you are already engaged in a vigorous research program that might encompass these or serve as an alternative. I pass them along for your consideration.\n\n1) An expanded imaging study during the Go/NoGo task, i.e., the use of many mice to establish cortical patterns instead of the one mouse as shown in Figures 8 and 9.\n\n2) Activation of the orofacial regions (forepaw, jaw, tongue, vibrissa) in motor and sensory cortices are certainly of interest.\n\n3) An imaging study from multi-animals study that looks at potential gender differences, with reasonable statistical bounds and controls, could fulfil this role.\n\n4) Other extensions, e.g., plasticity of cortical responses during learning, may be more feasible.\n\nReviewer #1:\n\nIn this \"Tools and Resources\" article, the authors report on an improved methodology for doing widefield imaging in the home cage. The opportunity to measure behavior and neural activity is important, and the authors highlight reasons such as reduced stress for the animal, and removal of experimenter-to-experimenter variability. Further, the chance for automated fixation in conjunction with widefield imaging is exciting, as widefield imaging is turning out to be an informative and high-throughput way to measure neural activity during behavior.\n\nThe method proposed here is an improvement over a previous automated head-fixing apparatus that the authors reported a few years ago. Here, the mice are willing to have daily fixation durations that are much longer. In about half of the mice, the observed 28+/-17 headfixations/day (18+/-13 mins/day).\n\nThe paper has a lot of strengths. It includes data from 44 mice and a total of >29K task related videos. The authors also created a relational database to allow analysis pipelining. This information, along with a lot of other useful information, such as the drawings for the head straightener that prevents the mouse from going in sideways, are all provided. There are also a lot of nice touches in the system, such as the text messaging system that alerts the experimenter if animals are in the tube too long; this is thoughtful from the point of view of animal welfare.\n\nThere were also a number of observations based on this large-scale approach that were interesting. These include the linked behaviors between mice (Figure 4A, B) and the fact that mice had higher success rates while headfixed although they preferred to be head free (Figure 5C). The fact that performance was similar over the 24-hour cycle was also interesting.\n\nFinally, to deal with hemodynamic contamination of the widefield data, the authors captured 440 nm light in addition to the GCaMP signal. They then subtracted one from the other. The authors acknowledged the really beautiful alternative methods for this developed by the Hillman lab, but justified their use of a simpler method based on some constraints of the device, which I thought was fine. Other papers in the literature have entirely omitted hemodynamic correction so I appreciated the efforts to do this.\n\nFirst, I was concerned that the authors didn't make sufficiently clear how this work is an advance on the Aoki, Benucci et al. approach that was published in 2017. The text states that, \"While an advance for training, this work did not longitudinally gather brain imaging data in an autonomous manner, nor were the systems of a footprint or cost appropriate for running at scale.\" But in the Aoki paper, 2-photon data was collected for 21 days (Figure 5D). The measurements were not taken in the home cage; if the authors feel that is a critical difference, they should state why. I also wasn't sure how to compare the cost difference with that approach and the one presented here. The Aoki apparatus does appear to be commercially available (http://ohara-time.co.jp/wordpress/wp-content/uploads/SelfHead-restraintOperantTestSystem_Pamphlet2017.pdf), but I wasn't sure of its price or the total price of the setup in the current paper so it was hard to compare.\n\nSecond, the authors included no female mice in their study. There seems to be no reason that a cage of all female mice might be tested as well. Inclusion of both sexes in a study is now required by many funding agencies, and is good scientific practice because it ensures that scientific conclusions are not made that only apply to one sex and not the other.\n\nThird, I wasn't sure whether the total fixation time of the mice provided sufficient time for measurement of behavior and neural activity. It seemed that the total fixation time wasn't very long – on order of about 18±5 minutes/day (subsection “Improvements to the cage hardware”) for the good performers. It would be helpful to see the distribution of total daily head-fix times for all mice, as well as the distribution of within-day times, at least for some of the good mice. It would also be helpful to see the distribution of total daily completed trials. The Bollu paper (which admittedly isn't a totally fair comparison) reported >2000 trials/day and the Aoki paper reported ~1000 trials/day. The reason I bring this up is that widefield measurements can be very noisy and so high trial counts are extremely helpful. This is especially true given that, as the authors point out, the signals reflect movements, the timing and magnitude of which vary a lot from trial-to-trial. If the approach in this paper is to be useful for sensory or cognitive tasks, large trial counts are needed.\n\nA related point, I couldn't quite understand the sentence comparing good and bad performers (see the aforementioned subsection). It stated that bad performers had less than 20 min of head-fixed time while good performers had while \"good performers averaged 15.8+-19.9 h.\" I think h can't mean hours since the animals didn't headfix 16 hours/day. But it can't mean minutes either, because that would imply that the good performers were fixating less than the poor performers.\n\nFinally, I found that the behavioral/widefield data included at the end (Figure 8 and the section titled, \"Task training error analysis examples\") was not very useful. The problem is that it is hard to conclude very much from such a small sample size. There is only 1 animal shown in Figure 8 and so the comparison between go and no-go trials (Figure 8B vs. Figure 8C) is not that informative. Further, the data didn't really add that much to the paper. The reason is that this paper is meant to be a tool and a resource. It does that well. The scientific observations in Figure 8 weren't really in the service of that goal. It might be better to include that (potentially interesting) data in a different paper with a larger cohort of subjects and a lot more analyses.\n\nReviewer #2:\n\nAchieving high-throughput behavioral training and subsequent automated task administration with physiology is a significant aim for the field of systems neuroscience. The Murphy lab and others have made notable contributions to this aim, which are described in the Introduction to this manuscript. The Murphy group, specifically, has previously reported automated methods for home cage behavioral training and for head-fixation with mesoscopic calcium imaging (in the absence of a specific behavioral paradigm). Here, they describe improvements on their previous head-fixation and imaging method and integration with behavioral training and task performance.\n\nSmall improvements in the design of the head-fixation system and refinement of their training protocol yield longer and more frequent head-fixations. Using their updated system, they successfully train 21/44 mice to undergo head-fixation to obtain water rewards. While head-fixed, the mice are trained to perform a detection task with varying degrees of success. Further, 10 mice progressed to a go/no-go task, but only 5 of those mice were able to perform the task (as assessed by a relatively lenient d' criterion), and there is no detailed presentation of this behavioral data in the manuscript. The mesoscopic imaging data presented reflects examples from a few selected mice without any analysis of results across mice, and the major benefits of the system (longitudinal mesoscopic imaging across training) are completely unexplored.\n\nThe challenge with this manuscript is that it does not present a compelling methodological result, nor does it describe a compelling experimental finding (it's neither fish nor fowl). The methodology is very promising and the authors should be applauded for the impressive level of detail with which they describe the construction of their system, however the methodology as presented appears to be an incremental improvement on the Murphy lab's previously published results (Murphy et al., 2016), and it's very difficult to tell how much of an improvement it actually represents. The first sentence of the Abstract notes a '4X improved automated methodology', but 4X of what? On the other hand, if the manuscript were focused on an experimental finding or data set, it falls short because of the rather incomplete data (one mouse in many experimental groups, little group level or longitudinal analysis), and further the somewhat opaque manner in which data are presented make it difficult to assess the results and compare to previous work by this group and others.\n\n1) The criterions employed, if any, to move mice between training stages are not described, hampering the reproducibility of the study.\n\n2) It is unclear how the length of individual head-fixations is determined. Can mice exit trials at any time? Further, there are no behavioral measures (other than repeated, \"clustered\" head-fixation) that indicate whether mice are comfortable in the head-fixation system. A lack of comfort could underlie the poor performance of many of the mice in the detection task.\n\n3) The study pools data from several iterations of the home cage system with different conditions across the cages. Further, cage 3 experienced an unexplained outage. This makes the presentation of data in Figure 3 confusing and difficult to interpret. Clarity would be greatly improved by reporting results for a group of cages with consistent and optimal conditions. At the very least, data should be separated clearly by mouse to make it easier to assess changes in performance. Further, the 23 mice that perform well in training should be presented separately from those that did not progress. Additionally, given that the training periods were of variable length, it is not possible to assess the data (as currently presented) relative to the different training milestones.\n\n4) The inclusion of data from non-head-fixed trials detracts from the manuscript. Given that the system is designed for imaging during behavior (which necessitates head-fixation), only data for head-fixed trials should be presented. There is no comparison of behavioral or imaging data collected with automated (infrequent, short-bout) head-fixation and extended manual head-fixation for comparison.\n\n5) Group-level and longitudinal analysis of behavioral data from the detection task (which should have sufficient N), would strengthen the manuscript. The inclusion of limited go/no-go behavioral data detracts from the manuscript, as it does not appear that the current training regimen has been optimized for performance of this task.\n\n6) In section 1, the authors claim that use of a Raspberry Pi camera module is not inferior, but they do not present any data to support this claim.\n\n7) The mesoscopic imaging presented is sufficient to demonstrate that imaging can be performed in their home cage system. However, that was already established by their previously published manuscript (Murphy et al., 2016). At the very least, group level or longitudinal analysis should be performed to facilitate comparison with results published by other groups with conventional imaging paradigms.\n\nReviewer #3:\n\nThe authors improved upon a home cage system they reported on previously in Murphy et al., 2016. Docking system was improved for greater mouse participation, with a more effective training period. Additional monitoring of animal's behavioral state was added. Functional data is provided for a go/no-go licking task.\n\nAll materials are provided, along with schematics and acquisition code, and are a benefit to the greater research community. Automated longitudinal population studies are important for furthering our understanding of the neural basis of behavior.\n\n1) Much of this work has already been published (Murphy et al., 2016). Details provided here are improvements upon the original methods, rather than new findings or techniques.\n\n2) Home cage system is described to be optimized for capturing long-term functional changes in cortex, yet the data showed no functional changes after training. Described results did not elucidate benefits of using this system.\n\n[Editors’ note: the authors resubmitted a revised version of the paper for consideration. What follows is the authors’ response to the first round of review.]\n\nReviewer #1:\n\n[…] First, I was concerned that the authors didn't make sufficiently clear how this work is an advance on the Aoki, Benucci et al. approach that was published in 2017. The text states that, \"While an advance for training, this work did not longitudinally gather brain imaging data in an autonomous manner, nor were the systems of a footprint or cost appropriate for running at scale.\" But in the Aoki paper, 2-photon data was collected for 21 days (Figure 5D). The measurements were not taken in the home cage; if the authors feel that is a critical difference, they should state why. I also wasn't sure how to compare the cost difference with that approach and the one presented here. The Aoki apparatus does appear to be commercially available (http://ohara-time.co.jp/wordpress/wp-content/uploads/SelfHead-restraintOperantTestSystem_Pamphlet2017.pdf), but I wasn't sure of its price or the total price of the setup in the current paper so it was hard to compare.\n\nWe have revised the statement around the Aoki paper and no longer mention cost as an issue as our system is open source and all parts are listed. “Automation has been extended to complex headfixed visual tasks with a relatively good success rate in mice (Aoki et al., 2017) and headfixed rats (Scott et al., 2013). […] Such supervised imaging following home cage training of individual animals will permit high-resolution microscopy in the context of extended automated training (Aoki et al., 2017).” The device sold by Ohara-Time is $32,000 USD/cage based a quote I just received, typically only 2 mice train in one of these cages at a time (not up to 8).\n\nAutomation of behavioral and imaging experiments is of keen interest to many in the field and has distinct advantages from an animal welfare standpoint as it may reduce variability leading to the use of fewer animals. We have produced a paper which describes an improved system incorporating many new features (over published work): including an online SQL database, lick detection, auto weight measurement, triggered behavioral cameras, and integration of mesoscale imaging during behavioral task. These features were not present in the original version. Furthermore, published work by others does not support multiple animals within the same cage identified by RFID. There is also little published data on headfixation across mice statistics (now at n=52 mice for our work) in the previous Aoki et al., 2017 work, only that it had been applied, but no daily headfixing records or stats on uptake for specific mice.\n\nSecond, the authors included no female mice in their study. There seems to be no reason that a cage of all female mice might be tested as well. Inclusion of both sexes in a study is now required by many funding agencies, and is good scientific practice because it ensures that scientific conclusions are not made that only apply to one sex and not the other.\n\nWe now include a cohort of 8 female mice that are presented as Figure 3—figure supplement 1. These mice were successfully trained in the lab during daily 8 h unsupervised sessions (rather than 24/7 animal facility training). Data from these mice are now in group mesoscale GCAMP analysis in figures 9 and 10.\n\nFrom Materials and methods, “Step 1. Acclimate group-housed male (or female) mice to large number housing; note the bulk of the data we report here uses male mice, it is expected that females would train in a similar manner as our previous work used all females (Murphy et al., 2016) and training of a small cohort of female mice yielded similar levels of head-fixed training as the males (Figure 3—figure supplement 1).” In total we show data from a total of 52 mice that went through this procedure and include the SQL database online with all behavioral data available. Female mice show similar levels of engagement.\n\nThird, I wasn't sure whether the total fixation time of the mice provided sufficient time for measurement of behavior and neural activity. It seemed that the total fixation time wasn't very long – on order of about 18±5 minutes/day (subsection “Improvements to the cage hardware”) for the good performers. It would be helpful to see the distribution of total daily head-fix times for all mice, as well as the distribution of within-day times, at least for some of the good mice. It would also be helpful to see the distribution of total daily completed trials. The Bollu paper (which admittedly isn't a totally fair comparison) reported >2000 trials/day and the Aoki paper reported ~1000 trials/day. The reason I bring this up is that widefield measurements can be very noisy and so high trial counts are extremely helpful. This is especially true given that, as the authors point out, the signals reflect movements, the timing and magnitude of which vary a lot from trial-to-trial. If the approach in this paper is to be useful for sensory or cognitive tasks, large trial counts are needed.\n\nA related point, I couldn't quite understand the sentence comparing good and bad performers (see the aforementioned subsection). It stated that bad performers had less than 20 min of head-fixed time while good performers had while \"good performers averaged 15.8+-19.9 h.\" I think h can't mean hours since the animals didn't head fix 16 hours/day. But it can't mean minutes either, because that would imply that the good performers were fixating less than the poor performers.\n\nWe now provide the successful trial number for all mice in Table 1 for the last 5\n\ndays of training. Typically, 5-6 trials are done in a 40 sec session of headfixation so on average mice were presented with about 100 trials/day. The average trials complete per day is now added in a new column in Table 1. We also have added discussion of trial number, see below.\n\nWhile we appreciate the elegant design of the Aoki et al., 2017, there is only data shown on n=2 mice in that paper (there is mention that 8 of 12 mice learned the visual task) and no detailed presentation of trial number data for individual mice could be found.\n\nWe have also clarified the statement about total time of headfixation. The number in hours was the summed time headfixed over all sessions. “In the current cage the cutoff for a poor performer was <20 min of total headfixation time, while good performers averaged 15.8+-19.9 h (summed time over all daily sessions).”\n\nWe have added the following discussion of headfixed trial number “While we have had success automating headfixing and training, it is clear that some mice are not interested in the system as headfixation is voluntary. […] Furthermore, in our case by combining the task with mesoscale imaging each trial needs to have at least 3 s of data collected before and after the cue given the relatively slow GCaMP kinetics making these trials longer than some motor tasks that were less than 2 s (Silasi et al., 2018) or evaluated reaching trajectories (Bollu et al., 2019)”\n\nFinally, I found that the behavioral/widefield data included at the end (Figure 8 and the section titled, \"Task training error analysis examples\") was not very useful. The problem is that it is hard to conclude very much from such a small sample size. There is only 1 animal shown in Figure 8 and so the comparison between go and no-go trials (Figure 8B vs. Figure 8C) is not that informative. Further, the data didn't really add that much to the paper. The reason is that this paper is meant to be a tool and a resource. It does that well. The scientific observations in Figure 8 weren't really in the service of that goal. It might be better to include that (potentially interesting) data in a different paper with a larger cohort of subjects and a lot more analyses.\n\nThere seems to be conflicting responses from the editor and the reviewers since we\n\nunderstand that the paper was likely rejected as there was not clear group data showing a finding that was supported by mesoscale imaging. The intention of the paper was to: introduce a cage with additional features, detailed instructions on how to build it, describe the training of animals, and show example data to prove that it works and can be used to derive some reported task-dependent activity. We have removed Figure 9 to Figure 8—figure supplement 1 as this was showing a single mouse at 2 training time points and have now add 2 new figures with group data from 13 mice total for the Go detection task.\n\nThe new group data figures show that different mice have unique regional patterns of cortical activation during this task and show strong correlations between movements and mesoscale GCAMP signals (Figure 9, subsection “Go and No-go task headfixed and non-headfixed task acquisition”). Also, consistent with recent data is the finding that individual mice have different strategies for task completion. This work also consistent with findings from the Helmchen lab (Gilad et al., 2018). In total this new group analysis of functional imaging data suggests that our home cage is a well-characterized system that can produce results linking mesoscale networks to behavior. The mesoscale GCAMP signals are also used to predict single trial-outcome using a model-based regression approach (Figure 10). Furthermore, the work provides information on the relative contribution of different cortical areas to the predictive value in the regression model extending very recent findings from multiple labs to the context of the home cage (Figure 10). We have also provided group data on the relationship between body movements and mesoscale brain activity. Here, consistent with other findings, there is a strong correlation between movement and mesoscale activity.\n\nReviewer #2:\n\n[…] The challenge with this manuscript is that it does not present a compelling methodological result, nor does it describe a compelling experimental finding (it's neither fish nor fowl). The methodology is very promising and the authors should be applauded for the impressive level of detail with which they describe the construction of their system, however the methodology as presented appears to be an incremental improvement on the Murphy lab's previously published results (Murphy et al., 2016), and it's very difficult to tell how much of an improvement it actually represents. The first sentence of the Abstract notes a '4X improved automated methodology', but 4X of what? On the other hand, if the manuscript were focused on an experimental finding or data set, it falls short because of the rather incomplete data (one mouse in many experimental groups, little group level or longitudinal analysis), and further the somewhat opaque manner in which data are presented make it difficult to assess the results and compare to previous work by this group and others.\n\nWe have removed the statement of a 4x improvement in headfixation from the Abstract and now present this argument in the Results only.\n\n“The time headfixed for these 23 good performers averaged 18+-13 min/day (Table 1, behavioral data from all mice is available in an online database, see Materials and methods). […] Assuming a session length of ~40 sec, which was the typical range in the previous system (Murphy et al., 2016), taking the top 50% of headfixing mice would result in an average of 6.5+-4 min of daily data acquisition or training per mouse using the older design.”\n\n1) The criterions employed, if any, to move mice between training stages are not described, hampering the reproducibility of the study.\n\nAdmittedly, the system was still under development during the testing of these 44 (plus 8 female) mice so we did not have strict criteria for advancement between stages. In general, we waited for the majority of mice to acquire a particular level i.e. entries into the fixation tube and drinking before advancement. The first version of the software used for cages 1-5 and Murphy et al., 2016 required that all mice be subjected to the same conditions as a group i.e. headfixed or not, or advance to Go and No-Go. The training protocol for cages 4-6 is largely invariant. For the last cohort of female mice we did develop software so that each mouse can have individual training parameters that will allow for future use of automated advancement as we have done in another home cage task with an albeit less complex lever task (Silasi et al., 2018).\n\n2) It is unclear how the length of individual head-fixations is determined. Can mice exit trials at any time? Further, there are no behavioral measures (other than repeated, \"clustered\" head-fixation) that indicate whether mice are comfortable in the head-fixation system. A lack of comfort could underlie the poor performance of many of the mice in the detection task.\n\nThere was no mechanism to automatically release mice using force sensing as all trials were short in duration. Trial length was set by the investigator and generally fixed in the range of no more than 45 sec of headfixation. The trial lengths did vary as we added extra time to allow mice to finish a trial once they started.\n\nFrom the Discussion section “While we have had success automating headfixing and training, it is clear that some mice are not interested in the system as headfixation is voluntary. […]Furthermore, in our case by combining the task with mesoscale imaging each trial needs to have at least 3 s of data collected before and after the cue given the relatively slow GCAMP kinetics making these trials longer than some motor tasks that were less than 2 s (Silasi et al., 2018) or evaluated reaching trajectories (Bollu et al., 2019).”\n\n3) The study pools data from several iterations of the home cage system with different conditions across the cages. Further, cage 3 experienced an unexplained outage. This makes the presentation of data in Figure 3 confusing and difficult to interpret. Clarity would be greatly improved by reporting results for a group of cages with consistent and optimal conditions. At the very least, data should be separated clearly by mouse to make it easier to assess changes in performance. Further, the 23 mice that perform well in training should be presented separately from those that did not progress. Additionally, given that the training periods were of variable length, it is not possible to assess the data (as currently presented) relative to the different training milestones.\n\nWithin Figure 3 we show data from all cages with a legend to indicate cage ID and tag number, we also include a database of all results. We also add a new cohort of female mice and do plot their cage separately in a new supplementary figure, Figure 3—figure supplement 1. We also stress that we include a full SQL database of all events allowing individual mice to be examined. All behavioral data (licking) and functional imaging data is from the 23 mice that performed well and an additional 5 female mice. In the revised group data Figure 9 we now show aggregate as well as individual mouse data and cages data relating body movements to GCAMP signals.\n\n4) The inclusion of data from non-head-fixed trials detracts from the manuscript. Given that the system is designed for imaging during behavior (which necessitates head-fixation), only data for head-fixed trials should be presented. There is no comparison of behavioral or imaging data collected with automated (infrequent, short-bout) head-fixation and extended manual head-fixation for comparison.\n\nWe agree that it would be interesting to compare automatically headfixed and extended manual headfixation. However, this was not our intention. Any data regarding the performance in headfixed and non-headfixed cases was included as it might aid someone attempting this work in the future. We should stress that all analysis of mesoscale GCAMP dynamics and task performance is done using fully headfixed trials. We did not use non-headfixed trials in behavioral measures that are presented in figures (such as licking data in Figure 5), other than the comparison of task performance in non-headfixed states.\n\n5) Group-level and longitudinal analysis of behavioral data from the detection task (which should have sufficient N), would strengthen the manuscript. The inclusion of limited go/no-go behavioral data detracts from the manuscript, as it does not appear that the current training regimen has been optimized for performance of this task.\n\nWe now add group level analysis in Figures 9 and 10 see these new Figures and the Results section This additional analysis is described below in the response to point 7.\n\n“Brain-behavior correlation\n\nZ-scored calcium activity from HL and ALM cortical areas were averaged across trials for each of 13 mice during successful GO trials (Figure 9A; we pooled data from all animals that had at least 20 trials[…] Scrambling brain-behavior pairs across mice resulted in a significant decrease in brain behavior correlation in HL and ALM regions, but not BC (p<0.05, Wilcoxon signed rank test) suggesting tight coupling between the brain task GCaMP and these body parts.”\n\n“Decoding of trial outcome\n\nZ-scored ΔF/F0 montages, averaged across mice (454 single trials over 13 mice, for each trial outcome), show different cortical calcium dynamics associated with each Go-trial outcome: correct (code +2), lick late (code -2), or lick early (code -4) (Figure 10A). […] Similarly, running the model using only ALM fluorescence signals (+ALM) yielded significantly higher accuracy than when using only M2 (+M2) or V1 (+V1) signals (Figure 10E, violins, p<0.001, repeated measures ANOVA with post-hoc Bonferroni correction).”\n\n6) In section 1, the authors claim that use of a Raspberry Pi camera module is not inferior, but they do not present any data to support this claim.\n\nWe have removed the claim that the Raspberry Pi camera is not inferior and have written this in a more conservative manner.\n\nFrom Results “wide field imaging is typically done with larger focal volume (~2-3 mm) to image curved brain surfaces and lower spatial resolution (pixels binned to ~ 32-49 μm) to sum more photons over a larger area (Lim et al., 2012). Thus, the intrinsically large depth of field and reduced resolution provided by the small lenses employed by compact cameras such as the Raspberry Pi Camera Module (Figure 1B, C) are not expected to degrade signals.”\n\n7) The mesoscopic imaging presented is sufficient to demonstrate that imaging can be performed in their home cage system. However, that was already established by their previously published manuscript (Murphy et al., 2016). At the very least, group level or longitudinal analysis should be performed to facilitate comparison with results published by other groups with conventional imaging paradigms.\n\nIn the revision we include group analysis of functional imaging data from a total of 13 mice. For simplicity, we analyze group data from a simple go-detection task and were able to create a model that predicts behavioral trial outcome based on brain activity data (Figure 10). This model was used to examine temporal features and regions which exhibited predictive value in the behavioral task. This is new data and is now presented in two additional figures. We have also provided group data on the relationship between body movements and mesoscale brain activity (Figure 9). Here, consistent with other findings, there is a strong correlation between movement and mesoscale activity. Also, consistent with recent data is the finding that individual mice have different strategies for task completion. This work also consistent with findings from the Helmchen lab (Gilad et al., 2018). In total this new group analysis of functional imaging data suggests that our home cage is a well-characterized system that can produce results linking mesoscale.\n\nReviewer #3:\n\nThe authors improved upon a home cage system they reported on previously in Murphy et al., 2016. Docking system was improved for greater mouse participation, with a more effective training period. Additional monitoring of animal's behavioral state was added. Functional data is provided for a go/no-go licking task.\n\nAll materials are provided, along with schematics and acquisition code, and are a benefit to the greater research community. Automated longitudinal population studies are important for furthering our understanding of the neural basis of behavior.\n\n1) Much of this work has already been published (Murphy et al., 2016). Details provided here are improvements upon the original methods, rather than new findings or techniques.\n\nWe now better describe how this cage is an advance over what was previously published. In addition to having more features, including lick detection, task integration, behavioral camera triggering, and automatic weighing, the current system is much better documented and includes a fully illustrated guide to cage construction (Home cage Construction Guide). We feel this guide is essential for anyone planning to construct one of these systems. We’ve also better documented the open-source Python acquisition software and include an SQL database for tracking mouse parameters which can be done using a live cloud-based format. 2) The home cage is now used with longitudinal GCAMP signals from 13 different animals (See Figures 9 and 10). In this group data, we are able to make several conclusions (described in the response to the subsequent comment) regarding the GCAMP dynamics and behavioral outcome in 2 new added figures.\n\n2) Home cage system is described to be optimized for capturing long-term functional changes in cortex, yet the data showed no functional changes after training. Described results did not elucidate benefits of using this system.\n\nIn the revision we include functional group analysis of functional imaging data from a total of 13 different mice, see subsection “Decoding of trial outcome”. For simplicity, we analyze group data from a simple go-detection task and were able to create a model that predicts behavioral trial outcome based on brain activity data. This model was used to examine temporal features and regions which exhibited predictive value in the behavioral task. This new analysis is now presented in two additional figures. We have also provided group data on the relationship between body movements and mesoscale brain activity. Here, consistent with other findings, there is a strong correlation between movement and mesoscale activity. Also, consistent with recent data is the finding that individual mice have different strategies for task completion. This work also consistent with findings from the Helmchen lab (Gilad et al., 2018). In total this new group analysis of functional imaging data suggests that our home cage is a well-characterized system that can produce results linking mesoscale networks to behavior."
    }
}
{
    "id": "dbpedia_6959_1",
    "rank": 89,
    "data": {
        "url": "https://encord.com/blog/foundation-models/",
        "read_more_link": "",
        "language": "en",
        "title": "The Full Guide to Foundation Models",
        "top_image": "https://images.prismic.io/encord/7d9206e9-0d93-4737-99e3-b873faf9a062_full+guide+to+foundation+models.png?auto=compress%2Cformat&fit=max",
        "meta_img": "https://images.prismic.io/encord/7d9206e9-0d93-4737-99e3-b873faf9a062_full+guide+to+foundation+models.png?auto=compress%2Cformat&fit=max",
        "images": [
            "https://images.prismic.io/encord/7d9206e9-0d93-4737-99e3-b873faf9a062_full+guide+to+foundation+models.png?auto=compress%2Cformat&fit=max&w=906&h=638",
            "https://images.prismic.io/encord/7d9206e9-0d93-4737-99e3-b873faf9a062_full+guide+to+foundation+models.png?auto=compress%2Cformat&fit=max&w=906&h=638",
            "https://encord.com/static/VectorDesktop-d6a994f2c668a0332ba39898992e598f.png",
            "https://encord.com/static/VectorTablet-5246b4eeb12ce3a011a59f9a65313af7.png",
            "https://encord.com/static/VectorDesktop-d6a994f2c668a0332ba39898992e598f.png",
            "https://encord.cdn.prismic.io/encord/ZmrVVZm069VX1tfd_Union.svg",
            "https://images.prismic.io/encord/c594c9c7-3edf-4357-9307-92b9ab4d4548_1629105749470.jfif?auto=compress%2Cformat&fit=max&w=80&h=80",
            "https://images.prismic.io/encord/c594c9c7-3edf-4357-9307-92b9ab4d4548_1629105749470.jfif?auto=compress%2Cformat&fit=max&w=80&h=80",
            "https://images.prismic.io/encord/Zky98iol0Zci9U3b_tryEncordCTADark.png?auto=format,compress",
            "https://images.prismic.io/encord/abac777a-10bc-4ce3-8592-096f0d59bc72_Boris+Eldagsen_+AI-generated+image+PSEUDOMNESIA+The+Electricia.png?auto=compress,format",
            "https://images.prismic.io/encord/9588a555-fbdc-4c2d-a7fa-825497e85053_How+does+ChatGPT+work_+from+ChatGPT.png?auto=compress,format",
            "https://images.prismic.io/encord/60e57368-4cb2-40b3-bad8-1799299a8f8d_Foundation+model+in+Action_Segment+Anything+Model+in+Encord.png?auto=compress,format",
            "https://images.prismic.io/encord/cf3473b2-cc61-406a-8c23-4b16eed34e38_Introducing+ChatGPT+and+how+it+works.png?auto=compress,format",
            "https://images.prismic.io/encord/5c931f67-5adc-494f-8bc2-304b968b4736_A_Recent_Entrance_to_Paradise_An+image+generated+by+a+GAN.jpg?auto=compress,format",
            "https://images.prismic.io/encord/49c31932-236e-40a7-86b0-fd4dd11c7fc9_Microsoft%E2%80%99s+partnership+with+OpenAI+.png?auto=compress,format",
            "https://images.prismic.io/encord/2bc9a488-bb74-47d3-bad8-e09e6ee6c9d6_ChatGPT+in+action.png?auto=compress,format",
            "https://encord.com/static/VectorDesktop-d6a994f2c668a0332ba39898992e598f.png",
            "https://encord.com/static/VectorTablet-5246b4eeb12ce3a011a59f9a65313af7.png",
            "https://encord.com/static/VectorDesktop-d6a994f2c668a0332ba39898992e598f.png",
            "https://encord.cdn.prismic.io/encord/ZmrVVZm069VX1tfd_Union.svg",
            "https://images.prismic.io/encord/c594c9c7-3edf-4357-9307-92b9ab4d4548_1629105749470.jfif?auto=compress%2Cformat&fit=max&w=80&h=80",
            "https://images.prismic.io/encord/c594c9c7-3edf-4357-9307-92b9ab4d4548_1629105749470.jfif?auto=compress%2Cformat&fit=max&w=80&h=80",
            "https://images.prismic.io/encord/65d884c43a605798c18c27dc_inDex%404x.jpg?auto=format%2Ccompress&fit=max",
            "https://encord.cdn.prismic.io/encord/83549f38-37be-426c-a312-5107f575c736_testing.svg?fit=max",
            "https://encord.cdn.prismic.io/encord/40c1a4a5-3714-4908-8b2e-d7526a58d413_Annotate.svg?auto=compress%2Cformat&fit=max",
            "https://encord.cdn.prismic.io/encord/684dae60-2526-4403-8005-847e6b65480b_annotate-icon.svg?fit=max",
            "https://images.prismic.io/encord/6939c450-7b22-4d84-844d-682cc72c89f9_ProductCrad1.png?auto=format%2Ccompress&fit=max",
            "https://encord.cdn.prismic.io/encord/fe00a610-84b5-4bed-987a-0fe349e61c74_productIcons.svg?fit=max",
            "https://cdn.drata.com/badge/soc2-dark.png",
            "https://images.prismic.io/encord/d5a5f02e-d8df-49c2-9413-5633a8e75e7d_soc2-certificate.png?auto=compress,format",
            "https://encord.cdn.prismic.io/encord/ZoZ1tR5LeNNTwyYw_g22024.svg",
            "https://dc.ads.linkedin.com/collect/?pid=4241362&fmt=gif"
        ],
        "movies": [],
        "keywords": [],
        "meta_keywords": [
            ""
        ],
        "tags": null,
        "authors": [
            "Nikolaj Buhl"
        ],
        "publish_date": "2023-04-21T14:33:12+00:00",
        "summary": "",
        "meta_description": "The Full Guide to Foundation Models, including GANs, LLMs, VAEs, Multimodal, and Computer Vision| Encord",
        "meta_lang": "en",
        "meta_favicon": "/apple-touch-icon.png",
        "meta_site_name": "",
        "canonical_link": "https://encord.com/blog/foundation-models/",
        "text": "Foundation models are massive AI-trained models that use huge amounts of data and computational resources to generate anything from text to images. Some of the most popular examples of foundation models include GANs, LLMs, VAEs, & Multimodal, powering well-known tools such as ChatGPT, DALLE-2, Segment Anything, and BERT.\n\nFoundation models are large-scale AI models trained unsupervised on vast amounts of unlabeled data.\n\nThe outcome are models that are incredibly versatile and can be deployed for numerous tasks and use cases, such as image classification, object detection, natural language processing, speech-to-text software, and the numerous AI tools that play a role in our everyday lives and work.\n\nArtificial intelligence (AI) models and advances in this field are accelerating at an unprecedented rate. Only recently, a German artistic photographer, Boris Eldagsen, won a prize in the creative category of the Sony World Photography Awards 2023 for his picture, “PSEUDOMNESIA: The Electricia.”\n\nIn a press release, the awards sponsor, Sony, described it as “a haunting black-and-white portrait of two women from different generations, reminiscent of the visual language of 1940s family portraits.”\n\nShortly after winning, Eldagsen rejected the award, admitting the image was AI-generated.\n\nFoundation models aren’t new. But their contribution to generative AI software and algorithms are starting to make a massive impact on the world. Is this image a sign of things to come and the massive potential impact of foundation models and generative AI?\n\nLeverage foundational model in your annotation process\n\nAn award-winning AI-generated image: A sign of things to come and the power of foundation models?\n\n(Source)\n\nIn this article, we go in-depth on foundation models, covering the following:\n\nWhat are foundation models?\n\n5 AI principles behind foundation models\n\nDifferent types of foundation models (e.g., GANs, LLMs, VAEs, Multimodal, and Computer Vision, etc.)\n\nUse cases, evolution, and metrics of foundation models;\n\nAnd how you can use foundation models in computer vision.\n\nLet’s dive in . . .\n\nWhat are Foundation Models?\n\nThe term “Foundation Models” was coined by The Stanford Institute for Human-Centered Artificial Intelligence's (HAI) Center for Research on Foundation Models (CRFM) in 2021. The CRFM was born out of Stanford’s HAI Center and brought together 175 researchers across 10 Stanford departments.\n\nIt’s far from the only academic institution conducting research into foundation models, but as the concept originated here, it’s worth noting the way foundation models were originally described.\n\nCRFM describes foundation models as “any model that is trained on broad data (generally using self-supervision at scale) that can be adapted (e.g., fine-tuned) to a wide range of downstream tasks.” For more information, their paper, On The Opportunities and Risks of Foundation Models, is worth reading.\n\nPercy Lang, CRFM’s Director and a Stanford associate professor of computer science, says, “When we hear about GPT-3 or BERT, we’re drawn to their ability to generate text, code, and images, but more fundamentally and invisibly, these models are radically changing how AI systems will be built.”\n\nIn other words, GPT-3 (now V4), BERT, and numerous others are examples and types of foundation models in action.\n\nLet’s explore the five core AI principles behind foundation models, use cases, types of AI-based models, and how you can use foundation models for computer vision use cases.\n\n5 AI principles behind foundation models\n\nHere are the five core AI principles that make foundation models possible.\n\nPre-trained on Vast Amounts of Data\n\nFoundation models, whether they’ve been fine-tuned, or are open or closed, are usually pre-trained on vast amounts of data.\n\nTake GPT-3, for example, it was trained on 500,000 million words, that’s 10 human lifetimes of nonstop reading! It includes 175 billion parameters, 100x more than GPT-3 and 10x more than other comparable LLMs.\n\nThat’s A LOT of data and parameters required to make such a vast model work. In practical terms, you need to be very well-funded and resourced to develop foundation models.\n\nOnce they’re out in the open, anyone can use them for countless commercial or open-source scenarios and projects. However, the development of these models involves enormous computational processing power, data, and the resources to make it possible.\n\nSelf-supervised Learning\n\nIn most cases, foundation models operate on self-supervised learning principles. Even with millions or billions of parameters, data and inputs are provided without labels. Models need to learn from the patterns in the data and generate responses/outputs from that.\n\nOverfitting\n\nDuring the pre-training and parameter development stage, overfitting is an important part of creating foundation models. In the same way that Encord uses overfitting in the development of our computer vision micro-models.\n\nFine-tuning and Prompt Engineering (Adaptable)\n\nFoundation models are incredibly adaptable. One of the reasons this is possible is the work that goes into fine-tuning them and prompt engineering. Not only during the development and training stages but when a model goes live, prompts enable transfer learning at scale.\n\nThe models are continuously improving and learning based on the prompts and inputs from users, making the possibilities for future development even more exciting.\n\nFor more information, check out our post on SegGPT: Segmenting everything in context [Explained].\n\nGeneralized\n\nFoundation models are generalized in nature. Because the majority of them aren’t trained on anything specific, the data inputs and parameters have to be as generalized as possible to make them effective.\n\nHowever, the nature of foundation models means they can be applied and adapted to more specific use cases as required. In many ways, making them even more useful for dozens of industries and sectors.\n\nWith that in mind, let’s consider the various use cases for foundation models . . .\n\nUse Cases for Foundation Models\n\nThere are hundreds of use cases for foundation models, including image generation, natural language processing (NLP), text-to-speech, generative AI applications, and numerous others.\n\nOpenAI’s ChatGPT (including the newest iteration, Version 4), DALL-E 2, and BERT, a Google-developed NLP-based masked-language model, are two of the most widely talked about examples of foundation models.\n\nAnd yet, as exciting and talked about as these are, there are dozens of other use cases and types of foundation models. Yes, these foundation models capable of generative AI downstream tasks, like creating marketing copy and images, are an excellent demonstration of outputs.\n\nHowever, data scientists can also train foundation models for more specialized tasks and use cases. Foundation models can be trained on anything from healthcare tasks to self-driving cars and weapons and analyzing satellite imagery.\n\nSource\n\nTypes of Foundation Models\n\nThere are numerous different types of foundation models, including generative adversarial networks (GANs), variational auto-encoders (VAEs), transformer-based large language models (LLMs), and multimodal models.\n\nOf course, there are others, such as variational auto-encoders (VAEs). But for the purposes of this article, we’ll explore GANs, multimodal, LLMs, and computer vision foundation models.\n\nComputer Vision Foundation Models\n\nComputer vision is one of many AI-based models. Dozens of different types of algorithmically-generated models are used in computer vision, and foundation models are one of them.\n\nExamples of Computer Vision Foundation Models\n\nOne example of this is Florence, “a computer vision foundation model aiming to learn universal visual-language representations that be adapted to various computer vision tasks, visual question answering, image captioning, video retrieval, among other tasks.”\n\nFlorence is pre-trained in image description and labeling, making it ideal for computer vision tasks using an image-text contrastive learning approach.\n\nMultimodal Foundation Models\n\nMultimodal foundation models combine image-text pairs as the inputs and correlate the two different modalities during their pre-training data stage. This proves especially useful when attempting to implement cross-modal learning for tasks, making strong semantic correlations between the data a multimodal model is being trained on.\n\nExamples of Multimodal Foundation Models\n\nAn example of a multimodal foundation model in action is Microsoft’s UniLM, “a unified pre-trained language model that reads documents and automatically generates content.”\n\nMicrosoft’s Research Asia unit started working on the problem of Document AI (synthesizing, analyzing, summarizing, and correlating vast amounts of text-based data in documents) in 2019. The solution the team came up with combined CV and NLP models to create LayoutLM and UniLM, pre-trained foundation models that specialize in reading documents.\n\nGenerative Adversarial Networks (GANs)\n\nGenerative Adversarial Networks (GANs) are a type of foundation model involving two neural networks that contest and compete against one another in a zero-sum game. One network's gain is the other’s loss. GANs are useful for semi-supervised, supervised, and reinforcement learning. Not all GANs are foundation models; however, there are several that fit into this category.\n\nAn American computer scientist, Ian Goodfellow, and his colleagues came up with the concept in 2014.\n\nExamples of GANs\n\nGenerative Adversarial Networks (GANs) have numerous use cases, including creating images and photographs, synthetic data creation for computer vision, video game image generation, and even enhancing astronomical images.\n\nTransformer-Based Large Language Models (LLMs)\n\nTransformer-Based Large Language Models (LLMs) are amongst the most widely-known and used foundation models. A transformer is a deep learning model that weighs the significance of every input, including the recursive output data.\n\nA Large Language Model (LLM) is a language model that consists of a neural network with many parameters, trained on billions of text-based inputs, usually through a self-supervised learning approach. Combining an LLM and a transformer gives us transformer-based large language models (LLMs).\n\nAnd there are numerous examples and use cases, as many of you know and probably already benefit from deploying in various workplace scenarios every day.\n\nExamples of LLMs\n\nSome of the most popular LLMs include OpenAI’s ChatGPT (including the newest iteration, Version 4), DALL-E 2, and BERT (an LLM created by Google).\n\nBERT stands for “Bidirectional Encoder Representations from Transformers” and actually pre-dates the concept of foundation models by several years.\n\nWhereas the “Chat” in OpenAI’s ChatGPT stands for “Generative Pre-trained Transformer.” Microsft was so impressed by ChatGPT-3’s capabilities that it made a significant investment in OpenAI and is now integrating its foundation model technology with its search engine, Bing.\n\nGoogle is making similar advances, using AI-based LLMs to enhance their search engine with a feature known as Bard. AI is about to shape the future of search as we know it.\n\nAs you can see, LLMs (whether transformer-based or not) are making a significant impact on search engines and people’s abilities to use AI to generate text and images with only a small number of prompts.\n\nAt Encord, we’re always keen to learn, understand, and use new tools, especially AI-based ones. Here’s what happened when we employed ChatGPT as an ML engineer for a day!\n\nEvaluation Metrics of Foundation Models\n\nFoundation models are evaluated in a number of ways, most of which fall into two categories: intrinsic (a model's performance set against tasks and subtasks) and extrinsic evaluation (how a model performs overall against the final objective).\n\nDifferent foundation models are measured against performance metrics in different ways; e.g., a generative model will be evaluated on its own basis compared to a predictive model.\n\nOn a high-level, here are the most common metrics used to evaluate foundation models:\n\nPrecision: Always worth measuring. How precise or accurate is this foundation model? Precision and accuracy are KPIs that are used across hundreds of algorithmically-generated models.\n\nF1 Score: Combines precision and recall, as these are complementary metrics, producing a single KPI to measure the outputs of a foundation model.\n\nArea Under the Curve (AUC): A useful way to evaluate whether a model can separate and capture positive results against specific benchmarks and thresholds.\n\nMean Reciprocal Rank (MRR): A way of evaluating how correct or not a response is compared to the query or prompt provided.\n\nMean Average Precision (MAP): A metric for evaluating retrieval tasks. MAP calculates the mean precision for each result received and generated.\n\nRecall-Oriented Understudy for Gisting Evaluation (ROUGE): Measures the recall of a model’s performance, used for evaluating the quality and accuracy of text generated. It’s also useful to check whether a model has “hallucinated”; come up with an answer whereby it’s effectively guessing, producing an inaccurate result.\n\nThere are numerous others. However, for ML engineers working on foundation models or using them in conjunction with CV, AI, or deep learning models, these are a few of the most useful evaluation metrics and KPIs.\n\nHow to Use Foundation Models in Computer Vision\n\nAlthough foundation models are more widely used for text-based tasks, they can also be deployed in computer vision. In many ways, foundation models are contributing to advances in computer vision, whether directly or not.\n\nMore resources are going into AI model development, and that has a positive knock-on effect on computer vision models and projects.\n\nMore directly, there are foundation models specifically created for computer vision, such as Florence. Plus, as we’ve seen, GAN foundation models are useful for creating synthetic data and images for computer vision projects and applications.\n\nFoundation Models Key Takeaways\n\nFoundation models play an important role in contributing to the widescale use and adoption of AI solutions and software in organizations of every size.\n\nWith an impressive range of use cases and applications across every sector, we expect that foundation models will encourage the uptake of other AI-based tools.\n\nFoundation models, such as generative AI tools are lowering the barrier to entry for enterprise to start adopting AI tools, such as automated annotation and labeling platforms for computer vision projects.\n\nA lot of what’s being done now wasn’t possible, thanks to AI platforms, demonstrating the kind of ROI organization’s can expect from AI tools.\n\nReady to scale and make your computer vision model production-ready?\n\nSign-up for an Encord Free Trial: The Active Learning Platform for Computer Vision, used by the world’s leading computer vision teams.\n\nAI-assisted labeling, model training & diagnostics, find & fix dataset errors and biases, all in one collaborative active learning platform, to get to production AI faster. Try Encord for Free Today.\n\nWant to stay updated?"
    }
}
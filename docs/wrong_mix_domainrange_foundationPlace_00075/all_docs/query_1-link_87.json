{
    "id": "wrong_mix_domainrange_foundationPlace_00075_1",
    "rank": 87,
    "data": {
        "url": "https://www.govinfo.gov/content/pkg/CHRG-108hhrg88231/html/CHRG-108hhrg88231.htm",
        "read_more_link": "",
        "language": "en",
        "title": "SUPERCOMPUTING: IS THE U.S. ON THE RIGHT PATH?",
        "top_image": "",
        "meta_img": "",
        "images": [],
        "movies": [],
        "keywords": [],
        "meta_keywords": [
            ""
        ],
        "tags": null,
        "authors": [],
        "publish_date": null,
        "summary": "",
        "meta_description": "",
        "meta_lang": "",
        "meta_favicon": "",
        "meta_site_name": "",
        "canonical_link": null,
        "text": "[House Hearing, 108 Congress] [From the U.S. Government Publishing Office] SUPERCOMPUTING: IS THE U.S. ON THE RIGHT PATH? ======================================================================= HEARING BEFORE THE COMMITTEE ON SCIENCE HOUSE OF REPRESENTATIVES ONE HUNDRED EIGHTH CONGRESS FIRST SESSION __________ JULY 16, 2003 __________ Serial No. 108-21 __________ Printed for the use of the Committee on Science Available via the World Wide Web: http://www.house.gov/science ______ 88-231 U.S. GOVERNMENT PRINTING OFFICE WASHINGTON : 2003 ____________________________________________________________________________ For Sale by the Superintendent of Documents, U.S. Government Printing Office Internet: bookstore.gpo.gov Phone: toll free (866) 512-1800; (202) 512�091800 Fax: (202) 512�092250 Mail: Stop SSOP, Washington, DC 20402�090001 COMMITTEE ON SCIENCE HON. SHERWOOD L. BOEHLERT, New York, Chairman LAMAR S. SMITH, Texas RALPH M. HALL, Texas CURT WELDON, Pennsylvania BART GORDON, Tennessee DANA ROHRABACHER, California JERRY F. COSTELLO, Illinois JOE BARTON, Texas EDDIE BERNICE JOHNSON, Texas KEN CALVERT, California LYNN C. WOOLSEY, California NICK SMITH, Michigan NICK LAMPSON, Texas ROSCOE G. BARTLETT, Maryland JOHN B. LARSON, Connecticut VERNON J. EHLERS, Michigan MARK UDALL, Colorado GIL GUTKNECHT, Minnesota DAVID WU, Oregon GEORGE R. NETHERCUTT, JR., MICHAEL M. HONDA, California Washington CHRIS BELL, Texas FRANK D. LUCAS, Oklahoma BRAD MILLER, North Carolina JUDY BIGGERT, Illinois LINCOLN DAVIS, Tennessee WAYNE T. GILCHREST, Maryland SHEILA JACKSON LEE, Texas W. TODD AKIN, Missouri ZOE LOFGREN, California TIMOTHY V. JOHNSON, Illinois BRAD SHERMAN, California MELISSA A. HART, Pennsylvania BRIAN BAIRD, Washington JOHN SULLIVAN, Oklahoma DENNIS MOORE, Kansas J. RANDY FORBES, Virginia ANTHONY D. WEINER, New York PHIL GINGREY, Georgia JIM MATHESON, Utah ROB BISHOP, Utah DENNIS A. CARDOZA, California MICHAEL C. BURGESS, Texas VACANCY JO BONNER, Alabama TOM FEENEY, Florida RANDY NEUGEBAUER, Texas C O N T E N T S July 16, 2003 Page Witness List..................................................... 2 Hearing Charter.................................................. 3 Opening Statements Statement by Representative Sherwood L. Boehlert, Chairman, Committee on Science, U.S. House of Representatives............ 14 Written Statement............................................ 15 Statement by Representative Ralph M. Hall, Minority Ranking Member, Committee on Science, U.S. House of Representatives.... 15 Written Statement............................................ 16 Prepared Statement by Representative Eddie Bernice Johnson, Member, Committee on Science, U.S. House of Representatives.... 17 Witnesses: Dr. Raymond L. Orbach, Director, Office of Science, Department of Energy Oral Statement............................................... 19 Written Statement............................................ 20 Biography.................................................... 27 Dr. Peter A. Freeman, Assistant Director, Computer and Information Science and Engineering Directorate, National Science Foundation Oral Statement............................................... 28 Written Statement............................................ 30 Biography.................................................... 33 Dr. Daniel A. Reed, Director, National Center for Supercomputing Applications, University of Illinois at Urbana-Champaign Oral Statement............................................... 35 Written Statement............................................ 37 Biography.................................................... 41 Financial Disclosure......................................... 42 Mr. Vincent F. Scarafino, Manager, Numerically Intensive Computing, Ford Motor Company Oral Statement............................................... 44 Written Statement............................................ 45 Biography.................................................... 47 Financial Disclosure......................................... 48 Discussion....................................................... 49 Appendix 1: Answers to Post-Hearing Questions Dr. Raymond L. Orbach, Director, Office of Science, Department of Energy......................................................... 76 Dr. Peter A. Freeman, Assistant Director, Computer and Information Science and Engineering Directorate, National Science Foundation............................................. 79 Dr. Daniel A. Reed, Director, National Center for Supercomputing Applications, University of Illinois at Urbana-Champaign....... 87 Appendix 2: Additional Material for the Record Dr. Raymond L. Orbach, insert concerning Japan's role in nanotechnology as it relates to supercomputing................. 92 SUPERCOMPUTING: IS THE U.S. ON THE RIGHT PATH? ---------- WEDNESDAY, JULY 16, 2003 House of Representatives, Committee on Science, Washington, DC. The Committee met, pursuant to call, at 10:21 a.m., in Room 2318 of the Rayburn House Office Building, Hon. Sherwood L. Boehlert (Chairman of the Committee) presiding. HEARING CHARTER COMMITTEE ON SCIENCE U.S. HOUSE OF REPRESENTATIVES Supercomputing: Is the U.S. on the Right Path? WEDNESDAY, JULY 16, 2003 10:00 A.M.-12:00 P.M. 2318 RAYBURN HOUSE OFFICE BUILDING 1. Purpose On Wednesday, July 16, 2003, the House Science Committee will hold a hearing to examine whether the United States is losing ground to foreign competitors in the production and use of supercomputers\\1\\ and whether federal agencies' proposed paths for advancing our supercomputing capabilities are adequate to maintain or regain the U.S. lead. --------------------------------------------------------------------------- \\1\\ Supercomputing is also referred to as high-performance computing, high-end computing, and sometimes advanced scientific computing. --------------------------------------------------------------------------- 2. Witnesses Dr. Raymond L. Orbach is the Director of the Office of Science at the Department of Energy. Prior to joining the Department, Dr. Orbach was Chancellor of the University of California at Riverside. Dr. Peter A. Freeman is Assistant Director for the Computer and Information Science and Engineering Directorate (CISE) at the National Science Foundation (NSF). Prior to joining NSF in 2002, he was professor and founding Dean of the College of Computing at Georgia Institute of Technology. Dr. Daniel A. Reed is the Director of the National Center for Supercomputing Applications (NCSA) at the University of Illinois at Urbana-Champaign. NCSA is the leader of one of NSF's two university- based centers for high-performance computing. Dr. Reed is also the Director of the National Computational Science Alliance and is a principal investigator in the National Science Foundation's TeraGrid project. Earlier this year, Dr. Reed was appointed to the President's Information Technology Advisory Committee (PITAC). Mr. Vincent Scarafino is the Manager of Numerically Intensive Computing at Ford Motor Company, where he focuses on providing flexible and reliable supercomputer resources for Ford's vehicle product development, including vehicle design and safety analysis. 3. Overarching Questions The hearing will address the following overarching questions: 1. Is the U.S. losing its leadership position in supercomputing? Do the available supercomputers allow United States science and industry to be competitive internationally? Are federal efforts appropriately targeted to deal with this challenge? 2. Are federal agencies pursuing conflicting supercomputing programs? What can be done to ensure that federal agencies pursue a coordinated policy for providing supercomputing to meet the future needs for science, industry, and national defense? 3. Is the National Science Foundation moving away from the policies and programs that in the past have provided broad national access to advanced supercomputers? 4. Can the U.S. fulfill its scientific and defense supercomputing needs if it continues to rely on machines designed for mass-market commercial applications? 4. Brief Overview High-performance computers (also called supercomputers) are an essential component of U.S. scientific, industrial, and military competitiveness. However, the fastest and most efficient supercomputer in the world today is in Japan, not the U.S. Some experts claim that Japan was able to produce a computer so far ahead of the American machines because the U.S. had taken an overly cautious or conventional approach for developing new high-performance computing capabilities. Users of high-performance computing are spread throughout government, industry, and academia, and different high-performance computing applications are better suited to different types of machines. As the U.S. works to develop new high-performance computing capabilities, extraordinary coordination among agencies and between government and industry will be required to ensure that creative new capabilities are developed efficiently and that all of the scientific, governmental, and industrial users have access to the high- performance computing hardware and software best suited to their applications. The National Science Foundation (NSF) currently provides support for three supercomputing centers: the San Diego Supercomputer Center, the National Center for Supercomputing Applications at Urbana-Champaign in Illinois, and the Pittsburgh Supercomputing Center. These centers, along with their partners at other universities, are the primary source of high-performance computing for researchers in many fields of science. Currently, support for these centers beyond fiscal year 2004 is uncertain, and in the past few years NSF has been increasing its investment in a nationwide computing grid, in which fast connections are built between many computers to allow for certain types of high-performance scientific computing and advanced communications and data management. It is not clear whether this ``grid computing'' approach will provide the high-performance computing capabilities needed in all the scientific fields that currently rely on the NSF supercomputing centers. At the Department of Energy, there are two programs aimed at advancing high-performance computing capabilities. One, in the National Nuclear Security Administration (NNSA), is the continuation of a long-term effort to provide supercomputers to be used for modeling nuclear weapons effects; these simulations are particularly important in light of existing bans on nuclear weapon testing. In the other program, the Office of Science is now proposing to supplement its current advanced scientific computing activities with a new effort designed to create the world's fastest supercomputers. 5. Current Issues Is the U.S. Competitive? Japan's Earth Simulator is designed to perform simulations of the global environment that allow researchers to study scientific questions related to climate, weather, and earthquakes. It was built by NEC for the Japanese government at a cost of at least $350 million and has been the fastest computer in the world since it began running in March 2002. When the first measures of its speed were performed in April 2002, researchers determined that the Earth Simulator was almost five times faster than the former record holder, the ASCI White System at Lawrence Livermore National Laboratory, and also used the machine's computing power significantly more efficiently.\\2\\ --------------------------------------------------------------------------- \\2\\ For the U.S. supercomputers, typical scientific applications usually only are able to utilize 5-10 percent of the theoretical maximum computing power, while the design of the Earth Simulator makes 30-50 percent of its power accessible to the majority of typical scientific applications. --------------------------------------------------------------------------- This new development caused a great deal of soul-searching in the high-performance computing community about the U.S. approach to developing new capabilities and the emphasis on using commercially available (not specialized or custom-made) components. Going forward, it is not clear whether or not such a commodity-based approach will allow the U.S. high-performance computing industry to remain competitive. It is also unclear if the new machines produced by this approach will be able provide American academic, industrial, and governmental users with the high-performance computing capabilities they need to remain the best in the world in all critical applications. Will All Users Be Served? Users of high-performance computing are spread throughout government, industry, and academia. Different high-performance computing applications are better suited to different types of machines. For example, weather modeling and simulations of nuclear weapons require many closely-related calculations, so machines for these applications must have components that communicate with each other quickly and often. Other applications, such as simulations of how proteins fold, can be efficiently performed with a more distributed approach on machines in which each component tackles a small piece of the problem and works in relative isolation. In the U.S., the major producers of high-performance computers include IBM, Hewlett-Packard, and Silicon Graphics, Inc., whose products lean toward the more distributed approach, and Cray, whose products are more suited to problems that require the performance of closely-related calculations. The Japanese (NEC, Fujitsu, and Hitachi), also produce this sort of machine. The concern is that the U.S. on the whole has moved away from developing and manufacturing the machines needed for problems with closely-related calculations because the more distributed machines have a bigger commercial market. The Japanese have been filling this gap, but the gap could still impact the access of American scientists to the types of supercomputers that they need for certain important research problems. Responsibility for providing high-performance computing capabilities to existing users and for developing new capabilities is distributed among 11 different federal agencies and offices and relies heavily on industry for development and production. In this environment, extraordinary amounts of coordination are needed to ensure that new capabilities are developed efficiently and that the most appropriate kinds of hardware and software are available to the relevant users--coordination among agencies and between government and industry, as well as cooperation among universities and hardware and software companies. The results of an ongoing interagency effort to produce a coherent high-performance computing roadmap and the influence this roadmap has on agencies' programs will be the first test. Where are the DOE Office of Science and the NSF Programs Headed? Both NSF and the DOE Office of Science are moving ahead in significant new directions. At NSF, no plans have been announced to continue the Partnerships for Advanced Computational Infrastructure program, which supports the supercomputer centers, beyond fiscal year 2004. In addition, a proposed reorganization of NSF's Computer and Information Sciences and Engineering Directorate was announced on July 9 that includes a merging of the Advanced Computational Infrastructure program (which includes the support for the supercomputer centers) and the Advanced Networking Infrastructure program (which supports efforts on grid computing--an alternative approach to high-performance computing). Some scientists have expressed concerns that NSF may be reducing its commitment to providing researchers with a broad range of supercomputing capabilities and instead focusing its attention on grid computing and other distributed approaches. For the DOE Office of Science, the fiscal year 2004 budget request proposes a new effort in next-generation computer architecture to identify and address major bottlenecks in the performance of existing and planned DOE science applications. In addition, the July 8 mark-up of the House Energy and Water Development Appropriations Subcommittee sets funding for the Advanced Scientific Computing Research initiative at $213.5 million, an increase of $40 million over the request and $46 million over the previous year. Decisions about the future directions for high-performance computing at NSF and DOE Office of Science are clearly being made now. The White House has an interagency effort underway, the High End Computing Revitalization Task Force (HECRTF), which is supposed to result in the agencies' submitting coordinated budget requests in this area for fiscal year 2005. 6. Background What is High-Performance Computing? High-performance computing--also called supercomputing, high-end computing, and sometimes advanced scientific computing--is a phrase used to describe machines or groups of machines that can perform very complex computations very quickly. These machines are used to solve complicated and challenging scientific and engineering problems or manage large amounts of data. There is no set definition of how fast a computer must be to be ``high- performance'' or ``super,'' as the relevant technologies improve so quickly that the high-performance computing achievements of a few years ago could be handled now by today's desktops. Currently, the fastest supercomputers are able to perform trillions of calculations per second. What is High-Performance Computing Used For? High-performance computing is needed for a variety of scientific, industrial, and national defense applications. Most often, these machines are used to simulate a physical system that is difficult to study experimentally. The goal can be to use the simulation as an alternative to actual experiments (e.g., for nuclear weapon testing and climate modeling), as a way to test our understanding of a system (e.g., for particle physics and astrophysics), or as a way to increase the efficiency of future experiments or product design processes (e.g., for development of new industrial materials or fusion reactors). Other major uses for supercomputers include performing massive complex mathematical calculations (e.g., for cryptanalysis) or managing massive amounts of data (e.g., for government personnel databases). Scientific Applications: There are a rich variety of scientific problems being tackled using high-performance computing. Large-scale climate modeling is used to examine possible causes and future scenarios related to global warming. In biology and biomedical sciences, researchers perform simulations of protein structure, folding, and interaction dynamics and also model blood flows. Astrophysics model planet formation and supernova, and cosmologists analyze data on light from the early universe. Particle physicists use the ultra-fast computers to perform the complex calculations needed to study quantum chromodynamics and improve our understanding of electrons and quarks, the basic building blocks of all matter. Geologists model the stresses within the earth to study plate tectonics, while civil engineers simulate the impact of earthquakes. National Defense Applications: There are a number of ways in which high-performance computing is used for national defense applications. The National Security Agency (NSA) is a major user and developer of high-performance computers for executing specialized tasks relevant to cryptanalysis (such as factoring large numbers). The Department of Energy's National Nuclear Security Administration is also a major user and developer of machines to be used for designing and modeling nuclear weapons. Other applications within the Department of Defense include armor penetration modeling, weather forecasting, and aerodynamics modeling. Many of the scientific applications also have direct or future defense applications. For example, computational fluid dynamics studies are also of interest to the military, e.g. for modeling turbulence around aircraft. The importance of high-performance computing in many military areas, including nuclear and conventional weapons design, means that machines that alone or when wired together are capable of superior performance at military tasks are subject to U.S. export controls. Industrial Applications: Companies use high-performance computing in a variety of ways. The automotive industry uses fast machines to maximize the effectiveness of computer-aided design and engineering. Pixar uses massive computer animation programs to produce films. Pharmaceutical companies simulate chemical interactions to help with drug design. The commercial satellite industry needs to manage huge amounts of data for mapping. Financial companies and other industries use large computers to process immense and unpredictable Web transaction volumes, mine databases for sales patterns or fraud, and measure the risk of complex investment portfolios. What Types of High-Performance Computers Are There? All of the above examples of high-performance computing applications require very fast machines, but they do not all require the same type of very fast machine. There are a number of different ways to build high-performance computers, and different configurations are better suited to different problems. There are many possible configurations, but they can be roughly divided into two classes: big, single-location machines and distributed collections of many computers (this approach is often called grid computing). Each approach has its benefits--the big machines can be designed for a specific problem and are often faster, while grid computing is attractive in part because by using a multitude of commercially-available computers, the purchase and storage cost is often lower than for a large specialized supercomputer. Since the late 1990's, the U.S. approach to developing new capabilities has emphasized using commercially available (not specialized) components as much as possible. This emphasis has resulted in an increased focus on grid computing, and, in large machines, has led to a hybrid approach in which companies use commercial processors (whose speed is increasing rapidly anyway) to build the machines and then further speed them up by increasing the number of processors and improving the speed at which information is passed between processors. There are a number of distinctions that can be made among large machines bases on how the processors are connected. The differences relate to how fast and how often the various components of the computer communicate with each other and how calculations are distributed among the components. Users thus have a number of options for high-performance computing. Each user must take into account all of the pros and cons of the different configurations when he is deciding what sort of machine to use and how to design software to allow that machine to most efficiently solve his problem. For example, some problems, like weather and climate modeling and cryptanalysis, require lots of communication among computer components and large quantities of stored data, while other applications, like large-scale data analysis for high energy physics experiments or bioinformatics projects, can be more efficiently performed on distributed machines each tackling its own piece of the problem in relative isolation. How Do Government and Industry Provide Existing and New High- Performance Computing Capabilities? The development and production of high-performance computing capabilities requires significant effort by both government and industry. For any of the applications of high- performance computing described above, the users need good hardware (the high-performance machine or group of machines) and good software (programs that allow them to perform their calculations as accurately and efficiently as possible). The role of government therefore includes (1) funding research on new approaches to building high-performance computing hardware, (2) in some cases, funding the development stage of that hardware (usually through security agencies), (3) purchasing the hardware to be used by researchers at universities and personnel at government agencies, (4) funding research on software and programs to use existing and new high- performance computing capabilities, and (5) supporting research that actually uses the hardware and software. The role of industry is complementary--i.e., it receives funding to do research and development on new hardware and software, and it is the seller of this hardware and software to government agencies, universities, and companies. The primary industries involved in producing high-performance computing capabilities are computer makers (such as IBM, Hewlett-Packard, Silicon Graphics, Inc., and Cray), chip makers (such as Intel), and software designers. Congress has long had concerns about the health of the U.S. supercomputing industry. In 1996, when the National Center for Atmospheric Research, a privately-run, federally-funded research center, tried to order a supercomputer from NEC for climate modeling, Congress blocked the purchase. Federal High-Performance Computing Programs: In 1991, Congress passed the High Performance Computing Act, establishing an interagency initiative (now called National Information Technology Research and Development (NITRD) programs) and a National Coordination Office for this effort. Currently 11 agencies or offices participate in the high- end computing elements of the NITRD program (See Table 1 in the appendix). The total requested by all 11 agencies in fiscal year 2003 for high-end computing was $846.5 million. The largest research and development programs are at the National Science Foundation (NSF), which requested $283.5 million, and the Department of Energy Office of Science, which requested $137.8 million. Other major agency activities (all between $80 and $100 million) are at the National Institutes of Health (NIH), the Defense Advanced Research Projects Agency (DARPA), the National Aeronautics and Space Administration (NASA), and the Department of Energy's National Nuclear Security Administration (NNSA). Different agencies concentrate on serving different user communities and on different stages of hardware and software development and application. (In addition to the research and development-type activities that are counted for the data included in Table 1 and referenced above, many agencies, such as NNSA and the National Oceanic and Atmospheric Administration (NOAA), devote significant funding to the purchase and operation of high-performance computers that perform these agencies' mission-critical applications.) \\3\\ --------------------------------------------------------------------------- \\3\\ For example, in FY 2003 NOAA spent $36 million on supercomputers--$10 million for machines for climate modeling and $26 million for machines for the National Weather Service. National Science Foundation: The NSF serves a very wide variety of scientific fields within the academic research community, mainly through a series of supercomputing centers, originally established in 1985 and currently funded under the Partnerships for Advanced Computational Infrastructure (PACI) program. The supercomputer centers provide researchers not only with access to high-performance computing capabilities but also with tools and expertise on how best to utilize these resources. The NSF also is supporting the development of the Extensible Terascale Facility (ETF), a nationwide grid of machines that can be used for high-performance computing and advanced communications and data management. Recently, some researchers within the high-performance computing community have expressed concern that NSF may be reducing its commitment to the supercomputer centers and increasing its focus on grid computing and distributed approaches to high-performance --------------------------------------------------------------------------- computing, such as would be used in the ETF. Department of Energy: The Department of Energy has been a major force in advancing high-performance computing for many years, and the unveiling of the fastest computer in the world in Japan in 2002 resulted in serious self-evaluation at the department, followed by a rededication to efforts to enhance U.S. supercomputing capabilities. The Department of Energy has two separate programs focused on both developing and applying high-performance computing. The Advanced Scientific Computing Research (ASCR) program in the Office of Science funds research in applied mathematics (to develop methods to model complex physical and biological systems), in network and computer sciences, and in advanced computing software tools. For fiscal year 2004, the department has proposed a new program on next- generation architectures for high-performance computing. The Accelerated Strategic Computing Initiative (ASCI) is part of the NNSA's efforts to provide advanced simulation and computing technologies for weapons modeling. DARPA: DARPA traditionally focuses on the development of new hardware, including research into new architectures and early development of new systems. On July 8, DARPA announced that Cray, IBM, and Sun Microsystems had been selected as the three contractor teams for the second phase of the High Productivity Computing Systems program, in which the goal is to provide a new generation of economically viable, scalable, high productivity computing systems for the national security and industrial user communities in the 2009 to 2010 timeframe. Other Agencies: NIH, NASA, and NOAA are all primarily users of high performance computing. NIH manages and analyzes biomedical data and models biological processes. NOAA uses simulations to do weather forecasting and climate change modeling. NASA has a variety of applications, including atmospheric modeling, aerodynamic simulations, and data analysis and visualization. The National Security Agency (NSA) both develops and uses high- performance computing for a number of applications, including cryptanalysis. As a user, NSA has a significant impact on the high-performance computing market, but due to the classified nature of its work, the size of its contributions to High End Computing Infrastructure and Applications and the amount of funding it uses for actual operation of computers is not included in any of the data. Interagency Coordination: The National Coordination Office (NCO) coordinates planning, budget, and assessment activities for the Federal Networking and NITRD Program through a number of interagency working groups. The NCO reports to the White House Office of Science and Technology Policy and the National Science and Technology Council. In 2003, NCO is also managing the High End Computing Revitalization Task Force (HECRTF), an interagency effort on the future of U.S. high- performance computing. The HECRTF is tasked with development of a roadmap for the interagency research and development for high-end computing core technologies, a federal high-end computing capacity and accessibility improvement plan, and a discussion of issues relating to federal procurement of high-end computing systems. The product of the HECRTF process is expected to guide future investments in this area, starting with agency budget submissions for fiscal year 2005. The Role of Industry: Industry plays a critical role in developing and providing high-performance computing capabilities to scientific, industrial, and defense users. Many supercomputers are purchased directly from computer companies like IBM, Hewlett-Packard, Silicon Graphics, Inc., and Cray, and the groups that do build their own high- performance clusters do so from commercially available computers and workstations. Industry is a recipient of federal funding for initial research into new architectures for hardware, for development of new machines, and for production of standard and customized systems for government and universities, but industry also devotes its own funding to support research and development. The research programs do not just benefit the high-performance computing community, as new architectures and faster chips lay the groundwork for better performing computers and processors in all commercial information technology products. The State of the Art in High-Performance Computing: Twice a year, a list of the 500 fastest supercomputers is compiled; the latest list was released on June 23, 2003 (see Table 2 in the appendix).\\4\\ The Earth Simulator supercomputer, built by NEC and installed last year at the Earth Simulator Center in Yokohama, Japan, continues to hold the top spot as the best performer. It is approximately twice as fast as the second place machine, the ASCI Q system at Los Alamos National Laboratory, built by Hewlett-Packard. Of the top twenty machines, eight are located at various Department of Energy national laboratories and two at U.S. universities,\\5\\ and nine were made by IBM and five by Hewlett-Packard. --------------------------------------------------------------------------- \\4\\ The top 500 list is compiled by researchers at the University of Mannheim (Germany), Lawrence Berkeley National Laboratory, and the University of Tennessee and is available on line at http:// www.top500.org/. For a machine to be included on this public list, its owners must send information about its configuration and performance to the list-keepers. Therefore, the list is not an entirely comprehensive picture of the high-performance computing world, as classified machines, such as those used by NSA, are not included. \\5\\ The two university machines are located at the Pittsburgh Supercomputing Center (supported primarily by NSF) and Louisiana State University's Center for Applied Information Technology and Learning. The remaining 12 machines include four in Europe, two in Japan, and one each at the National Oceanic & Atmospheric Administration, the National Center for Atmospheric Research, the Naval Oceanographic Office, and NASA. --------------------------------------------------------------------------- 7. Witness Questions The witnesses were asked to address the following questions in their testimony: Questions for Dr. Raymond L. Orbach The Office of Science appears to have embarked on a new effort in next-generation advanced scientific computer architecture that differs from the development path currently pursued by the National Nuclear Security Agency (NNSA), the lead developer for advanced computational capability at the Department of Energy (DOE). Why is the Office of Science taking this approach? How is the Office of Science cooperating with the Defense Advanced Research Projects Agency, which supports the development of advanced computers for use by the National Security Agency and other agencies within the Department of Defense? To what extent will the Office of Science be guided by the recommendations of the High-End Computing Revitalization Task Force? How will the Office of Science contribute to the Office of Science and Technology Policy plan to revitalize high-end computing? To what extent are the advanced computational needs of the scientific community and of the private sector diverging? What is the impact of any divergence on the advanced computing development programs at the Office of Science? Questions for Dr. Peter A. Freeman Some researchers within the computer science community have suggested that the NSF may be reducing its commitment to the supercomputer centers. Is this the case? To what extent does the focus on grid computing represent a move away from providing researchers with access to the most advanced computing equipment? What are the National Science Foundation's (NSF's) plans for funding the supercomputer centers beyond fiscal year 2004? To what extent will you be guided by the recommendation of the NSF Advisory Panel on Cyberinfrastructure to maintain the Partnerships for Advanced Computational Infrastructure, which currently support the supercomputer centers? To what extent will NSF be guided by the recommendations of the High-End Computing Revitalization Task Force? How will NSF contribute to the Office of Science and Technology Policy plan to revitalize high-end computing? To what extent are the advanced computational needs of the scientific community and of the private sector diverging? What is the impact of any such divergence on the advanced computing programs at NSF? Questions for Dr. Daniel A. Reed Some researchers within the computer science community have suggested that the National Science Foundation (NSF) may be reducing its commitment to provide advanced scientific computational capability to U.S. scientists and engineers. Have you detected any change in policy on the part of NSF? What advanced computing capabilities must the Federal Government provide the academic research community for the government's programs to be considered successful? Are the programs for developing the next-generation of advanced scientific computing that are currently underway at government agencies on track to provide these capabilities? If not, why not? For academic scientists and engineers, what is the difference between the advanced scientific computing capabilities provided by NSF and those provided by the Department of Energy? Questions for Mr. Vincent F. Scarafino How does Ford use high-performance computing? How do computing capabilities affect Ford's competitiveness nationally and internationally? What does Ford see as the role of the Federal Government in advancing high-performance computing capabilities and in making these capabilities accessible to users? Are current agency programs for developing the next-generation of advanced scientific computing adequate to provide these capabilities? If not, why not? Is the U.S. government cooperating appropriately with the private sector on high-performance computing, and is the level of cooperation adequate to sustain leadership and meet scientific and industrial needs? To what extent are the advanced computational needs of the scientific community and of the private sector diverging? What is the impact of any divergence on Ford's access to advanced computing capabilities? Chairman Boehlert. The hearing will come to order. And I apologize for being the delinquent Member of the group gathered here. My apologies to all. It is a pleasure to welcome everyone here this morning for this important hearing. At first blush, today's topic, supercomputing, may seem technical and arcane, of interest to just a few researchers who spend their lives in the most rarefied fields of science. But in reality, the subject of this hearing is simple and accessible, and it has an impact on all of us, because supercomputing affects the American economy and our daily lives, perhaps more so than so many, many other things that we focus a lot of time and attention on. Supercomputers help design our cars, predict our weather, and deepen our understanding of the natural forces that govern our lives, such as our climate. Indeed, computation is now widely viewed as a third way of doing science; building on the traditional areas of theory and experimentation. So when we hear that the U.S. may be losing its lead in supercomputing, that Japan now has the fastest supercomputer, that the U.S. may be returning to a time when our top scientists didn't have access to the best machines, that our government may have too fragmented a supercomputing policy, well, those are issues a red flag should be waved on to capture the attention of all of us. And those issues have captured our attention. The purpose of this hearing is to gauge the state of U.S. supercomputing and to determine how to deal with any emerging problems. I don't want to exaggerate, we are not at a point of crisis. Most of the world's supercomputers are still made by and used by Americans, but we are at a pivotal point when we need to make critical decisions to make sure that remains the case. And maintaining U.S. leadership requires a coordinated, concerted effort by the Federal Government. Let me stress that. Coordinated, concerted effort by the Federal Government. The Federal Government has long underwritten the basic research that fuels the computer industry, has purchased the highest end computers, and has ensured that those computers are available to a wide range of American researchers. This Committee has played an especially crucial role in ensuring access, pushing for the creation of the National Science Foundation Supercomputer Centers back in the early '80's. Government action is just as needed now. But what action? The Department of Energy is proposing to move away from our reliance on more mass-market supercomputers to pursue research on massive machines designed to solve especially complex problems. NSF appears to be moving away from super--supporting supercomputer centers to a more distributed computing approach. These policies need to be examined. So with that in mind, here are some of the questions we intend to pursue today. Is the U.S. losing its lead in supercomputing, and what can be done about it? What federal policies should be pursued to maintain our lead, and how should we judge whether they are succeeding? Is federal policy sufficiently coordinated? And I think the answer is clear. And are the new directions being pursued by NSF and the Department of Energy the proper approach to maintaining our lead? We have a distinguished group of experts, and I look forward to hearing their testimony. With that, it is my pleasure to yield to the distinguished Ranking Member, Mr. Hall of Texas. [The prepared statement of Mr. Boehlert follows:] Prepared Statement of Chairman Sherwood L. Boehlert It's a pleasure to welcome everyone here this morning for this important hearing. At first blush, today's topic, supercomputing, may seem technical and arcane--of interest to just a few researchers who spend their lives in the most rarefied fields of science. But in reality, the subject of this hearing is simple and accessible, and it has an impact on all of us because supercomputing affects the American economy and our daily lives. Supercomputers help design our cars, predict our weather, and deepen our understanding of the natural forces that govern our lives, such as our climate. Indeed, computation is now widely viewed as a third way of doing science--building on the traditional areas of theory and experimentation. So when we hear that the U.S. may be losing its lead in supercomputing, that Japan now has the fastest supercomputer, that the U.S. may be returning to a time when our top scientists didn't have access to the best machines, that our government may have too fragmented a supercomputing policy--well, those issues are a red flag that should capture the attention of all of us. And those issues have captured our attention. The purpose of this hearing is to gauge the state of U.S. supercomputing and to determine how to deal with any emerging problems. I don't want to exaggerate--we're not at a point of crisis--most of the world's supercomputers are still made by, and used by Americans. But we are at a pivotal point when we need to make critical decisions to make sure that remains the case. And maintaining U.S. leadership requires a coordinated, concerted effort by the Federal Government. The Federal Government has long underwritten the basic research that fuels the computer industry, has purchased the highest-end computers, and has ensured that those computers are available to a wide range of American researchers. This committee has played an especially crucial role in ensuring access, pushing for the creation of the National Science Foundation (NSF) Supercomputer Centers back in the early `80s. Government action is just as needed now. But what action? The Department of Energy is proposing to move away from our reliance on more mass-market supercomputers to pursue research on massive machines design to solve especially complex problems. NSF appears to be moving away from supporting supercomputer centers to a more distributed computing approach. These policies need to be examined. So, with that in mind, here are some of the questions we intend to pursue today: Is the U.S. losing its lead in supercomputing and what can be done about that? What federal policies should be pursued to maintain our lead and how should we judge whether they are succeeding? Is federal policy sufficiently coordinated and are the new directions being pursued by NSF and the Department of Energy the proper approach to maintaining our lead? We have a distinguished group of experts, and I look forward to hearing their testimony. Mr. Hall. Mr. Chairman, thank you. And I am pleased to join you today in welcoming our witnesses. And I thank you for your time, not only in your appearing here, but in preparation and travel. And thank you for your usual courtesy. Computation has become one of the, I guess, principal tools, along with the theory and experiment for conducting science and engineering research and development. There is no question that the U.S. preeminence in science and technology will not and cannot continue unless our scientists and engineers have access to the most powerful computers available. The Science Committee has had a deep and sustained interest in this subject since the emergence of supercomputing in the late 1970's. And the initial concern of the Committee was to ensure that the U.S. scientists and engineers, outside of the classified research world, had access to the most powerful computers. We have supported programs to provide this access, such as the supercomputer centers program at NSF. Moreover, the Committee has encouraged the efforts of the Federal R&D agencies to develop a coordinated R&D program to accelerate computing and networking developments. The High Performance Computing Act of 1991 formalized this interagency R&D planning and coordination process. The value and importance of the resulting interagency information technology R&D program is quite evident from its appearances as a formal presidential budget initiative through three different presidential administrations. So today, I think we want to assess a particular component of the federal information technology R&D effort. That is, the pathway being followed for the development of high-end computers and the provision being made to provide access to these machines by the U.S. research community. Questions have been raised as to whether we put all of our eggs in one basket by mainly focusing on supercomputers based on commodity components. The recent success of the specialized Japanese Earth Simulator computer has triggered a review of the computing needs of the scientific and technical community and reconsideration of the R&D and acquisition plan needed for the next several years and for the longer-term. So we would be very interested today in hearing from our witnesses about where we are now in terms of high-end computing capabilities and where we should be going to provide the kinds of computer systems needed to tackle the most important and certainly challenging of all problems. I also want to explore what the roles of the various federal agencies ought to be in the development of new classes of high-end computers and for providing access for the general U.S. research community to these essential tools. I appreciate the attendance of our witnesses, and I look forward to your discussion. I yield back my time. [The prepared statement of Mr. Hall follows:] Prepared Statement of Representative Ralph M. Hall Mr. Chairman, I am pleased to join you today in welcoming our witnesses, and I congratulate you on calling this hearing on federal R&D in support of high-performance computing. Computation has become one of the principal tools, along with theory and experiment, for conducting science and engineering research and development. There is no question that U.S. preeminence in science and technology will not continue unless our scientists and engineers have access to the most powerful computers available. The Science Committee has had a deep and sustained interest in this subject since the emergence of supercomputing in the late 1970s. The initial concern of the Committee was to ensure that U.S. scientists and engineers, outside of the classified research world, had access to the most powerful computers. we have supported programs to provide this access, such as the supercomputer centers program at NSF. Moreover, the Committee has encouraged the efforts of the federal R&D agencies to develop a coordinated R&D program to accelerate computing and networking developments. The High Performance Computing Act of 1991 formalized this interagency R&D planning and coordination process. The value and importance of the resulting interagency information technology R&D program is evident from its appearance as a formal presidential budget initiative through 3 different Administrations. Today, we want to assess a particular component of the federal information technology R&D effort. That is, the pathway being followed for the development of high-end computers and the provisions being made to provide access to these machines by the U.S. research community. Questions have been raised as to whether we have put all of our eggs into one basket by mainly focusing on supercomputers based on commodity components. The recent success of the specialized Japanese Earth Simulator computer has triggered a review of the computing needs of the scientific and technical community and a reconsideration of the R&D and acquisition plan needed for the next several years, and for the longer-term. I will be interested in hearing from our witnesses about where we are now in terms of high-end computing capabilities and where we should be going to provide the kinds of computer systems needed to tackle the most important and computationally challenging problems. I also want to explore what the roles of the various federal agencies ought to be in the development of new classes of high-end computers and for providing access for the general U.S. research community to these essential tools. I appreciate the attendance of our witnesses, and I look forward to our discussion. Chairman Boehlert. Thank you very much, Mr. Hall. And without objection, all other's opening statements will be made a part of the record at this juncture. [The prepared statement of Ms. Johnson follows:] Prepared Statement of Representative Eddie Bernice Johnson Thank you, Chairman for calling hearing to examine the very important issue of Supercomputing. I also want to thank our witnesses for agreeing to appear today. We are here to discuss whether the United States is losing ground to foreign competitors in the production and use of supercomputers and whether federal agencies' proposed paths for advancing our supercomputing capabilities are adequate to maintain or regain the U.S. lead. As we all know, a supercomputer is a broad term for one of the fastest computers currently available. Such computers are typically used for number crunching including scientific simulations, (animated) graphics, analysis of geological data (e.g., in petrochemical prospecting), structural analysis, computational fluid dynamics, physics, chemistry, electronic design, nuclear energy research and meteorology. Supercomputers are state-of-the-art, extremely powerful computers capable of manipulating massive amounts of data in a relatively short time. They are very expensive and are employed for specialized scientific and engineering applications that must handle very large databases or do a great amount of computation, among them meteorology, animated graphics, fluid dynamic calculations, nuclear energy research and weapon simulation, and petroleum exploration. Supercomputers are gaining popularity in all corners of corporate America. They are used to analyze vehicle crash test by auto manufacturers, evaluate human diseases and develop treatments by the pharmaceutical industry and test aircraft engines by the aero-space engineers. It quite evident that supercomputing will become more important to America's commerce in the future. I look forward to working with this committee on its advancement. Again, I wish thank the witnesses for coming here today help us conceptualize this goal. Chairman Boehlert. And just a little history. I can recall back in the early '80's, 1983 to be exact, when I was a freshman and Mr. Hall was an emerging power in the Congress. I sat way down in the front row on the end, and I didn't know what was going on. But I do remember very vividly the testimony of a Nobel Laureate, Dr. Ken Wilson, who at that time was at Cornell University. And he told us that the typical graduate student in the United Kingdom or Japan or Germany--the typical graduate student had greater access to the latest in computer technology than did he, a young Nobel Laureate, you know, one of the great resources of our nation. And he argued very forcibly and very persuasively for the Federal Government to get more actively involved. And boy, that--it was like the light bulb going on. I didn't even understand it, and I am not quite sure I do yet, this--the--all of the intricacies of this supercomputer technology, but I do remember then being a champion from day one getting this supercomputer initiative going for America. And in '85, NSF set up the Supercomputing Centers and--at Carnegie Mellon and Cornell and others--and boy, did we just go forward, leapfrog ahead. We did wonderful things. You know what? A little lethargy is setting in and I am getting concerned. I am deeply concerned, and I mention in my opening statement about five times, I should have mentioned it about 55 times, the importance of a well coordinated federal response to this issue. I don't want to be second to anybody, neither does Mr. Hall, neither do any Members of this committee. We have an opportunity, and we are going to seize it. And so we are looking at all of you as resources for this committee. You are all very distinguished in your own way. You are very knowledgeable. You will share with us, and hopefully we will get a few more light bulbs turned on up here. And we can go forward together. There is an awful lot at stake. And so I look forward to your testimony, and I hope that you will sign on here and now. Some of you have no choice. You have to, right? But sign on here and now to work cooperatively with us, because there is so much at stake. With that, let me introduce our witnesses. Witness consist--list consists of Dr. Raymond Orbach, Director of the Office of Science, Department of Energy. Dr. Orbach, good to have you back. Dr. Peter A. Freeman, Assistant Director, Computer and Information Science and Engineering Directorate at the National Science Foundation. It is good to see you once again. Dr. Daniel A. Reed, Director, National Center for Supercomputing Applications, University of Illinois at Urbana-Champaign. Dr. Reed. And Mr. Vincent Scarafino, Manager, Numerically Intensive Computing, Ford Motor Company. Mr. Scarafino, glad to have you here. We would ask all of our witnesses to try to summarize your complete statement, because we all have the benefit of the complete statement. And we will read those statements very carefully, but try to summarize in five minutes or so. I am not going to be arbitrary. This is too darn important to restrict your expert input to 300 seconds, but I would ask you to be close to the five minutes. And then we can have a good exchange in the dialogue. And hopefully we will all profit from this little exercise we are engaged in. Dr. Orbach. STATEMENT OF DR. RAYMOND L. ORBACH, DIRECTOR, OFFICE OF SCIENCE, DEPARTMENT OF ENERGY Dr. Orbach. Chairman Boehlert and Ranking Member Hall, Members of the Committee, I commend you for holding this hearing. And I deeply appreciate the opportunity to testify on behalf of the Office of Science at the Department of Energy on a subject of central importance to this Nation, as you have both outlined, our need for advanced supercomputing capability. Through the efforts of the DOE's Office of Science and other federal agencies, we are working to develop the next generation of advanced scientific computational capacity, a capability that supports economic competitiveness and America's scientific enterprise. The Bush Administration has forged an integrated and unified interagency road map to the critical problems that you have asked us to address today. In my opening statement, I would like to briefly address the four specific questions that the Committee has asked of me, and more extensive answers are contained in my written testimony. The first question that the Committee addressed to me concerned the development path for a next-generation advanced scientific computer and whether the Office of Science path differed from that of the National Nuclear Security Agency, the NNSA. The NNSA has stewardship responsibilities and the computer architectures, which they have used, are well suited for those needs. And indeed, they led the way in the massively parallel machine development. However, those machines operate at only something like five to 10 percent efficiency when applied to many problems of scientific interest and also industrial interest. And that reduces the efficiencies of these high peak speed machines. Other architectures have shown efficiencies closer to 60 percent for some of the physical problems that science and industry must address. We are working with NNSA as partners to explore these alternatives, which we believe, will be important to both of our areas of responsibility. For example, the Office of Science will be exploring computer architectures that may be of value for magnetic fusion to biology. NNSA is working with us as a partner to explore equations of state under high pressures and extreme temperatures, which, of course, is critical to stewardship issues. The second question that I was asked was are we cooperating with DARPA, the Defense Advanced Research Project Agency, and how does that relationship work. DARPA has historically invested in new architectures, which have been and are of great interest to the Office of Science. We are--we have a memorandum of understanding [MOU] currently under review between the Defense Department and the Department of Energy that establishes a framework for cooperation between DARPA and the DOE, including both the Office of Science and NNSA. The MOU will cover high-end computation performance evaluation, development of benchmarks, advanced computer architecture evaluation, development of mathematical libraries, and system software. This will bring together the complementary strengths of each agency. We will be able to draw on DARPA's strengths in advanced computer architectures and they on our decades of experience in evaluating new architectures and transforming them into tools for scientific discovery. The third question you asked was to what extent will the Office of Science be guided by the recommendations of the High- End Computing Revitalization Task Force and how will we contribute to the OSTP, Office of Science and Technology Policy, plan to revitalize high-end computation. The formation of the High-End Computation Revitalization Task Force by OSTP emphasizes the importance that the Administration places on the need for a coordinated approach to strengthening high-end computation. A senior official of the Office of Science is co- chairing that task force, and it includes many representatives from across the government and industry. Many of the task force findings and plans are actually based on Office of Science practices in advanced computing and simulation. We are working very closely with NNSA, the Department of Defense, NASA, the National Science Foundation, and the National Institutes of Health to assess how best to coordinate and leverage our agency's high-end computation investments now and in the future. We expect to play a major role in executing the plans that emerge from this task force in partnership with the other agencies and under the guidance of the President's Science Advisor. The last question you asked was how are the advanced computational needs of the scientific community and of the private sector diverging and how does that affect advanced computing development programs at the Office of Science. I don't believe there is a major divergence between the needs of the scientific community and those of industry's design engineers. The apparent divergence stems from the dominance of computer development by the needs of specific commercial applications: payroll, management information systems, and web servers. I will defer to Mr. Scarafino on this, but my own discussions with industry leaders suggest that the type of computer architectures that would meet the needs of the Office of Science would also support their requirements. It would give them the ability to create virtual prototypes of complex systems, allowing engineers to optimize different design parameters without having to build prototypes. This would reduce the time to market. It would decrease the costs, and it would increase economic competitiveness. These four questions were central, and I appreciate being asked them and given the opportunity to respond. I am gratified that this committee is intent on enabling us to pursue so many important computational opportunities for the sake of scientific discovery, technological innovation, and economic competitiveness. I thank you for inviting me, and I will be pleased to take questions. [The prepared statement of Dr. Orbach follows:] Prepared Statement of Raymond L. Orbach Mr. Chairman and Members of the Committee, I commend you for holding this hearing--and I appreciate the opportunity to testify on behalf of the Department of Energy's (DOE) Office of Science--on a subject of central importance to this nation: our need for advanced supercomputing capability. Through the efforts of DOE's Office of Science and other federal agencies, we are working to develop the next generation of advanced scientific computational capability, a capability that supports economic competitiveness and America's scientific enterprise. As will become abundantly clear in my testimony, the Bush Administration has forged an integrated and unified interagency roadmap to the critical problems you have asked us to address today. No one agency can--or should--carry all the weight of ensuring that our scientists have the computational tools they need to do their job, yet duplication of effort must be avoided. The President, and John Marburger, Office of Science and Technology Policy Director, understand this. That is why all of us here are working as a team on this problem. * * * Mr. Chairman, for more than half a century, every President and each Congress has recognized the vital role of science in sustaining this nation's leadership in the world. According to some estimates, fully half of the growth in the U.S. economy in the last 50 years stems from federal funding of scientific and technological innovation. American taxpayers have received great value for their investment in the basic research sponsored by the Office of Science and other agencies in our government. Ever since its inception as part of the Atomic Energy Commission immediately following World War II, the Office of Science has blended cutting edge research and innovative problem solving to keep the U.S. at the forefront of scientific discovery. In fact, since the mid- 1940's, the Office of Science has supported the work of more than 40 Nobel Prize winners, testimony to the high quality and importance of the work it underwrites. Office of Science research investments historically have yielded a wealth of dividends including: significant technological innovations; medical and health advances; new intellectual capital; enhanced economic competitiveness; and improved quality of life for the American people. Mr. Chairman and Members of this committee, virtually all of the many discoveries, advances, and accomplishments achieved by the Office of Science in the last decade have been underpinned by advanced scientific computing and networking tools developed by the Office of Advanced Scientific Computing Research (ASCR). The ASCR program mission is to discover, develop, and deploy the computational and networking tools that enable scientific researchers to analyze, model, simulate, and predict complex phenomena important to the Department of Energy--and to the U.S. and the world. In fact, by fulfilling this mission over the years, the Office of Science has played a leading role in maintaining U.S. leadership in scientific computation worldwide. Consider some of the innovations and contributions made by DOE's Office of Science: helped develop the Internet; pioneered the transition to massively parallel supercomputing in the civilian sector; began the computational analysis of global climate change; developed many of the DNA sequencing and computational technologies that have made possible the unraveling of the human genetic code; and opened the door for major advances in nanotechnology and protein crystallography. * * * Computational modeling and simulation are among the most significant developments in the practice of scientific inquiry in the latter half of the 20th Century. In the past century, scientific research has been extraordinarily successful in identifying the fundamental physical laws that govern our material world. At the same time, the advances promised by these discoveries have not been fully realized, in part because the real-world systems governed by these physical laws are extraordinarily complex. Computers help us to visualize, to test hypotheses, to guide experimental design, and most importantly to determine if there is consistency between theoretical models and experiment. Computer-based simulation provides a means for predicting the behavior of complex systems that can only be described empirically at present. Since the development of digital computers in mid-century, scientific computing has greatly advanced our understanding of the fundamental processes of nature, e.g., fluid flow and turbulence in physics, molecular structure and reactivity in chemistry, and drug-receptor interactions in biology. Computational simulation has even been used to explain, and sometimes predict, the behavior of such complex natural and engineered systems as weather patterns and aircraft performance. Within the past two decades, scientific computing has become a contributor to essentially all scientific research programs. It is particularly important to the solution of research problems that are (i) insoluble by traditional theoretical and experimental approaches, e.g., prediction of future climates or the fate of underground contaminants; (ii) hazardous to study in the laboratory, e.g., characterization of the chemistry of radionuclides or other toxic chemicals; or (iii) time-consuming or expensive to solve by traditional means, e.g., development of new materials, determination of the structure of proteins, understanding plasma instabilities, or exploring the limitations of the ``Standard Model'' of particle physics. In many cases, theoretical and experimental approaches do not provide sufficient information to understand and predict the behavior of the systems being studied. Computational modeling and simulation, which allows a description of the system to be constructed from basic theoretical principles and the available experimental data, are keys to solving such problems. Advanced scientific computing is indispensable to DOE's missions. It is essential to simulate and predict the behavior of nuclear weapons, accelerate the development of new energy technologies, and the aid in discovery of new scientific knowledge. As the lead government funding agency for basic research in the physical sciences, the Office of Science has a special responsibility to ensure that its research programs continue to advance the frontiers of science. All of the research programs in DOE's Office of Science--in Basic Energy Sciences, Biological and Environmental Research, Fusion Energy Sciences, and High-Energy and Nuclear Physics--have identified major scientific questions that can only be addressed through advances in scientific computing. This will require significant enhancements to the Office of Science's scientific computing programs. These include both more capable computing platforms and the development of the sophisticated mathematical and software tools required for large scale simulations. Existing highly parallel computer architectures, while extremely effective for many applications, including solution of some important scientific problems, are only able to operate at 5-10 percent of their theoretical maximum capability on other applications. Therefore, we have initiated a Next Generation Architecture program to evaluate the effectiveness of various different computer architectures in cooperation with the National Nuclear Security Administration (NNSA) and the Defense Advanced Research Project Agency to identify those architectures which are most effective in addressing specific types of simulations. To address the need for mathematical and software tools, and to develop highly efficient simulation codes for scientific discovery, the Office of Science launched the Scientific Discovery through Advanced Computing (SciDAC) program. We have assembled interdisciplinary teams and collaborations to develop the necessary state-of-the-art mathematical algorithms and software, supported by appropriate hardware and middleware infrastructure to use terascale computers effectively to advance fundamental scientific research essential to the DOE mission. These activities are central to the future of our mission. Advanced scientific computing will continue to be a key contributor to scientific research as we enter the twenty-first century. Major scientific challenges exist in all Office of Science research programs that can be addressed by advanced scientific supercomputing. Designing materials atom-by-atom, revealing the functions of proteins, understanding and controlling plasma turbulence, designing new particle accelerators, and modeling global climate change, are just a few examples. * * * Today, high-end scientific computation has reached a threshold which we were all made keenly aware of when the Japanese Earth Simulator was turned on. The Earth Simulator worked remarkably well on real physical problems at sustained speeds that have never been achieved before. The ability to get over 25 teraFLOPS in geophysical science problems was not only an achievement, but it truly opened a new world. So the question before us at today's hearing--``Supercomputing: Is the U.S. on the Right Path''--is very timely. There is general recognition of the opportunities that high-end computation provides, and this Administration has a path forward to meet this challenge. The tools for scientific discovery have changed. Previously, science had been limited to experiment and theory as the two pillars for investigation of the laws of nature. With the advent of what many refer to as ``Ultra-Scale'' computation,'' a third pillar--simulation-- has been added to the foundation of scientific discovery. Modern computational methods are developing at such a rapid rate that computational simulation is possible on a scale that is comparable in importance with experiment and theory. The remarkable power of these facilities is opening new vistas for science and technology. Tradition has it that scientific discovery is based on experiment, buttressed by theory. Sometimes the order is reversed, theory leads to concepts that are tested and sometimes confirmed by experiment. But more often, experiment provides evidence that drives theoretical reasoning. Thus, Dr. Samuel Johnson, in his Preface to Shakespeare, writes: ``Every cold empirick, when his heart is expanded by a successful experiment, swells into a theorist.'' Many times, scientific discovery is counter-intuitive, running against conventional wisdom. Probably the most vivid current example is the experiment that demonstrated that the expansion of our Universe is accelerating, rather than in steady state or contracting. We have yet to understand the theoretical origins for this surprise. During my scientific career, computers have developed from the now ``creaky'' IBM 701, upon which I did my thesis research, to the so- called massively parallel processors or MPP machines, that fill rooms the size of football fields, and use as much power as a small city. The astonishing speeds of these machines, especially the Earth Simulator, allow Ultra-Scale computation to inform our approach to science, and I believe social sciences and the humanities. We are now able to contemplate exploration of worlds never before accessible to mankind. Previously, we used computers to solve sets of equations representing physical laws too complicated to solve analytically. Now we can simulate systems to discover physical laws for which there are no known predictive equations. We can model physical or social structures with hundreds of thousands, or maybe even millions, of ``actors,'' interacting with one another in a complex fashion. The speed of our new computational environment allows us to test different inter-actor (or inter-personal) relations to see what macroscopic behaviors can ensue. Simulations can determine the nature of the fundamental ``forces'' or interactions between ``actors.'' Computer simulation is now a major force for discovery in its own right. We have moved beyond using computers to solve very complicated sets of equations to a new regime in which scientific simulation enables us to obtain scientific results and to perform discovery in the same way that experiment and theory have traditionally been used to accomplish those ends. We must think of high-end computation as the third of the three pillars that support scientific discovery, and indeed there are areas where the only approach to a solution is through high-end computation--and that has consequences. * * * American industry certainly is fully conversant with the past, present and prospective benefits of high-end computation. The Office of Science has received accolades for our research accomplishments from corporations such as General Electric and General Motors. We have met with the vice presidents for research of these and other member companies of the Industrial Research Institute. We learned, for example, that GE is using simulation very effectively to detect flaws in jet engines. What's more, we were told that, if the engine flaws identified by simulation were to go undetected, the life cycle of those GE machines would be reduced by a factor of two--and that would cause GE a loss of over $100,000,000.00. The market for high-end computation extends beyond science, into applications, creating a commercial market for ultra-scale computers. The science and technology important to industry can generate opportunities measured in hundreds of million, and perhaps billions of dollars. Here are just a few examples: From General Motors: ``General Motors currently saves hundreds of millions of dollars by using its in-house high performance computing capability of more than 3.5 teraFLOPS in several areas of its new vehicle design and development processes. These include vehicle crash simulation, safety models, vehicle aerodynamics, thermal and combustion analyses, and new materials research. The savings are realized through reductions in the costs of prototyping and materials used. However, the growing need to meet higher safety standards, greater fuel efficiency, and lighter but stronger materials, demands a steady yearly growth rate of 30 to 50 percent in computational capabilities but will not be met by existing architectures and technologies.. . .A computing architecture and capability on the order of 100 teraFLOPS for example would have quite an economic impact, on the order of billions of dollars, in the commercial sector in its product design, development, and marketing.'' And from General Electric: ``Our ability to model, analyze and validate complex systems is a critical part of the creation of many of our products and design. Today we make extensive use of high-performance computing based technologies to design and develop products ranging from power systems and aircraft engines to medical imaging equipment. Much of what we would like to achieve with these predictive models is out of reach due to limitations in current generation computing capabilities. Increasing the fidelity of these models demands substantial increases in high-performance computing system performance. We have a vital interest in seeing such improvements in the enabling high-performance computing technologies.. . .In order to stay competitive in the global marketplace, it is of vital importance that GE can leverage advances in high-performance computing capability in the design of its product lines. Leadership in high-performance computing technologies and enabling infrastructure is vital to GE if we wish to maintain our technology leadership.'' Consider the comparison between simulations and prototyping for GE jet engines. For evaluation of a design alternative for the purpose of optimization of a compressor for a jet engine design, GE would require 3.1 1018 floating point operations, or over a month at a sustained speed of one teraFLOP, which is today's state-of-the-art. To do this for the entire engine would require sustained computing power of 50 teraFLOPS for the same period. This is to be compared with millions of dollars, several years, and designs and re-designs for physical prototyping. Opportunities abound in other fields such as pharmaceuticals, oil and gas exploration, and aircraft design. The power of advance scientific computation is just beginning to be realized. One reason that I have emphasized this so much is that some seem to think that advanced scientific computation is the province of the Office of Science and other federal science agencies and therefore is not attractive to the vendors in this field. I believe that's incorrect. I believe instead that our leading researchers are laying out a direction and an understanding of available opportunities. These opportunities spur markets for high-end computation quite comparable to the commercial market which we have seen in the past but requiring the efficiencies and the speeds which high-end computation can provide. * * * Discovery through simulation requires sustained speeds starting at 50 to 100 teraFLOPS to examine problems in accelerator science and technology, astrophysics, biology, chemistry and catalysis, climate prediction, combustion, computational fluid dynamics, computational structural and systems biology, environmental molecular science, fusion energy science, geosciences, groundwater protection, high energy physics, materials science and nanoscience, nuclear physics, soot formation and growth, and more (see http://www.ultrasim.info/ doe-docs/). Physicists in Berkeley, California, trying to determine whether our universe will continue to expand or eventually collapse, gather data from dozens of distant supernovae. By analyzing the data and simulating another 10,000 supernovae on supercomputers (at the National Energy Research Scientific Computing Center or NERSC) the scientists conclude that the universe is expanding--and at an accelerating rate. I just returned from Vienna, where I was privileged to lead the U.S. delegation in negotiations on the future direction for ITER, an international collaboration that hopes to build a burning plasma fusion reactor, which holds out promise for the realization of fusion power. The United States pulled out of ITER in 1998. We're back in it this year. What changed were simulations that showed that the new ITER design will in fact be capable of achieving and sustaining burning plasma. We haven't created a stable burning plasma yet, but the simulations give us confidence that the experiments which we performed at laboratory scales could be realized in larger machines at higher temperatures and densities. Looking to the future, we are beginning a Fusion Simulation Project to build a computer model that will fully simulate a burning plasma to both predict and interpret ITER performance and, eventually, assist in the design of a commercially feasible fusion power reactor. Our best estimate, however is that success in this effort will require at least 50 teraFLOPS of sustained computing power. Advances in scientific computation are also vital to the success of the Office of Science's Genomes to Life program. The Genomes to Life program will develop new knowledge about how micro-organisms grow and function and will marry this to a national infrastructure in computational biology to build a fundamental understanding of living systems. Ultimately this approach will offer scientists insights into how to use or replicate microbiological processes to benefit the Nation. In particular, the thrust of the Genomes to Life program is aimed directly at Department of Energy concerns: developing new sources of energy; mitigating the long-term effects of climate change through carbon sequestration; cleaning up the environment; and protecting people from adverse effects of exposure to environmental toxins and radiation. All these benefits--and more--will be possible as long as the Genomes to Life program achieves a basic understanding of thousands of microbes and microbial systems in their native environments over the next 10 to 20 years. To meet this challenge, however, we must address huge gaps not only in knowledge but also in technology, computing, data storage and manipulation, and systems-level integration. The Office of Science also is a leader in research efforts to capitalize on the promise of nanoscale science. In an address to the American Association for the Advancement of Science in February 2002, Dr. John Marburger, Director of the Office of Science and Technology Policy, noted, ``. . .[W]e are in the early stages of a revolution in science nearly as profound as the one that occurred early in the last century with the birth of quantum mechanics,'' a revolution spurred in part by ``the availability of powerful computing and information technology.'' ``The atom-by-atom understanding of functional matter,'' Dr. Marburger continued, ``requires not only exquisite instrumentation, but also the capacity to capture, store and manipulate vast amounts of data. The result is an unprecedented ability to design and construct new materials with properties that are not found in nature.. . .[W]e are now beginning to unravel the structures of life, atom-by-atom using sensitive machinery under the capacious purview of powerful computing.'' In both nanotechnology and biotechnology, this revolution in science promises a revolution in industry. In order to exploit that promise, however, we will need both new instruments and more powerful computers, and the Office of Science has instituted initiatives to develop both. We have begun construction at Oak Ridge National Laboratory on the first of five Nanoscale Science Research Centers located to take advantage of the complementary capabilities of other large scientific facilities, such as the Spallation Neutron Source at Oak Ridge, our synchrotron light sources at Argonne, Brookhaven and Lawrence Berkeley, and semiconductor, microelectronics and combustion research facilities at Sandia and Los Alamos. When complete, these five Office of Science nanocenters will provide the Nation with resources unmatched anywhere else in the world. To determine the level of computing resources that will be required, the Office of Science sponsored a scientific workshop on Theory and Modeling in Nanoscience, which found that simulation will be critical to progress, and that new computer resources are required. As a first step to meeting that need, our Next Generation Architecture initiative is evaluating different computer architectures to determine which are most effective for specific scientific applications, including nanoscience simulations. There are many other examples where high-end computation has changed and will change the nature of the field. My own field is complex systems. I work in a somewhat arcane area called spin glasses, where we can examine the dynamic properties of these very complex systems, which in fact are related to a number of very practical applications. Through scientific simulation, a correlation length was predicted for a completely random material, a concept unknown before. Simulation led to the discovery that there was a definable correlation length in this completely random system. Our experiments confirmed this hypothesis. Again, insights were created that simply were not possible from a set of physical equations that needed solutions, with observable consequences. There are countless opportunities and examples where similar advances could be made. * * * As the Chairman and Members of this committee know, the Bush Administration shares Congress' keen interest in high-end computation for both scientific discovery and economic development. A senior official of the Office of Science is co-chairing the interagency High- End Computing Revitalization Task Force, which includes representatives from across the government and the private sector. We are working very closely with the NNSA, the Department of Defense, the National Aeronautics and Space Administration, the National Science Foundation, and the National Institutes of Health to assess how best to coordinate and leverage our agencies' high-end computation investments now and in the future. DOE is playing a major role in the task force through the Office of Science and the NNSA, and many of the task force's findings and plans are based on Office of Science practices in advanced computing and simulation. One of the major challenges in this area is one of metrics. How do we know what we are trying to accomplish, and how can we measure how we're getting there? What are the opportunities? What are the barriers? What should we be addressing as we begin to explore this new world? Our problem in the past has been that, where we have large computational facilities, we have cut them up in little pieces and the large-scale scientific programs that some researchers are interested in have never really had a chance to develop. There's nothing wrong with our process; it is driven by a peer review system. But for some promising research efforts, there simply have not been enough cycles or there wasn't an infrastructure which would allow large-scale simulations to truly develop and produce the kind of discoveries we hope to achieve. Recognizing this, the Office of Science has announced that ten percent of our National Energy Research Scientific Computing Center at Lawrence Berkeley National Laboratory--now at ten teraFLOP peak speed-- is going to be made available for grand challenge calculations. We are literally going to carve out 4.5 million processor hours and 100 terabytes of disk space for perhaps four or five scientific problems of major importance. We are calling this initiative INCITE--the Innovative and Novel Computational Impact on Theory and Experiment--and we expect to be ready to proceed with it around August 1, 2003. At that time, we will open the competition to all, whether or not they are affiliated with or funded by DOE. We are launching the INCITE initiative for two reasons. For one, it's the right thing to do: there are opportunities for major accomplishments in this field of science. In addition, there is also a ``sociology'' that we need to develop. Given the size and complexity of the machines required for sustained speeds in the 50 to 100 teraFLOPS regime, the sociology of high-end computation will probably have to change. One can think of the usage of ultra-scale computers as akin to that of our current light sources: large machines used by groups of users on a shared basis. Following the leadership of our SciDAC program, interdisciplinary teams and collaborators will develop the necessary state-of-the-art mathematical algorithms and software, supported by appropriate hardware and middleware infrastructure, to use terascale computers effectively to advance fundamental research in science. These teams will associate on the basis of the mathematical infrastructure of problems of mutual interest, working with efficient, balanced computational architectures. The large amount of data, the high sustained speeds, and the cost will probably lead to concentration of computing power in only a few sites, with networking useful for communication and data processing, but not for core computation at terascale speeds. Peer review of proposals will be used to allocate machine time. Industry will be welcome to participate, as has happened in our light sources. Teams will make use of the facilities as user groups, using significant portions (or all) of the machine, depending on the nature of their computational requirements. Large blocks of time will enable scientific discovery of major magnitude, justifying the large investment ultra- scale computation will require. We will open our computational facilities to everyone. Ten percent of NERSC's capability will be available to the entire world. Prospective users will not have to have a DOE contract, or grant, or connection. The applications will be peer reviewed, and will be judged solely on their scientific merit. We need to learn how to develop the sociology that can encourage and then support computation of this magnitude; this is a lot of computer time. It may be the case that teams rather than individuals will be involved. It even is possible that one research proposal will be so compelling that the entire ten percent of NERSC will be allocated to that one research question. The network that may be required to handle that amount of data has to be developed. There is an ES network which we are involved in, and we are studying whether or not it will be able to handle the massive amounts of data that could be produced under this program. We need to get scientific teams--the people who are involved in algorithms, the computer scientists, and the mathematicians--together to make the most efficient use of these facilities. That's what this opening up at NERSC is meant to do. We want to develop the community of researchers within the United States--and frankly around the world-- that can take advantage of these machines and produce the results that will invigorate and revolutionize their fields of study. But this is just the beginning. * * * As we develop the future high-end computational facilities for this nation and world, it is clearly our charge and our responsibility to develop scientific opportunities for everyone. This has been the U.S. tradition. It has certainly been an Office of Science tradition, and we intend to see that this tradition continues, and not just in the physical sciences. We are now seeing other fields recognizing that opportunities are available to them. In biology, we are aware that protein folding is a very difficult but crucial issue for cellular function. The time scales that biologists work with can scale from a femto-second to seconds-a huge span of time which our current simulation capabilities are unable to accommodate. High-performance computing provides a new window for researchers to observe the natural world with a fidelity that could only be imagined a few years ago. Research investments in advanced scientific computing will equip researchers with premier computational tools to advance knowledge and to solve the most challenging scientific problems facing the Nation. With vital support from this committee, the Congress and the Administration, we in the Office of Science will help lead the U.S. further into the new world of supercomputing. We are truly talking about scientific discovery. We are talking about a third pillar of support. We are talking about opportunities to understand properties of nature that have never before been explored. That's the concept, and it explains the enormous excitement that we feel about this most promising field. We are very gratified that this committee is so intent on enabling us to pursue so many important opportunities, for the sake of scientific discovery, technological innovation, and economic competitiveness. Thank you very much. Biography for Raymond L. Orbach Dr. Raymond L. Orbach was sworn in as the 14th Director of the Office of Science at the Department of Energy (DOE) on March 14, 2002. As Director of the Office of Science (SC), Dr. Orbach manages an organization that is the third largest federal sponsor of basic research in the United States and is viewed as one of the premier science organizations in the world. The SC fiscal year 2002 budget of $3.3 billion funds programs in high energy and nuclear physics, basic energy sciences, magnetic fusion energy, biological and environmental research, and computational science. SC, formerly the Office of Energy Research, also provides management oversight of the Chicago and Oak Ridge Operations Offices, the Berkeley and Stanford Site Offices, and 10 DOE non-weapons laboratories. Prior to his appointment, Dr. Orbach served as Chancellor of the University of California (UC), Riverside from April 1992 through March 2002; he now holds the title Chancellor Emeritus. During his tenure as Chancellor, UC-Riverside grew from the smallest to one of the most rapidly growing campuses in the UC system. Enrollment increased from 8,805 to more than 14,400 students with corresponding growth in faculty and new teaching, research, and office facilities. In addition to his administrative duties at UC-Riverside, Dr. Orbach maintained a strong commitment to teaching. He sustained an active research program; worked with postdoctoral, graduate, and undergraduate students in his laboratory; and taught the freshman physics course each winter quarter. As Distinguished Professor of Physics, Dr. Orbach set the highest standards for academic excellence. From his arrival, UC-Riverside scholars led the Nation for seven consecutive years in the number of fellows elected to the prestigious American Association for the Advancement of Science (AAAS). Dr. Orbach began his academic career as a postdoctoral fellow at Oxford University in 1960 and became an assistant professor of applied physics at Harvard University in 1961. He joined the faculty of the University of California, Los Angeles (UCLA) two years later as an associate professor, and became a full professor in 1966. From 1982 to 1992, he served as the Provost of the College of Letters and Science at UCLA. Dr. Orbach's research in theoretical and experimental physics has resulted in the publication of more than 240 scientific articles. He has received numerous honors as a scholar including two Alfred P. Sloan Foundation Fellowships, a National Science Foundation Senior Postdoctoral Fellowship, a John Simon Guggenheim Memorial Foundation Fellowship, the Joliot Curie Professorship at the Ecole Superieure de Physique et Chimie Industrielle de la Ville de Paris, the Lorentz Professorship at the University of Leiden in the Netherlands, and the 1991-1992 Andrew Lawson Memorial Lecturer at UC-Riverside. He is a fellow of the American Physical Society and the AAAS. Dr. Orbach has also held numerous visiting professorships at universities around the world. These include the Catholic University of Leuven in Belgium, Tel Aviv University, and the Imperial College of Science and Technology in London. He also serves as a member of 20 scientific, professional, or civic boards. Dr. Orbach received his Bachelor of Science degree in Physics from the California Institute of Technology in 1956. He received his Ph.D. degree in Physics from the University of California, Berkeley, in 1960 and was elected to Phi Beta Kappa. Dr. Orbach was born in Los Angeles, California. He is married to Eva S. Orbach. They have three children and seven grandchildren. Chairman Boehlert. Thank you very much, Dr. Orbach. You noticed the red light was on, and the Chair was generous with the time. As I said, I am not going to be arbitrary. I wish the appropriators were as generous with the funding for your office as we are with time for your views, but we are working continually together on that one. Dr. Freeman. STATEMENT OF DR. PETER A. FREEMAN, ASSISTANT DIRECTOR, COMPUTER AND INFORMATION SCIENCE AND ENGINEERING DIRECTORATE, NATIONAL SCIENCE FOUNDATION Dr. Freeman. Good morning, Mr. Chairman, Mr. Hall, and distinguished Members of the Committee. NSF deeply appreciates this committee's long time support and recognizes your special interest in computing, so I am delighted to be here today to discuss those topics with you. Supercomputing is a field that NSF, as you noted in your opening remarks, has championed for many years. And it is one in which we intend to continue to lead the way. At the same time, we are committed to realizing the compelling vision described in the report of the recent NSF Advisory Panel on Cyberinfrastructure, commonly known as the Atkins Committee. They have forcefully told us, and I quote, ``A new age has dawned in scientific and engineering research, '' that will be enabled by cyberinfrastructure. The term ``cyberinfrastructure'' sounds exotic and is sometimes confused as being something new, but in reality, it is intended to signify a set of integrated facilities and services essential to the conduct of leading edge science and engineering. Cyberinfrastructure must include a range of supercomputers as well as massive storage, high performance networks, databases, lots of software, and above all, highly trained people. An advanced cyberinfrastructure, with supercomputing as an important element, promises to revolutionize research in the 21st Century. The opportunities that are presented to us in this area must be exploited for the benefit of all of our citizens for their continuing health, security, education, and wealth. We are committed to a key recommendation of the Atkins Report, namely that NSF, in partnership with other agencies and other organizations, must make significant investments in the creation, deployment, and application of advanced cyberinfrastructure to empower continued U.S. leadership in science and engineering. To bring about this scientific revolution, we must maintain a broad discourse about cyberinfrastructure. Supercomputers are essential, but without software, networks, massive storage, databases, and trained people all integrated securely, they will not deliver their potential. Further, there are now many areas of science that need this integrated and balanced support or balanced approach more than they need any single element. My written testimony provides detailed answers to the questions you posed for this hearing, but allow me to summarize them. NSF will most definitely continue its commitment to supercomputing and will provide the support necessary to utilize it in all of science and engineering. Supercomputing capability is an essential component in the cyberinfrastructure vision, and we are committed to expanding this capability in the future. NSF's recent investments in grid computing underscore the importance of integrating supercomputing within a cyberinfrastructure. Indeed, the first increment of our funding and our terascale efforts was for a supercomputer, which at the time it wa"
    }
}
{
    "id": "dbpedia_439_2",
    "rank": 95,
    "data": {
        "url": "https://blog.google/technology/ai/google-gemini-ai/",
        "read_more_link": "",
        "language": "en",
        "title": "Introducing Gemini: our largest and most capable AI model",
        "top_image": "https://storage.googleapis.com/gweb-uniblog-publish-prod/images/Gemini_SS.width-1300.jpg",
        "meta_img": "https://storage.googleapis.com/gweb-uniblog-publish-prod/images/Gemini_SS.width-1300.jpg",
        "images": [
            "https://storage.googleapis.com/gweb-uniblog-publish-prod/images/SundarPichai_2x.max-208x156.format-webp.webp",
            "https://storage.googleapis.com/gweb-uniblog-publish-prod/images/Demis_headshot.max-244x184.format-webp.webp",
            "https://storage.googleapis.com/gweb-uniblog-publish-prod/images/final_keyword_header.width-600.format-webp.webp 600w, https://storage.googleapis.com/gweb-uniblog-publish-prod/images/final_keyword_header.width-1200.format-webp.webp 1200w, https://storage.googleapis.com/gweb-uniblog-publish-prod/images/final_keyword_header.width-1600.format-webp.webp 1600w",
            "https://storage.googleapis.com/gweb-uniblog-publish-prod/images/06_Foundation_01.width-100.format-webp.webp",
            "https://storage.googleapis.com/gweb-uniblog-publish-prod/original_images/gemini_final_text_table_amendment_13_12_23.gif ",
            "https://storage.googleapis.com/gweb-uniblog-publish-prod/original_images/gemini_final_multimodal_table_bigger_font_amendment_lines.gif ",
            "https://storage.googleapis.com/gweb-uniblog-publish-prod/images/Ariel_ScienceDemo_TaylorSebastian.width-100.format-webp.webp",
            "https://storage.googleapis.com/gweb-uniblog-publish-prod/images/Ariel_PhysicsHomework_Sam.width-100.format-webp.webp",
            "https://storage.googleapis.com/gweb-uniblog-publish-prod/images/Ariel_ACDemo_RemiGabi_v001.width-100.format-webp.webp",
            "https://storage.googleapis.com/gweb-uniblog-publish-prod/images/final_keyword_tpu.width-100.format-webp.webp ",
            "https://storage.googleapis.com/gweb-uniblog-publish-prod/original_images/Updated_Grid_5L3IYfa.gif",
            "https://blog.google/static/blogv2/images/newsletter-envelope-back.svg",
            "https://blog.google/static/blogv2/images/newsletter-envelope-letter-approved.svg",
            "https://blog.google/static/blogv2/images/newsletter-envelope-letter-google.svg",
            "https://blog.google/static/blogv2/images/newsletter-envelope-front.svg",
            "https://storage.googleapis.com/gweb-uniblog-publish-prod/original_images/Athlete_Hero.jpg",
            "https://storage.googleapis.com/gweb-uniblog-publish-prod/images/Screenshot_2024-08-07_at_10.12.max-1200x416.format-webp.webp",
            "https://storage.googleapis.com/gweb-uniblog-publish-prod/images/social_share_home.max-1200x416.format-webp.webp",
            "https://storage.googleapis.com/gweb-uniblog-publish-prod/images/Hero_Image_Frame_2.max-1200x416.format-webp.webp",
            "https://storage.googleapis.com/gweb-uniblog-publish-prod/images/wildfires_emea_hero_B.max-1200x416.format-webp.webp",
            "https://storage.googleapis.com/gweb-uniblog-publish-prod/images/AI_Workspace_Prompts.max-1200x416.format-webp.webp",
            "https://blog.google/static/blogv2/images/newsletter_toast.svg"
        ],
        "movies": [],
        "keywords": [],
        "meta_keywords": [
            "None"
        ],
        "tags": null,
        "authors": [
            "Sundar Pichai",
            "Demis Hassabis"
        ],
        "publish_date": "2023-12-06T15:00:00+00:00",
        "summary": "",
        "meta_description": "Gemini is our most capable and general model, built to be multimodal and optimized for three different sizes: Ultra, Pro and Nano.",
        "meta_lang": "en",
        "meta_favicon": "/favicon.ico",
        "meta_site_name": "Google",
        "canonical_link": "https://blog.google/technology/ai/google-gemini-ai/",
        "text": "A note from Google and Alphabet CEO Sundar Pichai:\n\nEvery technology shift is an opportunity to advance scientific discovery, accelerate human progress, and improve lives. I believe the transition we are seeing right now with AI will be the most profound in our lifetimes, far bigger than the shift to mobile or to the web before it. AI has the potential to create opportunities — from the everyday to the extraordinary — for people everywhere. It will bring new waves of innovation and economic progress and drive knowledge, learning, creativity and productivity on a scale we haven’t seen before.\n\nThat’s what excites me: the chance to make AI helpful for everyone, everywhere in the world.\n\nNearly eight years into our journey as an AI-first company, the pace of progress is only accelerating: Millions of people are now using generative AI across our products to do things they couldn’t even a year ago, from finding answers to more complex questions to using new tools to collaborate and create. At the same time, developers are using our models and infrastructure to build new generative AI applications, and startups and enterprises around the world are growing with our AI tools.\n\nThis is incredible momentum, and yet, we’re only beginning to scratch the surface of what’s possible.\n\nWe’re approaching this work boldly and responsibly. That means being ambitious in our research and pursuing the capabilities that will bring enormous benefits to people and society, while building in safeguards and working collaboratively with governments and experts to address risks as AI becomes more capable. And we continue to invest in the very best tools, foundation models and infrastructure and bring them to our products and to others, guided by our AI Principles.\n\nNow, we’re taking the next step on our journey with Gemini, our most capable and general model yet, with state-of-the-art performance across many leading benchmarks. Our first version, Gemini 1.0, is optimized for different sizes: Ultra, Pro and Nano. These are the first models of the Gemini era and the first realization of the vision we had when we formed Google DeepMind earlier this year. This new era of models represents one of the biggest science and engineering efforts we’ve undertaken as a company. I’m genuinely excited for what’s ahead, and for the opportunities Gemini will unlock for people everywhere.\n\n– Sundar\n\nIntroducing Gemini\n\nBy Demis Hassabis, CEO and Co-Founder of Google DeepMind, on behalf of the Gemini team\n\nAI has been the focus of my life's work, as for many of my research colleagues. Ever since programming AI for computer games as a teenager, and throughout my years as a neuroscience researcher trying to understand the workings of the brain, I’ve always believed that if we could build smarter machines, we could harness them to benefit humanity in incredible ways.\n\nThis promise of a world responsibly empowered by AI continues to drive our work at Google DeepMind. For a long time, we’ve wanted to build a new generation of AI models, inspired by the way people understand and interact with the world. AI that feels less like a smart piece of software and more like something useful and intuitive — an expert helper or assistant.\n\nToday, we’re a step closer to this vision as we introduce Gemini, the most capable and general model we’ve ever built.\n\nGemini is the result of large-scale collaborative efforts by teams across Google, including our colleagues at Google Research. It was built from the ground up to be multimodal, which means it can generalize and seamlessly understand, operate across and combine different types of information including text, code, audio, image and video.\n\nNext-generation capabilities\n\nUntil now, the standard approach to creating multimodal models involved training separate components for different modalities and then stitching them together to roughly mimic some of this functionality. These models can sometimes be good at performing certain tasks, like describing images, but struggle with more conceptual and complex reasoning.\n\nWe designed Gemini to be natively multimodal, pre-trained from the start on different modalities. Then we fine-tuned it with additional multimodal data to further refine its effectiveness. This helps Gemini seamlessly understand and reason about all kinds of inputs from the ground up, far better than existing multimodal models — and its capabilities are state of the art in nearly every domain.\n\nLearn more about Gemini’s capabilities and see how it works.\n\nSophisticated reasoning\n\nGemini 1.0’s sophisticated multimodal reasoning capabilities can help make sense of complex written and visual information. This makes it uniquely skilled at uncovering knowledge that can be difficult to discern amid vast amounts of data.\n\nIts remarkable ability to extract insights from hundreds of thousands of documents through reading, filtering and understanding information will help deliver new breakthroughs at digital speeds in many fields from science to finance.\n\nBuilt with responsibility and safety at the core\n\nAt Google, we’re committed to advancing bold and responsible AI in everything we do. Building upon Google’s AI Principles and the robust safety policies across our products, we’re adding new protections to account for Gemini’s multimodal capabilities. At each stage of development, we’re considering potential risks and working to test and mitigate them.\n\nGemini has the most comprehensive safety evaluations of any Google AI model to date, including for bias and toxicity. We’ve conducted novel research into potential risk areas like cyber-offense, persuasion and autonomy, and have applied Google Research’s best-in-class adversarial testing techniques to help identify critical safety issues in advance of Gemini’s deployment.\n\nTo identify blindspots in our internal evaluation approach, we’re working with a diverse group of external experts and partners to stress-test our models across a range of issues.\n\nTo diagnose content safety issues during Gemini’s training phases and ensure its output follows our policies, we’re using benchmarks such as Real Toxicity Prompts, a set of 100,000 prompts with varying degrees of toxicity pulled from the web, developed by experts at the Allen Institute for AI. Further details on this work are coming soon.\n\nTo limit harm, we built dedicated safety classifiers to identify, label and sort out content involving violence or negative stereotypes, for example. Combined with robust filters, this layered approach is designed to make Gemini safer and more inclusive for everyone. Additionally, we’re continuing to address known challenges for models such as factuality, grounding, attribution and corroboration.\n\nResponsibility and safety will always be central to the development and deployment of our models. This is a long-term commitment that requires building collaboratively, so we’re partnering with the industry and broader ecosystem on defining best practices and setting safety and security benchmarks through organizations like MLCommons, the Frontier Model Forum and its AI Safety Fund, and our Secure AI Framework (SAIF), which was designed to help mitigate security risks specific to AI systems across the public and private sectors. We’ll continue partnering with researchers, governments and civil society groups around the world as we develop Gemini.\n\nMaking Gemini available to the world\n\nGemini 1.0 is now rolling out across a range of products and platforms:\n\nGemini Pro in Google products\n\nWe’re bringing Gemini to billions of people through Google products.\n\nStarting today, Bard will use a fine-tuned version of Gemini Pro for more advanced reasoning, planning, understanding and more. This is the biggest upgrade to Bard since it launched. It will be available in English in more than 170 countries and territories, and we plan to expand to different modalities and support new languages and locations in the near future.\n\nWe’re also bringing Gemini to Pixel. Pixel 8 Pro is the first smartphone engineered to run Gemini Nano, which is powering new features like Summarize in the Recorder app and rolling out in Smart Reply in Gboard, starting with WhatsApp, Line and KakaoTalk — with more messaging apps coming next year.\n\nIn the coming months, Gemini will be available in more of our products and services like Search, Ads, Chrome and Duet AI.\n\nWe’re already starting to experiment with Gemini in Search, where it's making our Search Generative Experience (SGE) faster for users, with a 40% reduction in latency in English in the U.S., alongside improvements in quality.\n\nBuilding with Gemini\n\nStarting on December 13, developers and enterprise customers can access Gemini Pro via the Gemini API in Google AI Studio or Google Cloud Vertex AI.\n\nGoogle AI Studio is a free, web-based developer tool to prototype and launch apps quickly with an API key. When it's time for a fully-managed AI platform, Vertex AI allows customization of Gemini with full data control and benefits from additional Google Cloud features for enterprise security, safety, privacy and data governance and compliance.\n\nAndroid developers will also be able to build with Gemini Nano, our most efficient model for on-device tasks, via AICore, a new system capability available in Android 14, starting on Pixel 8 Pro devices. Sign up for an early preview of AICore.\n\nGemini Ultra coming soon\n\nFor Gemini Ultra, we’re currently completing extensive trust and safety checks, including red-teaming by trusted external parties, and further refining the model using fine-tuning and reinforcement learning from human feedback (RLHF) before making it broadly available.\n\nAs part of this process, we’ll make Gemini Ultra available to select customers, developers, partners and safety and responsibility experts for early experimentation and feedback before rolling it out to developers and enterprise customers early next year.\n\nEarly next year, we’ll also launch Bard Advanced, a new, cutting-edge AI experience that gives you access to our best models and capabilities, starting with Gemini Ultra."
    }
}
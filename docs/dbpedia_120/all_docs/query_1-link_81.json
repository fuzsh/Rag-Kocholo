{
    "id": "dbpedia_120_1",
    "rank": 81,
    "data": {
        "url": "https://towardsdatascience.com/an-overview-of-monte-carlo-methods-675384eb1694",
        "read_more_link": "",
        "language": "en",
        "title": "An Overview of Monte Carlo Methods",
        "top_image": "https://miro.medium.com/v2/resize:fit:966/1*aZyQ3wT7mTZXzACje3KNGQ.jpeg",
        "meta_img": "https://miro.medium.com/v2/resize:fit:966/1*aZyQ3wT7mTZXzACje3KNGQ.jpeg",
        "images": [
            "https://miro.medium.com/v2/resize:fill:64:64/1*dmbNkD5D-u45r44go_cf0g.png",
            "https://miro.medium.com/v2/resize:fill:88:88/1*-OATlwNAbWUpwEkkeFQeOA.jpeg",
            "https://miro.medium.com/v2/resize:fill:48:48/1*CJe3891yB1A1mzMdqemkdg.jpeg",
            "https://miro.medium.com/v2/resize:fill:144:144/1*-OATlwNAbWUpwEkkeFQeOA.jpeg",
            "https://miro.medium.com/v2/resize:fill:64:64/1*CJe3891yB1A1mzMdqemkdg.jpeg"
        ],
        "movies": [],
        "keywords": [],
        "meta_keywords": [
            ""
        ],
        "tags": null,
        "authors": [
            "Christopher Pease",
            "medium.com",
            "@chris.m.pease"
        ],
        "publish_date": "2018-09-06T16:22:33.992000+00:00",
        "summary": "",
        "meta_description": "Monte Carlo (MC) methods are a subset of computational algorithms that use the process of repeated random sampling to make numerical estimations of unknown parameters. They allow for the modeling of…",
        "meta_lang": "en",
        "meta_favicon": "https://miro.medium.com/v2/resize:fill:256:256/1*VzTUkfeGymHP4Bvav-T-lA.png",
        "meta_site_name": "Medium",
        "canonical_link": "https://towardsdatascience.com/an-overview-of-monte-carlo-methods-675384eb1694",
        "text": "Christopher Pease\n\n·\n\nFollow\n\nPublished in\n\nTowards Data Science\n\n·\n\n7 min read\n\n·\n\nSep 6, 2018\n\n--\n\nMonte Carlo (MC) methods are a subset of computational algorithms that use the process of repeated random sampling to make numerical estimations of unknown parameters. They allow for the modeling of complex situations where many random variables are involved, and assessing the impact of risk. The uses of MC are incredibly wide-ranging, and have led to a number of groundbreaking discoveries in the fields of physics, game theory, and finance. There are a broad spectrum of Monte Carlo methods, but they all share the commonality that they rely on random number generation to solve deterministic problems. I hope to outline some of the basic principles of MC, and perhaps infect you with a bit of the excitement that I have about their possible applications.\n\nThe concept was invented by Stanislaw Ulam, a mathematician who devised these methods as part of his contribution to the Manhattan Project. He used the tools of random sampling and inferential statistics to model likelihoods of outcomes, originally applied to a card game (Monte Carlo Solitaire). Ulam later worked with collaborator John von Neumann, using newly developed computer technologies to run simulations to better understand the risks associated with the nuclear project. As you can imagine, modern computational technology allows us to model much more complex systems, with a larger number of random parameters, like so many of the scenarios that we encounter during our everyday lives. Before we consider the complicated systems however, lets talk about a simple case; a game of blackjack.\n\nIf we wanted to find the probability of getting blackjack (an ace along with a ten-valued card), we could simply count the number of possible hands where this is the case, and divide by the total number of possible combinations of cards to get the probability (its around 1/21, if you are curious). But now imagine our sample space is much harder to compute, for example our deck of cards has thousands as opposed to just 52 cards, or better yet we don’t even know how many cards there are. There is another way to find this probability.\n\nWe could hunker down at the table and play a hundred games, recording the outcomes as we play. We might get a blackjack 19, 20 or even 28 times, and assign the probability using any of those values. Seems like a pretty bad way to assess our odds at the casino, but hopefully we’re just playing for fun. Now if we go again and play a thousand, ten thousand, millions of times, the Law of Large Numbers states:\n\n“As the number of identically distributed, randomly generated variables increases, their sample mean (average) approaches their theoretical mean.”\n\nBesides being easily one of the most important laws of statistics, this is the basis for Monte Carlo simulations and allows us to build a stochastic model by the method of statistical trials. Lets look at my favorite (and probably the simplest) example of a MC estimate.\n\nMonte Carlo Estimate of Pi\n\nAs we are all taught in grammar school geometry, pi is a constant that represents the ratio of the circumference to the diameter of a circle. It is irrational, meaning it has infinite digits which follow no pattern. Would you believe me if I told you we can estimate pi to as many digits as we like by simply playing a game of darts? To a certain degree of uncertainty, of course, after all we are playing a game of chance! Lets write a short python script to see how.\n\nAs you can see, we are generating random points within a box, and counting the number of points which fall within an embedded circle. For the sake of simplicity, we only look at the upper quadrant of our dartboard, which has a radius R and is mounted on a square piece of wood of the same width. If we throw 100 darts, and count the number of darts which successfully hit the dartboard, we might obtain the following plot.\n\nIf you’re thinking: “Why are we so bad at this?” you are asking the right question. For this example, the location of our darts must be uniformly distributed throughout the area, so we’re definitely not winning too many games here. From the setup of our game, the probability of a darts hitting the board will be π/4 . As we keep playing, and as n –> ∞ we approach this true value(Thanks Bernoulli!). We can see that this happens below as we increase n by an order of ten.\n\nYou might say: this is all well and good, but I already know that pi is 3.14159… and what in the real world is uniformly distributed? Have no fear, for when using MC methods to model higher-dimensional systems, we will need to sample all sorts of random variables, with different probability distributions that more accurately represent the effects of the parameters in our model. What kind of real world stuff can I do with this?\n\nHigh-Energy Physics\n\nOne major application of Monte Carlo that is near and dear to my heart is in the world of particle physics. In the quantum (very small-scale) world, things are not easily observable and this is especially true at the point of collision in a particle accelerator. MC methods allow physicists to run simulations of these events, based on the Standard Model, and parameters which have been determined from previous experiments. Huge scale projects like the LHC already produce an immense quantity of data, so N is already huge before we even start randomly sampling. So one small thing MC is useful for is probing the fundamental fabric of matter itself.\n\nFinance\n\nIf that doesn’t seem exciting enough to you, MC is extensively used in financial engineering for stock market forecasting. This makes intuitive sense, as the market is impossibly difficult to model, has unbelievably high dimensionality, and has plenty of data to be sampled from. The importance of risk is another large factor in why financial analysts use MC methods. One relatively straightforward application of Monte Carlo in this field is portfolio optimization. I highly recommend this blog post on the topic, which breaks down in detail how to write the code for this type of analysis, but to summarize:\n\nWhen picking a portfolio of stocks, you may be willing to take on different levels of risk depending on your goals. But regardless of your willingness to accept risk, you can maximize your returns per volatility of the portfolio by using Monte Carlo to find the optimal combinations and proportions of stocks. Using historical data one can generate hundreds of thousands of different combinations of stocks in different ratios, to see how each each would perform relative to each other during that time period. Then one can choose the optimal configuration using a metric called a Sharpe ratio (a measure of the performance of an investment’s returns given its risk.)"
    }
}
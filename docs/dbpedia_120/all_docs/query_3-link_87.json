{
    "id": "dbpedia_120_3",
    "rank": 87,
    "data": {
        "url": "https://onlinelibrary.wiley.com/doi/10.1002/dac.2959",
        "read_more_link": "",
        "language": "en",
        "title": "Data transfer minimization for financial derivative pricing using Monte Carlo simulation with GPU in 5G",
        "top_image": "https://onlinelibrary.wiley.com/cms/asset/b6ca840b-39b6-4bb9-b1fa-ca0dae806c09/dac2959-toc-0001-m.jpg?trick=1723203764300",
        "meta_img": "https://onlinelibrary.wiley.com/cms/asset/b6ca840b-39b6-4bb9-b1fa-ca0dae806c09/dac2959-toc-0001-m.jpg?trick=1723203764300",
        "images": [
            "https://onlinelibrary.wiley.com/pb-assets/hub-assets/pericles/logo-header-1690978619437.png",
            "https://onlinelibrary.wiley.com/pb-assets/hub-assets/pericles/mobilehublogo-1690978876347.png",
            "https://pubads.g.doubleclick.net/gampad/ad?iu=$googlePublisherCategory&sz=728x90&tile=1&c=$random",
            "https://onlinelibrary.wiley.com/pb-assets/journal-banners/10991131-1501384668380.jpg",
            "https://onlinelibrary.wiley.com/specs/products/acropolis/pericles/releasedAssets/images/pdf-icon-169a2eb30e52100e76dfa5f4b66998e6.png",
            "https://onlinelibrary.wiley.com/cms/asset/4d79491b-ad54-4919-932f-d6bcc3aae496/dac2959-math-0001.png",
            "https://onlinelibrary.wiley.com/cms/asset/2f1d0151-5372-4830-aabb-b3f264f8cc89/dac2959-math-0002.png",
            "https://onlinelibrary.wiley.com/cms/asset/ce08dba8-5b4f-4415-a06b-71ef1b1f135a/dac2959-math-0003.png",
            "https://onlinelibrary.wiley.com/cms/asset/bdb87b6f-4070-4520-81b9-f6af27b8f39c/dac2959-math-0004.png",
            "https://onlinelibrary.wiley.com/cms/asset/c696bc42-5bfa-421c-97f3-17932d7c8c88/dac2959-math-0005.png",
            "https://onlinelibrary.wiley.com/cms/asset/bc7ae6f1-c321-430e-b37a-2e5a34128e0b/dac2959-math-0006.png",
            "https://onlinelibrary.wiley.com/cms/asset/d45517ae-0a59-4dec-9eef-81c2daaf3fdc/dac2959-math-0007.png",
            "https://onlinelibrary.wiley.com/cms/asset/faef830a-9128-40ad-bda7-00af722c7658/dac2959-math-0008.png",
            "https://onlinelibrary.wiley.com/cms/asset/a8bb573b-5137-4c9f-905f-266494b69383/dac2959-math-0009.png",
            "https://onlinelibrary.wiley.com/cms/asset/98586271-4023-4dd2-addd-0d99e98d2131/dac2959-fig-0001-m.png",
            "https://onlinelibrary.wiley.com/cms/asset/84e68837-f721-43b1-8557-ea4c41d3684f/dac2959-fig-0002-m.png",
            "https://onlinelibrary.wiley.com/cms/asset/4588caa2-02b5-4a3e-a14a-f5677314c5d6/dac2959-fig-0003-m.png",
            "https://onlinelibrary.wiley.com/cms/asset/1739fc42-e6af-45b2-8b7f-5bcc85b6d3ba/dac2959-fig-0004-m.png",
            "https://onlinelibrary.wiley.com/cms/asset/2c8a4f72-51e7-4da2-8740-d6e32b0a8e4d/dac2959-gra-0001.png",
            "https://onlinelibrary.wiley.com/cms/asset/2c67808e-1101-44b7-ac21-c757044fbfa3/dac2959-math-0010.png",
            "https://onlinelibrary.wiley.com/cms/asset/97965478-28c2-4674-91e7-13bab4d069a2/dac2959-math-0011.png",
            "https://onlinelibrary.wiley.com/cms/asset/a75a55b0-ee5b-4318-93e1-da0364ed1218/dac2959-math-0012.png",
            "https://onlinelibrary.wiley.com/cms/asset/aefcfafe-d248-414d-be5b-83e23ac576d9/dac2959-math-0013.png",
            "https://onlinelibrary.wiley.com/cms/asset/8384ab1d-5586-4879-8f46-31107624d636/dac2959-fig-0005-m.png",
            "https://onlinelibrary.wiley.com/cms/asset/5eb96303-1354-44a3-8078-1a0f222ff9c8/dac2959-math-0014.png",
            "https://onlinelibrary.wiley.com/cms/asset/eedd3183-e006-4673-ab3e-6498c8e1bac5/dac2959-fig-0006-m.png",
            "https://onlinelibrary.wiley.com/cms/asset/37e2a551-c6d9-452d-a954-8870d5098919/dac.v29.16.cover.jpg",
            "https://pubads.g.doubleclick.net/gampad/ad?iu=$googlePublisherCategory&sz=160x600|160x320|160x160|120x600&tile=2&c=$random",
            "https://onlinelibrary.wiley.com/specs/products/acropolis/pericles/releasedAssets/images/spinner-1ffd60b3aabe5b09bc98c48345208fd9.gif",
            "https://pubads.g.doubleclick.net/gampad/ad?iu=$googlePublisherCategory&sz=160x600|160x320|160x160|120x600&tile=2&c=$random",
            "https://onlinelibrary.wiley.com/pb-assets/tmp-images/footer-logo-wiley-1510029248417.png",
            "https://onlinelibrary.wiley.com/pb-assets/hub-assets/pericles/logo-header-1690978619437.png",
            "https://onlinelibrary.wiley.com/pb-assets/hub-assets/pericles/logo-header-1690978619437.png",
            "https://onlinelibrary.wiley.com/pb-assets/hub-assets/pericles/logo-header-1690978619437.png"
        ],
        "movies": [],
        "keywords": [],
        "meta_keywords": [
            ""
        ],
        "tags": null,
        "authors": [
            "Solomon S",
            "Thulasiram R",
            "Thulasiraman P",
            "Mehrdoust F",
            "Rubinstein R",
            "Kroese D",
            "Feynman R",
            "Kac M",
            "Qiu M",
            "Chen Z"
        ],
        "publish_date": null,
        "summary": "",
        "meta_description": "The International Journal of Communication Systems is a communications journal publishing research papers on public and private communication technology systems.",
        "meta_lang": "en",
        "meta_favicon": "/favicon.ico",
        "meta_site_name": "Wiley Online Library",
        "canonical_link": "https://onlinelibrary.wiley.com/doi/10.1002/dac.2959",
        "text": "1 Introduction\n\nAs a future wireless networking standard, 5G will enable a large number of novel techniques using high performances bandwidth and wider wireless interconnections between mobile devices 1. Mobile apps will be designed and developed to utilize the full benefit of the enhancement of wireless networking capabilities 2. Moreover, the rapid hardware development will also provide future mobile computing with a bright picture that may allow mobile devices to have more capable CPUs and graphics processing units (GPUs). The growth of mobile apps will expand the scope of apps implementation in multiple fields, including the financial industry.\n\nComparing with the current mobile apps, mobile apps running within the 5G context will generate a greater number of random data. Data processing on mobile devices will become a challenge because of the over-heavy workloads and energy restrictions. The giant number of the random data will overload the hardware and communications when the size of communication data is large and continuously changing. For instance, stock analysis and ratings will turn into a difficult task when the massive information source is complicated and dynamic. Financial institutions will have a remarkable demand of using mobile apps for analyzing dynamic state data. The challenging issue is that applying the traditional approach may not be able to meet the requirements of mobile data processing in the upcoming 5G network 3. This paper proposes a novel paradigm that aims to solve this predicable challenge by minimizing data transferring via using GPUs.\n\nMoreover, addressing the random data issues, stochastic modeling approaches have been broadly implemented in multiple research fields when dealing with random variables. However, the feasibility of the evaluation is low because of the nature of distribution probability. This feature brings challenge to examining the deterministic formulas and the corresponding algorithms. One schema for increasing the reliability of the results is evaluating the results or performances by a huge number of repeated executions of the algorithm with randomly generated inputs.\n\nThis mechanism is termed as Monte Carlo simulation. For example, an estimation error is a random variable when evaluating the maximum likelihood estimator. It makes no sense to judge the performance of the maximum likelihood estimator based on one or two estimates that it outputs. Its performance can only be evaluated by repeating the estimation algorithm with a large number of random samples. Because of the huge number of repetitions, Monte Carlo simulations are usually time consuming and rely heavily on supercomputers 4. Large energy costs also restrict Monte Carlo simulations even though the increasing density of microprocessors keeps increasing the capacity of the supercomputer. The capacity of supercomputers usually has a positive relationship with its energy consumption so that the approach of applying supercomputers may only solve the proposed problems in a very limited scope. Furthermore, it is also a challenge for mobile devices to acquire the same computing capability as the current supercomputers when 5G is eventually deployed. The main thesis of this paper is to leverage GPUs to tackle the proposed problems by exploiting many simpler cores. The proposed method considers both high computation capacity and power consumptions 5, 6.\n\nGraphics processing unit is a multiple-core processor that provides single instruction multiple data (SIMD) parallel processing. It was originally devised to accelerate the graphics processing because of its parallel processing capabilities and the parallelism schemas in the graphics applications 7. Applications of GPUs have been embarking in multiple domains and represented higher-level performances, such as stock predictions and pricing equity products 8, 9. A 50× speedup by implementing GPUs has been achieved in some prior research, compared with the CPU counterparts 8, 9. Some implementations even achieved better performances, which reached a 100× speedup over the sequential CPU implementations when solving the pricing problems 10.\n\nMoreover, solving a set of stochastic differential equations is a mechanism for gaining financial derivative prices in the financial industry 11, 12. The main challenge of this approach is to solve the numeric stochastic differential equations. Fortunately, the Feynman–Kac theorem 13, 14 establishes the validity of pricing the financial derivatives through Monte Carlo simulation. A financial derivative is a contract between two parties, which specify the conditions of payment or payoff. The conditions usually include a few parameters, such as exchange rates, stock prices and interest rates, and maturity terms. Some conditions are random variables having certain probabilities of distribution, which results in the random payment or payoff. Therefore, financial derivative price is a stochastic method for determining the price of a derivative. To achieve better accuracy of prediction, a huge number of repeated simulation with random parameters characterizing the randomness of the market are performed. Such a process is usually time consuming on CPU-based machines because of the sequential execution of instructions.\n\nIn this paper, we proposed an innovative approach that implements Monte Carlo simulation for pricing total return swap (TRS) on the NVIDIA GPU GT520 (Nvidia Corporation, Santa Clara, CA, USA) and compare its execution time with that of the CPU-based process. We utilized the random number generator curand_norm in the CURAND Library (Nvidia Corporation) to generate random numbers. With the generated random numbers, we evaluated the payment of a TRS under different combinations of the market coefficients. Our experimental results showed up to 25.9× speedup in favor of GPU-based implementation.\n\nOur contribution in this paper can be summarized as follows:\n\nWe proposed a novel approach for processing the large amount of data by using GPUs 15.\n\nWe explored the effects of the various block sizes and different sizes of samples on the GPU speedup and concluded that a larger block size and sample size give out better speedup.\n\nWe developed the program in the way that host-device data transfer and global memory access are minimized, and we achieved a speedup of 25.9×.\n\nThe rest of this paper is organized as follows. Section 2 introduces the models and concepts related to our research. Section 3 presents our algorithm applied to the proposed model. In Section 4, we show our experimental results and explore the effect of different thread block configurations on the speedup of GPU implementations. A solid conclusion is given in Section 5.\n\n2 Models and Concepts\n\n2.1 Monte Carlo method\n\nMonte Carlo methods are a family of numerical computational algorithms calculating their results based on repeated random inputs. These methods are most often applied in various computing simulations, such as physics, chemistry, and mathematics. There are mainly two purposes of leveraging the Monte Carlo method.\n\nFirst, Monte Carlo methods are used for systems evolving stochastic features when the system metrics are random. To evaluate these metrics, Monte Carlo method should be used to remove the randomness. An example for this application is to evaluate the performance of a stock price predictor. The stock price can be considered as a random process. It is impossible to predict the exact price for each prediction because of random errors in prediction. The performance of the predictor can only be measured in terms of the average accuracy. To obtain this average accuracy, a large number of predictions are required with randomly picked historic records of the stocks. This process is defined as a Monte Carlo simulation.\n\nSecond, Monte Carlo method can be also used for mathematical calculations. Along this direction, Monte Carlo method is a complement of the theoretic derivations. A common example is to evaluate definite integrals with complicated boundary conditions. Considering that integral , computers can only approximate F by as discrete summation where ri are the randomly picked numbers within interval [a,b]. When n is sufficiently large, the approximation error is bounded by . Monte Carlo methods are especially efficient for multidimensional definite integrations with complicated boundary conditions.\n\n2.2 Total return swap\n\nTotal return swap is a financial derivative that transfers both the credit risk and market risk of an underlying asset 16. Assume that two parties enter a TRS with 1-year maturity. Party A borrows M dollars from party B at the beginning. At the maturity, party A will remit a return (pay) to party B.\n\n(1)\n\nwhere YT and BT denote the exchange rates at the maturity, for example, Japan Yen and Thai Baht to Dollar, respectively, and Y and B are the exchange rates at the transaction time. Because YT and BT are unknown at the transaction time, it is assumed that the exchange rates follow geometric Brownian motions through the time. To valuate the price of the TRS at transaction, we adopt a risk neutral valuation that assumes the following conditions 17, 18:\n\nThe Dollar to Yen exchange rate is a lognormal diffusion process with volatility σY.\n\nThe Dollar to Baht exchange rate is a lognormal diffusion process with volatility σB.\n\nσY and σB have correlation ρ.\n\nWith the aforementioned assumptions, the exchange rates YT and BT can be characterized by the following equations:\n\n(2)\n\n(3)\n\nwhere ϵY and ϵB are zero-mean Gaussian variables with unit variances and rD, rY, and rB are the risk-free interest rates of US Dollar, Japan Yen, and Thai Baht for 1 year, respectively.\n\nThe payment P is also a random number because of the randomness of the pair ϵY and ϵB. Therefore, it is impossible to determine the value of P merely by calculating it with one pair of (ϵY,ϵB). The Monte Carlo simulation can remove the effect of randomness in the payment P by repeatedly calculating the payments based on a large number of pairs of (ϵY,ϵB). Those volatility pairs are randomly generated according to zero-mean normal distribution with unit variance. Finally, the payment is evaluated by averaging all the possible payments.\n\n2.3 Graphics processing unit architecture and programming model\n\nGraphics processing unit was originally designed for accelerating applications in games and computer visualization. The previous GPUs have the architecture shown in Figure 1. It incorporates 128 cores in a single GPU chip and organizes them in 16 streaming multiprocessors (SMs) with each containing eight cores, termed as stream processors (SPs). The SPs in the same SM share the same data caches, control logic and shared memory, and the same set of special function units (SFUs). They work in SIMD mode. Such hardware architecture achieves great success in speeding up the fixed-function processing in graphic applications, because the graphic applications usually have a set of fixed stages. The multiple-core structure enables stage-wise pipelines in space, which means that the output of a stage processed by one processor can be fed into another processor for next stage processing. In this way, the parallelism among the stages can be exploited.\n\nRecently, with the emergence of energy walls in the supercomputing, the researchers are seeking to map the general purpose computations containing graphics processing-like parallelism to GPU platforms. To fully exploit the hardware architecture of the GPU for this demand, NVIDIA released compute unified device architecture (CUDA), a parallel computing architecture supporting general purpose computing on GPUs, in 2006. In CUDA, the GPU device is considered a compute device that is a coprocessor to the CPU host, in which running many threads concurrently launched from the CPU host. The GPU device is usually equipped with its on-broad dynamic random access memory (DRAM), which is a global memory of GPU.\n\nThe programming language CUDA-C is an extension of standard language C. CUDA source code consists of GPU code and the host code. The integrated source is converted into two parts, including GPU executable files and host executables as shown in Figure 2. The kernel in a CUDA program describes the operations performed on GPU. Because the GPU is a SIMD device, the kernel is invoked from the host as threads on different SPs, and each of which runs the same kernel code with different data. The data can be either generated by the threads or copied from the host memory. When the executable is initiated, the program proceeds in the way described in Figure 3.\n\nThe executable of CUDA program consists of host instructions and device instructions. The device part is also known as a kernel. Data can be copied to or from the device if there are data dependencies between the host and the kernel. When executing the threads, the CUDA architecture provides a hierarchy of memories, including registers, shared memories/L1 cache, L2 cache, local memories, and global memories. These hierarchical memories are shared by the threads running on the same SM. The local and global memories are actually the off-chip DRAM on the device board. They are accessible from all the threads in the series. Different types of memories have dramatically distinct latencies. The global memory is an off-chip double data rate (DDR) memory with the largest access latency. The registers and shared memories/L1 cache are located in each SM and shared by the threads residing in the SM. They have the smallest access latency. Memory access management is critical for the speedup of GPU implementations. We list the delays of different types of memories in Table 1.\n\nThe state-of-the-art NVIDIA GPU is the CUDA architecture named as Fermi. The current Fermi-based GPUs have up to 512 CUDA cores that are organized in 16 SMs of 32 CUDA cores each. The architecture of one Fermi SM is shown in Figure 4. Each SP has a fully pipelined arithmetic logic unit and floating point unit. These function units are optimized to provide up to eight times faster double precision operation. Compared with previous generations of GPUs, Fermi architecture provides parallel access to the shared memories within each SM and more bandwidth for L1 cache access. The parallelism among the threads is further increased. Furthermore, in Fermi architecture, the shared memories/L1 cache can be divided into a part of shared memory and a part of L1 cache, which makes the GPU more resilient to various applications.\n\nTo leverage the parallel architecture of the Fermi, we should exploit the parallelism in a given application maximally. With CUDA programming model, the application is first decomposed into multiple threads that can be executed independently. The threads are then organized into thread blocks. Finally, these thread blocks are scheduled on the available processor cores. Because of the independence of the threads, the thread blocks can be scheduled concurrently or sequentially.\n\nIn our implementation, the NVIDIA GT520 has one SM containing 48 SPs. There are 8 SFUs and 16 load/store units in the SM. The SFUs are designed to support transcendental instructions, like sine, reciprocal, and square root. The 16 load/store can perform memory access for 16 threads per clock.\n\n3 Algorithm\n\nIn this section, we present our algorithm of the Monte Carlo simulation for TRS pricing based on NVIDIA GPU GT520. The purpose of the Monte Carlo simulation for TRS pricing is to predict the average payments as a function of different combinations of exchange rate volatility σY and σB. Specifically, there is a value of P for each pair of σT and σB. Because the payments are random, the values of payments under each combination of (σY,σB) need to be repeatedly calculated with Gaussian distributed ϵY and ϵB and then averaged. The accuracy of the prediction increases as the number of the repeated calculation increases.\n\nFor TRS, the payments under different pairs of (σY,σB) are independent from each other. It is natural to put the calculation for one pair of (σY,σB) into one thread so that the parallelism among the threads can be maximized. In our implementation, we computed the payments with combinations of 16 σT and σB, respectively. Thus, there are, in total, 256 averaged payments P, and each of which is computed by one thread.\n\nNext, data exchanges among the kernels, global memory access, and host-device data transfers should be minimized in order to maximally exploit the computational power of the GPUs. There are three reasons for this operation. First, data exchanges among the kernels limit the parallelism of the kernels, thus lower the speedup of the GPU implementation. Second, global memory of GPU is the off-chip DRAM on the device board, and it has a large access latency compared with that of on-chip memories. Finally, host-device data transfer is time consuming because of the bandwidth of the Peripheral Component Interconnect (PCI) bus.\n\nIn addition, for the purpose of reducing the data dependence among the kernels, we assign each kernel to calculate one possible payment value with one pair of volatilities. Moreover, the data are copied to the registers of the cores executing the kernels to minimize global memory access. The only global memory access for each kernel is to store the final computation result, that is, payment of double precision due to a particular pair of volatilities, into the global memory.\n\nFurthermore, to minimize the host-device data transfer, we designed two kernels, which include the one generating the parameters for the calculation and the other one for calculating the final result. The first kernel generates the one pair of volatilities and one pair of random seeds. It stores them in the global memory, and the second kernel copies these parameters from the global memory to the registers instead of acquiring them from the host.\n\nThe function of the second kernel is to calculate the averaged payment with a particular pair of volatilities, which is implemented in a loop. In each loop, the only loop-dependent variables are random numbers ϵY and ϵB based on which a possible payment is calculated by formula 1-3. To increase the speedup, we split the formula 2, 3 into steps that consist of a loop-invariant part 4, 5 and a loop-dependent part 6, 7. In the loop-invariant part, the value of temp1 and temp2 is determined by constant input variables and calculated before the loop. Therefore, in each loop, only a new payment based on new YT and BT is calculated. To reduce the memory usage, we set only one variable to store the sum of the payments through the loops. We present the details of the program in Table 1.\n\n(4)\n\n(5)\n\n(6)\n\n(7)\n\nWe organize the kernels into one thread block with dimension of 32 × 32. Therefore, each thread calculates one payment of a pair of volatilities.\n\n4 Experiment\n\nIn this section, we present our experiment setup and experimental results with different configurations of the thread block size and sample sizes. We implemented the same function on CPU using language C to compare the performance of the GPU implementation and its counterpart on CPU. We compared the total time needed for obtaining the calculation results.\n\nThe total time of GPU implementation includes two aspects, including the time consumption during the data transfer and kernel executions. We perform the experiment on a desktop platform equipped with NVidia GeForce GT520 (Nvidia Corporation) and Intel Core 2 Duo processor E8400 (Intel Corporation, Santa Clara, CA, USA) running at 3.0 GHz. The SP clock of GT520 is 1620 MHz. The device has a global memory of 1-GB DDR3, and the host has 2-GB DDR2 memories. The experimental platform is shown in Table 2.\n\nIn our experiment, we evaluate the effect of thread organization and sample size on the GPU speedup. In total, 1024 threads are examined for the purpose of evaluating 32 × 32 combinations of volatility pairs. The threads are organized into thread blocks, and the number of threads in one block is limited by the GPU architecture. For GT520, the maximum allowed number of threads on one SM is 512. Based on this configuration, we divide the 1024 threads into two blocks with each containing 512 threads. We explored sample sizes from 10 to 100,000.\n\nThe results are exhibited in Figure 5a and 5b. Figure 5a displays the execution times of the implementations based on GPU and CPU, which shows that the GPU has a noticeable speedup. This result implies that the speedup increases as the number of the sample size increases. For a sample size of 10, the final payment for each pair of volatilities is the average of 10 possible payments, which contains 1024 × 10 repetitions. The speedup in this case is 17.5×. For a large sample size of 100,000, the total repetition is 1024 × 100,000. The speedup grows to be 25.9×. This is desirable for the Monte Carlo simulations that require larger-sized samples to obtain high-level accurate results.\n\nNext, we investigated the effect of block size on the speedup. We experimented with different block sizes ranging from four threads per block to 512 threads per block with a sample size of 100,000. We show the results in Figure 5c and 5d. It can be seen that the speedup increases as the block size grows. This is because CUDA parallels the threads within blocks on one stream multiprocessor. In GT520, there is only one stream multiprocessor. The CUDA scheduler can only schedule one block to execute in parallel. Therefore, the parallelism increases as the block size increases. As we can see, the speedup approximates a limit in Figure 5d, that is, 26. This is because the GT520 has 48 SPs that run at fGPU=1.62 GHz. The CPU clock is fCPU=3.0 GHz. Hence, the maximum speedup is .\n\nFinally, we show the payments calculated by our Monte Carlo simulation in Figure 6. In the Monte Carlo simulation, we set M = 100 and vary the volatilities σY and σB from 0.01 to 0.76 with step size of 0.025. The final payments under different pairs of volatilities form a surface. The payment is the highest under the condition {σY=0.01,σB=0.76} and the lowest under {σY=0.76,σB=0.01}. This plot provides a reference for financial operations.\n\nA real-time response is critical in the over-the-counter financial derivative trading. Many financial derivatives do not have analytic closed-form solutions. Monte Carlo simulation is an effective approach to determine the prices of these derivatives. However, traditional implementations of the simulations are based on CPUs, which takes a long operating time. GPU-based financial derivative pricing largely shortens the simulation time; thus, it allows financial institutions to evaluate various financial models and choose the one that lowers their risk the most.\n\nSpecial function units located in the SMs are dedicated for evaluating functions like sine, cosine, and reciprocal, and square root. The applications that contain more of those functions can have more speedups on GPUs. The approach of predicting the performance of an application based on its data movement, computational complexity, and GPU resource allocation is still an open problem. We leave this part as our future work.\n\n5 Conclusions\n\nIn this paper, we proposed an innovative schema for future mobile data processing in the 5G context. We developed the CUDA program in which global memory access, host-device data transfer, and data exchanges among threads are minimized by implementing a Monte Carlo simulation. We explored the effect of the number of threads per block on the speedup by experimenting with various block sizes. The experimental results showed that our approach can reach 25.9× speedup under a configuration of 512 threads per block and a sample size of 100,000.\n\nAcknowledgements\n\nThis work was supported in part by the National Science Foundation under Grants CNS-1457506, CNS-1359557, and CNS-1249223 (Prof. M. Qiu)."
    }
}
{
    "id": "dbpedia_120_2",
    "rank": 69,
    "data": {
        "url": "https://cloud.google.com/dataproc/docs/tutorials/monte-carlo-methods-with-hadoop-spark",
        "read_more_link": "",
        "language": "en",
        "title": "Monte Carlo methods using Dataproc and Apache Spark",
        "top_image": "https://cloud.google.com/_static/cloud/images/social-icon-google-cloud-1200-630.png",
        "meta_img": "https://cloud.google.com/_static/cloud/images/social-icon-google-cloud-1200-630.png",
        "images": [
            "https://www.gstatic.com/devrel-devsite/prod/vd185cef2092d5507cf5d5de6d49d6afd8eb38fe69b728d88979eb4a70550ff03/cloud/images/cloud-logo.svg",
            "https://www.gstatic.com/devrel-devsite/prod/vd185cef2092d5507cf5d5de6d49d6afd8eb38fe69b728d88979eb4a70550ff03/cloud/images/cloud-logo.svg",
            "https://cloud.google.com/static/docs/images/establish-ssh-connection-1.png"
        ],
        "movies": [],
        "keywords": [],
        "meta_keywords": [
            ""
        ],
        "tags": null,
        "authors": [],
        "publish_date": null,
        "summary": "",
        "meta_description": "",
        "meta_lang": "en",
        "meta_favicon": "https://www.gstatic.com/devrel-devsite/prod/vd185cef2092d5507cf5d5de6d49d6afd8eb38fe69b728d88979eb4a70550ff03/cloud/images/favicons/onecloud/favicon.ico",
        "meta_site_name": "Google Cloud",
        "canonical_link": "https://cloud.google.com/dataproc/docs/tutorials/monte-carlo-methods-with-hadoop-spark",
        "text": "Stay organized with collections Save and categorize content based on your preferences.\n\nDataproc and Apache Spark provide infrastructure and capacity that you can use to run Monte Carlo simulations written in Java, Python, or Scala.\n\nMonte Carlo methods can help answer a wide range of questions in business, engineering, science, mathematics, and other fields. By using repeated random sampling to create a probability distribution for a variable, a Monte Carlo simulation can provide answers to questions that might otherwise be impossible to answer. In finance, for example, pricing an equity option requires analyzing the thousands of ways the price of the stock could change over time. Monte Carlo methods provide a way to simulate those stock price changes over a wide range of possible outcomes, while maintaining control over the domain of possible inputs to the problem.\n\nIn the past, running thousands of simulations could take a very long time and accrue high costs. Dataproc enables you to provision capacity on demand and pay for it by the minute. Apache Spark lets you use clusters of tens, hundreds, or thousands of servers to run simulations in a way that is intuitive and scales to meet your needs. This means that you can run more simulations more quickly, which can help your business innovate faster and manage risk better.\n\nSecurity is always important when working with financial data. Dataproc runs on Google Cloud, which helps to keep your data safe, secure, and private in several ways. For example, all data is encrypted during transmission and when at rest, and Google Cloud is ISO 27001, SOC3, and PCI compliant.\n\nObjectives\n\nCreate a managed Dataproc cluster with Apache Spark pre-installed.\n\nRun a Monte Carlo simulation using Python that estimates the growth of a stock portfolio over time.\n\nRun a Monte Carlo simulation using Scala that simulates how a casino makes money.\n\nCosts\n\nIn this document, you use the following billable components of Google Cloud:\n\nCompute Engine\n\nDataproc\n\nTo generate a cost estimate based on your projected usage, use the pricing calculator. New Google Cloud users might be eligible for a free trial.\n\nWhen you finish the tasks that are described in this document, you can avoid continued billing by deleting the resources that you created. For more information, see Clean up.\n\nBefore you begin\n\nSet up a Google Cloud project\n\nSign in to your Google Cloud account. If you're new to Google Cloud, create an account to evaluate how our products perform in real-world scenarios. New customers also get $300 in free credits to run, test, and deploy workloads.\n\nIn the Google Cloud console, on the project selector page, select or create a Google Cloud project.\n\nGo to project selector\n\nMake sure that billing is enabled for your Google Cloud project.\n\nEnable the Dataproc and Compute Engine APIs.\n\nEnable the APIs\n\nInstall the Google Cloud CLI.\n\nTo initialize the gcloud CLI, run the following command:\n\ngcloud init\n\nIn the Google Cloud console, on the project selector page, select or create a Google Cloud project.\n\nGo to project selector\n\nMake sure that billing is enabled for your Google Cloud project.\n\nEnable the Dataproc and Compute Engine APIs.\n\nEnable the APIs\n\nInstall the Google Cloud CLI.\n\nTo initialize the gcloud CLI, run the following command:\n\ngcloud init\n\nCreating a Dataproc cluster\n\nFollow the steps to create a Dataproc cluster from the Google Cloud console. The default cluster settings, which includes two-worker nodes, is sufficient for this tutorial.\n\nDisabling logging for warnings\n\nBy default, Apache Spark prints verbose logging in the console window. For the purpose of this tutorial, change the logging level to log only errors. Follow these steps:\n\nUse ssh to connect to the Dataproc cluster's primary node\n\nThe primary node of the Dataproc cluster has the -m suffix on its VM name.\n\nIn the Google Cloud console, go to the VM instances page.\n\nGo to VM instances\n\nIn the list of virtual machine instances, click SSH in the row of the instance that you want to connect to.\n\nAn SSH window opens connected to the primary node.\n\nConnected, host fingerprint: ssh-rsa 2048 ... ... user@clusterName-m:~$\n\nChange the logging setting\n\nFrom the primary node's home directory, edit /etc/spark/conf/log4j.properties.\n\nsudo nano /etc/spark/conf/log4j.properties\n\nSet log4j.rootCategory equal to ERROR.\n\n# Set only errors to be logged to the console log4j.rootCategory=ERROR, console log4j.appender.console=org.apache.log4j.ConsoleAppender log4j.appender.console.target=System.err log4j.appender.console.layout=org.apache.log4j.PatternLayout log4j.appender.console.layout.ConversionPattern=%d{yy/MM/dd HH:mm:ss} %p %c{1}: %m%n\n\nSave the changes and exit the editor. If you want to enable verbose logging again, reverse the change by restoring the value for .rootCategory to its original (INFO) value.\n\nSpark programming languages\n\nSpark supports Python, Scala, and Java as programming languages for standalone applications, and provides interactive interpreters for Python and Scala. The language you choose is a matter of personal preference. This tutorial uses the interactive interpreters because you can experiment by changing the code, trying different input values, and then viewing the results.\n\nEstimating portfolio growth\n\nIn finance, Monte Carlo methods are sometimes used to run simulations that try to predict how an investment might perform. By producing random samples of outcomes over a range of probable market conditions, a Monte Carlo simulation can answer questions about how a portfolio might perform on average or in worst-case scenarios.\n\nFollow these steps to create a simulation that uses Monte Carlo methods to try to estimate the growth of a financial investment based on a few common market factors.\n\nStart the Python interpreter from the Dataproc primary node.\n\npyspark\n\nWait for the Spark prompt >>>.\n\nEnter the following code. Make sure you maintain the indentation in the function definition.\n\nimport random import time from operator import add def grow(seed): random.seed(seed) portfolio_value = INVESTMENT_INIT for i in range(TERM): growth = random.normalvariate(MKT_AVG_RETURN, MKT_STD_DEV) portfolio_value += portfolio_value * growth + INVESTMENT_ANN return portfolio_value\n\nPress return until you see the Spark prompt again.\n\nThe preceding code defines a function that models what might happen when an investor has an existing retirement account that is invested in the stock market, to which they add additional money each year. The function generates a random return on the investment, as a percentage, every year for the duration of a specified term. The function takes a seed value as a parameter. This value is used to reseed the random number generator, which ensures that the function doesn't get the same list of random numbers each time it runs. The random.normalvariate function ensures that random values occur across a normal distribution for the specified mean and standard deviation. The function increases the value of the portfolio by the growth amount, which could be positive or negative, and adds a yearly sum that represents further investment.\n\nYou define the required constants in an upcoming step.\n\nCreate many seeds to feed to the function. At the Spark prompt, enter the following code, which generates 10,000 seeds:\n\nseeds = sc.parallelize([time.time() + i for i in range(10000)])\n\nThe result of the parallelize operation is a resilient distributed dataset (RDD), which is a collection of elements that are optimized for parallel processing. In this case, the RDD contains seeds that are based on the current system time.\n\nWhen creating the RDD, Spark slices the data based on the number of workers and cores available. In this case, Spark chooses to use eight slices, one slice for each core. That's fine for this simulation, which has 10,000 items of data. For larger simulations, each slice might be larger than the default limit. In that case, specifying a second parameter to parallelize can increase the number slices, which can help to keep the size of each slice manageable, while Spark still takes advantage of all eight cores.\n\nFeed the RDD that contains the seeds to the growth function.\n\nresults = seeds.map(grow)\n\nThe map method passes each seed in the RDD to the grow function and appends each result to a new RDD, which is stored in results. Note that this operation, which performs a transformation, doesn't produce its results right away. Spark won't do this work until the results are needed. This lazy evaluation is why you can enter code without the constants being defined.\n\nSpecify some values for the function.\n\nINVESTMENT_INIT = 100000 # starting amount INVESTMENT_ANN = 10000 # yearly new investment TERM = 30 # number of years MKT_AVG_RETURN = 0.11 # percentage MKT_STD_DEV = 0.18 # standard deviation\n\nCall reduce to aggregate the values in the RDD. Enter the following code to sum the results in the RDD:\n\nsum = results.reduce(add)\n\nEstimate and display the average return:\n\nprint (sum / 10000.)\n\nBe sure to include the dot (.) character at the end. It signifies floating-point arithmetic.\n\nNow change an assumption and see how the results change. For example, you can enter a new value for the market's average return:\n\nMKT_AVG_RETURN = 0.07\n\nRun the simulation again.\n\nprint (sc.parallelize([time.time() + i for i in range(10000)]) \\ .map(grow).reduce(add)/10000.)\n\nWhen you're done experimenting, press CTRL+D to exit the Python interpreter.\n\nProgramming a Monte Carlo simulation in Scala\n\nMonte Carlo, of course, is famous as a gambling destination. In this section, you use Scala to create a simulation that models the mathematical advantage that a casino enjoys in a game of chance. The \"house edge\" at a real casino varies widely from game to game; it can be over 20% in keno, for example. This tutorial creates a simple game where the house has only a one-percent advantage. Here's how the game works:\n\nThe player places a bet, consisting of a number of chips from a bankroll fund.\n\nThe player rolls a 100-sided die (how cool would that be?).\n\nIf the result of the roll is a number from 1 to 49, the player wins.\n\nFor results 50 to 100, the player loses the bet.\n\nYou can see that this game creates a one-percent disadvantage for the player: in 51 of the 100 possible outcomes for each roll, the player loses.\n\nFollow these steps to create and run the game:\n\nStart the Scala interpreter from the Dataproc primary node.\n\nspark-shell\n\nCopy and paste the following code to create the game. Scala doesn't have the same requirements as Python when it comes to indentation, so you can simply copy and paste this code at the scala> prompt.\n\nval STARTING_FUND = 10 val STAKE = 1 // the amount of the bet val NUMBER_OF_GAMES = 25 def rollDie: Int = { val r = scala.util.Random r.nextInt(99) + 1 } def playGame(stake: Int): (Int) = { val faceValue = rollDie if (faceValue < 50) (2*stake) else (0) } // Function to play the game multiple times // Returns the final fund amount def playSession( startingFund: Int = STARTING_FUND, stake: Int = STAKE, numberOfGames: Int = NUMBER_OF_GAMES): (Int) = { // Initialize values var (currentFund, currentStake, currentGame) = (startingFund, 0, 1) // Keep playing until number of games is reached or funds run out while (currentGame <= numberOfGames && currentFund > 0) { // Set the current bet and deduct it from the fund currentStake = math.min(stake, currentFund) currentFund -= currentStake // Play the game val (winnings) = playGame(currentStake) // Add any winnings currentFund += winnings // Increment the loop counter currentGame += 1 } (currentFund) }\n\nPress return until you see the scala> prompt.\n\nEnter the following code to play the game 25 times, which is the default value for NUMBER_OF_GAMES.\n\nplaySession()\n\nYour bankroll started with a value of 10 units. Is it higher or lower, now?\n\nNow simulate 10,000 players betting 100 chips per game. Play 10,000 games in a session. This Monte Carlo simulation calculates the probability of losing all your money before the end of the session. Enter the follow code:\n\n(sc.parallelize(1 to 10000, 500) .map(i => playSession(100000, 100, 250000)) .map(i => if (i == 0) 1 else 0) .reduce(_+_)/10000.0)\n\nNote that the syntax .reduce(_+_) is shorthand in Scala for aggregating by using a summing function. It is functionally equivalent to the .reduce(add) syntax that you saw in the Python example.\n\nThe preceding code performs the following steps:\n\nCreates an RDD with the results of playing the session.\n\nReplaces bankrupt players' results with the number 1 and nonzero results with the number 0.\n\nSums the count of bankrupt players.\n\nDivides the count by the number of players.\n\nA typical result might be:\n\n0.998\n\nWhich represents a near guarantee of losing all your money, even though the casino had only a one-percent advantage.\n\nClean up\n\nDelete the project\n\nIn the Google Cloud console, go to the Manage resources page.\n\nGo to Manage resources\n\nIn the project list, select the project that you want to delete, and then click Delete.\n\nIn the dialog, type the project ID, and then click Shut down to delete the project.\n\nWhat's next\n\nFor more on submitting Spark jobs to Dataproc without having to use ssh to connect to the cluster, read Dataprocâ€”Submit a job"
    }
}
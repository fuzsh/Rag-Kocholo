{
    "id": "dbpedia_120_3",
    "rank": 75,
    "data": {
        "url": "https://bstaton1.github.io/au-r-workshop/ch4.html",
        "read_more_link": "",
        "language": "en",
        "title": "Introduction to R for Natural Resource Scientists",
        "top_image": "",
        "meta_img": "",
        "images": [
            "https://bstaton1.github.io/au-r-workshop/au-r-workshop_files/figure-html/unnamed-chunk-163-1.png",
            "https://bstaton1.github.io/au-r-workshop/au-r-workshop_files/figure-html/unnamed-chunk-165-1.png",
            "https://bstaton1.github.io/au-r-workshop/au-r-workshop_files/figure-html/unnamed-chunk-181-1.png",
            "https://bstaton1.github.io/au-r-workshop/au-r-workshop_files/figure-html/unnamed-chunk-183-1.png",
            "https://bstaton1.github.io/au-r-workshop/au-r-workshop_files/figure-html/unnamed-chunk-188-1.png",
            "https://bstaton1.github.io/au-r-workshop/au-r-workshop_files/figure-html/unnamed-chunk-196-1.png",
            "https://bstaton1.github.io/au-r-workshop/au-r-workshop_files/figure-html/unnamed-chunk-198-1.png",
            "https://bstaton1.github.io/au-r-workshop/au-r-workshop_files/figure-html/unnamed-chunk-204-1.png",
            "https://bstaton1.github.io/au-r-workshop/au-r-workshop_files/figure-html/unnamed-chunk-205-1.png",
            "https://bstaton1.github.io/au-r-workshop/au-r-workshop_files/figure-html/unnamed-chunk-211-1.png",
            "https://bstaton1.github.io/au-r-workshop/au-r-workshop_files/figure-html/unnamed-chunk-212-1.png",
            "https://bstaton1.github.io/au-r-workshop/au-r-workshop_files/figure-html/unnamed-chunk-214-1.png",
            "https://bstaton1.github.io/au-r-workshop/au-r-workshop_files/figure-html/unnamed-chunk-216-1.png",
            "https://bstaton1.github.io/au-r-workshop/au-r-workshop_files/figure-html/unnamed-chunk-224-1.png",
            "https://bstaton1.github.io/au-r-workshop/au-r-workshop_files/figure-html/unnamed-chunk-226-1.png",
            "https://bstaton1.github.io/au-r-workshop/au-r-workshop_files/figure-html/unnamed-chunk-228-1.png",
            "https://bstaton1.github.io/au-r-workshop/au-r-workshop_files/figure-html/unnamed-chunk-234-1.png"
        ],
        "movies": [],
        "keywords": [],
        "meta_keywords": [
            ""
        ],
        "tags": null,
        "authors": [
            "Ben Staton"
        ],
        "publish_date": null,
        "summary": "",
        "meta_description": "This is a first course in R programming for natural resource scientists. It was developed and has been primarily instructed at Auburn University.",
        "meta_lang": "",
        "meta_favicon": "",
        "meta_site_name": "",
        "canonical_link": null,
        "text": "4.3.2 The for() loop\n\nIn programming, a loop is a command that does something over and over until it reaches some point that you specify. R has a few types of loops: repeat(), while(), and for(), to name a few. for() loops are among the most common in simulation modeling. A for() loop repeats some action for however many times you tell it for each value in some vector. The syntax is:\n\nfor (var in seq) { expression(var) }\n\nThe loop calculates the expression for values of var for each element in the vector seq. For example:\n\nfor (i in 1:5) { print(i^2) }\n\n## [1] 1 ## [1] 4 ## [1] 9 ## [1] 16 ## [1] 25\n\nThe print() command will be executed 5 times: once for each value of i. It is the same as:\n\ni =1; print(i^2); i =2; print(i^2); i =3; print(i^2); i =4; print(i^2); i =5; print(i^2)\n\nIf you remove the print() function, see what happens:\n\nfor (i in 1:5) { i^2 }\n\nNothing is printed to the console. R did the calculation, but did not show you or store the result. Often, you’ll need to store the results of the calculation in a container object:\n\nresults =numeric(5)\n\nThis makes an empty numeric vector of length 5 that are all 0’s. You can store the output of your loop calculations in results:\n\nfor (i in 1:5) { results[i] = i^2 } results\n\n## [1] 1 4 9 16 25\n\nWhen i^2 is calculated, it will be placed in the element results[i]. This was a trivial example, because you would never use a for() loop to do things as simple as vectorized calculation. The expression (1:5)^2 would give the same result with significantly less code (see Section 1.6).\n\nHowever, there are times where it is advantageous to use a loop. Particularly in cases where:\n\nthe calculations in one element are determined from the value in previous elements, such as in time series models\n\nthe calculations have multiple steps\n\nyou wish to store multiple results\n\nyou wish to track the progress of your calculations\n\nAs an illustration for item (1) above, build a (very) basic population model. At the start of the first year, the population abundance is 1000 individuals and grows by an average factor of 1.1 per year (reproduction and death processes result in a growth rate of 10%) before harvest. The growth rate varies randomly, however. Each year, the 1.1 growth factor has variability introduced by small changes in survival and reproductive process. Model these variations as lognormal random variables. After production, 8% of the population is harvested. Simulate the abundance at the end of the year for 100 years:\n\nnt =100 # number of years N =NULL # container for abundance N[1] =1000 # first end-of-year abundance for (t in 2:nt) { # N this year is N last year * growth * # randomness * fraction that survive harvest N[t] = (N[t-1] *1.1 *rlnorm(1, 0, 0.1)) * (1 -0.08) }\n\nPlot the abundance time series:\n\nplot(N, type = \"l\", pch = 15, xlab = \"Year\", ylab = \"Abundance\")\n\nExamples of the other three utilities of using for() loops over replicate are shown in the example cases and exercises.\n\n4.6.2 Stochastic Power Analysis\n\nA power analysis is one where the analyst wishes to determine how much power they will have to detect an effect. Power is inversely related to the probability of making a Type II Error: failing to reject a false null hypothesis . In other words, having high power means that you have a high chance of detecting an effect if an effect truly exists. Power is a function of the effect size, the sample size n, and the variability in the data. Strong effects are easier to detect than weak ones, more samples increase the test’s sensitivity (the ability to detect weak effects), and lower variability results in more power.\n\nYou can conduct a power analysis using stochastic simulation (i.e., a Monte Carlo analysis). Here, you will write a power analysis to determine how likely are you to be able to correctly identify what you deem to be a biologically-meaningful difference in survival between two tagging procedures.\n\nYou know one tagging procedure has approximately a 10% mortality rate (10% of tagged fish die within the first 12 hours as result of the tagging process). Another cheaper, and less labor-intensive method has been proposed, but before implementing it, your agency wishes to determine if it will have a meaningful impact on the reliability of the study or on the ability of the crew to tag enough individuals that will survive long enough to be useful. You and your colleagues determine that if the mortality rate of the new tagging method reaches 25%, then gains in time and cost-efficiency would be offset by needing to tag more fish (because more will die). You have decided to perform a small-scale study to determine if using the new method could result in 25% or more mortality. The study will tag n individuals using both methods (new and old) and track the fraction that survived after 12 hours. Before performing the study however, you deem it important to determine how large n needs to be to answer this question. You decide to use a stochastic power analysis to help your research group. The small-scale study can tag a total of at most 100 fish with the currently available resources. Could you tag fewer than 100 total individuals and still have a high probability of detecting a statistically significant difference in mortality?\n\nThe stochastic power analysis approach works like this (this is called psuedocode):\n\nSimulate data under the reality that the difference is real with n observations per treatment, where n < 100/2\n\nFit the model that will be used when the real data are collected to the simulated data\n\nDetermine if the difference was detected with a significant p-value\n\nReplicate steps 1 - 3 many times\n\nReplicate step 4 while varying n over the interval from 10 to 50\n\nDetermine what fraction of the p-values were deemed significant at each n\n\nStep 2 will require fitting a generalized linear model; for a review, revisit Section 3.2 (specifically Section 3.2.1 on logistic regression).\n\nFirst, create a function that will generate data, fit the model, and determine if the p-value is significant (steps 1-3 above):\n\nsim_fit =function(n, p_old = 0.10, p_new = 0.25) { ### step 1: create the data ### # generate random response data dead_old =rbinom(n, size = 1, prob = p_old) dead_new =rbinom(n, size = 1, prob = p_new) # create the predictor variable method =rep(c(\"old\", \"new\"), each = n) # create a data.frame to pass to glm df =data.frame(dead = c(dead_old, dead_new), method = method) # relevel so old is the reference df$method =relevel(df$method, ref = \"old\") ### step 2: fit the model ### fit =glm(dead ~ method, data = df, family = binomial) ### step 3: determine if a sig. p-value was found ### # extract the p-value pval =summary(fit)$coef[2,4] # determine if it was found to be significant pval <0.05 }\n\nNext, for steps 4 and 5, set up a nested for loop. This will have two loops: one that loops over sample sizes (step 5) and one that loops over replicates of each sample size (step 4). First, create the looping objects and containers:\n\nI =500 # the number of replicates at each sample size n_try =seq(10, 50, 10) # the test sample sizes N =length(n_try) # count them # container: out =matrix(NA, I, N) # matrix with I rows and N columns\n\nNow perform the nested loop. The inner-loop iterations will be completed for each element of n in the sequence 1:N. The output (which is one element: TRUE or FALSE based on the significance of the p-value) is stored in the corresponding row and column for that iteration of that sample size.\n\nfor (n in 1:N) { for (i in 1:I) { out[i,n] =sim_fit(n = n_try[n]) } }\n\nYou now have a matrix of TRUE and FALSE elements that indicates whether a significant difference was found at the \\(\\alpha = 0.05\\) level if the effect was truly as large as you care about. You can obtain the proportion of all the replicates at each sample size that resulted in a significant difference using the mean() function with apply():\n\nplot(apply(out, 2, mean) ~ n_try, type = \"l\", xlab = \"Tagged Fish per Treatment\", ylab = \"Probability of Finding Effect (Power)\")\n\nEven if you tagged 100 fish total, you would only have a 49% chance of saying the effect (which truly is there!) is present under the null hypothesis testing framework.\n\nSuppose you and your colleagues aren’t relying on p-values in this case, and are purely interested in how precisely the effect size would be estimated. Adapt your function to determine how frequently you would be able to estimate the true mortality of the new method within +/- 5% based on the point estimate only (the estimate for the tagging mortality of the new method must be between 0.2 and 0.3 for a successful study). Change your function to calculate this additional metric and re-run the analysis:\n\nsim_fit =function(n, p_old = 0.10, p_new = 0.25) { # create the data dead_old =rbinom(n, size = 1, prob = p_old) dead_new =rbinom(n, size = 1, prob = p_new) # create the predictor variable method =rep(c(\"old\", \"new\"), each = n) # create a data.frame to pass to glm df =data.frame(dead = c(dead_old, dead_new), method = method) # relevel so old is the reference df$method =relevel(df$method, ref = \"old\") # fit the model fit =glm(dead ~ method, data = df, family = binomial) # extract the p-value pval =summary(fit)$coef[2,4] # determine if it was found to be significant sig_pval = pval <0.05 # obtain the estimated mortality rate for the new method p_new_est =predict(fit, data.frame(method = c(\"new\")), type = \"response\") # determine if it is +/- 5% from the true value prc_est = p_new_est >= (p_new -0.05) & p_new_est <= (p_new +0.05) # return a vector with these two elements c(sig_pval = sig_pval, prc_est = unname(prc_est)) } # containers: out_sig =matrix(NA, I, N) # matrix with I rows and N columns out_prc =matrix(NA, I, N) # matrix with I rows and N columns for (n in 1:N) { for (i in 1:I) { tmp =sim_fit(n = n_try[n]) # run sim out_sig[i,n] = tmp[\"sig_pval\"] # extract and store significance metric out_prc[i,n] = tmp[\"prc_est\"] # extract and store precision metric } } par(mfrow = c(1,2), mar = c(4,4,1,0)) plot(apply(out_sig, 2, mean) ~ n_try, type = \"l\", xlab = \"Tagged Fish per Treatment\", ylab = \"Probability of Finding Effect (Power)\") plot(apply(out_prc, 2, mean) ~ n_try, type = \"l\", xlab = \"Tagged Fish per Treatment\", ylab = \"Probability of a Precise Estimate\")\n\nIt seems that even if you tagged 50 fish per treatment, you would have a 60% chance of estimating that the mortality rate is between 0.2 and 0.3 if it was truly 0.25.\n\nYou and your colleagues consider these results and determine that you will need to somehow acquire more funds to tag more fish in the small-scale study in order to have a high level of confidence in the results.\n\n4.6.3 Harvest Policy Analysis\n\nIn this example, you will simulate population dynamics under a more realistic model than in Sections 4.3.2 and 4.4 for the purpose of evaluating different harvest policies.\n\nSuppose you are a fisheries research biologist, and a commercial fishery for pink salmon (Oncorhynchus gorbuscha) takes place in your district. For the past 10 years, it has been fished with an exploitation rate of 40% (40% of the fish that return each year have been harvested, exploitation rate is abbreviated by \\(U\\)), resulting in an average annual harvest of 8.5 million fish. The management plan is up for evaluation this year, and your supervisor has asked you to prepare an analysis that determines if more harvest could be sustained if a different exploitation rate were to be used in the future.\n\nBased on historical data, your best understanding implies that the stock is driven by Ricker spawner-recruit dynamics. That is, the total number of fish that return this year (recruits) is a function of the total number of fish that spawned (spawners) in the year of their birth. The Ricker model can be written this way:\n\n\\[\\begin{equation} R_t = \\alpha S_{t-1} e^{-\\beta S_{t-1} + \\varepsilon_t} ,\\varepsilon_t \\sim N(0,\\sigma) \\tag{4.1} \\end{equation}\\]\n\nwhere \\(\\alpha\\) is a parameter representing the maximum recruits per spawner (obtained at very low spawner abundances) and \\(\\beta\\) is a measure of the strength of density-dependent mortality. Notice that the error term is in the exponent, which makes \\(e^{\\varepsilon_t}\\) lognormal.\n\nYou have estimates of the parameters :\n\n\\(\\alpha = 6\\)\n\n\\(\\beta = 1 \\times 10^{-7}\\)\n\n\\(\\sigma = 0.4\\)\n\nYou decide that you can build a policy analysis by simulating the stock forward through time under different exploitation rates. With enough iterations of the simulation, you will be able to see whether a different exploitation rate can provide more harvest than what is currently being extracted.\n\nFirst, write a function for your population model. Your function must:\n\ntake the parameters, dimensions (number of years), and the policy variable (\\(U\\)) as input arguments\n\nsimulate the population using Ricker dynamics\n\ncalculate and return the average harvest and escapement over the number of future years you simulated.\n\n# Step #1: name the function and give it some arguments ricker_sim =function(ny, params, U) { # extract the parameters out by name: alpha = params[\"alpha\"] beta = params[\"beta\"] sigma = params[\"sigma\"] # create containers # this is a neat trick to condense your code: R = S = H =NULL # initialize the population in the first year # start the population at being fished at 40% # with lognormal error R[1] =log(alpha * (1 -0.4))/(beta * (1 -0.4)) *exp(rnorm(1, 0, sigma)) S[1] = R[1] * (1 - U) H[1] = R[1] * U # carry simulation forward through time for (y in 2:ny) { # use the ricker function with random lognormal noise R[y] = S[y-1] * alpha *exp(-beta * S[y-1] +rnorm(1, 0, sigma)) #harvest and spawners are the same as before S[y] = R[y] * (1 - U) H[y] = R[y] * U } # wrap output in a list object list( mean_H = mean(H), mean_S = mean(S) ) }\n\nUse the function once:\n\nparams =c(alpha = 6, beta = 1e-7, sigma = 0.4) out =ricker_sim(U = 0.4, ny = 20, params = params) #average annual harvest (in millions) round(out$mean_H/1e6, digits = 2)\n\n## [1] 7.99\n\nIf you completed the stochastic power analysis example (Section 4.6.2), you might see where this is going. You are going to replicate applying a fixed policy many times to a random system. This is the Monte Carlo part of the analysis. The policy part is that you will compare the output from several candidate exploitation rates to inform a decision about which is best. This time, set up your analysis using sapply() (to iterate over different values of \\(U\\)) and replicate() (to iterate over different random populations fished at each \\(U\\)) instead of performing a nested for() loop as in previous examples:\n\nU_try =seq(0.4, 0.6, 0.01) n_rep =2000 H_out =sapply(U_try, function(u) { replicate(n = n_rep, expr = { ricker_sim(U = u, ny = 20, params = params)$mean_H/1e6 }) })\n\nThe nested replicate() and sapply() method is a bit cleaner than a nested for() loop, but you have less control over the format of the output.\n\nPlot the output of your simulations using a boxplot. To make things easier, give H_out column names representing the exploitation rate:\n\ncolnames(H_out) = U_try boxplot(H_out, outline = F, xlab = \"U\", ylab = \"Harvest (Millions of Fish)\", col = \"tomato\", las = 1)\n\nIt appears the stock could produce more harvest than its current 8.5 million fish per year if it was fished harder. However, your supervisors also do not want to see the escapement drop below three-quarters of what it has been in recent history (75% of approximately 13 million fish). They ask you to obtain the expected average annual escapement as well as harvest. You can simply re-run the code above, but extracting S_mean rather than H_mean. Call this output S_out and plot it just like harvest (if you’re curious, this blue color is col = \"skyblue\"):\n\nAfter seeing this information, your supervisor realizes they are faced with a trade-off: the stock could produce more with high exploitation rates, but they are concerned about pushing the stock too low would be unsustainable. They tell you to determine the probability that the average escapement would not be pushed below 75% of 13 million at each exploitation rate, as well as the probability that the average annual harvests will be at least 20% greater than they are currently (approximately 8.5 million fish). Given your output, this is easy:\n\n# determine if each element meets escapement criterion Smeet = S_out > (0.75 *13) # determine if each element meets harvest criterion Hmeet = H_out > (1.2 *8.5) # calculate the probability of each occuring at a given exploitation rate # remember, mean of a logical vector calculate the proportion of TRUEs p_Smeet =apply(Smeet, 2, mean) p_Hmeet =apply(Hmeet, 2, mean)\n\nYou plot this for your supervisor as follows:\n\n# the U levels to highlight on plot plot_U =seq(0.4, 0.6, 0.05) # create an empty plot par(mar = c(4,4,1,1)) plot(p_Smeet ~ p_Hmeet, type = \"n\", xlab = \"Probability of Meeting Harvest Criterion\", ylab = \"Probability of Meeting Escapement Criterion\") # add gridlines abline(v = seq(0, 1, 0.1), col = \"grey\") abline(h = seq(0, 1, 0.1), col = \"grey\") #draw on the tradeoff curve lines(p_Smeet ~ p_Hmeet, type = \"l\", lwd = 2) # add points and text for particular U policies points(p_Smeet[U_try %in% plot_U] ~ p_Hmeet[U_try %in% plot_U], pch = 16, cex = 1.5) text(p_Smeet[U_try %in% plot_U] ~ p_Hmeet[U_try %in% plot_U], labels = U_try[U_try %in% plot_U], pos = c(1,1,1,2,2))\n\nEquipped with this analysis, your supervisor plans to go to the policy-makers with the recommendation of adjusting the exploitation rate policy to use \\(U = 0.5\\), because they think it balances the trade-off. Notice how if the status quo was maintained, your model suggests you would have complete certainty of staying where you are now: escapement will remain above 75% of its current level with a 100% chance, but you would have no chance of improving harvests to greater than 20% of their current level. Small increases in the exploitation rate (e.g., from 0.4 to 0.45) have a reasonably large gain in harvest performance, but hardly any losses for the escapement criterion. Your supervisor is willing to live with a 90% chance that the escapement will stay where they desire in order to gain a >80% chance of obtaining the desired amount of increases in harvest.\n\nThe utility of using Monte Carlo methods in this example is the ability to calculate the probability of some event you are interested in. There are analytical (i.e., not simulation-based) solutions to predict the annual harvest and escapement from a fixed \\(U\\) from a population with parameters \\(\\alpha\\) and \\(\\beta\\), but by incorporating randomness, you were able to obtain the relative weights of outcomes other than the expectation under the deterministic Ricker model, thereby allowing the assignment of probabilities to meeting the two criteria.\n\n4.7.1 The Bootstrap\n\nSay you have a fitted model from which you want to propagate the uncertainty in some derived quantity. Consider the case of the von Bertalanffy growth model. This is a non-linear model used to predict the size of an organism (weight or length) based on its age. The model can be written for a non-linear regression model (see Section 3.4) as:\n\n\\[\\begin{equation} L_i = L_{\\infty}\\left(1 - e^{-k(age_i-t_0)}\\right) + \\varepsilon_i, \\varepsilon_i \\sim N(0, \\sigma) \\tag{4.2} \\end{equation}\\]\n\nwhere \\(L_i\\) and \\(age_i\\) are the observed length and age of individual \\(i\\), respectively, and \\(L_{\\infty}\\), \\(k\\), and \\(t_0\\) are parameters to be estimated. The interpretations of the parameters are as follows:\n\n\\(L_{\\infty}\\): the maximum average length achieved\n\n\\(k\\): a growth coefficient linked to metabolic rate. It specifies the rate of increase in length as the fish ages early in life\n\n\\(t_0\\): the theoretical age when length equals zero (the x-intercept).\n\nUse the data set growth.csv for this example (see the instructions on acquiring data files). Read in and plot the data:\n\ndat =read.csv(\"../Data/growth.csv\") plot(length ~ age, data = dat, pch = 16, col = \"grey\")\n\nDue to a large amount of variability in individual growth rates, the relationship looks pretty noisy. Notice how you have mostly young fish in your sample: this is characteristic of “random” sampling of fish populations.\n\nSuppose you would like to obtain the probability that an average-sized fish of each age is sexually mature. You know that fish of this species mature at approximately 450 mm, and you simply need to determine the fraction of all fish at each age that are greater than 450 mm. However, you don’t have any observations for some ages (e.g., age 8), so you cannot simply calculate this fraction based on your raw data. You need to fit the von Bertalanffy growth model, then carry the statistical uncertainty from the fitted model forward to the predicted length-at-age. This would be difficult to obtain using only the coefficient estimates and their standard errors, because of the non-linear relationship between the \\(x\\) and \\(y\\) variables.\n\nEnter the bootstrap, which is a Monte Carlo analysis using an observed data set and a model. The pseudocode for a bootstrap analysis is:\n\nResample from the original data (with replacement)\n\nFit a model of interest\n\nDerive some quantity of interest from the fitted model\n\nRepeat steps 1 - 3 many times\n\nSummarize the randomized quantities from step 4\n\nIn this example, you will apply a bootstrap approach to obtain the distribution of expected fish lengths at each age, then use these distributions to quantify the probability that an averaged-sized fish of each age is mature (i.e., greater than 450 mm).\n\nYou will write a function for each of steps 1 - 3 above. The first is to resample the data:\n\nrandomize =function(dat) { # number of observed pairs n =nrow(dat) # sample the rows to determine which will be kept keep =sample(x = 1:n, size = n, replace = T) # retreive these rows from the data dat[keep,] }\n\nNotice the use of replace = T here: without this, there would be no bootstrap. You would just sample the same observations over and over, their order in the rows would just be shuffled. Next, write a function to fit the model (revisit Section 3.4 for more details on nls()):\n\nfit_vonB =function(dat) { nls(length ~ linf * (1 -exp(-k * (age - t0))), data = dat, start = c(linf = 600, k = 0.3, t0 = -0.2) ) }\n\nThis function will return a fitted model object when executed. Next, write a function to predict mean length-at-age:\n\n# create a vector of ages ages =min(dat$age):max(dat$age) pred_vonB =function(fit) { # extract the coefficients ests =coef(fit) # predict length-at-age ests[\"linf\"] * (1 -exp(-ests[\"k\"] * (ages - ests[\"t0\"]))) }\n\nNotice your function will use the object ages even though it was not defined in the function. This has to do with lexical scoping and environments, which are beyond the scope of this introductory material. If you’d like more details, see the section in Wickham (2015) on it . Basically, if an object with the same name as one defined in the function exists outside of the function, the function will use the one that is defined within the function. If there is no object defined in the function with that name, it will look outside of the function for that object.\n\nNow, use these three functions to perform one iteration:\n\npred_vonB(fit = fit_vonB(dat = randomize(dat = dat)))\n\nYou can wrap this inside of a replicate() call to perform step 4 above:\n\nset.seed(2) out =replicate(n = 100, expr = { pred_vonB(fit = fit_vonB(dat = randomize(dat = dat))) }) dim(out)\n\n## [1] 10 100\n\nIt appears the rows are different ages and the columns are different bootstrapped iterations. Summarize the random lengths at each age:\n\nsumm =apply(out, 1, function(x) c(mean = mean(x), quantile(x, c(0.025, 0.975))))\n\nPlot the data, the summarized ranges of mean lengths, and the length at which all fish are assumed to be mature (450 mm)\n\nplot(length ~ age, data = dat, col = \"grey\", pch = 16, ylim = c(0, max(dat$length, summ[\"97.5%\",])), ylab = \"Length (mm)\", xlab = \"Age (years)\") lines(summ[\"mean\",] ~ ages, lwd = 2) lines(summ[\"2.5%\",] ~ ages, col = \"grey\") lines(summ[\"97.5%\",] ~ ages, col = \"grey\") abline(h = 450, col = \"blue\")\n\nObtain the fraction of iterations that resulted in the mean length-at-age being greater than 450 mm. This is interpreted as the probability that the average-sized fish of each age is mature:\n\np_mat =apply(out, 1, function(x) mean(x >450)) plot(p_mat ~ ages, type = \"b\", pch = 17, xlab = \"Age (years)\", ylab = \"Probability of Average Fish Mature\")\n\nThis maturity schedule can be used by fishery managers in attempting to decide which ages should be allowed to be harvested and which should be allowed to grow more . Because each age has an associated expected length, managers can use what they know about the size selectivity of various gear types to set policies that attempt to target some ages more than others."
    }
}
{
    "id": "dbpedia_2452_0",
    "rank": 7,
    "data": {
        "url": "https://vdoc.pub/documents/limits-of-computation-from-a-programming-perspective-1760ea23ic38",
        "read_more_link": "",
        "language": "en",
        "title": "Limits Of Computation: From A Programming Perspective [PDF] [1760ea23ic38]",
        "top_image": "https://vdoc.pub/img/detail/1760ea23ic38.jpg",
        "meta_img": "https://vdoc.pub/img/detail/1760ea23ic38.jpg",
        "images": [
            "https://vdoc.pub/theme/static/images/header-logo3.png",
            "https://vdoc.pub/theme/static/images/logo-socudoc-square.png",
            "https://vdoc.pub/theme/static/images/logo-socudoc-square.png",
            "https://vdoc.pub/img/detail/1760ea23ic38.jpg",
            "https://vdoc.pub/img/crop/300x300/1760ea23ic38.jpg",
            "https://vdoc.pub/img/crop/300x300/39pnasvulq3g.jpg",
            "https://vdoc.pub/img/crop/300x300/5h7qp082ouo0.jpg",
            "https://vdoc.pub/img/crop/300x300/7o51i9n4mv90.jpg",
            "https://vdoc.pub/img/crop/300x300/3f7fctptf39g.jpg",
            "https://vdoc.pub/img/crop/300x300/4po4nqjj6ii0.jpg"
        ],
        "movies": [],
        "keywords": [],
        "meta_keywords": [
            ""
        ],
        "tags": null,
        "authors": [],
        "publish_date": null,
        "summary": "",
        "meta_description": "Limits Of Computation: From A Programming Perspective [PDF] [1760ea23ic38]. This textbook discusses the most fundamental and puzzling questions about the foundations of computing. In 23 lecture-si...",
        "meta_lang": "en",
        "meta_favicon": "https://vdoc.pub/theme/static/images/favicon/favicon-32x32.png",
        "meta_site_name": "",
        "canonical_link": "https://vdoc.pub/documents/limits-of-computation-from-a-programming-perspective-1760ea23ic38",
        "text": "Undergraduate Topics in Computer Science\n\nBernhard Reus\n\nLimits of Computation From a Programming Perspective\n\nUndergraduate Topics in Computer Science\n\nUndergraduate Topics in Computer Science (UTiCS) delivers high-quality instructional content for undergraduates studying in all areas of computing and information science. From core foundational and theoretical material to ﬁnal-year topics and applications, UTiCS books take a fresh, concise, and modern approach and are ideal for self-study or for a one- or two-semester course. The texts are all authored by established experts in their ﬁelds, reviewed by an international advisory board, and contain numerous examples and problems. Many include fully worked solutions.\n\nMore information about this series at http://www.springer.com/series/7592\n\nBernhard Reus\n\nLimits of Computation From a Programming Perspective\n\n123\n\nBernhard Reus Department of Informatics School of Engineering and Informatics University of Sussex Brighton UK Series editor Ian Mackie Advisory Board Samson Abramsky, University of Oxford, Oxford, UK Karin Breitman, Pontiﬁcal Catholic University of Rio de Janeiro, Rio de Janeiro, Brazil Chris Hankin, Imperial College London, London, UK Dexter Kozen, Cornell University, Ithaca, USA Andrew Pitts, University of Cambridge, Cambridge, UK Hanne Riis Nielson, Technical University of Denmark, Kongens Lyngby, Denmark Steven Skiena, Stony Brook University, Stony Brook, USA Iain Stewart, University of Durham, Durham, UK\n\nISSN 1863-7310 ISSN 2197-1781 (electronic) Undergraduate Topics in Computer Science ISBN 978-3-319-27887-2 ISBN 978-3-319-27889-6 (eBook) DOI 10.1007/978-3-319-27889-6 Library of Congress Control Number: 2015960818 © Springer International Publishing Switzerland 2016 This work is subject to copyright. All rights are reserved by the Publisher, whether the whole or part of the material is concerned, speciﬁcally the rights of translation, reprinting, reuse of illustrations, recitation, broadcasting, reproduction on microﬁlms or in any other physical way, and transmission or information storage and retrieval, electronic adaptation, computer software, or by similar or dissimilar methodology now known or hereafter developed. The use of general descriptive names, registered names, trademarks, service marks, etc. in this publication does not imply, even in the absence of a speciﬁc statement, that such names are exempt from the relevant protective laws and regulations and therefore free for general use. The publisher, the authors and the editors are safe to assume that the advice and information in this book are believed to be true and accurate at the date of publication. Neither the publisher nor the authors or the editors give a warranty, express or implied, with respect to the material contained herein or for any errors or omissions that may have been made. Printed on acid-free paper This Springer imprint is published by SpringerNature The registered company is Springer International Publishing AG Switzerland\n\nForeword\n\nComputer Science centers on questions about computational problems, and computer programs to solve problems: • What is a computational problem, what is an algorithm, what does it mean to have solved a problem, are some problems unsolvable by any computing device, are there problems intrinsically difﬁcult or impossible to solve automatically, are there problems intrinsically easier to solve than others, …? • How can a machine solve a problem, how to build such a machine, how to specify an algorithm for computer execution, how to design good algorithms, which “language” can a human use to direct the computer, how to design and “debug” programs for computer execution, how to build good programs, …? Good news: rapid progress has been made in both areas; we stand on the shoulders of giants in both theory and practice. The questions above have many and various answers. The ﬁrst questions led in mathematical directions to foundational studies: the theories of computability, recursive functions, automata theory and more. The second led in engineering directions: computer architectures, the architectures of programming languages, the art or discipline of programming, software engineering and more. Since the 1930s our understanding of both areas has developed hand in hand, led by theoreticians such as Kleene, Church, and Gödel; by hardware and software inventors such as Babbage, Von Neumann, and McCarthy; and by Alan Turing’s genius at the borderline between the two areas. This book focuses on the ﬁrst question area by an approach near the borderline: the theory of computability and complexity (C&C for short) is presented by using a simple programming language. In this language one is able to perform the (many) program constructions needed for the theory. This is done abstractly enough to reveal the great breadth and depth of C&C. Further, it is done concretely and precisely enough to satisfy practice-oriented readers about the constructions’ feasibility. Effect: a reader can see the relative efﬁciency of the constructions and understand the efﬁciency of what is constructed.\n\nv\n\nvi\n\nForeword\n\nMy 1997 C&C book was a step in this direction, but suffers from several problems: a scope too great for a single-semester university course; sections that (without warning) require more mathematical maturity than others; too few exercises; and too few historical and current references to research contexts. Bernhard Reus has succeeded very well in overcoming these problems, and writing a deep, interesting, up-to-date, and even entertaining book on computability and complexity. I recommend it highly, both for systematic study and for spot reading. Neil D. Jones DIKU, University of Copenhagen\n\nPreface\n\nAbout 12 years ago a student1 asked me after one of my lectures in Computability and Complexity why he had to write tedious Turing machine programs, given that everyone programmed in languages like Java. He had a point. What is the best way to teach a Computability and Complexity module in the twenty-ﬁrst century to a cohort of students who are used to programming in high level languages with modern tools and libraries; to students who live in a world of smart phones and 24h connectivity, and, even more importantly maybe, who have not been exposed to very much formal reasoning and mathematics? Turn the clock back only two or three decades. Then, a ﬁrst year in a computer science Bachelor degree mainly consisted of mathematics (analysis and linear algebra, later discrete maths, numerical analysis, and basic probability theory). When computability and complexity was taught in the second or third year, students were already acquainted with a formal and very mathematical language of discourse, maybe because computer science lecturers in the 80s were usually mathematicians by trade. Things have changed signiﬁcantly. Curriculum designers for Bachelor degrees are under pressure to push more and more new exciting material into a three-year degree program that should prepare students for their lives as working IT professionals. Any new module moved into the curriculum necessarily forces another one out. Often, allegedly “unpopular” modules, including formal theory and mathematics, are the victims. As a consequence, computer science students have to a degree lost the skills to digest material presented in an extremely formal and symbolic fashion while, at the same time, they are proliﬁc programmers and quite knowledgeable in the use of tools. So, Computability and Complexity, do they really have to be taught using Turing machines or µ-recursive functions? Do they have to be presented in the style a logician or mathematician would prefer? Seeking for alternatives, I eventually stumbled across Neil Jones’ fantastic book Computability and Complexity—From a Programming Perspective. The subtitle already gives away the book’s philosophy.\n\n1\n\nAlexis Petrounias.\n\nvii\n\nviii\n\nPreface\n\nThe leitmotif of Neil’s textbook is to present the most important results in computability and complexity theory “using programming techniques and motivated by programming language theory” as well as “using a novel model of computation, differing from traditional ones in crucial aspects”.2 The latter, WHILE, is a simple imperative language with one datatype of lists (s-expressions) à la LISP. Admittedly, this language is not Java, but it has the hallmarks of a modern high level language and is inﬁnitely more comfortable to program in than Turing machines or Gödel numbers. Java, or any similar powerful language, would be impractical for our purposes “since proofs about them would be too complex to be easily understood”.3 So when rebranding the module under the name Limits of Computation in 2008, I adopted Neil’s book as course textbook. Delivering an introductory, one semester ﬁnal-year module, I picked the most important and appealing chapters. This was easy as the design of the book was exactly made to mix and match.4 Soon, however, it turned out that students found Neil’s book tough going. In fact, this became more apparent as the years went past. There were several factors. First of all, Neil’s students would have had ML, a functional language with built in list type, as a ﬁrst programming language, whereas our students were raised on Java. Yet, the datatype of WHILE is a functional one, and this caused more problems to the students than anticipated. Second, and more importantly, I had not put enough attention to the prerequisites. Neil expected readers of his book to be senior undergraduate students “with good mathematical maturity.”5 It turned out that not all the third-year students had this maturity (given the heterogeneity of backgrounds and reduction of maths teaching in the undergraduate years one and two). As a response to mitigate the issues above, I started writing explicit notes to accompany my slides, intended as additional comments and explanations for the selected book chapters. I ended up adding more and more new material and rearranging it. The results of this effort are the 23 individual lectures of this book. A (British) semester is 12 weeks long, which usually requires 24 lectures to be delivered. The shortfall of one lecture is intentional, it acts as a buffer (in case things take more time) and also allows for extra events like invited talks or in-class tests. This book was heavily influenced by Neil Jones’s textbook, which is clearly visible in some chapters. To pay homage to his book, its telling subtitle “From a Programming Perspective” has been adopted. Brighton November 2015\n\n2\n\nBernhard Reus\n\nPreface of Neil’s book, page x. The book “Understanding Computation From Simple Machines to Impossible Programs” by Tom Stuart, published by O’Reilly in 2013, appears to follow the same idea and philosophy, using Ruby as programming language. It does not deal with complexity however. 4 Neil’s Preface, page xii. 5 Neil’s Preface, page xiii. 3\n\nFor Tutors\n\nThe students using this book are expected to be senior undergraduates who can master at least one imperative programming language. They are supposed to know arithmetic, Boolean algebra, graphs and some basic graph algorithms. Similarly, knowledge of basic set theory, function, and relations is needed, but Chap. 2 contains a short summary of basic deﬁnitions used in the book. Some exposition to formal reasoning would be helpful as well, but is not strictly required. To understand the probabilistic complexity classes in Chap. 21 some basic knowledge of probability theory will be needed. This book is divided into 23 chapters with the intention that each chapter corresponds to one lecture. If not all chapters can be delivered, the following chapters can be omitted without interrupting the natural narrative: • Chapter 10, Self-referencing Programs, but the self-producing program is a brain teaser which turns out to be very popular with students; • Chapter 21, How to Solve NP-complete Problems, which may, however, be the chapter that has the most impact on students’ future projects; • The last two chapters Molecular Computing (Chap. 22) and Quantum Computing (Chap. 23), but students ﬁnd this material particularly exciting. A few chapters are signiﬁcantly longer than others. These are Chap. 11, The Church-Turing Thesis, Chap. 16, Famous Problems in P, Chap. 20 Complete Problems, Chap. 21 How Solve NP-complete Problems?, and Chap. 22 Molecular Computing. The longer chapters allow the tutor to pick some of the sections and present them in the lecture, leaving the remaining ones for self-study or exercises.\n\nix\n\nAcknowledgements\n\nThis book would not have been possible without the help and influence of so many people. They all deserve a big thank you. First of all, I wish to thank Neil Jones for his book on Computability and Complexity, which has been inspirational. The results of many brilliant researchers have been reported in this textbook. Many of them have been mentioned and referenced but, this being an introductory textbook, some of them may have been left out. My sincerest apologies to those who were omitted. I am grateful for the feedback I have received over the years from my teaching assistants Matthew Wall, Jan Schwinghammer, Cristiano Solarino, Billiejoe Charlton, Ben Horsfall, and Shinya Sato. I enjoyed and beneﬁted from talking to many students on the Limits of Computation course I taught at Sussex. Thanks go in particular to Alexis Petrounias, Marek Es, Thomas Weedon Hume, Alex Jeffery, Susan Coleman, Sarah Aspery, Benjamin Hayward, Jordan Hobday, and Lucas Rijllart. The latter ﬁve also gave feedback on early versions of various chapters. I also beneﬁted from discussions with Thomas Streicher and my colleagues at Sussex: thanks go to Martin Berger, Matthew Hennessy, Julian Rathke, Des Watson, and particularly, George Parisis. Des and George also provided most welcome proof-reading services. Luca Cardelli was kind enough to discuss current topics in molecular computing and gave valuable pointers regarding Chap. 22. Neil Jones and Matthew Hennessy provided comments on an entire draft of this book which helped improve the presentation. All remaining errors are of course solely mine. I would also like to thank series editor Ian Mackie for his encouragement, Helen Desmond from Springer for her continuous support and Divya Meiyazhagan from the production team for all the last minute edits. The wonderful LaTeX typesetting and the TikZ (PGF) vector graph drawing systems have been used. Thanks to all those people who contributed to their development.\n\nxi\n\nxii\n\nAcknowledgements\n\nMy sister’s family and my friends deserve acknowledgment for moral support, and for being there for me when it counted. Finally, I would like to dedicate this book to the memory of my parents and my brother. His interest in the sciences and in computing aroused my curiosity already at a young age.\n\nContents\n\n1\n\nLimits? What Limits? . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1.1 Physical Limits of Computation . . . . . . . . . . . . . . . . . . 1.1.1 Fundamental Engineering Constraints to Semiconductor Manufacturing and Scaling . . . . 1.1.2 Fundamental Limits to Energy Efﬁciency . . . . . 1.1.3 Fundamental Physical Constraints on Computing in General . . . . . . . . . . . . . . . . . . . . . . . . . . . 1.2 The Limits Addressed . . . . . . . . . . . . . . . . . . . . . . . . . 1.2.1 Computability Overview . . . . . . . . . . . . . . . . . 1.2.2 Complexity Overview . . . . . . . . . . . . . . . . . . . References . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n\nPart I 2\n\n..... .....\n\n1 2\n\n..... .....\n\n2 3\n\n. . . . .\n\n. . . . .\n\n. . . . .\n\n. . . . .\n\n. . . . .\n\n3 4 4 6 9\n\n. . . . . . . . . . . . .\n\n. . . . . . . . . . . . .\n\n. . . . . . . . . . . . .\n\n. . . . . . . . . . . . .\n\n. . . . . . . . . . . . .\n\n13 14 14 16 17 17 21 22 22 23 24 26 27\n\nComputability\n\nProblems and Effective Procedures . . . . . . . . . 2.1 On Computability. . . . . . . . . . . . . . . . . . 2.1.1 Historical Remarks . . . . . . . . . . . 2.1.2 Effective Procedures . . . . . . . . . . 2.2 Sets, Relations and Functions . . . . . . . . . 2.2.1 Sets . . . . . . . . . . . . . . . . . . . . . 2.2.2 Relations . . . . . . . . . . . . . . . . . . 2.2.3 Functions . . . . . . . . . . . . . . . . . 2.2.4 Partial Functions . . . . . . . . . . . . 2.2.5 Total Functions . . . . . . . . . . . . . 2.3 Problems. . . . . . . . . . . . . . . . . . . . . . . . 2.3.1 Computing Solutions to Problems. References . . . . . . . . . . . . . . . . . . . . . . . . . . .\n\n. . . . . . . . . . . . .\n\n. . . . . . . . . . . . .\n\n. . . . . . . . . . . . .\n\n. . . . . . . . . . . . .\n\n. . . . . . . . . . . . .\n\n. . . . . . . . . . . . .\n\n. . . . . . . . . . . . .\n\n. . . . . . . . . . . . .\n\n. . . . . . . . . . . . .\n\n. . . . . . . . . . . . .\n\nxiii\n\nxiv\n\nContents\n\n3\n\nThe WHILE-Language . . . . . . . . . . . 3.1 The Data Type of Binary Trees . 3.2 WHILE-Syntax. . . . . . . . . . . . . 3.2.1 Expressions . . . . . . . . . 3.2.2 Commands . . . . . . . . . 3.2.3 Programs. . . . . . . . . . . 3.2.4 A Grammar for WHILE. 3.2.5 Layout Conventions and 3.3 Encoding Data Types as Trees . . 3.3.1 Boolean Values . . . . . . 3.3.2 Lists and Pairs . . . . . . . 3.3.3 Natural Numbers . . . . . 3.3.4 Finite Words . . . . . . . . 3.4 Sample Programs . . . . . . . . . . . 3.4.1 Addition . . . . . . . . . . . 3.4.2 List Reversal . . . . . . . . 3.4.3 Tail Recursion . . . . . . . 3.4.4 Analysis of Algorithms . References . . . . . . . . . . . . . . . . . . . .\n\n....... ....... ....... ....... ....... ....... ....... Brackets. ....... ....... ....... ....... ....... ....... ....... ....... ....... ....... .......\n\n. . . . . . . . . . . . . . . . . . .\n\n. . . . . . . . . . . . . . . . . . .\n\n. . . . . . . . . . . . . . . . . . .\n\n. . . . . . . . . . . . . . . . . . .\n\n. . . . . . . . . . . . . . . . . . .\n\n. . . . . . . . . . . . . . . . . . .\n\n. . . . . . . . . . . . . . . . . . .\n\n. . . . . . . . . . . . . . . . . . .\n\n. . . . . . . . . . . . . . . . . . .\n\n. . . . . . . . . . . . . . . . . . .\n\n. . . . . . . . . . . . . . . . . . .\n\n. . . . . . . . . . . . . . . . . . .\n\n. . . . . . . . . . . . . . . . . . .\n\n. . . . . . . . . . . . . . . . . . .\n\n. . . . . . . . . . . . . . . . . . .\n\n29 31 32 32 32 33 33 34 35 35 36 37 40 40 40 41 42 43 45\n\n4\n\nSemantics of WHILE . . . . . . . . 4.1 Stores . . . . . . . . . . . . . . 4.2 Semantics of Programs . . 4.3 Semantics of Commands . 4.4 Semantics of Expressions References . . . . . . . . . . . . . . .\n\n. . . . . .\n\n. . . . . .\n\n. . . . . .\n\n. . . . . .\n\n. . . . . .\n\n. . . . . .\n\n. . . . . .\n\n. . . . . .\n\n. . . . . .\n\n. . . . . .\n\n. . . . . .\n\n. . . . . .\n\n. . . . . .\n\n. . . . . .\n\n. . . . . .\n\n. . . . . .\n\n. . . . . .\n\n. . . . . .\n\n. . . . . .\n\n. . . . . .\n\n. . . . . .\n\n. . . . . .\n\n. . . . . .\n\n. . . . . .\n\n. . . . . .\n\n. . . . . .\n\n. . . . . .\n\n47 48 49 50 52 54\n\n5\n\nExtensions of WHILE . . . . . . . 5.1 Equality . . . . . . . . . . . . 5.2 Literals . . . . . . . . . . . . . 5.2.1 Number Literals . 5.2.2 Boolean Literals . 5.3 Adding Atoms . . . . . . . . 5.4 List Constructor . . . . . . . 5.5 Macro Calls. . . . . . . . . . 5.6 Switch Statement . . . . . . References . . . . . . . . . . . . . . .\n\n. . . . . . . . . .\n\n. . . . . . . . . .\n\n. . . . . . . . . .\n\n. . . . . . . . . .\n\n. . . . . . . . . .\n\n. . . . . . . . . .\n\n. . . . . . . . . .\n\n. . . . . . . . . .\n\n. . . . . . . . . .\n\n. . . . . . . . . .\n\n. . . . . . . . . .\n\n. . . . . . . . . .\n\n. . . . . . . . . .\n\n. . . . . . . . . .\n\n. . . . . . . . . .\n\n. . . . . . . . . .\n\n. . . . . . . . . .\n\n. . . . . . . . . .\n\n. . . . . . . . . .\n\n. . . . . . . . . .\n\n. . . . . . . . . .\n\n. . . . . . . . . .\n\n. . . . . . . . . .\n\n. . . . . . . . . .\n\n. . . . . . . . . .\n\n. . . . . . . . . .\n\n. . . . . . . . . .\n\n55 55 56 56 57 57 58 59 60 63\n\n6\n\nPrograms as Data Objects . . . . . . . . 6.1 Interpreters Formally . . . . . . . . 6.2 Abstract Syntax Trees . . . . . . . . 6.3 Encoding of WHILE-ASTs in D . Reference . . . . . . . . . . . . . . . . . . . . .\n\n. . . . .\n\n. . . . .\n\n. . . . .\n\n. . . . .\n\n. . . . .\n\n. . . . .\n\n. . . . .\n\n. . . . .\n\n. . . . .\n\n. . . . .\n\n. . . . .\n\n. . . . .\n\n. . . . .\n\n. . . . .\n\n. . . . .\n\n. . . . .\n\n. . . . .\n\n. . . . .\n\n. . . . .\n\n. . . . .\n\n. . . . .\n\n. . . . .\n\n65 66 67 67 70\n\nContents\n\nxv\n\n7\n\nA Self-interpreter for WHILE . . . . . . . . . . . . . . . . . . 7.1 A Self-interpreter for WHILE -Programs with One 7.1.1 General Tree Traversal for ASTs . . . . . . 7.1.2 The STEP Macro . . . . . . . . . . . . . . . . . 7.2 A Self-interpreter for WHILE . . . . . . . . . . . . . . . 7.2.1 Store Manipulation Macros . . . . . . . . . . References . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n\n8\n\nAn Undecidable (Non-computable) Problem . . . . . . . . 8.1 WHILE-Computability and Decidability. . . . . . . . . 8.2 The Halting Problem for WHILE . . . . . . . . . . . . . 8.3 Diagonalisation and the Barber “Paradox” . . . . . . . 8.4 Proof of the Undecidability of the Halting Problem References . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n\n. . . . . .\n\n. . . . . .\n\n. . . . . .\n\n. . . . . .\n\n. . . . . .\n\n9\n\nMore Undecidable Problems . . . . . . . . . . . . . . 9.1 Semi-decidability of the Halting Problem . 9.2 Rice’s Theorem . . . . . . . . . . . . . . . . . . . 9.3 The Tiling Problem . . . . . . . . . . . . . . . . 9.4 Problem Reduction . . . . . . . . . . . . . . . . . 9.5 Other (Famous) Undecidable Problems . . . 9.6 Dealing with Undecidable Problems . . . . . 9.7 A Fast-Growing Non-computable Function References . . . . . . . . . . . . . . . . . . . . . . . . . . .\n\n....... Variable . ....... ....... ....... ....... .......\n\n. . . . . . .\n\n. . . . . . .\n\n. . . . . . .\n\n71 72 72 73 81 83 86\n\n. . . . . .\n\n. . . . . .\n\n. . . . . .\n\n. . . . . .\n\n87 87 89 90 92 95\n\n. . . . . . . . .\n\n. . . . . . . . .\n\n. . . . . . . . .\n\n. . . . . . . . .\n\n. . . . . . . . .\n\n. . . . . . . . .\n\n. . . . . . . . .\n\n. . . . . . . . .\n\n. . . . . . . . .\n\n. . . . . . . . .\n\n. . . . . . . . .\n\n. . . . . . . . .\n\n. . . . . . . . .\n\n. . . . . . . . .\n\n. . . . . . . . .\n\n97 97 99 101 103 105 106 107 111\n\n. . . . .\n\n. . . . .\n\n. . . . .\n\n. . . . .\n\n. . . . .\n\n. . . . .\n\n. . . . .\n\n. . . . .\n\n. . . . .\n\n. . . . .\n\n. . . . .\n\n. . . . .\n\n. . . . .\n\n. . . . .\n\n. . . . .\n\n113 114 116 118 121\n\n11 The Church-Turing Thesis . . . . . . . . . . . . . . . . . . . 11.1 The Thesis . . . . . . . . . . . . . . . . . . . . . . . . . . 11.2 Semantic Framework for Machine-Like Models . 11.3 Turing Machines TM. . . . . . . . . . . . . . . . . . . . 11.4 GOTO-Language . . . . . . . . . . . . . . . . . . . . . . . 11.5 Register Machines RAM and SRAM . . . . . . . . . . 11.6 Counter Machines CM . . . . . . . . . . . . . . . . . . . 11.7 Cellular Automata . . . . . . . . . . . . . . . . . . . . . 11.7.1 2D: Game of Life. . . . . . . . . . . . . . . . 11.7.2 1D: Rule 110. . . . . . . . . . . . . . . . . . . 11.8 Robustness of Computability . . . . . . . . . . . . . . 11.8.1 The Crucial Role of Compilers . . . . . . 11.8.2 Equivalence of Models . . . . . . . . . . . . References . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n\n. . . . . . . . . . . . . .\n\n. . . . . . . . . . . . . .\n\n. . . . . . . . . . . . . .\n\n. . . . . . . . . . . . . .\n\n. . . . . . . . . . . . . .\n\n. . . . . . . . . . . . . .\n\n. . . . . . . . . . . . . .\n\n. . . . . . . . . . . . . .\n\n. . . . . . . . . . . . . .\n\n. . . . . . . . . . . . . .\n\n. . . . . . . . . . . . . .\n\n123 124 125 126 129 131 134 135 138 140 141 141 142 147\n\n10 Self-referencing Programs . . . . . . 10.1 The S-m-n Theorem . . . . . . . 10.2 Kleene’s Recursion Theorem . 10.3 Recursion Elimination. . . . . . References . . . . . . . . . . . . . . . . . .\n\n. . . . .\n\n. . . . .\n\n. . . . .\n\n. . . . .\n\n. . . . .\n\n. . . . .\n\n. . . . .\n\n. . . . .\n\n. . . . .\n\nxvi\n\nPart II\n\nContents\n\nComplexity\n\n12 Measuring Time Usage. . . . . . . . . . . . . . . . . . . . . . . . . . . 12.1 Unit-Cost Time Measure . . . . . . . . . . . . . . . . . . . . . . 12.2 Time Measure for WHILE . . . . . . . . . . . . . . . . . . . . . 12.3 Comparing Programming Languages Considering Time References . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n\n. . . . .\n\n. . . . .\n\n. . . . .\n\n. . . . .\n\n. . . . .\n\n. . . . .\n\n151 152 154 157 160\n\n13 Complexity Classes. . . . . . . . . . . . . . . . . . . . . . . . . . . . 13.1 Runtime Bounds . . . . . . . . . . . . . . . . . . . . . . . . . 13.2 Time Complexity Classes . . . . . . . . . . . . . . . . . . . 13.3 Lifting Simulation Properties to Complexity Classes . 13.4 Big-O and Little-o . . . . . . . . . . . . . . . . . . . . . . . . References . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n\n. . . . . .\n\n. . . . . .\n\n. . . . . .\n\n. . . . . .\n\n. . . . . .\n\n. . . . . .\n\n. . . . . .\n\n. . . . . .\n\n161 162 163 165 166 171\n\n14 Robustness of P . . . . . . . . . . . . . . . . . . . 14.1 Extended Church–Turing Thesis. . . . 14.2 Invariance or Cook’s Thesis . . . . . . 14.2.1 Non-sequential Models . . . . 14.2.2 Evidence for Cook’s Thesis . 14.2.3 Linear Time. . . . . . . . . . . . 14.3 Cobham–Edmonds Thesis . . . . . . . . References . . . . . . . . . . . . . . . . . . . . . . .\n\n. . . . . . . .\n\n. . . . . . . .\n\n. . . . . . . .\n\n. . . . . . . .\n\n. . . . . . . .\n\n. . . . . . . .\n\n. . . . . . . .\n\n. . . . . . . .\n\n. . . . . . . .\n\n. . . . . . . .\n\n. . . . . . . .\n\n. . . . . . . .\n\n. . . . . . . .\n\n. . . . . . . .\n\n. . . . . . . .\n\n. . . . . . . .\n\n. . . . . . . .\n\n. . . . . . . .\n\n. . . . . . . .\n\n173 174 174 175 176 178 179 181\n\n15 Hierarchy Theorems . . . . . . . . . . . . . 15.1 Linear Time Hierarchy Theorems . 15.2 Beyond Linear Time. . . . . . . . . . 15.3 Gaps in the Hierarchy. . . . . . . . . References . . . . . . . . . . . . . . . . . . . . .\n\n. . . . .\n\n. . . . .\n\n. . . . .\n\n. . . . .\n\n. . . . .\n\n. . . . .\n\n. . . . .\n\n. . . . .\n\n. . . . .\n\n. . . . .\n\n. . . . .\n\n. . . . .\n\n. . . . .\n\n. . . . .\n\n. . . . .\n\n. . . . .\n\n. . . . .\n\n. . . . .\n\n. . . . .\n\n183 184 189 192 193\n\n16 Famous Problems in P . . . . . . . . . . . . . . . . . . . . . 16.1 Decision Versus Optimisation Problems . . . . . 16.2 Predecessor Problem. . . . . . . . . . . . . . . . . . . 16.3 Membership Test for a Context Free Language 16.4 Primality Test . . . . . . . . . . . . . . . . . . . . . . . 16.5 Graph Problems . . . . . . . . . . . . . . . . . . . . . . 16.5.1 Reachability in a Graph . . . . . . . . . . 16.5.2 Shortest Paths in a Graph . . . . . . . . . 16.5.3 Maximal Matchings . . . . . . . . . . . . . 16.5.4 Min-Cut and Max-Flow . . . . . . . . . . 16.5.5 The Seven Bridges of Königsberg . . . 16.6 Linear Programming . . . . . . . . . . . . . . . . . . . References . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n\n. . . . . . . . . . . . .\n\n. . . . . . . . . . . . .\n\n. . . . . . . . . . . . .\n\n. . . . . . . . . . . . .\n\n. . . . . . . . . . . . .\n\n. . . . . . . . . . . . .\n\n. . . . . . . . . . . . .\n\n. . . . . . . . . . . . .\n\n. . . . . . . . . . . . .\n\n. . . . . . . . . . . . .\n\n. . . . . . . . . . . . .\n\n. . . . . . . . . . . . .\n\n195 197 198 201 202 203 203 204 206 207 208 210 215\n\n. . . . .\n\n. . . . .\n\n17 Common Problems Not Known to Be in P . . . . . . . . . . . . . . . . . . 217 17.1 The Travelling Salesman Problem (TSP) . . . . . . . . . . . . . . . . . 218 17.2 The Graph Colouring Problem . . . . . . . . . . . . . . . . . . . . . . . . 220\n\nContents\n\nxvii\n\n17.3 Max-Cut Problem. . . . . . . . . 17.4 The 0-1 Knapsack Problem . . 17.5 Integer Programming Problem 17.6 Does Not Being in P Matter? References . . . . . . . . . . . . . . . . . .\n\n. . . . .\n\n. . . . .\n\n. . . . .\n\n. . . . .\n\n. . . . .\n\n. . . . .\n\n. . . . .\n\n. . . . .\n\n. . . . .\n\n. . . . .\n\n. . . . .\n\n221 222 223 224 226\n\n18 The One-Million-Dollar Question . . . . . . . . . . . . . . . . . . 18.1 The Complexity Class NP . . . . . . . . . . . . . . . . . . . . 18.2 Nondeterministic Programs . . . . . . . . . . . . . . . . . . . 18.2.1 Time Measure of Nondeterministic Programs . 18.2.2 Some Basic Facts About NP . . . . . . . . . . . . 18.3 Robustness of NP . . . . . . . . . . . . . . . . . . . . . . . . . 18.4 Problems in NP . . . . . . . . . . . . . . . . . . . . . . . . . . . 18.5 The Biggest Open Problem in (Theoretical) Computer Science . . . . . . . . . . . . . . . . . . . . . . . . . References . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n\n. . . . . . .\n\n. . . . . . .\n\n. . . . . . .\n\n. . . . . . .\n\n. . . . . . .\n\n. . . . . . .\n\n. . . . . . .\n\n227 228 229 231 233 234 235\n\n. . . . . . . 237 . . . . . . . 239\n\n19 How Hard Is a Problem? . . . . . . . . . 19.1 Reminder: Effective Reductions . 19.2 Polynomial Time Reduction . . . 19.3 Hard Problems . . . . . . . . . . . . . References . . . . . . . . . . . . . . . . . . . .\n\n. . . . .\n\n. . . . .\n\n. . . . .\n\n. . . . .\n\n. . . . .\n\n. . . . .\n\n. . . . .\n\n. . . . .\n\n. . . . .\n\n. . . . .\n\n. . . . .\n\n. . . . .\n\n. . . . .\n\n. . . . .\n\n. . . . .\n\n. . . . .\n\n. . . . .\n\n. . . . .\n\n. . . . .\n\n. . . . .\n\n. . . . .\n\n. . . . .\n\n. . . . .\n\n. . . . .\n\n. . . . .\n\n. . . . .\n\n. . . . .\n\n. . . . .\n\n. . . . .\n\n. . . . .\n\n241 242 242 245 249\n\n20 Complete Problems . . . . . . . . . . . . . . . . 20.1 A First NP-complete Problem . . . . . 20.2 More NP-complete Problems . . . . . . 20.3 Puzzles and Games. . . . . . . . . . . . . 20.3.1 Chess . . . . . . . . . . . . . . . . 20.3.2 Sudoku . . . . . . . . . . . . . . . 20.3.3 Tile-Matching Games . . . . . 20.4 Database Queries . . . . . . . . . . . . . . 20.5 Policy Based Routing . . . . . . . . . . . 20.6 “Limbo” Problems . . . . . . . . . . . . . 20.7 Complete Problems in Other Classes 20.7.1 P-complete . . . . . . . . . . . . 20.7.2 RE-complete . . . . . . . . . . . References . . . . . . . . . . . . . . . . . . . . . . .\n\n. . . . . . . . . . . . . .\n\n. . . . . . . . . . . . . .\n\n. . . . . . . . . . . . . .\n\n. . . . . . . . . . . . . .\n\n. . . . . . . . . . . . . .\n\n. . . . . . . . . . . . . .\n\n. . . . . . . . . . . . . .\n\n. . . . . . . . . . . . . .\n\n. . . . . . . . . . . . . .\n\n. . . . . . . . . . . . . .\n\n. . . . . . . . . . . . . .\n\n. . . . . . . . . . . . . .\n\n. . . . . . . . . . . . . .\n\n. . . . . . . . . . . . . .\n\n. . . . . . . . . . . . . .\n\n. . . . . . . . . . . . . .\n\n. . . . . . . . . . . . . .\n\n. . . . . . . . . . . . . .\n\n. . . . . . . . . . . . . .\n\n251 252 255 256 258 259 260 261 264 266 268 268 269 273\n\n. . . . . . .\n\n. . . . . . .\n\n. . . . . . .\n\n. . . . . . .\n\n. . . . . . .\n\n. . . . . . .\n\n. . . . . . .\n\n. . . . . . .\n\n. . . . . . .\n\n. . . . . . .\n\n. . . . . . .\n\n. . . . . . .\n\n. . . . . . .\n\n. . . . . . .\n\n. . . . . . .\n\n. . . . . . .\n\n. . . . . . .\n\n. . . . . . .\n\n. . . . . . .\n\n275 276 276 281 282 282 284\n\nto Solve NP-Complete Problems Exact Algorithms . . . . . . . . . . . Approximation Algorithms . . . . Parallelism . . . . . . . . . . . . . . . Randomization. . . . . . . . . . . . . 21.4.1 The Class RP . . . . . . . 21.4.2 Probabilistic Algorithms\n\n. . . . . . .\n\n. . . . .\n\n. . . . .\n\n. . . . .\n\n21 How 21.1 21.2 21.3 21.4\n\n. . . . .\n\n. . . . .\n\n. . . . . . .\n\n. . . . . . .\n\nxviii\n\nContents\n\n21.5 Solving the Travelling Salesman Problem 21.5.1 Exact Solutions . . . . . . . . . . . . 21.5.2 Approximative Solutions . . . . . . 21.6 When Bad Complexity is Good News. . . References . . . . . . . . . . . . . . . . . . . . . . . . . .\n\n. . . . .\n\n. . . . .\n\n. . . . .\n\n. . . . .\n\n. . . . .\n\n. . . . .\n\n. . . . .\n\n. . . . .\n\n. . . . .\n\n. . . . .\n\n. . . . .\n\n. . . . .\n\n. . . . .\n\n. . . . .\n\n285 285 286 289 295\n\n22 Molecular Computing. . . . . . . . . . . . . . . . . . . . . . 22.1 The Beginnings of DNA Computing. . . . . . . . 22.2 DNA Computing Potential. . . . . . . . . . . . . . . 22.3 DNA Computing Challenges . . . . . . . . . . . . . 22.4 Abstract Models of Molecular Computation. . . 22.4.1 Chemical Reaction Networks (CRN) . . 22.4.2 CRNs as Effective Procedures. . . . . . . 22.4.3 Are CRNs Equivalent to Other Notions of Computation? . . . . . . . . . . . . . . . 22.4.4 Time Complexity for CRNs . . . . . . . . 22.4.5 Implementing CRNs . . . . . . . . . . . . . References . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n\n. . . . . . .\n\n. . . . . . .\n\n. . . . . . .\n\n. . . . . . .\n\n. . . . . . .\n\n. . . . . . .\n\n. . . . . . .\n\n. . . . . . .\n\n. . . . . . .\n\n. . . . . . .\n\n. . . . . . .\n\n. . . . . . .\n\n299 300 301 302 302 303 305\n\n. . . .\n\n. . . .\n\n. . . .\n\n. . . .\n\n. . . .\n\n. . . .\n\n. . . .\n\n. . . .\n\n. . . .\n\n. . . .\n\n. . . .\n\n. . . .\n\n308 309 309 315\n\n23 Quantum Computing . . . . . . . . . . . . . . . . . . 23.1 Molecular Electronics . . . . . . . . . . . . . . 23.2 The Mathematics of Quantum Mechanics 23.3 Quantum Computability and Complexity. 23.4 Quantum Algorithms . . . . . . . . . . . . . . 23.4.1 Shor’s Algorithm . . . . . . . . . . . 23.4.2 Grover’s Algorithm . . . . . . . . . 23.5 Building Quantum Computers . . . . . . . . 23.6 Quantum Computing Challenges . . . . . . 23.7 To Boldly Go … . . . . . . . . . . . . . . . . . References . . . . . . . . . . . . . . . . . . . . . . . . . .\n\n. . . . . . . . . . .\n\n. . . . . . . . . . .\n\n. . . . . . . . . . .\n\n. . . . . . . . . . .\n\n. . . . . . . . . . .\n\n. . . . . . . . . . .\n\n. . . . . . . . . . .\n\n. . . . . . . . . . .\n\n. . . . . . . . . . .\n\n. . . . . . . . . . .\n\n. . . . . . . . . . .\n\n. . . . . . . . . . .\n\n317 318 319 321 323 323 324 325 325 326 328\n\n. . . . . . . . . . .\n\n. . . . .\n\n. . . . . . . . . . .\n\n. . . . .\n\n. . . . . . . . . . .\n\n. . . . . . . . . . .\n\nFurther Reading—Computability and Complexity Textbooks . . . . . . . . 331 Glossary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 335 Index . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 341\n\nChapter 1\n\nLimits? What Limits?\n\nWhat kinds of limits are we studying and why?\n\nAs final year undergraduate (or postgraduate) students will have learned how a modern digital computing device works. They will know about these gadgets’ architecture, their operating systems, their input/output devices, their networking capabilities. They will appreciate the importance of compilers that can be used to translate programs in high-level languages into a form that the underlying hardware of the computer in use can interpret as instructions. This code the students will have learned to write in modern high-level programming languages like Java, Haskell or Scala. They also practise, sometimes quite painfully, how to debug their programs to find and eliminate bugs so that in the end the system designed and implemented does the job required. All these skills are very important (also for future employability) and building systems is, of course, the creative “fun part” of being a computer scientist. Yet, as the term “scientist” suggests, there is more to being a computer scientist than being able to write nifty programs and debug them (which is the “engineering” aspect of computing). Rocket scientists who design space ships need to know the laws of physics and the limits described by those laws. For instance, they need to know that nothing can go faster than the speed of light. Similarly, a computer scientist needs to know the limits of computation. These limits, however, should not just be the ones obtained by resource constraints of a given computing device. Since there are so many different devices, we are striving for limits that are more abstract, independent of whether one has a computer of brand A or B. Therefore, this chapter right at the beginning will stake the book’s territory. We have to answer some important questions: What kind of limits of computation will we discuss and what kind do we leave out? In Sect. 1.1 we briefly discuss\n\n© Springer International Publishing Switzerland 2016 B. Reus, Limits of Computation, Undergraduate Topics in Computer Science, DOI 10.1007/978-3-319-27889-6_1\n\n1\n\n2\n\n1 Limits? What Limits?\n\nsome physical limitations, discussing several types of physical limits in relation to computing that are well known. Then we give an overview of the content of the book (Sect. 1.2).\n\n1.1 Physical Limits of Computation When asked about limitations in the context of computing one most likely will think about the limits of the devices we use: the limited memory and limited clock speed. Or one might think about the limits memory or clock speed can theoretically reach in the production of computing devices. As this book is intended for Computer Science students with a focus on software and not hardware, we do not really address physical limits.1 In this section, however, we give a short overview discussing several types of physical limits in relation to computing that are well known (and usually one needs to understand quite some physics to get deeper into the subject).\n\n1.1.1 Fundamental Engineering Constraints to Semiconductor Manufacturing and Scaling • Limits on manufacturing There are limits to layering material on silicon on an extremely small scale using “precision optics and photochemical processes”. Lithography is limited but new technologies emerge and there is “additional progress in multiple patterning and directed self-assembly” which “promises to support photolithography beyond the 10 nm technology node” [5]. • Limits on interconnects “Metallic wires can be either fast or dense, but not both at the same time—smaller cross-section increases electrical resistance, while greater height or width increase parasitic capacitance with neighboring wires” [5]. The solution is to build so called ‘interconnect-stacks’ with varying thickness of wires according to purpose. New technology uses alternatively optical interconnects but this also is restricted by Maxwell’s equation on propagation speed of electromagnetic waves. • Limits on conventional transistors “Transistors are limited by their tiniest feature—the width of the gate dielectric,2 — which recently reached the size of several atoms” [5].\n\n1 Although\n\nwe briefly discuss some issues related to miniaturisation in Chap. 23. is the insulator (usually silicon dioxide) between gate and substrate of a metal-oxide semiconductor field-effect transistor (MOSFET). When the gate of the transistor is positive the dieletric is responsible for inducing a conducting channel between the source and drain of the transistor due to the so-called field effect.\n\n2 This\n\n1.1 Physical Limits of Computation\n\n3\n\n• Limits on design effort With the rising complexity of integrated circuits (ICs) in order to achieve higher speed at smaller scale and lower power usage, computer-aided design (CAD) is unavoidable and each new design required new CAD software. Clever algorithms for optimisations are required and hardware as well as software for the ICs need to be co-developed and verified. The limitations of the CAD software has therefore a direct limiting effect on the produced ICs.\n\n1.1.2 Fundamental Limits to Energy Efficiency • Limits on energy consumption, power supply and cooling In modern times of cloud computing and computing as a service, data centres play an important role. In the US they “consumed 2.2 % of total U.S. electricity in 2011” [5]. One can improve transmission and power conversion in datacenters, but on-chip power management soon reaches some limits. “Modern IC power management includes clock and power gating, per-core voltage scaling, charge recovery and, in recent processors, a CPU core dedicated to power scheduling. IC power consumption depends quadratically on supply voltage, which has decreased steadily for many years, but recently stabilized at 0.5–2 V…. Cooling technologies can improve too, but fundamental quantum limits bound the efficiency of heat removal” [5].\n\n1.1.3 Fundamental Physical Constraints on Computing in General Generally speaking, energy limits speed and entropy limits memory. In [4] a hypothetical “computer with a mass of 1 kg and a volume of 1 l, operating at the fundamental limits of speed and memory capacity fixed by physics” has been analysed. It is called the ‘ultimate’ laptop and serves to expose the limits of physical computation. Whereas current machines perform roughly (in terms of order of magnitude) 1010 operations 1 per second on 1010 bits,3 “the ultimate laptop performs 2mc2 π = 5.4258 × 1050 31 logical operations per second on approx 10 bits” [4] where m is the mass of the computer, c light speed, and is the Planck constant. However, “the ultimate laptop’s memory looks like a thermonuclear explosion or a little piece of the Big Bang!” [4]. Moreover, one achieves only an “overall processing rate of 1040 operations per second in ordinary matter” [4]. Those computed limits are based on the speed of light, the quantum scale, and the gravitational constant (see also [1]).\n\n3 This\n\nwas at the time of [4] in 2000 and has increased somewhat by now.\n\n4\n\n1 Limits? What Limits?\n\n1.2 The Limits Addressed The above mentioned physical limits are not only interesting but also most significant for the semiconductor and computing industry. However, we are interested in limitations inherent to computing independent of the hardware used to compute. In the first part, computability, we allow unlimited memory and time and investigate whether nonetheless there remain any limits to what one can compute. In the second part, complexity, we restrict the time allowed for computation, abstracting again from concrete details. We will be looking at the asymptotic behaviour of programs: how much does the computation time increase when we increase the size of the input. In this book we focus on time complexity only due to limitations of a 12 week semester module. It is expected that the diligent reader, once familiar with the concepts of computational time complexity, is able to read independently the literature on space complexity which addresses the size of memory needed for computation. The short last part of the book is dedicated to new emerging paradigms of computing: molecular computing and quantum computing and the question whether they can extend those limits. It might come as a surprise that the computational limits we will discuss, despite the idealisation, appear to be deeply connected with limits in our physical reality.\n\n1.2.1 Computability Overview Finding the limits of computation requires a clear-cut definition of what computing means. The origins of what is called “computability theory” go back to Alan M. Turing,4 a British mathematician, who achieved much more than cracking the German code used by the Enigma machines in WWII, but also wrote a world famous paper that, for the very first time, gave a definition of what it means “to compute”. He defined a computation device on paper that is now called Turing machine in his memory. At the time the term computer still referred to a human being carrying out a computation. Turing showed that, based on this definition, there is a function5 that cannot be computed in the proposed way. And this was in 1936 before the invention of the general purpose digital computer. We will look at this seminal paper [7] a bit later. Also, when searching for the limits of computation, we will simplify the kind of problems we consider to be computed. This is fine since if we have found limitations for a restricted class of problems we of course have also found limitations of a larger class of problems. After we have defined an adequate class of problems (Chap. 2) we will idealise our computation devices and investigate what cannot be computed using unbounded resources. 4 To\n\ncommemorate Turing’s 100th birthday, 2012 was the Alan Turing Year, see http://www. mathcomp.leeds.ac.uk/turing2012/. 5 At least one such function.\n\n1.2 The Limits Addressed\n\n5\n\nSo the first big quest is to find a problem that cannot be computed even with unlimited time (and memory). To be able to do this ourselves (in Chap. 8), we need to work with a particular model of computation. Of course we would like to work with a language that is similar to our modern high-level languages and not, for instance, with fiddly (Turing) machine programs. But on the other hand, we also want to be able to program an interpreter for the chosen language. Thus, the language should not be too complicated. We follow Neil Jones’s inspired choice from his textbook [3] and (in Chap. 3) pick WHILE, a simple “while language” with variables, assignment, while-loops and with binary trees as data type. We fix programs to have exactly one input and one output value. Again, this is a simplification but good enough to successfully go on our quest of non-computable problems. In Chap. 4 we ensure that WHILE programs are actually effectively computable and in Chap. 5 we present some additional language features, which facilitate programming further. Then we will learn how to use programs as objects (Chap. 6) such that we can write a WHILE interpreter in WHILE (Chap. 7). This will help us to further distinguish certain kinds of non-computable problems. We will then also meet several other (famous) undecidable problems in Chap. 9. We will learn that, roughly speaking, any interesting statement referring solely to the semantics of programs is undecidable. Historically, Hilbert’s Entscheidungsproblem6 is of interest. Hilbert and many others believed that there is a decision procedure for first-order logic sentences with axioms, for instance, arithmetic (number theory). Such a procedure could, given any arithmetic sentence, compute whether this given sentence is true or not. In 1936 this was disproved by Gödel, Turing and Church [2]. In Chap. 10 it is discussed how programs that are recursively defined, i.e. selfreferencing programs, can be interpreted despite the fact that WHILE does not possess any recursive features. The corresponding programming principle is called reflection. Next, we will have to ensure that our definition of computation is robust (Chap. 11). That means that our result does not depend on the choice of computing device or notion of computation. Otherwise, somebody might say that the problem we have found might have been computable with a different (e.g. more “powerful”) computing device. We will show that what is computable with unlimited resources does not depend on the computing device. The device must be of course reasonably powerful. There is a generalised thesis stating this called the Church-Turing-Thesis, and we will give evidence for it. The evidence will be the fact that all commonly used models of computation are of equivalent power. We can show this by writing compilers that translate programs or machines from one kind into another. This translation has to preserve the semantics of the original program or machine of course.\n\n6 German\n\nfor “decision problem”.\n\n6\n\n1 Limits? What Limits?\n\n1.2.2 Complexity Overview In the complexity part we investigate what can be computed with limited time resources, defining time complexity classes (those should already be known from an “Algorithms and Data Structures” module). Complexity classes for programs (or algorithms) are classes of programs that can be computed with a certain time bound. That means, we consider how much time is needed to compute a program in relation to the size of the input data (Chap. 12). It is clear that bigger input computations will usually take more time. We can then also define complexity classes for problems (Chap. 13). An interesting question is at what point the computations become intractable, i.e. too long to be useful in practice. By the way, we usually consider worst case time complexity when we consider complexity classes. The complexity class is a program (or problem) which depends on the worst input for the program (or decision procedure for the problem). “The worst input” denotes the input for which the run time is the longest. In Chap. 14 we address again the issue of robustness but now taking into consideration time consumption. In other words we show that with respect to polynomial time computation it does not matter which notion of computability we use. We will extend the Church-Turing thesis accordingly (and discuss variations such as Cook’s Thesis). To show that a problem is in a certain complexity class, it suffices to produce an algorithm that solves it and to show that the algorithm’s time bound (its runtime complexity) is within the class. It is much less straightforward though to show that a problem is not in a complexity class since one needs to prove that no algorithm solving the problem has the required complexity. A technique to show exactly that and in fact a hierarchy that shows that with more time one can solve more problems is presented in Chap. 15. We will look at some of those problems that are very much “real life” combinatorial optimisation problems, which are important in practical applications in industry and transport. The problems presented in Chap. 16 can all be solved in polynomial time but the ones in Chap. 17 are “hard” in the sense that the only algorithms known to solve them exactly have exponential complexity. This is not good enough since for medium-sized input the runtime complexity becomes astronomically large (this is down to what is called the “combinatorial explosion”). For instance, consider the problem of finding the shortest tour passing through a list of given cities. The number of possible tours to search through grows exponentially with the number of cities. There is no problem if the number of cities is small. For three cities there exists only one7 tour and for four cities there are only three. For 16 cities, however, there exist already several hundred billion possible tours and for 64 cities there exist orders of magnitude more tours than there are atoms in the known\n\n7 We\n\ndisregard whether travelling clockwise or counterclockwise.\n\n1.2 The Limits Addressed\n\n7\n\nuniverse.8 It is impossible to straightforwardly check which tour is the shortest. However, we don’t know yet whether there are some very clever algorithms that have acceptable runtime that just nobody has discovered yet.9 In order to define yet another complexity class based on verifying solutions rather than finding them. It contains all those optimisation problems encountered earlier, for which we don’t know yet whether they have polynomial solutions. Whether this class is equal to the class of problems decidable in polynomial time is the most famous open problem in theoretical computer science (Chap. 18). Of this class we can prove some more interesting properties in Chap. 19. It is possible to define the hardest problems in this class in a precise way by means of reduction. The resulting class does contain all the “hard” problems that we encounter in applications and, very remarkably, it is the case that either all problems in this class are tractable (i.e. have polynomial complexity) or none. Such problems can then be rightfully called intractable or infeasible. In Chap. 20 we then present even more problems that are as “hard” as the optimisation problems encountered in Chap. 17, among them are some famous games and puzzles as well as problems regarding database queries or computer networks. Regarding space complexity (which we will not cover in this book) the question is still open whether with polynomial space we can solve actually more problems than with polynomial time. Note that the complexity class, which was mentioned earlier and contains all the hard problems, lies in between these two classes. Again nobody knows if any of these classes (and if so which ones) are actually equal. After all this we seem to be left with a big dilemma: if there are no known feasible solutions to so many practical (combinatorial) problems what do we do? This is the topic of Chap. 21. One could be led to think that it helps to look for approximative solutions rather than optimal ones. Instead of looking for the optimal solution, one could just be as happy with computing one that is at most a certain percentage worse that the optimal solution. Alas, it turns out that computing these approximative solutions is as hard as computing the real thing most of the time.10 In some cases, however, changing the problem slightly might give rise to approximative solutions that can be computed in polynomial time. Another fix for this dilemma could be the usage of many parallel computers or processors. This will certainly speed up matters. But does it make otherwise intractable problems tractable? Also this is an open problem and subject of ongoing research. A big problem here is that one would need super-polynomially many processors to get a real polynomial speed-up, and communication between those is likely to take super-polynomial time. 8 “It\n\nis estimated that the there are between 1078 to 1082 atoms in the known, observable universe. In layman’s terms, that works out to between ten quadrillion vigintillion and one-hundred thousand quadrillion vigintillion atoms.” [8] 9 There are, of course, clever algorithms that work reasonably well for reasonably large input and we will get back to this in Chap. 21. 10 This is still an active research area.\n\n8\n\n1 Limits? What Limits?\n\nAnother question we will briefly address is this. Do probabilistic methods help? There are two versions of probabilistic algorithms: one is always correct but only probably fast. The other is always fast but only probably correct. Of course one needs to obtain a high probability to be able to use the latter kind of algorithms. In the 1970s very good polynomial time algorithms, e.g. for primality testing, were developed and the research area of probabilistic algorithms was established. However, recently it has been discovered that primality testing is polynomial even deterministically (the famous AKS algorithm after the initials of the inventors which will be discussed in Sect. 16.4). Anyway, it is still unknown whether probabilistic algorithms resolve our dilemma and give us good complexity algorithms for our hard problems. Another hope to achieve feasibility of hard problems is to consider new models of computation. One of those is the so-called molecular computing (also called DNA computing) discussed in Chap. 22. Molecular computing has been pioneered in the 1990s by Len Adleman11 . However, like quantum computing, this is still technically difficult (one needs the right experiment set up in a lab). The massive parallelism in those molecular computations stems from the massive parallelism in interaction in the “molecular soup”. Other fascinating aspects of DNA computing are that it can be carried out in vivo (inside living cells), that DNA information can be highly packed, and that it consumes very little energy. The latter two facts have been used already to successfully store information in DNA. It is currently unknown whether molecular computing could be used to resolve intractability. This is unlikely, as one needs too large a volume of DNA to obtain such dramatic speed-ups. However, molecular computing is a hot research area, due to the possible medical applications it could give rise to. Quantum computers are able to do massively parallel computations via so-called superposition of states in the world of quantum physics. The problem here is that it is extremely difficult to build quantum computers, even quantum storage, as on the quantum scale the environment can easily interfere with the quantum effects, from the casing to the wires. Moreover, it is difficult to harness the parallelism of quantum computers as any observation of the result state produces just one result and the magic of the superposition is subsequently lost. No more results can be observed. The area of quantum computing is an active and exciting research area. On the theoretical front, some breakthroughs have been already achieved. Shor’s12 algorithm [6] is a quantum algorithm for prime factoring. This algorithm can be executed on a quantum computer in polynomial time. But it should be pointed out that we still don’t know for sure whether factorisation is computable in polynomial time, i.e. feasibly, at least on “standard” hardware anyway. This will be covered in more detail in Chap. 23.\n\n11 Leonard Max Adleman (born December 31, 1945) is an American theoretical computer scientist,\n\nbest known for being a co-inventor of the RSA cryptosystem and molecular computing. He received the Turing award in 1977. 12 Peter Williston Shor (born August 14, 1959) is an American professor of applied mathematics at MIT. He won the Gödel Prize for his work on quantum computing.\n\n1.2 The Limits Addressed\n\n9\n\nThe problem whether the class of problems that can be solved in polynomial time is equal to the class of problem whose solutions of which can be verified in polynomial time appears to be strongly connected with the world of physics. In any case, whether it’s quantum computing or molecular computing, none of them would allow us to solve more problems than we already can with conventional computers. This fact, also known as the Church-Turing-thesis, is another major topic (see Chap. 11). What Next? We have determined what limits we will investigate and we have completed an overview of the topics of this book. Therefore we are prepared and ready to start with the first part about computability. The obvious first question here is: what does computable actually mean? We give an answer in the next chapter.\n\nReferences 1. Bremermann, D.: Optimization through evolution and recombination. In: Yovits, M.C., Jacobi, G.T., Goldstein, G.D. (eds.) Self-Organizing Systems, pp. 93–106. Spartan Books, Washington D.C. (1962) 2. Church, A.: A note on the Entscheidungsproblem. J. Symb. Log. 1(1), 40–41 (1936) 3. Jones, N.D.: Computability and complexity: From a Programming Perspective. MIT Press, Cambridge (1997). Available online at http://www.diku.dk/~neil/Comp2book.html 4. Lloyd, S.: Ultimate physical limits to computation. Nature 406(6799), 1047–1054 (2000) 5. Markov, I.L.: Limits on fundamental limits to computation. Nature 512(7513), 147–154 (2014) 6. Shor, P.W.: Polynomial-time algorithms for prime factorization and discrete logarithms on a quantum computer. SIAM J. Comput. 26(5), 1484–1509 (1997) 7. Turing, A.M.: On computable numbers, with an application to the Entscheidungsproblem. J. Math. 58, 345–363 (1936) 8. Villanueva, J.C.: How Many Atoms Are There in the Universe? Universe Today, 30 July 2009. Available via DIALOG http://www.universetoday.com/36302/atoms-in-the-universe/. Accessed 30 June 2015\n\nPart I\n\nComputability\n\nChapter 2\n\nProblems and Effective Procedures\n\nWhat does computable mean? What problems do we consider?\n\nTo a computer science student the question “what does computable mean?” might appear frivolous. We use computers every day, so don’t they—obviously—“compute things” for us, although one might say, very often they mainly retrieve and display information, for instance when we are browsing web pages or watching videos. And “computable” means “being able to be computed”, so what is the point? The ACM1 Computing Curricula [11, Sect. 2.1] states “computing to mean any goal-oriented activity requiring, benefiting from, or creating computers. Thus, computing includes . . . processing, structuring, and managing various kinds of information; finding and gathering information relevant to any particular purpose, and so on. The list is virtually endless, and the possibilities are vast.” This is uncontroversial but quite generic and does not really define what “computable” means. It should be clear that we need to pin down what computable means precisely and formally if we want to explore the limits of computation in a scientific manner. The same issue arises with the definition of problems. Everybody has their own understanding of what a problem is: from not being able to pay the rent to finding the shortest path in a graph. Also in that respect we will have to restrict the definition in order to be able to apply formal reasoning so that we can prove results. It is also important to understand the difference between a problem and a program. Computable problems will be the ones for which there are programs that “solve” them. All these concepts will be carefully defined below. We begin with a very short historical perspective (Sect. 2.1) introducing the notion of “effective procedure”. Sets and structures on sets, i.e. relations and functions,\n\n1 The Association for Computing Machinery (ACM), “the world’s largest educational and scientific\n\ncomputing society, delivers resources that advance computing as a science and a profession” [1]. It was founded in 1947 and has its headquarters in New York. © Springer International Publishing Switzerland 2016 B. Reus, Limits of Computation, Undergraduate Topics in Computer Science, DOI 10.1007/978-3-319-27889-6_2\n\n13\n\n14\n\n2 Problems and Effective Procedures\n\ntogether with their basic operations, are defined in Sect. 2.2 and some basic reasoning principles recalled. Finally, we define precisely what we mean by a “problem” in Sect. 2.3.\n\n2.1 On Computability In order to define the term “computable” we need to have a look at what is to be computed and how “computed” is actually defined. What is to be computed is generically called a problem. As computing in the 21st century is ubiquitous and microprocessors are not only in computers, but also in games consoles, mobile phones, music players and all kinds of consumer products, even washing machines, we need to restrict the problem domain since the “problem” in the context of washing will be very different from the “problem” in the context of game playing, or other areas.2 But first we address the “computable” question as the kind of problems we will look at depends on this definition.\n\n2.1.1 Historical Remarks Kleene wrote that the origin of algorithms3 goes back at least to Euclid4 ca. 330 B.C. according to [10] which provides an excellent historic overview. There have been many machines designed for calculation, from Gottfried Leibniz5 to Charles Babbage6 who wanted to automate calculations done in analysis. The Entscheidungsproblem, the decision problem for first order logic, was raised in the 1920s by David Hilbert7 and was described in [6]. The problem is to give a decision procedure “that allows one to decide the validity (respectively satisfiability) of a given logical expression by a finite number of operations” [6, pp. 72–73]. For Hilbert this was a fundamental problem of mathematical logic and played an important part 2 And\n\nevery reader may have their own problems, i.e. their own idea of what a “problem” is. name “algorithm” dates back to the name of the ninth century Persian mathematician Al-Khwarizmi. 4 Euclid (ca. 300 BC) was a Greek mathematician often called the “Father of Geometry” who also worked in number theory. He is the inventor of the common divisor algorithm. 5 Gottfried Wilhelm von Leibniz (July 1, 1646–November 14, 1716) was a German mathematician and philosopher, credited with the independent invention of differential and integral calculus. He also invented calculating machines. 6 Charles Babbage, (December 26, 1791–October 18, 1871) was an English mathematician, philosopher, inventor and mechanical engineer (and a Fellow of the Royal Society). He is also known for originating the concept of a programmable calculating machine. The London Science Museum has constructed two of his machines where they are on display. 7 David Hilbert (January 23, 1862–February 14, 1943) was a world-renowned German mathematician. 3 The\n\n2.1 On Computability\n\n15\n\nin his program of finding a finite axiomatisation of mathematics that is consistent, complete and decidable (in an automatic way). Gödel8 then proved in 1931 that no axiomatic system of arithmetic can exist that is consistent and complete. This result proves a significant inherent limitation of mathematical logic and deductive systems. He gave the definition of general recursive functions on natural numbers based on previous work by Herbrand, Skolem, Hilbert, and Péter. At the age of only 22 and still a student, Alan Turing . . . . . . worked on the problem for the remainder of 1935 and submitted his solution to the incredulous Newman on April 15, 1936. Turing’s monumental paper 1936 was distinguished because: (1) Turing analyzed an idealized human computing agent (a computer) which brought together the intuitive conceptions of a function produced by a mechanical procedure which had been evolving for more than two millenia from Euclid to Leibniz to Babbage and Hilbert; (2) Turing specified a remarkably simple formal device (Turing machine) and proved the equivalence of (1) and (2); (3) Turing proved the unsolvability of Hilberts Entscheidungsproblem which established mathematicians had been studying intently for some time; (4) Turing proposed a universal Turing machine, . . . an idea which was later to have great impact on the development of high speed digital computers and considerable theoretical importance. [10, Sect. 3]\n\nTuring used the term a-machine for his theoretical computing device but we now call them Turing machines in his honour. Independently, Alonzo Church9 proposed Church’s Thesis “which asserts that the effectively calculable functions should be identified with the recursive functions” [10].10 Church had initially intended this to be the definition of “effectively computable”. Nowadays one uses the term “Church-Turing thesis” which amalgamates both theses, identifying all the intuitive notions of computation and all the various formal definitions. What is to be subsumed under the notion of “intuitively computable”, is obviously up to interpretation. There have been suggestions that there are computational models much more powerful than Turing machines, called hypercomputation, but this is currently hotly debated. We will discuss this in more detail in Chap. 11 dedicated to the Church-Turing thesis. Despite general recursive functions and Turing machines being the first formal definitions of computability, this book will not use the former and only briefly look at the latter. The reason is that the former needs some mathematical background and the latter is tedious to program. We follow the idea of Neil Jones [7] and use a high-level programming language which will be introduced in the next chapter. Thus, we need to justify that our language qualifies as “intuitive notion of computability”. We must therefore understand what is required for such an intuitive notion of computation. 8 Kurt Friedrich Gödel (April 28, 1906–January 14, 1978) was an Austrian (and American) logician,\n\nmathematician, and philosopher and is considered one of the most significant logicians in history. He proved many important results, relevant here is the Incompleteness Theorem. 9 Alonzo Church (June 14, 1903–August 11, 1995) was an important American mathematician and logician. 10 Church’s first version that the computable functions are those definable by λ-terms [3] was initially rejected.\n\n16\n\n2 Problems and Effective Procedures\n\n2.1.2 Effective Procedures Effective Procedures, or effective algorithms are the programs that we understand to perform computations. The naming goes back to Alan Turing: “A function is said to be effectively calculable if its values can be found by some purely mechanical process. . . . We may take this statement literally, understanding by a purely mechanical process one which could be carried out by a machine [12, p. 166]. Note that Turing uses the word “calculable” here. In the 1930s computations usually referred to mathematical calculations. The machines he suggested have been called Turing machines and we will look at them more carefully in Chap. 11. So what is an effective procedure? Copeland gives the following definition in [4]: ‘Effective’ and its synonym ‘mechanical’ . . . do not carry their everyday meaning. A method, or procedure, M, for achieving some desired result is called ‘effective’ or ‘mechanical’ just in case 1. M is set out in terms of a finite number of exact instructions (each instruction being expressed by means of a finite number of symbols); 2. M will, if carried out without error, always produce the desired result in a finite number of steps; 3. M can (in practice or in principle) be carried out by a human being unaided by any machinery save paper and pencil; 4. M demands no insight or ingenuity on the part of the human being carrying it out.\n\nThe instructions of an effective procedure must therefore be executable in a mechanical way. This means that instructions (or commands) in programs must not be “vague”. For instance “find a number that has property P” which cannot be carried out effectively. How do we find the number? We need instructions that produce a number effectively such that it has the desired property. Therefore we cannot use oracles or choice axioms in our effective procedures. Moreover, we must be able to carry out the procedures in a finite amount of time. Infinite computations are by definition not effective. However, all notions of computation allow the definition of infinite computations as well. It will become clear in Chap. 8 why it is difficult to separate finite from infinite computations. The exact meaning of “intuitive” computable is to a certain degree subject to interpretation. Some researchers insist that the mechanical computability by Turing machines does not include the so-called interactive computation, where humans (or other potentially non-computable oracles) interact with the program (see [5]). This appears to be equivalent to Turing’s o-machines, Turing machines with an “oracle tape”, an extra tape on which the Turing machine can write a word w and then ask the environment, the oracle, to answer whether w is in a certain set A which can be arbitrarily complicated (in particular it does not have to be decidable by an a-machine). The resulting definition of computability by o-machines is called relative computability. Relative computability will not be covered in this introductory book. Also we will not discuss computability of infinite objects (e.g. real number computation).\n\n2.1 On Computability\n\n17\n\nTuring’s machine model extends the concept of a finite state automaton with extra memory. This memory is organised as a tape on which symbols can be written and read sequentially by a head that moves along the tape and that is controlled by the finite state automaton. Programming Turing machines is therefore a tedious and error-prone undertaking. For this reason, we don’t want to use them to prove anything in this book, but rather use a programming language close to what we use on a daily basis. In Chap. 3, a more convenient notion of “effective procedure” will thus be presented. Turing machines will, for the sake of completeness and historical importance, be presented in detail in Sect. 11.3. The following definition will be useful to compare languages later (for instance in Chap. 11 and Sect. 10.2).\n\n2.2 Sets, Relations and Functions Before we continue and define problems and solutions more formally, we recall some basic definitions that allow us to make formal statements throughout this book. Readers well familiar with those concepts can skip this section. We will discuss sets and structures on sets, namely relations and functions. We introduce operations on sets, fix notation, and recall some basic reasoning principles, which will be used throughout the book. A proper introduction to sets and logic for computing can be found e.g. in [8].\n\n2.2.1 Sets Sets are collections of objects. The collections can be finite or infinite. We will usually only consider homogeneous sets which means that the objects in a set are all of the same type.11 For each element of this type one must be able to say whether the element is in the given set or not. An example of a set of natural numbers is the set S10 containing the numbers from 1 to 10. In this case, number 3 is in the set S10 but number 42 is not. It is important to observe that one does not care how many times the objects appears in the set as one would do in a list or an array. A set thus abstracts away from the number of occurrences. An object simply is either in or out. If we have such knowledge for all objects of the given underlying type we have uniquely defined a set. Let us now fix some notation: Definition 2.1 (Sets) A finite set containing n different objects e1 , e2 , . . . , en is written {e1 , e2 , . . . , en } 11 This\n\ntype may be a set again.\n\n18\n\n2 Problems and Effective Procedures\n\nWe call those objects contained in a set, the elements of this set. The empty set is the unique set that contains no elements at all and is usually denoted {} or ∅. Let A be a set of elements of type T . The elementhood operation is a statement x∈A stating that element x is in set A (“belongs to A”, “is contained in A”). If the set is infinite, we cannot write down all the elements. In this case we usually write the “law” that states which elements are in the set as follows (which can also be used to describe finite sets). If S is a type and P(x) denotes a condition on variable x then {x ∈ S | P(x)} describes the set of all elements of type S that have property P. The type of all natural numbers is denoted N (which contains 0), the integer numbers is denoted Z and the real numbers is denoted R. The type of Boolean values {true, false} is denoted B. Example 2.1 Here are some examples of finite sets with objects (elements) in N: 1. {1, 10, 100}: the set of natural numbers containing the three elements 1, 10 and 100. 2. ∅: the empty set containing no natural number. 3. {x ∈ N | x is even}: the infinite set of all even natural numbers, which is the set {0, 2, 4, 6, 8, 10, 12, . . .}. The notation with . . . followed by a closing } is sometimes used to indicate an infinite set when the condition P used to define it is clear from the context. Note that in this example the condition P(x) is “x is even”. 4. {x ∈ N | x = 10n , 0 ≤ n ≤ 2}: the finite set containing the first three powers of 10, namely 1 = 100 , 10 = 101 and 100 = 102 . So in fact this set is equal to the first. More about equality of sets follows these examples. Definition 2.2 (Set equality and subsets) Let S1 and S2 be two sets ranging over the same type T of objects. We say that two sets S1 and S2 are equal, short S1 = S2 , if, and only if, they contain exactly the same elements. This confirms that it is enough to know which elements are in the set and which are not to uniquely define a set. We say that a set S1 is a subset of a set S2 (or S1 is contained in S2 ) if, and only if, every element of S1 is also an element of S2 . More formally we can also write S1 ⊆ S2 ⇐⇒ ∀x ∈ T. x ∈ S1 ⇒ x ∈ S2 S1 = S2 ⇐⇒ ∀x ∈ T. x ∈ S1 ⇔ x ∈ S2 where ⇒ denotes implication and ⇔ denotes equivalence and ∀x ∈ T. P denotes universal quantification over all elements of type T .\n\n2.2 Sets, Relations and Functions\n\n19\n\nIn the above definition we used the phrase “if, and only if” (in the formal version ⇔) and not just “if” (formally ⇐) for a good reason. For a definition, it is important to cover all cases exactly. Consider the following statement: “Sets A and B (over natural numbers) are equal if S and T are both the empty set.” This is obviously a correct statement about equality of sets A and B. But it is far from a definition of equality. The statement does not specify anything about the equality of non-empty sets. Clearly, its contraposition “if A and B are equal sets then A and B are both empty” is wrong. Thus the statement “sets A and B (over natural numbers) are equal if, and only if, S and T are both the empty set.” is equally wrong. As explained above the use of phrase “if, and only if” is important and we will encounter it often throughout the book. Therefore, we sometimes abbreviate it and simply write “iff” instead of “if, and only if”. In order to show equality of two sets, an important reasoning principle is often used: Proposition 2.1 Let S1 and S2 be sets of objects in T , then S1 = S2 if, and only if, S1 ⊆ S2 and S2 ⊆ S1 . In other words, S1 equals S2 if, and only if, S1 is a subset of S2 and vice versa. Proof We need to show the two directions of the “if, and only if”. The “only if” (⇒) and the “if” (⇐) direction. “⇒”: If S1 = S2 then by definition S1 ⊆ S2 , as being equal is a special (degenerated) case of being a subset of. Analogously, S2 ⊆ S1 . “⇐”: Assume S1 ⊆ S2 and S2 ⊆ S1 . To show that both sets are equal we must show that they contain exactly the same elements, i.e. for all x ∈ T it must hold that x ∈ S1 iff x ∈ S2 . Unfolding the meaning of “iff” we get two conditions for all x ∈ T , namely x ∈ S1 ⇒ x ∈ S2 and x ∈ S2 ⇒ x ∈ S1 . We can move the quantifier ∀x around both conditions separately without changing the meaning of the formula, so it suffices to show: and ∀x ∈ T. x ∈ S1 ⇒ x ∈ S2 ∀x ∈ T. x ∈ S2 ⇒ x ∈ S1 and thus by Definition 2.2 that S1 ⊆ S2 and S2 ⊆ S1 which were our assumptions. Definition 2.3 (Set operations) We will use the following standard operations on sets: union (S1 ∪ S2 ) intersection (S1 ∩ S2 ) and set difference (S1 \\S2 ). They are defined as follows: x ∈ S1 ∪ S2 ⇐⇒ x ∈ S1 ∨ x ∈ S2 x ∈ S1 ∩ S2 ⇐⇒ x ∈ S1 ∧ x ∈ S2 x ∈ S1 \\S2 ⇐⇒ x ∈ S1 ∧ ¬(x ∈ S2 ) where ∨ denotes logical disjunction (“or”), ∧ denotes logical conjunction (“and”), and ¬ denotes logical negation (“not”). If x is not contained in A, we usually abbreviate ¬(x ∈ A) by simply writing x∈ / A.\n\n20\n\n2 Problems and Effective Procedures\n\nIf S is a set of elements of type T , we call T \\S the complement of S, which is sometimes also abbreviated S. Example 2.2 Here are some concrete examples of set operations and their results: {3, 5, 7} ∪ {2, 4, 6, 8} = {2, 3, 4, 5, 6, 7, 8} {3, 5, 7} ∩ {2, 4, 6, 8} = {} {3, 5, 7}\\{3, 5, 8, 16} = {7} N\\{x ∈ N | x is even} = {x ∈ N | x is odd} Definition 2.4 (Cartesian product) Let S1 and S2 be sets. Then S1 × S2 , the Cartesian product 12 of S1 and S2 , is the set of pairs (i.e. tuples) (s, t) where a ∈ S1 and b ∈ S2 . In other words: S1 × S2 = {(s, t) | s ∈ S1 ∧ t ∈ S2 } Based on the cartesian product one can also form sets of tuples of length k over a given set or type: Definition 2.5 (k-tuples) Let S be a set. Then S k , the k-tuples over S, is defined as follows: S k = S × S ×S . . . × S k times\n\nsuch that elements of S k are tuples of length k, i.e. (s1 , s2 , . . . , sk−1 , sk ) where si ∈ S for all 1 ≤ i ≤ k. If we consider sets from a programming perspective as a kind of datatype then we would like to nest the set data type constructor. In other words, we would like to have set of sets, and so on. Definition 2.6 (Powerset) Let S be a set. Then Set(S), the powerset of S, denotes the set of all subsets of S, including the empty set, and the set S. Set(S) = {set s | s ⊆ S} Example 2.3 Set({1, 10, 100}) = {∅, {1}, {10}, {100}, {1, 10}, {1, 100}, {10, 100}, {1, 10, 100}} Set(N) = {set s | ∀x ∈ s. x ∈ N} 12 The Cartesian product is named in honour of Reneé Descartes (31 March 1596–11 February 1650), a French mathematician and philosopher, who spent most of his life in the Netherlands, and is famous for his saying “I think therefore I am” as well as for the development of (Cartesian) analytical geometry. He was invited to the court of Queen Christina of Sweden in 1649. “In Sweden—where, Descartes said, in winter men’s thoughts freeze like the water—the 22-year-old Christina perversely made the 53-year-old Descartes rise before 5:00 am to give her philosophy lessons, even though she knew of his habit of lying in bed until 11 o’clock in the morning” [9]. Consequently, Descartes caught pneumonia and died.\n\n2.2 Sets, Relations and Functions\n\n21\n\nAnother standard set used in this book is the set of words over a finite alphabet Σ as used for instance by finite state automata. These words are just finite strings of letters of the alphabet, including the empty string. Definition 2.7 (Set of words) Let Σ be a finite alphabet of symbols (or letters). Then we define a new set Σ ∗ by providing the rules to generate elements of this set and state that these are the only rules to generate elements of the set. The rules are as follows: ε ∈ Σ∗ aw ∈ Σ ∗ if a ∈ Σ ∧ w ∈ Σ ∗ This is an inductive definition. The first rule states that the empty word ε is a word which provides the termination case of the induction. The second rule describes how to generate new elements from already generated ones. Example 2.4 For alphabet Σ = {0, 1} here are some examples of words in Σ ∗ : • • • •\n\nε (the empty word) 0 01 11111001\n\nFinally, for the definition of partial functions in Sect. 2.2.4 we need a way to extend a set by a unique new symbol. Definition 2.8 (One-point-extension) Let S be a set over type T . Then we define a new set S⊥ in Set(T ) by adding to S a new element ⊥ (called13 “undefined”) that is assumed to be different from all elements in S. S⊥ = {x ∈ T ∪ {⊥} | x ∈ S ∨ x = ⊥}\n\n2.2.2 Relations Relations are special sets. We have already seen some relations in the previous section, the equality and subset relation are both binary relations on sets. A binary relation R is simply a set of pairs and we say that two elements s and t are in this relation R if the pair (s, t) is in the set R. Definition 2.9 (Relations) Relations are sets. We define: • a unary relation R over elements of type T is a subset R ⊆ T . An object t ∈ T is said to be in relation R iff t ∈ R. • a binary relation R over elements of type S × T is a subset R ⊆ S × T . A pair (s, t) is said to be in relation R iff (s, t) ∈ R. 13 The\n\nsymbol itself is called a “perp”.\n\n22\n\n2 Problems and Effective Procedures\n\n• a ternary relation R over elements of type S × T × U is a subset R ⊆ S × T × U . A triple ( s, t, u ) is said to be in relation R iff (s, t, u) ∈ R.14 Example 2.5 The quality relation and subset relation over sets of elements of type T as defined in Definition 2.2 are actually binary relations in the following sense: _ = _ ⊆ Set(T ) × Set(T ) _ ⊆ _ ⊆ Set(T ) × Set(T ) We usually write S1 ⊆ S2 (so-called “infix notation”) instead of (S1 , S2 ) ∈ _ ⊆ _.\n\n2.2.3 Functions We intuitively understand what the addition or multiplication functions are, and maybe also the factorial function. Functions describe maps from objects of a certain type into objects of another15 type. In functional programming languages, functions are first-class citizens. The programmer can define those functions syntactically. For us, however, functions are descriptive16 and not programs or part thereof. We can describe functions as special relations which we will do next.\n\n2.2.4 Partial Functions Definition 2.10 (Partial Functions) Let A and B be sets of possibly different types of elements, e.g. A ∈ Set(S) and B ∈ Set(T ). A partial function f from A to B is a subset of A × B (i.e. f ⊆ A × B) satisfying the following uniqueness condition: For all a ∈ A there is at most one b ∈ B such that (a, b) ∈ f . To abbreviate that f is a partial function from A to B we briefly write f : A → B⊥ , where we call A the argument type of f and B the result type. The reason for actually writing B⊥ (defined in Definition 2.8) in this notation will become clear shortly when we define the following binary application relation _@_ ⊆ (A → B⊥ ) × A for a partial function f : A → B⊥ and an element a ∈ A: f@a =\n\nb if (a, b) ∈ f ⊥ otherwise\n\nfirst glance, it is not obvious whether S × T × U means S × (T × U ) or (S × T ) × U . We assume cartesian products to be associative, identifying these two definitions, thus dropping the extra parentheses and writing (s, t, u) for triples, and similarly for n-tuples where n > 3. 15 Which may possibly be the same type. 16 Mathematical objects. 14 At\n\n2.2 Sets, Relations and Functions\n\n23\n\nInstead of f @ a we will be writing f (a) which is the common notation for function application also widespread in programming languages. If (a, b) ∈ f we therefore simply write f (a) = b and say that “ f applied to a equals b.” We also call a the argument of the function (application) and b the result of the application. By the uniqueness condition we know that there can only be one result which always lies in B⊥ . It is possible that no such b ∈ B exists, in which case f (a) = ⊥ and we say that f is undefined for a and often use the short notation f (a)↑ to express this. We also sometimes use f (a)↓ to express that f (a) is defined when we are not interested in the concrete result value. Functions with more than one argument are simply described by using a cartesian product as argument type. In this case the function takes a tuple as input. Example 2.6 Consider the integer division operator on natural numbers, div. This partial function takes two integers n and m and returns mn in case m = 0. Thus div : N × N → N⊥ . ((n, m), r ) ∈ div iff ∃k < m. m × r + k = n where m, n, r, k ∈ N. Note that we have that div (n, 0)↑ as there is no natural number k that is smaller than 0.\n\n2.2.5 Total Functions Total functions are total in the sense that function application always returns a defined value. Therefore, total functions are just a very special case of partial functions. Definition 2.11 (Total Functions) Let A and B be sets of possibly different types of elements, e.g. A ∈ Set(S) and B ∈ Set(T ). A total function f from A to B is a subset of A × B (i.e. f ⊆ A × B) satisfying the following two conditions (where the first is the uniqueness condition for partial functions): 1. For all a ∈ A there is at most one b ∈ B such that (a, b) ∈ f . 2. For all a ∈ A there is at least one b ∈ B such that (a, b) ∈ f . To abbreviate that f is a total function from A to B we briefly write f : A → B where we again call A the argument type of f and B the result type. We can take the binary application relation defined in Definition 2.10 for partial functions and restrict its type to _@_ ⊆ (A → B) × A for a total function f : A → B and argument a ∈ A. Since for total functions we know from the second condition that there must always be a b ∈ B for every a ∈ A (which is unique by the first condition) so we can never have f @a = ⊥. As for partial function application, we write f (a) for f @a and if (a, b) ∈ f we simply write f (a) = b and say that “ f applied to a equals b.”\n\n24\n\n2 Problems and Effective Procedures\n\nExample 2.7 Consider the factorial function on natural numbers, fac. Often the notation n! is used instead of application fac(n). This total function fac : N → N takes an integer n and returns the factorial of n defined as follows: (n, r ) ∈ fac iff (n = 0 ∧ r = 1) ∨ (n > 0 ∧ (n − 1, s) ∈ fac ∧ r = n × s where n, r, s ∈ N. This is a recursive (actually inductive) definition of fac as we use the function (functional relation) fac on the left and right hand side of the definition. The definition is, however, well defined as the argument for the application of fac on the right hand side uses a “smaller” argument n − 1 than the one on the left hand side (which uses n). When defining functions in this book we will normally not define the relation that defines the function but write the (equivalent) definition of function application, i.e. we define the result of f (n) rather than defining the relation (n, r ) ∈ f . For the factorial function we would typically write: fac(n) =\n\n1 if n = 0 n × fac(n − 1) otherwise\n\n2.3 Problems Our first quest is to find a problem that is not computable (or decidable by a computer program) to learn and understand that not everything is computable even with unlimited resources. In order to do this, we obviously need to define what we mean exactly by “problem” and what we mean exactly by “computable.” Whereas the former is easy to do the latter is a bit more tricky. We will allow ourselves to restrict the definition of problem. Since we are interested in the Limits of Computation, we are interested in negative results, i.e. what cannot be achieved in computing. If there is a problem of a restricted kind that is not computable then we still have found a problem that is not computable, so this restriction does not take anything away from our ambition. A problem of the kind we are interested in is characterised by two features: 1. It is a uniform class of questions. Uniform refers to the domain of the problem, i.e. what data the problem is about. The type of domain must be precisely definable. 2. It can be given a definite and finite answer. The type of the answer must be also precisely definable. The type in question can be any set, like for instance N, N⊥ , Σ ∗ and so on. Definition 2.12 Let S and T some well defined (finite) types. A function problem is a uniform set of questions, the answers of which have a finite type. The solution of a function problem is given as a partial function f : S →⊥ T as described in Sect. 2.2.4. The uniform question of this problem is of the sort: “given an x ∈ S, what is a y ∈ T such that a certain condition on x and y holds?”\n\n2.3 Problems\n\n25\n\nA decision problem is a relation R ⊆ S. The uniform question of this problem is of the sort: “given an x ∈ S, does x belong to R, i.e. x ∈ R? The solution of a decision problem is given as a total function χ : S → B, also called the characteristic function of R. Example 2.8 Here are some examples of function and decision problems: 1. For a tree t, what is its height? Domain: trees (apparently with arbitrary number of children). Answer for any given tree t: a natural number describing the height of t (and we know what the meaning of “height of a tree” is). The answer type is the type of natural numbers. 2. For a list of integers l, what does l look like when sorted? In other words, what is the sorted permutation of l using the usual ordering on integers? Domain and answer type are here the type of integer lists. 3. For a natural number n, is it even? Domain: natural numbers. Answer for any given number n: a Boolean,17 stating whether n is even or not (we understand what even and odd mean). The answer type is the type of boolean values. 4. For a given formula in number theory (arithmetic) φ, is it valid? Domain: formulae in arithmetic. Answer for a given formula φ: a Boolean, stating whether the formula φ is true (and we understand what it means for a formula to be true). The first two examples above are function problems, the last two examples are decision problems. Example 2.9 Here are some examples of problems that do not qualify as problems for us. 1. “What is the meaning of life?”18 This is not a uniform family of questions. Moreover, we do not know what the answer type is. If we’d expect a string as answer then it would still not qualify as we don’t know whether there is a definite answer. 2. “Is the number 5 even”? This is not a uniform class of questions, as this question only refers to the number 5.\n\n17 Boolean values are named after George Boole (2 November 1815–8 December 1864), an English mathematician and logician famous for his work on differential equations and algebraic logic. He is most famous for what is called Boolean algebra. Throughout this book, we will use the term “boolean” to indicate a truth value for which the corresponding algebra operations are available. 18 This question is easily confused with the one famously asked in Douglas Adam’s masterpiece: “The Hitchhiker’s Guide to the Galaxy” [2] which actually is called: “Ultimate Question of Life, The Universe, and Everything”. The computer in question, Deep Thought, after a considerable 7.5 million years answered famously: “42”. Alas, nobody understood the question. So Deep Thought suggested to build an even more powerful super-computer to produce the question to the answer. This computer was later revealed to be planet “Earth” which was unfortunately destroyed 5 min before completion of the calculations.\n\n26\n\n2 Problems and Effective Procedures\n\n2.3.1 Computing Solutions to Problems According to the two types of problems introduced, we will consider two concrete kinds of “solving a problem”: computing a function and deciding membership in a set. The data type of Turing machines is the set of finite words over a finite alphabet. Recall that Σ ∗ denotes all finite words over the alphabet Σ, including the empty word. The comparison test for tape symbols is built into the construction set and from that it is possible to implement equality of words. In a general notion of effective procedure, the data type should be general enough to encode finite words and their equality test. The latter must be effective so it must be terminating. This means that equality of infinite objects is likely to be problematic and thus we do not cover computability over infinite objects in this book. Definition 2.13 Provided a certain choice of effective procedures P, a (function or decision) problem is called P-computable if, and only if, its solution can be computed (calculated) by carrying out a specific such effective procedure in P. A decision problem that is computable is also called P-decidable. If the kind of effective procedures is known by the context we also simply use the unqualified terms computable and decidable. It is important to remember that programs are solutions to computable problems. The programs that solve computable decision problems are also called decision procedures. Example 2.10 The solutions to the computable and decidable, resp., problems in Example 2.8 are given below as programs. 1. For a tree t what is its height? The solution is a function program that takes a tree as input and computes its height. 2. For a list of integers l what is l sorted? The solution is a program that takes a list of integers as input and returns a sorted copy of the list. The program can use various well known sorting algorithms, e.g. bubble-sort, merge-sort, or quicksort. They all perform the same task eventually, but use different methods to achieve this and also may take different time. This is an issue we will discuss in the complexity part. 3. For a natural number n, is it even? The solution in a program takes a natural number as the input and returns the boolean value true if the input is even and false if it is odd. We call such a program also a decision procedure for the property of “being even”. 4. For a given formula in number theory (arithmetic) φ, is it valid? As discussed in the introduction, this is undecidable, so there can’t be any program that takes as input as an arithmetic formula (suitably encoded) and returns true if the formula is satisfiable and false if it is not.\n\n2.3 Problems\n\n27\n\nWhat Next? Now that we know what we mean by “computable” and have seen that the historically first definition of computability via a machine involves tedious low level programming, we want to define a high-level language that can do the job as well. So in the next chapter we introduce the language WHILE and in the following chapter we show that WHILE-programs can be legitimately chosen for effective procedures. Exercises 1. What is the “Entscheidungsproblem”? What is the type of its domain? Is it a decision problem? 2. Why did Alan Turing allow his (pencil and paper) computing device to use only finitely many symbols (on the tapes) and let the “state of mind” of the computer only glance at finitely many symbols at any given time? 3. Which of the following pairs of sets A and B are equal? Show either A = B or A = B. a. b. c. d. e.\n\nA = N × N and B = N2 A = {1, 3, 5} and B = {1, 3, 5, 6} A = {1, 3, 3, 3} and B = {1, 3} A = {x ∈ N | x = x + 1} and B = ∅ A = {x ∈ N | even(x) ∧ x < 11} and B = {0, 2, 4, 6, 8, 10}\n\n4. Describe the relation that one natural number can be divided by the second natural number without remainder as Rdivisible ⊆ N × N. 5. Give an example of a partial function of type N → N⊥ and an example of a total function N → N, respectively. 6. What is the difference between a decision problem and a function problem? 7. Give an example of a problem that is neither a decision nor a function problem. Why is it acceptable that we consider only those specific kinds of problems? 8. Give two other examples of decision and function problems, respectively, that have not been mentioned in this chapter. 9. Assume that we have fixed the notion of effective procedures P. When do we call a function problem P-computable? 10. Assume that we have fixed the notion of effective procedures P. When do we call a decision problem P-decidable?\n\nReferences 1. ACM Home Page, available via DIALOG, http://www.acm.org. Cited on 30 August 2015 2. Adams, D.: The Hitchhiker’s Guide to the Galaxy. Pan Books (1979) 3. Church, A.: An unsolvable problem of elementary number theory. Am. J. Math. 58(2), 345–363 (1936) 4. Copeland, J.: The Church-Turing Thesis. References on Alan Turing (2000). Available via DIALOG. http://www.alanturing.net/turing_archive/pages/Reference%20articles/The%20TuringChurch%20Thesis.html. Cited 2 June 2015\n\n28\n\n2 Problems and Effective Procedures\n\n5. Goldin, D., Wegner, P.: The church-turing thesis: breaking the myth. In: Cooper, S.B., Löwe, B., Torenvliet, L. (eds.) New Computational Paradigms. Lecture Notes in Computer Science, vol. 3526, pp. 152–168. Springer, Heidelberg (2005) 6. Hilbert, D., Ackermann, W.: Grundzüge der theoretischen Logik. Springer, Berlin (1928). (Principles of Mathematical Logic.) 7. Jones, N.D.: Computability and Complexity: From a Programming Perspective. MIT Press, Cambridge (1997). (Also available online at http://www.diku.dk/neil/Comp2book.html.) 8. Makinson, D.: Sets, Logic and Maths for Computing, 2nd edn. Springer, UTiCS Series (2012) 9. Reneé Descartes. Entry in Encyclopædia Britannica, http://www.britannica.com/biography/ Rene-Descartes/Final-years-and-heritage. Available via DIALOG. Cited on 2 Sept 2015 10. Soare, R.I.: The history and concept of computability. In: Griffor, E.R. (ed.) Handbook of Computability Theory, pp. 3–36. North-Holland (1999) 11. The Joint Task Force for Computing Curricula 2005 (ACM, AIS, IEEE-CS): Computing Curricula 2005. Available via DIALOG, http://www.acm.org/education/curric_vols/CC2005March06Final.pdf (2005) 12. Turing, A.: Systems of logic based on ordinals. Proc. London Math. Soc. 45(1), 161–228 (1939)\n\nChapter 3\n\nThe WHILE-Language\n\nWhat language do we use to write our “effective procedures”?\n\nIn the previous chapter, we have observed that Turing machines are tedious to program and thus not an ideal language to study computability and complexity issues. Moreover, we are used to program in high-level languages like Java, C, Haskell, Python, and so on. We thus prefer to program in a high-level language in order to write effective procedures in the sense of Turing. The notion of computable or decidable will then be based on the programmability in this language. We follow Neil Jones’ idea [2, Preface, Page X] and use the language WHILE since “The WHILE language seems to have just the right mix of expressive power and simplicity.” In Chap. 7 it will become clear why this particular mix is important and desired for the purposes of investigating questions of computability. For now it just appears to be a sensible choice to be able to write programs (relatively) easily in the way we are used to and also have a language that is simple enough to understand the semantics of programs. This chapter is a short introduction to this language. In the next chapter it will be argued that it is actually a good definition of “effective procedure” meeting Turing’s criteria. Before we discuss the details, we pause to think about the data type the language with the “right mix” is supposed to provide. Since we want a simple language, it will not feature explicit types and a type system but use just one basic data type. But since we want an expressive language, the built-in datatype needs to be flexible enough to encode any data type the programmer would like to use, be it integers or natural numbers, be it lists or even abstract syntax tre"
    }
}
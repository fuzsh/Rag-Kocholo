{
    "id": "dbpedia_2123_3",
    "rank": 47,
    "data": {
        "url": "https://pytorch.org/tutorials/intermediate/ddp_tutorial.html",
        "read_more_link": "",
        "language": "en",
        "title": "Getting Started with Distributed Data Parallel — PyTorch Tutorials 2.4.0+cu121 documentation",
        "top_image": "",
        "meta_img": "",
        "images": [
            "https://pytorch.org/tutorials/_static/images/view-page-source-icon.svg",
            "https://pytorch.org/tutorials/_static/images/pytorch-colab.svg",
            "https://pytorch.org/tutorials/_static/images/pytorch-download.svg",
            "https://pytorch.org/tutorials/_static/images/pytorch-github.svg",
            "https://pytorch.org/tutorials/_images/pencil-16.png",
            "https://pytorch.org/tutorials/_static/images/chevron-right-orange.svg",
            "https://pytorch.org/tutorials/_static/images/chevron-right-orange.svg",
            "https://www.facebook.com/tr?id=243028289693773&ev=PageView  &noscript=1",
            "https://www.googleadservices.com/pagead/conversion/795629140/?label=txkmCPmdtosBENSssfsC&guid=ON&script=0",
            "https://pytorch.org/tutorials/_static/images/pytorch-x.svg"
        ],
        "movies": [],
        "keywords": [],
        "meta_keywords": [
            ""
        ],
        "tags": null,
        "authors": [],
        "publish_date": null,
        "summary": "",
        "meta_description": "",
        "meta_lang": "en",
        "meta_favicon": "",
        "meta_site_name": "",
        "canonical_link": null,
        "text": "Getting Started with Distributed Data Parallel¶\n\nAuthor: Shen Li\n\nEdited by: Joe Zhu\n\nPrerequisites:\n\nPyTorch Distributed Overview\n\nDistributedDataParallel API documents\n\nDistributedDataParallel notes\n\nDistributedDataParallel (DDP) implements data parallelism at the module level which can run across multiple machines. Applications using DDP should spawn multiple processes and create a single DDP instance per process. DDP uses collective communications in the torch.distributed package to synchronize gradients and buffers. More specifically, DDP registers an autograd hook for each parameter given by model.parameters() and the hook will fire when the corresponding gradient is computed in the backward pass. Then DDP uses that signal to trigger gradient synchronization across processes. Please refer to DDP design note for more details.\n\nThe recommended way to use DDP is to spawn one process for each model replica, where a model replica can span multiple devices. DDP processes can be placed on the same machine or across machines, but GPU devices cannot be shared across processes. This tutorial starts from a basic DDP use case and then demonstrates more advanced use cases including checkpointing models and combining DDP with model parallel."
    }
}
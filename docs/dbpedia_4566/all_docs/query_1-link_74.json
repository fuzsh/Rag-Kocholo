{
    "id": "dbpedia_4566_1",
    "rank": 74,
    "data": {
        "url": "https://arxiv.org/html/2302.03460v3",
        "read_more_link": "",
        "language": "en",
        "title": "Mind the Gap! Bridging Explainable Artificial Intelligence and Human Understanding with Luhmann’s Functional Theory of Communication",
        "top_image": "",
        "meta_img": "",
        "images": [],
        "movies": [],
        "keywords": [],
        "meta_keywords": [
            ""
        ],
        "tags": null,
        "authors": [],
        "publish_date": null,
        "summary": "",
        "meta_description": "",
        "meta_lang": "en",
        "meta_favicon": "",
        "meta_site_name": "",
        "canonical_link": null,
        "text": "Bernard Keenan1\\equalcontrib, Kacper Sokol2,3\\equalcontrib\n\nAbstract\n\nOver the past decade explainable artificial intelligence has evolved from a predominantly technical discipline into a field that is deeply intertwined with social sciences. Insights such as human preference for contrastive – more precisely, counterfactual – explanations have played a major role in this transition, inspiring and guiding the research in computer science. Other observations, while equally important, have nevertheless received much less consideration. The desire of human explainees to communicate with artificial intelligence explainers through a dialogue-like interaction has been mostly neglected by the community. This poses many challenges for the effectiveness and widespread adoption of such technologies as delivering a single explanation optimised according to some predefined objectives may fail to engender understanding in its recipients and satisfy their unique needs given the diversity of human knowledge and intention. Using insights elaborated by Niklas Luhmann and, more recently, Elena Esposito we apply social systems theory to highlight challenges in explainable artificial intelligence and offer a path forward, striving to reinvigorate the technical research in the direction of interactive and iterative explainers. Specifically, this paper demonstrates the potential of systems theoretical approaches to communication in elucidating and addressing the problems and limitations of human-centred explainable artificial intelligence.\n\n1 Introduction\n\nResearch into explainable artificial intelligence (AI) is growing at a breakneck pace and becoming ever more complex. Researchers are exploring a number of diverse approaches, using different models of explainers across varying contexts, and devising a plethora of strategies for assessing the success of these distinct explainability tools, methods and media (Guidotti et al. 2018). When it comes to evaluating XAI, a whole new field of heterogeneous desiderata and criteria has emerged (Sokol and Flach 2020a; Sokol and Vogt 2024). The question of whether or not a user can understand a data-driven automated decision-making (ADM) system is not simply a binary yes or no issue, nor it is reducible to the measurable psychological state of individuals (Sokol and Flach 2021). As such, the complexity of the problem of explainable AI (XAI) has grown.\n\nIt seems that the more we progress in XAI research, the more intricate the question and the broader the range of possible solutions become. We posit that this is because XAI is not simply a technological challenge that can be resolved with technical refinements alone. It additionally requires us to pay attention to the elementary building blocks of communication itself. But it appears that the closer we look at how satisfactory explanations are generated, communicated and accepted in natural communication, the more complex the problem becomes.\n\nIn this paper we draw on social systems theory, as elaborated by Niklas Luhmann and Elena Esposito, to offer a sociological account of explainable AI and its challenges. The theory provides a genetic account of society as communication that allows us to productively reframe the XAI field and highlight possible theoretical reasons for common challenges that researchers encounter. Our work outlines a comprehensive selection of theoretical points, and then demonstrates them in an applied setting.\n\nThe paper proceeds as follows. We first discuss the current state of XAI and the influence of insights from social sciences and psychology on (various technical developments in) this discipline (Section 2). We then introduce fundamental axioms of Luhmann’s (2012) theory of society, which allows us to highlight why the problems of XAI are particularly resonant with systems theory’s basic tenets (Section 3). Next, we show how the elements of communication pertain to problems commonly encountered in XAI research (Section 4). With those epistemic concepts in place, we outline how the theory can be applied to XAI (Section 5), and provide a concrete case study of XAI in healthcare, suggesting the questions that it may lead to (Section 6). We then conclude by revisiting our findings (Section 7).\n\n2 Theorising Artificial Intelligence Explainability and Society\n\nExplainability has become one of the most important aspects of automated decision-making tools given their proliferation across different spheres of everyday life, especially in high stakes domains such as medicine, policing and justice (Rudin 2019). Until recently, akin to the “inmates running the asylum” (Miller, Howe, and Sonenberg 2017), many experts developing XAI systems relied predominantly on their personal intuition of what constitutes a “good explanation”, paying little attention to findings from philosophy, social psychology or cognitive sciences (Miller 2019). Work on explainable AI has since striven to incorporate relevant insights on explanations and cognition to inform research into, and design of, data-driven algorithmic programmes and the interfaces that communicate their operation. These efforts aim to make the black box of AI explainable, in turn engendering trust in automated decisions and making them accountable where transparent actions are socially expected.\n\nWhile some insights from social sciences have since been widely adopted into technical designs by the XAI community, others remain largely neglected. The most prominent finding – people’s preference for contrastive explanations (Miller 2019) – has led the researchers to embrace counterfactuals, which are statements of the form “had a certain aspect of a situation changed like so, the outcome would have differed in this particular way”. Their flexibility, general comprehensibility and potential for compliance with legal frameworks (Wachter, Mittelstadt, and Russell 2017) as well as availability of generation procedures accounting for various human-centred properties (Poyiadzi et al. 2020; Karimi, Schölkopf, and Valera 2021; Romashov et al. 2022) have further added to their appeal.\n\nIn contrast, humans’ propensity to interact with the explainer through a dialogue-like interface – another important insight from social sciences (Miller 2019) – is often overlooked in computer science literature (Schneider and Handali 2019). Since explanations are contextual, i.e., the understanding of an explanation is always relative to the explainee’s situatedness, beliefs, aims and assumptions about the explanation, presenting arbitrary insights into the functioning of data-driven systems is insufficient to generate trust, accountability and legal compliance, or to achieve other social ends. Explainability must thus be approached as an intrinsically social and interactive process of (natural language) communication (Finzel et al. 2021; Lakkaraju et al. 2022). Agency ought to be given to the explainees, who cannot be treated as passive receivers of information, but instead actively engage in bi-directional interaction with the explainer to facilitate their observation and selection of salient information leading to understanding (Sokol and Flach 2020a).\n\nIn view of these findings it may be difficult or even impossible to achieve meaningful and satisfactory algorithmic explainability without an appropriate communication framework in place. For example, algorithmic recourse – which is a popular realisation of XAI that offers a sequence of actions guiding an explainee towards the desired outcome – is only a partial realisation of this paradigm since human input is still limited or non-existent, making it a one-sided process (Poyiadzi et al. 2020; Karimi, Schölkopf, and Valera 2021). Formal communication schemas and interaction protocols, e.g., realised through a natural language dialogue, conversation or otherwise, have also been studied in this context, offering theoretical foundations, design patterns and frameworks for dynamic personalisation of explanations (Walton 2007, 2011, 2016; Arioua and Croitoru 2015; Madumal et al. 2019; Schneider and Handali 2019). These technical concepts aim to model and facilitate explanatory interactions between two intelligent agents, be they humans, machines or one of each, using explanatory and questioning (examination) dialogue modes – e.g., implemented with formal argumentation (Dung, Kowalski, and Toni 2009) – that allow the interlocutors to argue, dispute and challenge problematic aspects of (automated) decisions and their explanations. While insightful, these contributions are fairly abstract, predominantly theoretical and primarily grounded in technical research, yet difficult to implement, hence they have not yet been fully embraced by practical explainability tools with a few notable exceptions (Kuźba and Biecek 2020; Malandri et al. 2023).\n\nDesign, evaluation and deployment of XAI systems outside of academic research have proven to be challenging as well (Bhatt et al. 2020). This is especially true for high stakes applications in which ante-hoc interpretability – i.e., having a predictive model that is transparent by design – is preferred (Rudin 2019; Sokol and Vogt 2023). Nonetheless, achieving this realisation of XAI requires more effort and domain knowledge than working with more universal and ubiquitous post-hoc approaches, where explainability is retrofitted into pre-existing data-driven systems by constructing a separate explanatory model. While producing insights that look like plausible explanations, the latter techniques often cannot guarantee their correctness, hence may be untrustworthy and misleading (Sokol et al. 2022; Sokol and Flach 2024).\n\nAdditionally, the properties expected of XAI systems span a broad range of sociotechnical desiderata, many of which are context-dependent and some of which cannot be satisfied at the same time, requiring trade-offs (Sokol and Flach 2020a, 2021). The diversity of purposes to which end explainability is used and objectives had by distinct stakeholders involved in the process further complicate the creation, testing and deployment of XAI techniques in real life (Bhatt et al. 2020). The lack of a universally agreed evaluation protocol also contributes to the problem (Sokol and Vogt 2024). While generic frameworks exist, they cannot prescribe the exact procedure given the breadth of use cases and involvement of humans – as explainees – in the process, which itself may be iterative and interactive, thus further complicating explainability evaluation attempts.\n\nEven though realising explanatory communication in its full extent may be beyond our reach at the moment, its simplified version has been implemented for counterfactuals, allowing the explainees to customise and personalise their content via a rudimentary interaction schema (Sokol and Flach 2018, 2020b). This limited form of user-driven information exchange within a fixed protocol is nonetheless already an improvement over simply making the interface of an XAI technique interactive, which only offers an illusion of agency.\n\nMore recently, the technical advancements in interactive XAI have been interpreted though the lens of a bi-directional, iterative social process, and captured in a conceptual framework that allows us to better study explainability as a phenomenon that emerges from the operations of dynamic social systems (Rohlfing et al. 2020). Since the explainees are not passive receivers and their mental states evolve throughout explanatory episodes, XAI should go beyond naïve personalisation and model such encounters as interactions between two social agents: one providing cues that steer the explanations and the other responding with insights adapted to the explainee’s current need for information. This process allows the explainer and explainee to co-construct understanding by monitoring and scaffolding knowledge, which is built iteratively and kept contextually relevant at all times.\n\nSystems-based approaches have also contributed to theoretical analyses aimed at differentiating and elaborating the levels of cognitive and social understanding required for data-driven models to operate in contingent and complex social and regulatory environments. For example, Dazeley et al. (2021) have argued that designing tools for interaction with non-expert users necessitates developing conversational models spanning multiple sense-making contexts, an approach they label Broad-XAI. Their work adopts a constructivist approach to explanation, taking the cognitive process by which AI generates decisions as the model, and mapping different levels of explanation to the human social process involved in successful, trustworthy explanations. The technical realisation of an AI’s cognitive model of its world – referred to as its merkwelt – then serves as a guide to XAI:\n\nzero-order\n\nexplanations are reactive, they explain how an AI agent reacts to its perceived inputs;\n\nfirst-order\n\nexplanations concern the agent’s disposition and motivations towards its environment, placing the reaction in the context of the agent’s beliefs or desires;\n\nsecond-order\n\nexplanations are social, in that they explain the decisions in relation to the agent’s awareness of its own mental state and that of others; and\n\nnth-order\n\nexplanations concern the cultural expectations of other actors at a general level.\n\nThe Broad-XAI perspective is complementary to our systems theoretical methodology. Systems theory also entails a constructivist approach to reality – it proposes an abstract explanatory model of communication in action. We thus argue that it provides a useful representation of the general social communicative environment, offering a theoretical structure to the nth-order level described by Dazeley et al. (2021) and elaborated further in Appendix B.\n\nWe posit that systems theory offers a meta-theoretical framework that is useful for translating insights from across different social science approaches to XAI into technical requirements to be implemented by explainability tools. This can become especially helpful when ADM models operate in complex or high stakes environments, where the social importance of their explainers increases. In such cases, understanding constructed and contested notions like meaning and trust must go beyond matters of technical design. While the conceptual frameworks overviewed throughout this section fulfil many XAI desiderata (Miller 2019; Sokol and Flach 2020a) and offer a blueprint for designing human-compatible interactive explainers, the literature overlooks the nature of communication itself. Here we address this gap by looking beyond the (technical) protocol responsible for the exchange of information. To this end, we employ the theory of social systems and communication introduced by Luhmann (1992, 1995) and elaborated by others (Moeller 2011), most recently Esposito (2022).\n\nDespite the relevance of Luhmann’s (1995) systems theory to artificial intelligence explainability, the two concepts have only recently been connected, albeit superficially, to define explainability (Sariyar and Holm 2022). The theory offers a response to the problem of the sociotechnical gap separating social and technical systems, formulated by Ackerman (2000) as the chasm between “what we know we must support socially and what we can support technically”. As Ehsan et al. (2023) have recently pointed out, XAI exemplifies this problem. Systems theory allows us to reframe this gap as the distinction between social and technical systems. Observed this way, the theory offers a useful set of methodological heuristics for mapping the social environment in which ADM models and XAI tools operate, which promises to unlock further progress in human-centred XAI.\n\n3 Key Points of a Systemic Approach to XAI\n\nBefore analysing the individual elements of communication (Section 4) and applying systems theory to XAI (Section 5) we first introduce a relevant selection of its fundamental axioms. A more in-depth overview of the theory – specifically, the concepts of social systems and communication – is available in Appendix A.\n\nSociety as Communication\n\nXAI is not simply a matter of imparting information or measuring human understanding, but a question of enabling communication, which entails treating information and understanding as socially embedded phenomena. For systems theory, society is communication, and nothing but communication. Furthermore, only communication communicates; individuals do not communicate. Counter-intuitively, systems theory defines the individual as a psychic system embodied in a living system. Human consciousness is therefore coupled to communication by the body’s sensory and expressive capacities, but it cannot cross the systemic boundary. Similarly, technology does not communicate. Technical media like displays, speakers and input devices enable computers to be coupled to the social operations of communication, but not to communicate; computers and technical media are simply in communication’s environment. In Luhmann’s (1992) constructivist theory, communication is an emergent phenomena produced in social activity. The humans, texts, networks and devices that enable and shape communication are not in it.\n\nOperational Closure\n\nEach social system adapts and orients itself to its environment, reproducing in each operation the distinction between system and environment for itself. Luhmann (1992) insists on the operational closure of systems, meaning no system communicates directly with any other system and information cannot directly transfer from one system to another. Instead, each system observes other systems as elements in its environment. The environment, therefore, is not a given ontological reality, instead it is composed by the system itself; i.e., the environment is not the totality of what is “really there”, but it is rather a cognitive construction of a system. Like living organisms, systems must generate an image of the world for themselves by selecting observations and processing them. In order to communicate about communication, we thus require a model for reducing complexity and dealing with self-reference.\n\nStructural Coupling\n\nAlthough communicating systems are closed to one another, they nonetheless form structural couplings, i.e., points at which different systems can mutually resonate or “irritate” one another in a regularised and predictable manner. Structural couplings narrow the possibilities of each system’s response to other systems and in so doing “digitise analogue relations”. They reduce the uncertainty of the environment by providing specific interpretative grids for one system to observe another, allowing precise information to be formed within both systems. As mentioned above, the body’s sensory organs link the social system of communication with the psychic systems of human beings as communication occurs (Luhmann 2012). But structural couplings can be conceptual social artefacts too, for instance, a contract couples together legal communication about norms with economic communication about payments. A coupling can be observed where both sides of distinctly different modes of understanding are mutually irritating one another. The concepts of operational closure and structural coupling are further elaborated in Appendix B.\n\nSecond-order Observation\n\nFor systems theory, second-order observation – which concept describes observing the observations of other observers, thus differentiating what was observed from how it was observed – is the universal condition of modernity. Put another way, there are no universal truths in modernity except second-order observations. It does not matter whether the observer is human, machine, organisation or hybrid. Communication is therefore inherently marked by contingency; each statement is always open to being observed otherwise. In the context of explainability, an ADM system generates observations via operations that cannot themselves be communicated. The task of XAI is to bridge the gap between those technical operations and the social world of communication. The XAI observes how the ADM model observed, and reports on it. XAI research, in turn, observes the observations of human observers observing the observations of XAI. Therefore, the task of XAI research is also second-order observation, as is this paper, which sits within a complex web of other observers’ papers.\n\nAt all points, observations depend on reducing the complexity of the world in order to make communication possible. Additionally, rather than aiming at scientifically correct or perfect understandings of information, we instead need to consider how the contingency of XAI observations can be successfully operationalised by observing systems in their ongoing communication. Here, successful communication does not require perfect understanding, nor it is about pure information transfer from the XAI system to the user, it only requires observing systems to be able to accept or reject new information with sufficient confidence in a given situation (Sokol and Flach 2020a). Ultimately, the aim of XAI is to permit ADM systems to be deployed in complex social and organisational settings – by making them comprehensible – where data-driven predictions must be useful, trustworthy and reliable over time.\n\nFunctional Differentiation\n\nModern communication is functionally differentiated. Society is composed of differentiated systems of communication, each of which relies on its own self-referential operations to produce stable sense over time. Law and science are just two examples; in the first, one relies on legal procedures to produce normative decisions about what is legal and what is illegal, and in the second, one relies on experimental methodologies and assumptions to generate facts about what is true or false. The differences are obvious but serve to demonstrate how differentiated systems of communication depend on different criteria of observation, which Luhmann described as coding and programming.\n\nSystems use their coding to connect one communicative operation to another over time, allowing meaning to be stabilised in differentiated domains, thus providing society with a background “reality” against which new information can be appraised and understood. Systems make communication more likely to be successful by reducing the complexity of their environment and “absorbing” the contingency of their observations of new information. Put simply, when a highly unexpected event occurs, communication reacts in unpredictable and functionally differentiated ways; e.g., its legal implications are understood one way, just as its economic, political and scientific implications are each different again. The point is not to suggest that each of these differentiated systems of communication is isolated from one another, but to shift the perspective from individual humans to the systemic ways in which we understand the ever-changing social world.\n\nNotably, any individual person can observe events through the lens of any system. Each system prompts different understandings and, at the second-order level, reduces the contingency of unexpected events by making them understandable in different dimensions. Under this purview, therefore, “understanding” is not simply the mental understanding belonging to individual humans, but rather an emergent property of systems in general that unfolds inter-subjectively and knits communication together.\n\nThis perspective showcases why XAI is a multifaceted problem with different approaches required in different contexts. The significance and expectations of a useful explanation change depending on the observing social system – e.g., law, science, politics or education – with each one containing sub-systemic examples. The radical constructivist approach of systems theory implies that all communication, including statements about causal effects, are contingent constructions produced by a system about itself and its environment – constructions that could have been produced differently.\n\nThe self-referential quality of social systems led Luhmann to suggest that social systems are best understood as autopoietic. Luhmann adapted this biological concept to indicate that social systems reproduce themselves using only their own elements and that society is evolutionary, changing itself through the contingent emergence of adaptations, which are contingently picked up and stabilised by their own reproduction over time.\n\nAutopoietic Dynamics\n\nSystems theory argues that society is evolutionary. Communication systems that constantly reproduce themselves do so by communicating about their environments, which they construct for themselves. Because the environment includes other communicating systems, each operating on its own terms, the environment of each system is in constant flux. Communication is stimulated in its autopoietic reproduction by this inexhaustible dynamics of contingent environmental change and systemic adaptation. The introduction of XAI as a means of allowing ADM tools to be used in communication thus entails multiple adaptive changes across a broad range of systems – law, economy, politics, medicine, science and education are just some systems transforming themselves and their organisational forms. Therefore, just as XAI researchers adapt technical systems to the needs of social systems, social systems are inevitably adaptive to XAI.\n\nInformation Processing & Organisations\n\nDifferentiated social systems expect explanations for various reasons and provide distinct organisational and procedural modes of processing information. Ultimately, the point is to decide whether to accept or reject a given proposition in ongoing communication. Even if a proposition is rejected, communication continues and learning can occur, either by correction or adaptation. Traditionally, the provision of second-order explanations for information has relied on the work of human intermediaries in organisational settings, often specifically qualified to observe and evaluate evidence. Such observations are highly contextual and depend on the system in question as well as the importance of the statement in that system.\n\nEmbedding XAI in social settings is not simply a question of making an ADM model understandable to a given human observer; it is a question of evolving functional equivalence between XAI systems and the organisational systems and human expertise that previously ensured that decisions are trustworthy, reliable and generally accepted in communication (Luhmann 2018). Organisations learn by developing techniques for setting goals, generating information, deleting or retaining memory, distributing decision-making powers, conducting reviews, anticipating risks, carrying out plans, and reviewing the relationship of the organisation to its operational environment. The accountability requirements of one observing system will be different to the requirements of others such as users, investigators, lawyers, insurers and the public – or more precisely, the mass media that stand in for the public (Dazeley et al. 2021).\n\nThe nature of organisational communication, in other words, allows it to adapt and absorb the uncertainty of navigating a complex environment (Luhmann 2018). Within this framework, organisational responsibility involves the tailoring of accountability procedures in ways that are irreducible to the states of the psychic systems of the organisation’s individual members. A more in-depth discussion of the role of organisations in coping with the contingencies of communication – especially in terms of decision-making accountability – is provided in Appendix D.\n\nUnpredictability\n\nAs Esposito (2022) explains, data-driven models are confronting us with information that is interesting precisely because it is not planned or available in advance – unpredictable outputs that not even the programmers can directly explain. Artificial intelligence systems autonomously develop their procedures and identify patterns, later using them to generate contingent observations in response to our queries. The information output by AI does not precede the query, rather the algorithm generated it itself. In this context, XAI is tasked with enabling ADM models to function as effective interaction partners in society’s communication by allowing data to produce understanding that was not available in advance of the query. This is exactly the point of using data-driven algorithms – they generate and thrive on the contingency of observations. Indeed, the very unpredictability of algorithmic decisions exacerbates the need for XAI even further since, as pointed out by Esposito (2022), “we get information that often was not planned or available in advance and was unknown to the programmers themselves”, and, as noted above, it is offered as the basis for decisions about the future.\n\nSystems Theoretical Perspective on XAI\n\nWith the fundamental axioms of systems theory in place, we can offer a new perspective on XAI by interpreting it through systems theoretical terms. Specifically, XAI research constitutes an emerging sub-system of science that responds to the problems of integrating the contingencies of ADM algorithms into communication. The challenge for AI explainability tools is to stabilise the coupling of algorithmic contingency to social systems. Where that happens successfully, communication has evolved, potentially giving rise to new forms of communication media as we discuss further in Appendix C.\n\n4 Elements of Communication:\n\nInformation, Utterance and Understanding\n\nIn systems theory, communication has a tripartite structure: information, utterance and understanding (Luhmann 2012). Communication is not the successful transmission of information from one system to another, from point A𝐴Aitalic_A to point B𝐵Bitalic_B, but an internal process of an observing system; the system operates by producing information about its environment. Understanding is the observation of meaningful information differentiated from the utterance that carries it. Every observing system – be it a psychical system of an individual human mind, a biological system like a neuron, or a social system of communication – produces information for itself according to its own internal operations and criteria. The selections that it makes determine what is produced.\n\nThis is not the same sense of communication as used in Shannon and Weaver’s (1963) information theory, which treats this process as a technical problem of successfully transferring quantifiable bits of information on a channel from sender to receiver in the presence of noise (Shannon 1948; Weaver 1953; Shannon and Weaver 1963). Information theory deliberately excluded the question of what the transmitted information means. Systems theory, on the other hand, is concerned with the conditions and operations of meaningful communication, and presupposes that inter-systemic transfer is impossible.\n\nSimilarly, an XAI explainer must make selections from among the complex operations carried out by the ADM system. In order to make the black box of an ADM algorithm transparent to an explainee, the explainer cannot simply transcribe the steps that were taken to reach the output value, as this would be too complex to be informative and would not be useful to the explainee (Sokol and Vogt 2023). Instead, the explainer must selectively reduce the complexity of the ADM system in a manner that counts as pertinent information for the explainee (Sokol and Flach 2024). In practice, neither ADM nor XAI systems are monolithic; they are complex and modular, and how they are arranged and programmed determines how they produce information about the world (Sokol et al. 2022; Sokol and Vogt 2024).\n\nOnce information has been selected according to the parameters of the ADM model and XAI system in play, the selected information must be arranged and presented in a form, or forms, that make it comprehensible to the explainee (Sokol and Vogt 2024). This entails the selection of appropriate media that allow successful communication; for example, text, colour, numbers, graphs, charts, audio or voice. In systems theoretic terms, the rendering of information into mediated forms constitutes utterances.\n\nOnce information has been selected and uttered by the explainer, it can be observed by observers and understood (or misunderstood). In systems theoretic terms, understanding takes place when an observer generates meaning by differentiating information from utterance. For instance, understanding occurs when an observer extracts meaningful information from the representation of data on a chart, table or statement (Small et al. 2023; Xuan et al. 2023).\n\nWhen researching applied explainability, a common approach is to evaluate the accuracy and confidence of the understanding developed in the minds of explainees (usually through user studies) after exposing them to the operation of an ADM model and the XAI framework built around it. Systems theory complicates this picture by highlighting the importance of the system in which those tools are to operate. Individuals act as coupling points within observing systems and must communicate through them. For example, a doctor observes information on a screen of a medical device provided according to the protocols and priorities of the medical system. The systemic aim of such guiding principles is to ensure that the decisions expected of a doctor are made within the communication of the medical system and its organisational sub-systems. How a doctor understands such information – e.g., a prediction output by an ADM model – about a given patient and how he or she operationalises it are highly dependent on the protocols of the situation.\n\nThe point is that the correctness and confidence of the information transferred from an ADM model to the human mind is not alone sufficient to successfully operationalise this newly gained knowledge. Systemically effective understanding emerges according to the codes and programmes of the observing system as it observes and processes the information produced by the ADM model and its explainer. Therefore, XAI needs to incorporate these systemic elements, recognising that such technological tools always operate in the environment of a communicating social system.\n\n5 Systems Theory Applied to XAI\n\nWith the foundations of systems theory in place, we can now map the three basic elements of Luhmann’s (1992) concept of communication to artificial intelligence explainability as follows.\n\nInformation\n\nbecomes the salient (numerical) insights about an ADM model’s operation generated by an explainer.\n\nUtterance\n\nis embodied by the social construct used to convey the information to a user, e.g., explanation type, explanatory artefact as well as communication medium and protocol.\n\n(Mis)understanding\n\ncaptures successful sense construction of the ADM model in the explainee’s mind and its relationship to the social systems with which he or she is observing. The explainee’s mental state is, of course, measurable only insofar as it is made observable via communication, and successful to the extent that the communication can successfully incorporate the explanation as salient in order to use it in further communications, or reject it as inadequate for specific reasons.\n\nAs computers do not understand their own operations and do not know what they do not know – they are indifferent to the meaning of information they process and the objective of the processing algorithms – an elementary algorithmic explainer must be designed around the first two variables: information and utterance. The explainer must first select salient information from the operation of the ADM model in question, e.g., a decision that it outputs; these selections are contingent on the parameters and configuration of the explainer. Notably, the explainer is, in systems theoretical terms, a second-order digital observer of the underlying ADM model, which is in its environment (or merkwelt). How a given explainer selects information from its constructed environment is a technical question, as is how it internally processes the selected observations to produce an explanatory insight (Sokol and Flach 2024; Sokol and Vogt 2024).\n\nInformation selection can either be based on a predefined set of criteria, e.g., embedded in an optimisation objective that is formalised by the XAI designers (Romashov et al. 2022), or it can be delegated to the explainees, e.g., through a user interface or an interactive explanatory process (Sokol and Flach 2018, 2020b); a mixture of the two is also possible. With respect to how an XAI tool internally processes information, since its encoding needed to achieve high predictive power may be unintelligible to the envisaged audience, the explainer may need to translate it into a more suitable format, e.g., a domain-specific interpretable representation (Sokol and Flach 2024). Next, the explainer must be designed to select appropriate artefacts to present its findings. There are multiple possibilities, with XAI researchers exploring how diverse formats such as numerical, visual, graphical and textual media differ in their capacity to engender understanding (Small et al. 2023; Xuan et al. 2023), as well as on a more fundamental level how choosing contrastive or associative explanation forms impacts explainability (Celar and Byrne 2023).\n\nUltimately, understanding will depend upon the observer. The what of information is differentiated from the how of presentation to produce understanding, including the possibility that more information is needed or a different form of utterance is required (Sokol and Vogt 2024). While currently such situations often cannot be resolved by the XAI tool itself, they can, and should, be anticipated by its creators. The design task is thus finding combinations of selections that make successful communication more probable than not, and to design conversational systems that allow communication to continue in contexts and domains where particular characteristics of explanations count as informative. Each selection will always be open to second-order observation. The observer may ask “Why this and not that?” – contrastive or counterfactual explanations – or “What goal was being sought?” – a more complex functionalist explanation – and it will frequently be important to maintain the possibility of asking such questions for some time into the future after the decision was made, depending on the observing system in question, as different queries may become salient at different times (Corti et al. 2024).\n\nIf the explainer fails, then communication ceases; the social system rejects the information as it is understood. As discussed above, a key distinction between artificial and human intelligence is that AI cannot engage in understanding – the explainer can only simulate communication, it cannot engage in it. Options and limitations for the continuation of communication must therefore be designed and incorporated into the explainer in advance and this must allow for different levels of observation. Notably, there is no one true explanation in each case but rather a range of possible selections of information and utterance (Sokol, Small, and Xuan 2023). Explanations are second-order observations of indications but are at the same time second-order forms in a particular medium.\n\nThis perspective agrees with experimental findings that report study subjects feeling that “there is not enough information” provided by an explainer to allow them to trust that the explanation they received is a fair, non-arbitrary insight into the functioning of a predictive algorithm (Schoeffer, Kuehl, and Machowski 2022). Unless the user of an explainer is also a programmer with access to its coding and relevant expertise, there is little scope for recourse and reconciliation. Communication with an explainer is one-sided since neither the predictive model nor the explainer can “change its mind” because either lacks agency, ingenuity and creativity to adapt to human users. This leads us to agree with Esposito’s (2022) insight that the problem as a whole is not one of dealing with an artificial intelligence akin to a human interlocutor but rather a problem of designing for artificial communication.\n\nAn explanation, whether accurate or not, will change the state of the system or the mental model of the user when it is included. This in turn gives rise to other observations, whether directly related to the ADM model or other aspects of the explainee’s worldview. Such artefacts and externalities cannot be anticipated in their totality as it is not possible to include everyone’s views in advance or foresee spontaneous thoughts and ideas that an explanation may trigger. A naïve example of such a situation is the explainee incorrectly generalising an explanation to other, unrelated outputs of the predictive model in question (Small et al. 2023; Xuan et al. 2023). But systems thinking can provide structure and depth to the XAI design task that go past the psychic response of any given individual. Such a perspective allows us to move beyond the figure of the user and study decisions in their social systemic sense-making contexts. These considerations may include organisational forms, in which case the design of XAI systems must account for organisational procedures (see Appendix D for more details).\n\nAdditionally, the anticipated functions of the explained ADM model should be carefully examined to understand how they emerged as necessary elements in relation to the dominant elements of the social system in question, how the information necessary for such decisions has been selected in the past, and the forms in which it has been expressed and made available for second-order review. Notably, this perspective agrees with the aforementioned Broad-XAI approach (Dazeley et al. 2021), which encapsulates the need to investigate the social and cultural expectations in an agent’s environment, with a good explanation using them as its elements. Such an investigation may allow XAI designers to form a systemic understanding of how a particular decision-making point emerged socially as a problem to be solved, rather than taken for granted. Similarly, a systemic approach would ask how decisions are currently handled, the temporal frame in which they are processed and decided, and how they are assessed, accepted or rejected.\n\nFurthermore, the design process should identify the interested parties, stakeholders and potentially affected groups, organisations and individuals, along with the relevant legal, political and normative considerations that inflect on it. Some decision contexts will simply require a greater variety of available explanations than others. From this perspective, judging the effectiveness of XAI is a problem of selective understanding, hence a recursive social problem – rather than being a self-contained technical question, it has recursive and iterative dimensions. As technology is adapted to the contingencies of communication, communication in turn adapts to accommodate the contingencies of technology. Social systems thus adapt to changes in the structure of communication.\n\nWhile not explicitly evoked, Luhmann’s (1992) functional theory of communication can be seen throughout different areas of (technical) research dealing with explainability of data-driven models, especially so in the field of human–computer interaction, which aims to bridge the aforementioned sociotechnical gap inherent to XAI. For example, scholars in this domain assume a fixed explanation type – such as a counterfactual statement delivered in text – and tweak various aspects of its presentation and the underlying (explanatory) user interface in an attempt to make it – or in the systems theoretical terms the act of communication – more likely to be accepted by a selected group of explainees (Du et al. 2019; Hadash et al. 2022; Bove et al. 2023). Within this context, our contributions reinforce the, often neglected, purview that the explainee is not only a recipient of an explanation but also a communication partner.\n\nThe consequences of disregarding this facet of XAI can be seen in explanations output by state-of-the-art explainers deployed in real-life applications being ignored or rejected, which may simply be an overlooked symptom of inadequate communication (Sivaraman et al. 2023). This broadened perspective accounts for the operational context of XAI systems – including the stakeholders, purpose of explainability and the like – in a more holistic way, thus allows us to better understand how ADM models and explainers thereof fit within society and its spheres. The connection between XAI and Luhmann’s (1992) functional theory of communication outlined in this paper can therefore help us to better choose information and utterance that are appropriate and sufficient to make sense of what is happening inside an algorithmic black box in view of the world that surrounds it (Ehsan et al. 2023), thus bridging the gap between automated decision-making and human understanding, as well as reconciling the technical and social aspects of XAI research.\n\n6 Case Study of XAI in Healthcare\n\nFinding a suitable setting for the deployment of ADM models involves identifying current areas of practice where existing processes can be enhanced by data-driven analysis in a measurable way. Healthcare is a unique domain in this regard as even minute improvements in medical practice reap large societal and economical rewards (Johnson et al. 2023). To date, ADM tools have enhanced detection of diabetic retinopathy, classification of skin cancer and metastases from breast cancer (Gulshan et al. 2016; Esteva et al. 2017; Golden 2017); nonetheless, many such models remain black boxes despite their adoption and the benefit thereof. The challenge for XAI in this context is thus to enable better integration of data-driven predictive models into the social system of healthcare.\n\nExisting medical workflows and decision-making practices are well established (Chang 2020). The autopoiesis of the healthcare system is concerned with the management of patient trajectories as well as investigating, monitoring and curing medical conditions (Berg 1999). Patient treatment follows protocols established to assess and anticipate outcomes, and organisations – here, healthcare institutions – have workflows designed to implement treatment plans while absorbing the contingencies of unknown variables and unexpected changes.\n\nExactly this complexity makes the adoption of new technologies highly challenging. This can be seen in the translational barrier – a chasm between technical solutions and clinical applications – that is prevalent in the AI for healthcare research (Wiens et al. 2019; Kanjilal et al. 2020; Moehring et al. 2021; Adams et al. 2022; Markowetz 2024). Notably, it can be linked to the sociotechnical gap at the interface of society and technology that XAI strives to address. This phenomenon leads to healthcare remaining one of the least digitised social systems, with many open challenges despite significant (technological) progress in the recent decades (Berg 1999; Capobianco 2019; Spatharou, Hieronimus, and Jenkins 2020). While there are plenty of AI models claiming state-of-the-art performance on selected tasks, these tools rarely ever become integrated into real-life medical workflows or even tested in a clinical setting, which impedes their adoption as such (non-clinical) evaluation results are unlikely to directly translate to clinical efficacy (Topol 2019; Wardi et al. 2023).\n\nWhile technical challenges unequivocally contribute to the translational barrier, the inherent incompatibility of ADM models with individual, organisational and social aspects of decision-making workflows – especially in high stakes domains such as healthcare – is also a contributing factor (Simkute et al. 2022; Anjum 2023; Wosny, Strasser, and Hastings 2023). When building data-driven predictive tools the conception of their role (in deployment) tends to be assumed too narrow (Mueller et al. 2019; Topol 2019; Akata et al. 2020; Ferrario, Facchini, and Termine 2023). These systems are usually tasked with streamlining or automating (decision-making) tasks thus far handled by humans, for which they can display impressive evaluation results, often leading to a conclusion that they achieve performance higher than that of domain experts with decades of professional experience.\n\nOnce equipped with human-centred interpretability or explainability, it is argued, such data-driven predictive tools could become worthy replacements of fallible humans. But such a view of automation appears too limited to be successful in real life (Munn 2022). Decision-making is just one, albeit acutely observed, step in the complex network of tasks and duties held by every stakeholder, who may as well have multiple, possibly inter-dependent, roles with different responsibilities across a number of organisations (Siddarth et al. 2021). Replacing just one, artificially carved out, component in this network with data-driven automation may disrupt all the other well established processes, causing apprehension towards automation and leading to it being widely ignored (Sivaraman et al. 2023; Tricco et al. 2023). In systems theoretical terms, there is simply too much contingency in play, and too little trust in the reliability of AI decisions.\n\nReplacing humans with automation is also problematic because the attribution of responsibility for algorithmic decisions remains unclear (van Baalen and Boon 2015). Whether AI is used for full automation, where ADM tools make and execute a decision with only human supervision, or it simply supports human decisions, e.g., by offering a suggestion or justification of a particular data-driven action for the human to consider, approve/reject and implement, when either workflow brings unintended consequences assigning the blame is non-trivial. This is of particular importance in high stakes domains such as healthcare, where mistakes can have dire consequences and tend to be mediated through the legal system. Law places demands for regulatory compliance on providers and, where things go wrong, assigns liability.\n\nEthnographic observation of the use of ADM tools in healthcare practice has demonstrated how second-order observation occurs. For instance, when confronted with a prediction made by the Sepsis Watch model, doctors reported wanting to “interpret the model” in the sense of “understanding the causation”. Yet as the model was “totally uninterpretable”, the development focused on demonstrating its efficacy within pre-existing social relations (Elish 2018). In a similar vein we suggest not to introduce ADM tools for decision support, but rather to build systems that aid human reasoning (van Baalen, Boon, and Verhoef 2021). This distinction is especially pertinent given that AI decision support is often understood as implementing an ADM system that mimics a selected task that thus far has been in the domain of human agents, and presenting its conclusions, possibly along with algorithmic explanations, to people. But as we have discussed earlier, this view of an action being self-contained and atomic completely disregards the nature of the decision-making process and its broader systemic situatedness. Explanations that merely address the accuracy of an AI prediction and how it was technically produced are simply insufficient to meet the demands of complex social systems.\n\nIndeed, XAI can actively hinder the use of ADM tools (Kaur et al. 2024). Providing an AI prediction and justifying it with insights derived from an algorithmic explainer may lead to disengagement, under-utilisation of human expertise as well as automation bias, or in the worst case ignoring ADM tools altogether (Byrne 2023; Miller 2023). Instead, we should strive to create data-driven systems that the users can interact with to better understand the underlying decision-making environment (e.g., through exploration), appreciate the uncertainty inherent to this domain, hypothesise about various actions and their consequences, and the like. This approach can additionally help humans to identify important, previously overlooked, observations and learn (Crandall and Gamblian 1991; Crandall and Getchell-Reiter 1993; Hogarth 2001).\n\nOperating within the reasoning support framework also keeps the responsibility for the decisions with humans in the formal positions they occupy in systemic organisations. In such a scenario an ADM model is simply a tool, and as long as it behaves predictably – i.e., it is robust and trustworthy, which may, for example, be guaranteed via adequate certification processes – the insights developed by interacting with it can be safely incorporated into human decision-making. In this case, the users bear the ultimate responsibility for their actions, the accountability of which is achieved through their ability to justify individual decisions by supporting them with information or evidence provided by the ADM tool as well as their own knowledge, expertise and experience.\n\nThis schema has already been incorporated into clinical workflows, albeit with systems that are primarily based on deterministic technology and that passed medical certification, for example, a machine to analyse blood samples. Doctors thus do not normally question the validity of blood test results since the underlying mechanisms are guaranteed to produce reliable outputs and the contingencies have been shifted to other entities, e.g., the device manufacturing company, the relevant government certification body, the technician responsible for regular inspections of the device, and the like. This operational framework allows doctors to simply integrate the resulting biomarkers into their cognitive process spanning activities such as exploration, analysis, hypothesis formation and (in)validation, decision-making, and action execution. As systems theory holds, organisational forms evolve as communication adopts ways of reducing the complexity and risk of its contingent environment.\n\nTherefore, we should not just strive to automate a task but also to understand the (organisational) structure of the selected decision-making workflow, and ensure that the replacement ADM tool is designed with all the roles for all the stakeholders in mind, adhering to their expectations. Once accounted for, making data-driven models interpretable or explainable may still be insufficient to enable their integration with humans, and viewing this process through the lens of communication outlined by systems theory can offer useful guidance. In particular, even if an ADM tool is certified as reliable, this is no guarantee of perfect information transfer or systemic trust. Second-order observation means that unpredictable perspectives may be brought to bear, or contingencies emerge that cannot be designed for in advance. The complexity of the setting and the risks that ADM tools may add to the organisational operations therefore need to be carefully assessed.\n\nIntroducing data-driven automation without taking the aforementioned steps can inadvertently complicate matters that such systems are intended to simplify. In summary, a good starting point is to: (i) design for the programmes that currently exist, (ii) identify where an ADM model might be usefully deployed, (iii) tailor the explanations to the second-order questioning that communication generates, and (iv) ensure that the distribution of risk and responsibility within the organisational structure (e.g., of the medical system) is maintained and supported but not overturned by AI tools. By viewing these interactions through the lens of communication we can better design artificial intelligence and its interpretability or explainability.\n\n7 Conclusion\n\nWhile throughout this paper we have demonstrated the usefulness of systems theory for XAI, as it stands it remains just that, a highly abstract theory. According to Moeller (2011), Luhmann’s (1992) theory is a “fourth insult” to humanism. Freud (1920) identified three insults to anthropocentrism; Copernicus’ (1543) astronomy, Darwin’s (1859) evolutionary biology and Freud’s (1920) psychoanalytic theories respectively challenged the centrality of humanity in relation to the cosmos, to creation and to the mind. Moeller’s point is that Luhmann similarly de-centres humanity in society. Humans are in the environment of communication, which cannot be steered and has no unifying point of control. Rationality is contingent on systemic conditions and cannot be universalised or unified. The evolution of society is contingent on society’s own operations and therefore cannot be directly controlled or planned for.\n\nWe are at an inflexion point in the evolution of communication marked by an acceleration in the shift of information processing from psychic to computer systems. Algorithmic tools analyse information without relying on understanding. The challenge of XAI is therefore critically important as it forms the structural coupling point at which meaning and understanding emerge in relation to artificial intelligence agents. Echoing Esposito (2022), we suggest that XAI is key to the development of successful artificial communication. We could even think of XAI research as an emergent social system that is evolving its own techniques and programmes at the interface of algorithmic systems and communication. Inevitably, XAI research will attract criticism and critique. A successful explainer tool can be observed as enhancing trust and, at the same time, criticised for increasing automation bias (Schoeffer, Kuehl, and Machowski 2022). Such differences in perspective nonetheless cannot be definitively resolved scientifically as they embody second-order observation.\n\nIn this paper, we proposed an epistemic shift from minds to systems, from interpretations to communication. The task for researchers is to establish new structures that can be selected for use or indeed, in situations where understanding is legally or politically essential to the acceptability of a decision, for prohibition – a point forcefully made by Rudin (2019). But we must at the same time be attuned to the limits. Computers are not autopoietic systems and do not perceive themselves; they do not know what they do not know. One outcome of the deployment of ADM and XAI systems in society may be that firmer lines are drawn around values, decisions and organisational capacities that must be reserved to the psychic operations of human beings. Paradoxically, a theory that excludes us, human beings, from social operations might help us see how to better defend ourselves. We can specify the importance of human decision-making, meaning-creation and the possibility of accountability in differentiated domains. After all, “if the outcome of a traditional machine becomes unpredictable, we do not think that it is creative or original – we think that it is broken” as observed by von Foerster (1985) and reported by Esposito (2022).\n\nAcknowledgements\n\nThis research was conducted by the ARC Centre of Excellence for Automated Decision-Making and Society (project number CE200100005), funded by the Australian Government through the Australian Research Council. The work was done in part while the authors were visiting the Simons Institute for the Theory of Computing at UC Berkeley.\n\nReferences\n\nAckerman (2000) Ackerman, M. S. 2000. The intellectual challenge of CSCW: The gap between social requirements and technical feasibility. Human–Computer Interaction, 15(2-3): 179–203.\n\nAdams et al. (2022) Adams, R.; Henry, K. E.; Sridharan, A.; Soleimani, H.; Zhan, A.; Rawat, N.; Johnson, L.; Hager, D. N.; Cosgrove, S. E.; Markowski, A.; Klein, E. Y.; Chen, E. S.; Saheed, M. O.; Henley, M.; Miranda, S.; Houston, K.; Linton, R. C.; Ahluwalia, A. R.; Wu, A. W.; and Saria, S. 2022. Prospective, multi-site study of patient outcomes after implementation of the TREWS machine learning-based early warning system for sepsis. Nature Medicine, 28(7): 1455–1460.\n\nAkata et al. (2020) Akata, Z.; Balliet, D.; de Rijke, M.; Dignum, F.; Dignum, V.; Eiben, G.; Fokkens, A.; Grossi, D.; Hindriks, K. V.; Hoos, H. H.; Hung, H.; Jonker, C. M.; Monz, C.; Neerincx, M. A.; Oliehoek, F.; Prakken, H.; Schlobach, S.; van der Gaag, L. C.; van Harmelen, F.; van Hoof, H.; van Riemsdijk, B.; van Wynsberghe, A.; Verbrugge, R.; Verheij, B.; Vossen, P.; and Welling, M. 2020. A research agenda for hybrid intelligence: Augmenting human intellect with collaborative, adaptive, responsible, and explainable artificial intelligence. Computer, 53(08): 18–28.\n\nAnjum (2023) Anjum, B. 2023. A conversation with Ken Holstein: Fostering human–AI complementarity. Ubiquity, 2023(11): 1–6.\n\nAradau and Blanke (2022) Aradau, C.; and Blanke, T. 2022. Algorithmic reason: The new government of self and other. Oxford University Press.\n\nArioua and Croitoru (2015) Arioua, A.; and Croitoru, M. 2015. Formalizing explanatory dialogues. In International Conference on Scalable Uncertainty Management, 282–297. Springer.\n\nBateson (1970) Bateson, G. 1970. Form, substance and difference. Essential Readings in Biosemiotics, 501.\n\nBerg (1999) Berg, M. 1999. Patient care information systems and health care work: A sociotechnical approach. International Journal of Medical Informatics, 55(2): 87–101.\n\nBhatt et al. (2020) Bhatt, U.; Xiang, A.; Sharma, S.; Weller, A.; Taly, A.; Jia, Y.; Ghosh, J.; Puri, R.; Moura, J. M. F.; and Eckersley, P. 2020. Explainable machine learning in deployment. In Proceedings of the 2020 ACM Conference on Fairness, Accountability, and Transparency, 648–657.\n\nBordt et al. (2022) Bordt, S.; Finck, M.; Raidl, E.; and von Luxburg, U. 2022. Post-hoc explanations fail to achieve their purpose in adversarial contexts. In Proceedings of the 2022 Conference on Fairness, Accountability, and Transparency, 891–905.\n\nBove et al. (2023) Bove, C.; Lesot, M.; Tijus, C. A.; and Detyniecki, M. 2023. Investigating the intelligibility of plural counterfactual examples for non-expert users: An explanation user interface proposition and user study. In Proceedings of the 28th International Conference on Intelligent User Interfaces, 188–203.\n\nByrne (2023) Byrne, R. M. 2023. Good explanations in explainable artificial intelligence (XAI): Evidence from human explanatory reasoning. In IJCAI, 6536–6544.\n\nCapobianco (2019) Capobianco, E. 2019. Data-driven clinical decision processes: It’s time. Journal of Translational Medicine, 17: 1–2.\n\nCelar and Byrne (2023) Celar, L.; and Byrne, R. M. 2023. How people reason with counterfactual and causal explanations for artificial intelligence decisions in familiar and unfamiliar domains. Memory & Cognition, 1–16.\n\nChang (2020) Chang, A. C. 2020. Intelligence-based medicine: Artificial intelligence and human cognition in clinical medicine and healthcare. Academic Press.\n\nCobbe, Lee, and Singh (2021) Cobbe, J.; Lee, M. S. A.; and Singh, J. 2021. Reviewable automated decision-making: A framework for accountable algorithmic systems. In Proceedings of the 2021 ACM Conference on Fairness, Accountability, and Transparency, 598–609.\n\nCopernicus (1543) Copernicus, N. 1543. On the revolutions of heavenly spheres.\n\nCorti et al. (2024) Corti, L.; Oltmans, R.; Jung, J.; Balayn, A.; Wijsenbeek, M.; and Yang, J. 2024. “It is a moving process”: Understanding the evolution of explainability needs of clinicians in pulmonary medicine. In Proceedings of the CHI Conference on Human Factors in Computing Systems, 1–21.\n\nCrandall and Gamblian (1991) Crandall, B.; and Gamblian, V. 1991. Guide to early sepsis assessment in the NICU. Instruction manual prepared for the Ohio Department of Development under the Ohio SBIR Bridge Grant program by Klein Associates Inc.\n\nCrandall and Getchell-Reiter (1993) Crandall, B.; and Getchell-Reiter, K. 1993. Critical decision method: A technique for eliciting concrete assessment indicators from the intuition of NICU nurses. Advances in Nursing Science, 16(1): 42–51.\n\nDarwin (1859) Darwin, C. 1859. On the origin of species by means of natural selection, or the preservation of favoured races in the struggle for life. John Murray.\n\nDazeley et al. (2021) Dazeley, R.; Vamplew, P.; Foale, C.; Young, C.; Aryal, S.; and Cruz, F. 2021. Levels of explainable artificial intelligence for human-aligned conversational explanations. Artificial Intelligence, 299.\n\nDeakin and Markou (2022) Deakin, S.; and Markou, C. 2022. Evolutionary interpretation: Law and machine learning. Journal of Cross-Disciplinary Research in Computational Law, 1(2).\n\nDu et al. (2019) Du, F.; Plaisant, C.; Spring, N.; Crowley, K.; and Shneiderman, B. 2019. Eventaction: A visual analytics approach to explainable recommendation for event sequences. ACM Transactions on Interactive Intelligent Systems (TiiS), 9(4): 1–31.\n\nDung, Kowalski, and Toni (2009) Dung, P. M.; Kowalski, R. A.; and Toni, F. 2009. Assumption-based argumentation. In Argumentation in Artificial Intelligence, 199–218. Springer.\n\nEhsan et al. (2023) Ehsan, U.; Saha, K.; De Choudhury, M.; and Riedl, M. O. 2023. Charting the sociotechnical gap in explainable AI: A framework to address the gap in XAI. Proceedings of the ACM on Human–Computer Interaction, 7(CSCW1): 1–32.\n\nElish (2018) Elish, M. C. 2018. The stakes of uncertainty: Developing and integrating machine learning in clinical care. In Ethnographic Praxis in Industry Conference Proceedings, volume 2018, 364–380. Wiley Online Library.\n\nErasmus, Brunet, and Fisher (2021) Erasmus, A.; Brunet, T. D. P.; and Fisher, E. 2021. What is interpretability? Philosophy & Technology, 34(4): 833–862.\n\nEsposito (2022) Esposito, E. 2022. Artificial communication: How algorithms produce social intelligence. MIT Press.\n\nEsteva et al. (2017) Esteva, A.; Kuprel, B.; Novoa, R. A.; Ko, J.; Swetter, S. M.; Blau, H. M.; and Thrun, S. 2017. Dermatologist-level classification of skin cancer with deep neural networks. Nature, 542(7639): 115–118.\n\nFerrario, Facchini, and Termine (2023) Ferrario, A.; Facchini, A.; and Termine, A. 2023. Experts or authorities? The strange case of the presumed epistemic superiority of artificial intelligence systems. SSRN preprint 4561425.\n\nFinzel et al. (2021) Finzel, B.; Tafler, D. E.; Scheele, S.; and Schmid, U. 2021. Explanation as a process: User-centric construction of multi-level and multi-modal explanations. In KI 2021: Advances in Artificial Intelligence: 44th German Conference on AI, 80–94. Springer.\n\nFreud (1920) Freud, S. 1920. A general introduction to psychoanalysis. Boni and Liveright.\n\nGolden (2017) Golden, J. A. 2017. Deep learning algorithms for detection of lymph node metastases from breast cancer: Helping artificial intelligence be seen. JAMA, 318(22): 2184–2186.\n\nGreen (2022) Green, B. 2022. The flaws of policies requiring human oversight of government algorithms. Computer Law & Security Review, 45.\n\nGuidotti et al. (2018) Guidotti, R.; Monreale, A.; Ruggieri, S.; Turini, F.; Giannotti, F.; and Pedreschi, D. 2018. A survey of methods for explaining black box models. ACM Computing Surveys (CSUR), 51(5): 1–42.\n\nGulshan et al. (2016) Gulshan, V.; Peng, L. H.; Coram, M.; Stumpe, M. C.; Wu, D. J.; Narayanaswamy, A.; Venugopalan, S.; Widner, K.; Madams, T.; Cuadros, J. A.; Kim, R.; Raman, R.; Nelson, P.; Mega, J. L.; and Webster, D. R. 2016. Development and validation of a deep learning algorithm for detection of diabetic retinopathy in retinal fundus photographs. JAMA, 316(22): 2402–2410.\n\nHadash et al. (2022) Hadash, S.; Willemsen, M. C.; Snijders, C.; and Ijsselsteijn, W. A. 2022. Improving understandability of feature contributions in model-agnostic explainable AI tools. In Proceedings of the 2022 CHI Conference on Human Factors in Computing Systems, 1–9.\n\nHildebrandt (2018) Hildebrandt, M. 2018. Algorithmic regulation and the rule of law. Philosophical Transactions of the Royal Society A: Mathematical, Physical and Engineering Sciences, 376(2128): 20170355.\n\nHogarth (2001) Hogarth, R. M. 2001. Educating intuition. University of Chicago Press.\n\nJohnson et al. (2023) Johnson, M.; Patel, M.; Phipps, A.; van der Schaar, M.; Boulton, D.; and Gibbs, M. 2023. The potential and pitfalls of artificial intelligence in clinical pharmacology. CPT: Pharmacometrics & Systems Pharmacology, 12(3): 279–284.\n\nKanjilal et al. (2020) Kanjilal, S.; Oberst, M.; Boominathan, S.; Zhou, H.; Hooper, D. C.; and Sontag, D. 2020. A decision algorithm to promote outpatient antimicrobial stewardship for uncomplicated urinary tract infection. Science Translational Medicine, 12(568).\n\nKarimi, Schölkopf, and Valera (2021) Karimi, A.; Schölkopf, B.; and Valera, I. 2021. Algorithmic recourse: From counterfactual explanations to interventions. In Proceedings of the 2021 ACM Conference on Fairness, Accountability, and Transparency, 353–362.\n\nKaur et al. (2024) Kaur, H.; Conrad, M. R.; Rule, D.; Lampe, C.; and Gilbert, E. 2024. Interpretability gone bad: The role of bounded rationality in how practitioners understand machine learning. Proceedings of the ACM on Human–Computer Interaction, 8(CSCW1): 1–34.\n\nKuźba and Biecek (2020) Kuźba, M.; and Biecek, P. 2020. What would you ask the machine learning model? Identification of user needs for model explanations based on human-model conversations. In Joint European Conference on Machine Learning and Knowledge Discovery in Databases, 447–459. Springer.\n\nLakkaraju et al. (2022) Lakkaraju, H.; Slack, D.; Chen, Y.; Tan, C.; and Singh, S. 2022. Rethinking explainability as a dialogue: A practitioner’s perspective. arXiv preprint arXiv:2202.01875.\n\nLuhmann (1992) Luhmann, N. 1992. What is communication? Communication Theory, 2(3): 251–259.\n\nLuhmann (1995) Luhmann, N. 1995. Social systems. Stanford University Press.\n\nLuhmann (2012) Luhmann, N. 2012. Theory of society, volume 1. Stanford University Press.\n\nLuhmann (2018) Luhmann, N. 2018. Organization and decision. Cambridge University Press.\n\nMadumal et al. (2019) Madumal, P.; Miller, T.; Sonenberg, L.; and Vetere, F. 2019. A grounded interaction protocol for explainable artificial intelligence. In Proceedings of the 18th International Conference on Autonomous Agents and MultiAgent Systems, 1033–1041.\n\nMalandri et al. (2023) Malandri, L.; Mercorio, F.; Mezzanzanica, M.; and Nobani, N. 2023. ConvXAI: A system for multimodal interaction with any black-box explainer. Cognitive Computation, 15(2): 613–644.\n\nMarkowetz (2024) Markowetz, F. 2024. All models are wrong and yours are useless: Making clinical prediction models impactful for patients. NPJ Precision Oncology, 8(1): 54.\n\nMiller (2019) Miller, T. 2019. Explanation in artificial intelligence: Insights from the social sciences. Artificial Intelligence, 267: 1–38.\n\nMiller (2023) Miller, T. 2023. Explainable AI is dead, long live explainable AI! Hypothesis-driven decision support using evaluative AI. In Proceedings of the 2023 ACM Conference on Fairness, Accountability, and Transparency, 333–342.\n\nMiller, Howe, and Sonenberg (2017) Miller, T.; Howe, P.; and Sonenberg, L. 2017. Explainable AI: Beware of inmates running the asylum or: How I learnt to stop worrying and love the social and behavioural sciences. In Proceedings of the IJCAI Workshop on eXplainable Artificial Intelligence (XAI 2017).\n\nMoehring et al. (2021) Moehring, R. W.; Phelan, M.; Lofgren, E.; Nelson, A.; Ashley, E. D.; Anderson, D. J.; and Goldstein, B. A. 2021. Development of a machine learning model using electronic health record data to identify antibiotic use among hospitalized patients. JAMA Network Open, 4(3): e213460–e213460.\n\nMoeller (2011) Moeller, H. 2011. The radical Luhmann. Columbia University Press.\n\nMueller et al. (2019) Mueller, S. T.; Hoffman, R. R.; Clancey, W.; Emrey, A.; and Klein, G. 2019. Explanation in human–AI systems: A literature meta-review, synopsis of key ideas and publications, and bibliography for explainable AI. arXiv preprint arXiv:1902.01876.\n\nMunn (2022) Munn, L. 2022. Automation is a myth. Stanford University Press.\n\nPoyiadzi et al. (2020) Poyiadzi, R.; Sokol, K.; Santos-Rodriguez, R.; De Bie, T.; and Flach, P. 2020. FACE: Feasible and actionable counterfactual explanations. In Proceedings of the AAAI/ACM Conference on AI, Ethics, and Society, 344–350.\n\nRohlfing et al. (2020) Rohlfing, K. J.; Cimiano, P.; Scharlau, I.; Matzner, T.; Buhl, H. M.; Buschmeier, H.; Esposito, E.; Grimminger, A.; Hammer, B.; Häb-Umbach, R.; Horwath, I.; Hüllermeier, E.; Kern, F.; Kopp, S.; Thommes, K.; Ngonga Ngomo, A.; Schulte, C.; Wachsmuth, H.; Wagner, P.; and Wrede, B. 2020. Explanation as a social practice: Toward a conceptual framework for the social design of AI systems. IEEE Transactions on Cognitive and Developmental Systems, 13(3): 717–728.\n\nRomashov et al. (2022) Romashov, P.; Gjoreski, M.; Sokol, K.; Martinez, M. V.; and Langheinrich, M. 2022. BayCon: Model-agnostic Bayesian counterfactual generator. In IJCAI, 740–746.\n\nRudin (2019) Rudin, C. 2019. Stop explaining black box machine learning models for high stakes decisions and use interpretable models instead. Nature Machine Intelligence, 1(5): 206–215.\n\nSariyar and Holm (2022) Sariyar, M.; and Holm, J. 2022. Medical informatics in a tension between black-box AI and trust. In Informatics and Technology in Clinical Care and Public Health, 41–44. IOS Press.\n\nSchneider and Handali (2019) Schneider, J.; and Handali, J. 2019. Personalized explanation for machine learning: A conceptualization. In ECIS.\n\nSchoeffer, Kuehl, and Machowski (2022) Schoeffer, J.; Kuehl, N.; and Machowski, Y. 2022. “There is not enough information”: On the effects of explanations on perceptions of informational fairness and trustworthiness in automated decision-making. In Proceedings of the 2022 ACM Conference on Fairness, Accountability, and Transparency, 1616–1628.\n\nShannon (1948) Shannon, C. E. 1948. A mathematical theory of communication. The Bell System Technical Journal, 27(3): 379–423.\n\nShannon and Weaver (1963) Shannon, C. E.; and Weaver, W. 1963. The mathematical theory of communication. The University of Illinois Press.\n\nSiddarth et al. (2021) Siddarth, D.; Acemoglu, D.; Allen, D.; Crawford, K.; Evans, J.; Jordan, M.; and Weyl, E. 2021. How AI fails us. Justice, Health, and Democracy Impact Initiative & Carr Center for Human Rights Policy. The Edmond & Lily Safra Center for Ethics, Harvard University.\n\nSileno, Boer, and van Engers (2018) Sileno, G.; Boer, A.; and van Engers, T. M. 2018. The role of normware in trustworthy and explainable AI. arXiv preprint arXiv:1812.02471.\n\nSimkute et al. (2022) Simkute, A.; Surana, A.; Luger, E.; Evans, M.; and Jones, R. 2022. XAI for learning: Narrowing down the digital divide between “new” and “old” experts. In Adjunct Proceedings of the 2022 Nordic Human–Computer Interaction Conference, 1–6.\n\nSivaraman et al. (2023) Sivaraman, V.; Bukowski, L. A.; Levin, J.; Kahn, J. M.; and Perer, A. 2023. Ignore, trust, or negotiate: Understanding clinician acceptance of AI-based treatment recommendations in health care. In Proceedings of the 2023 CHI Conference on Human Factors in Computing Systems, 1–18.\n\nSmall et al. (2023) Small, E.; Xuan, Y.; Hettiachchi, D.; and Sokol, K. 2023. Helpful, misleading or confusing: How humans perceive fundamental building blocks of artificial intelligence explanations. In Proceedings of the ACM CHI 2023 Workshop on Human-Centered Explainable AI (HCXAI).\n\nSokol and Flach (2018) Sokol, K.; and Flach, P. 2018. Glass-Box: Explaining AI decisions with counterfactual statements through conversation with a voice-enabled virtual assistant. In IJCAI, 5868–5870.\n\nSokol and Flach (2020a) Sokol, K.; and Flach, P. 2020a. Explainability fact sheets: A framework for systematic assessment of explainable approaches. In Proceedings of the 2020 ACM Conference on Fairness, Accountability, and Transparency, 56–67.\n\nSokol and Flach (2020b) Sokol, K.; and Flach, P. 2020b. One explanation does not fit all. KI-Künstliche Intelligenz, 1–16.\n\nSokol and Flach (2021) Sokol, K.; and Flach, P. 2021. Explainability is in the mind of the beholder: Establishing the foundations of explainable artificial intelligence. arXiv preprint arXiv:2112.14466.\n\nSokol and Flach (2024) Sokol, K.; and Flach, P. 2024. Interpretable representations in explainable AI: From theory to practice. Data Mining and Knowledge Discovery, 1–39.\n\nSokol et al. (2022) Sokol, K.; Hepburn, A.; Santos-Rodriguez, R.; and Flach, P. 2022. What and how of machine learning transparency: Building bespoke explainability tools with interoperable algorithmic components. Journal of Open Source Education, 5(58): 175.\n\nSokol, Small, and Xuan (2023) Sokol, K.; Small, E.; and Xuan, Y. 2023. Navigating explanatory multiverse through counterfactual path geometry. In Workshop on Counterfactuals in Minds and Machines at 2023 International Conference on Machine Learning (ICML).\n\nSokol and Vogt (2023) Sokol, K.; and Vogt, J. E. 2023. (Un)reasonable allure of ante-hoc interpretability for high-stakes domains: Transparency is necessary but insufficient for comprehensibility. In 3rd Workshop on Interpretable Machine Learning in Healthcare (IMLH) at 2023 International Conference on Machine Learning (ICML).\n\nSokol and Vogt (2024) Sokol, K.; and Vogt, J. E. 2024. What does evaluation of explainable artificial intelligence actually tell us? A case for compositional and contextual validation of XAI building blocks. In Extended Abstracts of the 2024 CHI Conference on Human Factors in Computing Systems.\n\nSpatharou, Hieronimus, and Jenkins (2020) Spatharou, A.; Hieronimus, S.; and Jenkins, J. 2020. Transforming healthcare with AI: The impact on the workforce and organizations. McKinsey & Company, 10.\n\nSullivan (2022) Sullivan, E. 2022. Understanding from machine learning models. The British Journal for the Philosophy of Science.\n\nTopol (2019) Topol, E. J. 2019. High-performance medicine: The convergence of human and artificial intelligence. Nature Medicine, 25(1): 44–56.\n\nTricco et al. (2023) Tricco, A. C.; Hezam, A.; Parker, A.; Nincic, V.; Harris, C.; Fennelly, O.; Thomas, S. M.; Ghassemi, M.; McGowan, J.; Paprica, P. A.; and Straus, S. E. 2023. Implemented machine learning tools to inform decision-making for patient care in hospital settings: A scoping review. Open, 13(2): e065845.\n\nvan Baalen and Boon (2015) van Baalen, S.; and Boon, M. 2015. An epistemological shift: From evidence-based medicine to epistemological responsibility. Journal of Evaluation in Clinical Practice, 21(3): 433–439.\n\nvan Baalen, Boon, and Verhoef (2021) van Baalen, S.; Boon, M.; and Verhoef, P. 2021. From clinical decision support to clinical reasoning support systems. Journal of Evaluation in Clinical Practice, 27(3): 520–528.\n\nvon Foerster (1985) von Foerster, H. 1985. Cibernetica ed epistemologia: Storia e prospettive. La Sfida della Complessità, 112–140.\n\nWachter, Mittelstadt, and Russell (2017) Wachter, S.; Mittelstadt, B.; and Russell, C. 2017. Counterfactual explanations without opening the black box: Automated decisions and the GPDR. Harvard Journal of Law & Technology, 31: 841.\n\nWalton (2007) Walton, D. 2007. Dialogical models of explanation. ExaCt, 2007: 1–9.\n\nWalton (2011) Walton, D. 2011. A dialogue system specification for explanation. Synthese, 182(3): 349–374.\n\nWalton (2016) Walton, D. 2016. A dialogue system for evaluating explanations. In Argument Evaluation and Evidence, 69–116. Springer.\n\nWardi et al. (2023) Wardi, G.; Owens, R.; Josef, C.; Malhotra, A.; Longhurst, C.; and Nemati, S. 2023. Bringing the promise of artificial intelligence to critical care: What the experience with sepsis analytics can teach us. Critical Care Medicine.\n\nWeaver (1953) Weaver, W. 1953. Recent contributions to the mathematical theory of communication. ETC: A Review of General Semantics, 261–281.\n\nWiens et al. (2019) Wiens, J.; Saria, S.; Sendak, M.; Ghassemi, M.; Liu, V. X.; Doshi-Velez, F.; Jung, K.; Heller, K.; Kale, D.; Saeed, M.; Ossorio, P. N.; Thadaney-Israni, S.; and Goldenberg, A. 2019. Do no harm: A roadmap for responsible machine learning for health care. Nature Medicine, 25(9): 1337–1340.\n\nWosny, Strasser, and Hastings (2023) Wosny, M.; Strasser, L. M.; and Hastings, J. 2023. Experience of health care professionals using digital tools in the hospital: Qualitative systematic review. JMIR Human Factors, 10(1): e50357.\n\nXuan et al. (2023) Xuan, Y.; Small, E.; Sokol, K.; Hettiachchi, D.; and Sanderson, M. 2023. Can users correctly interpret machine learning explanations and simultaneously identify their limitations? arXiv preprint arXiv:2309.08438.\n\nAppendix A Differentiated Social Systems and Communication\n\nSocial systems theory is a model of society premised not on the agency of individuals or institutions, but on the conditions for the reproduction of communication. For Luhmann (1992), society is constructed exclusively of communication, and only communication communicates. This is not the same sense of “communication” as used in Shannon and Weaver’s (1963) information theory, which treats communication as a technical problem of successfully transferring quantifiable “bits” of information on a channel from sender to receiver in the presence of noise (Shannon 1948; Weaver 1953; Shannon and Weaver 1963). Information theory deliberately excluded the question of what the transmitted information means. Systems theory is instead concerned with the conditions and operations of meaningful communication. According to the theory, society is a complex of self-referential autonomous systems of communication. Communication is a self-organising process of differentiation that, independently of any central control, evolves and differentiates codes and structural processes, and does so using only its own processes. Systems make meaning possible by reducing the complexity of the world in order to communicate about it, which in turn makes society more complex as it communicates about itself and its environment.\n\nThe starting point is the distinction between system and environment. A system is constituted by the distinction between itself and its environment. The environment is always more complex than the system, which responds to the complexity by ordering itself so as to adapt and interpret the environment by generating information about it. Social systems are operationally closed but cognitively open to an environment that is, for the system, a correlate of the system’s own operations. The system indicates its environment by excluding it from the system, drawing a distinction between the world and itself. Paradoxically, this means that the “reality” of the distinction between system and environment cannot be grasped as a totality, as to indicate the distinction itself depends on drawing another distinction.\n\nEvery system observes and is made observable by the difference between itself and its environment – a distinction that is made within the system itself. Systems adapt and orient themselves to their environment, reproducing in each operation the distinction between system and environment. Luhmann (1992) insists on the operational closure of systems; no system communicates directly with any other system and information cannot “transfer” from one system to another. Rather, each system observes other systems as elements in its environment, and responds to its observations of other systems only on its own terms. The environment is not a given ontological reality, rather it is open-ended and mutable horizon of experience composed by the observing system in question. In other words, the environment is not the totality of what is “really there”, it is a cognitive construction that each system produces for itself. This way, order is generated from noise. Like living organisms, systems must discover the world by and for themselves – they produce information internally by selecting observations and processing them.\n\nWhen applied to social systems, there is no way to communicate about the distinction between society and the world without relying on communication. We must observe communication using communication. Hence totalising terms like “society” and “communication” serve only as references – symbolic terms that have particular valences when deployed in the ongoing communication of self-referential social systems. To understand and communicate about communication, we must rely on communication. Systems theory is in this way premised on paradoxes of observation, and this, as we suggested above, makes it a uniquely interesting way for charting the paradoxical problems of the sociotechnical gap (Ackerman 2000).\n\nAs explained in Section 4, the theory defines communication as a tripartite process of differentiation. Each communicative operation is composed of three distinct selections from a horizon of possibility. In each communication, understanding is the key moment; it occurs through the drawing of a distinction between selected information and utterance. Understanding – the making of meaning – is observer-dependent and arises in the decoding of what is communicated from how it is communicated. With each selection, an element is actualised from a horizon of potentiality by means of a distinction. What is indicated is distinguished from what is not indicated. This is why each communicative operation reinstates the boundary between an observing system and its environment. One side of the distinction is marked, the other side is excluded from indication.\n\nThe inherent contingency of each selection can, in turn, be observed via further second-order operations, which follow in time and can indicate both the previous indication and its unmarked side – making an indication of what was not selected. In this way, the first distinction re-enters the system. The system refers to its own previous operation and in so doing performs a new operation – a new instantiation of the distinction between system and environment. Thus each second-order observation entails the use of a further selection by means of a further distinction, the contingency of which can only be observed by means of a further second-order distinction, and so on.\n\nTherefore, there is no ultimate observation point; no privileged objective measure that is not made within a system. Each observation requires making a selection and thus produces a blind spot that can itself only be observed by way of further observations. If one continually utilises second-order observation to interrogate a previous observation, the system enters infinite regress. For this reason, systems have no “origin” and no prime causal factor. All attempts to identify a simple explanatory cause, or a foundation point for society’s operations, only results in further operations, further second-order observations of observations, which may agree or disagree. Systems theory therefore rejects any claims to objective causal explanations for society as these are precluded by its own autological structure. Causality can only be observed within the operations of social systems: legal causality, scientific causality or philosophical causality. These are, of course, different forms of causality that each depend on system-specific axioms and procedures. Moreover, in today’s complex society, they are all contingent on the specifics of the case in question.\n\nThis self-referential autonomy of social systems led Luhmann to suggest that social systems are best understood as autopoietic. This term, derived from studies of the reproduction of living cellular systems, indicates that social systems reproduce themselves using only their own elements and that society is evolutionary, meaning that it changes through the contingent emergence of changing adaptations, which are contingently picked up and stabilised by their own reproduction over time.\n\nIf we are all ultimately trapped in the labyrinth of communication, then we must be careful in how we theorise communication. Communication is concerned with the moment of understanding emergent in the observing system, but does not depend on accurate understanding. Rather, a “successful” communication is merely one that is accepted as the basis for further communication. To this extent, all communication can be regarded as productive misunderstanding – it is misunderstanding, clarification, disagreement and self-correction that keep the autopoiesis of society going as there is always something to say about what has just been said. Even silence is rich in potential meaning. What counts is whether uttered information is accepted as meaningful by observing systems.\n\nConsequently, society is the observations of the observations of observers, giving rise to an image of society as something comparable to organic complexity and contingency. As in life systems, no one social system can steer or determine the operations of another; instead, closed systems engage with one another only at the level of second-order observation. Every communicative event is therefore open to an unforeseeable range of possible second-order observations in its environment, which in turn can generate other environmental observations that in turn produce further responses.\n\nToday’s society is characterised for Luhmann by functional differentiation. Much of Luhmann’s (2012) extensive scholarly writing is concerned with tracing the evolution of social systems into higher degrees of complexity over time, which Luhmann attributes to communicative differentiation. He distinguishes between three broad structures. Segmentary societies are differentiated by the inclusion or exclusion of their members. Stratified societies assign members positions in vertically hierarchical structures. In contrast, society since the development of modernity is characterised by the functional differentiation of autonomous systems, non-hierarchically ordered, operating today on a planetary scale.\n\nBroad examples of function systems are law, politics, economics, science, art, religion, education or philosophy. Each social system can be identified by its own binary “coding”, i.e., core operative distinctions that are applied in that system’s communication. The application of the code unfolds through the aid of “programmes” – i.e., procedures, methods and structures of epistemic importance – that serve to reduce the complexity of the environment and give each system its self-referential identity and functional value in the social systems as a whole. Each system can observe the others and communicate about them: legal decisions have economic effects, philosophical arguments have political implications, and so on. But they cannot enter into each other’s communication directly.\n\nIt may be objected at this point that Luhmann’s (2012) depiction of society as strictly differentiated autonomous systems is too rigid and formal to capture the polycontextual reality of social life, which is full of mixtures and hybrids, or that it negates some determining force such as power or economics. However, as systems theory’s arguments apply to itself as much as to the rest of society, the theory designates itself as a sub-system in the social system of science. It insists that when we talk about social systems we can do so only by making use of another social system. In other words, the function of a system is defined by its environment and not by the system itself. The function of a social system is not found by studying the techniques and practices that constitute its internal structure. When considering XAI, similarly, the social function of a tool is not reducible to its computational operations but instead owes its meaningfulness to the observer.\n\nSecond-order observation is another key characteristic of functional differentiation. Social systems are concerned with not only what they observe of the world by distinguishing system from environment, but also with how the world is observed through that distinction. Today, we are used to second-order observation. Communication is reflexively adapted to constant changes in its environment, and contingency is expected.\n\nFor instance, the legal system, which operates by communicating about the coded distinction between legal and illegal, constantly adapts its rules and procedures to process and stabilise changing events in other systems. Similarly, new scientific methods replace older ones, creating new programmes for determining the application of the scientific code of truth and false, as applied to cognate facts, in the face of new research. In politics, new channels for processing political inclusions or exclusions can be opened (or closed off) by political decisions. Individuals observe themselves as others see them through social media profiles. The contingency of communication, and society’s self-awareness of this contingency, recursively increases the complexity of social systems, which become further differentiated and consequently generate further contingencies in a recursive process of self-differentiation.\n\nScience is, of course, an important social system for XAI. Observed functionally, it provides society with factual truths produced via second-order programmes that enable first-order observations to be verified. In order to be accepted as scientifically true, both the results and the methodology of an observation must be painstakingly recorded and verified, including by a community of peer-observers, and falsification is accepted as a possibility. There are no absolute scientific truths independent of conditions of observation. The what and how of observation condition one another. XAI researchers must manage the oscillation between the factual output of ADM systems and their manner of observation; in a sense, this oscillation is the central problem.\n\nWe can relate this point to the concept of “link certainty” developed by Sullivan (2022), which is crucial to understanding an ADM system, particularly when used to generate scientific or medical insights. It refers to “the extent to which the model fails to be empirically supported and adequately linked to the target phenomena”. As ADM algorithms do not follow the scientific method when producing their models from data, the meaningfulness of their outputs depends not on the transparency of the computational operations that determined how, technically, the ADM process arrived at a given output, but on the second-order question of the extent to which it works as a proxy for the same phenomena, understood scientifically. Link certainty therefore depends on the observational context. Its importance depends on what is at stake in the particular model’s functional case. And this, too, is a second-order question.\n\nAnother example relevant to XAI is the legal system. For Luhmann, law has the function of maintaining normative expectations in the face of disappointment. An event that has caused disappointment – i.e., “that should not have happened” – is assessed according to the programmes of the legal system, which absorbs and proceduralises disputes. Only after “due process” can the legal status of a contested event – the coding of legal and illegal – be applied and accepted as legitimate by all parties. What the law requires of an explanation for a contested decision depends on the situation.\n\nAn explanation in itself is not a legal remedy, although it may form the basis for deciding whether or not a remedy is owed. Thus decisions reached by way of ADM systems, even if they arrive at the “correct” answer in a given case, may not satisfy the legal system’s requirements for legal validity (Deakin and Markou 2022). Recalling Ackerman’s (2000) definition of the sociotechnical gap as the distinction between what we need socially and what we can do technically, it is possible that the normative expectation of understanding the reasons for a decision that affects legal rights (a social need) may lie beyond the technical capacity of machines (Hildebrandt 2018; Erasmus, Brunet, and Fisher 2021; Deakin and Markou 2022).\n\nHence researchers have warned against assuming that generating a successful understanding of a particular decision is sufficient to legitimise the use of an ADM system in contested adversarial contexts found in the legal system. In contrast to scientific research, which presupposes the alignment of interests between explainers and explainees, in adversarial legal scenarios the explainers and explainees likely have competing interests and each will be motivated to select explanations that suit their interests. As such, selecting an appropriate explainer cannot always be treated as a question of choosing the best – “most truthful” – option. Assuming otherwise, and building organisations on such assumptions, will ultimately produce trivial, partial, misleading or otherwise harmful effects by justifying illegitimate, unfair or discriminatory data-driven automation and decision-making (Bordt et al. 2022).\n\nThese examples illustrate how systems theory can incorporate a diverse range of sociotechnical literature. Indeed, all academic literature is part of the autopoiesis of society’s self-observation. Luhmann’s (1992) elementary contribution is that assessing the meaning of the output of an explainer depends on the second-order observations of the system in question. At the same time, social systems are adaptive to changes in their environment through self-reference, evolving in response via contingent differentiation and adaptation. XAI tools should therefore aim to make available the information necessary for social systems to incorporate outputs of ADM systems according to their own social coding, which is internal to the differentiated operations of each system. This is not a purely technical matter, but requires extensive situational engagement with the problems that the particular ADM tool is intended to solve.\n\nSecondarily, this means it is unsurprising that a novel problem like XAI is multivariate. The term spans a multiplicity of different problems for different systems, with science, philosophy, economics and law being obvious examples, each of which contains sub-systemic examples.\n\nAppendix B Operational Closure and Structural Coupling\n\nThe closure of social systems makes the theory an unusual way of thinking about society as human beings are excluded from society. Only communication communicates and communication is the systemic coupling of information, utterance and understanding. The “observer” is an abstraction, the necessary correlate of working with the primary distinction between system and environment. Who or what draws the distinction depends on the given situation. Observing the distinction requires a further distinction, which presupposes another observer, and so on. Hence the observer is always an abstraction – a role that could be played by a single person at multiple levels, by an organisation or by multiple observers mediated via technological processing. The abstraction makes the theory flexible across contexts.\n\nThe human being in the environment of communication is composed of the coupling of two operationally distinct autopoietic systems: the living system of the body and the psychic system of consciousness. The mind and body are coupled to society through the sensory organs as well as the linguistic and perceptual capacities of consciousness. Living, psychic and social systems are co-evolutionary. They evolve independently of one another using their own specific operations, yet they depend upon, influence and constrain one another in ways that are contingent and unpredictable.\n\nThe closure of systems has consequences for ontological claims about the world. For instance, rather than a physical quantity, information is treated as a difference in"
    }
}
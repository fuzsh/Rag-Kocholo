{
    "id": "dbpedia_1985_0",
    "rank": 47,
    "data": {
        "url": "https://www.science.gov/topicpages/d/demonstrates%2Bgood%2Baccuracy",
        "read_more_link": "",
        "language": "en",
        "title": "demonstrates good accuracy: Topics by Science.gov",
        "top_image": "",
        "meta_img": "",
        "images": [
            "https://www.science.gov/scigov/desktop/en/images/SciGov_logo.png"
        ],
        "movies": [],
        "keywords": [],
        "meta_keywords": [
            ""
        ],
        "tags": null,
        "authors": [],
        "publish_date": null,
        "summary": "",
        "meta_description": "",
        "meta_lang": "en",
        "meta_favicon": "",
        "meta_site_name": "",
        "canonical_link": null,
        "text": "A demonstration of a transportable radio interferometric surveying system with 3-cm accuracy on a 307-m base line\n\nNASA Technical Reports Server (NTRS)\n\nOng, K. M.; Macdoran, P. F.; Thomas, J. B.; Fliegel, H. F.; Skjerve, L. J.; Spitzmesser, D. J.; Batelaan, P. D.; Paine, S. R.; Newsted, M. G.\n\n1976-01-01\n\nA precision geodetic measurement system (Aries, for Astronomical Radio Interferometric Earth Surveying) based on the technique of very long base line interferometry has been designed and implemented through the use of a 9-m transportable antenna and the NASA 64-m antenna of the Deep Space Communications Complex at Goldstone, California. A series of experiments designed to demonstrate the inherent accuracy of a transportable interferometer was performed on a 307-m base line during the period from December 1973 to June 1974. This short base line was chosen in order to obtain a comparison with a conventional survey with a few-centimeter accuracy and to minimize Aries errors due to transmission media effects, source locations, and earth orientation parameters. The base-line vector derived from a weighted average of the measurements, representing approximately 24 h of data, possessed a formal uncertainty of about 3 cm in all components. This average interferometry base-line vector was in good agreement with the conventional survey vector within the statistical range allowed by the combined uncertainties (3-4 cm) of the two techniques.\n\nCLARREO Pathfinder Mission to ISS: Demonstrating Greatly Increased Accuracy for Reflected Solar Space Based Observations: Calibration and Intercalibration\n\nNASA Astrophysics Data System (ADS)\n\nWielicki, B. A.\n\n2016-12-01\n\nThe CLARREO (Climate Absolute Radiance and Refractivity) Pathfinder mission is a new mission started by NASA in 2016. CLARREO Pathfinder will fly a new generation of high accuracy reflected solar spectrometer in orbit on the Inernational Space Station (ISS) to demonstrate the ability to increase accuracy of reflected solar observations from space by a factor of 3 to 20. The spectrometer will use the sun and moon as calibration sources with a baseline objective of 0.3% (1 sigma) reflectance calibration uncertainty for the contiguous spectrum from 350nm to 2300nm, covering over 95% of the Earth's reflected solar spectrum. Spectral sampling is 3nm with resolution of 6nm. The spectrometer is mounted on a 2-axis gimbal enabling a new ability to use the same optical path to view the sun, moon, and Earth. Planned launch is 2020 with at least 1 year on orbit to demonstrate the new capability. The mission will also demonstrate the ability to use the new spectrometer as a reference transfer spectrometer in orbit to achieve intercalibration of reflected solar instruments to within 0.3% (1 sigma) using space, time, spectral, and angle matched observations across the full scan width of remote sensing instruments. Intercalibration to 0.3% will be demonstrated across the full scan width of the NASA CERES broadband radiometer and the NOAA VIIRS imager reflected solar spectral channels. This mission will demonstrate reflected solar intercalibration across the full swath width as opposed to current nadir only intercalibration used by GSICS (Global Space Based InterCalibration System). Intercalibration will include a new capability to determine scan angle dependence of polarization sensitivity of instruments like VIIRS. The high accuracy goals of this mission are driven primarily by the accuracy required to more rapidly and accurately observe climate change signals such as cloud feedback (see Wielicki et al. 2013 Bulletin of the American Meteorological Society). The new high accuracy\n\nUse of Selected Goodness-of-Fit Statistics to Assess the Accuracy of a Model of Henry Hagg Lake, Oregon\n\nNASA Astrophysics Data System (ADS)\n\nRounds, S. A.; Sullivan, A. B.\n\n2004-12-01\n\nAssessing a model's ability to reproduce field data is a critical step in the modeling process. For any model, some method of determining goodness-of-fit to measured data is needed to aid in calibration and to evaluate model performance. Visualizations and graphical comparisons of model output are an excellent way to begin that assessment. At some point, however, model performance must be quantified. Goodness-of-fit statistics, including the mean error (ME), mean absolute error (MAE), root mean square error, and coefficient of determination, typically are used to measure model accuracy. Statistical tools such as the sign test or Wilcoxon test can be used to test for model bias. The runs test can detect phase errors in simulated time series. Each statistic is useful, but each has its limitations. None provides a complete quantification of model accuracy. In this study, a suite of goodness-of-fit statistics was applied to a model of Henry Hagg Lake in northwest Oregon. Hagg Lake is a man-made reservoir on Scoggins Creek, a tributary to the Tualatin River. Located on the west side of the Portland metropolitan area, the Tualatin Basin is home to more than 450,000 people. Stored water in Hagg Lake helps to meet the agricultural and municipal water needs of that population. Future water demands have caused water managers to plan for a potential expansion of Hagg Lake, doubling its storage to roughly 115,000 acre-feet. A model of the lake was constructed to evaluate the lake's water quality and estimate how that quality might change after raising the dam. The laterally averaged, two-dimensional, U.S. Army Corps of Engineers model CE-QUAL-W2 was used to construct the Hagg Lake model. Calibrated for the years 2000 and 2001 and confirmed with data from 2002 and 2003, modeled parameters included water temperature, ammonia, nitrate, phosphorus, algae, zooplankton, and dissolved oxygen. Several goodness-of-fit statistics were used to quantify model accuracy and bias. Model\n\nBreast screen new South wales generally demonstrates good radiologic viewing conditions.\n\nPubMed\n\nSoh, BaoLin Pauline; Lee, Warwick; Diffey, Jennifer L; McEntee, Mark F; Kench, Peter L; Reed, Warren M; Brennan, Patrick C\n\n2013-08-01\n\nThis study measured reading workstation monitors and the viewing environment currently available within BreastScreen New South Wales (BSNSW) centres to determine levels of adherence to national and international guidelines. Thirteen workstations from four BSNSW service centres were assessed using the American Association of Physicists in Medicine Task Group 18 Quality Control test pattern. Reading workstation monitor performance and ambient light levels when interpreting screening mammographic images were assessed using spectroradiometer CS-2000 and chroma meter CL-200. Overall, radiologic monitors within BSNSW were operating at good acceptable levels. Some non-adherence to published guidelines included the percentage difference in maximum luminance between pairs of primary monitors at individual workstations (61.5Â % or 30.8Â % of workstations depending on specific guidelines), maximum luminance (23.1Â % of workstations), luminance non-uniformity (11.5Â % of workstations) and minimum luminance (3.8Â % of workstations). A number of ambient light measurements did not comply with the only available evidence-based guideline relevant to the methodology used in this study. Larger ambient light variations across sites are shown when monitors were switched off, suggesting that differences in ambient lighting between sites can be masked when a standard mammogram is displayed for photometric measurements. Overall, BSNSW demonstrated good adherence to available guidelines, although some non-compliance has been shown. Recently updated United Kingdom and Australian guidelines should help reduce confusion generated by the plethora and sometimes dated nature of currently available recommendations.\n\nGood Practices in Free-energy Calculations\n\nNASA Technical Reports Server (NTRS)\n\nPohorille, Andrew; Jarzynski, Christopher; Chipot, Christopher\n\n2013-01-01\n\nAs access to computational resources continues to increase, free-energy calculations have emerged as a powerful tool that can play a predictive role in drug design. Yet, in a number of instances, the reliability of these calculations can be improved significantly if a number of precepts, or good practices are followed. For the most part, the theory upon which these good practices rely has been known for many years, but often overlooked, or simply ignored. In other cases, the theoretical developments are too recent for their potential to be fully grasped and merged into popular platforms for the computation of free-energy differences. The current best practices for carrying out free-energy calculations will be reviewed demonstrating that, at little to no additional cost, free-energy estimates could be markedly improved and bounded by meaningful error estimates. In energy perturbation and nonequilibrium work methods, monitoring the probability distributions that underlie the transformation between the states of interest, performing the calculation bidirectionally, stratifying the reaction pathway and choosing the most appropriate paradigms and algorithms for transforming between states offer significant gains in both accuracy and precision. In thermodynamic integration and probability distribution (histogramming) methods, properly designed adaptive techniques yield nearly uniform sampling of the relevant degrees of freedom and, by doing so, could markedly improve efficiency and accuracy of free energy calculations without incurring any additional computational expense.\n\n40 CFR 92.127 - Emission measurement accuracy.\n\nCode of Federal Regulations, 2010 CFR\n\n2010-07-01\n\n... Emission measurement accuracy. (a) Good engineering practice dictates that exhaust emission sample analyzer... resolution read-out systems such as computers, data loggers, etc., can provide sufficient accuracy and...\n\n40 CFR 92.127 - Emission measurement accuracy.\n\nCode of Federal Regulations, 2014 CFR\n\n2014-07-01\n\n... Emission measurement accuracy. (a) Good engineering practice dictates that exhaust emission sample analyzer... resolution read-out systems such as computers, data loggers, etc., can provide sufficient accuracy and...\n\n40 CFR 92.127 - Emission measurement accuracy.\n\nCode of Federal Regulations, 2013 CFR\n\n2013-07-01\n\n... Emission measurement accuracy. (a) Good engineering practice dictates that exhaust emission sample analyzer... resolution read-out systems such as computers, data loggers, etc., can provide sufficient accuracy and...\n\n40 CFR 92.127 - Emission measurement accuracy.\n\nCode of Federal Regulations, 2012 CFR\n\n2012-07-01\n\n... Emission measurement accuracy. (a) Good engineering practice dictates that exhaust emission sample analyzer... resolution read-out systems such as computers, data loggers, etc., can provide sufficient accuracy and...\n\n40 CFR 92.127 - Emission measurement accuracy.\n\nCode of Federal Regulations, 2011 CFR\n\n2011-07-01\n\n... Emission measurement accuracy. (a) Good engineering practice dictates that exhaust emission sample analyzer... resolution read-out systems such as computers, data loggers, etc., can provide sufficient accuracy and...\n\nAn analysis and demonstration of clock synchronization by VLBI\n\nNASA Technical Reports Server (NTRS)\n\nHurd, W. J.\n\n1972-01-01\n\nA prototype of a semireal-time system for synchronizing the DSN station clocks by radio interferometry was successfully demonstrated. The system utilized an approximate maximum likelihood estimation procedure for processing the data, thereby achieving essentially optimum time synchronization estimates for a given amount of data, or equivalently, minimizing the amount of data required for reliable estimation. Synchronization accuracies as good as 100 nsec rms were achieved between DSS 11 and DSS 12, both at Goldstone, California. The accuracy can be improved by increasing the system bandwidth until the fundamental limitations due to position uncertainties of baseline and source and atmospheric effects are reached. These limitations are under ten nsec for transcontinental baselines.\n\nCommunication Accuracy in Magazine Science Reporting.\n\nERIC Educational Resources Information Center\n\nBorman, Susan Cray\n\n1978-01-01\n\nEvaluators with scientific expertise who analyzed the accuracy of popularized science news in mass circulation magazines found that the over-all accuracy of the magazine articles was good, and that the major problem was the omission of relevant information. (GW)\n\nAccuracy of Estimating Solar Radiation Pressure for GEO Debris with Tumbling Effect\n\nNASA Astrophysics Data System (ADS)\n\nChao, Chia-Chun George\n\n2009-03-01\n\nThe accuracy of estimating solar radiation pressure for GEO debris is examined and demonstrated, via numerical simulations, by fitting a batch (months) of simulated position vectors. These simulated position vectors are generated from a \"truth orbit\" with added white noise using high-precision numerical integration tools. After the long-arc fit of the simulated observations (position vectors), one can accurately and reliably determine how close the estimated value of solar radiation pressure is to the truth. Results of this study show that the inherent accuracy in estimating the solar radiation pressure coefficient can be as good as 1% if a long-arc fit span up to 180 days is used and the satellite is not tumbling. The corresponding position prediction accuracy can be as good as, in maximum error, 1 km along in-track, 0.3 km along radial and 0.1 km along cross-track up to 30 days. Similar accuracies can be expected when the object is tumbling as long as the rate of attitude change is different from the orbit rate. Results of this study reveal an important phenomenon that the solar radiation pressure significantly affects the orbit motion when the spin rate is equal to the orbit rate.\n\nThe deployable, inflatable wing technology demonstrator experiment aircraft looks good during a flig\n\nNASA Technical Reports Server (NTRS)\n\n2001-01-01\n\nThe deployable, inflatable wing technology demonstrator experiment aircraft looks good during a flight conducted by the NASA Dryden Flight Research Center, Edwards, California. The inflatable wing project represented a basic flight research effort by Dryden personnel. Three successful flights of the I2000 inflatable wing aircraft occurred. During the flights, the team air-launched the radio-controlled (R/C) I2000 from an R/C utility airplane at an altitude of 800-1000 feet. As the I2000 separated from the carrier aircraft, its inflatable wings 'popped-out,' deploying rapidly via an on-board nitrogen bottle. The aircraft remained stable as it transitioned from wingless to winged flight. The unpowered I2000 glided down to a smooth landing under complete control.\n\nFaith community nursing demonstrates good stewardship of community benefit dollars through cost savings and cost avoidance.\n\nPubMed\n\nBrown, Ameldia R; Coppola, Patricia; Giacona, Marian; Petriches, Anne; Stockwell, Mary Ann\n\n2009-01-01\n\nHealth systems seeking responsible stewardship of community benefit dollars supporting Faith Community Nursing Networks require demonstration of positive measurable health outcomes. Faith Community Nurses (FCNs) answer the call for measurable outcomes by documenting cost savings and cost avoidances to families, communities, and health systems associated with their interventions. Using a spreadsheet tool based on Medicare reimbursements and diagnostic-related groupings, 3 networks of FCNs have together shown more than 600 000 (for calendar year 2008) healthcare dollars saved by avoidance of unnecessary acute care visits and extended care placements. The cost-benefit ratio of support dollars to cost savings and cost avoidance demonstrates that support of FCNs is good stewardship of community benefit dollars.\n\nA Good Neighborhood for Cells: Bioreactor Demonstration System (BDS-05)\n\nNASA Technical Reports Server (NTRS)\n\nChung, Leland W. K.; Goodwin, Thomas J. (Technical Monitor)\n\n2002-01-01\n\nGood neighborhoods help you grow. As with a city, the lives of a cell are governed by its neighborhood connections Connections that do not work are implicated in a range of diseases. One of those connections - between prostate cancer and bone cells - will be studied on STS-107 using the Bioreactor Demonstration System (BDS-05). To improve the prospects for finding novel therapies, and to identify biomarkers that predict disease progression, scientists need tissue models that behave the same as metastatic or spreading cancer. This is one of several NASA-sponsored lines of cell science research that use the microgravity environment of orbit in an attempt to grow lifelike tissue models for health research. As cells replicate, they \"self associate\" to form a complex matrix of collagens, proteins, fibers, and other structures. This highly evolved microenvironment tells each cell who is next door, how it should grow arid into what shapes, and how to respond to bacteria, wounds, and other stimuli. Studying these mechanisms outside the body is difficult because cells do not easily self-associate outside a natural environment. Most cell cultures produce thin, flat specimens that offer limited insight into how cells work together. Ironically, growing cell cultures in the microgravity of space produces cell assemblies that more closely resemble what is found in bodies on Earth. NASA's Bioreactor comprises a miniature life support system and a rotating vessel containing cell specimens in a nutrient medium. Orbital BDS experiments that cultured colon and prostate cancers have been highly promising.\n\nPlasma-Generating Glucose Monitor Accuracy Demonstrated in an Animal Model\n\nPubMed Central\n\nMagarian, Peggy; Sterling, Bernhard\n\n2009-01-01\n\nIntroduction Four randomized controlled trials have compared mortality and morbidity of tight glycemic control versus conventional glucose for intensive care unit (ICU) patients. Two trials showed a positive outcome. However, one single-center trial and a large multicenter trial had negative results. The positive trials used accurate portable lab analyzers. The negative trial allowed the use of meters. The portable analyzer measures in filtered plasma, minimizing the interference effects. OptiScan Biomedical Corporation is developing a continuous glucose monitor using centrifuged plasma and mid-infrared spectroscopy for use in ICU medicine. The OptiScanner draws approximately 0.1 ml of blood every 15 min and creates a centrifuged plasma sample. Internal quality control minimizes sample preparation error. Interference adjustment using this technique has been presented at the Society of Critical Care Medicine in separate studies since 2006. Method A good laboratory practice study was conducted on three Yorkshire pigs using a central venous catheter over 6 h while performing a glucose challenge. Matching Yellow Springs Instrument glucose readings were obtained. Results Some 95.7% of the predicted values were in the Clarke Error Grid A zone and 4.3% in the B zone. Of those in the B zone, all were within 3.3% of the A zone boundaries. The coefficient of determination (R2) was 0.993. The coefficient of variance was 5.02%. Animal necropsy and blood panels demonstrated safety. Conclusion The OptiScanner investigational device performed safely and accurately in an animal model. Human studies using the device will begin soon. PMID:20144396\n\nOverlay accuracy fundamentals\n\nNASA Astrophysics Data System (ADS)\n\nKandel, Daniel; Levinski, Vladimir; Sapiens, Noam; Cohen, Guy; Amit, Eran; Klein, Dana; Vakshtein, Irina\n\n2012-03-01\n\nCurrently, the performance of overlay metrology is evaluated mainly based on random error contributions such as precision and TIS variability. With the expected shrinkage of the overlay metrology budget to < 0.5nm, it becomes crucial to include also systematic error contributions which affect the accuracy of the metrology. Here we discuss fundamental aspects of overlay accuracy and a methodology to improve accuracy significantly. We identify overlay mark imperfections and their interaction with the metrology technology, as the main source of overlay inaccuracy. The most important type of mark imperfection is mark asymmetry. Overlay mark asymmetry leads to a geometrical ambiguity in the definition of overlay, which can be ~1nm or less. It is shown theoretically and in simulations that the metrology may enhance the effect of overlay mark asymmetry significantly and lead to metrology inaccuracy ~10nm, much larger than the geometrical ambiguity. The analysis is carried out for two different overlay metrology technologies: Imaging overlay and DBO (1st order diffraction based overlay). It is demonstrated that the sensitivity of DBO to overlay mark asymmetry is larger than the sensitivity of imaging overlay. Finally, we show that a recently developed measurement quality metric serves as a valuable tool for improving overlay metrology accuracy. Simulation results demonstrate that the accuracy of imaging overlay can be improved significantly by recipe setup optimized using the quality metric. We conclude that imaging overlay metrology, complemented by appropriate use of measurement quality metric, results in optimal overlay accuracy.\n\nGood practices in free-energy calculations.\n\nPubMed\n\nPohorille, Andrew; Jarzynski, Christopher; Chipot, Christophe\n\n2010-08-19\n\nAs access to computational resources continues to increase, free-energy calculations have emerged as a powerful tool that can play a predictive role in a wide range of research areas. Yet, the reliability of these calculations can often be improved significantly if a number of precepts, or good practices, are followed. Although the theory upon which these good practices rely has largely been known for many years, it is often overlooked or simply ignored. In other cases, the theoretical developments are too recent for their potential to be fully grasped and merged into popular platforms for the computation of free-energy differences. In this contribution, the current best practices for carrying out free-energy calculations using free energy perturbation and nonequilibrium work methods are discussed, demonstrating that at little to no additional cost, free-energy estimates could be markedly improved and bounded by meaningful error estimates. Monitoring the probability distributions that underlie the transformation between the states of interest, performing the calculation bidirectionally, stratifying the reaction pathway, and choosing the most appropriate paradigms and algorithms for transforming between states offer significant gains in both accuracy and precision.\n\nRelationship between accuracy and complexity when learning underarm precision throwing.\n\nPubMed\n\nValle, Maria Stella; Lombardo, Luciano; Cioni, Matteo; Casabona, Antonino\n\n2018-06-12\n\nLearning precision ball throwing was mostly studied to explore the early rapid improvement of accuracy, with poor attention on possible adaptive processes occurring later when the rate of improvement is reduced. Here, we tried to demonstrate that the strategy to select angle, speed and height at ball release can be managed during the learning periods following the performance stabilization. To this aim, we used a multivariate linear model with angle, speed and height as predictors of changes in accuracy. Participants performed underarm throws of a tennis ball to hit a target on the floor, 3.42 m away. Two training sessions (S1, S2) and one retention test were executed. Performance accuracy increased over the S1 and stabilized during the S2, with a rate of changes along the throwing axis slower than along the orthogonal axis. However, both the axes contributed to the performance changes over the learning and consolidation time. A stable relationship between the accuracy and the release parameters was observed only during S2, with a good fraction of the performance variance explained by the combination of speed and height. All the variations were maintained during the retention test. Overall, accuracy improvements and reduction in throwing complexity at the ball release followed separate timing over the course of learning and consolidation.\n\nAccuracy of acoustic respiration rate monitoring in pediatric patients.\n\nPubMed\n\nPatino, Mario; Redford, Daniel T; Quigley, Thomas W; Mahmoud, Mohamed; Kurth, C Dean; Szmuk, Peter\n\n2013-12-01\n\nRainbow acoustic monitoring (RRa) utilizes acoustic technology to continuously and noninvasively determine respiratory rate from an adhesive sensor located on the neck. We sought to validate the accuracy of RRa, by comparing it to capnography, impedance pneumography, and to a reference method of counting breaths in postsurgical children. Continuous respiration rate data were recorded from RRa and capnography. In a subset of patients, intermittent respiration rate from thoracic impedance pneumography was also recorded. The reference method, counted respiratory rate by the retrospective analysis of the RRa, and capnographic waveforms while listening to recorded breath sounds were used to compare respiration rate of both capnography and RRa. Bias, precision, and limits of agreement of RRa compared with capnography and RRa and capnography compared with the reference method were calculated. Tolerance and reliability to the acoustic sensor and nasal cannula were also assessed. Thirty-nine of 40 patients (97.5%) demonstrated good tolerance of the acoustic sensor, whereas 25 of 40 patients (62.5%) demonstrated good tolerance of the nasal cannula. Intermittent thoracic impedance produced erroneous respiratory rates (>50Â bÂ·min(-1) from the other methods) on 47% of occasions. The biasÂ Â±Â SD and limits of agreement were -0.30Â Â±Â 3.5Â bÂ·min(-1) and -7.3 to 6.6Â bÂ·min(-1) for RRa compared with capnography; -0.1Â Â±Â 2.5Â bÂ·min(-1) and -5.0 to 5.0Â bÂ·min(-1) for RRa compared with the reference method; and 0.2Â Â±Â 3.4Â bÂ·min(-1) and -6.8 to 6.7Â bÂ·min(-1) for capnography compared with the reference method. When compared to nasal capnography, RRa showed good agreement and similar accuracy and precision but was better tolerated in postsurgical pediatric patients. Â© 2013 John Wiley & Sons Ltd.\n\nSocial Power Increases Interoceptive Accuracy\n\nPubMed Central\n\nMoeini-Jazani, Mehrad; Knoeferle, Klemens; de MoliÃ¨re, Laura; Gatti, Elia; Warlop, Luk\n\n2017-01-01\n\nBuilding on recent psychological research showing that power increases self-focused attention, we propose that having power increases accuracy in perception of bodily signals, a phenomenon known as interoceptive accuracy. Consistent with our proposition, participants in a high-power experimental condition outperformed those in the control and low-power conditions in the Schandry heartbeat-detection task. We demonstrate that the effect of power on interoceptive accuracy is not explained by participantsâ physiological arousal, affective state, or general intention for accuracy. Rather, consistent with our reasoning that experiencing power shifts attentional resources inward, we show that the effect of power on interoceptive accuracy is dependent on individualsâ chronic tendency to focus on their internal sensations. Moreover, we demonstrate that individualsâ chronic sense of power also predicts interoceptive accuracy similar to, and independent of, how their situationally induced feeling of power does. We therefore provide further support on the relation between power and enhanced perception of bodily signals. Our findings offer a novel perspectiveâa psychophysiological accountâon how power might affect judgments and behavior. We highlight and discuss some of these intriguing possibilities for future research. PMID:28824501\n\nMeta-analysis of stratus OCT glaucoma diagnostic accuracy.\n\nPubMed\n\nChen, Hsin-Yi; Chang, Yue-Cune\n\n2014-09-01\n\nTo evaluate the diagnostic accuracy of glaucoma in different stages, different types of glaucoma, and different ethnic groups using Stratus optical coherence tomography (OCT). We searched MEDLINE to identify available articles on diagnostic accuracy of glaucoma published between January 2004 and December 2011. A PubMed (National Center for Biotechnology Information) search using medical subject headings and keywords was executed using the following terms: \"diagnostic accuracy\" or \"receiver operator characteristic\" or \"area under curve\" or \"AUC\" and \"Stratus OCT\" and \"glaucoma.\" The search was subsequently limited to publications in English. The area under a receiver operator characteristic (AUC) curve was used to measure the diagnostic performance. A random-effects model was used to estimate the pooled AUC value of the 17 parameters (average retinal nerve fiber layer thickness, temporal quadrant, superior quadrant, nasal quadrant, inferior quadrant, and 1 to 12 o'clock). Meta-regression analysis was used to check the significance of some important factors: (1) glaucoma severity (five stages), (2) glaucoma types (four types), and (3) ethnicity (four categories). The orders of accuracy among those parameters were as follows: average > inferior > superior > 7 o'clock > 6 o'clock > 11 o'clock > 12 o'clock > 1 o'clock > 5 o'clock > nasal > temporal > 2 o'clock > 10 o'clock > 8 o'clock > 9 o'clock > 4 o'clock > 3 o'clock. After adjusting for the effects of age, glaucoma severity, glaucoma types, and ethnicity, the average retinal nerve fiber layer thickness provided highest accuracy compared with the other parameters of OCT. The diagnostic accuracy in Asian populations was significantly lower than that in whites and the other two ethnic types. Stratus OCT demonstrated good diagnostic capability in differentiating glaucomatous from normal eyes. However, we should be more cautious in applying this instrument in Asian groups in glaucoma management.\n\nUsing Language Sample Analysis in Clinical Practice: Measures of Grammatical Accuracy for Identifying Language Impairment in Preschool and School-Aged Children.\n\nPubMed\n\nEisenberg, Sarita; Guo, Ling-Yu\n\n2016-05-01\n\nThis article reviews the existing literature on the diagnostic accuracy of two grammatical accuracy measures for differentiating children with and without language impairment (LI) at preschool and early school age based on language samples. The first measure, the finite verb morphology composite (FVMC), is a narrow grammatical measure that computes children's overall accuracy of four verb tense morphemes. The second measure, percent grammatical utterances (PGU), is a broader grammatical measure that computes children's accuracy in producing grammatical utterances. The extant studies show that FVMC demonstrates acceptable (i.e., 80 to 89% accurate) to good (i.e., 90% accurate or higher) diagnostic accuracy for children between 4;0 (years;months) and 6;11 in conversational or narrative samples. In contrast, PGU yields acceptable to good diagnostic accuracy for children between 3;0 and 8;11 regardless of sample types. Given the diagnostic accuracy shown in the literature, we suggest that FVMC and PGU can be used as one piece of evidence for identifying children with LI in assessment when appropriate. However, FVMC or PGU should not be used as therapy goals directly. Instead, when children are low in FVMC or PGU, we suggest that follow-up analyses should be conducted to determine the verb tense morphemes or grammatical structures that children have difficulty with. Thieme Medical Publishers 333 Seventh Avenue, New York, NY 10001, USA.\n\n40 CFR 89.310 - Analyzer accuracy and specifications.\n\nCode of Federal Regulations, 2010 CFR\n\n2010-07-01\n\n... to Â§ 89.323. (c) Emission measurement accuracyâBag sampling. (1) Good engineering practice dictates... generally not be used. (2) Some high resolution read-out systems, such as computers, data loggers, and so..., using good engineering judgement, below 15 percent of full scale are made to ensure the accuracy of the...\n\n40 CFR 89.310 - Analyzer accuracy and specifications.\n\nCode of Federal Regulations, 2011 CFR\n\n2011-07-01\n\n... to Â§ 89.323. (c) Emission measurement accuracyâBag sampling. (1) Good engineering practice dictates... generally not be used. (2) Some high resolution read-out systems, such as computers, data loggers, and so..., using good engineering judgement, below 15 percent of full scale are made to ensure the accuracy of the...\n\n40 CFR 89.310 - Analyzer accuracy and specifications.\n\nCode of Federal Regulations, 2013 CFR\n\n2013-07-01\n\n... to Â§ 89.323. (c) Emission measurement accuracyâBag sampling. (1) Good engineering practice dictates... generally not be used. (2) Some high resolution read-out systems, such as computers, data loggers, and so..., using good engineering judgement, below 15 percent of full scale are made to ensure the accuracy of the...\n\n40 CFR 89.310 - Analyzer accuracy and specifications.\n\nCode of Federal Regulations, 2012 CFR\n\n2012-07-01\n\n... to Â§ 89.323. (c) Emission measurement accuracyâBag sampling. (1) Good engineering practice dictates... generally not be used. (2) Some high resolution read-out systems, such as computers, data loggers, and so..., using good engineering judgement, below 15 percent of full scale are made to ensure the accuracy of the...\n\n40 CFR 89.310 - Analyzer accuracy and specifications.\n\nCode of Federal Regulations, 2014 CFR\n\n2014-07-01\n\n... to Â§ 89.323. (c) Emission measurement accuracyâBag sampling. (1) Good engineering practice dictates... generally not be used. (2) Some high resolution read-out systems, such as computers, data loggers, and so..., using good engineering judgement, below 15 percent of full scale are made to ensure the accuracy of the...\n\nGood-to-Great Superintendents: An Examination of Jim Collins' Good-to-Great Level Five Leadership Attributes as Demonstrated by the Leadership Behaviors of Superintendents of High-Performing California Public Single-School Districts\n\nERIC Educational Resources Information Center\n\nBrown, James D.\n\n2010-01-01\n\nPurpose: The purpose of this study was to examine Collins' good-to-great Level Five leadership attributes, as demonstrated by the leadership behaviors of superintendents of high-performing California public single-school districts. Methodology: The researcher used a case study design to conduct this study. Personal interviews were conducted inâ¦\n\nAccuracy of Monte Carlo simulations compared to in-vivo MDCT dosimetry.\n\nPubMed\n\nBostani, Maryam; Mueller, Jonathon W; McMillan, Kyle; Cody, Dianna D; Cagnon, Chris H; DeMarco, John J; McNitt-Gray, Michael F\n\n2015-02-01\n\nThe purpose of this study was to assess the accuracy of a Monte Carlo simulation-based method for estimating radiation dose from multidetector computed tomography (MDCT) by comparing simulated doses in ten patients to in-vivo dose measurements. MD Anderson Cancer Center Institutional Review Board approved the acquisition of in-vivo rectal dose measurements in a pilot study of ten patients undergoing virtual colonoscopy. The dose measurements were obtained by affixing TLD capsules to the inner lumen of rectal catheters. Voxelized patient models were generated from the MDCT images of the ten patients, and the dose to the TLD for all exposures was estimated using Monte Carlo based simulations. The Monte Carlo simulation results were compared to the in-vivo dose measurements to determine accuracy. The calculated mean percent difference between TLD measurements and Monte Carlo simulations was -4.9% with standard deviation of 8.7% and a range of -22.7% to 5.7%. The results of this study demonstrate very good agreement between simulated and measured doses in-vivo. Taken together with previous validation efforts, this work demonstrates that the Monte Carlo simulation methods can provide accurate estimates of radiation dose in patients undergoing CT examinations.\n\nInfluence of non-level walking on pedometer accuracy.\n\nPubMed\n\nLeicht, Anthony S; Crowther, Robert G\n\n2009-05-01\n\nThe YAMAX Digiwalker pedometer has been previously confirmed as a valid and reliable monitor during level walking, however, little is known about its accuracy during non-level walking activities or between genders. Subsequently, this study examined the influence of non-level walking and gender on pedometer accuracy. Forty-six healthy adults completed 3-min bouts of treadmill walking at their normal walking pace during 11 inclines (0-10%) while another 123 healthy adults completed walking up and down 47 stairs. During walking, participants wore a YAMAX Digiwalker SW-700 pedometer with the number of steps taken and registered by the pedometer recorded. Pedometer difference (steps registered-steps taken), net error (% of steps taken), absolute error (absolute % of steps taken) and gender were examined by repeated measures two-way ANOVA and Tukey's post hoc tests. During incline walking, pedometer accuracy indices were similar between inclines and gender except for a significantly greater step difference (-7+/-5 steps vs. 1+/-4 steps) and net error (-2.4+/-1.8% for 9% vs. 0.4+/-1.2% for 2%). Step difference and net error were significantly greater during stair descent compared to stair ascent while absolute error was significantly greater during stair ascent compared to stair descent. The current study demonstrated that the YAMAX Digiwalker SW-700 pedometer exhibited good accuracy during incline walking up to 10% while it overestimated steps taken during stair ascent/descent with greater overestimation during stair descent. Stair walking activity should be documented in field studies as the YAMAX Digiwalker SW-700 pedometer overestimates this activity type.\n\nData accuracy assessment using enterprise architecture\n\nNASA Astrophysics Data System (ADS)\n\nNÃ¤rman, Per; Holm, Hannes; Johnson, Pontus; KÃ¶nig, Johan; Chenine, Moustafa; Ekstedt, Mathias\n\n2011-02-01\n\nErrors in business processes result in poor data accuracy. This article proposes an architecture analysis method which utilises ArchiMate and the Probabilistic Relational Model formalism to model and analyse data accuracy. Since the resources available for architecture analysis are usually quite scarce, the method advocates interviews as the primary data collection technique. A case study demonstrates that the method yields correct data accuracy estimates and is more resource-efficient than a competing sampling-based data accuracy estimation method.\n\nGood and Bad Public Prose.\n\nERIC Educational Resources Information Center\n\nCockburn, Stewart\n\n1969-01-01\n\nThe basic requirements of all good prose are clarity, accuracy, brevity, and simplicity. Especially in public prose--in which the meaning is the crux of the article or speech--concise, vigorous English demands a minimum of adjectives, a maximum use of the active voice, nouns carefully chosen, a logical argument with no labored or obscure points,â¦\n\nContinuous Glucose Monitoring and Trend Accuracy\n\nPubMed Central\n\nGottlieb, Rebecca; Le Compte, Aaron; Chase, J. Geoffrey\n\n2014-01-01\n\nContinuous glucose monitoring (CGM) devices are being increasingly used to monitor glycemia in people with diabetes. One advantage with CGM is the ability to monitor the trend of sensor glucose (SG) over time. However, there are few metrics available for assessing the trend accuracy of CGM devices. The aim of this study was to develop an easy to interpret tool for assessing trend accuracy of CGM data. SG data from CGM were compared to hourly blood glucose (BG) measurements and trend accuracy was quantified using the dot product. Trend accuracy results are displayed on the Trend Compass, which depicts trend accuracy as a function of BG. A trend performance table and Trend Index (TI) metric are also proposed. The Trend Compass was tested using simulated CGM data with varying levels of error and variability, as well as real clinical CGM data. The results show that the Trend Compass is an effective tool for differentiating good trend accuracy from poor trend accuracy, independent of glycemic variability. Furthermore, the real clinical data show that the Trend Compass assesses trend accuracy independent of point bias error. Finally, the importance of assessing trend accuracy as a function of BG level is highlighted in a case example of low and falling BG data, with corresponding rising SG data. This study developed a simple to use tool for quantifying trend accuracy. The resulting trend accuracy is easily interpreted on the Trend Compass plot, and if required, performance table and TI metric. PMID:24876437\n\nCertified ion implantation fluence by high accuracy RBS.\n\nPubMed\n\nColaux, Julien L; Jeynes, Chris; Heasman, Keith C; Gwilliam, Russell M\n\n2015-05-07\n\nFrom measurements over the last two years we have demonstrated that the charge collection system based on Faraday cups can robustly give near-1% absolute implantation fluence accuracy for our electrostatically scanned 200 kV Danfysik ion implanter, using four-point-probe mapping with a demonstrated accuracy of 2%, and accurate Rutherford backscattering spectrometry (RBS) of test implants from our quality assurance programme. The RBS is traceable to the certified reference material IRMM-ERM-EG001/BAM-L001, and involves convenient calibrations both of the electronic gain of the spectrometry system (at about 0.1% accuracy) and of the RBS beam energy (at 0.06% accuracy). We demonstrate that accurate RBS is a definitive method to determine quantity of material. It is therefore useful for certifying high quality reference standards, and is also extensible to other kinds of samples such as thin self-supporting films of pure elements. The more powerful technique of Total-IBA may inherit the accuracy of RBS.\n\nAccuracy of Monte Carlo simulations compared to in-vivo MDCT dosimetry\n\nDOE Office of Scientific and Technical Information (OSTI.GOV)\n\nBostani, Maryam, E-mail: mbostani@mednet.ucla.edu; McMillan, Kyle; Cagnon, Chris H.\n\nPurpose: The purpose of this study was to assess the accuracy of a Monte Carlo simulation-based method for estimating radiation dose from multidetector computed tomography (MDCT) by comparing simulated doses in ten patients to in-vivo dose measurements. Methods: MD Anderson Cancer Center Institutional Review Board approved the acquisition of in-vivo rectal dose measurements in a pilot study of ten patients undergoing virtual colonoscopy. The dose measurements were obtained by affixing TLD capsules to the inner lumen of rectal catheters. Voxelized patient models were generated from the MDCT images of the ten patients, and the dose to the TLD for allmoreÂ Â» exposures was estimated using Monte Carlo based simulations. The Monte Carlo simulation results were compared to the in-vivo dose measurements to determine accuracy. Results: The calculated mean percent difference between TLD measurements and Monte Carlo simulations was â4.9% with standard deviation of 8.7% and a range of â22.7% to 5.7%. Conclusions: The results of this study demonstrate very good agreement between simulated and measured doses in-vivo. Taken together with previous validation efforts, this work demonstrates that the Monte Carlo simulation methods can provide accurate estimates of radiation dose in patients undergoing CT examinations.Â«Â less\n\nAn analysis and demonstration of clock synchronization by VLBI. [Very Long Baseline Interferometry for Deep Space Net\n\nNASA Technical Reports Server (NTRS)\n\nHurd, W. J.\n\n1974-01-01\n\nA prototype of a semi-real time system for synchronizing the Deep Space Net station clocks by radio interferometry was successfully demonstrated on August 30, 1972. The system utilized an approximate maximum likelihood estimation procedure for processing the data, thereby achieving essentially optimum time sync estimates for a given amount of data, or equivalently, minimizing the amount of data required for reliable estimation. Synchronization accuracies as good as 100 ns rms were achieved between Deep Space Stations 11 and 12, both at Goldstone, Calif. The accuracy can be improved by increasing the system bandwidth until the fundamental limitations due to baseline and source position uncertainties and atmospheric effects are reached. These limitations are under 10 ns for transcontinental baselines.\n\nDemonstrating High-Accuracy Orbital Access Using Open-Source Tools\n\nNASA Technical Reports Server (NTRS)\n\nGilbertson, Christian; Welch, Bryan\n\n2017-01-01\n\nOrbit propagation is fundamental to almost every space-based analysis. Currently, many system analysts use commercial software to predict the future positions of orbiting satellites. This is one of many capabilities that can replicated, with great accuracy, without using expensive, proprietary software. NASAs SCaN (Space Communication and Navigation) Center for Engineering, Networks, Integration, and Communications (SCENIC) project plans to provide its analysis capabilities using a combination of internal and open-source software, allowing for a much greater measure of customization and flexibility, while reducing recurring software license costs. MATLAB and the open-source Orbit Determination Toolbox created by Goddard Space Flight Center (GSFC) were utilized to develop tools with the capability to propagate orbits, perform line-of-sight (LOS) availability analyses, and visualize the results. The developed programs are modular and can be applied for mission planning and viability analysis in a variety of Solar System applications. The tools can perform 2 and N-body orbit propagation, find inter-satellite and satellite to ground station LOS access (accounting for intermediate oblate spheroid body blocking, geometric restrictions of the antenna field-of-view (FOV), and relativistic corrections), and create animations of planetary movement, satellite orbits, and LOS accesses. The code is the basis for SCENICs broad analysis capabilities including dynamic link analysis, dilution-of-precision navigation analysis, and orbital availability calculations.\n\nReliability and accuracy analysis of a new semiautomatic radiographic measurement software in adult scoliosis.\n\nPubMed\n\nAubin, Carl-Eric; Bellefleur, Christian; Joncas, Julie; de Lanauze, Dominic; Kadoury, Samuel; Blanke, Kathy; Parent, Stefan; Labelle, Hubert\n\n2011-05-20\n\nRadiographic software measurement analysis in adult scoliosis. To assess the accuracy as well as the intra- and interobserver reliability of measuring different indices on preoperative adult scoliosis radiographs using a novel measurement software that includes a calibration procedure and semiautomatic features to facilitate the measurement process. Scoliosis requires a careful radiographic evaluation to assess the deformity. Manual and computer radiographic process measures have been studied extensively to determine the reliability and reproducibility in adolescent idiopathic scoliosis. Most studies rely on comparing given measurements, which are repeated by the same user or by an expert user. A given measure with a small intra- or interobserver error might be deemed as good repeatability, but all measurements might not be truly accurate because the ground-truth value is often unknown. Thorough accuracy assessment of radiographic measures is necessary to assess scoliotic deformities, compare these measures at different stages or to permit valid multicenter studies. Thirty-four sets of adult scoliosis digital radiographs were measured two times by three independent observers using a novel radiographic measurement software that includes semiautomatic features to facilitate the measurement process. Twenty different measures taken from the Spinal Deformity Study Group radiographic measurement manual were performed on the coronal and sagittal images. Intra- and intermeasurer reliability for each measure was assessed. The accuracy of the measurement software was also assessed using a physical spine model in six different scoliotic configurations as a true reference. The majority of the measures demonstrated good to excellent intra- and intermeasurer reliability, except for sacral obliquity. The standard variation of all the measures was very small: â¤ 4.2Â° for Cobb angles, â¤ 4.2Â° for the kyphosis, â¤ 5.7Â° for the lordosis, â¤ 3.9Â° for the pelvic angles, and â\n\nAccuracy versus transparency in pharmacoeconomic modelling: finding the right balance.\n\nPubMed\n\nEddy, David M\n\n2006-01-01\n\nAs modellers push to make their models more accurate, the ability of others to understand the models can decrease, causing the models to lose transparency. When this type of conflict between accuracy and transparency occurs, the question arises, \"Where do we want to operate on that spectrum?\" This paper argues that in such cases we should give absolute priority to accuracy: push for whatever degree of accuracy is needed to answer the question being asked, try to maximise transparency within that constraint, and find other ways to replace what we wanted to get from transparency. There are several reasons. The fundamental purpose of a model is to help us get the right answer to a question and, by any measure, the expected value of a model is proportional to its accuracy. Ironically, we use transparency as a way to judge accuracy. But transparency is not a very powerful or useful way to do this. It rarely enables us to actually replicate the model's results and, even if we could, replication would not tell us the model's accuracy. Transparency rarely provides even face validity; from the content expert's perspective, the simplifications that modellers have to make usually raise more questions than they answer. Transparency does enable modellers to alert users to weaknesses in their models, but that can be achieved simply by listing the model's limitations and does not get us any closer to real accuracy. Sensitivity analysis tests the importance of uncertainty about the variables in a model, but does not tell us about the variables that were omitted or the structure of the model. What people really want to know is whether a model actually works. Transparency by itself can't answer this; only demonstrations that the model accurately calculates or predicts real events can. Rigorous simulations of clinical trials are a good place to start. This is the type of empirical validation we need to provide if the potential of mathematical models in pharmacoeconomics is to be\n\nAnalysis article: accuracy of the DIDGET glucose meter in children and young adults with diabetes.\n\nPubMed\n\nKim, Sarah\n\n2011-09-01\n\nDiabetes is one of the most common chronic diseases among American children. Although studies show that intensive management, including frequent glucose testing, improves diabetes control, this is difficult to accomplish. Bayer's DIDGETÂ® glucose meter system pairs with a popular handheld video game system and couples good blood glucose testing habits with video-game-based rewards. In this issue, Deeb and colleagues performed a study demonstrating the accuracy of the DIDGET meter, a critical asset to this novel product designed to alleviate some of the challenges of managing pediatric diabetes. Â© 2011 Diabetes Technology Society.\n\nGood Agreements Make Good Friends\n\nPubMed Central\n\nHan, The Anh; Pereira, LuÃ­s Moniz; Santos, Francisco C.; Lenaerts, Tom\n\n2013-01-01\n\nWhen starting a new collaborative endeavor, it pays to establish upfront how strongly your partner commits to the common goal and what compensation can be expected in case the collaboration is violated. Diverse examples in biological and social contexts have demonstrated the pervasiveness of making prior agreements on posterior compensations, suggesting that this behavior could have been shaped by natural selection. Here, we analyze the evolutionary relevance of such a commitment strategy and relate it to the costly punishment strategy, where no prior agreements are made. We show that when the cost of arranging a commitment deal lies within certain limits, substantial levels of cooperation can be achieved. Moreover, these levels are higher than that achieved by simple costly punishment, especially when one insists on sharing the arrangement cost. Not only do we show that good agreements make good friends, agreements based on shared costs result in even better outcomes. PMID:24045873\n\nMagnetic resonance enterography has good inter-rater agreement and diagnostic accuracy for detecting inflammation in pediatric Crohn disease.\n\nPubMed\n\nChurch, Peter C; Greer, Mary-Louise C; Cytter-Kuint, Ruth; Doria, Andrea S; Griffiths, Anne M; Turner, Dan; Walters, Thomas D; Feldman, Brian M\n\n2017-05-01\n\nMagnetic resonance enterography (MRE) is increasingly relied upon for noninvasive assessment of intestinal inflammation in Crohn disease. However very few studies have examined the diagnostic accuracy of individual MRE signs in children. We have created an MR-based multi-item measure of intestinal inflammation in children with Crohn disease - the Pediatric Inflammatory Crohn's MRE Index (PICMI). To inform item selection for this instrument, we explored the inter-rater agreement and diagnostic accuracy of individual MRE signs of inflammation in pediatric Crohn disease and compared our findings with the reference standards of the weighted Pediatric Crohn's Disease Activity Index (wPCDAI) and C-reactive protein (CRP). In this cross-sectional single-center study, MRE studies in 48 children with diagnosed Crohn disease (66% male, median age 15.5Â years) were reviewed by two independent radiologists for the presence of 15 MRE signs of inflammation. Using kappa statistics we explored inter-rater agreement for each MRE sign across 10 anatomical segments of the gastrointestinal tract. We correlated MRE signs with the reference standards using correlation coefficients. Radiologists measured the length of inflamed bowel in each segment of the gastrointestinal tract. In each segment, MRE signs were scored as either binary (0-absent, 1-present), or ordinal (0-absent, 1-mild, 2-marked). These segmental scores were weighted by the length of involved bowel and were summed to produce a weighted score per patient for each MRE sign. Using a combination of wPCDAIâ¥12.5 and CRPâ¥5 to define active inflammation, we calculated area under the receiver operating characteristic curve (AUC) for each weighted MRE sign. Bowel wall enhancement, wall T2 hyperintensity, wall thickening and wall diffusion-weighted imaging (DWI) hyperintensity were most commonly identified. Inter-rater agreement was best for decreased motility and wall DWI hyperintensity (kappaâ¥0.64). Correlation between MRE\n\nMatters of accuracy and conventionality: prior accuracy guides children's evaluations of others' actions.\n\nPubMed\n\nScofield, Jason; Gilpin, Ansley Tullos; Pierucci, Jillian; Morgan, Reed\n\n2013-03-01\n\nStudies show that children trust previously reliable sources over previously unreliable ones (e.g., Koenig, ClÃ©ment, & Harris, 2004). However, it is unclear from these studies whether children rely on accuracy or conventionality to determine the reliability and, ultimately, the trustworthiness of a particular source. In the current study, 3- and 4-year-olds were asked to endorse and imitate one of two actors performing an unfamiliar action, one actor who was unconventional but successful and one who was conventional but unsuccessful. These data demonstrated that children preferred endorsing and imitating the unconventional but successful actor. Results suggest that when the accuracy and conventionality of a source are put into conflict, children may give priority to accuracy over conventionality when estimating the source's reliability and, ultimately, when deciding who to trust.\n\nMonte Carlo evaluation of accuracy and noise properties of two scatter correction methods for /sup 201/Tl cardiac SPECT\n\nNASA Astrophysics Data System (ADS)\n\nNarita, Y.; Iida, H.; Ebert, S.; Nakamura, T.\n\n1997-12-01\n\nTwo independent scatter correction techniques, transmission dependent convolution subtraction (TDCS) and triple-energy window (TEW) method, were evaluated in terms of quantitative accuracy and noise properties using Monte Carlo simulation (EGS4). Emission projections (primary, scatter and scatter plus primary) were simulated for three numerical phantoms for /sup 201/Tl. Data were reconstructed with ordered-subset EM algorithm including noise-less transmission data based attenuation correction. Accuracy of TDCS and TEW scatter corrections were assessed by comparison with simulated true primary data. The uniform cylindrical phantom simulation demonstrated better quantitative accuracy with TDCS than with TEW (-2.0% vs. 16.7%) and better S/N (6.48 vs. 5.05). A uniform ring myocardial phantom simulation demonstrated better homogeneity with TDCS than TEW in the myocardium; i.e., anterior-to-posterior wall count ratios were 0.99 and 0.76 with TDCS and TEW, respectively. For the MCAT phantom, TDCS provided good visual and quantitative agreement with simulated true primary image without noticeably increasing the noise after scatter correction. Overall TDCS proved to be more accurate and less noisy than TEW, facilitating quantitative assessment of physiological functions with SPECT.\n\nThe role of feedback and differences between good and poor decoders in a repeated word reading paradigm in first grade.\n\nPubMed\n\nvan Gorp, Karly; Segers, Eliane; Verhoeven, Ludo\n\n2017-04-01\n\nThe direct, retention, and transfer effects of repeated word and pseudoword reading were studied in a pretest, training, posttest, retention design. First graders (48 good readers, 47 poor readers) read 25 CVC words and 25 CVC pseudowords in ten repeated word reading sessions, preceded and followed by a transfer task with a different set of items. Two weeks after training, trained items were assessed again in a retention test. Participants either received phonics feedback, in which each word was spelled out and repeated; word feedback, in which each word was repeated; or no feedback. During the training, both good and poor readers improved in accuracy and speed. The increase in speed was stronger for poor readers than for good readers. The good readers demonstrated a stronger increase for pseudowords than for words. This increase in speed was most prominent in the first four sessions. Two weeks after training, the levels of accuracy and speed were retained. Furthermore, transfer effects on speed were found for pseudowords in both groups of readers. Good readers performed most accurately during the training when they received no feedback while poor readers performed most accurately during the training with the help of phonics feedback. However, feedback did not differentiate for reading speed or for effects after the training. The effects of repeated word reading were found to be stronger for poor readers than for good readers. Moreover, these effects were found to be stronger for pseudowords than for words. This indicates that repeated word reading can be seen as an important trigger for the improvement of decoding skills.\n\nThe Role of Pattern Goodness in the Reproduction of Backward Masked Patterns\n\nERIC Educational Resources Information Center\n\nBell, Herbert H.; Handel, Stephen\n\n1976-01-01\n\nThe purpose of the present work was to investigate the relation between pattern goodness and accuracy of reproduction in backward masking. It may be hypothesized that good patterns, being easier to encode as wholes, will be reproduced more easily than poorer patterns. Four experiments were performed. (Author)\n\nAccuracy testing of steel and electric groundwater-level measuring tapes: Test method and in-service tape accuracy\n\nUSGS Publications Warehouse\n\nFulford, Janice M.; Clayton, Christopher S.\n\n2015-10-09\n\nThe calibration device and proposed method were used to calibrate a sample of in-service USGS steel and electric groundwater tapes. The sample of in-service groundwater steel tapes were in relatively good condition. All steel tapes, except one, were accurate to Â±0.01 ft per 100 ft over their entire length. One steel tape, which had obvious damage in the first hundred feet, was marginally outside the accuracy of Â±0.01 ft per 100 ft by 0.001 ft. The sample of in-service groundwater-level electric tapes were in a range of conditionsâfrom like new, with cosmetic damage, to nonfunctional. The in-service electric tapes did not meet the USGS accuracy recommendation of Â±0.01 ft. In-service electric tapes, except for the nonfunctional tape, were accurate to about Â±0.03 ft per 100 ft. A comparison of new with in-service electric tapes found that steel-core electric tapes maintained their length and accuracy better than electric tapes without a steel core. The in-service steel tapes could be used as is and achieve USGS accuracy recommendations for groundwater-level measurements. The in-service electric tapes require tape corrections to achieve USGS accuracy recommendations for groundwater-level measurement.\n\nDiagnostic accuracy of eye movements in assessing pedophilia.\n\nPubMed\n\nFromberger, Peter; Jordan, Kirsten; Steinkrauss, Henrike; von Herder, Jakob; Witzel, Joachim; Stolpmann, Georg; KrÃ¶ner-Herwig, Birgit; MÃ¼ller, JÃ¼rgen Leo\n\n2012-07-01\n\nGiven that recurrent sexual interest in prepubescent children is one of the strongest single predictors for pedosexual offense recidivism, valid and reliable diagnosis of pedophilia is of particular importance. Nevertheless, current assessment methods still fail to fulfill psychometric quality criteria. The aim of the study was to evaluate the diagnostic accuracy of eye-movement parameters in regard to pedophilic sexual preferences. Eye movements were measured while 22 pedophiles (according to ICD-10 F65.4 diagnosis), 8 non-pedophilic forensic controls, and 52 healthy controls simultaneously viewed the picture of a child and the picture of an adult. Fixation latency was assessed as a parameter for automatic attentional processes and relative fixation time to account for controlled attentional processes. Receiver operating characteristic (ROC) analyses, which are based on calculated age-preference indices, were carried out to determine the classifier performance. Cross-validation using the leave-one-out method was used to test the validity of classifiers. Pedophiles showed significantly shorter fixation latencies and significantly longer relative fixation times for child stimuli than either of the control groups. Classifier performance analysis revealed an area under the curve (AUC)â=â0.902 for fixation latency and an AUCâ=â0.828 for relative fixation time. The eye-tracking method based on fixation latency discriminated between pedophiles and non-pedophiles with a sensitivity of 86.4% and a specificity of 90.0%. Cross-validation demonstrated good validity of eye-movement parameters. Despite some methodological limitations, measuring eye movements seems to be a promising approach to assess deviant pedophilic interests. Eye movements, which represent automatic attentional processes, demonstrated high diagnostic accuracy. Â© 2012 International Society for Sexual Medicine.\n\nOntario multidetector computed tomographic coronary angiography study: field evaluation of diagnostic accuracy.\n\nPubMed\n\nChow, Benjamin J W; Freeman, Michael R; Bowen, James M; Levin, Leslie; Hopkins, Robert B; Provost, Yves; Tarride, Jean-Eric; Dennie, Carole; Cohen, Eric A; Marcuzzi, Dan; Iwanochko, Robert; Moody, Alan R; Paul, Narinder; Parker, John D; O'Reilly, Daria J; Xie, Feng; Goeree, Ron\n\n2011-06-13\n\nComputed tomographic coronary angiography (CTCA) has gained clinical acceptance for the detection of obstructive coronary artery disease. Although single-center studies have demonstrated excellent accuracy, multicenter studies have yielded variable results. The true diagnostic accuracy of CTCA in the \"real world\" remains uncertain. We conducted a field evaluation comparing multidetector CTCA with invasive CA (ICA) to understand CTCA's diagnostic accuracy in a real-world setting. A multicenter cohort study of patients awaiting ICA was conducted between September 2006 and June 2009. All patients had either a low or an intermediate pretest probability for coronary artery disease and underwent CTCA and ICA within 10 days. The results of CTCA and ICA were interpreted visually by local expert observers who were blinded to all clinical data and imaging results. Using a patient-based analysis (diameter stenosis â¥50%) of 169 patients, the sensitivity, specificity, positive predictive value, and negative predictive value were 81.3% (95% confidence interval [CI], 71.0%-89.1%), 93.3% (95% CI, 85.9%-97.5%), 91.6% (95% CI, 82.5%-96.8%), and 84.7% (95% CI, 76.0%-91.2%), respectively; the area under receiver operating characteristic curve was 0.873. The diagnostic accuracy varied across centers (P < .001), with a sensitivity, specificity, positive predictive value, and negative predictive value ranging from 50.0% to 93.2%, 92.0% to 100%, 84.6% to 100%, and 42.9% to 94.7%, respectively. Compared with ICA, CTCA appears to have good accuracy; however, there was variability in diagnostic accuracy across centers. Factors affecting institutional variability need to be better understood before CTCA is universally adopted. Additional real-world evaluations are needed to fully understand the impact of CTCA on clinical care. clinicaltrials.gov Identifier: NCT00371891.\n\nAccuracy testing of electric groundwater-level measurement tapes\n\nUSGS Publications Warehouse\n\nJelinski, Jim; Clayton, Christopher S.; Fulford, Janice M.\n\n2015-01-01\n\nThe accuracy tests demonstrated that none of the electric-tape models tested consistently met the suggested USGS accuracy of Â±0.01 ft. The test data show that the tape models in the study should give a water-level measurement that is accurate to roughly Â±0.05 ft per 100 ft without additional calibration. To meet USGS accuracy guidelines, the electric-tape models tested will need to be individually calibrated. Specific conductance also plays a part in tape accuracy. The probes will not work in water with specific conductance values near zero, and the accuracy of one probe was unreliable in very high conductivity water (10,000 microsiemens per centimeter).\n\nDiagnostic Accuracy Assessment of Sensititre and Agar Disk Diffusion for Determining Antimicrobial Resistance Profiles of Bovine Clinical Mastitis Pathogensâ¿\n\nPubMed Central\n\nSaini, V.; Riekerink, R. G. M. Olde; McClure, J. T.; Barkema, H. W.\n\n2011-01-01\n\nDetermining the accuracy and precision of a measuring instrument is pertinent in antimicrobial susceptibility testing. This study was conducted to predict the diagnostic accuracy of the Sensititre MIC mastitis panel (Sensititre) and agar disk diffusion (ADD) method with reference to the manual broth microdilution test method for antimicrobial resistance profiling of Escherichia coli (n = 156), Staphylococcus aureus (n = 154), streptococcal (n = 116), and enterococcal (n = 31) bovine clinical mastitis isolates. The activities of ampicillin, ceftiofur, cephalothin, erythromycin, oxacillin, penicillin, the penicillin-novobiocin combination, pirlimycin, and tetracycline were tested against the isolates. Diagnostic accuracy was determined by estimating the area under the receiver operating characteristic curve; intertest essential and categorical agreements were determined as well. Sensititre and the ADD method demonstrated moderate to highly accurate (71 to 99%) and moderate to perfect (71 to 100%) predictive accuracies for 74 and 76% of the isolate-antimicrobial MIC combinations, respectively. However, the diagnostic accuracy was low for S. aureus-ceftiofur/oxacillin combinations and other streptococcus-ampicillin combinations by either testing method. Essential agreement between Sensititre automatic MIC readings and MIC readings obtained by the broth microdilution test method was 87%. Essential agreement between Sensititre automatic and manual MIC reading methods was 97%. Furthermore, the ADD test method and Sensititre MIC method exhibited 92 and 91% categorical agreement (sensitive, intermediate, resistant) of results, respectively, compared with the reference method. However, both methods demonstrated lower agreement for E. coli-ampicillin/cephalothin combinations than for Gram-positive isolates. In conclusion, the Sensititre and ADD methods had moderate to high diagnostic accuracy and very good essential and categorical agreement for most udder pathogen\n\nStory Goodness in Adolescents with Autism Spectrum Disorder (ASD) and in Optimal Outcomes from ASD\n\nERIC Educational Resources Information Center\n\nCanfield, Allison R.; Eigsti, Inge-Marie; de Marchena, Ashley; Fein, Deborah\n\n2016-01-01\n\nPurpose: This study examined narrative quality of adolescents with autism spectrum disorder (ASD) using a well-studied \"story goodness\" coding system. Method: Narrative samples were analyzed for distinct aspects of story goodness and rated by naÃ¯ve readers on dimensions of story goodness, accuracy, cohesiveness, and oddness. Adolescentsâ¦\n\n40 CFR 86.1338-84 - Emission measurement accuracy.\n\nCode of Federal Regulations, 2010 CFR\n\n2010-07-01\n\n... engineering practice dictates that exhaust emission sample analyzer readings below 15 percent of full scale... computers, data loggers, etc., can provide sufficient accuracy and resolution below 15 percent of full scale... spaced points, using good engineering judgement, below 15 percent of full scale are made to ensure the...\n\n40 CFR 86.1338-84 - Emission measurement accuracy.\n\nCode of Federal Regulations, 2013 CFR\n\n2013-07-01\n\n... engineering practice dictates that exhaust emission sample analyzer readings below 15 percent of full scale... computers, data loggers, etc., can provide sufficient accuracy and resolution below 15 percent of full scale... spaced points, using good engineering judgement, below 15 percent of full scale are made to ensure the...\n\n40 CFR 86.1338-84 - Emission measurement accuracy.\n\nCode of Federal Regulations, 2012 CFR\n\n2012-07-01\n\n... engineering practice dictates that exhaust emission sample analyzer readings below 15 percent of full scale... computers, data loggers, etc., can provide sufficient accuracy and resolution below 15 percent of full scale... spaced points, using good engineering judgement, below 15 percent of full scale are made to ensure the...\n\n40 CFR 86.1338-84 - Emission measurement accuracy.\n\nCode of Federal Regulations, 2011 CFR\n\n2011-07-01\n\n... engineering practice dictates that exhaust emission sample analyzer readings below 15 percent of full scale... computers, data loggers, etc., can provide sufficient accuracy and resolution below 15 percent of full scale... spaced points, using good engineering judgement, below 15 percent of full scale are made to ensure the...\n\nTracking accuracy assessment for concentrator photovoltaic systems\n\nNASA Astrophysics Data System (ADS)\n\nNorton, Matthew S. H.; Anstey, Ben; Bentley, Roger W.; Georghiou, George E.\n\n2010-10-01\n\nThe accuracy to which a concentrator photovoltaic (CPV) system can track the sun is an important parameter that influences a number of measurements that indicate the performance efficiency of the system. This paper presents work carried out into determining the tracking accuracy of a CPV system, and illustrates the steps involved in gaining an understanding of the tracking accuracy. A Trac-Stat SL1 accuracy monitor has been used in the determination of pointing accuracy and has been integrated into the outdoor CPV module test facility at the Photovoltaic Technology Laboratories in Nicosia, Cyprus. Results from this work are provided to demonstrate how important performance indicators may be presented, and how the reliability of results is improved through the deployment of such accuracy monitors. Finally, recommendations on the use of such sensors are provided as a means to improve the interpretation of real outdoor performance.\n\nAn anthropomorphic abdominal phantom for deformable image registration accuracy validation in adaptive radiation therapy.\n\nPubMed\n\nLiao, Yuliang; Wang, Linjing; Xu, Xiangdong; Chen, Haibin; Chen, Jiawei; Zhang, Guoqian; Lei, Huaiyu; Wang, Ruihao; Zhang, Shuxu; Gu, Xuejun; Zhen, Xin; Zhou, Linghong\n\n2017-06-01\n\nTo design and construct a three-dimensional (3D) anthropomorphic abdominal phantom for geometric accuracy and dose summation accuracy evaluations of deformable image registration (DIR) algorithms for adaptive radiation therapy (ART). Organ molds, including liver, kidney, spleen, stomach, vertebra, and two metastasis tumors, were 3D printed using contours from an ovarian cancer patient. The organ molds were molded with deformable gels made of different mixtures of polyvinyl chloride (PVC) and the softener dioctyl terephthalate. Gels with different densities were obtained by a polynomial fitting curve that described the relation between the Hounsfield unit (HU) and PVC-softener blending ratio. The rigid vertebras were constructed by molding of white cement and cellulose pulp. The final abdominal phantom was assembled by arranging all the fabricated organs inside a hollow dummy according to their anatomies, and sealed by deformable gel with averaged HU of muscle and fat. Fiducial landmarks were embedded inside the phantom for spatial accuracy and dose accumulation accuracy studies. Two channels were excavated to facilitate ionization chamber insertion for dosimetric measurements. Phantom properties such as deformable gel elasticity and HU stability were studied. The dosimetric measurement accuracy in the phantom was performed, and the DIR accuracies of three DIR algorithms available in the open source DIR toolkit-DIRART were also validated. The constructed deformable gel showed elastic behavior and was stable in HU values over times, proving to be a practical material for the deformable phantom. The constructed abdominal phantom consisted of realistic anatomies in terms of both anatomical shapes and densities when compared with its reference patient. The dosimetric measurements showed a good agreement with the calculated doses from the treatment planning system. Fiducial-based accuracy analysis conducted on the constructed phantom demonstrated the feasibility of\n\nContrast-enhanced small-animal PET/CT in cancer research: strong improvement of diagnostic accuracy without significant alteration of quantitative accuracy and NEMA NU 4-2008 image quality parameters.\n\nPubMed\n\nLasnon, Charline; Quak, Elske; Briand, MÃ©lanie; Gu, Zheng; Louis, Marie-HÃ©lÃ¨ne; Aide, Nicolas\n\n2013-01-17\n\nThe use of iodinated contrast media in small-animal positron emission tomography (PET)/computed tomography (CT) could improve anatomic referencing and tumor delineation but may introduce inaccuracies in the attenuation correction of the PET images. This study evaluated the diagnostic performance and accuracy of quantitative values in contrast-enhanced small-animal PET/CT (CEPET/CT) as compared to unenhanced small animal PET/CT (UEPET/CT). Firstly, a NEMA NU 4-2008 phantom (filled with 18F-FDG or 18F-FDG plus contrast media) and a homemade phantom, mimicking an abdominal tumor surrounded by water or contrast media, were used to evaluate the impact of iodinated contrast media on the image quality parameters and accuracy of quantitative values for a pertinent-sized target. Secondly, two studies in 22 abdominal tumor-bearing mice and rats were performed. The first animal experiment studied the impact of a dual-contrast media protocol, comprising the intravenous injection of a long-lasting contrast agent mixed with 18F-FDG and the intraperitoneal injection of contrast media, on tumor delineation and the accuracy of quantitative values. The second animal experiment compared the diagnostic performance and quantitative values of CEPET/CT versus UEPET/CT by sacrificing the animals after the tracer uptake period and imaging them before and after intraperitoneal injection of contrast media. There was minimal impact on IQ parameters (%SDunif and spillover ratios in air and water) when the NEMA NU 4-2008 phantom was filled with 18F-FDG plus contrast media. In the homemade phantom, measured activity was similar to true activity (-0.02%) and overestimated by 10.30% when vials were surrounded by water or by an iodine solution, respectively. The first animal experiment showed excellent tumor delineation and a good correlation between small-animal (SA)-PET and ex vivo quantification (r2 = 0.87, P < 0.0001). The second animal experiment showed a good correlation between CEPET/CT and\n\nMethodological quality of diagnostic accuracy studies on non-invasive coronary CT angiography: influence of QUADAS (Quality Assessment of Diagnostic Accuracy Studies included in systematic reviews) items on sensitivity and specificity.\n\nPubMed\n\nSchueler, Sabine; Walther, Stefan; Schuetz, Georg M; Schlattmann, Peter; Dewey, Marc\n\n2013-06-01\n\nTo evaluate the methodological quality of diagnostic accuracy studies on coronary computed tomography (CT) angiography using the QUADAS (Quality Assessment of Diagnostic Accuracy Studies included in systematic reviews) tool. Each QUADAS item was individually defined to adapt it to the special requirements of studies on coronary CT angiography. Two independent investigators analysed 118 studies using 12 QUADAS items. Meta-regression and pooled analyses were performed to identify possible effects of methodological quality items on estimates of diagnostic accuracy. The overall methodological quality of coronary CT studies was merely moderate. They fulfilled a median of 7.5 out of 12 items. Only 9 of the 118 studies fulfilled more than 75 % of possible QUADAS items. One QUADAS item (\"Uninterpretable Results\") showed a significant influence (P = 0.02) on estimates of diagnostic accuracy with \"no fulfilment\" increasing specificity from 86 to 90 %. Furthermore, pooled analysis revealed that each QUADAS item that is not fulfilled has the potential to change estimates of diagnostic accuracy. The methodological quality of studies investigating the diagnostic accuracy of non-invasive coronary CT is only moderate and was found to affect the sensitivity and specificity. An improvement is highly desirable because good methodology is crucial for adequately assessing imaging technologies. â¢ Good methodological quality is a basic requirement in diagnostic accuracy studies. â¢ Most coronary CT angiography studies have only been of moderate design quality. â¢ Weak methodological quality will affect the sensitivity and specificity. â¢ No improvement in methodological quality was observed over time. â¢ Authors should consider the QUADAS checklist when undertaking accuracy studies.\n\nMeasurement system with high accuracy for laser beam quality.\n\nPubMed\n\nKe, Yi; Zeng, Ciling; Xie, Peiyuan; Jiang, Qingshan; Liang, Ke; Yang, Zhenyu; Zhao, Ming\n\n2015-05-20\n\nPresently, most of the laser beam quality measurement system collimates the optical path manually with low efficiency and low repeatability. To solve these problems, this paper proposed a new collimated method to improve the reliability and accuracy of the measurement results. The system accuracy controlled the position of the mirror to change laser beam propagation direction, which can realize the beam perpendicularly incident to the photosurface of camera. The experiment results show that the proposed system has good repeatability and the measuring deviation of M2 factor is less than 0.6%.\n\nDidactic satellite based on Android platform for space operation demonstration and development\n\nNASA Astrophysics Data System (ADS)\n\nBen Bahri, Omar; Besbes, Kamel\n\n2018-03-01\n\nSpace technology plays a pivotal role in society development. It offers new methods for telemetry, monitoring and control. However, this sector requires training, research and skills development but the lack of instruments, materials and budgets affects the ambiguity to understand satellite technology. The objective of this paper is to describe a demonstration prototype of a smart phone device for space operations study. Therefore, the first task was carried out to give a demonstration for spatial imagery and attitude determination missions through a wireless communication. The smart phone's Bluetooth was used to achieve this goal inclusive of a new method to enable real time transmission. In addition, an algorithm around a quaternion based Kalman filter was included in order to detect the reliability of the prototype's orientation. The second task was carried out to provide a demonstration for the attitude control mission using the smart phone's orientation sensor, including a new method for an autonomous guided mode. As a result, the acquisition platform showed real time measurement with good accuracy for orientation detection and image transmission. In addition, the prototype kept the balance during the demonstration based on the attitude control method.\n\nSecure Fingerprint Identification of High Accuracy\n\nDTIC Science & Technology\n\n2014-01-01\n\nsecure ) solution of complexity O(n3) based on Gaussian elimination. When it is applied to biometrics X and Y with mX and mY minutiae, respectively...collections of biometric data in use today include, for example, fingerprint, face, and iris images collected by the US Department of Homeland Security ...work we focus on fingerprint data due to popularity and good accuracy of this type of biometry. We formulate the problem of private, or secure , finger\n\nAccuracy of forecasts in strategic intelligence\n\nPubMed Central\n\nMandel, David R.; Barnes, Alan\n\n2014-01-01\n\nThe accuracy of 1,514 strategic intelligence forecasts abstracted from intelligence reports was assessed. The results show that both discrimination and calibration of forecasts was very good. Discrimination was better for senior (versus junior) analysts and for easier (versus harder) forecasts. Miscalibration was mainly due to underconfidence such that analysts assigned more uncertainty than needed given their high level of discrimination. Underconfidence was more pronounced for harder (versus easier) forecasts and for forecasts deemed more (versus less) important for policy decision making. Despite the observed underconfidence, there was a paucity of forecasts in the least informative 0.4â0.6 probability range. Recalibrating the forecasts substantially reduced underconfidence. The findings offer cause for tempered optimism about the accuracy of strategic intelligence forecasts and indicate that intelligence producers aim to promote informativeness while avoiding overstatement. PMID:25024176\n\nElectroencephalography Predicts Poor and Good Outcomes After Cardiac Arrest: A Two-Center Study.\n\nPubMed\n\nRossetti, Andrea O; Tovar Quiroga, Diego F; Juan, Elsa; Novy, Jan; White, Roger D; Ben-Hamouda, Nawfel; Britton, Jeffrey W; Oddo, Mauro; Rabinstein, Alejandro A\n\n2017-07-01\n\nThe prognostic role of electroencephalography during and after targeted temperature management in postcardiac arrest patients, relatively to other predictors, is incompletely known. We assessed performances of electroencephalography during and after targeted temperature management toward good and poor outcomes, along with other recognized predictors. Cohort study (April 2009 to March 2016). Two academic hospitals (Centre Hospitalier Universitaire Vaudois, Lausanne, Switzerland; Mayo Clinic, Rochester, MN). Consecutive comatose adults admitted after cardiac arrest, identified through prospective registries. All patients were managed with targeted temperature management, receiving prespecified standardized clinical, neurophysiologic (particularly, electroencephalography during and after targeted temperature management), and biochemical evaluations. We assessed electroencephalography variables (reactivity, continuity, epileptiform features, and prespecified \"benign\" or \"highly malignant\" patterns based on the American Clinical Neurophysiology Society nomenclature) and other clinical, neurophysiologic (somatosensory-evoked potential), and biochemical prognosticators. Good outcome (Cerebral Performance Categories 1 and 2) and mortality predictions at 3 months were calculated. Among 357 patients, early electroencephalography reactivity and continuity and flexor or better motor reaction had greater than 70% positive predictive value for good outcome; reactivity (80.4%; 95% CI, 75.9-84.4%) and motor response (80.1%; 95% CI, 75.6-84.1%) had highest accuracy. Early benign electroencephalography heralded good outcome in 86.2% (95% CI, 79.8-91.1%). False positive rates for mortality were less than 5% for epileptiform or nonreactive early electroencephalography, nonreactive late electroencephalography, absent somatosensory-evoked potential, absent pupillary or corneal reflexes, presence of myoclonus, and neuron-specific enolase greater than 75 Âµg/L; accuracy was highest for\n\nDesign and Application of Automatic Falling Device for Different Brands of Goods\n\nNASA Astrophysics Data System (ADS)\n\nYang, Xudong; Ge, Qingkuan; Zuo, Ping; Peng, Tao; Dong, Weifu\n\n2017-12-01\n\nThe Goods-Falling device is an important device in the intelligent sorting goods sorting system, which is responsible for the temporary storage and counting of the goods, and the function of putting the goods on the conveyor belt according to certain precision requirements. According to the present situation analysis and actual demand of the domestic goods sorting equipment, a vertical type Goods - Falling Device is designed and the simulation model of the device is established. The dynamic characteristics such as the angular error of the opening and closing mechanism are carried out by ADAMS software. The simulation results show that the maximum angular error is 0.016rad. Through the test of the device, the goods falling speed is 7031/hour, the good of the falling position error within 2mm, meet the crawl accuracy requirements of the palletizing robot.\n\nAccuracy limitations of hyperbolic multilateration systems\n\nDOT National Transportation Integrated Search\n\n1973-03-22\n\nThe report is an analysis of the accuracy limitations of hyperbolic multilateration systems. A central result is a demonstration that the inverse of the covariance matrix for positional errors corresponds to the moment of inertia matrix of a simple m...\n\n\"Everyone just ate good food\": 'Good food' in Islamabad, Pakistan.\n\nPubMed\n\nHasnain, Saher\n\n2018-08-01\n\nIn recent years, consumption of alternatively produced foods has increased in popularity in response to the deleterious effects of rapidly globalising and industrialised food systems. Concerns over food safety in relation to these changes may result from elevated levels of risk and changing perceptions associated with food production practices. This paper explores how the middle class residents of Islamabad, Pakistan, use the concept of 'good food' to reconnect themselves with nature, changing food systems, and traditional values. The paper also demonstrates how these ideas relate to those of organic, local, and traditional food consumption as currently used in more economically developed states in the Global North. Through research based on participant observation and semi-structured interviews, this paper illustrates that besides price and convenience, purity, freshness, association with specific places, and 'Pakistani-ness' were considered as the basis for making decisions about 'good food'. The results show that while individuals are aware of and have some access to imported organic and local food, they prefer using holistic and culturally informed concepts of 'good food' instead that reconnect them with food systems. I argue that through conceptualisations of 'good food', the urban middle class in Islamabad is reducing their disconnection and dis-embeddedness from nature, the food systems, and their social identities. The paper contributes to literature on food anxieties, reconnections in food geography, and 'good food' perceptions, with a focus on Pakistan. Copyright Â© 2018. Published by Elsevier Ltd.\n\nInertial Measures of Motion for Clinical Biomechanics: Comparative Assessment of Accuracy under Controlled Conditions - Effect of Velocity\n\nPubMed Central\n\nLebel, Karina; Boissy, Patrick; Hamel, Mathieu; Duval, Christian\n\n2013-01-01\n\nBackground Inertial measurement of motion with Attitude and Heading Reference Systems (AHRS) is emerging as an alternative to 3D motion capture systems in biomechanics. The objectives of this study are: 1) to describe the absolute and relative accuracy of multiple units of commercially available AHRS under various types of motion; and 2) to evaluate the effect of motion velocity on the accuracy of these measurements. Methods The criterion validity of accuracy was established under controlled conditions using an instrumented Gimbal table. AHRS modules were carefully attached to the center plate of the Gimbal table and put through experimental static and dynamic conditions. Static and absolute accuracy was assessed by comparing the AHRS orientation measurement to those obtained using an optical gold standard. Relative accuracy was assessed by measuring the variation in relative orientation between modules during trials. Findings Evaluated AHRS systems demonstrated good absolute static accuracy (mean error < 0.5o) and clinically acceptable absolute accuracy under condition of slow motions (mean error between 0.5o and 3.1o). In slow motions, relative accuracy varied from 2o to 7o depending on the type of AHRS and the type of rotation. Absolute and relative accuracy were significantly affected (p<0.05) by velocity during sustained motions. The extent of that effect varied across AHRS. Interpretation Absolute and relative accuracy of AHRS are affected by environmental magnetic perturbations and conditions of motions. Relative accuracy of AHRS is mostly affected by the ability of all modules to locate the same global reference coordinate system at all time. Conclusions Existing AHRS systems can be considered for use in clinical biomechanics under constrained conditions of use. While their individual capacity to track absolute motion is relatively consistent, the use of multiple AHRS modules to compute relative motion between rigid bodies needs to be optimized according to\n\nSeven-Year Clinical Surveillance Program Demonstrates Consistent MARD Accuracy Performance of a Blood Glucose Test Strip.\n\nPubMed\n\nSetford, Steven; Grady, Mike; Mackintosh, Stephen; Donald, Robert; Levy, Brian\n\n2018-05-01\n\nMARD (mean absolute relative difference) is increasingly used to describe performance of glucose monitoring systems, providing a single-value quantitative measure of accuracy and allowing comparisons between different monitoring systems. This study reports MARDs for the OneTouch VerioÂ® glucose meter clinical data set of 80 258 data points (671 individual batches) gathered as part of a 7.5-year self-surveillance program Methods: Test strips were routinely sampled from randomly selected manufacturer's production batches and sent to one of 3 clinic sites for clinical accuracy assessment using fresh capillary blood from patients with diabetes, using both the meter system and standard laboratory reference instrument. Evaluation of the distribution of strip batch MARD yielded a mean value of 5.05% (range: 3.68-6.43% at Â±1.96 standard deviations from mean). The overall MARD for all clinic data points (N = 80 258) was also 5.05%, while a mean bias of 1.28 was recorded. MARD by glucose level was found to be consistent, yielding a maximum value of 4.81% at higher glucose (â¥100 mg/dL) and a mean absolute difference (MAD) of 5.60 mg/dL at low glucose (<100 mg/dL). MARD by year of manufacture varied from 4.67-5.42% indicating consistent accuracy performance over the surveillance period. This 7.5-year surveillance program showed that this meter system exhibits consistently low MARD by batch, glucose level and year, indicating close agreement with established reference methods whilste exhibiting lower MARD values than continuous glucose monitoring (CGM) systems and providing users with confidence in the performance when transitioning to each new strip batch.\n\nAcquisition and Retention of Sterile Compounding Accuracy Skills\n\nPubMed Central\n\nBrown, Michael C.; Valdovinos, Katie; Zavala, Pedro J.\n\n2017-01-01\n\nObjective. To determine the accuracy of dose of pharmacy studentsâ parenteral sterile preparation skills and to measure pharmacy studentsâ skill retention 1.5 years later. Methods. An exercise was designed to assess each studentâs accuracy in compounding a sterile preparation with the correct potency during a second and then third year course. Results. Initially, the mean (standard deviation) of 141 studentsâ compounded preparation dose was not significantly different than the desired dose. Additionally, 91.5% of products were within 10% of the desired dose. In the follow-up activity the next academic year, the mean dose was not significantly different than the original compounded dose. Similarly 92.9% were within 10% of the desired dose. Conclusion. Studentsâ overall accuracy of sterile compounding was good initially and well-retained more than a year later, with more than 90% of students being within 10% of the desired dose in both courses. PMID:28970616\n\nValidity of a commercial wearable sleep tracker in adult insomnia disorder patients and good sleepers.\n\nPubMed\n\nKang, Seung-Gul; Kang, Jae Myeong; Ko, Kwang-Pil; Park, Seon-Cheol; Mariani, Sara; Weng, Jia\n\n2017-06-01\n\nTo compare the accuracy of the commercial Fitbit Flex device (FF) with polysomnography (PSG; the gold-standard method) in insomnia disorder patients and good sleepers. Participants wore an FF and actigraph while undergoing overnight PSG. Primary outcomes were intraclass correlation coefficients (ICCs) of the total sleep time (TST) and sleep efficiency (SE), and the frequency of clinically acceptable agreement between the FF in normal mode (FFN) and PSG. The sensitivity, specificity, and accuracy of detecting sleep epochs were compared among FFN, actigraphy, and PSG. The ICCs of the TST between FFN and PSG in the insomnia (ICC=0.886) and good-sleepers (ICC=0.974) groups were excellent, but the ICC of SE was only fair in both groups. The TST and SE were overestimated for FFN by 6.5min and 1.75%, respectively, in good sleepers, and by 32.9min and 7.9% in the insomnia group with respect to PSG. The frequency of acceptable agreement of FFN and PSG was significantly lower (p=0.006) for the insomnia group (39.4%) than for the good-sleepers group (82.4%). The sensitivity and accuracy of FFN in an epoch-by-epoch comparison with PSG was good and comparable to those of actigraphy, but the specificity was poor in both groups. The ICC of TST in the FFN-PSG comparison was excellent in both groups, and the frequency of agreement was high in good sleepers but significantly lower in insomnia patients. These limitations need to be considered when applying commercial sleep trackers for clinical and research purposes in insomnia. Copyright Â© 2017 Elsevier Inc. All rights reserved.\n\nTo address accuracy and precision using methods from analytical chemistry and computational physics.\n\nPubMed\n\nKozmutza, Cornelia; PicÃ³, Yolanda\n\n2009-04-01\n\nIn this work the pesticides were determined by liquid chromatography-mass spectrometry (LC-MS). In present study the occurrence of imidacloprid in 343 samples of oranges, tangerines, date plum, and watermelons from Valencian Community (Spain) has been investigated. The nine additional pesticides were chosen as they have been recommended for orchard treatment together with imidacloprid. The Mulliken population analysis has been applied to present the charge distribution in imidacloprid. Partitioned energy terms and the virial ratios have been calculated for certain molecules entering in interaction. A new technique based on the comparison of the decomposed total energy terms at various configurations is demonstrated in this work. The interaction ability could be established correctly in the studied case. An attempt is also made in this work to address accuracy and precision. These quantities are well-known in experimental measurements. In case precise theoretical description is achieved for the contributing monomers and also for the interacting complex structure some properties of this latter system can be predicted to quite a good accuracy. Based on simple hypothetical considerations we estimate the impact of applying computations on reducing the amount of analytical work.\n\nConditions that influence the accuracy of anthropometric parameter estimation for human body segments using shape-from-silhouette\n\nNASA Astrophysics Data System (ADS)\n\nMundermann, Lars; Mundermann, Annegret; Chaudhari, Ajit M.; Andriacchi, Thomas P.\n\n2005-01-01\n\nAnthropometric parameters are fundamental for a wide variety of applications in biomechanics, anthropology, medicine and sports. Recent technological advancements provide methods for constructing 3D surfaces directly. Of these new technologies, visual hull construction may be the most cost-effective yet sufficiently accurate method. However, the conditions influencing the accuracy of anthropometric measurements based on visual hull reconstruction are unknown. The purpose of this study was to evaluate the conditions that influence the accuracy of 3D shape-from-silhouette reconstruction of body segments dependent on number of cameras, camera resolution and object contours. The results demonstrate that the visual hulls lacked accuracy in concave regions and narrow spaces, but setups with a high number of cameras reconstructed a human form with an average accuracy of 1.0 mm. In general, setups with less than 8 cameras yielded largely inaccurate visual hull constructions, while setups with 16 and more cameras provided good volume estimations. Body segment volumes were obtained with an average error of 10% at a 640x480 resolution using 8 cameras. Changes in resolution did not significantly affect the average error. However, substantial decreases in error were observed with increasing number of cameras (33.3% using 4 cameras; 10.5% using 8 cameras; 4.1% using 16 cameras; 1.2% using 64 cameras).\n\nApplications and accuracy of the parallel diagonal dominant algorithm\n\nNASA Technical Reports Server (NTRS)\n\nSun, Xian-He\n\n1993-01-01\n\nThe Parallel Diagonal Dominant (PDD) algorithm is a highly efficient, ideally scalable tridiagonal solver. In this paper, a detailed study of the PDD algorithm is given. First the PDD algorithm is introduced. Then the algorithm is extended to solve periodic tridiagonal systems. A variant, the reduced PDD algorithm, is also proposed. Accuracy analysis is provided for a class of tridiagonal systems, the symmetric, and anti-symmetric Toeplitz tridiagonal systems. Implementation results show that the analysis gives a good bound on the relative error, and the algorithm is a good candidate for the emerging massively parallel machines.\n\nHuman In Silico Drug Trials Demonstrate Higher Accuracy than Animal Models in Predicting Clinical Pro-Arrhythmic Cardiotoxicity.\n\nPubMed\n\nPassini, Elisa; Britton, Oliver J; Lu, Hua Rong; Rohrbacher, Jutta; Hermans, An N; Gallacher, David J; Greig, Robert J H; Bueno-Orovio, Alfonso; Rodriguez, Blanca\n\n2017-01-01\n\n(fast/late Na + and Ca 2+ currents) exhibit high susceptibility to depolarization abnormalities. Repolarization abnormalities in silico predict clinical risk for all compounds with 89% accuracy. Drug-induced changes in biomarkers are in overall agreement across different assays: in silico AP duration changes reflect the ones observed in rabbit QT interval and hiPS-CMs Ca 2+ -transient, and simulated upstroke velocity captures variations in rabbit QRS complex. Our results demonstrate that human in silico drug trials constitute a powerful methodology for prediction of clinical pro-arrhythmic cardiotoxicity, ready for integration in the existing drug safety assessm"
    }
}
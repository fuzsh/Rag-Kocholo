{
    "id": "dbpedia_1985_0",
    "rank": 55,
    "data": {
        "url": "https://arxiv.org/html/2402.06353v1",
        "read_more_link": "",
        "language": "en",
        "title": "Towards actionability for open medical imaging datasets: lessons from community-contributed platforms for data management and stewardship",
        "top_image": "",
        "meta_img": "",
        "images": [
            "https://arxiv.org/html/extracted/5348725/images/abstract.png",
            "https://arxiv.org/html/extracted/5348725/images/dataset_anatomy.png",
            "https://arxiv.org/html/x1.png"
        ],
        "movies": [],
        "keywords": [],
        "meta_keywords": [
            ""
        ],
        "tags": null,
        "authors": [],
        "publish_date": null,
        "summary": "",
        "meta_description": "",
        "meta_lang": "en",
        "meta_favicon": "",
        "meta_site_name": "",
        "canonical_link": null,
        "text": "HTML conversions sometimes display errors due to content that did not convert correctly from the source. This paper uses the following packages that are not yet supported by the HTML conversion tool. Feedback on these issues are not necessary; they are known and are being worked on.\n\nfailed: xpatch\n\nAuthors: achieve the best HTML results from your LaTeX submissions by following these best practices.\n\nLicense: CC BY-NC-SA 4.0\n\narXiv:2402.06353v1 [cs.CV] 09 Feb 2024\n\nTowards actionability for open medical imaging datasets: lessons from community-contributed platforms for data management and stewardship\n\nAmelia Jiménez-Sánchez , Natalia-Rozalia Avlona , Dovile Juodelyte , Théo Sourget , Caroline Vang-Larsen , Hubert Dariusz Zając and Veronika Cheplygina\n\nAbstract.\n\nDisclaimer: this is a working paper, and represents research in progress. We welcome contributions from the community, for comments or questions please email us at amji@itu.dk and vech@itu.dk.\n\nMedical imaging datasets are fundamental to artificial intelligence (AI) in healthcare. The accuracy, robustness and fairness of diagnostic algorithms depend on the data (and its quality) on which the models are trained and evaluated. Medical imaging datasets have become increasingly available to the public, and are often hosted on Community-Contributed Platforms (CCP), including private companies like Kaggle or HuggingFace. While open data is important to enhance the redistribution of data’s public value, we find that the current CCP governance model fails to uphold the quality needed and recommended practices for sharing, documenting, and evaluating datasets. In this paper we investigate medical imaging datasets on CCPs and how they are documented, shared, and maintained. We first highlight some differences between medical imaging and computer vision, particularly in the potentially harmful downstream effects due to poor adoption of recommended dataset management practices. We then analyze 20 (10 medical and 10 computer vision) popular datasets on CCPs and find vague licenses, lack of persistent identifiers and storage, duplicates and missing metadata, with differences between the platforms. We present “actionability” as a conceptual metric to reveal the data quality gap between characteristics of data on CCPs and the desired characteristics of data for AI in healthcare. Finally, we propose a commons-based stewardship model for documenting, sharing and maintaining datasets on CCPs and end with a discussion of limitations and open questions.\n\n††copyright: none\\xpatchcmd\\ps@firstpagestyle\n\nManuscript submitted to ACM \\xpatchcmd\\ps@standardpagestyleManuscript submitted to ACM \\@ACM@manuscriptfalse\n\n1. Introduction\n\nDatasets are fundamental to the fields of machine learning (ML) and computer vision (CV), from interpreting performance metrics and conclusions of research papers to assessing adverse impacts of algorithms on individuals, groups and society. Within these fields, medical imaging datasets are especially important to the safe realization of AI in healthcare. Although medical imaging datasets share certain similarities to general CV datasets, they also possess distinctive properties, and treating them as equivalent can lead to various harmful effects. In particular, we highlight three properties of medical imaging datasets: (i) de-identification is required for patient-derived data; (ii) since multiple images can belong to one patient, data splits should clearly differentiate images from each patient; and (iii) metadata containing crucial information such as demographics or hospital scanner is necessary, as models without this information could lead to inaccurate and biased results.\n\nIn the past, medical imaging datasets were frequently proprietary, confined to particular institutions, and stored in private repositories. In this particular setting, there is a pressing need for alternative models of data sharing, documentation, and governance. Within this context, the emergence of Community-Contributed Platforms (CCPs) presented a potential for the public sharing of medical datasets. Nowadays, more medical imaging datasets have become publicly available and are hosted on open platforms such as grand-challenges , or CCP - including private companies like Kaggle or HuggingFace.\n\nWhile the increasing availability of medical imaging datasets is generally an advancement for sharing and adding public value, the current practices also present many challenges. First, according to the FAIR (Findable, Accessible, Interoperable, Reusable) guiding principles for scientific data management and stewardship (Wilkinson et al., 2016), (meta)data should be released with a clear and accessible data usage license and should be permanently accessible. Second, it is increasingly difficult to track dataset versions, especially if publications use derived versions of datasets or the citation practices are not followed. Third, rich documentation is essential to avoiding over-optimistic and biased results, attributed to a lack of meta-data (indicating which images correspond to which patients and the demographics of the patients) in medical imaging datasets. Documentation needs to reflect all the stages in the dataset development cycle such as acquisition, storage and maintenance (Hutchinson et al., 2021). There have been several efforts by ML researchers raising awareness about the importance of dataset documentation and proposing guidelines, frameworks or datasheets (Holland et al., 2020; Bender and Friedman, 2018; Pushkarna et al., 2022; Gebru et al., 2021; Díaz et al., 2022). Researchers in medical image analysis are urged to adopt visibility practices for accountability and responsibility throughout the data development lifecycle. Although CCPs offer ways to alleviate some of these problems providing structured summaries, we find that these practices are not widely adopted.\n\nIn this paper, we investigate medical imaging datasets hosted on CCPs, particularly, how they are documented, shared, and maintained. We provide relevant background information, highlighting differences between open medical imaging and computer vision datasets, especially, in the potential for harmful downstream effects of poor documentation and distribution practices (Section 2). First, we analyze 20 popular datasets hosted on CCPs (10 medical and 10 computer vision). We find issues across platforms related to vague licenses, lack of persistent identifiers and storage, duplicates, and missing metadata (Section 3); as well as limited adoption of recommended documentation practices (Section 4). Second, we discuss the nature of datasets as dynamic processes that cannot be captured within static infrastructures (Section 5). Third, we present “actionability” as a conceptual metric to reveal the data quality gap between CCP data characteristics and ideal data for AI training, and propose a commons-based stewardship model (see Fig. 1) for the documentation, sharing, and maintenance of medical imaging datasets on CCPs (Section 6). Finally, we conclude with a discussion of limitations and open questions (Section 7).\n\n2. Background\n\nAnatomy of a medical imaging dataset\n\nA medical imaging dataset begins with a collection of images from various imaging modalities, such as X-rays, magnetic resonance imaging (MRI), computed tomography (CT) scans, and others. The scans are often initially captured for a clinical purpose, such as diagnosis or treatment planning, and are associated with a specific patient and their medical data. The scans might undergo various processing steps, such as denoising, registration (aligning different scans together) or segmentation (delineating anatomical structures or pathologies in the image). Clinical experts might then associate the scans with additional information, e.g. , free text reports or diagnostic labels.\n\nA collection of scans and associated annotations, i.e., a medical imaging dataset, might be later used for the purpose of training and evaluating ML models supporting the work of medical professionals (Zhou et al., 2020; Varoquaux and Cheplygina, 2022). However, before a dataset is “ready” for ML, further steps are required (Willemink et al., 2020), including cleaning (for example, removing scans that are too blurry), sampling (for example, only selecting scans with a particular disease), and removing identifying patient information. Furthermore, additional annotations, which were not collected during clinical practice, may be necessary to train ML models, e.g. , delineations of organs in a patient who does not need radiotherapy. Such annotations might be applied by clinical experts or others interested in the success of the ML model, including PhD students and paid annotators at tech companies.\n\nWe illustrate a simplified example of a medical imaging dataset in Fig. 2. This dataset has six brain MR images in total, which are associated with three patients, a male and a female patient who are healthy and a male patient with a disease. A purpose of this dataset could be to create a model which can output the diagnosis, given a patient’s brain scan.\n\nNot small computer vision\n\nWhile medical imaging datasets share some similarities with general CV datasets, they also have unique properties. Treating them as equivalent to benchmark CV datasets is problematic and leads to various harmful effects, also termed data cascades by (Sambasivan et al., 2021).\n\nFirst, in contrast to traditional CV datasets, medical images often require de-identification processes to remove personally identifiable data. These are more complex than complete anonymization; as for clinical tasks, some attributes, like sex and age, should be maintained. While the extra work of de-identification is often conducted and such attributes are included in an “original release” of medical imaging datasets, they might be removed later in a dataset’s lifecycle. For example, when medical datasets are shared on CCPs, often only the input desired by ML practitioners remains: inputs (images) and outputs (disease labels), as depicted in Fig. 1.\n\nSecond, medical imaging datasets may contain multiple images associated with a single patient. This can occur if a patient has multiple skin lesions, has follow-up chest X-rays, or has a single MR or CT scan - a 3D scan - that has been split up into multiple 2D images. If images from the same patient end up in both training and test data, over-optimistic results might be reported because the classifier memorises the patient rather than the disease characteristics. Therefore, data splits should be done on a patient level to not overfit models. This is adopted in the medical imaging community but may be easily overlooked if a dataset is simply shared as a general CV dataset, so a collection of images and labels.\n\nThird, medical imaging datasets should contain metadata about patient demographics. Several studies have shown how demographic data may alleviate systematic biases and differentiate the performance of disease classification in chest X-rays (Larrazabal et al., 2020; Seyyed-Kalantari et al., 2020) and skin lesions (Abbasi-Sureshjani et al., 2020). These datasets are often the subject of research on bias and fairness because they include variables for sex or gender (typically not described which) and age, but for many other medical imaging datasets, such variables are not available (possibly because they were removed in a ML-ifying step, not due to actual anonymization). Unlike computer vision datasets where bias can be shown by annotating individuals in the images based on their gender expression (Zhao et al., 2021), we often cannot recover such information back from medical images. As an additional problem, images may be duplicated; see, e.g. , (Cassidy et al., 2022) for an analysis of the ISIC skin lesion datasets, with overlaps between versions and duplication of cases between training and test sets.\n\nFinally, medical imaging datasets should include metadata about the origin of scans. Lack of such data, as in general CV datasets, may lead to “shortcuts” and other systematic biases. For example, if disease severity is correlated with the hospital where the scans were made (a general hospital vs. a cancer clinic), a model might learn the clinic’s scanner signature as a shortcut for the disease (Compton et al., 2023). In other words, the shortcut is a spurious correlation between an artifact in the image and the diagnostic label. Some examples of shortcuts include patient position in COVID-19 (DeGrave et al., 2021), chest drains in pneumothorax classification (Oakden-Rayner et al., 2020; Jiménez-Sánchez et al., 2023), or pen marks in skin lesion classification (Winkler et al., 2019; Bissoto et al., 2020; Combalia et al., 2022). These biases may not be apparent when using subsets of benchmark datasets for evaluation; the performance may appear high on average but still be poor on underrepresented groups of patients and not generalize to patients outside the dataset’s specific distribution. This cannot be detected without appropriate metadata.\n\nAvailability and trend of medical imaging datasets\n\nHistorically, medical imaging datasets were often proprietary, limited to specific institutions, and held in private repositories. Due to the privacy concerns and high cost associated with expert annotations, the sizes of medical imaging datasets were quite small, often in the tens or hundreds of patients, which limited the use of machine learning techniques. Lately, publicly available datasets have become more widespread. Over the years the datasets increased in size, for example INBreast (Moreira et al., 2012) and LIDC-IDRI (Armato III et al., 2011) with thousands, and some even with tens or hundreds of thousands of patients, like chest X-ray datasets (NIH-CXR14 (Wang et al., 2017), MIMIC-CXR (Johnson et al., 2019), CheXpert (Irvin et al., 2019)), and skin lesions datasets (ISIC (Codella et al., 2017; Combalia et al., 2022)). Some open-access medical imaging datasets, such as the radiologic imaging dataset RadImageNet (Mei et al., 2022), are explicitly crafted to address the challenge imposed by the limited size of the medical images and used for model pre-training. These datasets are increasingly used to benchmark general ML and CV research.\n\nOpen data initiatives\n\nThese datasets are collected from different sources, like academic and public institutions or tech companies, and are hosted on a variety of platforms. Some of them, like CCPs, are owned by private companies like HuggingFace and Kaggle, which recently partnered with tech companies, such as Amazon. The promise of open data to enhance the public value of/in data stumbles upon the history of Free and Open-Source Software (FOSS) and particularly the appropriation of Open Source Software’s value creation by tech companies. For this reason, and acknowledging the complexity that public/private constellations such as the one CCPs introduce, we are forced to rethink the “actionability” of the open data framework within this context. Alternative data sharing, documenting, and governance models are crucial to addressing power imbalances and enhancing data’s generation of value as a \"common good\" (Ostrom, 1990; Purtova and van Maanen, 2023; Tarkowski et al., 2022).\n\nCurrent debates about \"open\" AI systems have become of public interest and got the attention of the open-source-software and open data movements.They advocate for both adopting alternative data governance models, such as public trusts, or data cooperatives but also to adopt an alternative licensing regime that will ensure the public interest in data sharing (Tarkowski et al., 2022). Within this context, the emergence of CCPs, which present a potential for the public sharing of medical datasets, invokes crucial questions about data governance. Whilst the sharing of the datasets in these alternative community data infrastructures follows the values of Open Source Software Initiatives (i.e. the obligation to share the datasets in the CCP with Creative Commons licenses), an essential yet neglected matter is the enforcement of proper documentation and governance principles in these datasets.\n\n3. Issues with medical imaging datasets on community-contributed platforms\n\nAccording to the FAIR (Findable, Accessible, Interoperable, Reusable) guiding principles for scientific data management and stewardship (Wilkinson et al., 2016), (meta)data should be release with a clear and accessible data usage license and should be permanently accessible. We investigate dataset distribution, documentation and storage practices for the most cited CV and medical imaging datasets. We select the top-10 datasets from Papers with Code under Modality “Images” and “Medical” for CV and Medical Imaging, respectively. In particular, we investigate dataset distribution in Community-contributed platforms (CCP) such as Kaggle and HuggingFace (HF), and regulated platforms (RP) such as Tensorflow (TF), Keras, and PyTorch.\n\nVague licenses and lack of persistent identifiers and storage\n\nWe show in Table 1 the dataset distribution and storage practices for the most used CV (top) and medical imaging (bottom) datasets. We show the original source where each dataset is hosted, the distribution terms (for use, sharing, or access), the license, and other platforms where the datasets can be found (CCP, RP). Licenses, or terms of use represent legal agreements between dataset creators and users, yet we observe in Table 1 (top) that the majority of the most used CV datasets were not released with a clear license or terms of use. Regarding medical imaging datasets, we observe in Table 1 (bottom) that only half of the most used datasets were released with a license. Even if DRIVE dataset has a CC-BY-4.0 license, on Kaggle we find an instance with a description that reads \"No license was specified, yet all credits are due to the original authors.\" We also observe that CV datasets are widely available both on CCP and RP. This is not the case for medical datasets, that are not commonly accessible on RP, but some of them are on CCP. Regarding storage, we find that CV datasets are mostly hosted on authors or university websites. This is not aligned with the FAIR guiding principles. In contrast, medical datasets are hosted on a variety of websites: university, grand-challenge, or physionet. We find some examples of datasets that follow the FAIR principles, like HAM10000 with a persistent identifier, or MIMIC-CXR stored in physionet (Goldberger et al., 2000), which offers permanent access to datasets with a Digital Object Identifier (DOI). Dataset creators should include licenses to provide a legal framework for the use, distribution, and sharing of data. Besides, without a persistent identifier and storage, access to the (meta)data is uncertain, which is problematic for reproducibility.\n\nDuplicate datasets and missing metadata\n\nWe present a case study of “uncontrolled\" spread of open medical imaging datasets on CCP. In particular, we discuss ISIC, a group of skin lesion challenge datasets. The overall goal of ISIC challenges was to develop image analysis tool to enable the automated diagnosis of melanoma from dermoscopic images. Challenges comprised tasks such as lesion segmentation, detection and localization of visual dermoscopic patterns, or disease classification. The ISIC datasets originated from challenges held between 2016 and 2020 at different conferences. Each challenge introduced a new ISIC dataset with potentially overlapping data with previous instances. Cassidy et al. (2022) analyze the datasets and find duplicate images, both across and within the datasets, as well as training/test set overlaps. The ISIC datasets can be downloaded from https://challenge.isic-archive.com/data/ and depending on the dataset, researchers are requested to cite the challenge paper, and/or the original sources of the data (HAM10000 (Tschandl et al., 2018), BCN20000 (Combalia et al., 2019), MSK (Codella et al., 2018)).\n\nAs of January 2024 there are 19 datasets explicitly related to ISIC (i.e., ISIC is in the name) on HuggingFace. Some of these datasets are preprocessed (for example, cropped images), others provide extra annotations (for example, segmentation masks). Kaggle has a whopping 550 datasets explicitly related to ISIC. While the size of the original ISIC datasets is 38 GB, Kaggle stores 1.9 TB of data, this is illustrated in Fig. 3. Some of the highly downloaded versions contain a description of the datasets but are missing the original sources or licence information. The proliferation of numerous duplicate datasets not only imposes an unnecessary waste on resources but also poses a significant impediment to the reproducibility of research outcomes. Other datasets provide a subset that include images with dark corners or with visible hair on the skin. Besides ISIC, we find other examples of unnecessary duplication of data. On Kaggle, there are 9 instances of PAD-UFES-20 (one containing data from ISIC); and 3 datasets of INBreast, one of them with the rephrased description \"I’m just uploading here this data as a backup\". The lack of description of these datasets hampers tracking dataset usage and potentially violates sharing agreements or licenses. Moreover, due to the characteristics of medical imaging datasets, models trained on datasets missing metadata could result into overoptimistic performance due to data splits mixing patient data, or bias (Larrazabal et al., 2020) or shortcuts (Oakden-Rayner et al., 2020; Jiménez-Sánchez et al., 2023; Bissoto et al., 2020; Winkler et al., 2019; DeGrave et al., 2021).\n\n5. Datasets evolve within static infrastructures: in need of a tracking process\n\nData as a process\n\nThe creation of a dataset is a complex process that involves several stages from dataset creation or acquisition to its storage and maintenance. Hutchinson et al. (2021) proposed a framework that covers the documentation of each stage of the data development lifecycle. Their proposal is based on practices from software engineering and infrastructure. This framework recognizes that data evolves over time, gaining value through various processes and interactions. By approaching data as a process (Williamson, 2020), we foster a more dynamic and responsive mindset towards managing and using a dataset. This perspective can lead to better decision-making and improved overall data quality. Researchers working in the medical image analysis community need to adopt visibility practices that enable accountability and responsibility throughout the data development lifecycle.\n\nDatasets changes cannot be captured with traditional reviews\n\nA major obstacle when tracking dataset changes is their lack of a DOI or stable identifier. If they do, it might be more common to refer to them by a mix of name, description, URLs, or by referencing related papers. This makes even manual disambiguation challenging, let alone tracking datasets through bibliographic databases. Datasets can change explicitly through manual interventions or cleaning processes, and implicitly due to temporal variations. Effectively managing these changes is crucial for maintaining the quality of data for analysis and resulting applications.\n\nConcerning explicit changes, we observed that practices around documenting dataset changes are not formalized, for example, on the website hosting LIDC-IDRI updates are communicated using red font size to alert users about errors and updated labels. On the website hosting NIH-CXR14 we also found two folders pointing to two derived datasets: PruneCXR (Holste et al., 2023) and LongTailCXR (Holste et al., 2022). Part of the documentation of this dataset also contains PDF files with log versions or job announcements.\n\nRegarding implicit changes, datasets’ audits can uncover biases or spurious correlations (shortcuts). For example, several studies have shown differences in performance of disease classification in chest X-rays (Larrazabal et al., 2020; Seyyed-Kalantari et al., 2020) and skin lesions (Abbasi-Sureshjani et al., 2020) according to patient demographics. Spurious correlations could also lead to biased results, such as chest drains affecting pneumothorax classification (Oakden-Rayner et al., 2020; Jiménez-Sánchez et al., 2023) or pen marks influencing skin lesion classification (Winkler et al., 2019; Bissoto et al., 2020). Examples of individual efforts to track implicit or explicit dataset changes include Github repositories or Notion databases, where users technically can individually contribute, but without coordinated guidelines. Given the dynamic nature of datasets, there is a need to create living reviews. One forward-looking tool could be an overview website where community members could contribute derived datasets, or related research such as dataset reviews, new annotations or reporting shortcuts, survey papers, etc.\n\nMitigating dataset harms requires stewardship\n\nThe ethical concerns associated with a dataset can change over time. Several CV datasets have been retracted for that reason: MS-Celeb-1M (Guo et al., 2016), DukeMTMC (Ristani et al., 2016), and very recently, LAION-5b (Schuhmann et al., 2022). LAION-5b is the dataset which backs the popular Stable Diffusion model (Rombach et al., 2022). It is concerning that this paper has been retracted after receiving the Best Paper award at NeurIPS 2022 in the Track on Datasets and Benchmarks. Reporting the presence of shortcuts or error in labels represent a crucial step to prevent harm to downstream end-users using open medical datasets for their AI models. Earlier methods mainly focus on the creation of datasets (Holland et al., 2020; Bender and Friedman, 2018; Pushkarna et al., 2022; Gebru et al., 2021; Díaz et al., 2022). However, as exemplified, it is challenging to foresee and tackle ethical implications during the dataset creation phase. Effective harm mitigation necessitates ongoing stewardship throughout the entire life cycle of a dataset (Peng et al., 2021).\n\n6. Towards dataset actionability\n\nIn this paper, we investigated open medical imaging datasets available on CCPs. Particularly, we assessed the quality of available data and its storage and documentation practices. Our findings highlight the gap between the desired characteristics of data used for AI training (Whang et al., 2023) and the qualities of available open medical imaging data, namely: (1) the vague licensing and lack of persistent identifiers and storage, (2) missing metadata and lack of maintenance, and (3) insufficient implementation of documentation practices.\n\nIt is unlikely that commercially available clinical AI-based systems are solely trained on open datasets, if at all. In fact, corporate health-tech entities seem to ensure quality datasets, either by contractual agreements with the data sources (e.g., hospitals) or by procuring high-quality medical images (Rajpurkar et al., 2022). The value offering of entities selling medical imaging datasets or labelling services is related to the aforementioned challenges: licensing, documentation, ease of use, and the quality of data and annotations. Such proprietary datasets offer more customization and flexibility and are unaffected by the challenges above (Rajpurkar et al., 2022; Zając et al., 2023). Similarly, Zając & Avlona et al. (Zając et al., 2023) have shown how the obligation to regulatory compliance, as well as internal organizational requirements, transverse and often define the quality of the created datasets.\n\nThis asymmetry between the issues of open data and the value offered by proprietary datasets highlights the shortcomings of current open medical imaging data. Open data initiatives, like CCPs, offer the potential to redistribute data value for the common good and public interest. However, we have shown that in its current form such open medical imaging data cannot be reliably used to train high-performing, equitable, and responsible AI models. It is because of these shortcomings that we suggest rethinking and evaluating open datasets in CCPs through the concept of \"actionability\". Drawing upon the FAIR Guiding Principles (Wilkinson et al., 2016), we expanded the technical understanding of data actionability with three concerns focused on real-world application: access, quality, and documentation. We argue that these concerns need to be accounted for if the medical imaging datasets are to live up to the ideals of open data.\n\n6.1. Three concerns of open datasets actionability\n\nAccess to open datasets should be predictable, compliant with open licensing, and persistent.\n\nIn this paper, we show that a proper dataset infrastructure (both legal and technical) is crucial for their effective utilization. Open datasets must be properly licensed to prevent harm to end-users by models trained on legally ambiguous open data with the potential for bias and unfairness (Schumann et al., 2021; Leavy et al., 2021). Moreover, vague licensing pushes the users of open datasets into a legal grey zone (Gent, 2023). Contractor et al. (2022) noticed such a legal gap in the \"inappropriate\" use of open AI models and pointed out the danger of their possible unrestricted and unethical use. To ensure the responsible use of AI models, they envisioned enforceable licensing. Legal clarity should also span persistent and deterministic storage. As explored in this paper, the major open datasets are well-known and hosted by established academic institutions. However, the CCPs host a plethora of duplicated or altered open medical datasets (Gent, 2023). Instead of boosting the opportunities for AI creators, this abundance may become a hindrance when e.g. , developers cannot possibly track changes introduced between different versions of a dataset. Proper licensing does not mean the enforcement of traditional copyright but compliance with a diversity of open licences, for example, GNU, MIT, Creative Commons, etc. We argue that open data has to be predictably accessible under clear conditions and for clear purposes to support its actionability.\n\nOpen datasets should be evaluated against the context of real-world use.\n\nThe understanding of high-quality data for AI training purposes is constantly evolving (Whang et al., 2023). After a thorough evaluation focused on real-world use, medical imaging datasets, once considered high-quality (Irvin et al., 2019; Wang et al., 2017; Codella et al., 2018; Tschandl et al., 2018), were revealed to contain flaws questioning their clinical usefulness (Oakden-Rayner et al., 2020; Jiménez-Sánchez et al., 2023; Bissoto et al., 2020; Varoquaux and Cheplygina, 2022). Maintaining open datasets is often an endeavour that is too costly for their creators, resulting in the deteriorating quality of available datasets. Moreover, this paper showed the prevalence of information about shortcuts and missing metadata in the open medical imaging datasets hosted on CCPs. These issues can have adverse downstream effects, diminishing the clinical usefulness of developed systems and, in extreme scenarios, potentially causing harm to the intended beneficiaries. We encourage the FAccT, medical imaging and other ML communities to expand the understanding of high-quality data by incorporating rich metadata and emphasizing real-world evaluations, that include testing to uncover biases or shortcuts. As shown in this paper, these two aspects are detrimental to training fair, equitable, high-performing, and responsible AI models in healthcare.\n\nDocumentation of open datasets should be complete and up-to-date.\n\nResearch has shown that access to large amounts of data does not necessarily warrant the creation of responsible and equitable AI models (Pushkarna et al., 2022). Instead, it is the connection between the dataset’s size and the understanding of the work that resulted in the creation of a dataset. This connection is the premise behind the creation of proprietary datasets designed for use in private enterprises. When that direct connection is broken, a fairly common scenario in the case of open datasets, the knowledge of the decisions taken during dataset creation is lost. Critical data and data science scholars are concerned about the social and technical consequences of using such undocumented data. Thus, a range of documentation frameworks were proposed like the most popular and considered in this paper Datasheets for Datasets (Gebru et al., 2021), or Data Statements (Bender and Friedman, 2018), Data Nutrition Label (Holland et al., 2018), data briefs (Fabris et al., 2022), or the dataset development lifecycle (Hutchinson et al., 2021). Each documentation method slightly differs, focusing on various aspects of dataset quality. However, their overall goal is to introduce greater transparency and accountability in design choices. These conscious approaches aim to foster greater reproducibility of AI results and contribute to the development of responsible AI. Unfortunately, as shown in this paper, the real-world implementation of these frameworks is lacking. Even when a CCP supports a documentation framework, the content of provided documentations rarely aligns with the ideas behind these frameworks. Consequently, when the supplied documentation does not adhere to these principles, it fails to fulfil its intended purpose, thus, placing users of open datasets at a disadvantage compared to users of proprietary datasets. We argue that for open medical image datasets to be actionable, complete and up-to-date documentation is imperative to facilitate understanding and ensure purposeful and conscious use.\n\nWe consider the three concerns foundational to fulfilling the principles of open data and enabling more equitable, fair, and responsible AI development: predictable, compliant with open licensing, and persistent access; evaluation in the context of real-world use quality, and complete and up-to-date documentation. However, cautioned by the low adoption of documentation practices on the CCP, we will not present a new set of concepts for the sake of conceptualisation only. Valuing the commitment of the research communities to actionable contributions, we will discuss possible ways of improving the actionability of open medical imaging datasets on CCPs.\n\n6.2. Enforcing open data actionability\n\nCommons-based governance for medical imaging datasets on CCPs\n\nIn this complex territory of open data and CCPs, we argue for the establishment of a commons-based governance model inspired by Wikipedia. We have shown above that rethinking the quality of open datasets through \"actionability\" provides us with a useful conceptual shift to practically address the deficient practices in the documentation, sharing, and maintenance of medical imaging datasets hosted on CCPs. Considering that ’actionability’ needs to be ensured within the open community schema of the CCPs, enforcing it would involve applying norms and principles established in a commons-based governance model. This model is rooted in decentralization, yet adapted to the specificities of medical imaging datasets.\n\nThis endeavour entails the challenge of conceptualizing open medical imaging datasets not merely as unbound common goods-informational resources, such as the Wikipedia entries, or rivalry common pool resources, such as fishery (Forte et al., 2009; Apesteguia and Maier-Rigaud, 2006; Purtova and van Maanen, 2023; Ostrom, 1990), but rather as complex assemblages produced within and for the high-risk domain of healthcare, with the purpose of enhancing public value. As demonstrated earlier, to optimize clinical utility and prevent potential harmful effects of open medical datasets, we must consider ensuring access, quality, and documentation of these datasets as a of governance priority.\n\nWe suggest two levels of CCPs data stewardship inspired by the Wikipedia governance model as indicative, yet not exclusive, mechanisms that bring open dataset actionability into action. We show an illustration of our proposal in Fig. 1. In our context, we define data stewardship as decentralised human expertise responsible for taking care of the completeness, credibility, maintenance, and compliance of the datasets. Looking at the access levels in Wikipedia (Forte et al., 2009), we define the role of data administrator as the first-level of data stewardship, sanctioning mechanism that ensures proper (1) licensing, (2) persistent identifiers, and (3) completeness of metadata, the open medical datasets that enter the platform. We define as the second-level of data stewardship, the role of data steward, who will be responsible for the ongoing monitoring of the (1) maintenance, (2) storage, and (3) implementation of documentation practices.\n\nNevertheless, these data stewardship proposals, as modes of commons-based governance, need further exploration within a broader community of practitioners on CCPs. Acknowledging the scarcity of resources (monetary and/or human labour) in these CCPs initiative, we are very careful to suggest a complex governance system that would solely depend on the free labour of the dataset creators, bearing its enforcement as a matter of individual responsibility. Instead, we open up this direction as a valuable strand for future applied research, aiming to enact the \"actionability\" of the open medical imaging datasets through commons-based governance.\n\n7. Limitations and future work\n\nMedical imaging vs. other communities\n\nIn this paper, we focused on medical imaging datasets due to their unique characteristics which are different from computer vision. While we investigated only ten popular datasets, we believe they highlight the breadth of faced challenges. Furthermore, specifically for medical imaging, there is still not a lot of research addressing datasets. There are recent and important studies addressing the quality of the data, for example about missing metadata and patient underrepresented skin tones in dermatology datasets (Daneshjou et al., 2021) or auditing the labels in public chest X-ray datasets (Oakden-Rayner et al., 2020). However, such studies do not investigate how the datasets are shared or maintained.\n\nComputer vision and natural language processing datasets have received more attention, both from the perspective of what is (not) and the data and the corresponding harms (Buolamwini and Gebru, 2018; Birhane et al., 2023; Fabbrizzi et al., 2022; Meister et al., 2023), and the history of how and why the datasets were created (Denton et al., 2021; Raji et al., 2021; Hutchinson et al., 2021; Paullada et al., 2021). It is not clear whether this increased attention is due to more apparent immediate harms (a medical dataset may be biased, but an image and its caption would typically not be offensive), the size of the field, and/or other factors. We are inspired by these works and hope to bring more attention to research in our own fields. We would be interested in investigating this question in future work.\n\nThe neuroimaging community has a longer history of data sharing and reuse (Poline et al., 2012; Poldrack and Gorgolewski, 2014; Madan, 2022) and investigating data sharing is more commonplace, see for example (Borghi and Van Gulick, 2018; Paret et al., 2022). Note that while medical imaging papers sometimes focus on brain imaging datasets, and so does our illustrative example in Section 2, the neuroimaging community is relatively distinct from medical imaging.\n\nPerhaps we should, in general, note that when we talk about \"community\", we never explicitly define it but use our own experiences and views. In some research about community practices, the community seems to be often replaced by a proxy of publishing in specific venues. But this is a relatively narrow view, especially from the point of data creation and sharing, as the people doing this the most, might be excluded from said venues.\n\nAdditional context to community-contributed platforms\n\nWe only focus on two platforms which are used both in computer vision and medical imaging, Kaggle and HuggingFace. We are aware that the use of grand-challenge.org is common for medical image datasets. However, we decided not to include it for two main reasons. First, datasets hosted on grand-challenge.org are focused mostly on a specific proposed challenge, thus, we found their documentation more detailed. Second, the community-contributed part is more strictly defined here. In particular, there are various review processes involved. Often challenges are hosted at conferences and require submission of a proposal, which is then peer-reviewed.\n\nMore insights needed from all people involved\n\nA limitation of our study is that it is primarily based on our quantitative evidence and our subjective perceptions of the fields and practices we describe. To gain a better understanding of these questions, it would be valuable to do a qualitative analysis with medical imaging and machine learning practitioners to understand their use of datasets. For example, (Zając et al., 2023) is a recent study, based on interviews with researchers from companies and the public health sector, of how several medical (but not only imaging) datasets were created. In the scope of sharing and maintenance practices, it would be interesting to see similar studies into how researchers choose which datasets to work on, beyond correlations between popularity and quantitative metrics of the datasets.\n\nImportantly, we should not forget that understanding research practices around datasets is not a goal in itself, but that these datasets are in one way or another affecting people who are not represented at research conferences, so further research should involve these most affected groups (Thomas and Uminsky, 2020).\n\nThe funding gap\n\nWhile investigating these issues is pressing to some, the societal importance of a research question is not necessarily correlated with funding. Research has shown that elite institutions and tech companies are increasingly represented in highly regarded machine learning conferences (Birhane et al., 2021), and Papers with Code activity (Hutchinson et al., 2021). We sincerely hope that such institutions support more efforts aimed at improving the value of open datasets. For example, the NeurIPS Dataset and Benchmark Track and the recently introduced Journal of Data-centric Machine Learning Research (DMLR) are clear examples of interventions to reward dataset development. However, it is not clear how diverse the beneficiaries of these initiatives are. We wish to see more structural support, such as permanent and paid roles for data stewards (Plomp et al., 2019) and funding reallocation from “the usual suspects” to underrepresented early-career researchers.\n\n8. Conclusions\n\nMedical imaging datasets are crucial for the development of accurate, robust, and fair AI models in healthcare. Medical imaging datasets are progressively becoming more accessible to the public and can be found on Community-Contributed Platforms (CCPs). CCPs play a crucial role in amplifying the public value of data redistribution to society. Nonetheless, our observation reveals that the existing governance model falls short of maintaining the necessary quality standards and recommended practices for sharing, documenting, and evaluating open datasets. We introduce the term \"actionability\" as a conceptual metric to expose the disparity in data quality between the attributes of data on CCP and the ideals characteristics of data for AI training. In conclusion, we advocate for a commons-based stewardship model with two main roles: data administrator and data steward, to examine the content and keep updated the documentation of medical image imaging datasets on CCPs.\n\n9. Statements\n\n9.1. Researcher positionality statement\n\nWe are a group of researchers with somewhat diverse backgrounds but employed at academic institutions in Western Europe. Our views might therefore not cover the full spectrum of experiences of researchers in the communities that we write about.\n\n9.2. Discussions of limitations\n\nNext to the limitations discussed in the main text of the paper, we acknowledge that we did not contact the mentioned companies.\n\nAcknowledgments\n\nThis project has received funding from the Independent Research Council Denmark (DFF) Inge Lehmann 1134-00017B.\n\nReferences\n\n(1)\n\nhug ([n. d.) ]huggingface2024guide [n. d.]. HuggingFace Datasets Card Creation Guide. https://www.kaggle.com/docs/datasets. Accessed: 2024-01-10.\n\nkag ([n. d.) ]kaggle2024guide [n. d.]. Kaggle Datasets Documentation. https://www.kaggle.com/docs/datasets. Accessed: 2024-01-10.\n\nAbbasi-Sureshjani et al. (2020) Samaneh Abbasi-Sureshjani, Ralf Raumanns, Britt EJ Michels, Gerard Schouten, and Veronika Cheplygina. 2020. Risk of Training Diagnostic Algorithms on Data with Demographic Bias. arXiv preprint arXiv:2005.10050 (2020).\n\nApesteguia and Maier-Rigaud (2006) Jose Apesteguia and Frank P Maier-Rigaud. 2006. The role of rivalry: public goods versus common-pool resources. Journal of Conflict Resolution 50, 5 (2006), 646–663.\n\nArmato III et al. (2011) Samuel G Armato III, Geoffrey McLennan, Luc Bidaut, Michael F McNitt-Gray, Charles R Meyer, Anthony P Reeves, Binsheng Zhao, Denise R Aberle, Claudia I Henschke, Eric A Hoffman, et al. 2011. The lung image database consortium (LIDC) and image database resource initiative (IDRI): a completed reference database of lung nodules on CT scans. Medical physics 38, 2 (2011), 915–931.\n\nBender and Friedman (2018) Emily M. Bender and Batya Friedman. 2018. Data Statements for Natural Language Processing: Toward Mitigating System Bias and Enabling Better Science. Transactions of the Association for Computational Linguistics 6 (2018), 587–604. https://doi.org/10.1162/tacl_a_00041\n\nBirhane et al. (2021) Abeba Birhane, Pratyusha Kalluri, Dallas Card, William Agnew, Ravit Dotan, and Michelle Bao. 2021. The values encoded in machine learning research. arXiv preprint arXiv:2106.15590 (2021).\n\nBirhane et al. (2023) Abeba Birhane, Vinay Prabhu, Sang Han, Vishnu Naresh Boddeti, and Alexandra Sasha Luccioni. 2023. Into the LAIONs Den: Investigating Hate in Multimodal Datasets. arXiv preprint arXiv:2311.03449 (2023).\n\nBissoto et al. (2020) Alceu Bissoto, Eduardo Valle, and Sandra Avila. 2020. Debiasing skin lesion datasets and models? not so fast. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops. 740–741.\n\nBorghi and Van Gulick (2018) John A Borghi and Ana E Van Gulick. 2018. Data management and sharing in neuroimaging: Practices and perceptions of MRI researchers. PloS one 13, 7 (2018), e0200562.\n\nBuolamwini and Gebru (2018) Joy Buolamwini and Timnit Gebru. 2018. Gender shades: Intersectional accuracy disparities in commercial gender classification. In Conference on Fairness, Accountability and Transparency. 77–91.\n\nCassidy et al. (2022) Bill Cassidy, Connah Kendrick, Andrzej Brodzicki, Joanna Jaworek-Korjakowska, and Moi Hoon Yap. 2022. Analysis of the ISIC image datasets: Usage, benchmarks and recommendations. Medical image analysis 75 (2022), 102305.\n\nCoates et al. (2011) Adam Coates, Andrew Ng, and Honglak Lee. 2011. An Analysis of Single-Layer Networks in Unsupervised Feature Learning. In Proceedings of the Fourteenth International Conference on Artificial Intelligence and Statistics (Proceedings of Machine Learning Research, Vol. 15), Geoffrey Gordon, David Dunson, and Miroslav Dudík (Eds.). PMLR, Fort Lauderdale, FL, USA, 215–223. https://proceedings.mlr.press/v15/coates11a.html\n\nCodella et al. (2017) Noel CF Codella, David Gutman, M Emre Celebi, Brian Helba, Michael A Marchetti, Stephen W Dusza, Aadi Kalloo, Konstantinos Liopyris, Nabin Mishra, Harald Kittler, et al. 2017. Skin lesion analysis toward melanoma detection: A challenge at the 2017 International Symposium on Biomedical Imaging (ISBI), hosted by the International Skin Imaging Collaboration (ISIC). arXiv preprint arXiv:1710.05006 (2017).\n\nCodella et al. (2018) Noel CF Codella, David Gutman, M Emre Celebi, Brian Helba, Michael A Marchetti, Stephen W Dusza, Aadi Kalloo, Konstantinos Liopyris, Nabin Mishra, Harald Kittler, et al. 2018. Skin lesion analysis toward melanoma detection: A challenge at the 2017 international symposium on biomedical imaging (isbi), hosted by the international skin imaging collaboration (isic). In 2018 IEEE 15th international symposium on biomedical imaging (ISBI 2018). IEEE, 168–172.\n\nCombalia et al. (2022) Marc Combalia, Noel Codella, Veronica Rotemberg, Cristina Carrera, Stephen Dusza, David Gutman, Brian Helba, Harald Kittler, Nicholas R Kurtansky, Konstantinos Liopyris, Michael A Marchetti, Sebastian Podlipnik, Susana Puig, Christoph Rinner, Philipp Tschandl, Jochen Weber, Allan Halpern, and Josep Malvehy. 2022. Validation of artificial intelligence prediction models for skin cancer diagnosis using dermoscopy images: the 2019 International Skin Imaging Collaboration Grand Challenge. The Lancet Digital Health 4, 5 (2022), e330–e339. https://doi.org/10.1016/S2589-7500(22)00021-8\n\nCombalia et al. (2019) Marc Combalia, Noel CF Codella, Veronica Rotemberg, Brian Helba, Veronica Vilaplana, Ofer Reiter, Cristina Carrera, Alicia Barreiro, Allan C Halpern, Susana Puig, et al. 2019. Bcn20000: Dermoscopic lesions in the wild. arXiv preprint arXiv:1908.02288 (2019).\n\nCompton et al. (2023) Rhys Compton, Lily Zhang, Aahlad Puli, and Rajesh Ranganath. 2023. When More is Less: Incorporating Additional Datasets Can Hurt Performance By Introducing Spurious Correlations. arXiv preprint arXiv:2308.04431 (2023).\n\nContractor et al. (2022) Danish Contractor, Daniel McDuff, Julia Katherine Haines, Jenny Lee, Christopher Hines, Brent Hecht, Nicholas Vincent, and Hanlin Li. 2022. Behavioral Use Licensing for Responsible AI. In Proceedings of the 2022 ACM Conference on Fairness, Accountability, and Transparency (Seoul, Republic of Korea) (FAccT ’22). Association for Computing Machinery, New York, NY, USA, 778–788. https://doi.org/10.1145/3531146.3533143\n\nDaneshjou et al. (2021) Roxana Daneshjou, Mary P Smith, Mary D Sun, Veronica Rotemberg, and James Zou. 2021. Lack of transparency and potential bias in artificial intelligence data sets and algorithms: a scoping review. JAMA dermatology 157, 11 (2021), 1362–1369.\n\nDeGrave et al. (2021) Alex J DeGrave, Joseph D Janizek, and Su-In Lee. 2021. AI for radiographic COVID-19 detection selects shortcuts over signal. Nature Machine Intelligence (2021), 1–10.\n\nDenton et al. (2021) Emily Denton, Alex Hanna, Razvan Amironesei, Andrew Smart, and Hilary Nicole. 2021. On the genealogy of machine learning datasets: A critical history of ImageNet. Big Data & Society 8, 2 (2021), 20539517211035955.\n\nDíaz et al. (2022) Mark Díaz, Ian Kivlichan, Rachel Rosen, Dylan Baker, Razvan Amironesei, Vinodkumar Prabhakaran, and Emily Denton. 2022. Crowdworksheets: Accounting for individual and collective identities underlying crowdsourced dataset annotation. In Proceedings of the 2022 ACM Conference on Fairness, Accountability, and Transparency. 2342–2351.\n\nFabbrizzi et al. (2022) Simone Fabbrizzi, Symeon Papadopoulos, Eirini Ntoutsi, and Ioannis Kompatsiaris. 2022. A survey on bias in visual datasets. Computer Vision and Image Understanding 223 (2022), 103552.\n\nFabris et al. (2022) Alessandro Fabris, Stefano Messina, Gianmaria Silvello, and Gian Antonio Susto. 2022. Algorithmic fairness datasets: the story so far. Data Mining and Knowledge Discovery 36, 6 (2022), 2074–2152.\n\nForte et al. (2009) Andrea Forte, Vanesa Larco, and Amy Bruckman. 2009. Decentralization in Wikipedia governance. Journal of Management Information Systems 26, 1 (2009), 49–72.\n\nGarbin et al. (2021) Christian Garbin, Pranav Rajpurkar, Jeremy Irvin, Matthew P Lungren, and Oge Marques. 2021. Structured dataset documentation: a datasheet for CheXpert. arXiv preprint arXiv:2105.03020 (2021).\n\nGebru et al. (2021) Timnit Gebru, Jamie Morgenstern, Briana Vecchione, Jennifer Wortman Vaughan, Hanna Wallach, Hal Daumé Iii, and Kate Crawford. 2021. Datasheets for datasets. Commun. ACM 64, 12 (2021), 86–92.\n\nGent (2023) Edd Gent. 2023. Public AI Training Datasets Are Rife With Licensing Errors. https://spectrum.ieee.org/data-ai\n\nGoldberger et al. (2000) Ary L Goldberger, Luis AN Amaral, Leon Glass, Jeffrey M Hausdorff, Plamen Ch Ivanov, Roger G Mark, Joseph E Mietus, George B Moody, Chung-Kang Peng, and H Eugene Stanley. 2000. PhysioBank, PhysioToolkit, and PhysioNet: components of a new research resource for complex physiologic signals. circulation 101, 23 (2000), e215–e220.\n\nGuo et al. (2016) Yandong Guo, Lei Zhang, Yuxiao Hu, Xiaodong He, and Jianfeng Gao. 2016. Ms-celeb-1m: A dataset and benchmark for large-scale face recognition. In Computer Vision–ECCV 2016: 14th European Conference, Amsterdam, The Netherlands, October 11-14, 2016, Proceedings, Part III 14. Springer, 87–102.\n\nHolland et al. (2018) S Holland, A Hosny, S Newman, J Joseph, and K Chmielinski. 2018. The dataset nutrition label: a framework to drive higher data quality standards. arXiv, arXiv: 180503677. (2018).\n\nHolland et al. (2020) Sarah Holland, Ahmed Hosny, Sarah Newman, Joshua Joseph, and Kasia Chmielinski. 2020. The dataset nutrition label. Data Protection and Privacy 12, 12 (2020), 1.\n\nHolste et al. (2023) Gregory Holste, Ziyu Jiang, Ajay Jaiswal, Maria Hanna, Shlomo Minkowitz, Alan C Legasto, Joanna G Escalon, Sharon Steinberger, Mark Bittman, Thomas C Shen, et al. 2023. How Does Pruning Impact Long-Tailed Multi-label Medical Image Classifiers?. In International Conference on Medical Image Computing and Computer-Assisted Intervention. Springer, 663–673.\n\nHolste et al. (2022) Gregory Holste, Song Wang, Ziyu Jiang, Thomas C Shen, George Shih, Ronald M Summers, Yifan Peng, and Zhangyang Wang. 2022. Long-tailed classification of thorax diseases on chest x-ray: A new benchmark study. In MICCAI Workshop on Data Augmentation, Labelling, and Imperfections. Springer, 22–32.\n\nHoover et al. (2000) AD Hoover, Valentina Kouznetsova, and Michael Goldbaum. 2000. Locating blood vessels in retinal images by piecewise threshold probing of a matched filter response. IEEE Transactions on Medical imaging 19, 3 (2000), 203–210.\n\nHutchinson et al. (2021) Ben Hutchinson, Andrew Smart, Alex Hanna, Emily Denton, Christina Greer, Oddur Kjartansson, Parker Barnes, and Margaret Mitchell. 2021. Towards accountability for machine learning datasets: Practices from software engineering and infrastructure. In Proceedings of the 2021 ACM Conference on Fairness, Accountability, and Transparency. 560–575.\n\nIrvin et al. (2019) Jeremy Irvin, Pranav Rajpurkar, Michael Ko, Yifan Yu, Silviana Ciurea-Ilcus, Chris Chute, Henrik Marklund, Behzad Haghgoo, Robyn Ball, Katie Shpanskaya, et al. 2019. Chexpert: A large chest radiograph dataset with uncertainty labels and expert comparison. In AAAI Conference on Artificial Intelligence, Vol. 33. 590–597.\n\nJha et al. (2020) Debesh Jha, Pia H Smedsrud, Michael A Riegler, Pål Halvorsen, Thomas de Lange, Dag Johansen, and Håvard D Johansen. 2020. Kvasir-seg: A segmented polyp dataset. In MultiMedia Modeling: 26th International Conference, MMM 2020, Daejeon, South Korea, January 5–8, 2020, Proceedings, Part II 26. Springer, 451–462.\n\nJiménez-Sánchez et al. (2023) Amelia Jiménez-Sánchez, Dovile Juodelyte, Bethany Chamberlain, and Veronika Cheplygina. 2023. Detecting Shortcuts in Medical Images - A Case Study in Chest X-Rays. In 2023 IEEE 20th International Symposium on Biomedical Imaging (ISBI). 1–5. https://doi.org/10.1109/ISBI53787.2023.10230572\n\nJohnson et al. (2019) Alistair EW Johnson, Tom J Pollard, Seth J Berkowitz, Nathaniel R Greenbaum, Matthew P Lungren, Chih-ying Deng, Roger G Mark, and Steven Horng. 2019. MIMIC-CXR, a de-identified publicly available database of chest radiographs with free-text reports. Scientific data 6, 1 (2019), 317.\n\nKnoll et al. (2020) Florian Knoll, Jure Zbontar, Anuroop Sriram, Matthew J Muckley, Mary Bruno, Aaron Defazio, Marc Parente, Krzysztof J Geras, Joe Katsnelson, Hersh Chandarana, et al. 2020. fastMRI: A publicly available raw k-space and DICOM dataset of knee images for accelerated MR image reconstruction using machine learning. Radiology: Artificial Intelligence 2, 1 (2020), e190007.\n\nKrizhevsky et al. (2009) Alex Krizhevsky, Geoffrey Hinton, et al. 2009. Learning multiple layers of features from tiny images. (2009).\n\nLarrazabal et al. (2020) Agostina J Larrazabal, Nicolás Nieto, Victoria Peterson, Diego H Milone, and Enzo Ferrante. 2020. Gender imbalance in medical imaging datasets produces biased classifiers for computer-aided diagnosis. Proceedings of the National Academy of Sciences 117, 23 (2020), 12592–12594.\n\nLeavy et al. (2021) Susan Leavy, Eugenia Siapera, and Barry O’Sullivan. 2021. Ethical data curation for ai: An approach based on feminist epistemology and critical theories of race. In Proceedings of the 2021 AAAI/ACM Conference on AI, Ethics, and Society. 695–703.\n\nLeCun et al. (1998) Yann LeCun, Léon Bottou, Yoshua Bengio, and Patrick Haffner. 1998. Gradient-based learning applied to document recognition. Proc. IEEE 86, 11 (1998), 2278–2324.\n\nLiu et al. (2015) Ziwei Liu, Ping Luo, Xiaogang Wang, and Xiaoou Tang. 2015. Deep Learning Face Attributes in the Wild. In Proceedings of International Conference on Computer Vision (ICCV).\n\nMadan (2022) Christopher R Madan. 2022. Scan once, analyse many: using large open-access neuroimaging datasets to understand the brain. Neuroinformatics 20, 1 (2022), 109–137.\n\nMarcus et al. (2007) Daniel S Marcus, Tracy H Wang, Jamie Parker, John G Csernansky, John C Morris, and Randy L Buckner. 2007. Open Access Series of Imaging Studies (OASIS): cross-sectional MRI data in young, middle aged, nondemented, and demented older adults. Journal of cognitive neuroscience 19, 9 (2007), 1498–1507.\n\nMei et al. (2022) Xueyan Mei, Zelong Liu, Philip M Robson, Brett Marinelli, Mingqian Huang, Amish Doshi, Adam Jacobi, Chendi Cao, Katherine E Link, Thomas Yang, et al. 2022. RadImageNet: an open radiologic deep learning research dataset for effective transfer learning. Radiology: Artificial Intelligence 4, 5 (2022), e210315.\n\nMeister et al. (2023) Nicole Meister, Dora Zhao, Angelina Wang, Vikram V Ramaswamy, Ruth Fong, and Olga Russakovsky. 2023. Gender artifacts in visual datasets. In Proceedings of the IEEE/CVF International Conference on Computer Vision. 4837–4848.\n\nMoreira et al. (2012) Inês C. Moreira, Igor Amaral, Inês Domingues, António Cardoso, Maria João Cardoso, and Jaime S. Cardoso. 2012. INbreast. Academic Radiology 19, 2 (feb 2012), 236–248. https://doi.org/10.1016/j.acra.2011.09.014\n\nNetzer et al. (2011) Yuval Netzer, Tao Wang, Adam Coates, Alessandro Bissacco, Bo Wu, and Andrew Y Ng. 2011. Reading digits in natural images with unsupervised feature learning. (2011).\n\nOakden-Rayner et al. (2020) Lauren Oakden-Rayner, Jared Dunnmon, Gustavo Carneiro, and Christopher Ré. 2020. Hidden stratification causes clinically meaningful failures in machine learning for medical imaging. In ACM Conference on Health, Inference, and Learning. 151–159.\n\nOliveira et al. (2023) Marta Oliveira, Rick Wilming, Benedict Clark, Céline Budding, Fabian Eitel, Kerstin Ritter, and Stefan Haufe. 2023. Benchmark data to study the influence of pre-training on explanation performance in MR image classification. arXiv preprint arXiv:2306.12150 (2023).\n\nOstrom (1990) Elinor Ostrom. 1990. Governing the commons: The evolution of institutions for collective action. Cambridge university press.\n\nParet et al. (2022) Christian Paret, Nike Unverhau, Franklin Feingold, Russell A Poldrack, Madita Stirner, Christian Schmahl, and Maurizio Sicorello. 2022. Survey on open science practices in functional neuroimaging. Neuroimage 257 (2022), 119306.\n\nPaullada et al. (2021) Amandalynne Paullada, Inioluwa Deborah Raji, Emily M Bender, Emily Denton, and Alex Hanna. 2021. Data and its (dis) contents: A survey of dataset development and use in machine learning research. Patterns 2, 11 (2021).\n\nPeng et al. (2021) Kenny Peng, Arunesh Mathur, and Arvind Narayanan. 2021. Mitigating dataset harms requires stewardship: Lessons from 1000 papers. arXiv preprint arXiv:2108.02922 (2021).\n\nPlomp et al. (2019) Esther Plomp, Nicolas Dintzner, Marta Teperek, and Alastair Dunning. 2019. Cultural obstacles to research data management and sharing at TU Delft. Insights 32, 1 (2019).\n\nPoldrack and Gorgolewski (2014) Russell A Poldrack and Krzysztof J Gorgolewski. 2014. Making big data open: data sharing in neuroimaging. Nature neuroscience 17, 11 (2014), 1510–1517.\n\nPoline et al. (2012) Jean-Baptiste Poline, Janis L Breeze, Satrajit Ghosh, Krzysztof Gorgolewski, Yaroslav O Halchenko, Michael Hanke, Christian Haselgrove, Karl G Helmer, David B Keator, Daniel S Marcus, et al. 2012. Data sharing in neuroimaging research. Frontiers in neuroinformatics 6 (2012), 9.\n\nPurtova and van Maanen (2023) Nadya Purtova and Gijs van Maanen. 2023. Data as an economic good, data as a commons, and data governance. Law, Innovation and Technology (2023), 1–42.\n\nPushkarna et al. (2022) Mahima Pushkarna, Andrew Zaldivar, and Oddur Kjartansson. 2022. Data cards: Purposeful and transparent dataset documentation for responsible ai. In Proceedings of the 2022 ACM Conference on Fairness, Accountability, and Transparency. 1776–1826.\n\nRaji et al. (2021) Inioluwa Deborah Raji, Emily M Bender, Amandalynne Paullada, Emily Denton, and Alex Hanna. 2021. AI and the everything in the whole wide world benchmark. arXiv preprint arXiv:2111.15366 (2021).\n\nRajpurkar et al. (2022) Pranav Rajpurkar, Emma Chen, Oishi Banerjee, and Eric J Topol. 2022. AI in health and medicine. Nature medicine 28, 1 (2022), 31–38.\n\nRistani et al. (2016) Ergys Ristani, Francesco Solera, Roger Zou, Rita Cucchiara, and Carlo Tomasi. 2016. Performance measures and a data set for multi-target, multi-camera tracking. In European conference on computer vision. Springer, 17–35.\n\nRombach et al. (2022) Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and BjÃ¶rn Ommer. 2022. High-Resolution Image Synthesis with Latent Diffusion Models. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR). https://github.com/CompVis/latent-diffusionhttps://arxiv.org/abs/2112.10752\n\nRussakovsky et al. (2015) Olga Russakovsky, Jia Deng, Hao Su, Jonathan Krause, Sanjeev Satheesh, Sean Ma, Zhiheng Huang, Andrej Karpathy, Aditya Khosla, Michael Bernstein, et al. 2015. Imagenet large scale visual recognition challenge. International journal of computer vision 115 (2015), 211–252.\n\nSambasivan et al. (2021) Nithya Sambasivan, Shivani Kapania, Hannah Highfill, Diana Akrong, Praveen Kumar Paritosh, and Lora Mois Aroyo. 2021. \"Everyone wants to do the model work, not the data work\": Data Cascades in High-Stakes AI.\n\nSchuhmann et al. (2022) Christoph Schuhmann, Romain Beaumont, Richard Vencu, Cade Gordon, Ross Wightman, Mehdi Cherti, Theo Coombes, Aarush Katta, Clayton Mullis, Mitchell Wortsman, et al. 2022. Laion-5b: An open large-scale dataset for training next generation image-text models. Advances in Neural Information Processing Systems 35 (2022), 25278–25294.\n\nSchumann et al. (2021) Candice Schumann, Susanna Ricco, Utsav Prabhu, Vittorio Ferrari, and Caroline Pantofaru. 2021. A step toward more inclusive people annotations for fairness. In Proceedings of the 2021 AAAI/ACM Conference on AI, Ethics, and Society. 916–925.\n\nSetio et al. (2017) Arnaud Arindra Adiyoso Setio, Alberto Traverso, Thomas De Bel, Moira SN Berens, Cas Van Den Bogaard, Piergiorgio Cerello, Hao Chen, Qi Dou, Maria Evelina Fantacci, Bram Geurts, et al. 2017. Validation, comparison, and combination of algorithms for automatic detection of pulmonary nodules in computed tomography images: the LUNA16 challenge. Medical image analysis 42 (2017), 1–13.\n\nSeyyed-Kalantari et al. (2020) Laleh Seyyed-Kalantari, Guanxiong Liu, Matthew McDermott, Irene Y Chen, and Marzyeh Ghassemi. 2020. CheXclusion: Fairness gaps in deep chest X-ray classifiers. In Pacific Symposium on Biocomputing. World Scientific, 232–243.\n\nSorkhei et al. (2021) Moein Sorkhei, Yue Liu, Hossein Azizpour, Edward Azavedo, Karin Dembrower, Dimitra Ntoula, Athanasios Zouzos, Fredrik Strand, and Kevin Smith. 2021. Csaw-m: An ordinal classification dataset for benchmarking mammographic masking of cancer. arXiv preprint arXiv:2112.01330 (2021).\n\nStaal et al. (2004) Joes Staal, Michael D Abràmoff, Meindert Niemeijer, Max A Viergever, and Bram Van Ginneken. 2004. Ridge-based vessel segmentation in color images of the retina. IEEE transactions on medical imaging 23, 4 (2004), 501–509.\n\nTarkowski et al. (2022) Alek Tarkowski, Paul Keller, Francesco Vogelezano, and Jan J. Zygmuntowski. 2022. Public Data Commons – A public-interest framework for B2G data sharing in the Data Act. https://openfuture.eu/publication/public-data-commons/\n\nThomas and Uminsky (2020) Rachel Thomas and David Uminsky. 2020. The Problem with Metrics is a Fundamental Problem for AI. arXiv preprint arXiv:2002.08512 (2020).\n\nTschandl et al. (2018) Philipp Tschandl, Cliff Rosendahl, and Harald Kittler. 2018. The HAM10000 dataset, a large collection of multi-source dermatoscopic images of common pigmented skin lesions. Scientific data 5, 1 (2018), 1–9.\n\nVaroquaux and Cheplygina (2022) Gaël Varoquaux and Veronika Cheplygina. 2022. Machine learning for medical imaging: methodological failures and recommendations for the future. Nature Digital Medicine 5, 1 (2022), 1–8.\n\nWah et al. (2011) Catherine Wah, Steve Branson, Peter Welinder, Pietro Perona, and Serge Belongie. 2011. The caltech-ucsd birds-200-2011 dataset. (2011).\n\nWang et al. (2017) Xiaosong Wang, Yifan Peng, Le Lu, Zhiyong Lu, Mohammadhadi Bagheri, and Ronald M Summers. 2017. Chestx-ray8: Hospital-scale chest x-ray database and benchmarks on weakly-supervised classification and localization of common thorax diseases. In Computer Vision and Pattern Recognition. 2097–2106.\n\nWhang et al. (2023) Steven Euijong Whang, Yuji Roh, Hwanjun Song, and Jae-Gil Lee. 2023. Data collection and quality challenges in deep learning: A data-centric ai perspective. The VLDB Journal 32, 4 (2023), 791–813.\n\nWilkinson et al. (2016) Mark D Wilkinson, Michel Dumontier, IJsbrand Jan Aalbersberg, Gabrielle Appleton, Myles Axton, Arie Baak, Niklas Blomberg, Jan-Willem Boiten, Luiz Bonino da Silva Santos, Philip E Bourne, et al. 2016. The FAIR Guiding Principles for scientific data management and stewardship. Scientific data 3, 1 (2016), 1–9.\n\nWillemink et al. (2020) Martin J Willemink, Wojciech A Koszek, Cailin Hardell, Jie Wu, Dominik Fleischmann, Hugh Harvey, Les R Folio, Ronald M Summers, Daniel L Rubin, and Matthew P Lungren. 2020. Preparing Medical Imaging Data for Machine Learning. Radiology (2020), 192224.\n\nWilliamson (2020) Robert Williamson. 2020. Process and Purpose, Not Thing and Technique: How to Pose Data Science Research Challenges. (2020).\n\nWinkler et al. (2019) Julia K Winkler, Christine Fink, Ferdinand Toberer, Alexander Enk, Teresa Deinlein, Rainer Hofmann-Wellenhof, Luc Thomas, Aimilios Lallas, Andreas Blum, Wilhelm Stolz, et al. 2019. Association between surgical skin markings in dermoscopic images and diagnostic performance of a deep learning convolutional neural network for melanoma recognition. JAMA dermatology 155, 10 (2019), 1135–1141.\n\nXiao et al. (2017) Han Xiao, Kashif Rasul, and Roland Vollgraf. 2017. Fashion-mnist: a novel image dataset for benchmarking machine learning algorithms. arXiv preprint arXiv:1708.07747 (2017).\n\nZając et al. (2023) Hubert Dariusz Zając, Natalia Rozalia Avlona, Finn Kensing, Tariq Osman Andersen, and Irina Shklovski. 2023. Ground Truth Or Dare: Factors Affecting The Creation Of Medical Datasets For Training AI. In Proceedings of the 2023 AAAI/ACM Conference on AI, Ethics, and Society. 351–362.\n\nZhao et al. (2021) Dora Zhao, Angelina Wang, and Olga Russakovsky. 2021. Understanding and evaluating racial biases in image captioning. In Proceedings of the IEEE/CVF International Conference on Computer Vision. 14830–14840.\n\nZhou et al. (2014) Bolei Zhou, Agata Lapedriza, Jianxiong Xiao, Antonio Torralba, and Aude Oliva. 2014. Learning deep features for scene recognition using places database. Advances in neural information processing systems 27 (2014).\n\nZhou et al. (2020) S Kevin Zhou, Hayit Greenspan, Christos Davatzikos, James S Duncan, Bram van Ginneken, Anant Madabhushi, Jerry L Prince, Daniel Rueckert, and Ronald M Summers. 2020. A review of deep learning in medical imaging: Image traits, technology trends, case studies with progress highlights, and future promises. arXiv preprint arXiv:2008.09104 (2020)."
    }
}
{
    "id": "dbpedia_121_1",
    "rank": 11,
    "data": {
        "url": "https://www.altexsoft.com/blog/data-labeling/",
        "read_more_link": "",
        "language": "en",
        "title": "What Is Data Labeling",
        "top_image": "https://www.altexsoft.com/static/blog-post-featured/2023/10/f0974593-0027-455a-bdbd-0e76381a2e44.jpg",
        "meta_img": "https://www.altexsoft.com/static/blog-post-featured/2023/10/f0974593-0027-455a-bdbd-0e76381a2e44.jpg",
        "images": [
            "https://www.altexsoft.com/static/blog-post-featured/2023/10/f0974593-0027-455a-bdbd-0e76381a2e44.jpg",
            "https://www.altexsoft.com/media/2021/12/word-image-8.jpeg",
            "https://www.altexsoft.com/static/blog-post/2023/11/f923f4f7-ba17-4310-842b-fb329d2eebe2.jpg",
            "https://www.altexsoft.com/media/2021/12/object-detection-1.png",
            "https://www.altexsoft.com/media/2021/12/bike-annotated-with-the-help-of-a-polygon-source.png",
            "https://www.altexsoft.com/media/2021/12/differences-between-image-classification-object-d.jpeg",
            "https://www.altexsoft.com/media/2021/12/the-example-of-3d-cuboid-annotation-source-cogit.jpeg",
            "https://www.altexsoft.com/static/blog-post/2024/1/01e41c85-45c2-46c0-9bbf-8434a921e5f2.jpg",
            "https://www.altexsoft.com/_next/static/images/YoutubePlayButton-d3972df504617e41e724a8690caef270.svg",
            "https://www.altexsoft.com/media/2021/12/the-example-of-how-the-sentence-is-annotated-by-en.png",
            "https://www.altexsoft.com/static/blog-post/2023/11/7583aa04-b703-46c4-b32e-98612bd16f20.jpg",
            "https://www.altexsoft.com/media/2021/12/pos-tagging-example-source-towards-data-science.png",
            "https://www.altexsoft.com/static/blog-post/2023/11/4703fccc-98d8-440f-b897-62150b6ced36.jpg"
        ],
        "movies": [],
        "keywords": [],
        "meta_keywords": [
            ""
        ],
        "tags": null,
        "authors": [
            "Editor"
        ],
        "publish_date": "2021-12-21T00:00:00+00:00",
        "summary": "",
        "meta_description": "Our informative guide explains data labeling, its main types, and best practices to help your ML project reach the best possible results.",
        "meta_lang": "en",
        "meta_favicon": "https://www.altexsoft.com/static/favicon/apple-touch-icon.png",
        "meta_site_name": "AltexSoft",
        "canonical_link": "https://www.altexsoft.com/blog/data-labeling/",
        "text": "When people hear about artificial intelligence, deep learning, and machine learning, many think of movie-like robots that resemble or even outperform human intelligence. Others believe that such machines simply consume information and learn from it by themselves. Well... It's kind of far from the truth. Computer systems have limited capabilities without human guidance, and data labeling is the way to teach them to become \"smart.\"\n\nIn this article, you will find out what data labeling is, how it works, data labeling types, and the best practices to follow to make this process smooth as glass.\n\nWhat is data labeling?\n\nData labeling (sometimes referred to as data annotation) is the process of adding metadata, or tags, to raw data to show a machine learning model the target attributes — answers — it is expected to predict. A label or a tag is a descriptive element that tells a model what an individual data piece is so it can learn by example. Say the model needs to predict music genre. In this case, the training dataset will consist of multiple songs with labels showing genres like pop, jazz, rock, etc.\n\nIn this way, labeled data underlines data features (characteristics) to help the model analyze information and identify the patterns within historical data to make accurate predictions on new, relevantly similar inputs.\n\nThe process of labeling objects in a picture via the LabelImg graphical image annotation tool. Source: GitHub\n\nThe process of labeling data is one of the essential stages in preparing data for supervised machine learning workflows.\n\nMore on how data is preprocessed for machine learning can be found in our dedicated video and/or article on preparing a dataset for ML.\n\nhttps://www.youtube.com/watch?v=P8ERBy91Y90&ab_channel=AltexSoft\n\nSo, what challenges does data labeling involve?\n\nData labeling challenges\n\nHigh cost in terms of time and effort. Not only is it hard to get lots of data (particularly for highly specialized niches such as healthcare), but manually adding tags for each item of data is also a difficult, time-consuming task requiring the work of human labelers. Within the full cycle of ML development, data preparation (including labeling) takes up almost 80 percent of the project time.\n\nNeed for domain expertise. You may need to hire domain experts to add metadata. For example, if you are about to build an ML model capable of recognizing tumors on X-Ray scans, unprepared annotators will unlikely do that correctly.\n\nRisk of inconsistency. In most cases, cross-labeling — when several people label the same sets of data — leads to higher accuracy. But as people often have different levels of expertise, labeling criteria and labels themselves may be inconsistent, which is another challenge on the list. There might be disagreement on some tags between two or more annotators. For instance, one expert may score a hotel review as positive while the other may see it as sarcastic and give it a negative label.\n\nError proneness. No matter how experienced and attentive your labelers are, manual tagging is prone to human error. This is unavoidable because annotators usually deal with large sets of raw data. Imagine a person marking 150,000 pictures with up to 10 objects each!\n\nAnd yet, with all these shortcomings, data labeling is still the cornerstone of most machine learning projects. So, let’s deal with how this process happens.\n\nData labeling approaches\n\nThere are different ways to perform data annotation. The choice of the style depends on the complexity of the problem statement, the amount of data to be tagged, the size of a data science team, and, of course, your financial resources and available time. We’re going to quickly review each approach here. But you can go back to the dedicated article on how to organize data labeling for your machine learning project to dive deeper.\n\nIn-house data labeling. As the name suggests, in-house data labeling is carried out by specialists within an organization. It's the way to go when you have enough time, human and financial resources, and it provides the highest possible labeling accuracy. On the flip side, it is slow.\n\nCrowdsourcing. There are special crowdsourcing platforms, like Amazon Mechanical Turk (MTurk), where you can sign in as a requester and assign different labeling tasks to available contractors. Affordable and relatively fast, the approach can’t guarantee high quality of annotated data.\n\nOutsourcing. Another way to get things done is to outsource data labeling jobs to freelancers that can be found on multiple recruitment and freelance platforms like Upwork. Similar to the previous approach, outsourcing is a quick way to obtain data labeling services but the quality may sink.\n\nAutomatic data labeling. Apart from being performed manually, the process can be assisted by software. Tags can be identified and added to the training dataset automatically with the help of the technique known as active learning. Basically, human experts create an AI Auto-label model that marks raw, unlabeled data. After that, they identify whether the model has done the labeling correctly. In the case of failure, human labelers correct the errors and re-train the model.\n\nSynthetic data development. Synthetic data is an artificially generated dataset with labels that comes as an alternative to real-world data. It is created by computer simulations or algorithms and is often used to train machine learning models. In the context of labeling approaches, synthetic data is a great solution to the problems of data shortage and diversity. The solution is the production of artificial data from scratch. Dataset developers create 3D environments with objects and surroundings the model will need to recognize. It is possible to render as much synthetic data as needed for the project.\n\nHow data labeling works\n\nWhatever the approach, the process of data labeling works in the following chronological order.\n\nData collection. The starting point of any ML project is collecting the right amount of raw data (images, audio files, videos, texts, etc.) Sources may differ from one company to another. Some organizations have been accumulating information internally for years. Others use publicly available datasets. Either way, this data is often inconsistent, corrupted, or simply unsuitable for the case. So, it needs to undergo cleaning and preprocessing before any labels are created. As a rule, there should be a large amount of diverse data for a model to provide more accurate results.\n\nData annotation. Here comes the heart and soul of the process. Specialists go through the data and add metadata labels to it. In this way, they attach meaningful context that the model can use as ground truth — target variables you want your model to predict. These can be, for example, tags in images describing the depicted objects.\n\nQuality assurance. Data must be high quality, reliable, accurate, and consistent. The quality in datasets for ML model training is determined by how precisely the labels are added to a particular data point. To ensure the accuracy of metadata and optimize it when needed, there must be continuous QA checks in place.\n\nFor this purpose, labelers often use such QA algorithms as\n\nthe Consensus algorithm — data reliability is achieved through agreement on a single data point among different systems or individuals — and\n\nthe Cronbach’s alpha test — the average consistency of data items in a set is measured.\n\nAn often overlooked stage of data labeling, quality assurance contributes greatly to the accuracy of the results.\n\nModel training and testing. A logical follow-up to the previous stages is the use of labeled data containing the correct answers to train the model. The process typically involves testing the model on an unlabeled data set to see if it delivers the expected predictions or estimations. Based on the use case, you will decide on confidence scores or accuracy levels. Say, the model is considered to be successfully trained if the accuracy is 96 percent and higher, meaning it makes 960 good predictions out of 1000 examples.\n\nWith the basics explained, let’s move on to reviewing typical data labeling types for each AI domain, namely\n\nimage and video labeling,\n\ntext labeling, and\n\naudio labeling.\n\nImage and video labeling for computer vision tasks\n\nComputer vision (CV) is a subset of artificial intelligence (AI) that enables machines to “see.” While it sounds too futuristic, the ability of computers to see basically means that they can derive meaningful information from visual inputs like digital images, simulating human visual perception. To make that happen, computers need\n\nimage annotation — the process of assigning tags to images and\n\nvideo annotation — the process of adding labels to video frames (still images extracted from video footage).\n\nBased on the CV task, workers may use different types of image/video annotation.\n\nImage classification\n\nThe essential CV task, image classification is the process of attaching one label (single-label classification) or several labels (multi-label classification) to an image itself based on what class the depicted object belongs to. Whether there is a cat or two cats in the picture, it will be labeled as the “cat.” The same goes for video clip classification.\n\nImage classification example\n\nBounding boxes\n\nThe most common way to annotate images, bounding boxes are rectangular or square boxes that are drawn around objects to show their location in the image. The boxes are identified by the x and y-axis coordinates in the upper left and lower right corners of the rectangle.\n\nBounding boxes are mainly used in object detection and classification with localization tasks.\n\nObject detection deals with detecting and classifying multiple objects in an image or a video frame along with showing the positions of each one with bounding boxes.\n\nImage classification with localization means classifying images into a few predefined classes based on the objects depicted and drawing boxes around those objects. Unlike object detection, this approach is commonly used to define the location of a single object or several objects as one entity (see the picture below).\n\nImage classification with localization vs object detection\n\nPolygon annotation\n\nPolygon annotation uses closed polygonal chains to determine the location and shape of objects. Since non-rectangular shapes (like a bike) are more common for real-world environments, polygons are a more suitable image annotation variant than bounding boxes.\n\nBike annotated with the help of a polygon. Source: COCO dataset\n\nWith polygons, data annotators can take more lines and angles in work and change the directions of verticals to show the object's true shape more accurately.\n\nSemantic segmentation\n\nSemantic segmentation is an annotation approach used to assign a label to specific pixels of an image that belong to a corresponding class of what is shown. Annotators would draw polygons around a set of pixels they want to tag and group those of the same class as one entity, separating them from the background and other objects. For example, in the picture below, all sheep are segmented as one object while road and grass make the other two objects.\n\nDifferences between image classification, object detection, semantic segmentation, and instance segmentation. Source: Medium\n\nA similar approach is instance segmentation but it differentiates multiple objects of the same class and treats them as individual instances. For example, each sheep in the image is segmented as an individual object in addition to road and grass.\n\n3D cuboids\n\nThe example of 3D cuboid annotation. Source: Cogito Tech\n\n3D cuboids are 3D representations of objects, meaning they depict not only the length and width of each object but also its depth. In this way, annotators can show the volume feature in addition to the position. If the edges of the object are hidden from view or blocked by other objects, annotators will define them approximately.\n\nKey-point annotation\n\nKey-point annotation involves adding dots across the image and connecting them by edges. At the output, you get the x and y-axis coordinates of key points that are numbered in a certain order. The approach is used to identify small objects and shape variations that have the same structure (e.g., facial expressions and features, human body parts and poses).\n\nAn example of key-point annotation of a video clip. Source: Keymakr\n\nKey-point annotation is commonly used for labeling video frames.\n\nImage and video labeling use cases\n\nBoth video and image annotation sees a wide array of practical applications in different areas.\n\nIn healthcare, you can label disease symptoms in an X-ray, MRI, and CT scan as well as in microscopic images for diagnostic analysis and disease detection (for instance, detecting cancer cells at early stages).\n\nIn logistics and transportation, you can label barcodes, QR codes, and other identification codes to track goods and enable smart logistics.\n\nIn the automotive industry, you can segment vehicles, roads, buildings, pedestrians, cyclists, and other objects in images and videos to help autonomous cars distinguish these entities and avoid contact with them in real life.\n\nIn cybersecurity, you can annotate facial features and emotions to help AI systems identify individuals in images and security video footage.\n\nText labeling for natural language processing tasks\n\nNatural language processing or NLP is a subset of artificial intelligence that enables machines to interpret human language. NLP uses the power of linguistics, statistics, and machine learning to study the structure and rules of language and create smart systems that can derive meaning from text and speech. The algorithms can analyze various linguistic aspects such as semantics, syntax, pragmatics, and morphology and then apply this knowledge to perform desired tasks.\n\nHere are a few popular ways to annotate the text for NLP tasks.\n\nText classification\n\nText classification (also referred to as text categorization or text tagging) is a process of assigning one or several labels to the text blocks as a whole to classify them based on predefined categories, trends, subjects, or other parameters.\n\nSentiment annotation is used for sentiment analysis, which is the process of understanding whether a given text delivers a positive, negative, or neutral message. For example:\n\n“Customer service was top notch.” → Positive message\n\n“ They won’t respond to requests on the app and put me on hold for 30 mins.” → Negative message\n\n“The new design of the website is generally fine.” → Neutral message\n\nSpeaking of sentiment analysis, AltexSoft has built an ML-powered model capable of scoring hotel amenities based on customer reviews. It was an interesting journey we’d like to share with you.\n\nTopic categorization is the task of identifying the topic a piece of text conveys. Annotation involves adding theme labels such as Pricing or Ease of Use when analyzing customer reviews, for example.\n\nLanguage categorization is the task of detecting the language of a text. Annotation involves adding corresponding language labels to texts based on the language they are written in.\n\nEntity annotation\n\nEntity annotation is the act of detecting, locating, and tagging some fundamental entities inside each line of the unlabeled text. Unlike text classification, entity annotation deals with labeling individual words and phrases.\n\nThere are several types of entity annotation such as named entity recognition, keyphrase tagging, and Part-of-Speech tagging.\n\nNamed entity recognition extracts, chunks, and identifies entities and annotates them with proper names. The most common categories include names of people, products, organizations, locations, dates, and times, etc.\n\nAn example of how the sentence is annotated by named entities. Source: Towards Data Science\n\nLet’s imagine you are building an ML model designed to trade stocks depending on the events in the news. The algorithm goes through data and comes across the headline that looks like this:\n\nNews headline from the Financial Express resource\n\nWhile a human can easily understand the context, the machine won’t be able to tell if the word “Tesla'” refers to a company name, an item of a Tesla vehicle, or … you never know … Nicola Tesla. Named entity annotation does this job for a machine and explains the right meaning.\n\nKeyphrase or keyword extraction and tagging. In addition to annotating named entities, labelers may also tag keyphrases or keywords in the text. This type of text labeling can be used to summarize documents/paragraphs or index a corpus of texts. The procedure requires a thorough examination of each text block to extract the most meaningful keywords.\n\nPart-of-Speech or POS tagging. This approach involves annotating all the words within a sentence according to their parts of speech. The labels will include nouns, verbs, prepositions, adjectives, etc.\n\nPOS tagging example. Source: Towards Data Science\n\nIt is used for identifying relationships between words in a sentence and drawing out meanings. With proper POS tagging, algorithms can determine the exact representation of similar words in different situations and as such provide more accurate results.\n\nEntity linking\n\nEntity linking is the process of labeling certain entities in text and connecting them to larger repositories of data. Basically, that’s when tagged entities are linked to URLs with sentences, phrases, and facts offering more information about them. The procedure is especially important for cases when the corpus of text contains data that can be interpreted in multiple ways.\n\nText labeling use cases\n\nThere are many interesting applications for text annotation and NLP processes, namely\n\ntagging messages as spam and ham (non-spam) for spam detection,\n\nexplaining text meaning and word relationships for machine translation,\n\nclassifying documents,\n\ndesigning conversations for chatbots, and\n\nextracting consumer feelings and opinions from reviews for sentiment analysis.\n\nAudio labeling for speech recognition tasks\n\nSpeech recognition is a subfield of artificial intelligence that enables machines to understand voice narration by identifying spoken words and converting them into text. While audio information is clear to humans, computing machinery can't understand its semantic structure as effortlessly. To deal with this issue, there is audio labeling -- when you assign labels and transcripts to audio recordings and put them in a format for a machine learning model to understand.\n\nSpeaker identification\n\nSpeaker identification is a process of adding labeled regions to audio streams and identifying the start and end timestamps for different speakers. Basically, you break the input audio file into segments and assign labels to parts with speaker voices. Often, segments with music, background noise, and silence are marked too.\n\nAudio-transcription annotation\n\nAnnotation of linguistic data in audio files is a more complex process that requires adding tags for all surrounding sounds and transcripts for speech in addition to linguistic regions. Many audio and video annotation tools allow users to combine different inputs like audio and text into a single, straightforward audio-transcription interface.\n\nThe process of audio annotation using the Prodigy tool.\n\nOnce the spoken language is converted into a written form, previously discussed NLP tasks come into play.\n\nAudio classification\n\nAudio classification jobs require human annotators to listen to the audio recordings and classify them based on a series of predefined categories. The categories may describe the number or type of speakers, the intent, the spoken language or dialect, the background noise, or semantically related information.\n\nAudio emotion annotation\n\nAudio emotion annotation, as the name suggests, aims at identifying the speaker’s feelings such as happiness, sadness, anger, fear, and surprise, to name a few. This process is more accurate than textual sentiment analysis since audio streams provide a number of additional clues such as voice intensity, pitch, pitch jumps, or speech rate.\n\nAudio labeling use cases\n\nSince adding labels to audio and video files is a cornerstone of speech recognition, it finds use in\n\ndeveloping voice assistants like Siri and Alexa,\n\ntranscribing speech to text,\n\nproviding the context of conversations for advanced chatbots,\n\nmeasuring customer satisfaction for support calls, and\n\ndesigning apps for language learning and pronunciation assessment, to name a few.\n\nBest practices to nail data labeling\n\nAs you can see manual data labeling work isn’t a piece of cake. To overcome the challenges and make the most out of the process, there are some best practices you can follow.\n\nFind quality raw data\n\nThe big step towards accurate results of your machine learning model is high raw data quality. Before any annotations are assigned, make sure that data is adequate for the task, properly cleaned, and balanced. That said, if you are building a model to classify images into those containing cats and dogs, unlabeled data must include both animals in equal proportion and without any noisy elements.\n\nFind domain experts\n\nHalf the success of data labeling work is determined by the ability of annotators to understand the subject matter. It is always better to hire experts who know the industry in and out and will be able to tag data more accurately and faster.\n\nCreate an annotation handbook\n\nDecide on your annotation techniques and criteria and round up all important points in a handbook. It may include thoroughly-explained instances of correct, incorrect, and edge tag examples.\n\nQA test your datasets\n\nNever underestimate the importance of QA testing. You should incorporate QA processes into your data labeling pipeline to make sure that labeling goes in accordance with set criteria and mistakes are corrected in a timely fashion.\n\nAssign multiple annotators for cross-labeling\n\nWhatever the annotation job, it is always better to have several (up to three) annotators cross-label the same data to validate the tags. Following this practice, you will be able to ensure higher accuracy of the results."
    }
}
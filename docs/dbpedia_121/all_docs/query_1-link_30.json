{
    "id": "dbpedia_121_1",
    "rank": 30,
    "data": {
        "url": "https://dl.acm.org/doi/fullHtml/10.1145/3548636.3548652",
        "read_more_link": "",
        "language": "en",
        "title": "Multi-label Learning with User Credit Data in China Based on MLKNN",
        "top_image": "https://dl.acm.org/cms/attachment/a2effabd-2c64-401c-8e0e-b699dc509a2c/image57.png",
        "meta_img": "",
        "images": [
            "https://dl.acm.org/cms/attachment/a2effabd-2c64-401c-8e0e-b699dc509a2c/image57.png",
            "https://dl.acm.org/cms/attachment/05accddb-11c3-4b03-8c26-ab4f70826794/image58.png",
            "https://dl.acm.org/cms/attachment/a1aff21b-f740-415b-b2e8-6845b74e5ed3/image59.png",
            "https://dl.acm.org/cms/attachment/e31a4d78-0d96-4cff-b920-e964afbc13e6/image60.png",
            "https://dl.acm.org/cms/attachment/74bcf80d-b824-4898-9b81-b6b2fd3af66d/image61.png"
        ],
        "movies": [],
        "keywords": [],
        "meta_keywords": [
            ""
        ],
        "tags": null,
        "authors": [
            "Zhuangyi Zhang",
            "School of Management Science",
            "Central University of Finance",
            "Lu Han",
            "Muzi Chen"
        ],
        "publish_date": null,
        "summary": "",
        "meta_description": "",
        "meta_lang": "en",
        "meta_favicon": "",
        "meta_site_name": "",
        "canonical_link": null,
        "text": "3.1 MLKNN\n\nThe basic idea of the MLKNN algorithm is to learn from the idea of the KNN algorithm to find K samples adjacent to the predicted sample, count the number of each label in these K samples, and then calculate the probability that the test sample contains each label based on the posterior probability. At last consider the label with the predicted probability greater than a certain threshold as the label of the predicted sample. The specific mathematical form and symbolic expression of the algorithm are as follows.\n\nSuppose the training set is $X = \\{ {x_1},{x_2},{x_3} \\ldots {x_n}\\} $, indicates that there are n training samples; The label set is $Y = \\{ {y_{x1}},{y_{x2}},{y_{x3}} \\ldots {y_{xn}}\\} $, represents the label set corresponding to each sample; $L = \\{ {l_1},{l_2},{l_3} \\ldots {l_m}\\} $ is the sample label set, and the number of labels is m. Known training samples ${x_i} \\in X$,The corresponding label set is ${y_{{x_i}}} \\subseteq Y$. If ${y_{{x_i}}}({l_j}) = 1$, It means that the j-th label is included in the label set of the i-th sample. On the contrary, ${y_{{x_i}}}({l_j}) = 0$,indicates that the label set of the i-th sample does not contain the j-th label. In addition, suppose $N({x_i})$ Represents the sample set consisting of the k nearest samples of ${x_i}$. ${C_{{x_i}}}({l_j})$ refers to the number of samples with label ${l_j}$ among the k neighbors of ${x_i}$.$H_1^{{l_j}}$ indicates the event that sample ${x_i}$ with label ${l_j}$, $H_0^{{l_j}}$ indicates the event that sample ${x_i}$ not contain label ${l_j}$.$E_q^{{l_j}}$ means the event that among the k neighbors of sample ${x_i}$, there are q samples that contain with label ${l_j}$. At this time, the formula for MLKNN for multi-label classification obtained according to Bayes theorem is as following equation (1).\n\n\\begin{equation} {y_{{x_i}}}({l_j}) = \\mathop {\\arg \\max }\\limits_{b \\in \\{ 0,1\\} } \\frac{{P(H_b^{{l_j}})P(E_q^{{l_j}}|H_b^{{l_j}})}}{{P(E_q^{{l_j}})}} = \\mathop {\\arg \\max }\\limits_{b \\in \\{ 0,1\\} } P(H_b^{{l_j}})P(E_q^{{l_j}}|H_b^{{l_j}}) \\end{equation}\n\n(1)\n\nWhere, b is 0 or 1. If we want to know whether label ${l_j}$ belongs to sample ${x_i}$, we only need to calculate which value b takes to maximize the value of the formula. If the value of the formula is the largest when b=1, it proves that ${y_{{x_i}}}({l_j}) = 1$, i.e., sample ${x_i}$ has label ${l_j}$; on the contrary, if the value of the formula is the largest when b=0, it proves that ${y_{{x_i}}}({l_j}) = 0$, i.e., sample ${x_i}$ does not have label ${l_j}$.\n\nFor each label, the corresponding prior probability can be calculated by Equation (2).\n\n\\begin{equation} \\begin{array}{@{}l@{}} P(H_1^{{l_j}}) = (s + \\sum\\limits_{i = 1}^n {{y_{{x_i}}}} )/(s \\times 2 + n)\\\\ P(H_0^{{l_j}}) = 1 - P(H_1^{{l_j}}) \\end{array} \\end{equation}\n\n(2)\n\nWhere, s is a smoothing parameter. According to previous literature research, the value is generally 1. $\\sum\\limits_{i = 1}^n {{y_{{x_i}}}} $represents the total number of samples with label ${l_j}$ among n training samples.\n\nThe posterior probability can be calculated by Equation (3) and Equation (4).\n\n\\begin{equation} P(E_q^{{l_j}}|H_1^{{l_j}}) = (s + c[q])/(s \\times (k + 1) + \\sum\\nolimits_{p = 0}^k {c[p]} ) \\end{equation}\n\n(3)\n\n\\begin{equation} P(E_q^{{l_j}}|H_1^{{l_j}}) = (s + c'[q])/(s \\times (k + 1) + \\sum\\nolimits_{p = 0}^k {c'[p]} ) \\end{equation}\n\n(4)\n\nWhere, q represents the number of samples with label ${l_j}$ among the k nearest neighbor samples of the test sample ${x_i}$.\n\n$c[q]$ represents the number of samples that not only q samples have label ${l_j}$ among their k neighbor samples, but they also have label ${l_j}$ themselves. However $c'[q]$ represents the number of samples in which only q samples out of the k nearest neighbor samples have label ${l_j}$, but do not have label ${l_j}$ themselves. Then, this paper uses Equation(5) to calculate the probability that the sample ${x_i}$ contains the label ${l_j}$.\n\n\\begin{equation} P({l_j}) = P(H_1^{{l_j}})P(E_q^{{l_j}}|H_1^{{l_j}})/P(H_1^{{l_j}})P(E_q^{{l_j}}|H_1^{{l_j}}) + P(H_0^{{l_j}})P(E_q^{{l_j}}|H_0^{{l_j}}) \\end{equation}\n\n(5)\n\n4.1 Dataset\n\nThe datasets in this experiment is the credit data of some users from 2008 to 2012 provided by the Credit Information Center of the People's Bank of China, and it contains about 10,000 user records. The attribute content involved in the data basically includes three aspects: basic personal information, account opening information and credit activity information. There are 37 attributes, including 6 binary attributes, 12 nominal attributes and 19 numerical attributes. There are a total of 8 corresponding labels, which are considered from three aspects: personal development stability, frequency of credit activities, and credit status attention. This part of information is obtained from credit institutions. However, there are a lot of missing data and incomplete information in these data. Therefore, before performing experiments, data cleaning and data preprocessing are required to ensure the quality of the data used for model training.\n\nFirst of all, in the process of data cleaning, this paper selects 12 most valuable attributes from 37 attributes for experiments based on the relevance and importance of attributes (including 3 nominal attributes and 9 numerical attributes) ), and then deleted the missing data and obviously unreasonable data in these data, and finally selected 1000 data with complete information for experiment. The basic information of the datasets used in the experiment is described in Table 1, where Cardinality represents the average number of labels per sample; Density represents the label density, which is obtained by dividing Cardinality by the number of labels; Proportion represents the proportion of distinctive label sets to the number of samples.\n\nExamples Features Labels train test Nominal Numeric Numbers Cardinality Density Proportion 700 300 3 9 8 3 0.375 0.018\n\nSecondly, in the process of data preprocessing, since the original attribute data is different in nature and magnitude, this paper needs to uniformly transform the nominal attribute and the numerical attribute into discrete variables and perform segmentation processing. The specific The processing process is shown in Table 2.\n\nAttribute name Data conversion process education Primary school=1;Secondary technical school=2;Junior high school=3;Senior middle school =4;Junior college=5;University=6;Postgraduate=7 year_income 1∼10000RMB=1;10001∼50000RMB=2;50001∼100000RMB=3;100001∼500000RMB=4;500001∼1000000RMB=5;more than 1000000RMB=6; marital_status Sigle=1;Married=2;Divorce=3;Others (widowhood, remarriage, etc.)=4 career Soldier=1;Heads of state agencies, party organizations, enterprises, and institutions=2;Clerks and related personnel=3;Production personnel in agriculture, forestry, animal husbandry, fishery and water conservancy=4;Commercial and service industry personnel=5;Professional skill worker=6;Production and transportation equipment operators and related personnel=7 credit_account 1∼5=1;6∼10=2;11∼20=3;21∼50=4;more than 50=5; loan_strokecount 0∼2times=1;3∼5times=2;6∼8times=3;9∼11times=4;more than 11times=5; total_credit_amount 1∼10000RMB=1;10001∼50000RMB=2;50001∼100000RMB=3;100001∼500000RMB=4;500001∼1000000RMB=5;More than 1000000RMB=6; total_use_amount 1∼10000RMB=1;10001∼50000RMB=2;50001∼100000RMB=3;100001∼500000RMB=4;500001∼1000000RMB=5;More than 1000000RMB=6; credit_amount_utilization_rate 0∼0.3=1;0.3∼0.6=2;0.6∼0.9=3;0.9∼1=4; query 1∼5times=1;6∼10times=2;11∼20times=3;21∼50times=4;51∼100times=5;more than 100times=6; credit_over_amount No overdraft=0;Overdraft=1 total_over_amount No overdue=0;Overdue=1\n\nAfter processing the attributes, the label information also needs to be converted into values. The label information is obtained based on the empirical data of the credit institution. The specific label information and serial numbers are shown in Table 3.\n\ncoding 1 2 3 4 Labels Name Personal development stability Personal development instability Low frequency of credit activities Medium frequency of credit activities coding 5 6 7 8 Labels Name High frequency of credit activities Low attention to credit status Normal attention to credit status High attention to credit status\n\nIn the experiment, we use a matrix to indicate the belonging of the sample to the label, build an m*n matrix, let n be the number of samples, m is the number of labels, let ${m_i}{n_j}$= -1 or 1, where =1 means the j-th sample has a label i. Otherwise, it means that the sample does not have label i.\n\n4.3 Result analysis\n\nFollowing the above parameter settings, we use the MLKNN algorithm to perform multi-label learning on the credit data, and change the k value from 2 to 25 to observe the performance of the model performance on different k values.\n\nFrom Figure 1, firstly, we can clearly observe that the HammingLoss value of the MLKNN algorithm is between 0.125 and 0.15, and most of the values fall near 0.13, which proves that our model has achieved good results. Secondly, by comparing the Hanmming Loss values under different k values, we find that when k=14, the value of Hamming Loss reaches the minimum, which proves that the performance of the model is the best at this time. Therefore, if we only consider from the perspective of the prediction sample, we should choose k=14 as the best number of neighbors of the prediction sample.\n\nIt can be seen from Figure 2. that the value range of Average_Precision in the multi-label classification of credit information data using the MLKNN algorithm is between 0.89 and 0.92, which proves the outstanding performance of the algorithm in the application of credit information data. Among them, when k=14, the index value is the largest, which proves that the performance of the model at this time has reached the best.\n\nSince Ranking Loss and One error are both metrics considered based on the order of labels, and both are in the same order of magnitude, this paper considers these two metrics at the same time. The performance shown by the results is shown in Figure 3. It can be seen from the figure that the Ranking Loss and One error under the MLKNN algorithm are both lower than 0.1, which also shows the effectiveness of our application of this algorithm for multi-label classification of credit data. At the same time, we can also find that when k=8, 13, and 14, the sum of the two values has reached a lower level.\n\nFrom Figure 4, we can get the change of coverage under different K values. We can intuitively find that the values are between 2.75 and 3, indicating that when the K value changes from 2 to 25, MLKNN algorithm can find the real label belonging to the sample in the first four items of predicted label ranking, which belongs to the case of low level and relatively stable, and fully proves the effectiveness of the model. It can be seen from the figure that when k = 8, 10 and 11, the value of coverage is relatively minimum. Due to the small fluctuation of this value, we also take a positive attitude towards other smaller values such as 6, 7, 14, 22 for k.\n\nThrough the analysis and summary of the above five metrics, we comprehensively consider the different performance of different k values in the model performance and find that the MLKNN algorithm has the best classification performance for credit data when k=14. Therefore, we can set k=14 to predict the test sample data.\n\nAfter determining the value of k, we further performed a statistical analysis on the predicted labels. Typical label sets are shown in Figure 5.\n\nIt can be found from Figure 5 that the types of predicted label sets that account for 51% of the total predicted number sets are 136 and 137. According to the previous label information, it represents some individuals with stable development, low frequency of credit activities, and low attention to their own credit status. This is consistent with the actual credit status of most people in daily life. Secondly, the third is 236, accounting for about 12% of the total test sample. The users it represents are users with unstable personal development, low frequency of credit activities, and low attention to credit status; this gives us The enlightenment is that there is no direct or inevitable connection between the stability of personal development and credit activities, and there is a certain connection between the frequency of credit activities and the degree of attention to credit status. In addition, the fourth place is 158, accounting for about 9%. It represents users with stable personal development, high frequency of credit activities, and high attention to credit status. This confirms our previous conjecture that the higher the credit frequency, The higher the degree of attention to credit status, and vice versa. Therefore, in the process of commercial banks' credit evaluation of users, they should focus on users who are highly concerned about their credit status, and often these users have frequent credit activities."
    }
}
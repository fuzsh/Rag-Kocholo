{
    "id": "dbpedia_121_0",
    "rank": 31,
    "data": {
        "url": "https://link.springer.com/article/10.1365/s13291-024-00283-5",
        "read_more_link": "",
        "language": "en",
        "title": "Probabilistic Models and Statistics for Electronic Financial Markets in the Digital Age",
        "top_image": "https://static-content.springer.com/image/art%3A10.1365%2Fs13291-024-00283-5/MediaObjects/13291_2024_283_Fig1_HTML.jpg",
        "meta_img": "https://static-content.springer.com/image/art%3A10.1365%2Fs13291-024-00283-5/MediaObjects/13291_2024_283_Fig1_HTML.jpg",
        "images": [
            "https://link.springer.com/oscar-static/images/darwin/header/img/logo-springerlink-39ee2a28d8.svg",
            "https://media.springernature.com/w72/springer-static/cover-hires/journal/13291?as=webp",
            "https://media.springernature.com/w215h120/springer-static/image/art%3A10.1007%2Fs40745-024-00513-8/MediaObjects/40745_2024_513_Fig1_HTML.png",
            "https://media.springernature.com/w215h120/springer-static/image/art%3A10.1007%2Fs00500-018-3237-3/MediaObjects/500_2018_3237_Fig1_HTML.gif",
            "https://media.springernature.com/w92h120/springer-static/cover-hires/book/978-3-031-03861-7?as=webp",
            "https://media.springernature.com/lw685/springer-static/image/art%3A10.1365%2Fs13291-024-00283-5/MediaObjects/13291_2024_283_Fig1_HTML.jpg",
            "https://media.springernature.com/lw685/springer-static/image/art%3A10.1365%2Fs13291-024-00283-5/MediaObjects/13291_2024_283_Figb_HTML.png",
            "https://media.springernature.com/lw33/springer-static/image/art%3A10.1365%2Fs13291-024-00283-5/MediaObjects/13291_2024_283_IEq116_HTML.gif",
            "https://media.springernature.com/lw471/springer-static/image/art%3A10.1365%2Fs13291-024-00283-5/MediaObjects/13291_2024_283_Equ15_HTML.png",
            "https://media.springernature.com/lw374/springer-static/image/art%3A10.1365%2Fs13291-024-00283-5/MediaObjects/13291_2024_283_Equd_HTML.png",
            "https://media.springernature.com/lw400/springer-static/image/art%3A10.1365%2Fs13291-024-00283-5/MediaObjects/13291_2024_283_Eque_HTML.png",
            "https://media.springernature.com/lw424/springer-static/image/art%3A10.1365%2Fs13291-024-00283-5/MediaObjects/13291_2024_283_Equf_HTML.png",
            "https://media.springernature.com/lw318/springer-static/image/art%3A10.1365%2Fs13291-024-00283-5/MediaObjects/13291_2024_283_Equg_HTML.png",
            "https://media.springernature.com/lw544/springer-static/image/art%3A10.1365%2Fs13291-024-00283-5/MediaObjects/13291_2024_283_Equi_HTML.png",
            "https://media.springernature.com/lw419/springer-static/image/art%3A10.1365%2Fs13291-024-00283-5/MediaObjects/13291_2024_283_Equj_HTML.png",
            "https://media.springernature.com/lw685/springer-static/image/art%3A10.1365%2Fs13291-024-00283-5/MediaObjects/13291_2024_283_Figc_HTML.png",
            "https://media.springernature.com/lw685/springer-static/image/art%3A10.1365%2Fs13291-024-00283-5/MediaObjects/13291_2024_283_Figd_HTML.png",
            "https://media.springernature.com/lw685/springer-static/image/art%3A10.1365%2Fs13291-024-00283-5/MediaObjects/13291_2024_283_Fige_HTML.png",
            "https://media.springernature.com/lw434/springer-static/image/art%3A10.1365%2Fs13291-024-00283-5/MediaObjects/13291_2024_283_Equo_HTML.png",
            "https://media.springernature.com/lw143/springer-static/image/art%3A10.1365%2Fs13291-024-00283-5/MediaObjects/13291_2024_283_IEq269_HTML.gif",
            "https://media.springernature.com/lw685/springer-static/image/art%3A10.1365%2Fs13291-024-00283-5/MediaObjects/13291_2024_283_Fig2_HTML.jpg",
            "https://media.springernature.com/lw685/springer-static/image/art%3A10.1365%2Fs13291-024-00283-5/MediaObjects/13291_2024_283_Fig3_HTML.png",
            "https://media.springernature.com/lw685/springer-static/image/art%3A10.1365%2Fs13291-024-00283-5/MediaObjects/13291_2024_283_Fig4_HTML.jpg",
            "https://media.springernature.com/lw685/springer-static/image/art%3A10.1365%2Fs13291-024-00283-5/MediaObjects/13291_2024_283_Fig5_HTML.jpg",
            "https://media.springernature.com/lw685/springer-static/image/art%3A10.1365%2Fs13291-024-00283-5/MediaObjects/13291_2024_283_Figj_HTML.png",
            "https://media.springernature.com/lw685/springer-static/image/art%3A10.1365%2Fs13291-024-00283-5/MediaObjects/13291_2024_283_Figk_HTML.png",
            "https://link.springer.com/oscar-static/images/logo-springernature-white-19dd4ba190.svg"
        ],
        "movies": [],
        "keywords": [],
        "meta_keywords": [
            ""
        ],
        "tags": null,
        "authors": [],
        "publish_date": "2024-07-02T00:00:00",
        "summary": "",
        "meta_description": "The scope of this manuscript is to review some recent developments in statistics for discretely observed semimartingales which are motivated by application",
        "meta_lang": "en",
        "meta_favicon": "/oscar-static/img/favicons/darwin/apple-touch-icon-92e819bf8a.png",
        "meta_site_name": "SpringerLink",
        "canonical_link": "https://link.springer.com/article/10.1365/s13291-024-00283-5",
        "text": "The evolutions of stock prices are subject to market risk. Foundations of price models in continuous time typically refer back to Louis Bachelier. He did his PhD supervised by Henri Poincaré in Paris and defended his thesis “Théorie de la spéculation” in 1900. He is considered to be the first researcher who found the Brownian motion. I would see this as a great success, although the moral that price changes cannot be forecasted (Efficient Market Hypothesis) is rather negative for speculators. Naturally, forecasting of future prices is a worthwhile pursuit. The first Bachelor student I have supervised asked me about methods to do that, but I rather pointed him at risk forecasting. Looking at data from the DAX he concluded that “One can see clearly that there is a much higher autocorrelation for squared returns than for returns. This is an indication that it might be easier to forecast variance of returns than the returns themselves”. While forecasting price changes (=returns) would be desirable to make money, forecasting risk is more successful and an integral instrument of risk management. This is a main application of statistics for financial markets and the analysis of financial time series.\n\nThe Brownian motion, or Wiener process, \\((W_{t})\\) with \\(W_{0}=0\\) is defined by the properties:\n\nIts increments \\((W_{t_{2}}-W_{t_{1}},W_{t_{3}}-W_{t_{2}},\\ldots ,W_{t_{n}}-W_{t_{n-1}})\\) are independent for all \\(0< t_{1}\\le t_{2}\\le \\cdots \\le t_{n}<\\infty \\);\n\nIts increments are stationary, such that \\((W_{t+s}-W_{t})\\) is distributed as \\(W_{s}\\);\n\nThe expectation is \\(\\mathbb{E}[W_{t}]=0\\), for all \\(t\\);\n\nThe paths \\(t\\mapsto W_{t}\\) (=realizations) are continuous.\n\nAll random objects throughout the manuscript are defined on some probability space \\((\\Omega ,\\mathcal{F},\\mathbb{P})\\), with \\(\\sigma \\)-field ℱ and measure ℙ. We follow the standard notation not to write arguments \\(\\omega \\in \\Omega \\) for random objects. Brownian motion can be motivated as integrated continuous-time white noise, and as well as limit of a discrete-time random walk \\(X_{T}=\\sum _{t=1}^{T}\\epsilon _{t}\\), \\(T\\in \\mathbb{N}\\), where \\((\\epsilon _{t})\\) are independent, identically distributed (i.i.d.) with \\(\\mathbb{P}(\\epsilon _{t}=1)=1/2=\\mathbb{P}(\\epsilon _{t}=-1)\\). It holds that \\(T^{-1/2}X_{\\lfloor T s\\rfloor}\\rightarrow W_{s}\\), as \\(T\\to \\infty \\), with the floor function \\({\\lfloor \\,\\cdot \\,\\rfloor}\\). Note that continuous-time white noise does not exist in the sense of a measurable stochastic process related to the fact, that the paths of \\((W_{t})\\) are continuous, but nowhere differentiable. So each realization of a Brownian motion has this fascinating property like the Weierstrass function. The existence was proved by Wiener in 1923 and we refer to the textbook [50] for an overview on different constructions and various properties. The Brownian motion is really at the heart of the theory on continuous-time stochastic processes. From my point of view, stochastic processes is mainly the study of classes of processes which share one or some of the fundamental properties of the Brownian motion:\n\nIt is a Gaussian process. That means that all finite-dimensional distributions of \\(W_{t_{1}},\\ldots ,W_{t_{n}}\\) are normal for all \\(n\\in \\mathbb{N}\\), and arbitrary times. They are uniquely determined by the expectation and the covariance function. A Brownian motion is hence uniquely characterized as a continuous Gaussian process with \\(W_{0} = 0\\), \\(\\mathbb{E}[W_{t}]=0\\), for all \\(t\\), and the covariance function \\(\\mathbb{C}\\mbox{ov}(W_{s},W_{t})=\\min (s,t)\\).\n\nIt is a Lévy process, that is a process \\((X_{t})\\) with independent stationary increments, \\(X_{0}=0\\), and which satisfies \\(\\forall \\epsilon >0\\): \\(\\mathbb{P}(|X_{t+h}-X_{t}|>\\epsilon )\\to 0\\), as \\(h\\to 0\\). Writing \\(X_{t}=\\sum _{j=1}^{n}(X_{t_{j}}-X_{t_{j-1}})\\), \\(0=t_{0}< t_{1}\\le t_{2} \\le \\cdots \\le t_{n}=t\\), their study is related to studying sums of i.i.d. random variables. The second fundamental example of a Lévy process is a Poisson (jump) process.\n\nIt is a martingale whose conditional expectation satisfies \\(\\mathbb{E}[W_{t}|W_{s}]=W_{s}\\), almost surely for all \\(t\\ge s\\).\n\nIt is a Markov process, for which \\((W_{t+s}-W_{t})_{s\\ge 0}\\) is another Brownian motion independent of \\(\\{W_{u},0\\le u\\le t\\}\\).\n\nIt is self-similar such that \\(a^{-1/2}W_{at}\\) is distributed as \\(W_{t}\\), for all \\(a>0\\). Looking at a path in a plot with axes that have no labels, we could hence not say anything about the scaling.\n\nThe famous Black-Scholes price model follows Bachelier’s principles and describes the stock price \\(S_{t}\\) at time \\(t\\) by the stochastic differential equation\n\n$$\\begin{aligned} \\text{d}S_{t}=a S_{t}\\,\\text{d}t+\\sigma S_{t} \\text{d}W_{t} \\end{aligned}$$\n\n(1)\n\nwhich is solved by a geometric Brownian motion. This can be proved by a simple application of Itô’s lemma. Bachelier’s mantra, that there is no (short-term) expected profit for traders without insider information, is culminated in the “Fundamental Theorem of Asset Pricing”: In an arbitrage-free market, prices follow martingales. An extension to general no arbitrage conditions by [23] implies that log-prices should be semimartingales. These are processes that can be expressed on compact time intervals as sums of a martingale and a process of finite variation. Of course, the Brownian motion is a semimartingale. Interestingly, these processes do not only occur in this modern fundamental theorem of asset pricing, but are also the class of “good integrators” for stochastic integrals according to the Bichteler-Dellacherie theorem. We work with a continuous-time log-price modelled as an Itô semimartingale:\n\n$$\\begin{aligned} X_{t}=C_{t}+\\;\\text{JUMPS},~\\text{with}~ C_{t}=X_{0}+\\int _{0}^{t} \\mu _{s} \\,\\text{d}s+\\int _{0}^{t}\\sigma _{s}\\,\\text{d}W_{s},t\\in [0,1]\\,. \\end{aligned}$$\n\n(2)\n\nThroughout this work we focus on real-valued, one-dimensional processes modelling the price evolution of only one asset. Since the multivariate setting has some surprises in store, a visit is as well interesting. Most of my own work is in fact devoted to multivariate phenomena and we do not live in a one-dimensional world. However, for my selection of topics in this manuscript and simplicity it is sufficient to stick to a one-dimensional image space at a fixed time. Some recent aspects of a multi-dimensional analysis and its applications to portfolios are briefly mentioned in the outlook. In (2), \\((\\sigma _{s})\\) is the volatility process. Volatility is the prevalent concept to describe market risk. It will therefore be our main target of statistical inference. The first important advancement compared to the Black-Scholes model (1) is to include time-varying volatility which may be a stochastic process itself. A second important advancement is to consider price jumps which can describe rapid price adjustments in response to new information provided, e.g., by economic shocks or central bank announcements.\n\nThe realm of big data in financial markets can be viewed as a stroke of luck for statisticians and data analysts giving us huge data sets at hand, in particular when looking at intra-daily tick data. In electronic financial markets almost 70% of the volume is nowadays attributed to high-frequency trading. This should be very useful for risk quantification, but similar as in other fields, the picture how to efficiently exploit big data is not yet complete as the data sets are complex and noisy. Nevertheless, or maybe for this very reason, for applications to intra-day financial data in econometrics and also in macroeconomic studies, high-frequency statistics for semimartingales became highly relevant. The use of high-frequency financial data was promoted by [24] and [4], among others, around the turn of the millennium. Engle [24] called the situation when prices from all transactions are recorded “ultra-high frequency data”. Such data samples are rather small compared to the ones we now have available from limit order books, see Fig. 5 for a snapshot. The advent of ultra-high frequency data motivated to bridge several strands of research between statistics for stochastic processes, time series analysis and econometrics. Since then high-frequency statistics for semimartingales has evolved into a huge field of study. Many brilliant researchers made substantial contributions to this field including Yacine Aït-Sahalia, Ole Barndorff-Nielsen, Jean Jacod, Per Mykland and younger ones as Mark Podolskij and Viktor Todorov, to name just a few.\n\nHaving self-similar processes in price models, it might be a bit disappointing to learn that nevertheless different models are used when considering data over different time scales. While for instance, under a very high time resolution over a short interval discreteness of prices in the image space becomes relevant, what is not in line with a Brownian motion, looking at low time resolutions, e.g., daily data over a year, discrete time series might be adequate models. Depending on the time resolution and the concrete application, micro-, meso- or macroscopic models are used as devices to coherently describe price dynamics and to allow at the same time a good calibration of the model. Often the applied focus is on risk forecasting, portfolio allocation or asset pricing. Since the seminal works on AutoRegressive Conditional Heteroskedastic (ARCH) time series and its generalizations (GARCH), see [25],Footnote 1 in the late 1980s, time series models are the main workhorse to perform volatility forecasting, typically on a daily basis. GARCH models take into account several stylized facts as volatility clustering. In the era of intra-day high-frequency price recordings, one common approach is to infer volatility based on a continuous-time model and to plug the estimates into time series models used over daily time scales, see, e.g., [31]. Although it might appear odd to a mathematician, that the continuous-time model is then not used over all time scales, many econometricians are satisfied with the good empirical performance. In recent years the research on forecasting shifted more towards fractional time series and continuous-time fractional models. The rough volatility literature and the data example in Sect. 4 inherit a similar philosophy to combine estimates from different models for different time resolutions.\n\nStylized facts of ultra high-frequency data contradict a pure semimartingale model due to so-called market microstructure. An early paper reporting these empirical facts and which foreshadowed the very successful semimartingale with additive noise model to describe tick data was [57]. Various related estimation methods for the volatility in this model have been proposed including two-scale and multi-scale realized volatility by [56] and [55], pre-averaging by [35], the kernel estimator by [6], the Quasi-Maximum-Likelihood approach by [54] and many more. A lower bound for the asymptotic variance of integrated volatility estimators at optimal rate for this problem was established by [45]. In a related vein, the presented model for limit order book quotes in Sect. 5 preserves the idea of an underlying semimartingale efficient price and describes market microstructure effects by additive noise. The only difference, which is however crucial, is that we proceed from regular noise with expectation zero to irregular, non-negative one-sided noise.\n\nDeciding whether there are jumps in a price process or if it is continuous is beyond volatility estimation one of the most important problems in the literature on high-frequency data. Like “Cat or dog?” nowadays seems to be one of the big questions based on inputs from images in research on AI and machine learning, “Jump or no jump?” poses one of the main testing problems for statistics of financial markets. One reason is that it is crucial to select and work with an adequate price model. Moreover, volatility of continuous price movements and jumps are used to describe different kinds of market risks and it is important to distinguish between the two in economic studies. Based on continuous-time paths it was simply possible to see jumps, while based on discrete recordings, with a fix time \\(\\Delta \\) between subsequent observations, it is impossible to decide about the question. In a high-frequency asymptotic regime with distance \\(\\Delta _{n}\\to 0\\) between discrete recordings, the question yields an interesting problem. In Sect. 3 we address this problem based on methods from extreme value theory and the beautiful theory of order statistics placing Rényi’s representation at the forefront. Extreme value theory concerns outliers in stationary sequences of random variables or time series, in particular the asymptotic distribution of maxima of i.i.d. random variables. An outlier refers to a realized value which is far away from the mean level of a time series, e.g., a year with a once-in-a-century flood in a sequence of yearly rainfall data. The mean level before and after an outlier remains the same. The picture of a jump is different. For instance, if a stock price evolves around level \\(a\\) before a jump of size \\(b\\) in response to the communication of quarterly results of a company, the price will continue to move at the new level \\(a+b\\) after the event. Jumps and outliers are nevertheless closely related. In particular, as Fig. 1 reveals, jumps in a process can be detected as outliers in the sequence of corresponding increments. Applying methods from extreme value theory to increments is hence one promising starting point to construct jump tests. Figure 1 shows a simulated price based on a popular Heston model with constant drift and its specific stochastic volatility \\((\\sigma _{t})\\), to that we add compound Poisson jumps with jump sizes drawn from a Laplace distribution. Even though the five realized jumps are highlighted in the path of the process left-hand side in Fig. 1, it might be difficult to spot all of them, while it is easier to find the increments with jumps as outliers in the increments right-hand side.\n\nIn view of numerous references on the broader topic,Footnote 2 we only strive to sketch a picture of three selected recent developments. Concerning deeper, open questions, parts of the more recent literature in the field has become rather technical and sophisticated. It is the goal here to shed light on some key ideas and insights, and to point out their relation to concepts from probability and mathematical statistics. Our journey shall not be a random walk. We begin in Sect. 2 as a prologue with fundamental concepts and cornerstones of statistics for semimartingales based on high-frequency data. We highlight the impact of Jean Jacod. Section 3 reviews statistical tests for price jumps in high-frequency data based on extreme value theory. We complement the existing methods proposing alternative, original methods. Gumbel is the second researcher whose contribution is emphasized. The newly developed Rényi test in Theorem 1 is built upon a maximal difference between order statistics motivated by the Rényi representation. This is the first classical, elementary yet ingenious, concept from probability which we highlight in a box. A comment on the genius behind this is given in another box. In Sect. 4 we provide a brief review on rough fractional stochastic volatility with a data example. The review mentions recent results on the identifiability of the Hurst exponent under high-frequency asymptotics. Our Theorem 2 adds a new result which points out that the regularity of stochastic volatility in a more general sense is identifiable only in some cases and can be estimated only with a slower rate of convergence. This reveals that – even based on high-frequency data – there are frontiers in recovering path properties of a latent volatility from price recordings. The third part of the journey in Sect. 5 is admittedly captured to summarize and reflect some own research. The boxes place the spotlight on the taxi problem and the reflection principle for Brownian motion. The taxi problem is a popular example for estimating a boundary parameter. It is even contained in some school-books which develop the main estimation ideas in an intuitive manner. A nice feature of this example is that it is nevertheless deeper understood with concepts as complete and sufficient statistics. While the taxi problem is used to motivate the estimation based on local minima of best ask quotes, the reflection principle paves the way to determine distributions of functionals of Brownian motion which is an important ingredient of the asymptotic analysis of our boundary model.\n\nFor some first simple but useful insights consider the parametric model with log-price\n\n$$ X_{t}=\\mu \\cdot t+\\sigma \\cdot W_{t}\\,, $$\n\nwith a standard Brownian motion \\((W_{t})\\) and \\(\\mu \\in \\mathbb{R}\\), \\(\\sigma >0\\) unknown parameters. An obvious problem for statistics is to estimate the two parameters. Assume that we have discrete observations \\(X_{0},X_{t_{1}},\\ldots ,X_{t_{n}}\\) on an equidistant grid \\(t_{j}=j\\Delta _{n}\\), \\(0\\le j\\le n\\), available. In statistics for stochastic processes there are different asymptotic regimes, either \\(T=n\\Delta _{n}\\) is fix and \\(\\Delta _{n}\\to 0\\) (high-frequency), or \\(\\Delta _{n}=\\Delta \\) is fix and \\(T=n\\Delta \\to \\infty \\) (low-frequency), or even both is true and we have high-frequency data over an asymptotically large time interval. We have in our setting many observations of only one single path of the process. The inference and testing problems, e.g., whether there are jumps or not, are in this field formulated and addressed path-wise, i.e., we want to know if the realized path has jumps or not.\n\nStatistical inference is based on the increments\n\n$$\\begin{aligned} \\Delta _{j}^{n} X=X_{t_{j}}-X_{t_{j-1}},1\\le j\\le n\\,. \\end{aligned}$$\n\n(3)\n\nIn the parametric, equidistant setting it holds that \\(\\Delta _{j}^{n} X=\\mu \\cdot \\Delta _{n}+\\sqrt{\\Delta _{n}}\\cdot \\sigma \\cdot Z_{i}\\), with \\(Z_{i}\\) i.i.d. standard normal, denoted \\(\\mathcal{N}(0,1)\\), random variables, such that the increments have expectation \\(\\mu \\Delta _{n}\\) and variance \\(\\sigma ^{2} \\Delta _{n}\\). In this standard model, the maximum likelihood estimator\n\n$$\\begin{aligned} \\hat{\\mu}_{ML}&=\\frac{\\sum _{j=1}^{n}\\Delta _{j}^{n} X}{n\\Delta _{n}}= \\frac{X_{T}-X_{0}}{T}\\,, \\end{aligned}$$\n\n(4a)\n\n$$\\begin{aligned} \\hat{\\sigma}^{2}_{ML}&= \\frac{\\sum _{j=1}^{n}(\\Delta _{j}^{n} X)^{2}}{n\\Delta _{n}}-(\\hat{\\mu}_{ML})^{2} \\Delta _{n}= \\frac{\\sum _{j=1}^{n}(\\Delta _{j}^{n} X)^{2}}{n\\Delta _{n}}- \\frac{\\big(\\sum _{j=1}^{n}\\Delta _{j}^{n} X\\big)^{2}}{nT}\\,, \\end{aligned}$$\n\n(4b)\n\nhas the smallest possible variance. The quadratic risk of the estimated drift parameter\n\n$$\\begin{aligned} \\mathbb{E}\\big[\\big(\\hat{\\mu}_{ML}-\\mu \\big)^{2}\\big]=\\mathbb{E}\\Big[ \\Big(\\frac{\\sigma \\cdot W_{T}}{T}\\Big)^{2}\\Big]=\\frac{\\sigma ^{2}}{T} \\end{aligned}$$\n\n(5)\n\ndoes not depend on \\(\\Delta _{n}\\). It tends to zero only if \\(T\\to \\infty \\), and not under high-frequency asymptotics. Modelling intra-daily financial data, we have usually discrete observations available at very high frequencies, e.g. once per second. Naturally, we work in a high-frequency asymptotic regime. We learn from (5) that we cannot consistently estimate the drift in this situation. Looking at \\(\\hat{\\sigma}^{2}_{ML}\\), we see that the term with \\(\\hat{\\mu}_{ML}\\) tends to zero. The standard estimator for \\(\\sigma ^{2}\\) is therefore the realized volatility\n\n$$\\begin{aligned} \\hat{\\sigma}^{2}=T^{-1}\\sum _{j=1}^{n}(\\Delta _{j}^{n} X)^{2}\\,. \\end{aligned}$$\n\n(6)\n\nIt is an elementary exercise using moments of a normal distribution to compute its squared risk\n\n$$\\begin{aligned} \\mathbb{E}\\big[\\big(\\hat{\\sigma}^{2}-\\sigma ^{2}\\big)^{2}\\big]= \\frac{2\\sigma ^{4}\\Delta _{n}}{T}+ \\frac{4\\sigma ^{2}\\mu ^{2}\\Delta _{n}^{2}}{T}+\\mu ^{4}\\Delta _{n}^{2} \\,, \\end{aligned}$$\n\n(7)\n\nwhich tends to zero under high-frequency asymptotics. We should not use it in a low-frequency regime in that its variance tends to zero, but the bias does not. Throughout the remainder of this manuscript we set \\(T=1\\), without loss of generality. It is not surprising that a central limit theorem (clt)\n\n$$\\begin{aligned} \\sqrt{n}(\\hat{\\sigma}^{2}-\\sigma ^{2})\\stackrel{d}{\\longrightarrow } \\mathcal{N}(0,2\\sigma ^{4})\\,, \\end{aligned}$$\n\n(8)\n\nholds true. We write \\(\\stackrel{d}{\\longrightarrow }\\) for convergence in distribution (weak convergence). Note, however, that we have a triangular array of random variables here and not a sequence, that is, going from \\(n\\) to \\(n+1\\) is not just adding one observation, but all observation times depend on \\(n\\). For this reason, clts for triangular arrays need to be used in the high-frequency framework. The main benefit of a clt is to facilitate asymptotic confidence statements. These are feasible when we standardize the left-hand side in (8) with a consistent estimator of the asymptotic standard deviation. A more elegant method – which might give a slightly better approximation for finite samples – is a variance stabilization applying the \\(\\Delta \\)-method with the strictly increasing logarithm:\n\n$$\\begin{aligned} \\sqrt{n}(\\log \\big(\\hat{\\sigma}^{2}\\big)-\\log (\\sigma ^{2})) \\stackrel{d}{\\longrightarrow }\\mathcal{N}(0,2)\\,. \\end{aligned}$$\n\n(9)\n\nSince log is strictly increasing, confidence intervals for \\(\\log (\\sigma ^{2})\\) readily translate into confidence intervals for \\(\\sigma ^{2}\\). A current discussion comparing four approaches to perform asymptotic confidence based on a clt, which all work in this example, is given in [44]. Beyond parameter estimation, the analogy to the standard statistical model allows to transfer more methods, e.g., likelihood ratio tests.\n\nIn the more general model with high-frequency observations of a continuous semimartingale \\((C_{t})\\) from (2) with time-varying drift \\((\\mu _{s})\\) and volatility \\((\\sigma _{s})\\), the first goal is estimation of the integrated volatility \\(\\int _{0}^{1}\\sigma _{s}^{2}\\,\\text{d}s\\), e.g., integrated over trading days as a daily measure of risk. Since Itô’s isometry yields that\n\n$$\\begin{aligned} \\mathbb{E}\\big[(\\Delta _{j}^{n} C)^{2}\\big]=\\int _{(j-1)\\Delta _{n}}^{j \\Delta _{n}}\\sigma _{s}^{2}\\,\\text{d}s+\\mathcal{O}(\\Delta _{n}^{2})\\,, \\end{aligned}$$\n\nthe realized volatility (6) is still a suitable (and in fact optimal) estimator.\n\nA very strong asymptotic result under mild regularity conditions is the functional stable central limit theorem for realized volatility by [33]:\n\n$$\\begin{aligned} \\sqrt{n}\\,\\Big(\\sum _{j=1}^{\\lfloor nt\\rfloor }\\big(\\Delta _{j}^{n} C \\big)^{2}-\\int _{0}^{t}\\sigma _{s}^{2}\\,\\text{d}s\\Big) \\stackrel{st}{\\longrightarrow} \\int _{0}^{t}\\sqrt{2}\\sigma _{s}^{2}\\, \\text{d}B_{s}~,t\\in [0,1]\\,, \\end{aligned}$$\n\n(10)\n\nwith \\((B_{s})\\) a Brownian motion independent of \\((W_{s})\\) defined on an extension of the original probability space. This implies the marginal clt\n\n$$\\begin{aligned} \\sqrt{n}\\,\\Big(\\sum _{j=1}^{n}\\big(\\Delta _{j}^{n} C\\big)^{2}-\\int _{0}^{1} \\sigma _{s}^{2}\\,\\text{d}s\\Big)\\stackrel{st}{\\longrightarrow} \\mathcal{N}\\Big(0,\\int _{0}^{1} 2\\,\\sigma _{s}^{4}\\, \\text{d}s\\Big)\\,. \\end{aligned}$$\n\n(11)\n\nFor stochastic volatility, the variance of the limit distribution is random, the limit is then called mixed normal. For this reason it is important that the convergence is stable. This is a stronger mode of weak convergence equivalent to joint weak convergence with every measurable bounded random variable on the same space. Since it allows for a \\(\\Delta \\)-method and weak convergence after standardization, known as Slutsky’s lemma for weak convergence, it is a crucial ingredient to construct asymptotic confidence intervals. Beyond inference on the integrated volatility, the functional clt allows for various other statistical applications, for instance, a volatility change-point test of cusum-type as explained in Sect. 2 of [11].\n\nIt is not only relevant to infer the integrated volatility, the nonparametric estimation of the spot volatility process \\((\\sigma _{s}^{2})\\) is another central problem in high-frequency statistics. At this point, it is beneficial to introduce some rigorous assumption on the characteristics of the continuous semimartingale log-price process \\((C_{t})\\).\n\nAssumption 1\n\nThe drift \\((\\mu _{t})_{t\\ge 0}\\) is locally bounded and the volatility is strictly positive, \\(\\inf _{t\\in [0,1]}\\sigma _{t}>0\\), almost surely. For all \\(0\\leq t+s\\leq 1\\), \\(t\\ge 0\\), \\(s\\ge 0\\), with some constants \\(C_{\\sigma}>0\\), and \\(\\alpha >0\\), it holds that\n\n$$\\begin{aligned} \\mathbb{E}\\big[(\\sigma _{(t+s)}-\\sigma _{t})^{2}\\big] \\le C_{\\sigma} s^{2 \\alpha}\\,. \\end{aligned}$$\n\n(12)\n\nCondition (12) imposes a certain regularity \\(\\alpha \\) of the volatility process. Due to the expectation, it is not Hölder continuity and (12) does not rule out volatility jumps. The increments of some compound Poisson jump process for instance, over a time interval of length \\(s\\), equal a constant times \\((s^{2}+s)\\), if the jump size distribution has a second moment. Therefore, it satisfies (12) with \\(\\alpha =1/2\\). This is true for much more general jump processes. A continuous semimartingale and in particular Brownian motion satisfy (12) with \\(\\alpha =1/2\\). The Hölder condition (12) in quadratic mean is a convenient concept to describe the variability of a stochastic process. It is also used in other fields of probability, for instance, for functional data in [30]. In particular, the rate of convergence, with that \\((\\sigma _{s}^{2})\\) at some time \\(s\\in (0,1)\\) can be estimated, hinges on the regularity parameter \\(\\alpha \\). Using a local average of rescaled squared increments\n\n$$\\begin{aligned} \\hat{\\sigma}_{s}^{2}=\\frac{n}{k_{n}} \\sum _{j=\\lfloor s n\\rfloor +1}^{ \\lfloor s n\\rfloor +k_{n}} \\big(\\Delta _{j}^{n} C\\big)^{2}\\,, \\end{aligned}$$\n\n(13)\n\nas estimator yields with \\(k_{n}=n^{2\\alpha /(2\\alpha +1)}\\), for which the order of the squared bias \\((k_{n}\\Delta _{n})^{2\\alpha}\\) is the same as that of the variance \\(k_{n}^{-1}\\), the minimal root mean squared error of order \\(n^{-\\alpha /(2\\alpha +1)}\\). Given that the regularity parameter determines optimal spot volatility estimation, inference on an unknown \\(\\alpha \\) is certainly of interest. This, however, is an intricate problem not yet solved in general which we visit in Sect. 4. In the standard case \\(\\alpha =1/2\\), spot volatility can be estimated with rate \\(n^{-1/4}\\). In the best (non-constant) case \\(\\alpha =1\\), the rate is \\(n^{-1/3}\\).\n\nIf there are jumps, the realized volatility converges in probability, denoted \\(\\stackrel{\\mathbb{P}}{\\longrightarrow}\\), to the entire quadratic variation:\n\n$$ \\sum _{j=1}^{n}\\big(\\Delta _{j}^{n} X\\big)^{2} \\stackrel{\\mathbb{P}}{\\longrightarrow}\\int _{0}^{1}\\sigma _{s}^{2} \\text{d}s+\\sum _{s\\le 1}(\\Delta X_{s})^{2}\\,. $$\n\nWe write \\(\\Delta X_{s}=X_{s}-X_{s-}=X_{s}-\\lim _{t\\uparrow s} X_{t}\\). It is common notation to write jumps with sums over uncountable index sets, since the processes will always have only countably many random jump times. Most relevant is first to estimate the integrated volatility in presence of the nuisance jumps. To get rid of jumps, we need to discard in particular large jumps as in Fig. 1 which are contained in the large absolute increments. A natural approach is hence to truncate increments whose absolute values are above a certain threshold. For this purpose, define a sequence \\((u_{n})\\), with \\(u_{n}\\propto \\Delta _{n}^{\\tau}\\), \\(\\tau \\in (0,1/2)\\). Denote an indicator function which is 1 if \\(A\\) is true and 0, else. The idea is now to work out under which restrictions on the jumps the truncated realized volatility satisfies\n\n(14)\n\nthe same clt as for the realized volatility without jumps in (11). Sufficient is\n\nThe truncation method was pioneered by [38], is summarized in Chap. 13 of [34], and the community is still working on refinements, see, for instance, [26] and [3]. We can decompose the difference\n\nwith some \\(\\kappa \\in (0,1)\\), e.g., \\(\\kappa =1/2\\). For the term \\(\\text{I}_{n}\\), knowing that \\(\\max _{j}|\\Delta _{j}^{n} C|\\) is of stochastic order \\(\\log (n)\\sqrt{\\Delta _{n}}\\), what is contained in the next section, and that it is even almost surely smaller than \\(u_{n}\\) by the law of the iterated logarithm is sufficient. The argument by Jacod is more elementary and in the following way:\n\nfor any \\(N\\in \\mathbb{N}\\), and using moments of \\((\\Delta _{j}^{n} C)\\) and choosing \\(N\\) sufficiently large yields that \\(\\sqrt{n}\\,\\text{I}_{n} \\stackrel{\\mathbb{P}}{\\longrightarrow} 0\\), since \\(\\sqrt{n}\\,\\mathbb{E}\\big[|\\text{I}_{n}|\\big]\\to 0\\).\n\nThe two other terms require a closer look at the jump component \\((J_{t})\\) of the semimartingale \\((X_{t})\\), \\(X_{t}=C_{t}+J_{t}\\). Most literature in the area imposes the general structure of Itô semimartingales in the sense of Sect. 2.1.4 of [34] which admit a “Grigelionis representation”. The jumps are independent of \\((C_{t})\\) and separated in a martingale of compensated small jumps and larger jumps\n\nwith a Poisson random measure \\(\\mu \\) compensated by \\(\\nu (\\text{d}s,\\text{d}z)=\\lambda (\\text{d}z)\\otimes \\text{d}s\\), with a \\(\\sigma \\)-finite measure \\(\\lambda \\). The function \\(\\delta \\), for which the third argument \\(\\omega \\) is consequently also not written, is a predictable function, for which we assume that a non-negative deterministic function \\(\\gamma \\) exists, such that\n\n$$\\begin{aligned} \\sup _{\\omega ,x}|\\delta (t,x)|/\\gamma (x) \\end{aligned}$$\n\nis locally bounded. The benefit of such a meticulous definition of \\((J_{t})\\) is to preserve generality. For instance, the large class of Lévy jump processes is contained as a special case with \\(\\delta (s,x)=x\\). The main assumption on the jumps is captured in a jump activity index \\(r\\in [0,2]\\), for which\n\n$$\\begin{aligned} \\int _{\\mathbb{R}}(\\gamma ^{r}(z)\\wedge 1)\\lambda (\\text{d}z)< \\infty \\,. \\end{aligned}$$\n\n(15)\n\nBasically, this means summability of \\(\\sum _{s\\le 1}|\\Delta J_{s}|^{r}\\). For \\(r=0\\) this is a strong restriction with at most finitely many jumps on \\([0,1]\\) (finite-activity), and for \\(r=1\\) we assume the jump process to be of finite variation. The larger \\(r\\), the less restrictive is the condition.\n\nNow we have the toolbox to handle terms \\(\\text{II}_{n}\\) and \\(\\text{III}_{n}\\). In the sequel, let \\(K\\) be a generic constant. With Markov’s inequality and Cauchy-Schwarz, we obtain for \\(\\text{II}_{n}\\) that\n\nThe term \\(\\text{II}_{n}\\) can be thought of as an error by truncating also the continuous components whenever the threshold is exceeded. Therefore, the order gets smaller for smaller \\(\\tau \\), moving \\(u_{n}\\) farer away from \\(\\sqrt{\\Delta _{n}}\\), when only very large absolute increments are truncated. If \\(r\\tau <1/2\\), we conclude that \\(\\sqrt{n}\\text{II}_{n} \\stackrel{\\mathbb{P}}{\\longrightarrow} 0\\).\n\nMost difficult is term \\(\\text{III}_{n}\\) due to small jumps in non-truncated increments. We exploit the martingale structure of the small jumps to apply Burkholder’s inequality and to deduce\n\nThis term gets smaller for larger \\(\\tau \\), moving \\(u_{n}\\) closer to \\(\\sqrt{\\Delta _{n}}\\). Both terms decrease for smaller \\(r\\). To ensure that \\(\\sqrt{n}\\text{III}_{n} \\stackrel{\\mathbb{P}}{\\longrightarrow} 0\\), we need that \\(r<1\\), and \\(\\tau (2-r)>1/2\\). This is the main result about truncated realized volatility: If \\(r<1\\), with \\(\\tau \\in \\big((2(2-r))^{-1},1/2\\big)\\), what can be ensured by selecting \\(\\tau \\) close to \\(1/2\\), it satisfies (14). While we emphasize some key steps of the proof, the bounds for \\(\\text{II}_{n}\\) and \\(\\text{III}_{n}\\) admittedly lack some details. Most of them are elementary, as carefully using the triangle inequality, but a few are deeper. A less pedagogic but rigorous proof can be found in Chap. 13 of [34]. In particular, in the bound for \\(\\text{II}_{n}\\) we work under the event with at most one larger jump contained in one increment, for which the Poisson nature of the jumps yields precise estimates, see Step 5 in the proof of Thm. 13.1.1 in [34]. The restrictions on \\(r\\) for spot volatility estimation with truncation are less strict, \\(r<4/3\\), see Sect. 13.4.1 of [34], mainly since the rate is slower with that such a difference needs to tend to 0.\n\nThere are several different constructions of tests for jumps in high-frequency data. Let me focus here only on the most prominent one in financial economics by [37]. It is based on the maximal (absolute) normalized increment and exploits its asymptotic Gumbel distribution under the null hypothesis of no jumps. The test is sometimes called the Lee-Mykland test, or the Gumbel test in the literature. The asymptotic Gumbel distribution is traced back to the one of the maximum of i.i.d. \\(\\mathcal{N}(0,1)\\) random variables and thus classical extreme value theory.\n\nIn the sequel, we consider real-valued random variables on some probability space with measure ℙ. For random variables \\((X_{j})_{1\\le j\\le n}\\), we denote the order statistics \\(X_{(j)},1\\le j\\le n\\), with \\(X_{(1)}\\le X_{(2)}\\le \\ldots ,X_{(n)}\\). In particular, \\(X_{(1)}=\\min _{1\\le j\\le n} X_{j}\\) refers to the minimum and \\(X_{(n)}=\\max _{1\\le j\\le n} X_{j}\\) to the maximum. These are unique with probability 1 for random variables whose distributions are absolutely continuous with respect to the Lebesgue measure. It is a standard example in extreme value theory, see, e.g., Example 1.1.7 in [21], that for \\((X_{j})_{1\\le j\\le n}\\) i.i.d. \\(\\mathcal{N}(0,1)\\), the maximum satisfies\n\n$$\\begin{aligned} a_{n}^{-1}\\big(X_{(n)}-b_{n}\\big)\\stackrel{d}{\\longrightarrow } \\Lambda \\,,a_{n}^{-1}=\\sqrt{2\\log (n)}~, ~\\text{and}~b_{n}=a_{n}^{-1}- \\frac{\\log (4\\pi \\log (n))}{2\\sqrt{2\\log (n)}}\\,, \\end{aligned}$$\n\n(16)\n\nwith the Gumbel limit distribution \\(\\Lambda \\), i.e., it holds for all \\(x\\in \\mathbb{R}\\) that\n\n$$\\begin{aligned} \\lim _{n\\to \\infty}\\mathbb{P}\\Big(a_{n}^{-1}\\Big(X_{(n)}-b_{n}\\Big) \\Big)\\le x\\Big)= \\exp \\big({-e^{-x}}\\big)\\,. \\end{aligned}$$\n\nLimit distributions of maxima of i.i.d. random variables can only be of Gumbel, Weibull or Frechét type. If sequences \\((a_{n})\\) and \\((b_{n})\\) exist, such that for a cumulative distribution function (cdf) \\(F\\) the maxima of i.i.d. random variables with cdf \\(F\\) converge to \\(\\Lambda \\), write \\(F\\in \\text{MDA}(\\Lambda )\\) (maximum domain of attraction).\n\nOn the null hypothesis \\(H_{0}:\\sup _{\\tau \\in (0,1)}|X_{\\tau}- X_{\\tau -}|=0\\), based on high-frequency returns \\((\\Delta _{j}^{n} X)_{1\\le j\\le n}\\), Lee and Mykland [37] proved in their Thm. 2 that\n\n$$\\begin{aligned} \\sqrt{2\\log (2n)}\\Big(n^{1/2}\\,\\max _{1\\le j\\le n}\\bigg| \\frac{\\Delta _{j}^{n} X}{\\hat{\\sigma}_{j/n}}\\bigg|-\\Big(\\sqrt{2\\log (2n)}- \\frac{\\log (4\\pi \\log (2n))}{2\\sqrt{2\\log (2n)}}\\Big)\\Big) \\stackrel{d}{\\longrightarrow }\\Lambda , \\end{aligned}$$\n\n(17)\n\nwith a suitable estimator of the volatility \\((\\hat{\\sigma}_{j/n})\\), e.g., from (13). The proof is carried out under some assumptions on \\((\\mu _{t})\\) and \\((\\sigma _{t})\\), which can be generalized to rather weak regularity conditions. The similarity to (16) is striking. Indeed, the proof traces back the convergence to (16) showing that the normalized increments can be approximated by i.i.d. \\(\\mathcal{N}(0,1)\\) observations. The factor 2 in the logarithm is due to the absolute value in the statistic and exploits the symmetry of \\(\\mathcal{N}(0,1)\\). The normalizing sequences given in [37] are in fact slightly different, but asymptotically equivalent. Rejecting the null hypothesis when the statistic left-hand side in (17) exceeds \\(-\\log (-\\log (1-\\alpha ))\\), \\(\\alpha \\in (0,1)\\), hence yields a test with asymptotic level \\(\\alpha \\), i.e., the probability of a false rejection converges to \\(\\alpha \\). Under the alternative hypothesis \\(H_{1}:\\sup _{\\tau \\in (0,1)}|X_{\\tau}- X_{\\tau -}|>0\\), the test rejects correctly with asymptotic probability 1. There is moreover a rate of convergence. We can state equivalently that the test rejects correctly with asymptotic probability 1 under local alternatives\n\n$$ H_{1}:\\liminf _{n\\to \\infty}{{n^{\\beta}}}\\sup _{\\tau \\in (0,1)}|X_{\\tau}- X_{\\tau -}|>0,~\\text{for some}~{{ \\beta < 1/2}}\\,. $$\n\nThis means that we can not detect arbitrarily small jumps based on a fix number of \\((n+1)\\) discrete high-frequency recordings, but jumps which are larger than of order \\(n^{-1/2}\\). The test has several appealing properties. Critical values based on quantiles of the Gumbel distribution can be determined to test at a chosen level \\(\\alpha \\). Moreover, the associated argmaximum consistently estimates the time of the largest jump under \\(H_{1}\\). Based on sequential testing and the largest absolute increments thus jump times and jump sizes can be inferred.\n\nThe next paragraph advances research on high-frequency jump tests contributing alternative methods based on extreme value theory which have some advantages compared to the Gumbel test. For the construction, we make an excursion to the nice, classical theory of order statistics. The exponential distribution, \\(\\text{Exp}(\\lambda )\\), with parameter \\(\\lambda >0\\), with Lebesgue density \\(\\lambda \\exp (-\\lambda t),t>0\\), and tail function \\(\\mathbb{P}(X>t)=\\exp (-\\lambda t)\\), \\(t>0\\), \\(X\\sim \\text{Exp}(\\lambda )\\), takes an outstanding role in the theory of order statistics.\n\nWe build up our methods on the joint asymptotic distribution of the extreme order statistics.\n\nProposition 3.1\n\nLet \\((X_{j})_{1\\le j\\le n}\\) be i.i.d. real-valued random variables with \\(X_{j}\\sim F\\in \\textit{MDA}(\\Lambda )\\). For fix \\(r\\), there exist sequences \\((a_{n})\\) and \\((b_{n})\\), such that as \\(n\\rightarrow \\infty \\), it holds that\n\n$$\\begin{aligned} \\left ( \\textstyle\\begin{array}{c} a_{n}^{-1}(X_{(n)}-b_{n}) \\\\ a_{n}^{-1}(X_{(n-1)}-b_{n}) \\\\ \\vdots \\\\ a_{n}^{-1}(X_{(n-r+1)}-b_{n}) \\end{array}\\displaystyle \\right )\\stackrel{d}{\\longrightarrow }\\left ( \\textstyle\\begin{array}{c} -\\log ({E_{1}}) \\\\ -\\log ( E_{1}+ E_{2}) \\\\ \\vdots \\\\ -\\log (E_{1}+\\cdots + E_{r}) \\end{array}\\displaystyle \\right )\\,, \\end{aligned}$$\n\nwhere \\((E_{j})\\) are i.i.d. \\(\\textit{Exp}(1)\\).\n\nThis is a special case of Thm. 2.1.1 from [21] for distributions in the MDA of a Gumbel distribution. If \\(X_{j}\\sim \\mathcal{N}(0,1)\\), the sequences \\((a_{n})\\) and \\((b_{n})\\) coincide with the ones from (16). In particular, \\(-\\log ({E_{1}})\\) has a Gumbel distribution. The main ingredient of the proof of Proposition 3.1 is the convergence in distribution\n\n$$ n\\,\\big(\\tilde{E}_{(1)},\\tilde{E}_{(2)},\\ldots ,\\tilde{E}_{(r)}\\big) \\stackrel{d}{\\longrightarrow}\\big( E_{1}, E_{1}+E_{2},\\ldots , \\sum _{j=1}^{r} E_{j}\\big)\\,,\\,n\\to \\infty , r\\;\\text{fix}, $$\n\nfor \\((\\tilde{E}_{j})\\) i.i.d. \\(\\text{Exp}(1)\\), which is directly implied by Rényi’s representation. With a standard analytical condition for extreme value convergence, a change of variables and an (extended) continuous mapping theorem this yields the result.\n\nBased on Proposition 3.1, we derive that\n\n$$\\begin{aligned} \\sqrt{2\\log (n)}\\big(X_{(n)}-X_{(n-r)}\\big) \\stackrel{d}{\\longrightarrow }\\log \\Big( \\frac{E_{1}+\\cdots E_{r+1}}{E_{1}}\\Big)\\,. \\end{aligned}$$\n\n(18)\n\nThis motivates an interesting alternative to the Gumbel test to construct a test based on differences of ordered normalized increments related to the distribution of \\((X_{(n)}-X_{(n-r)})\\), e.g., for \\(r=1\\). Under jumps from a distribution with a Lebesgue density such a test will attain analogous asymptotic properties. The asymptotic distribution under the null hypothesis is, however, simpler, as it does not require the sequence \\((b_{n})\\) any more. In view of an arduous discussion about the finite-sample fit of different, asymptotically equivalent variants of \\((b_{n})\\), and that incorrect normalizations of the Gumbel test led to some problems in the applied literature, cf. [42], the advantage of getting rid of \\((b_{n})\\) in determining critical values of a jump test should not be underestimated.\n\nMore reasons to explore this path are in the beauty of the joint limit distribution of differences of extreme order statistics, again related to Rényi’s representation, and an improved, simplified detection of jumps under the alternative hypothesis. The latter implies a practical improvement compared to a sequential application of the Gumbel test which I expect to be of relevance for the current analysis of high-frequency data. We establish the main result along three auxiliary lemmas on the limit distributions in Proposition 3.1 and (18). They are suitable as exercises in courses on probability and analysis.\n\nOur first auxiliary result shows that interesting transformations of exponential random variables yield again exponential distributions.\n\nLemma 1\n\nLet \\(E_{1},\\ldots ,E_{N}\\) be i.i.d. \\(\\textit{Exp}(1)\\). Then\n\n$$\\begin{aligned} \\log \\Big(1+\\frac{E_{r+1}}{\\sum _{j=1}^{r} E_{j}}\\Big)\\sim \\textit{Exp}(r) \\,, \\end{aligned}$$\n\n(19)\n\nfor any \\(r\\), \\(1\\le r\\le N-1\\). In particular,\n\n$$\\begin{aligned} \\log \\Big(1+\\frac{E_{2}}{E_{1}}\\Big)\\sim \\textit{Exp}(1)\\,. \\end{aligned}$$\n\n(20)\n\nProof\n\nFor some non-negative, independent random variables \\(X\\) and \\(Y\\), with Lebesgue densities \\(f_{X}\\) and \\(f_{Y}\\), the change of variables\n\nyields the Lebesgue density of the ratio \\(X/Y\\). With the density of the Gamma(\\(r\\),1) distribution of \\(\\sum _{j=1}^{r} E_{j}\\), i.e., the \\(r\\)th convolution of \\(\\text{Exp}(1)\\), and independence, we obtain for the density \\(g\\) of \\(E_{r+1}/\\sum _{j=1}^{r} E_{j}\\):\n\n$$\\begin{aligned} g(z)&=\\int _{0}^{\\infty}\\exp (-yz)\\frac{y^{r-1}}{\\Gamma (r)}\\exp (-y)y \\,\\text{d}y \\\\ &=\\int _{0}^{\\infty}\\exp (-y(z+1))\\frac{y^{r}}{\\Gamma (r)}\\text{d}y \\\\ &=\\frac{r}{(z+1)^{r+1}}\\,,z>0. \\end{aligned}$$\n\nThe last identity is implied by the known moments of an exponential distribution with \\(\\lambda =(z+1)\\). Since \\(z\\mapsto \\log (1+z)\\) has inverse \\(u\\mapsto \\exp (u)-1\\), with derivative \\(\\exp (u)\\), a change of variables yields that \\(U=\\log \\Big(1+\\frac{E_{r+1}}{\\sum _{j=1}^{r} E_{j}}\\Big)\\) has the Lebesgue density\n\n$$\\begin{aligned} f_{U}(u)=\\exp (u)\\cdot \\frac{r}{\\exp (u(r+1))}=r\\cdot \\exp (-ru)\\,,u>0. \\end{aligned}$$\n\nHence, \\(U\\sim \\text{Exp}(r)\\). □\n\nI find it even more interesting that, although the same random variables enter the transformation (19) for different \\(r\\), we have an independence based on the next two auxiliary lemmas.\n\nLemma 2\n\nFor \\(E_{1}\\), \\(E_{2}\\), \\(E_{3}\\) i.i.d. \\(\\textit{Exp}(1)\\), the random variables \\(E_{1}/(E_{1}+E_{2})\\), \\((E_{1}+E_{2})/(E_{1}+E_{2}+E_{3})\\), and \\((E_{1}+E_{2}+E_{3})\\) are independent.\n\nProof\n\nWe show that the joint density equals the product of the marginal densities. Elementary computations yield the Jacobian of the inverse map\n\n$$\\begin{aligned} \\left ( \\textstyle\\begin{array}{c} u \\\\ v \\\\ w \\end{array}\\displaystyle \\right )\\mapsto \\left ( \\textstyle\\begin{array}{c} u\\cdot v\\cdot w \\\\ (1-u)\\cdot v\\cdot w \\\\ (1-v)\\cdot w \\end{array}\\displaystyle \\right )\\,, \\end{aligned}$$\n\nand its determinant \\(vw^{2}\\). Based on a (multivariate) change of variables and with the product exponential density of \\((E_{1},E_{2},E_{3})\\), we obtain the joint density\n\n$$\\begin{aligned} f_{U,V,W}(u,v,w)&=\\exp (-uvw)\\exp (-(1-u)vw)\\exp (-(1-v)w)vw^{2} \\\\ &=\\exp (-w)vw^{2}\\,,0\\le u,v\\le 1,w>0\\,. \\end{aligned}$$\n\nSince this equals the product of the marginal densities \\(f_{W}(w)=w^{2}e^{-w}/2\\), \\(w>0\\), of the Gamma(3,1) distribution of \\((E_{1}+E_{2}+E_{3})\\), \\(f_{V}(v)=2v\\), \\(v\\in [0,1]\\), of \\((E_{1}+E_{2})/(E_{1}+E_{2}+E_{3})\\), and , of the uniform distribution of \\(E_{1}/(E_{1}+E_{2})\\), we conclude the independence. □\n\nTransformations of independent random variables remain independent. For the general conclusion, we only need to extend Lemma 3 what can be done by induction.\n\nLemma 3\n\nFor \\(E_{1},\\ldots ,E_{r+1}\\), \\(r\\in \\mathbb{N}\\), i.i.d. \\(\\textit{Exp}(1)\\), the random variables \\(E_{1}/(E_{1}+E_{2})\\), …, \\((\\sum _{j=1}^{r} E_{j})/(\\sum _{j=1}^{r+1}E_{j})\\), and \\((\\sum _{j=1}^{r+1}E_{j})\\) are independent.\n\nProof\n\nFrom the inverse map\n\n$$\\begin{aligned} \\left ( \\textstyle\\begin{array}{c} u_{1} \\\\ u_{2} \\\\ \\vdots \\\\ u_{r} \\\\ u_{r+1} \\end{array}\\displaystyle \\right )\\mapsto \\left ( \\textstyle\\begin{array}{c} \\prod _{j=1}^{r+1}u_{j} \\\\ (1-u_{1})\\prod _{j=2}^{r+1}u_{j} \\\\ \\vdots \\\\ (1-u_{r-1})\\cdot u_{r}\\cdot u_{r+1} \\\\ (1-u_{r})\\cdot u_{r+1} \\end{array}\\displaystyle \\right )\\,, \\end{aligned}$$\n\nwe infer by induction the Jacobian\n\n$$\\begin{aligned} J^{r+1,r+1}=\\left ( \\textstyle\\begin{array}{c@{\\quad}c@{\\quad}c@{\\quad}c} J_{11}^{r,r}\\cdot u_{r+1}&\\ldots &J_{1r}^{r,r}\\cdot u_{r+1}&\\prod _{j=1}^{r} u_{j} \\\\ J_{21}^{r,r}\\cdot u_{r+1}&\\ldots & J_{2r}^{r,r}\\cdot u_{r+1}&(1-u_{1}) \\prod _{j=2}^{r}u_{j} \\\\ \\vdots &&\\vdots &\\vdots \\\\ J_{r1}^{r,r}\\cdot u_{r+1}&\\ldots &J_{rr}^{r,r}\\cdot u_{r+1}&(1-u_{r-1}) \\cdot u_{r} \\\\ 0&\\ldots & -u_{r+1}&(1-u_{r}) \\end{array}\\displaystyle \\right )\\,. \\end{aligned}$$\n\nWe write \\(A_{ij}\\) for the entry in the \\(i\\)th row and \\(j\\)th column of some matrix \\(A\\). Based on a Laplace expansion with respect to the last line, we obtain\n\n$$\\begin{aligned} \\det J^{r+1,r+1}&=(1-u_{r})\\cdot u_{r+1}^{r}\\cdot \\det J^{r,r}+u_{r+1} \\cdot u_{r}\\cdot u_{r+1}^{r-1}\\cdot \\det J^{r,r} \\\\ &=u_{r+1}^{r}\\cdot \\det J^{r,r}=\\prod _{j=1}^{r} u_{j}^{j-1} \\cdot u_{r+1}^{r} \\,. \\end{aligned}$$\n\nWith a telescoping sum in the exponent, similar as in the proof of Lemma 3, we obtain the joint density \\(g\\) of \\(U_{1},\\ldots ,U_{r+1}\\):\n\n$$\\begin{aligned} g(u_{1},\\ldots ,u_{r+1})=\\frac{\\exp (-u_{r+1})}{r!}\\cdot u_{r+1}^{r} \\cdot \\prod _{j=1}^{r}j\\cdot u_{j}^{j-1}\\,,\\,u_{1},\\ldots ,u_{r}\\in [0,1], \\,u_{r+1}>0, \\end{aligned}$$\n\nwhich equals the product of the marginal densities \\(f_{U_{r+1}}(u_{r+1})=e^{-u_{r+1}}u_{r+1}^{r}/{r!}\\), and \\(f_{U_{j}}(u_{j})=j\\cdot u_{j}^{j-1}\\), \\(j\\in \\{1,\\ldots ,r\\}\\). □\n\nSince the right and left tail behavior of the normal distribution are symmetric and since the differences between subsequent extreme order statistics dominate the ones of intermediate order statistics, the auxiliary lemmas and Proposition 3.1 suffice to conclude the main result.\n\nTheorem 1\n\nFor \\((X_{j})_{1\\le j\\le n}\\) i.i.d. \\(\\mathcal{N}(0,1)\\), it holds that\n\n$$\\begin{aligned} \\lim _{n\\to \\infty}\\mathbb{P}\\Big(\\sqrt{2\\log (n)}\\max _{2\\le j\\le n} \\big(X_{(j)}-X_{(j-1)}\\big)\\le x\\Big)=\\prod _{j=1}^{\\infty}\\big(1- \\exp (-j\\cdot x)\\big)^{2}\\,. \\end{aligned}$$\n\n(21)\n\nThe result combines (18) with the non-obvious, asymptotic independence of the differences. Note that the cdf of independent random variables equals the product of their cdfs. Since the differences are not identically distributed, the limit distribution does not belong to the class of standard extreme value distributions for maxima of i.i.d. sequences. Nevertheless, the limit cdf is remarkably simple and intuitive what I was not aware of before exploring this path. After (re-)discovering this result, I expected that it has been discussed in the literature and it was not difficult to find it as Thm. 1 in [22]. With the main focus on a related law of the iterated logarithm, Deheuvels [22] provides a rigorous, more technical and less intuitive proof of the convergence (21). I will therefore call the limit Deheuvels distribution. The square on the right-hand side of (21) is due to the symmetry of the tails. Looking only at one of the tails, we obtain the limit cdf without the square. This is useful when testing for positive and negative jumps separately. In order to compute quantiles based on (21), one can approximate the infinite product by a finite one up to some cut-off, or, even simpler, approximate it by \\(1-\\exp (-x)-\\exp (-2x)\\), for \\(x\\) not too small. This approximation exploits a telescoping sum and is very precise for all relevant quantiles.\n\nFigure 2 compares for \\((X_{j})_{1\\le j\\le n}\\) i.i.d. \\(\\mathcal{N}(0,1)\\) histograms of the statistics\n\n1.\n\n\\(\\sqrt{2\\log (n)}\\cdot \\big(X_{(n)}-\\sqrt{2\\log (n)}+\\log \\big(4\\pi \\log (n))/(2\\sqrt{2\\log (n)})\\big)\\) left-hand side,\n\n2.\n\n\\(\\sqrt{2\\log (n)}\\cdot \\big(X_{(n)}-X_{(n-1)}\\big)\\) in the middle,\n\n3.\n\n\\(\\sqrt{2\\log (n)}\\cdot \\max _{2\\le j\\le n}\\big(X_{(j)}-X_{(j-1)}\\big)\\) right-hand side,\n\nfor finite sample size \\(n=3600\\), corresponding to one price observation per second over one hour, based on a Monte Carlo simulation with 1,000,000 iterations, to the densities of the limit standard Gumbel, standard exponential and Deheuvels distributions. The derivative of the infinite product not having a nice closed form, I use a numerical approximation with Richardson’s extrapolation to evaluate the density.\n\nCrucial for the test is the precision of the fit in the high quantiles. We illustrate it based on our Monte Carlo simulation in Fig. 3 plotting empirical \\((90+j)\\)% percentiles, \\(0\\le j\\le 9\\), against their theoretical asymptotic counterparts. As common in quantile-quantile (q-q) plots, we draw a diagonal line and the closer the points are to the diagonal, the better the fit by the limit distribution. We see that all three limit distributions fit the empirical, finite-sample distributions reasonably well. In fact, the fit for the differences of order statistics are better than that of the Gumbel distribution. I did, however, not try different variants of \\((b_{n})\\) here which could further improve the Gumbel approximation, cf. [42].\n\nWe finish this section with our new Rényi test for jumps. Based on\n\n$$\\begin{aligned} \\sqrt{2\\log (n)}\\cdot \\max _{2\\le j\\le n}\\big(\\Delta ^{n} \\hat{X}_{(j)}- \\Delta ^{n} \\hat{X}_{(j-1)}\\big) \\stackrel{d}{\\longrightarrow } \\mathfrak{D}\\,, \\end{aligned}$$\n\nwhere \\(\\mathfrak{D}\\) is the Deheuvels distribution and \\(\\Delta ^{n} \\hat{X}=n^{1/2}(\\Delta _{1}^{n} X/\\hat{\\sigma}_{1/n}, \\ldots , \\Delta _{n}^{n} X/\\hat{\\sigma}_{1})\\) the vector of normalized increments, we reject the null if the statistic left-hand side exceeds the \\((1-\\alpha )\\) quantile of the Deheuvels distribution. The test has asymptotic level \\(\\alpha \\) and achieves the same rate of convergence as the Gumbel test.\n\nIn order to detect several jumps, the Gumbel test can be performed sequentially. In case of rejection, the time of the largest jump is estimated with the argmaximum. After discarding the largest absolute increment, the test is applied again. In case of another rejection, the next jump time is estimated. This is iterated until the test does not reject any more. For the Rényi test, there is a similar sequential application. In case of rejection, however, we can readily ascribe all increments above or below the maximal difference of the order statistics to jumps. Since the maximum can be taken between several increments which contain jumps, we nevertheless apply another test which may be based on (18).\n\nA fractional Brownian motion (fBm), \\((B_{t}^{H})_{t\\ge 0}\\), with Hurst exponent \\(H\\in (0,1)\\), is a Gaussian process with continuous paths uniquely determined by \\(\\mathbb{E}[B_{t}^{H}]=0\\) for all \\(t\\), and\n\n$$ \\mathbb{E}[B_{t}^{H}\\,B_{s}^{H}]=\\frac{1}{2} (t^{2H}+s^{2H}-|t-s|^{2H}) \\,,t,s\\ge 0\\,. $$\n\n\\((B_{t}^{H})\\) has stationary Gaussian increments \\((B^{H}_{t}-B^{H}_{s})\\sim N(0,|t-s|^{2H})\\), which are positively correlated for \\(H>1/2\\), and negatively correlated for \\(H<1/2\\). Except the case of a standard Brownian motion when \\(H=1/2\\), increments are thus not independent and \\((B_{t}^{H})\\) is not a Markov process and also not a semi-martingale. The fBm is self-similar with index \\(H\\) given by the Hurst exponent, such that \\(a^{-H}B^{H}_{at}\\) is distributed as \\(B_{t}^{H}\\) for all \\(a>0\\). Interested readers find a nice survey about fBm in [41]. Harold Edwin Hurst was in fact not a mathematician, but a British hydrologist who empirically found long-range dependence in a time series of his measurements of the water level in the Nile river. Long-range dependence refers to a high degree of persistence in the data and after fBm was introduced by [39], it can be modelled by a fBm with large Hurst exponents. Such long memory was attributed in finance to volatility processes and Comte and Renault [20] suggested a fractional Ornstein-Uhlenbeck process, with \\(H>1/2\\), as a model for the log-volatility. The Hurst exponent determines at the same time the regularity of the process in Assumption 1 and by the Kolmogorov-Chentsov continuity theorem the paths are Hölder continuous for any index strictly smaller than \\(H\\). A recent strand of literature considers a rough fractional stochastic volatility model built on the same kind of processes but with small Hurst exponents \\(H<1/2\\). This development was initiated by [27] and is mainly motivated by empirical evidence. It is important to point out that related literature is looking at volatility processes over longer time periods and not on an intra-daily basis over, e.g., just one single day. The strategy of [27] is to consider a time series of realized volatilities based on high-frequency, intra-daily data over some longer period. Modelling integrated volatilities, or realized volatilities directly, by a fractional process, the latent volatility becomes observable, either directly or with negligible noise from the estimation. Based on \\(\\sigma _{j\\Delta},0\\le j\\le n\\), they study the statistics\n\n$$\\begin{aligned} m(q,\\Delta ) = n^{-1}\\sum _{k=1}^{n}\\left |\\log (\\sigma _{k\\Delta})- \\log (\\sigma _{(k-1)\\Delta})\\right |^{q}\\,,q>0. \\end{aligned}$$\n\n(22)\n\nThe idea is to perform linear regressions what we motivate here differently than in [27]. Based on the defining properties of fBm above, we see for some time step \\(\\Delta \\) and \\(l,k\\in \\mathbb{N}\\), \\(l\\le k\\), that\n\n$$\\begin{aligned} \\log |\\sigma \\cdot B^{H}_{k\\Delta}-\\sigma \\cdot B^{H}_{(k-l)\\Delta}|&= \\log (\\sigma )+H\\cdot \\log (l\\Delta )+\\log |Z|\\,, \\end{aligned}$$\n\nwith \\(Z\\sim \\mathcal{N}(0,1)\\). This already resembles the model equation of a linear model, i.e., a linear function of \\(\\log (l\\Delta )\\) with slope \\(H\\). Having observations \\(\\sigma _{j\\Delta}\\), \\(0\\le j\\le n\\), we compute (22) over different coarser grids, or equivalently with \\(\\log (\\sigma _{k\\Delta})-\\log (\\sigma _{(k-l)\\Delta}), 1\\le l\\le L\\), up to some \\(L\\in \\mathbb{N}\\), and regress \\(m(q,l\\Delta )\\) on \\(\\log (l\\Delta )\\) to estimate intercept and slope with a simple linear regression. If \\((\\log (\\sigma _{t}))\\) was a fBM, or as well if it was a more general fractional process, we expect to find \\(q\\cdot H\\) as the slope in these regressions. This and also more refined estimators of the Hurst exponent yield in several empirical studies of financial data similar results with Hurst exponents smaller than 0.2.\n\nThe data sets from the Oxford-Man Institute used for illustrations in [27] are unfortunately not available any more. We replicate the same behavior of statistics \\(m(q,\\Delta )\\) as in Figs. 5-7 of [27] for a time series of 7021 quasi maximum likelihood daily volatility estimates from 1996 to 2023, based on the method by [54], inferred from intra-day high-frequency trade prices of the S&P 500 market ETF. The data is constructed from the Risk Lab on Dacheng Xiu’s websiteFootnote 3 and the S&P 500 is certainly a very relevant financial index. It is not important if we insert \\(\\sigma _{j\\Delta}^{2}\\), or a square root \\(\\sigma _{j\\Delta}\\), in (22). The definition without square is taken from [27], but we insert the estimates of squared volatility. The left plot in Fig. 4 illustrates the linear regressions for \\(q=j/2\\), \\(1\\le j\\le 4\\). The points give the computed statistics. The statistics \\(m(q,l\\Delta )\\), \\(1\\le l\\le 100\\), look as a function of \\(l\\) indeed almost perfectly logarithmic. This is confirmed by the good fit of the linear functions in the plot. The right-hand side of Fig. 4 compares the estimated slope, called \\(\\zeta _{q}\\) in [27], along different values of \\(q\\). From this illustration, we see the estimate \\(\\hat{H}\\approx 0{.16}\\) for this data. Again, we find empirical evidence for a small Hurst exponent fitting a fBm to the log-volatilities. Moreover, our data shows pronounced negative empirical autocorrelations which further indicates small Hurst exponents and would contradict large ones.\n\nThis new rough volatility paradigm already stimulated a considerable body of research, beyond the high-frequency literature, for instance, on financial implications, in [7] and [32]. The main motivation from econometrics to use this model is that it facilitates improved volatility forecasting, see [53], among others. Having a Gaussian process, optimal prediction is feasible and given by conditional expectation. While the puzzle of rough volatility vs. volatility persistence is now to a large extent – but not yet fully – understood, forecasting mainly exploits a correlation structure. From this point of view, large Hurst exponents and very small ones could both favour a similar good performance of prediction, while the opposite is the case for values close to 1/2. The application of rough volatility for forecasting uses the continuous-time model rather as a substitute of time series models over longer periods, where the latency of volatility is less crucial than within the framework of intra-daily high-frequency observations. The question if we can infer the Hurst exponent, or more general the regularity \\(\\alpha \\) from Assumption 1, based on observations of the log-price \\((X_{j\\Delta _{n}})\\) is nevertheless of great theoretical interest. Given its crucial role in spot volatility estimation in Sect. 2, it is moreover practically relevant.\n\nIt is known from [49] that, based on \\((n+1)\\) high-frequency observations, the Hurst exponent \\(\\alpha \\) of the latent volatility can be estimated with optimal rate \\(n^{-1/(4\\alpha +2)}\\), if \\(\\alpha >1/2\\), exploiting results from [28]. The important question for rough volatility, if this is true also in case that \\(\\alpha <1/2\\), is confirmed in the recent work [18]. Estimation methods and asymptotic confidence are furthermore established in the companion work [17]. This is shown for models in that the log-volatility follows a fractional process of similar nature as fBm. The Hurst exponent \\(\\alpha \\) is in this case not only the regularity from Assumption 1, but determines also the inter-dependence structure (persistence) and more. In joint work with Moritz Jirak, we are interested in the question, if the regularity \\(\\alpha \\) can be identified from high-frequency log-prices \\((X_{j\\Delta _{n}})\\) in the more general case. Since for direct observations, the rates are the same, and most estimators for the Hurst exponent in this framework are in fact constructed to assess the regularity, this could be expected. However, we obtain a rather negative result with the following lower bound. We impose regularity \\(\\alpha \\) in (23) and that the process exploits this regularity in the sense of a lower and an upper bound. It is clear that only the upper bound from Assumption 1 is not a suitable condition when we aim to estimate \\(\\alpha \\), since, e.g., constant functions satisfy this for any \\(\\alpha \\).\n\nTheorem 2\n\nSuppose that positive constants \\(c_{\\sigma}\\) and \\(C_{\\sigma}\\) exist, such that for \\(s,t\\ge 0\\):\n\n$$\\begin{aligned} c_{\\sigma} s^{2\\alpha}\\le \\mathbb{E}\\big[(\\sigma _{(t+s)}-\\sigma _{t})^{2} \\big] \\le C_{\\sigma} s^{2\\alpha}\\,. \\end{aligned}$$\n\n(23)\n\nThe minimax lower bound for estimation of \\(\\alpha \\) is determined by\n\n$$ \\exists \\delta >0:\\;\\liminf _{n\\to \\infty}\\inf _{\\hat{\\alpha}_{n}} \\max _{\\alpha \\in \\{\\alpha _{0},\\alpha _{0}+\\delta r_{n}\\}} { \\mathbb{P}_{ \\alpha _{0}}}\\big(|\\hat{\\alpha}_{n}-\\alpha _{0}|\\ge \\delta r_{n}\\big)>0 \\,, $$\n\nwith \\(r_{n}=(n^{-1/2+2\\alpha _{0}})/\\log (n)\\). That is, for any sequence of estimators \\(\\hat{\\alpha}_{n}\\) of the true parameter \\(\\alpha _{0}\\), \\(r_{n}\\) gives a lower bound on the rate with that the minimax risk decreases in \\(n\\).\n\nThe proof is provided in Sect. 7. Lower bounds for minimax rates typically rely on statistical groundwork by [51]. We exploit techniques and results from [51] for the proof of Theorem 2 and our construction mimics one used in [11] for a related, different lower bound pertaining change-points of \\(\\alpha \\). In our model, different from the one of [18], \\(\\alpha \\) determines only the regularity. The proof of the lower bound utilizes a sub-model which does not have the dependence structure of fBm. Since lower bounds extend to supersets but not to subsets, it does not apply to the more specific model with a fBm and is hence not in conflict with the result from [18]. In particular, we obtain \\(n^{-1/2+2\\alpha}\\), for \\(\\alpha <1/4\\), as a lower bound. This shows that a consistent estimator only exists for \\(\\alpha _{0}<1/4\\)! For \\(\\alpha _{0}\\) close to zero we get close to the standard parametric rate \\(n^{-1/2}\\). Both rates from [18] and Theorem 2 have in common that the rate hinges on the parameter and is better for smaller values. The comparison reveals that estimation of a latent volatility’s regularity, or the Hurst exponent imposing a model with a fractional process, are in general different problems. We conclude that estimating the regularity is statistically more difficult.\n\nLet me finish the section with a positive result. We use the stochastic Landau symbols \\(\\mathcal{O}_{\\mathbb{P}}\\) and \\(o_{\\mathbb{P}}\\). Write \\(\\hat{\\alpha}\\) for the sequence \\(\\hat{\\alpha}_{n} \\) and assume that it is consistent:\n\n$$\\begin{aligned} \\hat{\\alpha}-\\alpha =\\mathcal{O}_{\\mathbb{P}}\\big(n^{-1/2}\\Delta _{n}^{-2 \\alpha}\\big)\\,, \\end{aligned}$$\n\n(24)\n\nwith \\(n^{-1/2}\\Delta _{n}^{-2\\alpha}\\to 0\\), such that \\((\\hat{\\alpha}-\\alpha )\\stackrel{\\mathbb{P}}{\\longrightarrow} 0\\), and consider the spot volatility estimator (13) with optimal \\(k_{n}\\propto \\Delta _{n}^{-\\frac{2\\alpha}{2\\alpha +1}}\\). Not knowing \\(\\alpha \\), we replace it by \\(\\hat{\\alpha}\\), and call the resulting estimator \\(\\hat{\\sigma}_{s}^{2,ad}\\). The elementary identities\n\n$$\\begin{aligned} \\frac{\\hat{\\alpha}}{2\\hat{\\alpha}+1}-\\frac{\\alpha}{2\\alpha +1}= \\frac{\\hat{\\alpha}-\\alpha}{(2\\alpha +1)(2\\hat{\\alpha}+1)}\\,, \\end{aligned}$$\n\nand\n\n$$\\begin{aligned} \\Delta _{n}^{\\frac{\\hat{\\alpha}-\\alpha}{(2\\alpha +1)(2\\hat{\\alpha}+1)}}&= \\exp \\Big(\\frac{\\alpha -\\hat{\\alpha}}{(2\\alpha +1)(2\\hat{\\alpha}+1)} \\log \\Delta _{n}^{-1}\\Big) \\\\ &=1+\\frac{\\alpha -\\hat{\\alpha}}{(2\\alpha +1)(2\\hat{\\alpha}+1)}\\log \\Delta _{n}^{-1}+\\mathcal{O}_{\\mathbb{P}}\\Big(n^{-1}\\Delta _{n}^{-4 \\alpha}(\\log \\Delta _{n}^{-1})^{2}\\Big) \\\\ &=1+{o}_{\\mathbb{P}}(1)\\,, \\end{aligned}$$\n\nthen yield with the results from Sect. 3 that for two random variables \\(Z_{1}\\) and \\(Z_{2}\\):\n\n$$\\begin{aligned} \\hat{\\sigma}_{s}^{2}-\\hat{\\sigma}_{s}^{2,ad}&=\\sigma _{s}^{2}+Z_{1} \\, \\Delta _{n}^{\\frac{\\alpha}{2\\alpha +1}}-\\sigma _{s}^{2}-Z_{2}\\, \\Delta _{n}^{\\frac{\\hat{\\alpha}}{2\\hat{\\alpha}+1}} \\\\ &=\\Delta _{n}^{\\frac{\\alpha}{2\\alpha +1}}(Z_{1}-Z_{2}+{o}_{\\mathbb{P}}(1))=\\mathcal{O}_{\\mathbb{P}}\\big(\\Delta ^{ \\frac{\\alpha}{2\\alpha +1}}\\big)\\,. \\end{aligned}$$\n\nWe conclude that \\(\\hat{\\sigma}_{s}^{2,ad}\\) attains the same optimal rate of convergence as the estimator which exploits known \\(\\alpha \\).\n\nWhile we modelled high-frequency log-prices so far as discretizations of continuous-time stochastic processes, when having available data from a limit order book, there is not only one price at some given time. Figure 5 gives a snapshot of price dynamics of the Apple asset traded at Nasdaq over a 10 minutes time interval. We use Nasdaq data from Lobster.Footnote 4 A blue line shows the evolution of the best ask price, that is, the lowest price at which someone is willing to sell the asset. A red line shows the best bid price, that is, the highest price someone is offering to buy the asset. In between there is a bid-ask spread. The many points above the best ask illustrate many other active ask-limit orders and below the best bid active bid-limit orders. Trading usually takes places when market orders arrive with that someone buys or sells the asset at the best available price. These are executed against the available limit orders. For this reason trade prices bounce between the best ask and best bid what makes the illustration of all three in the same plot a bit overfraught. Trade prices are plotted in Fig. 5 as black dots. A prevalent concept for market microstructure in financial econometrics is to assume some underlying efficient, semimartingale log-price process in an arbitrage-free market modelling longer-term price dynamics, while high-frequency observations are diluted by an additive market microstructure noise. Therefore, the observation model to account for market microstructure is\n\n$$\\begin{aligned} Y_{i}=X_{t_{i}}+\\epsilon _{i}~,~0\\le i\\le n, \\end{aligned}$$\n\n(25)\n\nwith an Itô semimartingale \\((X_{t})\\) and noise \\((\\epsilon _{i})\\). Such a model was proposed in [5], among others, for trade prices with regular noise and there is a vast area of research on this model. Classical regular Market Microstructure Noise (MMN) \\((\\epsilon _{i})_{0\\le i\\le n}\\) is i.i.d. with \\(\\mathbb{E}[\\epsilon _{i}]=0\\). If a full limit order book is available, it is recently applied to mid quotes, i.e., averages of best bid and best ask quotes. If we model the prices of (best) ask quotes directly, a natural assumption is that they all lie above the efficient, semimartingale log-price \\((X_{t})\\). Reasons are that ask orders will typically be submitted at prices above the level that is seen as current fair price to make money and they also lie above the trade prices. This leads us to a stochastic boundary model with observations in the epigraph of a semimartingale boundary process. We hence use model (25) with Limit Order Microstructure Noise (LOMN) which satisfies\n\n$$\\begin{aligned} \\epsilon _{i}\\stackrel{i.i.d.}{\\sim}F_{\\eta},\\,\\epsilon _{i}\\ge 0\\,, \\end{aligned}$$\n\n(26)\n\nthat is, Lower-bounded, One-sided Microstructure Noise. The model was introduced in [10]. We assume that \\((\\epsilon _{i})_{0\\le i\\le n}\\) is exogenous with a cdf\n\n$$\\begin{aligned} F_{\\eta}(x) =\\eta x\\,\\big(1+{o}(1)\\big) ,\\; \\text{as}~x\\downarrow 0\\,. \\end{aligned}$$\n\n(27)\n\nBid prices are analogously modelled with noise that is upper bounded and both combined in practice. Although Fig. 5 shows prices in a discrete image space under a very high time resolution, it is standard to work with the real-valued process \\((X_{t})\\), to perform estimation of the volatility, or other daily quantities. It is then natural to consider continuous noise distributions also. Since our methods use differences between local minima or maxima of the data only, it is not crucial that the boundary of the noise is exactly zero. It can be some unknown constant instead, or even a regular function over time, what is meaningful to include compensation of market processing costs. This possible generalization is one reason why we model ask and bid prices separately in boundary models, e.g., instead of considering noise on a bounded interval. Moreover, a model with noise on an interval would not simplify the statistical problem but rather complicate the situation. Condition (27) does not impose a parametric form of the noise. The assumed standard behaviour of the cdf close to the boundary is satisfied by many common distributions, as a uniform distribution on some interval \\([0,A]\\), \\(A>0\\), an exponential distribution as we know from Sect. 3, and a heavy-tailed (shifted) Pareto distribution. Nevertheless, we currently work on generalizations of the model to allow for some general tail index which is 1 in (27). The irregular, non-negative noise leads to statistical inference based on local minima instead of local averages which are used under regular noise in the literature. This is motivated by the problem of estimating boundary parameters in parametric statistics. We explain the key idea looking at the prominent example of the taxi problem. An important advantage of LOMN and using order statistics compared to MMN is that no conditions on the right tail of the noise distribution or on the existence of moments of the noise are required.\n\nIn our stochastic boundary model we do of course not have a constant boundary to estimate as in the taxi problem, but want to recover a latent semimartingale boundary process. This situation is intricate, but – although the approach appears venturous – we approximate the boundary process locally constant over small time blocks. From the analogy to the taxi problem, it is then natural to estimate the efficient log-price locally by local block-wise minima\n\n$$\\begin{aligned} m_{k,n}=\\min _{i\\in \\mathcal{I}_{k}^{n}}Y_{i}\\,,~\\mathcal{I}_{k}^{n}= \\{t_{i}^{n}: ~t_{i}^{n} \\in (kh_{n},(k+1)h_{n})\\},\\,0\\le k\\le h_{n}^{-1}-1 \\,. \\end{aligned}$$\n\nLet us assume for simplicity equidistant observations again, \\(t_{i}^{n}=i/n\\), and \\(h_{n}^{-1}\\in \\mathbb{N}\\) being the sequence of number of blocks, and \\(nh_{n}\\in \\mathbb{N}\\) the number of observations per block. In our asymptotic high-frequency regime, \\(h_{n}\\to 0\\), and \\(nh_{n}\\to \\infty \\), as \\(n\\to \\infty \\). There is a balanced regime, \\(h_{n}\\propto n^{-2/3}\\), in which the stochastic order of the minimal error over a block and the movement of the boundary process over a block are the same, as\n\n$$\\begin{aligned} \\min _{i\\in \\mathcal{I}_{k}^{n}}\\epsilon _{i}=\\mathcal{O}_{\\mathbb{P}} \\big( (nh_{n})^{-1}\\big),~\\text{and}~\\big(X_{(k+1)h_{n}}-X_{kh_{n}} \\big)=\\mathcal{O}_{\\mathbb{P}}\\big(h_{n}^{1/2}\\big)\\,. \\end{aligned}$$\n\nBased on local minima in this balanced regime, a rate-optimal estimator of the integrated volatility has been established in [10].\n\nLike the estimators based on the maximum in the taxi problem converge faster than with the standard rate, Bibinger et al. [10] proved that their estimator attains an optimal rate \\(n^{-1/3}\\), with that the root mean squared error tends to zero, which improves upon the well-known standard rate \\(n^{-1/4}\\) for regular noise. Lower bounds for the rate and the asymptotic variance under regular noise in the parametric case were established by [29]. The distribution of local minima in the balanced regime is, however, involved which yet limited available results. In particular, in [10] we could not provide asymptotic confidence for the integrated volatility. The article [8] contributes a step forward in this direction and extends the probabilistic theory required to work with the boundary model. For the tail function of local minima, we conclude with conditioning, (25), (27), a Taylor expansion and dominated convergence that\n\n$$\\begin{aligned} &\\mathbb{P}\\Big(h_{n}^{-1/2}\\big(m_{k,n}-X_{kh_{n}}\\big)\\Big)>x \\sigma _{kh_{n}}\\Big) \\\\ ~&=\\mathbb{E}\\bigg[\\exp \\Big(\\sum _{i=knh_{n}+1}^{(k+1)nh_{n}}\\log \\big(1-F_{\\eta}\\big(h_{n}^{1/2}\\sigma _{kh_{n}}\\big(x-h_{n}^{-1/2}(W_{i/n}-W_{kh_{n}}) \\big)\\big)\\big)\\Big)\\bigg] \\\\ ~&=\\mathbb{E}\\Big[\\exp \\Big(-nh_{n}^{3/2}\\sigma _{kh_{n}}\\eta \\int _{0}^{1}(B_{t}-x)_{-} \\,\\text{d}t\\,(1+{o}(1))\\Big)\\Big] \\end{aligned}$$\n\nfor all \\(x<0\\), with a standard Brownian motion \\((B_{t})\\). To work with the integrated negative part of a Brownian motion in the last expression, we exploit and extend results about local time of Brownian motion. One main ingredient of the asymptotic analysis in [8] is an expansion of this tail function based on a generalized arcsine law. Here, we focus on a simpler idea which is nevertheless the most important step to approximate the distribution of the local minima. Selecting blocks slightly larger than in the balanced regime, we have \\(nh_{n}^{3/2} \\to \\infty \\) in the exponent, such that the probability tends to zero unless the integral yields zero. This is the case if and only if the event \\(\\{\\min _{0\\le t\\le 1} B_{t}\\ge x\\}\\) occurs. In this regime, we hence obtain that\n\n$$\\begin{aligned} &\\mathbb{P}\\Big(h_{n}^{-1/2}\\big(m_{k,n}-X_{kh_{n}}\\big)\\Big)>x \\sigma _{kh_{n}}\\Big) \\\\ ~&=\\mathbb{P}\\big(\\min _{0\\le t\\le 1} B_{t}\\ge x\\big)+{o}(1)\\,. \\end{aligned}$$\n\nThe distribution of the minimum of a Brownian motion over the interval \\([0,1]\\) is remarkably simple. This is due to the reflection principle connected with the strong Markov property of \\((B_{t})\\). We derive with the reflection principle from the above approximation that for \\(x<0\\), since \\(\\mathbb{P}\\big(\\min _{0\\le t\\le 1} B_{t}\\ge x\\big)=\\mathbb{P}\\big(|B_{1}| \\ge -x\\big)\\), that\n\n$$ -h_{n}^{-1/2}\\big(m_{k,n}-X_{kh_{n}}\\big) \\stackrel{st}{\\longrightarrow} H\\negthinspace M \\negthinspace N(0, \\sigma _{kh_{n}}^{2})\\,, $$\n\nas \\(nh_{n}^{3/2}\\to \\infty \\). The distribution of \\(|Z|\\), for \\(Z\\sim \\mathcal{N}(0,1)\\), is called half-normal distribution. Since our limit is distributed as the product \\(\\sigma _{kh_{n}}|Z|\\) then, we call it mixed half-normal.\n\nFor volatility estimation, with \\((B_{t})\\) and \\((\\tilde{B}_{t})\\) two independent standard Brownian motions, define\n\n$$\\begin{aligned} \\Psi _{\\negthinspace n}(\\sigma ^{2})=h_{n}^{-1} \\mathbb{E}\\Big[\\Big( \\min _{i\\in \\{0,\\ldots ,nh_{n}-1\\}}\\big(\\sigma B_{\\frac{i}{n}}+ \\epsilon _{i}\\big)-\\min _{i\\in \\{1,\\ldots ,nh_{n}\\}}\\big(\\sigma \\tilde{B}_{\\frac{i}{n}}+\\epsilon _{i}\\big)\\Big)^{2}\\Big]. \\end{aligned}$$\n\n(28)\n\nFrom the moments of the half-normal distribution, as \\(nh_{n}^{3/2}\\to \\infty \\), we obtain that\n\n$$\\begin{aligned} \\Psi _{\\negthinspace n}(\\sigma ^{2})=\\frac{2(\\pi -2)}{\\pi}\\sigma ^{2}+{o}(1)\\,. \\end{aligned}$$\n\n(29)\n\nNot having any lower bound for the integrated negative part in the above expression, the remainder decays however slowly in \\(n\\), and we require an asymptotic expansion and a numerical approximation of \\(\\Psi _{\\negthinspace n}\\) for an estimator with desirable properties. Nevertheless, with \\(K_{n}\\to \\infty \\), we first consider the simple estimator\n\n$$\\begin{aligned} \\hat{\\sigma}^{2}_{\\tau}=\\frac{\\pi}{2(\\pi -2)K_{n}}\\sum _{k=(\\lfloor h_{n}^{-1} \\tau \\rfloor -K_{n})\\vee 1}^{\\lfloor h_{n}^{-1}\\tau \\rfloor -1}h_{n}^{-1} \\big(m_{k,n}-m_{k-1,n})^{2}\\,, \\end{aligned}$$\n\n(30)\n\nin case without price jumps and a truncated version which is robust to nuisance jumps. A main result of [8] is that under Assumption 1 for \\(K_{n}=C_{K} h_{n}^{\\delta -2\\alpha /(1+2\\alpha )}\\), with some constants \\(C_{K}\\) and \\(\\delta \\), \\(0<\\delta <2\\alpha /(1+2\\alpha )\\), the estimator satisfies the stable clt\n\n$$\\begin{aligned} K_{n}^{1/2}\\Big(\\hat{\\sigma}^{2}_{\\tau}-\\frac{\\pi}{2(\\pi -2)}\\Psi _{ \\negthinspace n}\\big(\\sigma _{\\tau}^{2}\\big)\\Big) \\stackrel{st}{\\longrightarrow} \\mathcal{N}\\Big(0, \\frac{7\\pi ^{2}/4-2\\pi /3-12}{(\\pi -2)^{2}}\\sigma ^{4}_{\\tau}\\Big)\\,. \\end{aligned}$$\n\n(31)\n\nIn fact, we use the (approximated) function \\(\\Psi _{\\negthinspace n}\\) for a bias correction to obtain a clt at optimal rate. The asymptotic variance is derived with the expansion of the tail function based on the joint distribution of minimum and terminal value of a Brownian motion over \\([0,1]\\) concluded from the reflection principle. To this end we use one of the most important examples for applications of Fubini-Tonelli in probability that relates moments and the tail function: For some non-negative random variable \\(Z\\), with distribution \\(\\mathbb{P}_{Z}\\), and \\(k\\in \\mathbb{N}\\), it holds true that\n\n$$\\begin{aligned} \\mathbb{E}[Z^{k}]=\\int _{0}^{\\infty}z^{k}\\,\\text{d}\\mathbb{P}_{Z}(z)&= \\int _{0}^{\\infty}\\Big(\\int _{0}^{z} ky^{k-1}\\text{d}y\\Big)\\,\\text{d} \\mathbb{P}_{Z}(z) \\\\ &=k\\int _{0}^{\\infty}y^{k-1}\\int _{y}^{\\infty}\\text{d}\\mathbb{P}_{Z}(z) \\,\\text{d}y=k\\int _{0}^{\\infty}y^{k-1}\\mathbb{P}(Z>y)\\text{d}y\\,. \\end{aligned}$$\n\nIntegration with respect to the \\(\\sigma \\)-finite probability and Lebesgue measures is exchanged here. Extensions to covariances and real-valued random variables are available and allow us to use the form of the tail function from above.\n\nIn a recent preprint [12], we develop jump detection methods under LOMN including a Gumbel test for jumps. It is based on\n\n$$ T^{BHR}=\\max _{k=1,\\ldots ,h_{n}^{-1}-1}\\Big| \\frac{m_{k,n}-m_{k-1,n}}{\\big(\\hat{\\sigma}_{kh_{n}}^{2}\\big)^{1/2}} \\Big|. $$\n\nBased on extreme value theory, we show that under the null hypothesis\n\n$$ H_{0}:\\sup _{\\tau \\in [0,1]}|\\Delta X_{\\tau}|=0 $$\n\nand for \\((\\sigma _{t})\\in C^{\\alpha}\\), i.e., Hölder continuous with regularity \\(\\alpha \\), it holds with \\(h_{n}=2\\log (2h_{n}^{-1}-2)n^{-2/3}\\) that\n\n$$\\begin{aligned} n^{1/3}\\,T^{BHR}-2\\log (2h_{n}^{-1}-2)+\\log \\big(\\pi \\log (2h_{n}^{-1}-2) \\big)\\stackrel{d}{\\longrightarrow }\\Lambda \\,, \\end{aligned}$$\n\n(32)\n\nwith \\(\\Lambda \\) the standard Gumbel distribution. Under local alternatives\n\n$$ H_{1}:\\liminf _{n\\to \\infty}{{n^{\\beta}}}\\sup _{\\tau \\in (0,1)}|\\Delta X_{\\tau}|>0,~\\text{for some}~{{\\beta < 1/3}} \\,, $$\n\nthe test satisfies with \\(q_{1-a}^{\\thinspace \\Lambda}=-\\log \\left (-\\log \\left (1-a\\right ) \\right )\\) that\n\n$$\\begin{aligned} \\lim _{n\\to \\infty}\\mathbb{P}_{H_{1}}\\Big(n^{1/3}\\,T^{BHR}-B_{n}>q_{1-a}^{ \\thinspace \\Lambda}\\Big)=1\\,. \\end{aligned}$$\n\n(33)\n\nThe subscript of the measure is to indicate that we are under \\(H_{1}\\), and the path has at least one jump. Considering local alternatives, the question about which probability space(s) to work on is justified, but not particularly important here, since we can simply consider the distributions of the statistics directly to avoid an arduous construction.\n\nThe main insight of this result is that under LOMN smaller jumps can be identified compared to MMN. While we can detect jumps of size larger than \\(n^{-1/3}\\), only jumps of size larger than \\(n^{-1/4}\\) can be found under MMN. Moreover, working with order statistics to infer jumps has some nice advantages compared to local averages under MMN, where averaging over jump times is creating huge problems described as “pulverisation of jumps by pre-averages” by [40]. This is illustrated in Sect. 2 of [12]. One ingredient to show (32) is uniform consistency of the spot volatility estimation, for which we require the continuity of \\((\\sigma _{t})\\) under \\(H_{0}\\). Furthermore, the precise Gumbel convergence for differences between half-normal random variables is determined, since we cannot trace this one back to a standard example of extreme value theory. Our sequence is furthermore not i.i.d., but it is known that Gumbel convergences of maxima hold analogously more generally under weak dependence conditions."
    }
}
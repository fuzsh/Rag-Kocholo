{
    "id": "dbpedia_3934_2",
    "rank": 44,
    "data": {
        "url": "https://hackaday.com/tag/llama/",
        "read_more_link": "",
        "language": "en",
        "title": "Hackaday",
        "top_image": "https://hackaday.com/wp-content/uploads/2020/07/hackaday-logo-with-text-opengraph-default-image.jpg",
        "meta_img": "https://hackaday.com/wp-content/uploads/2020/07/hackaday-logo-with-text-opengraph-default-image.jpg",
        "images": [
            "https://hackaday.com/wp-content/themes/hackaday-2/img/logo.png",
            "https://hackaday.com/wp-content/uploads/2024/04/text-compression-featured.jpg?w=600&h=450",
            "https://hackaday.com/wp-content/uploads/2024/03/winamp-pi-main.png?w=600&h=450",
            "https://hackaday.com/wp-content/uploads/2024/02/local-llm-featured.jpg?w=600&h=450",
            "https://hackaday.com/wp-content/uploads/2023/03/AIcoding.jpg?w=600&h=450",
            "https://hackaday.com/wp-content/uploads/2023/12/logo.jpg?w=250",
            "https://hackaday.com/wp-content/uploads/2018/05/ros.jpg?w=600&h=450",
            "https://hackaday.com/wp-content/uploads/2017/08/memory.jpg?w=600&h=450",
            "https://hackaday.com/wp-content/uploads/2023/03/llama_feat.jpg?w=600&h=450",
            "https://hackaday.com/wp-content/uploads/2024/07/hadimg_pisound_thumb_bright.png?w=600&h=600",
            "https://hackaday.com/wp-content/uploads/2024/08/pico-2-web_thumbnail.png?w=600&h=600",
            "https://hackaday.com/wp-content/uploads/2016/07/i2c_montage_thumbnail.png?w=600&h=600",
            "https://hackaday.com/wp-content/uploads/2017/06/biological2_thumbnail.png?w=600&h=600",
            "https://hackaday.com/wp-content/uploads/2017/11/radioapocalypse-thumb.jpg?w=600&h=600",
            "https://hackaday.com/wp-content/uploads/2024/08/hadimg_router_cpu_thumb.jpg?w=600&h=600",
            "https://hackaday.com/wp-content/uploads/2015/09/links-thumb.jpg?w=600&h=600",
            "https://hackaday.com/wp-content/uploads/2020/05/Automation_thumbnail.png?w=600&h=600",
            "https://hackaday.com/wp-content/uploads/2016/05/microphone-thumb.jpg?w=600&h=600",
            "https://hackaday.com/wp-content/uploads/2016/01/darkarts-thumb.jpg?w=600&h=600",
            "https://hackaday.com/wp-content/uploads/2024/07/hadimg_pisound_thumb_bright.png?w=600&h=600",
            "https://hackaday.com/wp-content/uploads/2024/08/pico-2-web_thumbnail.png?w=600&h=600",
            "https://hackaday.com/wp-content/uploads/2016/07/i2c_montage_thumbnail.png?w=600&h=600",
            "https://hackaday.com/wp-content/uploads/2017/06/biological2_thumbnail.png?w=600&h=600",
            "https://hackaday.com/wp-content/uploads/2017/11/radioapocalypse-thumb.jpg?w=600&h=600",
            "https://hackaday.com/wp-content/uploads/2024/08/hadimg_router_cpu_thumb.jpg?w=600&h=600",
            "https://hackaday.com/wp-content/uploads/2015/09/links-thumb.jpg?w=600&h=600",
            "https://hackaday.com/wp-content/uploads/2020/05/Automation_thumbnail.png?w=600&h=600",
            "https://hackaday.com/wp-content/uploads/2016/05/microphone-thumb.jpg?w=600&h=600",
            "https://hackaday.com/wp-content/uploads/2016/01/darkarts-thumb.jpg?w=600&h=600",
            "https://hackaday.com/wp-content/themes/hackaday-2/img/logo.png",
            "https://analytics.supplyframe.com/trackingservlet/impression?action=pageImpression&zone=HDay_tag&extra=tag%3Dllama",
            "https://hackaday.com/wp-content/uploads/2018/04/close.png"
        ],
        "movies": [],
        "keywords": [],
        "meta_keywords": [
            ""
        ],
        "tags": null,
        "authors": [
            "Jenny List",
            "Bryan Cockfield",
            "Donald Papp",
            "Matthew Carlson"
        ],
        "publish_date": null,
        "summary": "",
        "meta_description": "",
        "meta_lang": "en",
        "meta_favicon": "https://hackaday.com/wp-content/themes/hackaday-2/favicon.ico?v=3",
        "meta_site_name": "Hackaday",
        "canonical_link": "https://hackaday.com/tag/llama/",
        "text": "There are many claims in the air about the capabilities of AI systems, as the technology continues to ascend the dizzy heights of the hype cycle. Some of them are true, others stretch definitions a little, while yet more cross the line into the definitely bogus. [J] has one that is backed up by real code though, a compression scheme for text using an AI, and while there may be limitations in its approach, it demonstrates an interesting feature of large language models.\n\nThe compression works by assuming that for a sufficiently large model, it’s likely that many source texts will exist somewhere in the training. Using llama.cpp it’s possible to extract the tokenization information of a piece of text contained in its training data and store that as the compressed output. The decompressor can then use that tokenization data as a series of keys to reassemble the original from its training. We’re not AI experts but we are guessing that a source text which has little in common with any training text would fare badly, and we expect that the same model would have to be used on both compression and decompression. It remains a worthy technique though, and no doubt because it has AI pixie dust, somewhere there’s a hype-blinded venture capitalist who would pay millions for it. What a world we live in!\n\nOddly this isn’t the first time we’ve looked at AI text compression.\n\nIn the late 90s as MP3s and various file sharing platforms became more common, most of us were looking for better players than the default media players that came with our operating systems, if they were included at all. To avoid tragedies like Windows Media Center, plenty of us switched to Winamp instead, a much more customizable piece of software that helped pave the way for the digital music revolution of that era. Although there are new, official versions of Winamp currently available, nothing really tops the nostalgia of the original few releases of the software which this project faithfully replicates in handheld form.\n\nThe handheld music player uses a standard Raspberry Pi (in this case, a 3B) and a 3.5″ TFT touchscreen display, all enclosed in a clear plastic case. With all of the Pi configuration out of the way, including getting the touchscreen working properly, the software can be set up. It uses QMMP as a media player with a Winamp skin since QMMP works well on Linux systems with limited resources. After getting it installed there’s still some configuration to do to get the Pi to start it at boot and also to fit the player perfectly into the confines of the screen without any of the desktop showing around the edges.\n\nAlthough it doesn’t use the original Winamp software directly, as that would involve a number of compatibility layers and/or legacy hardware at this point, we still think it’s a faithful recreation of how the original looked and felt on our Windows 98 machines. With a battery and a sizable SD card, this could have been the portable MP3 player many of us never knew we wanted until the iPod came out in the early 00s, and would certainly still work today for those of us not chained to a streaming service. A Raspberry Pi is not the only platform that can replicate the Winamp experience, though. This player does a similar job with the PyPortal instead.\n\nContinue reading “Raspinamp: It Really Replicates Questionable Activities Involving Llamas” →\n\nWith AI being all the rage at the moment it’s been somewhat annoying that using a large language model (LLM) without significant amounts of computing power meant surrendering to an online service run by a large company. But as happens with every technological innovation the state of the art has moved on, now to such an extent that a computer as small as a Raspberry Pi can join the fun. [Nick Bild] has one running on a Pi 4, and he’s gone further than just a chatbot by making into a voice assistant.\n\nThe brains of the operation is a Tinyllama LLM, packaged as a llamafile, which is to say an executable that provides about as easy a one-step access to a local LLM as it’s currently possible to get. The whisper voice recognition sytem provides a text transcript of the input prompt, while the eSpeak speech synthesizer creates a voice output for the result. There’s a brief demo video we’ve placed below the break, which shows it working, albeit slowly.\n\nPerhaps the most important part of this project is that it’s easy to install and he’s provided full instructions in a GitHub repository. We know that the quality and speed of these models on commodity single board computers will only increase with time, so we’d rate this as an important step towards really good and cheap local LLMs. It may however be a while before it can help you make breakfast.\n\nContinue reading “A Straightforward AI Voice Assistant, On A Pi” →\n\nWe all have a folder full of images whose filenames resemble line noise. How about renaming those images with the help of a local LLM (large language model) executable on the command line? All that and more is showcased on [Justine Tunney]’s bash one-liners for LLMs, a showcase aimed at giving folks ideas and guidance on using a local (and private) LLM to do actual, useful work.\n\nThis is built out from the recent llamafile project, which turns LLMs into single-file executables. This not only makes them more portable and easier to distribute, but the executables are perfectly capable of being called from the command line and sending to standard output like any other UNIX tool. It’s simpler to version control the embedded LLM weights (and therefore their behavior) when it’s all part of the same file as well.\n\nOne such tool (the multi-modal LLaVA) is capable of interpreting image content. As an example, we can point it to a local image of the Jolly Wrencher logo using the following command:\n\nllava-v1.5-7b-q4-main.llamafile --image logo.jpg --temp 0 -e -p '### User: The image has...\\n### Assistant:'\n\nWhich produces the following response:\n\nThe image has a black background with a white skull and crossbones symbol.\n\nWith a different prompt (“What do you see?” instead of “The image has…”) the LLM even picks out the wrenches, but one can already see that the right pieces exist to do some useful work.\n\nCheck out [Justine]’s rename-pictures.sh script, which cleverly evaluates image filenames. If an image’s given filename already looks like readable English (also a job for a local LLM) the image is left alone. Otherwise, the picture is fed to an LLM whose output guides the generation of a new short and descriptive English filename in lowercase, with underscores for spaces.\n\nWhat about the fact that LLM output isn’t entirely predictable? That’s easy to deal with. [Justine] suggests always calling these tools with the --temp 0 parameter. Setting the temperature to zero makes the model deterministic, ensuring that a same input always yields the same output.\n\nThere’s more neat examples on the Bash One-Liners for LLMs that demonstrate different ways to use a local LLM that lives in a single-file executable, so be sure to give it a look and see if you get any new ideas. After all, we have previously shown how automating tasks is almost always worth the time invested.\n\nLLMs (Large Language Models) for local use are usually distributed as a set of weights in a multi-gigabyte file. These cannot be directly used on their own, which generally makes them harder to distribute and run compared to other software. A given model can also have undergone changes and tweaks, leading to different results if different versions are used.\n\nTo help with that, Mozilla’s innovation group have released llamafile, an open source method of turning a set of weights into a single binary that runs on six different OSes (macOS, Windows, Linux, FreeBSD, OpenBSD, and NetBSD) without needing to be installed. This makes it dramatically easier to distribute and run LLMs, as well as ensuring that a particular version of LLM remains consistent and reproducible, forever.\n\nThis wouldn’t be possible without the work of [Justine Tunney], creator of Cosmopolitan, a build-once-run-anywhere framework. The other main part is llama.cpp, and we’ve covered why it is such a big deal when it comes to running self-hosted LLMs.\n\nThere are some sample binaries available using the Mistral-7B, WizardCoder-Python-13B, and LLaVA 1.5 LLMs. Just keep in mind that if you’re on a Windows platform, only the LLaVA 1.5 will run, because it’s the only one that squeaks under the 4 GB limit on executable files that Windows has. If you run into issues, check out the gotchas list for troubleshooting tips.\n\nThe world of AI is abuzz, or at least parts of it are, at the news of Meta’s release of Llama 2. This is an AI text model which is thought to surpass ChatGPT in capabilities, and which the social media turned VR turned own all your things company wants you to know is open to all. That’s right, the code is open source and you can download the model, and Meta want you to feel warm and fuzzy about it. Unfortunately all is not as it seems, because of course the model isn’t open-source and is subject to a licensing restriction which makes it definitely not free of charge for larger users. This is of course disappointing to anyone hoping for an AI chatbot without restrictions, but we’re guessing Meta would prefer not to inadvertently enable a competitor.\n\nHappily for the open source user large or small who isn’t afraid of a little work there’s an alternative in the form of OpenLLaMA, but we understand that won’t be for all users. Whichever LLM you use though, please don’t make the mistake of imagining that it possesses actual intelligence.\n\nThanks to the CoupledAI team for the tip!\n\nYou might have heard about LLaMa or maybe you haven’t. Either way, what’s the big deal? It’s just some AI thing. In a nutshell, LLaMa is important because it allows you to run large language models (LLM) like GPT-3 on commodity hardware. In many ways, this is a bit like Stable Diffusion, which similarly allowed normal folks to run image generation models on their own hardware with access to the underlying source code. We’ve discussed why Stable Diffusion matters and even talked about how it works.\n\nLLaMa is a transformer language model from Facebook/Meta research, which is a collection of large models from 7 billion to 65 billion parameters trained on publicly available datasets. Their research paper showed that the 13B version outperformed GPT-3 in most benchmarks and LLama-65B is right up there with the best of them. LLaMa was unique as inference could be run on a single GPU due to some optimizations made to the transformer itself and the model being about 10x smaller. While Meta recommended that users have at least 10 GB of VRAM to run inference on the larger models, that’s a huge step from the 80 GB A100 cards that often run these models.\n\nWhile this was an important step forward for the research community, it became a huge one for the hacker community when [Georgi Gerganov] rolled in. He released llama.cpp on GitHub, which runs the inference of a LLaMa model with 4-bit quantization. His code was focused on running LLaMa-7B on your Macbook, but we’ve seen versions running on smartphones and Raspberry Pis. There’s even a version written in Rust! A rough rule of thumb is anything with more than 4 GB of RAM can run LLaMa. Model weights are available through Meta with some rather strict terms, but they’ve been leaked online and can be found even in a pull request on the GitHub repo itself. Continue reading “Why LLaMa Is A Big Deal” →"
    }
}
{
    "id": "correct_birth_00132_2",
    "rank": 72,
    "data": {
        "url": "https://ecai2023.eu/acceptedpapers",
        "read_more_link": "",
        "language": "en",
        "title": "26th European Conference on Artificial Intelligence ECAI 2023",
        "top_image": "",
        "meta_img": "",
        "images": [
            "https://ecai2023.eu/conf-data/ecai2023/images/Slajd2%20(1).PNG",
            "https://ecai2023.eu/conf-data/ecai2023/images/Slajd2%20(1).PNG",
            "https://ecai2023.eu/conf-data/qm2021/images/LOGO_poziom_C100M55blue_A5.jpg",
            "https://ecai2023.eu/conf-data/ecai2023/images/POL_Jagiellonian_University_logo.png",
            "https://ecai2023.eu/conf-data/ecai2023/images/agh_znk_wbr_rgb_150ppi.jpg",
            "https://ecai2023.eu/conf-data/ecai2023/images/KPT_logo_ENG_3_linijki.jpg",
            "https://ecai2023.eu/conf-data/ecai2023/images/Sponsors/logo_RGB.png",
            "https://ecai2023.eu/conf-data/ecai2023/images/Sponsors/%C5%82%C4%85czone%20logo.png",
            "https://ecai2023.eu/conf-data/ecai2023/images/Sponsors/ncbr_ideas_MC%20(1).png",
            "https://ecai2023.eu/conf-data/ecai2023/images/pega_logo_horizontal_positive_rgb.png",
            "https://ecai2023.eu/conf-data/ecai2023/images/logo_transparent-bg.png",
            "https://ecai2023.eu/conf-data/ecai2023/images/Logo%20HPE%20&%20NVIDIA%20black%20letters.png",
            "https://ecai2023.eu/conf-data/ecai2023/images/belka_adcp_alexa_ring.png",
            "https://ecai2023.eu/conf-data/ecai2023/images/aptiv_logo_color_rgb.jpg",
            "https://ecai2023.eu/conf-data/ecai2023/images/Sponsors/%C5%81ukasiewicz%20-%20Instytut%20Technik%20Innowacyjnych%20EMAG_dop_pelna.png",
            "https://ecai2023.eu/conf-data/ecai2023/images/Logo%20I%20OCB%20I%20kolor%20I%20transparent.png",
            "https://ecai2023.eu/conf-data/ecai2023/images/aioai-z-napisem.png",
            "https://ecai2023.eu/conf-data/ecai2023/images/Techie's%20Space%20_logo.png",
            "https://ecai2023.eu/conf-data/ecai2023/images/wai_logo_black.png",
            "https://ecai2023.eu/conf-data/ecai2023/images/Ulotka.png",
            "https://ecai2023.eu/conf-data/ecai2023/images/plakat%20(1).png"
        ],
        "movies": [],
        "keywords": [],
        "meta_keywords": [
            ""
        ],
        "tags": null,
        "authors": [],
        "publish_date": null,
        "summary": "",
        "meta_description": "",
        "meta_lang": "en",
        "meta_favicon": "",
        "meta_site_name": "",
        "canonical_link": null,
        "text": "Accepted Papers\n\n19 Failure Handling in BDI Plans via Runtime Enforcement\n\nAuthors: Angelo Ferrando; Rafael C. Cardoso\n\n##MORE##Engineering a software system can be a complex process and prone to failure. This is exacerbated when the system under consideration presents some degree of autonomy, such as in cognitive agents. In this paper, we use runtime verification as a way to enforce safety properties on Belief-Desire-Intention (BDI) agents by enveloping certain plans in safety shields. These shields function as a failure handling mechanism, they can detect and avoid violations in shielded plans. The safety shields also provide automated failure recovery by attempting alternative execution paths to avoid violations.\n\n22 Instance-aware Diffusion Implicit Process for Box-based Instance Segmentation\n\nAuthors: Hao Ren ; Xingson Liu ; Junjian Huang ; Ru Wan ; Jian Pu ; Hong Lu\n\n##MORE##The diffusion model has demonstrated impressive performance in image generation, but its potential for discriminative tasks such as instance segmentation remains unexplored. In this paper, we propose an Instance-aware Diffusion Implicit Process (IDIP) framework for instance segmentation based on boxes. During training, IDIP diffuses ground-truth boxes across various time steps, extracting corresponding Region of Interest (RoI) features. Dynamic convolution is then used to predict boxes and categories for each RoI, and the mask head generates masks from these predictions. During inference, IDIP iteratively refines randomly generated boxes with the denoising diffusion implicit model, while the mask head derives final masks from RoIs based on the refined boxes. Our method surpasses existing approaches on the COCO benchmark, requiring fewer training steps and less memory resources due to its dynamic design and instance-aware characteristic.\n\n30 Stackelberg Attacks on Auctions and Blockchain Transaction Fee Mechanisms\n\nAuthors: Nikolaj I Schwartzbach ; Daji Landis\n\n##MORE##We study a multi-unit single-demand auction in a setting where agents can arbitrarily commit to strategies that may depend on the commitments of other agents. Such commitments non-trivially change the equilibria of the auction by inducing a metagame, in which agents commit to strategies. We demonstrate a strategy an attacker may commit to that ensures they receive one such item for free, while forcing the remaining agents to enter a lottery for the remaining items. The attack is detrimental to the auctioneer, who loses most of their revenue. We show that the strategy works as long as the agents have valuations that are somewhat concentrated. The attack is robust to a large fraction of the agents being either oblivious to the attack or having exceptionally high valuations. The attacker may coerce these agents into cooperating by promising them a free item. We show that the conditions for the attack to work hold with high probability when (1) the auction is not too congested, and (2) the valuations are sampled i.i.d. from either a uniform distribution or a Pareto distribution. The attack works for first-price auctions, second-price auctions, and the transaction fee mechanism EIP-1559 used by Ethereum.\n\n45 Invisible Backdoor Attacks Using Data Poisoning in Frequency Domain\n\nAuthors: Chang Yue ; Lv Peizhuo ; Ruigang Liang ; Kai Chen\n\n##MORE##Backdoor attacks have become a significant threat to deep neural networks (DNNs), whereby poisoned models perform well on benign samples but produce incorrect outputs when given specific inputs with a trigger. These attacks are usually implemented through data poisoning by injecting poisoned samples (samples patched with a trigger and mislabelled to the target label) into the dataset, and the models trained with that dataset will be infected with the backdoor. However, most current backdoor attacks lack stealthiness and robustness because of the fixed trigger patterns and mislabelling, which can be easily detected by humans or some backdoor defense methods. To address this issue, we propose a frequency-domain-based backdoor attack method that implements backdoor implantation without mislabeling the poisoned samples or accessing the training process. We evaluated our approach on three benchmark datasets and two popular scenarios, including no-label self-supervised learning and clean-label supervised learning. Our approach achieved a high attack success rate (above 90%) on all tasks without significant performance degradation on main tasks. Furthermore, our experiments demonstrate that our attack is highly robust against mainstream defense approaches.\n\n47 Enhanced Machine Reading Comprehension Method for Aspect Sentiment Quadruplet Extraction\n\nAuthors: Shuqin Ye ; Zepeng Zhai ; Ruifan Li\n\n##MORE##In the NLP domain, Aspect-Based Sentiment Analysis (ABSA) has gained significant attention in recent years due to its ability to perform fine-grained sentiment analysis. A challenging task in ABSA is Aspect Sentiment Quadruplet Extraction (ASQE), which involves the extraction of aspect terms and their associated opinion terms, sentiment polarities, and categories in the form of quadruplets. However, existing studies have ignored the strong dependence among the multiple subtasks involved in ASQE. In this paper, we propose a novel \\textit{Enhanced Machine Reading Comprehension} (EMRC) method and formalize ASQE task as a multi-turn MRC task. Our EMRC effectively learns and utilizes the relationships among different subtasks by incorporating previously generated query answers into the current queries. We design a hierarchical category classification strategy to perform the category prediction in a structured manner, enabling the model to tackle intricate categories with ease. Furthermore, we employ the bi-directional attention mechanism, i.e., context-to-query and query-to-context attentions, to map the context into a task-aware representation. We conduct extensive experiments on two benchmark datasets. The results demonstrate that EMRC outperforms the state-of-art baselines.\n\n52 Uncertainty-driven Trajectory Truncation for Data Augmentation in Offline Reinforcement Learning\n\nAuthors: Junjie Zhang ; Jiafei Lyu ; Xiaoteng Ma ; Jiangpeng Yan ; Jun Yang ; Le Wan ; Xiu Li\n\n##MORE##Equipped with the trained environmental dynamics, model-based offline reinforcement learning (RL) algorithms can often successfully learn good policies from fixed-sized datasets, even some datasets with poor quality. Unfortunately, however, it can not be guaranteed that the generated samples from the trained dynamics model are reliable (e.g., some synthetic samples may lie outside of the support region of the static dataset). To address this issue, we propose Trajectory Truncation with Uncertainty (TATU), which adaptively truncates the synthetic trajectory if the accumulated uncertainty along the trajectory is too large. We theoretically show the performance bound of TATU to justify its benefits. To empirically show the advantages of TATU, we first combine it with two classical model-based offline RL algorithms, MOPO and COMBO. Furthermore, we integrate TATU with several off-the-shelf model-free offline RL algorithms, e.g., BCQ. Experimental results on the D4RL benchmark show that TATU significantly improves their performance, often by a large margin.\n\n58 Time-Series Data Imputation via Realistic Masking-Guided Tri-Attention BiGRU\n\nAuthors: ZhiPeng Zhang ; Yiqun Zhang ; An Zeng ; Dan Pan ; Yuzhu Ji ; Zhipeng Zhang ; Jing Lin\n\n##MORE##Time series data are ubiquitous in real-world accompanied with missing values due to various collection faults. Thus Time-Series Data Imputation (TSDI) problem is crucial to many temporal data analysis tasks. Existing missing data imputation methods usually consider only one of the following issues: (1) temporal dependencies in each time series, and (2) inter-feature correlation. Moreover, to the best of our knowledge, existing approaches that mask the data values for self-supervised TSDI model training use random masking only, which is far from the real complex missing situations. To achieve advanced performance on TDSI, we design a novel imputation model that delicately preserves the short-term, long-term, and inter-feature dependencies by attention mechanisms in a delay error-reduced bidirectional architecture. That is, it leverages GRU recurrent neural networks to model short-term temporal dependencies, and adopts Self-Attention (SA) mechanisms hierarchically to capture long-term temporal dependencies and inter-feature correlations. The multiple SAs are nested in a bidirectional structure to alleviate the problem of delay errors in RNN-like structures. To facilitate a model training with higher generality w.r.t. TSDI, a masking strategy that mimics various more extreme real missing situations beyond the simple random ones has been adopted for generating self-supervised tasks. Comprehensive experiments on real datasets demonstrates that our proposed approach outperforms the state-of-the-art methods in various data missing scenarios, and boosts the performance of several main downstream time series data analysis tasks, i.e. , clustering, classification, and ranking.\n\n75 Structure-Aware Group Discrimination with Adaptive-View Graph Encoder: A Fast Graph Contrastive Learning Framework\n\nAuthors: Zhenshuo Zhang ; Yun Zhu ; Haizhou Shi ; Siliang Tang\n\n##MORE##Albeit having gained significant progress lately, large-scale graph representation learning remains expensive to train and deploy for two main reasons: (i) the repetitive computation of multi-hop message passing and non-linearity in graph neural networks (GNNs); (ii) the computational cost of complex pairwise contrastive learning loss. Two main contributions are made in this paper targeting this twofold challenge: we first propose an adaptive-view graph neural encoder (AVGE) with a limited number of message passing to accelerate the forward pass computation, and then we propose a structure-aware group discrimination (SAGD) loss in our framework which avoids inefficient pairwise loss computing in most common GCL and improves the performance of the simple group discrimination. By the framework proposed, we manage to bring down the training and inference cost on various large-scale datasets by a significant margin (250x faster inference time) without loss of the downstream-task performance.\n\n78 On the Robustness of Split Learning against Adversarial Attacks\n\nAuthors: Mingyuan Fan ; Cen Chen ; Chengyu Wang ; Wenmeng Zhou ; Jun Huang\n\n##MORE##Split learning enables collaborative deep learning model training while preserving data privacy and model security by avoiding direct sharing of raw data and model details (i.e., sever and clients only hold partial sub-networks and exchange intermediate computations). However, existing research has mainly focused on examining its reliability for privacy protection, with little investigation into model security. Specifically, by exploring full models, attackers can launch adversarial attacks, and split learning can mitigate this severe threat by only disclosing part of models to untrusted servers. This paper aims to evaluate the robustness of split learning against adversarial attacks, particularly in the most challenging setting where untrusted servers only have access to the intermediate layers of the model. Existing adversarial attacks mostly focus on the centralized setting instead of the collaborative setting, thus, to better evaluate the robustness of split learning, we develop a tailored attack called SPADV, which comprises two stages: 1) shadow model training that addresses the issue of lacking part of the model and 2) local adversarial attack that produces adversarial examples to evaluate. The first stage only requires a few unlabeled non-IID data, and, in the second stage, SPADV perturbs the intermediate output of natural samples to craft the adversarial ones. The overall cost of the proposed attack process is relatively low, yet the empirical attack effectiveness is significantly high, demonstrating the surprising vulnerability of split learning to adversarial attacks.\n\n88 On Quantified Observability Analysis in Multiagent Systems\n\nAuthors: Chunyan Mu ; Jun Pang\n\n##MORE##In multiagent systems (MASs), agents’ observation upon system behaviours may improve the overall team performance, but may also leak sensitive information to an observer. A quantified observability analysis can thus be useful to assist decision-making in MASs by operators seeking to optimise the relationship between performance effectiveness and information exposure through the observation in practice. This paper presents a novel approach to quantitatively analysing the observability properties in MASs. The concept of opacity is applied to formally express the characterisation of observability in MASs modelled as partially observable multiagent systems (POMASs). We propose a temporal logic oPATL to reason about agents’ observability with quantitative goals, which capture the probability of information transparency of system behaviours to an observer, and develop verification techniques for quantitatively analysing such properties.We also implement the approach as an extension of the probabilistic model checker PRISM, and illustrate its applicability via several examples.\n\n89 Verifying Belief-based Programs via Symbolic Dynamic Programming\n\nAuthors: Daxin Liu ; Qinfei Huang ; Vaishak Belle ; Gerhard Lakemeyer\n\n##MORE##Belief-based programming is a probabilistic extension of the Golog programming language family, where every action and sensing could be noisy and every test refers to the subjective beliefs of the agent. Such characteristics make it rather suitable for robot control in a partial-observable uncertain environment. Recently, efforts have been made in providing formal semantics for belief programs and investigating the hardness of verifying belief programs. Nevertheless, a general algorithm that actually conducts the verification is missing. In this paper, we propose an algorithm based on symbolic dynamic programming to verify belief programs, an approach that generalizes the dynamic programming technique for solving (partially observable) Markov decision processes, i.e. (PO)MDP, by exploiting the symbolic structure in the solution of first-order (PO)MDPs induced by belief program execution.\n\n98 On Explaining Agent Behaviour via Root Cause Analysis: A Formal Account Grounded in Theory of Mind\n\nAuthors: Shakil M Khan ; Maryam Rostamigiv\n\n##MORE##Inspired by a novel action-theoretic formalization of actual cause, Khan and Lespérance (2021) recently proposed a first account of causal knowledge that supports epistemic effects, models causal knowledge dynamics, and allows sensing actions to be causes of observed effects. To date, no other study has looked specifically at these issues. But their formalization is not sufficiently expressive enough to model explanations via causal analysis of mental states as it ignores a crucial aspect of theory of mind, namely motivations. In this paper, we build on their work to support causal reasoning about conative effects. In our framework, one can reason about causes of motivational states, and we allow motivation-altering actions to be causes of observed effects. We illustrate that this formalization along with a model of goal recognition can be utilized to explain agent behaviour in communicative multiagent contexts.\n\n99 GAN-generated Faces Detection: A Survey and New Perspectives\n\nAuthors: Xin Wang ; Hui Guo ; Shu Hu ; Ming-Ching Chang ; Siwei Lyu\n\n##MORE##Generative Adversarial Networks (GAN) have led to the generation of very realistic face images, which have been used in fake social media accounts and other disinformation matters that can generate profound impacts. Therefore, the corresponding GAN-face detection techniques are under active development that can examine and expose such fake faces. In this work, we aim to provide a comprehensive review of recent progress in GAN-face detection. We focus on methods that can detect face images that are generated or synthesized from GAN models. We classify the existing detection works into four categories: (1) deep learning-based, (2) physical-based, (3) physiological-based methods, and (4) evaluation and comparison against human visual performance. For each category, we summarize the key ideas and connect them with method implementations. We also discuss open problems and suggest future research directions.\n\n104 XFLT : Exploring Techniques for Generating Cross Lingual Factually Grounded Long Text\n\nAuthors: Bhavyajeet Singh ; Aditya Hari ; Rahul Mehta ; Tushar Abhishek ; Manish Gupta ; Vasudeva Varma\n\n##MORE##Multiple business scenarios require an automated generation of descriptive human-readable long text from structured input data, where the source is typically a high-resource language and the target is a low or medium resource language. We define the Cross-Lingual Fact to Long Text Generation (XFLT) as a novel natural language generation (NLG) task that involves generating descriptive and human-readable long text in a target language from structured input data (such as fact triples) in a source language. XFLT is challenging because of (a) hallucinatory nature of the state-of-the-art NLG models, (b) lack of good quality training data, and (c) lack of a suitable cross-lingual NLG metric. Unfortunately previous work focuses on different related problem settings (cross-lingual facts to short text or monolingual graph to text) and has made no efforts to handle hallucinations. In this paper, we propose a novel solution to the XFLT task which addresses these challenges by training multilingual Transformer-based encoder-decoder models with coverage prompts and grounded decoding. Further, it improves on the XFLT quality by defining task-specific reward functions and training on them using reinforcement learning. On a dataset with over 64,000 paragraphs across 12 different languages, we compare this novel solution with several strong baselines using a new metric, cross-lingual PARENT. We also make our code and data publicly available\n\n119 Region-Specific Prototype Customization for Weakly Supervised Semantic Segmentation\n\nAuthors: Ruiguo Yu ; Yihang Zhao ; Mei Yu ; Jie Gao ; Chenhan Wang ; Ruixuan Zhang ; Xuewei Li\n\n##MORE##It is well known that weakly supervised semantic segmentation requires only image-level labels for training, which greatly reduces the annotation cost. In recent years, prototype-based approaches, which prove to substantially improve the segmentation performance, have been favored by a wide range of researchers. However, we are surprised to find that there are semantic gaps between different regions within the same object, hindering the optimization of prototypes, so the traditional prototypes can not adequately represent the entire object. Therefore, we propose region-specific prototypes to adaptively describe the regions themselves, which alleviate the effect of semantic gap by separately obtaining prototypes for different regions of an object. In addition, to obtain more representative region-specific prototypes, a plug-and-play Spatially Fused Attention Module is proposed for combining the spatial correlation and the scale correlation of hierarchical features. Extensive experiments are conducted on PASCAL VOC 2012 and MS COCO 2014, and the results show that our method achieves state-of-the-art performance using only image-level labels.\n\n130 A Paraconsistency Framework for Inconsistency Handling in Qualitative Spatial and Temporal Reasoning\n\nAuthors: Yakoub Salhi ; Michael Sioutis\n\n##MORE##Inconsistency handling is a fundamental problem in knowledge representation and reasoning. In this paper, we study this problem in the context of qualitative spatio-temporal reasoning, a framework for reasoning about space and time in a symbolic, human-like manner, by following an approach similar to that used for defining paraconsistent logics; paraconsistency allows deriving informative conclusions from inconsistent knowledge bases by mainly avoiding the principle of explosion. Inspired by paraconsistent logics, such as Priest’s logic LPm, we introduce the notion of paraconsistent scenario (i.e., a qualitative solution), which can be seen as a scenario that allows a conjunction of base relations between two variables, e.g., x precedes ∧ follows y. Further, we present several interesting theoretical properties that concern paraconsistent scenarios, including computational complexity results, and describe two distinct approaches for computing paraconsistent scenarios and solving other related problems. Moreover, we provide implementations of our two methods for computing paraconsistent scenarios and experimentally evaluate them using different strategies/metrics. Finally, we show that our paraconsistent scenario notion allows us to adapt to qualitative reasoning one of the well-known inconsistency measures employed in the propositional case, namely, contension measure.\n\n136 Model-based Reinforcement Learning with Multi-step Plan Value Estimation\n\nAuthors: Haoxin Lin ; Yihao Sun ; Jiaji Zhang ; Yang Yu\n\n##MORE##A promising way to improve the sample efficiency of reinforcement learning is model-based methods, in which many explorations and evaluations can happen in the learned models to save real-world samples. However, when the learned model has a non-negligible model error, sequential steps in the model are hard to be accurately evaluated, limiting the model’s utilization. This paper proposes to alleviate this issue by introducing multi-step plans into policy optimization for model-based RL. We employ the multi-step plan value estimation, which evaluates the expected discounted return after executing a sequence of action plans at a given state, and updates the policy by directly computing the multi-step policy gradient via plan value estimation. The new model-based reinforcement learning algorithm MPPVE (Model-based Planning Policy Learning with Multi-step Plan Value Estimation) shows a better utilization of the learned model and achieves a better sample efficiency than state-of-the-art model-based RL approaches. The code is available at https://github.com/HxLyn3/MPPVE.\n\n140 Mitigating Long-tail Language Representation Collapsing via Cross-lingual Bootstrapped Unsupervised Fine-tuning\n\nAuthors: Ping Guo ; Yue Hu ; Yubing Ren ; Yunpeng Li ; Jiarui Zhang ; Xingsheng Zhang\n\n##MORE##Large Language Models have shown great capability to comprehend natural language and provide reasonable responses. However, previous research has shown weak performance of these models on low-resource (long-tail) languages. It remains to be a problem to mitigate the performance gap between long-tail languages and rich-resource ones, which is referred to as long-tail language representation collapsing. Though some previous works can generate pseudo parallel corpora with auto-regressive generation, this generation progress is time-consuming and remains low quality particularly for long-tail languages. In this paper, we propose a (X) Cross-lingual Bootstrapped Unsupervised Fine-tuning Framework (X-BUFF) to mitigate long-tail language representation collapsing. X-BUFF iteratively updates the cross-lingual PLM in a curriculum way. In each iteration of X-BUFF, we (1) select sentences with certain semantics from monolingual corpora in long-tail languages. (2) match these selected sentences with semantic equivalent sentences in many other languages to create parallel sentence pairs, which we then merge with previous sentence pairs to build a larger and more difficult bootstrapped parallel queue. (3) fine-tune the PLM with the bootstrapped parallel queue in a curriculum way. Extensive experiments show that X-BUFF can mitigate the long-tail language representation collapsing problem in cross-lingual PLMs and achieve significant improvements over the previous baseline on several cross-lingual evaluation benchmarks.\n\n145 Algorithmic Recognition of 2-Euclidean Preferences\n\nAuthors: Bruno Escoffier ; Olivier Spanjaard ; Magdalena Tydrichova\n\n##MORE##A set of voters' preferences on a set of candidates is 2-Euclidean if candidates and voters can be mapped to the plane so that the preferences of each voter decrease with the Euclidean distance between her position and the positions of candidates. Based on geometric properties, we propose a recognition algorithm, that returns either \"yes\" (together with a planar positioning of candidates and voters) if the preferences are 2-Euclidean, or \"no\" if it is able to find a concise certificate that they are not, or \"unknown\" if a time limit is reached. Our algorithm outperforms a quadratically constrained programming solver achieving the same task, both in running times and the percentage of instances it is able to recognize. In the numerical tests conducted on the PrefLib library of preferences, 91.5% (resp. 4.5%) of the available sets of complete strict orders are proven not to be (resp. to be) 2-Euclidean, and the status of only 4.5% of them could not be decided. Furthermore, for instances involving 5 (resp. 6, 7) candidates, we were able to find planar representations that are compatible with 87.4% (resp. 58.1%, 60.1%) of voters' preferences.\n\n152 Effective and Efficient Community Search with Graph Embeddings\n\nAuthors: Xiaoxuan Gou ; Xiaoliang Xu ; Xiangying Wu ; Runhuai Chen ; Yuxiang Wang ; Tianxing Wu ; Xiangyu Ke\n\n##MORE##Given a graph $G$ and a query node $q$, community search (CS) seeks a cohesive subgraph from $G$ that contains $q$. CS has gained much research interests recently. In the database research community, researchers aim to find the most cohesive subgraph satisfying a specific community model (e.g., $k$-core or $k$-truss) via graph traversal. These works obtain good precision, however suffering from the low efficiency issue. In the AI research community, a new thought of using the deep learning model to support CS without relying on graph traversal emerges. Supervised end-to-end models using GCN are presented, which perform efficiently, but leave a large room for precision improvement. None of them can achieve a good balance between the efficiency and effectiveness. This motivates our solution: First, we present an offline community-injected graph embedding method to preserve the community's cohesiveness features into the learned node representations. Second, we resort to a proximity graph (PG) built from node representations, to quickly return the community online. Moreover, we develop a self-augmented method based on KL divergence to further optimize node representations. Extensive experiments on seven real-world graphs show our solution's superiority on effectiveness (at least 39.3\\% improvement) and efficiency (one to two orders of magnitude faster).\n\n166 Theoretically Guaranteed Policy Improvement Distilled from Model-Based Planning\n\nAuthors: Chuming Li ; Ruonan Jia ; Jie Liu ; Yinmin Zhang ; Yazhe Niu ; Yaodong Yang ; Yu Liu ; Wanli Ouyang\n\n##MORE##Model-based reinforcement learning (RL) has demonstrated remarkable successes on a range of continuous control tasks due to its high sample efficiency. To save the computation cost of conducting planning online, recent practices tend to distill optimized action sequences into an RL policy during the training phase. Although the distillation can incorporate both the foresight of planning and the exploration ability of RL policies, the theoretical understanding of these methods is yet unclear. In this paper, we extend the policy improvement step of Soft Actor-Critic (SAC) by developing an approach to distill from model-based planning to the policy. We then demonstrate that such an approach of policy improvement has a theoretical guarantee of monotonic improvement and convergence to the maximum value defined in SAC. We discuss effective design choices and implement our theory as a practical algorithm---\\textit{\\textbf{M}odel-based \\textbf{P}lanning \\textbf{D}istilled to \\textbf{P}olicy (MPDP)}---that updates the policy jointly over multiple future time steps. Extensive experiments show that MPDP achieves better sample efficiency and asymptotic performance than both model-free and model-based planning algorithms on six continuous control benchmark tasks in MuJoCo.\n\n168 A Simple Debiasing Framework for Out-of-Distribution Detection in Human Action Recognition\n\nAuthors: Minho Sim ; Young Jun Lee ; Dongkun Lee ; Jongwhoa Lee ; Ho-Jin Choi\n\n##MORE##In real-world scenarios, detecting out-of-distribution (OOD) action is important when deploying a deep learning-based human action recognition (HAR) model. However, HAR models are easily biased to static information in the video (e.g., background), which can lead to performance degradation of OOD detection methods. In this paper, we propose a simple debiasing framework for out-of-distribution detection in human action recognition. Specifically, our framework eliminates patches with static bias in video using attention maps extracted from the video vision transformer model. Experimental results show that our framework achieves consistent performance improvement on multiple OOD action detection methods and challenging benchmarks. Furthermore, we introduce two new OOD action detection tasks, Kinetics-400 vs. Kinetics-600 exclusive and Kinetics-400 vs. Kinetics-700 exclusive, to validate our method in a setting close to the real-world scenario. With extensive experiments, we demonstrate the effectiveness of our attention-based masking, and in-depth analysis validates the effect of static bias on OOD action detection. The source code and supplementary materials are available at: https://github.com/Simcs/attention-masking.\n\n171 From Intermediate Representations to Explanations: Exploring Hierarchical Structures in NLP\n\nAuthors: Housam Babiker ; Mi-Young Kim ; Randy Goebel\n\n##MORE##Interpretation methods for learned models used in natural language processing (NLP) applications usually provide support for local (specific) explanations, such as quantifying the contribution of each word to the predicted class. But they typically ignore the potential interaction amongst those word tokens. Unlike currently popular methods, we propose a deep model which uses feature attribution and identification of dependencies to support the learning of interpretable representations that will support creation of hierarchical explanations. In addition, hierarchical explanations provide a basis for visualizing how words and phrases are combined at different levels of abstraction, which enables end-users to better understand the prediction process of a deep network. Our study uses multiple well-known datasets to demonstrate the effectiveness of our approach, and provides both automatic and human evaluation.\n\n175 Cardsformer: Grounding Language to Learn a Generalizable Policy in Hearthstone\n\nAuthors: Wannian Xia ; Yang Yiming ; Jingqing Ruan ; Xing Dengpeng ; Bo Xu\n\n##MORE##Hearthstone is a widely played collectible card game that challenges players to strategize using cards with various effects described in natural language. While human players can easily comprehend card descriptions and make informed decisions, artificial agents struggle to understand the game's inherent rules, let alone generalize their policies through natural language. To address this issue, we propose Cardsformer, a method capable of acquiring linguistic knowledge and learning a generalizable policy in Hearthstone. Cardsformer consists of a Prediction Model trained with offline trajectories to predict state transitions based on card descriptions and a Policy Model capable of generalizing its policy on unseen cards. To our knowledge, this is the first work to consider language knowledge in a card game. Experiments show that our approach significantly improves data efficiency and outperforms the state-of-the-art in Hearthstone even when there are untrained cards in the deck, inspiring a new perspective of tackling problems as such with knowledge representation from large language models. As the game constantly releases new cards along with new descriptions and new effects, the challenge in Hearthstone remains. To encourage further research, we will make our code publicly available and publish PyStone, the code base of Hearthstone on which we conducted our experiments, as an open benchmark.\n\n179 Dual-Scale Interest Extraction Framework with Self-Supervision for Sequential Recommendation\n\nAuthors: Liangliang Chen ; Hongzhan Lin ; Jinshan Ma ; Guang Chen\n\n##MORE##In the sequential recommendation task, the recommender generally learns multiple embeddings from a user's historical behaviors, to catch the diverse interests of the user. Nevertheless, the existing approaches just extract each interest independently for the corresponding sub-sequence while ignoring the global correlation of the entire interaction sequence, which may fail to capture the user's inherent preference for the potential interests generalization and unavoidably make the recommended items homogeneous with the historical behaviors. In this paper, we propose a novel Dual-Scale Interest Extraction framework (DSIE) to precisely estimate the user's current interests. Specifically, DSIE explicitly models the user's inherent preference with contrastive learning by attending over his/her entire interaction sequence at the global scale and catches the user's diverse interests in a fine granularity at the local scale. Moreover, we develop a novel interest aggregation module to integrate the multi-interests according to the inherent preference to generate the user's current interests for the next-item prediction. Experiments conducted on three real-world benchmark datasets demonstrate that DSIE outperforms the state-of-the-art models in terms of recommendation preciseness and novelty.\n\n181 Spectral Normalized-Cut Graph Partitioning with Fairness Constraints\n\nAuthors: Jia Li ; Yanhao Wang ; Arpit Merchant\n\n##MORE##Normalized-cut graph partitioning aims to divide the set of nodes in a graph into k disjoint clusters to minimize the fraction of the total edges between any cluster and all other clusters. In this paper, we consider a fair variant of the partitioning problem wherein nodes are characterized by a categorical sensitive attribute (e.g., gender or race) indicating membership to different demographic groups. Our goal is to ensure that each group is approximately proportionally represented in each cluster while minimizing the normalized cut value. To resolve this problem, we propose a two-phase spectral algorithm called FNM. In the first phase, we add an augmented Lagrangian term based on our fairness criteria to the objective function for obtaining a fairer spectral node embedding. Then, in the second phase, we design a rounding scheme to produce k clusters from the fair embedding that effectively trades off fairness and partition quality. Through comprehensive experiments on nine benchmark datasets, we demonstrate the superior performance of FNM compared with three baseline methods.\n\n190 Argument Attribution Explanations in Quantitative Bipolar Argumentation Frameworks\n\nAuthors: Xiang Yin ; Nico Potyka ; Francesca Toni\n\n##MORE##Argumentative explainable AI has been advocated by several in recent years, with an increasing interest on explaining the reasoning outcomes of Argumentation Frameworks (AFs). While there is a considerable body of research on qualitatively explaining the reasoning outcomes of AFs with debates/disputes/dialogues in the spirit of extension-based semantics, explaining the quantitative reasoning outcomes of AFs under gradual semantics has not received much attention, despite widespread use in applications. In this paper, we contribute to filling this gap by proposing a novel theory of Argument Attribution Explanations (AAEs) by incorporating the spirit of feature attribution from machine learning in the context of Quantitative Bipolar Argumentation Frameworks (QBAFs): whereas feature attribution is used to determine the influence of features towards outputs of machine learning models, AAEs are used to determine the influence of arguments towards topic arguments of interest. We study desirable properties of AAEs, including some new ones and some partially adapted from the literature to our setting. To demonstrate the applicability of our AAEs in practice, we conclude by carrying out two case studies in the scenarios of fake news detection and movie recommender systems.\n\n197 PLEASE: Generating Personalized Explanations in Human-Aware Planning\n\nAuthors: Stylianos Loukas Vasileiou ; William Yeoh\n\n##MORE##Model Reconciliation Problems (MRPs) and their variant, Logic-based MRPs (L-MRPs), have emerged as popular methods for explainable planning problems. Both MRP and L-MRP approaches assume that the explaining agent has access to an assumed model of the human user receiving the explanation, and it reconciles its own model with the human model to find the differences such that when they are provided as explanations to the human, they will understand them. However, in practical applications, the agent is likely to be fairly uncertain on the actual model of the human and wrong assumptions can lead to incoherent or unintelligible explanations. In this paper, we propose a less stringent requirement: the agent has access to a task-specific vocabulary known by the human and, if available, a human model capturing confidently known information. Our goal is to find a personalized explanation, which is an explanation that is at an appropriate abstraction level with respect to the human's vocabulary and model. Using a logic-based method called knowledge forgetting for generating abstractions, we propose a simple framework compatible with L-MRP approaches, and evaluate its efficacy through computational and human user experiments.\n\n198 Anytime Index-Based Search Method for Large-Scale Simultaneous Coalition Structure Generation and Assignment\n\nAuthors: Redha Taguelmimt ; Samir Aknine ; Djamila Boukredera ; Narayan Mr Changder\n\n##MORE##Organizing agents into disjoint groups is a crucial challenge in artificial intelligence, with many applications where quick runtime is essential. The Simultaneous Coalition Structure Generation and Assignment (SCSGA) problem involves partitioning a set of agents into coalitions and assigning each coalition to a task, with the goal of maximizing social welfare. However, this is an NP-complete problem, and only a few algorithms have been proposed to address it for both small and large-scale problems. In this paper, we address this challenge by presenting a novel algorithm that can efficiently solve both small and large instances of this problem. Our method is based on a new search space representation, where each coalition is codified by an index. We have developed an algorithm that can explore this solution space effectively by generating index vectors that represent coalition structures. The resulting algorithm is anytime and can scale to large problems with hundreds or thousands of agents. We evaluated our algorithm on a range of value distributions and compared its performance against state-of-the-art algorithms. Our experimental results demonstrate that our algorithm outperforms existing methods in solving the SCSGA problem, providing high-quality solutions for a wide range of problem instances.\n\n202 Synthesis of Procedural Models for Deterministic Transition Systems\n\nAuthors: Javier Segovia-Aguas ; Jonathan Ferrer Mestres ; Sergio Jiménez Celorrio\n\n##MORE##This paper introduces a general approach for synthesizing procedural models of the {\\em state-transitions} of a given {\\em discrete system}.\n\nThe approach is general in that it accepts different {\\em target languages} for modeling the state-transitions of a discrete system; different model acquisition tasks with different target languages, such as the synthesis of \\strips{} action models, or the update rule of a {\\em cellular automaton}, fit as particular instances of our general approach. We follow an inductive approach to synthesis meaning that a set of examples of state-transitions, represented as {\\em (pre-state, action, post-state)} tuples, are given as input. The goal is to synthesize a structured program that, when executed on a pre-state, outputs the associated post-state. Our synthesis method implements a combinatorial search in the space of well-structured terminating programs that can be built using a {\\em Random-Access Machine} (RAM), with a minimalist instruction set, and a finite amount of memory. The combinatorial search is guided with functions that asses the computational complexity of the candidate programs, as well as their fitness to the input set of examples.\n\n203 Individual Fairness Guarantee in Learning with Censorship\n\nAuthors: Wenbin Zhang ; Juyong Kim ; Zichong Wang ; Cheng Cheng ; Thomas Oommen ; Pradeep Ravikumar ; Jeremy C Weiss\n\n##MORE##Algorithmic fairness, the research field of making machine learning (ML) algorithms fair, is an established area in ML. As ML technologies expand their application domains, including ones with high societal impact, it becomes essential to take fairness into consideration during the building of ML systems. Yet, despite its wide range of socially sensitive applications, most work treats the issue of algorithmic bias as an intrinsic property of supervised learning, i.e. the class label is given as a precondition. Unlike prior studies in fairness, we propose an individual fairness measure and a corresponding algorithm that deal with censorship where there is uncertainty in class labels, while enforcing similar individuals to be treated similarly from a ranking perspective, free of the Lipchitz condition in the conventional individual fairness definition. We argue that this perspective represents a more realistic model of fairness research for real-world application deployment and show how learning with such a relaxed precondition draws new insights that better explains algorithmic fairness. We conducted experiments on four real-world datasets to evaluate our proposed method compared to other fairness models, demonstrating its superiority in minimizing discrimination while maintaining predictive performance with the presence of censorship.\n\n205 BiERL: A Meta Evolutionary Reinforcement Learning Framework via Bilevel Optimization\n\nAuthors: Junyi Wang ; Yuanyang Zhu ; Zhi Wang ; Yan Zheng ; Jianye Hao ; Chunlin Chen\n\n##MORE##Evolutionary reinforcement learning (ERL) algorithms recently raise attention in tackling complex reinforcement learning (RL) problems due to high parallelism, while they are prone to insufficient exploration or model collapse without carefully tuning hyperparameters (aka meta-parameters). In the paper, we propose a general meta ERL framework via bilevel optimization (BiERL) to jointly update hyperparameters in parallel to training the ERL model within a single agent, which relieves the need for prior domain knowledge or costly optimization procedure before model deployment. We design an elegant meta-level architecture that embeds the inner-level's evolving experience into an informative population representation, and we introduce a simple and feasible evaluation of the meta-level fitness function to facilitate learning efficiency. We perform extensive experiments in MuJoCo and Box2D domains to verify that as a general framework, BiERL outperforms various baselines and consistently improves the learning performance for a diversity of ERL algorithms.\n\n207 Exploration and Exploitation in Hierarchical Reinforcement Learning with Adaptive Scheduling\n\nAuthors: ZhiGang Huang ; Quan Liu\n\n##MORE##In hierarchical reinforcement learning (HRL), continuous options provide a knowledge carrier that is more aligned with human behavior, but reliable scheduling methods are not yet available. To design an available scheduling method for continuous options, in this paper, the hierarchical reinforcement learning with adaptive scheduling (HAS) algorithm is proposed. It focuses on achieving an adaptive balance between exploration and exploitation during the frequent scheduling of continuous options. It builds on multi-step static scheduling and makes switching decisions according to the relative advantages of the previous and the estimated options, enabling the agent to focus on different behaviors at different phases. The expected $t$-step distance is applied to demonstrate the superiority of adaptive scheduling in terms of exploration. Furthermore, an interruption incentive based on annealing is proposed to alleviate excessive exploration, accelerating the convergence rate. We develop a comprehensive experimental analysis scheme. The experimental results demonstrate the high performance and robustness of HAS. Furthermore, it provides evidence that the adaptive scheduling method has a positive effect both on the representation and option policies.\n\n208 Create and Find Flatness: Building Flat Training Spaces in Advance for Continual Learning\n\nAuthors: WenHang Shi ; Yiren Chen ; Zhe Zhao ; Wei Lu ; Kimmo Yan ; Xiaoyong Du\n\n##MORE##Catastrophic forgetting remains a critical challenge in the field of continual learning, where neural networks struggle to retain prior knowledge while assimilating new information. Most existing studies emphasize mitigating this issue only when encountering new tasks, overlooking the significance of the pre-task phase. Therefore, we shift the attention to the current task learning stage, presenting a novel framework, C&F (Create and Find Flatness), which builds a flat training space for each task in advance. Specifically, during the learning of the current task, our framework adaptively creates a flat region around the minimum in the the loss landscape. Subsequently, it finds the parameters' importance to the current task based on their flatness degrees. When adapting the model to a new task, constraints are applied according to the flatness and a flat space is simultaneously prepared for the impending task. We theoretically demonstrate the consistency between created and found flatness. In this manner, our framework not only accommodates ample parameter space for learning new tasks but also preserves the preceding knowledge of earlier tasks. Experimental results exhibit C&F's state-of-the-art performance as a standalone continual learning approach and its efficacy as a framework incorporating other methods.\n\n210 LoSS: Local Structural Separation Hypergraph Convolutional Neural Network\n\nAuthors: Bingde Hu ; Yang Gao ; Zunlei Feng ; Mingli Song ; Xinyu Wang ; Liying Liying\n\n##MORE##Graph classification is a classic problem with practical applications in many real-life scenes. Existing graph neural networks, including GCN, GAT, and GIN, are proposed to extract useful features from complex graph structures.\n\nHowever, the features extraction and aggregation manner of most existing methods inevitably mixes the useful and redundant features, which will disturb the final classification performance. In this paper, to handle the above drawback, we put forward Local Structural Separation Hypergraph Convolutional Neural Network (LoSS) based on two discoveries: most graph classification tasks only focus on a few groups of adjacent nodes, and different categories have their specific high response bits in graph embeddings. In LoSS, we first decouple the original graph into different hypergraphs and aggregate the features in each substructure, which aims to find useful features for the final classification. Next, the low-correlation feature suppression strategy is devised to suppress the irrelevant node-level and bit-level features in the forward inference process, which can effectively reduce the disturbance of redundant features. Experiments on five datasets show that the proposed LoSS can effectively locate and aggregate useful hypergraph features and achieve SOTA performance compared with existing methods.\n\n212 Truthful and Equitable Lateral Transshipment in Multi-Retailer Systems\n\nAuthors: Garima Shakya ; Sai Koti Reddy Danda ; Swaprava Nath ; Pankaj Dayama ; Surya Shravan Kumar Sajja\n\n##MORE##We consider a multi-retailer system where the sellers are connected with each other via a transportation network and the transactions with the consumers happen on a platform. Each consumer is serviced by only one retailer. Since the demands to the sellers (i.e., the retailers on the platform) are stochastic in nature, supplies can be either in excess or in deficit. Transshipping these items laterally among the retailers benefits both, the platform and the retailers. For retailers, excess supply leads to wastage and deficit to a loss of revenue, while via transshipment, they get a better outcome. The platform can also earn some revenue in facilitating this process. However, only the sellers know their excess (which can be salvaged at a price or transshipped to another seller) or the deficit (which can be directly procured from a supplier or transshipped from another seller), both of which have multiple information that is private. We propose a model that allows the lateral transshipment at a price and design mechanisms such that the sellers are incentivized to voluntarily participate and be truthful in the lateral transshipment. Experimenting on different types of network topologies, we find that the sellers at more central locations in the network get an unfair advantage in the classical mechanism that aims for economic efficiency. We, therefore, propose a modified mechanism with tunable parameters which can ensure that the mechanism is more equitable for non-central retailers. Our synthetic data experiments show that such mechanisms do not compromise too much on efficiency, and also reduce budget imbalance.\n\n223 FATRER: Full-Attention Topic Regularizer for Accurate and Robust Conversational Emotion Recognition\n\nAuthors: Yuzhao Mao ; Di Lu ; Yang Zhang ; Xiaojie Wang\n\n##MORE##This paper concentrates on the understanding of interlocutors’ emotions evoked in conversational utterances. Previous studies on this literature mainly focus on more accurate emotional predictions, while ignoring the model robustness when the local context is corrupted by adversarial attacks. To cope with the impact from local perturbations, we propose a full-attention topic regularizer that enables a global view when modeling local context for conversational emotion recognition. A joint topic modeling strategy is introduced to enforce regularization from both representation and loss perspectives. To avoid over-regularization, we drop the constraints on prior distributions that exist in traditional topic modeling and perform full-attention-based probabilistic approximations. Experiments show that our models obtain more favorable results than the state-of-the-art models, and gain convincing robustness under three types of adversarial attacks. Code: https://github.com/ludybupt/FATRER\n\n231 A semantics-driven methodology for high-quality image annotation\n\nAuthors: Fausto Giunchiglia ; Mayukh Bagchi ; Xiaolei Diao\n\n##MORE##Recent work in Machine Learning and Computer Vision has highlighted the presence of various types of systematic flaws inside ground truth object recognition benchmark datasets. Our basic tenet is that these flaws are rooted in the many-to-many mappings which exist between the visual information encoded in images and the intended semantics of the labels annotating them. The net consequence is that the current annotation process is largely under-specified, thus leaving too much freedom to the subjective judgment of annotators. In this paper, we propose vTelos, an integrated Natural Language Processing, Knowledge Representation, and Computer Vision methodology whose main goal is to make explicit the (otherwise implicit) intended annotation semantics, thus minimizing the number and role of subjective choices. vTelos is organized around four main design choices, each focusing on a specific aspect of the annotation process. In this respect, visual properties are used to specify the relevant elements of an image and labels are associated with natural language genus-differentia definitions which describe the selected visual properties. In turn, these definitions are organized into a lexico-semantic hierarchy that captures their intrinsic recursive structure. The methodology is validated on images populating a subset of the ImageNet hierarchy.\n\n236 Towards a Rigorous Calibration Assessment Framework: Advancements in Metrics, Methods, and Use\n\nAuthors: Lorenzo Famiglini ; Andrea Campagner ; Federico Cabitza\n\n##MORE##Calibration is paramount in developing and validating Machine Learning models, particularly in sensitive domains such as medicine. Despite its significance, existing metrics to assess calibration have been found to have shortcomings in regard to their interpretation and theoretical properties. This article introduces a novel and comprehensive framework to assess the calibration of Machine and Deep Learning models that addresses the above limitations. The proposed framework is based on a modification of the Expected Calibration Error (ECE), called the Estimated Calibration Index (ECI), which grounds on and extends prior research. ECI was initially formulated for binary settings, and we adapted it to fit multiclass settings. ECI offers a more nuanced, both locally and globally, and informative measure of a model's tendency towards over/underconfidence. The paper first outlines the issues related to the prevalent definitions of ECE, including potential biases that may arise in the evaluation of their measures. Then, we present the results of a series of experiments conducted to demonstrate the effectiveness of the proposed framework in supporting a more accurate understanding of a model's calibration level. Additionally, we discuss how to address and potentially mitigate some biases in calibration assessment.\n\n238 One-class classification approach to variational learning from biased positive unlabeled data\n\nAuthors: Jan Mielniczuk ; Adam Wawrzeńczyk\n\n##MORE##We discuss Empirical Risk Minimization approach in conjunction with one-class classification method to learn classifiers for biased Positive Unlabeled (PU) data. For such data, probability that an observation from a positive class is labeled may depend on its features. The proposed method extends Variational Autoencoder for PU data (VAE-PU) introduced in Na et al (2020) by proposing another estimator of a theoretical risk of a classifier to be minimized, which has important advantages over the previous proposal. This is based on one-class classification approach using generated pseudo-observations, which turns out to be an effective method of detecting positive observations among unlabeled ones. The proposed method leads to more precise estimation of the theoretical risk than the previous proposal. Experiments performed on real data sets show that the proposed VAE-PU+OCC algorithm works very promisingly in comparison to its competitors such as the original VAE-PU, SAR-EM and LBE methods in terms of accuracy and F1 score. The advantage is especially strongly pronounced for small labeling frequencies.\n\n246 TreeFlow: Going Beyond Tree-based Parametric Probabilistic Regression\n\nAuthors: Patryk Wielopolski ; Maciej Zieba\n\n##MORE##The tree-based ensembles are known for their outstanding performance in classification and regression problems characterized by feature vectors represented by mixed-type variables from various ranges and domains. However, considering regression problems, they are primarily designed to provide deterministic responses or model the uncertainty of the output with Gaussian or parametric distribution. In this work, we introduce TreeFlow, the tree-based approach that combines the benefits of using tree ensembles with the capabilities of modeling flexible probability distributions using normalizing flows. The main idea of the solution is to use a tree-based model as a feature extractor and combine it with a conditional variant of normalizing flow. Consequently, our approach is capable of modeling complex distributions for the regression outputs. We evaluate the proposed method on challenging regression benchmarks with varying volume, feature characteristics, and target dimensionality. We obtain the SOTA results for both probabilistic and deterministic metrics on datasets with multi-modal target distributions and competitive results on unimodal ones compared to tree-based regression baselines.\n\n247 Preserving Semantics in Textual Adversarial Attacks\n\nAuthors: David Herel ; Hugo Cisneros ; Tomas Mikolov\n\n##MORE##The growth of hateful online content, or hate speech, has been associated with a global increase in violent crimes against minorities (Laub, 2019). Harmful online content can be produced easily, automatically and anonymously. Even though, some form of auto-detection is already achieved through text classifiers in NLP, they can be fooled by adversarial attacks. To strengthen existing systems and stay ahead of attackers, we need better adversarial attacks. In this paper, we show that up to 70% of adversarial examples generated by adversarial attacks should be discarded because they do not preserve semantics. We address this core weakness and propose a new, fully supervised sentence embedding technique called Semantics-Preserving-Encoder (SPE). Our method outperforms existing sentence encoders used in adversarial attacks by achieving 1.2× ∼ 5.1× better real attack success rate. We release our code as a plugin that can be used in any existing adversarial attack to improve its quality and speed up its execution.\n\n250 Settling the Score: Portioning with Cardinal Preferences\n\nAuthors: Edith Elkind ; Warut Suksompong ; Nicholas Teh\n\n##MORE##We study a portioning setting in which a public resource such as time or money is to be divided among a given set of candidates, and each agent proposes a division of the resource. We consider two families of aggregation rules for this setting - those based on coordinate-wise aggregation and those that optimize some notion of welfare - as well as the recently proposed Independent Markets mechanism. We provide a detailed analysis of these rules from an axiomatic perspective, both for classic axioms, such as strategyproofness and Pareto optimality, and for novel axioms, which aim to capture proportionality in this setting. Our results indicate that a simple rule that computes the average of all proposals satisfies many of our axioms, including some that are violated by more sophisticated rules.\n\n253 Understanding and Improving Neural Active Learning on Heteroskedastic Distributions\n\nAuthors: Savya Khosla ; Kin Whye Chew ; Jordan Ash ; Cyril Zhang ; Kenji Kawaguchi ; Alex Lamb\n\n##MORE##Models that can actively seek out the best quality training data hold the promise of more accurate, adaptable, and efficient machine learning. Active learning techniques often tend to prefer examples that are the most difficult to classify. While this works well on homogeneous datasets, we find that it can lead to catastrophic failures when performed on multiple distributions with different degrees of label noise or heteroskedasticity. These active learning algorithms strongly prefer to draw from the distribution with more noise, even if their examples have no informative structure (such as solid color images with random labels). To this end, we demonstrate the catastrophic failure of these active learning algorithms on heteroskedastic distributions and propose a fine-tuning-based approach to mitigate these failures. Further, we propose a new algorithm that incorporates a model difference scoring function for each data point to filter out the noisy examples and sample clean examples that maximize accuracy, outperforming the existing active learning techniques on the heteroskedastic datasets. We hope these observations and techniques are immediately helpful to practitioners and can help to challenge common assumptions in the design of active learning algorithms.\n\n264 Privacy-enhanced Personal Assistants based on Dialogues and Case Similarity\n\nAuthors: Xiao Zhan ; Stefan Sarkadi ; Jose Such\n\n##MORE##Personal assistants (PAs) such as Amazon Alexa, Google Assistant and Apple Siri are now widespread. However, without adequate safeguards and controls their use may lead to privacy risks and violations. In this paper, we propose a model for privacy-enhanced PAs. The model is an interpretable AI architecture that combines 1) a dialogue mechanism for understanding the user and getting online feedback from them, with 2) a decision making mechanism based on case-based reasoning considering both user and scenario similarity. We evaluate our model using real data about users' privacy preferences, and compare its accuracy and demand for user involvement with both online machine learning and other, more interpretable, AI approaches. Our results show that our proposed architecture is more accurate and requires less intervention from the users than existing approaches.\n\n269 Causal Discovery and Knowledge Injection for Contestable Neural Networks\n\nAuthors: Fabrizio Russo ; Francesca Toni\n\n##MORE##Neural networks have proven to be effective at solving machine learning tasks but it is unclear whether they learn any relevant causal relationships, while their black-box nature makes it difficult for modellers to understand and debug them. We propose a novel method overcoming these issues by allowing a two-way interaction whereby neural-network-empowered machines can expose the underpinning learnt causal graphs and humans can contest the machines by modifying the causal graphs before re-injecting them into the machines, so that the learnt models are guaranteed to conform to the graphs and adhere to expert knowledge (some of which can also be given up-front). By building a window into the model behaviour and enabling knowledge injection, our method allows practitioners to debug networks based on the causal structure discovered from the data and underpinning the predictions. Experiments with real and synthetic tabular data show that our method improves predictive performance up to 2.4x while producing parsimonious networks, up to 7x smaller in the input layer, compared to SOTA regularised networks.\n\n275 Cartesian Abstractions and Saturated Cost Partitioning in Probabilistic Planning\n\nAuthors: Thorsten Klößner ; Jendrik Seipp ; Marcel Steinmetz\n\n##MORE##Stochastic shortest path problems (SSPs) capture probabilistic planning tasks with the objective of minimizing expected cost until reaching the goal. One of the strongest methods to solve SSPs optimally is heuristic search guided by an admissible (lower-bounding) heuristic function. Recently, probability-aware pattern database (PDB) abstractions have been highlighted as an efficient way of generating such lower bounds, with significant advantages over traditional determinization-based approaches. Here, we follow this work, yet consider a more general type, Cartesian abstractions, which have been used successfully in the classical setting. We show how to construct probability-aware Cartesian abstractions via a counterexample-guided abstraction refinement (CEGAR) loop akin to classical planning. This method is complete, meaning it guarantees convergence to the optimal expected cost if not terminated prematurely. Furthermore, we investigate the admissible combination of multiple such heuristics using saturated cost partitioning (SCP), marking its first application in the probabilistic setting. In our experiments, we show that probability-aware Cartesian abstractions yield much more informative heuristics than their determinization-based counterparts. Finally, we show that SCP yields probability-aware abstraction heuristics that are superior to the previous state of the art.\n\n276 Revision Transformers: Instructing Language Models to Change their Values\n\nAuthors: Felix Friedrich ; Wolfgang Stammer ; Patrick Schramowski ; Kristian Kersting\n\n##MORE##Current transformer language models (LM) are large-scale models with billions of parameters. They have been shown to provide high performances on a variety of tasks but are also prone to shortcut learning and bias. Addressing such incorrect model behavior via parameter adjustments is very costly. This is particularly problematic for updating dynamic concepts, such as moral values, which vary culturally or interpersonally. In this work, we question the current common practice of storing all information in the model parameters and propose the Revision Transformer (RiT) to facilitate easy model updating. The specific combination of a large-scale pre-trained LM that inherently but also diffusely encodes world knowledge with a clear-structured revision engine makes it possible to update the model's knowledge with little effort and the help of user interaction. We exemplify RiT on a moral dataset and simulate user feedback demonstrating strong performance in model revision even with small data. This way, users can easily design a model regarding their preferences, paving the way for more transparent AI models.\n\n278 GraphSA: Smart Contract Vulnerability Detection Combining Graph Neural Networks and Static Analysis\n\nAuthors: Long He ; Xiangfu Zhao ; Yichen Wang ; Jiahui Yang ; XueLei Sun\n\n##MORE##Security incidents in smart contracts still occur frequently, as the underlying code is often vulnerable to attacks. However, traditional methods to detect vulnerabilities in smart contracts are limited by certain rigid rules, reducing accuracy and scalability. In this work, we propose GraphSA, which combines graph neural networks (GNNs) and static analysis for smart contract vulnerability detection. First, we present the contract tree, which is obtained by converting the control flow graph (CFG) of a smart contract. Each node in the tree represents a crucial operation code (opcode) block, and each edge represents the control flow (execution order) between code blocks. Then, we propose an extended SAGConv and Topkpooling graph neural network (ST-GNN) to learn the features of each node in the tree. To enhance detection accuracy, we eliminate and merge some non-crucial nodes to highlight key nodes and execution orders. Finally, we evaluate our approach on 7,962 real-world smart contracts running on Ethereum and compare it with state-of-the-art approaches on six types of vulnerabilities. Experimental results show that our approach achieves higher detection accuracy than others.\n\n282 Worrisome Properties of Neural Network Controllers and Their Symbolic Representations\n\nAuthors: Jacek Cyranka ; Kevin E M Church ; Jean-Philippe Lessard\n\n##MORE##We raise concerns about controllers' robustness in simple reinforcement learning benchmark problems. We focus on neural network controllers and their low neuron and symbolic abstractions. A typical controller reaching high mean return values still generates an abundance of persistent low-return solutions, which is a highly undesirable property, easily exploitable by an adversary. We find that the simpler controllers admit more persistent bad solutions. We provide an algorithm for a systematic robustness study and prove the persistent solutions and, in some cases, periodic orbits, using a computer-assisted proof methodology.\n\n285 Piecewise-Stationary Combinatorial Semi-Bandit with Causally Related Rewards\n\nAuthors: Behzad Nourani Koliji ; Steven Bilaj ; Amir Rezaei Balef ; Setareh Maghsudi\n\n##MORE##We study the piecewise stationary combinatorial semi-bandit problem with causally related rewards. In our nonstationary environment, variations in the base arms' distributions, causal relationships between rewards, or both, change the reward generation process. In such an environment, an optimal decision-maker must follow both sources of change and adapt accordingly. The problem becomes aggravated in the combinatorial semi-bandit setting, where the decision-maker only observes the outcome of the selected bundle of arms. The core of our proposed policy is the Upper Confidence Bound (UCB) algorithm. We assume the agent relies on an adaptive approach to overcome the challenge. More specifically, it employs a change-point detector based on the Generalized Likelihood Ratio test. Besides, we introduce the notion of group restart as a new alternative restarting strategy in the decision making process in structured environments. Finally, our algorithm integrates a mechanism to trace the variations of the underlying graph structure, which captures the causal relationships between the rewards in the bandit setting. Theoretically, we establish a regret upper bound that reflects the effects of the number of structural- and distribution changes on the performance. The outcome of our numerical experiments in real-world scenarios exhibits applicability and superior performance of our proposal compared to the state-of-the-art benchmarks.\n\n293 What Wikipedia Misses about Yuriko Nakamura? Predicting Missing Biography Content by Learning Latent Life Patterns\n\nAuthors: Yijun Duan ; Xin Liu ; Adam Jatowt ; Chenyi Zhuang ; Hai-Tao Yu ; Steven Lynden ; Kyoung-Sook Kim ; Akiyoshi Matono\n\n##MORE##Action-related KnowledGe (AKG) is important for facilitating deeper understanding of people's life patterns, objectives and motivations. In this study, we present a novel framework for automatically predicting missing human biography records in Wikipedia by generating such knowledge. The generation method, which is based on a neural network matrix factorization model, is capable of encoding action semantics from diverse perspectives and discovering latent inter-action relations. By correctly predicting missing information and correcting errors, our work can effectively improve the quality of data about the behavioral records of historical figures in the knowledge base (e.g., biographies in Wikipedia), thus contributing to the understanding and study of human actions by the general public on the one hand, and can be considered as a new paradigm for managing action-related knowledge in digital libraries on the other. Extensive experiments demonstrate that the AKG we generate can capture well missing or \"forgotten\" human biography related information in Wikipedia.\n\n294 CompLung: Comprehensive Computer-Aided Diagnosis of Lung Cancer\n\nAuthors: Adam Pardyl ; Dawid Damian Rymarczyk ; Joanna Jaworek-Korjakowska ; Dariusz Kucharski ; Andrzej Brodzicki ; Julia Lasek ; Zofia Schneider ; Iwona Kucybała ; Andrzej Urbanik ; Rafał Obuchowicz ; Zbisław Tabor ; Bartosz Zieliński\n\n##MORE##Lung cancer is a leading cause of cancer-related deaths, and early diagnosis is crucial for its effective treatment. That is why computer-aided tools have been developed to support particular steps of CT scan analysis, including lung segmentation, suspicious region detection, and patient-level diagnosis. However, none of the previous approaches addressed this process comprehensively. To fill this gap, we introduce CompLung, a comprehensive tool for lung cancer diagnosis that performs all of the above-listed steps in an end-to-end manner. We have trained the CompLung architecture using the publicly available LIDC-IDRI dataset extended with lung segmentation masks obtained from our internal radiologists, which we make publicly available to boost the research on this emerging topic. Finally, we conduct extensive experiments and demonstrate the superior performance and interpretability of CompLung compared to existing methods for lung cancer diagnosis.\n\n298 The Problem of Coherence in Natural Language Explanations of Recommendations\n\nAuthors: Jakub Raczyński ; Mateusz Lango ; Jerzy Stefanowski\n\n##MORE##Providing explanations for predictions of complex machine learning algorithms, including recommender systems, remains one of the biggest research challenges in AI. One form of such explanation that is particularly useful from the perspective of a non-expert user is an explanation expressed in natural language. Several methods for providing such explanations have recently been proposed for the recommendation task, however, we argue that an important aspect of explanation quality has been overlooked in their experimental evaluation. Specifically, the coherence between generated text and predicted rating, which is a necessary condition for an explanation to be useful, is not properly captured by currently used evaluation measures. In this paper, we highlight the issue of explanation and prediction coherence by 1) presenting results from a manual verification of explanations generated by one of the state-of-the-art approaches 2) proposing a method of automatic coherence evaluation 3) introducing a new transformer-based method that aims to produce more coherent explanations than the state-of-the-art approaches 4) performing an experimental evaluation which demonstrates that this method significantly improves the explanation coherence without affecting the other aspects of recommendation performance.\n\n299 An Empirical Study of Retrieval-enhanced Graph Neural Networks\n\nAuthors: Dingmin Wang ; Shengchao Liu ; Hanchen Wang ; Bernardo Cuenca Grau ; Linfeng Song ; Jian Tang ; Le Song ; Liu Qi\n\n##MORE##Graph Neural Networks (GNNs) are effective tools for graph representation learning. Most GNNs rely on a recursive neigh- borhood aggregation scheme, named message passing, thereby their theoretical expressive power is limited to the first-order Weisfeiler- Lehman test (1-WL). An effective approach to this challenge is to explicitly retrieve some annotated examples used to enhance GNN models. While retrieval-enhanced models have been proved to be ef- fective in many language and vision domains, it remains an open question how effective retrieval-enhanced GNNs are when applied to graph datasets. Motivated by this, we want to explore how the retrieval idea can help augment the useful information learned in the graph neural networks, and we design a retrieval-enhanced scheme called GRAPHRETRIEVAL, which is agnostic to the choice of graph neural network models. In GRAPHRETRIEVAL, for each input graph, similar graphs together with their ground-true labels are retrieved from an existing database. Thus they can act as a potential enhance- ment to complete various graph property predictive tasks. We conduct comprehensive experiments over 13 datasets, and we observe that GRAPHRETRIEVAL is able to reach substantial improvements over existing GNNs. Moreover, our empirical study also illustrates that retrieval enhancement is a promising remedy for alleviating the long-tailed label distribution problem.\n\n306 Multi-Domain Learning From Insufﬁcient Annotations\n\nAuthors: Rui He ; Shengcai Liu ; Jiahao WU ; Shan He ; Ke Tang\n\n##MORE##Multi-domain learning (MDL) refers to simultaneously constructing a model or a set of models on datasets collected from different domains. Conventional approaches emphasize domain-shared information extraction and domain-private information preservation, following the shared-private framework (SP models), which offers signiﬁcant advantages over single-domain learning. However, the limited availability of annotated data in each domain considerably hinders the effectiveness of conventional supervised MDL approaches in real-world applications. In this paper, we introduce a novel method called multi-domain contrastive learning (MDCL) to alleviate the impact of insufﬁcient annotations by capturing both semantic and structural information from both labeled and unlabeled data. Speciﬁcally, MDCL comprises two modules: inter-domain semantic alignment and intra-domain contrast. The former aims to align annotated instances of the same semantic category from distinct domains within a shared hidden space, while the latter focuses on learning a cluster structure of unlabeled instances in a private hidden space for each domain. MDCL is readily compatible with many SP models, requiring no additional model parameters and allowing for end-to-end training. Experimental results across ﬁve textual and image multi-domain datasets demonstrate that MDCL brings noticeable improvement over various SP models. Furthermore, MDCL can further be employed in multi-domain active learning (MDAL) to achieve a superior initialization, eventually leading to better overall performance.\n\n313 Improved Analysis of Greedy Algorithm on $k$-Submodular Knapsack\n\nAuthors: Zhongzheng Tang ; Chenhao Wang\n\n##MORE##A $k$-submodular function is a generalization of submodular functions that takes $k$ disjoint subsets as input and outputs a real value. It captures many problems in combinatorial optimization and machine leaning such as influence maximization, sensor placement, feature selection, etc. In this paper, we consider the monotone $k$-submodular maximization problem under a knapsack constraint, and explore the performance guarantee of a greedy-based algorithm: enumerating all size-2 solutions and extending every singleton solution greedily; the best outcome is returned. We provide a novel analysis framework and prove that this algorithm achieves an approximation ratio of at least $0.328$. This is the best-known result of combinatorial algorithms on $k$-submodular knapsack maximization.\n\nIn addition, within the framework, we can further improve the approximation ratio to a value approaching $\\frac13$ with any desirable accuracy, by enumerating sufficiently large base solutions. The results can even be extended to non-monotone $k$-submodular functions.\n\n321 Adam Accumulation to Reduce Memory Footprints of both Activations and Gradients for Large-scale DNN Training\n\nAuthors: Yijia Zhang ; Yibo Han ; Shijie Cao ; Guohao Dai ; Youshan Miao ; Ting Cao ; Fan Yang ; Ningyi Xu\n\n##MORE##Running out of GPU memory has become a main bottleneck for large-scale DNN training. How to reduce the memory footprint during training has received intensive research attention. We find that previous gradient accumulation reduces activation memory but fails to be compatible with gradient memory reduction due to a contradiction between preserving gradients and releasing gradients. To address this issue, we propose a novel optimizer accumulation method for Adam, named Adam Accumulation (AdamA), which enables reducing both activation and gradient memory. Specifically, AdamA directly integrates gradients into optimizer states and accumulates optimizer states over micro-batches, so that gradients can be released immediately after use. We mathematically and experimentally demonstrate AdamA yields the same convergence properties as Adam. Evaluated on transformer-based models, AdamA achieves up to 23% memory reduction compared to gradient accumulation with less than 2% degradation in training throughput. Notably, AdamA can work together with memory reduction methods for optimizer states to fit 1.26x~3.14x larger models over PyTorch and DeepSpeed baseline on GPUs with different memory capacities.\n\n323 Gaifman Graphs in Lifted Planning\n\nAuthors: Rostislav Horčík ; Daniel Fišer\n\n##MORE##We introduce the metric induced by Gaifman graphs into lifted planning. We analyze what kind of information this metric carries and how it can be utilized for constructing lifted delete-free relaxation heuristics. In particular, we prove how the action dynamics influence the distances between objects. As a corollary, we derive a lower bound on the length of any plan. Finally, we apply our theoretical findings on the Gaifman graphs to improve the delete-free relaxation heuristics induced by PDDL homomorphisms.\n\n330 Adversarial Benchmark Evaluation Rectified by Controlling for Difficulty\n\nAuthors: Behzad Mehrbakhsh ; Fernando Martínez-Plumed ; José Hernández-Orallo\n\n##MORE##Adversarial benchmark construction, where harder instances challenge new generations of AI systems, is becoming the norm. While this approach may lead to better machine learning models —on average and for the new benchmark—, it is unclear how these models behave on the original distribution. Two opposing effects are intertwined here. On the one hand, the adversarial benchmark has a higher proportion of difficult instances, with lower expected performance. On the other hand, models trained on the adversarial benchmark may improve on these difficult instances (but may also neglect some easy ones). To disentangle these two effects we can control for difficulty, showing that we can recover the performance on the original distribution, provided the harder instances were obtained from this distribution in the first place. We show this difficulty-aware rectification works in practice, through a series of experiments with several benchmark construction schemas and the use of a populational difficulty metric. As a take-away message, instead of distributional averages we recommend using difficulty- conditioned characteristic curves when evaluating models built with adversarial benchmarks.\n\n336 Exploring Information Bottleneck for Weakly Supervised Semantic Segmentation\n\nAuthors: Jie Qin ; Yueming Lyu ; Xingang Wang\n\n##MORE##Image-level weakly supervised semantic segmentation (WSSS) has attracted much attention due to the easily acquired class labels. Most existing methods resort to utilizing Class Activation Maps (CAMs) obtained from the classification network to play as the initial pseudo labels. However, the classifiers only focus on the most discriminative regions of the target objects, which is referred to as the information bottleneck from the perspective of the information theory. To alleviate this information bottleneck limitation, we propose an Information Perturbation Module (IPM) to explicitly obtain the information difference maps, which provide the accurate direction and magnitude of the information compression in the classification network. After that, an information bottleneck breakthrough mechanism with three branches is proposed to overcome the information bottleneck in the classification network for segmentation. Additionally, a diversity regularization on the generated two information difference maps is proposed to improve the diversity of the output CAMs. Extensive experiments on PASCAL VOC2012 val and test sets demonstrate that the proposed method can effectively improve the weakly supervised semantic segmentation performance of the advanced approaches.\n\n340 BridgeHand2Vec Bridge Hand Representation\n\nAuthors: Anna Sztyber-Betley ; Filip Kołodziej ; Jan Betley ; Piotr Duszak\n\n##MORE##Contract bridge is a game characterized by incomplete information, posing an exciting challenge for artificial intelligence methods. This paper proposes the BridgeHand2Vec approach, which leverages a neural network to embed a bridge player's hand (consisting of 13 cards) into a vector space. The resulting representation reflects the strength of the hand in the game and enables interpretable distances to be determined between different hands. This representation is derived by training a neural network to estimate the number of tricks that a pair of players can take. In the remainder of this paper, we analyze the properties of the resulting vector space and provide examples of its application in reinforcement learning, opening bid classification, and as an analysis tool for players. Although this was not our main goal, the neural network used for the vectorization achieves SOTA results on the DDBP2 problem (estimating the number of tricks for two given hands).\n\n352 PARIS: Planning Algorithms for Reconfiguring Independent Sets\n\nAuthors: Remo Christen ; Salome Eriksson ; Michael Katz ; Christian Muise ; Alice Petrov ; Florian Pommerening ; Jendrik Seipp ; Silvan Sievers ; David Speck\n\n##MORE##Combinatorial reconfiguration studies how one solution of a combinatorial problem can be transformed into another. The transformation can only make small local changes and may not leave the solution space. An important example is the independent set reconfiguration (ISR) problem, where an independent set of a graph (a subset of its vertices without edges between them) has to be transformed into another one by a sequence of modifications that remove a vertex or add another that is not adjacent to any vertex in the set. The 1st Combinatorial Reconfiguration Challenge (CoRe Challenge 2022) was a competition focused on the ISR problem. The PARIS team participated with two solvers that model the ISR problem as a planning problem and employ different planning techniques for solving it. The solvers successfully competed in the challenge and were awarded 4 first, 3 second, and 3 third places across 9 tracks. In this work, we show how to model ISR problems as planning tasks and describe the planning techniques used by these solvers. For a fair comparison to competing ISR approaches, we re-run the entire competition under equal computational conditions. Besides showcasing the success of planning technology, we hope that this work will create a cross-fertilization of the two research fields.\n\n354 Transparency in Sum-Product Network Decompilation\n\nAuthors: Giannis Papantonis ; Vaishak Belle\n\n##MORE##Sum-product networks guarantee that conditionals and marginals can be computed efficiently, for a wide range of models, bypassing the hardness of inference. However, this advantage comes at the expense of transparency, since it is unclear how variables interact in sum-product networks. Due to this, a series of decompilation algorithms transform sum-product networks back to Bayesian networks. In this work, we first study the transparency and causal utility of the resulting Bayesian networks. We then propose a novel decompilation algorithm to address the identified limitations.\n\n355 Reinforcement Learning for Bandits with Continuous Actions and Large Context Spaces\n\nAuthors: Paul Duckworth ; Bruno Lacerda ; Katherine Vallis ; Nick Hawes\n\n##MORE##We consider the challenging scenario of contextual bandits with continuous actions and large context spaces. This is an increasingly important application area in personalised healthcare where an agent is requested to make dosing decisions based on a patient's single image scan. In this paper, we first adapt a reinforcement learning (RL) algorithm for continuous control to outperform contextual bandit algorithms specifically hand-crafted for continuous action spaces. We empirically demonstrate this on a suite of standard benchmark datasets for vector contexts. Secondly, we demonstrate that our RL agent can generalise problems with continuous actions to large context spaces, providing results that outperform previous methods on image contexts. Thirdly, we introduce a new contextual bandits test domain with multi-dimensional continuous action space and image contexts which existing tree-based methods cannot handle. We provide initial results with our RL agent.\n\n359 Representative Answer Sets: Collecting Something of Everything\n\nAuthors: Elisa Böhl ; Sarah Alice Gaggl ; Dominik Rusovac\n\n##MORE##Answer set programming (ASP) is a popular problem solving paradigm with applications in planning and configuration. In practice, the number of answer sets can be overwhelmingly high, which naturally causes interest in a concise characterisation of the solution space in terms of representative answer sets. We establish a notion of representativeness that refers to the entropy of specified target atoms within a collection of answer sets. Accordingly, we propose different approaches for collecting such representative answer sets, based on answer set navigation. Finally, we conduct experiments using our prototypical implementation, which reveals promising results.\n\n369 Multi-Source Domain Adaptation through Dataset Dictionary Learning in Wasserstein Space\n\nAuthors: Eduardo F Montesuma ; Fred Maurice Ngole Mboula ; Antoine Souloumiac\n\n##MORE##Multi-Source Domain Adaptation (MSDA) is an important machine learning problem that aims to mitigate data distribution shifts when transferring knowledge from multiple labeled source domains to an unlabeled target domain. We propose a novel MSDA approach based on a dictionary of empirical distributions. Our dictionary expresses each domain in MSDA as an interpolation in the Wasserstein hull of our dictionary atoms, i.e., through Wasserstein barycenters. We evaluate our method in 3 MSDA benchmarks: (i) Caltech-Office 10, (ii) Refurbished-Office 31, and (iii) CWRU. We improve previous state-of-the-art by 3.15\\%, 2.29\\%, and 7.71\\% in terms of average domain adaptation performance. Furthermore, we show that interpolations in the Wasserstein hull of learned atoms provide data that can generalize to an unlabeled target domain.\n\n371 On the Structural Complexity of Grounding - Tackling the ASP Grounding Bottleneck via Epistemic Programs and Treewidth\n\nAuthors: Viktor Besin ; Markus Hecher ; Stefan Woltran\n\n##MORE##Answer Set Programming is widely applied research area for knowledge representation and for solving industrial domains. One of the challenges of this formalism focuses on the so-called grounding bottleneck, which addresses the efficient replacement of first-order variables by means of domain values. Recently, there have been several works in this direction, ranging from lazy grounding, hybrid solving, over translational approaches. Inspired by a translation from non-ground normal programs to ground disjunctive programs, we attack the grounding bottleneck from a more general angle. We provide a polynomial reduction for grounding disjunctive programs of bounded domain size by reducing to epistemic logic programs (ELPs). By slightly adapting our reduction, we show new complexity results for non-ground programs that adhere to the measure treewidth. We complement these results by matching lower bounds under the exponential time hypothesis, ruling out significantly better algorithms.\n\n373 The Sensitivity of Max Regret With Respect to Scaling of the Objectives\n\nAuthor: Nic Wilson\n\n##MORE##In a multi-objective optimisation problem, when there is uncertainty regarding the correct user preference model, max regret is a natural measure for how far an alternative is from being necessarily optimal (i.e., optimal with respect to every candidate preference model). It can be used for recommending a relatively safe choice to the user, or used in the generation of an informative query, and in the decision to terminate the user interaction, because an alternative is sufficiently close to being necessarily optimal. We consider a common and simple form of user preference model: a weighted average over the objectives (with unknown weights). However, changing the scale of an objective by a linear factor leads to an essentially different set of preference models, and this changes the max regret values (and potentially their relative ordering), sometimes very considerably. Since the scaling of the objectives is often partly subjective and somewhat arbitrary, it is important to be aware of how sensitive the max regret values are to the choices of scaling of the objectives. We give mathematical results that characterise and enable computation of this variability, along with an asymptotic analysis.\n\n374 Ensembling Uncertainty Measures to Improve Safety of Black-Box Classifiers\n\nAuthors: Tommaso Zoppi ; Andrea Ceccarelli ; Andrea Bondavalli\n\n##MORE##Machine Learning (ML) algorithms that perform classification may predict the wrong class, experiencing misclassifications. It is well-known that misclassifications may have cascading effects on the encompassing system, possibly resulting in critical failures. This paper proposes SPROUT, a Safety wraPper thROugh ensembles of UncertainTy measures, which suspects misclassifications by computing uncertainty measures on the inputs and outputs of a black-box classifier. If a misclassification is detected, SPROUT blocks the propagation of the output of the classifier to the encompassing system. The resulting impact on safety is that SPROUT transforms erratic outputs (misclassifications) into data omission failures, which can be easily managed at the system level. SPROUT has a broad range of applications as it fits binary and multi-class classification, comprising image and tabular datasets. We experimentally show that SPROUT always identifies a huge fraction of the misclassifications of supervised classifiers, and it is able to detect all misclassifications in specific cases. SPROUT implementation contains pre-trained wrappers, it is publicly available and ready to be deployed with minimal effort.\n\n379 Evolutionary Explainable Rule Extraction from (Modal) Random Forests\n\nAuthors: Michele Ghiotti ; Federico Manzella ; Giovanni Pagliarini ; Guido Sciavicco ; Eduard Ionel Stan\n\n##MORE##Symbolic learning is the subfield of machine learning concerned with learning predictive models with knowledge represented in logical form, such as decision tree and decision list models. Ensemble learning methods, such as random forests, are usually deployed to improve the performance of decision trees; unfortunately, interpreting tree ensembles is challenging. In order to deal with unstructured (e.g., temporal or spatial) data, moreover, decision trees and random forests have been recently generalized to the use of modal logics, which are harder to interpret than their propositional counterpart. Recently, a methodology for extracting simple rules from propositional random forests, based on a sequence of optimization steps, was proposed. In this work, we generalize this approach along two directions: from propositional to modal logic and from a sequence of optimization steps to a single multi-objective optimization problem. Even if confined to the temporal domain, our experimental results, based on open-source implementations and public data, show that our method is robust, scalable, and able to extract small, accurate, and informative decision lists even for complex classification problems.\n\n380 Neuro-Symbolic Procedural Semantics for Reasoning-Intensive Visual Dialogue Tasks\n\nAuthors: Lara Verheyen ; Jérôme Botoko Ekila ; Jens Nevens ; Paul Van Eecke ; Katrien Beuls\n\n##MORE##This paper introduces a novel approach to visual dialogue that is based on neuro-symbolic procedural semantics. The approach builds further on earlier work on procedural semantics for visual question answering and expands it on the one hand with neuro-symbolic reasoning operations, and on the other hand with mechanisms that handle the challenges that are inherent to dialogue, in particular the incremental nature of the information that is conveyed. Concretely, we introduce (i) the use of a conversation memory as a data structure that explicitly and incrementally represents the information that is expressed during the subsequent turns of a dialogue, and (ii) the design of a neuro-symbolic procedural semantic representation that is grounded in both visual input and the conversation memory. We validate the methodology using the reasoning-intensive MNIST Dialog and CLEVR-Dialog benchmark challenges and achieve a question-level accuracy of 99.8% and 99.2% respectively. The methodology presented in this paper responds to the growing interest in the field of artificial intelligence in solving tasks that involve both low-level perception and high-level reasoning using a combination of neural and symbolic techniques.\n\n388 Graph Neural Networks For Mapping Variables Between Programs\n\nAuthors: Pedro Orvalho ; Jelle Piepenbrock ; Mikolas Janota ; Vasco Manquinho\n\n##MORE##Automated program analysis is a pivotal research domain in many areas of Computer Science --- Formal Methods and Artificial Intelligence, in particular. Due to the undecidability of the problem of program equivalence, comparing two programs is highly challenging. Typically, in order to compare two programs, a relation between both programs' sets of variables is required. Thus, mapping variables between two programs is useful for a panoply of tasks such as program equivalence, program analysis, program repair, and clone detection. In this work, we propose using graph neural networks (GNNs) to map the set of variables between two programs based on both programs' abstract syntax trees (ASTs). To demonstrate the strength of variable mappings, we present three use-cases of these mappings on the task of program repair to fix well-studied and recurrent bugs among novice programmers in introductory programming assignments (IPAs). Experimental results on a dataset of 4166 pairs of incorrect/correct programs show that our approach correctly maps 83% of the evaluation dataset. Moreover, our experiments show that the current state-of-the-art on program repair, greatly dependent on the programs' structure, can only repair about 72% of the incorrect programs. In contrast, our approach, which is solely based on variable mappings, can repair around 88.5%.\n\n389 Normalized Stochastic Heavy Ball with Adaptive Momentum\n\nAuthors: Ziqing Wen ; Xiaoge Deng ; Tao Sun ; Dongsheng Li\n\n##MORE##The heavy ball momentum technique is widely used in accelerating the machine learning training process, which has demonstrated significant practical success in optimization tasks. However, most heavy ball methods require a preset hyperparameter that will result in excessive tuning, and a calibrated fixed hyperparameter may not lead to optimal performance. In this paper, we propose an adaptive criterion for the choice of the normalized momentum-related hyperparameter, motivated by the quadratic optimization training problem, to eliminate the adverse for tuning the hyperparameter and thus allow for a computationally efficient optimizer. We theoretically prove that our proposed adaptive method promises convergence for L-Lipschitz functions. In addition, we verify its practical efficiency on existing extensive machine learning benchmarks for image classification tasks. The numerical results show that besides the speed improvement, our proposed methods enjoy advantages, including more robust to large learning rates and better generalization.\n\n396 Obstruction Logic: a Strategic Temporal Logic to Reason about Dynamic Game Models\n\nAuthors: Davide Catta ; Jean Leneutre ; Vadim Malvone\n\n##MORE##Games that are played in a dynamic (i.e., changing) game model have been studied in several contexts, such as cybersecurity and planning. In this paper, we introduce a logic for reasoning about a particular class of games with temporal goals played in a dynamic game model. In such games, the actions of a player can modify the game model itself. We show that the model-checking problem for our logic is decidable in polynomial-time. Then, using this logic, we show how to express interesting properties of cybersecurity games defined on attack graphs.\n\n397 Exploiting epistemic uncertainty at inference time for early-exit power saving\n\nAuthors: Jack Dymond ; Steve R Gunn ; Sebastian Stein\n\n##MORE##Distinguishing epistemic from aleatoric uncertainty is a central idea to out-of-distribution (OOD) detection, by interpreting adversarial and OOD inputs from this perspective, we can collect them into a single unclassifiable group. Rejecting such inputs early in the classification process will reduce resource usage, which is of particular importance in resource-constrained environments. To achieve this, we apply deep k-nearest neighbour (KNN) classifiers to the embedding space of branched neural networks. These are implemented alongside conventional early exiting policies, introducing a novel means of saving power: an early-exit reject. Our technique works out-of-the-box on any branched neural network and can be competitive on OOD benchmarks, achieving an area under receiver operator characteristic (AUROC) of over 0.9 in most datasets, and scores of 0.95+ when identifying perturbed "
    }
}
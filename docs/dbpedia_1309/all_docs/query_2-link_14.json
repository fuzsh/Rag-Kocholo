{
    "id": "dbpedia_1309_2",
    "rank": 14,
    "data": {
        "url": "https://nap.nationalacademies.org/read/6323/chapter/6",
        "read_more_link": "",
        "language": "en",
        "title": "Funding a Revolution: Government Support for Computing Research",
        "top_image": "https://nap.nationalacademies.org/cover/6323/450",
        "meta_img": "https://nap.nationalacademies.org/cover/6323/450",
        "images": [
            "https://nap.nationalacademies.org/read/img/openbook-header-print.png",
            "https://nap.nationalacademies.org/cover/6323/450",
            "https://nap.nationalacademies.org/images/hdr/logo-nasem-wht-lg.png"
        ],
        "movies": [],
        "keywords": [],
        "meta_keywords": [
            ""
        ],
        "tags": null,
        "authors": [],
        "publish_date": null,
        "summary": "",
        "meta_description": "Read chapter 4 The Organization of Federal Support: A Historical Review: The past 50 years have witnessed a revolution in computing and related communicat...",
        "meta_lang": "en",
        "meta_favicon": "",
        "meta_site_name": "The National Academies Press",
        "canonical_link": "https://nap.nationalacademies.org/read/6323/chapter/6",
        "text": "BOX 4.1 Project Whirlwind and SAGE\n\nTwo closely connected computing projects, Whirlwind and SAGE, demonstrate the influence of federal research and development programs during the early days of computing. They not only generated technical knowledge and human resources, but they also forged a unique relationship among government, universities, and industry. The Whirlwind computer was originally intended to be part of a general-purpose flight simulator, but it evolved into the first real-time, general-purpose digital computer. SAGE, an air-defense system designed to protect against enemy bombers, made several important contributions to computing in areas as diverse as computer graphics, time-sharing, digital communications, and ferrite-core memories. Together, these two projects shared a symbiotic relationship that strengthened the early computer industry.\n\nWhirlwind originated in 1944 as part of the Navy's Airplane Stability and Control Analyzer (ASCA) project. At that time, the Navy made extensive use of flight simulators to test new aircraft designs and train pilots; however, each new aircraft design required a separate computer specially created for its particular design. ASCA was intended to negate the need to build individual computers for the flight simulators by serving as a general-purpose simulator that could emulate any design programmed into it. Jay Forrester, the leader of the computer portion of the ASCA project, soon recognized that analog computers (which were typically used on aircraft simulators) would not be fast enough to operate the trainer in real time. Learning of work in electronic digital computing as part of ENIAC at the University of Pennsylvania, Forrester began investigating the potential for real-time digital computers for Whirlwind. By early 1946, Forrester decided to pursue the digital route, expanding the goal of the Whirlwind program from building a generalizable aircraft simulator to designing a real-time, general-purpose digital computer that could serve many functions other than flight simulation.\n\nPursuing a digital computer required dramatic increases in computing speeds and reliability, both of which hinged on development of improved computer memoryâ€”an innovation that was also needed to handle large amounts of data about incoming airplanes. Mercury delay-line memories, which used sonic pulses to record information and were being pursued by several other research centers, were too slow for the machine Forrester envisioned. He decided instead to use electrostatic storage tubes in which bits of information could be stored as an electrical charge and which claimed read-and-write times of a few milliseconds. Such tubes proved to be expensive, limited in storage capacity, and unreliable. Looking for a new memory alternative, Forrester came across a new magnetic ceramic called Deltamax and began working on the first magnetic core memory, a project to which he later assigned a graduate student, Bill Papian.\n\nThe expansion of Whirlwind's technical objectives resulted in expanding project budgets that eventually undermined support for the project. Forrester originally planned Whirlwind as a 2-year, $875,000 program, but he increased his cost estimate for the Whirlwind computer itself to $1.9 million in March 1946 and to almost $3 million by 1947 (Campbell-Kelly and Aspray, 1996, pp. 161-163). By 1949, Whirlwind made up nearly 65 percent of the Office of Naval Research (ONR) mathematics research budget and almost 10 percent of ONR's entire contract research\n\nbudget (Edwards, 1996, p. 79). As a part of a general Department of Defense initiative to centralize computer research in 1951, ONR planned to reduce Whirlwind's annual budget from $1.15 million to $250 thousand in 1951, threatening the viability of the project (Edwards, 1996, p. 91). Support for the project was salvaged only after George Valley, Jr., a professor of physics at the Massachusetts Institute of Technology (MIT) and chairman of the Air Defense System Engineering Committee, realized that Whirlwind might play a critical role in a new air-defense program, SAGE, and convinced the Air Force to provide additional funding for the project, thereby adding to its credibility.\n\nIn 1949, Valley began lobbying the Air Force to improve U.S. air-defense capability in the face of the nation's growing vulnerability to Soviet bombers (Freeman, 1995, p. 2). Valley was put in charge of the Air Defense Systems Engineering Committee to investigate possible solutions. The resulting Project Charles Summer Study Group recommended that the Air Force ask MIT to build a laboratory to carry out the experimental and field research necessary to develop a system to safeguard the United States (Freeman, 1995, p. 6). In response, MIT created Project Lincoln, now known as Lincoln Laboratory, to create the Semi-Automatic Ground Environment, or SAGE, system.\n\nThrough SAGE, the Air Force became the major sponsor of Whirlwind, enabling the project to move toward completion. By late 1951, a prototype ferrite-core memory system was demonstrated, and by 1953, the Whirlwind's entire memory was replaced with core memory boasting a 9-microsecond access time, effectively ending the research phase of the program. The Air Force subsequently purchased production versions of the computer (designed in a cooperative effort between MIT and IBM) to equip each of its 23 Direction Centers. Each center had two IBM-manufactured versions of Whirlwind: one operating live and one operating in standby mode for additional reliability. The machines accepted input from over 100 different information sources (typically from ground, air, and seaborne radars) and displayed relevant information on cathode-ray-tube displays for operators to track and identify aircraft.\n\nThe first SAGE Direction Center was activated in 1958, and deployment continued until 1963, when final deployment of 23 centers was completed at an estimated cost of $8 billion to $12 billion. Although a technical success, SAGE was already outdated by the time of its completion. The launch of Sputnik shifted the most feared military threat to the United States from long-range bombers to intercontinental ballistic missiles. SAGE command centers continued to operate into the middle of the 1980s but with a reduced urgency.\n\nAll told, ONR spent roughly $3.6 million on Whirlwind, the Air Force, $13.8 million. In return, Whirlwind and SAGE generated a score of innovations. On the hardware side, Whirlwind and SAGE pioneered magnetic-core memory, digital phone-line transmission and modems, the light pen (one of the first graphical user interfaces), and duplexed computers. In software, they pioneered use of real-time software; concepts that later evolved into assemblers, compilers, and interpreters; software diagnosis programs; time-shared operating systems; structured program modules; table-driven software; and data description techniques. Five years after its introduction in Whirlwind, ferrite-core memory replaced every other type of com\n\nputer memory, and remained the dominant form of computer memory until 1973. Royalties to MIT from nongovernment sales amounted to $25 million, as MIT licensed the technology broadly.1\n\nIn addition, SAGE accelerated the transfer of these technologies throughout the nascent computer industry. While Lincoln Laboratory was given primary responsibility for SAGE, the project also involved several private firms such as IBM, RAND, Systems Development Corporation (the spin-off from RAND), Burroughs, Western Electric, RCA, and AT&T.2 Through this complex relationship between academia, industry, and the military, SAGE technologies worked their way into commercial products and helped establish the industry leaders. SAGE was a driving force behind the formation of the American computer and electronics industry (Freeman, 1995, p. 33). IBM built 56 computers for SAGE, earning over $500 million, which helped contribute to its becoming the world's largest computer manufacturer (Edwards, 1996, pp. 101-102; Freeman, 1995, p. 33). At its peak, between 7,000 and 8,000 IBM employees worked on the project. SAGE technology contributed substantially to the SABRE airline reservation system marketed by IBM in 1964, which later became the backbone of the airline industry (Edwards, 1996, p. 102). Kenneth Olsen, who worked on Whirlwind before founding Digital Equipment Corporation, called Whirlwind the first minicomputer and states that his company was based entirely on Whirlwind technology (Old Associates, 1981, p. 23).\n\nSAGE also contributed to formalizing the programming profession. While developing software for the system, the RAND Corporation spun off the Systems Development Corporation (SDC) to handle the software for SAGE. SDC trained thousands of programmers who eventually moved into the workforce. Numerous computer engineers from both IBM and SDC started their own firms with the knowledge they acquired from SAGE.\n\nSAGE also established an influential precedent for organizational management. Lincoln Laboratory was structured in the same style as MIT had run the Radiation Laboratory during World War II, in that it had much less management involvement than other equivalent organizations. As a result, researchers had a large amount of freedom to pursue their own solutions to problems at hand. Norman Taylor, one of the key individuals who designed SAGE at Lincoln Laboratory credited the management style for the projects' successes:\n\nI think Bob [Everett] put his finger on one important thing: the freedom to do something without approval from top management. Take the case of the 65,000 word memory. . . . We built that big memory, and we didn't go to the steering committee to get approval for it. We didn't go up there and say, \"Now, here's what we ought to do, it's going to cost this many million dollars, it's going to take us this long, and you must give us approval for it.\" We just had a pocket of money that was for advanced research. We didn't tell anybody what it was for; we didn't have to. (Freeman, 1995, p. 20)\n\nThis management style contrasted with the more traditional bureaucratic style of most American corporations of the time. It was subsequently adopted by Digital Equipment Corporation (under Kenneth Olsen's leadership) and eventually imitated\n\nBOX 4.2 Project MAC and Computer Time-sharing\n\nThe development of computer time-sharing and the advent of minicomputers set the technological stage for the 1970s. Time-sharing systems divide computation power cyclically between many users over a network. Properly designed time-sharing computers can switch among processes quickly enough so that users do not recognize any delay, making it appear as though each user has the computer's full attention. Such systems took advantage of design and manufacturing peculiarities of mainframes that resulted in the power of a mainframe computer varying as the square of cost of the computer.1 Therefore, building one computer for twice the cost of a smaller machine created four times the power. Time-sharing systems took advantage of this phenomena by allowing several users to share a single larger computer instead of several smaller machines. Development of such systems emerged from the complementary efforts of industry, universities, and government. Key to these efforts were Project MAC and its predecessors, funded by the Advanced Research Projects Agency and the National Science Foundation (NSF). While Project MAC was not responsible for the first time-sharing system, it played a significant role in the technology's development.\n\nProject MAC was started by IPTO in 1963, with funding going to the Massachusetts Institute of Technology (MIT). MAC stood for Man and Computer, Machine-Aided Cognition, and Multi-Access Computer. J.C.R. Licklider chose MIT as the site for Project MAC because of the large variety of computer disciplines being studied at MIT. Project MAC brought together, for example, Marvin Minsky's artificial intelligence work, Douglas Ross's computer-aided design systems, Herbert Teager's studies in languages and devices, and Martin Greenberger's work with human-machine systems. While the program was justified to the military as a command-and-control program, Licklider's goal was much broader. He sought \"the possibility of a profound advance, which will be almost literally an advance in the way of thinking about computing.\" In an interview with the Charles Babbage Institute, Licklider said, \"I wanted interactive computing, I wanted time-sharing. I wanted themes like: computers are as much for communication as they are for calculation\" (Norberg and O'Neil, 1996, pp. 97-98). Project MAC would eventually receive $25 million in total from 1963 to 1970 (Reed et al., 1990, Chapter 19, p. 14).\n\nThe core of Project MAC involved the design of a time-sharing computer system. Project MAC was not the first time-sharing initiative, but it significantly pushed the state of the art. Time-sharing systems had previously been developed in the MIT Computation Center, at System Development Corporation, and at Bolt, Beranek and Newman. At first, Project MAC used the MIT Computation Center's Compatible Time-Sharing System (CTSS), which had been designed under a grant from NSF. The system was built on an IBM 7090/94 and became operational in 1961. This was the first system enabling users to write their own programs online (Reed et al., 1990, pp. 19-2 to 19-3). In 1964, CTSS was connected to 24 terminals across the MIT campus. Eventually, 160 terminals were in place and 30 could be in use at one time. However, the CTSS still could not provide as much power as researchers desired, and it lacked necessary data access security.\n\nBeginning in 1965, Project MAC began to create a second system with the help of General Electric and Bell Laboratories: MULTICS (Multiplexed Information and Computing Service), was completed in 1969 and would eventually support 1,000 terminals at MIT with 300 in use at any one time (Campbell-Kelly and Aspray, 1996, pp. 214-215). MULTICS also incorporated a multiuser file system and a complex virtual-memory system that allowed application programs to function as if available memory were much larger than the memory actually attached to the processor. It featured an automatically managed three-level memory system, controlled sharing and protection of data and programs accessed by multiple users, and the ability to reallocate its resources dynamically without interruption. MULTICS had a multiuser file system that allowed each user to work as if on an independent computer (Flamm, 1987, p. 58).\n\nProject MAC led to many advances beyond time-sharing. MIT's Artificial Intelligence Laboratory received $1 million in funding through Project MAC for work to further the objectives of interactive computing (of which time-sharing was an integral part) and intelligent assistance (Norberg and O'Neill, 1996). Funds also went toward research in input/output devices. One of the earliest computer-aided design systems, KLUDGE, was developed through Project MAC. Project MAC's ability to compose and edit programs and documents online laid the groundwork for word processors and interactive programming. The idea for the spreadsheet, later popularized by Lotus 123 and subsequently Microsoft's Excel, also came from two students who worked on Project MAC. This idea spurred development of the first spreadsheet on the personal computer, VisiCalc, from Software Arts. The first real networking of the personal computer (the first version of Internet protocols for the PC) also came from MIT's Project MAC (renamed the Laboratory for Computer Science by then), which led to the company called FTP Software. FTP sold the first Internet protocol suite for DOS.\n\nAnother lasting spin-off from Project MAC was the popular operating system, Unix. The difficulty that Bell Laboratories had in developing the MULTICS operating system led to a new philosophy of software design stressing simplicity and elegance. In 1969, when Bell Laboratories realized that a commercial product was still many years away, it withdrew from Project MAC. Over the next 5 years, Bell researchers Kenneth Thompson and Dennis Ritchie, along with others who had been working with MAC and had become frustrated with MULTICS's complexity, developed Unix, which was based on MULTICS but was much simpler. It offered quick responses, had minimal system overhead, and ran on minicomputers instead of more expensive mainframes with special memory management systems.\n\nBeyond the technical advances in time-sharing, Project MAC influenced an industrywide movement toward developing time-sharing computers. When searching for a contractor to supply the hardware for MULTICS, MIT turned down its traditional supplier, IBM, and hired General Electric (GE) because of IBM's unwillingness to modify their machines for the project. The early results of Project MAC, though, convinced IBM and other manufacturers that they would have to pursue time-sharing (Campbell-Kelly and Aspray, 1996, p. 215). By 1967, 20 firms were competing for a $20 million industry providing time-shared computer services to businesses across the nation including GE, Telcomp, Tymshare, Keydata, and University Computing\n\nCompany. By the mid-1970s, almost every mainframe computer sold incorporated time-sharing technology (Reed et al., 1990, pp. 9-14).\n\nProject MAC was largely responsible for bringing the computer out of the laboratory and business and leading it to the home. Licklider's desire to create a \"new way of thinking\" about computers succeeded. Project MAC developed technology and ideas that allowed interactive computing to become a reality. . . .\" As a result of Project MAC and other computer time-sharing research programs in the late 1960s, the concept of computer utilities became widely accepted in the computer and business world. In 1964, only one year after Project MAC began, Martin Greenberger wrote, \"Barring unforeseen obstacles, an on-line interactive computer service, provided commercially by an information utility, may be as commonplace by 2000 A.D. as the telephone service is today'' (Campbell-Kelly and Aspray, 1996, p. 217). The image Greenberger described is remarkably similar to the Internet. Before time-sharing became a reality, computing remained available only to large businesses, academic institutions, and the government. However, as more users could simultaneously use a single machine, the cost of computing dramatically decreased, and usage increased accordingly. Project MAC played a large role in the public's change of philosophy regarding the use of computers.\n\nBOX 4.3 Roots of the Personal Computer\n\nThe development of the personal computer (PC) is illustrative of the symbiosis between government and industry in the evolving computer industry. While the PC stands as a monument to industrial innovation and the foresight and tenacity of individual entrepreneurs, federally sponsored research also played a role. The Macintosh operating system and Microsoft Windows, which trace their lineage to the Alto computer developed by Xerox between 1973 and 1978, incorporate concepts first explored by researchers working with federal support.\n\nIn the 1960s, the Advanced Research Projects Agency (ARPA) and the National Aeronautics and Space Administration provided funding for Douglas Engelbart to create a new research program at the Stanford Research Institute to work on improving human-computer interactions. Engelbart's research concentrated on using computers to augment the abilities of an individual as opposed to automating those abilities. In 1968, at the Joint Computer Conference, Engelbart presented the NLS (On-Line System), a computerized office system that his group developed. The NLS was the first system to use a mouse and the first to use windows. The invention of the mouse and its use as part of a graphical user interface represented a dramatic change from the standard command-line operation of computers. Most mainframe and time-sharing systems at the time relied on typed commands that computer novices found cryptic and difficult to use. Text on the screen could often be edited only by referencing the line number as opposed to changing the text in place. The use of a mouse and graphical user interface began the trend to make computers usable by anyone.\n\nDesigners at the Xerox Palo Alto Research Center (PARC) later incorporated Engelbart's advances into a graphical user interface for Xerox's Alto computer. The Alto was designed for users including \"children from age 5 or 6 and 'noncomputer adults' such as secretaries, librarians, architects, musicians, housewives, doctors and so on\" (ACM, 1993, p. 29). The Alto also drew upon the ideas described in Alan Kay's doctoral thesis, work that was also supported by ARPA while Kay was at the University of Utah. Kay described a computer called FLEX that would act as \"an interactive tool which can aid in the visualization and realization of provocative notions. It must be simple enough so that one does not have to become a systems programmer (one who understands the arcane rites) to use it. It must be cheap enough to be owned (like a grand piano). It must do more than just be able to realize computable functions; it has to be able to form the abstractions in which the user deals. FLEX is an idea debugger and as such, it is hoped that it is also an idea media.\"1 Kay envisioned this computer of the future to be the size of a notebook, one that could handle all of an individual's personal information management and manipulation needs. Kay later called this computer the Dynabook. Kay was not able to build an operational Dynabook for his thesis, but the new computing context was influential. \"Since at first people shared computers, the idea that everyone should have their own was a breakthrough\" (ACM, 1993, p. 31).\n\nRobert Taylor, the associate manager of the Computer Science Laboratory (CSL) at PARC recruited Alan Kay for the Xerox System Science Laboratory (SSL) in an attempt to integrate the SSL and CSL in working toward a shared goal. Taylor was a former director of ARPA's Information Processing Techniques Office and used his\n\nknowledge of the field and the key researchers in it to staff the laboratory and provide direction. He followed the same principles he used at ARPA: enlisting the most talented researchers and giving them the freedom to follow their own imagination.2 Taylor planned for the CSL to create the hardware infrastructure for distributed personal computing and for SSL to design software and applications for it (Smith and Alexander, 1988, pp. 70-71). While working in the SSL, Kay developed the SmallTalk language on which most of Alto's software was developed. SmallTalk was the first object-oriented programming language.\n\nXerox was never able to market the Alto successfully, but its influence is noticeable in most business and home computers in use today. In 1979, Steve Jobs was invited to tour Xerox PARC. Jobs realized the potential for the Alto system. He told the demonstrator of the system, Larry Tesler, ''Why isn't Xerox marketing this? . . . You could blow everything away\" (Smith and Alexander, 1988, p. 241). Jobs then incorporated many aspects of the Alto into the Apple Lisa, first produced in 1983, and its successor, the Maclntosh. The popularity of graphical user interfaces grew rapidly. Eventually Microsoft introduced Windows, beginning the conversion of x86 PCs from the command-line operating system DOS to the operating systems prevalent today.\n\nBOX 4.4 Accomplishments of DARPA's Very Large Scale Integrated Circuit Program\n\nDARPA's Very Large Scale Integrated Circuit (VLSI) program supported research on a number of innovations that revolutionized computing and computing research. Work on computer workstations, reduced instruction set computing, and semiconductor fabrication services for university researchers, in particular, benefited from DARPA support. In each of these areas, DARPA identified ongoing research of interest and provided the support necessary to bring the work to fruition.\n\nComputer Workstations\n\nAlthough industry efforts to develop computer workstations were under way at companies such as Apollo Computer, they received a significant boost from DARPA-sponsored research. DARPA supported the work of Forest Baskett, a specialist in computer architecture at Stanford University, who submitted a proposal to DARPA to create the Stanford University Network (SUN). As part of this effort, he planned to build a powerful single-user workstation, combining a 32-bit microprocessor (like Motorola's new 68000) and a wide-screen display. Baskett set Andreas Bechtolsheim to work on the hardware. He also interacted with James Clark, whose work on a high-speed graphics engine Baskett saw as critical to scientific and engineering applications of the system. The prototype SUN workstation was successfully demonstrated in 1981.\n\nDARPA and Stanford University encouraged Bechtolsheim to commercialize the workstation, which he originally did through a company called VLSI Systems, which was to produce the workstation boards for other computer manufacturers. After reviewing proposals from potential computer manufacturers and seeing Apollo announce its own workstation, however, Bechtolsheim realized he would have to move quickly and design his own machines. Key to his plan was using Unix, recently expanded by Bill Joy at UC-Berkeley under another DARPA VLSI contract to enhance its multitasking, multiuser, and networking capabilities. With help from Vinod Khosla and Scott McNealy (both Stanford University MBAs), Bechtolsheim was able to solicit Joy's participation and attract needed venture capital. The team established Sun Microsystems, Inc., in February 1982, and its first product was launched in 1983.1 DARPA extended funds to a number of academic institutions to allow them to purchase workstations for institutional users and networks. Such purchases accounted for 80 percent of Sun Microsystems' sales in its first year of business.2 Since then, Sun has become a major force in the computing industry as both a manufacturer of computer workstations and the developer of the Java programming language.\n\nRISC\n\nReduced instruction set computing (RISC) computers promised significant gains in performance by optimizing the flow of instructions through the processing unit.3 Although pioneering work on RISC architectures was conducted by IBM as part of its 801 computer, IBM did not move quickly to commercialize the technology for fear that it would detract from burgeoning sales of its mainframe computers; nor was such\n\nwork well publicized, although its existence became known in academic research circles (Hennessy and Patterson, 1990).4 DARPA sponsored two university-based programs to develop RISC as a workable technology under VLSI: one, led by David Patterson at UC-Berkeley, developed the RISC I and RISC II architectures; the other, led by John Hennessy at Stanford University, resulted in the MIPS architecture. Both were general-purpose designs aimed at achieving more efficient interaction between computational, storage, and communications units within a device structure by employing pipelined architectures and processors closely linked with memory and communication circuits.\n\nBoth designs were adopted rapidly by industry. The newly formed Sun Microsystems, Inc., licensed the RISC II architecture from the University of California and hired Patterson as a consultant to help develop the scalable processor architecture, a RISC-based design that it subsequently incorporated into its workstations. This technology enabled Sun to fend off growing competition from companies such as Digital Equipment Corporation, Hewlett-Packard, and Steve Jobs' NeXT Corporation, which were planning their own entries into the workstation market. Hennessy and his colleagues at Stanford University founded MIPS Computer Systems to commercialize their RISC architecture. The company licensed five major chip producers to produce devices based on the technology and five other companies to use the MIPS architecture in their own computers. MIPS Computer Systems was subsequently purchased by Silicon Graphics, Inc. (SGI), although SGI is currently spinning off the company.\n\nOther Architecture Projects\n\nThe VLSI program supported research on a number of innovative computer architectures other than RISC. Most of this work centered on designs for parallel computers. A range of projects supported a variety of configurations for linking microprocessors and memory, from the connection machine to the cube machines for general-purpose computing and the WARP architectures for special-purpose applications, such as signal processing. Several of these approaches were commercialized through start-up companies, such as Thinking Machines Corporation, or established firms, such as Intel Corporation. Although successful technologically, many of these designs failed to achieve commercial success.\n\nMOSIS\n\nDARPA also worked to establish ongoing technical and human infrastructure for VLSI. Of note was establishment of the Metal Oxide Silicon Implementation Service (MOSIS). Based on the innovative MultiProject Chip (MPC) service created by Lynn Conway at Xerox PARC (Conway, 1981), MOSIS provided university researchers with a means of quickly manufacturing limited numbers of custom or semicustom microelectronic devices at reasonable cost. New designs could be implemented in silicon within 4 to 10 weeks (less than the duration of an academic term). Prior to MOSIS (and the original MPC service), academic researchers had few economical ways of implementing and testing new semiconductor designs, few universities could afford their own fabrication lines, and the proliferation of different commercial systems of rules for specifying semiconductor circuit designsâ€”most of which were kept proprietaryâ€”made collaboration between universities and industry difficult. With\n\nMOSIS, researchers could submit designs for fabrication in a standardized format through the ARPANET or, subsequently, e-mail. Requests from different researchers were pooled into common lots and run through the fabrication process, after which completed chips were returned to the researchers. This system obviated the need for direct access to a fabrication line or for dealing with the complexity of arranging fabrication time at an industrial facility, by providing access to a qualified group of fabrication facilities through a single interface.\n\nMOSIS was widely used by the academic research community and contributed to many novel systems. Access to MOSIS was originally limited to the VLSI research community and other Department of Defense contractors who linked to it through the ARPANET. After the National Science Foundation (NSF) assumed responsibility for administering MOSIS in 1982, access was expanded to include NSF-sponsored researchers and affiliated educational institutions. In 1984, access was expanded to other qualified users as well. Altogether, MOSIS was used by researchers at more than 360 institutions by 1989. The number of projects run through MOSIS increased from 258 in 1981 to 1,880 in 1989. RISC-based designs, such as RISC I, RISC II, and MIPS, and the geometry engine later commercialized by SGI were all run through MOSIS during their early design and testing phases. Prominent VLSI researcher Charles Seitz commented that MOSIS represented the first period since the pioneering work of Eckert and Mauchley on the ENIAC in the late 1940s that universities and small companies had access to state-of-the-art digital technology.5\n\nDesign Tools\n\nDARPA also supported development of tools for designing VLSI devices. In 1978 and 1979, DARPA funded development of a program for step-level improvement in the layout of microelectronic devices. The result was Caesar, an interactive VLSI layout editor that was written in C, enabling it to run on VAX computers using the Berkeley version of Unix developed by Bill Joy. Caesar produced CalTech intermediate form files for use with the MOSIS system and was used to develop the RISC I, RISC II, and MIPS designs. Further modification made the tool suitable for more widespread use. A later, more advanced design technology created at UC-Berkeley, Magic, became even more widely used and formed the basis for several computer-assisted design systems, including those by VLSI Technology, Cadence, Valid Logic, Daisy, Mentor Graphics, and Viewlogic."
    }
}
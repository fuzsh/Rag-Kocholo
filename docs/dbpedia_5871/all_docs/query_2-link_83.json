{
    "id": "dbpedia_5871_2",
    "rank": 83,
    "data": {
        "url": "https://www.ias.edu/math/csdm/03-04/abstracts",
        "read_more_link": "",
        "language": "en",
        "title": "abstracts",
        "top_image": "https://www.ias.edu/themes/custom/veblen/images/favicons/favicon-16x16.png",
        "meta_img": "https://www.ias.edu/themes/custom/veblen/images/favicons/favicon-16x16.png",
        "images": [],
        "movies": [],
        "keywords": [],
        "meta_keywords": [
            ""
        ],
        "tags": null,
        "authors": [],
        "publish_date": "2008-09-04T16:59:52-04:00",
        "summary": "",
        "meta_description": "",
        "meta_lang": "en",
        "meta_favicon": "/themes/custom/veblen/images/favicons/favicon.ico",
        "meta_site_name": "Institute for Advanced Study",
        "canonical_link": "https://www.ias.edu/math/csdm/03-04/abstracts",
        "text": "Monday, October 27, 2003\n\nNicholas Pippenger, Princeton University\n\nProbability Theory and Covering Problems\n\nAbstract: Many of the combinatorial problems arising in the theory of computation can be expressed as covering problems: given a collection of sets of points, how few sets can be chosen to cover all the points? One way to obtain bounds for such problems is to consider choosing the sets at random. Since its introduction in the 1950s, this method has grown steadily in sophistication through the addition of various auxiliary probabilistic techniques. The goal of this talk is to survey this development through the presentation of some illuminating examples.\n\nMonday, November 3, 2003\n\nScott Aaronson, University of California Berkeley\n\nMultilinear Formulas and Skepticism of Quantum Computing\n\nAbstract: Several researchers, including Leonid Levin, Gerard 't Hooft, and Stephen Wolfram, have argued that quantum mechanics will break down before the factoring of large numbers becomes possible. If this is true, then there should be a natural \"Sure/Shor separator\" -- that is, a set of quantum states that can account for all experiments performed to date, but not for Shor's factoring algorithm. We propose as a candidate the set of states expressible by a polynomial number of additions and tensor products. Using a recent lower bound on multilinear formula size due to Raz, we then show that states arising in quantum error-correction require n^{Omega(log n)} additions and tensor products even to approximate, which incidentally yields the first superpolynomial gap between general and multilinear formulas. More broadly, we introduce a complexity classification of pure quantum states, and prove many basic facts about this classification. Our goal is to refine vague ideas about a breakdown of quantum mechanics into specific hypotheses that might be experimentally testable in the near future.\n\nTuesday, November 4, 2003\n\nRussell Impagliazzo, IAS\n\nPriority Algorithms: Greedy Graph Algorithms, and Beyond\n\nAbstract: Borodin, Nielsen, and Rackoff introduced the notion of priority algorithms. The priority algorithm model is an abstract model which captures the intrinsic power and limitations of greedy algorithms. The original paper was limited to scheduling problems; but subsequent work extended the model to other domains. We generalize their notion to general optimization problems, and apply it, in particular, to graph problems. We characterize the best performance of algorithms in each model in terms of a combinatorial game between a Solver and an Adversary.\n\nWe prove bounds on the approximation ratio achievable by such algorithms for basic graph problems such as shortest path, metric Steiner trees, independent set and vertex cover. For example, we show no fixed priority algorithm can achieve any approximation ratio (even a function of the graph size). In contrast, the well-known Dijkstra's algorithm shows that an adaptive priority algorithm can find optimal paths. We prove that the approximation ratio for vertex cover achievable by adaptive priority algorithms is exactly 2, matching the known upper bound.\n\nThe above is joint work with Sashka Davis. The priority model, in addition to capturing the limits of greedy methods for approximation algorithms, can be used as a starting point to address the limits of standard algorithmic techniques in a variety of settings.\n\nWe will also discuss how to extend the priority framework to model back-tracking and dynamic programming algorithms. We'll also give priority models for $k$-SAT. We can then pose some open problems: for what densities of random SAT formulas can greedy heuristics find solutions? (Note that most of the lower bounds for the threshold for satisfiability involve analyzing such heuristics.) How well do standard DPLL-like techniques work on satisfiable instances?\n\nThis is work in progress with Angelopolous, Beame, Borodin, Buresh-Oppenheim, Davis, Pitassi, and whoever else we can get interested in it.\n\nMonday, November 10, 2003\n\nErik Demaine, MIT\n\nFolding and Unfolding in Computational Geometry\n\nAbstract: When can a linkage of rigid bars be untangled or folded into a desired configuration? What polyhedra can be cut along their surface and unfolded into a flat piece of paper without overlap? What shapes can result by folding a piece of paper flat and making one complete straight cut? Folding and unfolding is a branch of discrete and computational geometry that addresses these and many other intriguing questions. I will give a taste of the many results that have been proved in the past few years, as well as the several exciting open problems that remain open. Many folding problems have applications in areas including manufacturing, robotics, graphics, and protein folding.\n\nTuesday, November 11, 2003\n\nDorit Aharonov, Hebrew University\n\nApproximating the Shortest and Closest Vector in a Lattice to within Sqrt(n) Lie in NP Intersect coNP\n\nAbstract: I will describe this new result with Oded Regev. The result (almost) subsumes the three mutually-incomparable previous results regarding these lattice problems: Lagarias, Lenstra and Schnorr [1990], Goldreich and Goldwasser [2000], and Aharonov and Regev [2003]. Our technique is based on a simple fact regarding succinct approximation of functions using their Fourier transform over the lattice. This technique might be useful elsewhere--I will demonstrate this by giving a simple and efficient algorithm for one other lattice problem (CVPP) improving on previous results.\n\nMonday, November 17, 2003\n\nClaire Kenyon, ï¿½cole Polytechnique and Institut Universitaire de France\n\nApproximation Algorithms for Packing\n\nAbstract: This talk will discuss design and analysis techniques for bin-packing type problems. We will review classical bin-packing algorithms, focusing on average-case analysis, and present the \"Sum-of-Squares\" algorithm, a simple online algorithm with great average-case performance. In terms of worst-case performance, we will recall the classical approximation scheme of de la Vega and Lueker and explore how it can be extended to several higher-dimensional settings, and in particular present an asymptotic (i.e., as OPT/(maximum rectangle height) goes to infinity) approximation scheme for dynamic storage allocation.\n\nTuesday, November 18, 2003\n\nMikhail Alekhnovitch, IAS\n\nJoint with Edward A. Hirsch and Dmitry Itsykson\n\nExponential Lower Bounds for the Running Time of DPLL Algorithms on Satisfiable Formulas\n\nAbstract: DPLL (for \\emph{Davis}, \\emph{Putnam}, \\emph{Logemann}, and \\emph{Loveland}) algorithms form the largest family of contemporary algorithms for SAT (the propositional satisfiability problem) and are widely used in applications. The recursion trees of DPLL algorithm executions on unsatisfiable formulas are equivalent to tree-like resolution proofs. Therefore, lower bounds for tree-like resolution (which are known since 1960s) apply to them.\n\nHowever, these lower bounds say nothing about the behavior of such algorithms on satisfiable formulas. Proving exponential lower bounds for them in the most general setting would be equivalent to proving $\\mathbf{P}\\neq\\mathbf{NP}$. In this paper, we give exponential lower bounds for two families of DPLL algorithms: generalized \"myopic\" algorithms (that read upto $n^{1-\\epsilon}$ of clauses at each step and see the remaining part of the formula without negations) and \"drunk\" algorithms (that choose a variable using any complicated rule and then pick its value at random).\n\nMonday, November 24, 2003\n\nAdam Kalai, TTI\n\nBoosting in the Presence of Noise\n\nAbstract: Boosting algorithms are procedures that \"boost\" low-accuracy weak learning algorithms to achieve arbitrarily high accuracy. Over the past decade, boosting has been widely used in practice and has become a major research topic in computational learning theory. One of the difficulties associated with boosting in practice is noisy data. We study boosting in the presence of random classification noise, giving an algorithm and a matching lower bound.\n\nThis is joint work with Rocco Servedio.\n\nTuesday, November 25, 2003\n\nOmer Reingold, AT & T and IAS\n\nPCP Testers: Towards a Cominatorial Proof of the PCP Theorem\n\nAbstract: In this work we look back into the proof of the PCP theorem, with the goal of finding new proofs that are either simpler or \"more combinatorial\". For that we introduce the notion of a PCP-Tester. This natural object is a strengthening of the standard PCP verifier, and enables simpler composition that is truly modular (i.e. one can compose two testers without any assumptions on how they are constructed, as opposed to the current proof in which one has to construct the two PCPs with respect to \"compatible encoding\" of their variables). Based on this notion, we present two main results:\n\n1. The first is a new proof of the PCP theorem. This proof relies on a very weak PCP given as a \"black box\". From this, we construct combinatorially the full PCP, relying on composition and a new combinatorial aggregation technique.\n\n2. Our second construction is a \"standalone\" combinatorial construction showing NP subset PCP[polylog, 1]. This implies, for example, that max-SAT is quasi-NP-hard.\n\nJoint work with Irit Dinur\n\nMonday, December 1, 2003\n\nSanjeev Arora, Princeton University\n\nExpander Flows and a sqrt{log n}-Approximation for Graph Expansion/Sparsest Cut\n\nAbstract: In graph separation problems such as EXPANSION, BALANCED CUT etc., the goal is to divide the graph into two roughly equal halves so as to minimize the number of edges in this cut. They arise in a variety of settings, including divide-and- conquer algorithms and analysis of Markov chains. Finding optimal solutions is NP-hard.\n\nClassical algorithms for these problems include eigenvalue approaches (Alon and Millman 1985) and multicommodity flows (Leighton and Rao 1988). The latter yields a O(log n)-approximation, and it has been an open problem to improve this ratio. We describe a new O(\\sqrt{log n})-approximation algorithm.\n\nThe algorithm relies on semidefinite relaxations that use the triangle inequality constraints. Analysing these relaxations has been an important open problem as well and our work may be seen as significant progress in that direction.\n\nWe also introduce the notion of {\\em expander flows}, which constitute an interesting ``certificate'' of a graph's expansion. We use them to prove a surprising new theorem about graph embeddings:\n\nfor every n-node graph it is possible to embed an n-node expander graph in it with appropriate dilation and congestion. We think this embedding theorem may have other applications.\n\nOur techniques are an interesting mix of graph theory and high-dimensional geometry. The talk will be self-contained.\n\n(Joint work with Satish Rao and Umesh Vazirani)\n\nTuesday, December 2, 2003\n\nAvi Wigderson, IAS\n\nDerandomized \"low degree\" tests via \"epsilon-biased\" sets, with Applications\n\nto short Locally Testable Codes and PCPs\n\nAbstract: We present the first explicit construction of Probabilistically Checkable Proofs (PCPs) and Locally Testable Codes (LTCs) of fixed constant query complexity which have almost-linear size. Such objects were recently shown to exist (nonconstructively) by Goldreich and Sudan.\n\nThe key to these constructions is a nearly optimal randomness-efficient version of the Rubinfeld-Sudan low degree test. The original test uses a random line in the given vector space. The number of such lines is quadratic in the size of the space, which implied a similar blow up in previous constructions of LTCs. Goldreich and Sudan showed that there exists a nearly linear sized sample space of lines such that running the low-degree test on a random line from this collection is a good test. We give an explicit sample space with this property.\n\nIn a similar way we give a randomness-efficient version of the Blum-Rubinfeld-Sudan linearity test (which is used, for instance, in locally testing the Hadamard code).\n\nBoth derandomizations are obtained through epsilon-biased sets for vector spaces over finite fields.The sample space consists of the lines defined by the edges of the Cayley expander graph generated by the epsilon-biased set.\n\nThe analysis of the derandomized tests rely on alternative views of epsilon-biased sets --- as generating sets of Cayley expander graphs for the low degree test, and as defining good linear error-correcting codes for the linearity test.\n\nJoint work with Eli Ben-Sasson, Madhu Sudan and Salil Vadhan\n\nMonday, December 8, 2003\n\nValentine Kabanets, Simon Fraser University\n\nComplexity of Succinct Zero-sum Games\n\nAbstract: We study the complexity of solving succinct zero-sum games, i.e., the games whose payoff matrix M is given implicitly by a Boolean circuit C such that M(i,j)=C(i,j). We complement the known EXP-hardness of computing the exact value of a succinct zero-sum game by several results on approximating the value.\n\nWe prove that approximating the value of a succinct zero-sum game to within an additive factor is complete for the class promise-S_2^p, the ``promise'' version of S_2^p. To the best of our knowledge, it is the first natural problem shown complete for this class.\n\nWe describe a ZPP^{NP} algorithm for constructing approximately optimal strategies, and hence for approximating the value, of a given succinct zero-sum game. As a corollary, we obtain, in a uniform fashion, several complexity-theoretic results, e.g., a ZPP^{NP} algorithm for learning circuits for SAT~\\cite{BCGKT96} and a recent result by Cai~\\cite{Cai01} that S_2^p\\subseteq ZPP^{NP}.\n\nJoint work with Lance Fortnow, Russell Impagliazzo, and Chris Umans.\n\nTuesday, December 9, 2003\n\nRyan O'Donnell, IAS\n\nLearning Mixtures of Product Distributions\n\nAbstract: In this talk we give a general method for learning mixtures of unknown product distributions on R^n. In particular we give:\n\nA (weakly) polynomial time algorithm for learning a mixture of any constant number of axis-aligned Gaussians in R^n (the Gaussians need not be spherical). Our algorithm constructs a highly accurate approximation to the unknown mixture of Gaussians, and unlike previous algorithms, makes no assumptions about the minimum separation between the centers of the Gaussians.\n\nA poly(n) time algorithm for learning a mixture of any constant number of product distributions over the boolean cube {0,1}^n. Previous efficient algorithms could only learn a mixture of two such product distributions. We also give evidence that no polynomial time algorithm can learn a mixutre of a superconstant number of such distributions.\n\nThis is joint work with Jon Feldman and Rocco Servedio of Columbia University.\n\nMonday, January 19, 2004\n\nThomas Hayes, Toyota Technological Institute, Chicago\n\nRandomly Sampling Graph Colorings\n\nAbstract: For a given graph $G$, and a positive integer $k$, we consider the problem of sampling proper $k$-colorings of $G$ almost-uniformly at random. When $k$ is larger than the maximum degree of $G$, there is a greedy algorithm for _constructing_ such a coloring in linear time. However, even with this many colors, it is not known whether colorings can be sampled nearly uniformly in polynomial time.\n\nWe recently showed that, assuming $G$ has high girth and high maximum degree, that when $k > (1+\\epsilon) \\times$max degree, $k$-colorings can be sampled efficiently. The factor $(1+\\epsilon)$ improves the previously best known factor 1.489..., which also required similar assumptions on the graph. The best known factor which does not require any assumptions on the graph is 11/6.\n\nThe coloring algorithm we study is a very simple Markov chain on the space of proper graph colorings. A main focus of this talk will be new extensions to the coupling technique for proving rapid \"mixing\" of such chains. This is joint work with Eric Vigoda, of Univ. of Chicago.\n\nTuesday, January 20, 2004\n\nRan Raz, Weizmann Institute\n\nMulti-Linear Formulas for Permanent and Determinant are of Super-Polynomial Size\n\nAbstract: Arithmetic formulas for computing the determinant and the permanent of a matrix have been studied since the 19th century. Are there polynomial size formulas for these functions ? Although the determinant and the permanent are among the most extensively studied computational problems, polynomial size formulas for these functions are not known. An outstanding open problem in complexity theory is to prove that polynomial size formulas for these functions do not exist. Note, however, that super-polynomial lower bounds for the size of arithmetic formulas are not known for any explicit function and that questions of this type are considered to be among the most challenging open problems in theoretical computer science. I will talk about a recent solution of this problem for the subclass of multi-linear formulas.\n\nAn arithmetic formula is multi-linear if the polynomial computed by each of its sub-formulas is multi-linear, that is, in each of its monomials the power of every input variable is at most one. Multi-linear formulas are restricted, as they do not allow the intermediate use of higher powers of variables in order to finally compute a certain multi-linear function. Note, however, that for many multi-linear functions, formulas that are not multi-linear are very counter-intuitive, as they require a \"magical\" cancellation of all high powers of variables. Note also that both the determinant and the permanent are multi-linear functions and that many of the well known formulas for these functions are multi-linear formulas.\n\nWe prove that any multi-linear arithmetic formula for the determinant or the permanent of an $n$ dimensional matrix is of size super-polynomial in $n$. Previously, no lower bound was known (for any explicit function) even for the special case of multi-linear formulas of constant depth.\n\nMonday, January 26, 2004\n\nMario Szegedy, Rutgers University\n\nSpectra of Quantized Walks and a $sqrt{\\delta\\epsilon}$-Rule\n\nAbstract: We introduce quantized bipartite walks, compute their spectra, generalize the algorithms of Grover and Ambainis and interpret them as quantum walks with memory. We compare the performance of walk based classical and quantum algorithms and show that the latter run much quicker in general. Let $P$ be a symmetric Markov chain with transition probabilities $P[i,j]$, $(i ,j\\in [n])$. Some elements of the state space are marked. We are promised that the set of marked elements has size either zero or at least $\\epsilon n$. The goal is to find out with great certainty which of the above two cases holds. Our model is a black box that can answer certain yes/no questions and can generate random elements picked from certain distributions. More specifically, by request the black box can give us a uniformly distributed random element for the cost of $\\wp_{0}$. Also, when ``inserting'' an element $i$ into the black box we can obtain a random element $j$, where $j$ is distributed according to $P[i,j]$. The cost of the latter operation is $\\wp_{1}$. Finally, we can use the black box to test if an element $i$ is marked, and this costs us $\\wp_{2}$. If $\\delta$ is the eigenvalue gap of $P$, there is a simple classical algorithm with cost $O(\\wp_{0} + (\\wp_{1}+\\wp_{2})/\\delta\\epsilon)$ that solves the above promise problem. (The algorithm is efficient if $\\wp_{0}$ is much larger than $\\wp_{1}+\\wp_{2}$.) In contrast, we show that for the ``quantized'' version of the algorithm it costs only $O(\\wp_{0} + (\\wp_{1}+\\wp_{2})/\\sqrt{\\delta\\epsilon})$ to solve the problem. We refer to this as the $\\sqrt{\\delta\\epsilon}$ rule. Among the technical contributions we give a formula for the spectrum of the product of two general reflections.\n\nTuesday, January 27, 2004\n\nJames R. Lee, Berkeley University\n\nMetric Decomposition: Coping with Boundaries\n\nAbstract: In recent years, \"stochastic\" metric decomposition has become a significant tool in the study of discrete metric spaces. In this talk, I will survey the emerging structural theory, with an eye toward applications in mathematics and computer science.\n\nIn particular, I will describe some recent work with Assaf Naor on a classical problem in geometry and analysis, that of extending Lipschitz functions. This shows that metric decomposition also yields important insights in the continuous setting.\n\nIn the finite setting, I will sketch a new proof of Bougain's embedding theorem based on metric decomposition and a technique we call \"measured descent,\" which answers some open problems in the field and lends new insights. (Joint work with R. Krauthgamer, M. Mendel, and A. Naor; a more detailed exposition of this result will be given at Princeton during the preceding week, but I think it is instructive to see a sketch in the broader context.)\n\nMonday, February 2, 2004\n\nAner Shalev, Hebrew University\n\nProbabilistic Generation of Finite Simple Groups, Random Walks, Fuchsian Groups and\n\nAbstract: We use character theory and probabilistic methods to solve several seemingly unrelated problems of combinatorial and geometric flavor involving finite and infinite groups. These include random generation of finite simple groups, determining the mixing time of certain random walks on symmetric groups and matrix groups, finding the subgroup growth of Fuchsian groups, and giving a probabilistic proof to a conjecture of Higman on their finite quotients. If time allows I will also discuss further applications to representation varieties, and to counting branched coverings of Riemann surfaces. A main tool in the proofs is the study of the so called Witten's zeta function encoding the character degrees of certain groups.\n\nTuesday, February 3, 2004\n\nNoga Alon, Tel-Aviv University\n\nCutNorm, Grothendieck's Inequality, and Approximation Algorithms for Dense Graphs\n\nAbstract: The cut-norm of a real matrix A is the maximum absolute value of the sum of all elements in a submatrix of it. This concept plays a major role in the design of efficient approximation algorithms for dense graph and matrix problems. After briefly explaining this role I will show that the problem of approximating the cut-norm of a given real matrix is computationally hard, and describe an efficient approximation algorithm. This algorithm finds, for a given matrix A, a submatrix A' so that the absolute value of the sum of all entries of A' is at least c times the cut-norm of A, where c > 0.56. The algorithm combines semidefinite programming with a novel rounding technique based on Grothendieck's Inequality.\n\nJoint work with Assaf Naor.\n\nMonday, February 9, 2004\n\nNathan Segerlind, IAS\n\nUsing Hypergraph Homomorphisms to Guess Three Secrets\n\nAbstract: The problem of ``guessing k secrets'' is the following two-player game between an adversary and a seeker: The adversary has k binary strings and the seeker wishes to learn as much as he can by asking boolean questions about strings. For each question, the adversary chooses one of his secret strings and provides an answer for that particular string. In particular, if the seeker asks a question such that the adversary has a string for which the answer YES and a string for which the answer is NO, the adversary may provide either answer. This problem was introduced by Chung, Graham and Leighton to model difficulties encountered in internet routing by Akamai.\n\nWe present the first polynomial-time strategy for solving the problem for guessing three secrets (previous work had solved only the case for two secrets). We give both adaptive and oblivious strategies. The queries of the oblivious strategy are built from a special separating system that we call a prefix-separating system. The algorithm for recovering the solution from the queries is based on the homomorphism structure of three-uniform, intersecting hypergraphs. Along the way we prove some combinatorial facts about three-uniform, intersecting hypergraph cores that may be of independent interest (in this setting a core is a hypergraph that admits no proper endomorphism).\n\nTuesday, February 10, 2004\n\nPeter Winkler, Bell Labs and IAS\n\nSome Vexing Combinatorial and Mixing Problems\n\nAbstract: Included: Euler tours, Thorp shuffles, particles in space, and a conjecture motivated by optical networking which generalizes theorems of two Halls.\n\nMonday, February 16, 2004\n\nChristos Papadimitriou, University California Berkeley\n\nNash Equilibria and Complexity\n\nAbstract: Using the Nash equilibrium problem as a departure point, we explore the intricate and largely mysterious interplay between computational complexity and existence proofs in combinatorics. We present polynomial-time algorithms and complexity results for congestion games.\n\nTuesday, February 17, 2004\n\nMaria Chudnovsky, Princeton, CMI and IAS\n\nThe Structure of Clawfree Graphs\n\nAbstract: A graph is said to be clawfree if it has no induced subgraph isomorphic to $K_{1,3}$. Line graphs are one well-known class of clawfree graphs, but there others, such as circular arc graphs and subgraphs of the Schl\\\"{a}fli graph. It has been an open question to describe the structure of all clawfree graphs. Recently, in joint work with Paul Seymour, we were able to prove that all clawfree graphs can be constructed from basic pieces (which include the graphs mentioned above, as well as a few other ones) by gluing them together in prescribed ways. This talk will survey the main ideas of the proof, as well as some examples of claw-free graphs that turned out to be important in the course of this work, and some applications.\n\nWednesday, February 18, 2004\n\nRoy Meshulam, Technion, Haifa\n\nLaplacians, Homology and Hypergraph Matching\n\nAbstract: We'll discuss some relations between the expansion of a graph and the topology of certain complexes associated with the graph. Applications include a lower bound on the homological connectivity of the independence complex, in terms of a new graph parameter defined via certain vector representations of the graph. This in turn implies Hall type theorems for matchings in hypergraphs. Joint work with R. Aharoni and E. Berger.\n\nMonday, February 23 2004\n\nRavi Kumar, IBM Almaden Reseaerch Center\n\nOn Separating Nondeterminism and Randomization inCommunication Complexity\n\nAbstract: In the two-party communication complexity model, we show that the tribes function on n inputs has two-sided error randomized complexity Omega(n), while its nondeterminstic complexity and co-nondeterministic complexity are both Theta(sqrt(n)). This separation between randomized and nondeterministic complexity is the best possible and it settles an open problem posed by Beame--Lawry and Kushilevitz--Nisan.\n\nOur lower bound is obtained using generalizations of information complexity, which quantifies the minimum amount of information that will have to be revealed about the inputs by every correct communication protocol.\n\n(Joint work with T. S. Jayram and D. Sivakumar)\n\nMonday, February 23, 2004\n\nMark Braverman, University of Toronto\n\nOn the Computability of Julia Sets\n\nAbstract: While the computer is a discrete device, it is often used to solve problems of a continuous nature. The field of Real Computation addresses the issues of computability in the continuous setting. We will discuss different models of computation for subsets of R^n. The main definition we use has a computer graphics interpretation (in the case n=2), as well as a deeper mathematical meaning. The Julia sets are particularly well studied sets arising from complex dynamics. In the talk we will present the basic facts about Julia sets and some computability results for them. Our computability results come in contrast to the Julia sets noncomputability results presented by Blum/Cucker/Shub/Smale. This discrepancy follows from the fact that we are using a different computability model.\n\nTuesday, February 24, 2004\n\nStephen Cook, University of Toronto\n\nMaking Sense of Bounded Arithmetic\n\nAbstract: We present a unified treatment of logical theories for each of the major complexity classes between AC0 and P, and give simple translations into the quantified propositional calculus.\n\nMonday, March 1, 2004\n\nYonatan Bilu, Hebrew University\n\nQuasi-Ramanujan 2-lifts - A New Construction of Expander Graphs\n\nAbstract: The adjacency matrix of a graph on n vertices is a 0-1 nxn matrix, with the (i,j) entry being 1, iff there is an edge between vertices i and j. Often, understanding the eigenvalues of the adjacency matrix sheds light on combinatorial properties of a graph. In a d-regular graph, the largest eigenvalue is d. If the absolute value all other eigenvalues is small, we say that such a graph is an expander. Optimal constructions of such graphs are known, but they rely on group representation theory. In this work we describe a simple, combinatorial construction that is close to being optimal. A useful property of expander graphs is the so-called Expander Mixing Lemma, which bounds the deviation of the number of edges between two sets of vertices from what is expected in a random graph. We show a converse to this lemma, namely, that if the deviation is small then the graph is an expander. The implication is surprisingly strong, in comparison to other combinatorial properties (edge expansion, vertex expansion) which also imply a bound on the eigenvalues.\n\nThe key tool in the construction is a signing of the edges of a d-regular graph by +1 and -1. This yields a 2-lift of a graph - graph on twice as many vertices which is also d-regular. Getting an expander graph in this way reduces to finding a signing of a graph such that the spectral radius of the signed adjacency matrix is small. We show that for all d-regular graphs such a signing exists, and that if the graph we start from is an expander, then such a signing can be found efficiently. This is joint work with Nati Linial.\n\nTuesday, March 2, 2004\n\nToniann Pitassi, IAS\n\nOn a Model for Backtracking\n\nAbstract: Most known efficient algorithms fit into a few basic paradigms: divide-and-conquer, dynamic programming, greedy algorithms, hill-climbing, and linear programming. There has been a growing interest in trying to formalize precise models for these and other algorithmic paradigms, most notably in the context of approximation algorithms. For example, models of local search algorithms have been studied by several researchers; more recently, Borodin, Nielsen and Rackoff introduce priority algorithms to model greedy algorithms, and Arora, Bollobas and Lovasz provide integrality gaps for a wide class of LP formulations.\n\nWe continue in this direction by studying a new model for backtracking (BT). Informally, a BT algorithm is a levelled tree where each level corresponds to the probing of an input item and branches correspond to different possible irrevocable decisions that can be made about this item. The complexity of a BT algorithm is the number of nodes in the tree. We study several classic problems within this framework (knapsack, interval selection and satisfiability), and present a number of upper and lower bounds, and inapproximability results on the power of BT algorithms for these problems. Finally we discuss connections with other models (i.e. dynamic programming, DPLL), and many open problems.\n\nThis is work in progress with Allan Borodin, Russell Impagliazzo, Josh Buresh-Oppenheim, and Avner Magen.\n\nMonday, March 8, 2004\n\nAbraham Neyman, Institute of Mathematics, Hebrew University\n\nOnline Concealed Correlation by Boundedly Rational Players\n\nAbstract: Joint paper with Gilad Bavly (Hebrew University)\n\nIn a repeated game with perfect monitoring, correlation among a group of players may evolve in the common course of play (called, online correlation). Such a correlation may be `concealed' from a boundedly rational player. We show that ``strong'' players, i.e., players whose strategic complexity is less stringently bounded, can orchestrate online correlation of the actions of ``weak'' players, in a manner that is concealed from an opponent of ``intermediate'' strength.\n\nThe result is illustrated in two models, each captures another aspect of bounded rationality. In the first, players use bounded recall strategies. In the second, players use strategies that are implementable by finite automata.\n\nTuesday, March 9, 2004\n\nBoaz Barak, IAS\n\nExtracting Randomness from Few Independent Sources\n\nAbstract: We consider the problem of extracting truly random bits from several independent sources of data that contains entropy. The best previously known explicit constructions extracted randomness from two independent samples of distributions over $\\bits^n$ such that each has min-entropy at least $n/2$. The optimal, non-explicit construction only requires the min-entropy to be more than $\\log n$.\n\nIn this work, we manage to go beyond this $n/2$ ``barrier'' and give an explicit construction for extracting randomness from distributions over $\\bits^n$ with $\\delta n$ entropy for every constant $\\delta>0$. The number of samples we require is a constant (depending polynomially on $1/\\delta$).\n\nOur main tools are results from additive number theory and in particular a recent result by Bourgain, Katz and Tao (GAFA, to appear).\n\nWe also consider the related problem of constructing randomness dispersers, and construct an almost optimal disperser that requires the input distribution only to have min-entropy at least $\\Omega(\\log n)$, with the caveat that all the samples have to come from the \\emph{same} distribution. The main tool we use is a variant of the ``stepping-up lemma'' used in establishing lower bound on the Ramsey number for hypergraphs (Erdos and Hajnal, 71).\n\nJoint work with Russell Impagliazzo and Avi Wigderson.\n\nMonday, March 15, 2004\n\nAmir Shpilka, The Weizmann Institute\n\nLocally Testable Cyclic Codes\n\nAbstract: Cyclic codes are codes which are invariant under a cyclic shift of their coordinates. Locally testable codes are, roughly, codes for which we can verify, by querying a small number of positions, whether a given word is in the code or far from being a code word. It is a long standing open problem in coding theory whether there exist good cyclic codes. It is a more recent question, rising from the study of probabilistically checkable proofs, whether there exist good locally testable codes. In this talk we address the intersection of the two problems and show that there are no good locally testable cyclic codes. In particular our results imply that for certain block lengths there are no good cyclic codes, without any local testability assumption. The techniques we use are algebraic in nature and rely on the beautiful connection between cyclic codes and cyclotomic polynomials. We will try to give as many details from the proof as possible. This is a joint work with Laci Babai and Daniel Stefankovic from the university of Chicago.\n\nTuesday, March 16, 2004\n\nSubhash Khot, IAS\n\nBCH Codes, Augmented Tensor Products and Hardness of theShortest Vector Problem in Lattices\n\nAbstract: The Shortest Vector Problem in lattices has been studied by mathematicians for two centuries. Given a basis for an n-dimensional lattice, the problem is to find the shortest non-zero vector in the lattice. The approximation version of the problem asks for a non-zero lattice vector that is guaranteed to be within a certain factor of the shortest vector.\n\nThere is a rich set of results associated with SVP. For example,\n\nGauss' algorithm that works for 2-dimensional lattices.\n\nMinkowski's Convex Body Theorem that shows existence of a short lattice vector.\n\nThe famous LLL algorithm of Lenstra, Lenstra and Lovasz that achieves 2^n approximation to SVP. It was improved to 2^{o(n)} by Schnorr. This algorithm has numerous applications in mathematics, computer science and cryptography.\n\nAjtai's reduction from worst-case hardness of approximating SVP to its average case hardness.\n\nAjtai-Dwork's public-key cryptosystem based on (conjectured) worst case hardness of approximating SVP. Also, a recent alternate construction by Regev.\n\nResults showing that a gap-version of SVP with factor n or \\sqrt{n} is \"unlikely\" to be NP-hard, e.g. Lagarias, Lenstra and Schnorr; Goldreich and Goldwasser; and recently Aharonov and Regev.\n\nHowever, there has been very little progress in actually proving hardness of approximation results for SVP. Even the NP-hardness of exact version of SVP came only in 1998 (by Ajtai). It was strengthened to a hardness of approximation result with factor \\sqrt{2} by Micciancio. This left a huge gap of \\sqrt{2} vs 2^{o(n)} between the best hardness result and the best algorithmic result.\n\nIn this talk, we greatly improve the hardness factor. We show that assuming NP \\not\\in BPP, there is no constant factor approximation for SVP (in polytime). Assuming NP \\not\\in BPTIME(2^{polylog n}}, we show that SVP has no approximation with factor 2^{(\\log n)^{1/2-\\eps}} where \\eps > 0 is an arbitrarily small constant. In our opinion, this gives evidence that there is no efficient algorithm that achieves polynomial factor approximaiton to SVP, a major open problem in algorithms.\n\nWe first give a new (randomized) reduction from Closest Vector Problem (CVP) to SVP that achieves *some* constant factor hardness. The reduction is based on BCH Codes. Its advantage is that the SVP instances produced by the reduction behave well under the augmented tensor product, a new variant of tensor product that we introduce. This enables us to boost the hardness factor to an arbitrarily large constant assuming NP \\not\\in BPP, and to factor 2^{(\\log n)^{1/2-\\epsilon}} assuming the stronger complexity assumption.\n\nMonday, March 22, 2004\n\nMoni Naor, Weizmann Institute of Science\n\nSpam and Pebbling\n\nAbstract: Consider the following simple technique for combating spam:\n\nIf I don't know you, and you want your e-mail to appear in my inbox, then you must attach to your message an easily verified \"proof of computational effort\", just for me and just for this message.\n\nTo apply this approach one needs to be able to come up with computational problems where solving them requires significant expenditure of resources while verifying a solution can be done easily. Recent work dealt with the choice of computational problems for which most of the work is in retrieving information from memory. In this talk I will describe this approach and describe the connection to pebbling problems.\n\nThe talk is based on two papers: Cynthia Dwork, Andrew Goldberg and Moni Naor: On Memory-Bound Functions for Fighting Spam. Cynthia Dwork, Moni Naor and Hoeteck Wee: work in progress\n\nTuesday, March 23, 2004\n\nManindra Agrawal, IAS\n\nEfficient Primality Testing\n\nAbstract: Since the discovery of polynomial time primality test, a number of improvements have been made to the original algorithm. Amongst the most notable are:\n\nAn O(log^{10.5} n) time algorithm with a completely elementary proof. This eliminates the dependence on a difficult analytic number theory lemma used in the original proof.\n\nAn O(log6 n) time deterministic algorithm. This matches the conjectured running time of the original algorithm.\n\nAn O(log4 n) time randomized algorithm that produces primality certificates. This is the fastest provable algorithm of its kind.\n\nIn this talk, I will discuss details of these algorithms and their proofs.\n\nMonday, March 29, 2004\n\nMike Capalbo, DIMACS\n\nGraph Products are (almost!) Practical\n\nAbstract: We are given an infinite family of expanders. We would like to use this family of expanders to construct routing networks with N inputs, N outputs, bounded degree, and O(N log N) edges, where the routing can be done in a distributed fashion in O( log N) time (using vertex-disjoint paths).\n\nIn the early 1990's, Pippenger devised a method to construct such routing networks from an arbitrary family of expanders. The drawback to this method is that the constants hidden under the O-notation are very, very large. Here, using a graph product, we present a new method to construct such routing networks, where the constants hidden behind the O-notation are much, much smaller, even using the simple Gabber-Galil expanders.\n\nTuesday, March 30, 2004\n\nAndris Ambainis, IAS\n\nSearch by Quantum Walks\n\nAbstract: I will present two new quantum algorithms:\n\nO(N^{2/3}) quantum algorithm for element distinctness;\n\nO(\\sqrt{N\\log N}) quantum algorithm for search on 2-dimensional grid.\n\nThe algorithms are based on using a quantum walk (a quantum process similar to a random walk) to search graphs. This improves over the standard quantum search by using the structure of a graph.\n\nThe talk will be self-contained and no previous knowledge of quantum computation will be assumed.\n\nThe second algorithm is a joint work with Julia Kempe and Alexander Rivosh.\n\nMonday, April 5, 2004\n\nVan Vu, University of California, Dan Diego\n\nA Near Optimal Bound on Erdos Distinct Distances in high Dimensions\n\nAbstract: One of the oldest and most well known problems of Erdos in discrete geometry is the following: what is the minimum number of distances between n points in R^d ? (here d is fixed and n tends to infinity)\n\nIn this talk, I will give a brief review about the history of the problem and discuss a recent joint result with Solymosi, which proves a near sharp bound in high dimensions. Our proof uses tools from theoretical computer science, in particular a cutting lemma by Chazelle et al.\n\nTuesday, April 6, 2004\n\nJan Krajicek, IAS\n\nStrong Proof Systems and Hard Tautologies\n\nAbstract: I shall discuss what we know about strong (propositional) proof systems, and describe a new method how to construct them. In particular, given a proof system P I define a possibly much stronger proof system iP. The system iP operates with an exponentially long P-proof described ``implicitly'' by a polynomial size circuit and with an ordinary P-proof of the \"correctness\" of the long proof. I will give some evidence that iP is indeed much stronger than P, discuss the iteration of the construction, and describe some applications in proof-complexity.\n\nAn issue important for potential lower bounds is to have \"reasonable\" candidates of tautologies that could be hard for strong proof systems. I recall some known facts and then show that implicit proof systems are relevant also in this context. In particular, I will show that lower bounds for proofs of implicit formulas in implicit versions of \"weak\" proof systems can give, in principle, lower bounds for ordinary proofs of ordinary formulas in \"strong\" proof systems.\n\nMonday, April 12, 2004\n\nBenny Sudakov, Princeton University\n\nSolving Extremal Problems Using Stability Approach\n\nAbstract: In this talk we discuss a \"stability approach\" for solving extremal problems. Roughly speaking, it can be described as follows. In order to show that given configuration is a unique optimum for an extremal problem, we first prove an approximate structure theorem for all constructions whose value is close to the optimum and then use this theorem to show that any imperfection in the structure must lead to a suboptimal configuration. To illustrate this strategy, we discuss three recent results in which stability approach was used to answer a question of Erdos-Rothschild and to resolve two conjectures of Sos and Frankl.\n\nAll the results in this talk are co-authored with P. Keevash and the first one is in addition co-authored with N. Alon and J. Balogh.\n\nTuesday, April 13, 2004\n\nAlexander Razborov, IAS\n\nGuessing More Secrets via List Decoding\n\nAbstract: We consider the following game introduced by Chung, Graham and Leighton. One player, A picks k>1 secrets from a universe of N possible secrets, and another player, B tries to gain as much information about this set as possible by asking binary questions f:{1,2,..,N}-->{0,1}. Upon receiving such a question f, A adversarially chooses one of her k secrets, and answers f according to it. For any constant number of secrets k we present an explicit set of $O(\\log N)$ questions along with an $O(\\log^2 N)$ recovery algorithm that achieve B's goal in this game (previously such algorithms were known only for k=2,3) thus solving one of the central open problems in this area. Our strategy is based on the list decoding of Reed-Solomon codes, and it extends and generalizes ideas introduced earlier by Alon, Guruswami, Kaufman and Sudan.\n\nMonday, April 19, 2004\n\nAndrew Yao, Princeton University\n\nSome Optimality Results in Bounded-Storage Cryptography\n\nAbstract: We study the amount of storage needed for two parties to agree on a secret key in the bounded-storage model proposed by Maurer. Assume that a public random string of length $n$ is available and any adversary has a storage of at most $\\nu n$ bits, for some constant $\\nu<1$. We show that to have a secure protocol, one of the legitimate parties must have a storage of $\\Omega(\\sqrt{n})$ bits. This can be generalized to the case where two parties can share a private key beforehand and then try to agree on a longer secret. We show that with a private key of length $r$, one of the legitimate parties now needs a storage of $\\Omega(\\sqrt{n/2^r})$ bits. Our lower bounds are optimal within constant factors as we have protocols with storage requirements matching these bounds. This is joint work with C.J. Lu and D.W. Wang.\n\nTuesday, April 20, 2004\n\nAndris Ambainis, IAS\n\nSearch by Quantum Walks II\n\nAbstract: I will continue my talk from Tuesday, March 30. I will present an O(\\sqrt{N\\log N}) quantum algorithm for spatial search and then describe the common mathematical structure behind the two algorithms (element distinctness, presented on March 30 and spatial search).\n\nMonday, April 26, 2004\n\nJon Kleinberg, Cornell University\n\nNetwork Failure Detection and Graph Connectivity\n\nAbstract: Measuring the properties of a large, unstructured network can be difficult: one may not have full knowledge of the network topology, and detailed global measurements may be infeasible. A valuable approach to such problems is to take measurements from selected locations within the network and then aggregate them to infer large-scale properties. One sees this notion applied in settings that range from Internet topology discovery tools to remote software agents that estimate the download times of popular Web pages. Some of the most basic questions about this type of approach, however, are largely unresolved at an analytical level. How reliable are the results? How much does the choice of measurement locations affect the aggregate information one infers about the network?\n\nWe describe algorithms that yield provable guarantees for a problem of this type: detecting a network failure. In particular, we provide methods for placing a small set of ``agents'' at nodes in a network, in such a way that any significant partition of the network will be detected by the separation of some pair of these agents. We find that the number of agents required can be bounded in terms of certain natural parameters of the failure, and independently of the size of the network itself. These bounds establish connections between graph separators and the notion of VC-dimension, employing results related to non-bipartite matchings and the disjoint paths problem. In recent joint work with Mark Sandler and Alex Slivkins we have obtained further improvements to these bounds in terms of the underlying edge-connectivity, making use of connections to the cactus representation of the minimum cuts in a graph.\n\nTuesday, April 27, 2004\n\nRyan O'Donnell, IAS\n\nOptimal Inapproximability Results for MAX-CUT and Other 2-Variable CSPs\n\nAbstract: In this paper we give evidence that it is hard to polynomial-time approximate MAX-CUT to within a factor of alpha_GW + eps, for all eps > 0. Here alpha_GW denotes the approximation ratio achieved by the Goemans-Williamson algorithm [GW95], alpha_GW = .878567. We show that the result follows from two conjectures: a) the Unique Games conjecture of Khot [Khot02]; and, b) a widely-believed Fourier-theoretic conjecture we call the Majority Is Stablest conjecture. Our results suggest that the naturally hard ``core'' of MAX-CUT is the set of instances in which the graph is embedded on a high-dimensional Euclidean sphere and the weight of an edge is given by the squared distance between the vertices it connects.\n\nThe same two conjectures also imply that it is hard to (beta + eps)-approximate MAX-2SAT, where beta = .943943 is the minimum of [2 + (2/pi) theta]/[3 - cos theta] on (pi/2, pi). Motivated by our proof techniques, we show that if the MAX-2CSP and MAX-2SAT problems are slightly restricted --- in a way that seems to retain all their hardness --- then they have polynomial-time (alpha_GW - eps)- and (beta - eps)-approximation algorithms, respectively.\n\nAlthough we are unable to prove the Majority Is Stablest conjecture, we give some partial results and indicate possible directions of attack. Our partial results are enough to imply that MAX-CUT is hard to (3/4 + 1/2pi + eps)-approximate (about .909155) assuming only the Unique Games conjecture.\n\nFinally, we discuss the possibility of equivalence between the Unique Games conjecture and the hardness of distinguishing (1 - eps)-satisfiable and eps-satisfiable instances of two-variable linear equations mod q, for large q.\n\nThis is joint work with Subhash Khot (IAS), Guy Kindler (DIMACS), and Elchanan Mossel (Berkeley).\n\nMonday, May 3, 2004\n\nSean Hallgren, NEC Research, Princeton\n\nFast Quantum Algorithms for Computing the Unit Group and Class Group of a Number Field\n\nAbstract: Computing the unit group and class group of a number field are two of the main tasks in computational algebraic number theory. Factoring integers reduces to a special case of computing the unit group, but a reduction in the other direction is not known and appears more difficult. We give polynomial-time quantum algorithms for computing the unit group and class group when the number field has constant degree.\n\nTuesday, May 4, 2004\n\nSubhash Khot, IAS\n\nRuling Out PTAS for Graph Min-Bisection\n\nAbstract: Graph Min-Bisection is the following problem : Given a graph, partition it into two equal parts so as to minimize the number of crossing edges. The problem arises as a subroutine in many graph algorithms that rely on divide-and-conquer strategy. Feige and Krauthgamer gave an O(log2 n) approximation algorithm for this problem. On the other hand, no inappro- ximability result was known. It was one of the central open questions in (in)approximability theory whether Min-Bisection has a Polynomial Time Approximation Scheme (i.e. (1+\\eps)-approximation algorithm for every \\eps > 0). The result is this talk resolves the above question, ruling out PTAS for Min-Bisection and two other important problems called Densest Subgraph and Bipartite Clique. Recently, Feige ruled out a PTAS for these problems assuming a certain conjecture about average-case hardness of Random 3SAT. Our result needs only a (standard) assumption that NP has no subexponential time algorithms. The result follows via a new construction of a PCP where the queries of the verifier \"look random\". To be precise, the verifier makes q queries and for any set of half the bits in the proof, the probability that all queries fall into this set is essentially 1/2^q. We introduce several new ideas and techniques, in addition to using variations/generalizations of algebraic techniques used to prove the PCP Theorem. I will try to make the talk as self-contained as possible.\n\nMonday, May 10, 2004\n\nManoj Prabhakaran, Princeton University\n\nNew Notions of Security: Universal Composability without Trusted Setup\n\nAbstract: We propose a modification to the framework of Universally Composable (UC) security [Canetti'01], which enables us to give secure protocols for tasks for which no secure protocol is possible in the original UC framework (except with trusted setup). Our new notion, involves comparing the protocol executions with an ideal execution involving ideal functionalities (just as in UC-security), but allowing the environment and adversary access to some super-polynomial computational power. We argue the meaningfulness of the new notion, which in particular subsumes many of the traditional notions of security. We generalize the Universal Composition theorem of [Canetti] to the new setting. Then under new computational assumptions, we realize secure multiparty computation (for static adversaries), without a common reference string or any other setup assumptions, in the new framework. This is known to be impossible under the UC framework. Joint work with Amit Sahai.\n\nMonday, May 11, 2004\n\nYuval Peres, University of California, Berkeley\n\nTwo Topics on the Interface of Probability and Algorithms\n\nAbstract:\n\nUnbiasing and simulation given a coin with unknown bias: Suppose that we are given a coin with an unknown probability p of heads. By tossing this coin repeatedly, a classical trick shows we can simulate an unbiased coin. How about simulating a coin with probability 2p of heads (when p<1/2)? and with probability f(p) of heads? Solutions to these questions, that start with von Neumann (1952), have led to connections with automata theory, Polya's theorem on positive polynomials, approximation theory and complex analysis. There's an intriguing open problem relating pushdown automata to Algebraic functions. (based on joint works with E. Mossel, S. Nacu.)\n\nLet F be a random k-SAT formula on n variables, formed by selecting uniformly and independently m = rn out of all possible k-clauses. It is well-known that if r>2^k ln 2, then the formula F is unsatisfiable with probability that tends to 1 as n grows. We prove that if r < 2^k ln 2 - O(k), then the formula F is satisfiable with probability that tends to 1 as n grows. E.g., When k=10 our lower bound is 704.94 while the upper bound is 708.94. Related methods (with harder analysis) also lead to good bounds for random Max-k-sat. These results raise the question: \"Are there different thresholds for the existence of solutions to random satisfiability problems, and for existence of solutions that can be found by polynomial-time algorithms?\" (This part based on work with D. Achlioptas and A. Naor).\n\nMonday, May 17, 2004\n\nYuval Rabani, Technion, on Sabbatical at Cornell University\n\nTwo Topics on the Interface of Probability and Algorithms\n\nAbstract: The study of low distortion embeddings into $\\ell_1$ is closely related to the study of the integrality gaps of conventional relaxations for some optimization problems on graphs. One obvious way to strengthen the relaxations is to add valid constraints on small subsets of points. We study the effect of such constraints by examining the distortion of embedding metrics that satisfy them into $\\ell_1$ and other classes of metrics. Joint work with Ilan Newman and Yuri Rabinovich.\n\nMonday, May 24, 2004\n\nDana Moshkovitz\n\nAlgorithmic Construction of Sets for k-Restrictions\n\nAbstract: In k-restriction problems one wishes to find a small set of strings that satisfies the following property: if one observes any k indices, and seeks for some specific restriction on them (out of a large set of possible restrictions given as input), then at least one of the strings meets this restriction.\n\nProblems of this type arise in many fields in Computer Science. A few prominent examples are group testing and generalized hashing.\n\nThe standard approach for deterministically solving such problems is via almost k-wise independence or k-wise approximations for other distributions.\n\nWe offer a generic algorithmic method that yields considerably smaller constructions. Specifically it allows us to derive substantial improvements for the problems mentioned above.\n\nTo this end, we generalize a previous work of Naor, Schulman and Srinivasan.\n\nAmong other results, we greatly enhance the combinatorial objects in the heart of their method, called /splitters/, using the topological /Necklace Splitting Theorem/.\n\nWe are also able to derive an improved inapproximability result for /Set-Cover/ under the assumption P != NP.\n\nJoint work with Noga Alon and Muli Safra\n\nTuesday, May 18, 2004\n\nPeter Winkler, Bell Labs and IAS\n\nTournaments, Boxes and Non-Transitive Dice\n\nAbstract: Many gymnasts compete in 2k-1 events, and are ranked in each with no ties. One competitor is deemed to have \"beaten\" another if she outscores the other in a majority of the events.\n\nAfterward the organizers are embarrassed to discover that no matter how they award the prizes, there will always be a gymnast who didn't get a prize but beat every gymnast who did.\n\nHow many prizes should the organizers have had on hand to be sure this wouldn't happen?\n\n(A meandering blackboard presentation based on recent work with N. Alon, G. Brightwell, H. Kierstead, and A. Kostochka.)"
    }
}
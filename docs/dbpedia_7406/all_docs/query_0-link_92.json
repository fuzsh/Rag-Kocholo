{
    "id": "dbpedia_7406_0",
    "rank": 92,
    "data": {
        "url": "https://www.linkedin.com/posts/tural-suleymani_apache-kafka-for-distributed-systems-activity-7153980340724781057-S42n",
        "read_more_link": "",
        "language": "en",
        "title": "Tural Suleymani on LinkedIn: Apache Kafka for Distributed Systems",
        "top_image": "https://media.licdn.com/dms/image/sync/D4E27AQGK1s_DDwzYqA/articleshare-shrink_800/0/1712258830405?e=2147483647&v=beta&t=YyxWINSlNs0Iw72py6OfWOZlaYuJ0IDO4Mtpu8eCGsY",
        "meta_img": "https://media.licdn.com/dms/image/sync/D4E27AQGK1s_DDwzYqA/articleshare-shrink_800/0/1712258830405?e=2147483647&v=beta&t=YyxWINSlNs0Iw72py6OfWOZlaYuJ0IDO4Mtpu8eCGsY",
        "images": [
            "https://media.licdn.com/dms/image/v2/D4E16AQGOuhSPHjs_-w/profile-displaybackgroundimage-shrink_200_800/profile-displaybackgroundimage-shrink_200_800/0/1714939944461?e=2147483647&v=beta&t=JpJp-xY0ggrinfzvh5ue0yhxyCVYpTHfNtJyZ30lEAI"
        ],
        "movies": [],
        "keywords": [],
        "meta_keywords": [
            ""
        ],
        "tags": null,
        "authors": [
            "Tural Suleymani"
        ],
        "publish_date": "2024-01-19T05:23:47.729000+00:00",
        "summary": "",
        "meta_description": "I have just published my course &#39;Apache Kafka for Distributed Systems&#39; on Udemy. \nThis course will help you to learn Apache Kafka from scratch and you&#39;ll beâ€¦",
        "meta_lang": "en",
        "meta_favicon": "https://static.licdn.com/aero-v1/sc/h/al2o9zrvru7aqj8e1x2rzsrca",
        "meta_site_name": "",
        "canonical_link": "https://www.linkedin.com/posts/tural-suleymani_apache-kafka-for-distributed-systems-activity-7153980340724781057-S42n",
        "text": "\"Excited to announce my completion certification in Apache Kafka! ğŸš€ After weeks of dedicated learning and hands-on practice, I've successfully completed the Apache Kafka certification program. Kafka has revolutionized the way data is handled and processed, and I'm thrilled to have acquired in-depth knowledge and skills in this powerful distributed streaming platform. Through this certification journey, I've gained a comprehensive understanding of Kafka's core concepts, architecture, and ecosystem. From topics and partitions to producers and consumers, I've delved into the intricacies of Kafka's design and functionality. I'm particularly enthusiastic about applying Kafka's capabilities in real-world scenarios, such as building scalable and fault-tolerant data pipelines, implementing event-driven architectures, and enabling real-time analytics. I'm grateful for the support of the Apache Kafka community and the guidance of my mentors throughout this learning journey. I look forward to leveraging my newfound expertise to drive innovation and solve complex data challenges. #ApacheKafka #Certification #DataEngineering #StreamingPlatform #RealTimeAnalytics\n\nKafka Fundamentals Apache Kafka is an open-source distributed event streaming platform used for building real-time data pipelines and streaming applications. It was originally developed by LinkedIn and is now maintained by the Apache Software Foundation. Key concepts and components of Apache Kafka include: 1. Topics: Kafka organizes messages into topics, which are similar to channels or categorizations of data. Producers publish messages to topics, and consumers subscribe to topics to receive messages. 2. Producers: Producers are applications that write data to Kafka topics. They can be any kind of system or application that generates data, such as web servers, sensors, or log files. 3. Consumers: Consumers are applications that read data from Kafka topics. They can process data in real-time or store it in a database for further analysis. 4. Brokers: Brokers are Kafka servers that store and manage the topic partitions. They are responsible for distributing and replicating messages across the cluster. 5. Partition: Topics in Kafka are divided into partitions, which allow for scalability and parallel processing of messages. Each partition is replicated across multiple brokers for fault tolerance. 6. ZooKeeper: Kafka relies on ZooKeeper for distributed coordination and configuration management. ZooKeeper helps maintain metadata about the cluster, such as broker health and topic configurations. 7. Producers and consumers communicate with brokers using the Kafka protocol, which is a binary protocol that allows for high-throughput communication between distributed systems. Overall, Apache Kafka is designed for high-throughput, fault-tolerant, and real-time data processing. It is commonly used in use cases such as real-time analytics, log aggregation, and event-driven microservices architectures. Learning the fundamentals of Kafka can help developers build scalable and reliable messaging systems for their applications. Want to know more? Follow me or connectğŸ¥‚ Please don't forget to likeâ¤ï¸ and commentğŸ’­ and repostâ™»ï¸, thank youğŸŒ¹ğŸ™ #backend #fullStack #developer #Csharp #github #EFCore #dotnet #dotnetCore #programmer #azure #visualstudio\n\nğŸš€ Just completed an intensive Udemy course on Apache Kafka, and I'm thrilled to share some key learnings: ğŸ” Understanding Kafka Ecosystem: Got a comprehensive grasp of the ecosystem, including architecture, core concepts, and operations. Kafka's distributed nature is truly fascinating! ğŸ› ï¸ Mastering Core Concepts: Delved deep into topics like Topics, Partitions, Brokers, Producers, and Consumers. Understanding these foundational elements is crucial for leveraging Kafka effectively. ğŸ’» Setting up Kafka Environment: Established my personal Kafka development environment. Excited to start building and experimenting with Kafka on my own! ğŸ”§ Command Line Tools: Mastered essential CLI tools like kafka-topics, kafka-console-producer, kafka-console-consumer, kafka-consumer-groups, and kafka-configs. These will streamline my Kafka workflow significantly. â˜• Java Integration: Successfully created Producers and Consumers in Java to interact seamlessly with Kafka. Ready to dive into Java-based Kafka development projects! ğŸ¦ Real-World Application: Programmed a Twitter Producer and ElasticSearch Consumer, bridging the gap between social media data and real-time analytics. Excited to explore more real-world applications of Kafka! ğŸ“Š Extended APIs Overview: Explored advanced topics like Kafka Connect, Kafka Streams, and delved into case studies and Big Data architecture. Understanding these extended APIs opens up a world of possibilities for complex data processing scenarios. ğŸ“ Log Compaction: Practiced and understood Log Compaction, a vital concept in Kafka for efficient data storage and retrieval. This knowledge will be invaluable in optimizing Kafka workflows. Feeling empowered and ready to apply these learnings in real-world projects. Let's Kafka and roll! ğŸš€ #ApacheKafka #BigData #RealTimeProcessing #DataEngineering\n\nHello ğŸ‘‹ Let's explore main concepts of Apache Kafka Apache Kafka is a distributed streaming platform for building real-time data pipelines and streaming applications. It's known for its high throughput, fault tolerance, and scalability. Let's dive into some key concepts ğŸ“Œ EVENTS An event records the fact that \"something happened\" in the world or in your business. When you read or write data to Kafka, you do this in the form of events. ğŸ’â™€ï¸ Conceptually, an event has a key, value, timestamp, and optional metadata headers. Here's an example event: Event key: \"Alice\" Event value: \"Made a payment of $200 to Bob\" Event timestamp: \"Jun. 25, 2020 at 2:06 p.m. ğŸ“Œ PRODUCERS Producers are those client applications that publish (write) events to Kafka ğŸ“Œ CONSUMERS consumers are those that subscribe to (read and process) these events ğŸ’â™€ï¸ In Kafka, producers and consumers are fully decoupled and agnostic of each other, which is a key design element to achieve the high scalability that Kafka is known for. FOR EXAMPLE, producers never need to wait for consumers ğŸ“Œ TOPICS Events are organized and durably stored in topics. Very simplified, a topic is similar to a folder in a filesystem, and the events are the files in that folder. ğŸ‘©ğŸ’» Topics in Kafka are always multi-producer and multi-subscriber: a topic can have zero, one, or many producers that write events to it, as well as zero, one, or many consumers that subscribe to these events. ğŸ“Œ PARTITIONS Topics are partitioned, meaning a topic is spread over a number of \"buckets\" located on different Kafka brokers. This distributed placement of data is very important for scalability. When a new event is published to a topic, it is actually appended to one of the topic's partitions. ğŸ“Œ Events with the same event key (e.g., a customer or vehicle ID) are written to the same partition This example topic in below pic has four partitions P1â€“P4. Two different producer clients are publishing, independently from each other. #kafka #softwaredevelopment #100daysofcode #100daysoflearning #java\n\nğŸš€ Understanding Apache Kafka: A Quick Insight ğŸš€ Apache Kafka is a powerful tool for building real-time data pipelines and streaming applications. It's designed to handle high-throughput, low-latency data streams, making it a popular choice for modern distributed systems. ğŸ”¹ How Kafka Works: 1. Producer: The producer sends messages to Kafka topics. These messages are then distributed across different partitions within the topic. 2. Broker: Kafka brokers handle the storage and retrieval of messages. They ensure data durability and replication across a cluster of servers. 3. Consumer: Consumers read messages from Kafka topics. They can read from specific partitions and keep track of their read position using consumer groups. 4. Topic and Partition: Kafka topics are categories where messages are stored. Each topic is divided into partitions, enabling parallel processing and fault tolerance. ğŸ”¹ Key Features: 1. Scalability: Easily scale horizontally by adding more brokers to handle increased load. 2. Durability: Ensures data persistence and replication to prevent data loss. 3. Fault Tolerance: Supports distributed data storage with high availability. 4. High Throughput: Designed to handle millions of messages per second. To give you a practical example, I have created a simple Spring Boot application that demonstrates a Kafka producer and consumer in action. This application uses a RestController to send messages through the producer, showcasing the integration between Spring Boot and Kafka. ğŸ‘‰ Check out my Spring Boot Kafka application here: https://lnkd.in/g2avEz5v Feel free to explore and share your thoughts! Happy streaming! ğŸŒŸ #Kafka #SpringBoot #RealTimeData #DataStreaming #BigData #TechInnovation #DistributedSystems #Java #OpenSource #Microservices\n\nğŸš€ Project Description: Exploring Kafka and AWS ğŸ’ I embarked on this project to deepen my understanding of Kafka while gaining hands-on experience with AWS resources. The journey began by setting up an EC2 instance where I installed and deployed the Kafka server. Using SSH, I connected from my local machine and executed scripts (located inside the repo) to initialize Zookeeper, deploy Kafka, create a topic, and set up a producer and a consumer. ğŸ’ Simultaneously, I ran a Python script in a notebook on my local machine, leveraging the Kafka library. As part of a test, I used a CSV file with numerous rows, randomly sending data to the consumer. The consumer's logic involved creating an S3 bucket to store received elements, simulating real-time data transmission. ğŸ’ With a substantial amount of data in the bucket, I employed a crawler to extract file schemas, and after creating a database in Athena for efficient querying. ğŸ’ This project provided an in-depth understanding of Kafka's inner workings and the interaction between its services. Overcoming challenges during the deployment of Kafka services on EC2 was a significant learning curve, but persistence paid off. ğŸ’ Additionally, by implementing logic in Python, I explored available libraries to connect with Kafka, expanding my knowledge in the data engineering world. Looking ahead, my next milestone involves delving into Airflow and exploring ways to integrate these two powerful tools. ğŸ› ï¸ Technologies Used: Kafka AWS (EC2, S3, Athena, Glue) Python ğŸŒ Project Repository: First comment Thank you to Darshil Parmar for sharing his knowledge on his YouTube Channel. #DataEngineering #Kafka #AWS #Python #DataAnalysis #LearningJourney\n\nEver wondered how Apache Kafka works? Let's break it down together! I spent hours studying Apache Kafka, and here are the key ingredients to get you started: - ğğ«ğ¨ğ¤ğğ« The heart of Kafka is the Broker. Think of it as the traffic cop, directing messages. You also need Zookeeper, acting like the behind-the-scenes coordinator. - ğ“ğ¨ğ©ğ¢ğœğ¬ Topics in Kafka act like channels. For instance, think of your TV channels. Use topics to organize your messages efficiently. - ğğšğ«ğ­ğ¢ğ­ğ¢ğ¨ğ§ğ¬ Kafka is characteristic for its scalability, and partitions play a crucial role. Start by distributing data into partitions, and then replicate them for reliability until you achieve a robust, scalable system. What have you learned about Apache Kafka? Share your insights in the comments below! #apachekafka #dataprocessing #distributedsystems #bigdata #softwarearchitecture #techinsights #programming #codingcommunity #softwaredevelopment #developers #techtalk #technology #codingjourney #datamanagement #dataengineering #softwareengineering #techexplained #codenewbie #learntocode #programmingtips #techcommunity #datascience #dataanalytics #cloudcomputing #javadevelopment #pythonprogramming #javascript #reactjs #springboot #databasemanagement #nosql #devops #opensource #techlearning #itworld #innovation #digitaltransformation #programminglanguages #softwaredesign #webdevelopment #codeislife #geeklife #codenerd #codeeverything #continuouslearning #techtrends #datadriven #softwareskills #careerdevelopment #linkedinlearning #ai"
    }
}
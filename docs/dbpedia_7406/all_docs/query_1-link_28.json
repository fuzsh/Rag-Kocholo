{
    "id": "dbpedia_7406_1",
    "rank": 28,
    "data": {
        "url": "https://link.springer.com/article/10.1007/s10664-022-10250-2",
        "read_more_link": "",
        "language": "en",
        "title": "Mastering uncertainty in performance estimations of configurable software systems",
        "top_image": "https://static-content.springer.com/image/art%3A10.1007%2Fs10664-022-10250-2/MediaObjects/10664_2022_10250_Fig1_HTML.png",
        "meta_img": "https://static-content.springer.com/image/art%3A10.1007%2Fs10664-022-10250-2/MediaObjects/10664_2022_10250_Fig1_HTML.png",
        "images": [
            "https://link.springer.com/oscar-static/images/darwin/header/img/logo-springerlink-39ee2a28d8.svg",
            "https://media.springernature.com/w72/springer-static/cover-hires/journal/10664?as=webp",
            "https://media.springernature.com/w215h120/springer-static/image/art%3A10.1007%2Fs10270-018-0662-9/MediaObjects/10270_2018_662_Fig1_HTML.gif",
            "https://media.springernature.com/w215h120/springer-static/image/art%3A10.1007%2Fs10515-020-00273-8/MediaObjects/10515_2020_273_Fig1_HTML.png",
            "https://media.springernature.com/w215h120/springer-static/image/art%3A10.1007%2Fs10664-017-9573-6/MediaObjects/10664_2017_9573_Fig1_HTML.gif",
            "https://media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10664-022-10250-2/MediaObjects/10664_2022_10250_Fig1_HTML.png",
            "https://media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10664-022-10250-2/MediaObjects/10664_2022_10250_Fig2_HTML.png",
            "https://media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10664-022-10250-2/MediaObjects/10664_2022_10250_Fig3_HTML.png",
            "https://media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10664-022-10250-2/MediaObjects/10664_2022_10250_Figa_HTML.png",
            "https://media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10664-022-10250-2/MediaObjects/10664_2022_10250_Figb_HTML.png",
            "https://media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10664-022-10250-2/MediaObjects/10664_2022_10250_Figc_HTML.png",
            "https://media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10664-022-10250-2/MediaObjects/10664_2022_10250_Figd_HTML.png",
            "https://media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10664-022-10250-2/MediaObjects/10664_2022_10250_Fige_HTML.png",
            "https://media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10664-022-10250-2/MediaObjects/10664_2022_10250_Figf_HTML.png",
            "https://media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10664-022-10250-2/MediaObjects/10664_2022_10250_Fig4_HTML.png",
            "https://media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10664-022-10250-2/MediaObjects/10664_2022_10250_Figg_HTML.png",
            "https://media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10664-022-10250-2/MediaObjects/10664_2022_10250_Figp_HTML.png",
            "https://media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10664-022-10250-2/MediaObjects/10664_2022_10250_Fig5_HTML.png",
            "https://media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10664-022-10250-2/MediaObjects/10664_2022_10250_Figq_HTML.png",
            "https://media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10664-022-10250-2/MediaObjects/10664_2022_10250_Fig6_HTML.png",
            "https://media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10664-022-10250-2/MediaObjects/10664_2022_10250_Fig7_HTML.png",
            "https://media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10664-022-10250-2/MediaObjects/10664_2022_10250_Figr_HTML.png",
            "https://media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10664-022-10250-2/MediaObjects/10664_2022_10250_Fig8_HTML.png",
            "https://media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10664-022-10250-2/MediaObjects/10664_2022_10250_Fig9_HTML.png",
            "https://media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10664-022-10250-2/MediaObjects/10664_2022_10250_Fig10_HTML.png",
            "https://media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10664-022-10250-2/MediaObjects/10664_2022_10250_Figs_HTML.png",
            "https://media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10664-022-10250-2/MediaObjects/10664_2022_10250_Fig11_HTML.png",
            "https://media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10664-022-10250-2/MediaObjects/10664_2022_10250_Fig12_HTML.png",
            "https://media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10664-022-10250-2/MediaObjects/10664_2022_10250_Fig13_HTML.png",
            "https://media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10664-022-10250-2/MediaObjects/10664_2022_10250_Figt_HTML.png",
            "https://media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10664-022-10250-2/MediaObjects/10664_2022_10250_Fig14_HTML.png",
            "https://media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10664-022-10250-2/MediaObjects/10664_2022_10250_Fig15_HTML.png",
            "https://link.springer.com/oscar-static/images/logo-springernature-white-19dd4ba190.svg"
        ],
        "movies": [],
        "keywords": [],
        "meta_keywords": [
            ""
        ],
        "tags": null,
        "authors": [],
        "publish_date": "2023-01-19T00:00:00",
        "summary": "",
        "meta_description": "Understanding the influence of configuration options on the performance of a software system is key for finding optimal system configurations, system under",
        "meta_lang": "en",
        "meta_favicon": "/oscar-static/img/favicons/darwin/apple-touch-icon-92e819bf8a.png",
        "meta_site_name": "SpringerLink",
        "canonical_link": "https://link.springer.com/article/10.1007/s10664-022-10250-2",
        "text": "In this section, we describe our approach of incorporating uncertainty into performance-influence models. Figure 2 provides an overview of all steps involved. In a nutshell, we perform the following tasks: First, we preprocess a given set of measured configurations (i.e., the training set) to ensure that inference\n\n(i)\n\ndoes not break and that it\n\n(ii)\n\nfinishes in a reasonable time, and\n\n(iii)\n\nyields interpretable models.\n\nSecond, we apply probabilistic programming to build a Bayesian model for a selection of options and interactions thereof. It is key for scalability that this selection comprises the actual set of influencing options and interactions. Third, we estimate the priors for the model’s random variables (i.e., options and interactions) and compute a fitted model with Bayesian inference.\n\n3.1 Data Preprocessing\n\nOur approach relies on a training set consisting of a number of sampled configurations that are attributed with their performance. Thus, our approach can be combined with any sampling strategy, such as feature-wise, t-wise (Johansen et al. 2012), or random sampling (Gogate and Dechter 2006). However, it is important to process the sample set to avoid inference failures and to promote interpretability, as we explain next.\n\nSimilar to Ordinary Least Squares, Bayesian inference is prone to failure if multicollinearity exists in the training set, which occurs when the values of independent variables are intercorrelated (Hill and Adkins 2007; Farrar and Glauber 1967). Let us consider the following training set for an exemplary software system with options X, Y, Z, and M, illustrating multicollinearity:\n\nOption B is mandatory. It represents the base functionality of the system, which results from configuration-independent parts of the code. Options X, Y, and Z form an alternative group, that is, the system’s constraints enforce that exactly one of them is active in each configuration. An important insight is that an alternative group introduces multicollinearity to a training set because the selection of any single option is determined by the remaining options, for example: Z = 1 −X −Y. Multicollinearity not only hinders inference, but also interpretability. Considering the training set above, we see that the following performance-influence models are accurate with respect to the measurements, but assigning different contributions of individual options:\n\n$$ \\begin{array}{@{}rcl@{}} \\pi(c) &=& {\\kern5pt}0 \\cdot c(\\textsf{B}) + 10 \\cdot c(\\textsf{X}) + 20 \\cdot c(\\textsf{Y}) + 30 \\cdot c(\\textsf{Z}) \\\\ \\pi(c) &=& {\\kern5pt}5 \\cdot c(\\textsf{B}) + {\\kern7pt}5 \\cdot c(\\textsf{X}) + 15 \\cdot c(\\textsf{Y}) + 25 \\cdot c(\\textsf{Z}) \\\\ \\pi(c) &=& 10 \\cdot c(\\textsf{B}) + {\\kern7pt}0 \\cdot c\\textsf{(X}) + 10 \\cdot c(\\textsf{Y}) + 20 \\cdot c(\\textsf{Z}) \\end{array} $$\n\nBecause exactly one option of the alternative group is active in each configuration, the base performance of a software system can be attributed to the base functionality B and the options of an alternative with any ratio. For example, option X can have an influence of 10, 5, or none, depending on how we assign the performance to the system’s base functionality. Therefore, performance influence models for such systems are difficult to compare and interpret. Here, we do not even know whether an option (e.g., X) is influential at all. This is a problem that related approaches share (Siegmund et al. 2012a; Guo et al. 2013).\n\nChoosing a default configuration provides remedy for multicollinearity inference failures and interpretability problems. That is, we select a default option for each alternative group using domain knowledge or at random. We then remove these options from the training set to achieve the following effects:\n\nDefault options’ performance influences are set to 0.\n\nMulticollinearity arising from alternative groups is reduced, since the selection of a single remaining option of an alternative group cannot be determined without the removed default option (i.e., Z = 1 −X −Y does not hold anymore if any of these options is removed from the training set).\n\nMandatory options, which must be selected in each configuration, introduce a special case of multicollinearity. Option M is mandatory and therefore present in each configuration and indistinguishable from the base influence. Similar to alternative groups, a model can split the base influence between mandatory options and the base influence with any ratio.\n\nMoreover, we can see that such an option does not contribute any information to the model by computing the Shannon information entropy (Shannon 1948):\n\n$$ H(o) = - \\sum\\limits_{x = 0}^{1} P_{o} (x ) \\cdot \\log_{2} \\big( P_{o} (x ) \\big) $$\n\n(7)\n\nAs M is selected in each configuration, its only selection value is 1, with selection probability PM(1) = 1. We see that, therefore, the information entropy of M is 0:\n\n$$ \\begin{array}{@{}rcl@{}} H (\\textsf{M} ) & =& - P_{\\textsf{M}}(1) \\cdot \\log_{2} \\big( P_{\\textsf{M}}(1) \\big) - P_{\\textsf{M}}(0) \\cdot \\log_{2} \\big( P_{\\textsf{M}}(0) \\big) \\\\ & =& -1 \\cdot \\log_{2} 1 - 0 \\cdot \\log_{2} 0 = 0 \\end{array} $$\n\n(8)\n\nFor that reason, we can safely remove mandatory options from the training set. The same applies for dead options, which are never active.\n\nNote that options may only appear to be dead or mandatory as an artifact of the sampling process. That is, it is insufficient to query only the system’s variability model for its constraints to detect mandatory or dead options. Hence, we perform constraint mining on the sample set rather than the whole system to overcome this problem. We use the Shannon information entropy in (7) as a means to determine dead options and scan the set of options for combinations that appear to be alternative groups.\n\n3.2 Model Composition\n\nTo build a Bayesian model with probabilistic programming, we first need to specify which options and interactions are present in the model. Subsequently, we create random variables from this model structure to account for epistemic and aleatoric uncertainty.\n\n3.2.1 Option and Interaction Filtering\n\nComposing a model from all options and all potential interactions, whose number is exponential in the number of options, is impractical for large software systems, because models with high numbers of parameters are difficult to interpret and, more importantly, inference may become computationally infeasible (Gareth et al. 2013). Therefore, we apply model selection to constrain the number of parameters. In particular, we use a subset selection approach (Alan 2002), because it yields a subset of unaltered options from a parent set, which is not the case for other approaches, such as dimensionality reduction (Maaten et al. 2009). We build the parent set of available options \\(\\mathcal {S}\\) from all options \\(\\mathcal {O}\\) of the system in question as well as all pair-wise interactions \\(\\mathcal {I}\\) with \\(\\mathcal {S} = \\mathcal {O} \\cup \\mathcal {I}\\). We map each pair-wise interaction i to a virtual option with respect to its constituting options or and os:\n\n$$ c(o_{n + p}) = c(o_{r}) \\cdot c(o_{s}) \\quad \\text{with} \\quad 1 \\leq p \\leq | \\mathcal{I} | ~ \\wedge ~ r \\neq s $$\n\n(9)\n\nCompared to higher-order interactions, pair-wise interactions have been found to frequently influence performance (Siegmund et al. 2012a) and to be the most common kind of interaction (Kolesnikov et al. 2019). However, we acknowledge that considering higher-order interactions may improve the accuracy of our approach (Siegmund et al. 2012a), at the cost of possibly leading to computationally intractable models. The explicit modeling of interactions introduces non-linearity to the otherwise linear model structure.\n\nSubset selection approaches define a filter function \\(F: \\mathcal {S} \\mapsto \\left \\{0,1 \\right \\}\\), which yield 1 if an option or interaction of the parent set \\(\\mathcal {S}\\) should be considered by the model, and 0, otherwise. The result of subset selection consists of filtered options and interactions:\n\n$$ \\mathcal{V} = \\big\\{ s | \\forall s \\in \\mathcal{S} \\text{ and } F(s) = 1 \\big\\} $$\n\n(10)\n\nSimilar to previous work (Ha and Zhang 2019b), we apply Lasso regression (Tibshirani 1996) on the preprocessed training set. As a result, Lasso assigns zero performance influence to less- and non-influential options and interactions, and it distributes the NFP influence among the remaining elements in \\(\\mathcal {V}\\). Our Lasso filter selects \\(v_{l} \\in \\mathcal {V}\\), whose NFP influence \\(I_{\\pi _{\\textit {Lasso}}}(v_{l})\\) is non-zero according to Lasso regression:\n\n$$ F_{\\textit{Lasso}}(v_{l}) = \\left\\{\\begin{array}{ll} 0 & ~~~ I_{\\pi_{\\textit{Lasso}}}(v_{l}) = 0 \\\\ 1 & ~~~ I_{\\pi_{\\textit{Lasso}}}(v_{l}) \\neq 0 \\end{array}\\right. $$\n\n(11)\n\n3.2.2 Applied Probabilistic Programming\n\nWe follow related approaches for performance modeling of configurable software systems and choose an additive model to make the uncertainty of the options’ and interactions’ performance influence explicit. We start with a model that takes the form of (4) (which represents the state of the art) with two differences:\n\n1.\n\nInstead of scalar influences \\(\\boldsymbol {\\beta } \\in \\mathbb {R}^{n}\\), we use a probability vector \\(\\hat {\\boldsymbol {\\beta }}\\), whose elements each have a PDF and form the coefficients as explained in Section 2.2.\n\n2.\n\nWe use the filtered options and interactions \\(\\mathcal {V}\\) from Section 3.2.1 and thus enable our model to capture non-linear performance influence:\n\n$$ \\begin{array}{@{}rcl@{}} \\pi_{\\mathit{ep}}(c) = \\hat{\\beta}_{0} & +& \\hat{\\beta}_{1} \\cdot c(o_{1}) + {\\dots} + \\hat{\\beta}_{n} \\cdot c(o_{n}) \\\\ & + &\\hat{\\beta}_{n+1} \\cdot c(o_{n+1}) + {\\dots} + \\hat{\\beta}_{n+| \\mathcal{I} | } \\cdot c(o_{n+| \\mathcal{I} | }) \\end{array} $$\n\n(12)\n\nTo infer the distribution of an option, we need to specify a prior distribution for the probability vector \\(\\hat {\\boldsymbol {\\beta }}\\). This distribution should be continuous (i.e., defined over all \\(\\beta \\in \\mathbb {R}\\)) and have non-zero mass for any \\(\\beta \\in \\mathbb {R}\\), not to exclude certain values entirely. For performance modeling, we choose the normal distribution \\(\\mathcal {N}(\\mu , \\sigma )\\). It has a mode that, other than the uniform distribution, lets us encode an influence area of high probability. That is, an option’s or interaction’s influence has a normally distributed probability to fall into an interval to be inferred by probabilistic programming. Note that, even if a normal distribution is not the best fit for all random variables, Bayesian inference can adjust them. We describe how to determine the parameters for chosen prior distributions, such as the mean μ and the standard deviation σ for the normal distribution \\(\\mathcal {N}(\\mu , \\sigma )\\), in Section 3.3.\n\nAt this point, we have constructed πep, a model that incorporates epistemic uncertainty in \\(\\boldsymbol {\\hat {\\beta }}\\). To account for aleatoric uncertainty (i.e., the uncertainty in the training set), we use two different models, one for homoscedastic (constant variance) and heteroscedastic (variance depending on true performance) aleatoric uncertainty. These models build on πep. We adopt the common prior of a normal distribution for both models.\n\nHomoscedastic Model\n\nIf we assume that the variance of uncertainty is equal for all training set samples, we can complete our Bayesian model with a normal distribution around πep(c):\n\n$$ \\pi_{\\mathit{ho}}(c) = \\mathcal{N}(\\pi_{\\mathit{ep}}(c),\\sigma) $$\n\n(13)\n\nThis normal distribution is modeled as an additional random variable, whose σ parameter captures the variance of absolute errors in training set samples.\n\nHeteroscedastic Model\n\nTo account for errors in the training set that are relative to the training set sample performance, we introduce σrel, a random variable that captures uncertainty about the error ratio. As an error ratio is in \\(\\mathbb {R}_{>0}\\) (i.e., a continuous, positive variable), we choose the Gamma distribution as prior for σrel. The Gamma distribution with a shape parameter a and a spread parameter b can take a (possibly skewed) bell shape with non-negative values:\n\n$$ \\sigma_{\\mathit{rel}} = \\mathcal{G}(a, b) $$\n\n(14)\n\nSimilar to the homoscedastic model, we define the heteroscedastic model as a normal distribution around πep(c), but with the product of the epistemic performance prediction and the relative error ratio σrel as standard deviation:\n\n$$ \\pi_{\\mathit{he}}(c) = \\mathcal{N}(\\pi_{\\mathit{ep}}(c), \\pi_{\\mathit{ep}}(c) \\cdot \\sigma_{\\mathit{rel}}) $$\n\n(15)\n\n3.3 Prior Estimation\n\nRegular Bayesian inference requires the user to estimate prior distributions for the model’s random variables from domain knowledge or personal experience. Distributions that are too uninformative (i.e., very wide) can lead to a hold of the inference, whereas distributions that are too informative will also slow down inference if they are imprecise (Salvatier et al. 2016). Our approach automatically chooses which options and interactions are modeled as random variables, such that the user does not need to know which random variables need priors beforehand. For that reason, we employ an automatic prior estimation following the empirical Bayes approach (Robbins 1956), which differs from the regular Bayesian approach in that it estimates priors from the training data. As a result, every aspect of Bayesian modeling is automated for the user.\n\n3.3.1 Epistemic Uncertainty Priors\n\nWe capture epistemic uncertainty in our Bayesian model in random variables for the base influence and the influences for options and interactions, whose assumed normally distributed priors rely on means μ and standard deviations σ.\n\nWe propose a prior estimation algorithm that uses the influence values of other additive models to estimate priors. As models, we use instances of Elastic Net (Zou and Hastie 2017) with r evenly distributed ratios of l1 ∈ [0,1]. For l1 = 1, Elastic Net behaves like Lasso, for l1 = 0 it behaves like Ridge regression and it interpolates the error functions of both approaches for 0 < l1 < 1. We fit 50 Elastic Nets with evenly distributed l1 parameter on the training set. This way, we obtain a set of 50 models \\({\\mathscr{M}}\\) with different performance influences I(⋅) for the previously selected options and interactions. In initial experiments, increasing the number of Elastic Nets did not improve accuracy of Bayesian models. Next, we determine the empirical distribution of influences for each option and interaction:\n\n$$ \\boldsymbol{\\hat{I}}_{\\mathcal{M}}(v_{l}) = \\left\\{ I_{m}(v_{l}) | \\forall m \\in \\mathcal{M} \\right\\} $$\n\n(16)\n\nWe could use the mean and standard deviation of \\(\\boldsymbol {\\hat {I}}_{{\\mathscr{M}}}\\) as prior μ and σ for each option and interaction. However, not all models in \\({\\mathscr{M}}\\) will fit the training data well. To reduce the influence for unfit models, we weigh each model according to its average error on the training set \\(\\bar {\\varepsilon }(\\cdot )\\):\n\n$$ \\boldsymbol{w} = \\left\\{ \\frac{-\\bar{\\varepsilon}(m_{i})}{{\\sum}_{i=1}^{\\|\\mathcal{M}\\|}{-\\bar{\\varepsilon}(m_{i})}} \\forall m_{i} \\in \\mathcal{M} \\right\\} $$\n\n(17)\n\nWe compute the weighted mean μw(t) and weighted standard deviation σw(t) for a specific option or interaction t as follows:\n\n$$ \\mu_{w}(t) = \\frac{1}{{\\sum}_{i=1}^{\\|\\boldsymbol{w}\\|} w_{i}^{\\gamma} } \\sum\\limits_{\\forall i \\in \\hat{I}_{\\mathcal{M}}(t)}{w_{i}^{\\gamma} i} $$\n\n(18)\n\n$$ \\sigma_{w}(t) = \\sqrt{ \\frac{1}{{\\sum}_{i=1}^{\\|\\boldsymbol{w}\\|} w_{i}^{\\gamma} } \\sum\\limits_{\\forall i \\in \\hat{I}_{\\mathcal{M}}(t)}{w_{i}^{\\gamma} ( \\mu_{w}(t) - i )^{2}} } $$\n\n(19)\n\nWe added the tuning parameter γ to enable polynomial weighting. That is, the influence of models with the lowest average error \\(\\bar {\\varepsilon }\\) is increased for γ > 1. In a pre-study, we empirically evaluated different values for γ and found that γ = 3 yields accurate priors.\n\n3.3.2 Aleatoric Uncertainty Priors\n\nWe model aleatoric uncertainty (i.e., uncertainty in each training set sample) as a normal distribution for the homoscedastic model πho and as a gamma distribution as the relative uncertainty in the heteroscedastic model πhe. We build the set of all absolute prediction errors for all models \\(m \\in {\\mathscr{M}}\\) over the samples in the training set and fit a normal distribution using maximum likelihood estimation to estimate a prior for the aleatoric uncertainty in πho. Likewise, we estimate a prior for the gamma distribution in πho, but we compute relative prediction errors, instead, to model the error ratio (cf. (15)).\n\n3.4 Bayesian Inference and Prediction\n\nAs discussed in Section 2.3, Bayesian inference uses prior assumptions on PDFs of random variables that form a generative model, called likelihood, to compute a posterior, that is, an updated belief about the random variable’s PDFs. Unfortunately, the posterior to many Bayesian inference problems cannot be computed directly, so recent research in this field has developed algorithms that can estimate the posterior approximately. Two notable classes of inference algorithms are variational inference and Markov chain Monte Carlo (Murphy 2012).\n\nVariational inference algorithms tune the prior distribution’s parameters without changing the types of the distributions (i.e., a prior normal distribution stays a normal distribution) (Roeder and Yuhuai 2017). This method is preferred for quick results that do not need to be precise.\n\nMarkov chain Monte Carlo (MCMC) algorithms draw samples from the posterior distributions and are able to estimate arbitrary posterior distributions in theory (a prior normal distribution may by transformed to a skewed distribution). MCMC algorithms are considered more precise, but also slower than variational inference.\n\nWe follow a combined approach by first estimating an approximate solution with variational inference (Roeder and Yuhuai 2017) and subsequently fine-tune with the No-U-Turn Sampler (NUTS) (Hoffman and Gelman 2014), an MCMC algorithm. We allow 200,000 iterations for variational inference, but abort on convergence. Initialized with the intermediate result of variational inference, NUTS first draws 3,000 samples for initial tuning and then acquires 12,000 of each random variable’s posterior distribution. The number of posterior samples determines the granularity of subsequent analyses. For example, a number of 100 posterior samples bounds the granularity of computed confidence intervals to 1%, at best. To facilitate fine-granular confidence intervals covering low-probability values for which MCMC draws fewer posterior samples, such as a 95%-confidence interval, we need more posterior samples. While we acquire a large number 12,000 posterior samples, a lower number of MCMC samples in the order of 1000 may be a more economic choice.\n\nPrediction\n\nTo predict the performance of a configuration c, we insert c’s option selection values into \\(o_{1}(c), \\dots , o_{n}(c)\\) and determine active interactions according to (9). We can now draw a number of posterior samples to approximate the distribution for the prediction. Increasing the number of posterior samples makes the approximation more accurate, but also slows down prediction. We draw 1000 posterior samples to yield a good approximation. With this approximation, we can make different kinds of predictions, for which we introduce individual notations. The most informative kind of prediction is the sampled approximation itself (\\( \\widetilde {\\pi }\\)). Using \\( \\widetilde {\\pi }\\), we can compute a confidence interval for a desired confidence \\(\\alpha _{\\mathit {ci}} \\in \\left [0 \\%, 100 \\% \\right ]\\) (\\( \\bar {\\pi }_{\\alpha }\\)). This yields the interval around the mode of prediction over which the predicted distribution integrates to αci. We use \\(\\bar {\\pi }\\) to indicate the 95%-confidence interval by default. The mode of the approximation also serves as a single-point estimate prediction (\\(\\dot {\\pi }\\)).\n\nFigure 3 illustrates P4’s predictions process. For an Apache configuration with seven active options, it shows that two options (ECDSA and TLS) have an individual influence in addition to the core influence, which is always present. One option (ComprLvl9) increases energy consumption when it interacts with the other two options (ECDSA and TLS). Each influence’s marginal posterior distribution propagates into a predictive distribution displaying a wide 95%-confidence interval.\n\nTo evaluate our approach, we state three research questions that are in line with related work and are also concerned with the new possibilities of obtaining a confidence interval for performance predictions. Specifically, we answer the following research questions:\n\nThis research question places our approach in relation to a state-of-the-art approach that resorts only to a scalar value. Although this is not the main usage scenario, we evaluate whether our approach has a comparable accuracy.\n\nRQ2 refers to the ability that users can specify a confidence interval of predictions. This can substantially affect prediction accuracy and evaluates the strength of our approach.\n\nThe third research questions aims at providing a deeper understanding of confidence intervals and incorporated uncertainties in our approach. In contrast to RQ2, RQ3 is not concerned with prediction accuracy. Instead, we evaluate whether predicted intervals with higher confidence contain the correct value more often and, hence, truly capture the uncertainty in the predictions.\n\nIn addition to the research questions answered in this articles’ preceding conference paper (Dorn et al. 2020), we aim at explaining the occurrence of high prediction errors by studying the distribution of uncertainty within P4 models. To this end, we first analyze the nature of posterior influence distributions and then analyze the cause for inaccurate predictions, answering two further research questions:\n\nWith this research question, we study whether Gaussian prior distributions are the right choice to model option and interaction influences. Although our experiments in Section 5 use Gaussian priors, the inferred posteriors may be distributed differently. Changing the prior distribution is possible since we use the Markov chain Monte Carlo algorithm for inference.\n\nAs the lack of training data is one source of epistemic uncertainty, we expect that building P4 models on a larger portion of the configuration space will reduce epistemic uncertainty. However, since we compose the performance-influence model in step 2 in an automated way using Lasso regression, increasing the training data might automatically increase model complexity such that even more training data might be needed. With RQ5, we investigate which effect prevails for real-world software systems.\n\nRQ6 focuses on epistemic uncertainty, which captures the uncertainty of the individual option and interaction influences. For some software systems, all options and interaction influences may be similarly uncertain. However, P4 may reveal instances where uncertainty of a single or few unreliable options and interactions overshadows the rest, which can indicate an unbalanced training set or highly non-deterministic option behavior. As a result, predictions for configurations containing such an unreliable option or interaction will be substantially more uncertain. With RQ6, we answer if there are unreliable options and study their cause.\n\n5.1 General Experiment Setup\n\nWe implement our approach with the PyMC3 (Salvatier et al. 2016) framework. PyMC3 offers implementations for MCMC, variational inference, as well as confidence interval computation for model parameters β and predictions. For maximum likelihood prior estimations, we rely on SciPy (Virtanen et al. 2020).\n\nTo answer our research questions, we infer Bayesian models with absolute and relative error with P4 for the chosen subject systems using three training sets \\(\\mathcal {T}_{1}, \\mathcal {T}_{2}, \\mathcal {T}_{3}\\) on a cluster of machines with Intel Xeon E5-2690v2 CPU and 64GB memory. For the ten subject systems by Kaltenecker et al. (2019), we use the training sets provided at their supplementary Web site. For the remaining subject systems, we sample new training sets with SPL Conqueror (Siegmund et al. 2012b).\n\nFor t = 1, t-wise sampling equals option-wise sampling, which yields \\(n \\leq | \\mathcal {O} |\\) samples. Since we want to evaluate our approach also for learning interactions among options, creating \\(n+| \\mathcal {I} |\\) random variables leads to a modeling problem with more variables than observations. We avoid this situation by excluding interactions from our model for \\(\\mathcal {T}_{1}\\). This might affect prediction accuracy especially compared to other approaches that do not exclude interactions. We will discuss this in RQ1.\n\nTo account for stochastic elements in MCMC, we run the inference for each system’s training set with 5 repetitions for the purpose of our experiments. Nonetheless, these repetitions are not necessary to use P4 in practice, as an individual run already quantifies uncertainty based on the priors and the training data. The average fitting time over all experiments was \\(8 \\min \\limits \\). Although in the worst case, πhe needs \\(245.8 \\min \\limits \\) to fit VP9’s \\(\\mathcal {T}_{3}\\) training set, overall, 80% of all models were fitted within \\(6 \\min \\limits \\). The models for the \\(\\mathcal {T}_{1}\\) and \\(\\mathcal {T}_{2}\\) training sets were fitted in only \\(3 \\min \\limits \\) and \\(5 \\min \\limits \\) on average. They also contained fewer options and interactions than the models for the \\(\\mathcal {T}_{3}\\) training sets. However, models with more options and interactions do not always require more fitting time as we observe only a moderate Kendall rank correlation (Kendall 1938) (τ = 0.52) between the number of options and interactions in a model and its fitting time. We detail the fitting times in our experiments on our supplementary website. As explained in Section 3.4, we recommend an order of magnitude less inference samples, which also aids the reduction of fitting time.\n\n5.2 RQ1: Accuracy of Scalar Predictions\n\nP4 is designed to both predict confidence intervals and point estimates. To assess the accuracy of the point estimates, we compare P4 to state-of-the-art models, which can only predict point estimates. RQ1 is concerned with whether P4 is competitive even for point estimates and, thus, can replace traditional models. In this scenario, we neglect P4’s capability to predict confidence intervals as P4 is, to the best of our knowledge, the first approach to model uncertainty of performance prediction. Hence, there is no baseline to compare to.\n\n5.2.1 Setup\n\nWe chose SPL Conqueror for comparison because it shares the additive model structure with our approach and is used as baseline in the literature (Kaltenecker et al. 2019; Oh et al. 2017). For comparison, we rely on accuracies of SPL Conqueror as reported by Kaltenecker et al. (2019). That is, we consider for RQ1 the ten subject systems that the original authors have used. Another benefit is that Kaltenecker et al. provided raw measurements of the whole population, so we have a reliable ground truth.\n\nWe use the inferred performance-influence models to predict the performance of the whole populations of our subject systems. We adopt the Mean Absolute Percentage Error (MAPE) from previous work (Kaltenecker et al. 2019) to quantify prediction accuracy. That is, we first compute the absolute percentage error (APE) for each configuration \\(c \\in \\mathcal {C}\\) with the measured performance πtrue(c) and predicted scalar performance \\(\\dot {\\pi }(c)\\) for our models πho and πhe:\n\n$$ \\text{APE}(c) = \\frac{ \\left | \\pi_{\\mathit{true}}(c) - \\dot{\\pi}(c) \\right |}{\\pi_{\\mathit{true}}(c)} $$\n\n(20)\n\nWe then compute the MAPE as the average over all APEs:\n\n$$ \\text{MAPE}(\\mathcal{C}) = \\frac{{\\sum}_{c \\in \\mathcal{C}} \\text{APE}(c)}{| \\mathcal{C} |} $$\n\n(21)\n\n5.2.2 Results\n\nAs Fig. 4 shows, P4 achieves MAPE scores comparable to SPL Conqueror. Table 2 allows for a more fine-grained view. We see that the overall accuracy is higher when using SPL Conqueror, which is to be expected as only the mode is taken from the performance distribution provided as predictions by our approach. Nevertheless, we observe that, for many systems, especially when using πhe, the model with relative error, we obtain a similar or even better prediction indicated by underscored values. The mean error is, thus, distorted by some larger outliers, such as HIPAcc and VP9. These systems have many alternative options such that there is a larger uncertainty involved and since we are not using the provided confidence interval, we deprive our approach of its strength.\n\nInterestingly, compared to \\(\\mathcal {T}_{1}\\), for some subject systems P4 performs worse on \\(\\mathcal {T}_{2}\\). The reason is that the increased number of random variables in P4, due to the additional modeling of interactions, requires more measurements as provided by \\(\\mathcal {T}_{2}\\) to effectively infer performance distributions. Moreover, we see a clear trend that, with an increasing number of measurements, P4 closes the gap in prediction accuracy with SPL Conqueror and even outperforms it for \\(\\mathcal {T}_{3}\\) and πhe for 7 out of 10 systems.\n\n5.3 RQ2: Accuracy of Confidence Intervals\n\nOne of P4’ s key novelties is the ability to predict confidence intervals. With RQ2, we ask how accurate P4’ s predicted confidence intervals are. However, due to the lack of a baseline, we cannot compare P4 to any other approach. Instead, we adopt the error metric from RQ1 to confidence intervals instead of scalars.\n\n5.3.1 Setup\n\nConfidence intervals with confidence αCI ∈ [0%, 100%] specify a range in which a given PDF integrates to αCI. For predictions, a 95% confidence interval specifies a performance range for which the model is 95% confident that it contains the true performance value of the corresponding configuration. Consequently, we can expect the true performance to lie outside the 95% confidence interval in 5% of predictions. Although we can expect to always capture the true performance with a 100% confidence interval, such an interval will likely approach \\([-\\infty , +\\infty ]\\) for PDFs that are defined over \\(\\mathbb {R}\\).\n\nSimilar to RQ1, we use a relative error metric to answer RQ2. However, for RQ2, we use P4 to predict confidence intervals as prediction, which is the actual strength and novel feature of our approach. Instead of using the APE of a scalar prediction, we compute the confidence interval’s APECI with relation to the closest endpoint of the confidence interval \\(\\bar {\\Pi }_{\\alpha }\\) for an outlying true performance; we define APECI = 0 for an α confidence interval that includes the measured performance:\n\n$$ \\text{APE}_{\\mathit{CI}}(c) = \\frac{\\min_{\\forall \\pi \\in \\bar{\\pi}_{\\alpha}(c)} {\\big | \\pi_{\\mathit{true}}(c) - \\pi \\big |}}{\\pi_{\\mathit{true}}(c)} $$\n\n(22)\n\nHence, the MAPECI is the average over all APECI, similar to Equation 21. For our models πho and πhe, we report the MAPECI for predicted confidence intervals with αCI = 95% for highly confident predictions and αCI = 50% for less confident predictions, for which we expect a narrower interval and, consequently, a higher error.\n\n5.3.2 Results\n\nThe dotted lines in Fig. 4 illustrate a substantial decrease in prediction error when using a confidence interval rather than a scalar prediction. Note that we report in Fig. 4 only MAPECI’s for πhe; we provide similar results for πho at our supplementary Web site. Table 2 provides further data for πho. It reveals that the predicted confidence intervals for 7z, BDB-C, lrzip, and x264 contain all measured performance values when training the absolute model πho on \\(\\mathcal {T}_{3}\\).\n\nWe illustrate how more training samples allow P4 to decrease uncertainty in internal parameters to achieve better prediction accuracy using the variance inflation factor (VIF). The VIF is an indicator for multicollinearity, which can be computed for the activation values of an option oj in the training set \\(\\mathcal {T}\\). It is based on the coefficient of determination R2. To determine R2 for an option oj, we fit a linear regression function fj to predict whether oj is active in a configuration c ∖ oj with the remaining options as predictors.\n\nWe compute the VIF as follows:\n\n$$ \\begin{array}{@{}rcl@{}} \\text{VIF}_{j} & =& \\frac{1}{1-{R^{2}_{j}}} \\end{array} $$\n\n(23)\n\n$$ \\begin{array}{@{}rcl@{}} {R^{2}_{j}} & =& 1- \\frac{\\sum\\limits_{\\forall c \\in \\mathcal{T}} \\big(c(o_{j}) - \\bar{c}(o_{j})\\big)^{2}}{\\sum\\limits_{\\forall c \\in \\mathcal{T}}^{} \\big(c(o_{j}) - f_{j}(c \\setminus o_{j})\\big)^{2}} \\end{array} $$\n\n(24)\n\nA VIF of 0 indicates an option with no multicollinearity in the training set, while higher values mark increasingly problematic multicollinearity. We adopt the thresholds of 5 and 10 (O’Brien 2007; Wooldridge 2012) to indicate moderate and highly problematic multicollinearity, respectively.\n\nAlthough we could use the VIF as a filter for feature selection (cf. Section 3.3) to remove options with high multicollinearity in the training set, the computational effort required to calculate all \\({R^{2}_{j}}\\) makes it infeasible in practice. Hence, we compute the VIF only for the 5 most uncertain options in \\(\\mathcal {T}_{1}\\) to analyze whether multicollinearity is a possible cause for uncertainty of options’ influences. To compute the uncertainty of an option influence βj?, we use its confidence interval \\(\\bar {\\beta }_{j}\\) and point estimate \\(\\dot {\\beta }_{j}\\). To remove the influence of differing influence scales between software systems, we determine the scaled confidence interval width as the ratio of the absolute confidence interval width \\(| \\bar {\\beta }_{j} |\\) and the point estimate:\n\n$$ \\beta^?_{j} = \\frac{| \\bar{\\beta}_{j} |}{\\dot{\\beta}_{j}} $$\n\n(25)\n\nLooking at Table 3, we see that all five options exhibit either a moderate or even a high VIF for the training set \\(\\mathcal {T}_{1}\\). This points to a situation in which the learning procedure cannot safely assign a performance ratio to the specific option. Investigating this closer, four options are part of an alternative group despite our efforts to avoid multicollinearity by removing one alternative from each alternative group. For option threads_4, we found that it was active in almost every configuration (13 out of 16), reducing the contained information according to (7).\n\nTo further confirm our hypothesis that multicollinearity can be a possible cause, we show in Table 3 the uncertainty βj? and the VIF for these five options using the larger training set \\(\\mathcal {T}_{3}\\). We see a substantial reduction in uncertainty for three options in line with the reduction of the VIF. This strongly indicates that a reduced multicollinearity reduces also the uncertainty of an option’s influence on performance. Options Files_30 and BlockSize_1024 have no uncertainty as they were chosen by P4 to be removed from the alternative group in \\(\\mathcal {T}_{3}\\).\n\nOverall, πho yields better results than πhe in most cases, but both approaches always show substantially lower relative errors than scalar predictions. Of course, it would be easy for a model to predict all performance values correctly with a sufficiently large confidence interval. However, our findings for RQ3 demonstrate that P4’s prediction confidence intervals are reliable, as we will discuss in Section 5.4.\n\n5.4 RQ3: Reliability of Prediction Confidence Intervals\n\nContrary to RQ2, RQ3 is not concerned with the distance of predicted confidence intervals to the measured performance intervals. Instead, we are interested whether P4 judges the predicted uncertainty correctly and when predicted confidence intervals may be too wide or too narrow. If P4 finds the sweet spot between too wide and too narrow predictions, we call its prediction confidence intervals reliable.\n\n5.4.1 Setup\n\nAs predictions, our approach can yield confidence intervals with any given confidence level \\(\\alpha _{\\mathit {CI}} \\in \\left [0 \\%,100 \\%\\right ]\\). We call a model’s predicted confidence intervals reliable if predictions with an αCI confidence interval contain the measured performance with a similar observed frequency αobs (i.e., \\( \\alpha _{\\textit {obs}}\\left (\\alpha _{\\mathit {CI}}\\right ) \\approx \\alpha _{\\mathit {CI}}\\)). To compute the observed frequency αobs(αCI) for an αCI confidence interval, we first define the function within, which returns 1 if the measured performance πtrue(c) lies in a predicted confidence interval \\(\\bar {\\pi }(c)\\), and 0, otherwise:\n\n$$ \\text{within} \\big( \\pi_{\\mathit{true}}(c), \\bar{\\pi}(c) \\big) = \\begin{cases} 1 & \\pi_{\\mathit{true}}(c) \\in \\bar{\\pi}(c) \\\\ 0 & \\text{else} \\end{cases} $$\n\n(26)\n\nSecond, the observed frequency is computed as the average of within over all configurations of a subject system and their measured performance πtrue(c):\n\n$$ \\alpha_{obs}(\\alpha_{CI}) = \\frac{ {\\sum}_{c \\in \\mathcal{C}} \\text{within}\\left( \\pi_{\\mathit{true}}(c), \\bar{\\pi}(c) \\right ) }{\\left | \\mathcal{C} \\right |} $$\n\n(27)\n\nIf \\( \\alpha _{\\mathit {CI}} \\gg \\alpha _{\\textit {obs}}\\left (\\alpha _{\\mathit {CI}}\\right )\\), the predicted confidence interval is inaccurate more often than we expect and should have been broader; conversely, the predicted confidence interval should be more narrow and thus more informative if \\( \\alpha _{\\mathit {CI}} \\ll \\alpha _{\\textit {obs}}\\left (\\alpha _{\\mathit {CI}}\\right )\\). Since using confidence intervals for performance predictions is novel, we have no baseline to which we can compare. Hence, we report the observed frequencies for confidence levels αCI from 5% to 95% in steps of 5% as well as the average error in percentage to answer RQ3. In addition, we report the MAPECI for all confidence intervals.\n\n5.4.2 Results\n\nFigure 5 shows a calibration plot that compares αCI with αobs using dashed lines. A model with αCI = αobs for all αCI would yield values along the dashed gray diagonal. Values above the diagonal indicate too broad confidence intervals (i.e., our predictions are more accurate than they should be), values below it signal confidence intervals that are too narrow.\n\nThe solid lines in Fig. 5 show the mean MAPECI over all subject systems for both the relative and the absolute model. The shaded area around it constitutes a 95% confidence interval.\n\nWhen analyzing the dashed lines, we see that using the absolute error πho yields intervals that are closer to the diagonal than when using the relative error πhe. Moreover, there is a clear trend that, when using more measurements, the intervals become either nearly perfectly aligned or are underestimating the models prediction accuracy. Hence, we see a picture that resembles the picture when using the mode for scalar performance prediction: The approach requires a certain number of measurements to become accurate, but then works robustly.\n\nWe can make a further interesting observation when comparing the confidence intervals (dashed lines) with the MAPECI (solid lines). First and most importantly, we see that using confidence intervals of varying sizes has a clear monotonic relationship with the prediction error. That is, increasing the interval decreases the error. Second, the errors fall rapidly, especially for \\(\\mathcal {T}_{2}\\) and \\(\\mathcal {T}_{3}\\), already when using a narrow interval, such as 25%. This is good news as this clearly indicates that narrow confidence intervals yield accurate predictions. Third, we observe that (for the solid lines) the uncertainty is higher with fewer measurements, as indicated by the colored area. That is, the model is aware that the measurements are insufficient to actually make trustworthy predictions. This is a feature missing in all scalar prediction approaches. For example, for SPL Conqueror, there is no way to determine whether the model is confident with a certain prediction. With P4, we have a means to quantify this confidence.\n\n5.5 RQ4: Distribution of P4’ s Posteriors\n\nP4 uses a normal distribution as the default prior distribution for term influences. Variational inference—the first step in P4’s inference—uses these priors as an initial guess. Hence, the priors naturally influence the outcome of the inference. Therefore, we are interested in whether the posteriors inferred with P4 in our experiments challenge our choice of the default prior.\n\n5.5.1 Setup\n\nAs the result of MCMC sampling, the influence of each term is inferred as the marginal posterior πβ, which is represented as a set of 12,000 MCMC samples.\n\nAcross the set of all models π that were inferred in our previous experiments, including both πho and πhe, we first obtain the set of all marginal posteriors πβ:\n\n$$ {\\Pi}^{\\beta} = \\left\\{ \\pi^{\\beta} | \\forall \\beta \\in \\pi, \\forall \\pi \\in {\\Pi} \\right\\} $$\n\n(28)\n\nTo test for normality, we apply a Shapiro-Wilk (Shapiro and Wilk 1965) test and refute the h0 hypothesis of πβ being normally distributed for α < 0.05. For non-normal posterior distributions, we detect multi-modality with the dip test for uni-modality (Hartigan and Hartigan 1985). In addition, we compute the standardized skewness and the excess kurtosis (Zwillinger and Kokoska 1999) to quantify the effect size of non-normality for non-normal uni-modal posteriors. For skewness values outside [− 0.5,0.5], a distribution is considered skewed. In this case, the distribution has considerably more weight on one tail than the other. The excess kurtosis measures how the weight of a distribution’s tails deviates from those of a normal distribution. For positive excess kurtosis, the distribution has heavier tails than anticipated by the prior. Conversely, the distribution has thinner tails if the excess kurtosis is negative. Posterior distributions with heavy tails, such as the Cauchy distribution, may be problematic for the computation of confidence intervals because outliers of confident intervals approach \\(\\pm \\infty \\) quickly. This property makes confidence intervals of heavy-tailed distributions less useful and can lead to wildly varying single-point predictions; nevertheless, knowing the kurtosis can alarm a domain engineer, who would be otherwise ignorant of the inherent uncertainty given only a scalar influence of a traditional performance-influence model.\n\nTo evaluate the choice of normal-distributed marginal priors in an unbiased experiment setting, we replicate our previous experiments using flat marginal priors instead of normal-distributed ones. Flat priors have a probability of 0 for any possible influence. Thus, they avoid any bias through the choice of prior, but will result in more uncertainty in the marginal posteriors. We apply a Shapiro-Wilk test to determine the number of normal-distributed marginal posteriors after inference.\n\n5.5.2 Results\n\nWhen P4 uses flat priors instead of normal-distributed priors, 83% of all inferred marginal posterior distributions are normal-distributed using the \\(\\mathcal {T}_{1}\\) training set. Because we do not consider interactions for \\(\\mathcal {T}_{1}\\), P4 has enough training data to change the shape of the influence of the limited number of terms. Figure 6 shows that 48% and 55% of the flat priors are inferred normal-distributed with \\(\\mathcal {T}_{2}\\) and \\(\\mathcal {T}_{3}\\), even though the P4 models are more complex and require more training data to change the shape of the marginal priors. Our results indicate that, with enough training data, even flat priors are inferred normal-distributed.\n\nWhen P4 regularly uses normal-distributed priors, 82% of all inferred posterior distributions are normal-distributed, confirming the appropriateness of the choice of a normal-distributed prior. Still, 18% of posterior distributions are non-normal, for different reasons. Firstly, 35% of non-normal distributions are multi-modal. That is, these influences have two or more distinct value ranges of high probability. For these influences, P4 allows us to consider more than one probable influence value, while traditional performance-influence models consider only one. Secondly, 15% of non-normal distributions are skewed. This means that, for these distributions, P4’s scalar prediction will not be in the center of it’s confidence interval prediction. For skewed distributions, P4 therefore provides the information that, with respect to the most likely value, other likely values will be either higher or lower. Thirdly, we observe higher absolute kurtosis values for non-normal distributions, which is to be expected because normal distributions by definition have a excess kurtosis close to 0 whereas non-normal distributions may deviate. However, as Fig. 7 shows, there are distributions with kurtosis values as high as 160. This option Ref_9 of x264, which is inferred with the maximum kurtosis for \\(\\mathcal {T}_{1}\\) training data, sets the number of reference video frames to 9. It is part of an alternative group of other reference frame numbers. Due to P4’s pre-processing, we avoid multicollinearity problems such that Ref_9 has an unproblematic VIF of only 1.6 in \\(\\mathcal {T}_{1}\\) despite its membership in an alternative group. Hence, we conjecture that \\(\\mathcal {T}_{1}\\) may be too small to allow inference with low uncertainty in this case.\n\nInterestingly, Fig. 6 reveals that there are fewer non-normal marginal posteriors with increasing training set size. Increasing the training set size from \\(\\mathcal {T}_{1}\\) and \\(\\mathcal {T}_{2}\\) allows P4 to learn interactions and produces the largest decrease of normal-distributed marginal posteriors. We argue therefore that non-normality among marginal posteriors is partly an artifact of an undersized training set and an under-complex model structure (e.g., when no interactions are modeled). For example, P4 infers a non-zero distribution for only a single option (and the base influence) with BDB-C’s \\(\\mathcal {T}_{1}\\) training set, which consequently is inferred as non-normal. By contrast, P4 infers eight influential terms using \\(\\mathcal {T}_{2}\\). Overall, our results emphasize the limitations of regular point-estimate models and show that P4 provides fine-grained information on option and interaction influences. Moreover, by explicitly modeling uncertainty, we can, for the first time, rationalize about the size of the training set and its implications on the suitable model complexity.\n\n5.6 RQ5: The Effect of More Training Data on Uncertainty\n\nIn theory, increasing the training set size of a model should decrease its epistemic uncertainty. However, this expectation may hold only when we keep the model complexity (i.e., the number of terms) constant. We designed P4 such that, with more training data, P4 can build models with more terms. This way, the prediction uncertainty may increase despite lower uncertainty for individual terms, because adding up a large number of uncertainties may counteract that the uncertainties are smaller. Consequently, we take both the uncertainty within learned internal influences as well as the uncertainty in predictions into account. Studying this relationship of prediction uncertainty and uncertainty of model terms is of practical interest since increasing the complexity of the model with an increasing training set size is sensible (e.g., fitting a quadratic curve with a linear function will not work no matter how many training points we supply). So, there might be a fine line in the relation of the growth of the training set size and in the growth of model complexity. As mentioned in Section 3.2.1, we use Lasso to naturally limit the growth of the model complexity. RQ5 helps also answering whether Lasso is too restrictive or too lax in this sense.\n\n5.6.1 Setup\n\nTo study the effect of increasing the training data set size, we analyze P4 models trained on the \\(\\mathcal {T}_{2}\\) and \\(\\mathcal {T}_{3}\\) data sets. We exclude \\(\\mathcal {T}_{1}\\) data sets as P4 does not learn interaction influences on them. We quantify the uncertainty of P4 with two metrics: mean term-influence uncertainty and mean prediction uncertainty.\n\nMean term-influence uncertainty\n\nThe mean term-influence uncertainty \\(\\bar {\\pi }_{95 \\%}^{\\bar {\\beta }}\\) captures the uncertainty within the inferred influences of P4 models. We compute it by averaging the 95%-confidence interval widths \\(W(\\bar {\\pi }_{95 \\%}^{\\beta })\\) of all terms \\(\\beta \\in \\mathcal {I}\\) inside a given model π:\n\n$$ \\bar{\\pi}_{95 \\%}^{\\bar{\\beta}} = \\frac{{\\sum}_{\\beta \\in \\mathcal{I}} W(\\bar{\\pi}_{95 \\%}^{\\beta})}{| \\mathcal{I} | } $$\n\n(29)\n\nMean prediction uncertainty\n\nThe mean prediction uncertainty \\(\\overline {\\bar {\\pi }_{\\mathit {rel}}}(c)\\) captures the uncertainty in P4’s predictions. Similar to the MAPE, the mean prediction uncertainty is relative to the mode (i.e., the most likely value) of the prediction. However, instead of computing a prediction error in an evaluation setting where the target value is known, the mean prediction uncertainty relies solely on the predicted uncertainty. It is based on the scaled confidence interval width \\(\\bar {\\pi }_{\\mathit {rel}}(c)\\) for a given configuration \\(c \\in \\mathcal {C}\\), which we compute by dividing the 95%-confidence interval width \\(W(\\bar {\\pi }_{95 \\%}(c) )\\) by the prediction’s mode \\(\\dot {\\pi }(c)\\):\n\n$$ \\bar{\\pi}_{\\mathit{rel}}(c) = \\frac{W( \\bar{\\pi}_{95 \\%}(c) )}{\\dot{\\pi}(c)} $$\n\n(30)\n\nConsequently, the mean prediction uncertainty \\(\\overline {\\bar {\\pi }_{\\mathit {rel}}}\\) is the average across the \\(\\bar {\\pi }_{\\mathit {rel}}\\) of all valid configurations:\n\n$$ \\overline{\\bar{\\pi}_{\\mathit{rel}}} = \\frac{{\\sum}_{c \\in \\mathcal{C}} \\bar{\\pi}_{\\mathit{rel}}(c)}{ | \\mathcal{C} | } $$\n\n(31)\n\nAggregation\n\nFor a given software system, attribute, training set, and model type (πho and πhe), there are 5 models corresponding to the 5 repetitions we perform in our experiments. To report a difference between \\(\\mathcal {T}_{2}\\) and \\(\\mathcal {T}_{3}\\), we take the median of a metric for the P4 models resulting from the 5 repetitions. We choose the median because it is more robust against outlier repetitions compared to the mean.\n\n5.6.2 Results\n\nMean term-influence uncertainty\n\nThe median differences in uncertainty of using \\(\\mathcal {T}_{2}\\) versus \\(\\mathcal {T}_{3}\\) are detailed in Fig. 8 for all subject systems and attributes. Here, each bar shows the difference between the median \\(\\overline {\\bar {\\pi }_{\\mathit {rel}}}\\) of the 5 repetitions for \\(\\mathcal {T}_{2}\\) versus \\(\\mathcal {T}_{3}\\). Across all subject systems and attributes, we observe that P4 infers less uncertain influences using \\(\\mathcal {T}_{3}\\) with the notable exception of x264 (Energy) with πhe. In this instance, the median term-influence uncertainty increases by over 100% using \\(\\mathcal {T}_{3}\\) compared to \\(\\mathcal {T}_{2}\\). Moreover, one of the two inference failures in our experiments occurred for a repetition of the experiment using \\(\\mathcal {T}_{3}\\), while the other occurred for a repetition of the corresponding experiment using \\(\\mathcal {T}_{2}\\). We discuss possible reasons for this effect in Section 5.7. Excluding the πho result for x264 (Energy) with \\(\\mathcal {T}_{3}\\), the remaining term influence confidence intervals are 34% smaller in πho models and 45% smaller in πhe models trained on \\(\\mathcal {T}_{3}\\) in comparison to \\(\\mathcal {T}_{2}\\).\n\nWe present a more detailed view of individual term-influence uncertainty difference for two experimentsFootnote 3 on the left of Fig. 9. Here, each column color-encodes a term influence confidence interval width for P4 trained on \\(\\mathcal {T}_{2}\\) (upper row) and \\(\\mathcal {T}_{3}\\) (lower row), sorted in descending order for \\(\\mathcal {T}_{3}\\). Figure 9a shows that, for LLVM (Energy), influences inferred with \\(\\mathcal {T}_{2}\\) become less uncertain (less saturated) when using \\(\\mathcal {T}_{3}\\). P4 adds new terms with \\(\\mathcal {T}_{3}\\), which all are less uncertain than the most uncertain terms of \\(\\mathcal {T}_{2}\\).\n\nHowever, this observation is not consistent across all experiments. For example, we see in Fig. 9 that not all \\(\\bar {\\pi }_{95 \\%}^{\\beta }\\) become smaller for BerkeleyDBC (Time). We observe that most options that were more uncertain with \\(\\mathcal {T}_{2}\\) remain more uncertain with \\(\\mathcal {T}_{3}\\). The interaction between HAVE_CRYPTO and PS32K, the most uncertain term with \\(\\mathcal {T}_{2}\\), becomes considerably less uncertain with \\(\\mathcal {T}_{2}\\), whereas HAVE_CRYPTO, the most uncertain term with \\(\\mathcal {T}_{3}\\), was far less uncertain with \\(\\mathcal {T}_{2}\\). In this case, P4 selects too many interactions containing HAVE_CRYPTO, including the second-most uncertain term. We conjecture that \\(\\mathcal {T}_{3}\\) does not contain enough data to sufficiently differentiate between the influence of option HAVE_CRYPTO and its interactions. Next, we study whether additional interactions may lead to more uncertain predictions using the mean prediction uncertainty.\n\nMean prediction uncertainty\n\nP4 achieves decreased mean prediction uncertainty \\(\\overline {\\bar {\\pi }_{\\mathit {rel}}}(c)\\) for 4 out of 16 inference settings (software system & attribute) using πho and for 10 out of 16 inference settings using πhe. However, this improvement stays behind the reduction in term-influence uncertainty. The highly uncertain term influences of x264 (Energy) with πhe result in the highest increase of mean prediction uncertainty with factor 3870. This increase exceeds the mean term-influence uncertainty because the individual term-influence uncertainties accumulate for all the terms that are active in a predicted configuration. Excluding this inference setting, the mean prediction uncertainty using \\(\\mathcal {T}_{3}\\), on average, increases by 37% for πho and decreases by 4% for πhe in comparison to \\(\\mathcal {T}_{2}\\). Figure 10 visualizes the difference in mean prediction uncertainty of \\(\\mathcal {T}_{3}\\) versus \\(\\mathcal {T}_{2}\\) and illustrates that πhe performs better than πho overall. In particular, we present the prediction uncertainties of two inference settings on the right in Fig. 9. Figure 9b illustrates that πhe consistently increases its energy prediction uncertainty for LLVM configurations using \\(\\mathcal {T}_{3}\\). The uniformly distributed values for \\(\\mathcal {T}_{3}\\) stem from the uniformly distributed term influences uncertainties displayed in Fig. 9a. Moreover, we show πho time predictions for BerkeleyDBC in Fig. 9d. Similar to LLVM, the distribution of prediction uncertainties follows the distribution of its term-influence uncertainties in Fig. 9c. That is, as HAVE_CRYPTO’s uncertainty increases but the uncertainty of the interaction between HAVE_CRYPTO and PS32K decreases with \\(\\mathcal {T}_{3}\\), one subset of the most uncertain predictions with \\(\\mathcal {T}_{2}\\) becomes less uncertain while the another subset remains among the most uncertain predictions with \\(\\mathcal {T}_{2}\\).\n\nAlthough the mean prediction uncertainty does not improve for a number of systems and attributes, P4 still improves prediction accuracy in terms of MAPE and MAPECI as displayed in Fig. 4. That is, although the 95%-confidence interval width of predicted probability distributions does not become more narrow with \\(\\mathcal {T}_{3}\\) in general, its mode is still accurate enough to match the state of the art. Moreover, the uncertainty calibration in Fig. 5 shows that, with \\(\\mathcal {T}_{2}\\), both πho and πhe were overconfident in their 95%-confidence interval. This means that the correct value was not inside this interval for 95% of all predictions, but in only less than 70% of the cases. P4 improved its calibration with \\(\\mathcal {T}_{3}\\), which can be due to improved accuracy or adjusted confidence intervals. Our results contain both effects: P4 became more accurate as measured by the MAPECI and it increased predicted confidence interval widths. Moreover, Fig. 5 shows that πho becomes under-confident with \\(\\mathcal {T}_{3}\\), which means that it predicts confidence intervals that are wider than necessary. This is explainable by the mean prediction uncertainty of πho, which increases more than πhe. As both models incorporate the same terms and only differ in the ability to model a relative error, we conjecture that the relative error in πhe can model prediction error more accurately than πho given enough training data.\n\n5.7 RQ6: Unreliable Options and Their Cause\n\nEvaluation of frequentist point-estimate models on a test set cannot provide information about the cause of inaccurate predictions. By contrast, P4 provides a degree of uncertainty for each option and interaction involved in a prediction. This way, we can identify individual options and interactions that are responsible for uncertain predictions. We refer to these terms in a model as unreliable terms. Unreliable terms empower developers to identify code regions associated to these options that may exhibit large variations of performance or are uncertain by nature (e.g., due to non-determinism). Knowing unreliable terms also allows us to improve the composition of a training set by acquiring more samples that add information on the corresponding configuration options or interactions.\n\n5.7.1 Setup\n\nDuring prediction, P4 performs a convolution on the posteriors of the terms that are active in the given configuration. Hence, extraordinarily uncertain predictions must involve unreliable terms. We call a term unreliable if its influence’s confidence interval is substantially wider than the confidence intervals of the model’s remaining options and interactions. We quantify this property using the scaled confidence interval width \\(W_{\\mathit {rel}}(\\bar {\\pi }^{\\beta })\\), which specifies the degree of uncertainty relative to the mean within a model. It results from dividing a term’s 95%-confidence interval width \\(W(\\bar {\\pi }_{95 \\%}^{\\beta })\\) by the average 95%-confidence interval width of the model:\n\n$$ W_{\\mathit{rel}}(\\bar{\\pi}^{\\beta}) = \\frac{W(\\bar{\\pi}_{95 \\%}^{\\beta})}{ \\frac{1}{| \\mathcal{I} |} {\\sum}_{\\beta \\in \\mathcal{I}}W(\\bar{\\pi}_{95 \\%}^{\\beta})}. $$\n\n(32)\n\nTo answer RQ6, we concentrate on the top 5% terms with the largest scaled confidence interval widths. In what follows, we present the distribution of unreliable terms as well as a qualitative root-cause analysis on the 8 most unreliable terms across all experiments.\n\n5.7.2 Results\n\nThroughout all experiments, P4 identifies a similar number of unreliable options with both the πho (i.e., modeling an absolute error) and πhe (i.e., modeling a relative error) model: 541 unreliable terms using πho and 491 using πhe. In Fig. 11, we show the number of unreliable terms per subject system and model. Each row depicts the number of unreliable terms in the respective model. With both approaches, BerkeleyDBC, LLVM, PSQLDB, VP9 (Energy), and x264 have less than 30 unreliable terms across all training sets. Moreover, using πhe, P4 infers no unreliable terms for PSQLDB, VP9 (Energy), and x264 (Time). By contrast, using either πho or πhe, JavaGC and HIPAcc have over 60 unreliable terms. This shows that unreliable options are not common for all subject systems and attributes. We see that P4 finds the highest number of unreliable terms for the execution time of JavaGC using πho and for the execution time of VP9 using πhe. Both models yield a high number of unreliable terms for HIPAcc. At the same time, P4 identifies the highest number of terms for JavaGC, VP9, and HIPAcc, as Fig. 4 illustrates.\n\nA possible explanation could be that the absolute number of terms for these systems is high, but the relative number of uncertain terms remains moderate. To examine if P4 generally identifies more unreliable options for subject models with more terms, we compute Pearson’s correlation coefficient for the number of unreliable terms and the total number of terms inferred for each model in our experiments. The moderate correlation of r = 0.47 indicates that P4 models with more terms also tend to contain more unreliable terms. Furthermore, we color-coded the training sizes to compare the absolute number of unreliable terms per subject system. Figure 11 shows that many subject systems have fewer unreliable terms using \\(\\mathcal {T}_{1}\\) compared to \\(\\mathcal {T}_{2}\\) and \\(\\mathcal {T}_{3}\\), such as JavaGC and HIPAcc. By contrast, we find numerous unreliable terms using \\(\\mathcal {T}_{1}\\) for x264 (Time), PSQLDB (Energy), and 7z (Time). For these systems, P4 infers a similar number of terms for \\(\\mathcal {T}_{1}\\) and \\(\\mathcal {T}_{3}\\). This is in line with previous observations that more unreliable terms occur with more terms in a model.\n\nWe list the 8 terms with the highest scaled confidence interval width \\(W_{rel}(\\bar {\\pi }^{\\beta })\\) in Table 4. Out of these 8 most unreliable terms, the most common subject systems are x264, JavaGC, and VP9. In what follows, we therefore concentrate on these systems.\n\nx264\n\nCompared to the other subject systems, x264 (Energy) has few inferred terms using \\(\\mathcal {T}_{3}\\). Although x264 (Energy) matches the previously described trend of higher numbers of unreliable terms with a larger number of terms in a model, it has Ref_9 as the term with the highest overall \\(W_{\\mathit {rel}}(\\bar {\\pi }^{\\beta })\\) using πho. Ref_9 is part of an alternative group. Its 95%-confidence interval is 14 times as wide as the mean of all intervals in the inferred model and will drastically increase the uncertainty of predictions in which the option is active. In fact, we observed and discussed these highly uncertain predictions in RQ5. Despite our efforts, the \\(\\mathcal {T}_{3}\\) training data still has a highly problematic VIF of over 10 for this option, as well as for Ref_5 (the 7th-most unreliable term). This reduces the amount of information for these options that is present in the training data. As a result, the πho model cannot differentiate between the two alternative group options and the base influence (the third-most unreliable term) such that all three are unreliable. In addition, the influences of all three terms are multi-modal. Looking at Ref_9’s marginal posterior distribution in Fig. 12, we can clearly identify three modes at 1.1 kJ, 1.5 kJ, and 2.4 kJ. Thus, P4 has identified several probable value ranges due to remaining multicollinearity and the epistemic uncertainty due to the πho model structure. These observations result in an important insight: Alternative options may need individually more measurements to account for the inherent multicollinearity than other options. We are not aware of any existing sampling approach that adds to this insight.\n\nJavaGC\n\nAmong the most unreliable terms, we find two pairwise interactions for JavaGC between UseAdaptiveSizePolicy and options of the NewRatio alternative group. The two NewRatio options set the maximum ratio of the young versus the tenured generation in the Java heap to 1:16 (using πho) and 1:32 (using πhe), respectively. As the default NewRatio is 2, these values are comparably high and will lead to a small young generation and cause more frequent minor garbage collections. The UseAdaptiveSizePolicy option on its own will automatically adjust the young and the tenured generation size over time. However, activating both options is discouraged and will lead to unexpected program behavior. Figure 13 displays the marginal posterior distribution of the interaction between NewRatio_16 and UseAdaptiveSizePolicy in blue. In comparison to the marginal posterior histogram of the option UseAdaptiveSizePolicy from the same model shown in red, we see that, although both are normal-distributed, the influence of NewRatio_16 is more uncertain by a large margin. Again, this observation produces an insight not seen in related work: Uncertainties of interactions may be traced to external factors, and, henceforth, cannot be reduced by more measurements or a more complex model. P4 is a valuable detector of such scenarios.\n\nVP9\n\nVP9’s option cpuUsed_4 is over nine times more uncertain than the other influences in the πhe model trained on \\(\\mathcal {T}_{3}\\). This is due to the distance between its two modes at − 22s and − 5s, which are shown in Fig. 14. Similarly to x264’s option Ref_9, we find a highly problematic VIF of 23.7 in the training data for this option, explaining why cpuUsed_4 is unreliable. Interestingly, cpuUsed_4’s uncertainty is tied to the uncertainty of the base influence, which is multi-modal and has a \\(W_{\\mathit {rel}}(\\bar {\\pi }^{\\beta })\\) of over 9. In this instance, we did not entirely avoid the multicollinearity of the cpuUsed alternative group which allowed cpuUsed_4’s influence to be partly assigned to the base influence. Therefore, both become uncertain. However, despite sharing the same training data \\(\\mathcal {T}_{3}\\), the option cpuUsed_4 is never inferred unreliable in any πho model. To investigate this discrepancy, we display the base influence and cpuUsed_4’s influence for both a πho and a πhe model in Fig. 14. It shows that the πho influences for both terms are inferred uni-modal with the same spread as the individual modes. While the uni-modality of the πho model influences provide smaller confidence interval widths, it pushes the influence to more extreme values, which is an undesirable effect of multicollinearity. This effect is not limited to the aforementioned influences, but is present throughout all inferred influences of πho. Figure 15 visualizes all inferred influences of πho and πhe and shows that the modes of πho’s influences go beyond ± 40s with absolute confidence interval widths below 4s. These data do not hint at issues in the training data, whereas πhe’s influences of ± 20s with absolute confidence interval widths of up to 18s certainly do. This indicates that the πhe model is able to signal the uncertainty owing to multi-modality by inferring multiple modes. We therefore recommend πhe when multicollinearity cannot be ruled out entirely."
    }
}
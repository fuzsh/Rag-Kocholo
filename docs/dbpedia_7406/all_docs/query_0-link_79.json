{
    "id": "dbpedia_7406_0",
    "rank": 79,
    "data": {
        "url": "https://medium.com/%40ApacheDolphinScheduler/apache-dolphinscheduler-automated-packaging-and-standalone-cluster-deployment-a1fcfbd7c45d",
        "read_more_link": "",
        "language": "en",
        "title": "Apache DolphinScheduler Automated Packaging and Standalone/Cluster Deployment",
        "top_image": "https://miro.medium.com/v2/resize:fit:1200/1*0-0FwHNtwcjQOUwpQaTw8w.jpeg",
        "meta_img": "https://miro.medium.com/v2/resize:fit:1200/1*0-0FwHNtwcjQOUwpQaTw8w.jpeg",
        "images": [
            "https://miro.medium.com/v2/resize:fill:64:64/1*dmbNkD5D-u45r44go_cf0g.png",
            "https://miro.medium.com/v2/resize:fill:88:88/1*Xhp5andXLUcD4NDzUogK2A.jpeg",
            "https://miro.medium.com/v2/resize:fill:144:144/1*Xhp5andXLUcD4NDzUogK2A.jpeg"
        ],
        "movies": [],
        "keywords": [],
        "meta_keywords": [
            ""
        ],
        "tags": null,
        "authors": [
            "Apache DolphinScheduler",
            "medium.com"
        ],
        "publish_date": "2023-09-13T12:31:38.606000+00:00",
        "summary": "",
        "meta_description": "Apache DolphinScheduler is an open-source distributed task scheduling system designed to help users automate the scheduling and management of complex tasks. DolphinScheduler supports multiple task…",
        "meta_lang": "en",
        "meta_favicon": "https://miro.medium.com/v2/5d8de952517e8160e40ef9841c781cdc14a5db313057fa3c3de41c6f5b494b19",
        "meta_site_name": "Medium",
        "canonical_link": "https://medium.com/@ApacheDolphinScheduler/apache-dolphinscheduler-automated-packaging-and-standalone-cluster-deployment-a1fcfbd7c45d",
        "text": "Apache DolphinScheduler is an open-source distributed task scheduling system designed to help users automate the scheduling and management of complex tasks. DolphinScheduler supports multiple task types and can be run in both single-node and cluster environments. Below, we will introduce how to achieve automated packaging and single-node/cluster deployment of DolphinScheduler.\n\nAutomated Packaging\n\nPrerequisites: Maven, JDK\n\nTo perform code retrieval and packaging, execute the following shell script. The packaged output will be located at/opt/action/dolphinscheduler/dolphinscheduler-dist/target/apache-dolphinscheduler-dev-SNAPSHOT-bin.tar.gz.\n\n# Clone the repository and switch to the dev branch\n\nclone_init(){\n\nsudo su - root <<EOF\n\ncd /opt/action\n\ngit clone git@github.com:apache/dolphinscheduler.git\n\ncd Dolphinscheduler\n\ngit fetch origin dev\n\ngit checkout -b dev origin/dev\n\nEOF\n\n}\n\n# Build the project\n\nbuild(){\n\nsudo su - root <<EOF\n\ncd /opt/action/Dolphinscheduler\n\nmvn -B clean install -Prelease -Dmaven.test.skip=true -Dcheckstyle.skip=true -Dmaven.javadoc.skip=true\n\nEOF\n\n}\n\nStandalone Deployment\n\nRequired Environment\n\nJDK, ZooKeeper, MySQL\n\nSet up the required environment: JDK, ZooKeeper, and MySQL.\n\nInitialize the ZooKeeper environment (recommended version 3.8 or higher).\n\nYou can download the installation package from the official website.\n\nbash\n\nsudo su - root <<EOF\n\n# Navigate to the /opt directory (you can choose your installation directory)\n\ncd /opt\n\n# Extract the ZooKeeper installation package\n\ntar -xvf apache-zookeeper-3.8.0-bin.tar.gz\n\n# Rename the extracted folder\n\nsudo mv apache-zookeeper-3.8.0-bin zookeeper\n\n# Navigate to the zookeeper directory\n\ncd zookeeper/\n\n# Create a directory called zkData under /opt/zookeeper to store ZooKeeper data files\n\nmkdir zkData\n\n# Navigate to the conf folder\n\ncd conf/\n\n# Copy the zoo_sample.cfg file and rename it to zoo.cfg, as ZooKeeper recognizes only zoo.cfg as the configuration file\n\ncp zoo_sample.cfg zoo.cfg\n\n# Modify the zoo.cfg configuration\n\nsed -i 's/\\/tmp\\/zookeeper/\\/opt\\/zookeeper\\/conf/g' zoo.cfg\n\n# Stop any previous ZooKeeper services\n\nps -ef | grep QuorumPeerMain | grep -v grep | awk '{print \"kill -9 \" $2}' | sh\n\n# Use 'vim zoo.cfg' to further edit the zoo.cfg configuration\n\nsh /opt/zookeeper/bin/zkServer.sh start\n\nEOF\n\nJDK and MySQL installation details are omitted for brevity.\n\nInitialization Configuration\n\nConfiguration files initialization\n\nCreate directories for initialization files. (In this example, /opt/action/tool is used.)\n\nmkdir -p /opt/action/tool\n\nmkdir -p /opt/Dsrelease\n\nCreate an initialization file named common.properties in /opt/action/tool.\n\nLicensed to the Apache Software Foundation (ASF) under one or more\n\n# contributor license agreements. See the NOTICE file distributed with\n\n# this work for additional information regarding copyright ownership.\n\n# The ASF licenses this file to You under the Apache License, Version 2.0\n\n# (the \"License\"); you may not use this file except in compliance with\n\n# the License. You may obtain a copy of the License at\n\n#\n\n# http://www.apache.org/licenses/LICENSE-2.0\n\n#\n\n# Unless required by applicable law or agreed to in writing, software\n\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n\n# See the License for the specific language governing permissions and\n\n# limitations under the License.\n\n#\n\n# user data local directory path, please make sure the directory exists and have read write permissions\n\ndata.basedir.path=/tmp/dolphinscheduler# resource storage type: HDFS, S3, NONE\n\nresource.storage.type=HDFS# resource store on HDFS/S3 path, resource file will store to this hadoop hdfs path, self configuration, please make sure the directory exists on hdfs and have read write permissions. \"/dolphinscheduler\" is recommended\n\nresource.upload.path=/dolphinscheduler# whether to startup kerberos\n\nhadoop.security.authentication.startup.state=false# java.security.krb5.conf path\n\njava.security.krb5.conf.path=/opt/krb5.conf# login user from keytab username\n\nlogin.user.keytab.username=hdfs-mycluster@ESZ.COM# login user from keytab path\n\nlogin.user.keytab.path=/opt/hdfs.headless.keytab# kerberos expire time, the unit is hour\n\nkerberos.expire.time=2\n\n# resource view suffixs\n\n#resource.view.suffixs=txt,log,sh,bat,conf,cfg,py,java,sql,xml,hql,properties,json,yml,yaml,ini,js\n\n# if resource.storage.type=HDFS, the user must have the permission to create directories under the HDFS root path\n\nhdfs.root.user=root\n\n# if resource.storage.type=S3, the value like: s3a://dolphinscheduler; if resource.storage.type=HDFS and namenode HA is enabled, you need to copy core-site.xml and hdfs-site.xml to conf dir\n\nfs.defaultFS=file:///\n\naws.access.key.id=minioadmin\n\naws.secret.access.key=minioadmin\n\naws.region=us-east-1\n\naws.endpoint=http://localhost:9000\n\n# resourcemanager port, the default value is 8088 if not specified\n\nresource.manager.httpaddress.port=8088\n\n# if resourcemanager HA is enabled, please set the HA IPs; if resourcemanager is single, keep this value empty\n\nyarn.resourcemanager.ha.rm.ids=192.168.xx.xx,192.168.xx.xx\n\n# if resourcemanager HA is enabled or not use resourcemanager, please keep the default value; If resourcemanager is single, you only need to replace aws2 to actual resourcemanager hostname\n\nyarn.application.status.address=http://aws2:%s/ws/v1/cluster/apps/%s\n\n# job history status url when application number threshold is reached(default 10000, maybe it was set to 1000)\n\nyarn.job.history.status.address=http://aws2:19888/ws/v1/history/mapreduce/jobs/%s# datasource encryption enable\n\ndatasource.encryption.enable=false# datasource encryption salt\n\ndatasource.encryption.salt=!@#$%^&*# data quality option\n\ndata-quality.jar.name=dolphinscheduler-data-quality-dev-SNAPSHOT.jar#data-quality.error.output.path=/tmp/data-quality-error-data# Network IP gets priority, default inner outer# Whether hive SQL is executed in the same session\n\nsupport.hive.oneSession=false# use sudo or not, if set true, executing user is tenant user and deploy user needs sudo permissions; if set false, executing user is the deploy user and doesn't need sudo permissions\n\nsudo.enable=true# network interface preferred like eth0, default: empty\n\n#dolphin.scheduler.network.interface.preferred=# network IP gets priority, default: inner outer\n\n#dolphin.scheduler.network.priority.strategy=default# system env path\n\n#dolphinscheduler.env.path=dolphinscheduler_env.sh# development state\n\ndevelopment.state=false# rpc port\n\nalert.rpc.port=50052# Url endpoint for zeppelin RESTful API\n\nzeppelin.rest.url=http://localhost:8080\n\nCreate initialization files named core-site.xml\n\n<?xml-stylesheet type=\"text/xsl\" href=\"configuration.xsl\"?>\n\n<!--\n\nLicensed under the Apache License, Version 2.0 (the \"License\");\n\nyou may not use this file except in compliance with the License.\n\nYou may obtain a copy of the License at\n\nhttp://www.apache.org/licenses/LICENSE-2.0 Unless required by applicable law or agreed to in writing, software\n\ndistributed under the License is distributed on an \"AS IS\" BASIS,\n\nWITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n\nSee the License for the specific language governing permissions and\n\nlimitations under the License. See accompanying LICENSE file.\n\n--><!-- Put site-specific property overrides in this file. --><configuration>\n\n<property>\n\n<name>fs.defaultFS</name>\n\n<value>hdfs://aws1</value>\n\n</property>\n\n<property>\n\n<name>ha.zookeeper.quorum</name>\n\n<value>aws1:2181</value>\n\n</property>\n\n<property>\n\n<name>hadoop.proxyuser.root.hosts</name>\n\n<value>*</value>\n\n</property>\n\n<property>\n\n<name>hadoop.proxyuser.root.groups</name>\n\n<value>*</value>\n\n</property>\n\n</configuration>\n\nCreate initialization files hdfs-site.xml in /opt/action/tool.\n\n<?xml-stylesheet type=\"text/xsl\" href=\"configuration.xsl\"?>\n\n<!--\n\nLicensed under the Apache License, Version 2.0 (the \"License\");\n\nyou may not use this file except in compliance with the License.\n\nYou may obtain a copy of the License at\n\nhttp://www.apache.org/licenses/LICENSE-2.0 Unless required by applicable law or agreed to in writing, software\n\ndistributed under the License is distributed on an \"AS IS\" BASIS,\n\nWITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n\nSee the License for the specific language governing permissions and\n\nlimitations under the License. See accompanying LICENSE file.\n\n--><!-- Put site-specific property overrides in this file. --><configuration>\n\n<property>\n\n<name>dfs.replication</name>\n\n<value>1</value>\n\n</property>\n\n<property>\n\n<name>dfs.namenode.name.dir</name>\n\n<value>/opt/bigdata/hadoop/ha/dfs/name</value>\n\n</property>\n\n<property>\n\n<name>dfs.datanode.data.dir</name>\n\n<value>/opt/bigdata/hadoop/ha/dfs/data</value>\n\n</property>\n\n<property>\n\n<name>dfs.namenode.secondary.http-address</name>\n\n<value>aws2:50090</value>\n\n</property>\n\n<property>\n\n<name>dfs.namenode.checkpoint.dir</name>\n\n<value>/opt/bigdata/hadoop/ha/dfs/secondary</value>\n\n</property>\n\n<property>\n\n<name>dfs.nameservices</name>\n\n<value>aws1</value>\n\n</property>\n\n<property>\n\n<name>dfs.ha.namenodes.aws1</name>\n\n<value>nn1,nn2</value>\n\n</property>\n\n<property>\n\n<name>dfs.namenode.rpc-address.aws1.nn1</name>\n\n<value>aws1:8020</value>\n\n</property>\n\n<property>\n\n<name>dfs.namenode.rpc-address.aws1.nn2</name>\n\n<value>aws2:8020</value>\n\n</property>\n\n<property>\n\n<name>dfs.namenode.http-address.aws1.nn1</name>\n\n<value>aws1:50070</value>\n\n</property>\n\n<property>\n\n<name>dfs.namenode.http-address.aws1.nn2</name>\n\n<value>aws2:50070</value>\n\n</property>\n\n<property>\n\n<property>\n\n<name>dfs.datanode.address</name>\n\n<value>aws1:50010</value>\n\n</property>\n\n<property>\n\n<name>dfs.datanode.ipc.address</name>\n\n<value>aws1:50020</value>\n\n</property>\n\n<property>\n\n<name>dfs.datanode.http.address</name>\n\n<value>aws1:50075</value>\n\n</property>\n\n<property>\n\n<name>dfs.datanode.https.address</name>\n\n<value>aws1:50475</value>\n\n</property><name>dfs.namenode.shared.edits.dir</name>\n\n<value>qjournal://aws1:8485;aws2:8485;aws3:8485/mycluster</value>\n\n</property>\n\n<property>\n\n<name>dfs.journalnode.edits.dir</name>\n\n<value>/opt/bigdata/hadoop/ha/dfs/jn</value>\n\n</property>\n\n<property>\n\n<name>dfs.client.failover.proxy.provider.aws1</name>\n\n<value>org.apache.hadoop.hdfs.server.namenode.ha.ConfiguredFailoverProxyProvider</value>\n\n</property>\n\n<property>\n\n<name>dfs.ha.fencing.methods</name>\n\n<value>sshfence</value>\n\n</property>\n\n<property>\n\n<name>dfs.ha.fencing.ssh.private-key-files</name>\n\n<value>/root/.ssh/id_dsa</value>\n\n</property>\n\n<property>\n\n<name>dfs.ha.automatic-failover.enabled</name>\n\n<value>true</value>\n\n</property>\n\n</configuration>\n\nUpload the initialization JARs mysql-connector-java-8.0.16.jar and ojdbc8.jar to /opt/action/tool.\n\nReplace the initialization files\n\ncd /opt/Dsrelease\n\nsudo rm -r $today/\n\necho \"rm -r $today\"cd /opt/releasecp $packge_tar /opt/Dsreleasecd /opt/Dsreleasetar -zxvf $packge_tar\n\nmv $packge $today\n\np_api_lib=/opt/Dsrelease/$today/api-server/libs/\n\np_master_lib=/opt/Dsrelease/$today/master-server/libs/\n\np_worker_lib=/opt/Dsrelease/$today/worker-server/libs/\n\np_alert_lib=/opt/Dsrelease/$today/alert-server/libs/\n\np_tools_lib=/opt/Dsrelease/$today/tools/libs/\n\np_st_lib=/opt/Dsrelease/$today/standalone-server/libs/\n\np_api_conf=/opt/Dsrelease/$today/api-server/conf/\n\np_master_conf=/opt/Dsrelease/$today/master-server/conf/\n\np_worker_conf=/opt/Dsrelease/$today/worker-server/conf/\n\np_alert_conf=/opt/Dsrelease/$today/alert-server/conf/\n\np_tools_conf=/opt/Dsrelease/$today/tools/conf/\n\np_st_conf=/opt/Dsrelease/$today/standalone-server/conf/\n\ncp $p0 $p4 $p_api_lib\n\ncp $p0 $p4 $p_master_lib\n\ncp $p0 $p4 $p_worker_lib\n\ncp $p0 $p4 $p_alert_lib\n\ncp $p0 $p4 $p_tools_lib\n\ncp $p0 $p4 $p_st_libecho \"cp $p0 $p_api_lib\"cp $p1 $p2 $p3 $p_api_conf\n\ncp $p1 $p2 $p3 $p_master_conf\n\ncp $p1 $p2 $p3 $p_worker_conf\n\ncp $p1 $p2 $p3 $p_alert_conf\n\ncp $p1 $p2 $p3 $p_tools_conf\n\ncp $p1 $p2 $p3 $p_st_confecho \"cp $p1 $p2 $p3 $p_api_conf\"\n\n}define_param(){packge_tar=apache-dolphinscheduler-dev-SNAPSHOT-bin.tar.gz\n\npackge=apache-dolphinscheduler-dev-SNAPSHOT-bin\n\np0=/opt/action/tool/mysql-connector-java-8.0.16.jar\n\np1=/opt/action/tool/common.properties\n\np2=/opt/action/tool/core-site.xml\n\np3=/opt/action/tool/hdfs-site.xml\n\np4=/opt/action/tool/ojdbc8.jartoday=`date +%m%d`\n\n}\n\nReplace the configuration file content\n\nsed -i 's/spark2/spark/g' /opt/Dsrelease/$today/worker-server/conf/dolphinscheduler_env.sh\n\ncd /opt/Dsrelease/$today/bin/env/\n\nsed -i '$a\\export SPRING_PROFILES_ACTIVE=permission_shiro' dolphinscheduler_env.sh\n\nsed -i '$a\\export DATABASE=\"mysql\"' dolphinscheduler_env.sh\n\nsed -i '$a\\export SPRING_DATASOURCE_DRIVER_CLASS_NAME=\"com.mysql.jdbc.Driver\"' dolphinscheduler_env.sh\n\n#自定义修改mysql配置\n\nsed -i '$a\\export SPRING_DATASOURCE_URL=\"jdbc:mysql://ctyun6:3306/dolphinscheduler?useUnicode=true&characterEncoding=UTF-8&allowMultiQueries=true&allowPublicKeyRetrieval=true\"' dolphinscheduler_env.sh\n\nsed -i '$a\\export SPRING_DATASOURCE_USERNAME=\"root\"' dolphinscheduler_env.sh\n\nsed -i '$a\\export SPRING_DATASOURCE_PASSWORD=\"root@123\"' dolphinscheduler_env.sh\n\necho \"替换jdbc配置成功\"\n\n#自定义修改zookeeper配置\n\nsed -i '$a\\export REGISTRY_TYPE=${REGISTRY_TYPE:-zookeeper}' dolphinscheduler_env.sh\n\nsed -i '$a\\export REGISTRY_ZOOKEEPER_CONNECT_STRING=${REGISTRY_ZOOKEEPER_CONNECT_STRING:-ctyun6:2181}' dolphinscheduler_env.sh\n\necho \"替换zookeeper配置成功\"\n\nsed -i 's/resource.storage.type=HDFS/resource.storage.type=NONE/' /opt/Dsrelease/$today/master-server/conf/common.properties\n\nsed -i 's/resource.storage.type=HDFS/resource.storage.type=NONE/' /opt/Dsrelease/$today/worker-server/conf/common.properties\n\nsed -i 's/resource.storage.type=HDFS/resource.storage.type=NONE/' /opt/Dsrelease/$today/alert-server/conf/common.properties\n\nsed -i 's/resource.storage.type=HDFS/resource.storage.type=NONE/' /opt/Dsrelease/$today/api-server/conf/common.properties\n\nsed -i 's/hdfs.root.user=root/resource.hdfs.root.user=root/' /opt/Dsrelease/$today/master-server/conf/common.properties\n\nsed -i 's/hdfs.root.user=root/resource.hdfs.root.user=root/' /opt/Dsrelease/$today/worker-server/conf/common.properties\n\nsed -i 's/hdfs.root.user=root/resource.hdfs.root.user=root/' /opt/Dsrelease/$today/alert-server/conf/common.properties\n\nsed -i 's/hdfs.root.user=root/resource.hdfs.root.user=root/' /opt/Dsrelease/$today/api-server/conf/common.properties\n\nsed -i 's/fs.defaultFS=file:/resource.fs.defaultFS=file:/' /opt/Dsrelease/$today/master-server/conf/common.properties\n\nsed -i 's/fs.defaultFS=file:/resource.fs.defaultFS=file:/' /opt/Dsrelease/$today/worker-server/conf/common.properties\n\nsed -i 's/fs.defaultFS=file:/resource.fs.defaultFS=file:/' /opt/Dsrelease/$today/alert-server/conf/common.properties\n\nsed -i 's/fs.defaultFS=file:/resource.fs.defaultFS=file:/' /opt/Dsrelease/$today/api-server/conf/common.properties\n\nsed -i '$a\\resource.hdfs.fs.defaultFS=file:///' /opt/Dsrelease/$today/api-server/conf/common.properties\n\necho \"替换common.properties配置成功\"\n\n# 替换master worker内存 api alert也可进行修改，具体根据当前服务器硬件配置而定，但要遵循Xms=Xmx=2Xmn的规律\n\ncd /opt/Dsrelease/$today/\n\nsed -i 's/Xms4g/Xms2g/g' worker-server/bin/start.sh\n\nsed -i 's/Xmx4g/Xmx2g/g' worker-server/bin/start.sh\n\nsed -i 's/Xmn2g/Xmn1g/g' worker-server/bin/start.sh\n\nsed -i 's/Xms4g/Xms2g/g' master-server/bin/start.sh\n\nsed -i 's/Xmx4g/Xmx2g/g' master-server/bin/start.sh\n\nsed -i 's/Xmn2g/Xmn1g/g' master-server/bin/start.sh\n\necho \"master worker内存修改完成\"}\n\nDelete HDFS configuration\n\necho \"start to delete hdfs configuration\"\n\nsudo rm /opt/Dsrelease/$today/api-server/conf/core-site.xml\n\nsudo rm /opt/Dsrelease/$today/api-server/conf/hdfs-site.xml\n\nsudo rm /opt/Dsrelease/$today/worker-server/conf/core-site.xml\n\nsudo rm /opt/Dsrelease/$today/worker-server/conf/hdfs-site.xml\n\nsudo rm /opt/Dsrelease/$today/master-server/conf/core-site.xml\n\nsudo rm /opt/Dsrelease/$today/master-server/conf/hdfs-site.xml\n\nsudo rm /opt/Dsrelease/$today/alert-server/conf/core-site.xml\n\nsudo rm /opt/Dsrelease/$today/alert-server/conf/hdfs-site.xml\n\necho \"stop deleting hdfs configuration\"\n\n}\n\nMySQL initialization\n\ninit_mysql(){\n\nsql_path=\"/opt/Dsrelease/$today/tools/sql/sql/dolphinscheduler_mysql.sql\"\n\nsourceCommand=\"source $sql_path\"\n\necho $sourceCommand\n\necho \" start source：\"\n\nmysql -hlocalhost -uroot -proot@123 -D \"dolphinscheduler\" -e \"$sourceCommand\"\n\necho \"stop source：\"\n\n}\n\nStart DolphinScheduler service\n\nstop_all_server(){\n\ncd /opt/Dsrelease/$today\n\n./bin/dolphinscheduler-daemon.sh stop api-server\n\n./bin/dolphinscheduler-daemon.sh stop master-server\n\n./bin/dolphinscheduler-daemon.sh stop worker-server\n\n./bin/dolphinscheduler-daemon.sh stop alert-server\n\nps -ef|grep api-server|grep -v grep|awk '{print \"kill -9 \" $2}' |sh\n\nps -ef|grep master-server |grep -v grep|awk '{print \"kill -9 \" $2}' |sh\n\nps -ef|grep worker-server |grep -v grep|awk '{print \"kill -9 \" $2}' |sh\n\nps -ef|grep alert-server |grep -v grep|awk '{print \"kill -9 \" $2}' |sh\n\n}\n\nrun_all_server(){\n\ncd /opt/Dsrelease/$today\n\n./bin/dolphinscheduler-daemon.sh start api-server\n\n./bin/dolphinscheduler-daemon.sh start master-server\n\n./bin/dolphinscheduler-daemon.sh start worker-server\n\n./bin/dolphinscheduler-daemon.sh start alert-server\n\n}\n\nCluster Deployment\n\nOpen external ports for MySQL and ZooKeeper.\n\nDeploy the clusters and start\n\nCopy the initialized folders to the specified servers and start the designated services to complete the cluster deployment. Make sure to include a common ZooKeeper and MySQL instance."
    }
}
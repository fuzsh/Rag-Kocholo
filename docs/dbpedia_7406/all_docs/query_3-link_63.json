{
    "id": "dbpedia_7406_3",
    "rank": 63,
    "data": {
        "url": "https://docs.aws.amazon.com/sagemaker/latest/dg/distributed-training.html",
        "read_more_link": "",
        "language": "en",
        "title": "Distributed training in Amazon SageMaker",
        "top_image": "https://docs.aws.amazon.com/assets/images/favicon.ico",
        "meta_img": "https://docs.aws.amazon.com/assets/images/favicon.ico",
        "images": [
            "https://docs.aws.amazon.com/images/sagemaker/latest/dg/images/distributed/Distributed-Training-in-SageMaker-image.png",
            "https://d1ge0kk1l5kms0.cloudfront.net/images/G/01/webservices/console/warning.png"
        ],
        "movies": [],
        "keywords": [],
        "meta_keywords": [
            "SageMaker",
            "Amazon SageMaker",
            "machine learning",
            "notebook instance"
        ],
        "tags": null,
        "authors": [],
        "publish_date": null,
        "summary": "",
        "meta_description": "Learn about distributed training in Amazon SageMaker.",
        "meta_lang": "en",
        "meta_favicon": "/assets/images/favicon.ico",
        "meta_site_name": "",
        "canonical_link": "https://docs.aws.amazon.com/sagemaker/latest/dg/distributed-training.html",
        "text": "SageMaker provides distributed training libraries and supports various distributed training options for deep learning tasks such as computer vision (CV) and natural language processing (NLP). With SageMakerâs distributed training libraries, you can run highly scalable and cost-effective custom data parallel and model parallel deep learning training jobs. You can also use other distributed training frameworks and packages such as PyTorch DistributedDataParallel (DDP), torchrun, MPI (mpirun), and parameter server. Throughout the documentation, instructions and examples focus on how to set up the distributed training options for deep learning tasks using the SageMaker Python SDK.\n\nBefore you get started\n\nSageMaker Training supports distributed training on a single instance as well as multiple instances, so you can run any size of training at scale. We recommend you to use the framework estimator classes such as PyTorch and TensorFlow in the SageMaker Python SDK, which are the training job launchers with various distributed training options. When you create an estimator object, the object sets up distributed training infrastructure, runs the CreateTrainingJob API in the backend, finds the Region where your current session is running, and pulls one of the pre-built AWS deep learning container prepackaged with a number of libraries including deep learning frameworks, distributed training frameworks, and the EFA driver. If you want to mount an FSx file system to the training instances, you need to pass your VPC subnet and security group ID to the estimator. Before running your distributed training job in SageMaker, read the following general guidance on the basic infrastructure setup.\n\nAvailability zones and network backplane\n\nWhen using multiple instances (also called nodes), itâs important to understand the network that connects the instances, how they read the training data, and how they share information between themselves. For example, when you run a distributed data-parallel training job, a number of factors, such as communication between the nodes of a compute cluster for running the AllReduce operation and data transfer between the nodes and data storage in Amazon Simple Storage Service or Amazon FSx for Lustre, play a crucial role to achieve an optimal use of compute resources and a faster training speed. To reduce communication overhead, make sure that you configure instances, VPC subnet, and data storage in the same AWS Region and Availability Zone.\n\nGPU instances with faster network and high-throughput storage\n\nYou can technically use any instances for distributed training. For cases where you need to run multi-node distributed training jobs for training large models, such as large language models (LLMs) and diffusion models, which require faster inter-node commutation, we recommend EFA-enabled GPU instances supported by SageMaker. Especially, to achieve the most performant distributed training job in SageMaker, we recommend P4d and P4de instances equipped with NVIDIA A100 GPUs. These are also equipped with high-throughput low-latency local instance storage and faster intra-node network. For data storage, we recommend Amazon FSx for Lustre that provides high throughput for storing training datasets and model checkpoints.\n\nGet started with distributed training in Amazon SageMaker\n\nIf youâre already familiar with distributed training, choose one of the following options that matches your preferred strategy or framework to get started. If you want to learn about distributed training in general, see Basic distributed training concepts.\n\nThe SageMaker distributed training libraries are optimized for the SageMaker training environment, help adapt your distributed training jobs to SageMaker, and improve training speed and throughput. The libraries offer both data parallel and model parallel training strategies. They combine software and hardware technologies to improve inter-GPU and inter-node communications, and extend SageMakerâs training capabilities with built-in options that require minimal code changes to your training scripts.Â\n\nUse the SageMaker distributed data parallelism (SMDDP) library\n\nThe SMDDP library improves communication between nodes with implementations of AllReduce and AllGather collective communication operations that are optimized for AWS network infrastructure and Amazon SageMaker ML instance topology. You can use the SMDDP library as the backend of PyTorch-based distributed training packages: PyTorch distributed data parallel (DDP), PyTorch fully sharded data parallelism (FSDP), DeepSpeed, and Megatron-DeepSpeed. The following code example shows how to set a PyTorch estimator for launching a distributed training job on two ml.p4d.24xlarge instances.\n\nfrom sagemaker.pytorch import PyTorch estimator = PyTorch( ..., instance_count=2, instance_type=\"ml.p4d.24xlarge\", # Activate distributed training with SMDDP distribution={ \"pytorchddp\": { \"enabled\": True } } # mpirun, activates SMDDP AllReduce OR AllGather # distribution={ \"torch_distributed\": { \"enabled\": True } } # torchrun, activates SMDDP AllGather # distribution={ \"smdistributed\": { \"dataparallel\": { \"enabled\": True } } } # mpirun, activates SMDDP AllReduce OR AllGather )\n\nTo learn how to prepare your training script and launch a distributed data-parallel training job on SageMaker, see Run distributed training with the SageMaker distributed data parallelism library.\n\nUse the SageMaker model parallelism library (SMP)\n\nSageMaker provides the SMP library and supports various distributed training techniques, such as sharded data parallelism, pipelining, tensor parallelism, optimizer state sharding, and more. To learn more about what the SMP library offers, see Core Features of the SageMaker Model Parallelism Library.\n\nTo use SageMaker's model parallelism library, configure the distribution parameter of the SageMaker framework estimators. Supported framework estimators are PyTorch and TensorFlow. The following code example shows how to construct a framework estimator for distributed training with the model parallelism library on two ml.p4d.24xlarge instances.\n\nfrom sagemaker.framework import Framework distribution={ \"smdistributed\": { \"modelparallel\": { \"enabled\":True, \"parameters\": { ... # enter parameter key-value pairs here } }, }, \"mpi\": { \"enabled\" : True, ... # enter parameter key-value pairs here } } estimator = Framework( ..., instance_count=2, instance_type=\"ml.p4d.24xlarge\", distribution=distribution )\n\nTo learn how to adapt your training script, configure distribution parameters in the estimator class, and launch a distributed training job, see SageMaker's model parallelism library (see also Distributed Training APIs in the SageMaker Python SDK documentation).\n\nUse open source distributed training frameworks\n\nSageMaker also supports the following options to operate mpirun and torchrun in the backend.\n\nBasic distributed training concepts\n\nSageMakerâs distributed training libraries use the following distributed training terms and features.\n\nDatasets and Batches\n\nTraining\n\nInstances and GPUs\n\nDistributed Training Solutions\n\nAdvanced concepts\n\nMachine Learning (ML) practitioners commonly face two scaling challenges when training models: scaling model size and scaling training data.Â While model size and complexity can result in better accuracy, there is a limit to the model size you can fit into a single CPU or GPU. Furthermore, scaling model size may result in more computations and longer training times.\n\nNot all models handle training data scaling equally well because they need to ingest all the training data in memory for training. They only scale vertically, and to bigger and bigger instance types. In most cases, scaling training data results in longer training times.\n\nDeep Learning (DL) is a specific family of ML algorithms consisting of several layers of artificial neural networks. The most common training method is with mini-batch Stochastic Gradient Descent (SGD). In mini-batch SGD, the model is trained by conducting small iterative changes of its coefficients in the direction that reduces its error. Those iterations are conducted on equally sized subsamples of the training dataset called mini-batches. For each mini-batch, the model is run in each record of the mini-batch, its error measured and the gradient of the error estimated. Then the average gradient is measured across all the records of the mini-batch and provides an update direction for each model coefficient. One full pass over the training dataset is called an epoch. Model trainings commonly consist of dozens to hundreds of epochs. Mini-batch SGD has several benefits: First, its iterative design makes training time theoretically linear of dataset size. Second, in a given mini-batch each record is processed individually by the model without need for inter-record communication other than the final gradient average. The processing of a mini-batch is consequently particularly suitable for parallelization and distribution.Â\n\nParallelizing SGD training by distributing the records of a mini-batch over different computing devices is calledÂ data parallel distributed training, and is the most commonly used DL distribution paradigm. Data parallel training is a relevant distribution strategy to scale the mini-batch size and process each mini-batch faster. However, data parallel training comes with the extra complexity of having to compute the mini-batch gradient average with gradients coming from all the workers and communicating it to all the workers, a step called allreduce that can represent a growing overhead, as the training cluster is scaled, and that can also drastically penalize training time if improperly implemented or implemented over improper hardware subtracts.Â\n\nData parallel SGD still requires developers to be able to fit at least the model and a single record in a computing device, such as a single CPU or GPU. When training very large models such as large transformers in Natural Language Processing (NLP), or segmentation models over high-resolution images, there may be situations in which this is not feasible. An alternative way to break up the workload is to partition the model over multiple computing devices, an approach called model-parallel distributed training.\n\nStrategies\n\nDistributed training is usually split by two approaches: data parallel and model parallel. Data parallel is the most common approach to distributed training: You have a lot of data, batch it up, and send blocks of data to multiple CPUs or GPUs (nodes) to be processed by the neural network or ML algorithm, then combine the results. The neural network is the same on each node. A model parallel approach is used with large models that wonât fit in a nodeâs memory in one piece; it breaks up the model and places different parts on different nodes. In this situation, you need to send your batches of data out to each node so that the data is processed on all parts of the model.\n\nThe terms network and model are often used interchangeably: A large model is really a large network with many layers and parameters. Training with a large network produces a large model, and loading the model back onto the network with all your pre-trained parameters and their weights loads a large model into memory. When you break apart a model to split it across nodes, youâre also breaking apart the underlying network. A network consists of layers, and to split up the network, you put layers on different compute devices.\n\nA common pitfall of naively splitting layers across devices is severe GPU under-utilization. Training is inherently sequential in both forward and backward passes, and at a given time, only one GPU can actively compute, while the others wait on the activations to be sent. Modern model parallel libraries solve this problem by using pipeline execution schedules to improve device utilization. However, only the Amazon SageMaker's distributed model parallel library includes automatic model splitting. The two core features of the library, automatic model splitting and pipeline execution scheduling, simplifies the process of implementing model parallelism by making automated decisions that lead to efficient device utilization.\n\nTrain with data parallel and model parallel\n\nIf you are training with a large dataset, start with a data parallel approach. If you run out of memory during training, you may want to switch to a model parallel approach, or try hybrid model and data parallelism. You can also try the following to improve performance with data parallel:\n\nTry gradient compression (FP16, INT8):\n\nTry reducing the input size:\n\nCheck if you use batch normalization, since this can impact convergence. When you use distributed training, your batch is split across GPUs and the effect of a much lower batch size can be a higher error rate thereby disrupting the model from converging. For example, if you prototyped your network on a single GPU with a batch size of 64, then scaled up to using four p3dn.24xlarge, you now have 32 GPUs and your per-GPU batch size drops from 64 to 2. This will likely break the convergence you saw with a single node.\n\nStart with model-parallel training when:\n\nTo learn more about the SageMaker distributed libraries, see the following:\n\nOptimize distributed training\n\nCustomize hyperparameters for your use case and your data to get the best scaling efficiency. In the following discussion, we highlight some of the most impactful training variables and provide references to state-of-the-art implementations so you can learn more about your options. Also, we recommend that you refer to your preferred frameworkâs distributed training documentation.\n\nBatch Size\n\nSageMaker distributed toolkits generally allow you to train on bigger batches. For example, if a model fits within a single device but can only be trained with a small batch size, using either model-parallel training or data parallel training enables you to experiment with larger batch sizes.\n\nBe aware that batch size directly influences model accuracy by controlling the amount of noise in the model update at each iteration. Increasing batch size reduces the amount of noise in the gradient estimation, which can be beneficial when increasing from very small batches sizes, but can result in degraded model accuracy as the batch size increases to large values.Â\n\nA number of techniques have been developed to maintain good model convergence when batch is increased.\n\nMini-batch size\n\nIn SGD, the mini-batch size quantifies the amount of noise present in the gradient estimation. A small mini-batch results in a very noisy mini-batch gradient, which is not representative of the true gradient over the dataset. A large mini-batch results in a mini-batch gradient close to the true gradient over the dataset and potentially not noisy enoughâlikely to stay locked in irrelevant minima.\n\nTo learn more about these techniques, see the following papers:\n\nScenarios\n\nThe following sections cover scenarios in which you may want to scale up training, and how you can do so using AWS resources.\n\nScaling from a Single GPU to Many GPUs\n\nThe amount of data or the size of the model used in machine learning can create situations in which the time to train a model is longer that you are willing to wait. Sometimes, the training doesnât work at all because the model or the training data is too large. OneÂ solution is to increase the number of GPUs you use for training.Â On an instance with multiple GPUs, like a p3.16xlarge that has eight GPUs, the data and processing is split across the eight GPUs. When you use distributed training libraries, this can result in a near-linear speedup in the time it takes to train your model. It takes slightly over 1/8 the time it would have taken on p3.2xlarge with one GPU.\n\nScaling from a single instance to multiple instances\n\nIf you want to scale your training even further, you can use more instances. However, you should choose a larger instance type before you add more instances. Review the previous table to see how many GPUs are in each p3 instance type.\n\nIf you have made the jump from a single GPU on a p3.2xlarge to four GPUs on a p3.8xlarge, but decide that you require more processing power, you may see better performance and incur lower costs if you choose a p3.16xlarge before trying to increase instance count. Depending on the libraries you use, when you keep your training on a single instance, performance is better and costs are lower than a scenario where you use multiple instances.\n\nWhen you are ready to scale the number of instances, you can do this with SageMaker Python SDKÂ estimatorÂ function by setting yourÂ instance_count. For example, you can set instance_type = p3.16xlarge andÂ Â instance_count = 2. Instead of the eight GPUs on a single p3.16xlarge, you have 16 GPUs across two identical instances. The following chart shows scaling and throughput starting with eight GPUs on a single instance and increasing to 64 instances for a total of 256 GPUs.\n\nCustom training scripts\n\nWhile SageMaker makes it simple to deploy and scale the number of instances and GPUs, depending on your framework of choice, managing the data and results can be very challenging, which is why external supporting libraries are often used.Â This most basic form of distributed training requires modification of your training script to manage the data distribution."
    }
}
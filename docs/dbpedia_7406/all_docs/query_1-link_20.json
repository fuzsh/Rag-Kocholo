{
    "id": "dbpedia_7406_1",
    "rank": 20,
    "data": {
        "url": "https://arxiv.org/html/2402.05979v1",
        "read_more_link": "",
        "language": "en",
        "title": "1 Introduction",
        "top_image": "",
        "meta_img": "",
        "images": [
            "https://arxiv.org/html/x1.png",
            "https://arxiv.org/html/x2.png",
            "https://arxiv.org/html/extracted/5395897/interfaceDesign_LG3.png",
            "https://arxiv.org/html/extracted/5395897/treemap_big.png"
        ],
        "movies": [],
        "keywords": [],
        "meta_keywords": [
            ""
        ],
        "tags": null,
        "authors": [],
        "publish_date": null,
        "summary": "",
        "meta_description": "",
        "meta_lang": "en",
        "meta_favicon": "",
        "meta_site_name": "",
        "canonical_link": null,
        "text": "License: arXiv.org perpetual non-exclusive license\n\narXiv:2402.05979v1 [cs.SE] 07 Feb 2024\n\nmarginparsep has been altered.\n\ntopmargin has been altered.\n\nmarginparpush has been altered.\n\nThe page layout violates the ICML style. Please do not change the page layout, or include packages like geometry, savetrees, or fullpage, which change it for you. We’re not able to reliably undo arbitrary changes to the style. Please remove the offending package(s), or layout-changing commands and try again.\n\nOn the Standardization of Behavioral Use Clauses and Their Adoption for Responsible Licensing of AI\n\nDaniel McDuff Tim Korjakow Scott Cambo Jesse Josua Benjamin Jenny Lee Yacine Jernite Carlos Muñoz Ferrandis Aaron Gokaslan Alek Tarkowski Joseph Lindley A. Feder Cooper Danish Contractor\n\nAbstract\n\nGrowing concerns over negligent or malicious uses of AI have increased the appetite for tools that help manage the risks of the technology. In 2018, licenses with behaviorial-use clauses (commonly referred to as Responsible AI Licenses) were proposed to give developers a framework for releasing AI assets while specifying their users to mitigate negative applications. As of the end of 2023, on the order of 40,000 software and model repositories have adopted responsible AI licenses licenses. Notable models licensed with behavioral use clauses include BLOOM (language) and LLaMA2 (language), Stable Diffusion (image), and GRID (robotics). This paper explores why and how these licenses have been adopted, and why and how they have been adapted to fit particular use cases. We use a mixed-methods methodology of qualitative interviews, clustering of license clauses, and quantitative analysis of license adoption. Based on this evidence we take the position that responsible AI licenses need standardization to avoid confusing users or diluting their impact. At the same time, customization of behavioral restrictions is also appropriate in some contexts (e.g., medical domains). We advocate for “standardized customization” that can meet users’ needs and can be supported via tooling.\n\n1 Introduction\n\nOpenness is a tenet of scientific research and plays an important role in the development of new technologies. By making assets available to third parties, scientific results can be verified, and systems can be interrogated, tested and audited (Resnick, 2006; von Krogh and Spaeth, 2007). In AI, significant advances have been made possible thanks to the open sharing of data, code, models, and applications (Gokaslan et al., 2019; 2023; Workshop et al., 2022; Raffel et al., 2020; Rombach et al., 2022; Touvron et al., 2023). The fact that researchers and developers can use, modify or extend what others have built enables an important form of AI decentralization and supports accessibility.\n\nHowever, openness has also come with significant tensions, particularly for pretrained (so-called base or foundation) models. On the one hand, these models can be used in a variety of domains, often with little or no finetuning (Radford et al., 2019; Brown et al., 2020). On the other, this versatility means that they can be used by different actors in ways that are not aligned with the applications intended by their creators (Lee et al., 2023). Some of these uses may be overtly harmful (e.g., generating content to deceive or harass a person/people) and others may present unintended higher risks (e.g., generating diagnoses that may sometimes be inaccurate or biased, extracting PII from training data) (Fergusson et al., 2023; Nasr et al., 2023; Brundage et al., 2022). Combined with decentralization, such downstream uses can present challenges for accountability and recourse (Cooper et al., 2022; 2023).\n\nUsage restrictions on contractual agreements. The release of assets by private organizations is usually accompanied by a provider-user contractual agreement. Users must agree to terms specified by the provider for unhindered and continued access to the service. While traditionally such terms were geared towards compliance with laws, mitigating legal risks and protecting intellectual property, some AI providers have begun to include additional clauses that govern usage. For example, Open AI’s policies disallow generation of content for dissemination in electoral campaigns (OpenAI, 2024), Microsoft’s FaceAPI services are also subject to “specific license terms” and are only available in limited access to “customers managed by Microsoft” (Microsoft, 2022).\n\nTo release or not to release. Not all researchers or research teams have the resources to create customized legal agreements for the AI models or source code they would like to release. The choice often becomes to release the code with no restrictions or to not release it at all. This forces the creator to decide between openness and democratization on one hand, and responsible use on the other.\n\nWhile tools for software licensing do exist Bretthauer (2001), historically the most frequently adopted licenses (e.g., MIT, Apache) do not contain restrictions on how code, models or applications are to be used. Adapting licenses as a tool for responsibly releasing AI software was proposed by Contractor et al. Contractor et al. (2022b). These licenses contain behavioral use clauses that enable software and models to be released with restrictions around how they are used. The paper, proposed that these licenses could be implemented to complement existing responsible AI guidelines.\n\nLicenses with Behaviorial-use Clauses. Over the past five years, licenses with behavioral use clauses (BUC) have been gaining adoption at an increasely rapid rate (see Fig. 1). In this paper, we refer to responsible AI licenses as a broad category of licenses that incorporate BUC. Licenses that allow adaptation and reuse of software, models, data, or applications can be made dependent on behavioral use conditions. However, each different type of asset has their own ideosyncracies.\n\nA study by OpenFutures of 39,000 repositories found a clear trend towards the adoption of responsible AI licenses Keller and Bonato (2023). OpenRAIL licenses (Contractor et al., 2022a), a specific variant of RAIL licenses, were the second most used license category. To date, such licenses have primarily been applied to AI models.\n\nFor example, BLOOM Workshop et al. (2022) is a large parameter multilingual language model, and accompanying BigScience OpenRAIL license enables derivative uses but restricts applications amongst others that violate laws, generate or disseminate verifiably false information, or predict if an individual will commit fraud/crime. Stable diffusion Rombach et al. (2022) “provided weights under a license to prevent misuse and harm as informed by the model card Mitchell et al. (2019b), but otherwise remains permissive.” Subsequently the LLaMA 2 Touvron et al. (2023) and FALCON Almazrouei et al. (2023) models were released with intersecting behavioral use restrictions, yet FALCON has a smaller set of clauses than the others.\n\nThe use of responsible AI licenses is not restricted only to foundation models, Robotics platforms (GRID) Vemprala et al. (2023), edge IoT systems Janapa Reddi et al. (2023) and medical sensors Liu et al. (2023) that use AI components have also adopted similar clauses. In the realm of training data, similar approaches to licensing have also been the subject of discussion Li et al. (2023a) and experimentation. For instance, AI2 created the ImpACT license to apply broadly to ML artifacts that include both models and data Allen Institute for AI (2024) and used an ImpACT license for the DOLMA dataset release Soldaini et al. (2023).\n\nContributions. While the growth of interest in, and adoption of, these licenses is very apparant, there is no single standard license. In this paper we explore the reasons for the trend towards behavioral use licensing, the proliferation of different licenses and choice of clauses, and the need for standardization. We perform a quantitative analysis of the licenses used in over 170,000 model repositories and highlight the growing trend toward adopting responsible AI licenses. We also qualtitatively evaluate the similarities and differences between the specific license clauses included in these artifacts. We then report on semi-structured interviews conducted with researchers who have released high-profile AI models and software with responsible AI licenses (across computer vision, natural language, and robotics).We take the position that responsible AI licenses need standardization to avoid confusing users or diluting their impact. At the same time, customization of behavioral restrictions is necessary and appropriate in some contexts (e.g., medical domains) and can be supported by tooling.\n\n2 Regulation and Licensing for AI\n\nRegulation and licensing of AI systems is difficult because of their complex supply chains and the uncertainty about how copyright rules apply to AI systems Lee et al. (2023). One must consider the licensing of the data, the code, the models, and the machine learning libraries used to train the models. New considerations are being raised as the technology is developed; for example, courts are now examining the relationship between the licensing of the data and the models where they have not in the past. Outcome-based regulation of relatively simple classifiers has proven difficult Cooper et al. (2022), let alone regulation for large foundation models. Some academics believe that training AI models on copyrighted data is fair use under current US copyright law (Lemley and Casey, 2021; Klosek and Blumenthal, 2024), while others argue that there is likely not a blanket fair-use rationale for all possible systems and their possible outputs (Lee et al., 2023; Samuelson, 2023; Sag, 2023). Courts and legislators, if asked existential questions about whether large-scale AI models should exist, may realistically decide that using copyrighted data to train AI models does not constitute copyright infringement, and that the fair use doctrine may be invoked to shield such use.\n\nHow can regulation help?: Regardless of the currently-unresolved intellectual property regime governing AI systems, the aim of regulation is to provide an enforceable mechanism to govern how AI is or is not used – for example, by focusing on data privacy. Many countries, states, and cities are using regulations to avoid undesired uses of AI. For instance, the city of San Francisco banned the use facial regulation in CCTV cameras, and imposed a series of certifications for the use of self-driving cars in its streets. In Europe, the General Data Protection Regulation GDPR requires data collectors to state the purpose for which data is being collected, to obtain consent, and then to collect only the minimum amount of data required to accomplish the task for which consent was received. Additionally, it imposes requirements on data processors to be able to verifiably remove data when consent is revoked.\n\nThe recently proposed EU AI Act, bans certain uses of AI (like the use of biometric identification in public spaces by governments) and imposes conditions (such as conformity assessment, trustworthy AI properties, and human oversight) for the use of AI in certain domains considered high-risk, such as hiring and education. However, solely relying on regulations to prevent undesired applications of AI presents several difficulties.\n\nFirst, in contrast to other fields of science, AI is moving very rapidly and there is a lack of specific regulation at the present time. Significantly more advanced models and systems are being released every month. The introduction of transformers Vaswani et al. (2017), coupled with the increased availability of computing resources, has resulted in rapid advancement in the areas of natural language generation and multi-modal reasoning, in under five years. We argue that while government regulation processes are best equipped to handle the effect of a new technology in the long term, they leave a gap in terms of governance in the short term.\n\nSecond, often a responsible approach to releasing AI systems needs to consider the capabilities and limitations of a specific AI model. For instance, if a machine translation system has a high word-error rate, it may not be a problem if used for recreational purposes, but could be dangerous when employed in other contexts (e.g., translating documents with medical or legal implications). Such case-specific determinations can be difficult to encode in a legislation that aims to specify general rules to balance different broad categories of risks, and even harder to operationalize in the absence of meaningful and generally applicable thresholds. Further, different parties, developers or researchers may want to impose specific restrictions on the use of the AI models they build in cases where they feel government regulation is too weak or does not exist.\n\nCan licenses help? Recognizing these challenges, the use of contractual and copyright law have been recently suggested as a means of controlling end-use Contractor et al. (2022b). This approach holds promise, after all much of the software in use today is accompanied by a license governing its terms-of-use. Such terms-of-use can govern commercial use, compliance with local laws and regulation, dispute resolution mechanisms, disclaimers of warranties and risks and intended-use. This is not only true for software sold by companies, source code that is open-sourced is accompanied by a license which grants the permissions for licensee to distribute, re-use, modify the accompanying software and source code. In some cases, terms referred to as “copy-left” clauses are included in some open-source licenses, which require that any downstream software or code reusing any component of the licensed artifact is subject to the same requirements as the original license. We argue that licensing will become increasingly important as AI continues to develop at a rapid pace.\n\nPromoting Informed-use: A number of tools have been proposed as a way to organize infrormation about machine learning models in a consistent manner. AI factsheets Arnold et al. (2019) and AI model cards Mitchell et al. (2019a) both help to achieve these ends. However, they themselves do not have an explicit mechanism for enforcement about how the models are used.\n\n3 A Study of AI Licenses\n\n3.1 Methodology\n\nTo motivate our position on how responsible AI licenses should evolve, we conducted a study of the current state of the licensing in AI. Given the complex socio-technical nature of of the domain we pursue a mixed-methods approach combining insights from interviews of licenses users, quantitative analysis of license adoption and a review of existing licenses including clustering of their behavioral use clauses. The interviews involved practitioners and subject matter experts who have adopted licenses for high profile AI projects. We analyzed over 170,000 machine learning model repositories. To cluster the licenses we manually categorized behavioral use clauses in the most heavily utilized license types.\n\n3.2 Interviews with License Adopters\n\nParticipants. We performed semi-structured 30-minute interviews with four AI researchers and software developers. The interviewees has a mean of 18 years of post-graduate experience in the field of AI (8, 30, 10, 24 years). The interviews explored the questions listed in Fig. 2. The interviewees had all release projects in the past two years under a responsible AI license. These included: large language and code models, and robotics and computer vision software.\n\nAnalysis. A common theme the interviewees discussed was the tension between the “willingness to release assets on an open basis” on one hand and the “concerns within the community and internal values about uses of the model.” (P1). This was a motivating factor for the choice to select a responsible AI license rather than a traditional OS license. With the evolving nature of AI there is a “need for people to think about how to use [models] responsibly” (P2).\n\nThe interviewees spoke about the evolving nature of AI technology as one of the reasons they chose to adapt use clauses in the licenses. One interviewer summarized a common theme describing that “existing OS licenses didn’t perfectly match our mission” (P3). One roboticist, commented on the fact that physical instantiations of AI required terms that restricted “destruction of physical people of property” that other licenses did not include. A second point was that existing OS licenses were “not suitable for [other assets]” (e.g., datasets) (P3).\n\nIn the process of considering their options, a practice that multiple interviewees used was to review existing OS licenses to see if any satisfied their needs. However, licenses like Apache 2.0 were not “ML or LLM specific” (P1) and needed to be adapted. They noted that adaptation was not trivial. One participant, who was an ML researcher and entrepenur, said licensing was “not our expertise, and would have required resources that we didn’t have” (P2). Another commented on the ambiquity introduced by changing licenses ad hoc - “[It is difficult] for someone to know the difference between license A and B, that is why we didn’t modify an OS license” (P4). When asked about tooling, the participants identified that there were other situations in which agreements could be “generated via questionnaire … by ticking boxes” and that having a template would be easier.\n\n3.3 Analysis of Licenses and their Clauses\n\nQuantitative Analysis. We performed an analysis of the HuggingFace Model Hub. The Model Hub API provides information about the models, licenses and domains/applications, when the repository was created and how many times the model has been downloaded. At the time of the analysis (January 2024) 174,163 model repositories had licenses associated with them.\n\nOf these, the most common licenses used were Apache (38.1%), RAIL (24.0%), MIT (17.5%) and Creative Commons (7.5%) (see Fig. 1). RAIL licenses are the most common type of license with behavioral use clauses and were the second most adopted license type overall (see Fig. 1). The share of RAIL licenses has grown from 1% (N=209) in September 2022 to 10% (N=4,035) in January 2023 to 24.0% (N=41,700) in January 2024. In comparison to counting the number of repositories licensed under RAIL licenses, RAIL licenses measured by the number of downloads make up the fourth biggest category behind Apache, MIT and CC licenses (see Fig. 5 in the appendix). These results support the conclusion that RAIL adoption is considerable and growing; however, some of the most popular models are not using RAIL licenses.\n\nRAIL licenses have been used with repositories across many different application domains, including text, image, speech processing and many methods, including supervised and unsupervised learning, reinforcement learning, graph learning, classification and generative methods. Given the adoption by StableDiffusion Rombach et al. (2022), text-to-image translation was the most common model type to adopt a behavioral use license (see Fig. 3).\n\nQualitative Analysis. To better understand the types of behavioral uses that have been restricted in the first wave of responsible AI licenses we conducted a qualitative analysis of 77777777 different legal clauses from 7777 different responsible AI licenses that are currently in use: AI Pubs RAIL – which is also available for use by researchers publishing at AAAI conference venues, BigScience OpenRAIL Workshop et al. (2022), CodeML OpenRAIL Li et al. (2023b), Llama-2 Touvron et al. (2023), FALCON Almazrouei et al. (2023), ImpACT Licenses Allen Institute for AI (2024) and GRID Vemprala et al. (2023).\n\nWe guided our analysis with the following questions: Q1: What artifacts are being restricted?, Q2: What are the uses being restricted?, and Q3: How are they being restricted? For Q1, we coded for the specific language referencing the artifact whose use is being restricted in the clause. For Q2, two of the authors independently coded broadly for any concepts that could help us understand the values and social norms of the developer community. Each author independently conducted an axial coding phase Braun and Clarke (2006) to refine the granularity of our analysis. Next, we mapped the resulting codes to existing Responsible AI frameworks like the NIST AI RMF Tabassi (2023) and the CSET Harm Taxonomy Hoffman and Frase (2023). Each of these frameworks are designed to facilitate a more standardized and informed, yet flexible and adaptable approach for measuring, monitoring, and mitigating AI harm. For Q3333, we learned early on that nearly all restrictions in the clauses we analyzed restrict a particular use of an artifact by simply prohibiting it. While the results of the question as initially posed would not be interesting with our data, we did observe that some use restrictions had caveats.\n\nThe result was 22 clauses (see Table 1). We find that almost all licenses include clauses that govern discrimination, disinformation and violations of the law. Most contain privacy related restriction. The other two main groups of clauses are heathcare and military applications of which some licenses include restrictions.\n\nAdditional Restrictions on Distribution. Apart from behavioral-use clauses applicable to the use and distribution of the artifacts, licenses sometimes also include additional requirements (See table 2). For instance, the AI Pubs RAIL License permits the use of the licensed code or model only for “Research-use” while the LLaMA-2 license explicitly prohibits the use of the model for anything except creating a derivative of LLaMA-2 with the same terms. It also stipulates additional licensing requirements if the monthly active users of the model exceed 700 million. The ImpACT licenses permit the creation of derivatives and allow distribution for low-risk (L) use-cases but do not allow derivatives for high-risk (H) use-cases and they also do not permit derivatives for both, medium (M) and high-risk (H) use-cases.\n\nThe flavor of RAIL licenses, known as the OpenRAIL family of licenses, avoid non-behavioral use restrictions (such as “research only” requirements). Currently, the “OpenRAIL” moniker is given to license terms that (1) incorporate behavioral-use clauses, and (2) allow for the otherwise unlimited use, modification, and distribution of applicable artifacts, as long as the behavioral-use restrictions are included. The “open” designation has been important to certain actors within the AI eco-system who wish to emphasize that the IP they are sharing can be treated much like other open source code (freely share-able and modifiable), albeit with behavioral-use restrictions. Both the BigScience and CodeML RAIL Licenses are OpenRAIL Licenses.\n\n4 Discussion\n\nResponsible AI Licenses Are Being Adopted. Our quantitative analysis of the repositories shows that RAIL licenses are being adopted at a relatively large scale. This has been driven by a number of high profile and widely adopted projects and the associated derivative projects. The adoption is large enough to suggest that it is necessary to seriously study these types of licenses and their implications for the AI and open source communities, the latter of whom have been grappling with the definition of Open Source AI.\n\nThe rapid rise of licenses with behavioral use clauses, particularly amongst projects involving large parameter models, signals a perception that existing open source licenses do not contain sufficient restrictions for responsibly distributing capable machine learning models. All of our interviewees highlighted that conundrum.\n\nResponsible AI Licenses Are Being Adapted. Our analysis of behavioral use clauses included in responsible AI licenses shows that there is a large amount of overlap in restrictions. However, they differ in important ways. Some licenses (e.g., FALCON) are more permissive and include a smaller subset of clauses while others (e.g., OpenRAIL, ImpACT) include a much larger list. Some licenses (e.g. CodeML OpenRAIL) include domain specific restrictions.\n\nStandardization of Responsible AI Licenses is Necessary. Based on our analysis we take the position that the proilferation of responsible AI licensing is good, as it provides options to developers who are worried about misuse. However, there is a need for standardization to avoid confusing users or diluting impact while enabling some customization. We believe that there are tools that could support customizing licenses that still satisfy a set of core criteria. This is pertinent given that AI developers who aim to mitigate harmful uses of the artifacts they share will need to tailor licenses for the specific domain, context of use, and type of AI system. Below, we gather implications for license tooling development, considering how possible tools may support people (most likely without exhaustive legal expertise) in their practice of licensing the AI artifacts they are creating.\n\nA Community-Oriented License Generator. An actionable route to standardized and customizable licenses is a generator purpose-built for AI licenses. Existing RAIL licenses already follow a topology ameanable to structured generation (e.g., Open-/ vs. RAIL, sub-specifications for Application/Model/Source Code artifacts). Such generators are an established means to allow people without commensurate legal experise to choose among potential licenses for their artifacts. For instance, the Creative Commons (CC) initative has published a tool which takes users through the process of deciding how people may or may not use their creative work. In the case of AI licenses, standardized customization along use restrictions may arguably allow for more ML-specialized tailoring (as called for by the interviewees); which in turn places specific demands on a license generator.\n\nResponsible AI licenses, unlike CC-type creative works, are not broadly artifact-agnostic: in practice it matters whether one is licensing a ready made application, a set of source code, or a model, andthere may be specific restrictions and conditions that need to apply to each. For instance, one could choose to release a model with behaviorial-use restrictions but release the accompying source code under an open source license. In terms of a license generator, a design priority would be the ability to specify the artifact(s) to be licensed (e.g., Data, Application, Model, Source Code) with behavioral-use clauses.\n\nSpecifying use restrictions gives AI developers further means to appropriately customize licenses. However, the large number of possible use restrictions risks leading to many different licenses that could be generated could confuse 1) the licensor who may not be able to easily judge how restrictive a license should be, 2) the licensee who may find it difficult to know what the assets are, and are not, permitted to be used for. Thus, aggregating potential use restrictions by domain could help (e.g., privacy, disinformation, health).\n\nWe developed an interface design for such a license generator (see Fig. 4), that incorporates the above considerations while also providing a “live” preview of the RAIL-type license that is currently being generated. The protoype shown includes a base set of mandatory use-restrictions, that were selected based on our analysis of existing licenses. Icons related to the domains of selected restrictions are added to the top of the license being generated.\n\nCode Scans for AI Licenses. Similar to a license generator, it is feasible to construct tools which act as aids for AI developers when sharing and maintaining their created artifacts on platforms such as Huggingface and GitHub. An open question as far as licenses are concerned is the compability of dependencies (e.g., other repositories, third-party code, etc.) with the requirements of a responsible AI license. When dependencies exist that are licensed under a GPL license, for instance, a developer may not be able to select an OpenRAIL license for their artifacts due to requirements imposed by the GPL license. Alternatively, even when there are dependencies licensed under different responsible AI licenses, the arising artifact, will most likely need to be compliant with the stricter set of usage restrictions.\n\nIn practice, AI systems—just like any contemporary software system—may integrate many dependencies, complicating this process further. To this end, a tool that scans existing dependencies for their licenses may aid AI developers in figuring out whether and which clauses are actually applicable. Such a tool could crawl the repositories of a given platform, and be integrated, for instance, as a service in GitHub Marketplace or in a Huggingface Space; or be offered as a command line tool for local use. In either form, such support for the workflow of AI developers in the ways they manage and share their artifact(s) may encourage a more reflective licensing practice that reflects the actual constraints of given dependencies.\n\n5 Conclusion\n\nWith the growth in large parameter AI models the need for tools to help responsibly release models has increased. The use of responsible AI licenses have grown substantially over the past two years, thanks to adoption from several high-profile model releases (e.g., BLOOM, LLaMA2, StarCoder, Stable Diffusion, FALCON). Open Source licensing has been influential thanks to the clear definition and standardization of licenses such as MIT and Apache 2.0. The proliferation of different responsible AI licenses risks becoming confusing and we take the position that standarized forms of customization, along with appropriate tooling could make them more user-friendly.\n\nImpact Statement\n\nAs is increasingly well understood by the community, machine learning and AI projects can have significant, broad, and uncertain impacts. The development of new forms of licensing provides researchers with the ability and resources to release assets while restricting uses that they anticipate will lead to negative consequences. Standardization of licenses does involve making some value judgements about what types of behaviors are higher risk than others; however, providing the ability for customization can add flexibility. For licenses directed at responsible use such as RAILs, this can be achieved practically by allowing for revision and extension of mandatory and additional use restrictions in individual license instantiations.\n\nReferences\n\nAllen Institute for AI (2024) Allen Institute for AI. AI2 ImpACT Licenses, 2024. URL https://allenai.org/impact-license.\n\nAlmazrouei et al. (2023) Ebtesam Almazrouei, Hamza Alobeidli, Abdulaziz Alshamsi, Alessandro Cappelli, Ruxandra Cojocaru, Merouane Debbah, Etienne Goffinet, Daniel Heslow, Julien Launay, Quentin Malartic, et al. Falcon-40B: an open large language model with state-of-the-art performance. Findings of the Association for Computational Linguistics: ACL, 2023:10755–10773, 2023.\n\nArnold et al. (2019) M. Arnold, R. K. E. Bellamy, M. Hind, S. Houde, S. Mehta, A. Mojsilović, R. Nair, K. N. Ramamurthy, A. Olteanu, D. Piorkowski, D. Reimer, J. Richards, J. Tsay, and K. R. Varshney. FactSheets: Increasing trust in AI services through supplier’s declarations of conformity. IBM Journal of Research and Development, 63(4/5):6:1–6:13, 2019.\n\nBraun and Clarke (2006) Virginia Braun and Victoria Clarke. Using thematic analysis in psychology. Qualitative Research in Psychology, 3(2):77–101, 2006. doi: 10.1191/1478088706qp063oa. URL https://www.tandfonline.com/doi/abs/10.1191/1478088706qp063oa.\n\nBretthauer (2001) David Bretthauer. Open Source Software: A History. Published Works, 7, 2001.\n\nBrown et al. (2020) Tom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel M. Ziegler, Jeffrey Wu, Clemens Winter, Christopher Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, and Dario Amodei. Language Models are Few-Shot Learners. arXiv preprint arXiv:2005.14165, 2020.\n\nBrundage et al. (2022) Miles Brundage, Katie Mayer, Tyna Eloundou, Sandhini Agarwal, Steven Adler, Gretchen Krueger, Jan Leike, and Pamela Mishkin. Lessons learned on language model safety and misuse, 2022. URL https://openai.com/research/language-model-safety-and-misuse.\n\nContractor et al. (2022a) Danish Contractor, Carlos Muñoz Ferrandis, Jenny Lee, and Daniel McDuff. From RAIL to Open RAIL: Topologies of RAIL Licenses, 2022a. URL https://www.licenses.ai/blog/2022/8/18/naming-convention-of-responsible-ai-licenses.\n\nContractor et al. (2022b) Danish Contractor, Daniel McDuff, Julia Katherine Haines, Jenny Lee, Christopher Hines, Brent Hecht, Nicholas Vincent, and Hanlin Li. Behavioral use licensing for responsible ai. In Proceedings of the 2022 ACM Conference on Fairness, Accountability, and Transparency, pages 778–788, 2022b.\n\nCooper et al. (2022) A. Feder Cooper, Emanuel Moss, Benjamin Laufer, and Helen Nissenbaum. Accountability in an Algorithmic Society: Relationality, Responsibility, and Robustness in Machine Learning. In 2022 ACM Conference on Fairness, Accountability, and Transparency, FAccT ’22, page 864–876, New York, NY, USA, 2022. Association for Computing Machinery. ISBN 9781450393522. doi: 10.1145/3531146.3533150.\n\nCooper et al. (2023) A. Feder Cooper, Katherine Lee, James Grimmelmann, Daphne Ippolito, Christopher Callison-Burch, Christopher A. Choquette-Choo, Niloofar Mireshghallah, Miles Brundage, David Mimno, Madiha Zahrah Choksi, Jack M. Balkin, Nicholas Carlini, Christopher De Sa, Jonathan Frankle, Deep Ganguli, Bryant Gipson, Andres Guadamuz, Swee Leng Harris, Abigail Z. Jacobs, Elizabeth Joh, Gautam Kamath, Mark Lemley, Cass Matthews, Christine McLeavey, Corynne McSherry, Milad Nasr, Paul Ohm, Adam Roberts, Tom Rubin, Pamela Samuelson, Ludwig Schubert, Kristen Vaccaro, Luis Villa, Felix Wu, and Elana Zeide. Report of the 1st Workshop on Generative AI and Law. arXiv preprint arXiv:2311.06477, 2023.\n\nFergusson et al. (2023) Grant Fergusson, Caitriona Fitzgerald, Chris Frascella, Megan Iorio, Tom McBrien, Calli Schroeder, Ben Winters, and Enid Zhou. Generating Harms: Generative AI’s Impact & Paths Forward. Technical report, Electronic Privacy Information Center, 2023.\n\nGokaslan et al. (2019) Aaron Gokaslan, Vanya Cohen, Ellie Pavlick, and Stefanie Tellex. Openwebtext corpus, 2019.\n\nGokaslan et al. (2023) Aaron Gokaslan, A. Feder Cooper, Jasmine Collins, Landan Seguin, Austin Jacobson, Mihir Patel, Jonathan Frankle, Cory Stephenson, and Volodymyr Kuleshov. CommonCanvas: An Open Diffusion Model Trained with Creative-Commons Images. arXiv preprint arXiv:2310.16825, 2023.\n\nHoffman and Frase (2023) Mia Hoffman and Heather Frase. Adding structure to ai harm: An introduction to cset’s ai harm framework, July 2023. URL https://cset.georgetown.edu/publication/adding-structure-to-ai-harm/.\n\nJanapa Reddi et al. (2023) Vijay Janapa Reddi, Alexander Elium, Shawn Hymel, David Tischler, Daniel Situnayake, Carl Ward, Louis Moreau, Jenny Plunkett, Matthew Kelcey, Mathijs Baaijens, et al. Edge impulse: An mlops platform for tiny machine learning. Proceedings of Machine Learning and Systems, 5, 2023.\n\nKeller and Bonato (2023) Paul Keller and Nicolò Bonato. Growth of responsible AI licensing. Analysis of license use for ML models published on. Open Future, feb 7 2023. https://openfuture.pubpub.org/pub/growth-of-responsible-ai-licensing.\n\nKlosek and Blumenthal (2024) Katherine Klosek and Marjory S. Blumenthal. Training Generative AI Models on Copyrighted Works Is Fair Use, 2024. URL https://www.arl.org/blog/training-generative-ai-models-on-copyrighted-works-is-fair-use/.\n\nLee et al. (2023) Katherine Lee, A. Feder Cooper, and James Grimmelmann. Talkin’ ’Bout AI Generation: Copyright and the Generative-AI Supply Chain. arXiv preprint arXiv:2309.08133, 2023.\n\nLemley and Casey (2021) Mark Lemley and Bryan Casey. Fair Learning. Texas Law Review, 99:743, 2021.\n\nLi et al. (2023a) Hanlin Li, Nicholas Vincent, Yacine Jernite, Nick Merrill, Jesse Josua Benjamin, and Alek Tarkowski. Can licensing mitigate the negative implications of commercial web scraping? In Companion Publication of the 2023 Conference on Computer Supported Cooperative Work and Social Computing, CSCW ’23 Companion, page 553–555, New York, NY, USA, 2023a. Association for Computing Machinery. ISBN 9798400701290. doi: 10.1145/3584931.3611276. URL https://doi.org/10.1145/3584931.3611276.\n\nLi et al. (2023b) Raymond Li, Loubna Ben Allal, Yangtian Zi, Niklas Muennighoff, Denis Kocetkov, Chenghao Mou, Marc Marone, Christopher Akiki, Jia Li, Jenny Chim, et al. Starcoder: may the source be with you! arXiv preprint arXiv:2305.06161, 2023b.\n\nLiu et al. (2023) Xin Liu, Girish Narayanswamy, Akshay Paruchuri, Xiaoyu Zhang, Jiankai Tang, Yuzhe Zhang, Soumyadip Sengupta, Shwetak Patel, Yuntao Wang, and Daniel McDuff. rPPG-Toolbox: Deep Remote PPG Toolbox. In Thirty-seventh Conference on Neural Information Processing Systems Datasets and Benchmarks Track, 2023.\n\nMicrosoft (2022) Microsoft. Responsible AI investments and safeguards for Facial Recognition, 2022. URL https://azure.microsoft.com/en-us/updates/facelimitedaccess/.\n\nMitchell et al. (2019a) Margaret Mitchell, Simone Wu, Andrew Zaldivar, Parker Barnes, Lucy Vasserman, Ben Hutchinson, Elena Spitzer, Inioluwa Deborah Raji, and Timnit Gebru. Model cards for model reporting. In Proceedings of the Conference on Fairness, Accountability, and Transparency, FAT* ’19, page 220–229, New York, NY, USA, 2019a. Association for Computing Machinery. ISBN 9781450361255. doi: 10.1145/3287560.3287596. URL https://doi.org/10.1145/3287560.3287596.\n\nMitchell et al. (2019b) Margaret Mitchell, Simone Wu, Andrew Zaldivar, Parker Barnes, Lucy Vasserman, Ben Hutchinson, Elena Spitzer, Inioluwa Deborah Raji, and Timnit Gebru. Model cards for model reporting. In Proceedings of the conference on fairness, accountability, and transparency, pages 220–229, 2019b.\n\nNasr et al. (2023) Milad Nasr, Nicholas Carlini, Jonathan Hayase, Matthew Jagielski, A. Feder Cooper, Daphne Ippolito, Christopher A. Choquette-Choo, Eric Wallace, Florian Tramèr, and Katherine Lee. Scalable Extraction of Training Data from (Production) Language Models. arXiv preprint arXiv:2311.17035, 2023.\n\nOpenAI (2024) OpenAI. Usage policies, 2024. URL https://openai.com/policies/usage-policies.\n\nRadford et al. (2019) Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever, et al. Language models are unsupervised multitask learners. OpenAI blog, 1(8):9, 2019.\n\nRaffel et al. (2020) Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J. Liu. Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer. Journal of Machine Learning Research, 21:1, 2020.\n\nResnick (2006) David B. Resnick. Openness versus Secrecy in Scientific Research Abstract. Episteme, pages 135 – 147, February 2006.\n\nRombach et al. (2022) Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Björn Ommer. High-resolution image synthesis with latent diffusion models. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 10684–10695, 2022.\n\nSag (2023) Matthew Sag. Copyright Safety for Generative AI. Houston Law Review, 2023. Forthcoming.\n\nSamuelson (2023) Pamela Samuelson. Generative AI meets copyright. Science, 381(6654):158–161, 2023. doi: 10.1126/science.adi0656. URL https://www.science.org/doi/abs/10.1126/science.adi0656.\n\nSoldaini et al. (2023) Luca Soldaini, Rodney Kinney, Akshita Bhagia, Dustin Schwenk, David Atkinson, Russell Authur, Khyathi Chandu, Jennifer Dumas, Li Lucy, Xinxi Lyu, et al. Dolma: An Open Corpus of 3 Trillion Tokens for Language Model Pretraining Research. Allen Institute for AI, Tech. Rep, 2023.\n\nTabassi (2023) Elham Tabassi. Artificial intelligence risk management framework (ai rmf 1.0), 2023-01-26 05:01:00 2023. URL https://tsapps.nist.gov/publication/get_pdf.cfm?pub_id=936225.\n\nTouvron et al. (2023) Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al. Llama 2: Open foundation and fine-tuned chat models. arXiv preprint arXiv:2307.09288, 2023.\n\nVaswani et al. (2017) Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Łukasz Kaiser, and Illia Polosukhin. Attention is all you need. In Advances in neural information processing systems, pages 5998–6008, 2017.\n\nVemprala et al. (2023) Sai Vemprala, Shuhang Chen, Abhinav Shukla, Dinesh Narayanan, and Ashish Kapoor. Grid: A platform for general robot intelligence development. arXiv preprint arXiv:2310.00887, 2023.\n\nvon Krogh and Spaeth (2007) Georg von Krogh and Sebastian Spaeth. The open source software phenomenon: Characteristics that promote research. The Journal of Strategic Information Systems, 16(3):236–253, 2007. ISSN 0963-8687. doi: https://doi.org/10.1016/j.jsis.2007.06.001. URL https://www.sciencedirect.com/science/article/pii/S096386870700025X.\n\nWorkshop et al. (2022) BigScience Workshop, Teven Le Scao, Angela Fan, Christopher Akiki, Ellie Pavlick, Suzana Ilić, Daniel Hesslow, Roman Castagné, Alexandra Sasha Luccioni, François Yvon, et al. Bloom: A 176b-parameter open-access multilingual language model. arXiv preprint arXiv:2211.05100, 2022.\n\nAppendix A Tree Map of Models by License"
    }
}
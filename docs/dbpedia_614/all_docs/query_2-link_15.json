{
    "id": "dbpedia_614_2",
    "rank": 15,
    "data": {
        "url": "https://sc16.supercomputing.org/2016/08/25/acm-gordon-bell-prize-recognizes-top-accomplishments-running-science-apps-hpc/index.html",
        "read_more_link": "",
        "language": "en",
        "title": "ACM Gordon Bell Prize Recognizes Top Accomplishments in Running Science Apps on HPC",
        "top_image": "https://sc16.supercomputing.org/wp-content/uploads/2016/08/640px-Gordon_Bell-2.jpg",
        "meta_img": "https://sc16.supercomputing.org/wp-content/uploads/2016/08/640px-Gordon_Bell-2.jpg",
        "images": [
            "https://sc16.supercomputing.org/wp-content/themes/sc16-theme/images/facebook-icon.png",
            "https://sc16.supercomputing.org/wp-content/themes/sc16-theme/images/twitter-icon.png",
            "https://sc16.supercomputing.org/wp-content/themes/sc16-theme/images/google-icon.png",
            "https://sc16.supercomputing.org/wp-content/themes/sc16-theme/images/linkedin-icon.png",
            "https://sc16.supercomputing.org/wp-content/themes/sc16-theme/images/youtube-icon.png",
            "https://sc16.supercomputing.org/wp-content/themes/sc16-theme/images/flickr.png",
            "https://sc16.supercomputing.org/wp-content/themes/sc16-theme/images/rss-icon.png",
            "https://sc16.supercomputing.org/wp-content/themes/sc16-theme/images/sc16logo.png",
            "https://sc16.supercomputing.org/wp-content/themes/sc16-theme/images/sponsors.png",
            "https://sc16.supercomputing.org/wp-content/themes/sc16-theme/images/ieee.png",
            "https://sc16.supercomputing.org/wp-content/themes/sc16-theme/images/acm.png",
            "https://sc16.supercomputing.org/wp-content/themes/sc16-theme/images/sighpcsmall.png",
            "https://sc16.supercomputing.org/wp-content/uploads/2015/11/exhibit-box2.png",
            "https://sc16.supercomputing.org/wp-content/uploads/2016/08/640px-Gordon_Bell-2.jpg",
            "http://2.gravatar.com/avatar/5bbad91a1af72c1c9b21fb1abe43d379?s=96&d=mm&r=g",
            "https://sc16.supercomputing.org/wp-content/themes/sc16-theme/images/content-filler.png",
            "https://sc16.supercomputing.org/wp-content/themes/sc16-theme/images/facebook-icon.png",
            "https://sc16.supercomputing.org/wp-content/themes/sc16-theme/images/twitter-icon.png",
            "https://sc16.supercomputing.org/wp-content/themes/sc16-theme/images/google-icon.png",
            "https://sc16.supercomputing.org/wp-content/themes/sc16-theme/images/linkedin-icon.png",
            "https://sc16.supercomputing.org/wp-content/themes/sc16-theme/images/youtube-icon.png",
            "https://sc16.supercomputing.org/wp-content/themes/sc16-theme/images/flickr.png",
            "https://sc16.supercomputing.org/wp-content/themes/sc16-theme/images/rss-icon.png"
        ],
        "movies": [],
        "keywords": [],
        "meta_keywords": [
            ""
        ],
        "tags": null,
        "authors": [
            "Gordon Bell says"
        ],
        "publish_date": "2016-08-25T00:00:00",
        "summary": "",
        "meta_description": "For the first time, SC conference releases a comprehensive list of all Gordon Bell winners since 1987. Here is that historical perspective. Each year at SC, two key indicators of HPC performance are highlighted. Both were created to provide a more accurate measure of overall performance than the theoretical peak performance levels sometimes touted by …",
        "meta_lang": "en",
        "meta_favicon": "../../../../wp-content/themes/sc16-theme/images/favicon.ico",
        "meta_site_name": "SC16",
        "canonical_link": "https://sc16.supercomputing.org/index.html",
        "text": "For the first time, SC conference releases a comprehensive list of all Gordon Bell winners since 1987. Here is that historical perspective.\n\nEach year at SC, two key indicators of HPC performance are highlighted. Both were created to provide a more accurate measure of overall performance than the theoretical peak performance levels sometimes touted by vendors.\n\nAs Alan Laub, who was the first head of the Department of Energyâs SciDAC program, once said, âPeak performance â the manufacturerâs guarantee that you canât compute faster than that.â So, coming up with metrics that provide defensible results is critical.\n\nOne, the TOP500 list, details how fast the LINPACK benchmark runs on the top 500 supercomputers of those who submit their performance data.\n\nWhile the focus of the TOP500 list is on the systems themselves, the ACM Gordon Bell Prize recognizes the scientists who push those systems to get the most scientific productivity out of them, with particular emphasis on rewarding innovation in applying high-performance computing to applications in science, engineering, and large-scale data analytics.\n\nAt SC16, six finalists will compete for this yearâs prize. Three of those submissions are based on calculations performed on the newest No. 1 system on the TOP500 list, the Sunway TaihuLight machine in China.\n\nThe Gordon Bell Prize, was created in 1987 by Gordon Bell, who rose to fame as a computer designer for Digital Equipment Corp. Prizes may be awarded for peak performance or special achievements in scalability and time-to-solution on important science and engineering problems. Financial support of the $10,000 award is provided by Bell and the recognition itself is highly valued by the scientists whose scientific applications push the sustained performance of leading edge supercomputers.\n\nâOur community is fortunate to have the Gordon Bell Prize, because it documents scientific progress at the frontier of both supercomputing architectures and important computational science applications,â said Jeff Vetter, who leads the Future Technologies Group at Oak Ridge National Laboratory and was a member of the team that won the Gordon Bell Prize in 2010. In his 2013 book, âContemporary High Performance Computing: From Petascale to Exascale,â Vetter wrote notes that the prize is âthe most well-known scientific accomplishmentâ for HPC performance on real scientific problems.\n\nHorst Simon, deputy director of Lawrence Berkeley National Laboratory, was a member of the teams that won the Gordon Bell Prize in 1988 and 2009. âI was absolutely elated to be a member of the winning team in 1988 â I was early in my career and thought that an award for parallel performance was a great idea,â Simon said of the 1988 prize, which still hangs in his office. âI was fortunate to be in a great group at Boeing and be part of a great team at NASA. The award came at a time when parallel computing was a hot topic and it was a great career boost â it established my credentials in HPC.â\n\nSimon has continued to add to those credentials, including serving as one of four editors of the twice-yearly TOP500 list, which was created in 1993.\n\nBut the Gordon Bell Prizes also provide insight into how scientific computing capabilities have changed over the years. The prizes for peak performance usually go to researchers using the worldâs fastest supercomputer at the time. For example, the 1998 winning team got access to a 1,024 processor Cray T3E that was still on the factory floor in Chippewa Falls.\n\nWith the debut of the Earth Simulator in 2002, teams running applications on the system took home the Gordon Bell Prize for peak performance in 2002, 2003 and 2004. A similar pattern ensued when BlueGeneL ended the Earth Simulatorâs dominance in late 2004. At SC16, several of the Gordon Bell Prize entries are based on results from the Sunway TaihuLight machine in China that grabbed the top spot on the latest TOP500 list in June.\n\nInterestingly, though, despite the importance of the prize in the HPC community, there is no comprehensive list of winners online. But there are pieces. For SC2000, Blaise Barney of Lawrence Livermore National Laboratory compiled a list of all winners from 1987 to 1999, even noting that no prize was awarded in 1991.\n\nStarting in 2006, ACM began co-sponsoring the prize and maintains a list of âACM Gordon Bell Prizeâ winners from 2006 to the present, with links to the award papers. But for 2000? 2001? Up to 2005? Itâs an online scavenger hunt â and an omission that even Wikipedia hasnât addressed.\n\nHowever, thanks to the efforts of SC communicators and awards chairs, the information was recorded and virtually squirreled away on conference websites. By digging through old press releases and awards list, we have come up with the list below.\n\nNo doubt there may be are some errors/omissions and in the interest of historical accuracy, we welcome any corrections and the sources. Please send them to communications@info.supercomputing.org\n\nFollowing is the current list:\n\nACM Gordon Bell Prize Winners\n\n1987\n\nGeneral Purpose Computer\n\nFirst Place: Robert Benner, John Gustafson, Gary Montry, Sandia National Laboratories; âBeam Stress Analysis, Surface Wave Simulation, Unstable fluid flow model,â 400 â 600 speedup on a 1,024 node N-CUBE\n\nHonorable Mention: Robert Chervin, NCAR; âGlobal Ocean Model,â 450 Mflops on a Cray X/MP48\n\nHonorable Mention: Marina Chen, Yale University; Erik Benedictus, Bell Labs; Geoffrey Fox, Caltech; Jingke Li, Yale University; David Walker, Caltech; âQCD and Circuit Simulation,â Speedups ranging from 39-458 on three applications run on CM hypercubes\n\nHonorable Mention: Stavros Zenios, University of Pennsylvania; âNonlinear network optimization,â 1.5 sec. Execution time on a Connection Machine\n\n1988\n\nPeak Performance\n\nFirst Place: Phong Vu, Cray Research; Horst Simon, NASA Ames; Cleve Ashcraft, Yale University; Roger Grimes and John Lewis, Boeing Computer Services; Barry Peyton, Oak Ridge National Laboratory; âStatic finite element analysis,â 1 Gflops on 8-proc. Cray Y-MP, Running time reduced from 15 min. to 30 sec.\n\nPrice-Performance\n\nHonorable Mention: Richard Pelz, Rutgers University; âFluid flow problem using the spectral method,â 800 speedup on a 1,024 node N-CUBE Compiler Parallelization\n\nHonorable Mention: Marina Chen, Young-il Choo, Jungke Li and Janet Wu, Yale University; Eric De Benedictus, Ansoft Corp.; âAutomatic parallelization of a financial application,â 350 times speedup on a 1,024 N-CUBE and 50 times speedup on a 64 node Intel iPSC-2.\n\n1989\n\nPeak Performance\n\nFirst Place: Mark Bromley, Harold Hubschman, Alan Edelman, Bob Lordi, Jacek Myczkowski and Alex Vasilevsky, Thinking Machines; Doug McCowan and Irshad Mufti, Mobil Research; âSeismic data processing,â 6 Gflops on a CM-2 (also, 500 Mflops/$1M)\n\nHonorable Mention: Sunil Arvindam, University of Texas, Austin; Vipin Kumar, University of Minnesota; V. Nageshwara Rao, University of Texas, Austin; âParallel search for VLSI design,â 1,100 speedup on a 1,024 processor CM\n\nPrice-Performance\n\nFirst Place: Philip Emeagwali, University of Michigan; âOil reservoir modeling,â 400 Mflops/$1M on a CM-2\n\nHonorable Mention: Daniel Lopresti, Brown University; William Holmes, IDA Supercomputer Research Center; âDNA sequence matching,â 77k MIPs/$1M\n\n1990\n\nPeak Performance\n\nHonorable Mention: Mark Bromley, Steve Heller, Cliff Lasser, Bob Lordi, Tim McNerney, Jacek Myczkowski, Irshad Mufti, Guy Steele, Jr. and Alex Vasilevsky, Thinking Machines; Doug McCowan, Mobil Research; âSeismic data processing,â 14 Gflops on a CM-2\n\nPrice-Performance\n\nFirst Place: Al Geist and G. Malcom Stocks, Oak Ridge National Laboratory; Beniamino Ginatempo, University of Messina, Italy; William Shelton, U.S. Naval Research Laboratory; âElectronic structure of a high-temperature superconductor,â 800 Mflops/$1M on a 128-node Intel iPSC/860\n\nCompiler Parallelization\n\nSecond Place: Gary Sabot, Lisa Tennies and Alex Vasilevsky, Thinking Machines; Richard Shapiro, United Technologies; âGrid generation program used to solve partial differential equations,â 1,900 speedup on a 2,048 node CM-2 (2.3 Gflops)\n\nHonorable Mention: Eran Gabber, Amir Averbuch and Amiram Yihudai, Tel Aviv University; âParallelizing Pascal Compiler,â 25x on a 25 node Sequent Symmetry\n\n1991\n\nNo prize awarded\n\n1992\n\nPeak Performance\n\nFirst Place: Michael Warren, Los Alamos National Laboratory; John K. Salmon, Caltech; âSimulation of 9 million gravitating stars by parallelizing a tree code,â 5 Gflops on an Intel Touchstone Delta.\n\nPrice-Performance\n\nFirst Place: Hisao Nakanishi and Vernon Rego, Purdue University; Vaidy Sunderam, Emory University; âSimulation of polymer chains parallelized over a heterogeneous collection of distributed machines,â 1 Gflops/$1M.\n\nSpeedup\n\nFirst Place: Mark T. Jones and Paul Plassmann, Argonne National Laboratory; âLarge, sparse linear system solver that enabled the solution of vortex configurations in superconductors and the modeling of the vibration of piezo-electric crystals,â 4 Gflops on an Intel Touchstone Delta. Speedups between 350 and 500.\n\n1993\n\nPeak Performance\n\nFirst Place: Lyle N. Long and Matt Kamon, Penn. State University; Denny Dahl, Mark Bromley, Robert Lordi, Jack Myczkowski and Richard Shapiro, Thinking Machines; âModeling of a shock front using the Boltzmann Equation,â 60 Gflops on a 1,024 processor CM-5\n\nHonorable Mention: Peter S. Lomdahl, Pablo Tamayo, Niels Gronbech-Jensen and David M. Beazley, Los Alamos National Laboratory; âSimulating the micro-structure of grain boundaries in solids,â 50 Gflops on a 1,024 processor CM-5\n\nPrice/Performance\n\nFirst Place: Robert W. Means and Bret Wallach, HNC Inc.; Robert C. Lengel Jr., Tracor Applied Sciences; âImage analysis using the bispectrum analysis algorithm,â 6.5 Gflops/$1M on a custom-built machine called SNAP\n\n1994\n\nPeak Performance\n\nFirst Place: David Womble, David Greenberg, Stephen Wheat and Robert Benner, Sandia National Laboratories; Marc Ingber, University of New Mexico; Greg Henry and Satya Gupta, Intel; âStructural mechanics modeling using the boundary element method,â 140 Gflops on a 1,904 node Intel Paragon\n\nPrice/Performance\n\nFirst Place: Stefan Goedecker, Cornell University; Luciano Colombo, UniversitÃ di Milano; âQuantum mechanical interactions among 216 silicon atoms,â 3 Gflops/$1M on a cluster of eight HP workstations\n\nHonorable Mention: H. Miyoshi, Foundation for Promotion of Material Science and Technology of Japan, M. Fukuda, T. Nakamura, M. Tuchiya, M. Yoshida, K. Yamamoto, Y. Yamamoto, S. Ogawa, Y. Matsuo and T. Yamane National Aerospace Laboratory; M. Takamura, M. Ikeda, S. Okada, Y. Sakamoto, T. Kitamura and H. Hatama, Fujitsu Limited; M. Kishimoto, Fujitsu Laboratories Limited; âIsotropic Turbulence and other CFD codes,â 120 Gflops on a 140 processor Numerical Wind Tunnel\n\n1995\n\nPeak Performance\n\nFirst Place: Masahiro Yoshida, Masahiro Fukuda and Takashi Nakamura, National Aerospace Laboratory (Japan); Atushi Nakamura, Yamagata University; Shini Hoiki, Hiroshima University; âQuantum chromodynamics simulation,â 179 Gflops on 128 processors of the Numerical Wind Tunnel\n\nPrice/Performance\n\nFirst Place: Panayotis Skordos, MIT; âModeling of air flow in flue pipes,â 3.6 Gflops/$1M on a cluster of 20 HP workstations\n\nSpecial-Purpose Machines\n\nFirst Place: Junichiro Makino and Makoto Taiji, University of Tokyo; âSimulation of the motion of 100,000 stars,â 112 Gflops using the Grape-4 machine with 288 processors\n\n1996\n\nPeak Performance\n\nFirst Place: Toshiyuki Iwamiya, Masahiro Yoshida, Yuichi Matsuo, Masahiro Fukuda and Takashi Nakamura, National Aerospace Laboratory (Japan); â Fluid dynamics problem,â 111 Gflops on 166 processor Numerical Wind Tunnel\n\nHonorable Mention: Toshiyuki Fukushige and Junichiro Makino, University of Tokyo; âSimulation of the motion of 780,000 stars,â 333 Gflops using the Grape-4 machine w/ 1,269 processors\n\nPrice/Performance\n\nFirst Place: Adolfy Hoisie, Cornell University; Stefan Goedecker and Jurg Hutter, Max Planck Institute; âElectronic structures calculations,â 6.3 Gflops/$1M on an SGI Power Challenge with 6 MIPS R8000 processors\n\n1997\n\nPeak Performance\n\nFirst Prize-Part 1: Michael S. Warren, Los Alamos, National Laboratory; John K. Salmon, Caltech; âSimulating the motion of 322,000,000 self-gravitating particles,â 430 Gflops on ASCI Red using 4,096 processors\n\nPrice/Performance\n\nFirst Prize: Nhan Phan-Thien and Ka Yan Lee, University of Sydney; David Tullock, Los Alamos National Laboratory; âModeling suspensions,â 10.8 Gflops/$1M on 28 DEC Alpha machines\n\nFirst Prize-Part 2: Michael S. Warren, Los Alamos, National Laboratory; John K. Salmon, Caltech; Donald J. Becker, NASA Goddard; M. Patrick Goda, Los Alamos National Laboratory; Thomas Sterling, Caltech; Gregoire S. Winckelmans, Universite Catholique de Louvain (Belgium); âTwo problems: vortex fluid flow modeled with 360,000 particles; galaxy formation following 10,000,000 self-gravitating particles,â 18 Gflops/$1M on a cluster of 16 Intel Pentium Pros (200 Mhz.)\n\n1998\n\nPeak Performance\n\nFirst Prize: Balazs Ujfalussy, Xindong Wang, Xiaoguang Zhang, Donald M. C. Nicholson, William A. Shelton and G. Malcolm Stocks, Oak Ridge National Laboratory; Andrew Canning, Lawrence Berkeley National Laboratory; Yang Wang , Pittsburgh Supercomputing Center; Balazs L. Gyorffy, H. H. Wills Physics Laboratory, UK; âFirst principles calculation, of a unit cell (512 atoms) model of non-collinear magnetic arrangements for metallic magnets using a variation of the locally self-consistent multiple scattering method,â 657 Gflops on a 1,024-PE Cray T3E system (600 Mhz)\n\nSecond Prize: Mark P. Sears, Sandia National Laboratories; Ken Stanley, University of California, Berkeley; Greg Henry, Intel; âElectronic structures: a silicon bulk periodic unit cell of 3072 atoms, and an aluminum oxide surface unit cell of 2160 atoms, using a complete dense generalized Hermitian eigenvalue-eigenvector calculation,â 605 Gflops on the ASCI Red machine with 9200 processors (200 Mhz.)\n\nPrice/Performance\n\nFirst Prize: Dong Chen, MIT; Ping Chen, Norman H. Christ, George Fleming, Chulwoo Jung, Adrian Kahler, Stephen Kasow, Yubing Luo, Catalin Malureanu and Cheng Zhong Sui, Columbia University; Robert G. Edwards and Anthony D. Kennedy, Florida State University; Alan Gara, Robert D. Mawhinney, John Parsons, Pavlos Vranas and Yuri Zhestkov, Columbia University; Sten Hansen, Fermi National Accelerator Laboratory; Greg Kilcup, Ohio State University; â3 lattice quantum chromodynamics computations,â 79.7 Gflops/$1M on a custom system with 2,048 PEâs using a Texas Instruments chip (32-bit floating point ops.)\n\nSecond Prize: Michael S. Warren, Timothy C. Germann, Peter S. Lomdahl and David M. Beazley, Los Alamos National Laboratory; John K. Salmon, Caltech; âSimulation of a shock wave propagating through a structure of 61 million atoms,â 64.9 Gflops/$1M using a 70 PE system of DEC Alphaâs (533 Mhz.)\n\n1999\n\nPeak Performance\n\nFirst Prize: A. A. Mirin, R. H. Cohen, B. C. Curtis, W. P. Dannevik, A. M. Dimits, M. A. Duchaineau, D. E. Eliason and D. R. Schikore, Lawrence Livermore National Laboratory; S. E. Anderson, D. H. Porter and R. Woodward, University of Minnesota; L. J. Shieh and S. W. White, IBM; âVery high resolution simulation of fluid turbulence in compressible flows,â 1.18 Tflop/s on short run on 5832 CPUâs on ASCI Blue Pacific, 1.04 Tflop/s sustained on one-hour run, 600 Gflop/s on one-week run on 3840 CPUâs\n\nPrice/Performance\n\nFirst Prize: Atsuchi Kawai, Toshiyuki Fushushige and Junichiro Makino, University of Tokyo; âAstrophysical n-body simulation,â 144 Gflops/$1M on custom-built GRAPE-5 32-processor system\n\nSpecial\n\nFirst Prize, Shared: W. K. Anderson, NASA Langley Research Center; W. D. Gropp, D, K. Kaushik, B.F. Smith, Argonne National Laboratory; D. E. Keyes, Old Dominion University, Lawrence Livermore National Laboratory, and ICASE, NASA Langley Research Center; âUnstructured tetrahedral mesh fluid dynamics using PETSc library,â 156 Gflop/s on 2048 nodes of ASCI Red, using one CPU per node for computation\n\nFirst Prize, Shared: H. M. Tufo, University of Chicago; P. F. Fischer, Argonne National Laboratory; âSpectral element calculation using a sparse system solver,â 319 Gflop/s on 2048 nodes of ASCI Red, using two CPUâs per node for computation\n\n2000\n\nPeak Performance\n\nCompetitors for this year’s prize for best performance tied, each achieving 1.34 teraflops.\n\nFirst place: Tetsu Narumi, Ryutaro Susukita, Takahiro Koishi, Kenji Yasuoka, Hideaki Furusawa, Atsushi Kawai and Thoshikazu Ebisuzaki; âMolecular Dynamic Simulation for NaCl for a Special Purpose Computer: MDM,â 1.34 Tflops.\n\nFirst place: Junichiro Makino, Toshiyuki Fukushige and Masaki Koga; âSimulation of Black Holes in a Galactic Center on GRAPE-6,â 1.349 Tflops.\n\nPrice/Performance\n\nFirst place: Douglas Aberdeen, Jonathan Baxter and Robert Edwards; â92 cents/Mflops Ultra-Large Scale Neural Network Training on a PIII Cluster.â\n\nHonorable Mention: Thomas Hauser, Timothy I. Mattox, Raymond P. LeBeau, Henry G. Dietz and P. George Huang, University of Kentucky; âHigh-Cost CFD on a Low-Cost Cluster.”\n\nSpecial\n\nAlan Calder, B.C. Curtis, Jonathan Dursi, Bruce Fryxell, G. Henry, P. MacNeice, Kevin Olson, Paul Ricker, Robert Rosner, Frank Timmes, Henry Tufo, James Truran and Michael Zingale; âHigh-Peformance Reactive Fluid Flow Simulations Using Adaptive Mesh Refinement on Thousands of Processors.â\n\n2001\n\nPeak Performance\n\nToshiyuki Fukushige and Junichiro Makino; âSimulation of black holes in a galactic center,” 11.55 Tflop/s.\n\nPrice/Performance\n\nJoon Hwang, Seung Kim and Chang Lee, “Study of impact locating on aircraft structure,âÂ by low-cost cluster cost 24.6 cents/Mflop/s, or less than 25 cents per 1-million floating operations per second.\n\nSpecial\n\nGabrielle Allen, Thomas Dramlitsch, Ian Foster, Nick Karonis, Matei Ripeanu, Edward Seidel and Brian Toonen for supporting efficient execution in the heterogeneous distributed computing environments with Cactus and Globus.\n\n2002\n\nPeak Performance\n\nSatoru Shingu, Yoshinori Tsuda, Wataru Ohfuchi, Kiyoshi Otsuka, Earth Simulator Center, Japan Marine Science and Technology Center; Hiroshi Takahara, Takashi Hagiwara, Shin-ichi Habata, NEC Corporation; Hiromitsu Fuchigami, Masayuki Yamada, Yuji Sasaki, Kazuo Kobayashi, NEC Informatec Systems; Mitsuo Yokokawa, National Institute of Advanced Industrial Science and Technology; Hiroyuki Itoh, National Space Development Agency of Japan. âA 26.58 Tflops Global Atmospheric Simulation with the Spectral Transform Method on the Earth Simulator,â 26.58 Tflops simulation of a complex climate system using an atmospheric circulation model called AFES.\n\nSpecial Award for Language\n\nHitoshi Sakagami, Himeji Institute of Technology; Hitoshi Murai, Earth Simulator Center, Japan Marine Science and Technology Center; Yoshiki Seo, NEC Corporation; Mitsuo Yokokawa, Japan Atomic Energy Research Institute; â14.9 Tflops Three-dimensional Fluid Simulation for Fusion Science with HPF on the Earth Simulator,â 14.9 Tflops run of a parallelized version of IMPACT-3D, an application written in High Performance Fortran that simulates the instability in an imploding system, such as the ignition of a nuclear device.\n\nSpecial\n\nMitsuo Yokokawa, Japan Atomic Energy Research Institute; Ken’ichi Itakura, Atsuya Uno, Earth Simulator Center, Japan Marine Science and Technology Center; Takashi Ishihara, Yukio Kaneda, Nagoya University; â16.4-Tflops Direct Numerical Simulation of Turbulence by a Fourier Spectral Method on the Earth Simulator.â New methods for handling the extremely data-intensive calculation of a three-dimensional Fast Fourier Transform on the Earth Simulator have allowed researchers to overcome a major hurdle for high performance simulations of turbulence.\n\nManoj Bhardwaj, Kendall Pierson, Garth Reese, Tim Walsh, David Day, Ken Alvin, James Peery, Sandia National Laboratories; Charbel Farhat, Michel Lesoinne, University of Colorado at Boulder; âSalinas: A Scalable Software for High Performance Structural and Solid Mechanics Simulation.â The structural mechanics community has embraced Salinas, engineering software over 100,000 lines long that has run on a number of advanced systems, including a sustained 1.16 Tflops performance on 3,375 ASCI White processors.\n\nJames C. Phillips, Gengbin Zheng, Sameer Kumar, Laxmikant V. Kale, University of Illinois at Urbana-Champaign; âNAMD: Biomolecular Simulation on Thousands of Processors.â Researchers achieved unprecedented scaling of NAMD, a code that renders an atom-by-atom blueprint of large biomolecules and biomolecular systems.\n\n2003\n\nPeak Performance\n\nDimitri Komatitsch, Chen Ji, and Jeroen Tromp, California Institute of Technology; and Seiji Tsuboi, Institute for Frontier Research on Earth Evolution, JAMSTEC; âA 14.6 Billion Degrees of Freedom, 5 Teraflop/s, 2.5 Terabyte Earthquake Simulation on the Earth Simulator.” The researchers used 1,944 processors of the Earth Simulator to model seismic wave propagation resulting from large earthquakes.\n\nSpecial Achievement\n\nVolkan Akcelik, Jacobo Bielak, Ioannis Epanomeritakis, Antonio Fernandez, Omar Ghattas, Eui Joong Kim, Julio Lopez, David O’Hallaron and Tiankai Tu, Carnegie Mellon University; George Biros, Courant Institute, New York University; and John Urbanic, Pittsburgh Supercomputing Center; “High Resolution Forward and Inverse Earthquake Modeling on Terascale Computers.” The researchers developed earthquake simulation algorithms and tools and used them to carry out simulations of the 1994 Northridge earthquake in the Los Angeles Basin using 100 million grid points.\n\nSpecial Achievement (“lifetime”)\n\nJunichiro Makino and Hiroshi Daisaka, University of Tokyo; Eiichiro Kokubo, National Astronomical Observatory of Japan; and Toshiyuki Fukushige, University of Tokyo; “Performance Evaluation and Tuning of GRAPE-6âTowards 40 ‘Real’ Tflop/s.” The researchers benchmarked GRAPE-6, a sixth-generation special-purpose computer for gravitational many-body problems, and presented the measured performance for a few real applications with a top speed of 35.3 teraflops.\n\n2004\n\nPeak Performance\n\nAkira Kageyama, Masanori Kameyama, Satoru Fujihara, Masaki Yoshida, Mamoru Hyodo, and Yoshinori Tsuda, JAMSTEC; âA 15.2 TFlops Simulation of Geodynamo on the Earth Simulator,â 15.2 TFlop/s on 4,096 processors of the Earth Simulator.\n\nSpecial\n\nMark.F. Adams, Sandia National Laboratories; Harun H. Bayraktar, Abuqus Corp.; Tony M. Keaveny and Panayiotis Papadopoulos, University of California, Berkeley; âUlltrascalable implicit finite element analyses in solid mechanics with over half a billion degrees of freedom.â\n\n2005\n\nPeak Performance\n\nFrederick H. Streitz, James N. Glosli, Mehul V. Patel, Bor Chan, Robert K. Yates, Bronis R. de Supinski, Lawrence Livermore National Laboratory; James Sexton and John A. Gunnels, IBM; â100+ TFlop Solidification Simulations on BlueGene/L.â The team achieved up to 107 teraflop/s (trillion operations per second) with a sustained rate of 101.7 teraflop/s over a seven-hour run on the IBM BlueGeneL’s 131,072 processors.\n\n2006\n\nPrice/Performance\n\nFrancois Gygi University of California, Davis; Erik W. Draeger, Martin Schulz and Bronis R. de Supinski, Lawrence Livermore National Laboratory; John A. Gunnels, Vernon Austel and James C. Sexton, IBM Watson Research Center; Franz Franchetti, Carnegie Mellon University; Stefan Kral, Christoph W. Ueberhuber and Juergen Lorenz; Vienna University of Technology; âLarge-scale electronic structure calculations of high-Z metals on the BlueGene/L platform.â A sustained peak performance of 207.3 TFlop/s was measured on 65,536 nodes, corresponding to 56.5% of the theoretical full machine peak using all 128k CPUs.\n\nAward paper\n\nHonorable Mention: Tetsu Narumi, Yousuke Ohno, Noriaki Okimoto, Takahiro Koishi, Atsushi Suenaga, Futatsugi, Ryoko Yanai, Ryutaro Himeno, Shigenori Fujikawa and Makoto Taiji, all of RIKEN; and Mitsuru Ikei, Intel Corp.; âA 185 Tflop/s Simulation of Amyloid-forming Peptides from Yeast Prion Sup35 with the Special-Purpose Computer System MD-GRAPE3.â\n\nSpecial Achievement\n\nPavlos Vranas, Gyan Bhanot, Matthias Blumrich, Dong Chen, Alan Gara, Philip Heidelberger, Valentina Salapura and James C. Sexton, all of IBM Watson Research Center; âThe BlueGene/L supercomputer and quantum ChromoDynamics,â QCD simulation that achieved 12.2 Teraflops sustained performance with perfect speedup to 32K CPU cores.\n\nAward paper\n\n2007\n\nPeak Performance\n\nJames N. Glosli, David F. Richards, Kyle J. Caspersen, Robert E. Rudd and Frederick H. Streitz, all of Lawrence Livermore National Laboratory; and John Gunnels of IBM Watson Research Center; âExtending Stability Beyond CPU Millennium: A Micron-Scale Simulation of Kelvin-Helmholtz Instability.â The team that won the 2005 Gordon Bell Prize for a simulation investigating the solidification in tantalumand uranium at extreme temperatures and pressure, with simulations ranging in size from 64,000 atoms to 524 million atoms, used an expanded machine to conduct simulations of up to 62.5 billion atoms. The optimized ddcMD code is benchmarked at 115.1 Tflop/s in their scaling study and 103.9 Tflop/s in a sustained science run.\n\nAward paper\n\n2008\n\nPeak Performance\n\nGonzalo Alvarez, Michael S. Summers, Don E. Maxwell, Markus Eisenbach, Jeremy S. Meredith, Thomas A. Maier, Paul R. Kent, Eduardo D’Azevedo and Thomas C. Schulthess, all of Oak Ridge National Laboratory; and Jeffrey M. Larkin and John M. Levesque, both of Cray, Inc.; âNew Algorithm to Enable 400+ TFlop/s Sustained Performance in Simulations of Disorder Effects in High-Tc.â\n\nAward paper\n\nAlgorithm Innovation\n\nLin-Wang Wang, Byounghak Lee, Hongzhang Shan, Zhengji Zhao, Juan Meza, Erich Strohmaier, and David H. Bailey, Lawrence Berkeley National Laboratory; âLinear Scaling Divide-and-Conquer Electronic Structure Calculations for Thousand Atom Nanostructures,â for special achievement in high performance computing for their research into the energy harnessing potential of nanostructures. Their method, which was used to predict the efficiency of a new solar cell material, achieved impressive performance and scalability.\n\nAward paper\n\n2009\n\nPeak Performance\n\nMarkus Eisenbach and Donald M. Nicholson, Oak Ridge National Laboratory; Cheng-gang Zhou, J.P. Morgan Chase; Gregory Brown, Florida State University; Jeffrey Larkin, Cray Inc.; and Thomas Schulthess, ETH Zurich; âA scalable method for ab initio computation of free energies in nanoscale systems,â on the Cray XT5 system at ORNL, sustaining 1.03 Petaflop/s in double precision on 147,464 cores.\n\nAward paper\n\nPrice/Performance\n\nTsuyoshi Hamada, Nagasaki University; Tetsu Narumi, University of Electro-Communications, Tokyo; Rio Yokota, University of Bristol; Kenji Yasuoka, Keio University, Yokohama; Keigo Nitadori and Makoto Taiji, RIKEN Advanced Science Institute; â42 TFlops hierarchical N-body simulations on GPUs with applications in both astrophysics and turbulence.â The maximum corrected performance is 28.1TFlops for the gravitational simulation, which results in a cost performance of 124 MFlops/$1M.\n\nAward paper\n\nSpecial Category\n\nDavid E. Shaw, Ron O. Dror, John K. Salmon, J. P. Grossman, Kenneth M. Mackenzie, Joseph A. Bank, Cliff Young, Martin M. Deneroff, Brannon Batson, Kevin J. Bowers, Edmond Chow, Michael P. Eastwood, Douglas J. Ierardi, John L. Klepeis, Jeffrey S. Kuskin, Richard H. Larson, Kresten Lindorff-Larsen, Paul Maragakis, Mark A. Moraes, Stefano Piana, Yibing Shan and Brian Towles, all of D.F. Shaw Research; âMillisecond-scale molecular dynamics simulations on Anton.â\n\nAward paper\n\n2010\n\nPeak Performance\n\nAbtin Rahimian and Ilya Lashuk, Georgia Tech; Shravan Veerapaneni, NYU; Aparna Chandramowlishwaran, Dhairya Malhotra, Logan Moon and Aashay Shringarpure, Georgia Tech; Rahul Sampath and Jeffrey Vetter, Oak Ridge National Laboratory; Richard Vuduc and George Biros, Georgia Tech; Denis Zorin, NYU; âPetascale Direct Numerical Simulation of Blood Flow on 200K Cores and Heterogeneous Architectures,â achieved 0.7 Petaflops/s of sustained performance on Jaguar.\n\nAward paper\n\nHonorable mention (first): Anton Kozhevnikov, Institut for Theoretical Physics, ETH Zurich; Adolfo G. Eguiluz, The University of Tennessee, Knoxville; and Thomas C. Schulthess, Swiss National Supercomputer Center and Oak Ridge National Laboratory; âToward First Principles Electronic Structure Simulations of Excited States and Strong Correlations in Nano- and Materials Science.â\n\nHonorable mention (second): Tsuyoshi Hamada, Nagasaki University; and Keigo Nitadori, RIKEN Advanced Science Institute; â190 TFlops Astrophysical N-body Simulation on a Cluster of GPUs.â\n\n2011\n\nSustained Performance\n\nYukihiro Hasegawa, Next-Generation Supercomputer R&D Center, Riken; Jun-Ichi Iwata, Miwako Tsuji and Daisuke Takahashi, University of Tskuba;Â Atsushi Oshiyama, University of Tokyo; Kazuo Minami, Taisuke Boku, University of Tskuba; Fumiyoshi Shoji, Atsuya Uno and Motoyoshi Kurokawa, Next-Generation Supercomputer R&D Center, Riken; Hikaru Inoue and Ikuo Miyoshi, Fujitsu Ltd.; and Mitsuo Yokokawa, Next-Generation Supercomputer R&D Center, Riken; âFirst-principles calculations of electron states of a silicon nanowire with 100,000 atoms on the K computer.â A 3.08 petaflops sustained performance was measured for one iteration of the SCF calculation in a 107,292-atom Si nanowire calculation using 442,368 cores, which is 43.63% of the peak performance of 7.07 Pflop/s.\n\nAward paper\n\nScalability and Time to Solution\n\nTakashi Shimokawabe, Takayuki Aoki, Tomohiro Takaki, Toshio Endo, Akinori Yamanaka, Naoya Maruyama, Akira Nukada, and Satoshi Matsuoka, all of Tokyo Institute of Technology; âPetascale phase-field simulation for dendritic solidification on the TSUBAME 2.0 supercomputer,â simulations on the GPU-rich TSUBAME 2.0 supercomputer at the Tokyo Institute of Technology have demonstrated good weak scaling and achieved 1.017 PFlops in single precision for our largest configuration, using 4,000 GPUs along with 16,000 CPU cores.\n\nAward paper\n\nBecause of the unusually high quality of all of the ACM Gordon Bell Prize finalists, the committee took the unusual step of awarding Honorable Mentions to the remaining three finalists papers:\n\nâAtomistic nanoelectronics device engineering with sustained performances up to 1.44 Pflop/sâ by Mathieu Luisier et al.,\n\nâPetaflop biofluidics simulations on a two million-core system,â by Simone Melchionna et al., and\n\nâA new computational paradigm in multiscale simulations: Application to brain blood flow,â by Leopold Grinberg et al.\n\n2012\n\nScalability and Time to Solution\n\nTomoaki Ishiyama, Keigo Nitadori, University of Tskuba; and Junichiro Makino, Tokyo Institute of Technology; â4.45 Pflops astrophysical N-body simulation on K computer: the gravitational trillion-body problem,â The average performance on 24576 and 82944 nodes of K computer are 1.53 and 4.45 Pflop/s, which correspond to 49% and 42% of the peak speed.\n\nAward paper\n\n2013\n\nBest Performance of a High Performance Application\n\nDiego Rossinelli, Babak Hejazialhosseini, Panagiotis Hadjidoukas and Petros Koumoutsakos, all of ETH Zurich; Costas Bekas and Alessandro Curioni of IBM Zurich Research Laboratory; and Steffen Schmidt and Nikolaus Adams of Technical University Munich; â11 Pflop/s simulations of cloud cavitation collapse,â high throughput simulations of cloud cavitation collapse on 1.6 million cores of Sequoia reaching 55% of its nominal peak performance, corresponding to 11 Pflop/s.\n\nAward paper\n\n2014\n\nBest Performance of a High Performance Application\n\nDavid E. Shaw, J.P. Grossman, Joseph A. Bank, Brannon Batson, J. Adam Butts, Jack C. Chao, Martin M. Deneroff, Ron O. Dror, Amos Even, Christopher H. Fenton, Anthony Forte, Joseph Gagliardo, Gennette Gill, Brian Greskamp, C. Richard Ho, Douglas J. Ierardi, Lev Iserovich, Jeffrey S. Kuskin, Richard H. Larson, Timothy Layman, Li-Siang Lee, Adam K. Lerer, Chester Li, Daniel Killebrew, Kenneth M. Mackenzie, Shark Yeuk-Hai Mok, Mark A. Moraes, Rolf Mueller, Lawrence J. Nociolo, Jon L. Peticolas, Terry Quan, Daniel Ramot, John K. Salmon, Daniele P. Scarpazza, U. Ben Schafer, Naseer Siddique, Christopher W. Snyder, Jochen Spengler, Ping Tak Peter Tang, Michael Theobald, Horia Toma, Brian Towles, Benjamin Vitale, Stanley C. Wang and Cliff Young: all of D.E. Shaw Research; âAnton 2: raising the bar for performance and programmability in a special-purpose molecular dynamics supercomputer.â Anton 2 is the first platform to achieve simulation rates of multiple microseconds of physical time per day for systems with millions of atoms. Demonstrating strong scaling, the machine simulates a standard 23,558-atom benchmark system at a rate of 85 Î¼s/dayâ180 times faster than any commodity hardware platform or general-purpose supercomputer.\n\nAward paper\n\n2015\n\nOutstanding Achievement in High-performance Computing Scalability\n\nJohann Rudi and Tobin Isaac, Omar Ghattas University of Texas at Austin; A. Cristiano I. Malossi, Peter W. J. Staar, Yves Ineichen, Costas Bekas, Alessandro Curioni, IBM Research, Zurich; Georg Stadler, New York University; and Michael Gurnis, Caltech; âAn extreme-scale implicit solver for complex PDEs: highly heterogeneous flow in earth’s mantle,â scaled to 1.5 million cores for severely nonlinear, ill-conditioned, heterogeneous, and anisotropic PDEs.\n\nAward paper\n\nList compiled by Jon Bashor, SC16 Communications Committee member from Lawrence Berkeley National Lab."
    }
}
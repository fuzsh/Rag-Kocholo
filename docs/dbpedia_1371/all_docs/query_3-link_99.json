{
    "id": "dbpedia_1371_3",
    "rank": 99,
    "data": {
        "url": "https://www.science.gov/topicpages/a/accurate%2Bspatial%2Brepresentation",
        "read_more_link": "",
        "language": "en",
        "title": "accurate spatial representation: Topics by Science.gov",
        "top_image": "",
        "meta_img": "",
        "images": [
            "https://www.science.gov/scigov/desktop/en/images/SciGov_logo.png"
        ],
        "movies": [],
        "keywords": [],
        "meta_keywords": [
            ""
        ],
        "tags": null,
        "authors": [],
        "publish_date": null,
        "summary": "",
        "meta_description": "",
        "meta_lang": "en",
        "meta_favicon": "",
        "meta_site_name": "",
        "canonical_link": null,
        "text": "Exploring the Structure of Spatial Representations\n\nPubMed Central\n\nMadl, Tamas; Franklin, Stan; Chen, Ke; Trappl, Robert; Montaldi, Daniela\n\n2016-01-01\n\nIt has been suggested that the map-like representations that support human spatial memory are fragmented into sub-maps with local reference frames, rather than being unitary and global. However, the principles underlying the structure of these âcognitive mapsâ are not well understood. We propose that the structure of the representations of navigation space arises from clustering within individual psychological spaces, i.e. from a process that groups together objects that are close in these spaces. Building on the ideas of representational geometry and similarity-based representations in cognitive science, we formulate methods for learning dissimilarity functions (metrics) characterizing participantsâ psychological spaces. We show that these learned metrics, together with a probabilistic model of clustering based on the Bayesian cognition paradigm, allow prediction of participantsâ cognitive map structures in advance. Apart from insights into spatial representation learning in human cognition, these methods could facilitate novel computational tools capable of using human-like spatial concepts. We also compare several features influencing spatial memory structure, including spatial distance, visual similarity and functional similarity, and report strong correlations between these dimensions and the grouping probability in participantsâ spatial representations, providing further support for clustering in spatial memory. PMID:27347681\n\nRepresentation Elements of Spatial Thinking\n\nNASA Astrophysics Data System (ADS)\n\nFiantika, F. R.\n\n2017-04-01\n\nThis paper aims to add a reference in revealing spatial thinking. There several definitions of spatial thinking but it is not easy to defining it. We can start to discuss the concept, its basic a forming representation. Initially, the five sense catch the natural phenomenon and forward it to memory for processing. Abstraction plays a role in processing information into a concept. There are two types of representation, namely internal representation and external representation. The internal representation is also known as mental representation; this representation is in the human mind. The external representation may include images, auditory and kinesthetic which can be used to describe, explain and communicate the structure, operation, the function of the object as well as relationships. There are two main elements, representations properties and object relationships. These elements play a role in forming a representation.\n\nPopulation Coding of Visual Space: Comparison of Spatial Representations in Dorsal and Ventral Pathways\n\nPubMed Central\n\nSereno, Anne B.; Lehky, Sidney R.\n\n2011-01-01\n\nAlthough the representation of space is as fundamental to visual processing as the representation of shape, it has received relatively little attention from neurophysiological investigations. In this study we characterize representations of space within visual cortex, and examine how they differ in a first direct comparison between dorsal and ventral subdivisions of the visual pathways. Neural activities were recorded in anterior inferotemporal cortex (AIT) and lateral intraparietal cortex (LIP) of awake behaving monkeys, structures associated with the ventral and dorsal visual pathways respectively, as a stimulus was presented at different locations within the visual field. In spatially selective cells, we find greater modulation of cell responses in LIP with changes in stimulus position. Further, using a novel population-based statistical approach (namely, multidimensional scaling), we recover the spatial map implicit within activities of neural populations, allowing us to quantitatively compare the geometry of neural space with physical space. We show that a population of spatially selective LIP neurons, despite having large receptive fields, is able to almost perfectly reconstruct stimulus locations within a low-dimensional representation. In contrast, a population of AIT neurons, despite each cell being spatially selective, provide less accurate low-dimensional reconstructions of stimulus locations. They produce instead only a topologically (categorically) correct rendition of space, which nevertheless might be critical for object and scene recognition. Furthermore, we found that the spatial representation recovered from population activity shows greater translation invariance in LIP than in AIT. We suggest that LIP spatial representations may be dimensionally isomorphic with 3D physical space, while in AIT spatial representations may reflect a more categorical representation of space (e.g., ânext toâ or âaboveâ). PMID:21344010\n\nAn investigation of spatial representation of pitch in individuals with congenital amusia.\n\nPubMed\n\nLu, Xuejing; Sun, Yanan; Thompson, William Forde\n\n2017-09-01\n\nSpatial representation of pitch plays a central role in auditory processing. However, it is unknown whether impaired auditory processing is associated with impaired pitch-space mapping. Experiment 1 examined spatial representation of pitch in individuals with congenital amusia using a stimulus-response compatibility (SRC) task. For amusic and non-amusic participants, pitch classification was faster and more accurate when correct responses involved a physical action that was spatially congruent with the pitch height of the stimulus than when it was incongruent. However, this spatial representation of pitch was not as stable in amusic individuals, revealed by slower response times when compared with control individuals. One explanation is that the SRC effect in amusics reflects a linguistic association, requiring additional time to link pitch height and spatial location. To test this possibility, Experiment 2 employed a colour-classification task. Participants judged colour while ignoring a concurrent pitch by pressing one of two response keys positioned vertically to be congruent or incongruent with the pitch. The association between pitch and space was found in both groups, with comparable response times in the two groups, suggesting that amusic individuals are only slower to respond to tasks involving explicit judgments of pitch.\n\nAuditory spatial representations of the world are compressed in blind humans.\n\nPubMed\n\nKolarik, Andrew J; Pardhan, Shahina; Cirstea, Silvia; Moore, Brian C J\n\n2017-02-01\n\nCompared to sighted listeners, blind listeners often display enhanced auditory spatial abilities such as localization in azimuth. However, less is known about whether blind humans can accurately judge distance in extrapersonal space using auditory cues alone. Using virtualization techniques, we show that auditory spatial representations of the world beyond the peripersonal space of blind listeners are compressed compared to those for normally sighted controls. Blind participants overestimated the distance to nearby sources and underestimated the distance to remote sound sources, in both reverberant and anechoic environments, and for speech, music, and noise signals. Functions relating judged and actual virtual distance were well fitted by compressive power functions, indicating that the absence of visual information regarding the distance of sound sources may prevent accurate calibration of the distance information provided by auditory signals.\n\nSpectral-spatial hyperspectral image classification using super-pixel-based spatial pyramid representation\n\nNASA Astrophysics Data System (ADS)\n\nFan, Jiayuan; Tan, Hui Li; Toomik, Maria; Lu, Shijian\n\n2016-10-01\n\nSpatial pyramid matching has demonstrated its power for image recognition task by pooling features from spatially increasingly fine sub-regions. Motivated by the concept of feature pooling at multiple pyramid levels, we propose a novel spectral-spatial hyperspectral image classification approach using superpixel-based spatial pyramid representation. This technique first generates multiple superpixel maps by decreasing the superpixel number gradually along with the increased spatial regions for labelled samples. By using every superpixel map, sparse representation of pixels within every spatial region is then computed through local max pooling. Finally, features learned from training samples are aggregated and trained by a support vector machine (SVM) classifier. The proposed spectral-spatial hyperspectral image classification technique has been evaluated on two public hyperspectral datasets, including the Indian Pines image containing 16 different agricultural scene categories with a 20m resolution acquired by AVIRIS and the University of Pavia image containing 9 land-use categories with a 1.3m spatial resolution acquired by the ROSIS-03 sensor. Experimental results show significantly improved performance compared with the state-of-the-art works. The major contributions of this proposed technique include (1) a new spectral-spatial classification approach to generate feature representation for hyperspectral image, (2) a complementary yet effective feature pooling approach, i.e. the superpixel-based spatial pyramid representation that is used for the spatial correlation study, (3) evaluation on two public hyperspectral image datasets with superior image classification performance.\n\nTransformations and representations supporting spatial perspective taking\n\nPubMed Central\n\nYu, Alfred B.; Zacks, Jeffrey M.\n\n2018-01-01\n\nSpatial perspective taking is the ability to reason about spatial relations relative to anotherâs viewpoint. Here, we propose a mechanistic hypothesis that relates mental representations of oneâs viewpoint to the transformations used for spatial perspective taking. We test this hypothesis using a novel behavioral paradigm that assays patterns of response time and variation in those patterns across people. The results support the hypothesis that people maintain a schematic representation of the space around their body, update that representation to take anotherâs perspective, and thereby to reason about the space around their body. This is a powerful computational mechanism that can support imitation, coordination of behavior, and observational learning. PMID:29545731\n\nAudio Spatial Representation Around the Body\n\nPubMed Central\n\nAggius-Vella, Elena; Campus, Claudio; Finocchietti, Sara; Gori, Monica\n\n2017-01-01\n\nStudies have found that portions of space around our body are differently coded by our brain. Numerous works have investigated visual and auditory spatial representation, focusing mostly on the spatial representation of stimuli presented at head level, especially in the frontal space. Only few studies have investigated spatial representation around the entire body and its relationship with motor activity. Moreover, it is still not clear whether the space surrounding us is represented as a unitary dimension or whether it is split up into different portions, differently shaped by our senses and motor activity. To clarify these points, we investigated audio localization of dynamic and static sounds at different body levels. In order to understand the role of a motor action in auditory space representation, we asked subjects to localize sounds by pointing with the hand or the foot, or by giving a verbal answer. We found that the audio sound localization was different depending on the body part considered. Moreover, a different pattern of response was observed when subjects were asked to make actions with respect to the verbal responses. These results suggest that the audio space around our body is split in various spatial portions, which are perceived differently: front, back, around chest, and around foot, suggesting that these four areas could be differently modulated by our senses and our actions. PMID:29249999\n\nThe Ability of Young Korean Children to Use Spatial Representations\n\nERIC Educational Resources Information Center\n\nKim, Minsung; Bednarz, Robert; Kim, Jaeyil\n\n2012-01-01\n\nThe National Research Council emphasizes using tools of representation as an essential element of spatial thinking. However, it is debatable at what age the use of spatial representation for spatial thinking skills should begin. This study investigated whether young Korean children possess the potential to understand map-like representation usingâ¦\n\nRetrieving Enduring Spatial Representations after Disorientation\n\nPubMed Central\n\nLi, Xiaoou; Mou, Weimin; McNamara, Timothy P.\n\n2012-01-01\n\nFour experiments tested whether there are enduring spatial representations of objectsâ locations in memory. Previous studies have shown that under certain conditions the internal consistency of pointing to objects using memory is disrupted by disorientation. This disorientation effect has been attributed to an absence of or to imprecise enduring spatial representations of objectsâ locations. Experiment 1 replicated the standard disorientation effect. Participants learned locations of objects in an irregular layout and then pointed to objects after physically turning to face an object and after disorientation. The expected disorientation was observed. In Experiment 2, after disorientation, participants were asked to imagine they were facing the original learning direction and then physically turned to adopt the test orientation. In Experiment 3, after disorientation, participants turned to adopt the test orientation and then were informed of the original viewing direction by the experimenter. A disorientation effect was not observed in Experiment 2 or 3. In Experiment 4, after disorientation, participants turned to face the test orientation but were not told the original learning orientation. As in Experiment 1, a disorientation effect was observed. These results suggest that there are enduring spatial representations of objectsâ locations specified in terms of a spatial reference direction parallel to the learning view, and that the disorientation effect is caused by uncertainty in recovering the spatial reference direction relative to the testing orientation following disorientation. PMID:22682765\n\nNo functional role of attention-based rehearsal in maintenance of spatial working memory representations.\n\nPubMed\n\nBelopolsky, Artem V; Theeuwes, Jan\n\n2009-10-01\n\nThe present study systematically examined the role of attention in maintenance of spatial representations in working memory as proposed by the attention-based rehearsal hypothesis [Awh, E., Jonides, J., & Reuter-Lorenz, P. A. (1998). Rehearsal in spatial working memory. Journal of Experimental Psychology--Human Perception and Performance, 24(3), 780-790]. Three main issues were examined. First, Experiments 1-3 demonstrated that inhibition and not facilitation of visual processing is often observed at the memorized location during the retention interval. This inhibition was caused by keeping a location in memory and not by the exogenous nature of the memory cue. Second, Experiment 4 showed that inhibition of the memorized location does not lead to any significant impairment in memory accuracy. Finally, Experiment 5 connected current results to the previous findings and demonstrated facilitation of processing at the memorized location. Importantly, facilitation of processing did not lead to more accurate memory performance. The present results challenge the functional role of attention in maintenance of spatial working memory representations.\n\nSpatial Representation by Blind and Sighted Children\n\nERIC Educational Resources Information Center\n\nMillar, Susanna\n\n1976-01-01\n\nProblem studied: How children represent haptic spatial information in memory. Question aimed at: Whether, and if so in what ways, children's spatial representations differ according to the main modality of prior experience. (JH)\n\nRepresentation control increases task efficiency in complex graphical representations.\n\nPubMed\n\nMoritz, Julia; Meyerhoff, Hauke S; Meyer-Dernbecher, Claudia; Schwan, Stephan\n\n2018-01-01\n\nIn complex graphical representations, the relevant information for a specific task is often distributed across multiple spatial locations. In such situations, understanding the representation requires internal transformation processes in order to extract the relevant information. However, digital technology enables observers to alter the spatial arrangement of depicted information and therefore to offload the transformation processes. The objective of this study was to investigate the use of such a representation control (i.e. the users' option to decide how information should be displayed) in order to accomplish an information extraction task in terms of solution time and accuracy. In the representation control condition, the participants were allowed to reorganize the graphical representation and reduce information density. In the control condition, no interactive features were offered. We observed that participants in the representation control condition solved tasks that required reorganization of the maps faster and more accurate than participants without representation control. The present findings demonstrate how processes of cognitive offloading, spatial contiguity, and information coherence interact in knowledge media intended for broad and diverse groups of recipients.\n\nRepresentation control increases task efficiency in complex graphical representations\n\nPubMed Central\n\nMeyerhoff, Hauke S.; Meyer-Dernbecher, Claudia; Schwan, Stephan\n\n2018-01-01\n\nIn complex graphical representations, the relevant information for a specific task is often distributed across multiple spatial locations. In such situations, understanding the representation requires internal transformation processes in order to extract the relevant information. However, digital technology enables observers to alter the spatial arrangement of depicted information and therefore to offload the transformation processes. The objective of this study was to investigate the use of such a representation control (i.e. the users' option to decide how information should be displayed) in order to accomplish an information extraction task in terms of solution time and accuracy. In the representation control condition, the participants were allowed to reorganize the graphical representation and reduce information density. In the control condition, no interactive features were offered. We observed that participants in the representation control condition solved tasks that required reorganization of the maps faster and more accurate than participants without representation control. The present findings demonstrate how processes of cognitive offloading, spatial contiguity, and information coherence interact in knowledge media intended for broad and diverse groups of recipients. PMID:29698443\n\nRepresentations and processes of human spatial competence.\n\nPubMed\n\nGunzelmann, Glenn; Lyon, Don R\n\n2011-10-01\n\nThis article presents an approach to understanding human spatial competence that focuses on the representations and processes of spatial cognition and how they are integrated with cognition more generally. The foundational theoretical argument for this research is that spatial information processing is central to cognition more generally, in the sense that it is brought to bear ubiquitously to improve the adaptivity and effectiveness of perception, cognitive processing, and motor action. We describe research spanning multiple levels of complexity to understand both the detailed mechanisms of spatial cognition, and how they are utilized in complex, naturalistic tasks. In the process, we discuss the critical role of cognitive architectures in developing a consistent account that spans this breadth, and we note some areas in which the current version of a popular architecture, ACT-R, may need to be augmented. Finally, we suggest a framework for understanding the representations and processes of spatial competence and their role in human cognition generally. Copyright Â© 2011 Cognitive Science Society, Inc.\n\nGender differences in the use of external landmarks versus spatial representations updated by self-motion.\n\nPubMed\n\nLambrey, Simon; Berthoz, Alain\n\n2007-09-01\n\nNumerous data in the literature provide evidence for gender differences in spatial orientation. In particular, it has been suggested that spatial representations of large-scale environments are more accurate in terms of metric information in men than in women but are richer in landmark information in women than in men. One explanatory hypothesis is that men and women differ in terms of navigational processes they used in daily life. The present study investigated this hypothesis by distinguishing two navigational processes: spatial updating by self-motion and landmark-based orientation. Subjects were asked to perform a pointing task in three experimental conditions, which differed in terms of reliability of the external landmarks that could be used. Two groups of subjects were distinguished, a mobile group and an immobile group, in which spatial updating of environmental locations did not have the same degree of importance for the correct performance of the pointing task. We found that men readily relied on an internal egocentric representation of where landmarks were expected to be in order to perform the pointing task, a representation that could be updated during self-motion (spatial updating). In contrast, women seemed to take their bearings more readily on the basis of the stable landmarks of the external world. We suggest that this gender difference in spatial orientation is not due to differences in information processing abilities but rather due to the differences in higher level strategies.\n\nSqueezing, Striking, and Vocalizing: Is Number Representation Fundamentally Spatial?\n\nERIC Educational Resources Information Center\n\nNunez, Rafael; Doan, D.; Nikoulina, Anastasia\n\n2011-01-01\n\nNumbers are fundamental entities in mathematics, but their cognitive bases are unclear. Abundant research points to linear space as a natural grounding for number representation. But, is number representation fundamentally spatial? We disentangle number representation from standard number-to-line reporting methods, and compare numericalâ¦\n\nSpatial representation of pitch height: the SMARC effect.\n\nPubMed\n\nRusconi, Elena; Kwan, Bonnie; Giordano, Bruno L; UmiltÃ , Carlo; Butterworth, Brian\n\n2006-03-01\n\nThrough the preferential pairing of response positions to pitch, here we show that the internal representation of pitch height is spatial in nature and affects performance, especially in musically trained participants, when response alternatives are either vertically or horizontally aligned. The finding that our cognitive system maps pitch height onto an internal representation of space, which in turn affects motor performance even when this perceptual attribute is irrelevant to the task, extends previous studies on auditory perception and suggests an interesting analogy between music perception and mathematical cognition. Both the basic elements of mathematical cognition (i.e. numbers) and the basic elements of musical cognition (i.e. pitches), appear to be mapped onto a mental spatial representation in a way that affects motor performance.\n\n3D hierarchical spatial representation and memory of multimodal sensory data\n\nNASA Astrophysics Data System (ADS)\n\nKhosla, Deepak; Dow, Paul A.; Huber, David J.\n\n2009-04-01\n\nThis paper describes an efficient method and system for representing, processing and understanding multi-modal sensory data. More specifically, it describes a computational method and system for how to process and remember multiple locations in multimodal sensory space (e.g., visual, auditory, somatosensory, etc.). The multimodal representation and memory is based on a biologically-inspired hierarchy of spatial representations implemented with novel analogues of real representations used in the human brain. The novelty of the work is in the computationally efficient and robust spatial representation of 3D locations in multimodal sensory space as well as an associated working memory for storage and recall of these representations at the desired level for goal-oriented action. We describe (1) A simple and efficient method for human-like hierarchical spatial representations of sensory data and how to associate, integrate and convert between these representations (head-centered coordinate system, body-centered coordinate, etc.); (2) a robust method for training and learning a mapping of points in multimodal sensory space (e.g., camera-visible object positions, location of auditory sources, etc.) to the above hierarchical spatial representations; and (3) a specification and implementation of a hierarchical spatial working memory based on the above for storage and recall at the desired level for goal-oriented action(s). This work is most useful for any machine or human-machine application that requires processing of multimodal sensory inputs, making sense of it from a spatial perspective (e.g., where is the sensory information coming from with respect to the machine and its parts) and then taking some goal-oriented action based on this spatial understanding. A multi-level spatial representation hierarchy means that heterogeneous sensory inputs (e.g., visual, auditory, somatosensory, etc.) can map onto the hierarchy at different levels. When controlling various machine\n\nSpatially variant morphological restoration and skeleton representation.\n\nPubMed\n\nBouaynaya, Nidhal; Charif-Chefchaouni, Mohammed; Schonfeld, Dan\n\n2006-11-01\n\nThe theory of spatially variant (SV) mathematical morphology is used to extend and analyze two important image processing applications: morphological image restoration and skeleton representation of binary images. For morphological image restoration, we propose the SV alternating sequential filters and SV median filters. We establish the relation of SV median filters to the basic SV morphological operators (i.e., SV erosions and SV dilations). For skeleton representation, we present a general framework for the SV morphological skeleton representation of binary images. We study the properties of the SV morphological skeleton representation and derive conditions for its invertibility. We also develop an algorithm for the implementation of the SV morphological skeleton representation of binary images. The latter algorithm is based on the optimal construction of the SV structuring element mapping designed to minimize the cardinality of the SV morphological skeleton representation. Experimental results show the dramatic improvement in the performance of the SV morphological restoration and SV morphological skeleton representation algorithms in comparison to their translation-invariant counterparts.\n\nAccurate metacognition for visual sensory memory representations.\n\nPubMed\n\nVandenbroucke, Annelinde R E; Sligte, Ilja G; Barrett, Adam B; Seth, Anil K; Fahrenfort, Johannes J; Lamme, Victor A F\n\n2014-04-01\n\nThe capacity to attend to multiple objects in the visual field is limited. However, introspectively, people feel that they see the whole visual world at once. Some scholars suggest that this introspective feeling is based on short-lived sensory memory representations, whereas others argue that the feeling of seeing more than can be attended to is illusory. Here, we investigated this phenomenon by combining objective memory performance with subjective confidence ratings during a change-detection task. This allowed us to compute a measure of metacognition--the degree of knowledge that subjects have about the correctness of their decisions--for different stages of memory. We show that subjects store more objects in sensory memory than they can attend to but, at the same time, have similar metacognition for sensory memory and working memory representations. This suggests that these subjective impressions are not an illusion but accurate reflections of the richness of visual perception.\n\nImage Quality Assessment Using the Joint Spatial/Spatial-Frequency Representation\n\nNASA Astrophysics Data System (ADS)\n\nBeghdadi, Azeddine; Iordache, RÄzvan\n\n2006-12-01\n\nThis paper demonstrates the usefulness of spatial/spatial-frequency representations in image quality assessment by introducing a new image dissimilarity measure based on 2D Wigner-Ville distribution (WVD). The properties of 2D WVD are shortly reviewed, and the important issue of choosing the analytic image is emphasized. The WVD-based measure is shown to be correlated with subjective human evaluation, which is the premise towards an image quality assessor developed on this principle.\n\nCommon Neural Representations for Visually Guided Reorientation and Spatial Imagery\n\nPubMed Central\n\nVass, Lindsay K.; Epstein, Russell A.\n\n2017-01-01\n\nAbstract Spatial knowledge about an environment can be cued from memory by perception of a visual scene during active navigation or by imagination of the relationships between nonvisible landmarks, such as when providing directions. It is not known whether these different ways of accessing spatial knowledge elicit the same representations in the brain. To address this issue, we scanned participants with fMRI, while they performed a judgment of relative direction (JRD) task that required them to retrieve real-world spatial relationships in response to either pictorial or verbal cues. Multivoxel pattern analyses revealed several brain regions that exhibited representations that were independent of the cues to access spatial memory. Specifically, entorhinal cortex in the medial temporal lobe and the retrosplenial complex (RSC) in the medial parietal lobe coded for the heading assumed on a particular trial, whereas the parahippocampal place area (PPA) contained information about the starting location of the JRD. These results demonstrate the existence of spatial representations in RSC, ERC, and PPA that are common to visually guided navigation and spatial imagery. PMID:26759482\n\nExperience-Dependency of Reliance on Local Visual and Idiothetic Cues for Spatial Representations Created in the Absence of Distal Information.\n\nPubMed\n\nDraht, Fabian; Zhang, Sijie; Rayan, Abdelrahman; SchÃ¶nfeld, Fabian; Wiskott, Laurenz; Manahan-Vaughan, Denise\n\n2017-01-01\n\nSpatial encoding in the hippocampus is based on a range of different input sources. To generate spatial representations, reliable sensory cues from the external environment are integrated with idiothetic cues, derived from self-movement, that enable path integration and directional perception. In this study, we examined to what extent idiothetic cues significantly contribute to spatial representations and navigation: we recorded place cells while rodents navigated towards two visually identical chambers in 180Â° orientation via two different paths in darkness and in the absence of reliable auditory or olfactory cues. Our goal was to generate a conflict between local visual and direction-specific information, and then to assess which strategy was prioritized in different learning phases. We observed that, in the absence of distal cues, place fields are initially controlled by local visual cues that override idiothetic cues, but that with multiple exposures to the paradigm, spaced at intervals of days, idiothetic cues become increasingly implemented in generating an accurate spatial representation. Taken together, these data support that, in the absence of distal cues, local visual cues are prioritized in the generation of context-specific spatial representations through place cells, whereby idiothetic cues are deemed unreliable. With cumulative exposures to the environments, the animal learns to attend to subtle idiothetic cues to resolve the conflict between visual and direction-specific information.\n\nExperience-Dependency of Reliance on Local Visual and Idiothetic Cues for Spatial Representations Created in the Absence of Distal Information\n\nPubMed Central\n\nDraht, Fabian; Zhang, Sijie; Rayan, Abdelrahman; SchÃ¶nfeld, Fabian; Wiskott, Laurenz; Manahan-Vaughan, Denise\n\n2017-01-01\n\nSpatial encoding in the hippocampus is based on a range of different input sources. To generate spatial representations, reliable sensory cues from the external environment are integrated with idiothetic cues, derived from self-movement, that enable path integration and directional perception. In this study, we examined to what extent idiothetic cues significantly contribute to spatial representations and navigation: we recorded place cells while rodents navigated towards two visually identical chambers in 180Â° orientation via two different paths in darkness and in the absence of reliable auditory or olfactory cues. Our goal was to generate a conflict between local visual and direction-specific information, and then to assess which strategy was prioritized in different learning phases. We observed that, in the absence of distal cues, place fields are initially controlled by local visual cues that override idiothetic cues, but that with multiple exposures to the paradigm, spaced at intervals of days, idiothetic cues become increasingly implemented in generating an accurate spatial representation. Taken together, these data support that, in the absence of distal cues, local visual cues are prioritized in the generation of context-specific spatial representations through place cells, whereby idiothetic cues are deemed unreliable. With cumulative exposures to the environments, the animal learns to attend to subtle idiothetic cues to resolve the conflict between visual and direction-specific information. PMID:28634444\n\nLanguage, Perception, and the Schematic Representation of Spatial Relations\n\nERIC Educational Resources Information Center\n\nAmorapanth, Prin; Kranjec, Alexander; Bromberger, Bianca; Lehet, Matthew; Widick, Page; Woods, Adam J.; Kimberg, Daniel Y.; Chatterjee, Anjan\n\n2012-01-01\n\nSchemas are abstract nonverbal representations that parsimoniously depict spatial relations. Despite their ubiquitous use in maps and diagrams, little is known about their neural instantiation. We sought to determine the extent to which schematic representations are neurally distinguished from language on the one hand, and from rich perceptualâ¦\n\nThe relation between body semantics and spatial body representations.\n\nPubMed\n\nvan Elk, Michiel; Blanke, Olaf\n\n2011-11-01\n\nThe present study addressed the relation between body semantics (i.e. semantic knowledge about the human body) and spatial body representations, by presenting participants with word pairs, one below the other, referring to body parts. The spatial position of the word pairs could be congruent (e.g. EYE / MOUTH) or incongruent (MOUTH / EYE) with respect to the spatial position of the words' referents. In addition, the spatial distance between the words' referents was varied, resulting in word pairs referring to body parts that are close (e.g. EYE / MOUTH) or far in space (e.g. EYE / FOOT). A spatial congruency effect was observed when subjects made an iconicity judgment (Experiments 2 and 3) but not when making a semantic relatedness judgment (Experiment 1). In addition, when making a semantic relatedness judgment (Experiment 1) reaction times increased with increased distance between the body parts but when making an iconicity judgment (Experiments 2 and 3) reaction times decreased with increased distance. These findings suggest that the processing of body-semantics results in the activation of a detailed visuo-spatial body representation that is modulated by the specific task requirements. We discuss these new data with respect to theories of embodied cognition and body semantics. Copyright Â© 2011 Elsevier B.V. All rights reserved.\n\nIntegrating spatially explicit representations of landscape perceptions into land change research\n\nUSGS Publications Warehouse\n\nDorning, Monica; Van Berkel, Derek B.; Semmens, Darius J.\n\n2017-01-01\n\nPurpose of ReviewHuman perceptions of the landscape can influence land-use and land-management decisions. Recognizing the diversity of landscape perceptions across space and time is essential to understanding land change processes and emergent landscape patterns. We summarize the role of landscape perceptions in the land change process, demonstrate advances in quantifying and mapping landscape perceptions, and describe how these spatially explicit techniques have and may benefit land change research.Recent FindingsMapping landscape perceptions is becoming increasingly common, particularly in research focused on quantifying ecosystem services provision. Spatial representations of landscape perceptions, often measured in terms of landscape values and functions, provide an avenue for matching social and environmental data in land change studies. Integrating these data can provide new insights into land change processes, contribute to landscape planning strategies, and guide the design and implementation of land change models.SummaryChallenges remain in creating spatial representations of human perceptions. Maps must be accompanied by descriptions of whose perceptions are being represented and the validity and uncertainty of those representations across space. With these considerations, rapid advancements in mapping landscape perceptions hold great promise for improving representation of human dimensions in landscape ecology and land change research.\n\nEffects of Spatial Cueing on Representational Momentum\n\nERIC Educational Resources Information Center\n\nHubbard, Timothy L.; Kumar, Anuradha Mohan; Carp, Charlotte L.\n\n2009-01-01\n\nEffects of a spatial cue on representational momentum were examined. If a cue was present during or after target motion and indicated the location at which the target would vanish or had vanished, forward displacement of that target decreased. The decrease in forward displacement was larger when cues were present after target motion than when cuesâ¦\n\nLinguistic and Perceptual Mapping in Spatial Representations: An Attentional Account.\n\nPubMed\n\nValdÃ©s-Conroy, Berenice; Hinojosa, JosÃ© A; RomÃ¡n, Francisco J; Romero-Ferreiro, VerÃ³nica\n\n2018-03-01\n\nBuilding on evidence for embodied representations, we investigated whether Spanish spatial terms map onto the NEAR/FAR perceptual division of space. Using a long horizontal display, we measured congruency effects during the processing of spatial terms presented in NEAR or FAR space. Across three experiments, we manipulated the task demands in order to investigate the role of endogenous attention in linguistic and perceptual space mapping. We predicted congruency effects only when spatial properties were relevant for the task (reaching estimation task, Experiment 1) but not when attention was allocated to other features (lexical decision, Experiment 2; and color, Experiment 3). Results showed faster responses for words presented in Near-space in all experiments. Consistent with our hypothesis, congruency effects were observed only when a reaching estimate was requested. Our results add important evidence for the role of top-down processing in congruency effects from embodied representations of spatial terms. Copyright Â© 2017 Cognitive Science Society, Inc.\n\nThinking Egyptian: Active Models for Understanding Spatial Representation.\n\nERIC Educational Resources Information Center\n\nSchiferl, Ellen\n\nThis paper highlights how introductory textbooks on Egyptian art inhibit understanding by reinforcing student preconceptions, and demonstrates another approach to discussing space with a classroom exercise and software. The alternative approach, an active model for spatial representation, introduced here was developed by adapting classroomâ¦\n\nBehavioral and Neural Representations of Spatial Directions across Words, Schemas, and Images.\n\nPubMed\n\nWeisberg, Steven M; Marchette, Steven A; Chatterjee, Anjan\n\n2018-05-23\n\nModern spatial navigation requires fluency with multiple representational formats, including visual scenes, signs, and words. These formats convey different information. Visual scenes are rich and specific but contain extraneous details. Arrows, as an example of signs, are schematic representations in which the extraneous details are eliminated, but analog spatial properties are preserved. Words eliminate all spatial information and convey spatial directions in a purely abstract form. How does the human brain compute spatial directions within and across these formats? To investigate this question, we conducted two experiments on men and women: a behavioral study that was preregistered and a neuroimaging study using multivoxel pattern analysis of fMRI data to uncover similarities and differences among representational formats. Participants in the behavioral study viewed spatial directions presented as images, schemas, or words (e.g., \"left\"), and responded to each trial, indicating whether the spatial direction was the same or different as the one viewed previously. They responded more quickly to schemas and words than images, despite the visual complexity of stimuli being matched. Participants in the fMRI study performed the same task but responded only to occasional catch trials. Spatial directions in images were decodable in the intraparietal sulcus bilaterally but were not in schemas and words. Spatial directions were also decodable between all three formats. These results suggest that intraparietal sulcus plays a role in calculating spatial directions in visual scenes, but this neural circuitry may be bypassed when the spatial directions are presented as schemas or words. SIGNIFICANCE STATEMENT Human navigators encounter spatial directions in various formats: words (\"turn left\"), schematic signs (an arrow showing a left turn), and visual scenes (a road turning left). The brain must transform these spatial directions into a plan for action. Here, we investigate\n\nSpatial Representation of Pitch Height: The SMARC Effect\n\nERIC Educational Resources Information Center\n\nRusconi, Elena; Kwan, Bonnie; Giordano, Bruno L.; Umilta, Carlo; Butterworth, Brian\n\n2006-01-01\n\nThrough the preferential pairing of response positions to pitch, here we show that the internal representation of pitch height is spatial in nature and affects performance, especially in musically trained participants, when response alternatives are either vertically or horizontally aligned. The finding that our cognitive system maps pitch heightâ¦\n\nThink spatial: the representation in mental rotation is nonvisual.\n\nPubMed\n\nLiesefeld, Heinrich R; Zimmer, Hubert D\n\n2013-01-01\n\nFor mental rotation, introspection, theories, and interpretations of experimental results imply a certain type of mental representation, namely, visual mental images. Characteristics of the rotated representation can be examined by measuring the influence of stimulus characteristics on rotational speed. If the amount of a given type of information influences rotational speed, one can infer that it was contained in the rotated representation. In Experiment 1, rotational speed of university students (10 men, 11 women) was found to be influenced exclusively by the amount of represented orientation-dependent spatial-relational information but not by orientation-independent spatial-relational information, visual complexity, or the number of stimulus parts. As information in mental-rotation tasks is initially presented visually, this finding implies that at some point during each trial, orientation-dependent information is extracted from visual information. Searching for more direct evidence for this extraction, we recorded the EEG of another sample of university students (12 men, 12 women) during mental rotation of the same stimuli. In an early time window, the observed working memory load-dependent slow potentials were sensitive to the stimuli's visual complexity. Later, in contrast, slow potentials were sensitive to the amount of orientation-dependent information only. We conclude that only orientation-dependent information is contained in the rotated representation. (PsycINFO Database Record (c) 2013 APA, all rights reserved).\n\nDynamic Circuitry for Updating Spatial Representations: III. From Neurons to Behavior\n\nPubMed Central\n\nBerman, Rebecca A.; Heiser, Laura M.; Dunn, Catherine A.; Saunders, Richard C.; Colby, Carol L.\n\n2008-01-01\n\nEach time the eyes move, the visual system must adjust internal representations to account for the accompanying shift in the retinal image. In the lateral intraparietal cortex (LIP), neurons update the spatial representations of salient stimuli when the eyes move. In previous experiments, we found that split-brain monkeys were impaired on double-step saccade sequences that required updating across visual hemifields, as compared to within hemifield (Berman et al. 2005; Heiser et al. 2005). Here we describe a subsequent experiment to characterize the relationship between behavioral performance and neural activity in LIP in the split-brain monkey. We recorded from single LIP neurons while split-brain and intact monkeys performed two conditions of the double-step saccade task: one required across-hemifield updating and the other within-hemifield updating. We found that, despite extensive experience with the task, the split-brain monkeys were significantly more accurate for within-hemifield as compared to across-hemifield sequences. In parallel, we found that population activity in LIP of the split-brain monkeys was significantly stronger for within-hemifield as compared to across-hemifield conditions of the double-step task. In contrast, in the normal monkey, both the average behavioral performance and population activity showed no bias toward the within-hemifield condition. Finally, we found that the difference between within-hemifield and across-hemifield performance in the split-brain monkeys was reflected at the level of single neuron activity in LIP. These findings indicate that remapping activity in area LIP is present in the split-brain monkey for the double-step task and co-varies with spatial behavior on within-hemifield compared to across-hemifield sequences. PMID:17493922\n\nA study of kindergarten children's spatial representation in a mapping project\n\nNASA Astrophysics Data System (ADS)\n\nDavis, Genevieve A.; Hyun, Eunsook\n\n2005-02-01\n\nThis phenomenological study examined kindergarten children's development of spatial representation in a year long mapping project. Findings and discussion relative to how children conceptualised and represented physical space are presented in light of theoretical notions advanced by Piaget, van Hiele, and cognitive science researchers Battista and Clements. Analyses of the processes the children used and their finished products indicate that children can negotiate meaning for complex systems of geometric concepts when given opportunities to debate, negotiate, reflect, evaluate and seek meaning for representing space. The complexity and \"holistic\" nature of spatial representation of young children emerged in this study.\n\nAn Accurate Projector Calibration Method Based on Polynomial Distortion Representation\n\nPubMed Central\n\nLiu, Miao; Sun, Changku; Huang, Shujun; Zhang, Zonghua\n\n2015-01-01\n\nIn structure light measurement systems or 3D printing systems, the errors caused by optical distortion of a digital projector always affect the precision performance and cannot be ignored. Existing methods to calibrate the projection distortion rely on calibration plate and photogrammetry, so the calibration performance is largely affected by the quality of the plate and the imaging system. This paper proposes a new projector calibration approach that makes use of photodiodes to directly detect the light emitted from a digital projector. By analyzing the output sequence of the photoelectric module, the pixel coordinates can be accurately obtained by the curve fitting method. A polynomial distortion representation is employed to reduce the residuals of the traditional distortion representation model. Experimental results and performance evaluation show that the proposed calibration method is able to avoid most of the disadvantages in traditional methods and achieves a higher accuracy. This proposed method is also practically applicable to evaluate the geometric optical performance of other optical projection system. PMID:26492247\n\nSpatial representations elicit dual-coding effects in mental imagery.\n\nPubMed\n\nVerges, Michelle; Duffy, Sean\n\n2009-08-01\n\nSpatial aspects of words are associated with their canonical locations in the real world. Yet little research has tested whether spatial associations denoted in language comprehension generalize to their corresponding images. We directly tested the spatial aspects of mental imagery in picture and word processing (Experiment 1). We also tested whether spatial representations of motion words produce similar perceptual-interference effects as demonstrated by object words (Experiment 2). Findings revealed that words denoting an upward spatial location produced slower responses to targets appearing at the top of the display, whereas words denoting a downward spatial location produced slower responses to targets appearing at the bottom of the display. Perceptual-interference effects did not obtain for pictures or for words lacking a spatial relation. These findings provide greater empirical support for the perceptual-symbols system theory (Barsalou, 1999, 2008). Copyright Â© 2009 Cognitive Science Society, Inc.\n\nDiagrammatic Representational Constraints of Spatial Scale in Earth-Moon System Astronomy Instruction\n\nERIC Educational Resources Information Center\n\nTaylor, Roger S.; Grundstrom, Erika D.\n\n2011-01-01\n\nGiven that astronomy heavily relies on visual representations it is especially likely for individuals to assume that instructional materials, such as visual representations of the Earth-Moon system (EMS), would be relatively accurate. However, in our research, we found that images in middle-school textbooks and educational webpages were commonlyâ¦\n\nSpatial, Temporal and Spectral Satellite Image Fusion via Sparse Representation\n\nNASA Astrophysics Data System (ADS)\n\nSong, Huihui\n\nRemote sensing provides good measurements for monitoring and further analyzing the climate change, dynamics of ecosystem, and human activities in global or regional scales. Over the past two decades, the number of launched satellite sensors has been increasing with the development of aerospace technologies and the growing requirements on remote sensing data in a vast amount of application fields. However, a key technological challenge confronting these sensors is that they tradeoff between spatial resolution and other properties, including temporal resolution, spectral resolution, swath width, etc., due to the limitations of hardware technology and budget constraints. To increase the spatial resolution of data with other good properties, one possible cost-effective solution is to explore data integration methods that can fuse multi-resolution data from multiple sensors, thereby enhancing the application capabilities of available remote sensing data. In this thesis, we propose to fuse the spatial resolution with temporal resolution and spectral resolution, respectively, based on sparse representation theory. Taking the study case of Landsat ETM+ (with spatial resolution of 30m and temporal resolution of 16 days) and MODIS (with spatial resolution of 250m ~ 1km and daily temporal resolution) reflectance, we propose two spatial-temporal fusion methods to combine the fine spatial information of Landsat image and the daily temporal resolution of MODIS image. Motivated by that the images from these two sensors are comparable on corresponding bands, we propose to link their spatial information on available Landsat- MODIS image pair (captured on prior date) and then predict the Landsat image from the MODIS counterpart on prediction date. To well-learn the spatial details from the prior images, we use a redundant dictionary to extract the basic representation atoms for both Landsat and MODIS images based on sparse representation. Under the scenario of two prior Landsat\n\nDifferent brains process numbers differently: structural bases of individual differences in spatial and nonspatial number representations.\n\nPubMed\n\nKrause, Florian; Lindemann, Oliver; Toni, Ivan; Bekkering, Harold\n\n2014-04-01\n\nA dominant hypothesis on how the brain processes numerical size proposes a spatial representation of numbers as positions on a \"mental number line.\" An alternative hypothesis considers numbers as elements of a generalized representation of sensorimotor-related magnitude, which is not obligatorily spatial. Here we show that individuals' relative use of spatial and nonspatial representations has a cerebral counterpart in the structural organization of the posterior parietal cortex. Interindividual variability in the linkage between numbers and spatial responses (faster left responses to small numbers and right responses to large numbers; spatial-numerical association of response codes effect) correlated with variations in gray matter volume around the right precuneus. Conversely, differences in the disposition to link numbers to force production (faster soft responses to small numbers and hard responses to large numbers) were related to gray matter volume in the left angular gyrus. This finding suggests that numerical cognition relies on multiple mental representations of analogue magnitude using different neural implementations that are linked to individual traits.\n\nSpatial frequency supports the emergence of categorical representations in visual cortex during natural scene perception.\n\nPubMed\n\nDima, Diana C; Perry, Gavin; Singh, Krish D\n\n2018-06-11\n\nIn navigating our environment, we rapidly process and extract meaning from visual cues. However, the relationship between visual features and categorical representations in natural scene perception is still not well understood. Here, we used natural scene stimuli from different categories and filtered at different spatial frequencies to address this question in a passive viewing paradigm. Using representational similarity analysis (RSA) and cross-decoding of magnetoencephalography (MEG) data, we show that categorical representations emerge in human visual cortex at â¼180â¯ms and are linked to spatial frequency processing. Furthermore, dorsal and ventral stream areas reveal temporally and spatially overlapping representations of low and high-level layer activations extracted from a feedforward neural network. Our results suggest that neural patterns from extrastriate visual cortex switch from low-level to categorical representations within 200â¯ms, highlighting the rapid cascade of processing stages essential in human visual perception. Copyright Â© 2018 The Authors. Published by Elsevier Inc. All rights reserved.\n\nVisual-Spatial Attention Aids the Maintenance of Object Representations in Visual Working Memory\n\nPubMed Central\n\nWilliams, Melonie; Pouget, Pierre; Boucher, Leanne; Woodman, Geoffrey F.\n\n2013-01-01\n\nTheories have proposed that the maintenance of object representations in visual working memory is aided by a spatial rehearsal mechanism. In this study, we used two different approaches to test the hypothesis that overt and covert visual-spatial attention mechanisms contribute to the maintenance of object representations in visual working memory. First, we tracked observersâ eye movements while remembering a variable number of objects during change-detection tasks. We observed that during the blank retention interval, participants spontaneously shifted gaze to the locations that the objects had occupied in the memory array. Next, we hypothesized that if attention mechanisms contribute to the maintenance of object representations, then drawing attention away from the object locations during the retention interval would impair object memory during these change-detection tasks. Supporting this prediction, we found that attending to the fixation point in anticipation of a brief probe stimulus during the retention interval reduced change-detection accuracy even on the trials in which no probe occurred. These findings support models of working memory in which visual-spatial selection mechanisms contribute to the maintenance of object representations. PMID:23371773\n\nA Simple Iterative Model Accurately Captures Complex Trapline Formation by Bumblebees Across Spatial Scales and Flower Arrangements\n\nPubMed Central\n\nReynolds, Andrew M.; Lihoreau, Mathieu; Chittka, Lars\n\n2013-01-01\n\nPollinating bees develop foraging circuits (traplines) to visit multiple flowers in a manner that minimizes overall travel distance, a task analogous to the travelling salesman problem. We report on an in-depth exploration of an iterative improvement heuristic model of bumblebee traplining previously found to accurately replicate the establishment of stable routes by bees between flowers distributed over several hectares. The critical test for a model is its predictive power for empirical data for which the model has not been specifically developed, and here the model is shown to be consistent with observations from different research groups made at several spatial scales and using multiple configurations of flowers. We refine the model to account for the spatial search strategy of bees exploring their environment, and test several previously unexplored predictions. We find that the model predicts accurately 1) the increasing propensity of bees to optimize their foraging routes with increasing spatial scale; 2) that bees cannot establish stable optimal traplines for all spatial configurations of rewarding flowers; 3) the observed trade-off between travel distance and prioritization of high-reward sites (with a slight modification of the model); 4) the temporal pattern with which bees acquire approximate solutions to travelling salesman-like problems over several dozen foraging bouts; 5) the instability of visitation schedules in some spatial configurations of flowers; 6) the observation that in some flower arrays, bees' visitation schedules are highly individually different; 7) the searching behaviour that leads to efficient location of flowers and routes between them. Our model constitutes a robust theoretical platform to generate novel hypotheses and refine our understanding about how small-brained insects develop a representation of space and use it to navigate in complex and dynamic environments. PMID:23505353\n\nThe spatial representation of power in children.\n\nPubMed\n\nLu, Lifeng; Schubert, Thomas W; Zhu, Lei\n\n2017-11-01\n\nPrevious evidence demonstrates that power is mentally represented as vertical space by adults. However, little is known about how power is mentally represented in children. The current research examines such representations. The influence of vertical information (motor cues) was tested in both an explicit power evaluation task (judge whether labels refer to powerless or powerful groups) and an incidental task (judge whether labels refer to people or animals). The results showed that when power was explicitly evaluated, vertical motor responses interfered with responding in children and adults, i.e., they responded to words representing powerful groups faster with the up than the down cursor key (and vice versa for powerless groups). However, this interference effect disappeared in the incidental task in children. The findings suggest that children have developed a spatial representation of power before they have been taught power-space associations formally, but that they do not judge power spontaneously.\n\nAuditory Spatial Attention Representations in the Human Cerebral Cortex\n\nPubMed Central\n\nKong, Lingqiang; Michalka, Samantha W.; Rosen, Maya L.; Sheremata, Summer L.; Swisher, Jascha D.; Shinn-Cunningham, Barbara G.; Somers, David C.\n\n2014-01-01\n\nAuditory spatial attention serves important functions in auditory source separation and selection. Although auditory spatial attention mechanisms have been generally investigated, the neural substrates encoding spatial information acted on by attention have not been identified in the human neocortex. We performed functional magnetic resonance imaging experiments to identify cortical regions that support auditory spatial attention and to test 2 hypotheses regarding the coding of auditory spatial attention: 1) auditory spatial attention might recruit the visuospatial maps of the intraparietal sulcus (IPS) to create multimodal spatial attention maps; 2) auditory spatial information might be encoded without explicit cortical maps. We mapped visuotopic IPS regions in individual subjects and measured auditory spatial attention effects within these regions of interest. Contrary to the multimodal map hypothesis, we observed that auditory spatial attentional modulations spared the visuotopic maps of IPS; the parietal regions activated by auditory attention lacked map structure. However, multivoxel pattern analysis revealed that the superior temporal gyrus and the supramarginal gyrus contained significant information about the direction of spatial attention. These findings support the hypothesis that auditory spatial information is coded without a cortical map representation. Our findings suggest that audiospatial and visuospatial attention utilize distinctly different spatial coding schemes. PMID:23180753\n\nFast and accurate grid representations for atom-based docking with partner flexibility.\n\nPubMed\n\nde Vries, Sjoerd J; Zacharias, Martin\n\n2017-06-30\n\nMacromolecular docking methods can broadly be divided into geometric and atom-based methods. Geometric methods use fast algorithms that operate on simplified, grid-like molecular representations, while atom-based methods are more realistic and flexible, but far less efficient. Here, a hybrid approach of grid-based and atom-based docking is presented, combining precalculated grid potentials with neighbor lists for fast and accurate calculation of atom-based intermolecular energies and forces. The grid representation is compatible with simultaneous multibody docking and can tolerate considerable protein flexibility. When implemented in our docking method ATTRACT, grid-based docking was found to be â¼35x faster. With the OPLSX forcefield instead of the ATTRACT coarse-grained forcefield, the average speed improvement was >100x. Grid-based representations may allow atom-based docking methods to explore large conformational spaces with many degrees of freedom, such as multiple macromolecules including flexibility. This increases the domain of biological problems to which docking methods can be applied. Â© 2017 Wiley Periodicals, Inc. Â© 2017 Wiley Periodicals, Inc.\n\nChildren's Spatial Representations: 3- and 4-Year-Olds are Affected by Irrelevant Peripheral References.\n\nPubMed\n\nKrÃ¼ger, Markus; Jahn, Georg\n\n2015-01-01\n\nChildren as young as 3 years can remember an object's location within an arrangement and can retrieve it from a novel viewpoint (Nardini et al., 2006). However, this ability is impaired if the arrangement is rotated to compensate for the novel viewpoint, or, if the arrangement is rotated and children stand still. There are two dominant explanations for this phenomenon: self-motion induces an automatic spatial updating process which is beneficial if children move around the arrangement, but misleading if the children's movement is matched by the arrangement and not activated if children stand still and only the arrangement is moved (see spatial updating; Simons and Wang, 1998). Another explanation concerns reference frames: spatial representations might depend on peripheral spatial relations concerning the surrounding room instead on proximal relations within the arrangement, even if these proximal relations are sufficient or more informative. To evaluate these possibilities, we rotated children (N = 120) aged between 3 and 6 years with an occluded arrangement. When the arrangement was in misalignment to the surrounding room, 3- and 4-year-olds' spatial memory was impaired and 5-year-olds' was lightly impaired suggesting that they relied on peripheral references of the surrounding room for retrieval. In contrast, 6-years-olds' spatial representation seemed robust against misalignment indicating a successful integration of spatial representations.\n\nUnbounding the mental number lineânew evidence on children's spatial representation of numbers\n\nPubMed Central\n\nLink, Tanja; Huber, Stefan; Nuerk, Hans-Christoph; Moeller, Korbinian\n\n2014-01-01\n\nNumber line estimation (i.e., indicating the position of a given number on a physical line) is a standard assessment of children's spatial representation of number magnitude. Importantly, there is an ongoing debate on the question in how far the bounded task version with start and endpoint given (e.g., 0 and 100) might induce specific estimation strategies and thus may not allow for unbiased inferences on the underlying representation. Recently, a new unbounded version of the task was suggested with only the start point and a unit fixed (e.g., the distance from 0 to 1). In adults this task provided a less biased index of the spatial representation of number magnitude. Yet, so far there are no children data available for the unbounded number line estimation task. Therefore, we conducted a cross-sectional study on primary school children performing both, the bounded and the unbounded version of the task. We observed clear evidence for systematic strategic influences (i.e., the consideration of reference points) in the bounded number line estimation task for children older than grade two whereas there were no such indications for the unbounded version for any one of the age groups. In summary, the current data corroborate the unbounded number line estimation task to be a valuable tool for assessing children's spatial representation of number magnitude in a systematic and unbiased manner. Yet, similar results for the bounded and the unbounded version of the task for first- and second-graders may indicate that both versions of the task might assess the same underlying representation for relatively younger childrenâat least in number ranges familiar to the children assessed. This is of particular importance for inferences about the nature and development of children's magnitude representation. PMID:24478734\n\nThe Impact of Conflicting Spatial Representations in Airborne Unmanned Aerial System Sensor Control\n\nDTIC Science & Technology\n\n2016-02-01\n\nSpatial Discordance 1 Running head: SPATIAL DISCORDANCE IN AIRBORNE UAS OPERATIONS The impact of conflicting spatial...representations in airborne unmanned aerial system sensor control Joseph W Geeseman, James E Patrey, Caroline Davy, Katherine Peditto, & Christine Zernickow...system (UAS) simulation while riding in the fuselage of an airborne Lockheed P-3 Orion. The P-3 flew a flight profile of intermittent ascending\n\nSpatial Representation in Blind Children. 3: Effects of Individual Differences.\n\nERIC Educational Resources Information Center\n\nFletcher, Janet F.\n\n1981-01-01\n\nData from a study of spatial representation in blind children were subjected to two stepwise regression analyses to determine the relationships between several subject related variables and responses to \"map\" (cognitive map) and \"route\" (sequential memory) questions about the position of furniture in a recently explored room. (Author/SBH)\n\nChildrenâs Spatial Representations: 3- and 4-Year-Olds are Affected by Irrelevant Peripheral References\n\nPubMed Central\n\nKrÃ¼ger, Markus; Jahn, Georg\n\n2015-01-01\n\nChildren as young as 3 years can remember an objectâs location within an arrangement and can retrieve it from a novel viewpoint (Nardini et al., 2006). However, this ability is impaired if the arrangement is rotated to compensate for the novel viewpoint, or, if the arrangement is rotated and children stand still. There are two dominant explanations for this phenomenon: self-motion induces an automatic spatial updating process which is beneficial if children move around the arrangement, but misleading if the childrenâs movement is matched by the arrangement and not activated if children stand still and only the arrangement is moved (see spatial updating; Simons and Wang, 1998). Another explanation concerns reference frames: spatial representations might depend on peripheral spatial relations concerning the surrounding room instead on proximal relations within the arrangement, even if these proximal relations are sufficient or more informative. To evaluate these possibilities, we rotated children (N = 120) aged between 3 and 6 years with an occluded arrangement. When the arrangement was in misalignment to the surrounding room, 3- and 4-year-oldsâ spatial memory was impaired and 5-year-oldsâ was lightly impaired suggesting that they relied on peripheral references of the surrounding room for retrieval. In contrast, 6-years-oldsâ spatial representation seemed robust against misalignment indicating a successful integration of spatial representations. PMID:26617537\n\nA tesselated probabilistic representation for spatial robot perception and navigation\n\nNASA Technical Reports Server (NTRS)\n\nElfes, Alberto\n\n1989-01-01\n\nThe ability to recover robust spatial descriptions from sensory information and to efficiently utilize these descriptions in appropriate planning and problem-solving activities are crucial requirements for the development of more powerful robotic systems. Traditional approaches to sensor interpretation, with their emphasis on geometric models, are of limited use for autonomous mobile robots operating in and exploring unknown and unstructured environments. Here, researchers present a new approach to robot perception that addresses such scenarios using a probabilistic tesselated representation of spatial information called the Occupancy Grid. The Occupancy Grid is a multi-dimensional random field that maintains stochastic estimates of the occupancy state of each cell in the grid. The cell estimates are obtained by interpreting incoming range readings using probabilistic models that capture the uncertainty in the spatial information provided by the sensor. A Bayesian estimation procedure allows the incremental updating of the map using readings taken from several sensors over multiple points of view. An overview of the Occupancy Grid framework is given, and its application to a number of problems in mobile robot mapping and navigation are illustrated. It is argued that a number of robotic problem-solving activities can be performed directly on the Occupancy Grid representation. Some parallels are drawn between operations on Occupancy Grids and related image processing operations.\n\nExclusion of agricultural lands in spatial conservation prioritization strategies: consequences for biodiversity and ecosystem service representation\n\nPubMed Central\n\nDurÃ¡n, AmÃ©rica P.; Duffy, James P.; Gaston, Kevin J.\n\n2014-01-01\n\nAgroecosystems have traditionally been considered incompatible with biological conservation goals, and often been excluded from spatial conservation prioritization strategies. The consequences for the representativeness of identified priority areas have been little explored. Here, we evaluate these for biodiversity and carbon storage representation when agricultural land areas are excluded from a spatial prioritization strategy for South America. Comparing different prioritization approaches, we also assess how the spatial overlap of priority areas changes. The exclusion of agricultural lands was detrimental to biodiversity representation, indicating that priority areas for agricultural production overlap with areas of relatively high occurrence of species. By contrast, exclusion of agricultural lands benefits representation of carbon storage within priority areas, as lands of high value for agriculture and carbon storage overlap little. When agricultural lands were included and equally weighted with biodiversity and carbon storage, a balanced representation resulted. Our findings suggest that with appropriate management, South American agroecosystems can significantly contribute to biodiversity conservation. PMID:25143040\n\nSpatial Language Processing in the Blind: Evidence for a Supramodal Representation and Cortical Reorganization\n\nPubMed Central\n\nStruiksma, Marijn E.; Noordzij, Matthijs L.; Neggers, Sebastiaan F. W.; Bosker, Wendy M.; Postma, Albert\n\n2011-01-01\n\nNeuropsychological and imaging studies have shown that the left supramarginal gyrus (SMG) is specifically involved in processing spatial terms (e.g. above, left of), which locate places and objects in the world. The current fMRI study focused on the nature and specificity of representing spatial language in the left SMG by combining behavioral and neuronal activation data in blind and sighted individuals. Data from the blind provide an elegant way to test the supramodal representation hypothesis, i.e. abstract codes representing spatial relations yielding no activation differences between blind and sighted. Indeed, the left SMG was activated during spatial language processing in both blind and sighted individuals implying a supramodal representation of spatial and other dimensional relations which does not require visual experience to develop. However, in the absence of vision functional reorganization of the visual cortex is known to take place. An important consideration with respect to our finding is the amount of functional reorganization during language processing in our blind participants. Therefore, the participants also performed a verb generation task. We observed that only in the blind occipital areas were activated during covert language generation. Additionally, in the first task there was functional reorganization observed for processing language with a high linguistic load. As the visual cortex was not specifically active for spatial contents in the first task, and no reorganization was observed in the SMG, the latter finding further supports the notion that the left SMG is the main node for a supramodal representation of verbal spatial relations. PMID:21935391\n\nDaytime Sleep Enhances Consolidation of the Spatial but Not Motoric Representation of Motor Sequence Memory\n\nPubMed Central\n\nAlbouy, GeneviÃ¨ve; Fogel, Stuart; Pottiez, Hugo; Nguyen, Vo An; Ray, Laura; Lungu, Ovidiu; Carrier, Julie; Robertson, Edwin; Doyon, Julien\n\n2013-01-01\n\nMotor sequence learning is known to rely on more than a single process. As the skill develops with practice, two different representations of the sequence are formed: a goal representation built under spatial allocentric coordinates and a movement representation mediated through egocentric motor coordinates. This study aimed to explore the influence of daytime sleep (nap) on consolidation of these two representations. Through the manipulation of an explicit finger sequence learning task and a transfer protocol, we show that both allocentric (spatial) and egocentric (motor) representations of the sequence can be isolated after initial training. Our results also demonstrate that nap favors the emergence of offline gains in performance for the allocentric, but not the egocentric representation, even after accounting for fatigue effects. Furthermore, sleep-dependent gains in performance observed for the allocentric representation are correlated with spindle density during non-rapid eye movement (NREM) sleep of the post-training nap. In contrast, performance on the egocentric representation is only maintained, but not improved, regardless of the sleep/wake condition. These results suggest that motor sequence memory acquisition and consolidation involve distinct mechanisms that rely on sleep (and specifically, spindle) or simple passage of time, depending respectively on whether the sequence is performed under allocentric or egocentric coordinates. PMID:23300993\n\nExamination about the Spatial Representation of PM2.5 Obtained from Limited Stations Using a Network Observation\n\nNASA Astrophysics Data System (ADS)\n\nShi, X.; Zhao, C.\n\n2017-12-01\n\nHaze aerosol pollution has been a focus issue in China, and its characteristics is highly demanded. With limited observation sites, aerosol properties obtained from a single site is frequently used to represent the haze condition over a large domain, such as tens of kilometers. This could result in high uncertainties in the haze characteristics due to their spatial variation. Using a network observation from November 2015 to February 2016 over an urban city in North China with high spatial resolution, this study examines the spatial representation of ground site observations. A method is first developed to determine the representative area of measurements from limited stations. The key idea of this method is to determine the spatial variability of particulate matter with diameters less than 2.5 Î¼m (PM2.5) concentration using a variance function in 2km x 2km grids. Based on the high spatial resolution (0.5km x 0.5km) measurements of PM2.5, the grids in which PM2.5 have high correlations and weak value differences are determined as the representation area of measurements at these grids. Note that the size representation area is not exactly a circle region. It shows that the size representation are for the study region and study period ranges from 0.25 km2 to 16.25 km2. The representation area varies with locations. For the 20 km x 20 km study region, 10 station observations would have a good representation of the PM2.5 observations obtained from current 169 stations at the four-month time scale.\n\nAnalysis of studentsâ spatial thinking in geometry: 3D object into 2D representation\n\nNASA Astrophysics Data System (ADS)\n\nFiantika, F. R.; Maknun, C. L.; Budayasa, I. K.; Lukito, A.\n\n2018-05-01\n\nThe aim of this study is to find out the spatial thinking process of students in transforming 3-dimensional (3D) object to 2-dimensional (2D) representation. Spatial thinking is helpful in using maps, planning routes, designing floor plans, and creating art. The student can engage geometric ideas by using concrete models and drawing. Spatial thinking in this study is identified through geometrical problems of transforming a 3-dimensional object into a 2-dimensional object image. The problem was resolved by the subject and analyzed by reference to predetermined spatial thinking indicators. Two representative subjects of elementary school were chosen based on mathematical ability and visual learning style. Explorative description through qualitative approach was used in this study. The result of this study are: 1) there are different representations of spatial thinking between a boy and a girl object, 2) the subjects has their own way to invent the fastest way to draw cube net.\n\nSpatial representations in blind people: the role of strategies and mobility skills.\n\nPubMed\n\nSchmidt, Susanna; Tinti, Carla; Fantino, Micaela; Mammarella, Irene C; Cornoldi, Cesare\n\n2013-01-01\n\nThe role of vision in the construction of spatial representations has been the object of numerous studies and heated debate. The core question of whether visual experience is necessary to form spatial representations has found different, often contradictory answers. The present paper examines mental images generated from verbal descriptions of spatial environments. Previous evidence had shown that blind individuals have difficulty remembering information about spatial environments. By testing a group of congenitally blind people, we replicated this result and found that it is also present when the overall mental model of the environment is assessed. This was not always the case, however, but appeared to correlate with some blind participants' lower use of a mental imagery strategy and preference for a verbal rehearsal strategy, which was adopted particularly by blind people with more limited mobility skills. The more independent blind people who used a mental imagery strategy performed as well as sighted participants, suggesting that the difficulty blind people may have in processing spatial descriptions is not due to the absence of vision per se, but could be the consequence of both, their using less efficient verbal strategies and having poor mobility skills. Copyright Â© 2012 Elsevier B.V. All rights reserved.\n\nIn (or outside of) your neck of the woods: laterality in spatial body representation\n\nPubMed Central\n\nHach, Sylvia; SchÃ¼tz-Bosbach, Simone\n\n2014-01-01\n\nBeside language, space is to date the most widely recognized lateralized systems. For example, it has been shown that even mental representations of space and the spatial representation of abstract concepts display lateralized characteristics. For the most part, this body of literature describes space as distal or something outside of the observer or actor. What has been strangely absent in the literature on the whole and specifically in the spatial literature until recently is the most proximal space imaginable â the body. In this review, we will summarize three strands of literature showing laterality in body representations. First, evidence of hemispheric asymmetries in body space in health and, second in body space in disease will be examined. Third, studies pointing to differential contributions of the right and left hemisphere to illusory body (space) will be summarized. Together these studies show hemispheric asymmetries to be evident in body representations at the level of simple somatosensory and proprioceptive representations. We propose a novel working hypothesis, whereby neural systems dedicated to processing action-oriented information about oneâs own body space may ontogenetically serve as a template for the perception of the external world. PMID:24600421\n\nExclusion of agricultural lands in spatial conservation prioritization strategies: consequences for biodiversity and ecosystem service representation.\n\nPubMed\n\nDurÃ¡n, AmÃ©rica P; Duffy, James P; Gaston, Kevin J\n\n2014-10-07\n\nAgroecosystems have traditionally been considered incompatible with biological conservation goals, and often been excluded from spatial conservation prioritization strategies. The consequences for the representativeness of identified priority areas have been little explored. Here, we evaluate these for biodiversity and carbon storage representation when agricultural land areas are excluded from a spatial prioritization strategy for South America. Comparing different prioritization approaches, we also assess how the spatial overlap of priority areas changes. The exclusion of agricultural lands was detrimental to biodiversity representation, indicating that priority areas for agricultural production overlap with areas of relatively high occurrence of species. By contrast, exclusion of agricultural lands benefits representation of carbon storage within priority areas, as lands of high value for agriculture and carbon storage overlap little. When agricultural lands were included and equally weighted with biodiversity and carbon storage, a balanced representation resulted. Our findings suggest that with appropriate management, South American agroecosystems can significantly contribute to biodiversity conservation. Â© 2014 The Author(s) Published by the Royal Society. All rights reserved.\n\nDeployment of spatial attention towards locations in memory representations. An EEG study.\n\nPubMed\n\nLeszczyÅski, Marcin; Wykowska, Agnieszka; Perez-Osorio, Jairo; MÃ¼ller, Hermann J\n\n2013-01-01\n\nRecalling information from visual short-term memory (VSTM) involves the same neural mechanisms as attending to an actually perceived scene. In particular, retrieval from VSTM has been associated with orienting of visual attention towards a location within a spatially-organized memory representation. However, an open question concerns whether spatial attention is also recruited during VSTM retrieval even when performing the task does not require access to spatial coordinates of items in the memorized scene. The present study combined a visual search task with a modified, delayed central probe protocol, together with EEG analysis, to answer this question. We found a temporal contralateral negativity (TCN) elicited by a centrally presented go-signal which was spatially uninformative and featurally unrelated to the search target and informed participants only about a response key that they had to press to indicate a prepared target-present vs. -absent decision. This lateralization during VSTM retrieval (TCN) provides strong evidence of a shift of attention towards the target location in the memory representation, which occurred despite the fact that the present task required no spatial (or featural) information from the search to be encoded, maintained, and retrieved to produce the correct response and that the go-signal did not itself specify any information relating to the location and defining feature of the target.\n\nDNA methylation regulates neurophysiological spatial representation in memory formation\n\nPubMed Central\n\nRoth, Eric D.; Roth, Tania L.; Money, Kelli M.; SenGupta, Sonda; Eason, Dawn E.; Sweatt, J. David\n\n2015-01-01\n\nEpigenetic mechanisms including altered DNA methylation are critical for altered gene transcription subserving synaptic plasticity and the retention of learned behavior. Here we tested the idea that one role for activity-dependent altered DNA methylation is stabilization of cognition-associated hippocampal place cell firing in response to novel place learning. We observed that a behavioral protocol (spatial exploration of a novel environment) known to induce hippocampal place cell remapping resulted in alterations of hippocampal Bdnf DNA methylation. Further studies using neurophysiological in vivo single unit recordings revealed that pharmacological manipulations of DNA methylation decreased long-term but not short-term place field stability. Together our data highlight a role for DNA methylation in regulating neurophysiological spatial representation and memory formation. PMID:25960947\n\nSparse orthogonal population representation of spatial context in the retrosplenial cortex.\n\nPubMed\n\nMao, Dun; Kandler, Steffen; McNaughton, Bruce L; Bonin, Vincent\n\n2017-08-15\n\nSparse orthogonal coding is a key feature of hippocampal neural activity, which is believed to increase episodic memory capacity and to assist in navigation. Some retrosplenial cortex (RSC) neurons convey distributed spatial and navigational signals, but place-field representations such as observed in the hippocampus have not been reported. Combining cellular Ca 2+ imaging in RSC of mice with a head-fixed locomotion assay, we identified a population of RSC neurons, located predominantly in superficial layers, whose ensemble activity closely resembles that of hippocampal CA1 place cells during the same task. Like CA1 place cells, these RSC neurons fire in sequences during movement, and show narrowly tuned firing fields that form a sparse, orthogonal code correlated with location. RSC 'place' cell activity is robust to environmental manipulations, showing partial remapping similar to that observed in CA1. This population code for spatial context may assist the RSC in its role in memory and/or navigation.Neurons in the retrosplenial cortex (RSC) encode spatial and navigational signals. Here the authors use calcium imaging to show that, similar to the hippocampus, RSC neurons also encode place cell-like activity in a sparse orthogonal representation, partially anchored to the allocentric cues on the linear track.\n\nFrom innervation density to tactile acuity: 1. Spatial representation.\n\nPubMed\n\nBrown, Paul B; Koerber, H Richard; Millecchia, Ronald\n\n2004-06-11\n\nWe tested the hypothesis that the population receptive field representation (a superposition of the excitatory receptive field areas of cells responding to a tactile stimulus) provides spatial information sufficient to mediate one measure of static tactile acuity. In psychophysical tests, two-point discrimination thresholds on the hindlimbs of adult cats varied as a function of stimulus location and orientation, as they do in humans. A statistical model of the excitatory low threshold mechanoreceptive fields of spinocervical, postsynaptic dorsal column and spinothalamic tract neurons was used to simulate the population receptive field representations in this neural population of the one- and two-point stimuli used in the psychophysical experiments. The simulated and observed thresholds were highly correlated. Simulated and observed thresholds' relations to physiological and anatomical variables such as stimulus location and orientation, receptive field size and shape, map scale, and innervation density were strikingly similar. Simulated and observed threshold variations with receptive field size and map scale obeyed simple relationships predicted by the signal detection model, and were statistically indistinguishable from each other. The population receptive field representation therefore contains information sufficient for this discrimination.\n\nEvidence for cue-independent spatial representation in the human auditory cortex during active listening.\n\nPubMed\n\nHiggins, Nathan C; McLaughlin, Susan A; Rinne, Teemu; Stecker, G Christopher\n\n2017-09-05\n\nFew auditory functions are as important or as universal as the capacity for auditory spatial awareness (e.g., sound localization). That ability relies on sensitivity to acoustical cues-particularly interaural time and level differences (ITD and ILD)-that correlate with sound-source locations. Under nonspatial listening conditions, cortical sensitivity to ITD and ILD takes the form of broad contralaterally dominated response functions. It is unknown, however, whether that sensitivity reflects representations of the specific physical cues or a higher-order representation of auditory space (i.e., integrated cue processing), nor is it known whether responses to spatial cues are modulated by active spatial listening. To investigate, sensitivity to parametrically varied ITD or ILD cues was measured using fMRI during spatial and nonspatial listening tasks. Task type varied across blocks where targets were presented in one of three dimensions: auditory location, pitch, or visual brightness. Task effects were localized primarily to lateral posterior superior temporal gyrus (pSTG) and modulated binaural-cue response functions differently in the two hemispheres. Active spatial listening (location tasks) enhanced both contralateral and ipsilateral responses in the right hemisphere but maintained or enhanced contralateral dominance in the left hemisphere. Two observations suggest integrated processing of ITD and ILD. First, overlapping regions in medial pSTG exhibited significant sensitivity to both cues. Second, successful classification of multivoxel patterns was observed for both cue types and-critically-for cross-cue classification. Together, these results suggest a higher-order representation of auditory space in the human auditory cortex that at least partly integrates the specific underlying cues.\n\nEvidence for cue-independent spatial representation in the human auditory cortex during active listening\n\nPubMed Central\n\nMcLaughlin, Susan A.; Rinne, Teemu; Stecker, G. Christopher\n\n2017-01-01\n\nFew auditory functions are as important or as universal as the capacity for auditory spatial awareness (e.g., sound localization). That ability relies on sensitivity to acoustical cuesâparticularly interaural time and level differences (ITD and ILD)âthat correlate with sound-source locations. Under nonspatial listening conditions, cortical sensitivity to ITD and ILD takes the form of broad contralaterally dominated response functions. It is unknown, however, whether that sensitivity reflects representations of the specific physical cues or a higher-order representation of auditory space (i.e., integrated cue processing), nor is it known whether responses to spatial cues are modulated by active spatial listening. To investigate, sensitivity to parametrically varied ITD or ILD cues was measured using fMRI during spatial and nonspatial listening tasks. Task type varied across blocks where targets were presented in one of three dimensions: auditory location, pitch, or visual brightness. Task effects were localized primarily to lateral posterior superior temporal gyrus (pSTG) and modulated binaural-cue response functions differently in the two hemispheres. Active spatial listening (location tasks) enhanced both contralateral and ipsilateral responses in the right hemisphere but maintained or enhanced contralateral dominance in the left hemisphere. Two observations suggest integrated processing of ITD and ILD. First, overlapping regions in medial pSTG exhibited significant sensitivity to both cues. Second, successful classification of multivoxel patterns was observed for both cue types andâcriticallyâfor cross-cue classification. Together, these results suggest a higher-order representation of auditory space in the human auditory cortex that at least partly integrates the specific underlying cues. PMID:28827357\n\nDynamics of hippocampal spatial representation in echolocating bats\n\nPubMed Central\n\nUlanovsky, Nachum; Moss, Cynthia F.\n\n2009-01-01\n\nThe âplace fieldsâ of hippocampal pyramidal neurons are not static. For example, upon a contextual change in the environment, place-fields may âremapâ within typical timescales of ~1 minute. A few studies have shown more rapid dynamics in hippocampal activity, linked to internal processes, such as switches between spatial reference frames or changes within the theta cycle. However, little is known about rapid hippocampal place-field dynamics in response to external, sensory stimuli. Here, we studied this question in big brown bats, echolocating mammals in which we can readily measure rapid changes in sensory dynamics (sonar signals), as well as rapid behavioral switches between distal and proximal exploratory modes. First, we show that place-field size was modulated by the availability of sensory information, on a timescale of ~300-milliseconds: Bat hippocampal place-fields were smallest immediately after an echolocation call, but place-fields âdiffusedâ with the passage of time after the call, when echo information was no longer arriving. Second, we show rapid modulation of hippocampal place-fields as the animal switched between two exploratory modes. Third, we compared place fields and spatial-view fields of individual neurons and found that place tuning was much more pronounced than spatial-view tuning. In addition, dynamic fluctuations in spatial-view tuning were stronger than fluctuations in place tuning. Taken together, these results suggest that spatial representation in mammalian hippocampus can be very rapidly modulated by external sensory and behavioral events. PMID:20014379\n\nAn implicit higher-order spatially accurate scheme for solving time dependent flows on unstructured meshes\n\nNASA Astrophysics Data System (ADS)\n\nTomaro, Robert F.\n\n1998-07-01\n\nThe present research is aimed at developing a higher-order, spatially accurate scheme for both steady and unsteady flow simulations using unstructured meshes. The resulting scheme must work on a variety of general problems to ensure the creation of a flexible, reliable and accurate aerodynamic analysis tool. To calculate the flow around complex configurations, unstructured grids and the associated flow solvers have been developed. Efficient simulations require the minimum use of computer memory and computational times. Unstructured flow solvers typically require more computer memory than a structured flow solver due to the indirect addressing of the cells. The approach taken in the present research was to modify an existing three-dimensional unstructured flow solver to first decrease the computational time required for a solution and then to increase the spatial accuracy. The terms required to simulate flow involving non-stationary grids were also implemented. First, an implicit solution algorithm was implemented to replace the existing explicit procedure. Several test cases, including internal and external, inviscid and viscous, two-dimensional, three-dimensional and axi-symmetric problems, were simulated for comparison between the explicit and implicit solution procedures. The increased efficiency and robustness of modified code due to the implicit algorithm was demonstrated. Two unsteady test cases, a plunging airfoil and a wing undergoing bending and torsion, were simulated using the implicit algorithm modified to include the terms required for a moving and/or deforming grid. Secondly, a higher than second-order spatially accurate scheme was developed and implemented into the baseline code. Third- and fourth-order spatially accurate schemes were implemented and tested. The original dissipation was modified to include higher-order terms and modified near shock waves to limit pre- and post-shock oscillations. The unsteady cases were repeated using the higher\n\nA critical review of the allocentric spatial representation and its neural underpinnings: toward a network-based perspective\n\nPubMed Central\n\nEkstrom, Arne D.; Arnold, Aiden E. G. F.; Iaria, Giuseppe\n\n2014-01-01\n\nWhile the widely studied allocentric spatial representation holds a special status in neuroscience research, its exact nature and neural underpinnings continue to be the topic of debate, particularly in humans. Here, based on a review of human behavioral research, we argue that allocentric representations do not provide the kind of map-like, metric representation one might expect based on past theoretical work. Instead, we suggest that almost all tasks used in past studies involve a combination of egocentric and allocentric representation, complicating both the investigation of the cognitive basis of an allocentric representation and the task of identifying a brain region specifically dedicated to it. Indeed, as we discuss in detail, past studies suggest numerous brain regions important to allocentric spatial memory in addition to the hippocampus, including parahippocampal, retrosplenial, and prefrontal cortices. We thus argue that although allocentric computations will often require the hippocampus, particularly those involving extracting details across temporally specific routes, the hippocampus is not necessary for all allocentric computations. We instead suggest that a non-aggregate network process involving multiple interacting brain areas, including hippocampus and extra-hippocampal areas such as parahippocampal, retrosplenial, prefrontal, and parietal cortices, better characterizes the neural basis of spatial representation during navigation. According to this model, an allocentric representation does not emerge from the computations of a single brain region (i.e., hippocampus) nor is it readily decomposable into additive computations performed by separate brain regions. Instead, an allocentric representation emerges from computations partially shared across numerous interacting brain regions. We discuss our non-aggregate network model in light of existing data and provide several key predictions for future experiments. PMID:25346679\n\nSpatial representation of organic carbon and active-layer thickness of high latitude soils in CMIP5 earth system models\n\nDOE Office of Scientific and Technical Information (OSTI.GOV)\n\nMishra, Umakant; Drewniak, Beth; Jastrow, Julie D.\n\nSoil properties such as soil organic carbon (SOC) stocks and active-layer thickness are used in earth system models (F.SMs) to predict anthropogenic and climatic impacts on soil carbon dynamics, future changes in atmospheric greenhouse gas concentrations, and associated climate changes in "
    }
}
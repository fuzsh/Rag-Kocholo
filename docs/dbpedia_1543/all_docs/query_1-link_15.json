{
    "id": "dbpedia_1543_1",
    "rank": 15,
    "data": {
        "url": "https://www.linkedin.com/company/goldensource",
        "read_more_link": "",
        "language": "en",
        "title": "GoldenSource",
        "top_image": "https://media.licdn.com/dms/image/v2/C4D0BAQFjU2jr-SRBtQ/company-logo_200_200/company-logo_200_200/0/1631325274496?e=2147483647&v=beta&t=4M_pYXdP0dalv_zWmE4jGExgPjL7uu7wAq2gM1oOprg",
        "meta_img": "https://media.licdn.com/dms/image/v2/C4D0BAQFjU2jr-SRBtQ/company-logo_200_200/company-logo_200_200/0/1631325274496?e=2147483647&v=beta&t=4M_pYXdP0dalv_zWmE4jGExgPjL7uu7wAq2gM1oOprg",
        "images": [
            "https://media.licdn.com/dms/image/v2/D563DAQGmXC1zrO5CPw/image-scale_191_1128/image-scale_191_1128/0/1701386253764/goldensource_cover?e=2147483647&v=beta&t=-Ih2aYSr6WvSCPdV5_1zy-UqvtZqu7Dt_pIAvkGBg_g"
        ],
        "movies": [],
        "keywords": [],
        "meta_keywords": [
            ""
        ],
        "tags": null,
        "authors": [],
        "publish_date": null,
        "summary": "",
        "meta_description": "GoldenSource | 23,610 followers on LinkedIn. Enterprise Data Management | THE GOLD STANDARD OF DATA MANAGEMENT\nBy the end of the decade, GoldenSource will be the world‚Äôs leading data management SaaS solution, solving data mastering, reporting and analysis for 250 of the globe‚Äôs most important financial institutions.\n\nGoldenSource makes it easy to manage critical reference, market, risk and ESG data. We offer integrated Enterprise Data Management (EDM) software solutions for the securities and investment management community.",
        "meta_lang": "en",
        "meta_favicon": "https://static.licdn.com/aero-v1/sc/h/al2o9zrvru7aqj8e1x2rzsrca",
        "meta_site_name": "",
        "canonical_link": "https://www.linkedin.com/company/goldensource",
        "text": "Comparing Portfolio Physical Risk Volumes of company-level physical risk data are generated globally and regionally. What are the angles by which you can aggregate these forecasts to assess the risk profile of your investments overall? A fundamental decision is whether to look at the physical risk based on sector and sensitivity. How much consideration needs to be given to the company‚Äôs industry, how do certain hazards affect that specific type of business? Drought for example is a much bigger factor for an agricultural company than for, say, a warehouse or IT consulting firm, even if located in the same geographic region. Other aspects are whether an average or a maximum risk model is more applicable. Taking an average model, the individual market value sizes are solely driving the overall risk the investor understands to be taking on, only marginally affected by individual outliers which are potentially perilous for some investments. With a maximum risk approach, extreme exposure of some investments ‚Äì particularly when highly sensitive to that type of hazard ‚Äì will strongly influence the determined overall portfolio risk. Climate risk assessment agencies maintain different types and levels of detail. Flood risk for example can be broken into fluvial, pluvial, and coastal, however, not all data providers differentiate at this granularity. Their respective strengths may also lie in different regions, hence you likely want to work with multiple sources to gain a robust composite view. One pragmatic approach is to harmonize data to the event types used by the WMO (World Meteorological Organization). Once a standardized risk profile for the major climate scenarios is established, near- to long-term, with the chosen aggregation type, sensitive to sector and nature of event, it becomes relatively easy to perform What-If Analysis for specific investments and determine their suitability for your portfolio from a physical risk exposure perspective.\n\nThe FRTB P&L Attribution Test Volatility is the most important parameter in quantitative finance. It is used to estimate the probability that price changes will lead to cash flows for options and other derivatives. There are two ways that volatility can be estimated. The first is by extracting it from the market prices of traded options. This approach aligns with no-arbitrage valuation principles and the resulting implied volatilities (IVs) act as the risk-neutral probabilities of price changes. The official daily P&L of a bank typically uses IVs when calculating derivative valuations. The second method for estimating volatility is statistical, by reading historical time-series of prices. Market risk calculations like value-at-risk (VaR) use this second type of volatility estimate. In derivatives pricing theory probabilities of price changes, i.e., volatilities, backed out of empirical time-series data are referred to as real-world probabilities. Regulators require that banks estimate worst-case losses of their portfolios using VaR. Distributions of risk factor price changes are simulated and used to create portfolio P&Ls for each day of the VaR period. VaR is then determined using the worst P&L days. The simulations are generated using historical time series. Under the HistSim VaR method they are generated by sorting the price changes from highest to lowest directly from the time series. The parametric and Monte Carlo methods, on the other hand, use statistically calculated estimates of volatility for the VaR period by using techniques such as GARCH (1,1) and EWMA volatility. The FRTB PLAT tests whether the P&L distributions generated by the VaR simulations are statistically close to the distributions of P&L generated by saving and storing market data each day over the same period and re-running the no-arbitrage valuation processes. Historical P&Ls calculated using HistSim and GARCH for use in VaR are referred to as risk-theoretical P&L (RTPL). Historical P&Ls generated using saved down no-arbitrage IV surfaces are referred to as hypothetical P&L (HPL). Both RTPL and HPL are calculated using the latest position data available. The PLAT will always generate different P&L distributions for RTPL and HPL as the processes for generating them use two different methods for volatility estimation.\n\nA short history of interest rate models When demand for complex IR derivatives increased in the 1980s and 90s, new IR models were required, triggering innovation. The no-arbitrage valuation framework that exists today results from this period. It requires traders and valuations teams to regularly re-calibrate model parameters to market data. While this has the benefit of market consistency, the approach has circular and self-re-enforcing characteristics. The first IR models were based on the BSM model. Assumptions of a single, constant discount rate were incorrect as were assumptions that bond prices were lognormal and had constant volatility. While the Black 1976 model helped with the constant vol problem, it lacked a mechanism for incorporating the term structure of interest rates. The first term structure models were equilibrium models. Model parameters were specified using assumptions about interest rate behaviour derived from empirical observation. The no-arbitrage models that succeeded them, on the other hand, calibrated those model parameters to observable market data. No-arbitrage pricing and the calibration process was a paradigm shift. It ensured that curves would always automatically fit the market consensus ‚Äì with little incentive to challenge the consensus. By choosing the no-arbitrage approach over the equilibrium approach, the derivatives industry was implicitly deciding that market quotes for benchmark instruments contained more information about the behaviour of the term structure than available historical data. The emergence of complex IR derivatives that contained vega and other risks led to traders needing to find market prices for multiple offsetting hedging trades. This accelerated the no-arbitrage transformation as in parallel it made sense for model parameters to be calibrated to the same instruments that were used for hedging. The BSM-calculated prices for caps, floors and swaptions had a new purpose: they became the hedging instruments for complex derivatives and the benchmarking instruments for model calibration. As derivative valuation frameworks evolved, they did so around the no-arbitrage paradigm. The HJM and LMM models emerged to model and calibrate the entire term structure. With everybody calibrating to everyone else and regulators enforcing punitive model reserves and market risk capital on banks that did not match market prices, again the incentives to revert to the historical data and research-oriented approaches used in the original equilibrium models were not there. It took financial crises of different sorts for models to be adjusted. Adjacent modelling paradigms emerged to address the market dislocations that created phenomena such as volatility smiles and the multi-curve framework.\n\nPortfolio Adviser today highlights a key finding from CSC's SPV Global Outlook 2024: a concerning disconnect between the importance of regulations and the timing of regulatory planning. While 60% of private market managers recognize regulations as a focus, a surprising 34% postpone planning until just before implementation. Despite this challenge, the outlook remains optimistic, with 76% of executives foreseeing improved SPV market conditions in the coming years. However, it's important to remember that with growth comes increased regulatory scrutiny. GoldenSource's own Jeremy Katzeff, CFA, Head of Buy-Side Solutions, shares his expert insights on the implications of this trend. ‚ÄúFirms operating in private markets may have relied on last-minute compliance strategies in the past, but with a looming increase in global regulatory oversight, this approach is no longer viable. The findings from CSC highlight a market unprepared for potential regulatory changes. It's crucial for asset managers and market participants to proactively update and scale their risk management processes and underlying data strategies to navigate the complexities ahead.‚Äù https://lnkd.in/egdWW7WR #GoldenSource\n\nSome great insights here on how to overcome the problems associated with #Data silos. I particularly appreciate Toney Jennings‚Äô emphasis on the offline elements that need to be in place for effective #DataGovernance - training, policies, and clarity of ownership. The main focus in this article is security but that‚Äôs not the only reason to ensure that data and its management is not fragmented. The value that data can generate is significantly enhanced when it works as a unified whole, but that demands the kinds of governance being described here. Worth your time to read. #DataStrategy #Cybersecurity\n\nThe growing demand for climate risk data An ESG Cloud Warehouse Hub for climate risk data provides insights into how a company (as the owner of its properties) or an entire portfolio of investments is exposed to the consequences of climate change. This data ‚Äì generated by various agencies, some of which are also very proficient in conducting credit risk analysis ‚Äì is used to perform risk projections and assessments vis a vis the economic outlook of an investee or entire portfolio. It is possible to make risk assessments based on low, medium, medium-high, or high climate change scenarios. Composite risk assumptions are based on projections for the different climate events ‚Äì for example, floods, extreme temperatures, storms, wildfires, and droughts ‚Äì and how combinations of them can disrupt a company overall. The task is to determine mid-term and long-term adverse effects climate conditions may have on a business, and whether there are investment alternatives which would increase the likelihood a portfolio will meet its performance target. For example, if you‚Äôre looking at an agricultural company and determine it has farmland that is heavily susceptible to drought in five to ten years, this will influence your ‚Äì and others‚Äô ‚Äì investment decisions. Something similar applies to a company with e.g. a manufacturing facility in an increasingly high-risk region where workforces are becoming harder to recruit. The fact is, many climate changes are happening even faster than initially projected. They are having a profound effect on the way portfolio managers are assessing potential investments and complying with industry recommendations and their custom base‚Äôs expectations concerning climate-related disclosures. As a result, we are observing strongly increased interest in accessing the most comprehensive and reliable climate risk data possible.\n\n‚öî A tug-of-war between banks and regulators over new capital rules is intensifying, with the industry pushing for a softening of how capital charges are calculated under the commonly used ‚Äòinternal model approach‚Äô. üôÖ‚ôÄÔ∏è Experts say the world‚Äôs biggest banks plan to shun the IMA at scale in favour of the so-called ‚Äòstandardised approach‚Äô. This is because the cost and complexity of using it under incoming rules ‚Äî the Basel IV‚Äôs Fundamental Review of the Trading Book ‚Äî outweigh any capital benefit. ü§∑‚ôÇÔ∏è ‚ÄúThe assumption by the regulators was that as many banks as possible would go [for] the IMA, but the industry turned around and said: ‚ÄòWell, no, we‚Äôre going to go with the standardised approach‚Äô,‚Äù says Charlie Browne of GoldenSource. \"It‚Äôs a kind of a ‚Äòbe careful what you wish for‚Äô message from the banking industry back to the regulators.‚Äù üëá Story by James King. Read more below. https://lnkd.in/e-n2DSfQ More insights from Tim Clarsen at Murex, Panayiotis Dionysopoulos, CFA, at ISDA and David Kelly of BIP."
    }
}
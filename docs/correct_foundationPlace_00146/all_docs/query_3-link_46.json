{
    "id": "correct_foundationPlace_00146_3",
    "rank": 46,
    "data": {
        "url": "https://www.forbes.com/sites/tiriasresearch/2023/02/23/ibms-cloud-ai-supercomputer-vela-builds-ai-foundation-models-for-enterprise/",
        "read_more_link": "",
        "language": "en",
        "title": "IBM’s Cloud AI Supercomputer Vela Builds AI Foundation Models For Enterprise",
        "top_image": "https://imageio.forbes.com/specials-images/imageserve/63f7c2d49afeed52918ba854/0x0.jpg?format=jpg&height=900&width=1600&fit=bounds",
        "meta_img": "https://imageio.forbes.com/specials-images/imageserve/63f7c2d49afeed52918ba854/0x0.jpg?format=jpg&height=900&width=1600&fit=bounds",
        "images": [
            "https://blogs-images.forbes.com/tiriasresearch/files/2016/01/Kevin-Krewell_avatar_1453785549-400x400.jpg",
            "https://static-cdn.spot.im/assets/community-guidelines/community-guidelines-symbol.svg"
        ],
        "movies": [],
        "keywords": [],
        "meta_keywords": [
            "IBM Research",
            "Vela",
            "LLM",
            "Nvidia",
            "ChatGPT",
            "Foundation Models",
            "AI"
        ],
        "tags": null,
        "authors": [
            "Kevin Krewell"
        ],
        "publish_date": "2023-02-23T00:00:00",
        "summary": "",
        "meta_description": "The recent publicity around ChatGPT and other large language models (LLM) has brought attention to the use of foundation models as a fundamental business tool. IBM Research developed its own AI supercomputer, Vela, designed to scale. Optimized for cloud architecture offering flexible operation.",
        "meta_lang": "en",
        "meta_favicon": "https://i.forbesimg.com/48X48-F.png",
        "meta_site_name": "Forbes",
        "canonical_link": "https://www.forbes.com/sites/tiriasresearch/2023/02/23/ibms-cloud-ai-supercomputer-vela-builds-ai-foundation-models-for-enterprise/",
        "text": "The recent publicity around ChatGPT and other large language models (LLM) has brought attention to the use of foundation models as a fundamental business tool. While the consumer-facing part of generative AI using LLM has a lot of data to draw from, the reliability of the responses have shown issues when pushed to the limit. Enterprises, government agencies, and researchers may have issues using public models or have very private data pools they need to train on. If this is the case, training unique AI foundation models can be an expensive process. As these AI foundation models have continued to grow, a virtual supercomputer is needed for training the large models. These are now being referred to as AI Supercomputers. Nvidia was one of the earliest companies to describe its systems as such.\n\nThe main functional difference between supercomputers and AI supercomputers is often the math format they use for computation. Traditional supercomputers used in HPC and advanced research focus on double-precision (64-bit) floating-point performance (see the TOP500 list). AI supercomputers, on the other hand, focus on lower-precision math, which may scale down to 8-bit floating point, used for model training because neural nets do not require higher precision. AI supercomputers, like their predecessors, often use GPUs for compute acceleration.\n\nA traditional supercomputer is often a locked down, on-prem machine with a very specific bare-metal design and unique networking backbones. For example, the OpenAI supercomputer built by Microsoft Azure was “purpose-Built” with a specialized 400 gigabits per second network connectivity for each GPU server. These systems are massive, with thousands of CPUs and GPUs.\n\nIBM Research developed its own AI supercomputer design, called Vela, originally for internal use. Vela has been designed to scale, but it’s also optimized for cloud architecture, allowing more flexible operation and use. IBM has also focused on a more traditional cloud infrastructure design by using Ethernet to network the racks, not InfiniBand or other specialized networks. Using cloud architecture allows the use of industry best-practice tools and allows for easier collaboration, more agility, and flexibility while running multiple workloads.\n\nVela is composed of multiple nodes each consisting of eight Nvidia A100 GPUs (80GB versions), which are interconnected by NVLink and NVSwitch. Each node has two 2nd Generation Intel Xeon Scalable processors (Cascade Lake), with 1.5TB of DRAM and four 3.2TB NVMe drives each. These compute nodes are connected via multiple 100G network interfaces. The total number of nodes was not revealed by IBM, but is being used to train models with tens of billions of parameters.\n\nIBM’s Vela offers users more flexibility by virtualizing the bare metal, yet at the same time, IBM can get nearly 95% of the performance of bare metal by optimizing the way they do the virtualization. Running on top of Vela, IBM used a cloud native scheduler, MCAD, an open-source distributed computing runtime, RAY, and PyTorch as an enabler for distributed AI training. The company also leveraged OpenShift’s open advanced networking configuration tools to drive near bare-metal network performance in containers. The primary programming model was an extended version of PyTorch to hide network latency by scheduling GPU workflow. IBM has also been working on PyTorch FSDP to improve efficiency on the IBM Cloud and has been working of a full workflow for building new foundation models including data pre-processing, training and consuming optimizations.\n\nWhile Vela was originally developed for internal use, IBM is exploring customer engagements where it can apply its foundation model development process to customer problems. IBM’s goal is to focus specifically on solving B2B problems, not consumer workloads like ChatGPT or winning on Jeopardy. As it did with the recently announced application of building foundation models with NASA, IBM will work with customers to co-create custom models. There are also foundation models needed for other data including code, sensor data, materials/chemistry, etc. There’s a lot of data to build models on beyond language. Some optimized models don’t need to use huge data sets. While generative AI does better with larger models, there’s often a different sweet spot for other applications. IBM Research also has an extensive research initiative to build models that use lower-precision math to reduce model energy, time, and cost. Vela can support hybrid cloud for training and the whole operations stack is portable.\n\nWhile there’s a lot of marketing hype surrounding LLM models and generative AI, there’s many business applications for building custom foundation models on IBM’s flexible and agile AI supercomputing cloud."
    }
}
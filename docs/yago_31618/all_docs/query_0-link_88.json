{
    "id": "yago_31618_0",
    "rank": 88,
    "data": {
        "url": "https://avuncular5.rssing.com/chan-3979681/all_p1.html",
        "read_more_link": "",
        "language": "en",
        "title": "Cleve’s Corner: Cleve Moler on Mathematics and Computing",
        "top_image": "",
        "meta_img": "",
        "images": [
            "https://pixel.quantserve.com/pixel/p-KygWsHah2_7Qa.gif",
            "https://blogs.mathworks.com/images/cleve/fibmat_01.png",
            "https://feeds.feedburner.com/~r/mathworks/moler/~4/DkoSApB6Edk",
            "https://blogs.mathworks.com/images/moler/debut_eq26509.png",
            "https://blogs.mathworks.com/images/moler/debut_eq26509.png",
            "https://blogs.mathworks.com/images/moler/debut_eq26509.png",
            "https://blogs.mathworks.com/images/moler/debut_eq26509.png",
            "https://blogs.mathworks.com/images/moler/debut_eq26509.png",
            "https://blogs.mathworks.com/images/moler/debut_eq26509.png",
            "https://feeds.feedburner.com/~r/mathworks/moler/~4/OvRbNSYjD8E",
            "https://blogs.mathworks.com/images/cleve/biorhythms_01.png",
            "https://blogs.mathworks.com/images/cleve/biorhythms_02.png",
            "https://blogs.mathworks.com/images/cleve/biorhythms_03.png",
            "https://blogs.mathworks.com/images/cleve/biorhythms_04.png",
            "https://blogs.mathworks.com/images/cleve/biorhythms_05.png",
            "https://blogs.mathworks.com/images/cleve/biorhythms_06.png",
            "https://feeds.feedburner.com/~r/mathworks/moler/~4/OPjXnOKwdu8",
            "https://blogs.mathworks.com/images/cleve/spacewar_01.png",
            "https://blogs.mathworks.com/images/cleve/spacewar_02.png",
            "https://blogs.mathworks.com/images/cleve/spacewar_03.png",
            "https://blogs.mathworks.com/images/cleve/spacewar_04.png",
            "https://blogs.mathworks.com/images/cleve/spacewar_05.png",
            "https://blogs.mathworks.com/images/cleve/spacewar_06.png",
            "https://feeds.feedburner.com/~r/mathworks/moler/~4/mPQd7psAlOQ",
            "https://blogs.mathworks.com/images/cleve/exponential_01.png",
            "https://blogs.mathworks.com/images/cleve/exponential_02.png",
            "https://blogs.mathworks.com/images/cleve/exponential_03.png",
            "https://feeds.feedburner.com/~r/mathworks/moler/~4/sPxCMzqDuOc",
            "https://feeds.feedburner.com/~r/mathworks/moler/~4/5VrQJAXEX1I",
            "https://blogs.mathworks.com/images/cleve/friday13_01.png",
            "https://feeds.feedburner.com/~r/mathworks/moler/~4/xeTVvkvp0tU",
            "https://blogs.mathworks.com/images/cleve/pchips_01.png",
            "https://blogs.mathworks.com/images/cleve/pchips_02.png",
            "https://blogs.mathworks.com/images/cleve/pchips_03.png",
            "https://blogs.mathworks.com/images/cleve/pchips_04.png",
            "https://blogs.mathworks.com/images/cleve/pchips_05.png",
            "https://blogs.mathworks.com/images/cleve/pchips_06.png",
            "https://blogs.mathworks.com/images/cleve/pchips_07.png",
            "https://feeds.feedburner.com/~r/mathworks/moler/~4/D2m06zrxdjc",
            "https://blogs.mathworks.com/images/cleve/balancing_01.png",
            "https://feeds.feedburner.com/~r/mathworks/moler/~4/Zn58B9nq96Y",
            "https://blogs.mathworks.com/images/cleve/pythag_blog_01.png",
            "https://feeds.feedburner.com/~r/mathworks/moler/~4/pS99QMkaess",
            "https://blogs.mathworks.com/images/cleve/drums_1_01.png",
            "https://blogs.mathworks.com/images/cleve/drums_1_02.png",
            "https://blogs.mathworks.com/images/cleve/drums_1_03.png",
            "https://feeds.feedburner.com/~r/mathworks/moler/~4/mjiWEIaDXfQ",
            "https://blogs.mathworks.com/images/cleve/drums_2_01.png",
            "https://blogs.mathworks.com/images/cleve/drums_2_02.png",
            "https://blogs.mathworks.com/images/cleve/drums_2_03.png",
            "https://blogs.mathworks.com/images/cleve/drums_2_04.png",
            "https://blogs.mathworks.com/images/cleve/drums_2_05.png",
            "https://blogs.mathworks.com/images/cleve/drums_2_06.png",
            "https://blogs.mathworks.com/images/cleve/drums_2_07.png",
            "https://blogs.mathworks.com/images/cleve/drums_2_08.png",
            "https://blogs.mathworks.com/images/cleve/drums_2_09.png",
            "https://blogs.mathworks.com/images/cleve/drums_2_10.png",
            "https://blogs.mathworks.com/images/cleve/drums_2_11.png",
            "https://feeds.feedburner.com/~r/mathworks/moler/~4/JToqJx6iF-I",
            "https://blogs.mathworks.com/images/cleve/drums_3_01.png",
            "https://blogs.mathworks.com/images/cleve/drums_3_02.png",
            "https://blogs.mathworks.com/images/cleve/drums_3_03.png",
            "https://blogs.mathworks.com/images/cleve/drums_3_04.png",
            "https://feeds.feedburner.com/~r/mathworks/moler/~4/I5Vs7CnZsRI",
            "https://blogs.mathworks.com/images/cleve/potted_palm.jpg",
            "https://blogs.mathworks.com/images/cleve/Jacks_Compaq.jpg",
            "https://feeds.feedburner.com/~r/mathworks/moler/~4/1VDLb72tn9M",
            "https://blogs.mathworks.com/images/cleve/block_movie.gif",
            "https://blogs.mathworks.com/images/cleve/blinker_movie.gif",
            "https://blogs.mathworks.com/images/cleve/glider_movie.gif",
            "https://blogs.mathworks.com/images/cleve/glider_gun_2.png",
            "https://feeds.feedburner.com/~r/mathworks/moler/~4/IbED_wCloDs",
            "https://blogs.mathworks.com/images/cleve/achim_movie.gif",
            "https://feeds.feedburner.com/~r/mathworks/moler/~4/2bn9O3mttRQ",
            "https://blogs.mathworks.com/images/cleve/r_pent_pic.gif",
            "https://blogs.mathworks.com/images/cleve/dozen_pic.gif",
            "https://blogs.mathworks.com/images/cleve/canada_goose_init.gif",
            "https://blogs.mathworks.com/images/cleve/washerwoman_init.gif",
            "https://blogs.mathworks.com/images/cleve/spacefiller_init.gif",
            "https://blogs.mathworks.com/images/cleve/R2D2.gif",
            "https://blogs.mathworks.com/images/cleve/L_logo_init.gif",
            "https://feeds.feedburner.com/~r/mathworks/moler/~4/lSoI_KLFTHw",
            "https://blogs.mathworks.com/images/cleve/supremum_01.png",
            "https://blogs.mathworks.com/images/cleve/supremum_02.png",
            "https://feeds.feedburner.com/~r/mathworks/moler/~4/N3Em1PQl4MA",
            "https://blogs.mathworks.com/images/cleve/chebfun_1_01.png",
            "https://blogs.mathworks.com/images/cleve/chebfun_1_02.png",
            "https://feeds.feedburner.com/~r/mathworks/moler/~4/QBG4X_nZ-HM",
            "https://blogs.mathworks.com/images/cleve/chebfun_2_01.png",
            "https://blogs.mathworks.com/images/cleve/chebfun_2_02.png",
            "https://blogs.mathworks.com/images/cleve/chebfun_2_03.png",
            "https://blogs.mathworks.com/images/cleve/chebfun_2_04.png",
            "https://feeds.feedburner.com/~r/mathworks/moler/~4/U3nTPEa3u6k",
            "https://media.mwcradio.com/mimesis/2013-03/20/greg%20basped.jpg",
            "https://mychristianpsychic.com/wp-content/uploads/2021/08/Shanann-Watts-and-Chris.png",
            "https://www.homesnacks.com/images/tx/crockett-tx-0.jpg",
            "https://i1.wp.com/3.bp.blogspot.com/-xvc6hEY4CUg/V2nbkOWvz-I/AAAAAAAADoM/Ix21woxZpmwyUYG7ywF9Ymr3Sbiw0t4OACLcB/s1600/Dagadi%2BChaawl%2B%25282015%2529%2BPoster.jpg?resize=350%2C450&ssl=1",
            "https://a2.mzstatic.com/us/r30/Purple1/v4/08/25/56/0825565e-8100-b566-ee09-aa660e56f559/screen1136x1136.jpeg",
            "https://i.imgur.com/V8Qf3KI.png",
            "https://www.inettutor.com/wp-content/uploads/2019/02/Online-Grading-System-with-Grade-Viewing-Conceptual-Framework.png",
            "https://community.cadence.com/resized-image/__size/714x422/__key/communityserver-discussions-components-files/48/pastedimage1720743245337v1.png",
            "https://i.imgur.com/UCxxo.jpg",
            "https://lh3.googleusercontent.com/m-ZU04Z6MTLN8Y4Z3YjY7fEhV8GuUG4AFeJ1G7XeHhOtou6wJVK8AcroO756NdNHuMuo_HUhLfbf7VsUMOxDCj3xm86ch5GuNRUAkylvtjubg8XovPKx5tx_wapUMEmxO1wpDXme",
            "https://thepost.s3.amazonaws.com/wp-content/uploads/2014/01/0CA0C8U0-150x150.jpg",
            "https://photos-a.propertyimages.ie/media/5/9/5/3035595/c8986a32-f6d4-4bf9-aeb1-4cfc4103f0b5_m.jpg",
            "https://busyteacher.org/uploads/posts/2014-03/thumbs/1396229583_completing-food-worksheet-1.png",
            "https://augustacrime.com/wp-content/uploads/2024/07/hailey-hecker-18-of-warrenville-narcotics-possession-drug-offense-giving-false-info-identity-fraud-to-obtain-employment-or-avoid-detection-driving-under-suspension-150x150.jpg",
            "https://images.qvc.com/is/image/pic/co/Danjob.jpg",
            "https://1.bp.blogspot.com/-YOPGWJWdd3c/WToRtjhqKNI/AAAAAAABFDI/6P7-_HaxmtQlo643FAb5TLZCJx7dqX_dwCLcB/s1600/18893232_10155410415748501_7328552017510958888_n.jpg",
            "https://assets.rappler.com/612F469A6EA84F6BAE882D2B94A4B421/img/B521301123094E2382D4CFC2BC3031ED/fake-prayforjapan-october-16-2019.jpg",
            "https://www.ksstradio.com/wp-content/uploads/2019/07/Trondamion-Andrzhel-Cleveland.jpg",
            "https://i.imgur.com/QfNCYCP.png",
            "https://assets.suredone.com/1517/media-pics/cp049425-rear-license-plate-holder-vw-golf-mk3-north-american-tub-tray-1hm-853-481-d.jpg",
            "https://i.imgur.com/GjPLUKr.png",
            "https://s3.us-west-2.amazonaws.com/assets.eastidahonews.com/wp-content/uploads/2024/08/Gibson.jpg",
            "https://s3.us-west-2.amazonaws.com/assets.eastidahonews.com/wp-content/uploads/2024/08/Gun-testing.jpg",
            "https://www.rappler.com/tachyon/2024/08/Screenshot_20240821-184900_Samsung-Internet.jpg?fit=1024%2C610",
            "https://www.thesun.co.uk/wp-content/uploads/2024/08/VIRALPRESS_CROC_ATTACK__3_of_25_jpg-JS926588521.jpg?strip=all&w=960",
            "https://www.thescottishsun.co.uk/wp-content/uploads/sites/2/2024/08/rain-forecast-sees-locals-tourists-923058805.jpg?strip=all&w=960",
            "https://i.etsystatic.com/6165833/r/il/43a370/4486494183/il_570xN.4486494183_64g7.jpg",
            "https://i.etsystatic.com/9729164/r/il/4a9683/1889315419/il_570xN.1889315419_tftl.jpg",
            "https://i.etsystatic.com/39456002/r/il/4790b5/5330755061/il_570xN.5330755061_2sra.jpg",
            "https://i.oodleimg.com/item/7375529485t_1s_cats_in_saint_petersburg_fl/?1724200735"
        ],
        "movies": [],
        "keywords": [],
        "meta_keywords": [
            ""
        ],
        "tags": null,
        "authors": [],
        "publish_date": null,
        "summary": "",
        "meta_description": "",
        "meta_lang": "en",
        "meta_favicon": "//www.rssing.com/favicon.ico",
        "meta_site_name": "",
        "canonical_link": null,
        "text": "Contents\n\nA bug report\n\nMathWorks recently received a bug report involving the matrix\n\nX = [ 63245986, 102334155 102334155, 165580141]\n\nX = 63245986 102334155 102334155 165580141\n\nThe report claimed that MATLAB computed an inaccurate result for the determinant of X.\n\nformat long detX = det(X)\n\ndetX = 1.524897739291191\n\nThe familiar high school formula gives a significantly different result.\n\ndetX = X(1,1)*X(2,2) - X(1,2)*X(2,1)\n\ndetX = 2\n\nWhile checking out the report, I computed the elementwise ratio of the rows of X and discovered an old friend.\n\nratio = X(2,:)./X(1,:)\n\nratio = 1.618033988749895 1.618033988749895\n\nThis is $\\phi$, the Golden ratio.\n\nphi = (1 + sqrt(5))/2\n\nphi = 1.618033988749895\n\nSo, I decided to investigate further.\n\nPowers of a Fibonacci matrix\n\nLet's call the following 2-by-2 matrix the Fibonacci matrix.\n\n$$F = \\pmatrix{0 & 1 \\cr 1 & 1}$$\n\nGenerate the matrix in MATLAB\n\nF = [0 1; 1 1]\n\nF = 0 1 1 1\n\nThe elements of powers of $F$ are Fibonacci numbers. Let $f_n$ be the $n$-th Fibanacci number. Then the $n$-th power of $F$ contains three successive $f_n$.\n\n$$F^n = \\pmatrix{f_{n-1} & f_n \\cr f_n & f_{n+1}}$$\n\nFor example\n\nF2 = F*F F3 = F*F2 F4 = F*F3 F5 = F*F4\n\nF2 = 1 1 1 2 F3 = 1 2 2 3 F4 = 2 3 3 5 F5 = 3 5 5 8\n\nThe matrix in the bug report is $F^{40}$.\n\nX = F^40\n\nX = 63245986 102334155 102334155 165580141\n\nThese matrix powers are computed without any roundoff error. Their elements are \"flints\", floating point numbers whose values are integers.\n\nDeterminants\n\nThe determinant of $F$ is clearly $-1$. Hence\n\n$$\\mbox{det}(F^n) = (-1)^n$$\n\nSo det(F^40) should be 1, not 1.5249, and not 2.\n\nLet's examine the computation of determinants using floating point arithmetic. We expect these to be +1 or -1.\n\ndet1 = det(F) det2 = det(F^2) det3 = det(F^3)\n\ndet1 = -1 det2 = 1 det3 = -1\n\nSo far, so good. However,\n\ndet40 = det(F^40)\n\ndet40 = 1.524897739291191\n\nThis has barely one digit of accuracy.\n\nWhat happened?\n\nIt is instructive to look at the accuracy of all the determinants.\n\nd = zeros(40,1); for n = 1:40 d(n) = det(F^n); end format long d\n\nd = -1.000000000000000 1.000000000000000 -1.000000000000000 0.999999999999999 -0.999999999999996 1.000000000000000 -0.999999999999996 0.999999999999996 -1.000000000000057 0.999999999999872 -0.999999999999943 0.999999999997726 -0.999999999998209 1.000000000000227 -1.000000000029786 1.000000000138243 -0.999999999990905 1.000000000261935 -1.000000000591172 0.999999999999091 -0.999999996060069 1.000000003768946 -0.999999926301825 0.999999793712050 -1.000000685962732 0.999998350307578 -0.999995890248101 0.999983831774443 -0.999970503151417 1.000005397945643 -0.999914042185992 0.999138860497624 -0.997885793447495 0.998510751873255 -0.996874589473009 0.973348170518875 -0.989943694323301 0.873688660562038 -0.471219316124916 1.524897739291191\n\nWe see that the accuracy deteriorates as n increases. In fact, the number of correct digits is roughly a linear function of n, reaching zero around n = 40.\n\nA log plot of the error\n\nsemilogy(abs(abs(d)-1),'.') set(gca,'ydir','rev') title('error in det(F^n)') xlabel('n') ylabel('error') axis([0 41 eps 1])\n\nComputing determinants\n\nMATLAB computes determinants as the product of the diagonal elements of the triangular factors resulting from Gaussian elimination.\n\nLet's look at $F^{12}$.\n\nformat long e F12 = F^12 [L,U] = lu(F12)\n\nF12 = 89 144 144 233 L = 6.180555555555555e-01 1.000000000000000e+00 1.000000000000000e+00 0 U = 1.440000000000000e+02 2.330000000000000e+02 0 -6.944444444428655e-03\n\nSince $det(L) = -1$,\n\ndet(L)\n\nans = -1\n\nwe have\n\ndet12 = -prod(diag(U))\n\ndet12 = 9.999999999977263e-01\n\nWe can see that, for $F^{12}$, the crucial element U(2,2) has lost about 5 significant digits in the elimination and that this is reflected in the accuracy of the determinant.\n\nHow about $F^{40}$?\n\n[L,U] = lu(F^40) det40 = -prod(diag(U))\n\nL = 6.180339887498949e-01 1.000000000000000e+00 1.000000000000000e+00 0 U = 1.023341550000000e+08 1.655801410000000e+08 0 -1.490116119384766e-08 det40 = 1.524897739291191e+00\n\nThe troubling value of 1.5249 from the bug report is a direct result of the subtractive cancellation involved in computing U(2,2). In order for the computed determinant to be 1, the value of U(2,2) should have been\n\nU22 = -1/U(1,1)\n\nU22 = -9.771908508943080e-09\n\nCompare U22 with U(2,2). The value of U(2,2) resulting from elimination has lost almost all its accuracy.\n\nThis prompts us to check the condition of $F^{40}$.\n\ncond40 = cond(F^40)\n\ncond40 = Inf\n\nWell, that's not much help. Something is going wrong in the computation of cond(F^40).\n\nGolden Ratio\n\nThere is an intimate relationship between Fibonacci numbers and the Golden Ratio. The eigenvalues of $F$ are $\\phi$ and its negative reciprocal.\n\n$$\\phi = (1+\\sqrt{5})/2$$\n\n$$\\bar{\\phi} = (1-\\sqrt{5})/2$$\n\nformat long phibar = (1-sqrt(5))/2 phi = (1+sqrt(5))/2 eigF = eig(F)\n\nphibar = -0.618033988749895 phi = 1.618033988749895 eigF = -0.618033988749895 1.618033988749895\n\nBecause powers of $\\phi$ dominate powers of $\\bar{\\phi}$, it is possible to generate $F^n$ by rounding a scaled matrix of powers of $\\phi$ to the nearest integers.\n\n$$F^n = \\mbox{round}\\left(\\pmatrix{\\phi^{n-1} & \\phi^n \\cr \\phi^n & \\phi^{n+1}}/\\sqrt{5}\\right)$$\n\nn = 40 F40 = round( [phi^(n-1) phi^n; phi^n phi^(n+1)]/sqrt(5) )\n\nn = 40 F40 = 63245986 102334155 102334155 165580141\n\nBefore rounding, the matrix of powers of $\\phi$ is clearly singular, so the rounded matrix $F^n$ must be regarded as \"close to singular\", even though its determinant is $+1$. This is yet another example of the fact that the size of the determinant cannot be a reliable indication of nearness to singularity.\n\nCondition\n\nThe condition number of $F$ is the ratio of its singular values, and because $F$ is symmetric, its singular values are the absolute value of its eigenvalues. So\n\n$$\\mbox{cond}(F) = \\phi/|\\bar{\\phi}| = \\phi^2 = 1+\\phi$$\n\ncondF = 1+phi\n\ncondF = 2.618033988749895\n\nThis agrees with\n\ncondF = cond(F)\n\ncondF = 2.618033988749895\n\nWith the help of the Symbolic Toolbox, we can get an exact expression for $\\mbox{cond}(F^{40}) = \\mbox{cond}(F)^{40}$\n\nPhi = sym('(1+sqrt(5))/2'); cond40 = expand((1+Phi)^40)\n\ncond40 = (23416728348467685*5^(1/2))/2 + 52361396397820127/2\n\nThe numerical value is\n\ncond40 = double(cond40)\n\ncond40 = 5.236139639782013e+16\n\nThis is better than the Inf we obtain from cond(F^40). Note that it is more than an order of magnitude larger than 1/eps.\n\nBackward error\n\nJ. H. Wilkinson showed us that the L and U factors computing by Gaussian elimination are the exact factors of some matrix within roundoff error of the given matrix. With hindsight and the Symbolic Toolbox, we can find that matrix.\n\ndigits(25) X = vpa(L)*vpa(U)\n\nX = [ 63245986.00000000118877885, 102334154.9999999967942319] [ 102334155.0, 165580141.0]\n\nOur computed L and U are the actual triangular decomposition of this extended precision matrix. And the determinant of this X is the shaky value that prompted this investigation.\n\ndet(X)\n\nans = 1.524897739291191101074219\n\nThe double precision matrix closest to X is precisely $F^{40}$.\n\ndouble(X)\n\nans = 63245986 102334155 102334155 165580141\n\nRetrospective backward error analysis confirms Wilkinson's theory in this example.\n\nThe high school formula\n\nThe familiar formula for 2-by-2 determinants\n\n$$\\mbox{det}(X) = x_{1,1} x_{2,2} - x_{1,2} x_{2,1}$$\n\ngives +1 or -1 with no roundoff error for all $X = F^n$ with $n < 40$. It fails completely for $n > 40$. The behavior for exactly $n = 40$ is interesting. Set the output format to\n\nformat bank\n\nThis format allows us to see all the digits generated when converting binary floating point numbers to decimal for printing. Again, let\n\nX = F^40\n\nX = 63245986.00 102334155.00 102334155.00 165580141.00\n\nLook at the last digits of these two products.\n\np1 = X(1,1)*X(2,2) p2 = X(2,1)*X(1,2)\n\np1 = 10472279279564026.00 p2 = 10472279279564024.00\n\nThe 6 at the end of p1 is correct because X(1,1) ends in a 6 and X(2,2) ends in a 1. But the 4 at the end of p2 is incorrect. It should be a 5 because both X(2,1) and X(1,2) end in 5. However, the spacing between floating point numbers of this magnitude is\n\nformat short delta = eps(p2)\n\ndelta = 2\n\nSo near p2 only even flints can be represented. In fact, p1 is the next floating point number after p2. The true product of X(2,1) and X(1,2) falls halfway between p1 and p2 and must be rounded to one or the other. The familiar formula cannot possibly produce the correct result.\n\nHistorical irony\n\nFor many years, the det function in MATLAB would apply the round function to the computed value if all the matrix entries were integers. So, these old versions would have returned exactly +1 or -1 for det(F^n) with n < 40. But they would round det(F^40) to 2. This kind of behavior was the reason we got rid of the rounding.\n\neig and svd\n\nLet's try to compute eigenvalues and singular values of $F^{40}$.\n\nformat long e eig(F^40) svd(F^40)\n\nans = 0 228826127 ans = 2.288261270000000e+08 0\n\nUnfortunately, the small eigenvalue and small singular value are completely lost. We know that the small value should be\n\nphibar^40\n\nans = 4.370130339181083e-09\n\nBut this is smaller than roundoff error in the large value\n\neps(phi^40)\n\nans = 2.980232238769531e-08\n\nLog plots of the accuracy loss in the computed small eigenvalue and the small singular value are similar to our plot for the determinant. I'm not sure how LAPACK computes eigenvalues and singular values of 2-by-2 matrices. Perhaps that can be the subject of a future blog.\n\nGet the MATLAB code\n\nPublished with MATLAB® 7.14\n\nBiorhythms were invented over 100 years ago and entered our popular culture in the 1960s. You can still find many Web sites today that offer to prepare personalized biorhythms, or that sell software to compute them. Biorhythms are based on the notion that three sinusoidal cycles influence our lives. The physical cycle has a period of 23 days, the emotional cycle has a period of 28 days, and the intellectual cycle has a period of 33 days. For any individual, the cycles are initialized at birth. All the people on earth born on one particular day share the biorhythm determined by that date.\n\nContents\n\nPersonal biorhythm\n\nI hope you can download the latest version of my biorhythm program from MATLAB Central at this link. If you already happen to have access to the programs from my book \"Experiments with MATLAB\", you will find an earlier version of biorhythm there. Either of these programs will allow you to compute your own personal biorhythm.\n\nVectorized plots\n\nBiorhythms are an excellent way to illustrate MATLAB's vector plotting capabilities. Let's start with a column vector of time measured in days.\n\nt = (0:28)';\n\nThe first way you might think of to compute the biorhythm for these days is create this array with three columns.\n\ny = [sin(2*pi*t/23) sin(2*pi*t/28) sin(2*pi*t/33)];\n\nOr, we can get fancy by moving the square brackets inside the sin(...) evaluation and, while we're at it, define an anonymous function. This vectorized calculation is the core of our biorhythm function.\n\nbio = @(t) sin(2*pi*[t/23 t/28 t/33]); y = bio(t);\n\nWe're now ready for our first plot. Because t goes from 0 to 28, the 23-day blue curve covers more than its period, the 28-day green curve covers exactly one period, and the 33-day red curve covers less its period.\n\nclf plot(t,y) axis tight\n\nDate functions\n\nMATLAB has several functions for computations involving calendars and dates. They are all based on a date number, which is the amount of time, in units of days, since an arbitrary origin in year zero. The current datenum is provided by the function now. I am writing this post on June 10, 2012, and every time I publish it, the value of now changes. Here is the current value.\n\nformat compact format bank date = now\n\ndate = 735030.87\n\nA readable form of the current date is provided by datestr.\n\ndatestr(now)\n\nans = 10-Jun-2012 20:45:47\n\nThe integer part of now is a date number for the entire day. This value is used in biorhythm.\n\nformat short today = fix(now)\n\ntoday = 735030\n\nNewborn\n\nWith no input arguments, my latest version of biorhythm plots the biorhythm for a baby born four weeks before today. The plot covers the eight week period from the baby's birth until a date four weeks in the future. You can see the three curves all initialized at birth. The blue physical cycle has a 23-day period, so it passes through zero five days before today. The green emotional cycle has a 28-day period, so it hits zero today. And, the red intellectual cycle will be zero five days from today.\n\nbiorhythm\n\nTwenty-first Birthday\n\nLet's look at the biorhythm for someone whose 21st birthday is today. The date vector for such a birthday is obtained by subtracting 21 from the first component of today's datevec. This is a pretty unexciting, but typical, biorhythm. All three cycles are near their midpoints. Blue and red peaked about a week ago and green will peak a little more than a week from now.\n\nbirthday = datevec(today) - [21 0 0 0 0 0]; biorhythm(birthday)\n\nRebirth\n\nDoes your biorhythm ever start over? Yes, it does, at a time t when t/23, t/28 and t/33 are all integers. Since the three periods are relatively prime, the first such value of t is their product.\n\ntzero = 23*28*33\n\ntzero = 21252\n\nHow many years and days is this?\n\ndpy = 365+97/400 yzero = fix(tzero/dpy) dzero = dpy*mod(tzero/dpy,1)\n\ndpy = 365.2425 yzero = 58 dzero = 67.9350\n\nSo, your biorhythm starts over when you are 58 years old, about 68 days after your birthday.\n\nbiorhythm(today-tzero)\n\nNearly Perfect Day\n\nIs there ever a perfect day, one where all three cycles reach their maximum at the same time? Well, not quite. That would require a time t when, for p = 23, 28, 33, t/p = n+1/4, where n is an integer. Then sin(2*pi*t/p) would equal sin(pi/2), which is the maximum. But, since the values of p are relatively prime, there is no such value of t. But we can get close. To find the nearly perfect day, look for the maximum value of the sum of the three cycles.\n\nt = (1:tzero)'; y = bio(t); s = sum(y,2); top = find(s==max(s)) biorhythm(today-top)\n\ntop = 17003\n\nHow old are you on this nearly perfect day?\n\ntop/dpy\n\nans = 46.5526\n\nSo, half-way through your 46th year.\n\nDetail\n\nBut the nearly perfect day is not perfection. The three cyles are not quite at their peaks.\n\nbio(top)\n\nans = 0.9977 1.0000 0.9989\n\nLet's zoom in to a two day window around top, the location of the maximum sum. Measure time in hours.\n\nclf t = (-1:1/24:1)'; y = 100*bio(top+t); plot(24*t,y) set(gca,'xaxislocation','top','xlim',[-24 24],'xtick',-24:6:24,... 'ylim',[96.0 100.4]) title(['biorhythms near day ' int2str(top) ', time in hours'])\n\nWe can see that the three peaks occur six hours apart. This is the closest we get to perfection, and the only time in the entire 58-year cycle when we get even this close.\n\nHave a good day.\n\nGet the MATLAB code\n\nPublished with MATLAB® 7.14\n\nThis is the probably the first article ever written with the title \"Symplectic Spacewar\". If I Google that title today, including the double quotes to keep the two words together, I do not get any hits. But as soon as Google notices this post, I should be able to see at least one hit.\n\nContents\n\nSymplectic\n\nSymplectic is an infrequently used mathematical term that describes objects joined together smoothly. It also has something to do with fish bones. For us, it is a fairly new class of numerical methods for solving certain special types of ordinary differential equations.\n\nSpacewar\n\nSpacewar is now generally recognized as the world's first video game. It was written by Steve \"Slug\" Russell and some of his buddies at MIT in 1962. Steve was a reseach assistant for Professor John McCarthy, who moved from MIT to Stanford in 1963. Steve came to California with McCarthy, bringing Spacewar with him. I met Steve then and played Spacewar a lot while I was a grad student at Stanford.\n\nSpacewar ran on the PDP-1, Digital Equipment Corporation's first computer. Two space ships, controlled by players using switches on the console, shoot space torpedoes at each other. The space ships and the torpedoes orbit around a central star. Here is a screen shot.\n\nX = imread('http://blogs.mathworks.com/images/cleve/Spacewar1.png'); imshow(X)\n\nAnd here is a photo, taken at the Vintage Computer Fair in 2006, of Steve and the PDP-1. The graphics display, an analog cathrode ray tube driven by the computer, can be seen over Steve's shoulder. The bank of sense switches is at the base of the console.\n\nX = imread('http://blogs.mathworks.com/images/cleve/Steve_Russell.png'); imshow(X)\n\nA terrific Java web applet, written by Barry and Brian Silverman and Vadim Gerasimov, provides a simulation of PDP-1 machine instructions running Spacewar. It is available here. Their README files explains how to use the keyboard in place of the sense switches. You should start by learning how to turn the spaceship and fire its rockets to avoid being dragged into the star and destroyed.\n\nCircle generator\n\nThe gravitational pull of the star causes the ships and torpedos to move in elliptical orbits, like the path of the torpedo in the screen shot. Steve's program needed to compute these trajectories. At the time, there was nothing like MATLAB. Programs were written in machine language, with each line of the program corresponding to a single machine instruction. I don't think there was any floating point arithmetic hardware; floating point was probably done in software. In any case, it was desirable to avoid evaluation of trig functions in the orbit calculations.\n\nThe orbit-generating program would have looked something like this.\n\nx = 0 y = 32768 L: plot x y load y shift right 2 add x store in x change sign shift right 2 add y store in y go to L\n\nWhat does this program do? There are no trig functions, no square roots, no multiplications or divisions. Everything is done with shifts and additions. The initial value of y, which is $2^{15}$, serves as an overall scale factor. All the arithmetic involves a single integer register. The \"shift right 2\" command takes the contents of this register, divides it by $2^2$, and discards any remainder.\n\nNotice that the current value of y is used to update x, then this new x is used to update y. This optimizes both instruction count and storage requirements because it is not necessary to save the current x to update y. But, as we shall see, this is also the key to the method's numerical stability.\n\nOriginal\n\nIf the Spacewar orbit generator were written today in MATLAB, it would look something the following. There are two trajectories, with different step sizes. The blue trajectory has h = 1/4, corresponding to \"shift right 2\". The green trajectory has h = 1/32, corresponding to \"shift right 5\". We are no longer limited to integer values, so I have changed the scale factor from $2^{15}$ to $1$. The trajectories are not exact circles, but in one period they return to near the starting point. Notice, again, that the current y is used to update x and then the new x is used to update y.\n\nclf axis(1.5*[-1 1 -1 1]) axis square bg = 'blue'; for h = [1/4 1/32] x = 0; y = 1; line(x,y,'marker','o','color','k') for t = 0:h:2*pi line(x,y,'marker','.','color',bg) x = x + h*y; y = y - h*x; end bg = [0 2/3 0]; end title('Original')\n\nEuler's method\n\nAn exact circle would be generated by solving this system of ordinary differential equations.\n\n$$\\dot{x}_1 = x_2$$\n\n$$\\dot{x}_2 = -x_1$$\n\nThis can be written in vectorized form as\n\n$$\\dot{x} = A x$$\n\nwhere\n\n$$A = \\pmatrix{0 & 1 \\cr -1 & 0}$$\n\nThe simplest method for computing an approximate numerical solution to this system, Euler's method, is\n\n$$x(t+h) = x(t) + h A x(t)$$\n\nIn the vectorized MATLAB code, all components of x are updated together. This causes the trajectories to spiral outward. Decreasing the step size decreases the spiraling rate, but does not eliminate it.\n\nclf axis(1.5*[-1 1 -1 1]) axis square bg = 'blue'; A = [0 1; -1 0]; for h = [1/4 1/32] x = [0 1]'; line(x(1),x(2),'marker','o','color','k') for t = 0:h:6*pi x = x + h*A*x; line(x(1),x(2),'marker','.','color',bg) end bg = [0 2/3 0]; end title('Euler')\n\nImplicit Euler\n\nThe implicit Euler method is intended to illustrate methods for stiff equations. This system is not stiff, but let's try implicit Euler anyway. Implicit methods usually involve the solution of a nonlinear algebraic system at each step, but here the algebraic system is linear, so backslash does the job.\n\n$$(I - h A) \\ x(t+h) = x(t)$$\n\nAgain, all the components of the numerical solution are updated simultaneously. Now the trajectories spiral inward.\n\nclf axis(1.5*[-1 1 -1 1]) axis square bg = 'blue'; I = eye(2); A = [0 1; -1 0]; for h = [1/4 1/32] x = [0 1]'; line(x(1),x(2),'marker','o','color','k') for t = 0:h:6*pi x = (I - h*A)\\x; line(x(1),x(2),'marker','.','color',bg) end bg = [0 2/3 0]; end title('Implicit Euler')\n\nEigenvalues\n\nEigenvalues are the key to understanding the behavior of these three circle generators. Let's start with the explicit Euler. The trajectories are given by\n\n$$ x(t+h) = E x(t) $$\n\nwhere\n\n$$ E = I + h A = \\pmatrix{1 & h \\cr -h & 1} $$\n\nThe matrix $E$ is not symmetric. Its eigenvalues are complex, hence the circular behavior. The eigenvalues satisfy\n\n$$ \\lambda_1 = \\bar{\\lambda}_2 $$\n\n$$ \\lambda_1 + \\lambda_2 = \\mbox{trace} (E) = 2 $$\n\n$$ \\lambda_1 \\cdot \\lambda_2 = \\mbox{det} (E) = 1 + h^2 $$\n\nThe determinant is larger than 1 and the product of the eigenvalues is the determinant, so they must be outside the unit circle. The powers of the eigenvalues grow exponentially and hence so do the trajectories. We can reach this conclusion without actually finding the eigenvalues, even though that would be easy in this case.\n\nThe implicit Euler matrix is the inverse transpose of the explicit matrix.\n\n$$ x(t+h) = E^{-T} x(t) $$\n\nThe eigenvalues of $E^{-T}$ are the reciprocals of the eigenvalues of $E$, so they are inside the unit circle. Their powers decay exponentially and hence so do the trajectories.\n\nToday, the spacewar circle generator would be called \"semi-implicit\". Explicit Euler's method is used for one component, and implicit Euler for the other.\n\n$$\\pmatrix{1 & 0 \\cr h & 1} x(t+h) = \\pmatrix{1 & h \\cr 0 & 1} x(t)$$\n\nSo\n\n$$x(t+h) = S x(h)$$\n\nwhere\n\n$$S = \\pmatrix{1 & 0 \\cr h & 1}^{-1} \\pmatrix{1 & h \\cr 0 & 1} = \\pmatrix{1 & h \\cr -h & 1-h^2}$$\n\nThe eigenvalues satisfy\n\n$$ \\lambda_1 + \\lambda_2 = \\mbox{trace} (S) = 2 - h^2 $$\n\n$$ \\lambda_1 \\cdot \\lambda_2 = \\mbox{det} (S) = 1 $$\n\nThe key is the determinant. It is equal to 1, so we can conclude (without actually finding the eigenvalues)\n\n$$ |\\lambda_1| = |\\lambda_2| = 1$$\n\nThe powers $\\lambda_1^n$ and $\\lambda_2^n$ remain bounded for all $n$.\n\nIt turns out that if we define $\\theta$ by\n\n$$ \\cos{\\theta} = 1 - h^2/2 $$\n\nthen\n\n$$ \\lambda_1^n = \\bar{\\lambda}_2^n = e^{i n \\theta} $$\n\nIf, instead of an inverse power of 2, the step size $h$ happens to correspond to a value of $\\theta$ that is $2 \\pi / p$, where $p$ is an integer, then the spacewar circle produces only $p$ discrete points before it repeats itself.\n\nHow close does our circle generator come to actually generating circles? The matrix $S$ is not symmetric. Its eigenvectors are not orthogonal. This can be used to show that the generator produces ellipses. As the step size $h$ gets smaller, the ellipses get closer to circles. It turns out that the aspect ratio of the ellipse, which is the ratio of its major axis to its minor axis, is equal to the condition number of the matrix of eigenvectors.\n\nSymplectic Integrators\n\nSymplectic methods for the numerical solution of ordinary differential equations apply to the class of equations derived from conserved quantities known as Hamiltonians. The components of the solution belong to two subsets, $p$ and $q$, and the Hamiltonian is a function of these two components, $H(p,q)$. The differential equations are\n\n$$\\dot{p} = \\frac{\\partial H(p,q)}{\\partial q}$$\n\n$$\\dot{q} = -\\frac{\\partial H(p,q)}{\\partial p}$$\n\nFor our circle generator, $p$ and $q$ are the coordinates $x$ and $y$, and $H$ is one-half the square of the radius.\n\n$$H(x,y) = \\textstyle{\\frac{1}{2}}(x^2 + y^2)$$\n\nHamiltonian systems include models based on Newton's Second Law of Motion, $F = ma$. In this case $p$ is the position, $q$ is the velocity, and $H(p,q)$ is the energy.\n\nSymplectic methods are semi-implicit. They extend the idea of using the current value of $q$ to update $p$ and then using the new value of $p$ to update $q$. This makes it possible to conserve the value of $H(p,q)$, to within the order of accuracy of the method. The spacewar circle generator is a first order symplectic method. The radius is constant, to within an accuracy proportional to the step size $h$.\n\nFor other examples of symplectic methods, including the n-body problem of orbital mechanics, see the Orbits chapter and the orbits.m program of Experiments with MATLAB. Here is a screen shot showing the inner planets of the solar system.\n\nX = imread('http://blogs.mathworks.com/images/cleve/solar2.png'); imshow(X)\n\nSteve Russell certainly didn't know that his Spacewar was using a symplectic integrator. That term wasn't invented until years later. It is serendipity that the shortest machine language program has the best numerical properties.\n\nReferences\n\n[1] http://en.wikipedia.org/wiki/Spacewar!, Wikipedia article on Spacewar.\n\n[2] http://en.wikipedia.org/wiki/File:Steve_Russell_and_PDP-1.png, Steve Russell and the Computer History Museum's PDP-1 at the Vintage Computer Fair 2006.\n\nGet the MATLAB code\n\nPublished with MATLAB® 7.14\n\nWhat, exactly, is exponential growth? What is e and what does it have to do with exponential growth? A simple MATLAB interactive graphic introduces these concepts.\n\nContents\n\nExponential growth.\n\nTo most people \"exponential growth\" simply means \"very rapid growth\". But, more precisely, a time varying quantity grows expontially if the rate of growth is proportional to size of the quantity itself. The rate can even be negative, in which case it is \"exponential decay\".\n\nI think that students who have taken calculus in high school or college should understand the mathematical ideas involved in exponential growth, but I'm afraid that most of them don't. When I ask students to tell me the derivative of $t^3$, they can usually respond $3t^2$. When I ask them \"why?\", they say \"take the $3$, put it out in front, and subtract $1$ from the exponent\". Finding derivatives is a purely mechanical process, like adding fractions or solving quadratic equations. When I ask for the derivative of $3^t$, some will even apply the same process to get $t 3^{t-1}$. There is no understanding of the relationship between differentiation and rate of change.\n\nA function $f(t)$ is growing exponentially if its growth rate, its derivative, is proportional to the function itself. Perhaps the most important function in all of mathematics is the one where this proportionality constant is equal to one, so the function is its own derivative. Let's discover that function.\n\nApproximate derivative.\n\nWe can get numerical values and graphs of derivatives without actually differentiating anything. For our purposes, approximate derivatives based on the notion of rate of change are, in some ways, even preferable to actual derivatives. We just have to pick a small step size $h$, say $h = .0001$. Then the approximate derivative of $f(t)$ is\n\n$$ \\dot{f}(t) = \\frac{f(t+h)-f(t)}{h} $$\n\n2^t\n\nWhat do we mean by the function\n\n$$ f(t) = 2^t $$\n\nIf $t$ is a positive integer, then $2^t$ is $2$ multiplied by itself $t$ times.\n\n$$ 2^0 = 1, \\ \\ 2^1 = 2, \\ \\ 2^2 = 4, ... $$\n\nIf $t$ is a negative integer, then $2^t$ is $1/2$ multiplied by itself $|t|$ times.\n\n$$ 2^{-1} = 1/2, \\ \\ 2^{-2} = 1/4, ... $$\n\nIf $t = p/q$ is a rational number, the ratio of two integers, $2^{p/q}$ is the $q$-th root of the $p$-th power of $2$.\n\n$$ 2^{1/2} = \\sqrt{2} = 1.4142, \\ \\ 2^{355/113} = \\sqrt[113]{2^{355}} = 8.8250, ... $$\n\nTheoretically, for floating point arithmetic, this is all we need to know. All floating point numbers are ratios of two integers. We do not have to be concerned yet about the definition of $2^t$ for irrational $t$. If MATLAB can compute powers and roots, we can plot the graph of $2^t$.\n\nInteractive interface.\n\nThe function expgui is included with the software for the book Experiments with MATLAB. I invite you to download the function and run it. It plots the graph of $a^t$ and its approximate derivative. Here is the code that generates the initial plot, with $a = 2$. You can see that the derivative, in green, has the same shape as the function, in blue. This is exponential growth.\n\nt = 0:1/64:2; h = .0001; a = 2.0; y = a.^t; ydot = (a.^(t+h) - a.^t)/h; plot(t,[y; ydot]) axis([0 2 0 9]) fs = get(0,'defaulttextfontsize')+2; text(0.3,6.0,'a = 2.000','fontsize',fs,'fontweight','bold') title('y = a^t','fontsize',fs,'fontweight','bold') legend('y','dy/dt','location','northwest') xlabel('t') ylabel('y')\n\nAnimation.\n\nAt this point, if you are actually running expgui, you can move the blue line with your mouse, changing the value of $a$. If you don't have MATLAB, or haven't downloaded expgui, you can click on this movie to see a simulation of the animation. I hope you get to move the line yourself with expgui. The tactile experience is much more satisfying that just watching the movie.\n\npi^t\n\nIn case you are not able to run expgui or watch the movie, here is the plot of $\\pi^t$ and its approximate derivative.\n\na = pi; y = a.^t; ydot = (a.^(t+h) - a.^t)/h; p = get(gca,'children'); set(p(3),'ydata',y) set(p(2),'ydata',ydot) set(p(1),'string','a = 3.142')\n\nFinding e.\n\nYou should soon see that the graph of the derivative of $a^t$ always has the same shape as the graph of $a^t$ itself. If $a$ is less than $2.7$ the derivative is below the function, while if $a$ is greater than $2.8$ the derivative is above the function. By moving the mouse carefully you can find a value in between where the curves lie on top of each other, The critical value of $a$ is 2.718. You have discovered $e$ and $e^t$, the only function in the world that is equal to its own derivative. And, you didn't have to differentiate anything. Here is the final graph.\n\ny = exp(t); p = get(gca,'children'); set(p(3),'ydata',y) set(p(2),'ydata',y) set(p(1),'string','a = 2.718')\n\ne^t\n\nIn contrast to its equally famous cousin, $\\pi$, the actual numerical value of $e$ is not so important. It's the function $e^t$, or exp(t) as it's known in MATLAB, that is fundamental. If you ever need to know the value of $e$, you can always use\n\nformat long e = exp(1)\n\ne = 2.718281828459046\n\nIt's pretty easy to memorize the first ten significant figures.\n\nfprintf('e = %12.9f\\n',e)\n\ne = 2.718281828\n\nGet the MATLAB code\n\nPublished with MATLAB® 7.14\n\nAn esoteric fact about matrices is that any real matrix can be written as the product of two symmetric matrices. I've known about this fact for years, but never seriously explored the computational aspects. So I'm using this post to clarify my own understanding of what I'll call the symmetric pair decomposition. It turns out that there are open questions. I don't think we know how to reliably compute the factors. But I also have to admit that, even if we could compute them, I don't know of any practical use.\n\nContents\n\nTheorem\n\nNot many people know about this theorem.\n\nTheorem: Any real matrix is equal to the product of two real symmetric matrices.\n\nAlmost Proof\n\nAt first glance this theorem has nothing to do with eigenvalues. But here is the beginning of a proof, and a possible algorithm. Suppose that a real matrix $A$ has real, distinct eigenvalues. Then it can be diagonalized by the matrix $V$ of its eigenvectors.\n\n$$A = V D V^{-1}$$\n\nBecause we are assuming there are no multiple eigenvalues, the matrix $V$ exists and is nonsingular, and the matrix $D$ is real and diagonal. Let\n\n$$S_1 = V D V^T$$\n\n$$S_2 = V^{-T} V^{-1}$$\n\nThen $S_1$ and $S_2$ are real, symmetric, and their product is\n\n$$S_1 S_2 = A$$\n\nThis argument is not a proof. It just makes the theorem plausible. The challenge comes when the matrix has repeated eigenvalues and lacks a full set of eigenvectors, so it cannot be diagonalized. A complete proof would transform the matrix to its Rational Canonical Form or its Jordan Canonical form and construct explicit symmetric factors for the blocks in the canonical form.\n\nDegrees of Freedom\n\nIf $A$ is $n$ -by- $n$ and\n\n$$A = S_1 S_2$$\n\nwhere each of the symmetric matrices has $n(n+1)/2$ independent elements, then this is $n^2$ nonlinear equations in $n^2+n$ unknowns. It looks like there could be an $n$-parameter family of solutions. In my almost proof, each eigenvector is determined only up to a scale factor. These $n$ scale factors show up in $S_1$ and $S_2$ in complicated, nonlinear ways. I suspect that allowing complex scale factors parameterizes the complete set of solutions, but I'm not sure.\n\nMoler's Rules\n\nMy two Golden Rules of computation are:\n\nThe hardest things to compute are things that do not exist.\n\nThe next hardest things to compute are things that are not unique.\n\nFor the symmetric pair decomposition, our obscure theorem says the decomposition exists, but the degrees of freedom observation says it is probably not unique. Worse yet, the only algorithm we have requires a full set of eigenvectors, which may not exist. We will have to worry about these things.\n\nLU Decomposition\n\nThe most important decomposition in numerical linear algebra, the one we use to solve systems of simultaneous linear equations, is the LU decomposition. It expresses a permuted matrix as the product of two triangular factors.\n\n$$P A = L U$$\n\nThe permutation matrix $P$ gives us existence and numerical stability. Putting ones on the diagonal of $L$ eliminates $n$ degrees of freedom and give us uniqueness.\n\nMagic Square\n\nOur first example involves one of my favorite matrices.\n\nA = magic(3)\n\nA = 8 1 6 3 5 7 4 9 2\n\nUse the Symbolic Toolbox to compute the eigenvalues and vectors exactly.\n\n[V,D] = eig(sym(A))\n\nV = [ (2*6^(1/2))/5 - 7/5, - (2*6^(1/2))/5 - 7/5, 1] [ 2/5 - (2*6^(1/2))/5, (2*6^(1/2))/5 + 2/5, 1] [ 1, 1, 1] D = [ -2*6^(1/2), 0, 0] [ 0, 2*6^(1/2), 0] [ 0, 0, 15]\n\nNotice that the elements of V and D involve $\\sqrt{6}$ and so are irrational. Now let\n\nS1 = simplify(V*D*V') S2 = simplify(inv(V*V'))\n\nS1 = [ 1047/25, -57/25, 27/5] [ -57/25, 567/25, 123/5] [ 27/5, 123/5, 15] S2 = [ 3/16, 1/12, 1/16] [ 1/12, 1/2, -1/4] [ 1/16, -1/4, 25/48]\n\nThe $\\sqrt{6}$ has disappeared. You can see that S1 and S2 are symmetric, have rational entries, and, as advertised, their product is\n\nProduct = S1*S2\n\nProduct = [ 8, 1, 6] [ 3, 5, 7] [ 4, 9, 2]\n\nLet's play with the scale factors a bit. I particularly like\n\nV(:,3) = 2 S1 = simplify(V*D*V')/48 S2 = 48*simplify(inv(V*V'))\n\nV = [ (2*6^(1/2))/5 - 7/5, - (2*6^(1/2))/5 - 7/5, 2] [ 2/5 - (2*6^(1/2))/5, (2*6^(1/2))/5 + 2/5, 2] [ 1, 1, 2] S1 = [ 181/100, 89/100, 21/20] [ 89/100, 141/100, 29/20] [ 21/20, 29/20, 5/4] S2 = [ 5, 0, -1] [ 0, 20, -16] [ -1, -16, 21]\n\nNow S1 has decimal fraction entries, and S2 has integer entries, including two zeros. Let's leave the symbolic world.\n\nS1 = double(S1) S2 = double(S2) Product = S1*S2\n\nS1 = 1.8100 0.8900 1.0500 0.8900 1.4100 1.4500 1.0500 1.4500 1.2500 S2 = 5 0 -1 0 20 -16 -1 -16 21 Product = 8 1 6 3 5 7 4 9 2\n\nI can't promise to get such pretty results with other examples.\n\nNearly Defective\n\nSuppose I want to compute the symmetric pair decomposition of this perturbation of a Jordan block.\n\ne = sym('e','positive'); A = [2 1 0 0; 0 2 1 0; 0 0 2 1; e 0 0 2]\n\nA = [ 2, 1, 0, 0] [ 0, 2, 1, 0] [ 0, 0, 2, 1] [ e, 0, 0, 2]\n\nHere is the eigenvalue decomposition. The vectors have been scaled so that the last component is equal to 1. The eigenvalues are located on a circle in the complex plane centered at 2, with a radius of e^(1/4), which is the signature of an eigenvalue of multiplicity 4.\n\n[V,D] = eig(A); V = simplify(V) D = simplify(D)\n\nV = [ -i/e^(3/4), i/e^(3/4), 1/e^(3/4), -1/e^(3/4)] [ -1/e^(1/2), -1/e^(1/2), 1/e^(1/2), 1/e^(1/2)] [ i/e^(1/4), -i/e^(1/4), 1/e^(1/4), -1/e^(1/4)] [ 1, 1, 1, 1] D = [ 2 - e^(1/4)*i, 0, 0, 0] [ 0, e^(1/4)*i + 2, 0, 0] [ 0, 0, e^(1/4) + 2, 0] [ 0, 0, 0, 2 - e^(1/4)]\n\nHere is the symmetric pair decomposition resulting from this eigenvalue decomposition.\n\nS1 = simplify(V*D*V.') S2 = simplify(inv(V*V.'))\n\nS1 = [ 0, 4/e, 8/e, 0] [ 4/e, 8/e, 0, 0] [ 8/e, 0, 0, 4] [ 0, 0, 4, 8] S2 = [ 0, 0, e/4, 0] [ 0, e/4, 0, 0] [ e/4, 0, 0, 0] [ 0, 0, 0, 1/4]\n\nWell, this sort of does the job. S1 and S2 are symmetric and their product is equal to A.\n\nProduct = S1*S2\n\nProduct = [ 2, 1, 0, 0] [ 0, 2, 1, 0] [ 0, 0, 2, 1] [ e, 0, 0, 2]\n\nBut I am worried that the factors are very badly scaled. As I make e smaller, the large elements in S1 get larger, and the small elements in S2 get smaller. The decomposition breaks down.\n\nA Better Decomposition\n\nA better decomposition is just a rotation. These two matrices are symmetric and their product is A.\n\nS2 = sym(rot90(eye(size(A)))) S1 = A/S2 Product = S1*S2\n\nS2 = [ 0, 0, 0, 1] [ 0, 0, 1, 0] [ 0, 1, 0, 0] [ 1, 0, 0, 0] S1 = [ 0, 0, 1, 2] [ 0, 1, 2, 0] [ 1, 2, 0, 0] [ 2, 0, 0, e] Product = [ 2, 1, 0, 0] [ 0, 2, 1, 0] [ 0, 0, 2, 1] [ e, 0, 0, 2]\n\nCan I reproduce this decomposition by rescaling the eigenvectors? Here is code that uses the symbolic solve function to compute new scale factors. If you want to see how it works, download this M-file using the link at the end of this post, remove the semicolons in this section, and run or publish it again.\n\ns = sym('s',[4,1]); V = V*diag(s); T = simplify(inv(V*V.')); soln = solve(T(:,1)-S2(:,1)); s = [soln.s1(1); soln.s2(1); soln.s3(1); soln.s4(1)]\n\ns = (e^(3/4)*i)^(1/2)/2 (-e^(3/4)*i)^(1/2)/2 e^(3/8)/2 (-e^(3/4))^(1/2)/2\n\nThese scale factors are complex numbers with magnitude e^(3/8)/2. Let's rescale the eigenvectors. Of course, the eigenvalues don't change.\n\n[V,D] = eig(A); V = simplify(V*diag(s))\n\nV = [ -(-1)^(3/4)/(2*e^(3/8)), (-1)^(1/4)/(2*e^(3/8)), 1/(2*e^(3/8)), -i/(2*e^(3/8))] [ -(-1)^(1/4)/(2*e^(1/8)), -1/(-1)^(1/4)/(2*e^(1/8)), 1/(2*e^(1/8)), i/(2*e^(1/8))] [ ((-1)^(3/4)*e^(1/8))/2, -((-1)^(1/4)*e^(1/8))/2, e^(1/8)/2, -(e^(1/8)*i)/2] [ ((-1)^(1/4)*e^(3/8))/2, (1/(-1)^(1/4)*e^(3/8))/2, e^(3/8)/2, (e^(3/8)*i)/2]\n\nNow these eigenvectors produce the same stable decomposition as the rotation.\n\nS1 = simplify(V*D*V.') S2 = simplify(inv(V*V.'))\n\nS1 = [ 0, 0, 1, 2] [ 0, 1, 2, 0] [ 1, 2, 0, 0] [ 2, 0, 0, e] S2 = [ 0, 0, 0, 1] [ 0, 0, 1, 0] [ 0, 1, 0, 0] [ 1, 0, 0, 0]\n\nCan carefully choosing the scaling of the eigenvectors be the basis for a sound numerical algorithm? I doubt it. We're still trying to compute something that is not unique, using factors that almost do not exist. It's pretty shaky.\n\nGet the MATLAB code\n\nPublished with MATLAB® 7.14\n\nWe all know that Friday the 13th is unlucky, but is it unlikely?\n\nContents\n\nYear 2012\n\nI plan to post this article during the second week of July, 2012. The Friday in this week is a Friday the 13th, the third we've had so far this year. There were also ones in January and April. That seems like a lot. How often do we have three Friday the 13ths in the first seven months of a year? Well, it's not all that often. It usually happens only once every 28 years. The next time will be the year 2040. But sometimes, around the turn of centuries, it happens twice in 12 years. I mention all this to establish that our calendar does not have a simple periodic behavior. By the way, not to worry, after this week, it will be 14 months until the next Friday the 13th, in September, 2013.\n\nFriday the 13th\n\nWhich brings us to the central topic of this post:\n\nWhat is the probability that the 13th of a month falls on a Friday?\n\nAn obvious response is\n\nEasy question, the probability is 1/7.\n\nAfter all, there are seven days in a week and the 13th of a month is equally likely to fall on any one of them. Well, as we shall see, that's close, but not exactly right.\n\nCalendars and Leap Years\n\nLeap years make our calendar a nontrivial mathematical object. The leap year rule can be implemented by this anonymous function.\n\nleapyear = @(y) mod(y,4)==0 & mod(y,100)~=0 | mod(y,400)==0;\n\nThis says that leap years happen every four years, except the turn of a century not divisible by 400 is skipped. Try a few year numbers.\n\ny = [2012 2013 2000 2100]'; disp([y leapyear(y)])\n\n2012 1 2013 0 2000 1 2100 0\n\nSo, this year is a leap year, next year is not, 2000 was a leap year, 2100 is not.\n\nThe leap year rule implies that our calendar has a period of 400 years. The calendar from 1601 to 2000 is being reused from 2001 to 2400. (Except the Gregorian calendar had not been invented in 1601, so I'm talking about the calendar that would have been used back then if they could have used today's calendar, but never mind.)\n\nIn a 400 year period, there are 97 leap years, 4800 months, 20871 weeks, and 146097 days. So the average number of days in a calendar year is not 365.25, but\n\nformat short dpy = 365+97/400\n\ndpy = 365.2425\n\nWe can compute the probability that the 13th of a month is a Friday by counting how many times that happens in 4800 months. The correct probability is then that count divided by 4800. Since 4800 is not divisible by 7, the probability does not reduce to 1/7.\n\nClock\n\nMATLAB has a number of functions for doing computations involving calendars and dates. Many of these functions are in the MATLAB Toolbox, but some of the more specialized ones are in the Finance Toolbox. We encountered a few of these functions in my blog about biorhythms. The basis for all the functions is clock, which reads the system's clock and returns a 6-element vector\n\n[year, month, date, hour, minute, seconds]\n\nThe first five elements have integer values. The sixth element has a fractional part whose accuracy depends upon the computer's internal clock. Here is the output generated when I publish this blog.\n\nc = clock; fprintf('clock = [ %4d %4d %5d %5d %5d %8.3f ]\\n',c)\n\nclock = [ 2012 7 5 11 53 14.258 ]\n\nDatenum\n\nThe datenum function facilitates computations involving calendars by collapsing the clock vector into one value, the serial date number. This value is the number of days, and fractions of a day, since a reference time 20 centuries ago when clock would have been all zeros. Here are a couple of examples of the use of datenum. If you run this code yourself, your results should be different.\n\nt = now; fprintf('current_date_number = %10.3f\\n',t) date_string = datestr(t) tday = fix(t) tday_string = datestr(tday) [week_day,week_day_name] = weekday(tday)\n\ncurrent_date_number = 735055.495 date_string = 05-Jul-2012 11:53:14 tday = 735055 tday_string = 05-Jul-2012 week_day = 5 week_day_name = Thu\n\nCalendar number\n\nThe calendar for any year is determined by two pieces of information, the weekday of January 1st and whether or not the year is a leap year. So we need only 14 calendars. We could number all possible calendars, with the units digit specifying the starting week day and the tens digits indicating leap years. The 14 numbers would be [1:7 11:17].\n\ncalendar_number = @(y) weekday(datenum(y,1,1)) + 10*leapyear(y);\n\nIf the calendar industry used this numbering scheme, here are the calendars you would need for the next 21 years.\n\ny = (2012:2032)'; disp([y calendar_number(y)])\n\n2012 11 2013 3 2014 4 2015 5 2016 16 2017 1 2018 2 2019 3 2020 14 2021 6 2022 7 2023 1 2024 12 2025 4 2026 5 2027 6 2028 17 2029 2 2030 3 2031 4 2032 15\n\nFriday the 13th is likely\n\nWe are now ready to use the weekday function to count the number of times in a 400-year calendar cycle that the 13th of a month occurs on each of the various days of the week.\n\nc = zeros(1,7); for y = 1601:2000 for m = 1:12 d = datenum([y,m,13]); w = weekday(d); c(w) = c(w) + 1; end end c\n\nc = 687 685 685 687 684 688 684\n\nA bar graph, with a line at a probability of 1/7, and week day axis labels.\n\nbar(c) axis([0 8 680 690]) avg = 4800/7; line([0 8], [avg avg],'linewidth',4,'color','black') set(gca,'xticklabel',{'Su','M','Tu','W','Th','F','Sa'})\n\nThe probability for Friday is\n\np = c(6)/4800; fprintf('p = %8.6f\\n',p) fprintf('1/7 = %8.6f\\n',1/7)\n\np = 0.143333 1/7 = 0.142857\n\nSo, the 13th of a month is more likely to occur on Friday that any other day of the week. Only slightly more likely, I admit, but still ...\n\nGet the MATLAB code\n\nPublished with MATLAB® 7.14\n\nMATLAB has two different functions for piecewise cubic interpolation, spline and pchip. Why are there two? How do they compare?\n\nContents\n\nData\n\nHere is the data that I will use in this post.\n\nx = 1:6 y = [16 18 21 17 15 12]\n\nx = 1 2 3 4 5 6 y = 16 18 21 17 15 12\n\nHere is a plot of the data.\n\nset(0,'defaultlinelinewidth',2) clf plot(x,y,'-o') axis([0 7 7.5 25.5]) title('plip')\n\nplip\n\nWith line type '-o', the MATLAB plot command plots six 'o's at the six data points and draws straight lines between the points. So I added the title plip because this is a graph of the piecewise linear interpolating polynomial. There is a different linear function between each pair of points. Since we want the function to go through the data points, that is interpolate the data, and since two points determine a line, the plip function is unique.\n\nThe PCHIP Family\n\nA PCHIP, a Piecewise Cubic Hermite Interpolating Polynomial, is any piecewise cubic polynomial that interpolates the given data, AND has specified derivatives at the interpolation points. Just as two points determine a linear function, two points and two given slopes determine a cubic. The data points are known as \"knots\". We have the y-values at the knots, so in order to get a particular PCHIP, we have to somehow specify the values of the derivative, y', at the knots.\n\nConsider these two cubic polynomials in $x$ on the interval $1 \\le x \\le 2$ . These functions are formed by adding cubic terms that vanish at the end points to the linear interpolatant. I'll tell you later where the coefficients of the cubics come from.\n\n$$ s(x) = 16 + 2(x-1) + \\textstyle{\\frac{49}{18}}(x-1)^2(x-2) - \\textstyle{\\frac{89}{18}}(x-1)(x-2)^2 $$\n\n$$ p(x) = 16 + 2(x-1) + \\textstyle{\\frac{2}{5}}(x-1)^2(x-2) - \\textstyle{\\frac{1}{2}}(x-1)(x-2)^2 $$\n\nThese functions interpolate the same values at the ends.\n\n$$ s(1) = 16, \\ \\ \\ s(2) = 18 $$\n\n$$ p(1) = 16, \\ \\ \\ p(2) = 18 $$\n\nBut they have different first derivatives at the ends. In particular, $s'(1)$ is negative and $p'(1)$ is positive.\n\n$$ s'(1) = - \\textstyle{\\frac{53}{18}}, \\ s'(2) = \\textstyle{\\frac{85}{18}} $$\n\n$$ p'(1) = \\textstyle{\\frac{3}{2}}, \\ \\ \\ p'(2) = \\textstyle{\\frac{12}{5}} $$\n\nHere's a plot of these two cubic polynomials. The magenta cubic, which is $p(x)$, just climbs steadily from its initial value to its final value. On the other hand, the cyan cubic, which is $s(x)$, starts off heading in the wrong direction, then has to hurry to catch up.\n\nx = 1:1/64:2; s = 16 + 2*(x-1) + (49/18)*(x-1).^2.*(x-2) - (89/18)*(x-1).*(x-2).^2; p = 16 + 2*(x-1) + (2/5)*(x-1).^2.*(x-2) - (1/2)*(x-1).*(x-2).^2; clf axis([0 3 15 19]) box on line(x,s,'color',[0 3/4 3/4]) line(x,p,'color',[3/4 0 3/4]) line(x(1),s(1),'marker','o','color',[0 0 3/4]) line(x(end),s(end),'marker','o','color',[0 0 3/4])\n\nIf we piece together enough cubics like these to produce a piecewise cubic that interpolates many data points, we have a PCHIP. We could even mix colors and still have a PCHIP. Clearly, we have to be specific when it comes to specifying the slopes.\n\nOne possibility that might occur to you briefly is to use the slopes of the lines connecting the end points of each segment. But this choice just produces zeros for the coefficients of the cubics and leads back to the piecewise linear interpolant. After all, a linear function is a degenerate cubic. This illustrates the fact that the PCHIP family includes many functions.\n\nspline\n\nBy far, the most famous member of the PCHIP family is the piecewise cubic spline. All PCHIPs are continuous and have a continuous first derivative. A spline is a PCHIP that is exceptionally smooth, in the sense that its second derivative, and consequently its curvature, also varies continuously. The function derives its name from the flexible wood or plastic strip used to draw smooth curves.\n\nStarting about 50 years ago, Carl de Boor developed much of the basic theory of splines. He wrote a widely adopted package of Fortran software, and a widely cited book, for computations involving splines. Later, Carl authored the MATLAB Spline Toolbox. Today, the Spline Toolbox is part of the Curve Fitting Toolbox.\n\nWhen Carl began the development of splines, he was with General Motors Research in Michigan. GM was just starting to use numerically controlled machine tools. It is essential that automobile parts have smooth edges and surfaces. If the hood of a car, say, does not have continuously varying curvature, you can see wrinkles in the reflections in the show room. In the automobile industry, a discontinuous second derivative is known as a \"dent\".\n\nThe requirement of a continuous second derivative leads to a set of simultaneous linear equations relating the slopes at the interior knots. The two end points need special treatment, and the default treatment has changed over the years. We now choose the coefficients so that the third derivative does not have a jump at the first and last interior knots. Single cubic pieces interpolate the first three, and the last three, data points. This is known as the \"not-a-knot\" condition. It adds two more equations to set of equations at the interior points. If there are n knots, this gives a well-conditioned, almost symmetric, tridiagonal $n$ -by- $n$ linear system to solve for the slopes. The system can be solved by the sparse backslash operator in MATLAB, or by a custom, non-pivoting tridiagonal solver. (Other end conditions for splines are available in the Curve Fitting Toolbox.)\n\nAs you probably realized, the cyan function $s(x)$ introduced above, is one piece of the spline interpolating our sample data. Here is a graph of the entire function, produced by interpgui from NCM, Numerical Computing with MATLAB.\n\nx = 1:6; y = [16 18 21 17 15 12]; interpgui(x,y,3)\n\nsppchip\n\nI just made up that name, sppchip. It stands for shape preserving piecewise cubic Hermite interpolating polynomial. The actual name of the MATLAB function is just pchip. This function is not as smooth as spline. There may well be jumps in the second derivative. Instead, the function is designed so that it never locally overshoots the data. The slope at each interior point is taken to be a weighted harmonic mean of the slopes of the piecewise linear interpolant. One-sided slope conditions are imposed at the two end points. The pchip slopes can be computed without solving a linear system.\n\npchip was originally developed by Fred Fritsch and his colleagues at Lawrence Livermore Laboratory around 1980. They described it as \"visually pleasing\". Dave Kahaner, Steve Nash and I included some of Fred's Fortran subroutines in our 1989 book, Numerical Methods and Software. We made pchip part of MATLAB in the early '90s.\n\nHere is a comparison of spline and pchip on our data. In this case the spline overshoot on the first subinterval is caused by the not-a-knot end condition. But with more data points, or rapidly varying data points, interior overshoots are possible with spline.\n\ninterpgui(x,y,3:4)\n\nspline vs. pchip\n\nHere are eight subplots comparing spline and pchip on a slightly larger data set. The first two plots show the functions $s(x)$ and $p(x)$. The difference between the functions on the interior intervals is barely noticeable. The next two plots show the first derivatives. You can see that the first derivative of spline, $s'(x)$, is smooth, while the first derivative of pchip, $p'(x)$, is continuous, but shows \"kinks\". The third pair of plots are the second derivatives. The spline second derivative $s''(x)$ is continuous, while the pchip second derivative $p''(x)$ has jumps at the knots. The final pair are the third derivatives. Because both functions are piecewise cubics, their third derivatives, $s'''(x)$ and $p'''(x)$, are piecewise constant. The fact that $s'''(x)$ takes on the same values in the first two intervals and the last two intervals reflects the \"not-a-knot\" spline end conditions.\n\nsplinevspchip\n\nLocality\n\npchip is local. The behavior of pchip on a particular subinterval is determined by only four points, the two data points on either side of that interval. pchip is unaware of the data farther away. spline is global. The behavior of spline on a particular subinterval is determined by all of the data, although the sensitivity to data far away is less than to nearby data. Both behaviors have their advantages and disadvantages.\n\nHere is the response to a unit impulse. You can see that the support of pchip is confined to the two intervals surrounding the impulse, while the support of spline extends over the entire domain. (There is an elegant set of basis functions for cubic splines known as B-splines that do have compact support.)\n\nx = 1:8; y = zeros(1,8); y(4) = 1; interpgui(x,y,3:4)\n\ninterp1\n\nThe interp1 function in MATLAB, has several method options. The 'linear', 'spline', and 'pchip' options are the same interpolants we have been discussing here. We decided years ago to make the 'cubic' option the same as 'pchip' because we thought the monotonicity property of pchip was generally more desirable than the smoothness property of spline.\n\nThe 'v5cubic' option is yet another member of the PCHIP family, which has been retained for compatibility with version 5 of MATLAB. It requires the x's to be equally spaced. The slope of the v5 cubic at point $x_n$ is $(y_{n+1} - y_{n-1})/2$. The resulting piecewise cubic does not have a continuous second derivative and it does not always preserve shape. Because the abscissa are equally spaced, the v5 cubic can be evaluated quickly by a convolution operation.\n\nHere is our example data, modified slightly to exaggerate behavior, and interpgui modified to include the 'v5cubic' option of interp1. The v5 cubic is the black curve between spline and pchip.\n\nx = 1:6; y = [16 18 21 11 15 12]; interpgui_with_v5cubic(x,y,3:5)\n\nResources\n\nA extensive collection of tools for curve and surface fitting, by splines and many other functions, is available in the Curve Fitting Toolbox.\n\ndoc curvefit\n\n\"NCM\", Numerical Computing with MATLAB, has more mathematical details. NCM is available online. Here is the interpolation chapter. Here is interpgui. SIAM publishes a print edition.\n\nHere are the script splinevspchip.m and the modified version of interpgui interpgui_with_v5cubic.m that I used in this post.\n\nGet the MATLAB code\n\nPublished with MATLAB® 7.14\n\nI have been interested in the computation of the matrix exponential, $e^A$, for a long time. A recent query from a user provides a provocative example.\n\nContents\n\nNineteen Dubious Ways\n\nIn 1978, Charlie Van Loan and I published a paper in SIAM Review entitled \"Nineteen Dubious Ways to Compute the Exponential of a Matrix\". The paper does not pick a \"best of the 19\", but cautiously suggests that the \"scaling and squaring\" algorithm might be OK. This was about the time I was tinkering with the first MATLAB and consequently every version of MATLAB has had an expm function, based on scaling and squaring. The SIAM Review paper proved to be very popular and in 2003 we published a followup, \"Nineteen Dubious Ways ..., Twenty-Five Years Later\". A PDF is available from Charlie's web site.\n\nOur colleague Nick Higham reconsided the matrix exponential in 2005. Nick did a careful error analysis of scaling and squaring, improved the efficiency of the algorithm, and wrote a paper for the SIAM Journal on Numerical Analysis, \"The scaling and squaring method for the matrix exponential revisited\". A PDF is available from the University of Manchester's web site. The current version of expm in MATLAB is Nick's implementation of scaling and squaring.\n\nA more recent review of Nick's work on the matrix exponential is provided by these slides for a talk he gave at a meeting in Rome in 2008.\n\nA Query from a User\n\nA few weeks ago, MathWorks Tech Support received a query from a user about the following matrix. Note that the elements of A range over 18 orders of magnitude.\n\nformat long g a = 2e10; b = 4e8/6; c = 200/3; d = 3; e = 1e-8; A = [0 e 0; -(a+b) -d a; c 0 -c]\n\nA = 0 1e-08 0 -20066666666.6667 -3 20000000000 66.6666666666667 0 -66.6666666666667\n\nThe computed matrix exponential has huge elements.\n\nE = expm(A)\n\nE = 1.7465684381715e+17 -923050477.783131 -1.73117355055901e+17 -3.07408665108297e+25 1.62463553675545e+17 3.04699053651329e+25 1.09189154376804e+17 -577057840.468934 -1.08226721572342e+17\n\nThe report claimed that the right answer, obtained from a MATLAB competitor, differs from E by many orders of magnitude.\n\n[ 0.446849, 1.54044*10^-9, 0.462811, -5.74307*10^6, -0.015283, -4.52654*10^6 0.447723, 1.5427*10^-9, 0.463481]\n\nans = 0.446849 1.54044e-09 0.462811 -5743070 -0.015283 -4526540 0.447723 1.5427e-09 0.463481\n\nSymbolic\n\nLet's generate the symbolic representation of A.\n\na = sym(2e10); b = sym(4e8)/6; c = sym(200)/3; d = sym(3); e = sym(1e-8); S = [0 e 0; -(a+b) -d a; c 0 -c]\n\nS = [ 0, 1/100000000, 0] [ -60200000000/3, -3, 20000000000] [ 200/3, 0, -200/3]\n\nNow have the Symbolic Toolbox compute the matrix exponential, then convert the result to floating point. We can regard this as the \"right answer\". We see that it agrees with the user's expectations.\n\nX = real(double(expm(S)))\n\nX = 0.446849468283175 1.54044157383952e-09 0.462811453558774 -5743067.77947947 -0.0152830038686819 -4526542.71278401 0.447722977849494 1.54270484519591e-09 0.463480648837651\n\nClassic MATLAB\n\nI ran my old Fortran MATLAB from 1980. Here is the output. It got the right answer.\n\nThe Three Demos\n\nIn addition to expm, MATLAB has for many years provided three demo functions that illustrate popular methods for computing $e^A$. The function expmdemo1 is a MATLAB implementation of the scaling and squaring algorithm that was used in the builtin expm before Higham's improvements. The function expmdemo2 implements the Taylor power series that is often the definition of $e^A$, but which is one of the worst of the nineteen ways because it is slow and numerically unreliable. The function expmdemo3 uses eigenvalues and eigenvectors, which is OK only if the eigenvector matrix is well conditioned. (MATLAB also has a function expm1, which computes the scalar function $e^x\\!-\\!1$ without computing $e^x$. The m in the name is for minus, not matrix. This function has nothing to do with the matrix exponential.)\n\nLet's see what the three demo functions do with our example.\n\nE1 = expmdemo1(A)\n\nE1 = 0.446848323199335 1.54043901480671e-09 0.462810666904014 -5743177.01871262 -0.0152833835375292 -4526656.46142213 0.447721814330828 1.54270222301338e-09 0.46347984316225\n\nE2 = expmdemo2(A)\n\nE2 = -3627968682.81884 0.502451507654604 -3062655286.68657 -1.67974375988037e+19 3498209047.28622 -2.27506724048955e+19 15580992163.692 7.53393732504015 4987630142.66227\n\nE3 = expmdemo3(A)\n\nE3 = 0.446849468283181 1.54044157383954e-09 0.462811453558778 -5743067.77947891 -0.01528300386868 -4526542.71278343 0.4477229778495 1.54270484519593e-09 0.463480648837654\n\nYou can see that both eigdemo1, the outdated scaling and squaring, and eigdemo3, eigenvectors, get the right answer, while eigdemo2, Taylor series, blows up.\n\nScaling, Squaring, and Pade Approximations\n\nIn outline, the scaling and squaring algorithm for computing $e^A$ is:\n\nPick an integer $s$ and $\\sigma = 2^s$ so that $||A/\\sigma|| \\approx 1$.\n\nFind a Pade approximation, $P \\approx \\mbox{exp}(A/\\sigma)$.\n\nUse repeated squaring to compute $e^A \\approx P^\\sigma$.\n\nWe have two implementations of scaling and squaring, the outdated one in expmdemo1 and the current one in expm. It turns out that, for this matrix, the old implementation decides the scale factor should be 2^37 while the current implementation chooses 2^32. Using the new scale factor will save five matrix multiplications in the unscaling by repeated squaring.\n\nThe key to the comparison of these two implementations lies in the eigenvalues of the Pade approximants.\n\nP = expmdemo1(A/2^37); e = eig(P) P = expm(A/2^32); e = eig(P)\n\ne = 0.999999999539043 0.999999999954888 0.999999999999176 e = 0.999999974526488 1.00000000930672 0.999999999946262\n\nIn this case, the old code produces eigenvalues that are less than one. Powers of these eigenvalues, and hence powers of P, remain bounded. But the current code happens to produce an eigenvalue slightly larger than one. The powers of e and of P blow up.\n\ne.^(2^32)\n\nans = 3.05317714952674e-48 2.28895048607366e+17 0.793896973586281\n\nBalancing\n\nOne cure, at least in this instance, is balancing. Balancing is a diagonal similarity transformation that tries to make the matrix closer to symmetric by making the row norms equal to the column norms. This may improve the accuracy of computed eigenvalues, but seriously alter the eigenvectors. The MATLAB documentation has a good discussion of the effect of balancing on eigenvectors.\n\nBalancing the Exponential\n\nBalancing can have sometimes have a beneficial effect in the computation of $e^A$. For the example in this blog the elements of the diagonal similarity transform are powers of 2 that vary over a wide range.\n\n[T,B] = balance(A); T log2T = diag(log2(diag(T)))\n\nT = 9.5367431640625e-07 0 0 0 2048 0 0 0 1.9073486328125e-06 log2T = -20 0 0 0 11 0 0 0 -19\n\nIn the balanced matrix $B = T^{-1} A T$, the tiny element $a_{1,2}=10^{-8}$ has been magnified to be comparable with the other elements.\n\nB\n\nB = 0 21.47483648 0 -9.3442698319753 -3 18.6264514923096 33.3333333333333 0 -66.6666666666667\n\nComputing the $e^B$ presents no difficulties. The final result obtained by reversing the scaling is what we have come to expect for this example.\n\nM = T*expm(B)/T\n\nM = 0.446849468283175 1.54044157383952e-09 0.462811453558774 -5743067.77947947 -0.0152830038686819 -4526542.712784 0.447722977849495 1.54270484519591e-09 0.46348064883765\n\nCondition\n\nNick Higham has contributed his Matrix Function Toolbox to MATLAB Central. The Toolbox has many useful functions, including expm_cond, which computes the condition number of the matrix exponential function. Balancing improves the conditioning of this example by 16 orders of magnitude.\n\naddpath('../../MFToolbox') expm_cond(A) expm_cond(B)\n\nans = 3.16119437847582e+18 ans = 238.744549689702\n\nShould We Use Balancing?\n\nBob Ward, in the original 1977 paper on scaling and squaring, recommended balancing. Nick Higham includes balancing in the pseudocode algorithm in his 2005 paper, but in recent email with me he was reluctant to recommend it. I am also reluctant to draw any conclusions from this one case. Its scaling is too bizarre. Besides, there is a better solution, avoid overscaling.\n\nOverscaling\n\nIn 2009 Nick Higham's Ph. D. Student, Awad H. Al-Mohy, wrote a dissertation entitled \"A New Scaling and Squaring Algorithm for the Matrix Exponential\". The dissertation described a MATLAB function expm_new. A PDF of the dissertation and a zip file with the code are available from the University of Manchester's web site.\n\nIf $A$ is not a normal matrix, then the norm of the power, $||A^k||$, can grow much more slowly than the power of the norm, $||A||^k$. As a result, it is possible to suffer significant roundoff error in the repeated squaring. The choice of the scale factor $2^s$ involves a delicate compromise between the accuracy of the Pade approximation and the number of required squarings.\n\nHere is one experiment with our example and various choices of $s$. The function padexpm is the order 13 Pade approximation taken from expm. We see that $s = 32$, which is the choice made by expm, is the worst possible choice. The old choice, $s = 37$, is much better. These large values of $s$ result from the fact that this particular matrix has a large norm. For this example, values of $s$ less than 10 are much better.\n\nwarning('off','MATLAB:nearlySingularMatrix') err = zeros(40,1); for s = 1:40 P = padexpm(A/2^s); for k = 1:s P = P*P; end err(s) = norm((P-X)./X,inf); end semilogy(err) xlabel('s') ylabel('error')\n\nexpm_new\n\nSo how does the latest expm from Manchester do on this example? It chooses $s = 8$ and does a fine job.\n\nexpm_new(A)\n\nans = 0.446849468283145 1.54044157383964e-09 0.46281145355881 -5743067.77947979 -0.0152830038686872 -4526542.71278561 0.447722977849464 1.54270484519604e-09 0.463480648837687\n\nWhat Did I Learn?\n\nThis example is an extreme outlier, but it is instructive. The condition number of the problem is terrible. Small changes in the data might make huge changes in the result, but I haven't investigated that. The computed result might be the exact result for some matrix near the given one, but I haven't pursued that. The current version of expm in MATLAB computed an awful result, but it was sort of unlucky. We have seen at least half a dozen other functions, including classic MATLAB and expm with balancing, that get the right answer. It looks like expm_new should find its way into MATLAB.\n\nGet the MATLAB code\n\nPublished with MATLAB® 7.14\n\nHow do you compute the hypotenuse of a right triangle without squaring the lengths of the sides and without taking any square roots?\n\nContents\n\nSome Important Operations\n\nThese are all important operations.\n\nCompute the 2-norm of a vector, $||v||_2$.\n\nFind complex magnitude, $|x + iy|$.\n\nConvert from cartestesian to polar coordinates, $x + iy = r e^{i \\theta}$\n\nCompute an arctangent, $\\theta = \\arctan{y/x}$\n\nFind a plane rotation that zeros one component of a two-vector.\n\nFind an orthogonal reflection that zeros $n-1$ components of an $n$-vector.\n\nAll of them involve computing\n\n$$ \\sqrt{x^2 + y^2} $$\n\nin some way or another.\n\nPythagorean Addition\n\nLet's introduce the notation $\\oplus$ for what we call Pythagorean addition.\n\n$$ x \\oplus y = \\sqrt{x^2 + y^2} $$\n\nThis has some of the properties of ordinary addition, at least on the nonnegative real numbers.\n\nYou can use Pythagorean addition repeatedly to compute the 2-norm of a vector $v$ with components $v_1, v_2, \\ldots, v_n$.\n\n$$||v||_2 = v_1 \\oplus v_2 \\oplus \\ldots \\oplus v_n$$\n\nIt is easy to see how Pythagorean addition is involved in the other operations listed above.\n\nUnderflow and Overflow\n\nComputationally, it is essential to avoid unnecessary overflow and underflow of floating point numbers. IEEE double precision has the following range. Any values outside this range are too small or too large to be represented.\n\nformat compact format short e range = [eps*realmin realmax]\n\nrange = 4.9407e-324 1.7977e+308\n\nThis crude attempt to implement Pythagorean addition is not satisfactory because the intermediate results underflow or overflow.\n\nbad_pythag = @(x,y) sqrt(x^2 + y^2)\n\nbad_pythag = @(x,y)sqrt(x^2+y^2)\n\nIf x and y are so small that their squares underflow, then bad_pythag(x,y) will be zero even though the true result can be represented.\n\nx = 3e-200 y = 4e-200 z = 5e-200 z = bad_pythag(x,y)\n\nx = 3.0000e-200 y = 4.0000e-200 z = 5.0000e-200 z = 0\n\nIf x and y are so large that their squares overflow, then bad_pythag(x,y) will be infinity even though the true result can be represented.\n\nx = 3e200 y = 4e200 z = 5e200 z = bad_pythag(x,y)\n\nx = 3.0000e+200 y = 4.0000e+200 z = 5.0000e+200 z = Inf\n\nDon Morrison\n\nDon Morrison was a mathematician and computer pioneer who spent most of his career at Sandia National Laboratory in Albuquerque. He left Sandia in the late '60s, founded the Computer Science Department at the University of New Mexico, and recruited me to join the university a few years later.\n\nDon had all kinds of fascinating mathematical interests. He was an expert on cryptography. He developed fair voting systems for multi-candidate elections. He designed an on-demand public transportation system for the city of Albuquerque. He constructed a kind of harmonica that played computer punched cards. He discovered the Fast Fourier Transform before Culley and Tuckey, and published the algorithm in the proceedings of a regional ACM conference.\n\nThis is the first of two or three blogs that I intend to write about things I learned from Don.\n\nDon's Diagram\n\nDon was sitting in on a class I was teaching on mathematical software. I was talking about the importance of avoiding underflow and overflow while computing the 2-norm of a vector. (It was particularly important back then because the IBM mainframes of the day had especially limited floating point exponent range.) We tried to do the computation with just one pass over the data, to avoid repeated access to main memory. This involved messy dynamic rescaling. It was also relatively expensive to compute square roots. Before the end of the class Don had sketched something like the following.\n\npythag_pic(4,3)\n\nWe are at the point $(x,y)$, with $|y| \\le |x|$. We want to find the radius of the black circle without squaring $x$ or $y$ and without computing any square roots. The green line leads from point $(x,y)$ to its projection $(x,0)$ on the $x$-axis. The blue line extends from the origin through the midpoint of the green line. The red line is perpendicular to the blue line. The red line intersects the circle in the point $(x+,y+)$. Don realized that $x+$ and $y+$ could be computed from $x$ and $y$ with a few safe rational operations, and that $y+$ would be much smaller than $y$, so that $x+$ would be a much better approximation to the radius than $x$. The process could then be repeated a few times to get an excellent approximation to the desired result.\n\nFunction Pythag\n\nHere, in today's MATLAB, is the algorithm. It turns out that the iteration is cubically convergent, so at most three iterations produces double precision accuracy. It is not worth the trouble to check for convergence in fewer than three iterations.\n\ntype pythag\n\nfunction x = pythag(a,b) % PYTHAG Pythagorean addition % pythag(a,b) = sqrt(a^2+b^2) without unnecessary % underflow or overflow and without any square roots. if a==0 && b==0 x = 0; else % Start with abs(x) >= abs(y) x = max(abs(a),abs(b)); y = min(abs(a),abs(b)); % Iterate three times for k = 1:3 r = (y/x)^2; s = r/(4+r); x = x + 2*s*x; y = s*y; end end end\n\nComputing r = (y/x)^2 is safe because the square will not overflow and, if it underflows, it is negligible. There are only half a dozen other floating point operations per iteration and they are all safe.\n\nIt is not obvious, but the quantity $x \\oplus y$ is a loop invariant.\n\nSurprisingly, this algorithm cannot be used to compute square roots.\n\nExamples\n\nStarting with $x = y$ is the slowest to converge.\n\nformat long e format compact pythag_with_disp(1,1) sqrt(2)\n\n1 1 1.400000000000000e+00 2.000000000000000e-01 1.414213197969543e+00 1.015228426395939e-03 1.414213562373095e+00 1.307981162604408e-10 ans = 1.414213562373095e+00 ans = 1.414213562373095e+00\n\nIt's fun to compute Pythagorean triples, which are pairs of integers whose Pythagorean sum is another integer.\n\npythag_with_disp(4e-300,3e-300)\n\n4.000000000000000e-300 3.000000000000000e-300 4.986301369863013e-300 3.698630136986302e-301 4.999999974188252e-300 5.080526329415360e-304 5.000000000000000e-300 1.311372652398298e-312 ans = 5.000000000000000e-300\n\npythag_with_disp(12e300,5e300)\n\n1.200000000000000e+301 5.000000000000000e+300 1.299833610648919e+301 2.079866888519135e+299 1.299999999999319e+301 1.331199999999652e+295 1.300000000000000e+301 3.489660928000008e+282 ans = 1.300000000000000e+301\n\nAugustin Dubrulle\n\nAugustin Dubrulle is a French-born numerical analyst who was working for IBM in Houston in the 1970s on SSP, their Scientific Subroutine Package. He is the only person I know of who ever improved on an algorithm of J. H. Wilkinson. Wilkinson and Christian Reinsch had published, in Numerische Mathematik, two versions of the symmetric tridiagonal QR algorithm for matrix eigenvalues. The explicit shift version required fewer operations, but the implicit shift version had better numerical properties. Dubrulle published a half-page paper in Numerische Mathematik that said, in effect, \"make the following change to the inner loop of the algorithm of Wilkinson and Reinsch\" to combine the superior properties of both versions. This is the algorithm we use today in MATLAB to compute eigenvalues of symmetric matrices.\n\nAugustin came to New Mexico to get his Ph. D. and became interested in the pythag algorithm. He analyzed its convergence properties, showed its connection to Halley's method for computing square roots, and produced higher order generalizations. The three of us published two papers back to back, Moler and Morrison, and Dubrulle, in the IBM Journal of Research and Development in 1983.\n\nPythag Today?\n\nWhat has become of pythag? Its functionality lives on in hypot, which is part of libm, the fundamental math library support for C. There is a hypot function in MATLAB.\n\nOn Intel chips, and on chips that use Intel libraries, we rely upon the Intel Math Kernel Library to compute hypot. Modern Intel architectures have two features that we did not have in the old days. First, square root is an acceptably fast machine instruction, so this implementation would be OK.\n\ntype ok_hypot\n\nfunction r = ok_hypot(x,y) if x==0 && y==0 r = 0; elseif abs(x) >= abs(y) r = abs(x)*sqrt(1+abs(y/x)^2); else r = abs(y)*sqrt(1+abs(x/y)^2); end end\n\nBut even that kind of precaution isn't necessary today because of the other relevant feature of the Intel architecture, the extended floating point registers. These registers are accessible only to library developers working in machine language. They provide increased precision and, more important in this situation, increased exponent range. So, if you start with standard double precision numbers and do the entire computation in the extended registers, you can get away with bad_pythag. In this case, clever hardware obviates the need for clever software.\n\nGet the MATLAB code\n\nPublished with MATLAB® 7.14\n\nThe title of this multi-part posting is also the title of a 1966 article by Marc Kac in the American Mathematical Monthly [1]. This first part is about isospectrosity.\n\nContents\n\nIsospectrosity\n\nKac's article is not actually about a drum, which is three-dimensional, but rather about the two-dimensional drum head, which is more like a tambourine or membrane. The vibrations are modeled by the partial differential equation\n\n$$ \\Delta u + \\lambda u = 0 $$\n\nwhere\n\n$$ \\Delta u(x,y) = \\frac{\\partial^{2} u}{\\partial x^{2}} + \\frac{\\partial^{2} u}{\\partial y^{2}} $$\n\nThe boundary conditions are the key. Requiring $u(x,y) = 0$ on the boundary of a region in the plane corresponds to holding the membrane fixed on that boundary. The values of $\\lambda$ that allow nonzero solutions, the eigenvalues, are the squares of the frequencies of vibration, and the corresponding functions $u(x,y)$, the eigenfunctions, are the modes of vibration.\n\nThe MathWorks logo comes from this partial differential equation, on an L-shaped domain [2], [3], but this article is not about our logo.\n\nA region determines its eigenvalues. Kac asked about the opposite implication. If one specifies all of the eigenvalues, does that determine the region?\n\nIn 1991, Gordon, Webb and Wolpert showed that the answer to Kac's question is \"no\". They demonstrated a pair of regions that had different shapes but exactly the same infinite set of eigenvalues [4]. In fact, they produced several different pairs of such regions. The regions are known as \"isospectral drums\". Wikipedia has a good background article on Kac's problem [5].\n\nI am interested in finite difference methods for membrane eigenvalue problems. I want to show that the finite difference operators on these regions have the same sets of eigenvalues, so they are also isospectral.\n\nI was introduced to isospectral drums by Toby Driscoll, a professor at the University of Delaware. A summary of Toby's work is available at his Web site [6]. A reprint of his 1997 paper in SIAM Review is also available from his Web site [7]. Toby developed methods, not involving finite differences, for computing the eigenvalues very accurately.\n\nThe isospectral drums are not convex. They have reentrant $270^\\circ$ corners. These corners lead to singularities in most of the eigenfunctions -- the gradients are unbounded. This affects the accuracy and the rate of convergence of finite difference methods. It is possible that for convex regions the answer to Kac's question is \"yes\".\n\nVertices\n\nI will look at the simplest known isospectral pair. The two regions are specified by the xy-coordinates of their vertices.\n\ndrum1 = [0 0 2 2 3 2 1 1 0 0 1 3 2 2 1 1 0 0]; drum2 = [1 0 0 2 2 3 2 1 1 0 1 2 2 3 2 1 1 0]; vertices = {drum1,drum2};\n\nLet's first plot the regions.\n\nclf shg set(gcf,'color','white') for d = 1:2 vs = vertices{d}; subplot(2,2,d) plot(vs(1,:),vs(2,:),'k.-'); axis([-0.1 3.1 -0.1 3.1]) axis square title(sprintf('drum%d',d)); end\n\nFinite difference grid\n\nI want to investigate simple finite difference methods for this problem. The MATLAB function inpolygon determines the points of a rectangular grid that are in a specified region.\n\nngrid = 5; h = 1/ngrid; [x,y] = meshgrid(0:h:3); for d = 1:2 vs = vertices{d}; [in,on] = inpolygon(x,y,vs(1,:),vs(2,:)); in = xor(in,on); subplot(2,2,d) plot(vs(1,:),vs(2,:),'k-',x(in),y(in),'b.',x(on),y(on),'k.') axis([-0.1 3.1 -0.1 3.1]) axis square title(sprintf('drum%d',d)); grid{d} = double(in); end\n\nFinite difference Laplacian\n\nDefining the 5-point finite difference Laplacian involves numbering the points in the region. The delsq function generates a sparse matrix representation of the operator and a spy plot of the nonzeros in the matrix shows its the band structure.\n\nfor d = 1:2 G = grid{d}; p = find(G); G(p) = (1:length(p))'; fprintf('grid%d =',d); minispy(flipud(G)) A = delsq(G); subplot(2,2,d) markersize = 6; spy(A,markersize) title(sprintf('delsq(grid%d)',d)); end\n\ngrid1 = . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 56 . . . . . . . . . . . . . . 48 55 . . . . . . . . . . . . . 41 47 54 . . . . . . . . . . . . 35 40 46 53 . . . . . . . . . . . 30 34 39 45 52 60 63 65 66 . . . . . . 26 29 33 38 44 51 59 62 64 . . . . . . 18 25 28 32 37 43 50 58 61 . . . . . . 11 17 24 27 31 36 42 49 57 . . . . . . 5 10 16 23 . . . . . . . . . . . . 4 9 15 22 . . . . . . . . . . . . 3 8 14 21 . . . . . . . . . . . . 2 7 13 20 . . . . . . . . . . . . 1 6 12 19 . . . . . . . . . . . . . . . . . . . . . . . . . . . grid2 = . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 57 . . . . . . . . . . . . . . . 56 62 . . . . . . . . . . . . . . 55 61 65 . . . . . . . . . . . . . 54 60 64 66 . . 5 11 18 26 30 34 38 42 46 50 53 59 63 . . . 4 10 17 25 29 33 37 41 45 49 52 58 . . . . 3 9 16 24 28 32 36 40 44 48 51 . . . . . 2 8 15 23 27 31 35 39 43 47 . . . . . . 1 7 14 22 . . . . . . . . . . . . . 6 13 21 . . . . . . . . . . . . . . 12 20 . . . . . . . . . . . . . . . 19 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n\nCompare eigenvalues\n\nThe Arnoldi method implemented in the eigs function readily computes the eigenvalues and eigenvectors. Here are the first twenty eigenvalues.\n\neignos = 20; ngrid = 32; h = 1/ngrid; [x,y] = meshgrid(0:h:3); inpoints = (7*ngrid-2)*(ngrid-1)/2; lambda = zeros(eignos,2); V = zeros(inpoints,eignos,2); for d = 1:2 vs = vertices{d}; [in,on] = inpolygon(x,y,vs(1,:),vs(2,:)); in = xor(in,on); G = double(in); p = find(G); G(p) = (1:length(p))'; grid{d} = G; A = delsq(G)/h^2; [V(:,:,d),E] = eigs(A,eignos,0); lambda(:,d) = diag(E); end format long lambda = flipud(lambda)\n\nlambda = 10.165879621248976 10.165879621248965 14.630600866993314 14.630600866993335 20.717633982094974 20.717633982094966 26.115126153750651 26.115126153750744 28.983478457829726 28.983478457829822 36.774063407607287 36.774063407607301 42.283017757114649 42.283017757114735 46.034233949715428 46.034233949715471 49.213425509524797 49.213425509524747 52.126973962396391 52.126973962396420 57.063486161172889 57.063486161173024 63.350675017756231 63.350675017756316 67.491111510445137 67.491111510445251 70.371453210957782 70.371453210957867 75.709992784621917 75.709992784622003 83.153242199788878 83.153242199788878 84.673734481953829 84.673734481954000 88.554340162610046 88.554340162610202 94.230337192953044 94.230337192953215 97.356922250794412 97.356922250794540\n\nHow about a proof?\n\nVarying the number of eigenvalues, eignos, and the grid size, ngrid, in this script provides convincing evidence that the finite difference Laplacians on the two domains are isospectral. But this is not a proof. For the continuous problem, Chapman [8] outlines an approach where any eigenfunction on one of the domains can be constructed from triangular pieces of the corresponding eigenfunction on the other domain. It is necessary to prove that these pieces fit together smoothly and that the differential equation continues to be satisfied across the boundaries. For this proof Chapman refers to a paper by Berard [9]. I will explore the discrete analog of these arguments in a later post.\n\nReferences\n\nIf you are interested in pursuing this topic, see the PDE chapter of Numerical Computing with MATLAB, and check out pdegui.m.\n\nGet the MATLAB code\n\nPublished with MATLAB® 7.14\n\nThis is the third part of a series of posts about Marc Kac's 1966 paper in the American Mathematical Monthly [1]. This part is devoted to the proof that the drums have the same eigenvalues.\n\nContents\n\nTransplantation\n\nThis is a portion of the story behind a process known as transplantation that proves the drums are isospectral. The deeper mathematical aspects are in papers by Chapman [2], Berard [3], [4], Gordon and Webb [5], Gordon, Webb and Wolpert [6], and the work they reference.\n\nSeven triangles.\n\nEach drum is made up of seven isosceles right triangles. Any eigenvector on either drum can be constructed from triangular pieces of the corresponding eigenvector on the other drum. The same argument applies to the continuous eigenfunction of the partial differential operator, and to the discrete eigenvector of the finite difference operator with any grid size. Since all pairs of eigenfunctions or eigenvectors have the same eigenvalues, the drums are isospectral.\n\nclear all drum1 = [0 0 2 2 3 2 1 1 0 0 1 3 2 2 1 1 0 0]; drum2 = [1 0 0 2 2 3 2 1 1 0 1 2 2 3 2 1 1 0]; vertices = {drum1,drum2}; Tx = zeros(7,4,2); Ty = zeros(7,4,2); Tx(:,:,1) = [0 0 1 0; 0 1 1 0; 0 1 1 0; 1 1 2 1; 1 2 2 1; 1 2 2 1; 2 3 2 2]; Ty(:,:,1) = [0 1 0 0; 1 1 0 1; 1 2 1 1; 1 2 1 1; 2 2 1 2; 2 3 2 2; 2 2 1 2]; Tx(:,:,2) = [1 0 1 1; 0 1 1 0; 0 0 1 0; 1 1 2 1; 1 2 2 1; 2 2 3 2; 2 2 3 2]; Ty(:,:,2) = [0 1 1 0; 1 2 1 1; 1 2 2 1; 1 2 1 1; 2 2 1 2; 1 2 2 1; 2 3 2 2];\n\nThe map.\n\nHere is a roadmap for the transplantation. Let's go from drum1 to drum2, although we could just as easily go from drum2 to drum1. Three copies of an eigenvector are broken into triangular pieces, rotated and possibly transposed according to the following formulas. For example, B-A'+G means the B triangle is unaltered, the G triangle is rotated clockwise $90^\\circ$ to fit on top of B, and the A triangle, regarded as a lower triangular matrix, is transposed to create an upper triangular matrix, and subtracted from B+rot90(G,-1).\n\nUsing three pieces of the one eigenvector makes it possible for the difference operator to work properly across the interfaces. The boundary values add or cancel so that the mapped vector conforms to drum2. Since only one eigenvector is mapped, the same eigenvalue applies on the second drum, establishing isospectrality. Jon Chapman has a very helpful description of this process in terms of paper folding [2].\n\nS = {'B-A''+G'; 'A+C+E'; 'B-C''+D'; 'D-A''+F'; 'E-B''-F'''; 'F-C''+G'; ... 'E-D''-G'''}; for d = 1:2 clf set(gcf,'color','white') axis([-0.1 3.1 -0.1 3.1]) axis square off for t = 1:7 line(Tx(t,:,d),Ty(t,:,d),'color','black','linewidth',2); if d == 1 txt = char('A'+t-1); else txt = S{t}; end text(mean(Tx(t,1:3,d)),mean(Ty(t,1:3,d)),txt,'horiz','center') end snapnow end\n\nTransplant any eigenvector.\n\neignum = 1\n\neignum = 1\n\nFinite differences.\n\nngrid = 64; h = 1/ngrid; [x,y] = meshgrid(0:h:3); inpoints = (7*ngrid-2)*(ngrid-1)/2; lambda = zeros(eignum,2); V = zeros(inpoints,eignum,2); for d = 1:2 vs = vertices{d}; [in,on] = inpolygon(x,y,vs(1,:),vs(2,:)); in = xor(in,on); G = double(in); p = find(G); G(p) = (1:length(p))'; grid{d} = G; Delta{d} = delsq(G)/h^2; [V(:,:,d),E] = eigs(Delta{d},eignum,0); lambda(:,d) = diag(E); end\n\nTriangular pieces.\n\nU is the pieces of the eigenvector on drum1. W will be the pieces of the transplanted eigenvector on drum2.\n\nU = zeros(ngrid+1,ngrid+1,7); W = zeros(ngrid+1,ngrid+1,7); d = 1; G = grid{d}; p = find(G); u = zeros(size(G)); u(p) = V(:,1,1); contourf(x,y,u) title(['norm = ' num2str(norm(u(:)))]) axis square for t = 1:7 in = inpolygon(x,y,Tx(t,:,d),Ty(t,:,d)); [i,j] = find(in); U(:,:,t) = flipud(full(sparse(i-min(i)+1,j-min(j)+1,u(in)))); end\n\nNow we transplant the finite difference eigenvector. The arrays A through G are the seven pieces. Each triangular piece of the transplanted eigenvector is the sum of three pieces of the original vector. The matrix transposes and rotations reproduce the dotted and dashed edges in Chapman's figure 2b.\n\nA = U(:,:,1); B = U(:,:,2); C = U(:,:,3); D = U(:,:,4); E = U(:,:,5); F = U(:,:,6); G = U(:,:,7); a = B - A' + rot90(G,-1); b = rot90(A) + C + rot90(E,-1); c = rot90(B) - rot90(C',2) + rot90(D,-1); d = D - rot90(A',2) + rot90(F,-1); e = E - rot90(B',2) - rot90(F'); f = rot90(F,2) - rot90(C',2) + G; g = rot90(E,2) - rot90(D',2) - rot90(G'); W(:,:,1) = a; W(:,:,2) = b; W(:,:,3) = c; W(:,:,4) = d; W(:,:,5) = e; W(:,:,6) = f; W(:,:,7) = g; d = 2; G = grid{d}; p = find(G); w = zeros(size(G)); for t = 1:7 in = inpolygon(x,y,Tx(t,:,d),Ty(t,:,d)); [i,j] = find(in); v = zeros(length(i),1); for k = 1:length(i) v(k) = W(max(i)+1-i(k),j(k)-min(j)+1,t); end w(in) = v; end\n\nSurprise.\n\nI was very surprised when I found that the $l_2$ norm of most transplants is $\\sqrt{2}$.\n\nnorm_w = norm(w(:))\n\nnorm_w = 1.4142\n\nThe only exceptions are transplants of eigenvectors, like $v_9$, that are also eigenvectors of the embedded triangle. Then the norm is $3$ because the transplant mapping is just adding up three copies of the same vector.\n\nFour checks for an eigenvector.\n\nCheck one, visual. This contour plot looks like an eigenvector of drum2.\n\ncontourf(x,y,w) title(['norm = ' num2str(norm(w(:)))]) axis square\n\nWe can now limit w to the grid points and use the discrete Laplacian from drum2.\n\nw = w(p); Delta = Delta{2};\n\nCheck two, compare the Rayleigh quotient to the expected eigenvalue. The error is tiny.\n\nrho = (w'*Delta*w)./(w'*w) error = rho - lambda(1,2)\n\nrho = 10.1584 error = 2.4869e-13\n\nCheck three, apply the Laplacian. The residual is tiny.\n\nresidual = norm(Delta*w - rho*w)\n\nresidual = 1.3243e-11\n\nCheck four, compare to the eigenvector computed by eigs. The error is tiny.\n\nerror = norm(w/norm_w - V(:,2))\n\nerror = 5.7914e-15\n\nWhere does $\\sqrt2$ come from?\n\nI have been working on this post off and on for weeks. All this time I have been baffled by the $\\sqrt{2}$ that shows up as the norm of the transplanted eigenvector. It doesn't affect the eigenvector property because a scalar multiple of an eigenvector is still an eigenvector, but it sure was curious. I asked my friends who had written about the isospectral drums and they did not have any ready answers.\n\nOn Monday of the week I am writing this post, I sent email to Jon Chapman at Oxford. In 1995 he had written about the paper folding interpretation of transplantation. I asked if he could explain the $\\sqrt{2}$. On Tuesday he replied that he couldn't, but a few hours later he wrote again and told me about an amazing symbolic computation he had somehow been inspired to do in Mathematica. I can reproduce it here with the MATLAB Symbolic Toolbox.\n\nsyms A B C D E F G norm_u_sq = A^2 + B^2 + C^2 + D^2 + E^2 + F^2 + G^2; norm_w_sq = (B-A+G)^2 + (A+C+E)^2 + (B-C+D)^2 + (D-A+F)^2 + ... (E-B-F)^2 + (F-C+G)^2 + (E-D-G)^2; z = simplify(2*norm_u_sq - norm_w_sq)\n\nz = -(B - A - C + D - E + F + G)^2\n\nI use the letter z because the quantity turns out to be zero. The Laplacian of the vector in parentheses is zero and the vector vanishes on the boundary of a triangle, so the vector must be zero throughout the triangle. That is, unless the vector is an eigenvector of the triangle.\n\nJon told me in a subsequent transatlantic phone call that he thinks of this as folding all seven triangles into one. The pieces of an eigenvector annihilate each other.\n\nThis calculation and analysis implies\n\n$$ 2 ||u||^2 - ||w||^2 = 0 $$\n\nConsequently\n\n$$ ||w|| = \\sqrt{2} ||u|| $$\n\nThanks, Jon.\n\nReferences\n\nGet the MATLAB code\n\nPublished with MATLAB® 7.14\n\nThis is part one of a series of posts about John Conway's Game of Life. One deceptively simple rule leads to an incredible variety of patterns, puzzles, and unsolved mathematical problems, and a beautiful use of MATLAB sparse matrices.\n\nContents\n\nThe Rule\n\nThe \"Game of Life\" was invented by John Horton Conway, a British-born mathematician who is now a professor at Princeton. The game made its public debut in the October 1970 issue of Scientific American, in the Mathematical Games column written by Martin Gardner. At the time, Gardner wrote\n\nThis month we consider Conway's latest brainchild, a fantastic solitaire pastime he calls \"life\". Because of its analogies with the rise, fall and alternations of a society of living organisms, it belongs to a growing class of what are called \"simulation games\"--games that resemble real-life processes. To play life you must have a fairly large checkerboard and a plentiful supply of flat counters of two colors.\n\nToday Conway's creation is known as a cellular automaton and we can run the simulations on our computers instead of checkerboards.\n\nThe universe is an infinite, two-dimensional rectangular grid. The population is a collection of grid cells that are marked as alive. The population evolves at discrete time steps known as generations. At each step, the fate of each cell is determined by the vitality of its eight nearest neighbors and this rule:\n\nA live cell with two live neighbors, or any cell with three live neighbors, is alive at the next step.\n\nThe fascination of Conway's Game of Life is that this deceptively simple rule leads to an incredible variety of patterns, puzzles, and unsolved mathematical problems -- just like real life.\n\nBlock\n\nIf the initial population consists of three live cells then, because of rotational and reflexive symmetries, there are only two different possibilities; the population is either L-shaped or I-shaped.\n\nOur first population starts with live cells in an L-shape. All three cells have two live neighbors, so they survive. The dead cell that they all touch has three live neighbors, so it springs to life. None of the other dead cells have enough live neighbors to come to life. So the result, after one step, is the stationary four-cell population known as the block. Each of the live cells has three live neighbors and so lives on. None of the other cells can come to life. The four-cell block lives forever.\n\nBlinker\n\nThe other three-cell initial population is I-shaped. The two possible orientations are shown in the two steps of the blinker. At each step, two end cells die, the middle cell stays alive, and two new cells are born to give the orientation shown in the other step. If nothing disturbs it, this three-cell blinker keeps blinking forever. It repeats itself in two steps; this is known as its period.\n\nGlider\n\nFour steps in the evolution of one of the most interesting five-cell initial populations, the glider, are shown here. At each step two cells die and two new ones are born. After four steps the original population reappears, but it has moved diagonally down and across the grid. It moves in this direction forever, continuing to exist in the infinite universe.\n\nInfinite Universe\n\nSo how, exactly, does an infinite universe work? The same question is being asked by astrophysicists and cosmologists about our own universe. Over the years, I have offered three different MATLAB Game of Life programs that have tackled this question three different ways.\n\nThe MATLAB /toolbox/matlab/demos/ directory contains a program life that Ned Gulley and I wrote 20 years ago. This program uses toroidal boundary conditions and random starting populations. These are easy to implement, but I have to say now that they do not provide particularly satisfactory Game of Life simulations. With toroidal boundary conditions cells that reach an edge on one side reenter the universe on the opposite side. So the universe isn't really infinite. And the random starting populations are unlikely to generate the rich configurations that make Life so interesting.\n\nA few years ago, I published the book Experiments with MATLAB that includes a chapter on Game of Life and a program lifex. The program reallocates storage in the sparse data structure as necessary to accommodate expanding populations and thereby simulates an infinite universe. The viewing window remains finite, so the outer portions of the expanding populating leave the field of view.\n\nThe starting populations for lifex are obtained from the Life Lexicon, a valuable Web resource cataloging several hundred terms related to the Game of Life. Life Lexicon includes over four hundred interesting starting populations. You can also visit a graphic version of the Lexicon.\n\nMy latest program, which I am describing in this and later posts of this blog, uses the same dynamic storage allocation as lifex. But it features an expanding viewing window, so the entire population is always in sight. The individual cells get smaller as the view point recedes. The program also accesses Life Lexicon, so I've named it life_lex. It is now available from the MATLAB Central File Exchange; see the submission Game of Life.\n\nThe static screen shot and the movies in this post are from life_lex.\n\nFull Screen Video Playback\n\nI had hoped to capture the output from life_lex in such a way that you could play it back at a reasonable resolution and frame rate. But that involves video codecs and YouTube intellectual property agreements and all kinds of other stuff that I did not want to get involved in for this week's blog. Maybe later.\n\nIt is easy to insert half a dozen lines of code into a MATLAB program to produce animated GIF files -- an ancient technology that is good enough for our purposes today. But it is not practical to capture every step. The resulting .gif files are too large and the playback is too slow. However the following two examples show it is possible to produce acceptable movies by not recording every frame.\n\nGlider Gun\n\nBill Gosper developed his Glider Gun at MIT in 1970. The portion of the population between the two static blocks oscillates back and forth. Every 30 steps, a glider emerges. The result is an infinite stream of gliders that fly off into the expanding universe. This was the first example discovered of a finite starting population whose growth is unbounded.\n\nHere is a movie, Gosper Glider Gun Movie, that captures every fifth step for 140 steps. The first glider just reaches the edge of the frame and life_lex is about the resize its view when I stop the recording. You will have to download the code and run it yourself to see how the resizing works.\n\nNoah's Ark\n\nI think this is an amazing evolution. The Lexicon says\n\nThe name comes from the variety of objects it leaves behind: blocks, blinkers, beehives, loaves, gliders, ships, boats, long boats, beacons and block on tables.\n\nWe've learned about a few of these things -- blocks, blinkers, and gliders. But what are the others? Welcome to the world of the Game of Life.\n\nFor this movie I've captured every 50th step of 2000 steps. You can see why the expanding universe and the life_lex resizing are necessary, Noah's Ark Movie.\n\nGet the MATLAB code\n\nPublished with MATLAB® 7.14\n\nThe Game of Life, including the infinitely expanding universe, is a gorgeous application of MATLAB sparse matrices.\n\nContents\n\nThe Code\n\nThe universe in the Game of Life is represented by a sparse matrix X that is mostly all zero. The only nonzero elements are ones for the live cells. Let's begin by describing the code that implements Conway's Rule:\n\nA live cell with two live neighbors, or any cell with three live neighbors, is alive at the next step.\n\nAt any particular step in the evolution, X represents only a finite portion of the universe. If any cells get near the edge of this portion, we reallocate storage to accommodate the expanding population. This only involves adding more column pointers so it does not represent a significant amount of additional memory.\n\nA basic operation is counting live neighbors. This involves an index vector p that avoids the edge elements.\n\nHere is the code that creates a sparse matrix N with elements between 0 and 8 giving the count of live neighbors.\n\nThis is one of my all-time favorite MATLAB statements. With MATLAB matrix logical operations on sparse matrices, this is Conways Rule:\n\nThe Glider\n\nLet's see how this works with the glider.\n\nX = sparse(7,7); X(3:5,3:5) = [0 1 0; 0 0 1; 1 1 1]; disp('X') t = int2str(X); t(t=='0') = '.'; disp(t)\n\nX . . . . . . . . . . . . . . . . . 1 . . . . . . . 1 . . . . 1 1 1 . . . . . . . . . . . . . . . .\n\nCount how many of the eight neighbors are alive. We get a cloud of values around the glider providing a census of neighbors.\n\nm = size(X,1); p = 2:m-1; N = sparse(m,m); N(p,p) = X(p-1,p-1) + X(p,p-1) + X(p+1,p-1) + X(p-1,p) + ... X(p-1,p+1) + X(p,p+1) + X(p+1,p+1) + X(p+1,p); disp('N') t = int2str(N); t(t=='0') = '.'; disp(t)\n\nN . . . . . . . . . 1 1 1 . . . . 1 1 2 1 . . 1 3 5 3 2 . . 1 1 3 2 2 . . 1 2 3 2 1 . . . . . . . .\n\nOnly the nose of the glider is alive and has two live neighbors.\n\ndisp('X & (N == 2)') t = int2str(X & (N == 2)); t(t=='0') = '.'; disp(t)\n\nX & (N == 2) . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1 . . . . . . . . . . . . . . . .\n\nFour other cells have three live neighbors\n\ndisp('N == 3') t = int2str(N == 3); t(t=='0') = '.'; disp(t)\n\nN == 3 . . . . . . . . . . . . . . . . . . . . . . . 1 . 1 . . . . . 1 . . . . . . 1 . . . . . . . . . .\n\n\"OR-ing\" these last two matrices together with \"|\" gives the next orientation of the glider.\n\ndisp('(X & (N == 2)) | (N == 3)') t = int2str((X & (N == 2)) | (N == 3)); t(t=='0') = '.'; disp(t)\n\n(X & (N == 2)) | (N == 3) . . . . . . . . . . . . . . . . . . . . . . . 1 . 1 . . . . . 1 1 . . . . . 1 . . . . . . . . . .\n\nRepeating this three more times moves the glider down and to the right one step.\n\nLife Lexicon\n\nLife Lexicon is a cultural treasure. It should be listed as a UNESCO World Heritage Site. The primary site is maintained by Stephen A. Silver. He has help from lots of other folks. I gave these two links in part one of the blog last week. Just go poke around. It's great fun.\n\nText version: <http://www.argentum.freeserve.co.uk/lex_home.htm>\n\nGraphic version: <http://www.bitstorm.org/gameoflife/lexicon>\n\nThe lexicon has 866 entries. About half of them are of historical and computational complexity interest. Read them to learn the history of the Game of Life. For example, the starting population known as the \"ark\" takes 736692 steps to stabilize. Half of them, 447, are matrices that we can use as starting populations.\n\nAchim p144\n\nlife_lex reads the text version of the lexicon and caches a local copy if one doesn't already exist. It then uses random entries as starting configurations. As just one example, we learn from the lexicon that the following population was found by Achim Flammenkamp, Dean Hickerson, and David Bell in 1994 and that its period is 144. We're showing every fourth step."
    }
}
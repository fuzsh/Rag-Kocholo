{
    "id": "wrong_mix_random_starring_00033_0",
    "rank": 42,
    "data": {
        "url": "https://mavric.umd.edu/news",
        "read_more_link": "",
        "language": "en",
        "title": "MAVRIC",
        "top_image": "https://mavric.umd.edu/sites/default/files/2020-02/Students_Mall_09192019_0726.jpg",
        "meta_img": "https://mavric.umd.edu/sites/default/files/2020-02/Students_Mall_09192019_0726.jpg",
        "images": [
            "https://mavric.umd.edu/sites/default/files/MAVRIC-inverse-logo.png",
            "https://mavric.umd.edu/sites/default/files/styles/optimized/public/2023-03/MIXR.jpg?itok=E-jHJbe3",
            "https://mavric.umd.edu/sites/default/files/inline-images/VR%20Alumni%20Event.jpg",
            "https://mavric.umd.edu/sites/default/files/inline-images/Montgomery%20County%27s%20Oakley%20Cabin.png",
            "https://mavric.umd.edu/sites/default/files/inline-images/Montgomery%20County%27s%20Oakley%20Cabin%20in%20AR%20.png",
            "https://mavric.umd.edu/sites/default/files/inline-images/Artist%20AndreZachery.jpg",
            "https://mavric.umd.edu/sites/default/files/inline-images/VR%20Alumni%20Event%202024%281%29.jpg",
            "https://mavric.umd.edu/sites/default/files/inline-images/VR-Crowd.jpg",
            "https://mavric.umd.edu/sites/default/files/inline-images/VOA%20LogOn.png",
            "https://mavric.umd.edu/sites/default/files/inline-images/ImmersiveLearningClass.jpg",
            "https://mavric.umd.edu/sites/default/files/inline-images/IEEEPulseFigure1.jpg",
            "https://mavric.umd.edu/sites/default/files/inline-images/IEEEPulseFigure2.jpg",
            "https://mavric.umd.edu/sites/default/files/inline-images/IEEEPulseFigure3.jpg",
            "https://mavric.umd.edu/sites/default/files/inline-images/IEEEPulseFigure4.jpg",
            "https://mavric.umd.edu/sites/default/files/inline-images/IEEEPulseFigure5.jpg",
            "https://mavric.umd.edu/sites/default/files/inline-images/Summ-AR.png",
            "https://mavric.umd.edu/sites/default/files/inline-images/Extended%20Reality%20Simulation%20and%20Control%20Lab.png",
            "https://mavric.umd.edu/sites/default/files/inline-images/HoopersIslandCauseway_1920x1080.jpg",
            "https://mavric.umd.edu/sites/default/files/inline-images/UMDAerospaceVRSimulator.png",
            "https://mavric.umd.edu/sites/default/files/inline-images/DrUmbertoSaetti.jpg",
            "https://mavric.umd.edu/sites/default/files/inline-images/UMD-Students-learn-supply-chain-management-with-immersive-vr.jpg",
            "https://mavric.umd.edu/sites/default/files/inline-images/UStreetCollage_0.jpg",
            "https://mavric.umd.edu/sites/default/files/inline-images/Arts-for-All-students.png",
            "https://mavric.umd.edu/sites/default/files/inline-images/ImmersiveArt.jpg",
            "https://mavric.umd.edu/sites/default/files/inline-images/UStreetBlackBroadway.jpg",
            "https://mavric.umd.edu/sites/default/files/inline-images/270089_web.jpg",
            "https://mavric.umd.edu/sites/default/files/inline-images/LASSR.jpg",
            "https://mavric.umd.edu/sites/default/files/inline-images/ArtechouseArtsforAll_0.jpg",
            "https://mavric.umd.edu/sites/default/files/inline-images/StocksyImage.jpg",
            "https://mavric.umd.edu/sites/default/files/inline-images/EEG-testing.jpg",
            "https://mavric.umd.edu/sites/default/files/inline-images/SpaceIllustration.jpg",
            "https://mavric.umd.edu/sites/default/files/inline-images/MorseandEastman_0.jpg",
            "https://mavric.umd.edu/sites/default/files/inline-images/MorseandEastman.jpg",
            "https://mavric.umd.edu/sites/default/files/inline-images/MBRCVRheadset_0.jpg",
            "https://mavric.umd.edu/sites/default/files/inline-images/VRofViolinPerformance.jpg",
            "https://mavric.umd.edu/sites/default/files/inline-images/AR%20Holiday%20Marketing.png",
            "https://mavric.umd.edu/sites/default/files/inline-images/Yosemite_1920x1080.jpg",
            "https://mavric.umd.edu/sites/default/files/inline-images/GrandDiningRoom.jpg",
            "https://mavric.umd.edu/sites/default/files/inline-images/VirtualYosemite.jpg",
            "https://mavric.umd.edu/sites/default/files/inline-images/VirtualYosemiteTour.jpg",
            "https://mavric.umd.edu/sites/default/files/inline-images/YosemiteFalls.jpg",
            "https://mavric.umd.edu/sites/default/files/inline-images/GalenStetsyuMikhailSorokin.jpg",
            "https://mavric.umd.edu/sites/default/files/inline-images/GalenStetsyuMikhailSorokinMPLEX.jpg",
            "https://mavric.umd.edu/sites/default/files/inline-images/VRofViolinPerformance_0.jpg",
            "https://mavric.umd.edu/sites/default/files/inline-images/AutismApp.png",
            "https://mavric.umd.edu/sites/default/files/inline-images/CyKeenerArcticIce.jpg",
            "https://mavric.umd.edu/sites/default/files/inline-images/VR%20Memory_0.png",
            "https://mavric.umd.edu/sites/default/files/inline-images/IciarAndreu_0.jpg",
            "https://mavric.umd.edu/sites/default/files/inline-images/VR%20Memory.png",
            "https://mavric.umd.edu/sites/default/files/inline-images/ARmakeSurgerySafer.jpg",
            "https://mavric.umd.edu/sites/default/files/inline-images/NewseumVRBerlinWall_0.jpg",
            "https://mavric.umd.edu/sites/default/files/inline-images/UMDSociologyDept.jpg",
            "https://mavric.umd.edu/sites/default/files/inline-images/Augmentarium.jpg",
            "https://mavric.umd.edu/sites/default/files/inline-images/Augmentarium_1.png",
            "https://mavric.umd.edu/sites/default/files/inline-images/Augmentarium.png",
            "https://mavric.umd.edu/sites/default/files/inline-images/ARillustrationsLR-medical.jpg",
            "https://mavric.umd.edu/sites/default/files/inline-images/Record%20%2431M%20Gift.png",
            "https://mavric.umd.edu/sites/default/files/inline-images/Record%20%2431M%20Gift%20page%202.png",
            "https://mavric.umd.edu/sites/default/files/inline-images/BrendanIribeOculus.jpg",
            "https://mavric.umd.edu/sites/default/files/inline-images/Virtual%20Reality%2C%20Made%20More%20Real%20.png"
        ],
        "movies": [],
        "keywords": [],
        "meta_keywords": [
            ""
        ],
        "tags": null,
        "authors": [],
        "publish_date": null,
        "summary": "",
        "meta_description": "",
        "meta_lang": "en",
        "meta_favicon": "/themes/contrib/umd_terp/favicon.ico",
        "meta_site_name": "MAVRIC",
        "canonical_link": "https://mavric.umd.edu/news",
        "text": "An AR-Aided View of Black History\n\nUMD, Montgomery County Recreate Life at Historic African American Cabin\n\nWith just a smart phone, visitors at Montgomery County's Oakley Cabin can learn about African American life after emancipation through an augmentative reality experience, including virtual park guides, property fly-overs and 3D models of the interior as it was in the 19th century. Photos by Maggie Haslam.\n\nBy Brianna Rhodes ’17 Jun 18, 2024\n\nVisitors pointing their phones at the unassuming log cabin tucked along a wooded road in Olney, Md., may see a 19th-century wash basin still wet with laundry just outside the back door, chickens roaming around a wooden coop or a neighboring log cabin just yards away.\n\nBut when they lower their phones, all that remains is an empty yard and a deeper understanding of this property’s complicated past.\n\nThrough augmented reality (AR), a technology that overlays live scenes with computer-generated imagery, visitors to this African American heritage site in Montgomery County can now envision the livelihoods of Black families who lived in Oakley Cabin after the Civil War. Developed by the University of Maryland in collaboration with Montgomery County’s parks and technology departments, the project of digital recreations is believed to be the first AR experience created for an African American historic site in Maryland.\n\n“This really important period of American history and the Black experience is underrepresented in academic research, in public sites and interpretation,” said Stefan Woehlke M.A. ’13, Ph.D. ’21, a postdoctoral associate in UMD’s Historic Preservation Program. “It was really important to me to work on a site where the county government is putting resources into and sharing this period of history through these digital tools.”\n\nUMD's Postdoctoral Associate Stefan Woehlke (below, left) and graduate student Rachel Wilkerson gathered documentation of the site to help shape the experience. Photo by Maggie Haslam.\n\nOakley Cabin was one of three cabins originally built in the 1820s as part of the 100-acre Oakley farm. The 1½-story oak and chestnut log cabin housed enslaved laborers until the emancipation of enslaved Marylanders in 1864; it was home to free Black tenant families, up to 10 people at a time, well into the late 20th century. Montgomery Parks took over the property in 1976 to transform it into a museum to highlight Black life post-emancipation.\n\nCounty officials tapped Woehlke as a project partner last summer. With graduate student Rachel Wilkerson ’23, M.A. ’25, immersive media design student Isabelle Klimanov ’24 and history alum Kyle Houston ’23, Woehlke used digital laser scanning and photogrammetry to turn thousands of images of the site into 3D models and flyovers of the cabin's interior, exterior and landscape. Tech specialists used the imagery to create interior and exterior recreations that include historic artifacts and the two long-demolished cabins. By scanning QR codes, visitors can immerse themselves in eight different AR experiences, including virtual park guides in seven languages, videos that recreate the historical surroundings, 3D models of the cabin’s interior and exterior, and interactive games.\n\nBecause Oakley Cabin is one of 17 historic sites across the county and staffing is limited, the interior of the cabin is staffed and accessible to visitors only from April to October on two Saturdays a month. Through AR programs, visitors can learn about the history of a property 365 days a year, especially during observances like Black History Month and Juneteenth.\n\n“There are so many important historical sites across Maryland filled with artifacts and information from the lived experiences of these communities, and programs like this make them so much more accessible,” said Klimanov.\n\nWoehlke has been conducting similar research in Black historic neighborhoods in Maryland for the past 10 years, including the Lakeland community in College Park and North Brentwood, the second town incorporated by a Black community in Maryland and the first in Prince George’s County. He has experimented with integrating imagery and oral histories into video game technology to develop digital tours that highlight significant sites, like Sis’s Tavern in North Brentwood, one of the region’s famed jazz haunts. This past spring, he and students used photogrammetry tools to scan North Brentwood’s AME Zion Church to build a digital interpretation of the site.\n\nMontgomery Parks’ cultural resources stewardship supervisor, Cassandra Michaud, said that with the Olney AR cabin project, the county hopes to help shape a new digital era of historical site discovery and exploration throughout the region. It’s also looking to collaborate with Woehlke in the future at other African American historic sites such as the Josiah Henson Museum and Park in North Bethesda and Johnson Local Park at Emory Grove in Gaithersburg.\n\n“This technology can create a new experience of something that is no longer there,” Michaud said. “We really want people to go to the place, experience it and [develop] their own understanding of it.”\n\nFormer Montgomery County Historical Preservation Commissioner Warren Fleming, who helped conduct the first site visit and restoration to the cabin, said the technology can finally help tell Black history the right way.\n\n“In order to get the true meaning of how life was for African Americans during this period, it's important to come out now to this site and get the true education,” he said.\n\nhttps://today.umd.edu/an-ar-aided-view-of-black-history\n\nInterdisciplinary artist explores identity through technology and dance\n\nTolu Talabi\n\nApril 25, 2024\n\nAs interdisciplinary artist André Zachery began to dance, red lines formed on a screen, revealing the outlines of three Black men who tragically died in Chicago.\n\nSet to the audio of a poem by Gwendolyn Brooks about the death of a young Black boy, Zachery attempts to fit his body — a computer graphic version on the screen that follows his movements — into the mold of the men.\n\nThis performance — presented by the University of Maryland’s theatre, dance and performance studies school, immersive media design program and Arts for All — was the genesis of a high-tech journey Zachery took his audience on during his solo performance at The Clarice Smith Performing Arts Center Dance Theatre Saturday.\n\nZachery performed an excerpt titled “SALT: Work-in-Progress,” a reflection on his coming of age in the ’80s and ’90s as a Black man and Chicago native.\n\n“What do you do with the weight of those histories and how do you live with them, but at the same time how do you move forward?” Zachery, who is also an assistant arts professor at New York University, said.\n\nZachery answered this question by confronting the past, present and future.\n\nIn one segment, he began pulling and working through an invisible string as red clustered strings on a map of Chicago appeared behind him. Here, the performance alluded to and resisted the racial segregation of redlining in Chicago.\n\nThe image on the screen transitioned to blue swirls that mimicked water currents. Zachery’s body slowed before he began flapping and flailing his arms like wings. A reading of Salt, a book by Earl Lovelace that explores neo-colonial independence in Trinidad and Tobago, resounded through the theater as Zachery’s movements responded to the text.\n\nZachery also wove African mythology and the legend of flying Africans into his piece because he grew up listening to these stories. It was important to include this aspect in his performance because myth and magic are rooted in Black and African culture, which inspires his artistic medium, he said.\n\n“Magic is important,” Zachery said. “It’s through these artistic statements that you’re able to actually use that magic to dream, create and ultimately inspire in some way.”\n\nA soundscape of ocean waves filled the theater, with swirls on the screen moving in accordance with Zachery’s body. He said the displayed images were programmed and altered by the level of sound and the position of his body.\n\nHe used this water scene to educate the audience about the origin of flying Africans, which stem from the captivity of enslaved Igbo people from Nigeria.\n\nAfter sailing across the Atlantic Ocean and resisting the entire way, the Igbo jumped into the sea at Igbo Landing and flew back to Igbo land through the water, Zachery said.\n\nThis portion of the performance enthralled Jonathan David Martin, a lecturer in the immersive media design program and the host of Zachery’s performance.\n\n“It made me feel like he was conjuring spirits almost through the technology,” Martin said. “We forget that [technology] can be in service of creating a really unique emotional response.”\n\nKate Ladenheim, an artist in residence at this university and a host of the symposium said Zachery’s performance moved her. She applauded his ability to use technology to revive his ancestors’ lost stories and persevere in the face of his ancestors’ struggles.\n\nZachery seamlessly segued into hip hop dancing, performing backflips and engaging in fast paced movements as “Started From the Bottom” by Drake played in the background. He recontextualized the song to speak about how Africans came from the bottom of the ocean and paved the way for who he is as a Black man today, he said.\n\n“We started from the bottom and now we’re here,” Zachery said. “We flying now. Cool. We up there.”\n\nhttps://dbknews.com/2024/04/25/interdisciplinary-artist-explore-identity-innovative-technology-dance/\n\nHow Emerging Technologies are Crafting New Worlds and Revolutionizing Learning\n\nMarch 6, 2024\n\nLaurie Robinson\n\nResearch with KidsTeam and local neighborhoods examines how youth use emerging technologies like AI and VR\n\nThe University of Maryland (UMD) College of Information Studies (INFO) Associate Professor Tamara Clegg, who is an affiliate faculty member at the College of Education (EDUC), INFO Assistant Research Professor beth Bonsignore, and other collaborators are working on a Meta-funded project to better understand how young people are using emerging technologies known as “XR”, or extended reality, which encompasses a range of systems or experiences that include virtual reality (VR), augmented reality (AR), and mixed reality (hybrid/MR). Many of these XR technologies may integrate AI as well. For example, in an immersive XR environment, youth might interact with AR-representations of AI-based agents like Alexa or Siri.\n\nA core goal for the Meta-supported project is to develop foundational ethical principles for engaging in XR research with youth and families. For instance, how do we ensure that all youth have access to these immersive emerging technologies, and how do you protect the privacy of their experiences while also mitigating any possible risks (e.g., does “VR motion sickness” affect children differently than adults?)\n\nThe researchers have started working with KidsTeam, an interdisciplinary group that is a part of the UMD Human-Computer Interaction Lab (HCIL), to explore how children as young as 7-8 years old interact with VR headsets typically used by adults. While continuing with KidsTeam, the researchers are also planning summer co-design sessions with youth in local communities. The HCIL has been a joint partnership between the University of Maryland Institute for Advanced Computer Studies (UMIACS) and INFO for four decades, with KidsTeam being an integral co-design subgroup within HCIL for 25 years.\n\nThe Meta XR project stands out for its engagement with a diverse cohort of youth. This project, known as the “XR for Youth Ethics Consortium,” comprises seven research universities across the United States. The lead institution is the University of Iowa, and UMD is one of six other partner institutions, including Boise State University, Northeastern University, University of Baltimore, University of Minnesota, and University of Washington. While consortium members like University of Iowa, Boise State, and University of Minnesota may explore XR with rural communities, UMD and University of Baltimore are uniquely positioned to partner with minoritized and under-resourced communities in the urban and inner-ring suburban neighborhoods around DC, Baltimore, and in Prince George’s County. Northeastern will be focusing on neurodiverse youth and their families/communities. With such a broad consortium of groups contributing, the project reflects a commitment to diversity. Meta’s funding of the project underscores their interest in inclusive research that can inform not only the effective design and development of their tools, but also the ethical principles by which they design and research these technologies overall.\n\nVisualizing a New Landscape\n\nVR headsets are reshaping our digital existence. They serve as windows to immersive panoramas, bridging the void between the tangible and the imagined. The headset is an extension of our senses—an architect of visual and auditory illusions, synthesizing landscapes ranging from the hyper-realistic to the surreally fantastical. Wearers look around, and the environment responds in real-time, they reach out, and their digital avatars mirror their movements with delicate precision.\n\nThe educational potential of VR is vast. Historical recreations within VR can transport students back in time to witness events first-hand, while scientific simulations can unravel the complexities of human anatomy or take learners on a tour of the solar system. These experiences have the potential to revolutionize teaching methodologies, bringing abstract concepts to life and making the learning process a vivid and interactive journey.\n\n“People can use them for all types of things–learning environments, simulations, games–and so we’re trying to figure out some of the ways they might be used with young people. What are some of the ethical issues involved with that?” says Clegg. “We’re helping kids understand what these technologies can do. We’ll give youth and communities an opportunity to play around with the tools and explore them. We will have them co-design ways they want to use them. Eventually, it could get to what kind of new technologies they want to design. One of our focus areas is on understanding what new uses and experiences they want to have with some of these tools.”\n\n“Relatedly, another goal for our project is to investigate what ethical concerns, challenges, or opportunities that youth imagine might come along with these new tools and immersive experiences, such as limited access to some children over others, or inadvertently integrating human bias or hidden controls into the systems,” says Bonsignore.\n\nThe Limitations of Emerging Technologies\n\nSocial scientists and technologists debate about the potential of VR to cause isolation. In a society where virtual spaces can sometimes eclipse reality, striking a balance between the allure of virtual worlds and the grounding force of physical interactions becomes critical.\n\nDebates around generative AI are similar. Considering these tools for educational purposes, it’s crucial to recognize both their potential to foster engagement and the possible constraints they may introduce. For instance, children using generative AI for art creation learn to instruct the system on crafting a desired image or photo. This requires them to creatively engage with the technology. However, the research team is considering the implications of such technology on traditional art creation: what creative experiences might children miss out on, and what sorts of imaginative opportunities could be lost when using AI to visualize for them?\n\nEmerging technologies are changing the social and cognitive landscape for children, prompting a reevaluation of educational needs. “From a learning perspective, it’s causing us to rethink what kids need to learn. What do you need to learn to be literate in society now? I think those things are shifting and changing,” says Clegg.\n\nYouth need new skills for proficiency with tools like generative AI and VR. As these technologies integrate into learning, it’s important to consider what may become obsolete. How might traditional modalities such as pen-and-paper tasks remain relevant? The challenge is determining which skills should be retained and how to blend them with modern technological capabilities.\n\nPreparing Learners to be Digital Citizens\n\nAnother challenge emerging technologies pose is their infringement on the privacy and security of children’s data. In daily life, the division between online and offline realms has become indistinguishable for youth. This seamless existence raises critical questions about their online privacy and data generation that Clegg and other researchers, including INFO Associate Professor Jessica Vitak and PhD student Elana Blinder, have been looking into through an ongoing National Science Foundation-funded project.\n\nTechnology has become a dominant force in education, drawing children into virtual environments that, while innovative, create vast pools of data. These data points, captured by schools and the companies behind these digital tools, hold valuable insights into learning patterns and behaviors. However, they also attract the interest of tech corporations, which have a history of leveraging personal data for various purposes–some unwelcome.\n\nIn this age of ubiquitous data, it’s vital to ask: what type of data should be collected from learners? The consideration of caregivers, educators, and learners themselves in this equation cannot be overstated. The responsibility of caregivers in the realm of privacy and security extends beyond mere protection—it’s about equipping youth with the knowledge and autonomy to influence their digital footprint.\n\nTheir research suggests that while children place substantial trust in their parents and teachers regarding social dilemmas encountered on digital platforms, they exhibit wariness towards sharing personal information with companies and gaming sites. To bring the concepts of privacy and security to the forefront of a child’s consciousness, the researchers suggest introducing them to real-world scenarios that accentuate the importance of these issues.\n\nTraditionally, the decision-making power regarding a child’s data has been entrusted to adults. While this offers a safeguard, it simultaneously strips children of their agency, preventing them from developing informed self-governance over their data. The goal is not to eliminate parental oversight but to foster an environment where children can learn to make judicious decisions about their online presence.\n\nThe research teams from both projects advocate for a balanced approach, one that preserves the trust children have in their caregivers while encouraging them to cultivate an understanding of their digital rights. Adults must prepare children to navigate the complexities of technology with both confidence and caution. In doing so, they empower them to become savvy digital citizens who understand the permanence of their online actions and the significance of their digital identities.\n\nhttps://ischool.umd.edu/news/how-emerging-technologies-are-crafting-new-worlds-and-revolutionizing-learning/\n\nFrom Magical Machines to Real-World Design\n\nLegend of Zelda Inspires New Engineering Course\n\nBy Robert Herschbach\n\nWith nearly 20 million copies sold since May, The Legend of Zelda: Tears of the Kingdom isn’t just the fastest-selling Nintendo game of all time. As the basis for a new engineering course at the University of Maryland, the video game could be at the forefront of a new movement in higher education.\n\nPlayers of the open-world action-adventure game–the latest in a popular 37-year-old franchise–control protagonist Link as he and the eponymous Princess Zelda navigate the world of Hyrule and contend with an unknown evil presence. Some of Link’s explorations, which include a subterranean realm and a series of floating sky islands, rely on the creation of gliders, rockets and other machines.\n\nWhen UMD Associate Professor Ryan D. Sochol realized how important the players’ design of these gadgets is to completing the game’s quest, he devised a course that incorporates the game in place of traditional computer-aided design (CAD) and engineering software.\n\n“As I played through Tears of the Kingdom, I couldn’t believe how much I was relying on my engineering training,” said Sochol. “The more experience I had with the game’s CAD assembly interface, numerous machine elements and sophisticated physics, the more I felt it offered unique means to help students hone their skills in machine design.”\n\nJust a few months after the game’s release, Sochol’s “The Legend of Zelda: A Link to Machine Design” course launched for this semester to provide undergraduate students with an uncommon opportunity to gain experience designing, prototyping and testing new types of vehicles, robots and machines—all within the virtual world of the game.\n\nSamuel Graham, Jr., dean of the A. James Clark School of Engineering, said the course exemplifies a new approach to teaching in higher education, geared toward greater incorporation of immersive media–including virtual, augmented, and mixed reality—in the classroom.\n\n“The Clark School prides itself on providing our students a rich mix of classroom and hands-on experiences, preparing them to tackle big challenges and position themselves for success in the workforce,” Graham said. “Gaming is a doorway for young people to become interested in engineering and computer science, and create simulation tools that help us solve real world challenges. Our Legend of Zelda class plays on that appeal and provides a powerful mix of intellectual and practical tools.”\n\nIn one of the course’s projects, students created a transforming robot that can run on land and swim in water, and then raced their robots in the game to see whose was fastest. The project, along with others based on aerial vehicles, are designed to help students build their proficiencies in machine design and engineering—but it won’t necessarily make them better at Zelda, Sochol cautioned.\n\n“The machines created for the design projects aren’t too useful if you’re looking to beat the game, but it enables us to teach engineering in the way it ideally should be taught—as something that is engaging, challenging, exciting and fun,” he said.\n\nLuke Rose '26, a mechanical engineering major who also plans to minor in robotics and autonomous systems, was part of the team that won the in-class competition.\n\n\"The biggest impact this course has had on me was that it offered me a different approach to machine design, allowing me to more easily think about constructing mechanical systems as a sum of the components rather than the (far more complex) whole,\" Rose said. \"This has helped me in other major-related courses because, similar to groups of animals moving together, the individual components are following relatively simple rules which lead to very complex motion on the whole.\"\n\nThe game world allowed the class to put into action things they'd learned in theory in other classes, and which would be impractical to build as students in the real world, said mechanical engineering major Rheanna King '26.\n\n\"It felt like I was actually experiencing an engineer's job firsthand by doing calculations, designing, building and even going back to square one and starting over,\" King said. \"The course showed me what it's like to be an engineer and how what we're learning in other classes can be tied together and applied.\"\n\nThis is not the first foray into gamified engineering for Sochol, who joined the UMD mechanical engineering faculty in 2015. He and researchers in his Bioinspired Advanced Manufacturing Laboratory made headlines when they demonstrated a 3D-printed soft robotic hand by playing Super Mario Bros.—work that led to a $3 million grant from the National Institutes of Health to build soft robots for neurosurgery.\n\nInterest is high in the melding of gaming and machine design; a “Hyrule Engineering” group on Reddit amassed more than 150,000 subscribers, and the waitlist for Sochol’s class this semester was double the number of available seats.\n\nFortunately, Sochol plans to offer the Zelda course every semester for the foreseeable future.\n\nWatch video about: \"The Legend of Zelda: A Link to Machine Design\" | UMD's Newest Engineering Course\n\nhttps://today.umd.edu/from-magical-machines-to-real-world-design\n\nIEEE Pulse: New Center Primes the Extended Reality Frontier\n\nAugust 31, 2023\n\nAuthor(s): Leslie Mertz\n\nCollaboration between academia, industry, and government lays the groundwork for medical AR and VR technologies\n\nExtended reality has reached a critical point in biomedicine. The technology is accelerating and excitement about potential health care benefits is mounting but bridging the gap between envisioning what could be and actually making it happen is still a work in progress. Part of the challenge is coordinating all the major players to ease the development pipeline so virtual, augmented, and mixed reality can reach their potential in everything from diagnostics and surgeries to clinician training and telemedicine.\n\nA new university, industry, and U.S. government collaboration is now in place to address that challenge [1]. Called the Center for Medical Innovations in Extended Reality (MIXR), it will bring together the researchers pursuing virtual, augmented, and mixed reality technologies that clinicians need and want; the companies seeking to develop and make products that have both strong market potential and can earn the necessary approvals from the U.S. Food and Drug Administration (FDA); and FDA officials who are working through the agency’s regulatory responsibilities and procedures as medical extended-reality projects surge.\n\nFunded with a $5 million grant from the National Science Foundation (NSF) [2], [3], [4], MIXR is a joint center between the University of Maryland, College Park; the University of Maryland, Baltimore; and the University of Michigan. The FDA and several technology companies, including industry giants Sony and Microsoft, are also partners.\n\nIts time has come\n\nThe timing is right for MIXR, said MIXR lead-site principal investigator (PI) Amitabh Varshney, Ph.D., professor of computer science at the University of Maryland and IEEE Fellow (Figure 1). “Just over the last few years, virtual and augmented reality devices have become commoditized so you can go to electronics stores and buy very high-powered headsets, which was not possible before. There have been amazing advances in artificial intelligence and machine learning, and we now have 5G and soon 6G wireless capabilities,” he said. “This confluence makes this a perfect time to invest the effort and resources to advance the field in a direction that will benefit society at large.”\n\nFigure 1. Long-time researcher in virtual reality (since his doctoral studies at the University of North Carolina under noted computer architect Fred Brooks), Amitabh Varshney is helping to start a new university, industry, and government collaboration called the Center for Medical Innovations in Extended Reality (MIXR). Varshney, Ph.D., is a professor of computer science and the dean of the College of Computer, Mathematical, and Natural Sciences at the University of Maryland, and a lead-site principal investigator for MIXR. (Photo courtesy of the University of Maryland.)\n\nThe tipping point between dream and reality is here, agreed Rishi Reddy, M.D., M.B.A., a MIXR partner-site PI and the director of the Center for Surgical Innovation at the University of Michigan (Figure 2). “I don’t think we would have been able to make much headway just five years ago, but with 5G, we have arrived at an opportune time to really plan out thoughtfully what we can accomplish with extended reality in health care,” he said.\n\nFigure 2. Rishi Reddy (left) is the director of the Center for Surgical Innovation and José José Alvarez research professor in thoracic surgery at the University of Michigan. He is also partner-site PI for MIXR. (Photo courtesy of the University of Maryland.)\n\nExtended reality cannot come soon enough, remarked partner-site PI Sarah Murthi, M.D., of the University of Maryland School of Medicine (Figure 3). While surgeons are already able to access real-time computed tomography (CT), ultrasound, and other images during procedures, the images are 2D and appear on separate computer screens. That means surgeons must continually look back and forth from patient to various screens as they carry out often-complex operations, she said. “You can’t see the images without turning away, and you can’t blend one image with another, so it’s still much like it was in the 1950s. Extended reality, on the other hand, can disrupt how data is displayed, and allow providers to see different data streams, fuse data, and overlay real-time images right on the patient” (Figure 4).\n\nFigure 3. Sarah Murthi is the director of the critical-care ultrasound program at the R. Adams Cowley Shock Trauma Center in Baltimore, a professor of surgery at the University of Maryland School of Medicine, and a MIXR PI. (Photo courtesy of the University of Maryland.)\n\nReddy views student training as having great potential, too, by allowing them to see what the surgeon is seeing, including the 3D overlays on the patient, so they can ask for clarifications as the real-time operation progresses. He believes extended reality will also be very helpful for preoperative patient counseling, and to allow patients to connect more meaningfully with physicians during a telehealth visit.\n\nFigure 4. Murthi wears a virtual reality headset during an ultrasound procedure. She foresees extended-reality technologies that will allow clinicians to see different data streams, fuse data, and overlay real-time images on the patient. (Photo courtesy of the University of Maryland.)\n\n“Overall, these technologies provide a lot of powerful opportunities to positively impact health care and the way that we currently function,” Reddy said. “Our goal with MIXR is to do that in a consistent way, which includes validating that these virtual, augmented, and mixed reality systems do make surgeries safer, improve the patient experience, and improve training and education.”\n\nSmoothing the way\n\nMIXR will evaluate the entire extended-reality pipeline (Figure 5). That evaluation begins with acquisition of the virtual scene, Varshney said. In training medical students on a surgical procedure, that would mean acquiring the entire technique so the students can observe in close range every step the surgeon takes. “To make it truly scalable, you would need an array of cameras to capture the procedure, so the students can place themselves in the shoes of the surgeon and see it as it is being performed.”\n\nFigure 5. Varshney, Murthi, and Reddy (left to right) hope MIXR will speed the development of safe and effective extended-reality technologies for health care, whether that be for diagnosis, treatment, medical training, or patient education. (Photo courtesy of the University of Maryland.)\n\nDisplays in extended-reality headsets will require evaluation to ensure they perform well. This includes such technical aspects as resolution and field of view, but also issues of user fatigue, Varshney said. “Right now, people can only wear headsets for a short period before they get headaches or nausea, so if a surgeon is starting a 6-hour operation but can only wear the headset for 20 minutes, that is not good. We are looking at what the industry and university roadmap should be so that we design and evolve headsets that are truly effective, ergonomic, and comfortable for people to wear for hours on end so they can be productive at their task.”\n\nOther focus areas for MIXR include best practices for using artificial intelligence and machine learning in the context of visualization and visual immersion for medical diagnosis and medical procedures, and an examination of how next-generation wireless technologies may impact extended reality. As an example of the latter, Varshney proposed that 6G might support a technology to allow a doctor to appear via hologram in a rural or remote location, such as a battlefield, and provide medical direction to on-site personnel who can provide immediate care for a patient.\n\nMIXR will look at the full range of health-related applications. One area that Reddy sees the technology playing an important role is in helping clinicians perform at a high level. “This could be a great boon to procedural training, because there are certain procedures that we surgeons only do once or twice a year, so augmented reality or virtual simulation is a way to practice and maintain skill sets for rare procedures,” he remarked.\n\nCreating a new path\n\nOne more critical MIXR emphasis lies in facilitating regulatory approval for extended reality, because it falls outside the normal realm of the FDA, noted Murthi. “If it is a drug, or if it is something that is completely automated, such as a robot doing a tumor resection, that is clearly something for the FDA. A physician decision, such as my decision to choose one antibiotic over another or whether to read certain procedural information before surgery, is not something the FDA oversees,” she said. “Here is the question: What happens with something like augmented reality, where a physician is making a decision but that physician is heavily influenced by this technology? That falls somewhere in between and makes things much murkier for the FDA.”\n\nWithin these muddy waters, the FDA hopes to restructure the approval process for extended-reality devices in surgery and medicine, according to Varshney. Rather than going through a lengthy process to approve one-by-one requests to use a certain company’s headset for a specific procedure, the FDA is proposing that universities and companies create teams to define more inclusive guidelines. For example, a guideline might state that any extended-reality headset meeting a certain set of specifications for brightness, resolution, field of view, and weight is appropriate for use with X, Y, and Z surgical procedures, he said. “This way, the whole approval process would be much more streamlined.”\n\nThis type of streamlining would be a game-changer both in the United States and abroad, commented Reddy. “Similar groups around the world are working on accelerating the extended-reality field, but MIXR is in the unique position of having the FDA as a partner. Because the U.S. is such a big market for all these extended-reality use cases, if the FDA approval process becomes more efficient, that really opens up a lot of opportunities for companies wherever they’re based.”\n\nMIXR will also have an impact on the research end, Varshney said. “We are already beginning to fund specific research projects that have been collaboratively designed and defined by the FDA, industry, and MIXR,” Varshney said. Although the center is only funding research projects at the three partner universities, he added, “the outcomes of these research projects will help accelerate the evolution of the virtual/augmented reality field for medicine.”\n\nThe NSF-funded MIXR to jumpstart the entire extended-reality field, not to push through specific research or products, Murthi noted. “This is specifically pre-commercial, so the point is not for the University of Maryland or the University of Michigan to develop a company to turn a profit, or for a certain tech company to make and sell a product. The idea is to address fundamental questions and issues that are inhibiting or could accelerate the whole field.”\n\nPut another way, the center will advance regulatory standards and lead to foundational insights in a precommercial way, such that all companies in the extended-reality space will benefit, Varshney said. He added. “It’s a win-win-win across university, government, and industry, so it is really an amazing center in that regard.”\n\nReferences\n\nMIXR: Center for Medical Innovations in Extended Reality. Accessed: Apr. 11, 2023. [Online]. Available: https://www.mixrcenter.org/\n\nIUCRC Phase I: University of Maryland, College Park: Center for Medical Innovations in Extended Reality (MIXR), Award Abstract # 2137229. Accessed: Apr. 11, 2023. [Online]. Available: https://www.nsf.gov/awardsearch/showAward?AWD_ID=2137229&HistoricalAwards=false\n\nIUCRC Phase I: University of Maryland, College Park: Center for Medical Innovations in Extended Reality (MIXR), Award Abstract # 2137187. Accessed: Apr. 11, 2023. [Online]. Available: https://www.nsf.gov/awardsearch/showAward?AWD_ID=2137187&HistoricalAwards=false\n\nIUCRC Phase I: University of Maryland, College Park: Center for Medical Innovations in Extended Reality (MIXR), Award Abstract # 2137207. Accessed: Apr. 11, 2023. [Online]. Available: https://www.nsf.gov/awardsearch/showAward?AWD_ID=2137207&HistoricalAwards=false\n\nhttps://www.embs.org/pulse/articles/new-center-primes-the-extended-reality-frontier/\n\nJune 21, 2023\n\nEngineering’s New Immersive Flight Simulator Will Test the Rigors of Flight for Safer Skies\n\nBy Maggie Haslam\n\nLast week, I made an unexpected stop at the McDonald’s drive-thru in the most literal sense. As I barreled toward the parking lot, the rotors of the Sikorsky S-76 helicopter I was flying sheared the golden arches clean off the building’s façade.\n\nNo one was hurt, and I didn’t scrap a $20 million chopper in the process. The removal of my virtual reality (VR) headset transported me instantly (and with relief) to a spacious lab on the fourth floor of the IDEA Factory at the University of Maryland. But my churning stomach and frayed nerves reflected an all-too-realistic seven minutes in the cockpit, and a grim realization: Flying is hard.\n\nThis crash-and-burn scenario was enabled by the new Extended Reality Flight Simulation and Control Lab, launched by the Department of Aerospace Engineering this spring. It’s the first university-based facility in the United States to reproduce different flying conditions in various types of aircraft through motion-based VR simulation and haptics, which provide information through tactile feedback like rumbling and vibrations.\n\n“Our objective is to increase immersion and recreate scenarios that are difficult to simulate otherwise,” said Assistant Professor Umberto Saetti, who founded and directs the lab—“and ultimately, increase flight safety.”\n\nSaetti’s setup is about as immersive as you can get without leaving terra firma: What looks like an upscale gaming chair bolted to an elevated platform seems to magically transform once you buckle in and gear up. VR goggles conjure an empty cockpit and a full instrument panel at your virtual fingertips, with a sun-drenched tarmac just beyond. Headphones flood your ears with the roar of the engine—and in my case, the thump of the rotor blades—with the platform oscillating in response to the controls: a gentle nudge of the cyclic (or control) stick forward and my chopper comes to life, pitching me up and out over the small town beyond the airport.\n\nHe developed his lab after seeing the limitations of conventional flight simulators, which use big-screen projectors built on top of gigantic motion bases, with a price tag of several millions of dollars (compared to Saetti’s set up, which runs around $400,000). And while they excel at training pilots to operate just one type of aircraft, at UMD’s lab, the sky’s the limit: With just a few keystrokes, a variety of aircraft can be flown through the simulator, from a Black Hawk helicopter to an F-18 fighter jet. Within a few months, a lunar lander will join the rotation.\n\nThe team is also testing a full-body haptic feedback suit, traditionally used for gaming, to explore new methods for providing sensory cues to help navigate low-visibility situations, hostile flight scenarios or visually impaired pilots. Made by Teslasuit, the snug, Catwoman-like getup I wriggled into in the IDEA Factory ladies’ room is outfitted with over 100 transcutaneous electrical muscle stimulation patches, similar to what is used in physical therapy, to replicate everything from the feeling of rain on your skin to the jolt of a bullet puncturing body armor. Gentle tingling on my shoulders during a separate, blindfolded simulation nudged me to maneuver left or right to keep my plane level.\n\n“If you only feel the motion of the aircraft and can still fly without vision, that could be useful in a number of scenarios,” said Michael Morcos, a graduate student working in Saetti’s lab. “We’re trying to prove that’s actually possible.”\n\nCertain weather-related flight conditions, such as the fog encountered by John F. Kennedy Jr. off the coast of Martha’s Vineyard during his fatal flight in 1999, can put the cues a pilot gets from their instruments (which are correct) in direct opposition with what they are feeling; haptics, said Saetti, could help eliminate confusion about what’s up and what’s down. The suit—which monitors biometrics like heart rate, pulse and cardiorespiratory activity—could also help pilots track external activity like approaching aircraft.\n\nSaetti’s team is currently conducting research for the U.S. Army and Navy, as well as NASA, which together have provided the lab with $1.78 million in funding so far this year. In the future, the researchers plan to work with kinesiology Professor Bradley Hatfield to monitor brain activity and track stress and other human responses to flight.\n\nSomeday, the lab’s haptic innovations could take flight on actual aircraft to reduce a pilot workload and enable more difficult missions without compromising safety. “Our job is to come up with and demonstrate new ideas, then the companies can do the rest,” said Saetti.\n\nAlthough precision flying runs in my blood—my grandfather piloted B-24 bombers during WWII—it’s clear I have a lot to learn. That’s the beauty of the simulators, said Saetti: Whether you’re a seasoned military or commercial flier or a land-loving writer, you can undergo incredibly realistic, often tricky flight experiences without leaving the ground.\n\nOr imperiling someone’s Big Mac.\n\nhttps://today.umd.edu/up-in-the-air-without-leaving-the-ground\n\nMay 05, 2023\n\nPerformance at The Clarice and its Virtual Reality Element Bring Listeners to Chesapeake Bay\n\nBy Sala Levin ’10\n\nOn a precariously narrow two-lane roadway, bordered on both sides by water lapping nearly at the asphalt surface, a string ensemble performs, seemingly oblivious to its surroundings.\n\nThat’s because the four musicians of the Tesla Quartet aren’t really performing on this causeway, which leads to Hoopers Island—three watermen’s villages perched in the Chesapeake Bay off the coast of Dorchester County. They’re there thanks to augmented reality (AR) which blends the virtual world with the physical one, to tell a musical story about how climate change is ravaging this part of Maryland.\n\nOn Sunday, The Clarice Smith Performing Arts Center will present “Rising Tides,” a new musical performance comprising a series of commissioned pieces reflecting how the state of Maryland, especially the water-centric Eastern Shore, is seeing communities, farmland and public infrastructure increasingly succumb to rising sea levels and other consequences of climate change. Concertgoers can watch and listen to the pieces in AR through the app ImmerSphere, which places the musicians in the spots that influenced the composers.\n\nRichard Scerbo, artistic planning program director at The Clarice, said he has long been considering “how the music that we’re programming here at The Clarice can have an impact on our communities and speak to social issues of our time,” he said. “Climate change has been on my list.”\n\nSo he, along with the Tesla Quartet (who are being presented as part of The Clarice’s Visiting Artist Program), approached Maryland-based composers Alexandra Gardner and Adrian B. Sims ’22 to see if they might be interested in writing pieces that spoke to the impact of climate change on the Chesapeake Bay and the people who live on its shores. “The project sounded right up my alley,” said Gardner. “A lot of my work is inspired by the natural world and natural sciences, and of course I’m concerned about climate change.”\n\nThe timing was serendipitous: Gardner and her friends had recently visited Hoopers Island and been struck by the proximity of the water to the skinny causeway that connects the islands to the mainland. “It was a really sobering experience to drive out there and see the ditches beside the road as you approach the island full of water on a dry summer day,” she said. “We were like, ‘Wow, what’s it like to drive this when it’s actually raining, or at night when there’s no light?’”\n\nGardner’s piece is made up of three movements, titled “Causeway,” “Ceremony” and “Ghost Pines.” Each is inspired by a location on Hoopers Island, locations that can be seen—and heard—through ImmerSphere. In the app, the Tesla Quartet’s music is complemented by the ambient sounds of the environment surrounding them: the gentle splashing of water, the whoosh of wind through tall grasses, or the eerie silence of a pine forest whose trees have been killed off by saltwater encroachment.\n\nThe “Ghost Pines” movement is “noisy and scratchy,” Gardner said, echoing “what I imagine (is) the sound of a tree having the nutrients sucked out of it.”\n\nGardner hopes that the performance—and its AR/VR component—will encourage people to face the issue of climate change in the state. “People often don’t realize there’s a problem until they see it or are in it, so this is a way to immerse people in this environment so they can experience a little bit of it, or a feeling of it, even if they can’t actually be there,” she said.\n\nhttps://today.umd.edu/hearing-and-seeing-climate-change-through-music-and-ar\n\nMarch 7th, 2023\n\nGreg Muraski\n\nStudents studying supply chain management at the University of Maryland’s Robert H. Smith School of Business are learning their way around a warehouse – without even having to leave their dorm rooms.\n\nThe Smith School’s Humberto Coronado is reimagining the way undergraduate supply chain students learn about warehouse management, using virtual reality technology for a completely new immersive learning experience.\n\nThanks to a University of Maryland Teaching and Learning Innovation grant, Coronado was able to buy 45 pairs of VR headsets and adapt an existing supply chain management course to include an experiential learning module driven by immersive technology. He won the grant last summer as one of the 90 projects to receive funding in the university’s multimillion-dollar commitment to transform teaching and learning. Coronado is the academic director of the Master of Science in Supply Chain Management program and teaches both undergrad and graduate students and hopes to eventually also use the technology with master’s students.\n\n“Using VR is a game-changer for the way we can teach many supply chain concepts,” says Coronado, who had never used the technology before getting the grant to buy the goggles. He worked with an outside company to design the virtual warehouse space where his students will use the technology to explore and learn.\n\nStudents received the VR goggles and accompanying handheld controllers on Feb. 10, experiencing the virtual reality platform together – many for the first time. They learned the basics of how to use the technology in class, and each student got to take a set with them for the month-long series of learning modules they’ll be completing both individually and in teams. Outside of class, teams will agree on meeting times, when they’ll put on their VR goggles wherever they are – at home, in their dorm room, anywhere – and meet up in the virtual warehouse together to complete assignments.\n\n“Before this technology, I would have to stand in class and ask students to imagine walking into a distribution facility. But how can I ask my students to imagine something they’ve never seen?” Coronado says. “This technology allows me to get rid of the ‘imagine’ and say, ‘Put on your goggles, go into this facility.”\n\nSo what do they see when they put on the headsets? They’re in a huge distribution warehouse together, where they see each other’s avatars and, using the handheld controller, they’re able to move around, interact with each other and assess the facility together. In some cases, they’ll be pinpointing problems – like safety hazards or inefficiencies that could cause slow-downs and mistakes – and how to fix them.\n\nAfter trying the headsets for the first time, Coronado’s students were surprised at just how realistic the virtual world felt.\n\n“It gave a totally different perspective than just watching a video,” said Krystal Taveras ’24. “I’m really excited about how much learning I’m going to be able to do. I’m a supply chain major and I think it’s going to be really beneficial for me to get a real look at what companies use and what they do.”\n\nAnthony Marcelli ’24 is also excited for the learning opportunity and how it might help prepare him for a career. “I’ve talked to a lot of different employers and recruiters in supply chain and 90% of what you learn happens in the warehouse, hands-on, in-person, and that’s just something you don’t get sitting in a classroom,” he said. “I think this is just going to give us some opportunities that we never would have had before.”\n\nCoronado said that’s really the goal – for students to use the experience to help get the jobs they want when they graduate.\n\n“They will be prepared with a higher level of knowledge and capabilities – far beyond what they can get just from a textbook,” he says. “They’ll be able to go into a job interview with real examples from these facilities.”\n\nCoronado says large companies with logistics and distribution operations – such as Amazon, DHL – are already using this kind of technology to train their employees and Smith students will have a leg up when entering the job market.\n\n“They are going to be exposed to these technologies that are driving this field and they will get an idea of the new skill sets that are required.”\n\nCoronado hopes future iterations of the course could include training content from companies to create a pipeline of students they could hire.\n\nHe says in the near future, many warehouse facilities are going to be fully automated, with no people inside the facility, where everybody will be sitting somewhere else doing everything from computers and using this VR technology.\n\n“I want our students to see all the technologies that are playing into this new field that is growing so fast and requiring new skill sets. I want them to be exposed to that understanding that supply chain management is driven by technology. You can't do anything in supply chain management if it's not with technology.”\n\nhttps://www.rhsmith.umd.edu/news/smith-students-learn-supply-chain-management-immersive-vr\n\nMaryland The Daily Record\n\nSeptember 11, 2022\n\nGina Gallucci-White\n\nWhen people think of virtual and augmented reality as well as other immersive media technologies, many focus on the entertainment aspect, but these tools have become an asset in medical care.\n\nIn May, the University of Maryland School of Medicine announced a partnership with the University of Maryland, College Park, and the University of Michigan to create the Center for Medical Innovations in Extended Reality (MIXR). Established through $5 million from the National Science Foundation’s Industry-University Cooperative Research Centers program, the center aims to accelerate the development of these technologies to use in clinical trials and eventually more broadly in medical care.\n\nCompanies like Microsoft, Meta, Google and others will also be providing funding and expertise to the team to develop, test and certify these technologies to use in the medical field.\n\n“Virtual reality has many uses in a health care setting,” said Amitabh Varshney, dean and professor at the University of Maryland, College Park, College of Computer, Mathematical and Natural Sciences and the MIXR lead-site principal investigator. “For training, we’ve already done studies that show people can retain information better— nearly 9% better—than if they were to view the same information on a 2-D desktop screen.”\n\nStaff has also developed virtual reality training prototypes for specialized surgical techniques like an emergency fasciotomy where the fascia is cut to relieve tension or pressure to treat the resulting loss of circulation to an area of tissue or muscle.\n\nFor augmented reality, the team has developed a point-of-care ultrasound prototype that displays information directly on the patient so the physician does not have to keep looking away to a monitor.\n\n“These examples are just the beginning,” notes Varshney. “With the added momentum and synergy that our new center will bring — including working with federal regulatory experts to bring new devices and technologies to clinical settings more quickly — we anticipate a time in the not-too-distant future when immersive headsets will be just as commonplace in a hospital setting as a stethoscope.”\n\nOfficials note MIXR is needed because of the rapid movement in the private sector to advance new immersive technologies used for gaming, entertainment, education and training. This has filtered down to scientists and physicians using these same visualization tools in a clinical setting or for advanced medical training.\n\n“We believe our new center will serve as a focal point for industry to collaborate — at the highest level — with academia and health care professionals to build, test and certify new devices that can greatly improve patient care and medical training,” Varshney said.\n\nIn 2017, Varshney along with Dr. Sarah Murthi launched the initial work with the Maryland Blended Reality Center (MBRC).\n\n“It has been tremendously exciting and rewarding to work with Dr. Sarah Murthi and her colleagues in Baltimore,” he said. “They represent the very best in emergency medicine. Now, with added participation from technology leaders like Google, Microsoft and others, we believe we’ve developed a critical mass to move our ideas forward quickly and efficiently. The common theme of using technology to improve patient outcomes has been driving our efforts from the start. This is particularly satisfying for me as a computer scientist.”\n\nMBRC will continue to work on other immersive projects that are not directly related to medicine and health care including implicit bias training and using immersive environments to train foreign language professionals at a very high level. They have also collaborated with artists and performers to bring new ideas in classical music and opera to the stage.\n\n“So, while some of the new activities of MIXR may overlap with our work in MBRC, we see them as separate, yet complementary, entities,” Varshney said.\n\nSome of the new activities that MIXR staff are exploring have not yet been used in a medical setting to a great extent. Murthi is working on helping patients cope with physical and emotional trauma through immersion in another world with a focus on quadriplegic patients who are hospitalized with acute spinal cord injury. Another collaborator, Dr. Luana Colloca, is a physician scientist using immersive technologies to reduce the need for addictive opioid pain medications.\n\nVarshney and his UM colleagues are in the process of finalizing a HoloCamera studio featuring more than 300 immersive cameras fused together to create a 3-D visualization technology images to help train health care providers performing difficult medical procedures.\n\n“We are in the final stages of addressing technical challenges that have arisen in our fusing together 300 immersive cameras,” he said. “The system works but we need it to work seamlessly for what we have in mind — high-end training for emergency medical procedures. We anticipate working with our partners in Baltimore on user-study training scenarios within the next six months.”\n\nThe collaborators have planned a three-day kickoff in College Park in October to bring all the MIXR partners together including scientists, physicians, private technology firms, and federal regulatory experts. The event is designed to brainstorm their agenda for the immediate future and the next five years. “We are certainly excited for what is yet to come,” Varshney said.\n\nhttps://thedailyrecord.com/2022/09/11/new-center-for-medical-innovations-in-extended-reality-launched-at-um-umsom/\n\nNew York Times\n\nSome care facilities are giving older adults a way to visit their pasts to boost their well-being.\n\nMay 6, 2022\n\nJohn Faulkner, 76, was becoming emotionally withdrawn before he arrived at Central Parke Assisted Living and Memory Care, the community where he lives in Mason, Ohio. He had once been an avid traveler, but cognitive decline ended that, and he became socially isolated. By the time Mr. Faulkner arrived at Central Parke, he would sit alone in his room for hours, according to Esther Mwilu, who organizes activities for the community.\n\nHis treatment plan for dementia-related anxiety included antipsychotic drugs and reminiscence therapy, a decades-old practice in which older adults engage with reminders of their youth — like music or personal photographs — meant to bring about memories and cultivate joy and meaning.\n\nMr. Faulkner was underwhelmed by the nostalgia. So the staff at Central Parke tried again but used virtual reality. While studies suggest that traditional reminiscence therapy can significantly improve the well-being of older people, V.R. has the potential to make it more immersive and impactful. By putting on a headset, Mr. Faulkner could walk along the virtual Cliffs of Moher in western Ireland, just as he’d done with his wife several years earlier.\n\nThat was a turning point. Now, three months later, he has a 45-minute V.R. reminiscence therapy session every Monday. Ms. Mwilu said he requires less medication for anxiety and is more social. He has even started teaching classes for other residents like how to make paper airplanes.\n\nRoughly a half-dozen companies today focus on providing V.R. reminiscence therapy for seniors in care communities. One of the largest of these, Rendever, works with more than 450 facilities in the United States, Canada and Australia, while another, MyndVR, has partnered with several hundred.\n\nThey are part of a growing trend of using virtual reality in health care, including treating patients with trauma and chronic pain. And with the number of people over age 65 expected almost to double by 2060 in America, the need for technological aides like V.R. for elder care is only increasing. More than 11 million Americans act as unpaid caregivers for a relative with dementia. The middle-aged “sandwich generation,” juggling careers and multiple care-taking roles, is looking to V.R. and other technologies, such as robo-pets, for support.\n\nEddie Rayden of Rhode Island said his 91-year-old mother, Eileen, brightened when using V.R. to see the Cleveland neighborhood where she grew up. “She immediately lit up,” he said. “All of a sudden, she was standing in front of the house she hadn’t been to in 80-plus years.”\n\nHow it works\n\nThe concept of reminiscence therapy goes back to 1963. Many psychiatrists at the time discouraged anything that seemed like living in the past, but one, Robert Butler, who later founded the National Institute of Aging, argued that seniors could get therapeutic value from putting their lives into perspective. Since then, psychologists have increasingly recommended using old wedding videos or favorite childhood meals as tools to benefit older people, including those with dementia. Experts say seniors troubled by declines in short-term memory often feel reassured when recalling the distant past, especially their young adulthood.\n\nOver the past decade, faster and more powerful computing have made virtual reality more realistic and have led to studies showing how older people can use V.R. to re-experience meaningful parts of their lives. In 2018, researchers from the Massachusetts Institute of Technology found that virtual reality reduced depression and isolation among seniors. Other studies have suggested that V.R. reminiscence improves morale, engagement, anxiety and cognition by stimulating mental activity, though it cannot necessarily reverse cognitive decline.\n\nStill, larger studies are needed before everyone over the age of 75 is putting on a headset. Dr. Jeremy Bailenson, director of Stanford’s Virtual Human Interaction Lab, is currently leading a clinical trial in 12 states to try to get more data at scale.\n\n“I would never want V.R. to completely replace non-V.R. reminiscence therapy,” he said, but “different people need different tools.”\n\nSenior communities today can pay companies for headsets and access to a library of virtual experiences, many of which are designed for reminiscence therapy. Seniors can participate individually or, more typically, in group sessions.\n\nPrescriptions are not required, and participants often outnumber the headsets. Caretakers and researchers said they start to see benefits after multiple sessions over one to two months. Stephen Eatman, a vice president for Sunshine Retirement Living, which manages Central Parke, said the company’s use of antipsychotics has decreased as much as 70 percent in seniors using V.R. therapy.\n\nIn addition to reliving trips to places like Ireland, users can teleport to nightclubs that remind them of their youth. MyndVR offers visits to flamenco, ragtime and classical music venues, complete with musicians and actors dressed in the style of the day.\n\nFamily members have created location-based life stories, including vacations and childhood homes, for those undergoing V.R. therapy.\n\nBut users are not limited to prepackaged nostalgic experiences. Relatives, friends and caretakers can also record a 3-D video of a wedding or other event that the person can virtually attend over and over to reinforce new memories. Other family members search Google Streetview for important places in a senior’s life that can be converted into V.R. realms.\n\nDorothy Yu, a business consultant from Weston, Mass., had the streets around the University of Missouri campus converted to V.R. so her father could see the buildings where he’d been a professor. Now a 90-something resident of Maplewood Senior Living in Massachusetts, it helps him remember the work he did there with pride, both during the session and afterward, she said.\n\n“I’ve never seen anything like the reactions to this technology,” said Brian Geyser, a vice president at Maplewood, which now offers V.R. in each of its 17 communities, which are mostly in the Northeast.\n\nNot right for everyone\n\nTo participate in V.R. therapy, you have to strap on a headset that covers your eyes and blocks all light, but for the 3-D world you enter. For some older people who didn’t grow up with computers, such immersive technology can be overwhelming, said Amanda Lazar, a human-computer interaction researcher at the University of Maryland.\n\n“The face is a very personal part of the body,” said Davis Park, vice president of the Front Porch Center for Innovation and Wellbeing, a nonprofit that brings technology, including V.R., to senior communities. Someone with dementia may worry when their eyes are covered or have trouble understanding the purpose of strapping a machine over their face at all, Mr. Park said.\n\nTo mitigate these risks, Sunshine Retirement limits V.R. activities to certain rooms where seniors can move around safely. They also avoid showing seniors places that could set off traumatic memories, said Mr. Eatman, but people’s reactions are tough to predict.\n\nMost providers also limit V.R. reminiscence sessions to 45 minutes, though even at that length, it can cause dizziness and headaches, especially with certain medications. Headsets may also be too heavy for some older adults’ necks or may not account for hearing and vision impairments.\n\nWhile companies like Rendever have V.R. simulations that can bring back good memories, headsets can sometimes overwhelm patients, especially those with dementia or who are easily confused.\n\nAnother downside: V.R. can be socially isolating. Traditionally, reminiscence therapy has encouraged groups of seniors to bond over special memories with one another and caretakers. “If someone puts on a headset, the people around them are blocked out,” said Dr. Lazar.\n\nThe Iona Washington Home Center in Southeast D.C., tries to solve this by projecting seniors’ V.R. experiences onto a 2-D screen for others to watch and discuss. The center, run by a nonprofit, received its V.R. headsets through a government grant, which is common for retirement communities. “People around here don’t have much money,” said Keith Jones, the program specialist. “Most of them didn’t get to see the world.” When he takes groups to another country in V.R., Mr. Jones positions the few members who’ve been there at the head of the table to share their memories.\n\nThe future of the memory metaverse\n\nIn the future, V.R. may offer another way for seniors to combat loneliness — by stepping into the experience with their loved ones.\n\nTamara Afifi, a researcher at the University of California, Santa Barbara, has studied V.R. and dementia and is investigating new technologies that let relatives take trips together. Ms. Rayden, who is a 91-year-old resident of Maravilla Senior Living, a community in Santa Barbara, participated in Dr. Afifi’s research. She and her 66-year-old son, Mr. Rayden, took a tour of her old Cleveland neighborhood together, despite his being in Rhode Island.\n\n“I showed him where we played hopscotch and sledded in winter,” she said. “It was important that he knew the home we had and the neighborhood. It was my childhood. It brought back wonderful memories.”\n\nSince Ms. Rayden’s husband died in 2019, she’s struggled with sadness and loneliness. Virtual reality has allowed her to take her son to Florida’s Intracoastal Waterway, where she’d enjoyed fishing vacations with her husband. “He loved fishing,” she said. “Such happy memories.”\n\nRuth Grande, executive director at Maravilla, said that adult children can “stop being caretakers for 30 minutes” when they have these experiences with their loved ones. “They remember what it’s like to enjoy being with their relative,” she said.\n\nMatt Fuchs is a freelance writer based in Silver Spring, Md.\n\nhttps://nyti.ms/3FpUuAe\n\nMay 9, 2022\n\nAmitabh Varshney, professor of computer science and dean of the College of Computer, Mathematical, and Natural Sciences, leads a new multi-institutional center to advance medical innovations and regulatory science for extended reality technologies.\n\nUltrasound data displayed directly on a patient via augmented reality headsets. Immersive “grand rounds” for medical students and faculty even when they’re in different locations. Virtual reality landscapes matched with classical opera to transport people with painful injuries outside of themselves, reducing the need for potentially addictive opioids.\n\nThese medical examples of extended reality (XR)—the umbrella term used for technology based in virtual and augmented reality or other immersive media—are already being prototyped or tested in clinical trials. But its widespread use in hospitals and other health care settings is currently hampered by technical challenges and sparse regulatory guidelines.\n\nNow, with $5 million from the U.S. National Science Foundation (NSF) and technology titans including Google, Microsoft and Meta (formerly known as Facebook), a trio of academic institutions are collaborating with industry and the federal government to develop, test and certify XR technologies in medicine and health care.\n\nThe new Center for Medical Innovations in Extended Reality, known as MIXR, joins University of Maryland computer scientists and engineers with physicians and clinicians at the University of Maryland School of Medicine in Baltimore and the University of Michigan to improve medical training, patient management and health care outcomes across all areas of clinical practice.\n\nThe award is part of NSF’s Industry-University Cooperative Research Centers (IUCRC) program, designed to jumpstart breakthrough research by enabling close and sustained engagement between industry innovators, world-class academic teams and government agencies.\n\nBehrooz Shirazi, acting deputy division director of the NSF’s Division of Computer and Network Systems and a program director for IUCRC, called MIXR one of the first national centers at the intersection of medical and computing sciences. “We expect this vibrant collaboration to produce significant societal and health care impacts,” he said.\n\nIn addition to Google, Microsoft and Meta, other technology companies involved in MIXR are Sony, Magic Leap, Health2047, GigXR, Brainlab and apoQlar.\n\nAnother key partner in the MIXR initiative will be federal regulatory experts working at the U.S. Food and Drug Administration, ensuring that safe, effective and innovative clinical solutions make it to patients as soon as possible.\n\n“We’ll work closely with our industry and government partners to answer any scientific questions regarding regulatory evaluations and decisions needed for the widescale clinical use of these devices,” said Amitabh Varshney, professor and dean of the College of Computer, Mathematical, and Natural Sciences at the University of Maryland.\n\nVarshney is the lead site principal investigator on the project, and is joined by partner site PI’s Sarah Murthi, M.D., an associate professor of surgery at the University of Maryland School of Medicine, and Mark Cohen, M.D., a professor and vice chair of surgery at the University of Michigan Medical School with appointments in pharmacology and biomedical engineering. All three have extensive experience developing and using immersive technologies in a medical setting.\n\nVarshney and Murthi co-direct the Maryland Blended Reality Center (MBRC), launched in 2017 as part of MPowering the State, the strategic partnership between the University of Maryland, College Park and the University of Maryland, Baltimore.\n\nEarly projects out of MBRC focused on prototyping new diagnostic tools to assist physicians at the renowned R Adams Cowley Shock Trauma Center in Baltimore, where Murthi is a critical care doctor and director of the critical care ultrasound program. This includes innovative AR medical displays that could improve how bedside procedures are done.\n\nMBRC clinicians also teamed up with the Maryland Institute College of Art to test a virtual reality platform that can help patients deal with physical and emotional trauma through immersion in another world, with a focus on quadriplegic patients who are hospitalized with acute spinal cord injury.\n\nIn 2018, Murthi and Varshney co-authored an op-ed in the Harvard Business Review that detailed how augmented reality could improve patient care and lower costs in hospital settings.\n\n“Immersive technologies have the potential to fundamentally change, improve and reduce the cost of medical training and of maintaining clinical skills across all aspects of health care,” Murthi said.\n\nCohen, who leads the Center for Surgical Innovation and trains new physicians at Michigan, said that using XR in medical rounds—adding in virtual reality-based illustrations or augmented reality data overlaid on a patient—makes for a richer experience for both teacher and student, both in-person or virtually observing from hundreds of miles away.\n\n“We realized that having this ability to interact virtually with both the patients and residents—pulling up holographic windows and showing diagnostic imaging and labs—would greatly enhance the educational experience in these traditional grand rounds,” he said.\n\nIn an interview published last year, Cohen said he is also interested in combining XR technology with machine learning, hoping to leverage sophisticated immersive diagnostic imaging resources with artificial intelligence algorithms to “better predict when diseases will flare up, and how to improve the way we follow and treat chronic diseases like heart failure, cancer and diabetes.”\n\nThe MIXR initiative is heavily dependent on powerful computing resources. At Maryland, those resources will be handled by the University of Maryland Institute for Advanced Computer Studies. This includes building and maintaining a soon-to-be-unveiled “HoloCamera” studio, where more than 300 immersive cameras are fused together to bring unique visualization technology to bear on immersive medical environment captures.\n\nThe new camera system can be used to record cinematic-quality 3D demos of surgeons teaching intricate procedures like a lower extremity fasciotomy, a limb-saving technique of cutting the sheath of tissue encasing a muscle to treat for loss of circulation.\n\nJoseph JaJa, professor and chair of the Department of Electrical and Computer Engineering at UMD, is also providing expertise. As the lead-site co-PI, he will support the integration of high-performance computing and machine learning into the XR technology being developed and use his extensive experience working with industry to foster stronger collaborative efforts.\n\nBarbara Brawn, currently working with Murthi and Varshney as the associate director of MBRC, will act as the industry liaison contact between MIXR researchers and technology companies keen to see their latest hardware and software tools used to save lives and improve medical training.\n\n“The synergy in MIXR will be contagious,” Varshney said. “Our industry partners will push forward new ideas and novel technologies. The scientists and physicians will help refine and test those ideas. And we both will work with the FDA to bring these technologies from the lab to the proper health care setting where they can have an exponential impact.”\n\n—Story by Tom Ventsias\n\nThe Department welcomes comments, suggestions and corrections. Send email to editor@cs.umd.edu.\n\nhttps://www-hlb.cs.umd.edu/article/2022/05/bringing-health-care%E2%80%99s-vision-tomorrow-focus\n\nApril 28, 2022\n\nby Hayleigh Moore\n\nTwo immersive media projects developed by iSchool students featured in Arts for All showcase.\n\nAJ Rudd, an HCIM student, and Aishwarya Tare, an Information Science student, participated in the Immersive Media + Arts for All Showcase earlier this month which immersed attendees into interactive media exhibits developed by current students and other members of the UMD community, as well as guest artists. Held from April 2-8, 2022, the Showcase demonstrates the ways that immersive media can bring the arts into dialogue with cutting-edge digital technology to transform public spaces and further social good through installations, performances and talks.\n\nAJ Rudd, an HCIM student, and Aishwarya Tare, an Information Science student, participated in the Immersive Media + Arts for All Showcase earlier this month which immersed attendees into interactive media exhibits developed by current students and other members of the UMD community, as well as guest artists. Held from April 2-8, 2022, the Showcase demonstrates the ways that immersive media can bring the arts into dialogue with cutting-edge digital technology to transform public spaces and further social good through installations, performances and talks.\n\nSpray AR\n\nTo address the issues of accessibility and the “emphemeral nature of graffiti,” AJ Rudd joined a student team to develop an original app called Spray AR, which allows a user to experience spray painting using their mobile device without using actual paint. AJ’s co-collaborator, Jason Alexander Fotso-Puepi, initially presented the idea for a graffiti experience using augmented reality (AR). The app was developed using Unity, a cross-platform game engine for creating 2D and 3D multiplatform games and interactive experiences.\n\n“Due to the issue of public destruction of property, graffiti is not always accessible to everyone. AR afforded us the ability to provide anyone with the experience of spray painting a building, a bridge or another area of their choosing,” AJ said.\n\nA demo of Spray AR is available to view on YouTube which shows how a user can create graffiti on nearly any surface. The basic mechanics already in place will later be refined by AJ and his co-collaborators before introducing more complex functionalities, such as one of AJ’s original ideas to integrate blockchain technologies within the app. Blockchain would allow for verification of artworks in the real world which directly addresses the issue of graffiti’s short life in physical spaces.\n\nThe desire to bridge the gap between art and technology while also remaining local helped inspire AJ’s decision to enroll in the iSchool’s Master’s of Human-Computer Interaction (HCIM) program. AJ previously earned his Bachelor’s degree in studio art with an emphasis on sculpture from UMD in 2019. He will be graduating this May after successfully defending his Master’s thesis with plans to pursue a career in AR and eventually a PhD to land a more research-oriented role.\n\n“Sincerely, Ecocriticism”\n\nThe idea of sending a postcard to your loved ones is all about telling them what you’re experiencing, but behind the scenery found on these postcards, there is often a sinister, not-so-picture-perfect reality. The “Sincerely, Ecocriticisms” project created by iSchool undergraduate student, Aishwarya Tare, challenges the divide between our everyday lives and the natural world by using a physical form that many of us hold nostalgia toward: postcards. Using 100% recycled card stock, Photoshop, and Unity, Aishwarya created different versions of “picture perfect” postcards to show things often omitted from the scenery that display unsustainable behaviors.\n\n“We tend to only send postcards that have really pretty pictures, but the truth is that even on the prettiest beaches, there is trash, and boats, surfers, and other human footprints. Having to send the picture to a loved one even when it isn’t the most beautiful untouched image, forces us to rethink how we view nature as a commodity,” said Aishwarya.\n\nAishwarya said this idea was inspired by an event she had attended at the Smithsonian called FUTURES with artist-in-residence Carlos Carmonamedia. He had created these postcards that you can send to yourself in the future which forced attendees to reflect on their current reality.\n\nOver the past two years, Aishwarya has been working on her startup, Chat Health, a virtual assistant she developed to help students more easily access their campus health resources and receive personalized and empathy-driven health information. Chat Health won the Quattrone Venture Track’s second-place honors at the 2022 Pitch Dingman Competition, an annual competition that provides the university’s most talented student entrepreneurs the opportunity to compete for seed funding and venture development resources. She is also an intern for the Mixed Augmented Virtual Reality Innovation Center (MAVRIC), and a Do Good Institute Fellow, where she is building an augmented reality community garden for mental health.\n\nThe Immersive Media + Arts for All Showcase is a weeklong event produced and presented by the University of Maryland’s Immersive Media Design (IMD) program, the College of Arts and Humanities, the College of Computer, Mathematical, and Natural Sciences and the campuswide Arts for All initiative.\n\nMore About Arts for All:\n\nThe University of Maryland’s new Arts for All initiative partners the arts with the sciences, technology and other disciplines to develop new and reimagined curricular and experiential offerings that nurture different ways of thinking to spark dialogue, understanding, problem solving and action. It bolsters a campus-wide culture of creativity and innovation, making Maryland a national leader in leveraging the combined power of the arts, technology and social justice to collaboratively address grand challenges.\n\nhttps://ischool.umd.edu/news/putting-the-ar-into-art/\n\nFebruary 11, 2022\n\nBy Annie Krakower\n\nStudents Create Virtual Tour to Showcase Decades of U Street Culture\n\nEven though she grew up with U Street practically in her backyard, Montgomery County native Maxine Hsu ’25 never knew just how many cultural gems lined the D.C. corridor: the stages of jazz and big band legends, longstanding family-owned shops and restaurants, and homes and haunts of Black trailblazers. Now, the computer science major and a team of Terps are helping others step out on “Black Broadway”—whether they live a few miles away or a few thousand.\n\nDuring a three-week micro-internship this winter with local extended reality marketing company Capitol Interactive, Hsu and fellow students Saniya Nazeer ’25 and Kia Williams ’24 used a 360-degree camera to create an interactive tour of historic U Street, providing VR views to immerse users in locations that played a key role in Black Washington, ranging from restaurants to theaters to banks. The project will become part of Black Broadway on U, a multiplatform initiative created by alum Shellée Haynesworth ’84 to amplify the stories of the community.\n\n“It’s just interesting to me that there’s so much history in a place that’s so local, that’s so nearby,” Hsu said. “I would just love to be a part of the movement to preserve that history in D.C. because U Street is actually being gentrified, and so a lot of history is being lost.”\n\nThe collaboration was among Break Through Tech DC at UMD’s first Sprinternships—quick, jam-packed programs that offer students who identify as women or nonbinary and other underrepresented students real-world experience through a “tangible project that can be implemented,” said Kate Atchison, UMD’s site lead at Break Through Tech DC, which strives to make the technology industry more inclusive. The Terp trio working at Capitol Interactive, one of 15 host organizations, landed on the virtual tour idea, thanks to founder Joseph Cathey’s connection with Haynesworth.\n\nThe broadcast journalism alum launched the Black Broadway on U website in 2014 after driving around 14th and U streets with her grandmother, who had lived and worked there as a barber. She couldn’t believe how much the area—which endured widespread damage from 1968 riots and has been held up in recent decades as a prime example of D.C.’s gentrification—had changed.\n\nThat led Haynesworth on a historical deep dive, where she discovered rich stories that went beyond music and entertainment by Cab Calloway and Billie Holliday to include details of civil rights activists like suffragette Mary Church Terrell, Black researchers and pioneers in science such as blood bank pioneer Dr. Charles Drew, and buildings designed by Black architects and financed by the Black community.\n\n“The history was so fragmented that we needed a platform or a destination where people could go to learn. The goal was to tell the story at the intersection of technology so it could have a digital destination and reach a wider audience,” she said. “I felt that it’s important to expand the narrative of the African American experience.”\n\nThe Sprinternship project fit perfectly with that mission. The three students met virtually with Haynesworth, then used the Black Broadway on U website as a guide for initial research. In the winter term’s final week, after Haynesworth had helped them contact and coordinate meetings at various U Street locations, Hsu photographed those places with a 360-degree camera while Williams and Haynesworth wrote the tour’s narrative and worked with Nazeer to compile everything. Haynesworth also secured a narrator and provided music and archival photos to round out the tour.\n\nThe team was able to capture around a dozen locations, including Ben’s Chili Bowl, a culinary community staple since 1958; Industrial Bank, one of the largest Black-owned and -operated commercial banks in the United States; and the Whitelaw Hotel, an important lodging and social center for African Americans during segregation.\n\nThe group is planning to share the virtual experience with the public in the coming weeks on the Black Broadway on U website.\n\n“It’s just amazing what you can do with technology. It makes me more excited to delve deeper into computer science,” Hsu said. “There are some technological innovations that haven’t even happened yet. Maybe I can help with that, and maybe I can help with other projects that can preserve history in this way.”\n\nFollow Black Broadway on U on Facebook and Instagram @blackbroadwayonu and on Twitter @blkbroadwayonu.\n\nhttps://today.umd.edu/a-vr-view-of-black-broadway\n\nVictoria Stavish\n\nNovember 5, 2021\n\nThe University of Maryland and Jigsaw, a unit within Google, announced a partnership to create virtual reality training aimed at improving police de-escalation and communication across the United States.\n\nThe partnership was born out of Jigsaw’s “Trainer” platform, which aims to use technology to improve the interactions between police and the communities they serve. Trainer provides its partners with improved virtual reality technology and systems to help their partners conduct research to better understand how officer training can be improved.\n\nRashawn Ray, a sociology professor and executive director of this university’s Lab for Applied Social Science Research, said that this university was chosen as a partner because of its prowess in the field of criminal justice and policing research, along with the University of Cincinnati, Georgetown University Law Center and Morehouse College.\n\nAs a partner, the lab can continue its research with better equipment and more collaboration across different departments and the other participating schools.\n\n“[Trainer] gives us the opportunity to collaborate more broadly, and bring more people in,” Ray said. “We’re moving at it in a very, very big way.”\n\nThe research team ultimately wants police departments to use virtual reality regularly in their training. This would be less expensive for police departments, add a more evaluative component to police training and allow for better communication and de-escalation training, said Connor Powelson, a sociology doctoral student working on this research.\n\n“What’s important is that [officers are] trained not on these lethal encounters, which are relatively rare. We need to train officers on these very common social interactions where they’re talking to people,” Powelson said.\n\nTheir partnership with the Trainer platform can help them do just that.\n\n“This program is focused on, ‘How do people talk to people?’ ‘How do officers talk to civilians in these policing situations?’” Powelson said. “We need to train officers on these more mild cases where they’re just talking to people as necessary.”\n\nLASSR has evaluated officer biases using virtual reality training for about four years. Previously, the research team asked police departments across the country to implement virtual reality headsets in their implicit bias training, which allowed the researchers to better understand whether current implicit bias training is effective.\n\n“What those trainings turn into, those in classroom implicit bias trainings … is a checkbox on a sheet instead of an understanding of, ‘Is this actually making officers better?’” Powelson said. “We want to add meat to the bones of other training programs.”\n\nThe research team focused on evaluating unconscious race biases, officer use of respectful or disrespectful language and how officers perceive situations based on the likelihood of criminality or victimhood, Powelson said.\n\nWhat they found was that officers who have more unconscious biases typically treat Black men with less respect. This outcome is more likely in situations where perceived criminality or victimhood of a person is unclear, Powelson said.\n\n“One huge reason that we started this program is because there’s no national database on police use of force,” Powelson said. “There’s very poor data on officer use of force that we can use to evaluate discrimination behaviors across policies, across departments.”\n\nThe new partnership with Trainer allows them to branch out their research and take advantage of more resources than before.\n\nPowelson said new collaborations with the computer science department at this university will allow the team to vary conditions such as character attitudes, skin tone, size, age and ability.\n\nThey also plan to work with neuroscientists to better track and understand facial expressions and eye movements in officers.\n\nGenesis Fuentes, a sociology doctoral student working on this research, said now that they are more collaborative and can work with virtual reality technology alongside people from other disciplines, it’s easier to develop and work with the software.\n\n“We understand that we’re social scientists,” Fuentes said. “We’re not going to sit here and try to create … virtual reality software.”\n\nThe team’s virtual reality software is useful beyond directly training police officers, Powelson said. They also use their technology and research to improve community relations by facilitating a positive and educational environment for officers to interact with the community.\n\nPowelson said the team has taken their virtual reality headsets to school assemblies, where they allow officers and students to experience the same situation virtually and then discuss why they acted differently than each other in the virtual situation.\n\n“It creates this conversation where trust can be built,” Powelson said. “Understanding can be built and ultimately it does a lot of good for the community.”\n\nhttps://dbknews.com/2021/11/05/umd-google-virtual-reality-police/\n\nSeptember 20, 2021\n\nby Marlia Nash\n\nOn Sept. 17, students filled up Kay Theatre to watch Maryland Night Live. For two hours, students doubled over in laughter at skits and cried at heartfelt musical performances by AwBi and Benny Roman.\n\nFollowing the performance, students met in the front entrance of The Clarice Smith Performing Arts Center to be greeted by the Festival Snapshots with Monumental Magazine marketplace.\n\nNextNOW Fest historically has been held at The Clarice, full of art galleries, musical performances and other interactive exhibits. In 2019, students would expect to have just a weekend full of these activities.\n\nMegan Pagado Wells, associate director of programming, wanted to focus on the expansion of NextNOW, expanding across the campus and disciplines. This was the first year students could see aspects of technology disciplines introduced to the arts, which made the festival to be inclusive to those who have those interests.\n\n“We’ve expanded to seven days of programming across campus from Monday to Sunday,” Wells shared. “We’ve [also] partnered with amazing partners … like Studio A and Stamp Gallery at the Parren J. Mitchell Art-Sociology building, with the UMD Art Gallery and the Michelle Smith Collaboratory for Visual Culture, with the UMD book lab … and of course the immersive media design showcase.”\n\nThe immersive media design program is a new component to both NextNOW and the university, with its premier being fall 2020. The immersive media design exhibit was featured in the Iribe Center and The Clarice.\n\nThe projects selected by current immersive media design students were popular among students strolling by. Students such as Caroline Dinh enjoyed these exhibitions and even picked out a favorite. Dinh, a freshman immersive media design major, shared that they loved “Within Reality.”\n\nBrayan Pinto, senior studio art major, is the creator of the project “Within Reality.”\n\n“My goal was just to make sure people have fun, come see it, like dance around pretty much,” Pinto shared. And students did exactly that. “My project is a visual effect project that takes the input from a camera and then outputs a special effect on top of it.”\n\nPinto revealed that he actually spent his summer on campus with other immersive media design students working on this project under the supervision of Jonathan Martin, lecturer in the immersive media design program.\n\n“It started with them coming up with their concept of designing, and then testing and prototyping it, and that was both something that they did individually,” Martin said.\n\nOne particular fan of the exhibit was the dean of the arts and humanities college, Bonnie Thornton Dill.\n\n“I loved watching the dancers as that went on, but I also really liked the meditative piece,” Dill said, referring to Pinto’s project and junior immersive media design and English major, Cassiel Arcilla’s project, “Meditations.”\n\nDill came and spoke at the Iribe to students about the Arts for All Initiative.\n\n“The arts, plus tech, plus [the] social justice piece. [We] really wanted people on all parts of the campus,” Dill said. “It’s like really hearing the students talk about what they’re doing and also seeing the kinds of the double majors people are doing or the various interests that they have and how they come together.”\n\nOn the other side of the campus at The Clarice, ARTECHOUSE had its own interactive installations. This organization is currently based in only three locations: New York, Miami and Washington, D.C. But for students at NextNOW Fest, they had exclusive access to the exhibit right here on the campus.\n\nDanielle Azu, ARTECHOUSE representative, shared that the installation, Renewal 2121 featured in the Grand Pavilion, was a focus on traditional Japanese scenes and cultural aspects but set in the year 2121. The objective was to focus on climate change and its effects on Japan in the future.\n\nThis is the second time ARTECHOUSE has made an appearance at NextNOW and students absolutely enjoyed it once again. Freshman computer science major Vruti Soni’s awe at the exhibit encompasses the objective of creating a more inclusive arts environment for people with interests in technology.\n\n“When I first came, I thought that colors really drew you to the exhibit. All of them had art in tech, and I hadn’t seen that before,” Soni said.\n\nAmy Yim, junior math major with a minor in arts leadership, was a student curator for NextNOW and touched on the merging of disciplines with art.\n\n“I want more of the sciences to be integrated because there are so often very separate ideas in people’s mind[s],” Yim said.\n\nNextNOW coordinators and student curators had much more to offer than these two exhibits. The line up had something for everyone from comedy, to art and to even technology.\n\nEven if you weren’t able to come to NextNOW, it came to you. Whether you were crossing McKeldin listening to Terrapin Brass or walking through Stamp with the Ignis Wind Quintet, you had a small taste of NextNOW.\n\nCORRECTION: Due to a reporting error, a previous version of this story misquoted Megan Pagado Wells. Wells said that NextNOW Fest partnered with the Michelle Smith Collaboratory for Visual Culture, not the Michelle Smith Performing Arts Library. This story has been updated.\n\nhttps://dbknews.com/2021/09/20/nextnow-fest-umd-clarice-fall-2021/\n\nUMD Social Media Expert Explains Why Facebook and Others Want to Create an Alternate, Online World\n\nBy Chris Carroll\n\nAugust 11, 2021\n\nWhile those of a certain age might recall exotic novelties like accessing primitive chat rooms, logging onto AltaVista or receiving an “E-Mail,” the sense of wonder that accompanied the early internet is long gone.\n\nNow a group of companies is placing a bet on a new internet frontier—one they hope could reinvigorate that sense of technological awe and endless possibilities for connection—with plans to build a 3D virtual world online that users can explore and interact in, called the “metaverse.”\n\nTech watchers took notice in April when Epic Games, founded by CEO Tim Sweeney ’93, announced it had raised $1 billion in funding to develop its metaverse vision after players of its smash hit Fortnite began hanging out in the game world (for instance, for a virtual concert by Ariana Grande) when not competing.\n\nInterest in the idea went into overdrive, however, when Facebook founder and CEO Mark Zuckerberg announced last month that the world’s dominant social media company saw its future in the metaverse. Just as what the metaverse will look like is unclear, so are details of Facebook’s planned shift. But expect the company to deploy huge resources and energy both to stay ahead of the tech curve and to divert attention from some of its current problems, said Professor of Information Studies Jennifer Golbeck, a University of Maryland computer scientist who studies social media algorithms.\n\nGolbeck spoke to Maryland Today about what the internet of the future could look like:\n\nThe metaverse seems to be a big deal, but what is it, actually?\n\nThe idea of a metaverse has been around for years. Facebook didn’t just develop it, and it wouldn’t be a Facebook product, like Instagram. They’re one company that’s part of it. What it actually is, is still somewhat undefined; you could possibly see it interacting with the real world through augmented reality, like with the old Google Glass, with a visual overlay on the real world providing information as you walked around. Where Facebook is really focusing is a virtual space. Most of us have seen or maybe tried the Oculus virtual reality goggles—Facebook owns Oculus, and that seems to be their vision of how you experience the metaverse.\n\nWhat would you actually see wearing the goggles? A cartoon world, or photorealistic?\n\nI think it will be a blend. As with many online experiences, a lot of how it looks will depend on your hardware. It’s not going to be a platform run by a single corporation or an app like (online 3D virtual world) Second Life. That would suggest that you've got some more heterogeneity to how it looks and how it works. So theoretically, you could walk from something built by one company or organization to another, and the world would completely change.\n\nDoes Facebook’s interest make the metaverse more likely to take hold?\n\nFacebook is definitely going to pour a lot of money into it, and they have access to a big user base, so yes. If you want to see the metaverse realized in a way where it features a whole lot of interesting stuff going on, and is a place you can interact with people, this is good for that. But Facebook tries a lot of things people don’t want, and I’ve wondered if this is a solution in search of a problem. Are people really clamoring en masse to leave the real world behind? I’m not so sure they are.\n\nWhy is Facebook betting so much on this, then?\n\nAfter all the issues they’ve had, Facebook wants to reestablish the reputation that they had originally, which is someone who's coming along and doing really innovative new stuff that makes the world a better place, like something that helps us maintain relationships with people we might have lost touch with. I don’t have any insider knowledge of what goes on in Facebook, but I think there must be intense pressure to know what the next big thing is going to be and stay out in front of it.\n\nHow could this go bad?\n\nI think all the problems Facebook has had with people using its platform for bad ends will be intensified in a metaverse application, because a lot of what goes on at Facebook these days is just trying to manage the bad stuff. There’s a lot of talk about the anti-vax stuff and the insurrection-related stuff, but there’s also child porn and many other things that don’t get a lot of attention, and I have to say, Facebook does a really good job stopping a lot of that content, and their moderators suffer for it.\n\nThe problem for the metaverse is that we have technology that can automatically flag text or images that might have child porn or c"
    }
}
{
    "id": "dbpedia_3529_2",
    "rank": 46,
    "data": {
        "url": "https://medium.com/%40anthony.cherbuin/learning-ml-basics-and-using-it-to-become-rich-in-2024-by-predicting-swiss-ice-hockey-results-99aa8e093933",
        "read_more_link": "",
        "language": "en",
        "title": "Learning ML basics and using it to become rich in 2024 by predicting Swiss Ice Hockey results💰",
        "top_image": "https://miro.medium.com/v2/resize:fit:1200/1*6sOWjMevIH_WI7Eu0O-XMw.jpeg",
        "meta_img": "https://miro.medium.com/v2/resize:fit:1200/1*6sOWjMevIH_WI7Eu0O-XMw.jpeg",
        "images": [
            "https://miro.medium.com/v2/resize:fill:64:64/1*dmbNkD5D-u45r44go_cf0g.png",
            "https://miro.medium.com/v2/resize:fill:88:88/1*a9STVanRGIDXilFfK3p6hQ.jpeg",
            "https://miro.medium.com/v2/resize:fill:144:144/1*a9STVanRGIDXilFfK3p6hQ.jpeg"
        ],
        "movies": [],
        "keywords": [],
        "meta_keywords": [
            ""
        ],
        "tags": null,
        "authors": [
            "Anthony",
            "medium.com",
            "@anthony.cherbuin"
        ],
        "publish_date": "2024-07-20T03:26:36.700000+00:00",
        "summary": "",
        "meta_description": "If you directly want to test the final product: https://t.me/LNA_prediction_bot (I use some free hosting and free APIs, it can take up to 5min to wake up the bot when using it and it can also reach…",
        "meta_lang": "en",
        "meta_favicon": "https://miro.medium.com/v2/1*m-R_BkNf1Qjr1YbyOIJY2w.png",
        "meta_site_name": "Medium",
        "canonical_link": "https://medium.com/@anthony.cherbuin/learning-ml-basics-and-using-it-to-become-rich-in-2024-by-predicting-swiss-ice-hockey-results-99aa8e093933",
        "text": "This title might be a small clickbait…but if you’re here, why not read the article anyway 🤷 ?\n\nIf you directly want to test the final product: https://t.me/LNA_prediction_bot (I use some free hosting and free APIs, it can take up to 5min to wake up the bot when using it and it can also reach the limit of 500 API request/day)\n\nIntroduction\n\nAI, ML, LLM have been in the mouth of every developer and digital enthusiast those last years. It’s surely a crazy time to live in and the idea of being able to breakthrough a lot of unsolved historical problems using those technologies is surely motivating. I always have a lot of interest for new technological stuff (it does certainly comes from my geeky side).\n\nHowever, I also know by experience that a lot of those early interest that start as small side hobbies can quickly drop me in the dark side of the Mordor once I’ve scratched the surface and try to understand more about the subject (like FPV drones, IOT or 3d printing…where debugging take more time and nights than the hobby itself… Knows who knows).\n\nThis is why I want to say this now before you read more of this text: this article is by no mean a tutorial or a source of truth, it’s more an explanation of my curiosity journey toward the learning and development of my first ML project on the web as a Frontend and Creative engineer.\n\nFeel free to enjoy the idea and development of the project by reading it while drinking a coffee. If you feel like you’re more in the mood of learning the technical side of this subject properly by someone who is more experienced, I invite you to read this article https://python.plainenglish.io/creating-machine-learning-models-to-predict-football-game-outcomes-70b6bf02885c or this one. Those are really good posts with detailed explanations.\n\nThe beginning\n\nI have not a precise idea at where to start at this point. But one think I am pretty sure of, is that I’ll need a basic dataset to start my project. For football (soccer) you’ll find tones of nice dataset on Kaggle, but none for the Swiss hockey league. So let’s get to work and create or own starting dataset. This is something we can do by scrapping datas online using Selenium, puppeteer or some chrome extension for the more basic stuff like teams, location, dates or results. (We can go wayyyyy more complex here with the parameters, but let’s go slowly and scrap what we can easily, like: match date, scores, home and away team, …).\n\n✨Magic happen✨\n\nWooo, after this step we have all the match from 1856BC to 2024. Now what else ?\n\nUsing just the basic informations of the match would not bring a lot and also I don’t think that results from to far in the past would give us any useful datas for our predictions. So let’s firstly remove all match that happened before 2008.\n\nWhat other data could be useful ?\n\nAfter reading some articles online I quickly understood the complexity of building a good and accurate dataset (creating a good one will take weeks/months and a lot of testing…), as said, I am by no mean experienced in ML and don’t want to spend 1 year on the dataset only…, so as first project I wanted to keep my focus on an easy solution to learn the basics, but I still wanted a solution that could lead me to get some nice results.\n\nThis is when I fumbled on this really good video from Crypto Wizards :\n\nThe idea is to use the historical datas from bookies and the real match results as a key value points for predictions and try to beat the brookies on the bet. This idea looked pretty interesting for me as I can find those data online and I don’t need to start listing and cross merging complex data in my dataset (team players, referee, …). So let’s get those datas and add them in our dataset. Here some parts of the python code used during the process:\n\n# Load the datas from our 2csv for merging\n\ndf1 = pd.read_csv('saison_2008_2020.csv', delimiter=';')\n\ndf2 = pd.read_csv('Odds_2008_2020.csv')\n\n# Define the mapping function to standardize team names as it comes from different sources\n\ndef standardize_team_names(team_name):\n\nmapping = {\n\n'HC Ambri-Piotta': 'Ambri-Piotta',\n\n'EV Zug': 'Zug',\n\n'Fribourg-Gottéron': 'Fribourg',\n\n'SCL Tigers': 'Langnau Tigers',\n\n'SC Bern': 'Bern',\n\n'Lausanne HC': 'Lausanne',\n\n'EHC Biel-Bienne': 'Biel',\n\n'ZSC Lions': 'Zurich',\n\n'HC Lugano': 'Lugano',\n\n'Genève-Servette HC': 'Servette',\n\n'SC Rapperswil-Jona Lakers': 'Rapperswil',\n\n'HC Davos': 'Davos',\n\n'HC Ajoie': 'Ajoie',\n\n'HC La Chaux-de-Fonds':'La Chaux-de-Fonds'\n\n}\n\nreturn mapping.get(team_name, team_name) # Return original if not in mapping\n\n# Since File 1 might not be in datetime format, we adjust this line to ensure it is\n\ndf1['Datum'] = pd.to_datetime(df1['Datum'], format='%d.%m.%y')\n\n# Merge the datasets on 'Datum', 'Home' ('TeamA' in File 2), and 'Away' ('TeamB' in File 2)\n\nmerged_data = pd.merge(df1, df2, left_on=['Datum', 'Home', 'Away'], right_on=['Datum', 'TeamA', 'TeamB'], how='left')\n\n# Remove rows with missing data in any of the odds columns\n\noriginal_count = len(df)\n\ndf.dropna(subset=cont_vars, inplace=True)\n\nremoved_count = original_count - len(df)\n\nprint(f\"\\nRemoved {removed_count} rows with missing odds data.\")\n\nI will use this basic dataset for this project, as said we could go way more complex if we wanted to improve our precision. But as learned from an episode in this lesson it’s always good to create a first model as soon as possible before to spend to much time on the improvement of the dataset :)\n\nLet’s start with the training.\n\nBTW, I made this dataset open sourceon Kaggle here if some of my fellow Swiss coders or hockey fans want to use it or improve it.\n\nTraining our model with fastai\n\nIn this section, we’ll dive into the process of training our machine learning model using Fastai, a popular deep learning library built on top of PyTorch. Fastai provides a high-level API that simplifies the process of training neural networks, which is what we need right now :)\n\nWe do have our data ready, so let’s train a model:\n\n# Define preprocessing steps\n\nprocs = [Categorify, FillMissing, Normalize]\n\n# Define categorical and continuous columns again if needed\n\ncat_vars = ['Home', 'Away']\n\ncont_vars = ['OddsTeamA', 'OddsTeamB', 'OddsX']\n\n# Define splits for training and validation\n\nsplits = RandomSplitter(valid_pct=0.2)(df)\n\n# Prepare your TabularPandas object, ensuring all arguments are correctly utilized\n\nto = TabularPandas(df, procs=procs, cat_names=cat_vars, cont_names=cont_vars,\n\ny_names='Winner', splits=splits)\n\n# Print the first few rows of the processed training set to inspect\n\nprint(\"Processed training set preview:\")\n\nprint(to.train.xs.head())\n\n# Create DataLoaders\n\ndls = to.dataloaders(bs=64)\n\nlearn = tabular_learner(dls, layers=[200, 100], metrics=[accuracy, Precision()])\n\n# Find the optimal learning rate\n\nlearn.lr_find()\n\n# Train the model using the fit_one_cycle method\n\nlearn.fit_one_cycle(5, 1e-2)\n\n# Show results to evaluate the model\n\nlearn.show_results()\n\nIt’s a pretty basic model yes, but as a first I am happy with that for now.\n\nUsing psychology and UX/UI to create an addictive gambling app…\n\nKidding, I will create a Telegram bot… because it’s easier and less time consuming for a side project. Here is a basic idea of the project structure:\n\nWe can split this futur project like that: Our client (Telegram app), our telegram Python bot script, external API that return the futur matchs/odds and our ML Python endpoint that return the estimation of the result.\n\nPreparing our Python APIs using Flask\n\nHaving a working project locally is always nice. But my goal is to be able to integrate my ML as an API. This way I’ll be able to use it everywhere in the future and I’ll be able to call it from my Telegram bot script 🤖\n\nLet’s create that. I wanted to keep all my work on Python and avoid any unnecessary code migration. This is why I decided to use Flask for Python in order to create my API endpoint.\n\nUsing our fastai model anywhere\n\nWe trained our fastai model and we want to be able to use it in our API. In order to do that we can serialize it using the command:\n\nlearn.export('model_v1.pkl')\n\nThis will create a pickle file (.PKL) that we can later load in our code in order to use the trained model.\n\n#Loading and using the exported model\n\nlearn = load_learner('model_v1.pkl')\n\n#...\n\n# Convert the incoming data to a DataFrame\n\nnew_data = pd.DataFrame({\n\n'Home': [Home],\n\n'Away': [Away],\n\n'OddsTeamA': [OddsTeamA],\n\n'OddsTeamB': [OddsTeamB],\n\n'OddsX': [OddsX]\n\n})\n\ndl = learn.dls.test_dl(new_data)\n\n#Here we get the predictions\n\npreds, _ = learn.get_preds(dl=dl)\n\nprediction = preds[0].tolist()\n\nNow, if we integrate this in our /predict route created with Flask\n\n@app.route(‘/predict’, methods=[‘POST’]) we get an endpoint that we can call from anywhere by passing to it the parameters we defined:\n\nHosting\n\nAs a big user of Vercel for all my Frontend deployment I wanted to see if I was able to use it to host my Python API. The answer is “yes you can…but it’s not the easiest way right now in my opinion”. I was able to make it work but faced some weird issues when using Vercel cli locally to run their Python runtime (still Beta as I write this article so let’s wait for the future).\n\nIn the end I decided to use gunicorn with Flask. I hosted my ML api code on render.com and I hosted the telegram bot on a Jetlastic instance (hosted by Infomaniak).\n\nI found it way easier and less restrictive in my workflow, the prices of the services are also ok in my opinion if we need an upgrade in the future.\n\nA bit of betting logic for our Bot\n\nGiving back the probability of win to the end user is cool. But when it comes to betting, simply knowing the probability of a win isn’t enough to make an good decision. While having an accurate win probability is crucial, it’s not the only factor one should consider before placing a bet.\n\nEqually important are the odds offered by bookmakers. If these odds are less favorable than the implied probability of the win then we can say that the bet might not be worth the risk.\n\nExample\n\nWin Probability: This is the chance that a particular outcome will occur given our ML API response. For instance, if a team has a 70% chance of winning a game, the win probability returned is 0.7\n\nOdds: These are the bookmakers’ way of expressing the likelihood of an outcome and determining payouts. For example, odds of 1.2 mean that for every 1$ you bet, you would win 1.20$ if the bet is successful.\n\nTo decide whether a bet is worth placing, you need to compare the implied probability derived from the odds with your own estimated win probability. So we can say that:\n\nOddsWorthBettingFrom=1/Probability\n\nFor example if the win probability given by our ML is 62% we can say that a bet is worth placing if the odds given by the brooker are higher than 1/0.62=1.61\n\nNow that we understand this, we can use our code to determine if a bet is worth the risk, we can provide a useful information to the end user, telling him which odds are worth betting on or not, instead of just sending back a win probability.\n\n#we calculate the implied probability\n\noddsOkFrom = round(1 / ml_probability, 2)\n\n#We return a better response\n\nresponse = (\n\nf\"🏆 {winning_team} have {ml_probability * 100:.1f}% chance to win this game.\\n\"\n\nf\"✅ It's worth betting on {winning_team} if odds of the booker are higher than {oddsOkFrom}\\n\"\n\n)\n\nWith this done, we now will receive all the informations for the next match.\n\nOur Telegram Bot\n\nThe telegram bot commands are straight forward in this V1 with just one command that we define:\n\n/next_match → will provide the next match or a list of the next matchs happening at the same time and the expected outcome for each match.\n\nMain part of the bot code is this function that will call the different API and manage the matchs return:\n\n# Async function to handle the 'next_match' command\n\nasync def next_match(update: Update, context: ContextTypes.DEFAULT_TYPE):\n\n#...\n\n# Make the request to the API\n\nresponse = requests.get(url, headers=headers, params=payload)\n\nmatches = response.json()\n\nns_matches = [match for match in matches['response'] if match['status']['short'] == 'NS']\n\nif ns_matches:\n\n# Get the earliest match time\n\nearliest_match_time = min(ns_matches, key=lambda x: x['timestamp'])['timestamp']\n\n# Get all matches that happen at the earliest match time\n\nearliest_matches = [match for match in ns_matches if match['timestamp'] == earliest_match_time]\n\nfor match in earliest_matches:\n\n# Extract relevant match information\n\nhome_team = match['teams']['home']['name']\n\naway_team = match['teams']['away']['name']\n\nmatch_date = match['date']\n\n# Convert the match date to a readable format\n\ndate_object = datetime.fromisoformat(match_date)\n\nformatted_date = date_object.strftime(\"%d %B %Y at %H:%M\")\n\n#....\n\nWe can now interact with our bot at anytime:\n\nConclusion\n\nThis project is by no means a final product that one could rely on. In fact, I am pretty sure you will achieve better results by trusting your instincts rather than depending on my model, it also tends to always make an away team favorite for now…\n\nThe good news is that with the current structure of the code, I can from now on easily focus 100% on training a model to enhance accuracy without modifying a single line of the project’s core code.\n\nIt was a fun and educational experiment that allowed me to explore new possibilities, learn the very basics of machine learning, and gain insights into the current capabilities of web-based ML APIs. Additionally, it was my first experience integrating a Telegram bot into such a project. I believe this approach could be useful for specific use cases in the future, offering a fast and convenient way to fetch basic data without the need to develop a frontend.\n\n…now I just need to wait the beginning of the 2024–2025 Hockey season and update the model to someV̶2̶ V43 in order to get some descent estimations 😄\n\nFeel free to give me some advices, ideas.I would love to have your opinion on the subject ! ❤\n\n… and hopp Gotteron 🐉"
    }
}
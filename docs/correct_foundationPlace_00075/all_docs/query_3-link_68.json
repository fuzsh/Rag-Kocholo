{
    "id": "correct_foundationPlace_00075_3",
    "rank": 68,
    "data": {
        "url": "https://groups.google.com/g/comp.databases.sybase/c/eCioPIla2B4/m/qkd8eSb8atIJ",
        "read_more_link": "",
        "language": "en",
        "title": "Sybase FAQ: 1/19 - index",
        "top_image": "https://www.gstatic.com/images/branding/product/1x/groups_32dp.png",
        "meta_img": "https://www.gstatic.com/images/branding/product/1x/groups_32dp.png",
        "images": [
            "https://fonts.gstatic.com/s/i/productlogos/groups/v9/web-48dp/logo_groups_color_1x_web_48dp.png",
            "https://lh3.googleusercontent.com/a/default-user=s40-c",
            "https://lh3.googleusercontent.com/a/default-user=s40-c",
            "https://lh3.googleusercontent.com/a/default-user=s40-c",
            "https://lh3.googleusercontent.com/a/default-user=s40-c",
            "https://lh3.googleusercontent.com/a/default-user=s40-c",
            "https://lh3.googleusercontent.com/a/default-user=s40-c",
            "https://lh3.googleusercontent.com/a/default-user=s40-c",
            "https://lh3.googleusercontent.com/a/default-user=s40-c",
            "https://lh3.googleusercontent.com/a/default-user=s40-c",
            "https://lh3.googleusercontent.com/a/default-user=s40-c",
            "https://lh3.googleusercontent.com/a/default-user=s40-c",
            "https://lh3.googleusercontent.com/a/default-user=s40-c",
            "https://lh3.googleusercontent.com/a/default-user=s40-c",
            "https://lh3.googleusercontent.com/a/default-user=s40-c",
            "https://lh3.googleusercontent.com/a/default-user=s40-c",
            "https://lh3.googleusercontent.com/a/default-user=s40-c",
            "https://lh3.googleusercontent.com/a/default-user=s40-c",
            "https://lh3.googleusercontent.com/a/default-user=s40-c",
            "https://lh3.googleusercontent.com/a/default-user=s40-c",
            "https://lh3.googleusercontent.com/a/default-user=s40-c",
            "https://lh3.googleusercontent.com/a/default-user=s40-c",
            "https://lh3.googleusercontent.com/a-/ALV-UjX12Nwt99DrO2iFni2TQ8k9PLNhcWM_iDzzvQuhI6RaCyyGLQ=s40-c"
        ],
        "movies": [],
        "keywords": [],
        "meta_keywords": [
            ""
        ],
        "tags": null,
        "authors": [],
        "publish_date": null,
        "summary": "",
        "meta_description": "",
        "meta_lang": "en",
        "meta_favicon": "//www.gstatic.com/images/branding/product/1x/groups_32dp.png",
        "meta_site_name": "",
        "canonical_link": "https://groups.google.com/g/comp.databases.sybase/c/eCioPIla2B4/m/qkd8eSb8atIJ",
        "text": "Posted-By: auto-faq 3.3.1 beta (Perl 5.005)\n\nArchive-name: databases/sybase-faq/part1\n\nURL: http://www.isug.com/Sybase_FAQ\n\nVersion: 1.2\n\nMaintainer: David Owen\n\nLast-modified: 2000/06/07\n\nPosting-Frequency: posted every 3rd month\n\nA how-to-find-the-FAQ article is posted on the intervening months.\n\nSybase Frequently Asked Questions\n\nSybase FAQ Home Page Adaptive Server Enterprise FAQ Adaptive Server Anywhere\n\nFAQ Replication Server FAQ Search the FAQ\n\nSybase FAQ\n\nMain Page\n\n*Where can I get the latest release of this FAQ?\n\n*What's new in this release?\n\n*How can I help with the FAQ?\n\n*Who do I tell about problems in the FAQ?\n\n*Acknowledgements and Thanks\n\n*Hall of Fame\n\n*Disclaimer\n\n*General Index\n\nMain | ASE | ASA | REP | Search\n\n-------------------------------------------------------------------------------\n\nWhere can I get the latest release of this FAQ?\n\nInternational Sybase User Group\n\nThe main page for this site is http://www.isug.com/Sybase_FAQ. It is hosted\n\nthere by kind permission of the International Sybase User Group (http://\n\nwww.isug.com) as a service to the Sybase community.\n\nTo get a text version of this FAQ:\n\nftp://ftp.midsomer.org/pub/FAQ_txt_tar.Z\n\nor\n\nftp://ftp.midsomer.org/pub/FAQ_txt.zip\n\nIf you want uncompressed versions of the various sections, they can be got\n\nfrom ASE, ASA & REP.\n\nTo get the HTML for this FAQ:\n\nftp://ftp.midsomer.org/pub/FAQ_html_tar.Z\n\nor\n\nftp://ftp.midsomer.org/pub/FAQ_html.zip\n\nSybase FAQ version 1.2, released 2000/06/07.\n\nBack to Top\n\n-------------------------------------------------------------------------------\n\nWhat's new in this release?\n\nRelease 1.2\n\nNew FAQ items:\n\n*A general index for all sections.\n\n*Added a link to Thierry Antinolfi's DBA Devil site.\n\n*Added a link to WinSQL site.\n\n*Added a link to Imran Hussains site.\n\n*After a little provoking Jim Tench (jim....@btinternet.com) wrote two\n\nvery good articles on connecting to Sybase using Open Client and ODBC.\n\n*The start of a complete new replication section. Even at this early\n\nstage it shows potential, even though I say so myself. Thanks to the\n\nfollowing for contributions and postings to sybase.public.rep-server:\n\n+Manish I Shah\n\n+Jason Webster\n\n+Robert Waywell\n\n+A special thanks also go to Bill Bell of Sybase and to the DBAs at\n\nTrimac of Calgary. Without their willingness to trust me, none of it\n\nwould have gotten written since I would absolutely nothing about\n\nreplication, rather than the very little I know now!\n\nUpdated FAQ items:\n\n*Added links to Eric Miner's (eric....@sybase.com) Techwave 1999\n\npresentations to ASE Q1.5.1.\n\n*Added a link to Jeffrey Garbus's (je...@soaringeagleltd.com) Techwave\n\n1999 presentation on DBA tasks to ASE Q1.2.10.\n\nDeleted FAQ items:\n\n*None.\n\nRelease 1.1\n\nThe biggest addition for this release is the start of the FAQ for ASA. Its not\n\nmuch, but definitely a start. Thanks to Leo Tohill for providing some good\n\nideas and reviewing the stuff I put together. Maybe some more people can be\n\npersuaded to help.\n\nI have also done a little bit of a re-number again. Sorry (especially to Todd\n\nBoss) if this causes any hassle. I am trying to make it easier for people to\n\nfind stuff, please bear with me. I have made sure that all of the questions\n\nhave a unique number, so it is possible to refer to Qx.y.z of the ASE FAQ for\n\ninstance. I am working on building an automatic index and cross reference,\n\nthat will be part of the search stuff.\n\nI spent quite a bit of time trying to correct problems with the search scripts\n\nso that links and icons worked after a search. I know that I need to modify\n\nand improve the search engine, which I will work on over the next couple of\n\nmonths.\n\nNew FAQ items:\n\n*Started a new section for platform specific ASE issues.\n\n*Started a new section for issues to do with Open Client.\n\n*Improved the ASA section. This now needs some really good stuff on\n\nperformance and tuning, trouble shooting, admin etc etc.\n\n*Links to De Clarke's Sybtcl tools.\n\n*Links to John Knox's website.\n\n*Added text only version of site.\n\nUpdated FAQ items:\n\n*Updated the links to Ed Barlow's site.\n\n*Re-added numbers for FAQs throughout.\n\n*Added a new method for the generation of sequence numbers thanks to John\n\nDrevicky.\n\n*Changes to the Hall of Fame!\n\nDeleted FAQ items:\n\n*Dead links to a number of User Groups.\n\nBack to Top\n\nRelease 1.0.1\n\nSection 9 was a bit of a mess, all of the hyperlinks had gone south. Fixed one\n\nor two other links that I found were bad.\n\nBack to Top\n\nRelease 1.0\n\nI have updated quite a number of questions, correcting the odd typo and making\n\nchanges relevant to System 11.x throughout. This last process is continuing,\n\nso do not get upset that your favourite 11.9 trick is not mentioned; get even:\n\ntell me about it!\n\nThis is my first release as maintainer, and most of the work this time around\n\nhas been a restructure that will allow for the incorporation of FAQs for\n\nReplication, Adaptive Server and the other Sybase products. (Please see \"How\n\nCan I Help\").\n\nAnother big change is the location. When I took over maintaining the FAQ it\n\noccurred to me that we were going to have yet another web address for it.\n\nSince Pablo has left SGI, the original link pointing to Tom O'Connell's site\n\ndisappeared. Tom will add a link pointing to the new site, but what happens\n\nwhen I hand over the reigns (after winning the lottery of course :-)? Michael\n\nPeppler suggested the ISUG site as a permanent home, and this fitted in\n\nperfectly with my own ideas. I must emphasize that this FAQ is not part of the\n\nISUG or Sybase. It is independent of both those bodies and is not officially\n\nendorsed by either. Please see the Disclaimer.\n\nI have updated quite a number of questions, correcting the odd typo and making\n\nchanges relevant to System 11.x where ever I found a reference. This last\n\nprocess is continuing, so do not get angry that your favourite 11.9 trick is\n\nnot mentioned, get even: tell me about it!\n\nNew FAQ items:\n\n*Row Level Locking\n\n*CIS\n\n*Thanks to Rob Verschoor for links to his site (Certification, Quick Ref\n\nGuide and Dynamic SQL).\n\n*Eric McGrane provided a useful means of extending a master database on a\n\nfull master device.\n\n*Sean Kiely provided a good solution for recovering databases if the\n\ntransaction log is full and the system will not recover.\n\n*Linked to Anthony Mandic's Sybase on Solaris paper.\n\nUpdated FAQ items:\n\n*A million minor changes. No major rewrites.\n\nDeleted FAQ items:\n\n*Dead links to a number of User Groups.\n\nI am sorry if you have links pointing to particular sections, but I felt that a\n\ncomplete reorganisation was necessary in order to support the incorporation of\n\nnew FAQs for the other products. Feedback is always welcome of course!\n\nBack to Top\n\n-------------------------------------------------------------------------------\n\nHow can I help with the FAQ?\n\nI have had offers from a couple of people to write sections (I will be in touch\n\nto get your sections added for the next release, thanks), but if you feel that\n\nyou are in a position to add support for a section, or if you have some FAQs to\n\nadd, please let me know. This is a resource that we should all support, so\n\nsend me the stuff and I will include it.\n\nCurrently I am looking for maintainers of a section for Replication, Adaptive\n\nServer Anywhere, IQ server, MPP Server and Open Server. I am not sure whether\n\nto add a section for Omni Server. I sort of feel that since Omni has been\n\nsubsumed into ASE as CIS that any questions FAQs should really be incorporated\n\nthere. However, if you know of some good Omni gotchas or tips, whether they\n\nare still there in CIS or not, please send them in. I certainly plan to have a\n\nsubsection of ASE dealing with CIS even if Omni does not get its own major\n\nsection. I also think that we need sections on some of the really new stuff.\n\nJaguar and the new engines also deserve a spot.\n\nAnother very useful way that you can help is in getting people to update their\n\nlinks. I have seen lots of links recently, some still pointing to Pablo's\n\noriginal, some pointing to Tom's site but referring to it as coming from the\n\nSGI site.\n\nBack to Top\n\n-------------------------------------------------------------------------------\n\nWho do I tell about problems in the FAQ?\n\nThe current maintainer is David Owen (do...@midsomer.org) and you can send\n\nerrors in the FAQ directly to me. If you have an FAQ item (both the question\n\nand the answer) send it to syb...@midsomer.org and I will include it.\n\nDo not send email to any of the officials at the ISUG, they are simply hosting\n\nthe FAQ and are not responsible for its contents.\n\nAlso, do not send email to Sybase, they are not responsible for the contents\n\neither. See the Disclaimer.\n\nBack to Top\n\n-------------------------------------------------------------------------------\n\nAcknowledgements and Thanks\n\nSpecial thanks must go to the following people for their help in getting this\n\nFAQ to where it is today.\n\n*Pablo Sanchez for getting the FAQ off the ground in the first place and\n\nfor many years of dedicated work in maintaining it.\n\n*Anthony Mandic (a...@peppler.org) for a million things. Patiently\n\nanswering questions in all of the Sybase news groups, without which most\n\nbeginners would be lost. For supporting and encouraging me in getting this\n\nFAQ together and for providing some pretty neat graphics.\n\n*The ISUG, especially Luc Van der Veurst (lu...@az.vub.ac.be) and Michael\n\nPeppler (mpep...@peppler.org), for hosting this FAQ and providing support\n\nin setting up the website.\n\n*The members of the various news groups and mailing lists who, like\n\nAnthony, provide unstinting support. The list is fairly long, but I think\n\nthat Bret Halford (br...@sybase.com) deserves a mention. If you go to Deja\n\nNews and do a search, he submits even more replies than Anthony.\n\nBack to Top\n\n-------------------------------------------------------------------------------\n\nHall of Fame\n\nI am not sure how Pablo chose his select list, there is certainly no question\n\nas to their inclusion. I know that there are a couple of awards that the ISUG\n\ngive out each year for the people that the ISUG members believe have\n\ncontributed most to the Sybase community that year. I think that this section\n\nshould honour those people that deserve an award each and every year. If you\n\nknow of a candidate, let me know and I will consider his or her inclusion.\n\nSelf nominations are not acceptable :-)\n\nThe following people have made it to the Sybase FAQ Hall of Fame:\n\n*Michael Peppler (mpep...@peppler.org) For Sybperl and all of the other\n\ntools of which he is author or instigator plus the ceaseless support that\n\nhe provides through countless mailing lists, newsgroups and directly via\n\nemail.\n\n*Scott Gray (gr...@voicenet.com) Father of sqsh, much more than simply a\n\nreplacement for isql. How anyone developing or administering Sybase can\n\nsurvive without it, I will never know.\n\n*Pablo Sanchez (pa...@divideview.com) Pablo got the first web based FAQ\n\noff the ground, wrote all of the first edition and then maintained it for a\n\nnumber of years. He did a fantastic job, building a resource that is\n\nworth its weight in gold.\n\nBack to Top\n\n-------------------------------------------------------------------------------\n\nDisclaimer\n\nThis article is provided as is without any express or implied warranties.\n\nWhilst every endeavour has been taken to ensure the accuracy of the information\n\ncontained in this article, the author, nor any of the contributors, assume\n\nresponsibility for errors or omissions, or for damages resulting from the use\n\nof the information contained herein.\n\nIf you are not happy about performing any of the suggestions contained within\n\nthis FAQ, you are probably better off calling Sybase Technical Support.\n\nBack to Top\n\n-------------------------------------------------------------------------------\n\nASE\n\nBasic ASE Admin\n\n1.1.1 What is SQL Server and ASE anyway?\n\n1.1.2 How do I start/stop SQL Server when the CPU reboots?\n\n1.1.3 How do I move tempdb off of the master device?\n\n1.1.4 How do I correct timeslice -201?\n\n1.1.5 The how's and why's on becoming a Certified Sybase\n\nProfessional DBA (CSPDBA)?\n\n1.1.6 RAID and Sybase\n\n1.1.7 How to swap a db device with another\n\n1.1.8 Server naming and renaming\n\n1.1.9 How do I interpret the tli strings in the interface file?\n\n1.1.10How can I tell the datetime my Server started?\n\n1.1.11Raw partitions or regular files?\n\n1.1.12Is Sybase Y2K (Y2000) compliant?\n\n1.1.13How Can I Run the SQL Server Upgrade Manually?\n\nUser Database Administration\n\n1.2.1 Changing varchar(m) to varchar(n)\n\n1.2.2 Frequently asked questions on Table partitioning\n\n1.2.3 How do I manually drop a table?\n\n1.2.4 Why not create all my columns varchar(255)?\n\n1.2.5 What's a good example of a transaction?\n\n1.2.6 What's a natural key?\n\n1.2.7 Making a Stored Procedure invisible\n\n1.2.8 Saving space when inserting rows monotonically\n\n1.2.9 How to compute database fragmentation\n\n1.2.10 Tasks a DBA should do...\n\n1.2.11 How to implement database security\n\n1.2.12 How to shrink a database\n\n1.2.13 How do I turn on auditing of all SQL text sent to the server\n\nAdvanced Administration\n\n1.3.1 How do I clear a log suspend'd connection?\n\n1.3.2 What's the best value for cschedspins?\n\n1.3.3 What traceflags are available?\n\n1.3.4 How do I use traceflags 5101 and 5102?\n\n1.3.5 What is cmaxpktsz good for?\n\n1.3.6 What do all the parameters of a buildmaster -d<device> -yall\n\nmean?\n\n1.3.7 What is CIS and how do I use it?\n\n1.3.8 If the master device is full how do I make the master database\n\nbigger?\n\nGeneral Troubleshooting\n\n1.4.1 How do I turn off marked suspect on my database?\n\n1.4.2 On startup, the transaction log of a database has filled and\n\nrecovery has suspended, what can I do?\n\nPerformance and Tuning\n\n1.5.1 What are the nitty gritty details on Performance and Tuning?\n\n1.5.2 What is best way to use temp tables in an OLTP environment?\n\n1.5.3 What's the difference between clustered and non-clustered\n\nindexes?\n\n1.5.4 Optimistic versus Pessimistic locking?\n\n1.5.5 How do I force an index to be used?\n\n1.5.6 Why place tempdb and log on low numbered devices?\n\n1.5.7 Have I configured enough memory for ASE/SQL Server?\n\n1.5.8 Why should I use stored procedures?\n\n1.5.9 I don't understand showplan's output, please explain.\n\n1.5.10 Poor man's sp_sysmon.\n\n1.5.11 View MRU-LRU procedure cache chain.\n\n1.5.12 Improving Text/Image Type Performance\n\nPlatform Specific Issues\n\n2.1 How to Start ASE on Remote NT Servers\n\nDBCCs\n\n3.1 How do I set TS Role in order to run certain DBCCs...?\n\n3.2 What are some of the hidden/trick DBCC commands?\n\n3.3 The unauthorized DBCC list with doco - see Q11.4.1\n\n3.4 Fixing a Munged Log\n\n3.5 Another site with DBCC commands - see Q11.4.2\n\nisql\n\n4.1 How do I hide my password using isql?\n\n4.2 How do I remove row affected and/or dashes when using isql?\n\n4.3 How do I pipe the output of one isql to another?\n\nbcp\n\n5.1 How do I bcp null dates?\n\n5.2 Can I use a named pipe to bcp/dump data out or in?\n\n5.3 How do I exclude a column?\n\nSQL Fundamentals\n\n6.1.1 Are there alternatives to row at a time processing?\n\n6.1.2 When should I execute an sp_recompile?\n\n6.1.3 What are the different types of locks and what do they mean?\n\n6.1.4 What's the purpose of using holdlock?\n\n6.1.5 What's the difference between an update in place versus a\n\ndeferred update? - see Q1.5.9\n\n6.1.6 How do I find the oldest open transaction?\n\n6.1.7 How do I check if log truncation is blocked?\n\n6.1.8 The timestamp datatype\n\n6.1.9 Stored Procedure Recompilation and Reresolution\n\n6.1.10 How do I manipulate binary columns?\n\n6.1.11 Does Sybase support Row Level Locking?\n\n6.1.12 Why do my page locks not get escalated to a table lock after 200\n\nlocks?\n\nSQL Advanced\n\n6.2.1 How to emulate the Oracle decode function/crosstab\n\n6.2.2 How to implement if-then-else within a select-clause.\n\n6.2.3 deleted due to copyright hassles with the publisher\n\n6.2.4 How to pad with leading zeros an int or smallint.\n\n6.2.5 Divide by zero and nulls.\n\n6.2.6 Convert months to financial months.\n\n6.2.7 Hierarchy traversal - BOMs.\n\n6.2.8 Is it possible to call a UNIX command from within a stored\n\nprocedure or a trigger?\n\n6.2.9 Information on Identities and Rolling your own Sequential Keys\n\n6.2.10 How can I execute dynamic SQL with ASE/SQL Server?\n\nOpen Client\n\n7.1 What is Open Client?\n\n7.2 What is the difference between DB-lib and CT-lib?\n\n7.3 What is this TDS protocol?\n\n7.4 I have upgraded to MS SQL Server 7.0 and can no longer connect from\n\nSybase's isql.\n\n7.5 The Basics of Connecting to Sybase\n\n7.6 Connecting to Sybase using ODBC\n\nFreeware\n\n9.1 sp_freedevice - lists device, size, used and free.\n\n9.2 sp_whodo - augments sp_who by including additional columns: cpu,\n\nI/O...\n\n9.3 SQL and sh(1)to dynamically generate a dump/load database\n\ncommand.\n\n9.4 SybPerl - Perl interface to Sybase.\n\n9.5 dbschema.pl - SybPerl script to take a logical snap of a\n\ndatabase.\n\n9.6 Sybtcl - TCL interface to Sybase.\n\n9.7 Augmented system stored procedures.\n\n9.8 Examples of Open Client and Open Server programs -- see Q11.4.14\n\n.\n\n9.9 SQL to determine the space used for an index.\n\n9.10 xsybmon - an X interface to sp_monitor\n\n9.11 sp_dos - This procedure graphically displays the scope of a\n\nobject\n\n9.12 sqsh - a superset of dsql with local variables, redirection,\n\npipes and all sorts of goodies.\n\n9.13 sp_getdays - returns days in current month.\n\n9.14 ddl_insert.pl - creates insert DDL for a table.\n\n9.15 sp_ddl_create_table - creates DDL for all user tables in the\n\ncurrent database\n\n9.16 int.pl - converts interfaces file to tli\n\n9.17 How to access a SQL Server using Linux see also Q11.4.6\n\n9.18 sp__revroles - creates DDL to sp_role a mirror of your SQL Server\n\n9.19 sp__rev_configure - creates DDL to sp_configure a mirror of your\n\nSQL Server\n\n9.20 sp_servermap - overview of your SQL Server\n\n9.21 sp__create_crosstab - simplify crosstable queries\n\n9.22 update statistics script\n\n9.23 lightweight Sybase Access via Win95/NT\n\n9.24 Sybase on LinuxLinux Penguin\n\n9.25 How to configure shared-memory for Linux\n\n9.26 sp_spaceused_table\n\n9.27 sybdump - a Tcl script for dumping a database schema to disk\n\nMiscellany\n\n12.1 What can Sybase IQ do for me?\n\n12.2 Net-review of Sybase books\n\n12.3 email lists\n\n12.4 Finding Information at Sybase\n\nASA\n\n0.0 Preamble\n\n0.1 What is ASA?\n\n0.2 On what platforms is ASA supported?\n\n0.3 What applications is ASA good for?\n\n0.4 When would I choose ASA over ASE?\n\n0.5 Does ASA Support Replication?\n\n0.6 What is ASA UltraLite?\n\n0.7 Links for further information\n\nREP\n\nIntroduction to Replication Server\n\n1.1 Introduction\n\n1.2 Replication Server Components\n\n1.3 What is the Difference Between SQL Remote and Replication Server?\n\nReplication Server Introduction\n\n2.1 How can I improve throughput?\n\n2.2 Where should I install replication server?\n\n2.3 Using large raw partitions with Replication Server on Unix.\n\n2.4 How to replicate col = col + 1\n\nTroubleshooting Replication Server\n\n3.1 Why am I running out of locks on the replicate side?\n\n3.2 Someone was playing with replication and now the transaction log on\n\nOLTP is filling.\n\nAdditional Information/Links\n\n4.1 Links\n\n4.2 Newsgroups\n\nPosted-By: auto-faq 3.3.1 beta (Perl 5.005)\n\nArchive-name: databases/sybase-faq/part3\n\nURL: http://www.isug.com/Sybase_FAQ\n\nVersion: 1.2\n\nMaintainer: David Owen\n\nLast-modified: 2000/06/07\n\nPosting-Frequency: posted every 3rd month\n\nA how-to-find-the-FAQ article is posted on the intervening months.\n\nSybase Frequently Asked Questions =\n\n=20\n\n=\n\n=20\n\nSybase FAQ Home Page Adaptive Server Enterprise FAQ Adaptive Server An=\n\nywhere =20\n\nFAQ Repserver FAQ Search the FAQ =\n\n=20\n\n[bar] =\n\n=20\n\n=\n\n=20\n\nSybase Replication Server =\n\n=20\n\n=\n\n=20\n\n=20\n\n=20\n\n1. Introduction to Replication Server\n\n2. Replication Server Administration\n\n3. Troubleshooting Replication Server\n\n4. Additional Information/Links\n\n=20\n\nIntroduction to Replication Server =\n\n=20\n\n=\n\n=20\n\n=20\n\n=20\n\n1.1 Introduction\n\n1.2 Replication Server Components\n\n1.3 What is the Difference Between SQL Remote and Replication Serve=\n\nr?\n\nThanks go to Manish I Shah for major help with this introduction.\n\nnext prev ASE FAQ\n\n-----------------------------------------------------------------------=\n\n--------\n\n1.1 Introduction\n\n-----------------------------------------------------------------------=\n\n--------\n\nWhat is Replication Server\n\nReplication Server moves transactions (insert, updates and deletes) at =\n\nthe\n\ntable level from a source dataserver to one or more destination dataser=\n\nvers.\n\nThe dataserver could be ASE or other major DBMS flavour (including DB2,=\n\nInformix, Oracle). The source and destinations need not be of the same =\n\ntype.\n\nWhat can it do ?\n\n=20\n\n*Move data from one source to another.\n\n*Move only a subset of data from source to destination. So, you can =91=\n\nsubscribe=92 to a subset of data, or a subset of the columns, in th=\n\ne source\n\ntable, e.g. select * from clients where state =3D =91NY=92\n\n*Manipulation/transformation of data when moving from source to\n\ndestination. E.g. it can map data from a data-type in DB2 to an equ=\n\nivalent\n\nin Sybase.*\n\n*Provide a warm-standby system. Can be incorporated with Open Switch to=\n\nprovide a fairly seamless fail-over environment.\n\n*Merge data from several source databases into one destination database=\n\n(could be for a warehouse type environment for example).\n\n*Move data through a complicated network down to branch offices, say, o=\n\nnly\n\nsending the relevant data to each branch.\n\n(* This is one of Sybase replication's real strengths, the ability to d=\n\nefine\n\nfunction string classes which allow the conversion of statements from o=\n\nne SQL\n\ndialect to match the dialect of the destination machine. Ed)\n\nHow soon does the data move\n\nThe data moves asynchronously. The time it takes to reach the destinati=\n\non\n\ndepends on the size of your transaction, level of activity in that part=\n\nicular\n\ndatabase (a database as in Sybase systems), the length of the chain (on=\n\ne or\n\nmore replication servers that the transaction has to pass through to re=\n\nach the\n\ndestination), the thickness of pipe (network), how busy your replicatio=\n\nn server\n\nis etc. Usually, on a LAN, for small transactions, this is about a seco=\n\nnd.\n\nBack to top\n\n-----------------------------------------------------------------------=\n\n--------\n\n1.2 Replication Server Components\n\n-----------------------------------------------------------------------=\n\n--------\n\nBasic\n\nPrimary Dataserver\n\nThe source of data where client applications enter/delete and modify da=\n\nta. As\n\nmentioned before, this need not be ASE, it can be Microsoft SQL Server,=\n\nOracle,\n\nDB2, Informix. (I know that I should get a complete list.)\n\nReplication Agent/Log Transfer Manager\n\nLog Transfer Manager (LTM) is a separate program/process which reads\n\ntransaction log from the source server and transfers them to the replic=\n\nation\n\nserver for further processing. With ASE 11.5, this has become part of A=\n\nSE and\n\nis now called the Replication Agent. However, you still need to use an =\n\nLTM for\n\nnon-ASE sources. I imagine there is a version of LTM for each kind of s=\n\nource\n\n(DB2, Informix, Oracle etc). When replication is active, you see one\n\nconnection per each replicated database in the source dataserver (sp_wh=\n\no).\n\nReplication Server (s)\n\nThe replication server is an Open Server/Open Client application. The s=\n\nerver\n\npart receives transactions being sent by either the source ASE or the s=\n\nource\n\nLTM. The client part sends these transactions to the target server whic=\n\nh could\n\nbe another replication server or the final dataserver. As far as I know=\n\n, the\n\nserver does not include the client component of any of the other DBMSes=\n\nout of\n\nthe box.\n\nReplicate (target) Dataserver\n\nServer in which the final replication server (in the queue) will repeat=\n\nthe\n\ntransaction done on the primary. You will see a connection, one for eac=\n\nh target\n\ndatabase, in the target dataserver when the replication server is activ=\n\nely\n\ntransferring data (when idle, the replication server disconnects or fad=\n\nes out\n\nin replication terminology).\n\nBack to top\n\n-----------------------------------------------------------------------=\n\n--------\n\n1.3 What is the Difference Between Replication Server and SQL Remote?\n\n-----------------------------------------------------------------------=\n\n--------\n\nBoth SQL Remote and Replication Server perform replication. SQL Remote =\n\nwas\n\noriginally part of the Adaptive Server Anywhere tool kit and is intende=\n\nd for\n\nintermittent replication. (The classic example is that of a salesman\n\nconnecting on a daily basis to upload sales and download new prices and=\n\ninventory.) Replication Server is intended for near real-time replicati=\n\non\n\nscenarios.\n\nBack to top\n\n-----------------------------------------------------------------------=\n\n--------\n\nnext prev ASE FAQ\n\nReplication Server Administration =\n\n=20\n\n=\n\n=20\n\n=20\n\n=20\n\n=20\n\n2.1 How can I improve throughput?\n\n2.2 Where should I install replication server?\n\n2.3 Using large raw partitions with Replication Server on Unix.\n\n2.4 How to replicate col =3D col + 1\n\n=20\n\n=20\n\nnext prev ASE FAQ\n\n-----------------------------------------------------------------------=\n\n--------\n\n2.1 How can I improve throughput?\n\n-----------------------------------------------------------------------=\n\n--------\n\nCheck the Obvious\n\nFirst, ensure that you are only replicating those parts of the system t=\n\nhat need\n\nto be replicated. Some of this is obvious. Don't replicate any table th=\n\nat\n\ndoes not need to be replicated. Check that you are only replicating the=\n\ncolumns you need. Replication is very sophisticated and will allow you =\n\nto\n\nreplicate both a subset of the columns as well as a subset of the rows.=\n\nReplicate Minimum Columns\n\nOnce the replication is set up and synchronised, it is only necessary t=\n\no\n\nreplicate those parts of the primary system that actually change. You a=\n\nre only\n\nreplicating those rows and columns that need to be replicated, but you =\n\nonly\n\nneed to replicate the actual changes. Check that each replication defin=\n\nition\n\nis defined using the clause:\n\ncreate replication definition rep_def_name\n\nwith primary...\n\n...\n\nreplicate minimal columns\n\nSecond Replication Server\n\nThis might be appropriate in a simple environment on systems with spare=\n\ncycles\n\nand limited space on the network. When Sybase replicates from a primary=\n\nto a\n\nreplicate using only one replication server the data is transferred acr=\n\noss the\n\nnetwork uncompressed. However, the communication between two replicatio=\n\nn\n\nservers is compressed. By installing a second replication server it is\n\npossible to dramatically reduce the bandwidth needed to replicate your =\n\ndata.\n\nDedicated Network Card\n\nObviously, if replication is sharing the same network resources that al=\n\nl of the\n\nclients are using, there is the possibility for a bottleneck if the net=\n\nwork\n\nbandwidth is close to saturation. If a second replication server is not=\n\ngoing\n\nto cut it since you already have one or there are no spare cycles, then=\n\na\n\nsecond network card may be the answer.\n\nFirst, you will need to configure ASE to listen on two network connecti=\n\nons.\n\nThis is relatively straightforward. There is no change to the client\n\nconfiguration. They all continue to talk to Sybase using the same conne=\n\nction.\n\nWhen defining the replication server, ensure that the interfaces/sql.in=\n\ni entry\n\nthat it uses only has the second connection in it. This may involve som=\n\ne\n\njiggery pokery with environment variables, but should be possible, even=\n\non NT!\n\nYou need to be a little careful with network configuration. Sybase will=\n\ncommunicate with the two servers on the correct address, but if the und=\n\nerlying\n\noperating system believes that both clients and repserver can be servic=\n\ned by\n\nthe same card, then it will use the first card that it comes to. So, if=\n\nyou\n\nhad the situation that all of the clients, ASE and the replication serv=\n\ner were\n\non 192.168.1.0, and the host running ASE had two cards onto this same s=\n\negment,\n\nthen it would choose to route all packets through the first card. OK, s=\n\no this\n\nis a very simplistic error to correct, but similar things can happen wi=\n\nth more\n\nconvoluted and, superficially, better thought out configurations.\n\n+---------+ +-----------+ +-----------=\n\n+\n\n| |--> NE(1) --> All Clients... | | | =\n\n|\n\n| Primary | | repserver | | replicate =\n\n|\n\n| |--> NE(2) --------------------->| |-->| =\n\n|\n\n| | | | | =\n\n|\n\n+---------+ +-----------+ +-----------=\n\n+\n\nSo, configure NE(1) to be on 192.168.1.0, say, and NE(2) to be on 192.1=\n\n68.2.0\n\nand all should be well. OK, so my character art is not perfect, but I t=\n\nhink\n\nthat you get the gist!\n\nBack to top\n\n-----------------------------------------------------------------------=\n\n--------\n\n2.2 Where should I install replication server?\n\n-----------------------------------------------------------------------=\n\n--------\n\nA seemingly trivial question, but one that can cause novices a bit of w=\n\norry.\n\nThere are three answers: on the primary machine, on the replicate machi=\n\nne or on\n\na completely separate machine. There is no right answer, and if you are=\n\ndoing\n\nan initial install it probably pays to consider the future, consider th=\n\ne\n\nproposed configuration and have a look at the load on the available mac=\n\nhines.\n\nIt is probably fair to say that replication is not power hungry but nei=\n\nther is\n\nit free. If the primary is only just about coping with its current load=\n\n, then\n\nit might be as well looking into hosting it on another machine. The arg=\n\nument\n\napplies to the replicate. If you think that network bandwidth may be an=\n\nissue,\n\nand you may have to add a second replication server, you may be better =\n\noff\n\nstarting with repserver running on the primary. It is marginally easier=\n\nto add\n\na repserver to an existing configuration if the first repserver is on t=\n\nhe\n\nprimary.\n\nRemember that a production replication server on Unix will require raw =\n\ndevices\n\nfor the stable devices and that these can be more than 2GB in size. If =\n\nyou are\n\nrestricted in the number of raw partitions you have available on a part=\n\nicular\n\nmachine, then this may have a bearing. See Q2.3.\n\nInstalling replication server on its own machine will, of course, intro=\n\nduce all\n\nsorts of problems of its own, as well as answering some. The load on th=\n\ne\n\nprimary or the replicate is reduced considerably, but you are definitel=\n\ny going\n\nto add some load to the network. Remember that ASE->Rep and Rep->ASE is=\n\nuncompressed. It is only Rep->Rep that is compressed.\n\nBack to top\n\n-----------------------------------------------------------------------=\n\n--------\n\n2.3 Using large raw partitions with Replication Server on Unix.\n\n-----------------------------------------------------------------------=\n\n--------\n\nIt is a good practice with production installations of Replication Serv=\n\ner on\n\nUnix that you use raw partitions for the stable devices. This is for ju=\n\nst the\n\nsame reason that production ASE's use raw partitions. Raw devices can b=\n\ne a\n\nmaximum of 2GB with replication server up to release 11.5. (I have not =\n\nchecked\n\n12.)\n\nIn order to utilise a raw partition that is greater than 2GB in size yo=\n\nu can do\n\nthe following (remember all of the cautionary warnings about trying thi=\n\ns sort\n\nof stuff out in development first!):\n\nadd partition firstpartition on '/dev/rdsk/c0t0d0s0' with size 2024\n\ngo\n\nadd partition secondpartition on '/dev/rdsk/c0t0d0s0' with size 2024\n\nstarting at 2048\n\ngo\n\nNotice that the initial partition is sized at 2024MB and not 2048. I ha=\n\nve not\n\nfound this in the documentation, but replication certainly seems to hav=\n\ne a\n\nproblem allocating a full 2GB. Interestingly, do the same operation thr=\n\nough\n\nRep Server Manager and Sybase central caused no problems at all.\n\nBack to top\n\n-----------------------------------------------------------------------=\n\n--------\n\n2.4 How to replicate col =3D col + 1\n\n-----------------------------------------------------------------------=\n\n--------\n\nFirstly. While the rule that you never update a primary key may be a\n\nphilosophical choice in a non-replicated system, it is an architectural=\n\nrequirement of a replicated system.\n\nIf you use simple data replication, and your primary table is:\n\nid\n\n---\n\n1\n\n2\n\n3\n\nand you issue a:\n\nupdate table set id=3Did+1\n\nRep server will do this in the replicate:\n\nbegin tran\n\nupdate table set id=3D2 where id=3D1\n\nupdate table set id=3D3 where id=3D2\n\nupdate table set id=3D4 where id=3D3\n\ncommit tran\n\nHands up all who can see a bit of a problem with this! Remember, repser=\n\nver\n\ndoesn't replicate statements, it replicates the results of statements.\n\nOne way to perform this update is to build a stored procedure on both s=\n\nides\n\nthat executes the necessary update and replicate the stored procedure c=\n\nall.\n\nBack to top\n\n-----------------------------------------------------------------------=\n\n--------\n\nnext prev ASE FAQ\n\nReplication Server Trouble Shooting =\n\n=20\n\n=\n\n=20\n\n=20\n\n=20\n\n=20\n\n3.1 Why am I running out of locks on the replicate side?\n\n3.2 Someone was playing with replication and now the transaction lo=\n\ng on\n\nOLTP is filling.\n\n=20\n\n=20\n\nnext prev ASE FAQ\n\n-----------------------------------------------------------------------=\n\n--------\n\n3.1 Why am I running out of locks on the replicate side?\n\n-----------------------------------------------------------------------=\n\n--------\n\nSybase replication works by taking each transaction that occurs in the =\n\nprimary\n\ndataserver and applying to the replicate. Since replication works on th=\n\ne\n\ntransaction log, a single, atomic, update on the primary side that upda=\n\ntes a\n\nmillion rows will be translated into a million single row updates. This=\n\nmay\n\nseem very strange but is a simple consequence of how it works. On the p=\n\nrimary,\n\nthis million row update will attempt to escalate the locks that it has =\n\ntaken\n\nout to an exclusive table lock. However, on the replicate side each row=\n\nis\n\nupdated individually, much as if they were being updated within a curso=\n\nr loop.\n\nNow, Sybase only tries to escalate locks from a single atomic statement=\n\n(see\n\nASE Qx.y), so it will never try to escalate the lock. However, since th=\n\ne\n\nupdates are taking place within a single transaction, Sybase will need =\n\nto take\n\nout enough page locks to lock the million rows.\n\nSo, how much should you increase the locks parameter on the replicate s=\n\nide? A\n\ngood rule of thumb might be double it or add 40,000 whichever is the la=\n\nrger.\n\nThis has certainly worked for us.\n\nBack to top\n\n-----------------------------------------------------------------------=\n\n--------\n\n3.2 Someone was playing with replication and now the transaction log on=\n\nOLTP\n\nis filling.\n\n-----------------------------------------------------------------------=\n\n--------\n\nOnce replication has been configured, ASE adds another marker to the\n\ntransaction log. The first marker is the conventional one that marks wh=\n\nich\n\ntransactions have had their data written to disk. The second is there t=\n\no\n\nensure that the transactions have also been replicated. Clearly, if som=\n\neone\n\ninstalled replication and did not clean up properly after themselves, t=\n\nhis\n\nmarker will still be there and consequently the transaction log will be=\n\nfilling\n\nup. If you are certain that replication is not being used on your syste=\n\nm, you\n\ncan disable the secondary truncation marker with the following commands=\n\n:\n\n1> sp_role \"grant\", sybase_ts_role, sa\n\n2> go\n\n1> set role sybase_ts_role on\n\n2> go\n\n1> dbcc dbrepair(dbname, ltmignore)\n\n2> go\n\n1> sp_role \"revoke\", sybase_ts_role, sa\n\n2> go\n\nThis scenario is also very common if you load a copy of your replicated=\n\nproduction database into development.\n\nBack to top\n\n-----------------------------------------------------------------------=\n\n--------\n\nnext prev ASE FAQ\n\nAdditional Information/Links =\n\n=20\n\n=\n\n=20\n\n=20\n\n=20\n\n=20\n\n4.1 Links\n\n4.2 Newsgroups\n\n=20\n\n=20\n\nnext prev ASE FAQ\n\n-----------------------------------------------------------------------=\n\n--------\n\n4.1 Links\n\n-----------------------------------------------------------------------=\n\n--------\n\nThierry Antinolfi has a replication FAQ at his site http://pro.wanadoo.=\n\nfr/\n\ndbadevil that covers a lot of good stuff.\n\nRob Verschoor has a 'Replication Server Tips & Tricks' section on his s=\n\nite, as\n\nwell as an indispensible quick reference guide!\n\nBack to top\n\n-----------------------------------------------------------------------=\n\n--------\n\n4.2 Newsgroups\n\n-----------------------------------------------------------------------=\n\n--------\n\nThere are a number of newsgroups that can deal with questions. Sybase h=\n\nave\n\nseveral in their own forums area.\n\nFor Replication Server:\n\n=20\n\n=20\n\nsybase.public.rep-server\n\nsybase.public.rep-agent\n\nfor SQL Remote and the issues of replicating with ASA:\n\n=20\n\n=20\n\nsybase.public.sqlanywhere.replication\n\nand of course, there is always the ubiquitous\n\n=20\n\n=20\n\ncomp.databases.sybase.\n\nBack to top\n\n-----------------------------------------------------------------------=\n\n--------\n\nnext prev ASE FAQ\n\nPosted-By: auto-faq 3.3.1 beta (Perl 5.005)\n\nArchive-name: databases/sybase-faq/part5\n\nURL: http://www.isug.com/Sybase_FAQ\n\nVersion: 1.2\n\nMaintainer: David Owen\n\nLast-modified: 2000/06/07\n\nPosting-Frequency: posted every 3rd month\n\nA how-to-find-the-FAQ article is posted on the intervening months.\n\nUser Database Administration\n\n1.2.1 Changing varchar(m) to varchar(n)\n\n1.2.2 Frequently asked questions on Table partitioning\n\n1.2.3 How do I manually drop a table?\n\n1.2.4 Why not create all my columns varchar(255)?\n\n1.2.5 What's a good example of a transaction?\n\n1.2.6 What's a natural key?\n\n1.2.7 Making a Stored Procedure invisible\n\n1.2.8 Saving space when inserting rows monotonically\n\n1.2.9 How to compute database fragmentation\n\n1.2.10 Tasks a DBA should do...\n\n1.2.11 How to implement database security\n\n1.2.12 How to shrink a database\n\n1.2.13 How do I turn on auditing of all SQL text sent to the server\n\nnext prev ASE FAQ\n\n-------------------------------------------------------------------------------\n\n1.2.1: Changing varchar(m) to varchar(n)\n\n-------------------------------------------------------------------------------\n\nBefore you start:\n\nselect max(datalength(column_name))\n\nfrom affected_table\n\nIn other words, please be sure you're going into this with your head on\n\nstraight.\n\nHow To Change System Catalogs\n\nThis information is Critical To The Defense Of The Free World, and you would be\n\nWell Advised To Do It Exactly As Specified:\n\nuse master\n\ngo\n\nsp_configure \"allow updates\", 1\n\ngo\n\nreconfigure with override /* System 10 and below */\n\ngo\n\nuse victim_database\n\ngo\n\nselect name, colid\n\nfrom syscolumns\n\nwhere id = object_id(\"affected_table\")\n\ngo\n\nbegin tran\n\ngo\n\nupdate syscolumns\n\nset length = new_value\n\nwhere id = object_id(\"affected_table\")\n\nand colid = value_from_above\n\ngo\n\nupdate sysindexes\n\nset maxlen = maxlen + increase/decrease?\n\nwhere id=object_id(\"affected_table\")\n\nand indid = 0\n\ngo\n\n/* check results... cool? Continue... else rollback tran */\n\ncommit tran\n\ngo\n\nuse master\n\ngo\n\nsp_configure \"allow updates\", 0\n\ngo\n\nreconfigure /* System 10 and below */\n\ngo\n\nReturn to top\n\n-------------------------------------------------------------------------------\n\n1.2.2: FAQ on partitioning\n\n-------------------------------------------------------------------------------\n\nIndex of Sections\n\n*What Is Table Partitioning?\n\n+Page Contention for Inserts\n\n+I/O Contention\n\n+Caveats Regarding I/O Contention\n\n*Can I Partition Any Table?\n\n+How Do I Choose Which Tables To Partition?\n\n*Does Table Partitioning Require User-Defined Segments?\n\n*Can I Run Any Transact-SQL Command on a Partitioned Table?\n\n*How Does Partition Assignment Relate to Transactions?\n\n*Can Two Tasks Be Assigned to the Same Partition?\n\n*Must I Use Multiple Devices to Take Advantage of Partitions?\n\n*How Do I Create A Partitioned Table That Spans Multiple Devices?\n\n*How Do I Take Advantage of Table Partitioning with bcp in?\n\n*Getting More Information on Table Partitioning\n\nWhat Is Table Partitioning?\n\nTable partitioning is a procedure that creates multiple page chains for a\n\nsingle table.\n\nThe primary purpose of table partitioning is to improve the performance of\n\nconcurrent inserts to a table by reducing contention for the last page of a\n\npage chain.\n\nPartitioning can also potentially improve performance by making it possible to\n\ndistribute a table's I/O over multiple database devices.\n\nPage Contention for Inserts\n\nBy default, SQL Server stores a table's data in one double-linked set of pages\n\ncalled a page chain. If the table does not have a clustered index, SQL Server\n\nmakes all inserts to the table in the last page of the page chain.\n\nWhen a transaction inserts a row into a table, SQL Server holds an exclusive\n\npage lock on the last page while it inserts the row. If the current last page\n\nbecomes full, SQL Server allocates and links a new last page.\n\nAs multiple transactions attempt to insert data into the table at the same\n\ntime, performance problems can occur. Only one transaction at a time can obtain\n\nan exclusive lock on the last page, so other concurrent insert transactions\n\nblock each other.\n\nPartitioning a table creates multiple page chains (partitions) for the table\n\nand, therefore, multiple last pages for insert operations. A partitioned table\n\nhas as many page chains and last pages as it has partitions.\n\nI/O Contention\n\nPartitioning a table can improve I/O contention when SQL Server writes\n\ninformation in the cache to disk. If a table's segment spans several physical\n\ndisks, SQL Server distributes the table's partitions across fragments on those\n\ndisks when you create the partitions.\n\nA fragment is a piece of disk on which a particular database is assigned space.\n\nMultiple fragments can sit on one disk or be spread across multiple disks.\n\nWhen SQL Server flushes pages to disk and your fragments are spread across\n\ndifferent disks, I/Os assigned to different physical disks can occur in\n\nparallel.\n\nTo improve I/O performance for partitioned tables, you must ensure that the\n\nsegment containing the partitioned table is composed of fragments spread across\n\nmultiple physical devices.\n\nCaveats Regarding I/O Contention\n\nBe aware that when you use partitioning to balance I/O you run the risk of\n\ndisrupting load balancing even as you are trying to achieve it. The following\n\nscenarios can keep you from gaining the load balancing benefits you want:\n\n*You are partitioning an existing table. The existing data could be\n\nsitting on any fragment. Because partitions are randomly assigned, you run\n\nthe risk of filling up a fragment. The partition will then steal space from\n\nother fragments, thereby disrupting load balancing.\n\n*Your fragments differ in size.\n\n*The segment maps are configured such that other objects are using the\n\nfragments to which the partitions are assigned.\n\n*A very large bcp job inserts many rows within a single transaction.\n\nBecause a partition is assigned for the lifetime of a transaction, a huge\n\namount of data could go to one particular partition, thus filling up the\n\nfragment to which that partition is assigned.\n\nCan I Partition Any Table?\n\nNo. You cannot partition the following kinds of tables:\n\n1. Tables with clustered indexes\n\n2. SQL Server system tables\n\n3. Work tables\n\n4. Temporary tables\n\n5. Tables that are already partitioned. However, you can unpartition and then\n\nre-partition tables to change the number of partitions.\n\nHow Do I Choose Which Tables To Partition?\n\nYou should partition heap tables that have large amounts of concurrent insert\n\nactivity. (A heap table is a table with no clustered index.) Here are some\n\nexamples:\n\n1. An \"append-only\" table to which every transaction must write\n\n2. Tables that provide a history or audit list of activities\n\n3. A new table into which you load data with bcp in. Once the data is loaded\n\nin, you can unpartition the table. This enables you to create a clustered\n\nindex on the table, or issue other commands not permitted on a partition\n\ntable.\n\nDoes Table Partitioning Require User-Defined Segments?\n\nNo. By design, each table is intrinsically assigned to one segment, called the\n\ndefault segment. When a table is partitioned, any partitions on that table are\n\ndistributed among the devices assigned to the default segment.\n\nIn the example under \"How Do I Create A Partitioned Table That Spans Multiple\n\nDevices?\", the table sits on a user-defined segment that spans three devices.\n\nCan I Run Any Transact-SQL Command on a Partitioned Table?\n\nNo. Once you have partitioned a table, you cannot use any of the following\n\nTransact-SQL commands on the table until you unpartition it:\n\n1. create clustered index\n\n2. drop table\n\n3. sp_placeobject\n\n4. truncate table\n\n5. alter table table_name partition n\n\nHow Does Partition Assignment Relate to Transactions?\n\nA user is assigned to a partition for the duration of a transaction. Assignment\n\nof partitions resumes with the first insert in a new transaction. The user\n\nholds the lock, and therefore partition, until the transaction ends.\n\nFor this reason, if you are inserting a great deal of data, you should batch it\n\ninto separate jobs, each within its own transaction. See \"How Do I Take\n\nAdvantage of Table Partitioning with bcp in?\", for details.\n\nCan Two Tasks Be Assigned to the Same Partition?\n\nYes. SQL Server randomly assigns partitions. This means there is always a\n\nchance that two users will vie for the same partition when attempting to insert\n\nand one would lock the other out.\n\nThe more partitions a table has, the lower the probability of users trying to\n\nwrite to the same partition at the same time.\n\nMust I Use Multiple Devices to Take Advantage of Partitions?\n\nIt depends on which type of performance improvement you want.\n\nTable partitioning improves performance in two ways: primarily, by decreasing\n\npage contention for inserts and, secondarily, by decreasing i/o contention.\n\n\"What Is Table Partitioning?\" explains each in detail.\n\nIf you want to decrease page contention you do not need multiple devices. If\n\nyou want to decrease i/o contention, you must use multiple devices.\n\nHow Do I Create A Partitioned Table That Spans Multiple Devices?\n\nCreating a partitioned table that spans multiple devices is a multi-step\n\nprocedure. In this example, we assume the following:\n\n*We want to create a new segment rather than using the default segment.\n\n*We want to spread the partitioned table across three devices, data_dev1,\n\ndata_dev2, and data_dev3.\n\nHere are the steps:\n\n1. Define a segment:\n\nsp_addsegment newsegment, my_database,data_dev1\n\n2. Extend the segment across all three devices:\n\nsp_extendsegment newsegment, my_database, data_dev2\n\nsp_extendsegment newsegment, my_database, data_dev3\n\n3. Create the table on the segment:\n\ncreate table my_table\n\n(names, varchar(80) not null)\n\non newsegment\n\n4. Partition the table:\n\nalter table my_table partition 30\n\nHow Do I Take Advantage of Table Partitioning with bcp in?\n\nYou can take advantage of table partitioning with bcp in by following these\n\nguidelines:\n\n1. Break up the data file into multiple files and simultaneously run each of\n\nthese files as a separate bcp job against one table.\n\nRunning simultaneous jobs increases throughput.\n\n2. Choose a number of partitions greater than the number of bcp jobs.\n\nHaving more partitions than processes (jobs) decreases the probability of\n\npage lock contention.\n\n3. Use the batch option of bcp in. For example, after every 100 rows, force a\n\ncommit. Here is the syntax of this command:\n\nbcp table_name in filename -b100\n\nEach time a transaction commits, SQL Server randomly assigns a new\n\npartition for the next insert. This, in turn, reduces the probability of\n\npage lock contention.\n\nGetting More Information on Table Partitioning\n\nFor more information on table partitioning, see the chapter on controlling\n\nphysical data placement in the SQL Server Performance and Tuning Guide.\n\nReturn to top\n\n-------------------------------------------------------------------------------\n\n1.2.3: How to manually drop a table\n\n-------------------------------------------------------------------------------\n\nOccasionally you may find that after issuing a drop table command that the SQL\n\nServer crashed and consequently the table didn't drop entirely. Sure you can't\n\nsee it but that sucker is still floating around somewhere.\n\nHere's a list of instructions to follow when trying to drop a corrupt table:\n\n1.\n\nsp_configure allow, 1\n\ngo\n\nreconfigure with override\n\ngo\n\n2. Write db_id down.\n\nuse db_name\n\ngo\n\nselect db_id()\n\ngo\n\n3. Write down the id of the bad_table:\n\nselect id\n\nfrom sysobjects\n\nwhere name = bad_table_name\n\ngo\n\n4. You will need these index IDs to run dbcc extentzap. Also, remember that if\n\nthe table has a clustered index you will need to run extentzap on index\n\n\"0\", even though there is no sysindexes entry for that indid.\n\nselect indid\n\nfrom sysindexes\n\nwhere id = table_id\n\ngo\n\n5. This is not required but a good idea:\n\nbegin transaction\n\ngo\n\n6. Type in this short script, this gets rid of all system catalog information\n\nfor the object, including any object and procedure dependencies that may be\n\npresent.\n\nSome of the entries are unnecessary but better safe than sorry.\n\ndeclare @obj int\n\nselect @obj = id from sysobjects where name =\n\ndelete syscolumns where id = @obj\n\ndelete sysindexes where id = @obj\n\ndelete sysobjects where id = @obj\n\ndelete sysprocedures where id in\n\n(select id from sysdepends where depid = @obj)\n\ndelete sysdepends where depid = @obj\n\ndelete syskeys where id = @obj\n\ndelete syskeys where depid = @obj\n\ndelete sysprotects where id = @obj\n\ndelete sysconstraints where tableid = @obj\n\ndelete sysreferences where tableid = @obj\n\ndelete sysdepends where id = @obj\n\ngo\n\n7. Just do it!\n\ncommit transaction\n\ngo\n\n8. Gather information to run dbcc extentzap:\n\nuse master\n\ngo\n\nsp_dboption db_name, read, true\n\ngo\n\nuse db_name\n\ngo\n\ncheckpoint\n\ngo\n\n9. Run dbcc extentzap once for each index (including index 0, the data level)\n\nthat you got from above:\n\nuse master\n\ngo\n\ndbcc traceon (3604)\n\ngo\n\ndbcc extentzap (db_id, obj_id, indx_id, 0)\n\ngo\n\ndbcc extentzap (db_id, obj_id, indx_id, 1)\n\ngo\n\nNotice that extentzap runs twice for each index. This is because the\n\nlast parameter (the sort bit) might be 0 or 1 for each index, and you\n\nwant to be absolutely sure you clean them all out.\n\n10. Clean up after yourself.\n\nsp_dboption db_name, read, false\n\ngo\n\nuse db_name\n\ngo\n\ncheckpoint\n\ngo\n\nsp_configure allow, 0\n\ngo\n\nreconfigure with override\n\ngo\n\nReturn to top\n\n-------------------------------------------------------------------------------\n\n1.2.4: Why not max out all my columns?\n\n-------------------------------------------------------------------------------\n\nPeople occasionally ask the following valid question:\n\nSuppose I have varying lengths of character strings none of which should\n\nexceed 50 characters.\n\nIs there any advantage of last_name varchar(50) over this last_name varchar\n\n(255)?\n\nThat is, for simplicity, can I just define all my varying strings to be\n\nvarchar(255) without even thinking about how long they may actually be? Is\n\nthere any storage or performance penalty for this.\n\nThere is no performance penalty by doing this but as another netter pointed\n\nout:\n\nIf you want to define indexes on these fields, then you should specify the\n\nsmallest size because the sum of the maximal lengths of the fields in the\n\nindex can't be greater than 256 bytes.\n\nand someone else wrote in saying:\n\nYour data structures should match the business requirements. This way the\n\ndata structure themselves becomes a data dictionary for others to model\n\ntheir applications (report generation and the like).\n\nReturn to top\n\n-------------------------------------------------------------------------------\n\n1.2.5: What's a good example of a transaction?\n\n-------------------------------------------------------------------------------\n\nThis answer is geared for Online Transaction Processing (OTLP)\n\napplications.\n\nTo gain maximum throughput all your transactions should be in stored procedures\n\n- see Q1.5.8. The transactions within each stored procedure should be short and\n\nsimple. All validation should be done outside of the transaction and only the\n\nmodification to the database should be done within the transaction. Also, don't\n\nforget to name the transaction for sp_whodo - see Q9.2.\n\nThe following is an example of a good transaction:\n\n/* perform validation */\n\nselect ...\n\nif ... /* error */\n\n/* give error message */\n\nelse /* proceed */\n\nbegin\n\nbegin transaction acct_addition\n\nupdate ...\n\ninsert ...\n\ncommit transaction acct_addition\n\nend\n\nThe following is an example of a bad transaction:\n\nbegin transaction poor_us\n\nupdate X ...\n\nselect ...\n\nif ... /* error */\n\n/* give error message */\n\nelse /* proceed */\n\nbegin\n\nupdate ...\n\ninsert ...\n\nend\n\ncommit transaction poor_us\n\nThis is bad because:\n\n*the first update on table X is held throughout the transaction. The idea\n\nwith OLTP is to get in and out fast.\n\n*If an error message is presented to the end user and we await their\n\nresponse, we'll maintain the lock on table X until the user presses return.\n\nIf the user is out in the can we can wait for hours.\n\nReturn to top\n\n-------------------------------------------------------------------------------\n\n1.2.6: What's a natural key?\n\n-------------------------------------------------------------------------------\n\nLet me think back to my database class... okay, I can't think that far so I'll\n\nparaphrase... essentially, a natural key is a key for a given table that\n\nuniquely identifies the row. It's natural in the sense that it follows the\n\nbusiness or real world need.\n\nFor example, assume that social security numbers are unique (I believe it is\n\nstrived to be unique but it's not always the case), then if you had the\n\nfollowing employee table:\n\nemployee:\n\nssn char(09)\n\nf_name char(20)\n\nl_name char(20)\n\ntitle char(03)\n\nThen a natural key would be ssn. If the combination of _name and l_name were\n\nunique at this company, then another natural key would be f_name, l_name. As a\n\nmatter of fact, you can have many natural keys in a given table but in practice\n\nwhat one does is build a surrogate (or artificial) key.\n\nThe surrogate key is guaranteed to be unique because (wait, get back, here it\n\ngoes again) it's typically a monotonically increasing value. Okay, my\n\nmathematician wife would be proud of me... really all it means is that the key\n\nis increasing linearly: i+1\n\nThe reason one uses a surrogate key is because your joins will be faster.\n\nIf we extended our employee table to have a surrogate key:\n\nemployee:\n\nid identity\n\nssn char(09)\n\nf_name char(20)\n\nl_name char(20)\n\ntitle char(03)\n\nThen instead of doing the following:\n\nwhere a.f_name = b.f_name\n\nand a.l_name = a.l_name\n\nwe'd do this:\n\nwhere a.id = b.id\n\nWe can build indexes on these keys and since Sybase's atomic storage unit is\n\n2K, we can stash more values per 2K page with smaller indexes thus giving us\n\nbetter performance (imagine the key being 40 bytes versus being say 4 bytes...\n\nhow many 40 byte values can you stash in a 2K page versus a 4 byte value? --\n\nand how much wood could a wood chuck chuck, if a wood chuck could chuck wood?)\n\nDoes it have anything to do with natural joins?\n\nUm, not really... from \"A Guide to Sybase..\", McGovern and Date, p. 112:\n\nThe equi-join by definition must produce a result containing two identical\n\ncolumns. If one of those two columns is eliminated, what is left is called\n\nthe natural join.\n\nReturn to top\n\n-------------------------------------------------------------------------------\n\n1.2.7: Making a Stored Procedure invisible\n\n-------------------------------------------------------------------------------\n\nSystem 11.5 and above\n\nIt is now possible to encrypt your stored procedure code that is stored in the\n\nsyscomments table. This is preferred than the old method of deleting the data\n\nas deleting will impact future upgrades. You can encrypt the text with the\n\nsp_hidetext system procedure.\n\nPre-System 11.5\n\nPerhaps you are trying to prevent the buyer of your software from defncopy'ing\n\nall your stored procedures. It is perfectly safe to delete the syscomments\n\nentries of any stored procedures you'd like to protect:\n\nsp_configure \"allow updates\", 1\n\ngo\n\nreconfigure with override /* System 10 and below */\n\ngo\n\nuse affected_database\n\ngo\n\ndelete syscomments where id = object_id(\"procedure_name\")\n\ngo\n\nuse master\n\ngo\n\nsp_configure \"allow updates\", 0\n\ngo\n\nI believe in future releases of Sybase we'll be able to see the SQL that is\n\nbeing executed. I don't know if that would be simply the stored procedure name\n\nor the SQL itself.\n\nReturn to top\n\n-------------------------------------------------------------------------------\n\n1.2.8: Saving space when inserting rows monotonically\n\n-------------------------------------------------------------------------------\n\nIf the columns that comprise the clustered index are monotonically increasing\n\n(that is, new row key values are greater than those previously inserted) the\n\nfollowing System 11 dbcc tune will not split the page when it's half way full.\n\nRather it'll let the page fill and then allocate another page:\n\ndbcc tune(ascinserts, 1, \"my_table\")\n\nBy the way, SyBooks is wrong when it states that the above needs to be reset\n\nwhen the SQL Server is rebooted. This is a permanent setting.\n\nTo undo it:\n\ndbcc tune(ascinserts, 0, \"my_table\")\n\nReturn to top\n\n-------------------------------------------------------------------------------\n\n1.2.9: How to compute database fragmentation\n\n-------------------------------------------------------------------------------\n\nCommand\n\ndbcc traceon(3604)\n\ngo\n\ndbcc tab(production, my_table, 0)\n\ngo\n\nInterpretation\n\nA delta of one means the next page is on the same track, two is a short seek,\n\nthree is a long seek. You can play with these constants but they aren't that\n\nimportant.\n\nA table I thought was unfragmented had L1 = 1.2 L2 = 1.8\n\nA table I thought was fragmented had L1 = 2.4 L2 = 6.6\n\nHow to Fix\n\nYou fix a fragmented table with clustered index by dropping and creating the\n\nindex. This measurement isn't the correct one for tables without clustered\n\nindexes. If your table doesn't have a clustered index, create a dummy one and\n\ndrop it.\n\nReturn to top\n\n-------------------------------------------------------------------------------\n\n1.2.10: Tasks a DBA should do...\n\n-------------------------------------------------------------------------------\n\nA good presentation of a DBA's duties has been made available by Jeff Garbus (\n\nje...@soaringeagleltd.com) of Soaring Eagle Consulting Ltd (http://\n\nwww.soaringeagleltd.com) and numerous books can be found here. These are\n\nPowerpoint slides converted to web pages and so may be difficult to view with a\n\ntext browser!\n\nAn alternative view is catalogued below. (OK, so this list is crying out for a\n\nbit of a revamp since checkstorage came along Ed!)\n\nDBA Tasks\n\n+------------------------+---------------+--------------------------------+\n\n+------------------------+---------------+--------------------------------+\n\n| Task | Reason | Period |\n\n+------------------------+---------------+--------------------------------+\n\n| dbcc checkdb, | I consider | If your SQL Server permits, |\n\n| checkcatalog, | these the | daily before your database |\n\n| checkalloc | minimal | dumps. If this is not possible |\n\n| | dbcc's to | due to the size of your |\n\n| | ensure the | databases, then try the |\n\n| | integrity of | different options so that the |\n\n| | your database | end of, say, a week, you've |\n\n| | | run them all. |\n\n+------------------------+---------------+--------------------------------+\n\n| Disaster recovery | Always be | |\n\n| scripts - scripts to | prepared for | |\n\n| rebuild your SQL | the worst. | |\n\n| Server in case of | Make sure to | |\n\n| hardware failure | test them. | |\n\n+------------------------+---------------+--------------------------------+\n\n| scripts to logically | You can | Daily |\n\n| dump your master | selectively | |\n\n| database, that is bcp | rebuild your | |\n\n| the critical system | database in | |\n\n| tables: sysdatabases, | case of | |\n\n| sysdevices, syslogins, | hardware | |\n\n| sysservers, sysusers, | failure | |\n\n| syssegments, | | |\n\n| sysremotelogins | | |\n\n+------------------------+---------------+--------------------------------+\n\n| %ls -la disk_devices | A system | After any change as well as |\n\n| | upgrade is | daily |\n\n| | known to | |\n\n| | change the | |\n\n| | permissions. | |\n\n+------------------------+---------------+--------------------------------+\n\n| dump the user | CYA* | Daily |\n\n| databases | | |\n\n+------------------------+---------------+--------------------------------+\n\n| dump the transaction | CYA | Daily |\n\n| logs | | |\n\n+------------------------+---------------+--------------------------------+\n\n| dump the master | CYA | After any change as well as |\n\n| database | | daily |\n\n+------------------------+---------------+--------------------------------+\n\n| System 11 and beyond - | This is the | After any change as well as |\n\n| save the $DSQUERY.cfg | configuration | daily |\n\n| to tape | that you've | |\n\n| | dialed in, | |\n\n| | why redo the | |\n\n| | work? | |\n\n+------------------------+---------------+--------------------------------+\n\n| update statistics on | To ensure the | Depending on how often your |\n\n| frequently changed | performance | major tables change. Some |\n\n| tables and | of your SQL | tables are pretty much static |\n\n| sp_recompile | Server | (e.g. lookup tables) so they |\n\n| | | don't need an update |\n\n| | | statistics, other tables |\n\n| | | suffer severe trauma (e.g. |\n\n| | | massive updates/deletes/ |\n\n| | | inserts) so an update stats |\n\n| | | needs to be run either nightly |\n\n| | | /weekly/monthly. This should |\n\n| | | be done using cronjobs. |\n\n+------------------------+---------------+--------------------------------+\n\n| create a dummy SQL | See disaster | When time permits |\n\n| Server and do bad | recovery! | |\n\n| things to it: delete | | |\n\n| devices, destroy | | |\n\n| permissions... | | |\n\n+------------------------+---------------+--------------------------------+\n\n| Talk to the | It's better | As time permits. |\n\n| application | to work with | |\n\n| developers. | them than | |\n\n| | against them. | |\n\n+------------------------+---------------+--------------------------------+\n\n| Learn new tools | So you can | As time permits. |\n\n| | sleep! | |\n\n+------------------------+---------------+--------------------------------+\n\n| Read c.d.s | Passes the | Priority One! |\n\n| | time. | |\n\n+------------------------+---------------+--------------------------------+\n\n* Cover Your Ass\n\nReturn to top\n\n-------------------------------------------------------------------------------\n\n1.2.11: How to implement database security\n\n-------------------------------------------------------------------------------\n\nThis is a brief run-down of the features and ideas you can use to implement\n\ndatabase security:\n\nLogins, Roles, Users, Aliases and Groups\n\n*sp_addlogin - Creating a login adds a basic authorisation for an account\n\n- a username and password - to connect to the server. By default, no access\n\nis granted to any individual databases.\n\n*sp_adduser - A user is the addition of an account to a specific database.\n\n*sp_addalias - An alias is a method of allowing an account to use a\n\nspecific database by impersonating an existing database user or owner.\n\n*sp_addgroup - Groups are collections of users at the database level.\n\nUsers can be added to groups via the sp_adduser command.\n\nA user can belong to only one group - a serious limitation that Sybase\n\nmight be addressing soon according to the ISUG enhancements requests.\n\nPermissions on objects can be granted or revoked to or from users or\n\ngroups.\n\n*sp_role - A role is a high-level Sybase authorisation to act in a\n\nspecific capacity for administration purposes. Refer to the Sybase\n\ndocumentation for details.\n\nRecommendations\n\nMake sure there is a unique login account for each physical person and/or\n\nprocess that uses the server. Creating generic logins used by many people or\n\nprocesses is a bad idea - there is a loss of accountability and it makes it\n\ndifficult to track which particular person is causing server problems when\n\nlooking at the output of sp_who. Note that the output of sp_who gives a\n\nhostname - properly coded applications will set this value to something\n\nmeaningful (ie. the machine name the client application is running from) so you\n\ncan see where users are running their programs. Note also that if you look at\n\nmaster..sysprocesses rather than just sp_who, there is also a program_name.\n\nAgain, properly coded applications will set this (eg. to 'isql') so you can see\n\nwhich application is running. If you're coding your own client applications,\n\nmake sure you set hostname and program_name via the appropriate Open Client\n\ncalls. One imaginative use I've seen of the program_name setting is to\n\nincorporate the connection time into the name, eg APPNAME-DDHHMM (you have 16\n\ncharacters to play with), as there's no method of determining this otherwise.\n\nSet up groups, and add your users to them. It is much easier to manage an\n\nobject permissions system in this way. If all your permissions are set to\n\ngroups, then adding a user to the group ensures that users automatically\n\ninherit the correct permissions - administration is *much* simpler.\n\nObjects and Permissions\n\nAccess to database objects is defined by granting and/or revoking various\n\naccess rights to and from users or groups. Refer to the Sybase documentation\n\nfor details.\n\nRecommendations\n\nThe ideal setup has all database objects being owned by the dbo, meaning no\n\nordinary users have any default access at all. Specific permissions users\n\nrequire to access the database are granted explicitly. As mentioned above - set\n\npermissions for objects to a group and add users to that group. Any new user\n\nadded to the database via the group then automatically obtains the correct set\n\nof permissions.\n\nPreferably, no access is granted at all to data tables, and all read and write\n\nactivity is accomplished through stored procedures that users have execute\n\npermission on. The benefit of this from a security point of view is that access\n\ncan be rigidly controlled with reference to the data being manipulated, user\n\nclearance levels, time of day, and anything else that can be programmed via\n\nT-SQL. The other benefits of using stored procedures are well known (see Q1.5.8\n\n). Obviously whether you can implement this depends on the nature of your\n\napplication, but the vast majority of in-house-developed applications can rely\n\nsolely on stored procedures to carry out all the work necessary. The only\n\nserver-side restriction on this method is the current inability of stored\n\nprocedures to adequately handle text and image datatypes (see Q1.5.12). To get\n\naround this views can be created that expose only the necessary columns to\n\ndirect read or write access.\n\nViews\n\nViews can be a useful general security feature. Where stored procedures are\n\ninappropriate views can be used to control access to tables to a lesser extent.\n\nThey also have a role in defining row-level security - eg. the underlying table\n\ncan have a security status column joined to a user authorisation level table in\n\nthe view so that users can only see data they are cleared for. Obviously they\n\ncan also be used to implement column-level security by screening out sensitive\n\ncolumns from a table.\n\nTriggers\n\nTriggers can be used to implement further levels of security - they could be\n\nviewed as a last line of defence in being able to rollback unauthorised write\n\nactivity (they cannot be used to implement any read security). However, there\n\nis a strong argument that triggers should be restricted to doing what they were\n\ndesigned for - implementing referential integrity - rather being loaded up with\n\napplication logic.\n\nAdministrative Roles\n\nWith Sybase version 10 came the ability to grant certain administrative roles\n\nto user accounts. Accounts can have sa-level privilege, or be restricted to\n\nsecurity or operator roles - see sp_role.\n\nRecommendations\n\nThe use of any generic account is not a good idea. If more than one person\n\nrequires access as sa to a server, then it is more accountable and traceable if\n\nthey each have an individual account with sa_role granted.\n\nReturn to top\n\n-------------------------------------------------------------------------------\n\n1.2.12: How to Shrink a Database\n\n-------------------------------------------------------------------------------\n\nWarning: This document has not been reviewed. Treat it as alpha-test\n\nquality information and report any problems and suggestions to\n\nbr...@sybase.com\n\nIt has historically been difficult to shrink any database except tempdb\n\n(because it is created fresh every boot time). The two methods commonly used\n\nhave been:\n\n1. Ensure that you have scripts for all your objects (some tools like SA\n\nCompanion, DB Artisan or dbschema.pl from Sybperl can create scripts from\n\nan existing database), then bcp out your data, drop the database, recreate\n\nit smaller, run your scripts, and bcp in your data.\n\n2. Use a third-party tool such as DataTool's SQL Backtrack, which in essence\n\nautomates the first process.\n\nThis technote outlines a third possibility that can work in most cases.\n\nAn Unsupported Method to Shrink a Database\n\nThis process is fairly trivial in some cases, such as removing a recently added\n\nfragment or trimming a database that has a log fragment as its final\n\nallocation, but can also be much more complicated or time consuming than the\n\nscript and bcp method.\n\nGeneral Outline\n\nThe general outline of how to do it is:\n\n1. Make a backup of the current database\n\n2. Migrate data from sysusages fragments with high lstart values to fragments\n\nwith low lstart values.\n\n3. Edit sysusages to remove high lstart fragments that no longer have data\n\nallocations.\n\n4. Reboot sql server.\n\nDetails\n\n1. Dump your database. If anything goes wrong, you will need to recover from\n\nthis backup!\n\n2. Decide how many megabytes of space you wish to remove from your database.\n\n3. Examine sysusages for the database. You will be shrinking the database by\n\nremoving the fragments with the highest lstart values. If the current\n\nfragments are not of appropriate sizes, you may need to drop the database,\n\nrecreate it so there are more fragments, and reload the dump.\n\nA trivial case: An example of a time when you can easily shrink a\n\ndatabase is if you have just altered it and are sure there has been no\n\nactivity on the new fragment. In this case, you can directly delete the\n\nlast row in sysusages for the db (this row was just added by alter db)\n\nand reboot the server and it should come up cleanly.\n\n4. Change the segmaps of the fragments you plan to remove to 0. This will\n\nprevent future data allocations to these fragments.\n\nNote: If any of the fragments you are using have user defined segments\n\non them, drop those segments before doing this.\n\nsp_configure \"allow updates\", 1\n\ngo\n\nreconfigure with override -- not necessary in System 11\n\ngo\n\nupdate sysusages\n\nset segmap = 0\n\nwhere dbid = <dbid>\n\nand lstart = <lstart>\n\ngo\n\ndbcc dbrepair(<dbname>, remap)\n\ngo\n\nEnsure that there is at least one data (segmap 3) and one log (segmap 4)\n\nfragment, or one mixed (segmap 7) fragment.\n\nIf the server has been in use for some time, you can shrink it by deleting\n\nrows from sysusages for the db, last rows first, after making sure that no\n\nobjects have any allocations on the usages.\n\n5. Determine which objects are on the fragments you plan to remove.\n\ntraceon(3604)\n\ngo\n\ndbcc usedextents( dbid,0,0,1)\n\ngo\n\nFind the extent with the same value as the lstart of the first fragment you\n\nplan to drop. You need to migrate every object appearing from this point on\n\nin the output.\n\n6. Migrate these objects onto earlier fragments in the database.\n\nObjids other than 0 or 99 are objects that you must migrate or drop. You\n\ncan migrate a user table by building a new clustered index on the table\n\n(since the segmap was changed, the new allocations will not go on this\n\nfragment).\n\nYou can migrate some system tables (but not all) using the sp_fixindex\n\ncommand to rebuild its clustered index. However, there are a few system\n\ntables that cannot have their clustered indexes rebuilt, and if they have\n\nany allocations on the usage, you are out of luck.\n\nIf the objid is 8, then it is the log. You can migrate the log by ensuring\n\nthat another usage has a log segment (segmap 4 or 7). Do enough activity on\n\nthe database to fill an extents worth of log pages, then checkpoint and\n\ndump tran.\n\nOnce you have moved all the objects, delete the row from sysusages and\n\nreboot the server.\n\nRun dbcc checkdb and dbcc checkalloc on the database to be sure you are ok,\n\nthen dump the database again.\n\nReturn to top\n\n-------------------------------------------------------------------------------\n\n1.2.13: How do I audit the SQL sent to the server?\n\n-------------------------------------------------------------------------------\n\nThis does not seem to be well documented, so here is a quick means of auditing\n\nthe SQL text that is sent to the server. Note that this simply audits the SQL\n\nsent to the server. So, if your user process executes a big stored procedure,\n\nall you will see here is a call to the stored procedure. None of the SQL that\n\nis executed as part of the stored procedure will be listed.\n\nFirstly, you need to have installed Sybase security (which involves installing\n\nthe sybsecurity database and loading it using the script $SYBASE/scripts/\n\ninstallsecurity). Read the Sybase Security Administration Manual, you may\n\nwant to enable a threshold procedure to toggle between a couple of audit\n\ntables. Be warned, that the default configuration option \"suspend auditing\n\nwhen device full\" is set to 1. This means that the server will suspend all\n\nnormal SQL operations if the audit database becomes full and the sso logs in\n\nand gets rid of some data. You might want to consider changing this to 0\n\nunless yours is a particularly sensitive installation.\n\nOnce that is done, you need to enable auditing. If you haven't already, you\n\nwill need to restart ASE in order to start the audit subsystem. Then comes the\n\nbit that does not seem well documented, you need to select an appropriate audit\n\noption, and the one for the SQL text is \"cmdtext\". From the sybsecurity\n\ndatabase, issue\n\nsp_audit \"cmdtext\",<username>,\"all\",\"on\"\n\nfor each user on the system that wish to collect the SQL for. sp_audit seems\n\nto imply that you can replace \"<username>\" with all, but I get the error\n\nmessage \"'all' is not a valid user name\". Finally, enable auditing for the\n\nsystem as a whole using\n\nsp_configure \"auditing\",1\n\ngo\n\nIf someone knows where in the manuals this is well documented, I will add a\n\nlink/reference.\n\nNote: The stored procedure sp_audit had a different name under previous\n\nreleases. I think that it was called sp_auditoption. Also, to get a full list\n\nof the options and their names, go into sybsecurity and simply run sp_audit\n\nwith no arguments.\n\nReturn to top\n\n-------------------------------------------------------------------------------\n\nnext prev ASE FAQ\n\nPosted-By: auto-faq 3.3.1 beta (Perl 5.005)\n\nArchive-name: databases/sybase-faq/part4\n\nURL: http://www.isug.com/Sybase_FAQ\n\nVersion: 1.2\n\nMaintainer: David Owen\n\nLast-modified: 2000/06/07\n\nPosting-Frequency: posted every 3rd month\n\nA how-to-find-the-FAQ article is posted on the intervening months.\n\nSybase Frequently Asked Questions\n\nSybase FAQ Home Page Adaptive Server Enterprise FAQ Adaptive Server Anywhere\n\nFAQ Replication Server FAQ Search the FAQ Sybase FAQ\n\nAdaptive Server Enterprise\n\n0. What's in a name?\n\n1. ASE Administration\n\n1.1 Basic Administration\n\n1.2 User Database Administration\n\n1.3 Advanced Administration\n\n1.4 General Troubleshooting\n\n1.5 Performance and Tuning\n\n2. Platform Specific Issues\n\n3. DBCC's\n\n4. isql\n\n5. bcp\n\n6. SQL Development\n\n6.1 SQL Fundamentals\n\n6.2 SQL Advanced\n\n7. Open Client\n\n9. Freeware\n\n10.Sybase Technical News\n\n11.Additional Information\n\n12.Miscellany\n\n-------------------------------------------------------------------------------\n\nWhat's in a name?\n\nThroughout this FAQ you will find references to SQL Server and, starting with\n\nthis release, ASE or Adaptive Server Enterprise to give it its full name. You\n\nmight also be a little further confused, since Microsoft also seem to have a\n\nproduct called SQL Server.\n\nWell, back at about release 4.2 of Sybase SQL Server, the products were exactly\n\nthe same. Microsoft were to do the port to NT. Well, it is pretty well\n\ndocumented, but there was a falling out. Both companies kept the same name for\n\ntheir data servers and confusion began to reign. In an attempt to try and sort\n\nthis out, Sybase renamed their product Adaptive Server Enterprise (ASE)\n\nstarting with version 11.5.\n\nI found this quote in a Sybase manual the other day:\n\nSince changing the name of Sybase SQL Server to Adaptive Server Enterprise,\n\nSybase uses the names Adaptive Server and Adaptive Server Enterprise to refer\n\ncollectively to all supported versions of the Sybase SQL Server and Adaptive\n\nServer Enterprise. Version-specific references to Adaptive Server or SQL Server\n\ninclude version numbers.\n\nI will endeavour to try and do the same within the FAQ, but the job is far from\n\ncomplete!\n\nBack to Top\n\nBasic ASE Administration\n\n1.1.1 What is SQL Server and ASE anyway?\n\n1.1.2 How do I start/stop SQL Server when the CPU reboots?\n\n1.1.3 How do I move tempdb off of the master device?\n\n1.1.4 How do I correct timeslice -201?\n\n1.1.5 The how's and why's on becoming a Certified Sybase\n\nProfessional DBA (CSPDBA)?\n\n1.1.6 RAID and Sybase\n\n1.1.7 How to swap a db device with another\n\n1.1.8 Server naming and renaming\n\n1.1.9 How do I interpret the tli strings in the interface file?\n\n1.1.10How can I tell the datetime my Server started?\n\n1.1.11Raw partitions or regular files?\n\n1.1.12Is Sybase Y2K (Y2000) compliant?\n\n1.1.13How Can I Run the SQL Server Upgrade Manually?\n\nnext # ASE FAQ\n\n-------------------------------------------------------------------------------\n\n1.1.1: What is SQL Server and ASE?\n\n-------------------------------------------------------------------------------\n\nOverview\n\nBefore Sybase System 10 (as they call it) we had Sybase 4.x. Sybase System 10\n\nhas some significant improvements over Sybase 4.x product line. Namely:\n\n*the ability to allocate more memory to the dataserver without degrading\n\nits performance.\n\n*the ability to have more than one database engine to take advantage of\n\nmulti-processor cpu machines.\n\n*a minimally intrusive process to perform database and transaction dumps.\n\nBackground and More Terminology\n\nA SQL Server is simply a Unix process. It is also known as the database engine.\n\nIt has multiple threads to handle asynchronous I/O and other tasks. The number\n\nof threads spawned is the number of engines (more on this in a second) times\n\nfive. This is the current implementation of Sybase System 10, 10.0.1 and 10.0.2\n\non IRIX 5.3.\n\nEach SQL dataserver allocates the following resources from a host machine:\n\n*memory and\n\n*raw partition space.\n\nEach SQL dataserver can have up to 255 databases. In most implementations the\n\nnumber of databases is limited to what seems reasonable based on the load on\n\nthe SQL dataserver. That is, it would be impractical to house all of a large\n\ncompany's databases under one SQL dataserver because the SQL dataserver (a Unix\n\nprocess) will become overloaded.\n\nThat's where the DBA's experience comes in with interrogation of the user\n\ncommunity to determine how much activity is going to result on a given database\n\nor databases and from that we determine whether to create a new SQL Server or\n\nto house the new database under an existing SQL Server. We do make mistakes\n\n(and businesses grow) and have to move databases from one SQL Server to\n\nanother. And at times SQL Servers need to move from one CPU server to another.\n\nWith Sybase System 10, each SQL Server can be configured to have more than one\n\nengine (each engine is again a Unix process). There's one primary engine that\n\nis the master engine and the rest of the engines are subordinates. They are\n\nassigned tasks by the master.\n\nInterprocess communication among all these engines is accomplished with shared\n\nmemory.\n\nSome times when a DBA issues a Unix kill command to extinguish a maverick\n\nSQL Server, the subordinate engines are forgotten. This leaves the shared\n\nmemory allocated and eventually we may get in to situations where swapping\n\noccurs because this memory is locked. To find engines that belong to no\n\nmaster SQL Server, simple look for engines owned by /etc/init (process id\n\n1). These engines can be killed -- this is just FYI and is a DBA duty.\n\nBefore presenting an example of a SQL Server, some other topics should be\n\ncovered.\n\nConnections\n\nA SQL Server has connections to it. A connection can be viewed as a user login\n\nbut it's not necessarily so. That is, a client (a user) can spark up multiple\n\ninstances of their application and each client establishes its own connection\n\nto the SQL dataserver. Some clients may require two or more per invocation. So\n\ntypically DBA's are only concerned with the number of connections because the\n\nnumber of users typically does not provide sufficient information for us to do\n\nour job.\n\nConnections take up SQL Server resources, namely memory, leaving less\n\nmemory for the SQL Servers' available cache.\n\nSQL Server Buffer Cache\n\nIn Sybase 4.0.1 there was a limit to the amount of memory that could be\n\nallocated to a SQL Server. It was around 80MB, with 40MB being the typical max.\n\nThis was due to internal implementations of Sybase's data structures.\n\nWith Sybase System 10 there really is no limit. For instance, we have a SQL\n\nServer cranked up to 300MB.\n\nThe memory in a SQL Server is primarily used to cache data pages from disk.\n\nConsider that the SQL Server is a light weight Operating System: handling user\n\n(connections), allocating memory to users, keeping track of which data pages\n\nneed to be flushed to disk and the sort. Very sophisticated and complex.\n\nObviously if a data page is found in memory it's much faster to retrieve than\n\ngoing out to disk.\n\nEach connection takes away a little bit from the available memory that is used\n\nto cache disk pages. Upon startup, the SQL Server pre-allocates the memory that\n\nis needed for each connection so it's not prudent to configure 500 connections\n\nwhen only 300 are needed. We'd waste 200 connections and the memory associated\n\nwith that. On the other hand, it is also imprudent to under configure the\n\nnumber of connections; users have a way of soaking up a resource (like a SQL\n\nServer) and if users have all the connections a DBA cannot get into the server\n\nto allocate more connections.\n\nOne of the neat things about a SQL Server is that it reaches (just like a Unix\n\nprocess) a working set. That is, upon startup it'll do a lot of physical I/O's\n\nto seed its cache, to get lookup information for typical transactions and the\n\nlike. So initially, the first users have heavy hits because their requests have\n\nto be performed as a physical I/O. Subsequent transactions have less physical I\n\n/O and more logical I/O's. Logical I/O is an I/O that is satisfied in the SQL\n\nServers' buffer cache. Obviously, this is the preferred condition.\n\nDSS vs OLTP\n\nWe throw around terms like everyone is supposed to know this high tech lingo.\n\nThe problem is that they are two different animals that require a SQL Server to\n\nbe tuned accordingly for each.\n\nWell, here's the low down.\n\nDSS\n\nDecision Support System\n\nOLTP\n\nOnline Transaction Processing\n\nWhat do these mean? OLTP applications are those that have very short orders of\n\nwork for each connection: fetch this row and with the results of it update one\n\nor two other rows. Basically, small number of rows affected per transaction in\n\nrapid sucession, with no significant wait times between operations in a\n\ntransaction.\n\nDSS is the lumbering elephant in the database world (unless you do some\n\ntricks... out of this scope). DSS requires a user to comb through gobs of data\n\nto aggregate some values. So the transactions typically involve thousands of\n\nrows. Big difference than OLTP.\n\nWe never want to have DSS and OLTP on the same SQL Server because the nature of\n\nOLTP is to grab things quickly but the nature of DSS is to stick around for a\n\nlong time reading tons of information and summarizing the results.\n\nWhat a DSS application does is flush out the SQL Server's data page cache\n\nbecause of the tremendous amount of I/O's. This is obviously very bad for OTLP\n\napplications because the small transactions are now hurt by this trauma. When\n\nit was only OLTP a great percentage of I/O was logical (satisfied in the\n\ncache); now transactions must perform physical I/O.\n\nThat's why it's important in Sybase not to mix DSS and OLTP, at least until\n\nSystem 11 arrives.\n\nSybase System 11 release will allow for the mixing of OLTP and DSS by\n\nallowing the DBA to partition (and name) the SQL Server's buffer cache and\n\nassign it to different databases and/or objects. The idea is to allow DSS\n\nto only affect their pool of memory and thus allowing OLTP to maintain its\n\nworking set of memory.\n\nAsynchronous I/O\n\nWhy async I/O? The idea is that in a typical online transaction processing\n\n(OLTP) application, you have many connections (over 200 connections) and short\n\ntransactions: get this row, update that row. These transactions are typically\n\nspread across different tables of the databases. The SQL Server can then\n\nperform each one of these asynchronously without having to wait for others to\n\nfinish. Hence the importance of having async I/O fixed on our platform.\n\nEngines\n\nSybase System 10 can have more than one engine (as stated above). Sybase has\n\ntrace flags to pin the engines to a given CPU processor but we typically don't\n\ndo this. It appears that the master engine goes to processor 0 and subsequent\n\nsubordinates to the next processor.\n\nCurrently, Sybase does not scale linearly. That is, five engines do not make\n\nSybase perform five times as fast however we do max out with four engines.\n\nAfter that performance starts to degrade. This is supposed to be fixed with\n\nSybase System 11.\n\nPutting Everything Together\n\nAs previously mentioned, a SQL Server is a collection of databases with\n\nconnections (that are the users) to apply and retrieve information to and from\n\nthese containers of information (databases).\n\nThe SQL Server is built and its master device is typically built over a medium\n\nsized (50MB) raw partition. The tempdb is built over a cooked (regular - as\n\nopposed to a raw device) file system to realize any performance gains by\n\nbuffered writes. The databases themselves are built over the raw logical\n\ndevices to ensure their integrity.\n\nPhysical and Logical Devices\n\nSybase likes to live in its own little world. This shields the DBA from the\n\noutside world known as Unix (or VMS). However, it needs to have a conduit to\n\nthe outside world and this is accomplished via devices.\n\nAll physical devices are mapped to logical devices. That is, given a physical\n\ndevice (such as /lv1/dumps/tempdb_01.efs or /dev/rdsk/dks1ds0) it is mapped by\n\nthe DBA to a logical device. Depending on the type of the device, it is\n\nallocated, by the DBA, to the appropriate place (vague enough?).\n\nOkay, let's try and clear this up...\n\nDump Device\n\nThe DBA may decide to create a device for dumping the database nightly. The DBA\n\nneeds to create a dump device.\n\nWe'll call that logically in the database datadump_for_my_db but we'll map it\n\nto the physical world as /lv1/dumps/in_your_eye.dat So the DBA will write a\n\nscript that connects to the SQL Server and issues a command like this:\n\ndump database my_stinking_db to datadump_for_my_db\n\ngo\n\nand the backupserver (out of this scope) takes the contents of my_stinking_db\n\nand writes it out to the disk file /lv1/dumps/in_your_eye.dat\n\nThat's a dump device. The thing is that it's not preallocated. This special\n\ndevice is simply a window to the operating system.\n\nData and Log Devices\n\nAh, now we are getting into the world of pre-allocation. Databases are built\n\nover raw partitions. The reason for this is because Sybase needs to be\n\nguaranteed that all its writes complete successfully. Otherwise, if it posted\n\nto a file system buffer (as in a cooked file system) and the machine crashed,\n\nas far as Sybase is concerned the write was committed. It was not, however, and\n\nintegrity of the database was lost. That is why Sybase needs raw partitions.\n\nBut back to the matter at hand...\n\nWhen building a new SQL Server, the DBA determines how much space they'll need\n\nfor all the databases that will be housed in this SQL Server.\n\nEach production database is composed of data and log.\n\nThe data is where the actual information resides. The log is where the changes\n\nare kept. That is, every row that is updated/deleted/inserted gets placed into\n\nthe log portion then applied to the data portion of the database.\n\nThat's why DBA strives to place the raw devices for logs on separate disks\n\nbecause everything has to single thread through the log.\n\nA transaction is a collection of SQL statements (insert/delete/update) that are\n\ngrouped together to form a single unit of work. Typically they map very closely\n\nto the business.\n\nI'll quote the Sybase SQL Server System Administration guide on the role of the\n\nlog:\n\nThe transaction log is a write-ahead log. When a user issues a statement\n\nthat would modify the database, SQL Server automatically writes the changes\n\nto the log. After all changes for a statement have been recorded in the\n\nlog, they are written to an in-cache copy of the data page. The data page\n\nremains in cache until the memory is needed for another database page. At\n\nthat time, it is written to disk. If any statement in a transaction fails\n\nto complete, SQL Server reverses all changes made by the transaction. SQL\n\nServer writes an \"end transaction\" record to the log at the end of each\n\ntransaction, recording the status (success or failure) of the transaction\n\nAs such, the log will grow as user connections affect changes to the database.\n\nThe need arises to then clear out the log of all transactions that have been\n\nflushed to disk. This is performed by issuing the following command:\n\ndump transaction my_stinking_db to logdump_for_my_db\n\ngo\n\nThe SQL Server will write to the dumpdevice all transactions that have been\n\ncommitted to disk and will delete the entries from its copy, thus freeing up\n\nspace in the log. Dumping of the transaction logs is accomplished via cron (the\n\nUnix scheduler, NT users would have to resort to at or some third party tool) .\n\nWe schedule the heavily hit databases every 20 minutes during peak times.\n\nA single user can fill up the log by having begin transaction with no\n\ncorresponding commit/rollback transaction. This is because all their\n\nchanges are being applied to the log as an open-ended transaction, which is\n\nnever closed. This open-ended transaction cannot be flushed from the log,\n\nand therefore grows until it occupies all of the free space on the log\n\ndevice.\n\nAnd the way we dump it is with a dump device. :-)\n\nAn Example\n\nIf the DBA has four databases to plop on this SQL Server and they need a total\n\nof 800MB of data and 100MB of log (because that's what really matters to us),\n\nthen they'd probably do something like this:\n\n1. allocate sufficient raw devices to cover the data portion of all the\n\ndatabases\n\n2. allocate sufficient raw devices to cover the log portion of all the\n\ndatabases\n\n3. start allocating the databases to the devices.\n\nFor example, assuming the following database requirements:\n\nDatabase Requirements\n\n+----+------+-----+\n\n+----+------+-----+\n\n| DB | Data | Log |\n\n+----+------+-----+\n\n| a | 300 | 30 |\n\n+----+------+-----+\n\n| b | 400 | 40 |\n\n+----+------+-----+\n\n| c | 100 | 10 |\n\n+----+------+-----+\n\nand the following devices:\n\nDevices\n\n+---------------+----------+------+\n\n+---------------+----------+------+\n\n| Logical | Physical | Size |\n\n+---------------+----------+------+\n\n| dks3d1s2_data | /dev/ | 500 |\n\n| | rdsk/ | |\n\n| | dks3d1s2 | |\n\n+---------------+----------+------+\n\n| dks4d1s2_data | /dev/ | 500 |\n\n| | rdsk/ | |\n\n| | dks4d1s2 | |\n\n+---------------+----------+------+\n\n| dks5d1s0_log | /dev/ | 200 |\n\n| | rdsk/ | |\n\n| | dks5d1s0 | |\n\n+---------------+----------+------+\n\nthen the DBA may elect to create the databases as follows:\n\ncreate database a on dks3d1s2_data = 300 log on dks5d1s0_log = 30\n\ncreate database b on dks4d1s2_data = 400 log on dks5d1s0_log = 40\n\ncreate database c on dks3d1s2_data = 50, dks4d1s2_data = 50 log on\n\ndks5d1s0_log = 10\n\nSome of the devices will have extra space available because out database\n\nallocations didn't use up all the space. That's fine because it can be used for\n\nfuture growth. While the Sybase SQL Server is running, no other Sybase SQL\n\nServer can re-allocate these physical devices.\n\nTempDB\n\nTempDB is simply a scratch pad database. It gets recreated when a SQL Server is\n\nrebooted. The information held in this database is temporary data. A query may\n\nbuild a temporary table to assist it; the Sybase optimizer may decide to create\n\na temporary table to assist itself.\n\nSince this is an area of constant activity we create this database over a\n\ncooked file system which has historically proven to have better performance\n\nthan raw - due to the buffered writes provided by the Operating System.\n\nPort Numbers\n\nWhen creating a new SQL Server, we allocate a port to it (currently, DBA\n\nreserves ports 1500 through 1899 for its use). We then map a host name to the\n\ndifferent ports: hera, fddi-hera and so forth. We can actually have more than\n\none port number for a SQL Server but we typically don't do this.\n\nBack to top\n\n-------------------------------------------------------------------------------\n\n1.1.2: How to start/stop SQL Server when CPU reboots\n\n-------------------------------------------------------------------------------\n\nBelow is an example of the various files (on Irix) that are needed to start/\n\nstop a SQL Server. The information can easily be extended to any UNIX platform.\n\nThe idea is to allow as much flexibility to the two classes of administrators\n\nwho manage the machine:\n\n*The System Administrator\n\n*The Database Administrator\n\nAny errors introduced by the DBA will not interfere with the System\n\nAdministrator's job.\n\nWith that in mind we have the system startup/shutdown file /etc/init.d/sybase\n\ninvoking a script defined by the DBA: /usr/sybase/sys.config/\n\n{start,stop}.sybase\n\n/etc/init.d/sybase\n\nOn some operating systems this file must be linked to a corresponding entry in\n\n/etc/rc.0 and /etc/rc.2 -- see rc0(1M) and rc2(1M)\n\n#!/bin/sh\n\n# last modified: 10/17/95, sr.\n\n#\n\n# Make symbolic links so this file will be called during system stop/start.\n\n# ln -s /etc/init.d/sybase /etc/rc0.d/K19sybase\n\n# ln -s /etc/init.d/sybase /etc/rc2.d/S99sybase\n\n# chkconfig -f sybase on\n\n# Sybase System-wide configuration files\n\nCONFIG=/usr/sybase/sys.config\n\nif $IS_ON verbose ; then # For a verbose startup and shutdown\n\nECHO=echo\n\nVERBOSE=-v\n\nelse # For a quiet startup and shutdown\n\nECHO=:\n\nVERBOSE=\n\nfi\n\ncase \"$1\" in\n\n'start')\n\nif $IS_ON sybase; then\n\nif [ -x $CONFIG/start.sybase ]; then\n\n$ECHO \"starting Sybase servers\"\n\n/bin/su - sybase -c \"$CONFIG/start.sybase $VERBOSE &\"\n\nelse\n\n<error condition>\n\nfi\n\nfi\n\n;;\n\n'stop')\n\nif $IS_ON sybase; then\n\nif [ -x $CONFIG/stop.sybase ]; then\n\n$ECHO \"stopping Sybase servers\"\n\n/bin/su - sybase -c \"$CONFIG/stop.sybase $VERBOSE &\"\n\nelse\n\n<error condition>\n\nfi\n\nfi\n\n;;\n\n*)\n\necho \"usage: $0 {start|stop}\"\n\n;;\n\nesac\n\n/usr/sybase/sys.config/{start,stop}.sybase\n\nstart.sybase\n\n#!/bin/sh -a\n\n#\n\n# Script to start sybase\n\n#\n\n# NOTE: different versions of sybase exist under /usr/sybase/{version}\n\n#\n\n# Determine if we need to spew our output\n\nif [ \"$1\" != \"spew\" ] ; then\n\nOUTPUT=\">/dev/null 2>&1\"\n\nelse\n\nOUTPUT=\"\"\n\nfi\n\n# 10.0.2 servers\n\nHOME=/usr/sybase/10.0.2\n\ncd $HOME\n\n# Start the backup server\n\neval install/startserver -f install/RUN_BU_KEPLER_1002_52_01 $OUTPUT\n\n# Start the dataservers\n\n# Wait two seconds between starts to minimize trauma to CPU server\n\neval install/startserver -f install/RUN_FAC_WWOPR $OUTPUT\n\nsleep 2\n\neval install/startserver -f install/RUN_MAG_LOAD $OUTPUT\n\nexit 0\n\nstop.sybase\n\n#!/bin/sh\n\n#\n\n# Script to stop sybase\n\n#\n\n# Determine if we need to spew our output\n\nif [ -z \"$1\" ] ; then\n\nOUTPUT=\">/dev/null 2>&1\"\n\nelse\n\nOUTPUT=\"-v\"\n\nfi\n\neval killall -15 $OUTPUT dataserver backupserver sybmultbuf\n\nsleep 2\n\n# if they didn't die, kill 'em now...\n\neval killall -9 $OUTPUT dataserver backupserver sybmultbuf\n\nexit 0\n\nIf your platform doesn't support killall, it can easily be simulated as\n\nfollows:\n\n#!/bin/sh\n\n#\n\n# Simple killall simulation...\n\n# $1 = signal\n\n# $2 = process_name\n\n#\n\n#\n\n# no error checking but assume first parameter is signal...\n\n# what ya want for free? :-)\n\n#\n\nkill -$1 `ps -ef | fgrep $2 | fgrep -v fgrep | awk '{ print $1 }'`\n\nBack to top\n\n-------------------------------------------------------------------------------\n\n1.1.3: How do I move tempdb off of the Master Device?\n\n-------------------------------------------------------------------------------\n\nNote: I received a message from Sybase TS recommending that the FAQ no\n\nlonger advocate the physical removal of entries from the sysusages/\n\nsysdatabases tables. It makes recovery extremely painful.\n\nAfter reviewing their write-up I agree.\n\nA quick alternative - Sybase TS Preferred Method\n\nThis is the Sybase TS method of removing most activity from the master device:\n\n1. Alter tempdb on another device:\n\n1> alter database tempdb on ...\n\n2> go\n\n2. Use the tempdb:\n\n1> use tempdb\n\n2> go\n\n3. Drop the segments:\n\n1> sp_dropsegment \"default\", tempdb, master\n\n2> go\n\n1> sp_dropsegment \"logsegment\", tempdb, master\n\n2> go\n\n1> sp_dropsegment \"system\", tempdb, master\n\n2> go\n\nNote that there is still some activity on the master device. On a three\n\nconnection test that I ran:\n\nwhile ( 1 = 1 )\n\nbegin\n\ncreate table #x (col_a int)\n\ndrop table #x\n\nend\n\nthere was one write per second. Not bad.\n\nYet another alternative\n\nThe idea of this handy script is to simply fill the first 2MB of tempdb thus\n\neffectively blocking anyone else from using it. The slight gotcha with this\n\nscript, since we're using model, is that all subsequent database creates will\n\nalso have tempdb_filler installed. This is easily remedied by dropping the\n\ntable after creating a new database.\n\nThis script works because tempdb is rebuilt every time the SQL Server is\n\nrebooted. Very nice trick!\n\n/* this isql script creates a table in the model database. */\n\n/* Since tempdb is created from the model database when the */\n\n/* server is started, this effectively moves the active */\n\n/* portion of tempdb off of the master device. */\n\nuse model\n\ngo\n\n/* note: 2k row size */\n\ncreate table tempdb_filler(\n\na char(255) not null,\n\nb char(255) not null,\n\nc char(255) not null,\n\nd char(255) not null,\n\ne char(255) not null\n\n)\n\ngo\n\n/* insert 1024 rows */\n\ndeclare @i int\n\nselect @i = 1\n\nwhile (@i <= 1024)\n\nbegin\n\ninsert into tempdb_filler values('a','b','c','d','e')\n\nif (@i % 100 = 0) /* dump the transaction every 100 rows */\n\ndump tran model with truncate_only\n\nselect @i=@i+1\n\nend\n\ngo\n\nBack to top\n\n-------------------------------------------------------------------------------\n\n1.1.4: How do I correct timeslice -201\n\n-------------------------------------------------------------------------------\n\n(Note, this procedure is only really necessary with pre-11.x systems. In\n\nsystem 11 systems, these parameters are tunable using sp_configure.)\n\nWhy Increase It?\n\nBasically, it will allow a task to be scheduled onto the CPU for a longer time.\n\nEach task o"
    }
}
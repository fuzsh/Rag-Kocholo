{
    "id": "correct_foundationPlace_00075_3",
    "rank": 31,
    "data": {
        "url": "https://www.hireitpeople.com/resume-database/73-datawarehousing-etl-informatica-resumes/327663-etl-lead-onsite-resume-berkeley-heights-nj",
        "read_more_link": "",
        "language": "en",
        "title": "ETL Lead (Onsite) Resume Berkeley Heights, NJ",
        "top_image": "https://www.hireitpeople.com//images/cards/resumes/327663.jpg",
        "meta_img": "https://www.hireitpeople.com//images/cards/resumes/327663.jpg",
        "images": [
            "https://www.hireitpeople.com/images/logo.png",
            "https://www.hireitpeople.com/resume-database/73-datawarehousing-etl-informatica-resumes/assets/img/privacy-policy-icon.png",
            "https://www.hireitpeople.com/images/logo-white.png",
            "https://px.ads.linkedin.com/collect/?pid=4795882&fmt=gif"
        ],
        "movies": [],
        "keywords": [],
        "meta_keywords": [
            "Cobol",
            "JCL",
            "PL/SQL",
            "MQ Series",
            "PEGA",
            "UNIX",
            "ORACLE",
            "DB2",
            "Sybase 12",
            "SQL Server",
            "IBM Infosphere DataStage 7.5",
            "Infosphere DataStage 8.1IBM Quality Stage",
            "IBM Info Analyzer",
            "CIS(Cisco Information Server)",
            "Qlikview",
            "COGNOS",
            "Ab-Initio(High level concepts & working knowledge)",
            "Big Data Concepts.",
            "Stone soup architecture",
            "Insurance Application Architecture(IIA)",
            "Insurance Information Warehouse (IIW)",
            "IBM OS 390",
            "Windows",
            "Sun Solaris",
            "Insurance"
        ],
        "tags": null,
        "authors": [
            "Hire IT People"
        ],
        "publish_date": null,
        "summary": "",
        "meta_description": "9years of experience in IT industry with proficiency in requirements gathering, designing, development, testing, production implementation and production support.Over 4 years of experience in leading Multiple ETL projectswith team of3 - 6members.Worked extensively in anOnsite and offshore modeland lead the team.Good experience in the design, development and implementation of Informatica based projects.Good hands on experience inPerformance tuningof Data Stage mappings andquery optimization.Good",
        "meta_lang": "en",
        "meta_favicon": "images/favicon.ico",
        "meta_site_name": "Hire IT People",
        "canonical_link": "https://www.hireitpeople.com/resume-database/73-datawarehousing-etl-informatica-resumes/327663-etl-lead-onsite-resume-berkeley-heights-nj",
        "text": "EXPERIENCE SUMMARY:\n\n9years of experience in IT industry with proficiency in requirements gathering, designing, development, testing, production implementation and production support.\n\nOver 4 years of experience in leading Multiple ETL projectswith team of3 - 6members.\n\nWorked extensively in anOnsite and offshore modeland lead the team.\n\nGood experience in the design, development and implementation of Informatica based projects.\n\nGood hands on experience inPerformance tuningof Data Stage mappings andquery optimization.\n\nGood knowledge on various data modelling techniques including dimensional data model using star and snowflake schema.\n\nGood knowledge withInfosphere DataStage Quality Stage.\n\nExcellent hands on experience in writing complexSQL queriesin Oracle database.\n\nGood hands on experience inUNIXshell scripting.\n\nVast experience inschedulingETL jobs through Autosys.\n\nExcellent knowledge on Incident Management & Change Management tools.\n\nExcellent domain knowledge onInsurance.\n\nExpertise on project management activities including planning, estimation, resource management.\n\nMy main area of experience has beenin the fields ofData Warehousing, Data Integration, Data Virtualization, Data Visualization and Data Migrationand Developing Client server application using\n\nIBM Infosphere DataStage (9.1, 8.7, 8.5/8.1.1/7.5.2/7.5.1 ), Oracle, PL/SQL, DB2 UDB, Sybase, SQL\n\nServer, Netezza, CIS, Qlikview, IBM Quality Stage, Info Analyzer, Fast Track and Shell Scripts,Informatica MDM.\n\nHave a strong Industry experience in Insurance. My current role is an ETL onshore lead in a development project for Confidential .\n\nIBM Infosphere DataStage & Quality Stage (9.1, 8.7, 8.1, 7.5)\n\nInformatica MDM v10.0\n\nComposite (CIS) v7.0\n\nQlikview V11.0\n\nBasics in Big Data (Hive, Hbase etc)\n\nAb-Initio\n\nIBM WebSphere\n\nORACLE 10G\n\nNetezza\n\nDB2\n\nSybase, SQL Server\n\nMS SQL Server\n\nPEGA (Basics)\n\nExperience in all the phases of the Data warehouse life cycle involvingData analysis, design, and development and testing using ETL, Data Modeling, Online Analytical Processing & reportingtools.\n\n8 + years of ETLand data integration experience in developing ETL jobs usingDataStage 8.x/7.x (Designer, Director, Administrator)and developing Parallel jobs using various DataStage stages such asLookup, Join, Merge, Transformer, Aggregator, Funnel,\n\nRemove Duplicates, Surrogate key Generator, Change Capture, DB2, Sequential file.\n\nGood understanding ofRDBMSlikeOracle,Sybase, DB2andSQL Serverand extensively worked on data Integration using DataStage for the Extraction transformation and loading of data from various database source systems.\n\nExperience in data warehousing concepts, ETL programming usingIBM DataStage Parallel, cleansing, transforming, debugging/testing, and data loading across source systems.\n\nHas workedextensively with SQL&PL/SQL,including performing query optimizations.\n\nToStandardizeand cleaning the data usingQuality Stage.\n\nExperience in developing complexJobs, Sequencesandre-usablejobs/containers.\n\nExpert knowledge ofperformance tuningsource, mapping, target and sessions.\n\nKnowledge of fulllife cycle development in Data Warehousing.\n\nKnowledge on Administrative activities like maintaining jobs logs, setting environment variables, assigning Infosphere DataStageuser rolesusingweb consoleand administrator, creating the projects, configuring project, handling trace files andconfiguration file, unlockthe ETL jobs, etc.\n\nExperience working inInformatica MDM Huband completed a Master Data Management project in Informatica MDM software.\n\nGood Knowledge in Composite(CIS) - Cisco Information Server - Data Virtualization Platform\n\nWorking Knowledge in Qlikview (v10.0) - Data Visualization Platform.\n\nBasic Knowledge in Big data Concepts like (Hbase, Hive, Node configuration etc)\n\nExtensive knowledge in data warehousing concepts likeOLTP, OLAP, ODS,Star Schema,\n\nSnow Flake Schema, Dimension and Fact tables.\n\nExperience working with MS Office Suite (Word, Excel, Access and PowerPoint skills).\n\nHands on experience in analyzing and troubleshooting issues atfunctional and technical levels.\n\nGood functional knowledge to interface withend-usersand to identify the information needs and business requirements.\n\nPossess good communication and interpersonal skills and ability to work in a team as well as individually.\n\nInnovative in approach, enjoys learning new methods and ideas and putting them into daily practice.\n\nTECHNOLOGY:\n\nSoftware: Cobol, JCL, PL/SQL, MQ Series, PEGA,UNIX\n\nDatabase: ORACLE,DB2,Sybase 12, SQL Server\n\nETL & BI: IBM Infosphere DataStage 7.5, Infosphere DataStage 8.1IBM Quality Stage, IBM Info Analyzer, CIS(Cisco Information Server), Qlikview, COGNOS,Ab-Initio(High level concepts & working knowledge), Big Data Concepts.\n\nETL Architecture: Stone soup architecture, Insurance Application Architecture(IIA), Insurance Information Warehouse (IIW)\n\nHardware: IBM OS 390,Windows, Sun Solaris\n\nDomain Experience: Insurance\n\nPROFESSIONAL EXPERIENCE:\n\nConfidential, Berkeley Heights, NJ\n\nETL Lead (Onsite)\n\nResponsibilities:\n\nAnalyse multiple source and provide Input mapping for the source system with respect to the target.\n\nAnalyse & come up with a design to accommodate multi character set languages into Worker DataStore.\n\nDesign and prepare specification document for build and test phase.\n\nContinuous tracking of business requirements from users and documenting the same.\n\nAssigning individual modules for ETL development to offshore team and tracking it to closure.\n\nDesign and tracking the changes of Components and check for Error free code build across various instances throughout the project.\n\nQC,UAT and Implementation Support\n\nReconciliation report generation at the end of every month processing to tally out charges calculated.\n\nBuilding UNIX Scripts to perform various ETL tasks like Archiving, Multiple File processing, FTP, SFTP, Triggering automated mails based on various conditions.\n\nCode Review and testing of the all the Jobs developed as part of Confidential .\n\nDebugging the Infosphere DataStage Components using Infosphere DataStage director, whenever problem occurs\n\nDefect tracking and Resolutions\n\nOnsite and offshoreco-ordination.\n\nEnvironment: DB2, Infosphere DataStage 8.7, 9.1, Informatica MDM.\n\nConfidential, Berkeley Heights, NJ\n\nCIS (Composite) and Qlikview developer\n\nResponsibilities:\n\nAnalyse the existing time taken forWeb-servicecalls from different countries (Japan, Malaysia, UK) etc.\n\nDesign and build Application Specification document explaining the problem statement and the intended solution with Composite - CIS.\n\nBuild various Qlikview dashboards connecting CIS to extract data & generate report to attract business.\n\nDesign and tracking the changes of Components and check for Error free code build across various instances throughout the project.\n\nAnalyse the mapping document provided for foreign country processing.\n\nQC, UAT and Implementation Support\n\nReconciliation report generation after each cache refresh.\n\nCIS Script scheduling using Autosys.\n\nCode Review and testing of the all the Jobs developed for ODS migration.\n\nDebugging the CIS and Qlikview components, whenever problem occurs\n\nOnsite and offshoreco-ordination.\n\nEnvironment: Composite v 7.0(Cisco Information Server), Qlikview v11.0, Big data (basics)\n\nConfidential, New York City, NY\n\nETL Onshore Lead\n\nResponsibilities:\n\nAnalysis of various source system and design single extraction layout forIn-boundprocessing.\n\nInteract with business directly to validate the data loaded in IBM- RDM platform.\n\nBuild Autosys scripts to schedule the Jobs in production.\n\nPreparing Design specification documents for Cost Center - Confidential .\n\nContinuous tracking of business requirements from users and documenting the same.\n\nAssigning individual modules for ETL development to offshore team and tracking it to closure.\n\nDesign and tracking the changes of Components and check for Error free code build across various instances throughout the project.\n\nAnalyse the mapping document provided by business to integrate various source systems.\n\nQC, UAT and Implementation Support\n\nReconciliation report generation after each batch cycle.\n\nCode Review and testing of the all the Jobs developed as part of Confidential .\n\nDebugging the Infosphere DataStage Components using Infosphere DataStage director, whenever problem occurs\n\nDefect tracking and Resolutions\n\nOnsite and offshoreco-ordination.\n\nEnvironment: DB2, Infosphere DataStage 8.7, IDM - RDM, COGNOS.\n\nConfidential, Berkeley Heights, NJ\n\nETL Senior Developer\n\nResponsibilities:\n\nAnalysis of the existing System to enhance its capabilities to support multiple countries\n\nPreparing Design specification documents for financial processing and enhancements MONET and Guaranty Fund.\n\nContinuous tracking of business requirements from users and documenting the same.\n\nAssigning individual modules for ETL development to offshore team and tracking it to closure.\n\nDesign and tracking the changes of Components and check for Error free code build across various instances throughout the project.\n\nAnalyse the mapping document provided for foreign country processing and the guaranty fund feed processing.\n\nQC, UAT and Implementation Support\n\nReconciliation report generation at the end of the month processing to tally out the charges calculated.\n\nBuilding UNIX Scripts to perform various ETL tasks like Archiving, Multiple File processing, FTP, SFTP, Triggering automated mails based on various conditions, Performing file validations based data present in oracle table.\n\nCode Review and testing of the all the Jobs developed as part of Financials.\n\nDebugging the Infosphere DataStage Components using Infosphere DataStage director, whenever problem occurs\n\nDefect tracking and Resolutions\n\nOnsite and offshoreco-ordination.\n\nEnvironment: ORACLE, Infosphere DataStage 8.1, COGNOS.\n\nConfidential, Jersey city, NJ\n\nETL Lead\n\nResponsibilities:\n\nAnalysis of the existing System to identify the possibilities of performance tuning.\n\nPreparing Design specification documents for financial processing and enhancements of the existing ODS and RDW ETL jobs.\n\nContinuous tracking of the XSD structure change in Front End and incorporating the same in Infosphere DataStage Jobs.\n\nAssigning individual modules for ETL development to team and tracking it to closure.\n\nDesign and tracking the changes of Components and check for Error free code migration across various instances throughout the project.\n\nAnalyze the mapping document provided for Financials and converting the same into Infosphere DataStage Jobs.\n\nQC, UAT and Implementation Support\n\nMigration of existing claims from OCFE front end to accommodate the additional columns that are mapped to ODS and RDW.\n\nReconciliation report generation for ODS and RDW for financials\n\nCode Review and testing of the all the Jobs developed as part of Financials.\n\nDebugging the Infosphere DataStage Components using Infosphere DataStage director, whenever problem occurs\n\nDefect tracking and Resolutions\n\nOnsite and offshoreco-ordination.\n\nEnvironment: DB2 (Mainframe), SQL, Infosphere DataStage 8.1, COGNOS.\n\nConfidential, Jersey city, NJ\n\nInfosphere DataStage Developer\n\nResponsibilities:\n\nData analysis of the source Coventry, ASD and Employee/Location feed.\n\nPreparation of mapping documents for various modules from source (Coventry, ASD and Employee/Location) feed to target FNI IAA tables.\n\nDesign of XSD structure for source feed files.\n\nDesign and tracking the changes of Components and check for Error free code migration across various instances throughout the project.\n\nDesign of specification and design document for individual modules.\n\nStage In and Stage Out data model design for FNI data migration along with the ETL Job development to populate the staging tables.\n\nXML generation via DataStage with the migrated data for employee and location feed.\n\nInvolved in creation of Infosphere DataStage Jobs to efficiently reuse the existing Front End SP’s to load migration data.\n\nReconciliation and audit reports creation for the migrated claims between source and target.\n\nCode Review and testing of individual data migration module.\n\nDebugging the Infosphere DataStage Components using Infosphere DataStage director, whenever problem occurs\n\nManaging the Unit and System testing activities.\n\nDelivery of System test plan\n\nDefect tracking and Resolutions.\n\nQC,UAT and Implementation Support\n\nEnvironment: Oracle, DB2 (Mainframe), SQL, Stored Procedures, Infosphere DataStage 8.1\n\nConfidential, Jersey city, NJ\n\nInfosphere DataStage Developer\n\nResponsibilities:\n\nData analysis of the source GOALD and FLIC DB.\n\nContinuous tracking of the XSD structure change in Front End and incorporating the same in Infosphere DataStage Jobs.\n\nDevelopment of individual modules such as ID Population and mapping layer in Claims data Migration Construction.\n\nDesign and tracking the changes of Components and check for Error free code migration across various instances throughout the project.\n\nAnalyze the mapping document provided for migration and converting the same into Infosphere DataStage Jobs.\n\nStage In and Stage Out data model design for CDM migration along with the ETL Job development to populate the staging tables.\n\nInvolved in creating Sybase SPs creating for extraction, and testing the same.\n\nQC, UAT and Implementation Support\n\nReconciliation and audit reports creation for the migrated claims between source GOALD.FLIC and target one Claim.\n\nGenerate Claims XML with migrated data via Infosphere DataStage and test for its schematic perfection by consuming the claims via one claim Front End.\n\nCode Review and testing of the contacts data migration module.\n\nDebugging the Infosphere DataStage Components using Infosphere DataStage director, whenever problem occurs\n\nDefect tracking and Resolutions\n\nEnvironment: Sybase, SQL Server, DB2 (Mainframe), SQL, Stored Procedures, Infosphere DataStage 8.1\n\nConfidential, Jersey city, NJ\n\nInfosphere DataStage Developer\n\nResponsibilities:\n\nSource GOALD and FLIC Data Analysis.\n\nGenerating profiling reports for the source GOALD and FLIC Database to identify the quality of the data to be migrated.\n\nOwner for the Development and testing of Policy Instruction module.\n\nApproach and Mapping Document preparation.\n\nPreparation of data model design forStage-InandStage-Out tables.\n\nPreparation of test scripts for the migrated data.\n\nCoordinating with the Front end and Interface teams to check for the integrity of the Migrated Master data in one claim Front End. Ensured complete support to the testing team.\n\nQC, UAT and Implementation Support\n\nGeneration of Reconciliation and audit reports for the migrated data.\n\nDefect tracking and Resolutions\n\nEnvironment: Sybase, SQL Server, DB2 (Mainframe), SQL, Stored Procedures, Infosphere DataStage 8.1\n\nConfidential\n\nDeveloper\n\nResponsibilities:\n\nSource DB Analysis.\n\nApproach and Mapping Document preparation\n\nConstruction, Testing and delivery"
    }
}
{
    "id": "correct_foundationPlace_00075_2",
    "rank": 35,
    "data": {
        "url": "https://superset.apache.org/docs/configuration/databases/",
        "read_more_link": "",
        "language": "en",
        "title": "Connecting to Databases",
        "top_image": "https://superset.apache.org/img/favicon.ico",
        "meta_img": "https://superset.apache.org/img/favicon.ico",
        "images": [
            "https://superset.apache.org/img/superset-logo-horiz.svg",
            "https://superset.apache.org/img/superset-logo-horiz-dark.svg",
            "https://superset.apache.org/img/root-cert-example.png",
            "https://user-images.githubusercontent.com/52086618/138352958-a18ef9cb-8880-4ef1-88c1-452a9f1b8105.gif",
            "https://user-images.githubusercontent.com/52086618/138354340-df57f477-d3e5-42d4-b032-d901c69d2213.gif",
            "https://user-images.githubusercontent.com/27827808/125499607-94e300aa-1c0f-4c60-b199-3f9de41060a3.gif",
            "https://superset.apache.org/img/applitools.png",
            "https://superset.apache.org/img/community/line.png",
            "https://static.scarf.sh/a.png?x-pxid=39ae6855-95fc-4566-86e5-360d542b0a68"
        ],
        "movies": [],
        "keywords": [],
        "meta_keywords": [
            ""
        ],
        "tags": null,
        "authors": [],
        "publish_date": null,
        "summary": "",
        "meta_description": "Superset does not ship bundled with connectivity to databases. The main step in connecting",
        "meta_lang": "en",
        "meta_favicon": "/img/favicon.ico",
        "meta_site_name": "",
        "canonical_link": "https://superset.apache.org/docs/configuration/databases",
        "text": "Superset does not ship bundled with connectivity to databases. The main step in connecting Superset to a database is to install the proper database driver(s) in your environment.\n\nThis documentation tries to keep pointer to the different drivers for commonly used database engine.\n\nSuperset requires a Python DB-API database driver and a SQLAlchemy dialect to be installed for each database engine you want to connect to.\n\nYou can read more here about how to install new database drivers into your Superset configuration.\n\nSome of the recommended packages are shown below. Please refer to pyproject.toml for the versions that are compatible with Superset.\n\nDatabase\n\nPyPI packageConnection StringAWS Athenapip install pyathena[pandas] , pip install PyAthenaJDBCawsathena+rest://{access_key_id}:{access_key}@athena.{region}.amazonaws.com/{schema}?s3_staging_dir={s3_staging_dir}&... AWS DynamoDBpip install pydynamodbdynamodb://{access_key_id}:{secret_access_key}@dynamodb.{region_name}.amazonaws.com?connector=supersetAWS Redshiftpip install sqlalchemy-redshift redshift+psycopg2://<userName>:<DBPassword>@<AWS End Point>:5439/<Database Name>Apache Dorispip install pydorisdoris://<User>:<Password>@<Host>:<Port>/<Catalog>.<Database>Apache Drillpip install sqlalchemy-drilldrill+sadrill:// For JDBC drill+jdbc://Apache Druidpip install pydruiddruid://<User>:<password>@<Host>:<Port-default-9088>/druid/v2/sqlApache Hivepip install pyhivehive://hive@{hostname}:{port}/{database}Apache Impalapip install impylaimpala://{hostname}:{port}/{database}Apache Kylinpip install kylinpykylin://<username>:<password>@<hostname>:<port>/<project>?<param1>=<value1>&<param2>=<value2>Apache Pinotpip install pinotdbpinot://BROKER:5436/query?server=http://CONTROLLER:5983/Apache Solrpip install sqlalchemy-solrsolr://{username}:{password}@{hostname}:{port}/{server_path}/{collection}Apache Spark SQLpip install pyhivehive://hive@{hostname}:{port}/{database}Ascend.iopip install impylaascend://{username}:{password}@{hostname}:{port}/{database}?auth_mechanism=PLAIN;use_ssl=trueAzure MS SQLpip install pymssqlmssql+pymssql://UserName@presetSQL:TestPassword@presetSQL.database.windows.net:1433/TestSchemaClickHousepip install clickhouse-connectclickhousedb://{username}:{password}@{hostname}:{port}/{database}CockroachDBpip install cockroachdbcockroachdb://root@{hostname}:{port}/{database}?sslmode=disableCouchbaseDBpip install couchbase-sqlalchemycouchbasedb://{username}:{password}@{hostname}:{port}?truststorepath={ssl certificate path}Dremiopip install sqlalchemy_dremiodremio://user:pwd@host:31010/Elasticsearchpip install elasticsearch-dbapielasticsearch+http://{user}:{password}@{host}:9200/Exasolpip install sqlalchemy-exasolexa+pyodbc://{username}:{password}@{hostname}:{port}/my_schema?CONNECTIONLCALL=en_US.UTF-8&driver=EXAODBCGoogle BigQuerypip install sqlalchemy-bigquerybigquery://{project_id}Google Sheetspip install shillelagh[gsheetsapi]gsheets://Fireboltpip install firebolt-sqlalchemyfirebolt://{client_id}:{client_secret}@{database}/{engine_name}?account_name={name}Hologrespip install psycopg2postgresql+psycopg2://<UserName>:<DBPassword>@<Database Host>/<Database Name>IBM Db2pip install ibm_db_sadb2+ibm_db://IBM Netezza Performance Serverpip install nzalchemynetezza+nzpy://<UserName>:<DBPassword>@<Database Host>/<Database Name>MySQLpip install mysqlclientmysql://<UserName>:<DBPassword>@<Database Host>/<Database Name>OceanBasepip install oceanbase_pyoceanbase://<UserName>:<DBPassword>@<Database Host>/<Database Name>Oraclepip install cx_Oracleoracle://PostgreSQLpip install psycopg2postgresql://<UserName>:<DBPassword>@<Database Host>/<Database Name>Prestopip install pyhivepresto://Rocksetpip install rockset-sqlalchemyrockset://<api_key>:@<api_server>SAP Hanapip install hdbcli sqlalchemy-hana or pip install apache-superset[hana]hana://{username}:{password}@{host}:{port}StarRockspip install starrocksstarrocks://<User>:<Password>@<Host>:<Port>/<Catalog>.<Database>Snowflakepip install snowflake-sqlalchemysnowflake://{user}:{password}@{account}.{region}/{database}?role={role}&warehouse={warehouse}SQLiteNo additional library neededsqlite://path/to/file.db?check_same_thread=falseSQL Serverpip install pymssqlmssql+pymssql://Teradatapip install teradatasqlalchemyteradatasql://{user}:{password}@{host}TimescaleDBpip install psycopg2postgresql://<UserName>:<DBPassword>@<Database Host>:<Port>/<Database Name>Trinopip install trinotrino://{username}:{password}@{hostname}:{port}/{catalog}Verticapip install sqlalchemy-vertica-pythonvertica+vertica_python://<UserName>:<DBPassword>@<Database Host>/<Database Name>YugabyteDBpip install psycopg2postgresql://<UserName>:<DBPassword>@<Database Host>/<Database Name>\n\nNote that many other databases are supported, the main criteria being the existence of a functional SQLAlchemy dialect and Python driver. Searching for the keyword \"sqlalchemy + (database name)\" should help get you to the right place.\n\nIf your database or data engine isn't on the list but a SQL interface exists, please file an issue on the Superset GitHub repo, so we can work on documenting and supporting it.\n\nIf you'd like to build a database connector for Superset integration, read the following tutorial.\n\nSuperset requires a Python database driver to be installed for each additional type of database you want to connect to.\n\nIn this example, we'll walk through how to install the MySQL connector library. The connector library installation process is the same for all additional libraries.\n\nConsult the list of database drivers and find the PyPI package needed to connect to your database. In this example, we're connecting to a MySQL database, so we'll need the mysqlclient connector library.\n\nWe need to get the mysqlclient library installed into the Superset docker container (it doesn't matter if it's installed on the host machine). We could enter the running container with docker exec -it <container_name> bash and run pip install mysqlclient there, but that wouldn't persist permanently.\n\nTo address this, the Superset docker compose deployment uses the convention of a requirements-local.txt file. All packages listed in this file will be installed into the container from PyPI at runtime. This file will be ignored by Git for the purposes of local development.\n\nCreate the file requirements-local.txt in a subdirectory called docker that exists in the directory with your docker-compose.yml or docker-compose-non-dev.yml file.\n\nAdd the driver identified in step above. You can use a text editor or do it from the command line like:\n\nIf you are running a stock (non-customized) Superset image, you are done. Launch Superset with docker compose -f docker-compose-non-dev.yml up and the driver should be present.\n\nYou can check its presence by entering the running container with docker exec -it <container_name> bash and running pip freeze. The PyPI package should be present in the printed list.\n\nIf you're running a customized docker image, rebuild your local image with the new driver baked in:\n\nAfter the rebuild of the Docker images is complete, relaunch Superset by running docker compose up.\n\nNow that you've got a MySQL driver installed in your container, you should be able to connect to your database via the Superset web UI.\n\nAs an admin user, go to Settings -> Data: Database Connections and click the +DATABASE button. From there, follow the steps on the Using Database Connection UI page.\n\nConsult the page for your specific database type in the Superset documentation to determine the connection string and any other parameters you need to input. For instance, on the MySQL page, we see that the connection string to a local MySQL database differs depending on whether the setup is running on Linux or Mac.\n\nClick the âTest Connectionâ button, which should result in a popup message saying, \"Connection looks good!\".\n\nIf the test fails, review your docker logs for error messages. Superset uses SQLAlchemy to connect to databases; to troubleshoot the connection string for your database, you might start Python in the Superset application container or host environment and try to connect directly to the desired database and fetch data. This eliminates Superset for the purposes of isolating the problem.\n\nRepeat this process for each type of database you want Superset to connect to.\n\nThe recommended connector library to Ascend.io is impyla.\n\nThe expected connection string is formatted as follows:\n\nThe sqlalchemy-doris library is the recommended way to connect to Apache Doris through SQLAlchemy.\n\nYou'll need the following setting values to form the connection string:\n\nUser: User Name\n\nPassword: Password\n\nHost: Doris FE Host\n\nPort: Doris FE port\n\nCatalog: Catalog Name\n\nDatabase: Database Name\n\nHere's what the connection string looks like:\n\nPyAthenaJDBC is a Python DB 2.0 compliant wrapper for the Amazon Athena JDBC driver.\n\nThe connection string for Amazon Athena is as follows:\n\nNote that you'll need to escape & encode when forming the connection string like so:\n\nYou can also use the PyAthena library (no Java required) with the following connection string:\n\nThe PyAthena library also allows to assume a specific IAM role which you can define by adding following parameters in Superset's Athena database connection UI under ADVANCED --> Other --> ENGINE PARAMETERS.\n\nPyDynamoDB is a Python DB API 2.0 (PEP 249) client for Amazon DynamoDB.\n\nThe connection string for Amazon DynamoDB is as follows:\n\nTo get more documentation, please visit: PyDynamoDB WIKI.\n\nThe sqlalchemy-redshift library is the recommended way to connect to Redshift through SQLAlchemy.\n\nThis dialect requires either redshift_connector or psycopg2 to work properly.\n\nYou'll need to set the following values to form the connection string:\n\nUser Name: userName\n\nPassword: DBPassword\n\nDatabase Host: AWS Endpoint\n\nDatabase Name: Database Name\n\nPort: default 5439\n\nHere's what the SQLALCHEMY URI looks like:\n\nHere's what the SQLALCHEMY URI looks like:\n\nAmazon redshift cluster also supports generating temporary IAM-based database user credentials.\n\nYour superset app's IAM role should have permissions to call the redshift:GetClusterCredentials operation.\n\nYou have to define the following arguments in Superset's redshift database connection UI under ADVANCED --> Others --> ENGINE PARAMETERS.\n\nand SQLALCHEMY URI should be set to redshift+redshift_connector://\n\nRedshift serverless supports connection using IAM roles.\n\nYour superset app's IAM role should have redshift-serverless:GetCredentials and redshift-serverless:GetWorkgroup permissions on Redshift serverless workgroup.\n\nYou have to define the following arguments in Superset's redshift database connection UI under ADVANCED --> Others --> ENGINE PARAMETERS.\n\nTo use ClickHouse with Superset, you will need to install the clickhouse-connect Python library:\n\nIf running Superset using Docker Compose, add the following to your ./docker/requirements-local.txt file:\n\nThe recommended connector library for ClickHouse is clickhouse-connect.\n\nThe expected connection string is formatted as follows:\n\nHere's a concrete example of a real connection string:\n\nIf you're using Clickhouse locally on your computer, you can get away with using a http protocol URL that uses the default user without a password (and doesn't encrypt the connection):\n\nThe recommended connector library for CockroachDB is sqlalchemy-cockroachdb.\n\nThe expected connection string is formatted as follows:\n\nThe recommended connector library for CouchbaseDB is couchbase-sqlalchemy.\n\nThe expected connection string is formatted as follows:\n\nThe recommended connector library for CrateDB is crate. You need to install the extras as well for this library. We recommend adding something like the following text to your requirements file:\n\nThe expected connection string is formatted as follows:\n\nThe recommended connector library for Databend is databend-sqlalchemy. Superset has been tested on databend-sqlalchemy>=0.2.3.\n\nThe recommended connection string is:\n\nHere's a connection string example of Superset connecting to a Databend database:\n\nDatabricks now offer a native DB API 2.0 driver, databricks-sql-connector, that can be used with the sqlalchemy-databricks dialect. You can install both with:\n\nTo use the Hive connector you need the following information from your cluster:\n\nServer hostname\n\nPort\n\nHTTP path\n\nThese can be found under \"Configuration\" -> \"Advanced Options\" -> \"JDBC/ODBC\".\n\nYou also need an access token from \"Settings\" -> \"User Settings\" -> \"Access Tokens\".\n\nOnce you have all this information, add a database of type \"Databricks Native Connector\" and use the following SQLAlchemy URI:\n\nYou also need to add the following configuration to \"Other\" -> \"Engine Parameters\", with your HTTP path:\n\nOriginally Superset used databricks-dbapi to connect to Databricks. You might want to try it if you're having problems with the official Databricks connector:\n\nThere are two ways to connect to Databricks when using databricks-dbapi: using a Hive connector or an ODBC connector. Both ways work similarly, but only ODBC can be used to connect to SQL endpoints.\n\nTo connect to a Hive cluster add a database of type \"Databricks Interactive Cluster\" in Superset, and use the following SQLAlchemy URI:\n\nYou also need to add the following configuration to \"Other\" -> \"Engine Parameters\", with your HTTP path:\n\nFor ODBC you first need to install the ODBC drivers for your platform.\n\nFor a regular connection use this as the SQLAlchemy URI after selecting either \"Databricks Interactive Cluster\" or \"Databricks SQL Endpoint\" for the database, depending on your use case:\n\nAnd for the connection arguments:\n\nThe driver path should be:\n\n/Library/simba/spark/lib/libsparkodbc_sbu.dylib (Mac OS)\n\n/opt/simba/spark/lib/64/libsparkodbc_sb64.so (Linux)\n\nFor a connection to a SQL endpoint you need to use the HTTP path from the endpoint:\n\nThe recommended connector library for Dremio is sqlalchemy_dremio.\n\nThe expected connection string for ODBC (Default port is 31010) is formatted as follows:\n\nThe expected connection string for Arrow Flight (Dremio 4.9.1+. Default port is 32010) is formatted as follows:\n\nThis blog post by Dremio has some additional helpful instructions on connecting Superset to Dremio.\n\nThe recommended way to connect to Apache Drill is through SQLAlchemy. You can use the sqlalchemy-drill package.\n\nOnce that is done, you can connect to Drill in two ways, either via the REST interface or by JDBC. If you are connecting via JDBC, you must have the Drill JDBC Driver installed.\n\nThe basic connection string for Drill looks like this:\n\nTo connect to Drill running on a local machine running in embedded mode you can use the following connection string:\n\nConnecting to Drill through JDBC is more complicated and we recommend following this tutorial.\n\nThe connection string looks like:\n\nWe recommend reading the Apache Drill documentation and read the GitHub README to learn how to work with Drill through ODBC.\n\nA native connector to Druid ships with Superset (behind the DRUID_IS_ACTIVE flag) but this is slowly getting deprecated in favor of the SQLAlchemy / DBAPI connector made available in the pydruid library.\n\nThe connection string looks like:\n\nHere's a breakdown of the key components of this connection string:\n\nUser: username portion of the credentials needed to connect to your database\n\nPassword: password portion of the credentials needed to connect to your database\n\nHost: IP address (or URL) of the host machine that's running your database\n\nPort: specific port that's exposed on your host machine where your database is running\n\nWhen adding a connection to Druid, you can customize the connection a few different ways in the Add Database form.\n\nCustom Certificate\n\nYou can add certificates in the Root Certificate field when configuring the new database connection to Druid:\n\nWhen using a custom certificate, pydruid will automatically use https scheme.\n\nDisable SSL Verification\n\nTo disable SSL verification, add the following to the Extras field:\n\nCommon aggregations or Druid metrics can be defined and used in Superset. The first and simpler use case is to use the checkbox matrix exposed in your datasourceâs edit view (Sources -> Druid Datasources -> [your datasource] -> Edit -> [tab] List Druid Column).\n\nClicking the GroupBy and Filterable checkboxes will make the column appear in the related dropdowns while in the Explore view. Checking Count Distinct, Min, Max or Sum will result in creating new metrics that will appear in the List Druid Metric tab upon saving the datasource.\n\nBy editing these metrics, youâll notice that their JSON element corresponds to Druid aggregation definition. You can create your own aggregations manually from the List Druid Metric tab following Druid documentation.\n\nDruid supports post aggregation and this works in Superset. All you have to do is create a metric, much like you would create an aggregation manually, but specify postagg as a Metric Type. You then have to provide a valid json post-aggregation definition (as specified in the Druid docs) in the JSON field.\n\nThe recommended connector library for Elasticsearch is elasticsearch-dbapi.\n\nThe connection string for Elasticsearch looks like this:\n\nUsing HTTPS\n\nElasticsearch as a default limit of 10000 rows, so you can increase this limit on your cluster or set Supersetâs row limit on config\n\nYou can query multiple indices on SQL Lab for example\n\nBut, to use visualizations for multiple indices you need to create an alias index on your cluster\n\nThen register your table with the alias name logstash_all\n\nTime zone\n\nBy default, Superset uses UTC time zone for elasticsearch query. If you need to specify a time zone, please edit your Database and enter the settings of your specified time zone in the Other > ENGINE PARAMETERS:\n\nAnother issue to note about the time zone problem is that before elasticsearch7.8, if you want to convert a string into a DATETIME object, you need to use the CAST function,but this function does not support our time_zone setting. So it is recommended to upgrade to the version after elasticsearch7.8. After elasticsearch7.8, you can use the DATETIME_PARSE function to solve this problem. The DATETIME_PARSE function is to support our time_zone setting, and here you need to fill in your elasticsearch version number in the Other > VERSION setting. the superset will use the DATETIME_PARSE function for conversion.\n\nDisable SSL Verification\n\nTo disable SSL verification, add the following to the SQLALCHEMY URI field:\n\nThe recommended connector library for Exasol is sqlalchemy-exasol.\n\nThe connection string for Exasol looks like this:\n\nThe recommended connector library for Firebird is sqlalchemy-firebird. Superset has been tested on sqlalchemy-firebird>=0.7.0, <0.8.\n\nThe recommended connection string is:\n\nHere's a connection string example of Superset connecting to a local Firebird database:\n\nThe recommended connector library for Firebolt is firebolt-sqlalchemy.\n\nThe recommended connection string is:\n\nIt's also possible to connect using a service account:\n\nThe recommended connector library for BigQuery is sqlalchemy-bigquery.\n\nFollow the steps here about how to install new database drivers when setting up Superset locally via docker compose.\n\nWhen adding a new BigQuery connection in Superset, you'll need to add the GCP Service Account credentials file (as a JSON).\n\nCreate your Service Account via the Google Cloud Platform control panel, provide it access to the appropriate BigQuery datasets, and download the JSON configuration file for the service account.\n\nIn Superset, you can either upload that JSON or add the JSON blob in the following format (this should be the content of your credential JSON file):\n\nAdditionally, can connect via SQLAlchemy URI instead\n\nThe connection string for BigQuery looks like:\n\nbigquery://{project_id}\n\nGo to the Advanced tab, Add a JSON blob to the Secure Extra field in the database configuration form with the following format:\n\n{\n\n\"credentials_info\": <contents of credentials JSON file>\n\n}\n\nThe resulting file should have this structure:\n\n{\n\n\"credentials_info\": {\n\n\"type\": \"service_account\",\n\n\"project_id\": \"...\",\n\n\"private_key_id\": \"...\",\n\n\"private_key\": \"...\",\n\n\"client_email\": \"...\",\n\n\"client_id\": \"...\",\n\n\"auth_uri\": \"...\",\n\n\"token_uri\": \"...\",\n\n\"auth_provider_x509_cert_url\": \"...\",\n\n\"client_x509_cert_url\": \"...\"\n\n}\n\n}\n\nYou should then be able to connect to your BigQuery datasets.\n\nTo be able to upload CSV or Excel files to BigQuery in Superset, you'll need to also add the pandas_gbq library.\n\nCurrently, the Google BigQuery Python SDK is not compatible with gevent, due to some dynamic monkeypatching on python core library by gevent. So, when you deploy Superset with gunicorn server, you have to use worker type except gevent.\n\nGoogle Sheets has a very limited SQL API. The recommended connector library for Google Sheets is shillelagh.\n\nThere are a few steps involved in connecting Superset to Google Sheets. This tutorial has the most up to date instructions on setting up this connection.\n\nThe recommended connector library is sqlalchemy-hana.\n\nThe connection string is formatted as follows:\n\nThe pyhive library is the recommended way to connect to Hive through SQLAlchemy.\n\nThe expected connection string is formatted as follows:\n\nHologres is a real-time interactive analytics service developed by Alibaba Cloud. It is fully compatible with PostgreSQL 11 and integrates seamlessly with the big data ecosystem.\n\nHologres sample connection parameters:\n\nUser Name: The AccessKey ID of your Alibaba Cloud account.\n\nPassword: The AccessKey secret of your Alibaba Cloud account.\n\nDatabase Host: The public endpoint of the Hologres instance.\n\nDatabase Name: The name of the Hologres database.\n\nPort: The port number of the Hologres instance.\n\nThe connection string looks like:\n\nThe IBM_DB_SA library provides a Python / SQLAlchemy interface to IBM Data Servers.\n\nHere's the recommended connection string:\n\nThere are two DB2 dialect versions implemented in SQLAlchemy. If you are connecting to a DB2 version without LIMIT [n] syntax, the recommended connection string to be able to use the SQL Lab is:\n\nThe recommended connector library to Apache Impala is impyla.\n\nThe expected connection string is formatted as follows:\n\nThe recommended connector library for Kusto is sqlalchemy-kusto>=2.0.0.\n\nThe connection string for Kusto (sql dialect) looks like this:\n\nThe connection string for Kusto (kql dialect) looks like this:\n\nMake sure the user has privileges to access and use all required databases/tables/views.\n\nThe recommended connector library for Apache Kylin is kylinpy.\n\nThe expected connection string is formatted as follows:\n\nThe recommended connector library for MySQL is mysqlclient.\n\nHere's the connection string:\n\nHost:\n\nFor Localhost: localhost or 127.0.0.1\n\nDocker running on Linux: 172.18.0.1\n\nFor On Prem: IP address or Host name\n\nFor Docker running in OSX: docker.for.mac.host.internal Port: 3306 by default\n\nOne problem with mysqlclient is that it will fail to connect to newer MySQL databases using caching_sha2_password for authentication, since the plugin is not included in the client. In this case, you should use mysql-connector-python instead:\n\nThe nzalchemy library provides a Python / SQLAlchemy interface to IBM Netezza Performance Server (aka Netezza).\n\nHere's the recommended connection string:\n\nThe sqlalchemy-oceanbase library is the recommended way to connect to OceanBase through SQLAlchemy.\n\nThe connection string for OceanBase looks like this:\n\nThe recommended connector library for Ocient is sqlalchemy-ocient.\n\nThe format of the Ocient DSN is:\n\nThe recommended connector library is cx_Oracle.\n\nThe connection string is formatted as follows:\n\nThe recommended connector library for Apache Pinot is pinotdb.\n\nThe expected connection string is formatted as follows:\n\nThe expected connection string using username and password is formatted as follows:\n\nIf you want to use explore view or joins, window functions, etc. then enable multi-stage query engine. Add below argument while creating database connection in Advanced -> Other -> ENGINE PARAMETERS\n\nNote that, if you're using docker compose, the Postgres connector library psycopg2 comes out of the box with Superset.\n\nPostgres sample connection parameters:\n\nUser Name: UserName\n\nPassword: DBPassword\n\nDatabase Host:\n\nFor Localhost: localhost or 127.0.0.1\n\nFor On Prem: IP address or Host name\n\nFor AWS Endpoint\n\nDatabase Name: Database Name\n\nPort: default 5432\n\nThe connection string looks like:\n\nYou can require SSL by adding ?sslmode=require at the end:\n\nYou can read about the other SSL modes that Postgres supports in Table 31-1 from this documentation.\n\nMore information about PostgreSQL connection options can be found in the SQLAlchemy docs and the PostgreSQL docs.\n\nThe pyhive library is the recommended way to connect to Presto through SQLAlchemy.\n\nThe expected connection string is formatted as follows:\n\nYou can pass in a username and password as well:\n\nHere is an example connection string with values:\n\nBy default Superset assumes the most recent version of Presto is being used when querying the datasource. If youâre using an older version of Presto, you can configure it in the extra parameter:\n\nSSL Secure extra add json config to extra connection information.\n\nThe recommended connector library for RisingWave is sqlalchemy-risingwave.\n\nThe expected connection string is formatted as follows:\n\nThe connection string for Rockset is:\n\nGet your API key from the Rockset console. Find your API server from the API reference. Omit the https:// portion of the URL.\n\nTo target to a specific virtual instance, use this URI format:\n\nFor more complete instructions, we recommend the Rockset documentation.\n\nFollow the steps here about how to install new database drivers when setting up Superset locally via docker compose.\n\nThe recommended connector library for Snowflake is snowflake-sqlalchemy.\n\nThe connection string for Snowflake looks like this:\n\nThe schema is not necessary in the connection string, as it is defined per table/query. The role and warehouse can be omitted if defaults are defined for the user, i.e.\n\nMake sure the user has privileges to access and use all required databases/schemas/tables/views/warehouses, as the Snowflake SQLAlchemy engine does not test for user/role rights during engine creation by default. However, when pressing the âTest Connectionâ button in the Create or Edit Database dialog, user/role credentials are validated by passing âvalidate_default_parametersâ: True to the connect() method during engine creation. If the user/role is not authorized to access the database, an error is recorded in the Superset logs.\n\nAnd if you want connect Snowflake with Key Pair Authentication. Please make sure you have the key pair and the public key is registered in Snowflake. To connect Snowflake with Key Pair Authentication, you need to add the following parameters to \"SECURE EXTRA\" field.\n\nPlease note that you need to merge multi-line private key content to one line and insert \\n between each line\n\nIf your private key is stored on server, you can replace \"privatekey_body\" with âprivatekey_pathâ in parameter.\n\nThe sqlalchemy-solr library provides a Python / SQLAlchemy interface to Apache Solr.\n\nThe connection string for Solr looks like this:\n\nThe recommended connector library for Apache Spark SQL pyhive.\n\nThe expected connection string is formatted as follows:\n\nThe recommended connector library for SQL Server is pymssql.\n\nThe connection string for SQL Server looks like this:\n\nIt is also possible to connect using pyodbc with the parameter odbc_connect\n\nThe connection string for SQL Server looks like this:\n\nThe sqlalchemy-starrocks library is the recommended way to connect to StarRocks through SQLAlchemy.\n\nYou'll need to the following setting values to form the connection string:\n\nUser: User Name\n\nPassword: DBPassword\n\nHost: StarRocks FE Host\n\nCatalog: Catalog Name\n\nDatabase: Database Name\n\nPort: StarRocks FE port\n\nHere's what the connection string looks like:\n\nThe recommended connector library is teradatasqlalchemy.\n\nThe connection string for Teradata looks like this:\n\nThere's also an older connector named sqlalchemy-teradata that requires the installation of ODBC drivers. The Teradata ODBC Drivers are available here: https://downloads.teradata.com/download/connectivity/odbc-driver/linux\n\nHere are the required environment variables:\n\nWe recommend using the first library because of the lack of requirement around ODBC drivers and because it's more regularly updated.\n\nTimescaleDB is the open-source relational database for time-series and analytics to build powerful data-intensive applications. TimescaleDB is a PostgreSQL extension, and you can use the standard PostgreSQL connector library, psycopg2, to connect to the database.\n\nIf you're using docker compose, psycopg2 comes out of the box with Superset.\n\nTimescaleDB sample connection parameters:\n\nUser Name: User\n\nPassword: Password\n\nDatabase Host:\n\nFor Localhost: localhost or 127.0.0.1\n\nFor On Prem: IP address or Host name\n\nFor Timescale Cloud service: Host name\n\nFor Managed Service for TimescaleDB service: Host name\n\nDatabase Name: Database Name\n\nPort: default 5432 or Port number of the service\n\nThe connection string looks like:\n\nYou can require SSL by adding ?sslmode=require at the end (e.g. in case you use Timescale Cloud):\n\nLearn more about TimescaleDB!\n\nSupported trino version 352 and higher\n\nThe connection string format is as follows:\n\nIf you are running Trino with docker on local machine, please use the following connection URL\n\nYou can provide username/password in the connection string or in the Secure Extra field at Advanced / Security\n\nIn Connection String\n\ntrino://{username}:{password}@{hostname}:{port}/{catalog}\n\nIn Secure Extra field\n\n{\n\n\"auth_method\": \"basic\",\n\n\"auth_params\": {\n\n\"username\": \"<username>\",\n\n\"password\": \"<password>\"\n\n}\n\n}\n\nNOTE: if both are provided, Secure Extra always takes higher priority.\n\nIn Secure Extra field, config as following example:\n\nAll fields in auth_params are passed directly to the KerberosAuthentication class.\n\nNOTE: Kerberos authentication requires installing the trino-python-client locally with either the all or kerberos optional features, i.e., installing trino[all] or trino[kerberos] respectively.\n\nIn Secure Extra field, config as following example:\n\nAll fields in auth_params are passed directly to the CertificateAuthentication class.\n\nConfig auth_method and provide token in Secure Extra field\n\nTo use custom authentication, first you need to add it into ALLOWED_EXTRA_AUTHENTICATIONS allow list in Superset config file:\n\nThen in Secure Extra field:\n\nYou can also use custom authentication by providing reference to your trino.auth.Authentication class or factory function (which returns an Authentication instance) to auth_method.\n\nAll fields in auth_params are passed directly to your class/function.\n\nReference:\n\nTrino-Superset-Podcast\n\nThe recommended connector library is sqlalchemy-vertica-python. The Vertica connection parameters are:\n\nUser Name: UserName\n\nPassword: DBPassword\n\nDatabase Host:\n\nFor Localhost : localhost or 127.0.0.1\n\nFor On Prem : IP address or Host name\n\nFor Cloud: IP Address or Host Name\n\nDatabase Name: Database Name\n\nPort: default 5433\n\nThe connection string is formatted as follows:\n\nOther parameters:\n\nLoad Balancer - Backup Host\n\nYugabyteDB is a distributed SQL database built on top of PostgreSQL.\n\nNote that, if you're using docker compose, the Postgres connector library psycopg2 comes out of the box with Superset.\n\nThe connection string looks like:\n\nHere is the documentation on how to leverage the new DB Connection UI. This will provide admins the ability to enhance the UX for users who want to connect to new databases.\n\nThere are now 3 steps when connecting to a database in the new UI:\n\nStep 1: First the admin must inform superset what engine they want to connect to. This page is powered by the /available endpoint which pulls on the engines currently installed in your environment, so that only supported databases are shown.\n\nStep 2: Next, the admin is prompted to enter database specific parameters. Depending on whether there is a dynamic form available for that specific engine, the admin will either see the new custom form or the legacy SQLAlchemy form. We currently have built dynamic forms for (Redshift, MySQL, Postgres, and BigQuery). The new form prompts the user for the parameters needed to connect (for example, username, password, host, port, etc.) and provides immediate feedback on errors.\n\nStep 3: Finally, once the admin has connected to their DB using the dynamic form they have the opportunity to update any optional advanced settings.\n\nWe hope this feature will help eliminate a huge bottleneck for users to get into the application and start crafting datasets.\n\nWe added a new configuration option where the admin can define their preferred databases, in order:\n\nFor copyright reasons the logos for each database are not distributed with Superset.\n\nTo set the images of your preferred database, admins must create a mapping in the superset_text.yml file with engine and location of the image. The image can be host locally inside your static/file directory or online (e.g. S3)\n\nCurrently the new modal supports the following databases:\n\nPostgres\n\nRedshift\n\nMySQL\n\nBigQuery\n\nWhen the user selects a database not in this list they will see the old dialog asking for the SQLAlchemy URI. New databases can be added gradually to the new flow. In order to support the rich configuration a DB engine spec needs to have the following attributes:\n\nparameters_schema: a Marshmallow schema defining the parameters needed to configure the database. For Postgres this includes username, password, host, port, etc. (see).\n\ndefault_driver: the name of the recommended driver for the DB engine spec. Many SQLAlchemy dialects support multiple drivers, but usually one is the official recommendation. For Postgres we use \"psycopg2\".\n\nsqlalchemy_uri_placeholder: a string that helps the user in case they want to type the URI directly.\n\nencryption_parameters: parameters used to build the URI when the user opts for an encrypted connection. For Postgres this is {\"sslmode\": \"require\"}.\n\nIn addition, the DB engine spec must implement these class methods:\n\nbuild_sqlalchemy_uri(cls, parameters, encrypted_extra): this method receives the distinct parameters and builds the URI from them.\n\nget_parameters_from_uri(cls, uri, encrypted_extra): this method does the opposite, extracting the parameters from a given URI.\n\nvalidate_parameters(cls, parameters): this method is used for onBlur validation of the form. It should return a list of SupersetError indicating which parameters are missing, and which parameters are definitely incorrect (example).\n\nFor databases like MySQL and Postgres that use the standard format of engine+driver://user:password@host:port/dbname all you need to do is add the BasicParametersMixin to the DB engine spec, and then define the parameters 2-4 (parameters_schema is already present in the mixin).\n\nFor other databases you need to implement these methods yourself. The BigQuery DB engine spec is a good example of how to do that.\n\nIt is possible to tweak the database connection information using the parameters exposed by SQLAlchemy. In the Database edit view, you can edit the Extra field as a JSON blob.\n\nThis JSON string contains extra configuration elements. The engine_params object gets unpacked into the sqlalchemy.create_engine call, while the metadata_params get unpacked into the sqlalchemy.MetaData call. Refer to the SQLAlchemy docs for more information.\n\nDatabases like Postgres and Redshift use the schema as the logical entity on top of the database. For Superset to connect to a specific schema, you can set the schema parameter in the Edit Tables form (Sources > Tables > Edit record).\n\nSuperset can be configured to use an external store for database passwords. This is useful if you a running a custom secret distribution framework and do not wish to store secrets in Supersetâs meta database.\n\nExample: Write a function that takes a single argument of type sqla.engine.url and returns the password for the given connection string. Then set SQLALCHEMY_CUSTOM_PASSWORD_STORE in your config file to point to that function.\n\nA common pattern is to use environment variables to make secrets available. SQLALCHEMY_CUSTOM_PASSWORD_STORE can also be used for that purpose.\n\nYou can use the Extra field in the Edit Databases form to configure SSL:\n\nSuperset offers an experimental feature for querying across different databases. This is done via a special database called \"Superset meta database\" that uses the \"superset://\" SQLAlchemy URI. When using the database it's possible to query any table in any of the configured databases using the following syntax:\n\nFor example:\n\nSpaces are allowed, but periods in the names must be replaced by %2E. Eg:\n\nThe query above returns the same rows as SELECT * FROM \"examples.birth_names\", and also shows that the meta database can query tables from any table â even itself!\n\nBefore enabling this feature, there are a few considerations that you should have in mind. First, the meta database enforces permissions on the queried tables, so users should only have access via the database to tables that they originally have access to. Nevertheless, the meta database is a new surface for potential attacks, and bugs could allow users to see data they should not.\n\nSecond, there are performance considerations. The meta database will push any filtering, sorting, and limiting to the underlying databases, but any aggregations and joins will happen in memory in the process running the query. Because of this, it's recommended to run the database in async mode, so queries are executed in Celery workers, instead of the web workers. Additionally, it's possible to specify a hard limit on how many rows are returned from the underlying databases.\n\nTo enable the Superset meta database, first you need to set the ENABLE_SUPERSET_META_DB feature flag to true. Then, add a new database of type \"Superset meta database\" with the SQLAlchemy URI \"superset://\".\n\nIf you enable DML in the meta database users will be able to run DML queries on underlying databases as long as DML is also enabled in them. This allows users to run queries that move data across databases.\n\nSecond, you might want to change the value of SUPERSET_META_DB_LIMIT. The default value is 1000, and defines how many are read from each database before any aggregations and joins are executed. You can also set this value None if you only have small tables.\n\nAdditionally, you might want to restrict the databases to with the meta database has access to. This can be done in the database configuration, under \"Advanced\" -> \"Other\" -> \"ENGINE PARAMETERS\" and adding:"
    }
}
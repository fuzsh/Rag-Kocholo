{
    "id": "dbpedia_6980_3",
    "rank": 55,
    "data": {
        "url": "https://learn.microsoft.com/en-us/azure/ai-services/speech-service/releasenotes",
        "read_more_link": "",
        "language": "en",
        "title": "What's new - Speech service - Azure AI services",
        "top_image": "https://learn.microsoft.com/en-us/media/open-graph-image.png",
        "meta_img": "https://learn.microsoft.com/en-us/media/open-graph-image.png",
        "images": [],
        "movies": [],
        "keywords": [],
        "meta_keywords": [
            ""
        ],
        "tags": null,
        "authors": [
            "eric-urban"
        ],
        "publish_date": "2024-06-06T08:00:00+00:00",
        "summary": "",
        "meta_description": "Find out about new releases and features for Azure AI Speech.",
        "meta_lang": "en",
        "meta_favicon": "",
        "meta_site_name": "",
        "canonical_link": "https://learn.microsoft.com/en-us/azure/ai-services/speech-service/releasenotes",
        "text": "SDK\n\nCLI\n\nText to speech service\n\nSpeech to text service\n\nContainers\n\nSpeech SDK 1.40: 2024-August release\n\nNote: 1.39.0 isn't missing, it was an internal release.\n\nNew features\n\nAdded support for streaming of G.722 compressed audio in speech recognition.\n\nAdded support for pitch, rate and volume setting in input text streaming in speech synthesis.\n\nAdded support for personal voice input text streaming by introducing PersonalVoiceSynthesisRequest in speech synthesis. Note: This API is in preview and may be subject to change in future versions.\n\nAdded support for diarization of intermediate results when ConversationTranscriber is used.\n\nRemoved CentOS/RHEL 7 support due to CentOS 7 EOL and the end of RHEL 7 Maintenance Support 2.\n\nUse of embedded speech models now requires a model license instead of a model key. If you're an existing embedded speech customer and want to upgrade, please contact your support person at Microsoft for details on model updates.\n\nBug fixes\n\nBuilt Speech SDK binaries for Windows with the _DISABLE_CONSTEXPR_MUTEX_CONSTRUCTOR flag as mitigation for the Visual C++ runtime issue Access violation with std::mutex::lock after upgrading to VS 2022 version 17.10.0 - Developer Community (visualstudio.com). Note that Windows C++ applications using the Speech SDK may need to apply the same build configuration flag if their code uses std::mutex (see details in the linked issue).\n\nFixed OpenSSL 3.x detection not working on Linux arm64 (https://github.com/Azure-Samples/cognitive-services-speech-sdk/issues/2420).\n\nFixed the issue that when deploying a UWP app, libraries and model from MAS NuGet package wouldn't get copied to the deployment location.\n\nFixed a content provider conflict in Android packages (https://github.com/Azure-Samples/cognitive-services-speech-sdk/issues/2463).\n\nFixed postprocessing options not applying to intermediate speech recognition results.\n\nFixed .NET 8 warning about distribution specific runtime identifiers (https://github.com/Azure-Samples/cognitive-services-speech-sdk/issues/2244).\n\nSamples\n\nUpdated embedded speech samples to use a model license instead of a key.\n\nSpeech SDK 1.38.0: 2024-June release\n\nNew features\n\nUpgrade Speech SDK Linux platform requirements:\n\nThe new minimum baseline is Ubuntu 20.04 LTS or compatible with glibc 2.31 or newer.\n\nBinaries for Linux x86 are removed in accordance with Ubuntu 20.04 platform support.\n\nNote that RHEL/CentOS 7 remain supported until June 30 (CentOS 7 EOL and the end of RHEL 7 Maintenance Support 2). Binaries for them will be removed in the Speech SDK 1.39.0 release.\n\nAdd support for OpenSSL 3 on Linux.\n\nAdd support for g722-16khz-64kbps audio output format with speech synthesizer.\n\nAdd support for sending messages through a connection object with speech synthesizer.\n\nAdd Start/StopKeywordRecognition APIs in Objective-C and Swift.\n\nAdd API for selecting a custom translation model category.\n\nUpdate GStreamer usage with speech synthesizer.\n\nBug fixes\n\nFix \"Websocket message size cannot exceed 65536 bytes\" error during Start/StopKeywordRecognition.\n\nFix a Python segmentation fault during speech synthesis.\n\nSamples\n\nUpdate C# samples to use .NET 6.0 by default.\n\nSpeech SDK 1.37.0: 2024-April release\n\nNew features\n\nAdd support for input text streaming in speech synthesis.\n\nChange the default speech synthesis voice to en-US-AvaMultilingualNeural.\n\nUpdate Android builds to use OpenSSL 3.x.\n\nBug fixes\n\nFix occasional JVM crashes during SpeechRecognizer dispose when using MAS. (https://github.com/Azure-Samples/cognitive-services-speech-sdk/issues/2125)\n\nImprove detection of default audio devices on Linux. (https://github.com/Azure-Samples/cognitive-services-speech-sdk/issues/2292)\n\nSamples\n\nUpdated for new features.\n\nSpeech SDK 1.36.0: 2024-March release\n\nNew features\n\nAdd support for language identification in multi-lingual translation on v2 endpoints using AutoDetectSourceLanguageConfig::FromOpenRange().\n\nBug fixes\n\nFix SynthesisCanceled event not fired if stop is called during SynthesisStarted event.\n\nFix a noise issue in embedded speech synthesis.\n\nFix a crash in embedded speech recognition when running multiple recognizers in parallel.\n\nFix the phrase detection mode setting on v1/v2 endpoints.\n\nFixes to various issues with Microsoft Audio Stack.\n\nSamples\n\nUpdates for new features.\n\nSpeech SDK 1.35.0: February 2024 release\n\nNew features\n\nChange the default text to speech voice from en-US-JennyMultilingualNeural to en-US-AvaNeural.\n\nSupport word-level detail in embedded speech translation results using the detailed output format.\n\nBug fixes\n\nFix the AudioDataStream position getter API in Python.\n\nFix speech translation using v2 endpoints without language detection.\n\nFix a random crash and duplicate word boundary events in embedded text to speech.\n\nReturn a correct cancellation error code for an internal server error on WebSocket connections.\n\nFix the failure to load FPIEProcessor.dll library when MAS is used with C#.\n\nSamples\n\nMinor formatting updates for Embedded recognition samples.\n\nSpeech SDK 1.34.1: January 2024 release\n\nBreaking changes\n\nBug fixes only\n\nNew features\n\nBug fixes only\n\nBug fixes\n\nFix regression introduced in 1.34.0 where service endpoint url was constructed with bad locale info for users in several China regions.\n\nSpeech SDK 1.34.0: November 2023 release\n\nBreaking changes\n\nSpeechRecognizer has been updated to use a new endpoint by default (i.e. when not explicitly specifying a URL) which no longer supports query string parameters for most of the properties. Instead of setting query string parameters directly with ServicePropertyChannel.UriQueryParameter, please use the corresponding API functions.\n\nNew features\n\nCompatibility with .NET 8 (Fix for https://github.com/Azure-Samples/cognitive-services-speech-sdk/issues/2170 except for warning about centos7-x64)\n\nSupport for embedded speech performance metrics which can be used to evaluate the capability of a device to run embedded speech.\n\nSupport for source language identification in embedded multi-lingual translation.\n\nSupport for embedded speech-to-text, text to speech and translation for iOS and Swift/Objective-C released in preview.\n\nEmbedded support is provided in MicrosoftCognitiveServicesSpeechEmbedded-iOS Cocoapod.\n\nBug fixes\n\nFix for iOS SDK x2 times binary size growth Â· Issue #2113 Â· Azure-Samples/cognitive-services-speech-sdk (github.com)\n\nFix for Unable to get word level time stamps from azure speech to text api Â· Issue #2156 Â· Azure-Samples/cognitive-services-speech-sdk (github.com)\n\nFix for DialogServiceConnector destruction phase to disconnect events correctly. This was causing crashes occasionally.\n\nFix for exception during creation of a recognizer when MAS is used.\n\nFPIEProcessor.dll from Microsoft.CognitiveServices.Speech.Extension.MAS NuGet package for Windows UWP x64 and ARM64 had dependency on VC runtime libraries for native C++. The issue has been rectified by updating the dependency to correct VC runtime libraries (for UWP).\n\nFix for [MAS] Recurrent calls to recognizeOnceAsync lead to SPXERR_ALREADY_INITIALIZED when using MAS Â· Issue #2124 Â· Azure-Samples/cognitive-services-speech-sdk (github.com)\n\nFix for embedded speech recognition crash when phrase lists are used.\n\nSamples\n\nEmbedded iOS samples for speech-to-text, text to speech and translation.\n\nSpeech CLI 1.34.0: November 2023 release\n\nNew features\n\nSupport word boundary events output when synthesizing speech.\n\nBug fixes\n\nUpdated JMESPath dependency to the latest release, improves string evaluations\n\nSpeech SDK 1.33.0: October 2023 release\n\nBreaking change notice\n\nThe new NuGet package added for Microsoft Audio Stack (MAS) is now required to be included by applications that are using MAS in their package configuration files.\n\nNew features\n\nAdded the new NuGet package Microsoft.CognitiveServices.Speech.Extension.MAS.nupkg, which provides improved echo cancellation performance when using Microsoft Audio Stack\n\nPronunciation Assessment: added support for prosody and content evaluation, which can assess the spoken speech in terms of prosody, vocabulary, grammar, and topic.\n\nBug fixes\n\nFixed keyword recognition result offsets so that they correctly match the input audio stream since the beginning. The fix applies to both stand-alone keyword recognition and keyword-triggered speech recognition.\n\nFixed Synthesizer stopSpeaking doesn't return immediately SPXSpeechSynthesizer stopSpeaking() method cannot return immediately on iOS 17 - Issue #2081\n\nFixed Mac catalyst import issue on Swift module Support for mac catalyst with apple silicon. Issue #1948\n\nJS: AudioWorkletNode module loads now uses a trusted URL, with fallback for CDN browser includes.\n\nJS: Packed lib files now targets ES6 JS, with support for ES5 JS removed.\n\nJS: intermediate events for translation scenario targeting v2 endpoint are correctly handled\n\nJS: The language property for TranslationRecognitionEventArgs is now set for translation.hypothesis events.\n\nSpeech Synthesis: SynthesisCompleted event is guaranteed to be emit after all metadata events, so it could be used to indicate to the end of events. How to detect when visemes are received completely? Issue #2093 Azure-Samples/cognitive-services-speech-sdk\n\nSamples\n\nAdded sample to demonstrate MULAW streaming using Python)\n\nFix for speech-to-text NAudio sample\n\nSpeech CLI 1.33.0: October 2023 release\n\nNew features\n\nSupport word boundary events output when synthesizing speech.\n\nBug fixes\n\nnone\n\nSpeech SDK 1.32.1: September 2023 release\n\nBug fixes\n\nAndroid packages updates with latest security fixes from OpenSSL1.1.1v\n\nJS â WebWorkerLoadType property added to allow bypass of data URL load for timeout worker\n\nJS â Fix Conversation Translation disconnect after 10 minutes\n\nJS â Conversation Translation auth token from Conversation now propagates to Translation service connection\n\nSamples\n\nConversation transcription with Swift APIs\n\nSpeech SDK 1.31.0: August 2023 release\n\nNew Features\n\nSupport for real-time diarization is available in public preview with the Speech SDK 1.31.0. This feature is available in the following SDKs: C#, C++, Java, JavaScript, Python, and Objective-C/Swift.\n\nSynchronized speech synthesis word boundary and viseme events with audio playback\n\nBreaking changes\n\nThe former \"conversation transcription\" scenario is renamed to \"meeting transcription\". For example, use MeetingTranscriber instead of ConversationTranscriber, and use CreateMeetingAsync instead of CreateConversationAsync. Although the names of SDK objects and methods have changed, the renaming doesn't change the feature itself. Use meeting transcription objects for transcription of meetings with user profiles and voice signatures. See Meeting transcription for more information. The \"conversation translation\" objects and methods aren't affected by these changes. You can still use the ConversationTranslator object and its methods for meeting translation scenarios.\n\nFor real-time diarization, a new ConversationTranscriber object is introduced. The new \"conversation transcription\" object model and call patterns are similar to continuous recognition with the SpeechRecognizer object. A key difference is that the ConversationTranscriber object is designed to be used in a conversation scenario where you want to differentiate multiple speakers (diarization). User profiles and voice signatures aren't applicable. See the real-time diarization quickstart for more information.\n\nThis table shows the previous and new object names for real-time diarization and meeting transcription. The scenario name is in the first column, the previous object names are in the second column, and the new object names are in the third column.\n\nScenario name Previous object names New object names Real-time diarization N/A ConversationTranscriber Meeting transcription ConversationTranscriber\n\nConversationTranscriptionEventArgs\n\nConversationTranscriptionCanceledEventArgs\n\nConversationTranscriptionResult\n\nRemoteConversationTranscriptionResult\n\nRemoteConversationTranscriptionClient\n\nRemoteConversationTranscriptionResult\n\nParticipant1\n\nParticipantChangedReason1\n\nUser1 MeetingTranscriber\n\nMeetingTranscriptionEventArgs\n\nMeetingTranscriptionCanceledEventArgs\n\nMeetingTranscriptionResult\n\nRemoteMeetingTranscriptionResult\n\nRemoteMeetingTranscriptionClient\n\nRemoteMeetingTranscriptionResult\n\nParticipant\n\nParticipantChangedReason\n\nUser\n\nMeeting2\n\n1 The Participant, ParticipantChangedReason, and User objects are applicable to both meeting transcription and meeting translation scenarios.\n\n2 The Meeting object is new and is used with the MeetingTranscriber object.\n\nBug fixes\n\nFixed macOS minimum supported version https://github.com/Azure-Samples/cognitive-services-speech-sdk/issues/2017\n\nFixed Pronunciation Assessment bug:\n\nAddressed phoneme accuracy scores issue, ensuring they now accurately reflect only the specific mispronounced phoneme. https://github.com/Azure-Samples/cognitive-services-speech-sdk/issues/1917\n\nResolved an issue where the Pronunciation Assessment feature was inaccurately identifying entirely correct pronunciations as erroneous, particularly in situations where words could have multiple valid pronunciations. https://github.com/Azure-Samples/cognitive-services-speech-sdk/issues/1530\n\nSamples\n\nCSharp\n\nNew C# conversation transcription quickstart\n\nNew C# meeting transcription quickstart\n\nJavaScript\n\nNew JavaScript conversation transcription quickstart\n\nNew JavaScript meeting transcription quickstart\n\nNew NodeJS conversation transcription quickstart\n\nNew NodeJS meeting transcription quickstart\n\nSpeech SDK 1.30.0: July 2023 release\n\nNew Features\n\nC++, C#, Java - Added support for DisplayWords in Embedded Speech Recognition's detailed result.\n\nObjective-C/Swift - Added support for ConnectionMessageReceived event in Objective-C/Swift.\n\nObjective-C/Swift - Improved keyword-spotting models for iOS. This change has increased the size of certain packages, which contain iOS binaries (like NuGet, XCFramework). We're working to reduce the size for future releases.\n\nBug fixes\n\nFixed a memory leak when using speech recognizer with PhraseListGrammar, as reported by a customer (GitHub issue).\n\nFixed a deadlock in text to speech open connection API.\n\nAdditional notes\n\nJava - Some internally used, public Java API methods were changed to package internal, protected or private. This change shouldn't have an effect on developers, as we don't expect applications to be using those. Noted here for transparency.\n\nSamples\n\nNew Pronunciation Assessment samples on how to specify a learning language in your own application\n\nC#: See sample code.\n\nC++: See sample code.\n\nJavaScript: See sample code.\n\nObjective-C: See sample code.\n\nPython: See sample code.\n\nSwift: See sample code.\n\nSpeech SDK 1.29.0: June 2023 release\n\nNew Features\n\nC++, C#, Java - Preview of Embedded Speech Translation APIs. Now you can do speech translation without cloud connection!\n\nJavaScript - Continuous Language Identification (LID) now enabled for speech translation.\n\nJavaScript - Community contribution for adding LocaleName property to VoiceInfo class. Thank you GitHub user shivsarthak for the pull request.\n\nC++, C#, Java - Added support for resampling Embedded text to speech output from 16 kHz to 48 kHz sample rate.\n\nAdded support for hi-IN locale in Intent Recognizer with Simple Pattern Matching.\n\nBug fixes\n\nFixed a crash caused by a race condition in Speech Recognizer during object destruction, as seen in some of our Android tests\n\nFixed possible deadlocks in Intent Recognizer with Simple Pattern Matcher\n\nSamples\n\nNew Embedded Speech Translation samples\n\nSpeech SDK 1.28.0: May 2023 release\n\nBreaking change\n\nJavaScript SDK: Online Certificate Status Protocol (OCSP) was removed. This allows clients to better conform to browser and Node standards for certificate handling. Version 1.28 and onward will no longer include our custom OCSP module.\n\nNew Features\n\nEmbedded Speech Recognition now returns NoMatchReason::EndSilenceTimeout when a silence timeout occurs at the end of an utterance. This matches the behavior when doing recognition using the real-time speech service.\n\nJavaScript SDK: Set properties on SpeechTranslationConfig using PropertyId enum values.\n\nBug fixes\n\nC# on Windows - Fix potential race condition/deadlock in Windows audio extension. In scenarios that both dispose of the audio renderer quickly and also use the Synthesizer method to stop speaking, the underlying event wasn't reset by stop, and could cause the renderer object to never be disposed, all while it could be holding a global lock for disposal, freezing the dotnet GC thread.\n\nSamples\n\nAdded an embedded speech sample for MAUI.\n\nUpdated the embedded speech sample for Android Java to include text to speech.\n\nSpeech SDK 1.27.0: April 2023 release\n\nNotification about upcoming changes\n\nWe plan to remove Online Certificate Status Protocol (OCSP) in the next JavaScript SDK release. This allows clients to better conform to browser and Node standards for certificate handling. Version 1.27 is the last release that includes our custom OCSP module.\n\nNew Features\n\nJavaScript â Added support for microphone input from the browser with Speaker Identification and Verification.\n\nEmbedded Speech Recognition - Update support for PropertyId::Speech_SegmentationSilenceTimeoutMs setting.\n\nBug fixes\n\nGeneral - Reliability updates in service reconnection logic (all programming languages except JavaScript).\n\nGeneral - Fix string conversions leaking memory on Windows (all relevant programming languages except JavaScript).\n\nEmbedded Speech Recognition - Fix crash in French Speech Recognition when using certain grammar list entries.\n\nSource code documentation - Corrections to SDK reference documentation comments related to audio logging on the service.\n\nIntent recognition - Fix Pattern Matcher priorities related to list entities.\n\nSamples\n\nProperly handle authentication failure in C# Conversation Transcription (CTS) sample.\n\nAdded example of streaming pronunciation assessment for Python, JavaScript, Objective-C and Swift.\n\nSpeech SDK 1.26.0: March 2023 release\n\nBreaking changes\n\nBitcode has been disabled in all iOS targets in the following packages: Cocoapod with xcframework, NuGet (for Xamarin and MAUI) and Unity. The change is due to Apple's deprecation of bitcode support from Xcode 14 and onwards. This change also means if you're using Xcode 13 version or you have explicitly enabled the bitcode on your application using the Speech SDK, you may encounter an error saying \"framework doesn't contain bitcode and you must rebuild it\". To resolve this issue, make sure your targets have bitcode disabled.\n\nMinimum iOS deployment target has been upgraded to 11.0 in this release, which means armv7 HW is no longer supported.\n\nNew features\n\nEmbedded (on-device) Speech Recognition now supports both 8 and 16-kHz sampling rate input audio (16-bit per sample, mono PCM).\n\nSpeech Synthesis now reports connection, network and service latencies in the result to help end-to-end latency optimization.\n\nNew tie breaking rules for Intent Recognition with simple pattern matching. The more character bytes that are matched, will win over pattern matches with lower character byte count. Example: Pattern \"Select {something} in the top right\" will win over \"Select {something}\"\n\nBug fixes\n\nSpeech Synthesis: fix a bug where the emoji isn't correct in word boundary events.\n\nIntent Recognition with Conversational Language Understanding (CLU):\n\nIntents from the CLU Orchestrator Workflow now appear correctly.\n\nThe JSON result is now available via the property ID LanguageUnderstandingServiceResponse_JsonResult.\n\nSpeech recognition with keyword activation: Fix for missing ~150 ms audio after a keyword recognition.\n\nFix for Speech SDK NuGet iOS MAUI Release build, reported by customer (GitHub issue)\n\nSamples\n\nFix for Swift iOS sample, reported by customer (GitHub issue)\n\nSpeech SDK 1.25.0: January 2023 release\n\nBreaking changes\n\nLanguage Identification (preview) APIs have been simplified. If you update to Speech SDK 1.25 and see a build break, please visit the Language Identification page to learn about the new property SpeechServiceConnection_LanguageIdMode. This single property replaces the two previous ones SpeechServiceConnection_SingleLanguageIdPriority and SpeechServiceConnection_ContinuousLanguageIdPriority. Prioritizing between low latency and high accuracy is no longer necessary following recent model improvements. Now, you only need to select whether to run at-start or continuous Language Identification when doing continuous speech recognition or translation.\n\nNew features\n\nC#/C++/Java: Embedded Speech SDK is now released under gated public preview. See Embedded Speech (preview) documentation. You can now do on-device speech to text and text to speech when cloud connectivity is intermittent or unavailable. Supported on Android, Linux, macOS and Windows platforms\n\nC# MAUI: Support added for iOS and Mac Catalyst targets in Speech SDK NuGet (Customer issue)\n\nUnity: Android x86_64 architecture added to Unity package (Customer issue)\n\nGo:\n\nALAW/MULAW direct streaming support added for speech recognition (Customer issue)\n\nAdded support for PhraseListGrammar. Thank you GitHub user czkoko for the community contribution!\n\nC#/C++: Intent Recognizer now supports Conversational Language Understanding models in C++ and C# with orchestration on the Microsoft service\n\nBug fixes\n\nFix an occasional hang in KeywordRecognizer when trying to stop it\n\nPython:\n\nFix for getting Pronunciation Assessment results when PronunciationAssessmentGranularity.FullText is set (Customer issue)\n\nFix for gender property for Male voices not being retrieved, when getting speech synthesis voices\n\nJavaScript\n\nFix for parsing some WAV files that were recorded on iOS devices (Customer issue)\n\nJS SDK now builds without using npm-force-resolutions (Customer issue)\n\nConversation Translator now correctly sets service endpoint when using a speechConfig instance created using SpeechConfig.fromEndpoint()\n\nSamples\n\nAdded samples showing how to use Embedded Speech\n\nAdded Speech to text sample for MAUI\n\nSee Speech SDK samples repository.\n\nSpeech SDK 1.24.2: November 2022 release\n\nNew features\n\nNo new features, just an embedded engine fix to support new model files.\n\nBug fixes\n\nAll programing languages\n\nFixed an issue with encryption of embedded speech recognition models.\n\nSpeech SDK 1.24.1: November 2022 release\n\nNew features\n\nPublished packages for the Embedded Speech preview. See https://aka.ms/embedded-speech for more information.\n\nBug fixes\n\nAll programing languages\n\nFix embedded TTS crash when voice font isn't supported\n\nFix stopSpeaking() can't stop playback on Linux (#1686)\n\nJavaScript SDK\n\nFixed regression in how conversation transcriber gated audio.\n\nJava\n\nTemporarily Published updated POM and Javadocs files to Maven Central to enable the docs pipeline to update online reference docs.\n\nPython\n\nFix regression where Python speak_text(ssml) returns void.\n\nSpeech SDK 1.24.0: October 2022 release\n\nNew features\n\nAll programing languages: AMR-WB (16khz) added to the supported list of Text to speech audio output formats\n\nPython: Package added for Linux ARM64 for supported Linux distributions.\n\nC#/C++/Java/Python: Support added for ALAW & MULAW direct streaming to the speech service (in addition to existing PCM stream) using AudioStreamWaveFormat.\n\nC# MAUI: NuGet package updated to support Android targets for .NET MAUI developers (Customer issue)\n\nMac: Added separate XCframework for Mac, which doesn't contain any iOS binaries. This offers an option for developers who need only Mac binaries using a smaller XCframework package.\n\nMicrosoft Audio Stack (MAS):\n\nWhen beam-forming angles are specified, sound originating outside of specified range will be suppressed better.\n\nApproximately 70% reduction in the size of libMicrosoft.CognitiveServices.Speech.extension.mas.so for Linux ARM32 and Linux ARM64.\n\nIntent Recognition using pattern matching:\n\nAdd orthography support for the languages fr, de, es, jp\n\nAdded prebuilt integer support for language es.\n\nBug fixes\n\niOS: fix speech synthesis error on iOS 16 caused by compressed audio decoding failure (Customer Issue).\n\nJavaScript:\n\nFix authentication token not working when getting speech synthesis voice list (Customer issue).\n\nUse data URL for worker loading (Customer issue).\n\nCreate audio processor worklet only when AudioWorklet is supported in browser (Customer issue). This was a community contribution by William Wong. Thank you William!\n\nFix recognized callback when LUIS response connectionMessage is empty (Customer issue).\n\nProperly set speech segmentation timeout.\n\nIntent Recognition using pattern matching:\n\nNon-json characters inside models will now load properly.\n\nFix hanging issue when recognizeOnceAsync(text) was called during continuous recognition.\n\nSpeech SDK 1.23.0: July 2022 release\n\nNew features\n\nC#, C++, Java: Added support for languages zh-cn and zh-hk in Intent Recognition with Pattern Matching.\n\nC#: Added support for AnyCPU .NET Framework builds\n\nBug fixes\n\nAndroid: Fixed OpenSSL vulnerability CVE-2022-2068 by updating OpenSSL to 1.1.1q\n\nPython: Fix crash when using PushAudioInputStream\n\niOS: Fix \"EXC_BAD_ACCESS: Attempted to dereference null pointer\" as reported on iOS (GitHub issue)\n\nSpeech SDK 1.22.0: June 2022 release\n\nNew features\n\nJava: IntentRecognitionResult API for getEntities(), applyLanguageModels(), and recognizeOnceAsync(text) added to support the \"simple pattern matching\" engine.\n\nUnity: Added support for Mac M1 (Apple Silicon) for Unity package (GitHub issue)\n\nC#: Added support for x86_64 for Xamarin Android (GitHub issue)\n\nC#: .NET framework minimum version updated to v4.6.2 for SDK C# package as v4.6.1 has retired (see Microsoft .NET Framework Component Lifecycle Policy)\n\nLinux: Added support for Debian 11 and Ubuntu 22.04 LTS. Ubuntu 22.04 LTS requires manual installation of libssl1.1 either as a binary package from here (for example, libssl1.1_1.1.1l-1ubuntu1.3_amd64.deb or newer for x64), or by compiling from sources.\n\nBug fixes\n\nUWP: OpenSSL dependency removed from UWP libraries and replaced with WinRT websocket and HTTP APIs to meet security compliance and smaller binary footprint.\n\nMac: Fixed \"MicrosoftCognitiveServicesSpeech Module Not Found\" issue when using Swift projects targeting macOS platform\n\nWindows, Mac: Fixed a platform-specific issue where audio sources that were configured via properties to stream at a real-time rate sometimes fell behind and eventually exceeded capacity\n\nSamples (GitHub)\n\nC#: .NET framework samples updated to use v4.6.2\n\nUnity: Virtual-assistant sample fixed for Android and UWP\n\nUnity: Unity samples updated for Unity 2020 LTS version\n\nSpeech SDK 1.21.0: April 2022 release\n\nNew features\n\nJava & JavaScript: Added support for Continuous Language Identification when using the SpeechRecognizer object\n\nJavaScript: Added Diagnostics APIs to enable console logging level and (Node only) file logging, to help Microsoft troubleshoot customer-reported issues\n\nPython: Added support for Conversation Transcription\n\nGo: Added support for Speaker Recognition\n\nC++ & C#: Added support for a required group of words in the Intent Recognizer (simple pattern matching). For example: \"(set|start|begin) a timer\" where either \"set\", \"start\" or \"begin\" must be present for the intent to be recognized.\n\nAll programming languages, Speech Synthesis: Added duration property in word boundary events. Added support for punctuation boundary and sentence boundary\n\nObjective-C/Swift/Java: Added word-level results on the Pronunciation Assessment result object (similar to C#). The application no longer needs to parse a JSON result string to get word-level information (GitHub issue)\n\niOS platform: Added experimental support for ARMv7 architecture\n\nBug fixes\n\niOS platform: Fix to allow building for the target \"Any iOS Device\", when using CocoaPod (GitHub issue)\n\nAndroid platform: OpenSSL version has been updated to 1.1.1n to fix security vulnerability CVE-2022-0778\n\nJavaScript: Fix issue where wav header wasn't updated with file size (GitHub issue)\n\nJavaScript: Fix request ID desync issue breaking translation scenarios (GitHub issue)\n\nJavaScript: Fix issue when instantiating SpeakerAudioDestination with no stream (GitHub issue]\n\nC++: Fix C++ headers to remove a warning when compiling for C++17 or newer\n\nSamples GitHub\n\nNew Java samples for Speech Recognition with Language Identification\n\nNew Python and Java samples for Conversation Transcription\n\nNew Go sample for Speaker Recognition\n\nNew C++ and C# tool for Windows that enumerates all audio capture and render devices, for finding their Device ID. This ID is needed by the Speech SDK if you plan to capture audio from, or render audio to, a non-default device.\n\nSpeech SDK 1.20.0: January 2022 release\n\nNew features\n\nObjective-C, Swift, and Python: Added support for DialogServiceConnector, used for Voice-Assistant scenarios.\n\nPython: Support for Python 3.10 was added. Support for Python 3.6 was removed, per Python's end-of-life for 3.6.\n\nUnity: Speech SDK is now supported for Unity applications on Linux.\n\nC++, C#: IntentRecognizer using pattern matching is now supported in C#. In addition, scenarios with custom entities, optional groups, and entity roles are now supported in C++ and C#.\n\nC++, C#: Improved diagnostics trace logging using new classes FileLogger, MemoryLogger, and EventLogger. SDK logs are an important tool for Microsoft to diagnose customer-reported issues. These new classes make it easier for customers to integrate Speech SDK logs into their own logging system.\n\nAll programming languages: PronunciationAssessmentConfig now has properties to set the desired phoneme alphabet (IPA or SAPI) and N-Best Phoneme Count (avoiding the need to author a configuration JSON as per GitHub issue 1284). Also, syllable level output is now supported.\n\nAndroid, iOS, and macOS (all programming languages): GStreamer is no longer needed to support limited-bandwidth networks. SpeechSynthesizer now uses the operating system's audio decoding capabilities to decode compressed audio streamed from the text to speech service.\n\nAll programming languages: SpeechSynthesizer now supports three new raw output Opus formats (without container), which are widely used in live streaming scenarios.\n\nJavaScript: Added getVoicesAsync() API to SpeechSynthesizer to retrieve the list of supported synthesis voices (GitHub issue 1350)\n\nJavaScript: Added getWaveFormat() API to AudioStreamFormat to support non-PCM wave formats (GitHub issue 452)\n\nJavaScript: Added volume getter/setter and mute()/unmute() APIs to SpeakerAudioDestination (GitHub issue 463)\n\nBug fixes\n\nC++, C#, Java, JavaScript, Objective-C, and Swift: Fix to remove a 10-second delay while stopping a speech recognizer that uses a PushAudioInputStream. This is for the case where no new audio is pushed in after StopContinuousRecognition is called (GitHub issues 1318, 331)\n\nUnity on Android and UWP: Unity meta files were fixed for UWP, Android ARM64, and Windows Subsystem for Android (WSA) ARM64 (GitHub issue 1360)\n\niOS: Compiling your Speech SDK application on any iOS Device when using CocoaPods is now fixed (GitHub issue 1320)\n\niOS: When SpeechSynthesizer is configured to output audio directly to a speaker, playback stopped at the beginning in rare conditions. This was fixed.\n\nJavaScript: Use script processor fallback for microphone input if no audio worklet is found (GitHub issue 455)\n\nJavaScript: Add protocol to agent to mitigate bug found with Sentry integration (GitHub issue 465)\n\nSamples GitHub\n\nC++, C#, Python, and Java samples showing how to get detailed recognition results. The details include alternative recognition results, confidence score, Lexical form, Normalized form, Masked Normalized form, with word-level timing for each.\n\niOS sample added using AVFoundation as external audio source.\n\nJava sample added to show how to get SRT (SubRip Text) format using WordBoundary event.\n\nAndroid samples for Pronunciation Assessment.\n\nC++, C# showing usage of the new Diagnostics Logging classes.\n\nSpeech SDK 1.19.0: 2021-Nov release\n\nHighlights\n\nSpeaker Recognition service is generally available (GA) now. Speech SDK APIs are available on C++, C#, Java, and JavaScript. With Speaker Recognition, you can accurately verify and identify speakers by their unique voice characteristics. For more information about this topic, see the documentation.\n\nWe've dropped support for Ubuntu 16.04 in conjunction with Azure DevOps and GitHub. Ubuntu 16.04 reached end of life back in April of 2021. Migrate your Ubuntu 16.04 workflows to Ubuntu 18.04 or newer.\n\nOpenSSL linking in Linux binaries changed to dynamic. Linux binary size has been reduced by about 50%.\n\nMac M1 ARM-based silicon support added.\n\nNew features\n\nC++/C#/Java: New APIs added to enable audio processing support for speech input with Microsoft Audio Stack. Documentation here.\n\nC++: New APIs for intent recognition to facilitate more advanced pattern matching. This includes List and Prebuilt Integer entities as well as support for grouping intents and entities as models (Documentation, updates, and samples are under development and will be published in the near future).\n\nMac: Support for ARM64 (M1) based silicon for CocoaPod, Python, Java, and NuGet packages related to GitHub issue 1244.\n\niOS/Mac: iOS and macOS binaries are now packaged into xcframework related to GitHub issue 919.\n\niOS/Mac: Support for Mac catalyst related to GitHub issue 1171.\n\nLinux: New tar package added for CentOS7 About the Speech SDK. The Linux .tar package now contains specific libraries for RHEL/CentOS 7 in lib/centos7-x64. Speech SDK libraries in lib/x64 are still applicable for all the other supported Linux x64 distributions (including RHEL/CentOS 8) and won't work on RHEL/CentOS 7.\n\nJavaScript: VoiceProfile & SpeakerRecognizer APIs made async/awaitable.\n\nJavaScript: Support added for US government Azure regions.\n\nWindows: Support added for playback on Universal Windows Platform (UWP).\n\nBug fixes\n\nAndroid: OpenSSL security update (updated to version 1.1.1l) for Android packages.\n\nPython: Resolved bug where selecting speaker device on Python fails.\n\nCore: Automatically reconnect when a connection attempt fails.\n\niOS: Audio compression disabled on iOS packages due instability and bitcode build problems when using GStreamer. Details are available via GitHub issue 1209.\n\nSamples GitHub\n\nMac/iOS: Updated samples and quickstarts to use xcframework package.\n\n.NET: Samples updated to use .NET core 3.1 version.\n\nJavaScript: Added sample for Voice Assistants.\n\nSpeech SDK 1.18.0: 2021-July release\n\nNote: Get started with the Speech SDK here.\n\nHighlights summary\n\nUbuntu 16.04 reached end of life in April of 2021. With Azure DevOps and GitHub, we'll drop support for 16.04 in September 2021. Migrate ubuntu-16.04 workflows to ubuntu-18.04 or newer before then.\n\nNew features\n\nC++: Simple Language Pattern matching with the Intent Recognizer now makes it easier to implement simple intent recognition scenarios.\n\nC++/C#/Java: We added a new API, GetActivationPhrasesAsync() to the VoiceProfileClient class for receiving a list of valid activation phrases in Speaker Recognition enrollment phase for independent recognition scenarios.\n\nImportant: The Speaker Recognition feature is in Preview. All voice profiles created in Preview will be discontinued 90 days after the Speaker Recognition feature is moved out of Preview into General Availability. At that point the Preview voice profiles will stop functioning.\n\nPython: Added support for continuous Language Identification (LID) on the existing SpeechRecognizer and TranslationRecognizer objects.\n\nPython: Added a new Python object named SourceLanguageRecognizer to do one-time or continuous LID (without recognition or translation).\n\nJavaScript: getActivationPhrasesAsync API added to VoiceProfileClient class for receiving a list of valid activation phrases in Speaker Recognition enrollment phase for independent recognition scenarios.\n\nJavaScript VoiceProfileClient's enrollProfileAsync API is now async awaitable. See this independent identification code, for example, usage.\n\nImprovements\n\nJava: AutoCloseable support added to many Java objects. Now the try-with-resources model is supported to release resources. See this sample that uses try-with-resources. Also see the Oracle Java documentation tutorial for The try-with-resources Statement to learn about this pattern.\n\nDisk footprint has been significantly reduced for many platforms and architectures. Examples for the Microsoft.CognitiveServices.Speech.core binary: x64 Linux is 475KB smaller (8.0% reduction); ARM64 Windows UWP is 464KB smaller (11.5% reduction); x86 Windows is 343KB smaller (17.5% reduction); and x64 Windows is 451KB smaller (19.4% reduction).\n\nBug fixes\n\nJava: Fixed synthesis error when the synthesis text contains surrogate characters. Details here.\n\nJavaScript: Browser microphone audio processing now uses AudioWorkletNode instead of deprecated ScriptProcessorNode. Details here.\n\nJavaScript: Correctly keep conversations alive during long running conversation translation scenarios. Details here.\n\nJavaScript: Fixed issue with recognizer reconnecting to a mediastream in continuous recognition. Details here.\n\nJavaScript: Fixed issue with recognizer reconnecting to a pushStream in continuous recognition. Details here.\n\nJavaScript: Corrected word level offset calculation in detailed recognition results. Details here.\n\nSamples\n\nJava quickstart samples updated here.\n\nJavaScript Speaker Recognition samples updated to show new usage of enrollProfileAsync(). See samples here.\n\nSpeech SDK 1.17.0: 2021-May release\n\nNote\n\nGet started with the Speech SDK here.\n\nHighlights summary\n\nSmaller footprint - we continue to decrease the memory and disk footprint of the Speech SDK and its components.\n\nA new stand-alone Language Identification API allows you to recognize what language is being spoken.\n\nDevelop speech enabled mixed reality and gaming applications using Unity on macOS.\n\nYou can now use Text to speech in addition to speech recognition from the Go programming language.\n\nSeveral Bug fixes to address issues YOU, our valued customers, have flagged on GitHub! THANK YOU! Keep the feedback coming!\n\nNew features\n\nC++/C#: New stand-alone At-Start and Continuous Language Detection via the SourceLanguageRecognizer API. If you only want to detect the language(s) spoken in audio content, this is the API to do that. See details for C++ and C#.\n\nC++/C#: Speech Recognition and Translation Recognition now support both at-start and continuous Language Identification so you can programmatically determine which language(s) are being spoken before they're transcribed or translated. See documentation here for Speech Recognition and here for Speech Translation.\n\nC#: Added support Unity support to macOS (x64). This unlocks speech recognition and speech synthesis use cases in mixed reality and gaming!\n\nGo: We added support for speech synthesis text to speech to the Go programming language to make speech synthesis available in even more use cases. See our quickstart or our reference documentation.\n\nC++/C#/Java/Python/Objective-C/Go: The speech synthesizer now supports the connection object. This helps you manage and monitor the connection to the Speech service, and is especially helpful to pre-connect to reduce latency. See documentation here.\n\nC++/C#/Java/Python/Objective-C/Go: We now expose the latency and underrun time in SpeechSynthesisResult to help you monitor and diagnose speech synthesis latency issues. See details for C++, C#, Java, Python, Objective-C and Go.\n\nC++/C#/Java/Python/Objective-C: Text to speech now uses neural voices by default when you don't specify a voice to be used. This gives you higher fidelity output by default, but also increases the default price. You can specify any of our over 70 standard voices or over 130 neural voices to change the default.\n\nC++/C#/Java/Python/Objective-C/Go: We added a Gender property to the synthesis voice info to make it easier to select voices based on gender. This addresses GitHub issue #1055.\n\nC++, C#, Java, JavaScript: We now support retrieveEnrollmentResultAsync, getAuthorizationPhrasesAsync, and getAllProfilesAsync() in Speaker Recognition to ease user management of all voice profiles for a given account. See documentation for C++, C#, Java, JavaScript. This addresses GitHub issue #338.\n\nJavaScript: We added retry for connection failures that will make your JavaScript-based speech applications more robust.\n\nImprovements\n\nLinux and Android Speech SDK binaries have been updated to use the latest version of OpenSSL (1.1.1k)\n\nCode Size improvements:\n\nLanguage Understanding is now split into a separate \"lu\" library.\n\nWindows x64 core binary size decreased by 14.4%.\n\nAndroid ARM64 core binary size decreased by 13.7%.\n\nother components also decreased in size.\n\nBug fixes\n\nAll: Fixed GitHub issue #842 for ServiceTimeout. You can now transcribe long audio files using the Speech SDK without the connection to the service terminating with this error. However, we still recommend you use batch transcription for long files.\n\nC#: Fixed GitHub issue #947 where no speech input could leave your app in a bad state.\n\nJava: Fixed GitHub Issue #997 where the Speech SDK for Java 1.16 crashes when using DialogServiceConnector without a network connection or an invalid subscription key.\n\nFixed a crash when abruptly stopping speech recognition (for example, using CTRL+C on console app).\n\nJava: Added a fix to delete temporary files on Windows when using Speech SDK for Java.\n\nJava: Fixed GitHub issue #994 where calling DialogServiceConnector.stopListeningAsync could result in an error.\n\nJava: Fixed a customer issue in the virtual assistant quickstart.\n\nJavaScript: Fixed GitHub issue #366 where ConversationTranslator threw an error 'this.cancelSpeech isn't a function'.\n\nJavaScript: Fixed GitHub issue #298 where 'Get result as an in-memory stream' sample played sound out loud.\n\nJavaScript: Fixed GitHub issue #350 where calling AudioConfig could result in a 'ReferenceError: MediaStream isn't defined'.\n\nJavaScript: Fixed an UnhandledPromiseRejection warning in Node.js for long-running sessions.\n\nSamples\n\nUpdated Unity samples documentation for macOS here.\n\nA React Native sample for the Azure AI Speech recognition service is now available here.\n\nSpeech SDK 1.16.0: 2021-March release\n\nNote\n\nThe Speech SDK on Windows depends on the shared Microsoft Visual C++ Redistributable for Visual Studio 2015, 2017 and 2019. Download it here.\n\nNew features\n\nC++/C#/Java/Python: Moved to the latest version of GStreamer (1.18.3) to add support for transcribing any media format on Windows, Linux, and Android. See documentation here.\n\nC++/C#/Java/Objective-C/Python: Added support for decoding compressed TTS/synthesized audio to the SDK. If you set output audio format to PCM and GStreamer is available on your system, the SDK will automatically request compressed audio from the service to save bandwidth and decode the audio on the client. You can set SpeechServiceConnection_SynthEnableCompressedAudioTransmission to false to disable this feature. Details for C++, C#, Java, Objective-C, Python.\n\nJavaScript: Node.js users can now use the AudioConfig.fromWavFileInput API. This addresses GitHub issue #252.\n\nC++/C#/Java/Objective-C/Python: Added GetVoicesAsync() method for TTS to return all available synthesis voices. Details for C++, C#, Java, Objective-C, and Python.\n\nC++/C#/Java/JavaScript/Objective-C/Python: Added VisemeReceived event for TTS/speech synthesis to return synchronous viseme animation. See documentation here.\n\nC++/C#/Java/JavaScript/Objective-C/Python: Added BookmarkReached event for TTS. You can set bookmarks in the input SSML and get the audio offsets for each bookmark. See documentation here.\n\nJava: Added support for Speaker Recognition APIs. Details here.\n\nC++/C#/Java/JavaScript/Objective-C/Python: Added two new output audio formats with WebM container for TTS (Webm16Khz16BitMonoOpus and Webm24Khz16BitMonoOpus). These are better formats for streaming audio with the Opus codec. Details for C++, C#, Java, JavaScript, Objective-C, Python.\n\nC++/C#/Java: Added support for retrieving voice profile for Speaker Recognition scenario. Details for C++, C#, and Java.\n\nC++/C#/Java/Objective-C/Python: Added support for separate shared library for audio microphone and speaker control. This allows the developer to use the SDK in environments that don't have required audio library dependencies.\n\nObjective-C/Swift: Added support for module framework with umbrella header. This allows the developer to import Speech SDK as a module in iOS/Mac Objective-C/Swift apps. This addresses GitHub issue #452.\n\nPython: Added support for Python 3.9 and dropped support for Python 3.5 per Python's end-of-life for 3.5.\n\nKnown issues\n\nC++/C#/Java: DialogServiceConnector can't use a CustomCommandsConfig to access a Custom Commands application and will instead encounter a connection error. This can be worked around by manually adding your application ID to the request with config.SetServiceProperty(\"X-CommandsAppId\", \"your-application-id\", ServicePropertyChannel.UriQueryParameter). The expected behavior of CustomCommandsConfig will be restored in the next release.\n\nImprovements\n\nAs part of our multi-release effort to reduce the Speech SDK's memory usage and disk footprint, Android binaries are now 3% to 5% smaller.\n\nImproved accuracy, readability, and see-also sections of our C# reference documentation here.\n\nBug fixes\n\nJavaScript: Large WAV file headers are now parsed correctly (increases header slice to 512 bytes). This addresses GitHub issue #962.\n\nJavaScript: Corrected microphone timing issue if mic stream ends before stop recognition, addressing an issue with Speech Recognition not working in Firefox.\n\nJavaScript: We now correctly handle initialization promise when the browser forces mic off before turnOn completes.\n\nJavaScript: We replaced URL dependency with url-parse. This addresses GitHub issue #264.\n\nAndroid: Fixed callbacks not working when minifyEnabled is set to true.\n\nC++/C#/Java/Objective-C/Python: TCP_NODELAY will be correctly set to underlying socket IO for TTS to reduce latency.\n\nC++/C#/Java/Python/Objective-C/Go: Fixed an occasional crash when the recognizer was destroyed just after starting a recognition.\n\nC++/C#/Java: Fixed an occasional crash in the destruction of speaker recognizer.\n\nSamples\n\nJavaScript: Browser samples no longer require separate JavaScript library file download.\n\nSpeech SDK 1.15.0: 2021-January release\n\nNote\n\nThe Speech SDK on Windows depends on the shared Microsoft Visual C++ Redistributable for Visual Studio 2015, 2017 and 2019. Download it here.\n\nHighlights summary\n\nSmaller memory and disk footprint making the SDK more efficient.\n\nHigher fidelity output formats available for custom-neural voice private preview.\n\nIntent Recognizer can now get return more than the top intent, giving you the ability to make a separate assessment about your customer's intent.\n\nVoice assistants and bots are now easier to set up, and you can make it stop listening immediately, and exercise greater control over how it responds to errors.\n\nImproved on device performance through making compression optional.\n\nUse the Speech SDK on Windows ARM/ARM64.\n\nImproved low-level debugging.\n\nPronunciation Assessment feature is now more widely available.\n\nSeveral Bug fixes to address issues YOU, our valued customers, have flagged on GitHub! THANK YOU! Keep the feedback coming!\n\nImprovements\n\nThe Speech SDK is now more efficient and lightweight. We've started a multi-release effort to reduce the Speech SDK's memory usage and disk footprint. As a first step we made significant file size reductions in shared libraries on most platforms. Compared to the 1.14 release:\n\n64-bit UWP-compatible Windows libraries are about 30% smaller.\n\n32-bit Windows libraries aren't yet seeing a size improvement.\n\nLinux libraries are 20-25% smaller.\n\nAndroid libraries are 3-5% smaller.\n\nNew features\n\nAll: New 48 KHz output formats available for the private preview of custom-neural voice through the TTS speech synthesis API: Audio48Khz192KBitRateMonoMp3, audio-48khz-192kbitrate-mono-mp3, Audio48Khz96KBitRateMonoMp3, audio-48khz-96kbitrate-mono-mp3, Raw48Khz16BitMonoPcm, raw-48khz-16bit-mono-pcm, Riff48Khz16BitMonoPcm, riff-48khz-16bit-mono-pcm.\n\nAll: Custom voice is also easier to use. Added support for setting custom voice via EndpointId (C++, C#, Java, JavaScript, Objective-C, Python). Before this change, custom voice users needed to set the endpoint URL via the FromEndpoint method. Now customers can use the FromSubscription method just like prebuilt voices, and then provide the deployment ID by setting EndpointId. This simplifies setting up custom voices.\n\nC++/C#/Java/Objective-C/Python: Get more than the top intent fromIntentRecognizer. It now supports configuring the JSON result containing all intents and not only the top scoring intent via LanguageUnderstandingModel FromEndpoint method by using verbose=true uri parameter. This addresses GitHub issue #880. See updated documentation here.\n\nC++/C#/Java: Make your voice assistant or bot stop listening immediately. DialogServiceConnector (C++, C#, Java) now has a StopListeningAsync() method to accompany ListenOnceAsync(). This will immediately stop audio capture and gracefully wait for a result, making it perfect for use with \"stop now\" button-press scenarios.\n\nC++/C#/Java/JavaScript: Make your voice assistant or bot react better to underlying system errors. DialogServiceConnector (C++, C#, Java, JavaScript) now has a new TurnStatusReceived event handler. These optional events correspond to every ITurnContext resolution on the Bot and will report turn execution failures when they happen, for example, as a result of an unhandled exception, timeout, or network drop between Direct Line Speech and the bot. TurnStatusReceived makes it easier to respond to failure conditions. For example, if a bot takes too long on a backend database query (for example, looking up a product), TurnStatusReceived allows the client to know to reprompt with \"sorry, I didn't quite get that, could you please try again\" or something similar.\n\nC++/C#: Use the Speech SDK on more platforms. The Speech SDK NuGet package now supports Windows ARM/ARM64 desktop native binaries (UWP was already supported) to make the Speech SDK more useful on more machine types.\n\nJava: DialogServiceConnector now has a setSpeechActivityTemplate() method that was unintentionally excluded from the language previously. This is equivalent to setting the Conversation_Speech_Activity_Template property and will request that all future Bot Framework activities originated by the Direct Line Speech service merge the provided content into their JSON payloads.\n\nJava: Improved low-level debugging. The Connection class now has a MessageReceived event, similar to other programming languages (C++, C#). This event provides low-level access to incoming data from the service and can be useful for diagnostics and debugging.\n\nJavaScript: Easier setup for Voice Assistants and bots through BotFrameworkConfig, which now has fromHost() and fromEndpoint() factory methods that simplify the use of custom service locations versus manually setting properties. We also standardized optional specification of botId to use a non-default bot across the configuration factories.\n\nJavaScript: Improved on device performance through added string control property for websocket compression. For performance reasons, we disabled websocket compression by default. This can be reenabled for low-bandwidth scenarios. More details here. This addresses GitHub issue #242.\n\nJavaScript: Added support for lPronunciation Assessment to enable evaluation of speech pronunciation. See the quickstart here.\n\nBug fixes\n\nAll (except JavaScript): Fixed a regression in version 1.14, in which too much memory was allocated by the recognizer.\n\nC++: Fixed a garbage collection issue with DialogServiceConnector, addressing GitHub issue #794.\n\nC#: Fixed an issue with thread shutdown that caused objects to block for about a second when disposed.\n\nC++/C#/Java: Fixed an exception preventing an application from setting speech authorization token or activity template more than once on a DialogServiceConnector.\n\nC++/C#/Java: Fixed a recognizer crash due to a race condition in teardown.\n\nJavaScript: DialogServiceConnector didn't previously honor the optional botId parameter specified in BotFrameworkConfig's factories. This made it necessary to set the botId query string parameter manually to use a non-default bot. The bug has been corrected and botId values provided to BotFrameworkConfig's factories will be honored and used, including the new fromHost() and fromEndpoint() additions. This also applies to the applicationId parameter for CustomCommandsConfig.\n\nJavaScript: Fixed GitHub issue #881, allowing recognizer object reusage.\n\nJavaScript: Fixed an issue where the SKD was sending speech.config multiple times in one TTS session, wasting bandwidth.\n\nJavaScript: Simplified error handling on microphone authorization, allowing more descriptive message to bubble up when user hasn't allowed microphone input on their browser.\n\nJavaScript: Fixed GitHub issue #249 where type errors in ConversationTranslator and ConversationTranscriber caused a compilation error for TypeScript users.\n\nObjective-C: Fixed an issue where GStreamer build failed for iOS on Xcode 11.4, addressing GitHub issue #911.\n\nPython: Fixed GitHub issue #870, removing \"DeprecationWarning: the imp module is deprecated in favor of importlib\".\n\nSamples\n\nFrom-file sample for JavaScript browser now uses files for speech recognition. This addresses GitHub issue #884.\n\nSpeech SDK 1.14.0: 2020-October release\n\nNote\n\nThe Speech SDK on Windows depends on the shared Microsoft Visual C++ Redistributable for Visual Studio 2015, 2017 and 2019. Download it here.\n\nNew features\n\nLinux: Added support for Debian 10 and Ubuntu 20.04 LTS.\n\nPython/Objective-C: Added support for the KeywordRecognizer API. Documentation will be here.\n\nC++/Java/C#: Added support to set any HttpHeader key/value via ServicePropertyChannel::HttpHeader.\n\nJavaScript: Added support for the ConversationTranscriber API. Read documentation here.\n\nC++/C#: Added new AudioDataStream FromWavFileInput method (to read .WAV files) here (C++) and here (C#).\n\nC++/C#/Java/Python/Objective-C/Swift: Added a stopSpeakingAsync() method to stop text to speech synthesis. Read the Reference documentation here (C++), here (C#), here (Java), here (Python), and here (Objective-C/Swift).\n\nC#, C++, Java: Added a FromDialogServiceConnector() function to the Connection class that can be used to monitor connection and disconnection events for DialogServiceConnector. Read the Reference documentation here (C#), here (C++), and here (Java).\n\nC++/C#/Java/Python/Objective-C/Swift: Added support for Pronunciation Assessment, which evaluates speech pronunciation and gives speakers feedback on the accuracy and fluency of spoken audio. Read the documentation here.\n\nBreaking change\n\nJavaScript: PullAudioOutputStream.read() has a return type change from an internal Promise to a Native JavaScript Promise.\n\nBug fixes\n\nAll: Fixed 1.13 regression in SetServiceProperty where values with certain special characters were ignored.\n\nC#: Fixed Windows console samples on Visual Studio 2019 failing to find native DLLs.\n\nC#: Fixed crash with memory management if stream is used as KeywordRecognizer input.\n\nObjectiveC/Swift: Fixed crash with memory management if stream is used as recognizer input.\n\nWindows: Fixed coexistence issue with BT HFP/A2DP on UWP.\n\nJavaScript: Fixed mapping of session IDs to improve logging and aid in internal debug/service correlations.\n\nJavaScript: Added fix for DialogServiceConnector disabling ListenOnce calls after the first call is made.\n\nJavaScript: Fixed issue where result output would only ever be \"simple\".\n\nJavaScript: Fixed continuous recognition issue in Safari on macOS.\n\nJavaScript: CPU load mitigation for high request throughput scenario.\n\nJavaScript: Allow access to details of Voice Profile Enrollment result.\n\nJavaScript: Added fix for continuous recognition in IntentRecognizer.\n\nC++/C#/Java/Python/Swift/ObjectiveC: Fixed incorrect url for australiaeast and brazilsouth in IntentRecognizer.\n\nC++/C#: Added VoiceProfileType as an argument when creating a VoiceProfile object.\n\nC++/C#/Java/Python/Swift/ObjectiveC: Fixed potential SPX_INVALID_ARG when trying to read AudioDataStream from a given position.\n\nIOS: Fixed crash with speech recognition on Unity\n\nSamples\n\nObjectiveC: Added sample for keyword recognition here.\n\nC#/JavaScript: Added quickstart for conversation transcription here (C#) and here (JavaScript).\n\nC++/C#/Java/Python/Swift/ObjectiveC: Added sample for Pronunciation Assessment here\n\nXamarin: Updated quickstart to latest Visual Studio template here.\n\nKnown Issue\n\nDigiCert Global Root G2 certificate isn't supported by default in HoloLens 2 and Android 4.4 (KitKat) and needs to be added to the system to make the Speech SDK functional. The certificate will be added to HoloLens 2 OS images in the near future. Android 4.4 customers need to add the updated the certificate to the system.\n\nCOVID-19 abridged testing\n\nDue to working remotely over the last few weeks, we couldn't do as much manual verification testing as we normally do. We haven't made any changes we think could have broken anything, and our automated tests all passed. In the unlikely event that we missed something, please let us know on GitHub.\n\nStay healthy!\n\nSpeech SDK 1.13.0: 2020-July release\n\nNote\n\nThe Speech SDK on Windows depends on the shared Microsoft Visual C++ Redistributable for Visual Studio 2015, 2017 and 2019. Download and install it from here.\n\nNew features\n\nC#: Added support for asynchronous conversation transcription. See documentation here.\n\nJavaScript: Added Speaker Recognition support for both browser and Node.js.\n\nJavaScript: Added support for Language Identification/language ID. See documentation here.\n\nObjective-C: Added support for multi-device conversation and conversation transcription.\n\nPython: Added compressed audio support for Python on Windows and Linux. See documentation here.\n\nBug fixes\n\nAll: Fixed an issue that caused the KeywordRecognizer to not move forward the streams after a recognition.\n\nAll: Fixed an issue that caused the stream obtained from a KeywordRecognitionResult to not contain the keyword.\n\nAll: Fixed an issue that the SendMessageAsync doesn't really send the message over the wire after the users finish waiting for it.\n\nAll: Fixed a crash in Speaker Recognition APIs when users call VoiceProfileClient::SpeakerRecEnrollProfileAsync method multiple times and didn't wait for the calls to finish.\n\nAll: Fixed enable file logging in VoiceProfileClient and SpeakerRecognizer classes.\n\nJavaScript: Fixed an issue with throttling when browser is minimized.\n\nJavaScript: Fixed an issue with a memory leak on streams.\n\nJavaScript: Added caching for OCSP responses from NodeJS.\n\nJava: Fixed an issue that was causing BigInteger fields to always return 0.\n\niOS: Fixed an issue with publishing Speech SDK-based apps in the iOS App Store.\n\nSamples\n\nC++: Added sample code for Speaker Recognition here.\n\nCOVID-19 abridged testing\n\nDue to working remotely over the last few weeks, we couldn't do as much manual verification testing as we normally do. We haven't made any changes we think could have broken anything, and our automated tests all passed. In the unlikely event that we missed something, please let us know on GitHub.\n\nStay healthy!\n\nSpeech SDK 1.12.1: 2020-June release\n\nNew features\n\nC#, C++: Speaker Recognition Preview: This feature enables speaker identification (who is speaking?) and speaker verification (is the speaker who they claim to be?). Start with an overview, read the Speaker Recognition basics article, or the API reference docs.\n\nBug fixes\n\nC#, C++: Fixed microphone recording wasn't working in 1.12 in Speaker Recognition.\n\nJavaScript: Fixes for Text to speech in Firefox, and Safari on macOS and iOS.\n\nFix for Windows application verifier access violation crash on conversation transcription when using eight-channel stream.\n\nFix for Windows application verifier access violation crash on multi-device conversation translation.\n\nSamples\n\nC#: Code sample for Speaker Recognition.\n\nC++: Code sample for Speaker Recognition.\n\nJava: Code sample for intent recognition on Android.\n\nCOVID-19 abridged testing\n\nDue to working remotely over the last few weeks, we couldn't do as much manual verification testing as we normally do. We haven't made any changes we think could have broken anything, and our automated tests all passed. In the unlikely event that we missed something, please let us know on GitHub.\n\nStay healthy!\n\nSpeech SDK 1.12.0: 2020-May release\n\nNew features\n\nGo: New Go language support for Speech Recognition and custom voice assistant. Set up your dev environment here. For sample code, see the Samples section below.\n\nJavaScript: Added Browser support for text to speech. See documentation here.\n\nC++, C#, Java: New KeywordRecognizer object and APIs supported on Windows, Android, Linux & iOS platforms. Read the documentation here. For sample code, see the Samples section below.\n\nJava: Added multi-device conversation with translation support. See the reference doc here.\n\nImprovements & Optimizations\n\nJavaScript: Optimized browser microphone implementation improving speech recognition accuracy.\n\nJava: Refactored bindings using direct JNI implementation without SWIG. This change reduces by 10x the bindings size for all Java packages used for Windows, Android, Linux, and Mac and eases further development of the Speech SDK Java implementation.\n\nLinux: Updated support documentation with the latest RHEL 7 specific notes.\n\nImproved connection logic to attempt connecting multiple times when service and network errors occur.\n\nUpdated the portal.azure.com Speech Quickstart page to help developers take the next step in the Azure AI Speech journey.\n\nBug fixes\n\nC#, Java: Fixed an issue with loading SDK libraries on Linux ARM (both 32 bit and 64 bit).\n\nC#: Fixed explicit disposal of native handles for TranslationRecognizer, IntentRecognizer, and Connection objects.\n\nC#: Fixed audio input lifetime management for ConversationTranscriber object.\n\nFixed an issue where IntentRecognizer result reason wasn't set properly when recognizing intents from simple phrases.\n\nFixed an issue where SpeechRecognitionEventArgs result offset wasn't set correctly.\n\nFixed a race condition where SDK was trying to send a network message before opening the websocket connection. Was reproducible for TranslationRecognizer while adding participants.\n\nFixed memory leaks in the keyword recognizer engine.\n\nSamples\n\nGo: Added quickstarts for speech recognition and custom voice assistant. Find sample code here.\n\nJavaScript: Added quickstarts for Text to speech, Translation, and Intent Recognition.\n\nKeyword recognition samples for C# and Java (Android).\n\nCOVID-19 abridged testing\n\nDue to working remotely over the last few weeks, we couldn't do as much manual verification testing as we normally do. We haven't made any changes we think could have broken anything, and our automated tests all passed. If we missed something, please let us know on GitHub.\n\nStay healthy!\n\nSpeech SDK 1.11.0: 2020-March release\n\nNew features\n\nLinux: Added support for Red Hat Enterprise Linux (RHEL)/CentOS 7 x64.\n\nLinux: Added support for .NET Core C# on Linux ARM32 and ARM64. Read more here.\n\nC#, C++: Added UtteranceId in ConversationTranscriptionResult, a consistent ID across all the intermediates and final speech recognition result. Details for C#, C++.\n\nPython: Added support for Language ID. See speech_sample.py in GitHub repo.\n\nWindows: Added compressed audio input format support on Windows platform for all the win32 console applications. Details here.\n\nJavaScript: Support speech synthesis (text to speech) in NodeJS. Learn more here.\n\nJavaScript: Add new APIs to enable inspection of all send and received messages. Learn more here.\n\nBug fixes\n\nC#, C++: Fixed an issue so SendMessageAsync now sends binary message as binary type. Details for C#, C++.\n\nC#, C++: Fixed an issue where using Connection MessageReceived event may cause crash if Recognizer is disposed before Connection object. Details for C#, C++.\n\nAndroid: Audio buffer size from microphone decreased from 800 ms to 100 ms to improve latency.\n\nAndroid: Fixed an issue with x86 Android emulator in Android Studio.\n\nJavaScript: Added support for Regions in China with the fromSubscription API. Details here.\n\nJavaScript: Add more error information for connection failures from NodeJS.\n\nSamples\n\nUnity: Intent recognition public sample is fixed, where LUIS json import was failing. Details here.\n\nPython: Sample added for Language ID. Details here.\n\nCovid19 abridged testing: Due to working remotely over the last few weeks, we couldn't do as much manual device verification testing as we normally do. For example, we couldn't test microphone input and speaker output on Linux, iOS, and macOS. We haven't made any changes we think could have broken anything on these platforms, and our automated tests all passed. In the unlikely event that we missed something, let us know on GitHub.\n\nThank you for your continued support. As always, please post questions or feedback on GitHub or Stack Overflow.\n\nStay healthy!\n\nSpeech SDK 1.10.0: 2020-February release\n\nNew features\n\nAdded Python packages to support the new 3.8 release of Python.\n\nRed Hat Enterprise Linux (RHEL)/CentOS 8 x64 support (C++, C#, Java, Python).\n\nNote\n\nCustomers must configure OpenSSL according to these instructions.\n\nLinux ARM32 support for Debian and Ubuntu.\n\nDialogServiceConnector now supports an optional \"bot ID\" parameter on BotFrameworkConfig. This parameter allows the use of multiple Direct Line Speech bots with a single Speech resource. Without the parameter specified, the default bot (as determined by the Direct Line Speech channel configuration page) will be used.\n\nDialogServiceConnector now has a SpeechActivityTemplate property. The contents of this JSON string will be used by Direct Line Speech to prepopulate a wide variety of supported fields in all activities that reach a Direct Line Speech bot, including activities automatically generated in response to events like speech recognition.\n\nTTS now uses subscription key for authentication, reducing the first byte latency of the first synthesis result after creating a synthesizer.\n\nUpdated speech recognition models for 19 locales for an average word error rate reduction of 18.6% (es-ES, es-MX, fr-CA, fr-FR, it-IT, ja-JP, ko-KR, pt-BR, zh-CN, zh-HK, nb-NO, fi-FL, ru-RU, pl-PL, ca-ES, zh-TW, th-TH, pt-PT, tr-TR). The new models bring significant improvements across multiple domains including Dictation, Call-Center Transcription, and Video Indexing scenarios.\n\nBug fixes\n\nFixed bug where Conversation Transcriber didn't await properly in JAVA APIs\n\nAndroid x86 emulator fix for Xamarin GitHub issue\n\nAdd missing (Get|Set)Property methods to AudioConfig\n\nFix a TTS bug where the audioDataStream couldn't be stopped when connection fails\n\nUsing an endpoint without a region would cause USP failures for conversation translator\n\nID generation in Universal Windows Applications now uses an appropriately unique GUID algorithm; it previously and unintentionally defaulted to a stubbed implementation that often produced collisions over large sets of interactions.\n\nSamples\n\nUnity sample for using Speech SDK with Unity microphone and push mode streaming\n\nOther changes\n\nOpenSSL configuration documentation updated for Linux\n\nSpeech SDK 1.9.0: 2020-January release\n\nNew features\n\nMulti-device conversation: connect multiple devices to the same speech or text-based conversation, and optionally translate messages sent between them. Learn more in this article.\n\nKeyword recognition support added for Android .aar package and added support for x86 and x64 flavors.\n\nObjective-C: SendMessage and SetMessageProperty methods added to Connection object. See documentation here.\n\nTTS C++ api now supports std::wstring as synthesis text input, removing the need to convert a wstring to string before passing it to the SDK. See details here.\n\nC#: Language ID and source language config are now available.\n\nJavaScript: Added a feature to Connection object to pass through custom messages from the Speech service as callback receivedServiceMessage.\n\nJavaScript: Added support for FromHost API to ease use with on-premises containers and sovereign clouds. See documentation here.\n\nJavaScript: We now honor NODE_TLS_REJECT_UNAUTHORIZED thanks to a contribution from orgads. See details here.\n\nBreaking changes\n\nOpenSSL has been updated to version 1.1.1b and is statically linked to the Speech SDK core library for Linux. This may cause a break if your inbox OpenSSL hasn't been installed to the /usr/lib/ssl directory in the system. Check our documentation under Speech SDK docs to work around the issue.\n\nWe've changed the data type returned for C# WordLevelTimingResult.Offset from int to long to allow for access to WordLevelTimingResults when speech data is longer than 2 minutes.\n\nPushAudioInputStream and PullAudioInputStream now send wav header information to the Speech service based on AudioStreamFormat, optionally specified when they were created. Customers must now use the supported audio input format. Any other formats will get suboptimal recognition results or may cause other issues.\n\nBug fixes\n\nSee the OpenSSL update under Breaking changes above. We fixed both an intermittent crash and a performance issue (lock contention under high load) in Linux and Java.\n\nJava: Made improvements to object closure in high concurrency scenarios.\n\nRestructured our NuGet package. We removed the three copies of Microsoft.CognitiveServices.Speech.core.dll and Microsoft.CognitiveServices.Speech.extension.kws.dll under lib folders, making the NuGet package smaller and faster to download, and we added headers needed to compile some C++ native apps.\n\nFixed quickstart samples here. These were exiting without displaying \"microphone not found\" exception on Linux, macOS, Windows.\n\nFixed SDK crash with long speech recognition results on certain code paths like this sample.\n\nFixed SDK deployment error in Azure Web App environment to address this customer issue.\n\nFixed a TTS error while using multi <voice> tag or <audio> tag to address this customer issue.\n\nFixed a TTS 401 error when the SDK is recovered from suspended.\n\nJavaScript: Fixed a circular import of audio data thanks to a contribution from euirim.\n\nJavaScript: added support for setting service properties, as added in 1.7.\n\nJavaScript: fixed an issue where a connection error could result in continuous, unsuccessful websocket reconnect attempts.\n\nSamples\n\nAdded keyword recognition sample for Android here.\n\nAdded TTS sample for the server scenario here.\n\nAdded Multi-device conversation quickstarts for C# and C++ here.\n\nOther changes\n\nOptimized SDK core library size on Android.\n\nSDK in 1.9.0 and onwards supports both int and string types in the voice signature version field for Conversation Transcriber.\n\nSpeech SDK 1.8.0: 2019-November release\n\nNew features\n\nAdded a FromHost() API, to ease use with on-premises containers and sovereign clouds.\n\nAdded Source Language Identification for Speech Recognition (in Java and C++)\n\nAdded SourceLanguageConfig object for Speech Recognition, used to specify expected source languages (in Java and C++)\n\nAdded KeywordRecognizer support on Windows (UWP), Android and iOS through the NuGet and Unity packages\n\nAdded Remote Conversation Java API to do Conversation Transcription in asynchronous batches.\n\nBreaking changes\n\nConversation Transcriber functionalities moved under namespace Microsoft.CognitiveServices.Speech.Transcription.\n\nParts of the Conversation Transcriber methods are moved to new Conversation class.\n\nDropped support for 32-bit (ARMv7 and x86) iOS\n\nBug fixes\n\nFix for crash if local KeywordRecognizer is used without a valid Speech service subscription key\n\nSamples\n\nXamarin sample for KeywordRecognizer\n\nUnity sample for KeywordRecognizer\n\nC++ and Java samples for Automatic Source Language Identification.\n\nSpeech SDK 1.7.0: 2019-September release\n\nNew features\n\nAdded beta support for Xamarin on Universal Windows Platform (UWP), Android, and iOS\n\nAdded iOS support for Unity\n\nAdded Compressed input support for ALaw, Mulaw, FLAC, on Android, iOS, and Linux\n\nAdded SendMessageAsync in Connection class for sending a message to service\n\nAdded SetMessageProperty in Connection class for setting property of a message\n\nTTS added bindings for Java (JRE and Android), Python, Swift, and Objective-C\n\nTTS added playback support for macOS, iOS, and Android.\n\nAdded \"word boundary\" information for TTS.\n\nBug fixes\n\nFixed IL2CPP build issue on Unity 2019 for Android\n\nFixed issue with malformed headers in wav file input being processed incorrectly\n\nFixed issue with UUIDs not being unique in some connection properties\n\nFixed a few warnings about nullability specifiers in the Swift bindings (might require small code changes)\n\nFixed a bug that caused websocket connections to be closed ungracefully under network load\n\nFixed an issue on Android that sometimes results in duplicate impression IDs used by DialogServiceConnector\n\nImprovements to the stability of connections across multi-turn interactions and the reporting of failures (via Canceled events) when they occur with DialogServiceConnector\n\nDialogServiceConnector session starts will now properly provide events, including when calling ListenOnceAsync() during an active StartKeywordRecognitionAsync()\n\nAddressed a crash associated with DialogServiceConnector activities being received\n\nSamples\n\nQuickstart for Xamarin\n\nUpdated CPP Quickstart with Linux ARM64 information\n\nUpdated Unity quickstart with iOS information\n\nSpeech SDK 1.6.0: 2019-June release\n\nSamples\n\nQuickstart samples for Text To Speech on UWP and Unity\n\nQuickstart sample for Swift on iOS\n\nUnity samples for Speech & Intent Recognition and Translation\n\nUpdated quickstart samples for DialogServiceConnector\n\nImprovements / Changes\n\nDialog namespace:\n\nSpeechBotConnector has been renamed to DialogServiceConnector\n\nBotConfig has been renamed to DialogServiceConfig\n\nBotConfig::FromChannelSecret() has been remapped to DialogServiceConfig::FromBotSecret()\n\nAll existing Direct Line Speech clients continue to be supported after the rename\n\nUpdate TTS REST adapter to support proxy, persistent connection\n\nImprove error message when an invalid region is passed\n\nSwift/Objective-C:\n\nImproved error reporting: Methods that can result in an error are now present in two versions: One that exposes an NSError object for error handling, and one that raises an exception. The former are exposed to Swift. This change requires adaptations to existing Swift code.\n\nImproved event handling\n\nBug fixes\n\nFix for TTS: where SpeakTextAsync future returned without waiting until audio has completed rendering\n\nFix for marshaling strings in C# to enable full language support\n\nFix for .NET core app problem to load core library with net461 target framework in samples\n\nFix for occasional issues to deploy native libraries to the output folder in samples\n\nFix for web socket closing reliably\n\nFix for possible crash while opening a connection under heavy load on Linux\n\nFix for missing metadata in the framework bundle for macOS\n\nFix for problems with pip install --user on Windows\n\nSpeech SDK 1.5.1\n\nThis is a bug fix release and only affecting the native/managed SDK. It isn't affecting the JavaScript version of the SDK.\n\nBug fixes\n\nFix FromSubscription when used with Conversation Transcription.\n\nFix bug in keyword spotting for Voice Assistants.\n\nSpeech SDK 1.5.0: 2019-May release\n\nNew features\n\nKeyword spotting (KWS) is now available for Windows and Linux. KWS functionality might work with any microphone type, official KWS support, however, is currently limited to the microphone arrays found in the Azure Kinect DK hardware or the Speech Devices SDK.\n\nPhrase hint functionality is available through the SDK. For more information, see here.\n\nConversation transcription functionality is available through the SDK.\n\nAdd support for Voice Assistants using the Direct Line Speech channel.\n\nSamples\n\nAdded samples for new features or new services supported by the SDK.\n\nImprovements / Changes\n\nAdded various recognizer properties to adjust service behavior or service results (like masking profanity and others).\n\nYou can now configure the recognizer through the standard configuration properties, even if you created the recognizer FromEndpoint.\n\nObjective-C: OutputFormat property was added to SPXSpeechConfiguration.\n\nThe SDK now supports Debian 9 as a Linux distribution.\n\nBug fixes\n\nFixed a problem where the speaker resource was destructed too early in text to speech.\n\nSpeech SDK 1.4.2\n\nThis is a bug fix release and only affecting the native/managed SDK. It isn't affecting the JavaScript version of the SDK.\n\nSpeech SDK 1.4.1\n\nThis is a JavaScript-only release. No features have been added. The following fixes were made:\n\nPrevent web pack from loading https-proxy-agent.\n\nSpeech SDK 1.4.0: 2019-April release\n\nNew features\n\nThe SDK now supports the Text to speech service as a beta version. It's supported on Windows and Linux Desktop from C++ and C#. For more information, check the Text to speech overview.\n\nThe SDK now supports MP3 and Opus/OGG audio files as stream input files. This feature is available only on Linux from C++ and C# and is currently in beta (more details here).\n\nThe Speech SDK for Java, .NET core, C++ and Objective-C have gained macOS support. The Objective-C support for macOS is currently in beta.\n\niOS: The Speech SDK for iOS (Objective-C) is now also published as a CocoaPod.\n\nJavaScript: Support for non-default microphone as an input device.\n\nJavaScript: Proxy support for Node.js.\n\nSamples\n\nSamples for using the Speech SDK with C++ and with Objective-C on macOS have been added.\n\nSamples demonstrating the usage of the Text to speech service have been added.\n\nImprovements / Changes\n\nPython: Additional properties of recognition results are now exposed via the properties property.\n\nFor additional development and debug support, you can redirect SDK logging and diagnostics information into a log file (more details here).\n\nJavaScript: Improve audio processing performance.\n\nBug fixes\n\nMac/iOS: A bug that led to a long wait when a connection to the Speech service couldn't be established was fixed.\n\nPython: improve error handling for arguments in Python callbacks.\n\nJavaScript: Fixed wrong state reporting for speech ended on RequestSession.\n\nSpeech SDK 1.3.1: 2019-February refresh\n\nThis is a bug fix release and only affecting the native/managed SDK. It isn't affecting the JavaScript version of the SDK.\n\nBug fix\n\nFixed a memory leak when using microphone input. Stream based or file input isn't affected.\n\nSpeech SDK 1.3.0: 2019-February release\n\nNew features\n\nThe Speech SDK supports selection of the input microphone through the AudioConfig class. This allows you to stream audio data to the Speech service from a non-default microphone. For more information, see the documentation describing audio input device selection. This feature isn't yet available from JavaScript.\n\nThe Speech SDK now supports Unity in a beta version. Provide feedback through the issue section in the GitHub sample repository. This release supports Unity on Windows x86 and x64 (desktop or Universal Windows Platform applications), and Android (ARM32/64, x86). More information is available in our Unity quickstart.\n\nThe file Microsoft.CognitiveServices.Speech.csharp.bindings.dll (shipped in previous releases) isn't needed anymore. The functionality is now integrated into the core SDK.\n\nSamples\n\nThe following new content is available in our sample repository:\n\nAdditional samples for AudioConfig.FromMicrophoneInput.\n\nAdditional Python samples for intent recognition and translation.\n\nAdditional samples for using the Connection object in iOS.\n\nAdditional Java samples for translation with audio output.\n\nNew sample for use of the Batch Transcription REST API.\n\nImprovements / Changes\n\nPython\n\nImproved parameter verification and error messages in SpeechConfig.\n\nAdd support for the Connection object.\n\nSupport for 32-bit Python (x86) on Windows.\n\nThe Speech SDK for Python is out of beta.\n\niOS\n\nThe SDK is now built against the iOS SDK version 12.1.\n\nThe SDK now supports iOS versions 9.2 and later.\n\nImprove reference documentation and fix several property names.\n\nJavaScript\n\nAdd support for the Connection object.\n\nAdd type definition files for bundled JavaScript\n\nInitial support and implementation for phrase hints.\n\nReturn properties collection with service JSON for recognition\n\nWindows DLLs do now contain a version resource.\n\nIf you create a recognizer FromEndpoint, you can add parameters directly to the endpoint URL. Using FromEndpoint you can't configure the recognizer through the standard configuration properties.\n\nBug fixes\n\nEmpty proxy username and proxy password weren't handled correctly. With this release, if you set proxy username and proxy password to an empty string, they won't be submitted when connecting to the proxy.\n\nSessionId's created by the SDK weren't always truly random for some languages / environments. Added random generator initialization to fix this issue.\n\nImprove handling of authorization token. If you want to use an authorization token, specify in the SpeechConfig and leave the subscription key empty. Then create the recognizer as usual.\n\nIn some cases, the Connection object wasn't released correctly. This issue has been fixed.\n\nThe JavaScript sample was fixed to support audio output for translation synthesis also on Safari.\n\nSpeech SDK 1.2.1\n\nThis is a JavaScript-only release. No features have been added. The following fixes were made:\n\nFire end of stream at turn.end, not at speech.end.\n\nFix bug in audio pump that didn't schedule next send if the current send failed.\n\nFix continuous recognition with auth token.\n\nBug fix for different recognizer / endpoints.\n\nDocumentation improvements.\n\nSpeech SDK 1.2.0: 2018-December release\n\nNew features\n\nPython\n\nThe Beta version of Python support (3.5 and above) is available with this release. For more information, see here](../../quickstart-python.md).\n\nJavaScript\n\nThe Speech SDK for JavaScript has been open-sourced. The source code is available on GitHub.\n\nWe now support Node.js, more info can be found here.\n\nThe length restriction for audio sessions has been removed, reconnection will happen automatically under the cover.\n\nConnection object\n\nFrom the Recognizer, you can access a Connection object. This object allows you to explicitly initiate the service connection and subscribe to connect and disconnect events. (This feature isn't yet available from JavaScript and Python.)\n\nSupport for Ubuntu 18.04.\n\nAndroid\n\nEnabled ProGuard support during APK generation.\n\nImprovements\n\nImprovements in the internal thread usage, reducing the number of threads, locks, mutexes.\n\nImproved error reporting / information. In several cases, error messages haven't been propagated out all the way out.\n\nUpdated development dependencies in JavaScript to use up-to-date modules.\n\nBug fixes\n\nFixed memory leaks due to a type mismatch in RecognizeAsync.\n\nIn some cases exceptions were being leaked.\n\nFixing memory leak in translation event arguments.\n\nFixed a locking issue on reconnect in long running sessions.\n\nFixed an issue that could lead to missing final result for failed translations.\n\nC#: If an async operation wasn't awaited in the main thread, it was possible the recognizer could be disposed before the async task was completed.\n\nJava: Fixed a problem resulting in a crash of the Java VM.\n\nObjective-C: Fixed enum mapping; RecognizedIntent was returned instead of RecognizingIntent.\n\nJavaScript: Set default output format to 'simple' in SpeechConfig.\n\nJavaScript: Removing inconsistency between properties on the config object in JavaScript and other languages.\n\nSamples\n\nUpdated and fixed several samples (for example output voices for translation, etc.).\n\nAdded Node.js samples in the sample repository.\n\nSpeech SDK 1.1.0\n\nNew features\n\nSupport for Android x86/x64.\n\nProxy Support: In the SpeechConfig object, you can now call a function to set the proxy information (hostname, port, username, and password). This feature isn't yet available on iOS.\n\nImproved error code and messages. If a recognition returned an error, this did already set Reason (in canceled event) or CancellationDetails (in recognition result) to Error. The canceled event now contains two additional members, ErrorCode and ErrorDetails. If the server returned additional error information with the reported error, it will now be available in the new members.\n\nImprovements\n\nAdded additional verification in the recognizer configuration, and added additional error message.\n\nImproved handling of long-time silence in middle of an audio file.\n\nNuGet package: for .NET Framework projects, it prevents building with AnyCPU configuration.\n\nBug fixes\n\nFixed several exceptions found in recognizers. In addition, exceptions are caught and converted into Canceled event.\n\nFix a memory leak in property management.\n\nFixed bug in which an audio input file could crash the recognizer.\n\nFixed a bug where events could be received after a session stop event.\n\nFixed some race conditions in threading.\n\nFixed an iOS compatibility issue that could result in a crash.\n\nStability improvements for Android microphone support.\n\nFixed a bug where a recognizer in JavaScript would ignore the recognition language.\n\nFixed a bug preventing setting the EndpointId (in some cases) in JavaScript.\n\nChanged parameter order in AddIntent in JavaScript, and added missing AddIntent JavaScript signature.\n\nSamples\n\nAdded C++ and C# samples for pull and push stream usage in the sample repository.\n\nSpeech SDK 1.0.1\n\nReliability improvements and bug fixes:\n\nFixed potential fatal error due to race condition in disposing recognizer\n\nFixed potential fatal error when unset properties occur.\n\nAdded additional error and parameter checking.\n\nObjective-C: Fixed possible fatal error caused by name overriding in NSString.\n\nObjective-C: Adjusted visibility of API\n\nJavaScript: Fixed regarding events and their payloads.\n\nDocumentation improvements.\n\nIn our sample repository, a new sample for JavaScript was added.\n\nAzure AI Speech SDK 1.0.0: 2018-September release\n\nNew features\n\nSupport for Objective-C on iOS. Check out our Objective-C quickstart for iOS.\n\nSupport for JavaScript in browser. Check out our JavaScript quickstart.\n\nBreaking changes\n\nWith this release, a number of breaking changes are introduced. Check this page for details.\n\nAzure AI Speech SDK 0.6.0: 2018-August release\n\nNew features\n\nUWP apps built with the Speech SDK now can pass the Windows App Certification Kit (WACK). Check out the UWP quickstart.\n\nSupport for .NET Standard 2.0 on Linux (Ubuntu 16.04 x64).\n\nExperimental: Support Java 8 on Windows (64-bit) and Linux (Ubuntu 16.04 x64). Check out the Java Runtime Environment quickstart.\n\nFunctional change\n\nExpose additional error detail information on connection errors.\n\nBreaking changes\n\nOn Java (Android), the SpeechFactory.configureNativePlatformBindingWithDefaultCertificate function no longer requires a path parameter. Now the path is automatically detected on all supported platforms.\n\nThe get-accessor of the property EndpointUrl in Java and C# was removed.\n\nBug fixes\n\nIn Java, the audio synthesis result on the translation recognizer is implemented now.\n\nFixed a bug that could cause inactive threads and an increased number of open and unused sockets.\n\nFixed a problem, where a long-running recognition could terminate in the middle of the transmission.\n\nFixed a race condition in recognizer shutdown.\n\nAzure AI Speech SDK 0.5.0: 2018-July release\n\nNew features\n\nSupport Android platform (API 23: Android 6.0 Marshmallow or higher). Check out the Android quickstart.\n\nSupport .NET Standard 2.0 on Windows. Check out the .NET Core quickstart.\n\nExperimental: Support UWP on Windows (version 1709 or later).\n\nCheck out the UWP quickstart.\n\nNote that UWP apps built with the Speech SDK don't yet pass the Windows App Certification Kit (WACK).\n\nSupport long-running recognition with automatic reconnection.\n\nFunctional changes\n\nStartContinuousRecognitionAsync() supports long-running recognition.\n\nThe recognition result contains more fields. They're offset from the audio beginning and duration (both in ticks) of the recognized text and additional values that represent recognition status, for example, InitialSilenceTimeout and InitialBabbleTimeout.\n\nSupport AuthorizationToken for creating factory instances.\n\nBreaking changes\n\nRecognition events: NoMatch event type was merged into the Error event.\n\nSpeechOutputFormat in C# was renamed to OutputFormat to stay aligned with C++.\n\nThe return type of some methods of the AudioInputStream interface changed slightly:\n\nIn Java, the read method now returns long instead of int.\n\nIn C#, the Read method now returns uint instead of int.\n\nIn C++, the Read and GetFormat methods now return size_t instead of int.\n\nC++: Instances of audio input streams now can be passed only as a shared_ptr.\n\nBug fixes\n\nFixed incorrect return values in the result when RecognizeAsync() times out.\n\nThe dependency on media foundation libraries on Windows was removed. The SDK now uses Core Audio APIs.\n\nDocumentation fix: Added a regions page to describe the supported regions.\n\nKnown Issue\n\nThe Speech SDK for Android doesn't report speech synthesis results for translation. This issue will be fixed in the next release.\n\nAzure AI Speech SDK 0.4.0: 2018-June release\n\nFunctional changes\n\nAudioInputStream\n\nA recognizer now can consume a stream as the audio source. For more information, see the related how-to guide.\n\nDetailed output format\n\nWhen you create a SpeechRecognizer, you can request Detailed or Simple output format. The DetailedSpeechRecognitionResult contains a confidence score, recognized text, raw lexical form, normalized form, and normalized form with masked profanity.\n\nBreaking change\n\nChanged to SpeechRecognitionResult.Text from SpeechRecognitionResult.RecognizedText in C#.\n\nBug fixes\n\nFixed a possible callback issue in the USP layer during shutdown.\n\nIf a recognizer consumed an audio input file, it was holding on to the file handle longer than necessary.\n\nRemoved several deadlocks between the message pump and the recognizer.\n\nFire a NoMatch result when the response from service is timed out.\n\nThe media foundation libraries on Windows are delay loaded. This library is required for microphone input only.\n\nThe upload speed for audio data is limited to about twice the original audio speed.\n\nOn Windows, C# .NET assemblies now are strong named.\n\nDocumentation fix: Region is required information to create a recognizer.\n\nMore samples have been added and are constantly being updated. For the latest set of samples, see the Speech SDK samples GitHub repository.\n\nAzure AI Speech SDK 0.2.12733: 2018-May release\n\nThis release is the first public preview release of the Azure AI Speech SDK.\n\nSpeech CLI 1.40.0: August 2024 release\n\nUpdated to use Speech SDK 1.40.0\n\nNew features\n\nnone\n\nBug fixes\n\nnone\n\nSpeech CLI 1.38.0: June 2024 release\n\nUpdated to use Speech SDK 1.38.0\n\nNew features\n\nnone\n\nBug fixes\n\nnone\n\nSpeech CLI 1.37.0: April 2024 release\n\nUpdated to use Speech SDK 1.37.0\n\nNew features\n\nnone\n\nBug fixes\n\nnone\n\nSpeech CLI 1.36.0: March 2024 release\n\nUpdated to use Speech SDK 1.36.0\n\nNew features\n\nnone\n\nBug fixes\n\nnone\n\nSpeech CLI 1.35.0: February 2024 release\n\nUpdated to use Speech SDK 1.35.0\n\nNew features\n\nnone\n\nBug fixes\n\nUpdate JMESPath dependency to latest\n\nSpeech CLI 1.34.0: November 2023 release\n\nUpdated to use Speech SDK 1.34.0\n\nSpeech CLI 1.33.0: October 2023 release\n\nUpdated to use Speech SDK 1.33.0\n\nSpeech CLI 1.31.0: August 2023 release\n\nUpdated to use Speech SDK 1.31.0\n\nSpeech CLI 1.30.0: July 2023 release\n\nUpdated to use Speech SDK 1.30.0\n\nSpeech CLI 1.29.0: June 2023 release\n\nUpdated to use Speech SDK 1.29.0\n\nSpeech CLI 1.28.0: May 2023 release\n\nUpdated to use Speech SDK 1.28.0\n\nSpeech CLI 1.27.0: April 2023 release\n\nUpdated to use Speech SDK 1.27.0\n\nUpdate default endpoint to use v3.1 REST APIs for custom speech Recognition and Batch Speech Recognition.\n\nBug fixes\n\nFixes related to how query parameters are parsed/configured.\n\nSpeech CLI 1.26.0: March 2023 release\n\nUpdated to use Speech SDK 1.26.0.\n\nSpeech CLI 1.25.0: January 2023 release\n\nUpdated to use Speech SDK 1.25.0.\n\nSpeech CLI 1.24.0: October 2022 release\n\nUses Speech SDK 1.24.0.\n\nNew features\n\nExpanded \"spx check\" to support JMESPath queries against all spx events\n\nBug fixes\n\nVarious improvements to robustness against JMESPath query evaluations\n\nFix for truncations to file writes that may occur on resource-constrained machines\n\nSpeech CLI 1.23.0: July 2022 release\n\nUses Speech SDK 1.23.0.\n\nNew features\n\nBetter caption (--output vtt and --output srt) large result splitting (37 char max, 3 lines)\n\nDocumented spx synthesize --format options (see spx help synthesize format)\n\nDocumented most of spx csr commands/options (see spx help csr)\n\nAdded spx csr model copy command (see spx help csr model copy)\n\nAdded --check result option using JMES queries (see spx help check result)\n\nImproved error messages when specifying invalid command options\n\nMoved from .NET Core 3.1 to .NET 6.0. In order to run Speech CLI, you'll need to install the .NET 6.0 Runtime (or above).\n\nBug fixes\n\nUpdated all URLs to remove language (for example, \"en-US\")\n\nFixed version info to report properly in all cases (previously it sometimes showed a blank)\n\nSpeech CLI 1.22.0: June 2022 release\n\nUses Speech SDK 1.22.0.\n\nNew features\n\nAdded spx init command to guide users through the Speech resource key creation without going to Azure Web Portal.\n\nSpeech docker containers now have Azure CLI included, so the spx init command works out of the box.\n\nAdded timestamp as an event output option, to make SPX more useful when calculating latencies.\n\nSpeech CLI 1.21.0: April 2022 release\n\nUses Speech SDK 1.21.0.\n\nNew features\n\nWEBVTT Caption generation\n\nAdded --output vtt support to spx translate\n\nSupports --output vtt file FILENAME to override default VTT FILENAME\n\nSupports --output vtt file - to write to standard output\n\nIndividual VTT files are created for each target language (for example --target en;de;fr)\n\nSRT Caption generation\n\nAdded --output srt support to spx recognize, spx intent, and spx translate\n\nSupports --output srt file FILENAME to override default SRT FILENAME\n\nSupports --output srt file - to write to standard output\n\nFor spx translate, individual SRT files are created for each target language (for example --target en;de;fr)\n\nBug fixes\n\nCorrected WEBVTT timespan output to properly use hh:mm:ss.fff format\n\nSpeech CLI 1.20.0: January 2022 release\n\nNew features\n\nSpeaker recognition\n\nspx profile enroll and spx speaker [identify/verify] now support microphone input\n\nIntent recognition (spx intent)\n\n--keyword FILE.table\n\n--pattern and --patterns\n\n--output all/each intentid\n\n--output all/each entity json\n\n--output all/each ENTITY entity\n\n--once, --once+, --continuous (continuous now default)\n\n--output all/each connection EVENT\n\n--output all/each connection message (for example, text, path)\n\nCLI console output expectation checking/authoring:\n\n--expect PATTERN and --not expect PATTERN support on all commands\n\n--auto expect to assist authoring expected patterns\n\nSDK logging output expectation checking/authoring\n\n--log expect PATTERN and --not log expect PATTERN support on all commands\n\n--log auto expect [FILTER] support on all commands\n\n--log FILE support on spx profile and spx speaker\n\nAudio file input\n\n--format ANY support on all commands\n\n--file - support (reading from standard input, enabling pipe scenarios)\n\nAudio file output\n\n--audio output - Writing to standard output, enabling pipe scenarios\n\nOutput files\n\n--output all/each file - Write to standard output\n\n--output batch file - Write to standard output\n\n--output vtt file - Write to standard output\n\n--output json file - Write to standard output, for spx csr and spx batch commands\n\nOutput properties\n\n--output [â¦] result XXX property (PropertyId or string)\n\n--output [â¦] connection message received XXX property (PropertyId or string)\n\n--output [â¦] recognizer XXX property (PropertyId or string)\n\nAzure WebJob integration\n\nspx webjob now follows sub-command pattern\n\nUpdated WebJob help to reflect the sub-command pattern (see spx help webjob)\n\nBug fixes\n\nFixed bug when both --output vtt FILE and --output batch FILE are used at the same time\n\nspx [...] --zip ZIPFILENAME now includes all binaries required for all scenarios (if present)\n\nspx profile and spx speaker commands now return detailed error information on cancellation\n\n2021-May release\n\nNew features\n\nAdded support for Profile, Speaker ID, and Speaker verification - Try spx profile and spx speaker from the command-line.\n\nWe also added Dialog support - Try spx dialog from the command-line.\n\nImproved spx help. Please give us feedback about how this works for you by opening a GitHub issue.\n\nWe've decreased the size of the .NET tool install.\n\nCOVID-19 abridged testing\n\nAs the ongoing pandemic continues to require our engineers to work from home, pre-pandemic manual verification scripts have been significantly reduced. We test on fewer devices with fewer configurations, and the likelihood of environment-specific bugs slipping through may be increased. We still rigorously validate with a large set of automation. In the unlikely event that we missed something, please let us know on GitHub.\n\nStay healthy!\n\n2021-March release\n\nNew features\n\nAdded spx intent command for intent recognition, replacing spx recognize intent.\n\nRecognize and intent can now use Azure functions to calculate word error rate using spx recognize --wer url <URL>.\n\nRecognize can now output results as VTT files using spx recognize --output vtt file <FILENAME>.\n\nSensitive key info now obscured in debug/verbose output.\n\nAdded URL checking and error message for content field in batch transcription create.\n\nCOVID-19 abridged testing\n\nAs the ongoing pandemic continues to require our engineers to work from home, pre-pandemic manual verification scripts have been significantly reduced. We test on fewer devices with fewer configurations, and the likelihood of environment-specific bugs slipping through may be increased. We still rigorously validate with a large set of automation. In the unlikely event that we missed something, please let us know on GitHub.\n\nStay healthy!\n\n2021-January release\n\nNew features\n\nSpeech CLI is now available as a NuGet package and can be installed via .NET CLI as a .NET global tool you can call from the shell/command-line.\n\nThe custom speech DevOps Template repo has been updated to use Speech CLI for its custom speech workflows.\n\nCOVID-19 abridged testing\n\nAs the ongoing pandemic continues to require our engineers to work from home, pre-pandemic manual verification scripts have been significantly reduced. We test on fewer devices with fewer configurations, and the likelihood of environment-specific bugs slipping through may be increased. We still rigorously validate with a large set of automation. In the unlikely event that we missed something, please let us know on Gi"
    }
}
{
    "id": "dbpedia_3621_0",
    "rank": 56,
    "data": {
        "url": "https://www.science.gov/topicpages/m/malaysian%2Bsupercomputer%2Bresearch",
        "read_more_link": "",
        "language": "en",
        "title": "malaysian supercomputer research: Topics by Science.gov",
        "top_image": "",
        "meta_img": "",
        "images": [
            "https://www.science.gov/scigov/desktop/en/images/SciGov_logo.png",
            "https://www.science.gov/topicpages/m/images/arrow-up.gif",
            "https://www.science.gov/topicpages/m/images/arrow-down.gif"
        ],
        "movies": [],
        "keywords": [],
        "meta_keywords": [
            ""
        ],
        "tags": null,
        "authors": [],
        "publish_date": null,
        "summary": "",
        "meta_description": "",
        "meta_lang": "en",
        "meta_favicon": "",
        "meta_site_name": "",
        "canonical_link": null,
        "text": "Graphics supercomputer for computational fluid dynamics research\n\nNASA Astrophysics Data System (ADS)\n\nLiaw, Goang S.\n\n1994-11-01\n\nThe objective of this project is to purchase a state-of-the-art graphics supercomputer to improve the Computational Fluid Dynamics (CFD) research capability at Alabama A & M University (AAMU) and to support the Air Force research projects. A cutting-edge graphics supercomputer system, Onyx VTX, from Silicon Graphics Computer Systems (SGI), was purchased and installed. Other equipment including a desktop personal computer, PC-486 DX2 with a built-in 10-BaseT Ethernet card, a 10-BaseT hub, an Apple Laser Printer Select 360, and a notebook computer from Zenith were also purchased. A reading room has been converted to a research computer lab by adding some furniture and an air conditioning unit in order to provide an appropriate working environments for researchers and the purchase equipment. All the purchased equipment were successfully installed and are fully functional. Several research projects, including two existing Air Force projects, are being performed using these facilities.\n\nComputational fluid dynamics research at the United Technologies Research Center requiring supercomputers\n\nNASA Technical Reports Server (NTRS)\n\nLandgrebe, Anton J.\n\n1987-01-01\n\nAn overview of research activities at the United Technologies Research Center (UTRC) in the area of Computational Fluid Dynamics (CFD) is presented. The requirement and use of various levels of computers, including supercomputers, for the CFD activities is described. Examples of CFD directed toward applications to helicopters, turbomachinery, heat exchangers, and the National Aerospace Plane are included. Helicopter rotor codes for the prediction of rotor and fuselage flow fields and airloads were developed with emphasis on rotor wake modeling. Airflow and airload predictions and comparisons with experimental data are presented. Examples are presented of recent parabolized Navier-Stokes and full Navier-Stokes solutions for hypersonic shock-wave/boundary layer interaction, and hydrogen/air supersonic combustion. In addition, other examples of CFD efforts in turbomachinery Navier-Stokes methodology and separated flow modeling are presented. A brief discussion of the 3-tier scientific computing environment is also presented, in which the researcher has access to workstations, mid-size computers, and supercomputers.\n\nComputational fluid dynamics research at the United Technologies Research Center requiring supercomputers\n\nNASA Astrophysics Data System (ADS)\n\nLandgrebe, Anton J.\n\n1987-03-01\n\nAn overview of research activities at the United Technologies Research Center (UTRC) in the area of Computational Fluid Dynamics (CFD) is presented. The requirement and use of various levels of computers, including supercomputers, for the CFD activities is described. Examples of CFD directed toward applications to helicopters, turbomachinery, heat exchangers, and the National Aerospace Plane are included. Helicopter rotor codes for the prediction of rotor and fuselage flow fields and airloads were developed with emphasis on rotor wake modeling. Airflow and airload predictions and comparisons with experimental data are presented. Examples are presented of recent parabolized Navier-Stokes and full Navier-Stokes solutions for hypersonic shock-wave/boundary layer interaction, and hydrogen/air supersonic combustion. In addition, other examples of CFD efforts in turbomachinery Navier-Stokes methodology and separated flow modeling are presented. A brief discussion of the 3-tier scientific computing environment is also presented, in which the researcher has access to workstations, mid-size computers, and supercomputers.\n\nMalaysian and American Students' Perceptions of Research Ethics\n\nERIC Educational Resources Information Center\n\nBowman, Laura L.; Anthonysamy, Angela\n\n2006-01-01\n\nDifferences in perceptions of research ethics between Malaysian and American students were assessed using a questionnaire that measured perceptions of voluntary informed consent for adults and children, assessment of the risk/benefit ratio, issues of deception, and issues of privacy and confidentiality. As predicted, Malaysian students had lessâ¦\n\nImpact of Antecedent Factors on Collaborative Technologies Usage among Academic Researchers in Malaysian Research Universities\n\nERIC Educational Resources Information Center\n\nMohd Daud, Norzaidi; Zakaria, Halimi\n\n2017-01-01\n\nPurpose: The purpose of this paper is to investigate the impact of antecedent factors on collaborative technologies usage among academic researchers in Malaysian research universities. Design/methodology/approach: Data analysis was conducted on data collected from 156 academic researchers from five Malaysian research universities. This studyâ¦\n\nA Career Success Model for Academics at Malaysian Research Universities\n\nERIC Educational Resources Information Center\n\nAbu Said, Al-Mansor; Mohd Rasdi, Roziah; Abu Samah, Bahaman; Silong, Abu Daud; Sulaiman, Suzaimah\n\n2015-01-01\n\nPurpose: The purpose of this paper is to develop a career success model for academics at the Malaysian research universities. Design/methodology/approach: Self-administered and online surveys were used for data collection among 325 academics from Malaysian research universities. Findings: Based on the analysis of structural equation modeling, theâ¦\n\nOptimization of Supercomputer Use on EADS II System\n\nNASA Technical Reports Server (NTRS)\n\nAhmed, Ardsher\n\n1998-01-01\n\nThe main objective of this research was to optimize supercomputer use to achieve better throughput and utilization of supercomputers and to help facilitate the movement of non-supercomputing (inappropriate for supercomputer) codes to mid-range systems for better use of Government resources at Marshall Space Flight Center (MSFC). This work involved the survey of architectures available on EADS II and monitoring customer (user) applications running on a CRAY T90 system.\n\nImproved Access to Supercomputers Boosts Chemical Applications.\n\nERIC Educational Resources Information Center\n\nBorman, Stu\n\n1989-01-01\n\nSupercomputing is described in terms of computing power and abilities. The increase in availability of supercomputers for use in chemical calculations and modeling are reported. Efforts of the National Science Foundation and Cray Research are highlighted. (CW)\n\nDesktop supercomputer: what can it do?\n\nNASA Astrophysics Data System (ADS)\n\nBogdanov, A.; Degtyarev, A.; Korkhov, V.\n\n2017-12-01\n\nThe paper addresses the issues of solving complex problems that require using supercomputers or multiprocessor clusters available for most researchers nowadays. Efficient distribution of high performance computing resources according to actual application needs has been a major research topic since high-performance computing (HPC) technologies became widely introduced. At the same time, comfortable and transparent access to these resources was a key user requirement. In this paper we discuss approaches to build a virtual private supercomputer available at user's desktop: a virtual computing environment tailored specifically for a target user with a particular target application. We describe and evaluate possibilities to create the virtual supercomputer based on light-weight virtualization technologies, and analyze the efficiency of our approach compared to traditional methods of HPC resource management.\n\nA scientometrics and social network analysis of Malaysian research in physics\n\nNASA Astrophysics Data System (ADS)\n\nTan, H. X.; Ujum, E. A.; Ratnavelu, K.\n\n2014-03-01\n\nThis conference proceeding presents an empirical assessment on the domestic publication output and structure of scientific collaboration of Malaysian authors for the field of physics. Journal articles with Malaysian addresses for the subject area \"Physics\" and other sub-discipline of physics were retrieved from the Thomson Reuters Web of Knowledge database spanning the years 1980 to 2011. A scientometrics and social network analysis of the Malaysian physics field was conducted to examine the publication growth and distribution of domestic collaborative publications; the giant component analysis; and the degree, closeness, and betweenness centralisation scores for the domestic co-authorship networks. Using these methods, we are able to gain insights on the evolution of collaboration and scientometric dimensions of Malaysian research in physics over time.\n\nEnergy Efficient Supercomputing\n\nDOE Office of Scientific and Technical Information (OSTI.GOV)\n\nAnypas, Katie\n\n2014-10-17\n\nKatie Anypas, Head of NERSC's Services Department discusses the Lab's research into developing increasingly powerful and energy efficient supercomputers at our '8 Big Ideas' Science at the Theater event on October 8th, 2014, in Oakland, California.\n\nEnergy Efficient Supercomputing\n\nScienceCinema\n\nAnypas, Katie\n\n2018-05-07\n\nKatie Anypas, Head of NERSC's Services Department discusses the Lab's research into developing increasingly powerful and energy efficient supercomputers at our '8 Big Ideas' Science at the Theater event on October 8th, 2014, in Oakland, California.\n\nEdison - A New Cray Supercomputer Advances Discovery at NERSC\n\nDOE Office of Scientific and Technical Information (OSTI.GOV)\n\nDosanjh, Sudip; Parkinson, Dula; Yelick, Kathy\n\n2014-02-06\n\nWhen a supercomputing center installs a new system, users are invited to make heavy use of the computer as part of the rigorous testing. In this video, find out what top scientists have discovered using Edison, a Cray XC30 supercomputer, and how NERSC's newest supercomputer will accelerate their future research.\n\nEdison - A New Cray Supercomputer Advances Discovery at NERSC\n\nScienceCinema\n\nDosanjh, Sudip; Parkinson, Dula; Yelick, Kathy; Trebotich, David; Broughton, Jeff; Antypas, Katie; Lukic, Zarija, Borrill, Julian; Draney, Brent; Chen, Jackie\n\n2018-01-16\n\nWhen a supercomputing center installs a new system, users are invited to make heavy use of the computer as part of the rigorous testing. In this video, find out what top scientists have discovered using Edison, a Cray XC30 supercomputer, and how NERSC's newest supercomputer will accelerate their future research.\n\nIce Storm Supercomputer\n\nScienceCinema\n\nNone\n\n2018-05-01\n\nA new Idaho National Laboratory supercomputer is helping scientists create more realistic simulations of nuclear fuel. Dubbed \"Ice Storm\" this 2048-processor machine allows researchers to model and predict the complex physics behind nuclear reactor behavior. And with a new visualization lab, the team can see the results of its simulations on the big screen. For more information about INL research, visit http://www.facebook.com/idahonationallaboratory.\n\nMira: Argonne's 10-petaflops supercomputer\n\nScienceCinema\n\nPapka, Michael; Coghlan, Susan; Isaacs, Eric; Peters, Mark; Messina, Paul\n\n2018-02-13\n\nMira, Argonne's petascale IBM Blue Gene/Q system, ushers in a new era of scientific supercomputing at the Argonne Leadership Computing Facility. An engineering marvel, the 10-petaflops supercomputer is capable of carrying out 10 quadrillion calculations per second. As a machine for open science, any researcher with a question that requires large-scale computing resources can submit a proposal for time on Mira, typically in allocations of millions of core-hours, to run programs for their experiments. This adds up to billions of hours of computing time per year.\n\nMira: Argonne's 10-petaflops supercomputer\n\nDOE Office of Scientific and Technical Information (OSTI.GOV)\n\nPapka, Michael; Coghlan, Susan; Isaacs, Eric\n\n2013-07-03\n\nMira, Argonne's petascale IBM Blue Gene/Q system, ushers in a new era of scientific supercomputing at the Argonne Leadership Computing Facility. An engineering marvel, the 10-petaflops supercomputer is capable of carrying out 10 quadrillion calculations per second. As a machine for open science, any researcher with a question that requires large-scale computing resources can submit a proposal for time on Mira, typically in allocations of millions of core-hours, to run programs for their experiments. This adds up to billions of hours of computing time per year.\n\nNASA's supercomputing experience\n\nNASA Technical Reports Server (NTRS)\n\nBailey, F. Ron\n\n1990-01-01\n\nA brief overview of NASA's recent experience in supercomputing is presented from two perspectives: early systems development and advanced supercomputing applications. NASA's role in supercomputing systems development is illustrated by discussion of activities carried out by the Numerical Aerodynamical Simulation Program. Current capabilities in advanced technology applications are illustrated with examples in turbulence physics, aerodynamics, aerothermodynamics, chemistry, and structural mechanics. Capabilities in science applications are illustrated by examples in astrophysics and atmospheric modeling. Future directions and NASA's new High Performance Computing Program are briefly discussed.\n\nAutomated Help System For A Supercomputer\n\nNASA Technical Reports Server (NTRS)\n\nCallas, George P.; Schulbach, Catherine H.; Younkin, Michael\n\n1994-01-01\n\nExpert-system software developed to provide automated system of user-helping displays in supercomputer system at Ames Research Center Advanced Computer Facility. Users located at remote computer terminals connected to supercomputer and each other via gateway computers, local-area networks, telephone lines, and satellite links. Automated help system answers routine user inquiries about how to use services of computer system. Available 24 hours per day and reduces burden on human experts, freeing them to concentrate on helping users with complicated problems.\n\nSupercomputer Provides Molecular Insight into Cellulose (Fact Sheet)\n\nDOE Office of Scientific and Technical Information (OSTI.GOV)\n\nNot Available\n\n2011-02-01\n\nGroundbreaking research at the National Renewable Energy Laboratory (NREL) has used supercomputing simulations to calculate the work that enzymes must do to deconstruct cellulose, which is a fundamental step in biomass conversion technologies for biofuels production. NREL used the new high-performance supercomputer Red Mesa to conduct several million central processing unit (CPU) hours of simulation.\n\nPredicting Hurricanes with Supercomputers\n\nDOE Office of Scientific and Technical Information (OSTI.GOV)\n\nNone\n\n2010-01-01\n\nHurricane Emily, formed in the Atlantic Ocean on July 10, 2005, was the strongest hurricane ever to form before August. By checking computer models against the actual path of the storm, researchers can improve hurricane prediction. In 2010, NOAA researchers were awarded 25 million processor-hours on Argonne's BlueGene/P supercomputer for the project. Read more at http://go.usa.gov/OLh\n\nSupercomputer applications in molecular modeling.\n\nPubMed\n\nGund, T M\n\n1988-01-01\n\nAn overview of the functions performed by molecular modeling is given. Molecular modeling techniques benefiting from supercomputing are described, namely, conformation, search, deriving bioactive conformations, pharmacophoric pattern searching, receptor mapping, and electrostatic properties. The use of supercomputers for problems that are computationally intensive, such as protein structure prediction, protein dynamics and reactivity, protein conformations, and energetics of binding is also examined. The current status of supercomputing and supercomputer resources are discussed.\n\nIntelligent supercomputers: the Japanese computer sputnik\n\nDOE Office of Scientific and Technical Information (OSTI.GOV)\n\nWalter, G.\n\n1983-11-01\n\nJapan's government-supported fifth-generation computer project has had a pronounced effect on the American computer and information systems industry. The US firms are intensifying their research on and production of intelligent supercomputers, a combination of computer architecture and artificial intelligence software programs. While the present generation of computers is built for the processing of numbers, the new supercomputers will be designed specifically for the solution of symbolic problems and the use of artificial intelligence software. This article discusses new and exciting developments that will increase computer capabilities in the 1990s. 4 references.\n\nCode IN Exhibits - Supercomputing 2000\n\nNASA Technical Reports Server (NTRS)\n\nYarrow, Maurice; McCann, Karen M.; Biswas, Rupak; VanderWijngaart, Rob F.; Kwak, Dochan (Technical Monitor)\n\n2000-01-01\n\nThe creation of parameter study suites has recently become a more challenging problem as the parameter studies have become multi-tiered and the computational environment has become a supercomputer grid. The parameter spaces are vast, the individual problem sizes are getting larger, and researchers are seeking to combine several successive stages of parameterization and computation. Simultaneously, grid-based computing offers immense resource opportunities but at the expense of great difficulty of use. We present ILab, an advanced graphical user interface approach to this problem. Our novel strategy stresses intuitive visual design tools for parameter study creation and complex process specification, and also offers programming-free access to grid-based supercomputer resources and process automation.\n\nColor graphics, interactive processing, and the supercomputer\n\nNASA Technical Reports Server (NTRS)\n\nSmith-Taylor, Rudeen\n\n1987-01-01\n\nThe development of a common graphics environment for the NASA Langley Research Center user community and the integration of a supercomputer into this environment is examined. The initial computer hardware, the software graphics packages, and their configurations are described. The addition of improved computer graphics capability to the supercomputer, and the utilization of the graphic software and hardware are discussed. Consideration is given to the interactive processing system which supports the computer in an interactive debugging, processing, and graphics environment.\n\nComputer Electromagnetics and Supercomputer Architecture\n\nNASA Technical Reports Server (NTRS)\n\nCwik, Tom\n\n1993-01-01\n\nThe dramatic increase in performance over the last decade for microporcessor computations is compared with that for the supercomputer computations. This performance, the projected performance, and a number of other issues such as cost and the inherent pysical limitations in curent supercomputer technology have naturally led to parallel supercomputers and ensemble of interconnected microprocessors.\n\nJapanese supercomputer technology.\n\nPubMed\n\nBuzbee, B L; Ewald, R H; Worlton, W J\n\n1982-12-17\n\nUnder the auspices of the Ministry for International Trade and Industry the Japanese have launched a National Superspeed Computer Project intended to produce high-performance computers for scientific computation and a Fifth-Generation Computer Project intended to incorporate and exploit concepts of artificial intelligence. If these projects are successful, which appears likely, advanced economic and military research in the United States may become dependent on access to supercomputers of foreign manufacture.\n\nComprehensive efficiency analysis of supercomputer resource usage based on system monitoring data\n\nNASA Astrophysics Data System (ADS)\n\nMamaeva, A. A.; Shaykhislamov, D. I.; Voevodin, Vad V.; Zhumatiy, S. A.\n\n2018-03-01\n\nOne of the main problems of modern supercomputers is the low efficiency of their usage, which leads to the significant idle time of computational resources, and, in turn, to the decrease in speed of scientific research. This paper presents three approaches to study the efficiency of supercomputer resource usage based on monitoring data analysis. The first approach performs an analysis of computing resource utilization statistics, which allows to identify different typical classes of programs, to explore the structure of the supercomputer job flow and to track overall trends in the supercomputer behavior. The second approach is aimed specifically at analyzing off-the-shelf software packages and libraries installed on the supercomputer, since efficiency of their usage is becoming an increasingly important factor for the efficient functioning of the entire supercomputer. Within the third approach, abnormal jobs â jobs with abnormally inefficient behavior that differs significantly from the standard behavior of the overall supercomputer job flow â are being detected. For each approach, the results obtained in practice in the Supercomputer Center of Moscow State University are demonstrated.\n\nIntegration of Panda Workload Management System with supercomputers\n\nNASA Astrophysics Data System (ADS)\n\nDe, K.; Jha, S.; Klimentov, A.; Maeno, T.; Mashinistov, R.; Nilsson, P.; Novikov, A.; Oleynik, D.; Panitkin, S.; Poyda, A.; Read, K. F.; Ryabinkin, E.; Teslyuk, A.; Velikhov, V.; Wells, J. C.; Wenaus, T.\n\n2016-09-01\n\nThe Large Hadron Collider (LHC), operating at the international CERN Laboratory in Geneva, Switzerland, is leading Big Data driven scientific explorations. Experiments at the LHC explore the fundamental nature of matter and the basic forces that shape our universe, and were recently credited for the discovery of a Higgs boson. ATLAS, one of the largest collaborations ever assembled in the sciences, is at the forefront of research at the LHC. To address an unprecedented multi-petabyte data processing challenge, the ATLAS experiment is relying on a heterogeneous distributed computational infrastructure. The ATLAS experiment uses PanDA (Production and Data Analysis) Workload Management System for managing the workflow for all data processing on over 140 data centers. Through PanDA, ATLAS physicists see a single computing facility that enables rapid scientific breakthroughs for the experiment, even though the data centers are physically scattered all over the world. While PanDA currently uses more than 250000 cores with a peak performance of 0.3+ petaFLOPS, next LHC data taking runs will require more resources than Grid computing can possibly provide. To alleviate these challenges, LHC experiments are engaged in an ambitious program to expand the current computing model to include additional resources such as the opportunistic use of supercomputers. We will describe a project aimed at integration of PanDA WMS with supercomputers in United States, Europe and Russia (in particular with Titan supercomputer at Oak Ridge Leadership Computing Facility (OLCF), Supercomputer at the National Research Center \"Kurchatov Institute\", IT4 in Ostrava, and others). The current approach utilizes a modified PanDA pilot framework for job submission to the supercomputers batch queues and local data management, with light-weight MPI wrappers to run singlethreaded workloads in parallel on Titan's multi-core worker nodes. This implementation was tested with a variety of Monte-Carlo workloads\n\nINTEGRATION OF PANDA WORKLOAD MANAGEMENT SYSTEM WITH SUPERCOMPUTERS\n\nDOE Office of Scientific and Technical Information (OSTI.GOV)\n\nDe, K; Jha, S; Maeno, T\n\nAbstract The Large Hadron Collider (LHC), operating at the international CERN Laboratory in Geneva, Switzerland, is leading Big Data driven scientific explorations. Experiments at the LHC explore the funda- mental nature of matter and the basic forces that shape our universe, and were recently credited for the dis- covery of a Higgs boson. ATLAS, one of the largest collaborations ever assembled in the sciences, is at the forefront of research at the LHC. To address an unprecedented multi-petabyte data processing challenge, the ATLAS experiment is relying on a heterogeneous distributed computational infrastructure. The ATLAS experiment uses PanDA (Production and DatamoreÂ Â» Analysis) Workload Management System for managing the workflow for all data processing on over 140 data centers. Through PanDA, ATLAS physicists see a single computing facility that enables rapid scientific breakthroughs for the experiment, even though the data cen- ters are physically scattered all over the world. While PanDA currently uses more than 250000 cores with a peak performance of 0.3+ petaFLOPS, next LHC data taking runs will require more resources than Grid computing can possibly provide. To alleviate these challenges, LHC experiments are engaged in an ambitious program to expand the current computing model to include additional resources such as the opportunistic use of supercomputers. We will describe a project aimed at integration of PanDA WMS with supercomputers in United States, Europe and Russia (in particular with Titan supercomputer at Oak Ridge Leadership Com- puting Facility (OLCF), Supercomputer at the National Research Center Kurchatov Institute , IT4 in Ostrava, and others). The current approach utilizes a modified PanDA pilot framework for job submission to the supercomputers batch queues and local data management, with light-weight MPI wrappers to run single- threaded workloads in parallel on Titan s multi-core worker nodes. This implementation was tested with a\n\nScaling of data communications for an advanced supercomputer network\n\nNASA Technical Reports Server (NTRS)\n\nLevin, E.; Eaton, C. K.; Young, Bruce\n\n1986-01-01\n\nThe goal of NASA's Numerical Aerodynamic Simulation (NAS) Program is to provide a powerful computational environment for advanced research and development in aeronautics and related disciplines. The present NAS system consists of a Cray 2 supercomputer connected by a data network to a large mass storage system, to sophisticated local graphics workstations and by remote communication to researchers throughout the United States. The program plan is to continue acquiring the most powerful supercomputers as they become available. The implications of a projected 20-fold increase in processing power on the data communications requirements are described.\n\nAutomatic discovery of the communication network topology for building a supercomputer model\n\nNASA Astrophysics Data System (ADS)\n\nSobolev, Sergey; Stefanov, Konstantin; Voevodin, Vadim\n\n2016-10-01\n\nThe Research Computing Center of Lomonosov Moscow State University is developing the Octotron software suite for automatic monitoring and mitigation of emergency situations in supercomputers so as to maximize hardware reliability. The suite is based on a software model of the supercomputer. The model uses a graph to describe the computing system components and their interconnections. One of the most complex components of a supercomputer that needs to be included in the model is its communication network. This work describes the proposed approach for automatically discovering the Ethernet communication network topology in a supercomputer and its description in terms of the Octotron model. This suite automatically detects computing nodes and switches, collects information about them and identifies their interconnections. The application of this approach is demonstrated on the \"Lomonosov\" and \"Lomonosov-2\" supercomputers.\n\nSupercomputer Issues from a University Perspective.\n\nERIC Educational Resources Information Center\n\nBeering, Steven C.\n\n1984-01-01\n\nDiscusses issues related to the access of and training of university researchers in using supercomputers, considering National Science Foundation's (NSF) role in this area, microcomputers on campuses, and the limited use of existing telecommunication networks. Includes examples of potential scientific projects (by subject area) utilizingâ¦\n\nDemonstration of NICT Space Weather Cloud --Integration of Supercomputer into Analysis and Visualization Environment--\n\nNASA Astrophysics Data System (ADS)\n\nWatari, S.; Morikawa, Y.; Yamamoto, K.; Inoue, S.; Tsubouchi, K.; Fukazawa, K.; Kimura, E.; Tatebe, O.; Kato, H.; Shimojo, S.; Murata, K. T.\n\n2010-12-01\n\nIn the Solar-Terrestrial Physics (STP) field, spatio-temporal resolution of computer simulations is getting higher and higher because of tremendous advancement of supercomputers. A more advanced technology is Grid Computing that integrates distributed computational resources to provide scalable computing resources. In the simulation research, it is effective that a researcher oneself designs his physical model, performs calculations with a supercomputer, and analyzes and visualizes for consideration by a familiar method. A supercomputer is far from an analysis and visualization environment. In general, a researcher analyzes and visualizes in the workstation (WS) managed at hand because the installation and the operation of software in the WS are easy. Therefore, it is necessary to copy the data from the supercomputer to WS manually. Time necessary for the data transfer through long delay network disturbs high-accuracy simulations actually. In terms of usefulness, integrating a supercomputer and an analysis and visualization environment seamlessly with a researcher's familiar method is important. NICT has been developing a cloud computing environment (NICT Space Weather Cloud). In the NICT Space Weather Cloud, disk servers are located near its supercomputer and WSs for data analysis and visualization. They are connected to JGN2plus that is high-speed network for research and development. Distributed virtual high-capacity storage is also constructed by Grid Datafarm (Gfarm v2). Huge-size data output from the supercomputer is transferred to the virtual storage through JGN2plus. A researcher can concentrate on the research by a familiar method without regard to distance between a supercomputer and an analysis and visualization environment. Now, total 16 disk servers are setup in NICT headquarters (at Koganei, Tokyo), JGN2plus NOC (at Otemachi, Tokyo), Okinawa Subtropical Environment Remote-Sensing Center, and Cybermedia Center, Osaka University. They are connected on\n\nSupercomputer use in orthopaedic biomechanics research: focus on functional adaptation of bone.\n\nPubMed\n\nHart, R T; Thongpreda, N; Van Buskirk, W C\n\n1988-01-01\n\nThe authors describe two biomechanical analyses carried out using numerical methods. One is an analysis of the stress and strain in a human mandible, and the other analysis involves modeling the adaptive response of a sheep bone to mechanical loading. The computing environment required for the two types of analyses is discussed. It is shown that a simple stress analysis of a geometrically complex mandible can be accomplished using a minicomputer. However, more sophisticated analyses of the same model with dynamic loading or nonlinear materials would require supercomputer capabilities. A supercomputer is also required for modeling the adaptive response of living bone, even when simple geometric and material models are use.\n\nData-intensive computing on numerically-insensitive supercomputers\n\nDOE Office of Scientific and Technical Information (OSTI.GOV)\n\nAhrens, James P; Fasel, Patricia K; Habib, Salman\n\n2010-12-03\n\nWith the advent of the era of petascale supercomputing, via the delivery of the Roadrunner supercomputing platform at Los Alamos National Laboratory, there is a pressing need to address the problem of visualizing massive petascale-sized results. In this presentation, I discuss progress on a number of approaches including in-situ analysis, multi-resolution out-of-core streaming and interactive rendering on the supercomputing platform. These approaches are placed in context by the emerging area of data-intensive supercomputing.\n\nIntroducing Argonneâs Theta Supercomputer\n\nDOE Office of Scientific and Technical Information (OSTI.GOV)\n\nNone\n\nTheta, the Argonne Leadership Computing Facilityâs (ALCF) new Intel-Cray supercomputer, is officially open to the research community. Thetaâs massively parallel, many-core architecture puts the ALCF on the path to Aurora, the facilityâs future Intel-Cray system. Capable of nearly 10 quadrillion calculations per second, Theta enables researchers to break new ground in scientific investigations that range from modeling the inner workings of the brain to developing new materials for renewable energy applications.\n\nTOP500 Supercomputers for June 2004\n\nDOE Office of Scientific and Technical Information (OSTI.GOV)\n\nStrohmaier, Erich; Meuer, Hans W.; Dongarra, Jack\n\n2004-06-23\n\n23rd Edition of TOP500 List of World's Fastest Supercomputers Released: Japan's Earth Simulator Enters Third Year in Top Position MANNHEIM, Germany; KNOXVILLE, Tenn.;&BERKELEY, Calif. In what has become a closely watched event in the world of high-performance computing, the 23rd edition of the TOP500 list of the world's fastest supercomputers was released today (June 23, 2004) at the International Supercomputer Conference in Heidelberg, Germany.\n\nResearchers experience of misconduct in research in Malaysian higher education institutions.\n\nPubMed\n\nOlesen, Angelina Patrick; Amin, Latifah; Mahadi, Zurina\n\n2018-01-01\n\nThis article offers a qualitative analysis of research misconduct witnessed by researchers during their careers, either by research students or fellow researchers, when conducting or supervising research in their respective departments. Interviews were conducted with 21 participants from various research backgrounds and with a range of research experience, from selected universities in Malaysia. Our study found that misbehavior such as manipulating research data, misrepresentation of research outcomes, plagiarism, authorship disputes, breaching of research protocols, and unethical research management was witnessed by participants among junior and senior researchers, albeit for different reasons. This indicates that despite the steps taken by the institutions to monitor research misconduct, it still occurs in the research community in Malaysian institution of higher education. Therefore, it is important to admit that misconduct still occurs and to create awareness and knowledge of it, particularly among the younger generation of researchers. The study concludes that it is better for researchers to be aware of the behaviors that are considered misconduct as well as the factors that contribute to misconduct to solve this problem.\n\nA mass storage system for supercomputers based on Unix\n\nNASA Technical Reports Server (NTRS)\n\nRichards, J.; Kummell, T.; Zarlengo, D. G.\n\n1988-01-01\n\nThe authors present the design, implementation, and utilization of a large mass storage subsystem (MSS) for the numerical aerodynamics simulation. The MSS supports a large networked, multivendor Unix-based supercomputing facility. The MSS at Ames Research Center provides all processors on the numerical aerodynamics system processing network, from workstations to supercomputers, the ability to store large amounts of data in a highly accessible, long-term repository. The MSS uses Unix System V and is capable of storing hundreds of thousands of files ranging from a few bytes to 2 Gb in size.\n\nQCD on the BlueGene/L Supercomputer\n\nNASA Astrophysics Data System (ADS)\n\nBhanot, G.; Chen, D.; Gara, A.; Sexton, J.; Vranas, P.\n\n2005-03-01\n\nIn June 2004 QCD was simulated for the first time at sustained speed exceeding 1 TeraFlops in the BlueGene/L supercomputer at the IBM T.J. Watson Research Lab. The implementation and performance of QCD in the BlueGene/L is presented.\n\nDistributed user services for supercomputers\n\nNASA Technical Reports Server (NTRS)\n\nSowizral, Henry A.\n\n1989-01-01\n\nUser-service operations at supercomputer facilities are examined. The question is whether a single, possibly distributed, user-services organization could be shared by NASA's supercomputer sites in support of a diverse, geographically dispersed, user community. A possible structure for such an organization is identified as well as some of the technologies needed in operating such an organization.\n\nGREEN SUPERCOMPUTING IN A DESKTOP BOX\n\nDOE Office of Scientific and Technical Information (OSTI.GOV)\n\nHSU, CHUNG-HSING; FENG, WU-CHUN; CHING, AVERY\n\n2007-01-17\n\nThe computer workstation, introduced by Sun Microsystems in 1982, was the tool of choice for scientists and engineers as an interactive computing environment for the development of scientific codes. However, by the mid-1990s, the performance of workstations began to lag behind high-end commodity PCs. This, coupled with the disappearance of BSD-based operating systems in workstations and the emergence of Linux as an open-source operating system for PCs, arguably led to the demise of the workstation as we knew it. Around the same time, computational scientists started to leverage PCs running Linux to create a commodity-based (Beowulf) cluster that provided dedicatedmoreÂ Â» computer cycles, i.e., supercomputing for the rest of us, as a cost-effective alternative to large supercomputers, i.e., supercomputing for the few. However, as the cluster movement has matured, with respect to cluster hardware and open-source software, these clusters have become much more like their large-scale supercomputing brethren - a shared (and power-hungry) datacenter resource that must reside in a machine-cooled room in order to operate properly. Consequently, the above observations, when coupled with the ever-increasing performance gap between the PC and cluster supercomputer, provide the motivation for a 'green' desktop supercomputer - a turnkey solution that provides an interactive and parallel computing environment with the approximate form factor of a Sun SPARCstation 1 'pizza box' workstation. In this paper, they present the hardware and software architecture of such a solution as well as its prowess as a developmental platform for parallel codes. In short, imagine a 12-node personal desktop supercomputer that achieves 14 Gflops on Linpack but sips only 185 watts of power at load, resulting in a performance-power ratio that is over 300% better than their reference SMP platform.Â«Â less\n\nSupercomputing Sheds Light on the Dark Universe\n\nDOE Office of Scientific and Technical Information (OSTI.GOV)\n\nHabib, Salman; Heitmann, Katrin\n\n2012-11-15\n\nAt Argonne National Laboratory, scientists are using supercomputers to shed light on one of the great mysteries in science today, the Dark Universe. With Mira, a petascale supercomputer at the Argonne Leadership Computing Facility, a team led by physicists Salman Habib and Katrin Heitmann will run the largest, most complex simulation of the universe ever attempted. By contrasting the results from Mira with state-of-the-art telescope surveys, the scientists hope to gain new insights into the distribution of matter in the universe, advancing future investigations of dark energy and dark matter into a new realm. The team's research was named amoreÂ Â» finalist for the 2012 Gordon Bell Prize, an award recognizing outstanding achievement in high-performance computing.Â«Â less\n\nHigh Performance Distributed Computing in a Supercomputer Environment: Computational Services and Applications Issues\n\nNASA Technical Reports Server (NTRS)\n\nKramer, Williams T. C.; Simon, Horst D.\n\n1994-01-01\n\nThis tutorial proposes to be a practical guide for the uninitiated to the main topics and themes of high-performance computing (HPC), with particular emphasis to distributed computing. The intent is first to provide some guidance and directions in the rapidly increasing field of scientific computing using both massively parallel and traditional supercomputers. Because of their considerable potential computational power, loosely or tightly coupled clusters of workstations are increasingly considered as a third alternative to both the more conventional supercomputers based on a small number of powerful vector processors, as well as high massively parallel processors. Even though many research issues concerning the effective use of workstation clusters and their integration into a large scale production facility are still unresolved, such clusters are already used for production computing. In this tutorial we will utilize the unique experience made at the NAS facility at NASA Ames Research Center. Over the last five years at NAS massively parallel supercomputers such as the Connection Machines CM-2 and CM-5 from Thinking Machines Corporation and the iPSC/860 (Touchstone Gamma Machine) and Paragon Machines from Intel were used in a production supercomputer center alongside with traditional vector supercomputers such as the Cray Y-MP and C90.\n\nMalaysian Twin Registry.\n\nPubMed\n\nJahanfar, Shayesteh; Jaffar, Sharifah Halimah\n\n2013-02-01\n\nThe National Malaysian Twin Registry was established in Royal College of Medicine, Perak, University Kuala Lumpur (UniKL) in June 2008 through a grant provided by UniKL. The general objective is to facilitate scientific research involving participation of twins and their family members in order to answer questions of health and wellbeing relevant to Malaysians. Recruitment is done via mass media, poster, and pamphlets. We now have 266 adult and 204 children twins registered. Several research projects including reproductive health study of twins and the role of co-bedding on growth and development of children are carried out. Registry holds annual activities for twins and seeks to provide health-related information for twins. We seek international collaboration.\n\nTOP500 Supercomputers for November 2003\n\nDOE Office of Scientific and Technical Information (OSTI.GOV)\n\nStrohmaier, Erich; Meuer, Hans W.; Dongarra, Jack\n\n2003-11-16\n\n22nd Edition of TOP500 List of World s Fastest Supercomputers Released MANNHEIM, Germany; KNOXVILLE, Tenn.; BERKELEY, Calif. In what has become a much-anticipated event in the world of high-performance computing, the 22nd edition of the TOP500 list of the worlds fastest supercomputers was released today (November 16, 2003). The Earth Simulator supercomputer retains the number one position with its Linpack benchmark performance of 35.86 Tflop/s (''teraflops'' or trillions of calculations per second). It was built by NEC and installed last year at the Earth Simulator Center in Yokohama, Japan.\n\nSupercomputing in Aerospace\n\nNASA Technical Reports Server (NTRS)\n\nKutler, Paul; Yee, Helen\n\n1987-01-01\n\nTopics addressed include: numerical aerodynamic simulation; computational mechanics; supercomputers; aerospace propulsion systems; computational modeling in ballistics; turbulence modeling; computational chemistry; computational fluid dynamics; and computational astrophysics.\n\n48 CFR 225.7012 - Restriction on supercomputers.\n\nCode of Federal Regulations, 2014 CFR\n\n2014-10-01\n\n... 48 Federal Acquisition Regulations System 3 2014-10-01 2014-10-01 false Restriction on supercomputers. 225.7012 Section 225.7012 Federal Acquisition Regulations System DEFENSE ACQUISITION REGULATIONS... supercomputers. ...\n\n48 CFR 225.7012 - Restriction on supercomputers.\n\nCode of Federal Regulations, 2010 CFR\n\n2010-10-01\n\n... 48 Federal Acquisition Regulations System 3 2010-10-01 2010-10-01 false Restriction on supercomputers. 225.7012 Section 225.7012 Federal Acquisition Regulations System DEFENSE ACQUISITION REGULATIONS... supercomputers. ...\n\n48 CFR 225.7012 - Restriction on supercomputers.\n\nCode of Federal Regulations, 2013 CFR\n\n2013-10-01\n\n... 48 Federal Acquisition Regulations System 3 2013-10-01 2013-10-01 false Restriction on supercomputers. 225.7012 Section 225.7012 Federal Acquisition Regulations System DEFENSE ACQUISITION REGULATIONS... supercomputers. ...\n\n48 CFR 225.7012 - Restriction on supercomputers.\n\nCode of Federal Regulations, 2011 CFR\n\n2011-10-01\n\n... 48 Federal Acquisition Regulations System 3 2011-10-01 2011-10-01 false Restriction on supercomputers. 225.7012 Section 225.7012 Federal Acquisition Regulations System DEFENSE ACQUISITION REGULATIONS... supercomputers. ...\n\n48 CFR 225.7012 - Restriction on supercomputers.\n\nCode of Federal Regulations, 2012 CFR\n\n2012-10-01\n\n... 48 Federal Acquisition Regulations System 3 2012-10-01 2012-10-01 false Restriction on supercomputers. 225.7012 Section 225.7012 Federal Acquisition Regulations System DEFENSE ACQUISITION REGULATIONS... supercomputers. ...\n\nSupercomputer networking for space science applications\n\nNASA Technical Reports Server (NTRS)\n\nEdelson, B. I.\n\n1992-01-01\n\nThe initial design of a supercomputer network topology including the design of the communications nodes along with the communications interface hardware and software is covered. Several space science applications that are proposed experiments by GSFC and JPL for a supercomputer network using the NASA ACTS satellite are also reported.\n\nInput/output behavior of supercomputing applications\n\nNASA Technical Reports Server (NTRS)\n\nMiller, Ethan L.\n\n1991-01-01\n\nThe collection and analysis of supercomputer I/O traces and their use in a collection of buffering and caching simulations are described. This serves two purposes. First, it gives a model of how individual applications running on supercomputers request file system I/O, allowing system designer to optimize I/O hardware and file system algorithms to that model. Second, the buffering simulations show what resources are needed to maximize the CPU utilization of a supercomputer given a very bursty I/O request rate. By using read-ahead and write-behind in a large solid stated disk, one or two applications were sufficient to fully utilize a Cray Y-MP CPU.\n\nOpenMP Performance on the Columbia Supercomputer\n\nNASA Technical Reports Server (NTRS)\n\nHaoqiang, Jin; Hood, Robert\n\n2005-01-01\n\nThis presentation discusses Columbia World Class Supercomputer which is one of the world's fastest supercomputers providing 61 TFLOPs (10/20/04). Conceived, designed, built, and deployed in just 120 days. A 20-node supercomputer built on proven 512-processor nodes. The largest SGI system in the world with over 10,000 Intel Itanium 2 processors and provides the largest node size incorporating commodity parts (512) and the largest shared-memory environment (2048) with 88% efficiency tops the scalar systems on the Top500 list.\n\nAccess to Supercomputers. Higher Education Panel Report 69.\n\nERIC Educational Resources Information Center\n\nHolmstrom, Engin Inel\n\nThis survey was conducted to provide the National Science Foundation with baseline information on current computer use in the nation's major research universities, including the actual and potential use of supercomputers. Questionnaires were sent to 207 doctorate-granting institutions; after follow-ups, 167 institutions (91% of the institutionsâ¦\n\nSeismic signal processing on heterogeneous supercomputers\n\nNASA Astrophysics Data System (ADS)\n\nGokhberg, Alexey; Ermert, Laura; Fichtner, Andreas\n\n2015-04-01\n\nThe processing of seismic signals - including the correlation of massive ambient noise data sets - represents an important part of a wide range of seismological applications. It is characterized by large data volumes as well as high computational input/output intensity. Development of efficient approaches towards seismic signal processing on emerging high performance computing systems is therefore essential. Heterogeneous supercomputing systems introduced in the recent years provide numerous computing nodes interconnected via high throughput networks, every node containing a mix of processing elements of different architectures, like several sequential processor cores and one or a few graphical processing units (GPU) serving as accelerators. A typical representative of such computing systems is \"Piz Daint\", a supercomputer of the Cray XC 30 family operated by the Swiss National Supercomputing Center (CSCS), which we used in this research. Heterogeneous supercomputers provide an opportunity for manifold application performance increase and are more energy-efficient, however they have much higher hardware complexity and are therefore much more difficult to program. The programming effort may be substantially reduced by the introduction of modular libraries of software components that can be reused for a wide class of seismology applications. The ultimate goal of this research is design of a prototype for such library suitable for implementing various seismic signal processing applications on heterogeneous systems. As a representative use case we have chosen an ambient noise correlation application. Ambient noise interferometry has developed into one of the most powerful tools to image and monitor the Earth's interior. Future applications will require the extraction of increasingly small details from noise recordings. To meet this demand, more advanced correlation techniques combined with very large data volumes are needed. This poses new computational problems that\n\nThe role of graphics super-workstations in a supercomputing environment\n\nNASA Technical Reports Server (NTRS)\n\nLevin, E.\n\n1989-01-01\n\nA new class of very powerful workstations has recently become available which integrate near supercomputer computational performance with very powerful and high quality graphics capability. These graphics super-workstations are expected to play an increasingly important role in providing an enhanced environment for supercomputer users. Their potential uses include: off-loading the supercomputer (by serving as stand-alone processors, by post-processing of the output of supercomputer calculations, and by distributed or shared processing), scientific visualization (understanding of results, communication of results), and by real time interaction with the supercomputer (to steer an iterative computation, to abort a bad run, or to explore and develop new algorithms).\n\n48 CFR 252.225-7011 - Restriction on acquisition of supercomputers.\n\nCode of Federal Regulations, 2010 CFR\n\n2010-10-01\n\n... of supercomputers. 252.225-7011 Section 252.225-7011 Federal Acquisition Regulations System DEFENSE... CLAUSES Text of Provisions And Clauses 252.225-7011 Restriction on acquisition of supercomputers. As prescribed in 225.7012-3, use the following clause: Restriction on Acquisition of Supercomputers (JUN 2005...\n\n48 CFR 252.225-7011 - Restriction on acquisition of supercomputers.\n\nCode of Federal Regulations, 2014 CFR\n\n2014-10-01\n\n... of supercomputers. 252.225-7011 Section 252.225-7011 Federal Acquisition Regulations System DEFENSE... CLAUSES Text of Provisions And Clauses 252.225-7011 Restriction on acquisition of supercomputers. As prescribed in 225.7012-3, use the following clause: Restriction on Acquisition of Supercomputers (JUN 2005...\n\n48 CFR 252.225-7011 - Restriction on acquisition of supercomputers.\n\nCode of Federal Regulations, 2012 CFR\n\n2012-10-01\n\n... of supercomputers. 252.225-7011 Section 252.225-7011 Federal Acquisition Regulations System DEFENSE... CLAUSES Text of Provisions And Clauses 252.225-7011 Restriction on acquisition of supercomputers. As prescribed in 225.7012-3, use the following clause: Restriction on Acquisition of Supercomputers (JUN 2005...\n\n48 CFR 252.225-7011 - Restriction on acquisition of supercomputers.\n\nCode of Federal Regulations, 2013 CFR\n\n2013-10-01\n\n... of supercomputers. 252.225-7011 Section 252.225-7011 Federal Acquisition Regulations System DEFENSE... CLAUSES Text of Provisions And Clauses 252.225-7011 Restriction on acquisition of supercomputers. As prescribed in 225.7012-3, use the following clause: Restriction on Acquisition of Supercomputers (JUN 2005...\n\n48 CFR 252.225-7011 - Restriction on acquisition of supercomputers.\n\nCode of Federal Regulations, 2011 CFR\n\n2011-10-01\n\n... of supercomputers. 252.225-7011 Section 252.225-7011 Federal Acquisition Regulations System DEFENSE... CLAUSES Text of Provisions And Clauses 252.225-7011 Restriction on acquisition of supercomputers. As prescribed in 225.7012-3, use the following clause: Restriction on Acquisition of Supercomputers (JUN 2005...\n\nWhat Can We Take Home? Action Research for Malaysian Preservice TESOL Teachers in Australia\n\nERIC Educational Resources Information Center\n\nNeilsen, Rod\n\n2014-01-01\n\nAction Research (AR) is recognised as an effective way for language teachers to extend teaching skills and gain more understanding of teaching, learning and the classroom environment (Burns, 2010). It can also be a useful but challenging experience for trainee language teachers. This paper reports on the experiences of Malaysian trainee primaryâ¦\n\nThe Sky's the Limit When Super Students Meet Supercomputers.\n\nERIC Educational Resources Information Center\n\nTrotter, Andrew\n\n1991-01-01\n\nIn a few select high schools in the U.S., supercomputers are allowing talented students to attempt sophisticated research projects using simultaneous simulations of nature, culture, and technology not achievable by ordinary microcomputers. Schools can get their students online by entering contests and seeking grants and partnerships withâ¦\n\nTowards Efficient Supercomputing: Searching for the Right Efficiency Metric\n\nDOE Office of Scientific and Technical Information (OSTI.GOV)\n\nHsu, Chung-Hsing; Kuehn, Jeffery A; Poole, Stephen W\n\n2012-01-01\n\nThe efficiency of supercomputing has traditionally been in the execution time. In early 2000 s, the concept of total cost of ownership was re-introduced, with the introduction of efficiency measure to include aspects such as energy and space. Yet the supercomputing community has never agreed upon a metric that can cover these aspects altogether and also provide a fair basis for comparison. This paper exam- ines the metrics that have been proposed in the past decade, and proposes a vector-valued metric for efficient supercom- puting. Using this metric, the paper presents a study of where the supercomputing industry has beenmoreÂ Â» and how it stands today with respect to efficient supercomputing.Â«Â less\n\nTOP500 Supercomputers for June 2003\n\nDOE Office of Scientific and Technical Information (OSTI.GOV)\n\nStrohmaier, Erich; Meuer, Hans W.; Dongarra, Jack\n\n2003-06-23\n\n21st Edition of TOP500 List of World's Fastest Supercomputers Released MANNHEIM, Germany; KNOXVILLE, Tenn.;&BERKELEY, Calif. In what has become a much-anticipated event in the world of high-performance computing, the 21st edition of the TOP500 list of the world's fastest supercomputers was released today (June 23, 2003). The Earth Simulator supercomputer built by NEC and installed last year at the Earth Simulator Center in Yokohama, Japan, with its Linpack benchmark performance of 35.86 Tflop/s (teraflops or trillions of calculations per second), retains the number one position. The number 2 position is held by the re-measured ASCI Q system at Los AlamosmoreÂ Â» National Laboratory. With 13.88 Tflop/s, it is the second system ever to exceed the 10 Tflop/smark. ASCIQ was built by Hewlett-Packard and is based on the AlphaServerSC computer system.Â«Â less\n\nMalaysian registered nurses' professional learning.\n\nPubMed\n\nChiu, Lee H\n\n2006-01-01\n\nFindings of a study of the impact of professional learning on Malaysian registered nurses are reported. The offshore delivery post-registration nursing degree programme is a formal aspect of professional learning, which enables Malaysian registered nurses to upgrade their hospital-based training or diploma of nursing qualification to a degree. Using a qualitative case study approach, data were collected from twelve programme graduates, through individual and focus group interviews. The programme promoted their personal professional growth and enhanced their professional development. It increased self-confidence, knowledge, self-fulfillment, critical thinking ability, interpersonal skills, interest in research and research utilisation, and life-long learning. There was evidence of career mobility and a raised awareness of their professional role and responsibility.\n\nJapanese project aims at supercomputer that executes 10 gflops\n\nDOE Office of Scientific and Technical Information (OSTI.GOV)\n\nBurskey, D.\n\n1984-05-03\n\nDubbed supercom by its multicompany design team, the decade-long project's goal is an engineering supercomputer that can execute 10 billion floating-point operations/s-about 20 times faster than today's supercomputers. The project, guided by Japan's Ministry of International Trade and Industry (MITI) and the Agency of Industrial Science and Technology encompasses three parallel research programs, all aimed at some angle of the superconductor. One program should lead to superfast logic and memory circuits, another to a system architecture that will afford the best performance, and the last to the software that will ultimately control the computer. The work on logic and memorymoreÂ Â» chips is based on: GAAS circuit; Josephson junction devices; and high electron mobility transistor structures. The architecture will involve parallel processing.Â«Â less\n\nNSF Commits to Supercomputers.\n\nERIC Educational Resources Information Center\n\nWaldrop, M. Mitchell\n\n1985-01-01\n\nThe National Science Foundation (NSF) has allocated at least $200 million over the next five years to support four new supercomputer centers. Issues and trends related to this NSF initiative are examined. (JN)\n\nPractices of Management Development: A Malaysian Case Study\n\nERIC Educational Resources Information Center\n\nLaw, Kian Aun\n\n2008-01-01\n\nThis paper deals with a case study of Management Development (MD) practices at Malaysian Assurance Alliance (MAA). The aim of this research is to investigate how a large Malaysian insurance corporation developed and integrated MD initiatives with current organizational needs and tasks. Attempts were made to map and categorize the MD initiativesâ¦\n\nCharacterizing output bottlenecks in a supercomputer\n\nDOE Office of Scientific and Technical Information (OSTI.GOV)\n\nXie, Bing; Chase, Jeffrey; Dillow, David A\n\n2012-01-01\n\nSupercomputer I/O loads are often dominated by writes. HPC (High Performance Computing) file systems are designed to absorb these bursty outputs at high bandwidth through massive parallelism. However, the delivered write bandwidth often falls well below the peak. This paper characterizes the data absorption behavior of a center-wide shared Lustre parallel file system on the Jaguar supercomputer. We use a statistical methodology to address the challenges of accurately measuring a shared machine under production load and to obtain the distribution of bandwidth across samples of compute nodes, storage targets, and time intervals. We observe and quantify limitations from competing traffic,moreÂ Â» contention on storage servers and I/O routers, concurrency limitations in the client compute node operating systems, and the impact of variance (stragglers) on coupled output such as striping. We then examine the implications of our results for application performance and the design of I/O middleware systems on shared supercomputers.Â«Â less\n\nKnowledge and beliefs of Malaysian adolescents regarding cancer.\n\nPubMed\n\nAl-Naggar, Redhwan Ahmed; Jillson, Irene Anne; Abu-Hamad, Samir; Mumford, William; Bobryshev, Yuri V\n\n2015-01-01\n\nFew studies have explored the knowledge and attitudes of adolescents toward cancer prevention and treatment. This lack of research and its potential utility in the development of new educational initiatives and screening methods, or the reconstruction of existing ones, provided the impetus for this study. The primary research aim was to assess secondary school student knowledge of cancer and determine whether or not they possessed basic knowledge of cancer symptoms, risk factors, and treatments and to determine the relationship between cancer knowledge and key demographic factors. The Management and Science University conducted a cross-sectional study analyzing responses through cross-tabulation with the socio-demographic data collected. The findings of our quantitative analysis suggest that Malaysian youth generally possess a moderate knowledge about cancer. Quantitative analyses found that socioeconomic inequalities and bias in education present as important factors contributing to cancer awareness, prevention, and treatment among Malaysian adolescents. The findings indicate that Malaysian youth generally possess a moderate knowledge about cancer but the current deficiencies in initiatives directed to cancer awareness continue to hinder the improvement in prevention of cancer among Malaysian adolescents.\n\nMost Social Scientists Shun Free Use of Supercomputers.\n\nERIC Educational Resources Information Center\n\nKiernan, Vincent\n\n1998-01-01\n\nSocial scientists, who frequently complain that the federal government spends too little on them, are passing up what scholars in the physical and natural sciences see as the government's best give-aways: free access to supercomputers. Some social scientists say the supercomputers are difficult to use; others find desktop computers provideâ¦\n\nFloating point arithmetic in future supercomputers\n\nNASA Technical Reports Server (NTRS)\n\nBailey, David H.; Barton, John T.; Simon, Horst D.; Fouts, Martin J.\n\n1989-01-01\n\nConsiderations in the floating-point design of a supercomputer are discussed. Particular attention is given to word size, hardware support for extended precision, format, and accuracy characteristics. These issues are discussed from the perspective of the Numerical Aerodynamic Simulation Systems Division at NASA Ames. The features believed to be most important for a future supercomputer floating-point design include: (1) a 64-bit IEEE floating-point format with 11 exponent bits, 52 mantissa bits, and one sign bit and (2) hardware support for reasonably fast double-precision arithmetic.\n\nIntegration Of PanDA Workload Management System With Supercomputers for ATLAS and Data Intensive Science\n\nDOE Office of Scientific and Technical Information (OSTI.GOV)\n\nDe, K; Jha, S; Klimentov, A\n\n2016-01-01\n\nThe Large Hadron Collider (LHC), operating at the international CERN Laboratory in Geneva, Switzerland, is leading Big Data driven scientific explorations. Experiments at the LHC explore the fundamental nature of matter and the basic forces that shape our universe, and were recently credited for the discovery of a Higgs boson. ATLAS, one of the largest collaborations ever assembled in the sciences, is at the forefront of research at the LHC. To address an unprecedented multi-petabyte data processing challenge, the ATLAS experiment is relying on a heterogeneous distributed computational infrastructure. The ATLAS experiment uses PanDA (Production and Data Analysis) Workload ManagementmoreÂ Â» System for managing the workflow for all data processing on over 150 data centers. Through PanDA, ATLAS physicists see a single computing facility that enables rapid scientific breakthroughs for the experiment, even though the data centers are physically scattered all over the world. While PanDA currently uses more than 250,000 cores with a peak performance of 0.3 petaFLOPS, LHC data taking runs require more resources than Grid computing can possibly provide. To alleviate these challenges, LHC experiments are engaged in an ambitious program to expand the current computing model to include additional resources such as the opportunistic use of supercomputers. We will describe a project aimed at integration of PanDA WMS with supercomputers in United States, Europe and Russia (in particular with Titan supercomputer at Oak Ridge Leadership Computing Facility (OLCF), MIRA supercomputer at Argonne Leadership Computing Facilities (ALCF), Supercomputer at the National Research Center Kurchatov Institute , IT4 in Ostrava and others). Current approach utilizes modified PanDA pilot framework for job submission to the supercomputers batch queues and local data management, with light-weight MPI wrappers to run single threaded workloads in parallel on LCFs multi-core worker nodes. This\n\nGreen Supercomputing at Argonne\n\nScienceCinema\n\nPete Beckman\n\n2017-12-09\n\nPete Beckman, head of Argonne's Leadership Computing Facility (ALCF) talks about Argonne National Laboratory's green supercomputingÃ¢ÂÂeverything from designing algorithms to use fewer kilowatts per operation to using cold Chicago winter air to cool the machine more efficiently.\n\nValidity of the Brunel Mood Scale for use With Malaysian Athletes.\n\nPubMed\n\nLan, Mohamad Faizal; Lane, Andrew M; Roy, Jolly; Hanin, Nik Azma\n\n2012-01-01\n\nThe aim of the present study was to investigate the factorial validity of the Brunel Mood Scale for use with Malaysian athletes. Athletes (N = 1485 athletes) competing at the Malaysian Games completed the Brunel of Mood Scale (BRUMS). Confirmatory Factor Analysis (CFA) results indicated a Confirmatory Fit Index (CFI) of .90 and Root Mean Squared Error of Approximation (RMSEA) was 0.05. The CFI was below the 0.95 criterion for acceptability and the RMSEA value was within the limits for acceptability suggested by Hu and Bentler, 1999. We suggest that results provide some support for validity of the BRUMS for use with Malaysian athletes. Given the large sample size used in the present study, descriptive statistics could be used as normative data for Malaysian athletes. Key pointsFindings from the present study lend support to the validity of the BRUMS for use with Malaysian athletes.Given the size of the sample used in the present study, we suggest descriptive data be used as the normative data for researchers using the scale with Malaysian athletes.It is suggested that future research investigate the effects of cultural differences on emotional states experienced by athletes before, during and post-competition.\n\nDevelopment of seismic tomography software for hybrid supercomputers\n\nNASA Astrophysics Data System (ADS)\n\nNikitin, Alexandr; Serdyukov, Alexandr; Duchkov, Anton\n\n2015-04-01\n\nSeismic tomography is a technique used for computing velocity model of geologic structure from first arrival travel times of seismic waves. The technique is used in processing of regional and global seismic data, in seismic exploration for prospecting and exploration of mineral and hydrocarbon deposits, and in seismic engineering for monitoring the condition of engineering structures and the surrounding host medium. As a consequence of development of seismic monitoring systems and increasing volume of seismic data, there is a growing need for new, more effective computational algorithms for use in seismic tomography applications with improved performance, accuracy and resolution. To achieve this goal, it is necessary to use modern high performance computing systems, such as supercomputers with hybrid architecture that use not only CPUs, but also accelerators and co-processors for computation. The goal of this research is the development of parallel seismic tomography algorithms and software package for such systems, to be used in processing of large volumes of seismic data (hundreds of gigabytes and more). These algorithms and software package will be optimized for the most common computing devices used in modern hybrid supercomputers, such as Intel Xeon CPUs, NVIDIA Tesla accelerators and Intel Xeon Phi co-processors. In this work, the following general scheme of seismic tomography is utilized. Using the eikonal equation solver, arrival times of seismic waves are computed based on assumed velocity model of geologic structure being analyzed. In order to solve the linearized inverse problem, tomographic matrix is computed that connects model adjustments with travel time residuals, and the resulting system of linear equations is regularized and solved to adjust the model. The effectiveness of parallel implementations of existing algorithms on target architectures is considered. During the first stage of this work, algorithms were developed for execution on\n\nNASA's Pleiades Supercomputer Crunches Data For Groundbreaking Analysis and Visualizations\n\nNASA Image and Video Library\n\n2016-11-23\n\nThe Pleiades supercomputer at NASA's Ames Research Center, recently named the 13th fastest computer in the world, provides scientists and researchers high-fidelity numerical modeling of complex systems and processes. By using detailed analyses and visualizations of large-scale data, Pleiades is helping to advance human knowledge and technology, from designing the next generation of aircraft and spacecraft to understanding the Earth's climate and the mysteries of our galaxy.\n\nValidity of the Brunel Mood Scale for use With Malaysian Athletes\n\nPubMed Central\n\nLan, Mohamad Faizal; Lane, Andrew M.; Roy, Jolly; Hanin, Nik Azma\n\n2012-01-01\n\nThe aim of the present study was to investigate the factorial validity of the Brunel Mood Scale for use with Malaysian athletes. Athletes (N = 1485 athletes) competing at the Malaysian Games completed the Brunel of Mood Scale (BRUMS). Confirmatory Factor Analysis (CFA) results indicated a Confirmatory Fit Index (CFI) of .90 and Root Mean Squared Error of Approximation (RMSEA) was 0.05. The CFI was below the 0.95 criterion for acceptability and the RMSEA value was within the limits for acceptability suggested by Hu and Bentler, 1999. We suggest that results provide some support for validity of the BRUMS for use with Malaysian athletes. Given the large sample size used in the present study, descriptive statistics could be used as normative data for Malaysian athletes. Key points Findings from the present study lend support to the validity of the BRUMS for use with Malaysian athletes. Given the size of the sample used in the present study, we suggest descriptive data be used as the normative data for researchers using the scale with Malaysian athletes. It is suggested that future research investigate the effects of cultural differences on emotional states experienced by athletes before, during and post-competition. PMID:24149128\n\nThe Role of Social Media for Collaborative Learning to Improve Academic Performance of Students and Researchers in Malaysian Higher Education\n\nERIC Educational Resources Information Center\n\nAl-Rahmi, Waleed Mugahed; Othman, Mohd Shahizan; Yusuf, Lizawati Mi\n\n2015-01-01\n\nSocial media is widely considered to improve collaborative learning among students and researchers. However, there is a surprising lack of empirical research in Malaysian higher education to improve performance of students and researchers through the effective use of social media that facilitates desirable outcomes. Thus, this study offers aâ¦\n\nKriging for Spatial-Temporal Data on the Bridges Supercomputer\n\nNASA Astrophysics Data System (ADS)\n\nHodgess, E. M.\n\n2017-12-01\n\nCurrently, kriging of spatial-temporal data is slow and limited to relatively small vector sizes. We have developed a method on the Bridges supercomputer, at the Pittsburgh supercomputer center, which uses a combination of the tools R, Fortran, the Message Passage Interface (MPI), OpenACC, and special R packages for big data. This combination of tools now permits us to complete tasks which could previously not be completed, or takes literally hours to complete. We ran simulation studies from a laptop against the supercomputer. We also look at \"real world\" data sets, such as the Irish wind data, and some weather data. We compare the timings. We note that the timings are suprising good.\n\n\"All Abroad\": Malaysians' Reasons for Seeking an Overseas-Based Doctorate\n\nERIC Educational Resources Information Center\n\nTagg, Brendon\n\n2014-01-01\n\nThis article examines the process by which nine junior Malaysian academics came to complete doctoral degrees in non-Malaysian universities. It expands the scope and refines the focus of an existing study that considered international students' experiences in New Zealand. Part of the motivation for the current study was the researcher's recognitionâ¦\n\nThe impact of the U.S. supercomputing initiative will be global\n\nDOE Office of Scientific and Technical Information (OSTI.GOV)\n\nCrawford, Dona\n\n2016-01-15\n\nLast July, President Obama issued an executive order that created a coordinated federal strategy for HPC research, development, and deployment called the U.S. National Strategic Computing Initiative (NSCI). However, this bold, necessary step toward building the next generation of supercomputers has inaugurated a new era for U.S. high performance computing (HPC).\n\nSupercomputer Simulations Help Develop New Approach to Fight Antibiotic Resistance\n\nScienceCinema\n\nZgurskaya, Helen; Smith, Jeremy\n\n2018-06-13\n\nORNL leveraged powerful supercomputing to support research led by University of Oklahoma scientists to identify chemicals that seek out and disrupt bacterial proteins called efflux pumps, known to be a major cause of antibiotic resistance. By running simulations on Titan, the team selected molecules most likely to target and potentially disable the assembly of efflux pumps found in E. coli bacteria cells.\n\nParticle simulation on heterogeneous distributed supercomputers\n\nNASA Technical Reports Server (NTRS)\n\nBecker, Jeffrey C.; Dagum, Leonardo\n\n1993-01-01\n\nWe describe the implementation and performance of a three dimensional particle simulation distributed between a Thinking Machines CM-2 and a Cray Y-MP. These are connected by a combination of two high-speed networks: a high-performance parallel interface (HIPPI) and an optical network (UltraNet). This is the first application to use this configuration at NASA Ames Research Center. We describe our experience implementing and using the application and report the results of several timing measurements. We show that the distribution of applications across disparate supercomputing platforms is feasible and has reasonable performance. In addition, several practical aspects of the computing environment are discussed.\n\nMalaysian researchers talk about the influence of culture on research misconduct in higher learning institutions.\n\nPubMed\n\nOlesen, Angelina P; Amin, Latifah; Mahadi, Zurina\n\n2017-01-01\n\nBased on a previous survey by the Office of Research Integrity (ORI) in the USA, a considerable number of foreign research scientists have been found guilty of research misconduct. However, it remains unclear as to whether or not cultural factors really contribute to research misconduct. This study is based on a series of interviews with Malaysian researchers from the local universities regarding their own professional experiences involving working with researchers or research students from different countries or of different nationalities. Most of the researchers interviewed agreed that cultures do shape individual character, which influences the way that such individuals conduct research, their decision-making, and their style of academic writing. Our findings also showed that working culture within the institution also influences research practices, as well as faculty mentorship of the younger generation of researchers. Given the fact such misconduct might be due to a lack of understanding of research or working cultures or practices within the institution, the impact on the scientific community and on society could be destructive. Therefore, it is suggested that the institution has an important role to play in orienting foreign researchers through training, mentoring, and discussion with regard to the \"does\" and \"don'ts\" related to research, and to provide them with an awareness of the importance of ethics when it comes to conducting research.\n\nNational Test Facility civilian agency use of supercomputers not feasible\n\nDOE Office of Scientific and Technical Information (OSTI.GOV)\n\nNONE\n\n1994-12-01\n\nBased on interviews with civilian agencies cited in the House report (DOE, DoEd, HHS, FEMA, NOAA), none would be able to make effective use of NTF`s excess supercomputing capabilities. These agencies stated they could not use the resources primarily because (1) NTF`s supercomputers are older machines whose performance and costs cannot match those of more advanced computers available from other sources and (2) some agencies have not yet developed applications requiring supercomputer capabilities or do not have funding to support such activities. In addition, future support for the hardware and software at NTF is uncertain, making any investment by anmoreÂ Â» outside user risky.Â«Â less\n\nBarriers to Seeking School Counselling: Malaysian Chinese School Students' Perspectives\n\nERIC Educational Resources Information Center\n\nChen, Kai Shen; Kok, Jin Kuan\n\n2017-01-01\n\nSchool counselling services have always been unpopular among Malaysian students. Many researchers have studied what prevents students from seeking mental health services. However, there is a lack of study on the barriers to seeking help in the context of Malaysian school counselling services. Using a sample of Chinese high school students (N =â¦\n\nSupercomputers Of The Future\n\nNASA Technical Reports Server (NTRS)\n\nPeterson, Victor L.; Kim, John; Holst, Terry L.; Deiwert, George S.; Cooper, David M.; Watson, Andrew B.; Bailey, F. Ron\n\n1992-01-01\n\nReport evaluates supercomputer needs of five key disciplines: turbulence physics, aerodynamics, aerothermodynamics, chemistry, and mathematical modeling of human vision. Predicts these fields will require computer speed greater than 10(Sup 18) floating-point operations per second (FLOP's) and memory capacity greater than 10(Sup 15) words. Also, new parallel computer architectures and new structured numerical methods will make necessary speed and capacity available.\n\nIn Their Own Words: Research Misconduct from the Perspective of Researchers in Malaysian Universities.\n\nPubMed\n\nOlesen, Angelina P; Amin, Latifah; Mahadi, Zurina\n\n2017-12-16\n\nPublished data and studies on research misconduct, which focuses on researchers in Malaysia, is still lacking, therefore, we decided that this was an area for investigation. This study provides qualitative results for the examined issues through series of in-depth interviews with 21 researchers and lecturers in various universities in Malaysia. The aims of this study were to investigate the researchers' opinions and perceptions regarding what they considered to be research misconduct, their experience with such misconduct, and the factors that contribute to research misconduct. Our findings suggest that the most common research misconducts that are currently being witnessed in Malaysian universities are plagiarism and authorship disputes, however, researchers seldom report incidents of research misconduct because it takes too much time, effort and work to report them, and some are just afraid of repercussions when they do report it. This suggests possible loopholes in the monitoring system, which may allow some researchers to bypass it and engage in misconduct. This study also highlights the structural and individual factors as the most influential factors when it comes to research misconduct besides organizational, situational and cultural factors. Finally, this study highlights the concerns of all participants regarding the 'publish or perish' pressure that they believe would lead to a hostile working environment, thus enhancing research misconduct, as researchers tend to think about their own performance rather than that of whole team or faculty. Consequently this weakens the interpersonal relationships among researchers, which may compromise the teaching and supervision of junior researchers and research students.\n\nNASA Advanced Supercomputing Facility Expansion\n\nNASA Technical Reports Server (NTRS)\n\nThigpen, William W.\n\n2017-01-01\n\nThe NASA Advanced Supercomputing (NAS) Division enables advances in high-end computing technologies and in modeling and simulation methods to tackle some of the toughest science and engineering challenges facing NASA today. The name \"NAS\" has long been associated with leadership and innovation throughout the high-end computing (HEC) community. We play a significant role in shaping HEC standards and paradigms, and provide leadership in the areas of large-scale InfiniBand fabrics, Lustre open-source filesystems, and hyperwall technologies. We provide an integrated high-end computing environment to accelerate NASA missions and make revolutionary advances in science. Pleiades, a petaflop-scale supercomputer, is used by scientists throughout the U.S. to support NASA missions, and is ranked among the most powerful systems in the world. One of our key focus areas is in modeling and simulation to support NASA's real-world engineering applications and make fundamental advances in modeling and simulation methods.\n\nInclusion in Malaysian Integrated Preschools\n\nERIC Educational Resources Information Center\n\nSukumaran, Sailajah; Loveridge, Judith; Green, Vanessa A.\n\n2015-01-01\n\nInclusive education has been introduced through a number of policy developments in Malaysia over the last 10 years but there is little research investigating the extent and nature of inclusive education for preschoolers with special educational needs (SEN). This study surveyed both regular and special education teachers in Malaysian integratedâ¦\n\nPublic acceptance of nuclear power among Malaysian students\n\nNASA Astrophysics Data System (ADS)\n\nMuhamad Pauzi, Anas; Saad, Juniza Md; Arif Abu Bakar, Asyraf; Hannan Damahuri, Abdul; Syukri, Nur Syamim Mohd\n\n2018-01-01\n\nMalaysian governmentâs aim to include nuclear energy for electricity generation has triggered various reactions from all especially the public. The objective of this study is to have a better understanding on the knowledge, sources of information of nuclear power and sources of energy chosen by Malaysian in 20 yearsâ time. Besides that, we want to examine the level of acceptance and perception of Malaysian towards nuclear energy and we want to identify the correlation between public perceptions with the acceptance towards nuclear power in Malaysia, and also to study the differences between perception and acceptance of nuclear power with gender and educational level. For this research methodology, the research questions are given orally or through paper-pencil and also social networking site such as Facebook or through electronic media application such as WhatsApp and Google docs. The data were analysed using a SPSS version 22.0 (Statistical Package for the Social Sciences). Results showed that more than 50% of the respondents have the knowledge of nuclear energy. A part of from that, only 39 % are confident government can afford to build NPP in Malaysia and 41 % disagree nuclear energy is the best option for future energy. From analysis using SPSS 22 we estimate negative perception will give a negative acceptance in term of support towards the use of nuclear energy in power generation in Malaysia. There are also slight correlation that the higher the level of education of Malaysian, the more negative the perception of Malaysian in accepting nuclear energy as source of power in Malaysia. Therefore in shaping a positive acceptance of NPP in Malaysia, the authorities need to educate the people with the knowledge of nuclear in order to overcome the negative perception towards nuclear power.\n\nSupercomputing Drives Innovation - Continuum Magazine | NREL\n\nScience.gov Websites\n\nyears, NREL scientists have used supercomputers to simulate 3D models of the primary enzymes and Scientist, discuss a 3D model of wind plant aerodynamics, showing low velocity wakes and impact on\n\nProspects for Boiling of Subcooled Dielectric Liquids for Supercomputer Cooling\n\nNASA Astrophysics Data System (ADS)\n\nZeigarnik, Yu. A.; Vasil'ev, N. V.; Druzhinin, E. A.; Kalmykov, I. V.; Kosoi, A. S.; Khodakov, K. A.\n\n2018-02-01\n\nIt is shown experimentally that using forced-convection boiling of dielectric coolants of the Novec 649 Refrigerant subcooled relative to the saturation temperature makes possible removing heat flow rates up to 100 W/cm2 from modern supercomputer chip interface. This fact creates prerequisites for the application of dielectric liquids in cooling systems of modern supercomputers with increased requirements for their operating reliability.\n\nSupercomputer optimizations for stochastic optimal control applications\n\nNASA Technical Reports Server (NTRS)\n\nChung, Siu-Leung; Hanson, Floyd B.; Xu, Huihuang\n\n1991-01-01\n\nSupercomputer optimizations for a computational method of solving stochastic, multibody, dynamic programming problems are presented. The computational method is valid for a general class of optimal control problems that are nonlinear, multibody dynamical systems, perturbed by general Markov noise in continuous time, i.e., nonsmooth Gaussian as well as jump Poisson random white noise. Optimization techniques for vector multiprocessors or vectorizing supercomputers include advanced data structures, loop restructuring, loop collapsing, blocking, and compiler directives. These advanced computing techniques and superconducting hardware help alleviate Bellman's curse of dimensionality in dynamic programming computations, by permitting the solution of large multibody problems. Possible applications include lumped flight dynamics models for uncertain environments, such as large scale and background random aerospace fluctuations.\n\nIntegration Of PanDA Workload Management System With Supercomputers for ATLAS and Data Intensive Science\n\nNASA Astrophysics Data System (ADS)\n\nKlimentov, A.; De, K.; Jha, S.; Maeno, T.; Nilsson, P.; Oleynik, D.; Panitkin, S.; Wells, J.; Wenaus, T.\n\n2016-10-01\n\nThe.LHC, operating at CERN, is leading Big Data driven scientific explorations. Experiments at the LHC explore the fundamental nature of matter and the basic forces that shape our universe. ATLAS, one of the largest collaborations ever assembled in the sciences, is at the forefront of research at the LHC. To address an unprecedented multi-petabyte data processing challenge, the ATLAS experiment is relying on a heterogeneous distributed computational infrastructure. The ATLAS experiment uses PanDA (Production and Data Analysis) Workload Management System for managing the workflow for all data processing on over 150 data centers. Through PanDA, ATLAS physicists see a single computing facility that enables rapid scientific breakthroughs for the experiment, even though the data centers are physically scattered all over the world. While PanDA currently uses more than 250,000 cores with a peak performance of 0.3 petaFLOPS, LHC data taking runs require more resources than grid can possibly provide. To alleviate these challenges, LHC experiments are engaged in an ambitious program to expand the current computing model to include additional resources such as the opportunistic use of supercomputers. We will describe a project aimed at integration of PanDA WMS with supercomputers in United States, in particular with Titan supercomputer at Oak Ridge Leadership Computing Facility. Current approach utilizes modified PanDA pilot framework for job submission to the supercomputers batch queues and local data management, with light-weight MPI wrappers to run single threaded workloads in parallel on LCFs multi-core worker nodes. This implementation was tested with a variety of Monte-Carlo workloads on several supercomputing platforms for ALICE and ATLAS experiments and it is in full pro duction for the ATLAS since September 2015. We will present our current accomplishments with running PanDA at supercomputers and demonstrate our ability to use PanDA as a portal independent of the\n\nIntroducing Mira, Argonne's Next-Generation Supercomputer\n\nDOE Office of Scientific and Technical Information (OSTI.GOV)\n\nNone\n\n2013-03-19\n\nMira, the new petascale IBM Blue Gene/Q system installed at the ALCF, will usher in a new era of scientific supercomputing. An engineering marvel, the 10-petaflops machine is capable of carrying out 10 quadrillion calculations per second.\n\nTeachers' Perception of Mobile Edutainment for Special Needs Learners: The Malaysian Case\n\nERIC Educational Resources Information Center\n\nMohd Yusof, Anuar; Daniel, Esther Gnanamalar Sarojini; Low, Wah Yun; Ab. Aziz, Kamarulzaman\n\n2014-01-01\n\nStudy of Malaysian adoption of mobile learning (m-learning) is still in the early stages. However, there are numerous researchers in the country exploring the potential and application of m-learning in the Malaysian education system, including special education. A key question is whether teachers are prepared to incorporate mobile technology asâ¦\n\nDiscover Supercomputer 5\n\nNASA Image and Video Library\n\n2017-12-08\n\nTwo rows of the âDiscoverâ supercomputer at the NASA Center for Climate Simulation (NCCS) contain more than 4,000 computer processors. Discover has a total of nearly 15,000 processors. Credit: NASA/Pat Izzo To learn more about NCCS go to: www.nasa.gov/topics/earth/features/climate-sim-center.html NASA Goddard Space Flight Center is home to the nation's largest organization of combined scientists, engineers and technologists that build spacecraft, instruments and new technology to study the Earth, the sun, our solar system, and the universe.\n\nDiscover Supercomputer 3\n\nNASA Image and Video Library\n\n2017-12-08\n\nThe heart of the NASA Center for Climate Simulation (NCCS) is the âDiscoverâ supercomputer. In 2009, NCCS added more than 8,000 computer processors to Discover, for a total of nearly 15,000 processors. Credit: NASA/Pat Izzo To learn more about NCCS go to: www.nasa.gov/topics/earth/features/climate-sim-center.html NASA Goddard Space Flight Center is home to the nation's largest organization of combined scientists, engineers and technologists that build spacecraft, instruments and new technology to study the Earth, the sun, our solar system, and the universe.\n\nDiscover Supercomputer 2\n\nNASA Image and Video Library\n\n2017-12-08\n\nThe heart of the NASA Center for Climate Simulation (NCCS) is the âDiscoverâ supercomputer. In 2009, NCCS added more than 8,000 computer processors to Discover, for a total of nearly 15,000 processors. Credit: NASA/Pat Izzo To learn more about NCCS go to: www.nasa.gov/topics/earth/features/climate-sim-center.html NASA Goddard Space Flight Center is home to the nation's largest organization of combined scientists, engineers and technologists that build spacecraft, instruments and new technology to study the Earth, the sun, our solar system, and the universe.\n\nDiscover Supercomputer 4\n\nNASA Image and Video Library\n\n2017-12-08\n\nThis close-up view highlights one rowâapproximately 2,000 computer processorsâof the âDiscoverâ supercomputer at the NASA Center for Climate Simulation (NCCS). Discover has a total of nearly 15,000 processors. Credit: NASA/Pat Izzo To learn more about NCCS go to: www.nasa.gov/topics/earth/features/climate-sim-center.html NASA Goddard Space Flight Center is home to the nation's largest organization of combined scientists, engineers and technologists that build spacecraft, instruments and new technology to study the Earth, the sun, our solar system, and the universe.\n\nDiscover Supercomputer 1\n\nNASA Image and Video Library\n\n2017-12-08\n\nThe heart of the NASA Center for Climate Simulation (NCCS) is the âDiscoverâ supercomputer. In 2009, NCCS added more than 8,000 computer processors to Discover, for a total of nearly 15,000 processors. Credit: NASA/Pat Izzo To learn more about NCCS go to: www.nasa.gov/topics/earth/features/climate-sim-center.html NASA Goddard Space Flight Center is home to the nation's largest organization of combined scientists, engineers and technologists that build spacecraft, instruments and new technology to study the Earth, the sun, our solar system, and the universe.\n\nFactors associated with home hazards: Findings from the Malaysian Elders Longitudinal Research study.\n\nPubMed\n\nRomli, Muhammad H; Tan, Maw P; Mackenzie, Lynette; Lovarini, Meryl; Kamaruzzaman, Shahrul B; Clemson, Lindy\n\n2018-03-01\n\nPrevious studies have investigated home hazards as a risk factor for falls without considering factors associated with the presence of home hazards. The present study aimed to determine patterns of home hazards among urban community-dwelling older Malaysians, and to identify factors contributing to home hazards. Cross-sectional data from the initial wave of the Malaysian Elders Longitudinal Research study were used. Basic demographics were obtained from the Global Questionnaire. Basic and instrumental activities of daily living were measured using the Katz and Lawton-Brody scales, and home hazards were identified using the Home Falls and Accidents Screening Tool. Participants were also asked if they had fallen in the previous 12 months. Data were analyzed from 1489 participants. Hazards were frequently identified (>30%) in the toilet and bathroom areas (no grab rail, no non-slip mat, distant toilet), slippery floors, no bedside light access and inappropriate footwear. Lower educational attainment, traditional housing, Chinese ethnicity, greater number of home occupants, lower monthly expenditure, poor vision and younger age were the factors independently associated with home hazards. This study provides evidence that home hazards are a product of the interaction of the individual's function within their home environment. Hazards are also influenced by local sociocultural and environmental factors. The relationship between home hazards and falls appears complex and deserves further evaluation. Geriatr Gerontol Int 2018; 18: 387-395. Â© 2017 Japan Geriatrics Society.\n\nNASA Advanced Supercomputing (NAS) User Services Group\n\nNASA Technical Reports Server (NTRS)\n\nPandori, John; Hamilton, Chris; Niggley, C. E.; Parks, John W. (Technical Monitor)\n\n2002-01-01\n\nThis viewgraph presentation provides an overview of NAS (NASA Advanced Supercomputing), its goals, and its mainframe computer assets. Also covered are its functions, including systems monitoring and technical support.\n\nTraditional Postpartum Practices Among Malaysian Mothers: A Review.\n\nPubMed\n\nFadzil, Fariza; Shamsuddin, Khadijah; Wan Puteh, Sharifa Ezat\n\n2016-07-01\n\nTo briefly describe the postpartum practices among the three major ethnic groups in Malaysia and to identify commonalities in their traditional postpartum beliefs and practices. This narrative review collated information on traditional postpartum practices among Malaysian mothers through a literature search for published research papers on traditional postpartum practices in Malaysia. This review shows that Malaysian mothers have certain postpartum practices that they considered to be important for preventing future ill health. Despite the perceived differences in intra-ethnic postpartum practices, most Malaysian mothers, although from different ethnicities, share similarities in their postpartum regimens and practices in terms of beliefs and adherence to food taboos, use of traditional postpartum massage and traditional herbs, and acknowledgment of the role of older female family members in postpartum care. Health care providers should be aware of multiethnic traditional postpartum practices and use the commonalities in these practices as part of their postpartum care regimen.\n\nCalibrating Building Energy Models Using Supercomputer Trained Machine Learning Agents\n\nDOE Office of Scientific and Technical Information (OSTI.GOV)\n\nSanyal, Jibonananda; New, Joshua Ryan; Edwards, Richard\n\n2014-01-01\n\nBuilding Energy Modeling (BEM) is an approach to model the energy usage in buildings for design and retrofit purposes. EnergyPlus is the flagship Department of Energy software that performs BEM for different types of buildings. The input to EnergyPlus can often extend in the order of a few thousand parameters which have to be calibrated manually by an expert for realistic energy modeling. This makes it challenging and expensive thereby making building energy modeling unfeasible for smaller projects. In this paper, we describe the Autotune research which employs machine learning algorithms to generate agents for the different kinds of standardmoreÂ Â» reference buildings in the U.S. building stock. The parametric space and the variety of building locations and types make this a challenging computational problem necessitating the use of supercomputers. Millions of EnergyPlus simulations are run on supercomputers which are subsequently used to train machine learning algorithms to generate agents. These agents, once created, can then run in a fraction of the time thereby allowing cost-effective calibration of building models.Â«Â less\n\nDesign of multiple sequence alignment algorithms on parallel, distributed memory supercomputers.\n\nPubMed\n\nChurch, Philip C; Goscinski, Andrzej; Holt, Kathryn; Inouye, Michael; Ghoting, Amol; Makarychev, Konstantin; Reumann, Matthias\n\n2011-01-01\n\nThe challenge of comparing two or more genomes that have undergone recombination and substantial amounts of segmental loss and gain has recently been addressed for small numbers of genomes. However, datasets of hundreds of genomes are now common and their sizes will only increase in the future. Multiple sequence alignment of hundreds of genomes remains an intractable problem due to quadratic increases in compute time and memory footprint. To date, most alignment algorithms are designed for commodity clusters without parallelism. Hence, we propose the design of a multiple sequence alignment algorithm on massively parallel, distributed memory supercomputers to enable research into comparative genomics on large data sets. Following the methodology of the sequential progressiveMauve algorithm, we design data structures including sequences and sorted k-mer lists on the IBM Blue Gene/P supercomputer (BG/P). Preliminary results show that we can reduce the memory footprint so that we can potentially align over 250 bacterial genomes on a single BG/P compute node. We verify our results on a dataset of E.coli, Shigella and S.pneumoniae genomes. Our implementation returns results matching those of the original algorithm but in 1/2 the time and with 1/4 the memory footprint for scaffold building. In this study, we have laid the basis for multiple sequence alignment of large-scale datasets on a massively parallel, distributed memory supercomputer, thus enabling comparison of hundreds instead of a few genome sequences within reasonable time.\n\nIntricacies of modern supercomputing illustrated with recent advances in simulations of strongly correlated electron systems\n\nNASA Astrophysics Data System (ADS)\n\nSchulthess, Thomas C.\n\n2013-03-01\n\nThe continued thousand-fold improvement in sustained application performance per decade on modern supercomputers keeps opening new opportunities for scientific simulations. But supercomputers have become very complex machines, built with thousands or tens of thousands of complex nodes consisting of multiple CPU cores or, most recently, a combination of CPU and GPU processors. Efficient simulations on such high-end computing systems require tailored algorithms that optimally map numerical methods to particular architectures. These intricacies will be illustrated with simulations of strongly correlated electron systems, where the development of quantum cluster methods, Monte Carlo techniques, as well as their optimal implementation by means of algorithms with improved data locality and high arithmetic density have gone hand in hand with evolving computer architectures. The present work would not have been possible without continued access to computing resources at the National Center for Computational Science of Oak Ridge National Laboratory, which is funded by the Facilities Division of the Office of Advanced Scientific Computing Research, and the Swiss National Supercomputing Center (CSCS) that is funded by ETH Zurich.\n\nStructural Equation Models of Management and Decision-Making Styles with Job Satisfaction of Academic Staff in Malaysian Research University\n\nERIC Educational Resources Information Center\n\nAmzat, Ismail Hussein; Idris, Datuk Abdul Rahman\n\n2012-01-01\n\nPurpose: The purpose of this paper is to discuss the effect of management and decision-making styles on the job satisfaction of academic staff in a Malaysian Research University. Design/methodology/approach: The sample consisted of 218 respondents. The instruments used in the study were the Teacher Job Satisfaction Questionnaire and the Decisionâ¦\n\nAcademic Productivity as Perceived by Malaysian Academics\n\nERIC Educational Resources Information Center\n\nHassan, Aminuddin; Tym"
    }
}
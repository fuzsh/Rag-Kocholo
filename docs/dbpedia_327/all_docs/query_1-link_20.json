{
    "id": "dbpedia_327_1",
    "rank": 20,
    "data": {
        "url": "http://pubs.acs.org/cen/hotarticles/cenear/980112/crystal2.html",
        "read_more_link": "",
        "language": "en",
        "title": "C&EN: Chemistry Crystallizes Into Modern Science",
        "top_image": "",
        "meta_img": "",
        "images": [
            "http://pubs.acs.org/cen/hotarticles/cenear/980112/75tealbar.gif",
            "http://pubs.acs.org/cen/hotarticles/cenear/980112/gilbert.jpeg",
            "http://pubs.acs.org/cen/hotarticles/cenear/980112/hermann.jpeg",
            "http://pubs.acs.org/cen/hotarticles/cenear/980112/svedberg.jpeg",
            "http://pubs.acs.org/cen/hotarticles/cenear/980112/pi.gif",
            "http://pubs.acs.org/cen/hotarticles/cenear/980112/czech.jpeg",
            "http://pubs.acs.org/cen/hotarticles/cenear/980112/fritz.jpeg",
            "http://pubs.acs.org/cen/hotarticles/cenear/980112/heitler.jpeg",
            "http://pubs.acs.org/cen/hotarticles/cenear/980112/arnold.jpeg",
            "http://pubs.acs.org/cen/hotarticles/cenear/980112/fried.jpeg",
            "http://pubs.acs.org/cen/hotarticles/cenear/980112/robert.jpeg",
            "http://pubs.acs.org/cen/hotarticles/cenear/980112/1929fig.gif",
            "http://pubs.acs.org/cen/hotarticles/cenear/980112/bethe.jpeg",
            "http://pubs.acs.org/cen/hotarticles/cenear/980112/urey.jpeg",
            "http://pubs.acs.org/cen/hotarticles/cenear/980112/dorothy.jpeg",
            "http://pubs.acs.org/cen/hotarticles/cenear/980112/henry.jpeg",
            "http://pubs.acs.org/cen/hotarticles/cenear/980112/michael.jpeg",
            "http://pubs.acs.org/cen/hotarticles/cenear/980112/wallace.jpeg",
            "http://pubs.acs.org/cen/hotarticles/cenear/980112/hans.jpeg",
            "http://pubs.acs.org/cen/hotarticles/cenear/980112/rebok.jpeg",
            "http://pubs.acs.org/cen/hotarticles/cenear/980112/nature.jpeg",
            "http://pubs.acs.org/cen/hotarticles/cenear/980112/phys.jpeg",
            "http://pubs.acs.org/cen/hotarticles/cenear/980112/edwin.jpeg",
            "http://pubs.acs.org/cen/hotarticles/cenear/980112/carbon.jpeg",
            "http://pubs.acs.org/cen/hotarticles/cenear/980112/gertrude.jpeg",
            "http://pubs.acs.org/cen/hotarticles/cenear/980112/propose.jpeg",
            "http://pubs.acs.org/cen/hotarticles/cenear/980112/alpha1.gif",
            "http://pubs.acs.org/cen/hotarticles/cenear/980112/principles.jpeg",
            "http://pubs.acs.org/cen/hotarticles/cenear/980112/carl.jpeg",
            "http://pubs.acs.org/cen/hotarticles/cenear/980112/kenichi.jpeg",
            "http://pubs.acs.org/cen/hotarticles/cenear/980112/watson.jpeg",
            "http://pubs.acs.org/cen/hotarticles/cenear/980112/stanley.jpeg",
            "http://pubs.acs.org/cen/hotarticles/cenear/980112/frederick.jpeg",
            "http://pubs.acs.org/cen/hotarticles/cenear/980112/zeig.jpeg",
            "http://pubs.acs.org/cen/hotarticles/cenear/980112/giulio.jpeg",
            "http://pubs.acs.org/cen/hotarticles/cenear/980112/atomic.jpeg",
            "http://pubs.acs.org/cen/hotarticles/cenear/980112/rudolph.jpeg",
            "http://pubs.acs.org/cen/hotarticles/cenear/980112/pi.gif",
            "http://pubs.acs.org/cen/hotarticles/cenear/980112/pi.gif",
            "http://pubs.acs.org/cen/hotarticles/cenear/980112/pi.gif",
            "http://pubs.acs.org/cen/hotarticles/cenear/980112/union.jpeg",
            "http://pubs.acs.org/cen/hotarticles/cenear/980112/1957fig.gif",
            "http://pubs.acs.org/cen/hotarticles/cenear/980112/1961fig.gif",
            "http://pubs.acs.org/cen/hotarticles/cenear/980112/nirenberg.jpeg",
            "http://pubs.acs.org/cen/hotarticles/cenear/980112/bruce.jpeg",
            "http://pubs.acs.org/cen/hotarticles/cenear/980112/roald.jpeg",
            "http://pubs.acs.org/cen/hotarticles/cenear/980112/ion.jpeg",
            "http://pubs.acs.org/cen/hotarticles/cenear/980112/ernst.jpeg",
            "http://pubs.acs.org/cen/hotarticles/cenear/980112/paul.jpeg",
            "http://pubs.acs.org/cen/hotarticles/cenear/980112/formal.jpeg",
            "http://pubs.acs.org/cen/hotarticles/cenear/980112/1973fig.gif",
            "http://pubs.acs.org/cen/hotarticles/cenear/980112/rowland.jpeg",
            "http://pubs.acs.org/cen/hotarticles/cenear/980112/polanyi.jpeg",
            "http://pubs.acs.org/cen/hotarticles/cenear/980112/edith.jpeg",
            "http://pubs.acs.org/cen/hotarticles/cenear/980112/cech.jpeg",
            "http://pubs.acs.org/cen/hotarticles/cenear/980112/ozone.jpeg",
            "http://pubs.acs.org/cen/hotarticles/cenear/980112/1982fig.gif",
            "http://pubs.acs.org/cen/hotarticles/cenear/980112/curl.jpeg",
            "http://pubs.acs.org/cen/hotarticles/cenear/980112/bednorz.jpeg",
            "http://pubs.acs.org/cen/hotarticles/cenear/980112/ahmed.jpeg",
            "http://pubs.acs.org/cen/hotarticles/cenear/980112/chu.jpeg",
            "http://pubs.acs.org/cen/hotarticles/cenear/980112/1989fig.gif",
            "http://pubs.acs.org/cen/hotarticles/cenear/980112/1995fig.gif",
            "http://pubs.acs.org/cen/hotarticles/cenear/980112/75thth.gif"
        ],
        "movies": [],
        "keywords": [],
        "meta_keywords": [
            ""
        ],
        "tags": null,
        "authors": [],
        "publish_date": null,
        "summary": "",
        "meta_description": "",
        "meta_lang": "",
        "meta_favicon": "",
        "meta_site_name": "",
        "canonical_link": null,
        "text": "Chemistry Crystallizes Into Modern Science\n\nThe past 75 years have marked profound changes in the content, scope, and direction of the field\n\nStu Borman, Ron Dagani, Rebecca L. Rawls, and Pamela S. Zurer\n\nC&EN Washington\n\nL inus C. Pauling was a graduate student at California Institute of Technology in 1923, the year C&EN was born as the News Edition of Industrial & Engineering Chemistry. That year, he published his first paper, a report on the X-ray crystal structure of the mineral molybdenite in the Journal of the American Chemical Society.\n\nAt the University of Illinois, Roger Adams was exploring the use of platinum oxide as a catalyst to reduce organic compounds. He and his students meticulously characterized their products through melting and boiling points and elemental analysis and by making derivatives.\n\nThe same year, Thomas Midgley Jr. commissioned a safety study of tetraethyllead. Just two years earlier, the General Motors scientist had discovered that the compound was a superb antiknock agent in gasoline-powered auto engines.\n\nThe Svedberg, visiting at the University of Wisconsin during 1923, built the forerunner of the ultracentrifuge-an optical centrifuge to determine the particle size and size distribution of gold hydrosols. Joel H. Hildebrand, meanwhile, was investigating metallic solutions at the University of California, Berkeley.\n\nChemistry was a well-established science in 1923, with Ph.D.-granting departments at many universities in the U.S. The American Chemical Society, founded in 1876, was a going concern. In addition to JACS, many of the journals that are prominent publishers of chemical research today had been founded, including Nature, Science, and the Proceedings of the National Academy of Sciences.\n\nA substantial chemical industry thrived in Europe, and a growing one existed in the U.S., where synthetic chemists fashioned products that included dyes, drugs, and explosives. But there were no condensation polymers, no drugs developed through rational design, and zeolites were just a curiosity of nature.\n\nMore fundamentally, chemists had only a sketchy notion of what constituted a chemical bond or how kinetics and thermodynamics drove molecules to react with one another along particular pathways. And although the concept of huge macromolecules had been proposed, most chemists didn't believe it. As one historian of science has put it, 70 to 80% of what is interesting in chemical science today was not understood to exist in 1923.\n\nSince then, instruments and computers that can ease much of the work of analysis, modeling, and synthesis have greatly expanded the scope of questions that chemists can address. And they have dramatically increased the pace at which research frontiers advance. \"A Ph.D. dissertation's amount of work in 1940 was, by 1960, an afternoon in the lab; the dissertation of 1960 was an afternoon in the lab in 1980s; and we keep repeating that,\" says chemical historian Arnold Thackray, president of the Chemical Heritage Foundation in Philadelphia.\n\nIn the following pages, C&EN's Stu Borman, Ron Dagani, Rebecca Rawls, and Pamela Zurer describe some of the seminal developments in the science of chemistry over the past 75 years. The resulting historical perspective is a selective distillation. The events, ideas, and people presented were chosen because they in some way changed the direction of chemistry. Many others could have been included in addition or instead.\n\nChemical synthesis\n\nChemists not only probe the properties and behavior of existing molecules, they also create new ones. The ability to devise new routes to compounds created by nature and to design and construct entirely novel systems lies at the heart of how chemists define their science. The complexity of the molecules and materials that chemists can build, as well as the ease with which they can put their targets together, has increased enormously over the past 75 years.\n\nIn organic synthesis, for example, the Nobel Foundation described Hans Fischer's 1929 synthesis of hemin as a \"gigantic labor\" when he was awarded the Nobel Prize in Chemistry in 1930. Fischer had determined the exact structure of the pigment that gives blood its red color by painstaking synthesis of its decomposition products. He then proved the structure by assembling hemin itself from simple pyrrole starting materials.\n\nYet Fischer's goal looks modest when compared with the synthetic targets of the past decades. A skeleton analogous to hemin can be recognized as just one small part of Robert B. Woodward's synthetic masterwork, vitamin B-12. Today, chemists pursue goals that are increasingly stereochemically complex: the 64 chiral atoms of palytoxin put together by Yoshito Kishi's group at Harvard University, say, or the 11 fused rings of brevetoxin B assembled by K. C. Nicolaou and coworkers at Scripps Research Institute in La Jolla, Calif. Such achievements have become possible because of countless contributions from numerous researchers advancing chemistry on several fronts.\n\nChromatographic techniques not available in 1923 now make it easier to isolate and analyze substances of interest. At the same time, spectroscopic and other physical methods allow researchers to rapidly elucidate the structures of unknown compounds.\n\nWith a solid understanding of the mechanisms by which reactions proceed and the conformations organic molecules are likely to adopt, chemists now can plot the course of their synthetic routes much more accurately. In addition, a wealth of specialized reagents are available to carry out specific transformations. And the very design of organic synthesis-a blend of art and science-has been systematically analyzed and developed on a logical basis.\n\n\"At the early part of the century,\" says Harvard chemistry professor emeritus Frank H. Westheimer, \"organic synthesis was carried out by people who had done enormous amounts of lab work and read enormous amounts of literature, so they knew how things had been done before. But they didn't necessarily understand why reactions had gone as they had.\"\n\nIn the 1920s, however, Robert Robinson, Christopher K. Ingold, and others developed the electronic theory of organic reaction mechanisms, focusing on rearrangement of electron pairs as bonds are made and broken. That mechanistic advance in understanding organic chemistry was \"one of the great scientific revolutions of the 20th century,\" says Harvard chemistry professor E. J. Corey.\n\n\"It made an enormous mass of information into a coherent intellectual structure,\" Corey continues. \"It permitted chemists to predict the reactivity of an organic compound toward various kinds of reagents. The ability to predict made it possible to take on more and more complicated research projects and syntheses that wouldn't have been possible before mechanistic knowledge came about.\"\n\nA still more sophisticated way of looking at how organic molecules behave resulted when Derek H. R. Barton spelled out the principles of conformational analysis in 1950. Surveying steroids, Barton pointed out that their reactivity could be understood in light of the conformations the molecules can adopt-that is, the specific three-dimensional arrangements of atoms that can be interconverted by rotation around single bonds.\n\n\"Now, everybody looks at everything carefully in three dimensions,\" Barton says. \"When I was a student, nobody ever bothered. My paper showed the relationship between preferred conformation and chemical reactivity.\n\n\"The idea of conformational analysis was taken up straightaway by synthetic organic chemists, particularly steroid chemists,\" Barton continues. \"It spread into biochemistry, enzymology, molecular biology-it's key to how enzymes interact with their substrates.\"\n\nThe impact of conformational analysis was \"a bombshell,\" Corey notes. \"Most of the complicated organic molecules in the natural world are chiral molecules with stereochemistry. The avalanche of syntheses of such structures didn't set in until we had conformational analysis.\"\n\nParallel progress in the tools for synthesis also has allowed chemists to achieve multistep syntheses of much more complicated structures than they could 75 years ago. The exploding number and variety of available reactions and reagents let scientists choose ways to put together and modify molecules with just the right gradation of reactivity and selectivity.\n\nFor example, the reaction between dienes and dienophiles to yield six-membered rings was not recognized as general until the work of Otto P. H. Diels and Kurt Alder in the late 1920s. The Diels-Alder reaction is now one of the most versatile and widely used methods for constructing ring systems.\n\nMany other synthetic methods developed over the past 75 years involve inorganic or organometallic compounds. An important way to make carbon-carbon bonds, for instance, was discovered by Georg F. K. Wittig in 1954. In the Wittig reaction, a phosphorus ylide reacts with an aldehyde or ketone, replacing the carbon-oxygen double bond of the carbonyl with a carbon-carbon double bond.\n\nAnother example is the homogeneous catalyst reported in 1966 by Geoffrey Wilkinson for hydrogenating double bonds. Soluble chlorotris(triphenylphosphine)rhodium(I) can reduce olefins without affecting other functional groups in a molecule. Other researchers soon were experimenting with chiral phosphine ligands for enantioselective reductions. Chiral ligands of a variety of types now stand at the center of the burgeoning field of asymmetric synthesis.\n\nWilkinson earlier (in 1952, at the same time as Ernst O. Fischer working independently) had identified the unusual \" -sandwich\" structure of ferrocene. The recognition of bonding between the d orbitals of the cyclopentadienyl rings and the d orbitals on iron opened the doors to a whole new type of organometallic chemistry. Metallocenes and related compounds now serve as important catalysts in both research and in industrial processes.\n\nIn addition to such chemical tools, chemists also employ a logical tool for designing syntheses, an approach Corey calls retrosynthetic analysis. \"When I started in research,\" Corey says, \"each synthetic target was taken as an individual problem and solved in an individual way. There were no general problem-solving principles or procedures people could apply as they did in, say, calculus.\"\n\nIn the 1960s, initially as a way of teaching organic synthesis, Corey systematized a way of thinking about synthetic design strategically. In applying retrosynthetic analysis, chemists start with the target molecule and work backward, identifying strategic bond disconnections, generating a \"tree\" of pathways and intermediates, and eventually ending up with an array of possible starting points. Corey and W. Todd Wipke, who later founded Molecular Design Ltd. (now MDL), developed interactive computer programs that use the logic of retrosynthetic analysis to analyze molecules and suggest synthetic routes.\n\n\"The method greatly accelerates what you can do in a given amount of time if you really understand all the principles,\" Corey says. \"With retrosynthetic analysis, I can take a class of beginning graduate students and teach them in three months to do analyses at the level of experts.\"\n\nDrug discovery\n\nToday, rationally designed pharmaceutical agents are among the principal targets that chemists strive to reach using the ever-growing arsenal of tools for organic synthesis. But although synthesis of new drugs was also a key goal of chemists in 1923, the concept of \"designed drugs\" dates from only about 50 years ago. Until then, drug development was based primarily on modification of natural products or existing drugs, rather than on any understanding of how the compounds might work inside the body.\n\n\"When we started in the 1940s, there was no rational biochemical approach,\" says Gertrude B. Elion, emeritus chemist at Glaxo Wellcome, Research Triangle Park, N.C. \"Drugs weren't designed to speak of. They were made to look like existing drugs.\"\n\nElion and her colleague George H. Hitchings, however, developed a different strategy. In an attempt to interfere in DNA synthesis, they prepared close analogs of the purine and pyrimidine bases found in DNA. Their research at what was then Burroughs Wellcome led to a series of drugs that exploit the differences in the way cancer cells or disease organisms metabolize the modified bases as compared to normal cells. They won the 1988 Nobel Prize in Physiology or Medicine more in recognition of their rational approach to drug design, Elion says, than for the specific compounds they discovered.\n\nWhen Elion and Hitchings began their long collaboration, they at first did not even know exactly which enzymes were responsible for incorporating the bases into DNA. Today, researchers profit from the gold mine of information available from research on the structure and function of enzymes, receptors, and other biomolecules. With so much detail available, chemists can design molecules to fit in a particular pocket, say, or interfere with a particular amino acid residue.\n\nA prime example is the family of protease inhibitors, the new drugs for treating AIDS that block replication of the human immunodeficiency virus (HIV). Knowledge of the structure of the HIV protease enzyme, published in 1989, aided successful development of such drugs by a number of pharmaceutical companies.\n\nWith the advent of combinatorial chemistry in the early 1990s, a new twist was added to drug discovery. Instead of being synthesized one by one, potential therapeutic agents are being systematically produced by the thousands. Molecular building blocks are combined into so-called libraries that can be screened rapidly for biological activity. Through the use of robotics, automated technologies, and sophisticated data handling, the new combinatorial strategy aims to accelerate the process of drug discovery.\n\nPolymers\n\nChemists were deeply involved in polymer chemistry 75 years ago. Modified natural polymers such as viscose rayon and cellulose acetate had been in production for some time, as had totally synthetic polystyrene. Many chemists were working to achieve a practical process for making synthetic rubber.\n\nThey didn't understand the nature of the materials they were dealing with, however.\n\n\"Baekeland had invented Bakelite by cooking phenol with formaldehyde,\" says Westheimer, citing chemist Leo H. Baekeland's resin as an example. \"He got this messy, but useful, tarlike stuff. The chemistry of it wasn't even questioned.\"\n\nThe prevailing wisdom in 1923 held that polymers were aggregates of small molecules held together by weak intermolecular forces. The concept of long-chain molecules had just begun to be championed by German organic chemist Hermann Staudinger, who coined the term macromolecule in 1922. His ideas were at first rejected. But, as evidence from X-ray crystal structures and ultracentrifugation experiments accumulated over the next decade, Staudinger's view of polymers-as thousands of atoms linked by the same type of bonds as smaller organic compounds-came to be accepted.\n\nStaudinger's ideas cleared the path for the creation of a whole new class of synthetic materials: the condensation polymers invented by Wallace H. Carothers and his research group at DuPont. And Carothers' successful preparation of the new polymers helped end the macromolecular debate for good.\n\nCarothers used well-developed methods for putting together low molecular weight organic molecules to synthesize first aliphatic polyesters and then aliphatic polyamides. In 1935, the DuPont researchers created nylon 66 from adipic acid and 1,6-diaminohexane. With DuPont pushing the superior qualities of\" artificial silk\" for women's hosiery, nylon 66 entered commercial production in 1939.\n\nThe next developments in polymer chemistry to truly shift the direction of the science arose from German chemist Karl Ziegler's studies of polymerization catalysts. In 1953, he found that alkyl aluminum compounds combined with certain transition-metal complexes worked wonders on the polymerization of ethylene. Not only was the crystalline product unbranched and of very high molecular weight, but the polymerization also took place at low temperature and pressure.\n\nThe following year, Italy's Giulio Natta used a Ziegler catalyst to produce \"stereoregular\" polypropylene-a material with all of the side-chain methyl groups on the same side of the polymer backbone. By making small changes to the catalyst, Natta discovered, polymerization could be controlled so that side groups could all lie on one side of the backbone, alternate from side to side, or adopt a random configuration.\n\nZiegler and Natta's discoveries launched a burst of research leading to a wealth of new polymers. One outcome was the attainment of a long-sought goal: synthetic production of the exact duplicate of natural rubber, all-cis-polyisoprene.\n\nZeolites\n\nThe 75 years since 1923 also saw the first synthesis of a class of inorganic materials that have grown to be enormously important in several industries: zeolites. Crystalline microporous aluminosilicates, zeolites are three-dimensional networks of AlO4 and SiO4 tetrahedra linked by their oxygen atoms. Small ions and molecules can enter the pores of the zeolite lattice and can be separated on the basis of size, which is why the materials are often called molecular sieves.\n\nToday, zeolites with a wide variety of structures and compositions are used as catalysts, adsorbents, desiccants, and ion-exchange materials. Zeolite catalysts, for example, are used to crack petroleum to produce gasoline and jet fuel. The single largest current use is in detergents, where zeolites have largely replaced phosphates.\n\n\"The discovery and development of molecular sieve materials is among the most significant advances in inorganic materials in the second half of the 20th century,\" says Edith M. Flanigen, consultant with UOP in Tarrytown, N.Y. \"There are not many other materials that have generated a nearly billion-dollar industry and that are used so extensively.\"\n\nNatural, or mineral, zeolites that could selectively adsorb compounds were discovered in the 18th century. But it was not until 1948 that Richard M. Barrer reported the first definitive synthesis of an analog of a natural zeolite. Barrer used high temperature and pressure, mimicking the conditions under which he believed the mineral was formed in nature.\n\nInspired by Barrer's success, Robert M. Milton and Donald W. Breck at Union Carbide prepared a series of synthetic zeolites in the late 1940s and early '50s, including three that the company commercialized-zeolites A, X, and Y. Their gel-based synthetic conditions were much milder than Barrer's. \"Milton reasoned that mild conditions would give open, high-porosity zeolites,\" says their coworker Flanigen.\n\nIn 1962, researchers at Mobil Oil introduced a modified zeolite X for fluid catalytic cracking of petroleum to make gasoline. Later that decade, Mobil researchers reported the first high-silica zeolites, known as zeolite beta and ZSM-5. Unlike the hydrophilic zeolites known so far, the high-silica materials were organophilic, preferring organic compounds to water.\n\nIn the 1980s, Flanigen and her coworkers created a series of zeolite-type compounds with frameworks composed of other elements. These aluminophosphates (AlPOs) and related materials- many with structures unlike any known natural or synthetic zeolites-have a surface chemistry between that of traditional alumina-rich zeolites and high-silica zeolites.\n\nSuperconductors and fullerenes\n\nJust as synthetic zeolites did in the 1950s, the discovery of two other types of materials in the 1980s sent scientists in new directions. Unlike zeolites, however, high-temperature superconductors have not yet cracked any major markets. And fullerenes are not being used commercially at all. But research in both areas is booming.\n\nThe phenomenon of superconductivity (the ability to conduct electricity with no resistance) was discovered in 1911 in mercury cooled to liquid helium temperatures. From then until the mid-1980s, scientists looked mostly among metallic alloys for materials that became superconducting at temperatures high enough to be practical. But for decades, not much progress was made in the search for so-called high-temperature superconductors. \"We were all frustrated,\" says Paul C. W. Chu, professor of physics and director of the Texas Center for Superconductivity at the University of Houston.\n\nThen, in early 1986, J. Georg Bednorz and K. Alex Müller of IBM Zurich Research Laboratory in Switzerland discovered that lanthanum copper oxide doped with barium became superconducting at the unprecedented transition temperature (Tc) of 35 K. At first, many researchers thought their results were too good to be true. But as others confirmed the IBM workers' findings and rumors of even higher Tc began to circulate, a flurry of research began on the relatively unexplored oxides. Researchers at labs around the world worked nonstop, many reporting their results for the first time at a tumultuous session at the American Physical Society's March 1987 meeting in New York City.\n\nEven before that \"Woodstock of physics,\" however, the word was out about Chu and his coworkers finding superconductivity at a Tc of 93 K in a yttrium barium copper oxide. That temperature was high enough to be reached by cooling with cheap liquid nitrogen, which boils at 77 K.\n\n\"When I saw Chu's resistivity curve for the first time, my heart stopped,\" says Paul M. Grant, now an executive scientist at the Electric Power Research Institute (EPRI), Palo Alto, Calif., but at IBM Almaden at that time. \"Everybody believed his data. We were totally convinced it was superconductivity.\"\n\nIn the decade since, many other superconducting copper oxides have been identified. In addition to unraveling the science of superconductivity at such high temperatures, much current research focuses on methods to process the materials for practical applications. Viable technology for producing wires for power transmission, although still expensive, is already available, Grant says.\n\nChu notes that the intense study of the fragile superconducting ceramics also has led to improvement in existing techniques for characterizing materials and the development of new methods. For example, he points to the discovery that buckyballs doped with alkaline metals act as superconductors. \"The researchers knew how to quickly and accurately determine the existence of superconductivity in the original material, even though it was not so pure, as a result of work on high-Tc materials,\" he says.\n\nSuperconducting buckyballs are just one outgrowth of the work that has blossomed since Richard E. Smalley, Robert F. Curl Jr., and Harold W. Kroto and coworkers discovered a new form of elemental carbon in 1985. The structure of buckminsterfullerene-also known as C60 or buckyball-is a spherical arrangement of carbon atoms composed, like a soccer ball, of interconnecting hexagons and pentagons.\n\nResearch on fullerenes has accelerated since 1990, when a method for producing large quantities from graphite electrodes was developed. In the past few years, studies of carbon nanotubes- extended tubular members of the fullerene family-have overtaken work on the fullerenes themselves. Smalley foresees a whole new nanotechnology arising from carbon nanotube research.\n\nExpanding arsenal of tools\n\nAs Chu points out, the ability of scientists to answer questions depends heavily on the tools they have available. Some of the most important new tools developed over the past 75 years have been analytical instruments. But fundamental technologies such as the laser and structural analysis tools like X-ray crystallographic instruments have made enormous contributions over the past three-quarters of a century as well.\n\nEach new instrumental technique, as it has been developed, \"has opened up a realm of looking at things,\" says chemistry professor George M. Whitesides of Harvard University. \"If you can't measure it and can't analyze it, you can't understand it, and there's not much you can say about it.\"\n\nFor example, he says, \"organic chemistry could not exist as it does today without NMR [nuclear magnetic resonance] spectroscopy and mass spectrometry. Organic synthesis depends on being able to analyze a structure to tell what it is and how far you have progressed in synthesizing it. There would be no inorganic chemistry, as far as I can tell, without X-ray crystallography and ultraviolet spectroscopy, because those have been the most important set of eyes that have been used in that field. And the big movement in chemistry toward biology has been in large part because of NMR and crystallography for determining structures of proteins and sequencing methods for determining sequences of nucleic acids.\"\n\nLasers\n\nOne of the tools of fundamental importance to the progress of chemistry over the past 20 years or so is the laser.\" Lasers have transformed chemical research and provided unprecedented limits of detection, probing, and control of matter,\" says professor of chemical physics Ahmed H. Zewail of Caltech. \"The scope of application is enormous and in all areas of research.\"\n\nAccording to chemistry professor Richard N. Zare of Stanford University, \"Lasers have altered chemistry in a remarkable way. No major chemical research laboratory exists that doesn't use lasers, from spectroscopy to novel applications such as manipulating single molecules and small particles in solution, following chemical reactions in real time, and studying how bonds are made and broken.\n\n\"One of the main reasons why lasers are so useful is that they are so bright,\" says Zare. \"What that means, for example, is that Raman spectroscopy-which I would say was nearly totally dormant before the birth of the laser-has been revitalized. It is so easy to take lasers for granted, and it is easy to forget how many different places they are now used. For example, without a helium-neon laser to count the fringes, modern Fourier-transform infrared (FTIR) instruments would not be possible. And if we had to rely on copper wire as opposed to optical fiber, our ability to communicate large amounts of data would be severely limited.\"\n\nThe story of the laser can be traced back to physicist Albert Einstein, who in 1916 proposed that a photon hitting an atom in an excited energy state could cause the atom to give off another photon of the same energy. The essential requirement for generation of a laser is that the number of excited-state atoms exceeds the number of lower energy atoms in the laser medium-a condition called a population inversion because lower energy states normally tend to predominate.\n\nThe maser, the microwave-emitting precursor of the laser, was conceived and built in the early 1950s, and the first optical laser was built in 1960. Commercial lasers produced soon thereafter found their way into many chemical laboratories.\n\nIn the 1960s, chemical lasers, which use chemical reactions to produce population inversions, were conceived and developed. The major strength of chemical lasers turned out to be not in research but in their ability to generate high power for use in military laser weapons. The same decade saw the discovery of nonlinear optical phenomena (which made a wider range of laser frequencies accessible) and the development of semiconductor lasers, ion lasers, and pulsed and continuous dye lasers. Major laser advances from the 1970s on have included the development of excimer lasers, tunable dye lasers, free-electron lasers, and femtosecond lasers. And just last year, a physics group reported the first atom laser, which emits coherent atoms instead of radiation.\n\nLasers have made it possible to observe chemical reactions, including bond breaking, bond making, and transition states, and reaction mechanisms have been solved with lasers. \"Lasers give us the time resolution and the coherence needed to study the dynamics of the chemical bond,\" says Zewail. \"Without lasers, it would be impossible to see chemical and biological events occurring on the time scale of atomic motion-the femtosecond scale.\"\n\nLasers have also made it possible to achieve bond-specific chemistry-reactions of polyatomic molecules at specific bonds, yielding selective products. Laser vaporization of graphite was used to produce the first fullerenes in 1985, and lasers can be used to produce fullerenes and carbon nanotubes in research quantities. Lasers have enabled researchers to confirm quantum mechanical theories of reaction dynamics by experimental means. And they have been used to analyze components inside single living cells and cell organelles and to monitor reactions of individual molecules.\n\n\"The laser has allowed great creativity and inventiveness and will continue to, as we think of how to use coherent radiation to solve interesting chemical problems,\" says Zare.\n\nNMR spectroscopy\n\nSince its beginnings, \"NMR spectroscopy has bloomed into a major tool in analytical chemistry, a valuable method for studying physical phenomena from kinetics to superconductivity, a technique in structural biology that rivals X-ray crystallography, a routine procedure for imaging in diagnostic radiology, a way to study metabolic function in humans and animals, and an evolving method in materials science.\" So wrote NMR Section Chief Edwin D. Becker of the National Institutes of Health in a historical review of the field [Anal. Chem., 65,295A (1993)].\n\nThe first NMR observations were made in 1938 by physicist Isidor I. Rabi of Columbia University using molecular beams in high vacuum. NMR spectroscopy of bulk material was first carried out in 1945 by two physics groups working independently-those of Felix Bloch at Stanford University and Edward M. Purcell at Harvard University.\n\nBloch and coworker William W. Hansen filed a patent application on their work and signed an exclusive license with Varian Associates, Palo Alto, Calif., which was the first company to produce a commercial instrument.\n\nIn the early 1950s, researchers discovered that resonances of hydrogen nuclei located in different positions in a molecule were separated in a spectrum. This phenomenon, the chemical shift, is what makes it possible to use NMR to determine molecular structure.\n\nIn the mid-1950s, researchers determined for the first time that nuclear spins could interact with each other to produce spin-spin splittings of resonances, another key element of modern NMR analysis. And double-resonance techniques such as the nuclear Overhauser effect were discovered, facilitating both spectral peak assignment (to specific atoms) and molecular structure determination.\n\nInitially, chemical NMR spectrometry focused primarily on proton analysis, but in 1957, chemistry professor Paul C. Lauterbur, now at the University of Illinois, Urbana-Champaign, introduced carbon-13 NMR-a technique now widely used for the study of organic and biological compounds.\n\nNMR instruments of the 1950s were primitive by today's standards. For example, in 1953 commercial instruments used a proton radiofrequency of 30 MHz, whereas 800-MHz instruments are available today. For a particular nucleus, radiofrequency is proportional to magnetic field strength and resolution (the ability to spread out chemical shifts). Until the mid-1960s, increasingly larger iron-core electromagnets were used to boost NMR field strength, but at a radiofrequency level above 100 MHz this became impractical-a problem solved by the development of high-field superconducting magnets, which were first incorporated into commercial instruments in 1966.\n\nAlso in 1966, chemist Richard R. Ernst and physicist Weston A. Anderson of Varian Associates developed FT-NMR, a technique in which short radiofrequency pulses are used to excite resonances over a wide band of frequencies simultaneously. \"The development of FT-NMR methods truly revolutionized the field,\" Becker tells C&EN. \"Not only could sensitivity be enhanced by time averaging in a practical manner, but the speed of the pulse FT method could be exploited alternatively to study fast processes such as chemical reactions and time-dependent NMR phenomena.\"\n\nThe power of NMR was initially restricted largely to liquid samples because conventional methods applied to complex solid samples yielded only broad, featureless spectra. But in the late 1960s and 1970s, the development of multiple-pulse techniques and the application of methods such as cross polarization and magic-angle spinning led to the growing use of solid-state NMR in chemistry.\n\n\"The foundation of modern solid-state NMR is a class of intellectual principles and technologies that derive to a large extent\" from the school of chemistry professor John S. Waugh of Massachusetts Institute of Technology, says chemistry professor Alexander Pines of the University of California, Berkeley. \"Today, high-resolution solid-state NMR is widely applied to materials such as semiconductors, catalysts, polymers, and proteins.\"\n\nIn the 1970s, physicist Jean Jeener of the Free University of Brussels, Belgium, conceived, and Ernst and others developed, pulsed two-dimensional NMR, which provided more powerful ways to assign spectral peaks to atoms in complex molecules. Later, three- and even four-dimensional NMR techniques were developed as well.\n\nThe advent of multidimensional techniques led to the first NMR determination of the three-dimensional structure of a protein in 1985. Today, NMR is widely used to obtain structures of proteins in solution.\n\nNevertheless, the general public was not at all familiar with NMR technology until magnetic resonance imaging (MRI) appeared on the scene. In 1973, Lauterbur first demonstrated that spatial images of living objects could be obtained by imposing magnetic-field gradients across them. Since then, MRI has become a standard medical diagnostic technique worldwide and has had an enormous economic impact.\n\nX-ray crystallography\n\nLike NMR, X-ray crystallography has had a tremendous influence on chemistry. \"X-ray crystallography is a marvelous technique. It's still the single most important structure-determining technique,\" says John Meurig Thomas, professor of chemistry at the Royal Institution of Great Britain in London.\n\nThe diffraction of X-rays was first demonstrated in 1912 by Max von Laue. The application of X-ray diffraction to determining the structure of crystals was discovered one year later by William Lawrence Bragg-who, with his father, William Henry Bragg, obtained the first X-ray structures.\n\nInitially used to observe crystals of elements and simple polar inorganic compounds such as calcite, X-ray crystallography has evolved to such an extent that structures of organic molecules, molecules of life, and even whole viruses are no longer beyond its reach.\n\nIn the 1950s, researchers developed direct methods for phase determination, which increased the speed with which crystal structures of small molecules could be obtained. \"In the 1960s, the power of direct methods that had been developed a decade earlier began to dramatically shorten the time to analyze crystal structures,\" says chemistry professor Jon C. Clardy of Cornell University. \"Improvements in direct methods have continued, and recently, a structure with over 1,000 atoms was successfully solved.\"\n\nClardy notes that \"the development of synchrotron X-ray sources, which are many orders of magnitude brighter than conventional laboratory sources, dramatically enhanced the range of structural problems that could be studied. The intense beam allowed small samples to be studied, and the variable wavelength has added powerful new phasing techniques.\" Synchrotron sources also provide the basis for time-resolved crystallographic techniques, which can be used to obtain snapshots of protein structural changes on a nanosecond time scale and to depict molecular structures in action.\n\nChromatography\n\nIn the first decade of the 20th century, Russian botanist Mikhail Tswett showed how the components of a mixture could be separated by differential migration using a technique he called chromatography. Following his example, researchers for several decades thereafter practiced classic liquid chromatography (LC), which uses an adsorbent as the stationary phase and a liquid mobile phase (eluent). LC became particularly popular in the 1930s for the separation of complex natural samples.\n\nIn 1941, Archer J. P. Martin and Richard L. M. Synge of the Wool Industries Research Association, Leeds, England, first reported the concept of liquid-liquid partition chromatography-the idea that solutes could be separated by a series of multiple partitioning steps between two immiscible solvents, one of which (the stationary phase) could be fixed on a support.\n\nAccording to chromatographer Leslie S. Ettre, retired from Perkin-Elmer, Norwalk, Conn., \"Partition chromatography became popular through paper chromatography, where the stationary phase was distributed on the surface of pieces of filter paper. Paper chromatography was introduced in 1944 by Martin and coworkers, and it became immediately very popular among biochemists.\" Later on, partition chromatography led to the development of high-performance liquid chromatography (HPLC), which is used to separate nonvolatile and thermally labile substances.\n\nChemistry professor Barry L. Karger of Northeastern University, Boston, points out [J. Chem. Ed., 74, 45 (1997)] that in the early 1960s, LC instrumentation was generally homemade and primitive by modern standards. \"Since efficiency was quite low, columns were typically a meter or more in length, and multistory columns were sometimes used to obtain the necessary resolution. Separations took hours and sometimes days to complete. Ph.D. theses in the biological sciences could consist of a multiyear effort in the isolation of a protein from a tissue sample.\"\n\nBut improvements were on the way. Martin and Synge's classic 1941 paper on partition chromatography had predicted that HPLC would be possible if small stationary-phase particles were used and high pressure differences were applied across the column. In the 1960s, chemistry professor J. Calvin Giddings of the University of Utah, Salt Lake City, confirmed these predictions using theoretical models. Chemical engineering professor Csaba G. Horváth of Yale University and chemistry professor J. F. K. Huber, then at Eindhoven University of Technology, the Netherlands, \"were the first to appreciate and apply these predictions to the development of actual HPLC systems,\" says Lloyd R. Snyder of LC Resources, Orinda, Calif.\n\nSince then, improvements in separation efficiency have resulted from reductions in particle size and the development of innovative column packings, such as controlled-porosity particles and chemically bonded packings. Bonded phases made possible the revival in the early 1970s of reversed-phase LC, which had been dormant since its conception in 1950. Reversed-phase LC, in which separations are carried out with aqueous or mixed organic-aqueous mobile phases, has proven particularly applicable to separations of biological molecules such as peptides and proteins. Biochemical and pharmaceutical separations were also aided by the 1980 introduction of chiral stationary phases, which can be used to separate enantiomers.\n\nOther forms of LC that have become a key part of the scientific armamentarium are ion chromatography and affinity chromatography. Ion chromatography, developed in the early 1970s, is a technique in which ions-including charged biological molecules such as amino acids and proteins-are separated on ion-exchange resins and measured by conductivity detection. And affinity chromatography, in which noncovalent binding interactions cause selective retention, is particularly useful for biological samples.\n\nMeanwhile, gas chromatography (GC), with a gaseous mobile phase and adsorbents as stationary phase, was developed and used by a number of researchers in the 1940s, although the operating modes were different from those used today.\" The first real gas chromatograph-essentially identical to our present-day systems-was put together by professor Erika Cremer of the University of Innsbruck, Austria, in 1947,\" says Ettre. Commercial gas chromatographs were first introduced in 1954 and 1955 by several companies, including Perkin-Elmer. By 1957, a number of other companies had entered the commercial GC market.\n\nMost early gas chromatographs used thermal conductivity detectors, which respond to all substances but don't have high sensitivity. But the late 1950s saw the introduction of the argon ionization detector (later modified into the electron-capture detector) and flame-ionization detector, which were much more sensitive.\n\nThe 1950s and early 1960s also saw the first glimmerings of the use of infrared (IR) spectroscopy and mass spectrometry (MS) instruments as GC detectors. Both GC-IR and GC-MS provide more information about the chemical identity of separated components than \"plain-vanilla\" GC does.\n\nMass spectrometry\n\nSince its beginnings about 100 years ago, MS has become a ubiquitous scientific tool. \"Scientific breakthroughs made possible by MS have included the discovery of isotopes, exact determination of atomic weights, characterization of new elements, quantitative gas analysis, stable-isotope labeling, fast identification of trace pollutants and drugs, and characterization of molecular structure,\" says chemistry professor Fred W. McLafferty of Cornell University.\n\nIn the 1930s to 1950s, high mass resolution double-focusing instruments were developed \"for the purpose of accurately determining the exact atomic weights of the elements and their isotopes,\" says chemistry professor Klaus Biemann of MIT. Magnetic deflection instruments, both single focusing and double focusing, \"dominated high-performance mass spectrometry well into the 1990s,\" says Biemann. \"The cheaper time-of-flight, quadrupole, and ion-trap mass spectrometers evolved in parallel to the more expensive magnetic-deflection instruments.\"\n\nIn a time-of-flight analyzer, ions are separated by differences in their velocities as they move in a straight path toward a collector in order of increasing mass-to-charge ratio. Time-of-flight MS is fast and has proven applicable to chromatographic detection and the determination of large biomolecules.\n\nThe direct coupling of GC to MS was first achieved in the mid-1950s. \"The combination of gas chromatographs with mass spectrometers was, I think, the first' hyphenated' analytical technique,\" says Biemann. Today, GC-MS is used for environmental analysis, forensics, drug testing, and pharmacological studies.\n\nA type of instrument that proved to be ideal for coupling to a GC was the quadrupole mass filter. In a quadrupole device, radiofrequency energy and an electrical potential applied to four cylindrical metal rods are used to separate ions. Although quadrupole mass spectrometers are not as accurate and precise as double-focusing instruments, they are fast, which is important for GC detection.\n\nThe quadrupole ion trap, a device in which a radiofrequency voltage is used to generate a quadrupole electric field, was originally introduced commercially as a GC detector. But today, ion-trap instruments also serve as LC detectors and stand-alone mass spectrometers.\n\nNovel ionization techniques have extended the capabilities of MS beyond those afforded by the electron-impact source. In field ionization, the sample is ionized in a strong electric-field gradient. A variation, field desorption, widened the range of MS by making it possible to study nonvolatile or thermally unstable compounds. \"Field desorption really opened the door for biological MS by demonstrating feasibility,\" says chemistry professor Ronald D. Macfarlane of Texas A&M University, College Station.\n\nIn the 1960s, chemical ionization was developed as a \"soft\" ionization technique, in which volatilized molecules are ionized by reaction with gas ions. Chemical ionization is gentler than electron-impact ionization and generates fewer fragment ions.\n\nTwo recently developed MS techniques that have had a major impact on the ability to determine large biomolecules are electrospray ionization (ESI) and matrix-assisted laser desorption/ ionization (MALDI). In ESI, highly charged droplets dispersed from a capillary in an electric field are evaporated, and the resulting ions are drawn into an MS inlet. In MALDI, sample molecules are laser-desorbed from a solid or liquid matrix containing a highly UV-absorbing substance.\n\nTechniques such as ESI and MALDI have permitted large biomolecules to be analyzed by low-cost MS and MS/MS instruments of the quadrupole, ion-trap, and time-of-flight type. This feature has made biological MS available to hundreds of researchers who do not have access to one of the expensive magnetic sector machines. \"MALDI and ESI now promise a greatly expanded future with molecular characterization of proteins, DNA, and other large molecules using instruments providing high sensitivity, specificity, and speed at lower cost,\" says McLafferty.\n\nElectroanalytical chemistry\n\n\"Electrochemical methods like cyclic voltammetry are today routinely employed by all kinds of chemists, biochemists, and even physicists as part of their experimental tools,\" says Royce W. Murray, chemistry professor at the University of North Carolina, Chapel Hill, and editor of Analytical Chemistry. \"There also have been fusions of electrochemical methodology with materials science that have, for example, led to the solid-state oxygen sensors found in so many automobile engines.\"\n\nThe beginnings of analytical electrochemistry can be traced to 1922, when Czech physical chemist Jaroslav Heyrovsky developed polarography, a technique in which a dropping mercury electrode is used to obtain current-voltage curves for reducible or oxidizable compounds in solution. In 1925, Heyrovsky and coworker Masuzo Shikata invented the polarograph, an instrument for recording such curves automatically. The discovery of the three-electrode potentiostat in 1942 widened the applicability of polarography to sample types that had previously been inaccessible.\n\nIn the 1950s and 1960s, multipurpose electrochemical instruments based on operational amplifiers (op amps) were developed in academia. A commercial version of such an instrument, the model 170 electrochemical analyzer from Princeton Applied Research Corp., Princeton, N.J., \"was revolutionary for its time in that it permitted the electrochemist to perform a wide variety of different measurements on the same solution simply by changing some front-panel controls, and in that it could be operated by someone who had no knowledge of electronics,\" says physical chemist Jud B. Flato, who led a Princeton Applied Research group that developed the instrument. A similar multipurpose electrochemical instrument based on op amps was developed by Cambridge Instruments in England at about the same time.\n\nOne of the techniques popularized by op-amp-based instruments was pulse polarography, a method that improved the detection limits of conventional polarography by about two orders of magnitude. Pulse polarography was an offshoot of square-wave polarography, a technique developed in the early 1950s by Geoffrey C. Barker of the Atomic Energy Research Establishment, Harwell, U.K. Barker's technique was developed further by others in the 1970s and has evolved into a technique now known as Osteryoung square-wave voltammetry.\n\nIn 1983, an all-digital electrochemical analyzer that required no knobs, buttons, or gain controls to carry out 38 different types of electrochemical techniques was introduced by Bioanalytical Systems, West Lafayette, Ind. The current version of this instrument is controlled by a personal computer.\n\nPeter T. Kissinger, professor of chemistry at Purdue University and president of Bioanalytical Systems, notes that research on computer modeling of electrochemical processes and the development of algorithms for simulation of electrochemical experiments have been responsible for major advances in electrochemistry. As a result, \"work that required overnight computer runs can now be done on a Pentium computer in less than five seconds,\" says Kissinger.\n\nAn electrochemical device that has had an exceptionally broad impact on scientific research is the glass electrode. Invented in 1906, the glass electrode is used to measure hydrogen-ion concentration. The first pH meters based on it were created in 1928.\n\nIn 1934, chemistry professor Arnold O. Beckman at Caltech built a pH meter for a colleague. He tried to interest various companies in manufacturing the device, but they declined. As a result, he formed National Technical Laboratories, based in Pasadena, Calif., which developed the first commercial pH meters and was the beginning of what is now Beckman Instruments, Fullerton, Calif.\n\nOver the years, Beckman, other companies, and academic researchers made great improvements in pH meters as they developed into their modern form, with higher stability glass electrodes, microprocessor control, and light-emitting diode readouts. Beginning in the 1950s, electrodes were also developed for other ions, such as F-, Na+, K+, and Ag+. The pH meter and ion-selective electrode have now become indispensable scientific tools.\n\nAtomic and molecular spectroscopy\n\nDevelopments in atomic emission spectroscopy (AES), atomic absorption spectroscopy (AAS), ultraviolet-visible spectroscopy (UV-vis), IR spectroscopy, and Raman spectroscopy have revolutionized atomic and molecular analysis over the past 75 years.\n\nIn AES, atoms excited in flames, arcs, sparks, or plasmas emit optical frequencies that can be used to identify elements in samples and determine their concentrations. AES was known in the 19th and early 20th centuries, but 1930 through 1960 was a period of particularly rapid development, spurred in large part by growing interest in its use for metals analysis in the aluminum and steel industries. Electronically controlled spark sources developed in the late 1960s improved the precision of spark AES, were marketed commercially, and are still in extensive use.\n\nIn the mid-1960s, Stanley Greenfield and coworkers at Albright & Wilson Ltd., Oldbury, England, and chemistry professor Velmer A. Fassel and coworkers at Iowa State University, Ames, independently developed the inductively coupled plasma (ICP) atom source as an alternative to arc and spark sources, and commercial ICP-AES instrumentation appeared in 1972. ICP-AES offered a number of advantages over arc and spark AES-improved precision, wider dynamic range, greater freedom from interferences, ease of use, and more ready applicability to the analysis of solutions. As a result, ICP-AES has largely displaced arc and spark techniques for many elemental analysis applications. But arc and spark AES do not require dissolution of metal samples (as ICP-AES does) and thus are still widely used in the metals industry.\n\nIn addition to being a good atom source for AES, the ICP proved \"to be an ideal sample introduction system for the mass spectrometer,\" says retired spectroscopist Walter Slavin of Perkin-Elmer.\" The ICP-MS business is already almost as large as that of ICP-AES and is growing very much faster than any other part of atomic spectroscopy.\" Slavin points out that diode array detectors, which detect a range of wavelengths simultaneously and therefore provide a speed advantage, are now incorporated into many commercial ICP-AES instruments in place of conventional photomultiplier detectors.\n\nIn AAS, light from a source passes through a flame into which a sample is aspirated, and attenuation of the beam by sample atoms in the flame is compared to standards to determine elemental concentrations. The concept of atomic absorption dates back to the 19th century, but use of AAS as a general analytical technique was first proposed in 1955 by physicist Alan Walsh of the Commonwealth Scientific & Industrial Research Organization's Division of Industrial Chemistry, Melbourne, Australia, and independently by physicists C. Th. J. Alkemade and J. M. W. Milatz of the University of Utrecht, the Netherlands. The Australian and Dutch researchers pointed out that AAS had overriding advantages over AES-enhanced sensitivity and specificity and lower susceptibility to interferences-despite its inability to perform simultaneous multielement analysis, as AES could do.\n\nWalsh worked hard in the 1950s to promote AAS. One of his suggestions was the use of hollow cathode lamps (which provide very narrow spectral lines and exhibit low noise) as light sources to increase the technique's sensitivity and linear working range. Eventually, his efforts to popularize AAS succeeded, and commercial instrumentation, complete with hollow cathode lamp sources, emerged in the early 1960s.\n\nIn 1959, a heated graphite furnace atomizer was developed as an alternative to the conventional AAS flame atomizer by Boris V. L'vov of the department of analytical chemistry at Leningrad State Technical University (now St. Petersburg State Technical University). The graphite furnace, which offered higher sensitivity and the ability to analyze smaller samples, was also eventually introduced in commercial AAS instrumentation.\n\nIn UV-vis spectroscopy, the absorption of UV and visible radiation is used primarily to determine molecules and ions in solution. UV-vis spectroscopy was practiced in the 19th and early 20th centuries, but its popularity grew immensely in the 1940s after Beckman Instruments introduced its inexpensive and easy-to-use model DU single-beam spectrophotometer. After World War II, double-beam UV-vis instruments were introduced, permitting simultaneous monitoring of sample and reference channels and thus improving the convenience with which the technique could be carried out. Since then, UV-vis instruments have been spiffed up considerably with microprocessor controls, computer interfaces, and diode array detectors.\n\nThe popularity of IR spectroscopy (which probes vibrational energy levels of molecules) for sample identification, component analysis, and organic structure determination, among other applications, began to ratchet up in the 1930s and early 1940s. Commercial instruments from U.S. manufacturers such as Beckman Instruments and Perkin-Elmer began to appear during World War II. IR spectroscopists made major contributions to a number of important research projects during the war, including enemy-fuels analysis, the synthetic rubber program, and the effort to characterize and synthesize penicillin.\n\nDouble-beam recording IR spectrometers, which monitor the absorption of light by a sample and a reference simultaneously, were developed and released commercially in the 1940s. \"The development of commercial double-beam IR instruments, of which the Perkin-Elmer model 21 was an outstanding example, turned IR into a technique that became very widely used by organic chemists,\" says chemistry professor Norman Sheppard of the University of East Anglia, Norwich, U.K. \"Until NMR took general hold in the early 1960s, it was the principal method of analysis of organic structures and remained very versatile, as samples could be studied in very small quantities in the solid as well as solution states.\"\n\nIn 1950, Peter B. Fellgett of Cambridge University proposed that dramatic improvements in the signal-to-noise ratio of IR spectroscopy could be realized by using FT analysis to obtain IR spectra. Initially, the computations required to calculate the transforms were so laborious that any advantages of the technique were negated. But the publication in 1965 of the Cooley-Tukey fast FT algorithm changed that, and FTIR entered a takeoff phase. Today, it is the dominant form of commercial IR spectroscopy and is widely used both for organic analysis and GC detection.\n\nRaman spectroscopy monitors changes that occur in source radiation when it interacts with the vibrational energy levels of sample molecules. The technique, discovered in 1928 by Indian physicist Chandrasekhara Venkata Raman, provides information complementary to that obtained from IR spectroscopy.\n\nRecording of Raman spectra \"was initially by photographic plates, with limitations as to quantitative accuracy,\" says Sheppard. \"This was improved by the development of photoelectric recording during the war.\" Nevertheless, the Raman technique languished during the 1940s and 1950s, the period when IR spectroscopy was developing most rapidly. \"Sensitivity remained poor,\" says Sheppard, \"and fluorescence problems could occur from small amounts of impurity that absorbed in the visible and near-UV region.\"\n\nThis situation began to change in the early 1960s when lasers were introduced as Raman sources in place of conventional sources, such as mercury arcs. Laser excitation gave \"another lease on life to Raman spectroscopy for commercial analysis,\" says Sheppard. Laser sources made it possible to obtain spectra with much higher signal-to-noise ratios, and the use of Raman spectroscopy expanded considerably when commercial laser-based instruments became available. Although Raman spectroscopy \"is not yet as popular as IR spectroscopy,\" says retired chemistry professor Foil A. Miller of the University of Pittsburgh, \"it is growing rapidly and is widely used for spot analysis of microsamples, polymer analysis, organic structure determination, and other applications.\"\n\nOnly time will tell what instrumental discoveries will emerge over the next 75 years. But it's certain that as instruments continue to advance, the frontiers of science will follow.\n\nChemical theory of the bond\n\nDuring the past 75 years, chemists have sought to answer some very basic questions at the core of their science- questions such as \"What is a bond?\" and\" How do atoms and molecules react?\" Such questions have held a special fascination for theoretical chemists, who, about 75 years ago, began applying advanced mathematics, including quantum mechanics, in search of answers.\n\nAt the start of the 20th century, no one understood the force that held atoms together in a molecule, but there was little doubt that the force was electrical in nature. In 1904, for example, British physicist J. J. Thomson developed one of the first theories of valency involving the electron, the particle he had discovered just a few years earlier. Other theories followed-and faltered, because they could not explain the existence of both polar and nonpolar bonds.\n\nIn 1923, American chemist Gilbert N. Lewis published \"Valence and the Structure of Atoms and Molecules.\" In this book, which has been described as his masterpiece, Lewis expounded on his idea, first published in 1916, that a pair of electrons shared by two atomic nuclei constitutes a covalent (single) bond.\n\nLewis' theory superseded all earlier theories and eventually won widespread acceptance. It was successful, in part, because it showed that polar and nonpolar bonds are not different kinds of bonds, but are situated at opposite ends of a continuum, depending on whether the electron pair of the bond is shared equally or not by the two atoms. One immediate result of the theory was that it settled the long-festering debate about the difference between strong and weak electrolytes.\n\nAlthough Lewis' theory helped explain how atoms interact with one another to form molecules, the physical basis of both electron pairing and sharing remained \"murky,\" according to historian John W. Servos in his book \"Physical Chemistry from Ostwald to Pauling: The Making of a Science in America.\" Neither the Lewis theory nor its prime competitor-the planetary model of the atom developed by Danish physicist Niels H. D. Bohr in 1913-\"was fully competent to explain the nature of the chemical bond or the structure of atoms and molecules,\" Servos writes.\n\nBut waiting in the wings was a theory, developed by physicists, that would soon be applied to chemical problems. Quantum theory was born in 1900 when Max Planck's analysis of blackbody radiation led to his insight that energy at the atomic level cannot be emitted or absorbed continuously, but only in small discrete steps called quanta.\n\nBefore the advent in 1913 of Bohr's atomic model, which was itself an outgrowth of quantum theory, chemists\" rarely thought in terms of quanta,\" observes physical chemist Keith J. Laidler in his book \"The World of Physical Chemistry.\" But by the mid-1920s, they could hardly avoid it. Theoretical physics was in ferment as a burst of new ideas from Wolfgang Pauli, Werner K. Heisenberg, Paul A. M. Dirac, Erwin Schrödinger, and others was shaping a revolutionary new understanding of the atom. In 1926, for example, Schrödinger, in his celebrated wave equation for the hydrogen atom, treated electrons as having the properties of waves and thus helped to move scientific discourse from electron orbits to electron wave functions and orbitals.\n\nQuantum theory and its development as quantum mechanics in the 1920s \"completely transformed our thinking about chemical problems,\" writes Laidler. Experimental results that previously were difficult to explain, such as spectra and photochemical reactions, could now be readily understood.\n\nFrom this turbulent intellectual climate emerged two competing (and complementary) explanations of the chemical bond: the valence-bond (VB) theory and the molecular-orbital (MO) theory. These were the first quantum mechanical treatments of a chemical system.\n\nValence-bond theory\n\nThe VB approach was the brainchild of two young German physicists, Walter H. Heitler and Fritz W. London, who were working with Schrödinger at the University of Zurich. In a seminal paper written in 1927, they developed a wave equation for the hydrogen molecule with which it was possible to calculate approximate values of the molecule's ionization potential, heat of dissociation, and other constants. These predicted values were reasonably consistent with empirical values obtained by spectroscopic and chemical means. After their method was refined by other scientists, the fit between predictions and experimental data became\" astonishingly close,\" Servos writes.\n\nIn a 1928 paper, London further refined the VB theory, making it, in essence, \"a quantum mechanical version of Lewis' shared-pair theory,\" in Servos' words. Like Lewis, London assumed that the atoms forming the molecule retain their integrity and that valence bonds form from the pairing of two electrons of opposite spin. \"And like Lewis, he equated the valence of an atom with its number of unpaired electrons; electrons already coupled with partners of opposite spin were chemically inert,\" Servos writes.\n\nThese ideas intrigued the young Linus Pauling, who, at the time, was in Europe on a postdoctoral Guggenheim Fellowship, soaking up as much quantum mechanics as he could from the big names in the field like Bohr and Schrödinger. Pauling befriended Heitler and London, and by the time he returned to the U.S. in the spring of 1928, he had become an enthusiastic advocate of their ideas. Not only did Pauling write about the Heitler-London approach in a manner to make it as appealing as possible to chemists, but he also began the crucial work of extending it to bonds in molecules more complex than H2.\n\nIn this work, Pauling was able to use the VB approach to resolve the last major discrepancy remaining between the Bohr and Lewis views of the atom. That discrepancy involved the valences of the carbon atom. Neither model could account for the fact that carbon is usually quadrivalent, as in CH4, but it also can be bivalent, as in CO. The bivalency could be explained if carbon's four valence electrons were distributed in the ground state as follows: two paired electrons in the 2s orbital and one unpaired electron in each of two of the three 2p orbitals. However, if one of the 2s electrons is promoted to an empty 2p orbital, the atom could form four bonds to make CH4, although one C-H bond (the one involving the 2s electron) would be different from the others, which is not the case.\n\nUsing London's theory of valence as his foundation, Pauling came up with a solution: He suggested that the 2s and 2p subshells can be mixed or hybridized to form four equivalent orbitals, each containing one unpaired electron. Thus, four shared-pair bonds could be formed, each one directed to one corner of a tetrahedron.\n\nIn a series of papers between 1931 and 1933, Pauling derived simple rules for the electron-pair bond and then demonstrated, according to Servos, \"how these rules, in conjunction with a wide variety of experimental data, could be used to predict the structures, bond strengths, rotational motions, magnetic moments, and other properties of a wide variety of molecules and complex ions.\"\n\nIn these papers, published under the general title, \"The Nature of the Chemical Bond,\" Pauling also developed his notion of resonance. In \"his celebrated paper on benzene,\" Servos writes, he showed that \"the energy and other properties of the molecule could not be explained in terms of the wave function of a single valence-bond structure, but instead were the product of a linear combination of the wave functions of several structures, each making a specifiable contribution to the properties of the whole. Like the elephant in the tale of the blind men, the benzene molecule did not correspond to any one of its many descriptions, but rather was captured by all of them.\"\n\nPauling's use of hybridization and resonance to explain both the tetrahedral carbon atom and the stable benzene ring\" were triumphs of the new quantum chemistry,\" says Servos. The quantum mechanical treatment of bonds eventually was applied to \"the ionic crystals of geochemistry, the intermetallic compounds of metallurgy, the coordination compounds of inorganic chemistry, and the polymers of biochemistry.\"\n\nThe molecular-orbital approach\n\nThe VB approach, which had been created in a single paper by Heitler and London, was soon in competition with the MO approach, which was born in 1928 in the work of Friedrich Hund and Robert S. Mulliken, two physicists who were close friends but who never published jointly. The MO approach treated the hydrogen molecule not as a composite of two atoms held together by valence bonds (as in the VB approach), but rather as a distinct entity in which the constituent atoms had lost their integrity, Servos writes. The advantages of the MO method over VB theory won it many adherents in the 1930s.\n\nMO theory evolved over the years, enriched by contributions from many scientists. For example, the American physicist John C. Slater, who shares credit with Pauling for developing the idea of hybridization, also developed an approach that \"brought the Hund-Mulliken method into better harmony with the Heitler-London-Pauling method,\" writes Mary Jo Nye in her book \"Before Big Science: The Pursuit of Modern Chemistry and Physics, 1800-1940.\"\n\nGerman theoretical physicist Erich Hückel also made important contributions to MO theory by applying it to benzene and other aromatic molecules. He suggested the idea of Ó and bonds, for example, and showed that benzene's stability arose from its six Ó bonds in the plane of the ring and the \"delocalized\" cloud of six electrons above and below the plane. Hückel also stated the famous rule that a closed ring with (4n + 2) electrons would be aromatic.\n\nThe MO approach also was used to gain a better understanding of molecular spectra, thanks to the pioneering work of spectroscopist Gerhard Herzberg, among others. In 1929, for example, Herzberg was the first to explain chemical bonding\" in a simple and convincing manner in terms of bonding and antibonding electrons,\" says Laidler. \"No contribution to the application of quantum mechanics to spectroscopy has been more important than [Herzberg's].\"\n\nLaidler points out that, at first, there was \"considerable rivalry\" between advocates of the VB and MO methods. Later, when the two methods were refined and improved, scientists realized that they converge and offer useful, complementary views of a molecule. For studies involving more complex molecules or the calculation of molecular properties, though, MO theory usually offers a significant advantage, says Stephen G. Brush, a historian of science at the University of Maryland, College Park. As that advantage became increasingly clear in the 1950s and '60s, chemists switched their allegiance from VB to MO. For example, the great success of Hückel's 4n + 2 rule in predicting which molecules would be aromatic or nonaromatic convinced legions of organic chemists that MO theory was right, says Cornell University chemistry professor Roald Hoffmann.\n\nMO theory also gained luster from the work of American chemists Woodward and Hoffmann, who, between 1965 and 1969, formulated what have since become known as the Woodward-Hoffmann rules. These simple rules have been used very successfully to predict the course and stereochemistry of organic pericyclic reactions from the symmetry of the molecular orbitals in the excited and nonbonding states-an approach considered equivalent to Fukui's.\n\nBy the 1970s, \"MO was definitely considered the more correct picture,\" Brush notes in a forthcoming article on the benzene problem. He also makes the interesting observation that \"the success of Pauling's VB theory led chemists to accept quantum mechanics as a basis for chemical theory, and this [acceptance] in turn led them to reject VB because MO was a better way to use quantum mechanics.\"\n\nCrystal and ligand field theories\n\nWhile the VB and MO theories were harnessed to describe the bonding in both organic and inorganic compounds, a third quantum mechanical approach-crystal field theory-was developed specifically to deal with inorganic compounds, particularly coordination compounds. The foundation for this theory was laid by physicist Hans A. Bethe in Germany. In a classic 1929 paper, Bethe considered what happens to the energy levels of an isolated cation such as Na+ when it is placed in the electrostatic field that exists within an ionic crystal such as NaCl. He showed how the strength of the field and the symmetry affect the cation's electronic levels.\n\nIn the 1930s, American physicist John H. Van Vleck at Harvard University and others applied the new theory to transition-metal complexes. They showed that the crystal field approach was valuable for modeling the behavior of d electrons in metal complexes and thus was very useful for understanding the colors, magnetic properties, and other characteristics of complexes. Crystal field theory, though, offers a purely ionic model, and so has little to say about bonding, particularly covalent bonding (including double bonding) between the metal ion and the ligand.\n\nTo get beyond the limitations of crystal field theory, scientists in the 1950s combined its best features with molecular-orbital concepts to produce ligand field theory, which has been called \"the superior tool\" for dealing with metal complexes. Ligand field theory is concerned with how the inner (nonbonding) orbitals of the central metal are split into different energy levels by the surrounding ligands. The power of the theory \"to decipher the complicated stereochemistry of transition-metal complexes\" was revealed by the University of Oxford's Leslie E. Orgel, one of the theory's key developers, at a conference of coordination chemists in 1950, writes William H. Brock in \"The Norton History of Chemistry.\" Ligand field theory, Brock notes, \"made a dramatic contribution to the renaissance of inorganic chemistry.\"\n\nDuring the 1960s, the theory really took off, comments Derek A. Davenport, an emeritus professor of chemistry at Purdue University. In fact, Pauling's VB treatment of transition-metal complexes\" got pretty well wiped out by ligand field theory,\" he says.\n\nQuantum theory of chemical reactions\n\nAt the same time that quantum mechanics was transforming the way scientists viewed the covalent bond, it also was transforming the way they viewed the chemical reaction. A key person in both instances was London, the man who, with Heitler, had fashioned the first satisfactory quantum mechanical picture of the H2 molecule. In 1928, London extended the Heitler-London approach to H3, an unstable intermediate in the simplest three-atom exchange reaction, H + H2 > H2 + H. In this reaction, an energy barrier must be surmounted by the reactants as they are converted into products. London stated that all the properties of the energy barrier-height, location, and shape-are embodied in the quantum mechanical expression for the potential energy of the three-atom system.\n\nAlthough London's equation involved extreme approximations, it became central to scientists' understanding of the transition state. The equation \"was arrived at by an inspired process of groping,\" chemistry professor John C. Polanyi of the University of Toronto commented a few years ago. London published it without a derivation, but years later, according to Polanyi, other scientists \"showed how one could derive it by subjecting the Schrödinger equation to life-threatening surgery.\" The special virtue of London's equation is that \"it is composed of the same[ mathematical] building blocks that describe the quantum nature of binding in the reagents and products,\" Polanyi noted.\n\nIn 1931, an important advance was made at the Kaiser Wilhelm Institute in Berlin by Michael Polanyi (John Polanyi's father) and Henry Eyring, who was then a postdoctoral fellow. Melding London's equation with experimental data, they constructed the first potential energy surface for a reaction-specifically, the exchange reaction between H and H2.\n\nA potential energy surface is a topographic road map useful for visualizing a reaction because it shows the potential energy of the system as a function of the relative positions of all the atoms taking part in the reaction. As reactants evolve into products, the molecules migrate from one valley (low-energy state), over a \"mountain pass\" or saddle point, into another valley. The saddle point can be defined as the transition state-the intermediate stage from which the reaction proceeds irreversibly to products.\n\nA theory describing this transition state was published in 1935 by Eyring, then at Princeton University, and independently by Michael Polanyi and Meredith G. Evans at the University of Manchester, England. According to Laidler, the essence of their theory is that species close to the saddle point-the so-called activated complexes-are \"in a state of quasi-equilibrium with the reactants, so that their concentration can be calculated. The rate of reaction is then the concentration of these complexes multiplied by the frequency with which they are converted into products.\"\n\nIn a 1983 article reviewing the development of transition-state theory, Laidler and M. Christine King, an Ottawa colleague, described the three essential features of the theory, pointing out that not one of them was completely new. \"The genius of the 1935 work,\" they wrote,\" was in putting these three features together and arriving at a simple but general rate equation. In addition, Eyring made a very important and novel contribution in appreciating the great significance of the reaction coordinate,\" the path the reaction takes along the potential energy surface.\n\nAccording to Laidler and King's article, transition-state theory ran into \"a good deal of opposition in its early stages,\" for several reasons. The first papers were hard to follow and took \"very bold-some would say reckless-steps, and it was predictable that many would fail to understand their arguments.\"\n\nEventually, though, the theory's value for providing qualitative insight into chemical reaction rates became widely appreciated, even though it was less successful in its original goal-the calculation of absolute reaction rates. In 1962, British chemistry Nobelist George Porter, who made important contributions to the study of very fast reactions, wrote that transition-state theory, since its inception, \"has provided the basis of chemical kinetic theory; imperfect as it may be, it is undoubtedly the most useful theory that we possess.\" Its greatest success, he said, has been \"in providing a framework in terms of which even the most complicated reactions can be better understood.\"\n\nIt took about half a century for experiment to catch up with transition-state theory. In 1980, John Polanyi's group succeeded in obtaining spectroscopic evidence for transition species (in the reaction of a fluorine atom with Na2). And in 1987, using femtosecond laser techniques, Zewail's group at Caltech was able to directly observe transition-state structures during bond breaking (in the reaction ICN -> I + CN) and during bond making (in the reaction H + CO2 -> OH + CO).\n\nElectron-transfer reactions\n\nTransition-state theory did much in its early years to shed light on reactions involving the formation or cleavage of bonds. But it could not be applied to perhaps the most elementary type of reaction in chemistry-the hopping of an electron from one atomic ion to another-because there was no way to take into account the involvement of the surrounding solvent molecules. It fell to Caltech chemistry professor Rudolph A. Marcus to figure out a way to characterize the transition state for such an electron-transfer reaction.\n\nMarcus, who happens to have been born the same year as C&EN, became interested in electrostatics in the early 1950s-early in his research career. The study of electron-transfer rates was already very active at the time, and scientists had determined that electron transfer between small ions such as ferric and ferrous ions is relatively slow, whereas electron transfer between larger ions is faster. Chemist Willard F. Libby, then at the University of Chicago, had published an electrostatic calculation that purported to explain this observation by invoking the Franck-Condon principle: Since the orientation of solvent molecules around Fe2+ and Fe3+ is quite different, Libby reasoned, when the electron jumps from Fe2+ to Fe3+, the solvent environment around each ion is suddenly the wrong one because the solvent molecules can't rearrange themselves as fast as the electron jumps. The smaller the reactant, Libby proposed, the more inappropriate its environment becomes.\n\nWhen Marcus came across Libby's paper in 1955 and studied it, he sensed that something about Libby's explanation was \"not quite right,\" but he wasn't sure at first what it was. Within a month, though, Marcus realized that although Libby's use of the Franck-Condon principle was clever, the way it was implemented violated the law of conservation of energy. Marcus also realized that he had to find a way to calculate the free energy of formation of fluctuations in the interatomic distances and orientation of the solvent molecules around the ions in solution so as to allow the system to reach a transition state that satisfied the Franck-Condon principle. Marcus succeeded in carrying out this calculation, and from the result, he was able to characterize the transition state for the reaction and calculate the reaction rate constant itself.\n\nObtaining these results was \"one of the most thrilling moments of my scientific life,\" Marcus wrote many years later. He published the results in two papers in 1956. Then, over the next nine years, he further developed the \"Marcus theory,\" extending his original concept to include intramolecular vibrational effects, electrochemical electron-transfer reactions, and related processes.\n\nThe theory made numerous predictions, and experiments confirmed many of them almost immediately. However, one of the theory's more surprising predictions-that beyond a certain point, electron-transfer rates would decrease with increasing driving force-took 25 years to be verified experimentally. The theory is now being applied in areas such as photosynthesis, electrically conducting polymers, chemiluminescence, and corrosion.\n\nAlthough quantum mechanics clearly has revolutionized chemistry, it \"did not have the overwhelming effect on chemical research that some theoretical physicists predicted,\" Laidler wrote in a 1990 article. \"Dirac, for example, believed at one time that with the introduction of quantum mechanics, all chemical problems were solved, since everything could be done by setting up and solving the appropriate quantum mechanical equations; there would be no need for further experimental work.\"\n\nIn fact, Laidler pointed out, \"things have not worked out that way.\" In the real world, the equations are almost always too complicated to solve exactly. \"We are forced to make many approximations, and it is hard to know which to make.\" Chemists must continue to rely on experiments and simple approximate theories, \"and it seems likely that this will always be the case.\"\n\nWhat has changed in the past 75 years, though, is that theoretical chemistry has come into its own as a sophisticated and useful tool. As Caltech chemistry professor William A. Goddard III put it in a 1985 article, \"Theorists are now in a position to tackle many important chemical, biological, and materials problems on an equal footing with experimentalists.\"\n\nChemistry meets biology\n\nThe interface between chemistry and biology has been one of the most fertile areas of both disciplines during the past 75 years. In a sense, chemistry has \"met\" biology in the years since 1923, as chemists and their tools have gained the ability to study molecular systems of life-sized complexity. At the same time, biologists have begun to be able to dissect living systems down to the level of interactions between molecules, spawning in the process the very concept of \"molecular\" biology.\n\nWhen C&EN was first published, chemists were vigorously debating the idea, proposed about 1920 by German chemist Hermann Staudinger, that molecules could form enormously long chains that Staudinger called macromolecules. Chemical historian Thackray calls Staudinger's proposal the most important concept in the chemical sciences in the 20th century, providing the theoretical foundation for both polymer science and biotechnology.\" It has, is, and will transform our lives, even to transforming the very idea of what life is,\" Thackray says. \"In the world we now live in, 70 to 80% of what's interesting consists of huge molecules.\"\n\nIn Staudinger's day, many important biological substances were thought to be gelatinous colloids, \"sticky messes that you couldn't crystallize,\" Thackray calls them. Invention of the ultracentrifuge around 1923 by Swedish biochemist The Svedberg provided a way to determine the size and homogeneity of these substances without crystallizing them. By the mid-1930s, Svedberg and others had used the ultracentrifuge to purify a variety of proteins, showing that they were substances with uniform-and huge- molecular weights ranging in this early work from 17,200 for myoglobin to nearly 7 million for snail hemocyanin.\n\nEarly pioneers were also beginning to crystallize proteins: In 1926, James B. Sumner crystallized urease, a protein that could catalyze the decomposition of urea. J. D. Bernal and Dorothy Crowfoot (later Hodgkin) at Cambridge University recorded the X-ray diffraction pattern for crystalline pepsin in 1934, revealing an orderly, structured, but dauntingly complex molecule. It would be another 25 years before Max F. Perutz and John C. Kendrew, from the same laboratory, would be able to solve the structure of a protein by deciphering its X-ray diffraction pattern.\n\nThis early work, done without computers, was painfully slow. Researchers calculated power series and Fourier transforms by hand. Crowfoot herself, for example, recorded the X-ray diffraction pattern of crystalline insulin in 1935 but didn't solve its structure until 1969.\n\nPauling became attracted to the field in the mid-1930s. By the end of the decade, he and Caltech colleague Robert Corey were pinning down bond lengths and angles in amino acids and small peptides based on X-ray crystal data and beginning to use this information to predict protein structure. The peptide bond is rigid and planar, they found, but other bonds are more flexible, allowing amino acids to form hydrogen bonds with one another. By the early 1950s, their calculations predicted two repeating structures that amino acid chains in proteins were likely to assume-the alpha-helix and the beta-pleated sheet.\n\nBiochemists understood before 1923 that proteins had something to do with biocatalysis, but whether the protein itself was catalytic or just held within it a much smaller molecular catalyst was an open debate. By crystallizing urease and demonstrating that this enzyme was a protein, Sumner helped establish that proteins alone can be catalytic. A decade later, John H. Northrop, working with crystalline pepsin, trypsin, and chymotrypsin, made the case even more solidly by showing that the protein content of these three digestive enzymes correlated directly with their catalytic activity. These and other early experiments established that \"like garden variety chemicals, the molecules responsible for biological catalysis would crystallize in the test tube,\" says MIT bioinorganic chemist Stephen J. Lippard. By 1960, about 75 enzymes had been crystallized, all of them proteins, although many also contained non-protein prosthetic groups that were critical to their catalytic activity.\n\nDNA structure\n\nScientists were slower to realize that nucleic acids also form long-chain macromolecules of biological importance. In retrospect, the connection between these molecules and heredity seems obvious- 19th-century biochemists knew, for example, that the heads of sperm contain almost nothing except nucleic acid. But the molecules seemed too \"boring\" to be genetically important. There are only four nucleotides in DNA-compared with 20 amino acids in proteins-and, by the analytical sensitivity of the 1920s, all four appeared to be present in exact stoichiometric equivalence, making every piece of DNA seem identical to every other one.\n\nThe picture changed in 1944 when Oswald T. Avery and his colleagues showed that a cell extract containing only DNA could transmit a new trait to a bacterium that would be inherited by subsequent generations. Shortly afterward, more sensitive analytical techniques enabled Erwin Chargaff to show how genetic information could be stored in DNA. The nucleic acids in DNA form pairs, he found, and, although the concentration of one partner in the pair always matches that of the other one, the relative amounts of the two pairs vary when the DNA comes from different sources.\n\nThese findings set the stage for the 1953 discovery of the double-helical structure of DNA by James D. Watson and Francis H. C. Crick. Their now-famous model in which paired bases form hydrogen-bonded steps in a molecular spiral staircase is undoubtedly one of the best known scientific discoveries of the century. As Crick himself wrote decades later, the DNA double helix captured the imagination of both scientists and the public, in part because the structure itself has such style and intrinsic beauty. It's important, as well, because it conceptually forms the bridge between chemistry and biology. The Watson-Crick model of DNA revealed genes as chemical entities that could-at least hypothetically-be analyzed, understood, and possibly changed using the tools of chemistry.\n\nUsing the DNA structure as a platform, many biochemists in the 1950s and '60s isolated the enzymes that control DNA metabolism, from its self-replicating synthesis to its role as the template for producing messenger RNA, which, in turn, determines the sequence of amino acids in protein chains. This sequence is specified by a \"code\" of consecutive base triplets of RNA, which was deciphered throughout the 1960s by Marshall W. Nirenberg and his colleagues at the National Institutes of Health. The code is a nearly universal one, they found, so that a piece of DNA from a human cell, for example, inserted into a bacterium will direct that cell's machinery to make the same protein that the human cell would make.\n\nAmong the enzymes involved in DNA metabolism, the restriction endonucleases form a class of particular practical importance. These molecules break the DNA chain at very specific locations. Among other uses, they have become tools to break down huge DNA molecules into manageable fragments to learn their sequences. In 1972, Paul Berg at Stanford University used a restriction endonuclease to cut DNA and then pasted two cut strands together with the aid of another enzyme, a ligase, to form a circular hybrid molecule, a recombinant DNA. The following year, Stanley Cohen, also at Stanford, and Herbert Boyer at the University of California, San Francisco, took the work further by splicing together pieces of viral and bacterial DNA that conferred specific traits-resistance to two antibiotics-and inserting this hybrid DNA into bacteria. In the process, they transferred the antibiotic resistance to the bacteria and produced a recombinant DNA organism.\n\nThe commercial potential of such recombinant organisms led almost immediately to the founding of several small companies and of a new field-biotechnology. Boyer helped to found Genentech in 1976; a year earlier Cohen had become a director of Cetus, an existing pharmaceutical company that refocused its attention toward exploiting the potential of biotechnology; and Biogen, another important early company in biotechnology, was founded in 1977.\n\nMeanwhile, researchers were also developing much more rapid methods to first determine the sequence of and then synthesize biological macromolecules. In 1945, Frederick Sanger at Cambridge University developed a way of attaching a dye molecule to the terminal amino group of a protein that allowed the labeled amino acid residue to be identified chromatographically after the protein had been partially degraded. Using this procedure and various cleaving agents, Sanger determined the first complete amino acid sequence of a protein, bovine insulin, in 1953. At about the same time, Swedish chemist Pehr Edman used phenyl isothiocyanate to remove only the terminal amino acid in a protein chain, opening the way to an automated method of degradation. Sanger later turned his attention to determining DNA sequences, developing a method to do so in the late 1970s that also could be automated.\n\nGenes and proteins are such huge molecules that the practical way to determine their sequence is to first break them into manageable pieces, determine the sequence of each piece, and then combine this information to reconstruct the entire sequence of the original macromolecule. Keeping track of all this information and reassembling it correctly requires the high-speed, high-capacity computers that were being developed independently during the 1960s and '70s.\n\nAdvances in automated sequencing of biological macromolecules continued so rapidly during the 1980s that by the end of the decade, researchers were planning what Thackray calls \"the final, ultimate mechanization of chemical analysis\"-an international effort to sequence all 3 billion bases in human DNA. Known as the Human Genome Project, the program officially got under way in 1990. Currently about halfway through its expected 15-year lifetime, the program involves researchers from more than 50 countries. Built into the project's design is an expectation that sequencing methods will continue to become both more rapid and less expensive.\n\nBiochemists were able to synthesize peptide bonds long before 1923, thanks principally to the work of Emil Fischer in the late 19th century. But Fischer's synthesis was laborious, and its yield, when applied to large macromolecules, was infinitesimal. As Thackray points out: \"When you are dealing with tens of thousands or hundreds of thousands of atoms, it's no good having someone at the bench doing the synthesis one peptide bond at a time. You've got to find some more automated way.\"\n\nThe route to automation opened up in the late 1950s and early '60s when R. Bruce Merrifield of Rockefeller University developed a solid-phase method to synthesize polypeptides. Merrifield attached the first amino acid of a polypeptide to a solid polymer. Each synthetic cycle added another amino acid to the peptide chain growing from the polymer, while leaving by-products and unreacted starting materials in a liquid phase that could be washed away. A similar approach was quickly applied to the synthesis of nucleic acid macromolecules.\n\nIn the 1960s, Merrifield was able to synthesize a nine-residue peptide with an 85% yield. Steady technical improvements have boosted yields, allowing the synthesis of longer peptide chains. In 1992, for example, Stephen Kent used solid-phase synthesis to make a protein that has 99 amino acids, HIV protease. Even so, the method is generally used only to make relatively small polypeptides or ones containing nonnatural amino acids. The most widely used technique for making larger proteins or any protein on a large scale has become molecular cloning based on recombinant DNA technology.\n\nIn the 1970s, Michael Smith at the University of British Columbia worked out a technique called site-directed mutagenesis that allows researchers to harness the cell's protein-synthesizing machinery while changing single amino acids in proteins at specific locations. The technique has become a fundamental tool for studying the relationship between structure and function in proteins and for tailoring the properties of proteins.\n\nIn 1985, Kary B. Mullis and coworkers at Cetus devised a technique to amplify relatively small segments of DNA over and over so that millions of copies can be produced in a few hours. The technique, called the polymerase chain reaction, uses chemically synthesized DNA fragments to prime a bacterial enzyme, DNA polymerase, to copy the desired portion of a DNA strand. The original DNA and the new copy then serve as templates for another round of polymerase-directed synthesis, leading to an exponential amplification of the original DNA segment.\n\nBiochemical pathways\n\nMacromolecules are not the only molecules at the interface of chemistry and biology, and C&EN's first decades were a period of important discoveries on a number of other fronts as well. For example, many of the basic pathways for the chemical processes going on in cells were mapped out during this period.\n\nIn the early 1930s, German biochemists Hans A. Krebs and Kurt Henseleit, studying the formation of urea in slices of rat liver, found that catalytic amounts of certain amino acids could greatly increase urea production. They proposed a catalytic cycle in which urea is formed from ammonia and aspartate with three amino acids as intermediates. Five years later, Krebs carried the idea of metabolic cycles a major step further when he pulled together work on the metabolic breakdown of carbohydrates and the biological reduction of oxygen to carbon dioxide into an elaborate cycle of reactions often still referred to as the Krebs cycle.\n\nMany of the early studies of metabolic pathways were done by identifying agents that could block metabolism, and then measuring what molecules accumulated when these blocking agents were administered to living animals. The advent of radioactive isotopes provided a way to identify much smaller amounts of metabolic intermediates and to trace individual atoms as they moved through the steps of normal metabolism.\n\nGeorge C. de Hevesy at the University of Freiburg, Germany, used radioactive thorium to study the transport of lead in bean plants in 1923. When de Hevesy's colleague Rudolf Schoenheimer was forced from Germany by the Nazi government in 1933, he fled to Columbia University, where Harold C. Urey had discovered deuterium the year before. For the remainder of the decade, Schoenheimer worked out methods to introduce first deuterium (which is not radioactive) and later other isotopes at specific locations in fatty acids and amino acids, which he then fed to animals to determine how these molecules are metabolized.\n\nAt the end of World War II, the long-lived radioisotopes 14C and 3H became available for investigating biochemical pathways, ushering in what medical historian Frederic L. Holmes calls \"the golden age of laying down all the metabolic pathways. The whole metabolic map fell together rather rapidly in the immediate post-war period.\"\n\nAfter the 1950s, the focus in metabolic studies shifted from unraveling basic pathways to understanding how the processes are controlled, usually by enzymes. In fact, by mid-century, all roads-whether structural analysis, nucleic acid synthesis, or metabolic studies-seemed to lead to enzymes and their role as biocatalysts. \"Understanding proteins as enzymes is the number one thing that biologically oriented chemists have tried to do over the past 75 years,\" says MIT's Lippard. That effort still dominates the field. \"Much of what we know and what we do as chemists in biology is to try to understand the details of enzyme catalysis,\" says Harvard's Whitesides.\n\nOrigins of life\n\nIn the 1920s, biochemists began to seriously consider how living systems might have first evolved on prebiotic Earth. In that decade, Russian biochemist Alexander I. Oparin and British biochemist John B. S. Haldane independently suggested that simple organic molecules are difficult to transform into the more complex ones found in living systems in an oxygen-rich atmosphere such as the one now found on Earth. However, they argued, a world without photosynthesizing plants might well have a hydrogen-rich, reducing atmosphere in which such reactions would occur more easily.\n\nThese ideas were slow to take hold, but in 1953, Stanley L. Miller and Urey, by then at the University of Chicago, experimentally re-created a mixture of hydrogen, methane, ammonia, and water that mimicked the prebiotic atmosphere of Earth. When the mixture was exposed to an electrical discharge to simulate lightning, 10% of the carbon was converted into a rich mixture of aldehydes and carboxylic and amino acids. Later experiments by Miller and others found that, by slightly varying the reaction mix, they could generate more than half of the amino acids found in proteins as well as adenine and other nucleic acid bases.\n\nThe rich complexity of interactions between nucleic acids and proteins unraveled during the 1960s raised the question of whether either polymer could have preceded the other one in the evolution of life. Toward the end of the decade, several researchers independently hypothesized that RNA seemed the most likely initial biopolymer. They proposed that modern life, based on interactions between DNA, RNA, and proteins, might have evolved from an earlier RNA world.\n\nThis hypothesis received a substantial boost in 1983 when Thomas R. Cech and Sidney Altman independently discovered that some reactions in contemporary biology are catalyzed by RNA. The first reactions to be identified were self-splicing reactions in bacteria in which strands of RNA catalyze the removal of an extraneous section from the middle of the polymer. More recently, RNAs have been synthesized that can join together oligonucleotides using ATP as an energy source.\n\nThe RNA-world hypothesis is a work-in-progress. Though it has many adherents, several key steps in the process have yet to be explained, such as why both proteins and nucleic acid polymers are made exclusively of l-isomers when the Miller-Urey experiment and others like it generate their products as racemic mixtures. Recent "
    }
}
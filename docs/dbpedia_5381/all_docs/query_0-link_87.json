{
    "id": "dbpedia_5381_0",
    "rank": 87,
    "data": {
        "url": "https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6640856/",
        "read_more_link": "",
        "language": "en",
        "title": "Emotional Expressions Reconsidered: Challenges to Inferring Emotion From Human Facial Movements",
        "top_image": "https://www.ncbi.nlm.nih.gov/corehtml/pmc/pmcgifs/pmc-card-share.jpg?_=0",
        "meta_img": "https://www.ncbi.nlm.nih.gov/corehtml/pmc/pmcgifs/pmc-card-share.jpg?_=0",
        "images": [
            "https://www.ncbi.nlm.nih.gov/coreutils/uswds/img/favicons/favicon-57.png",
            "https://www.ncbi.nlm.nih.gov/coreutils/uswds/img/icon-dot-gov.svg",
            "https://www.ncbi.nlm.nih.gov/coreutils/uswds/img/icon-https.svg",
            "https://www.ncbi.nlm.nih.gov/coreutils/nwds/img/logos/AgencyLogo.svg",
            "https://www.ncbi.nlm.nih.gov/corehtml/pmc/pmcgifs/logo-nihpa.png",
            "https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6640856/bin/nihms-1021596-f0001.jpg",
            "https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6640856/bin/nihms-1021596-f0002.jpg",
            "https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6640856/bin/nihms-1021596-f0003.jpg",
            "https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6640856/bin/nihms-1021596-f0004.jpg",
            "https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6640856/bin/nihms-1021596-f0005.jpg",
            "https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6640856/bin/nihms-1021596-t0012.jpg",
            "https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6640856/bin/nihms-1021596-t0013.jpg",
            "https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6640856/bin/nihms-1021596-t0014.jpg",
            "https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6640856/bin/nihms-1021596-t0015.jpg",
            "https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6640856/bin/nihms-1021596-t0016.jpg",
            "https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6640856/bin/nihms-1021596-t0017.jpg",
            "https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6640856/bin/nihms-1021596-t0018.jpg",
            "https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6640856/bin/nihms-1021596-t0019.jpg",
            "https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6640856/bin/nihms-1021596-t0020.jpg",
            "https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6640856/bin/nihms-1021596-t0021.jpg",
            "https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6640856/bin/nihms-1021596-t0022.jpg",
            "https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6640856/bin/nihms-1021596-t0023.jpg",
            "https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6640856/bin/nihms-1021596-t0024.jpg",
            "https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6640856/bin/nihms-1021596-t0025.jpg",
            "https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6640856/bin/nihms-1021596-t0026.jpg",
            "https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6640856/bin/nihms-1021596-t0027.jpg",
            "https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6640856/bin/nihms-1021596-t0028.jpg",
            "https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6640856/bin/nihms-1021596-t0029.jpg",
            "https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6640856/bin/nihms-1021596-t0030.jpg",
            "https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6640856/bin/nihms-1021596-t0031.jpg",
            "https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6640856/bin/nihms-1021596-t0032.jpg",
            "https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6640856/bin/nihms-1021596-t0033.jpg",
            "https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6640856/bin/nihms-1021596-t0034.jpg",
            "https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6640856/bin/nihms-1021596-t0035.jpg",
            "https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6640856/bin/nihms-1021596-t0036.jpg",
            "https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6640856/bin/nihms-1021596-t0037.jpg",
            "https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6640856/bin/nihms-1021596-t0038.jpg",
            "https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6640856/bin/nihms-1021596-t0039.jpg",
            "https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6640856/bin/nihms-1021596-t0040.jpg",
            "https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6640856/bin/nihms-1021596-t0041.jpg",
            "https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6640856/bin/nihms-1021596-f0006.jpg",
            "https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6640856/bin/nihms-1021596-f0007.jpg",
            "https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6640856/bin/nihms-1021596-t0042.jpg",
            "https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6640856/bin/nihms-1021596-t0043.jpg",
            "https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6640856/bin/nihms-1021596-t0044.jpg",
            "https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6640856/bin/nihms-1021596-t0045.jpg",
            "https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6640856/bin/nihms-1021596-f0008.jpg",
            "https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6640856/bin/nihms-1021596-f0009.jpg",
            "https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6640856/bin/nihms-1021596-f0010.jpg",
            "https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6640856/bin/nihms-1021596-f0011.jpg"
        ],
        "movies": [],
        "keywords": [],
        "meta_keywords": [
            ""
        ],
        "tags": null,
        "authors": [
            "Lisa Feldman Barrett",
            "Ralph Adolphs",
            "Stacy Marsella",
            "Aleix Martinez",
            "Seth D. Pollak"
        ],
        "publish_date": "2019-07-15T00:00:00",
        "summary": "",
        "meta_description": "It is commonly assumed that a person’s emotional state can be readily inferred from the person’s facial movements, typically called “emotional expressions” or “facial expressions.” This assumption influences ...",
        "meta_lang": "en",
        "meta_favicon": "https://www.ncbi.nlm.nih.gov/coreutils/nwds/img/favicons/favicon.ico",
        "meta_site_name": "PubMed Central (PMC)",
        "canonical_link": "https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6640856/",
        "text": "Psychol Sci Public Interest. Author manuscript; available in PMC 2020 Jul 1.\n\nPublished in final edited form as:\n\nPMCID: PMC6640856\n\nNIHMSID: NIHMS1021596\n\nPMID: 31313636\n\nEmotional Expressions Reconsidered: Challenges to Inferring Emotion From Human Facial Movements\n\n,1,2,3,* ,4 ,1,5,6 ,7 and 8\n\nLisa Feldman Barrett\n\n1Northeastern University, Department of Psychology, Boston, MA\n\n2Massachusetts General Hospital, Department of Psychiatry and the Athinoula A. Martinos Center for Biomedical Imaging, Charlestown, MA\n\n3Harvard Medical School, Department of Psychiatry, Boston MA\n\nFind articles by Lisa Feldman Barrett\n\nRalph Adolphs\n\n4California Institute of Technology, Departments of Psychology, Neuroscience, and Biology,Pasadena, CA\n\nFind articles by Ralph Adolphs\n\nStacy Marsella\n\n1Northeastern University, Department of Psychology, Boston, MA\n\n5Northeastern University, College of Computer and Information Science, Boston, MA\n\n6University of Glasgow, Glasgow, Scotland\n\nFind articles by Stacy Marsella\n\nAleix Martinez\n\n7The Ohio State University, Department of Electrical and Computer Engineering, and Center for Cognitive and Brain Sciences, Columbus, OH\n\nFind articles by Aleix Martinez\n\nSeth D. Pollak\n\n8University of Wisconsin - Madison, Department of Psychology, Madison, WI\n\nFind articles by Seth D. Pollak\n\n1Northeastern University, Department of Psychology, Boston, MA\n\n2Massachusetts General Hospital, Department of Psychiatry and the Athinoula A. Martinos Center for Biomedical Imaging, Charlestown, MA\n\n3Harvard Medical School, Department of Psychiatry, Boston MA\n\n4California Institute of Technology, Departments of Psychology, Neuroscience, and Biology,Pasadena, CA\n\n5Northeastern University, College of Computer and Information Science, Boston, MA\n\n6University of Glasgow, Glasgow, Scotland\n\n7The Ohio State University, Department of Electrical and Computer Engineering, and Center for Cognitive and Brain Sciences, Columbus, OH\n\n8University of Wisconsin - Madison, Department of Psychology, Madison, WI\n\n*Correspondence to: Lisa F. Barrett (ude.uen@tterrab.l; 125 Nightingale Hall, Northeastern University, Boston MA 02115, USA).\n\nAssociated Data\n\nSupplementary Materials\n\nNIHMS1021596-supplement-1.pdf (1.2M)\n\nGUID: 5FC1BC86-AAEA-48C5-BBC7-C26E45A7EE70\n\nAbstract\n\nIt is commonly assumed that a person’s emotional state can be readily inferred from the person’s facial movements, typically called “emotional expressions” or “facial expressions.” This assumption influences legal judgments, policy decisions, national security protocols, and educational practices, guides the diagnosis and treatment of psychiatric illness, as well as the development of commercial applications, and pervades everyday social interactions as well as research in other scientific fields such as artificial intelligence, neuroscience, and computer vision. In this paper, we survey examples of this widespread assumption, which we refer to as the “common view”, and then examine the scientific evidence for this view with a focus on the six most popular emotion categories used by consumers of emotion research: anger, disgust, fear, happiness, sadness and surprise. The available scientific evidence suggests that people do sometimes smile when happy, frown when sad, scowl when angry, and so on, more than what would be expected by chance. Yet there is substantial variation in how people communicate anger, disgust, fear, happiness, sadness and surprise, across cultures, situations, and even within a single situation. Furthermore, similar configurations of facial movements variably express instances of more than one emotion category. In fact, a given configuration of facial movements, such as a scowl, often communicates something other than an emotional state. Scientists agree that facial movements convey a range of social information and are important for social communication, emotional or otherwise. But our review suggests there is an urgent need for research that examines how people actually move their faces to express emotions and other social information in the variety of contexts that make up everyday life, as well as careful study of the mechanisms by which people perceive instances of emotion in one another. We make specific research recommendations that will yield a more valid picture of how people move their faces to express emotions, and how they infer emotional meaning from facial movements, as situations of everyday life. This research is crucial to provide consumers of emotion research with the translational information they require.\n\nExecutive Summary\n\nIt is commonly assumed that a person’s face gives evidence of emotions because there is a reliable mapping between a certain configuration of facial movements, called a “facial expression,” and the specific emotional state that it supposedly signals. This common view of facial expressions remains entrenched in consumers of emotion research, as well as in some scientists, despite an emerging consensus among affective scientists that emotional expressions are considerably more context-dependent and variable. Nonetheless, this common view continues to fuel commercial applications in industry and government (e.g., automated detection of emotions from faces), guide how children are taught (e.g., with posters and books showing stereotyped facial expressions), and impact clinical and legal applications (e.g., diagnoses of psychiatric illnesses and courtroom decisions). In this paper, we evaluate the common view of facial expressions against a review of the evidence and conclude that it rests on a number of flawed assumptions and incorrect interpretations of research findings. Our review is the most comprehensive and systematic to date, encompassing studies of healthy adults across cultures, newborns and young children, as well as people who are congenitally blind, and confirms that specific emotion categories -- anger, disgust, fear, happiness, sadness, and surprise – are each expressed with a particular configuration of facial movements, more reliably than would be expected by mere chance, but contrary to the common view, instances of these emotion categories are NOT expressed with facial movements that are sufficiently reliable and specific across contexts, individuals, and cultures to be considered diagnostic displays of any emotional state. Nor do human perceivers, in fact, infer emotions from particular configurations of muscle movements in a sufficiently reliable and specific way that similarly generalizes. Studies of expression production and perception both demonstrate multiple sources of variability that contradict the common view that smiles, scowls, frowns, and the like, are reliable and specific “expressions of emotion.” We conclude the paper with specific recommendations for both scientists and consumers of science.\n\nIntroduction\n\nFaces are a ubiquitous part of everyday life for humans. We greet each other with smiles or nods. We have face-to-face conversations on a daily basis, whether in person or via computers. We capture faces with smartphones and tablets, exchanging photos of ourselves and of each other on Instagram, Snapchat, and other social media platforms. The ability to perceive faces is one of the first capacities to emerge after birth: an infant begins to perceive faces within the first few days of life, equipped with a preference for face-like arrangements that allows the brain to wire itself, with experience, to become expert at perceiving faces (Arcaro et al., 2017; Cassia et al., 2004; Grossmann, 2015; Ghandi et al., 2017; Smith et al., 2018; Turati, 2004; but see Young & Burton (2018) for a more qualified claim).1 Faces offer a rich, salient source of information for navigating the social world: they play a role in deciding who to love, who to trust, who to help, and who is found guilty of a crime (Todorov, 2017; Zebrowitz, 1997, 2017; Zhang, Chen & Yang, 2018). Dating back to the ancient Greeks (Aristotle, in 4th century BCE) and Romans (Cicero), various cultures have viewed the human face as a window on the mind. But to what extent can a raised eyebrow, a curled lip, or a narrowed eye reveal what someone is thinking or feeling, allowing a perceiver’s brain to guess what that someone will do next?2 The answers to these questions have major consequences for human outcomes as they unfold in the living room, the classroom, the courtroom and even on the battlefield. They also powerfully shape the direction of research in a broad array of scientific fields, from basic neuroscience to psychiatry research.\n\nUnderstanding what facial movements might reveal about a person’s emotions is made more urgent by the fact that many people believe we already know. Specific configurations of facial muscle movements appear as if they summarily broadcast or display a person’s emotions, which is why they are routinely referred to as “emotional expressions” and “facial expressions.”3 A simple Google search using the phrase “emotional facial expressions” [see Box 1, in supplementary on-line materials (SOM)] reveals the ubiquity with which, at least in certain parts of the world, people believe that certain emotion categories are reliably signaled or revealed by certain facial muscle movement configurations – a set of beliefs were refer to as the common view (also called the classical view; Barrett, 2017a). Similarly, many cultural products testify to the common view. Here are several examples:\n\nTechnology companies are investing tremendous resources to figure out how to objectively “read” emotions in people by detecting their presumed facial expressions, such as scowling faces, frowning faces and smiling faces in an automated fashion. Several companies claim to have already done it (e.g., https://www.affectiva.com/what/products/; https://azure.microsoft.com/en-us/services/cognitive-services/emotion/). For example, Microsoft’s Emotion API promises to take video images of a person’s face to detect what that individual is feeling. The application states: “The emotions detected are anger, contempt, disgust, fear, happiness, neutral, sadness, and surprise. These emotions are understood to be cross-culturally and universally communicated with particular facial expressions”(https://azure.microsoft.com/en-us/services/cognitive-services/emotion/).\n\nCountless electronic messages are annotated with emojis or emoticons that are schematized versions of the proposed facial expressions for various emotion categories (https://www.apple.com/newsroom/2018/07/apple-celebrates-world-emoji-day/).\n\nPutative emotional expressions are taught to preschool children by displaying scowling faces, frowning faces, smiling faces and so on, in posters (e.g., use “feeling chart for children” in a Google image search), games (https://www.amazon.com/s/ref=nb_sb_noss_2?url=search-alias%3Daps&field-keywords=miniland+emotion) and books (e.g., Cain, 2000; Parr, 2005), and on episodes of Sesame Street (among many examples, see https://www.youtube.com/watch?v=ZxfJicfyCdg, https://vimeo.com/108524970, or https://www.youtube.com/watch?v=y28GH2GoIvc).4\n\nTelevision shows (e.g., Lie to Me), movies (e.g., Inside Out) and documentaries (e.g., The Human Face, produced by the British Broadcasting Company) customarily depict certain facial configurations as universal expressions of emotions.\n\nMagazine and newspaper articles routinely feature stories in kind: facial configurations depicting a scowl are referred to as “expressions of anger,” facial configurations depicting a smile are referred to as “expressions of happiness,” facial configurations depicting a frown are referred to as “expressions of sadness,” and so on.\n\nAgents of the U.S. Federal Bureau of Investigations (FBI) and the Transportation Security Administration (TSA) were trained to detect emotions and other intentions using these facial configurations, with the goal of identifying and thwarting terrorists (Rhonda Heilig, special agent with the FBI, personal communication, December 15, 2014, 11:20 am; https://how-emotions-are-made.com/notes/Screening_of_Passengers_by_Observation_Techniques).5\n\nThe facial configurations that supposedly diagnose emotional states also figure prominently in the diagnosis and treatment of psychiatric disorders. One of the most widely used task in autism research, the “Reading the Mind in the Eyes Test”, asks patients to match photos of the upper (eye) region of a posed facial configuration with specific mental state words, including emotion words (Baron-Cohen t al., 2001). Treatment plans for people living with autism and other brain disorders often include learning to recognize these facial configurations as emotional expressions (Baron-Cohen et al., 2004; Kouo & Egel, 2016). This training does not generalize well to real-world skills, however (Bergren et al., 2018; Kouo & Egel, 2016).\n\n“Reading” the emotions of a defendant (in the words of Supreme Court Justice Anthony Kennedy -- to “know the heart and mind of the offender”) is one pillar of a fair trial in the U.S. legal system and in many legal systems in the Western world (see Riggins v. Nevada, 1992). Legal actors like jurors and judges routinely rely on facial movements to determine the guilt and remorse of a defendant (e.g., Bandes, 2014; Zebrowitz, 1997). For example, defendants who are perceived as untrustworthy receive harsher sentences than they otherwise would (Wilson & Rule, 2015, 2016), and such perceptions are more likely when a person appears to be angry (i.e., facial structure is similar to the hypothesized facial expression of anger, which is a scowl (Todorov, 2017). An incorrect inference about a defendant’s emotional state can cost someone her children, her freedom, or even her life (for recent examples, see Barrett, 2017, beginning on page 183).\n\nBut can a person’s emotional state be reasonably inferred from that person’s facial movements? In this paper, we offer a systematic review of the evidence, testing the common view that instances of emotion are signaled with a distinctive configuration of facial movements with enough consistently that it can serve as a diagnostic marker of those instances. We focus our review on evidence pertaining to six emotion categories that have received the lion’s share of attention in the scientific literature -- anger, disgust, fear, happiness, sadness and surprise – and that, correspondingly, are the focus of common view (as evidenced by our Google search, summarized in Box 1, SOM), but our conclusions apply to all emotion categories that have thus far been scientifically studied. We open the paper with a brief discussion of its scope, approach, and intended audience. We then summarize evidence on how people actually move their faces during episodes of emotion, referred to as studies of expression production studies, following which we examine evidence for which emotions are actually inferred from looking at facial movements, referred to as studies of emotion perception. We identify three key shortcomings in the scientific research that have contributed to a general misunderstanding about how emotions are expressed and perceived in facial movements, and that limit the translation of this scientific evidence for other uses:\n\nlimited reliability (instances of the same emotion category are neither reliably expressed with or perceived from a common set of facial movements);\n\nlack of specificity (there is no unique mapping between a single configuration of facial movements and instances of the same emotion category); and,\n\nlimited generalizability (the effects of context and culture have not been sufficiently documented and accounted for).\n\nWe then discuss our conclusions, followed by proposals for consumers on how they might use the existing scientific literature. We also provide recommendations for future research with consumers of emotion research in mind. We have included additional detail on some topics of import or interest in the supplementary on-line materials (SOM).\n\nScope, Approach and Intended Audience of Paper\n\nThe Common View: Reading an Inner Emotional State of Mind From A Set of Unique Facial Movements\n\nIn common English parlance, people refer to “emotions” or “an emotion” as if anger, happiness, or any emotion word refers to an object that is highly similar on every occurrence. But an emotion word refers not to a unitary entity, but to a category of instances that vary from one another in their physical features, such as facial expressions and bodily changes, and mental features. Few scientists who study emotion, if any, take the view that every instance of an emotion category, such as anger, is identical to every other instance, sharing a set of necessary and sufficient features across situations, people and cultures. For example, Keltner and Cordaro (2017) recently wrote, “there is no one-to-one correspondence between a specific set of facial muscle actions or vocal cues and any and every experience of emotion” (p. 62). Yet there is considerable scientific debate about the amount of the within-category variation, the specific features that vary, the causes of the within-category variation, and implications of this variation for the nature of emotion (see ).\n\nOne popular scientific framework, referred to as the basic emotion approach, hypothesizes that instances of an emotion category are expressed with facial movements that vary, to some degree, around a typical set of movements (called a prototype) (for example, see ). For example, it is hypothesized that in one instance, anger might be expressed with the expressive prototype (e.g., brows furrowed, eyes wide, lips tightened) plus additional facial movements, such as a widened mouth, whereas on other occasions, a facial movement in the prototype might be missing (e.g., anger might be expressed with narrowed eyes or without movement in the eyebrow region; for a discussion, see Box 2, in SOM). Nonetheless, the basic emotion approach still assumes that the core facial configuration – the prototype -- can be used to diagnose a person’s inner emotional state in much the same way that a fingerprint can be used to uniquely recognize a person. More substantial variation in expressions (e.g., smiling in anger, gasping with widened eyes in anger, and scowling not in anger, but in confusion or concentration) is typically explained as the result of some process that is independent of an emotion itself, such as display rules, emotion regulation strategies such as suppressing the expression, or culture-specific dialects (as proposed by various scientists, including Elfenbein, 2013, 2017; Ekman & Cordaro, 2011; Matsumoto, 1990; Matsumoto, Keltner, Shiota, Frank, & O’Sullivan, 2008; Tracy & Randles, 2011).\n\nTable 1.\n\nProposed Expressive Configurations Described as Facial Action UnitsEmotion CategoryMatsumoto, Keltner, Shiota, O’Sullivan &\n\nFrank (2008)Cordaro, Sun, Keltner, Kamble,\n\nHuddar & McNeil (2017)Keltner et al. (in press)Physical DescriptionDarnin’s (1872) DescriptionObserved in reseearchReference Configuration UsedInternational Core PatternAmusementNot listedNot listed6, 12, 26 or 27, 55 or 56, a “head bounce” (Shiota, Campos & Keltner, 2003)6, 7, 12, 16, 25, 26 or 27, 536+7+12+25+26+53Head back, Duchenne smile (6, 7, 12), lips separated, jaw droppedAnger4+ 5+ 24+ 384 + 5 or 7 +22+23+244 +5 + 7 + 23 (Ekman, Levenson & Friesen, 1983)4, 74+5+17+23+24Brows furrowed, eyes wide, lips tightened and pressed togetherAweNot listedNot listed1, 5, 26 or 27, 57 and visible inhalation (Shiota et al., 2003)1, 2, 5, 12, 25, 26 or 27, 53Not listedContempt9+ 10+ 22+ 41+ 61 or 6212 (unilateral) + 14 (unilateral)12 + 14 (Ekman et al., 1983)4, 14, 25Not listedDisgust10+ 16+ 22+ 25 or 269 or 10, 25 or 269+15+16 (Ekman et al., 1983)4, 6, 7, 9, 10, 25, 26 or 277+9+19+25+26Eyes narrowed, nose wrinkled, lips parted, jaw dropped, tongue showEmbarrassmentNot listedNot listed12, 24, 51, 54, 64 (Keltner & Buswell, 1997)6, 7, 12, 25, 54, participant dampens smile with 23, 24, frown, etc.)7+12+15+52+54+64Eyelids narrowed, controlled smile, head turned and down, (not scored with FACS: hand touches face)Fear1+2+5+201+2+4+5+20, 25 or 261+2+4+5+7+20+26 (Ekman et al., 1983)1, 2, 5, 7, 25, 26 or 27, participant suddenly shifts entire body backwards in chair1+2+4+5+7+20+25Eyebrows raised and pulled together, upper eyelid raised, lower eyelid tense, lips parted and stretchedHappiness6+126 + 126+12 (Ekman et al., 1983)6, 7, 12, 16, 25, 26 or 276+7+12+25+26Duchenne smile (6, 7, 12)PrideNot listedNot listed6, 12, 24, 53, a straightening of the back and pulling back of the shoulders to expose the chest (Shiota et al., 2003)7, 12, 53, participant sits up straight53+64Head up, eyes downSadness1 + 151+15, 4,171+4+5 (Ekman et al., 1983)4, 43, 541+4+6+15+17Brows knitted, eyes slightly tightened, lip corners depressed, lower lip raisedShameNot listedNot listed54, 64 (Keltner & Buswell, 1997)4, 17, 5454+64Head down, eyes downSurprise1+ 2 + 5+ 25 or 261+2+5+25 or 261+2+5+26 (Ekman et al., 1983)1, 2, 5, 25, 26 or 271+2+5+25+26Eyebrows raised, upper eyelid raised, lips parted, jaw dropped\n\nBy contrast, other scientific frameworks propose that expressions of the same emotion category, such as anger, substantially vary by design, in a way that is tied to the immediate context, which includes the internal context (e.g., the person’s metabolic condition, the past experiences that come to mind, etc.) and the outward context (e.g., whether a person is at work, at school, or at home, who else is present the broader cultural conditions, etc.), both of which vary in dynamic ways over time (see Box 2, SOM). These debates, while useful to scientists, provide little clear guidance for consumers of emotion research who are focused on the practical issue of whether various emotion categories are expressed with facial configurations of sufficient regularity and distinctiveness so that it is possible to read emotion in a person’s face.\n\nThe common view of emotional expressions persist, too, because scientists’ actions often don’t follow their claims in a transparent, straightforward way. Many scientists continue to design experiments, use stimuli and publish review papers that, ironically, leave readers with the impression that certain emotion categories each have a single, unique facial expression, even as those same scientists acknowledge that every emotion category can be expressed with a variable set of facial movements. Published studies typically test the hypothesis that there are unique emotion-expression links (for examples, see the reference lists in Elfenbein & Ambady, 2002; Matsumoto, Keltner, Shiota, O’Sullivan & Frank, 2008; Keltner, Sauter, Tracy & Cowen, in press; also see most of the studies reviewed in this paper, e.g., Cordaro, Sun, Keltner, Kamble, Huddar, & McNeil, 2017). The exact facial configuration tested varies slightly from study to study, but a core facial configuration is still assumed (see for examples). This pattern of testing the hypothesis that instances of one emotion category are expressed with a single core facial configuration reinforces (perhaps unintentionally) the common view that each emotion category is consistently and uniquely expressed with its own distinctive configuration of facial movements. Review articles (again, perhaps unintentionally) reinforce the impression of unique face-emotion mappings by including tables and figures that display a single, unique facial configuration for each emotion category, referred to as the expression, signal or display for that emotion ( presents two recent examples).6 Consumers of this research then assume that a distinctive configuration can be used to diagnose the presence of the corresponding emotion (e.g., that a scowl indicates the presence of anger).\n\nThe common view of emotional expressions has also been imported into other scientific disciplines with an interest in understanding emotions, such as neuroscience and artificial intelligence (AI). For example, from a published paper on AI:\n\n“American psychologist Ekman noticed that some facial expressions corresponding to certain emotions are common for all the people independently of their gender, race, education, ethnicity, etc. He proposed the discrete emotional model using six universal emotions: happiness, surprise, anger, disgust, sadness and fear.” (Brodny, Kolakowska, Landowska, Szwoch, Szwoch, & Wróbel, 2016, p. 1, italics in the original)\n\nSimilar examples come from our own papers. One paper series of papers focused on the brain structures involved in perceiving emotions from facial configurations (Adolphs, 2002; Adolphs et al., 1994) and the other focused on early life experiences (Pollak et al., 2000; Pollak & Kistler, 2002). These papers were framed in terms of “recognizing facial expressions of emotion” and exclusively presented participants with specific, posed photographs of scowling faces (the presumed facial expression for anger), wide-eyed gasping faces (the presumed facial expression for fear), and so on. Participants were shown faces of different individuals all posing the same facial configuration for each emotion category, ignoring the importance of context. One reason for this flawed approach to investigating the perception of emotion from faces was that then -- at the time these studies were conducted – as now, published experiments, review articles, and stimulus sets were dominated by the common view that certain emotion categories were signaled with an invariant set of facial configurations, referred to as “facial expressions of basic emotions.”\n\nIn this paper, we review the scientific evidence that directly tests two beliefs that form the common view of emotional expressions: that certain emotion categories are each routinely expressed by a unique facial configuration and, correspondingly, that people can reliably infer someone else’s emotional state from a set of facial movements. Our discussion is written for consumers of emotion research, whether they be scientists in other fields or non-scientists, who need not have deep knowledge of the various theories, debates, and broad range of findings in the science of emotion, with sufficient pointers to those discussions if they are of interest (see Box 2, SOM).\n\nIn discussing what this paper is about – the common view that a person’s inner emotional state is revealed in facial movements -- it bears mentioning what this paper is not about: This paper is not a referendum on “basic emotion” view we briefly mentioned earlier in this section, proposed by the psychologist Paul Ekman and his colleagues, or any other research program or psychologist’s view. Ekman’s theoretical approach has been highly influential in research on emotion for much of the past 50 years. We often cite studies inspired by the basic emotion approach for this reason. In addition, the common view of emotional expressions is also most readily associated with a simplified version of basic emotion approach, as exemplified by the quotes above. Critiques of Ekman’s basic emotion view (and related views) are numerous (e.g., Barrett, 2006a, 2007, 2011; Ortony & Turner, 1990; Russell, 1991, 1994, 1995), as are rejoinders that defend it (e.g., Ekman, 1992, 1994; Izard, 2007). Our paper steps back from this dialogue. We instead take as our focus the existing research on emotional expression and emotion perception and ask whether it is sufficiently strong to justify the way it is increasingly being used by those who consume it.\n\nA Systematic Approach for Evaluating the Scientific Evidence\n\nWhen you see someone smile and infer that the person is happy, you are making what is known as a reverse inference: you are assuming that the smile reveals something about the person’s emotional state that you cannot access directly (see ). Reverse inference requires calculating a conditional probability: the probability that a person is in a particular emotion episode (such as happiness) given the observation of a unique set of facial muscle movements (such as a smile). The conditional probability is written as:\n\np[emotion category∣a unique facial configuration])\n\nfor example,\n\np[happiness∣a smiling facial configuration])\n\nReverse inferences about emotion are ubiquitous in everyday life – whenever you experience someone as emotional, your brain has performed a reverse inference, guessing at the cause of a facial movement when only having access to the movement itself. Every time an app on a phone or computer measures someone’s facial muscle movements, identifies a facial configuration such as a frowning facial configuration, and proclaims that the target person is sad, that app has engaged in reverse inference, such as:\n\np[sadness∣a frowning facial configuration])\n\nWhenever a security agent infers anger from a scowl, the agent has assumed a strong likelihood for\n\np[anger∣a scowling facial configuration])\n\nFour criteria must be met to justify a reverse inference that a particular facial configuration expresses and therefore reveals a specific emotional state: reliability, specificity, generalizability and validity (explained in and ). These criteria are commonly encountered in the field of psychological measurement and over the last several decades there has been an ongoing dialogue about thresholds for these criteria as they apply in production and perception studies, with some consensus emerging for the first three criteria (see Haidt & Keltner, 1999). Only when a pattern of facial muscle movements strongly satisfies these four criteria can we justify calling it an “emotional expression.” If any of these criteria are not met, then we should instead refer to a facial configuration with more neutral, descriptive terms without making unwarranted inferences, simply calling it a smile (rather than an expression of happiness), a frown (rather than an expression of sadness), a scowl (rather than an expression of anger), and so on.7\n\nTable 2:\n\nExpression ProductionEmotion PerceptionReliabilityWhen a person is sad, the proposed expression (a frowning facial configuration) should be observed more frequently than would be expected by chance. Likewise, for every other emotion category that is subject to a commonsense belief. Reliability is related to a forward inference: given that someone is happy, what is the likelihood of observing a smile, p[set of facial muscle movements ∣ emotion category].When a person makes a scowling facial configuration, perceivers should consistently infer that the person is angry. Likewise, for every facial configuration that has been proposed as the expression of a specific emotion category. That is, perceivers must consistently make a reverse inference: given that someone is scowling, what is the likelihood that he is angry, p[emotion category ∣ set of facial muscle movements].Chance means that facial configurations occur randomly with no predictable relationship to a given emotional state. This would mean that the facial configuration in question carries no information about the presence or absence of an emotion category. For example, in an experiment that observes the facial configurations associated with instances of happiness and anger, chance levels of scowling or smiling would be 50%.Chance means that emotional states occur randomly with no predictable relationship to a given facial configuration. This would mean that the presence or absence of an emotion category cannot be inferred from the presence or absence of the facial configuration. For example, in an experiment that observes how people perceive 51 different facial configurations, chance levels for correctly labeling a scowling face as anger would be 2%.Reliability also depends on the base rate: how frequently people make a particularly facial configuration overall. For example, if a person frequently makes a scowling facial configuration during an experiment examining the expressions of anger, sadness and fear, he will seem to be consistently scowling in anger when in fact he is scowling indiscriminately.Reliability also depends on the base rate: how frequently people use a particular emotion label or make a particular emotional inference. For example, if a person frequently labels facial configurations as “angry” during an experiment examining scowling, smiling and frowning faces, she will seem to be consistently perceiving anger when in fact she is labeling indiscriminately.Reliability rates between 70% and 90% provide strong evidence for the commonsense view, between 40% and 69% provide moderate support for the commonsense view, and between 20% and 39% provide weak support (Ekman, 1994; Haidt & Keltner, 1999; Russell, 1994).Reliability rates between 70% and 90% provide strong evidence for the commonsense view, between 40% and 69% provide moderate support for the commonsense view, and between 20% and 39% provide weak support (Ekman, 1994; Haidt & Keltner, 1999; Russell, 1994).SpecificityIf a facial configuration is diagnostic of a specific emotion category, then the facial configuration should express instances of one and only one emotion category better than chance; it should not consistently express instances of any other mental event (emotion or otherwise) at better than chance levels. For example, to be considered the expression of anger, a scowling facial configuration must not express sadness, confusion, indigestion, an attempt to socially influence, etc. at better than chance levels.If a frowning facial configuration is perceived as the diagnostic expression of sadness, then a frowning facial configuration should only be labeled as sadness (or sadness should only be inferred from a frowning facial configuration) at above chance levels. And it should not be consistently perceived as expressions of any mental states other than sadness at better than chance levels.Estimates of specificity, like reliability, depend on base-rates and on how chance levels are defined.Estimates of specificity, like reliability, depend on base-rates and on how chance levels are defined.GeneralizabilityPatterns of reliability and specificity should replicate across studies, particularly when different populations are sampled, such as infants, congenitally blind individuals and individuals sampled from diverse cultural contexts, including small-scale, remote cultures. High generalizability across different circumstances ensures that scientific findings are generalizable.Patterns of reliability and specificity should replicate across studies, particularly when different populations are sampled, such as infants, congenitally blind individuals and individuals sampled from diverse cultural contexts, including small-scale, remote cultures. High generalizability across different circumstances ensures that scientific findings are generalizable.ValidityEven if a facial configuration is consistently and uniquely observed in relation to a specific emotion category across many studies (strong generalizability), it is necessary to demonstrate that the person in question is really in the expected emotional state. This is the only way that a given facial configuration leads to accurate inferences about a person’s emotional state. A facial configuration is valid as a display or a signal for emotion if and only if it is strongly associated with other measures of emotion, preferably those that are objective and do not rely on anyone’s subjective report (i.e., a facial configuration should be strongly and consistently related to perceiver-independent evidence about the emotional state of the expresser).Even if a facial configuration is consistently and uniquely labeled with a specific emotion word across many studies (strong generalizability), it is necessary to demonstrate that the person making the facial configuration is really in the expected emotional state. This is the only way that a given perception or inference of emotion is accurate. A perceiver can only be said to be recognizing an emotional expression if and only if the person being perceived is verifiably in the expected emotional state.\n\nThe Null Hypothesis and the Role of Context\n\nTests of reliability, specificity, generalizability and validity are almost always compared to what would be expected by sheer chance, if facial configurations (in studies of expression production) and inferences about facial configurations (in studies of emotion perception) occurred randomly with no relation to particular emotional states. In most studies, chance levels constitute the null hypothesis. An example of the null hypothesis for reliability is that people do not scowl when angry more frequently than would be expected by chance.8 If people are observed to scowl more frequently when angry than they would by chance, then the null hypothesis can be rejected based on the reliability of the findings. We can also test the null hypothesis for specificity: If people scowl more frequently than they would by chance not only when angry but also when fearful, sad, confused, hungry, etc., then the null hypothesis for specificity is retained.9\n\nIn addition to testing hypotheses about reliability and specificity, tests of generalizability are becoming more common in the research literature, again using the null hypothesis. Questions about generalizability test whether a finding in one experiment is reproduced in other experiments in different contexts, using different experimental methods or sampling people from different populations. There are two crucial questions about generalizability when it comes to the production and perception of emotional expressions: Do the findings from a laboratory experiment generalize to observations in the real world? And, do the findings from studies that sample participants from Westernized, Educated, Industrialized, Rich and Democratic (WEIRD; Henrich, Heine, & Norenzayan, 2010) populations generalize to people who live in small-scale, remote communities?\n\nQuestions of validity are almost never addressed in production and perception studies. Even if reliable and specific facial movements are observed across generalizable circumstances, it is a difficult and unresolved question as to whether these facial movements can justify an inference about a person’s emotion state. We have more to say about this later. In this paper, we evaluate the common view by reviewing evidence pertaining to the reliability, specificity, and generalizability of research findings from production and perception studies.\n\nA focus on rejecting the null hypothesis, defined by what would be expected by chance alone, provides necessary but not sufficient support for the common view of emotional expressions. A slightly above chance co-occurrence of a facial configuration and instances of an emotion category, such as scowling in anger – for example, a correlation coefficient around r = .20 to .39 (adapted from Haidt & Keltner, 1999) -- suggests that a person sometimes scowls in anger, but not most or even much of the time. Weak evidence for reliability suggests that other factors not measured in the experiment are likely causing people to scowl during an instance of anger. It also suggests that people may express anger with facial configurations other than a scowl, possibly in reliable and predictable ways. Following common usage, we refer to these unmeasured factors collectively as context. A similar situation can be described for studies of emotion perception: when participants label a scowling facial configuration as “anger” in a weakly reliable way (between .20 and .39 percent of the time; Haidt & Keltner, 1999), then this suggests the possibility of unmeasured context effects.\n\nIn principle, context effects make it possible to test the common view by comparing it directly to an alternative hypothesis that a person’s brain will be influenced by other causal factors (as opposed to comparing the findings to random chance). It is possible, for example, that a state of anger is expressed differently depending on various factors that can be studied, including the situational context (such as whether a person is at work, at school, or at home), social factors (such as who else is present in the situation and the relationship between the expresser and the perceiver), the person’s internal physical context (based on how much sleep they had, how hungry they are, etc.), a person’s internal mental context (such as the past experiences that come to mind or the evaluations they make), the temporal context (what just occurred a moment ago), differences between people (such as whether someone is male or female, warm or distant), and the cultural context, such as whether the expression is occurring in a culture that values the rights of individuals (vs. group cohesion), is open and allows for a variety of behaviors in a situation (vs. closed, having more rigid rules of conduct). Other theoretical approaches offer some of these specific alternative hypotheses (see Box 2 in SOM). In practice, however, experiments almost always test the common view against the null hypothesis for reliability and specificity and rarely test specific alternative hypotheses. When context is acknowledged and studied, it is usually examined as a factor that might moderate a common and universal emotional expression, preserving the core assumptions of the common view (e.g., Cordaro et al., 2017; for more discussion, see Box 3, SOM).\n\nA Focus on Six Emotion Categories: Anger, Disgust, Fear, Happiness, Sadness and Surprise\n\nOur critical examination of the research literature in this paper focuses primarily on testing the common view of facial expressions for six emotion categories -- anger, disgust, fear, happiness, sadness and surprise. We do not include a discussion of every emotion category ever studied in the science of emotion. We do not discuss the many emotion categories that exist in non-English speaking cultures, such as gigil, the irresistible urge to pinch or squeeze something cute, or liget, exuberant, collective aggression (for discussion of non-English emotion categories, see Mesquita & Frijda, 1992; Pavlenko, 2014; Russell, 1991). We do not discuss the various emotion categories that have been documented throughout history (e.g., Smith, 2016). Nor do we discuss every English emotion category for which a prototypical facial expression has been suggested. For example, recent studies motivated primarily by the basic emotion approach have suggested that there are “more than six distinct facial expressions …in fact, upwards of 20 multimodal expressions” (Keltner et al., in press, pg. 4), meaning that scientists have proposed a prototypic facial configuration as the facial expression for each of twenty or so emotion categories, including confusion, embarrassment, pride, sympathy, awe, and so on.\n\nThe reasons for our focus on six emotion categories are twofold. First, anger, disgust, fear, happiness, sadness and surprise categories anchor common beliefs about emotions and their expressions (as is evident from Box 4, in SOM) and therefore represent the clearest, strongest test of the common view. Second, these six emotion categories have been the primary focus of systematic research for almost a century and therefore provide the largest corpus of scientific evidence that can be evaluated. Unfortunately, the same cannot be said for any of other emotion categories in question. This is a particularly important point when considering the twenty plus emotion categories that are now the focus of research attention. A PsycInfo search for the term “facial expression” combined with “anger, disgust, fear, happiness, sadness, surprise” produced over 700 entries, but a similar search including “love, shame, contempt, hate, interest, distress, guilt” returned less than 70 entries (Duran & Fernandez-Dols, 2018). Almost all cross-cultural studies of emotion perception have focused on just anger, disgust, fear, happiness, sadness and surprise (plus or minus a few) and experiments that measure how people spontaneously move their faces to express instances of emotion categories other than these six remain rare. In particular, there are too few studies that measure spontaneous facial movements during episodes of other emotion categories (i.e., production studies) to conclude anything about reliability and specificity, and there are too few studies of how these additional emotion categories are perceived in small-scale, remote cultures to conclude anything about generalizability. In an era where the generalizability and robustness of psychological findings are under close scrutiny, it seemed prudent to focus on the emotion categories for which there are, by a factor of ten, the largest number of published experiments. Our discussion, which is based on a sample of six emotion categories, generalizes to emotion categories that have been studied, however.10\n\nThe proposed expressive facial configurations for each emotion category are presented in , and the origin of these facial configurations is discussed in Box 4 in SOM. They originated with Charles Darwin, who stipulated (rather than discovered) that certain facial configurations are expressions of certain emotion categories, inspired by photographs taken by Duchenne and drawings made by the Scottish anatomist Charles Bell (Darwin, 1872). These stipulations largely form the basis of the common view of emotional expressions.\n\nProducing Facial Expressions of Emotion: A Review of the Scientific Evidence\n\nIn this section, we first review the design of a typical experiment where emotions are induced and facial movements are measured. This review highlights several observations to keep in mind as we review the reliability, specificity and generalizability for expressions of anger, disgust, fear, happiness, sadness and surprise in a variety of populations, including adults in both urban and small-scale remote cultures, infants and children, and congenitally blind individuals. Our review is the most comprehensive to date and allows us to comment on whether the scientific findings generalize across different populations of individuals. The value of doing so becomes apparent when we observe how similar conclusions emerge from these research domains.\n\nThe Anatomy of a Typical Experiment Designed to Observe People’s Facial Movements During Episodes of Emotion\n\nIn the typical expression production experiment, scientists expose participants to objects, images or events that they (the scientists) believe will evoke an instance of emotion. It’s possible, in principle, to evoke a wide variety of instances for a given emotion category (e.g., Wilson-Mendenhall et al., 2015), but in practice, published studies evoke the most typical instances of each category, often elicited with a stimulus that is presented without context (e.g., a photograph, a short movie clip separated from the rest of the film, etc.). Scientists usually include some measure to verify that participants are in the expected emotional state (such as asking participants to describe how they feel by rating their experience against a set of emotion adjectives). They then observe participants’ facial movements during the emotional episode and then quantify how well the measure of emotion predicts the observed facial movements. When done properly, this yields estimates of reliability and specificity, and in principle provides data to assess generalizability. There are limitations to assessing the validity of a facial configuration as an expression of emotion, as we explain below.\n\nMeasuring facial movements.\n\nHealthy humans have a common set of 17 facial muscle groups on each side of the face that contract and relax in patterns.11 To create facial movements that are visible to the naked eye, facial muscles contract, changing the distance between facial features (Neth & Martinez, 2009) and shaping skin into folds and wrinkles on an underlying skeletal structure. Even when facial movements look the same to the naked eye, there may be differences in their execution under the skin. There are individual differences in mechanics of making a facial movement, including variation in the anatomical details (e.g., everyone has a slightly different configuration and relative size of the muscles, some people lack certain muscle components, etc.), in the neural control of those muscles (Cattaneo & Pavesi, 2014; Hutto & Vattoth, 2015; Muri, 2015), and in the underlying skeletal structure of the face (discussed in Box 5, in SOM).\n\nThere are three common procedures for measuring facial movements in a scientific experiment. The most sensitive, objective measure of facial movements detects the electrical activity from actual muscular contractions, called facial electromyography (again, see Box 5, in SOM). This is a perceiver-independent way of assessing facial movements that detects muscle contractions that are not necessarily visible to the naked eye (Tassinary & Cacioppo 1992). Facial EMG’s utility is unfortunately offset by its impracticality: facial EMG requires placing electrodes on a participant’s face, which can cause skin abrasions. In addition, a person can typically tolerate only a few electrodes on the face at a time. At the writing of this paper, there were relatively few published papers using facial EMG (we identified 123 studies), the overwhelming majority of which sparsely sampled the face, measuring the electrical signals for only a small number of muscles (between one to six); none of the studies measured naturalistic facial movements as they occur outside the lab, in everyday life. As a consequence, we focus our discussion on two other measurement methods: a perceiver-dependent method that describes visible facial movements, called facial actions, which uses human coders who indicate the presence or absence of a facial movement while viewing video recordings of participants, and automated methods for detecting of facial actions from photographs or videos.\n\nMeasuring facial movements with human coders.\n\nThe Facial Action Coding System, or FACS (Ekman et al., 2002), is a systematic approach to describe what a face looks like when facial movements have occurred. FACS codes describe the presence and intensity of facial movements. Importantly, FACS is purely descriptive and is therefore agnostic about whether those movements might express emotions or any other mental event.12 Human coders train for many weeks to reliably identify specific movements called “action units” or AUs. Each AU is hypothesized to correspond to the contraction of a distinct facial muscle or a distinct grouping of muscles that is visible as a specific facial movement. For example, the raising of the inner corners of the eyebrows (contracting the frontalis muscle pars medialis) corresponds to AU 1. Lowering of the inner corners of the brows (activation of the corrugator supercilii, depressor glabellae and depressor supercilii) corresponds to AU 4. AUs are scored and analyzed as independent elements, but the underlying anatomy of many facial muscles constrains them so they cannot move independently of one another, generating dependencies between AUs (e.g., see Hao, Wang, Peng, & Ji, 2018). Facial action units (AU) and their corresponding list of facial muscles can be found in . Expert FACS coders approach inter-rater reliabilities of .80 for individual AUs (Jeni, Cohn, & De la Torre, 2013). The first version of FACS (Ekman & Friesen, 1978) was largely based on the work of Swedish anatomist Carl-Herman Hjortsjö who catalogued the facial configurations described by Duchenne (Hjortsjö, 1969). In addition to the updated versions of FACS (Ekman et al., 2002), other facial coding systems have been devised for human infants (Izard et al., 1995; Oster, 2003), chimpanzees (Vick et al., 2007), and macaque monkeys (Parr et al., 2010).13 displays the common FACS codes for the configurations of facial movements that have been proposed as the expression of anger, disgust, fear, happiness, sadness and surprise.\n\nTable 3:\n\nAUDescriptionFacial muscles (type of activation)1Inner brow raiserFrontalis (pars medialis)2Outer brow raiserFrontalis (pars lateralis)4Brow lowererCorrugator supercilii, depressor supercilii5Upper lid raiserLevator palpebrae superioris6Cheek raiserOrbicularis oculi (pars orbitalis)7Lid tightenerOrbicularis oculi (pars palpebralis)9Nose wrinkleLevator labii superioris alaquae nasi10Upper lip raiserLevator labii superioris11Nasolabial deepenerZygomaticus minor12Lip corner pullerZygomaticus major13Cheeks pufferLevator anguli oris14DimplerBuccinator15Lip corner depressorDepressor anguli oris16Lower lip depressorDepressor labii inferioris17Chin raiserMentalis18Lip puckererIncisivii labii superioris and incisivii labii inferioris20Lip stretcherRisorius w/ platysma22Lip funnelerOrbicularis oris23Lip tightenerOrbicularis oris24Lip pressorOrbicularis oris25Lips partDepressor labii inferioris or relaxation of mentalis, or orbicularis oris26Jaw dropMasseter, relaxed temporalis and internal terygoid27Mouth stretchPterygoids, digastric28Lip suckOrbicularis oris41Lid Droop42Slit43Eyes Closed44Squint45Blink46Wink\n\nMeasuring facial movements with automated algorithms.\n\nHuman coders require time-consuming, intensive training and practice before they can reliably assign AU codes. After training, it is a slow process to code photographs or videos frame by frame making human FACS coding impractical to use on facial movements as they occur in everyday life. Large inventories of naturalistic photographs and videos, which have been curated only fairly recently (Benitez-Quiroz et al., 2016), would require decades to manually code. This problem is addressed by automated FACS coding systems using computer vision algorithms (Martinez & Du, 2012; Martinez, 2017; Valstar et al., 2017).14 Recently developed computer vision systems have automated the coding of some (but not all) facial AUs (e.g., Benitez-Quiroz et al., in press; Benitez-Quiroz et al., 2017b; Chu et al., 2017; Corneanu et al., 2016; Essa & Pentland, 1997; Martinez, 2017a; Martinez & Du, 2012; Valstar et al., 2017; see Box 6, SOM) making it more feasible to observe facial movements as they occur in everyday life, at least in principle (see Box 7, SOM). Automated FACS coding is accurate (>90%) when compared to the AU codes from expert human coders, provided that the images were captured under ideal laboratory conditions, where faces are viewed from the front, are well illuminated, are not occluded, and are posed in a controlled way (Benitez-Quiroz et al., 2016). Under ideal conditions, accuracy is highest (~99%) when algorithms are tested and trained on images from the same database (Benitez-Quiroz et al., 2016). The best of these algorithms works quite well when trained and tested on images from different databases (~90%), as long as the images are all taken in ideal conditions (Benitez-Quiroz et al., 2016). Accuracy (compared to human FACS coding) decreases substantially more when coding facial actions in still images or in video frames taken in everyday life where conditions are unconstrained and facial configurations are not stereotypical (e.g.,Yitzhak et al., 2017).15 For example, 38 automated FACS coding algorithms were recently trained on one million images (the 2017 EmotioNet Challenge; Benitez-Quiroz et al., 2017a) and evaluated against separate test images which were FACS coded by experts.16 In these less constrained conditions, accuracy dropped below 83% and a combined measure of precision and recall (a measure called F1, ranging from zero to one) was below .65 (Benitez-Quiroz et al., 2017a).17 These results indicate that current algorithms are not accurate enough in their detection of facial AUs to fully substitute for expert coders when describing facial movements in everyday life. Nonetheless, these algorithms offer a distinct practical advantage because they can be used in conjunction with human coders to speed up the study of facial configurations in millions of images in the wild. It is likely that automated methods will continue to improve as better and more robust algorithms are developed and as more diverse face images become available.\n\nMeasuring an emotional state.\n\nOnce an approach has been chosen for measuring facial movements, a clear test of the common view of emotional expressions depends on having valid measures that reliably and specifically characterize the instances of each emotion category in a generalizable way, to which the measurements of facial muscle movements can be compared. The methods that scientists use to assess people’s emotional states vary in their dependence on human inference, however, which raises questions about the validity of the measures.\n\nRelatively objective measures of an emotional instance.\n\nThe more objective end of the measurement spectrum includes dynamic changes in the autonomic nervous system (ANS), such as cardiovascular, respiratory or perspiration changes (measured as variations in skin conductance), and dynamic changes in the central nervous system, such as changes in blood flow or electrical activity in the brain. These measures are thought to be more objective because the measurements themselves (the numbers) do not require a human judgment (i.e., the measurements are perceiver-independent). Only the interpretation of the measurements (their psychological meaning) requires human inference. For example, a human observer does not judge whether skin conductance or neural activity increases or decreases; human judgment only comes into play when the measurements are interpreted for the emotional meaning.\n\nCurrently, there are no objective measures, either singly or as a pattern, that reliability and uniquely identify one emotion category from another in a replicable way. Statistical summaries of hundreds of experiments, called meta-analyses, show for example, that currently there is no relationship between an emotion category, such as anger, and a single, specific set of physical changes in ANS that accompany the instances of that category, even probabilistically (the most comprehensive study published to date is Siegel et al., 2018, but for earlier studies see Cacioppo et al., 2000; Stemmler, 2004; also see Box 8, SOM). In anger, for example, blood pressure can go up, go down, or stay the same (i.e., changes in blood pressure are not consistently associated with anger). And a rise in blood pressure is not unique to instances of anger; it also can occur during a range of other emotional episodes (i.e., changes in blood pressure do not specifically occur in anger and only in anger). 18Individual studies often find patterns of ANS measures that distinguish an instance of one emotion category from another, but those patterns don’t replicate and instead vary across studies, even when studies use the same methods and stimuli, and sample from the same population of participants (e.g., compare findings from Kragel & LaBar, 2013 with Stephens, Christie, & Friedman, 2010). Similar within-category variation is routinely observed for changes in neural activity measured with brain imaging (Lindquist et al., 2012) and single neuron recordings (Guillory & Bujarski, 2014). For example, pattern classification studies discover multivariate patterns of activity across the brain for emotion categories such as anger, sadness, fear, and so on, but these patterns do not replicate from study to study (e.g., Kragel & LaBar, 2015; Saarimäki et al., 2016; Wager et al., 2015; for a discussion, see Clark-Polner et al., 2017). This observed variation does not imply that biological variability during emotional episodes is random, but rather that it may be context-dependent (e.g., yellow and green zones of ). It may also be the case that current biological measures are simply insufficiently sensitive or comprehensive enough to capture situated variation in a precise way. If this is so, then such variation should be considered unexplained, rather than random.\n\nThere is a difficult circularity built into these studies that is worth pointing out, and that we encounter again a few paragraphs down: Scientists must use some criterion for identifying when instances of an emotion category are present in the first place (so as to draw conclusions about whether or not emotion categories can be distinguished by different patterns of physical measurements).19 In most studies that attempt to find bodily or neural “signatures” of emotions, the criterion is a subjective one, either reported by the participants or provided by the scientist, which introduces problems of its own, as we discuss in the next section.\n\nSubjective measures of an emotional instance.\n\nWithout objective measures to identify the emotional state of a participant, scientists typically rely on the relatively more subjective measures that anchor the other end of the measurement spectrum. The subjective judgments can come from the participants (who complete self-report measures), from other observers (who infer emotion in the participants), or from the scientists themselves (who use a variety of criteria, including commonsense, to infer the presence of an emotional episode). These are all examples of perceiver-dependent measurements because the measurements themselves, as well as their interpretation, directly rely on human inference.\n\nScientists often rely on their own judgments and intuitions to stipulate when an emotion is present or absent in participants (as Charles Darwin did). For example, snakes and spiders are said to evoke fear. So are situations that involve escaping from a predator. Sometimes scientists stipulate that certain actions indicate the presence of fear, such as freezing or fleeing or even attacking in defense. The conclusions that scientists draw about emotions depends on the validity of their initial assumptions. It is noteworthy that when it comes to emotions, scientists use exactly the same categories as non-scientists, which may give us cause for concern, as forewarned by William James (James, 1890, 1894)20\n\nInferences about emotional episodes can also come from other people, for example independent samples of study participants, who categorize the situations in which facial movements are observed. Scientists can ask observers to infer when participants are emotional by having them judge subjects’ behavior or tone of voice; for example, see our discussion of Camras et al. (2007) discussed in the section on infants and children, below.\n\nA third common strategy to identify the emotional state of participants is to simply ask them what they are experiencing. Their self-reports of emotional experience then become the criteria for deciding whether an emotional episode is present or absent. Self-reports are often considered imperfect measures of emotion because they depend on subjective judgements and beliefs and require translation into words. In addition, a person can be experiencing an emotional event yet be unaware of it and therefore unable to report on it (i.e., a person can be conscious but unaware of their experience and unable to report it), or may be unable to express how they feel using emotion words, a condition known as alexithymia. Despite questions about their validity, self-reports are the most common measure of emotion that scientists compare to facial AUs.\n\nHuman inference and assessing the presence of an emotional state.\n\nAt this point, it should be obvious that any measure of an emotional state, to which measurements of facial muscle movements can be compared, itself requires some degree of human inference; what varies is the amount of inference that is required. Herein lies a problem: To properly test the hypothesis that certain facial movements reliably and specifically express emotion, scientists (ironically) must first make a reverse inference that an emotional event is occurring – that is, they infer the emotional instance by observing changes in the body, brain, and behavior (e.g., only if blood pressure consistently and uniquely rises in anger can a rise in blood pressure be used as a marker of anger). Or they infer (a reverse inference) that an event or object evokes an instance of a specific emotion category (e.g., an electric shock elicits fear but not irritation, curiosity, or uncertainty). These reverse inferences are scientifically sound only if measures of emotion reliably, specifically and validly characterize the instances of the emotion category. So, any clear, scientific test of the common view of emotional expressions rests on a set of more basic inferences about whether an emotional episode is present or absent, and any conclusions that come from such a test are only as sound as those basic inferences.\n\nIf all measures of emotion (to which measurements of facial muscle movements are compared) rest on human judgment to some degree, then, in principle, this prevents a scientist from being sure that an emotional state is present, which in turn limits the validity of any experiment designed to test whether a facial configuration validly expresses a specific emotion category. All face-emotion associations that are observed in an experiment reflect human consensus, i.e., the degree of agreement between self-judgments (of the participants), expert-judgments (of the scientist), and/or judgments of other observers (of perceivers who are asked to infer emotion in the participants). These types of agreement are often incorrectly referred to as accuracy. We touch on this point again when we discuss studies that test whether certain facial configurations are routinely perceived as expressions of anger, disgust, fear, and so on.\n\nTesting the common view of emotional expressions: Interpreting the scientific observations.\n\nIf a specific facial configuration reliably expresses instances of a certain emotion category in any given experiment, then we would expect measurements of the face (e.g., facial AU codes) to co-occur with measurements that indicate that participants are in the target emotional state. In principle, those measures might be more objective, such as ANS changes during an emotional event, or they might be more subjective, deriving from the scientist, from other perceivers who make judgments about the study participants, or from the participants themselves. In practice, however, most experiments compare facial movements to subjective measures of emotion -- a scientist’s judgment about which emotions are evoked by a particular stimulus, perceivers judgments about participants’ emotional states, or participants’ self-reports of emotional experience -- because ANS and other more objective measurements do not themselves distinguish one emotion category from another in a reliable and specific way. For example, in an experiment, scientists might ask: Do the AUs that create a scowling facial configuration co-occur with self-reports of feeling angry? Do the AUs that create a pouting facial configuration co-occur with perceiver’s judgments that participants are sad? Do the AUs that create a wide-eyed gasping facial configuration co-occur when people are exposed to an electric shock? And so on. If such observations suggest that a configuration of muscle movements is reliably observed during episodes of a given emotion category, then those movements are said to express the emotion in question. As we will see, many studies show that some facial configurations occur more often than random chance, but are not observed with a high degree of reliability (according to the criteria from Haidt & Keltner (1999), outlined in and ).\n\nIf a specific facial configuration specifically (i.e., uniquely) expresses instances of a certain emotion category in any given experiment, then we would expect to observe little co-occurrence between measurements of the face and measurements indicating the presence of emotional instances from other categories, except what would be expected by chance (again, see and ). For example, in an experiment, scientists might ask: do the AUs that create a scowling facial configuration co-occur with self-reports of feeling sad, confused, or social motives such as dominance? Do the AUs that create a pouting facial configuration co-occur with perceiver’s judgments that participants are angry or afraid? Do the AUs that create a wide-eyed gasping facial configuration co-occur when people are exposed to a competitor whom they are trying to scare? And so on.\n\nIf a configuration of facial movements is observed in instances of a certain emotion category in a reliable, specific way within an experiment, so that we can infer that the movements are expressing an instance of the emotion in that study as hypothesized, then scientists can safely infer that the facial movements in question are an expression of that emotion category’s instances in that situation. One more step is required before we can infer that the facial configuration is the expression of that emotion: we must observe a similar pattern of facial configuration-emotion co-occurrences across different experiments, to some extent generalizing across the specific measures and methods used and the participants and contexts sampled. If the facial configuration-emotion co-occurrences replicate across experiments that sample people from the same culture, then the facial configuration in question can be reasonably be referred to as an emotional expression only in that culture; e.g., if a scowling facial configuration co-occurs with measures of anger (and only anger) across most studies conducted on adult participants in the US who are free from illness, then it is reasonable to refer to a scowl as an expression of anger in the US. If facial configuration-emotion co-occurrences generalize across cultures – that is, replicate across experiments that sample a variety of instances of that emotion category in people from different cultures -- then the facial configuration in question can be said to universally express the emotion category in question.\n\nStudies of Healthy Adults from the U.S. and Other Developed Nations\n\nWe now review the scientific evidence from studies that document how people spontaneously move their facial muscles during instances of anger, disgust, fear, happiness, sadness and surprise, and how they pose their faces when asked to indicate how they express each emotion category. We examine evidence gathered in the lab and in naturalistic settings, sampling healthy adults who live in a variety of cultural contexts. To evaluate the reliability, specificity and generalizability of the scientific findings, we adapted criteria set out by Haidt & Keltner (1999), as discussed in .\n\nSpontaneous facial movements in laboratory studies.\n\nA meta-analysis was recently conducted to test the hypothesis that the facial configuration in co-occur, as hypothesized, with specific emotion categories (Duran et al., 2017). This analysis was published in a book chapter. Thirty-seven published articles reported on how people moved their faces when exposed to objects or events that evoke emotion. Most studies included in the meta-analysis were conducted in the laboratory. The findings from these experiments were statistically summarized to assess the reliability of facial movements as expressions of emotion (see ). In all emotion categories tested, other than fear, participants moved their facial muscles into the expected configuration more consistently than what we would expect by chance. Consistency levels were weak, however, indicating that the proposed facial configurations in have limited reliability (and to some extent, limited generalizability; i.e., a scowling facial configuration is an expression of anger, but not the expression of anger. More often than not, people moved their faces in ways that were not consistent with the hypotheses of the common view. An expanded version of this meta-analysis (Duran & Fernandez-Dols, 2018) analyzed 89 effect sizes from 47 studies totaling 3599 participants, with similar results: the hypothesized facial configurations were observed, with average effect sizes of r = .32 (for the average correlation between the intensity of a facial configuration and a measure of emotion, with correlations for specific emotion categories ranging from .25 to .38, corresponding to weak evidence of reliability) and proportion = .19 (for the average proportion of the times that a facial configuration was observed during an emotional event, with proportions for specific emotion categories ranging from .15 to .25, interpreted as no evidence to weak evidence of reliability).21\n\nNo overall assessment of specificity was reported in either the original or the expanded meta-analysis because most published studies do not report the false positive rate (i.e., the frequency with which a facial AU is observed when an instance of the hypothesized emotion category was not present; see ). Nonetheless, some striking examples of specificity failures have been documented in the scientific literature. For example, a certain smile, called a “Duchenne” smile, is defined in terms of facial muscle contractions (i.e., in terms of facial morphology): it involves movement of the orbiculari oculis which raises the cheeks and causes wrinkles at the outer corners of the eyes in addition to movement of the zygomatic major which raises the corners of the lips into a smile. A Duchenne smile is thought to be a spontaneous expression of authentic happiness. Research shows that a Duchenne smile can be intentionally produced when people are not happy, however (Gunnery & Hall, 2014; Gunnery et al., 2013; also see Krumhuber & Manstead, 2009), consistent with evidence that Duchenne smiles often occur when people are signaling submission or affiliation rather than solely reflecting happiness (Rychlowska et al., 2017).\n\nSpontaneous facial movements in naturalistic settings.\n\nStudies of facial configuration-emotion category associations in naturalistic settings tend to yield similar results to studies that were conducted in more controlled laboratory settings (Fernandez-Dols, 2017; Fernandez-Dols & Crivelli, 2013). Some studies observe that people express emotions in real world settings by spontaneously making the facial muscle movements proposed in , but such observations do not replicate well across studies (e.g., compare Matsumoto & Willingham, 2006 vs. Crivelli, Carrera and Fernandez-Dols, 2015; Rosenberg & Ekman, 1994 vs. Fernandez-Dols, Sanchez, Carrera, & Ruiz-Belda, 1997). For example, two field studies of winning judo fighters recently demonstrated that so-called “Duchenne” smiles were better predicted by whether an athlete was interacting with an audience than the degree of happiness reported after winning their matches (Crivelli, Carrera, & Fernandez-Dols, 2015). Only eight of the 55 winning fighters produced a “Duchenne” smile in Study 1; all occurred during a social interaction. Only 25 out of 119 winning fighters produced a “Duchenne” smile in Study 2, documenting, at best, weak evidence for reliability.\n\nPosed facial movements.\n\nAnother source of evidence comes from asking participants sampled from various cultures to deliberately pose the facial configurations that they believe they use to express emotions. In these studies, participants are given a single emotion word or a single, brief statement to describe each emotion category and then asked to freely pose the expression that they believe they make. In this way, they directly examine common beliefs about emotional expressions. For example, one study provided college students from Canada and Gabon (in Central Africa) with dictionary definitions for ten emotion categories. After practicing in front of a mirror, participants posed the facial configurations so that “their friends would be able to understand easily what they feel” and their poses were FACS coded (Elfenbein et al., 2007, p. 134). Similarly, a recent study asked college students in China, India, Japan, Korea, and the US, to pose the facial movements they believe they make when expressing each of 22 emotion categories (Cordaro, Sun, Keltner, Kamble, Huddar & McNeil, 2017). Participants heard a brief scenario describing an event that might cause anger (“You have been insulted, and you are very angry about it”) and then were instructed to pose a facial (and non-verbal, vocal) expression of emotion, as if the events in the scenario were happening to them. Experimenters were present in the testing room as participants posed their responses. Both studies found moderate to strong evidence for a cross-cultural, common expressive pose for anger, fear, and surprise categories, and weak to moderate evidence for the happiness category, with cultural variation around those common poses; the findings were weaker for disgust and sadness categories ( ).\n\nNeither study compared participants’ posed expressions to observations of how they actually moved their faces when expressing emotion. Nonetheless, a quick comparison of the findings from both studies and the proportions of spontaneous facial movements made during emotional events (from the Duran et al. (2017) meta-analysis) makes it clear that posed and spontaneous movements differ, sometimes quite substantially (again, see ). When people pose a facial configuration that they believe expresses an emotion category, they make facial movements that more reliably agree with the hypothesized facial configurations in . The same cannot be said of people’s spontaneous facial movements during actual emotional episodes, however (for convergent evidence, see Motley & Camden, 1988; Namba et al., 2016). One possible interpretation of these findings is that posed and spontaneous facial muscle configurations correspond to distinct communication systems. Indeed, there is some evidence that volitional and involuntary facial movements are controlled by different circuits in the skeletomotor system (Rinn, 1984). Another factor that may contribute to the discrepancy between posed and spontaneous facial movements is that people’s beliefs about their own behavior often reflect their stereotypes or beliefs and do not necessarily correspond to how they actually behave in real life (see Robinson & Clore, 2002).\n\nSummary.\n\nOur review of the available evidence thus far is summarized in the first through third data rows in . The hypothesized facial configurations presented in spontaneously occur with weak reliability during instances of the predicted emotion category, suggesting that they sometimes serve to express the predicted emotion. Furthermore, the specificity of each facial configuration as an expression of a specific emotion category is largely unknown (because it is typically not reported in many studies). In our view, this pattern of findings is most compatible with the interpretation that hypothesized facial configurations are not made reliably or specifically enough to use them to infer a person’s emotional state. We are not suggesting that facial movements are meaningless and devoid of information. Instead, the data suggest that the meaning of any set of facial movements may be much more variable and context-dependent than hypothesized by the common view.\n\nTable 4:\n\nReliabilitySpecificityExpression Production Adults, Developed, Spontaneous, Labweakunknown Adults, Developed, Spontaneous, Naturalisticweakunknown Adults, Developed, Posedweak to strongunknown Adults, Remote, Spontaneousunclearunknown Adults, Remote, Posedweak to strongunknown Newborns, Infants, Toddlersunsupportedunsupported Congenitally Blindunsupported to weakunsupportedEmotion Perception Adults, Developed, Choice-From-Arraymoderate to strongunknown Adults, Developed, Reverse\n\nCorrelation (with Choice-From-Array)moderatemoderate Adults, Developed, Free-Labelingweak to moderateweak Adults, Developed, Virtual Humansunknownunknown Adults, Remote, Choice-From-Array (before 2008)moderate to strongunknown Adults, Remote, Choice-From-Array (after 2008)weak to moderateunsupported Adults, Remote, Free-Labeling (before 2008)unsupported to strongvariable Adults, Remote, Free-Labeling (after 2008)unsupportedunsupported Infants, Young Childrenunsupportedunsupported\n\nStudies of Healthy Adults Living in Small-Scale, Remote Cultures\n\nThe emotion categories that are at the heart of common view– anger, disgust, fear, happiness, sadness and surprise -- were derived from modern US English (Wierzbicka, 2014) and their proposed expressions (in ) derive from observations of people who live in urbanized, Western settings. Nonetheless, it is hypothesized that these are facial configurations evolved as emotion-specific expressions to signal socially-relevant emotional information (Shariff & Tracy, 2011) in the challenging situations that originated in our hunting and gathering hominin ancestors who lived on the African savannah during the Pleistocene era (Pinker, 1997; Tooby & Cosmides, 1990). It is further hypothesized that these facial configurations should therefore be observed during instances of the predicted emotion categories with strong reliability and specificity in people around the world, although the facial movements might be slightly modified by culture (Cordaro et al., 2017; Ekman, 1972). The strongest test of these hypotheses would be to sample participants who live in remote parts of the world with relatively little exposure to western cultural norms, practices and values (Norenzayan & Heine, 2005; Henrich et al., 2010) and observe their facial movements during emotional episodes.22 In our evaluation of the evidence, we continued to use the criteria summarized by Haidt & Keltner (1999; see ).\n\nSpontaneous facial movements in naturalistic settings.\n\nOur review of scientific studies that systematically measure the spontaneous facial movements in people of small-scale, remote cultures is brief by necessity: there aren’t any. At the time of publication, we were unable to identify even a single published report or manuscript registered on open-access, pre-print services that measured facial muscle movements in people of remote cultures as they experienced emotional events. Scientists have almost exclusively observed how people label facial configurations as emotional expressions (i.e., they study emotion perception, not production) to test the hypothesis that certain facial configurations evolved to express certain emotion categories in a reliable, specific and generalizable (i.e., universal) manner. Later in the paper we return to this issue and discuss the findings from these emotion perception studies.\n\nThere are nonetheless several descriptive reports that provide support for the common view of universal emotional expressions (similar to what Valente et al., 2017 refer to as an “observational approach”). For example, the US psychologist Paul Ekman and colleagues curated an archive of photographs of the Fore hunter-gatherers taken during his visits to Papua New Guinea in the 1960s (Ekman,1980). The photographs were taken as people went about their daily activities in the small hamlets of the eastern highlands of Papua New Guinea. Ekman used his knowledge of the situation in which each photograph was taken to assign each facial configuration to an emotion category, leading him to conclude that the Fore expressed emotions with the proposed facial configurations shown in . Yet different scientific methods yielded a contrasting conclusion. When Trobriand Islanders living in Papua New Guinea were asked to infer emotions in facial configurations by labeling these photographs in their native language, both by freely offering words and by choosing the best fitting emotion word from a list of nine choices, they did not label the facial configurations as proposed by Ekman and colleagues at above chance levels (Crivelli et al., 2017).23 In fact, the proposed fear expression -- the wide-eyed gasping face -- is actually interpreted as an expression of threat (intent to harm) and anger by the Maori of New Zealand and in the Trobriand Islanders in remote Papua New Guinea (Crivelli & Fridlund, 2016).\n\nA compendium of spontaneous human behavior published by the Austrian ethologist Irenäus Eibl-Eibesfeldt (Eibl-Eibesfeldt, 1989) is sometimes cited as evidence for the hypothesis that certain facial movements are universal signals for specific emotion categories. No systematic coding procedure was used in his investigations, however. Upon close examination, Eibl-Eibesfeldt’s detailed descriptions appear to be more consistent with the studies of people from more industrialized cultures that we reviewed above: people move their faces in a variety of ways during episodes belonging to the same emotion category. For example, as reported by Eibl-Eibesfeldt, a rapid eyebrow raise (called an eyebrow flash) is thought to express friendly recognition in some, but not all, cultures. This movement would be coded with FACS AU 1 (inner brow raise) and AU 2 (outer brow raise) that are part of the proposed expressions for surprise and fear (Ekman et al., 1983), sympathy (Haidt & Keltner, 1999) and awe (Shiota et al., 2003). Even Eibl-Eibesfeldt acknowledged that eyebrow flashes were not unique expressions of specific emotion categories, writing that they also served as a greeting, to invite social contact, as a sign of thanks, an initiation of flirting, and a general indication of “yes” in Samoans and other Polynesians, in the Eipo and Trobriand islanders in Papua New Guinea, and in the Yanomami of South America. In Japan, eyebrow flashes are considered an impolite way for adults to greet one another. In the US and Europe, an eyebrow flash was observed when friends greet one another, but not strangers.\n\nPosed facial movements.\n\nOne study read a brief emotion story to people who live in the remote Fore culture of Papua New Guinea and asked each person to “show how his face would appear” if he was the person described in the emotion stories (Ekman, 1972, p. 273; sample size was not reported). Videotapes of nine participants were shown to 34 US college students who were asked to judge which emotion was being expressed. US participants were asked to infer the emotional meaning of the facial poses by choosing an emotion word from six choices provided by the experimenter (called a choice-from-array task, see ). Participants inferred the intended emotional meaning above chance guessing for smiling (happiness, 73%), frowning (sadness, 68%), scowling (anger, 51%), and nose-wrinkling (disgust, 46%), but not for surprise and fear (27% and 18% respectively).\n\nTable 5:\n\nConcernsAdditional ObservationsGeneral ConsiderationsTest-retest reliability is rarely evaluated but is critical. A number of contextual factors are known to influence judgments, including a perceiver’s internal state.Test-retest assessments are rarely done for practical reasons.Participants are typically asked to infer emotional meaning in exaggerated facial configurations. This reduces the ecological validity of the findings for how people infer emotional meaning in faces in the real world. The facial configurations used in most experiments (see ) are caricatures – they are exaggerated to maximally distinguish one from the another. Caricatures are easier to label (categorize) than are typical stimuli, particularly when the categories in question are highly interrelated (Goldstone, Steyvers, & Rogosky, 2003).Exaggerated facial configurations have greater “source clarity” (Ekman, Friesen & Ellsworth, 1972)Participants are typically asked to infer emotional meaning in highly selected facial configurations.In early studies, a smaller set of exaggerated facial configurations were culled from much larger sets of posed faces (involving several thousand faces; for a discussion, see Gendron & Barrett, 2017; Russell, 1994).Participants are typically asked to infer emotional meaning in static, non-moving facial configurations (i.e., in photographs rather than movies). This reduces the ecological validity of the findings for how people infer emotional meaning in faces in the real world. In the real world, people have to infer when a set of movements begin and end; this is called discrimination or detection).There is information in the dynamics of facial movements (Jack & Schyns, 2017; Krumhuber, Kappas & Manstead, 2013), but dynamic facial movements, particularly when they are spontaneous, do not always produce higher levels of agreement in emotion perception studies. Dynamic movements add realism, intensity and improve levels of agreement primarily when movements are degraded or are artificialParticipants are typically asked to infer emotional meaning in posed, rather than spontaneous, facial configurations.Spontaneous or candid facial configurations typically produce much lower levels of agreement in emotion perception studies (e.g., Kayyal & Russell, 2013; Naab & Russell, 2007).Only a single task used in most experiments (i.e., participants are asked to infer emotion in facial configurations via one method of responding). Ideally, multiple tasks should be used with the same population of participants to see if convergent results are obtained.This approach is rarely taken, but for an example, see Crivelli et al., 2016; Gendron et al., 2014; Gendron et al., 2018).Most experiments ask participants to infer emotion in a disembodied face, alone, without context. This reduces the ecological validity of the findings for how people infer emotional meaning in faces in the real world.A growing number of experiments now show that context is an important, and sometimes dominant, source of information when people infer emotional meaning in a facial configuration. See Box 3 in SOM. For example, Situational information tends to dominate perception of emotion in faces both when situations are common, everyday (Carrera-Levillain & Fernández-Dols, 1994) and even when situations are more ambiguous than the exaggerated facial configurations being judged (Carroll & Russell, 1996, Study 3).Many studies do not report evidence about the specificity of emotion perceptions, or the frequency with which people infer the non-intended emotional meaning to a facial configuration.Until recently, the large majority of experiments included only one pleasant emotion category (happiness) among several unpleasant emotion categories (anger, fear, sadness, etc.). This may be one reason that agreement rates are so high for smiles.In the last few years, experiments are now including a larger variety of pleasant emotion categories (pride, awe, gratitude, etc.), but there continues to be debate over whether or not they expect these emotion categories are expressed with consistent, specific facial configurations.Choice-From Array: matching photos of facial configurations and emotion words (with or without brief stories)\n\nResponse options are limited to those provided in the taskWords influence how the brain processes visual inputs from faces (e.g., Gendron et al., 2012; Doyle & Lindquist, 2018). Stories can prime action perceptions, as well (Gendron et al., in press). More generally, choice-from-array tasks have been shown to encourage biased perceptual responding using a signal detection analysis (e.g., DeCarlo 2012).Choice-from-array tasks are easy and efficient.The fact that participants are exposed to the same facial configurations and emotion words over and over allows them to learn the intended pairings even if they don’t know them to begin with (Nelson & Russell, 2016).An emotion word does not necessarily have a unique correspondence to a single emotion category for all people in a given culture (i.e., they may differ in emotional granularity; Barrett, 2004, 2017; Lindquist & Barrett, 2008) or people from different cultures.Concerns about individual word meaning is why choice-from-array using stories is preferable. Also, choice-from-array tasks are usually straightforward for participants to understand.A small range of answers are pre-determined by the experimenter, making it easier for participants to provide the answers scientists expect. For example, by constraining which words participants were allowed to choose from, frowns were consensually labeled as fear, wide-eyed gasping faces were labeled as surprise (Russell, 1993). Scowling faces are more likely to be perceived as fearful when paired with the description of danger (Carroll & Russell, 1996, Study 1) and appear determined or puzzled depending on the story they are presented with (Carroll & Russell, 1996, Study 2).Choice-from-array responses are easy for scientists to score. Most studies using continuous judgments (rather than forced choice) find that participants do not infer emotional meaning in facial configurations in a yes/no or on/off sort of way (Russell, 1994).People are asked to make yes/no decisions about assigning a facial configuration to an emotion category. Multiple emotion words may apply to a single configuration (i.e., people might infer more than one emotional meaning in a face), but the option to infer multiple emotional meanings rarely given to participants.Continuous judgments, such as on a Likert-type scale ranging from one to seven, would solve both of these problems, and also allow analysis of the similarity among facial configurations (which evidence shows is important, e.g., Jack et al., 2016; Kayal & Russell, 2013). Similarity allows scientists to discover the emotional meanings that people implicitly assign to a facial configuration, rather than having people explicitly state them (see further discussion of similarity below).A participant might decide that no emotion word provided applies to a facial configuration, but the option to respond this way is rarely given to participants (they are usually forced to choose an emotion word; for discussion, see Frank & Stennett, 2001).See Cordaro et al. (2016) for an example of this design feature.If a participant hears a story and is choose between two faces (e.g., a scowl and smile), she can give the expected answer (e.g., scowl) simply by figuring out that smile is NOT correct. For example, after hearing a story about anger, a participant is shown a scowl and a smile and can choose the scowl merely by realizing the smile is not correct (on the basis of valence). This is similar to getting an answer right on a multiple-choice test by eliminating all the alternatives—you don’t actually know the right answer, but you figured it out because of the structure of the task. A similar point can be made about showing a single face and asking participants to label it with a word by selecting from among a small set of options. Participants use a process of elimination strategy: words that are not chosen on prior trials are selected more frequently, inflating agreement levels (DiGirolamo & Russell, 2017).If a participant hears a story about anger and must choose between a scowl and a smile, she can figure out that the scowl is correct merely because she is distinguishing between negative (scowl) and positive (smile). If a participant hears a story about anger and must choose between a scowl and a frown, he can figure out that the scowl is correct merely because he is distinguishing between high arousal (scowl) and low arousal (frown).In tasks that involve brief stories or vignettes about emotion, only one typical story is offered for each emotion category, making it more difficult to observe any variation within a category.Free Sorting: photos of facial configurations are sorted into groupings, such that each grouping represents a category\n\nFace-to-Cue Matching: matching photos of facial configurations to a recording of posed vocalizationMost participants still spontaneously use words to guide their sorting and organize their groupings.Ideal for preverbal participants or those with semantic deficits (e.g., Lindquist et al., 2014).Similarity Judgments Between Pairs of facial configurations\n\nPerceptual Matching: Indicating whether or not two photos of facial configurations belong to the same emotion categoryIt is inefficient and time consuming to judge the similarity of all pairs of facial configurations. For a set of 100 faces, this requires (100*100)/2 = 5,000 different similarity judgments.Participants can arrange face stimuli on a computer screen and all pairwise similarity judgments can be computed (the SPAM method proposed by Goldstone, 1994; e.g., see Hout et al.,2013). This procedure also solves the problem that the same pair of stimuli will have a different judged similarity depending on which item is presented first if face pairs are presented sequentially presented faces (the judged similarity of two objects, A and B, can depend on the order in which they are presented; the similarity of A vs B is not always judged to be the same as B vs A; Tversky, 1977). Other advantages are that categories can be discovered, rather than prescribed, and verbal associations are minimized. Analyses of similarity judgments typically yield more continuous similarity relations between emotion categories along affective dimensions (see Russell & Barrett, 1999).Free-Labeling: photos of facial configurations are labeled with words offered by participants (unconstrained by experimenter)Forcing people to translate faces into words is not a good match, since much of the information from faces cannot be easily captured in words (Ekman, 1994).This is not a special criticism of free labeling studies -- it applies to all studies that ask people to label a face with words, including the choice-from-array tasks.Facial expressions did not evolve to represent specific verbal labels (Ekman, 1994, p. 270).“Regardless of the language, of whether the culture is Western or Eastern, industrialized or preliterate, these facial expressions are labeled with the same emotion terms: happiness, sadness, anger, fear, disgust, and surprise\" (Ekman, 1972, p. 278).There is no widely accepted method for categorizing freely provided responses. (Ekman, 1994, p. 274).Most scientists group together similar words (synonyms), so that a variety of words can be used to show evidence of a correct response (e.g., a frowning face, which is the proposed expression for sadness, could be labeled as \"sad,\" \"grieving,\" \"disappointed,\" \"blue,\" \"despairing,\" and so on. Scientists routinely use databases that indicate synonyms, like WORDNET (used in Srinivasan & Martinez (2018). Also, it is possible to do data-driven groupings of emotion words into semantic categories (e.g., Jack et al., 2016; Shaver et al., 1987). The more serious problem is that early studies using free-labeling (e.g., Boucher & Carlson, 1980; Izard, 1971) did not provide enough information in the method sections about how freely provided labels were grouped.Using freely chosen labels in a study of different cultures is difficult because it may be hard to find adequate translations (Ekman, 1994, p. 274). A given emotion word, like sadness, can correspond to different emotion concepts (with different features) in different languages (e.g., Wierzbicka, 1986, 2014). A single emotion word in one language can refer to more than one concept in another language (e.g., Pavlenko, 2014). Some languages have no one-to-one translation for English emotion words and some emotion concepts in other languages are not directly translatable into English emotion words (see Barrett, 2017; Russell, 1991; Jack et al., 2016).This is not a special criticism of free labeling studies – it holds for any experiment that uses emotion words requiring translation, including choice-from-array tasks. A standard solution to this problem is to use both forward and backward translation (e.g., a word spoken in Hadzane is translated into English and then back translated into Hadzane; if there is no broken telephone, then the translation has fidelity). An even better method is to elicit features for the emotion "
    }
}
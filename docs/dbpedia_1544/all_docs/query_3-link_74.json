{
    "id": "dbpedia_1544_3",
    "rank": 74,
    "data": {
        "url": "https://kpmg.com/be/en/home/insights/2021/11/rc-what-constitutes-a-successful-bcbs-239-journey-for-organizations.html",
        "read_more_link": "",
        "language": "en",
        "title": "What constitutes a successful BCBS 239 journey for organizations?",
        "top_image": "https://assets.kpmg.com/is/image/kpmg/Business-woman-looking-at-data-center-banner-1500x536px-435:cq5dam.web.1200.630",
        "meta_img": "https://assets.kpmg.com/is/image/kpmg/Business-woman-looking-at-data-center-banner-1500x536px-435:cq5dam.web.1200.630",
        "images": [
            "https://assets.kpmg.com/is/image/kpmg/KPMG-12",
            "https://assets.kpmg.com/is/image/kpmg/KPMG-12",
            "https://assets.kpmg.com/is/image/kpmg/Business-woman-looking-at-data-center-banner-1500x536px-435:cq5dam.web.1400.350",
            "https://assets.kpmg.com/is/content/kpmg/11-BCBS-principles?scl=1",
            "https://assets.kpmg.com/is/content/kpmg/G-SIB-Ratings-by-Principle-in-2017-and-1019?scl=1",
            "https://assets.kpmg.com/is/image/kpmg/ADV-RISK-DataRegulation:cq5dam.web.440.280",
            "https://assets.kpmg.com/is/image/kpmg/ADV-RISK-DataRegulation:cq5dam.web.440.280",
            "https://assets.kpmg.com/is/image/kpmg/data-strategy-management-banner-1500x536:cq5dam.web.440.280",
            "https://assets.kpmg.com/is/image/kpmg/data-strategy-management-banner-1500x536:cq5dam.web.440.280",
            "https://assets.kpmg.com/is/image/kpmg/Business-man-city-banner-1500x536px:cq5dam.web.440.280",
            "https://assets.kpmg.com/is/image/kpmg/Business-man-city-banner-1500x536px:cq5dam.web.440.280"
        ],
        "movies": [],
        "keywords": [],
        "meta_keywords": [
            ""
        ],
        "tags": null,
        "authors": [],
        "publish_date": "2022-01-05T11:08:00+01:00",
        "summary": "",
        "meta_description": "Regulatory-driven data management.",
        "meta_lang": "en",
        "meta_favicon": "/etc/designs/default/kpmg/favicons/apple-touch-icon-180x180-precomposed.png",
        "meta_site_name": "KPMG",
        "canonical_link": "https://kpmg.com/be/en/home/insights/2021/11/rc-what-constitutes-a-successful-bcbs-239-journey-for-organizations.html",
        "text": "To state the obvious, data are among the most valuable assets your organization has. Are you managing them as such? Data management is essential and enables organizations to fully benefit from the value of their data, even beyond the compliance aspect to build trust and improve decision-making.\n\nSince business processes are increasingly data-driven and automated, a lack of transparency in the quality of data represents a risk. Regulators have been directing their attention to data management, leading to the creation of new regulations, which require financial institutions to improve their practices and standards in the field.\n\nSince regulations greatly affect the processing of data, the following article will present an overview of these regulations and explain how to transform these new challenges into opportunities. In this article, we are going to focus on BCBS 239 principles, giving insights beyond the scope of the regulation, and sharing experiences and challenges on how best to implement these principles.\n\nFollowing the global financial crisis of 2008, banking regulators questioned whether financial institutions were able to aggregate risk exposures and identify concentrations quickly and effectively. As stated by the Basel Committee, one of the lessons learned from the crisis is that âbanksâ IT and data architectures were inadequate to support the broad management of financial risks.â\n\nIn response to this argument, regulators included stronger data aggregation requirements as part of the Pillar 2 guidance. These 11 principles are standard practices divided in three categories: data governance, data documentation and data quality management.\n\nThe BCBS 239 principles were initially aimed at Global Systemically Important Banks (G-SIBs). Yet, the principles have become a standard across the banking industry, as local supervisors follow the BCBS recommendations and apply the principles to Domestic Systemically Important Banks (D-SIBs). This however doesnât mean that only these banks are impacted. We see a change in regulatory thinking and a clear push towards better data management beyond the BCBS 239 principles, thereby affecting all financial institutions. We also see more and more organizations (not only banks) eager to improve their data management and encourage an appreciation for the importance of data among their teams.\n\nThe BCBS 239 principles have inspired different local initiatives and regulations that aim to improve data management and to broaden the target audience and scope (e.g. NBB circular 2017_27, data management streams as part of on-site inspections). The topic of data is now almost systematically included in new regulations and inspections, reflecting its importance to the regulator.\n\nFew surveyed institutions are fully compliant with all principles, and some are even materially non-compliant. However, there has been a positive evolution towards full compliance over the last two years. Regulators understand that compliance takes time and showing that steps are being taken (as part of a clear roadmap) to improve the situation is usually well perceived. However, banks that have significant delays, compared to their peers, are more likely to face fines and/or increased capital requirements, as well as additional scrutiny from regulators by way of, for example, on-site inspections.\n\nThe principles on which institutions scored most poorly were:\n\nPrinciple 2 - Data architecture and IT architecture\n\nSome key issues regarding this principle identified by the ECB via its thematic review in 2018 were: a lack of integrated solutions for data aggregation and reporting, a lack of a homogenous data taxonomy, a lack of complete and automated consistency checks and no clear escalation process. In addition to these issues, the ECB highlighted some best practices, including the use of common data sources for the production of all reporting, the automation and documentation of controls, a constant search for data quality improvement across the whole organization and a clear escalation process, involving all levels of management.\n\nIt is important to avoid going too big too fast or too big too long. Instead, it is better to implement new standards progressively, starting with a limited scope of key data elements (typically, organizations start with Finance or Risk data elements - which are used in key reports - such as LCR , CET1, LR and PD). When prioritizing and scoping, it is essential to consider upstream and downstream dependencies of key data elements considered for the project, since it is expected standards get applied to all these dependencies. Clear selection criteria should be defined in order to avoid subjective selection based on data ownersâ preferences. Selection criteria can be, for example, the degree of importance for the regulator, the degree of importance for internal decision-making bodies or business processes, the possibility of reputational damage/erroneous investment or commercial decisions/financial losses, etc.\n\nEvolving requirements from the business, as the project develops and becomes more concrete (e.g. additional information to complete the business glossary), may decrease the efficiency of the exercise at some point. It is best to take the time at the beginning of the project to discuss expected deliverables with the key stakeholders, and to demonstrate what the solution will look like. Performing the implementation in phases with a pilot data domain is a good test and lessens the impact of any potential changes. The requirements coming from new regulations (e.g. different data frequencies) also place obstacles along the data journey and may require the allocation of key resources.\n\nInternal alignment on business concepts and data elements definitions (i.e. getting the whole company to speak the same language across divisions and teams), can take a lot of time to achieve. While harmonizing as much as possible should be the goal, it is important to understand some business concepts/terms can mean different things to different departments. One solution is to introduce the notion of âcontextsâ, i.e. allowing for different definitions in function of the use of the concept. For example, ratings are commonly used in the banking industry for models or regulatory reporting. However, individual teams might use external or internal ratings and use them differently, which changes the sense of the term. Another example of a notion challenging organizations is that of âbalances.â Multiple types of balances might be created by various teams and, depending on the scope, have different meanings. When these contexts are clarified and shared, it becomes easier to identify a data owner within the team and creates better understanding and a reinforced collaboration.\n\nCollaboration between IT departments and the business teams, as well as between the different business lines, can be difficult in siloed organizations, especially when business lines have different maturity levels of data management. It is important to have an overarching strategy in place, as well as appropriate data bodies such as a data office that coordinates data activities, and a change management team that ensures the embedding of the data mindset and potential new tools in work routines. To ensure good collaboration, regular alignment with all key stakeholders is necessary, as well as the support and sponsoring of executive members.\n\nThe implementation of a new data culture in the organization, including the governance and maintenance of all deliverables, is necessary to have a lasting impact and avoid wasteful efforts on a one-off exercise. It requires convincing and getting people on board. In order to do so, it is important to involve them from the beginning of the project and to communicate regularly. Proper communication on the project is important to avoid creating false expectations, fears and rumors. Measuring the progress and the benefits that the project provides, as well as communicating them, is a key element in its success. It is recommended to present the big picture, highlighting the short-term and long-term added value it will bring to the whole organization, rather than only focus on compliance aspects. Finally, defining clear roles and responsibilities and training people appropriately are essential.\n\nImplementing regulations, in particular when data and data management are impacted, is always a challenge because they leave room for interpretation, making it difficult for organizations to evaluate whether they correctly understood the expectations of the regulator. A good way to solve this is to reference public results of previous reviews performed by the regulator (such as the Thematic Review on effective risk data aggregation performed by the ECB), and to use the expertise of companies able to benchmark against more mature financial institutions.\n\nTo enforce data management policies and maintain long-term compliance with standards, technology is recommended because it can significantly reduce manual work and operational risks by automating tasks. There are multiple features available on the market, some must-haves and some nice-to-haves, depending on the needs of the company. Among them are:\n\nAutomated extraction/discovery of metadata from data sources to fill in a data catalogue and data dictionary. Tools have scanners connecting to very diverse technologies, such as databases, ETL and scripts. Metadata from these technologies are extracted, interpreted and loaded into a data dictionary or a data catalogue. Changes are traced and reflected automatically.\n\nAutomated building of data lineage. Based on extracts of metadata, tools connect data elements with each other, as well as with business concepts, showing the end-to-end journey of data. They can also document transformations based on automated ETL flows analysis.\n\nAutomated classification of data. Tools can classify data on a large scale, allowing for the identification of personal or critical data that must be protected. They can also create intelligent groups/clusters useful for sorting/browsing through data in the data catalogue.\n\nAutomated notifications to data roles. Tools enable the reception of customized notifications and an overview of actions that data roles must take on their data (e.g. review of changes in the documentation or approve a new usage of the data). This makes it possible to keep up with all changes.\n\nAutomated reporting on data management KPI. Tools make it possible to define and measure KPIs (e.g. number of data elements validated), which allows for automated reporting for management.\n\nAutomated audit trail and history. Tools can keep track of all changes performed, increasing trust in the reports and streamlining corrections and comparisons over time.\n\nAutomated measurement/monitoring of data quality. Tools make it possible to measure data quality on a regular basis and on a large scale. A problem is often the configuration of the data quality monitoring; however, modern tools include the automated suggestion of data quality rules via AI.\n\nAutomated detection of anomalies via data profiling. Tools exist that can analyze data and detect abnormal patterns.\n\nAutomated data cleansing. Tools can be used to apply large-scale corrections/cleaning strategies on data.\n\nSelf-service: Tools empower data users by giving them all they need to work autonomously without the need to regularly contact data stewards and data owners.\n\nThe choice of tool can differ from one institution to another, depending on the size and complexity of the organization and its IT landscape. There are many different tools on the market; some are more advanced (and costly) than others. An analysis of the institutionâs requirements should be performed to determine the benefits of using a specific tool. Once the tool is selected, proper implementation will ensure its use in the organizationâs daily operations."
    }
}
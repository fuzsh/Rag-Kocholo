{
    "id": "dbpedia_443_1",
    "rank": 76,
    "data": {
        "url": "https://arxiv.org/html/2309.12443v2",
        "read_more_link": "",
        "language": "en",
        "title": "Active Learning for Multilingual Fingerspelling Corpora",
        "top_image": "",
        "meta_img": "",
        "images": [
            "https://arxiv.org/html/x1.png",
            "https://arxiv.org/html/x2.png",
            "https://arxiv.org/html/x3.png",
            "https://arxiv.org/html/x4.png",
            "https://arxiv.org/html/x5.png",
            "https://arxiv.org/html/x6.png",
            "https://arxiv.org/html/x7.png",
            "https://arxiv.org/html/x8.png",
            "https://arxiv.org/html/x9.png",
            "https://arxiv.org/html/x10.png",
            "https://arxiv.org/html/x11.png",
            "https://arxiv.org/html/x12.png",
            "https://arxiv.org/html/x13.png",
            "https://arxiv.org/html/x14.png",
            "https://arxiv.org/html/x15.png"
        ],
        "movies": [],
        "keywords": [],
        "meta_keywords": [
            ""
        ],
        "tags": null,
        "authors": [],
        "publish_date": null,
        "summary": "",
        "meta_description": "",
        "meta_lang": "en",
        "meta_favicon": "",
        "meta_site_name": "",
        "canonical_link": null,
        "text": "\\nameShuai Wang \\emailshuai.wang@student.uva.nl\n\n\\nameEric Nalisnick \\emaile.t.nalisnick@uva.nl\n\n\\addrInformatics Institute\n\nUniversity of Amsterdam\n\nAmsterdam, Netherlands\n\nAbstract\n\nWe apply active learning to help with data scarcity problems in sign languages. In particular, we perform a novel analysis of the effect of pre-training. Since many sign languages are linguistic descendants of French sign language, they share hand configurations, which pre-training can hopefully exploit. We test this hypothesis on American, Chinese, German, and Irish fingerspelling corpora. We do observe a benefit from pre-training, but this may be due to visual rather than linguistic similarities.\n\nKeywords: Active Learning, Transfer Learning, Multilingual Sign Language\n\n1 Introduction\n\nWhile there has been recent advances in technologies for written languages (e.g. BERT (Devlin et al., 2019), GPT-3 (Brown et al., 2020)), signed languages have received little attention (Yin et al., 2021). One reason for this gap is that sign language processing (SLP) is inherently more challenging since identifying signs is a complex computer vision taskâ€”compared to string matching for written languages. Yet, the scarcity of sign language corpora is the largest hurdle to advances in SLP. There are, of course, relatively fewer users of signed languages, leading to less available training data. Moreover, the relative scarcity of native signers makes procuring high-quality labels and annotations for supervised learning difficult and costly.\n\nIn this work, we make progress on alleviating data scarcity in SLP by applying active learning (AL) (Settles, 2012) to multilingual fingerspelling corpora. Fingerspelling is a sub-task in sign language communication: the signer spells-out a word associated with a concept that does not have an associated sign, such as a proper noun. For example, if a signer wants to reference â€œGPT-3,â€ the signer would make a hand shape for each letter â€˜G,â€™ â€˜Pâ€™, and â€˜Tâ€™. See Figure 1 for example images. In addition to detecting sign-less concepts, recognizing fingerspelling is important because the hand configuration for some concepts are the same as the first letter of the associated word. For instance, in American sign language, the concept of â€˜readyâ€™ is signed with the hands in the â€˜râ€™-configuration.\n\nWhile fingerspelling recognition is a relatively easier task than full SLP, it can still be challenging for low-resource languages. Consider Irish sign language: it has just 5000500050005000 active deaf users, and to make matters more complicated, has variations depending on the gender of the signer. Collecting large, diverse data sets for languages such as this one requires great effort. Thus, we also perform transfer active learning by exploiting linguistic relations across sign languages. Specifically, American, Irish, German, and Chinese sign languages all are descendants of French sign language, thus resulting in shared hand configurations, among other features. We conjecture that learning can be done more quickly by first pre-training a model on a related sign language and then performing AL on an especially low-resource target language.\n\nWe report two sets of experiments. In the first, we perform AL on American, Irish, German, and Chinese sign languages, demonstrating improvements over random sampling. In the second, we perform transfer AL by first pre-training the model on one of the other three sign languages. This is the first work to investigate leveraging linguistic relationships for transfer AL. While we observe some benefits, our initial results suggest that pairing data sets with a similar visual style improves transfer AL more than having a strong linguistic relationship but dissimilar visual style.\n\n2 Data Sets: Multilingual Fingerspelling Corpora\n\nWe use the following publicly available fingerspelling corpora, all of which have a linguistic relationship due to being descendants from French sign language. In all cases, we standardize the image resolution to 28Ã—28282828\\times 2828 Ã— 28 pixels. For each data set, we resample the frequency of each letter so that it follows the character distribution of the corresponding written language. As a result, the data sets are class imbalanced.\n\n1.\n\nAmerican Fingerspelling (ASL): This data set is availabe on Kaggle and contains 34,6273462734,62734 , 627 grayscale images (resolution 28Ã—28282828\\times 2828 Ã— 28) for 24242424 alphabetic characters used in American sign language (ASL). The characters â€˜Jâ€™ and â€˜Zâ€™ are omitted because they are dynamic signs that cannot be well-represented by a static image. The images are closely cropped to the hand and in turn do not require pre-processing.\n\n2.\n\nChinese Fingerspelling (CSL) (Jiang et al., 2020): This data set consists of the alphabetic characters used in Chinese sign language (CSL). Specifically, there are 1320132013201320 images, each labeled as one of 30303030 lettersâ€”26262626 are the characters A through Z and 4444 are double syllable letters (ZH, CH, SH, and NG). The resolution is originally 256Ã—256256256256\\times 256256 Ã— 256, and we down-sample them to 28Ã—28282828\\times 2828 Ã— 28. We discard the double syllable letters since they are unamenable to transfer learning. We convert all images to grayscale and remove â€˜Jâ€™ and â€˜Zâ€™ so that the data set is aligned with the ASL data set.\n\n3.\n\nGerman Fingerspelling (GSL) (Dreuw et al., 2006): This data set contains alphanumeric characters used in German sign language (GSL). The original data set consists of video sequences for 35353535 gestures: the 26262626 letters A to Z, the 4444 German umlauts (SCH, Ã„, Ã–, Ãœ), and the numbers from 1 to 5. We extracted 20,9042090420,90420 , 904 static images for the letters from the video frames. We remove the background environment using a ResNet-101 model pre-trained for segmentation (Chen et al., 2018). We again convert the images to grayscale and remove â€˜Jâ€™ and â€˜Zâ€™.\n\n4.\n\nIrish Fingerspelling (ISL) (Oliveira et al., 2017): This data set contains the alphabetic characters used in Irish sign language (ISL). It consists of 58,1145811458,11458 , 114 images for the 26262626 letters A through Z. This data set is unique from the others in that the sign is repeated by moving the forearm from a vertical to near horizontal position. Again, we convert the images to grayscale and remove â€˜Jâ€™ and â€˜Zâ€™.\n\nFigure 1 shows images for the letters â€˜Aâ€™ and â€˜Pâ€™ from the four corpora listed above. The sign for â€˜Aâ€™ is very similar for ASL, GSL, and ISL. The sign for â€˜Pâ€™ is the same for ASL and GSL but different for CSL and ISL.\n\n3 Methodology: Active Learning\n\nOur aim is to obtain a predictive model for fingerspelling recognition. Given an input image, the model should be able to predict a label representing the alphabetic character being signed. Yet, given the resource constraints for signed languages (discussed in Section 1), we wish to train a highly accurate predictive model using as few labeled instances as possible. To achieve this, we rely on the methodology of active learning (AL) (Settles, 2012), which allows us to obtain a sequence of highly informative labels from an oracle (such as a human expert). The hope is that the intelligent selection of these labels allows the model to achieve satisfactory performance while minimizing queries of the oracle. Formally, we assume the model is first trained on an initial data set ğ’Ÿ0={ğ’™n,yn}n=1Nsubscriptğ’Ÿ0superscriptsubscriptsubscriptğ’™ğ‘›subscriptğ‘¦ğ‘›ğ‘›1ğ‘\\mathcal{D}_{0}=\\{{\\bm{x}}_{n},y_{n}\\}_{n=1}^{N}caligraphic_D start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT = { bold_italic_x start_POSTSUBSCRIPT italic_n end_POSTSUBSCRIPT , italic_y start_POSTSUBSCRIPT italic_n end_POSTSUBSCRIPT } start_POSTSUBSCRIPT italic_n = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_N end_POSTSUPERSCRIPT, with Nğ‘Nitalic_N being relatively small for the problem at hand. We also assume access to an unlabeled pool set ğ’³p={ğ’™m}m=1Msubscriptğ’³ğ‘superscriptsubscriptsubscriptğ’™ğ‘šğ‘š1ğ‘€\\mathcal{X}_{p}=\\{{\\bm{x}}_{m}\\}_{m=1}^{M}caligraphic_X start_POSTSUBSCRIPT italic_p end_POSTSUBSCRIPT = { bold_italic_x start_POSTSUBSCRIPT italic_m end_POSTSUBSCRIPT } start_POSTSUBSCRIPT italic_m = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_M end_POSTSUPERSCRIPT such that M>>Nmuch-greater-thanğ‘€ğ‘M>>Nitalic_M > > italic_N. The oracle has access to the corresponding labels for this pool set, denoted ğ’´p={ym}m=1Msubscriptğ’´ğ‘superscriptsubscriptsubscriptğ‘¦ğ‘šğ‘š1ğ‘€\\mathcal{Y}_{p}=\\{y_{m}\\}_{m=1}^{M}caligraphic_Y start_POSTSUBSCRIPT italic_p end_POSTSUBSCRIPT = { italic_y start_POSTSUBSCRIPT italic_m end_POSTSUBSCRIPT } start_POSTSUBSCRIPT italic_m = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_M end_POSTSUPERSCRIPT.\n\nAL for image data has been well-studied (Gal et al., 2017), and various acquisition functions have been proposed. In preliminary experiments, we studied the following: maximum entropy (Shannon, 1948), BALD (Houlsby et al., 2011), variation ratios (Freeman, 1965), and mean standard deviation. We found all methods to be comparable but with variation ratios to have a slight edge. In turn, we use variation ratios in all experiments. At time step tğ‘¡titalic_t, a point from the pool set is selecting according to:\n\nğ’™tâˆ—=arg maxğ’™âˆˆğ’³p,tâˆ’11âˆ’maxyâ¡pâ¢(y|ğ’™,ğœ½tâˆ’1)subscriptsuperscriptğ’™âˆ—ğ‘¡ğ’™subscriptğ’³ğ‘ğ‘¡1arg max1subscriptyğ‘conditionalyğ’™subscriptğœ½ğ‘¡1{\\bm{x}}^{\\ast}_{t}\\ \\ =\\ \\ \\underset{{\\bm{x}}\\in\\mathcal{X}_{p,t-1}}{\\text{% arg max}}\\quad 1-\\max_{{\\textnormal{y}}}p\\left({\\textnormal{y}}|{\\bm{x}},{\\bm{% \\theta}}_{t-1}\\right)bold_italic_x start_POSTSUPERSCRIPT âˆ— end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT = start_UNDERACCENT bold_italic_x âˆˆ caligraphic_X start_POSTSUBSCRIPT italic_p , italic_t - 1 end_POSTSUBSCRIPT end_UNDERACCENT start_ARG arg max end_ARG 1 - roman_max start_POSTSUBSCRIPT y end_POSTSUBSCRIPT italic_p ( y | bold_italic_x , bold_italic_Î¸ start_POSTSUBSCRIPT italic_t - 1 end_POSTSUBSCRIPT ) (1)\n\nwhere pâ¢(y|ğ’™,ğœ½tâˆ’1)ğ‘conditionalyğ’™subscriptğœ½ğ‘¡1p\\left({\\textnormal{y}}|{\\bm{x}},{\\bm{\\theta}}_{t-1}\\right)italic_p ( y | bold_italic_x , bold_italic_Î¸ start_POSTSUBSCRIPT italic_t - 1 end_POSTSUBSCRIPT ) is the modelâ€™s maximum class confidence after being fit during the previous time step. After obtaining ğ’™tâˆ—subscriptsuperscriptğ’™âˆ—ğ‘¡{\\bm{x}}^{\\ast}_{t}bold_italic_x start_POSTSUPERSCRIPT âˆ— end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT, the oracle is queried for the associated label, and the model is re-fit to data that includes the selected feature-label pair (to obtain ğœ½tsubscriptğœ½ğ‘¡{\\bm{\\theta}}_{t}bold_italic_Î¸ start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT).\n\nTransfer Active Learning\n\nKnowledge gained from other domains can be helpful to improve performance on the target domain. In fact, Tamkin et al. (2022) suggest that the ability to actively learn is an emergent property of pre-training. We combine this inspiration with the fact that ASL, CSL, GSL, and ISL are linguistically related, proposing to pre-train on a related fingerspelling corpora before performing AL on the target domain. Specifically, we assume access to an auxiliary data set ğ’Ÿâˆ’1={ğ’™l,yl}l=1Lsubscriptğ’Ÿ1superscriptsubscriptsubscriptğ’™ğ‘™subscriptğ‘¦ğ‘™ğ‘™1ğ¿\\mathcal{D}_{-1}=\\{{\\bm{x}}_{l},y_{l}\\}_{l=1}^{L}caligraphic_D start_POSTSUBSCRIPT - 1 end_POSTSUBSCRIPT = { bold_italic_x start_POSTSUBSCRIPT italic_l end_POSTSUBSCRIPT , italic_y start_POSTSUBSCRIPT italic_l end_POSTSUBSCRIPT } start_POSTSUBSCRIPT italic_l = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_L end_POSTSUPERSCRIPT. We train the model of interest on this data set before training it on the pool set ğ’Ÿ0subscriptğ’Ÿ0\\mathcal{D}_{0}caligraphic_D start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT. ğ’Ÿâˆ’1subscriptğ’Ÿ1\\mathcal{D}_{-1}caligraphic_D start_POSTSUBSCRIPT - 1 end_POSTSUBSCRIPT is seen by the model only once and not retained as part of the growing data set as AL proceeds.\n\n4 Experiments\n\nWe perform two types of experiment. For the first, we verify the effectiveness of AL for an individual fingerspelling corpus. For the second, we examine if AL can be made more sample efficient by pre-training on a linguistically related corpus. The following settings apply to all experiments. We construct the initial set ğ’Ÿ0subscriptğ’Ÿ0\\mathcal{D}_{0}caligraphic_D start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT by choosing two samples per class uniformly at random (N=48ğ‘48N=48italic_N = 48). For constructing test sets, we hold-out 30%percent3030\\%30 % of the data in CSL (because of its small size) and 10% for the other corpora. The remaining data is the pool set. The query size for each data set is different due their different sizes: 50505050 for ISL and GSL, 10101010 for ASL, and 5555 for CSL. The classifier is retrained from a random initialization whenever new labels are procured. The classifier is trained for 50505050 epochs using a batch size of 128128128128 with learning rate of 10âˆ’3superscript10310^{-3}10 start_POSTSUPERSCRIPT - 3 end_POSTSUPERSCRIPT.\n\n4.1 Single Corpus Active Learning\n\nImplementation Details\n\nWe compare against uniformly random acquisition as the baseline. We re-run experiments with three unique random seeds to account for sampling variation. The classifier is a neural network with two convolutional layers (ReLU activations and dropout with p=25%ğ‘percent25p=25\\%italic_p = 25 %) and two fully-connected layers (ReLU activations and dropout with p=50%ğ‘percent50p=50\\%italic_p = 50 %).\n\nResults\n\nFigure 2 reports the results, showing test set accuracy (y-axis) as more data is collected (x-axis). We see a clear improvement over random sampling in all cases. Moreover, AL reaches near full-data-set performance by seeing just 15%percent1515\\%15 % (or less) of the original data in three out of four cases (ASL, GSL, and ISL). GSL sees the least benefit from AL, but surprisingly, we see that with just 12%percent1212\\%12 % of the training data, the model starts to surpass full-data-set performance. We suspect that this is due to outliers (created by frame extraction and/or cropping mentioned in section 2) that the AL method has not yet seen but detract from performance.\n\n4.2 Transfer Active Learning\n\nImplementation Details\n\nWe next turn to the pre-training experiment. We use a ResNet-18 (He et al., 2016) as the backbone with the same structure as the previous experiment. We pre-train the classifier on the full data set from one corpus and then load the backbone of the pre-trained model with a re-initialized classification head. The entire structure of the neural network is retrained during AL. We re-run all experiments five times with different random seeds. As a control, we also perform a run with the model pre-trained on FashionMNIST (Xiao et al., 2017), another grayscale data set.\n\nResults\n\nFigure 3 reports test accuracy for all pre-training configurations, including no pre-training. The only noticeable trend that generalizes to all corpora is that pre-training on FashionMNIST degrades performance in comparison to pre-training on a fingerspelling corpus and using no pre-training. While this observation supports our motivating hypothesis for transfer AL, otherwise, we see modest gains with pre-training. The only clear improvement over no pre-training is demonstrated for ISL pre-trained on GSL (subfigure d, blue line). As these are the only two data sets that include the signerâ€™s forearm, we suspect that visual similarly is causing the benefit rather than linguistic relationship, or else we would have seen improvements in other data pairings. Yet, curiously, this benefit is not symmetric since pre-training with ISL did not improve GSL performance more than pre-training with other corpora. Figure 4 shows the accuracy per class (letter). To make the performance gap more obvious, we show performance at the time step with the largest gap (t=40ğ‘¡40t=40italic_t = 40). We expect to see the pink and blue lines (pre-training) to be higher than the green line (no pre-training) for letters shown in black (shared across languages). This is generally the case, but we also see improvements for non-shared letters, such as for â€˜Xâ€™ in ISL.\n\n5 Conclusion\n\nWe have reported the first transfer AL results for fingerspelling corpora. We observed a clear success (pretraining on GSL and performing AL on ISL), but this was not observed for other pairings nor is the ISL-GSL relationship symmetric. In future work, we plan to more carefully control for linguistic and visual relationships between data sets to better isolate the cause of the performance gains.\n\nReferences\n\nBrown et al. (2020) Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are few-shot learners. Advances in Neural Information Processing Systems, 2020.\n\nChen et al. (2018) Liang-Chieh Chen, Yukun Zhu, George Papandreou, Florian Schroff, and Hartwig Adam. Encoder-decoder with atrous separable convolution for semantic image segmentation. In European Conference on Computer Vision, 2018.\n\nCohn et al. (1996) David A. Cohn, Zoubin Ghahramani, and Michael I. Jordan. Active learning with statistical models. Journal of Artificial Intelligence Research, 1996.\n\nDesai et al. (2024) Aashaka Desai, Maartje De Meulder, Julie A. Hochgesang, Annemarie Kocab, and Alex X. Lu. Systemic biases in sign language AI research: A deaf-led call to reevaluate research agendas. In Proceedings of the LREC-COLING 2024 11th Workshop on the Representation and Processing of Sign Languages: Evaluation of Sign Language Resources, 2024.\n\nDevlin et al. (2019) Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: Pre-training of deep bidirectional transformers for language understanding. In Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, 2019.\n\nDinh et al. (2006) Thang B. Dinh, Van B. Dang, Duc A. Duong, Tuan T. Nguyen, and Duy-Dinh Le. Hand gesture classification using boosted cascade of classifiers. In International Conference onResearch, Innovation and Vision for the Future, 2006.\n\nDreuw et al. (2006) Philippe Dreuw, Thomas Deselaers, Daniel Keysers, and Hermann Ney. Modeling image variability in appearance-based gesture recognition. In ECCV Workshop on Statistical Methods in Multi-Image and Video Processing, 2006.\n\nElakkiya (2021) R. Elakkiya. Machine learning based sign language recognition: a review and its research frontier. Journal of Ambient Intelligence and Humanized Computing, 2021.\n\nElakkiya and Selvamani (2015) R. Elakkiya and K. Selvamani. An active learning framework for human hand sign gestures and handling movement epenthesis using enhanced level building approach. Procedia Computer Science, 2015.\n\nFreeman (1965) Linton G. Freeman. Elementary applied statistics for students in behavioral science. In Social Forces, 1965.\n\nGal et al. (2017) Yarin Gal, Riashat Islam, and Zoubin Ghahramani. Deep Bayesian active learning with image data. In Proceedings of the International Conference on Machine Learning, 2017.\n\nHao et al. (2021) Ruqian Hao, Khashayar Namdar, Lin Liu, and Farzad Khalvati. A transfer learningâ€“based active learning framework for brain tumor classification. Frontiers in Artificial Intelligence, 2021.\n\nHe et al. (2016) Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, 2016.\n\nHoulsby et al. (2011) Neil Houlsby, Ferenc Huszar, Zoubin Ghahramani, and MÃ¡tÃ© Lengyel. Bayesian active learning for classification and preference learning. arXiv, 2011.\n\nJiang et al. (2020) Xianwei Jiang, Bo Hu, Suresh Chandra Satapathy, Shuihua Wang, and Yudong Zhang. Fingerspelling identification for Chinese sign language via AlexNet-based transfer learning and adam optimizer. Scientific Programming, 2020.\n\nMercanoglu and Keles (2020) Ozge Mercanoglu and Hacer Keles. Autsl: A large scale multi-modal Turkish sign language dataset and baseline methods. IEEE Access, 2020.\n\nNatarajan et al. (2018) Kathiravan Natarajan, Truong-Huy D Nguyen, and Mutlu Mete. Hand gesture controlled drones: An open source library. In International Conference on Data Intelligence and Security, 2018.\n\nOliveira et al. (2017) Marlon Oliveira, Houssem Chatbri, Ylva Ferstl, Mohamed Farouk, Suzanne Little, Noel Oâ€™Connor, and A. Sutherland. A dataset for Irish sign language recognition. In Proceedings of the Irish Machine Vision and Image Processing Conference, 2017.\n\nSettles (2012) Burr Settles. Active learning. Synthesis Lectures on Artificial Intelligence and Machine Learning, 2012.\n\nShannon (1948) Claude Elwood Shannon. A mathematical theory of communication. The Bell System Technical Journal, 1948.\n\nShelmanov et al. (2019) Artem Shelmanov, Vadim Liventsev, Danil Kireev, Nikita Khromov, Alexander Panchenko, Irina Fedulova, and Dmitry V. Dylov. Active learning with deep pre-trained models for sequence tagging of clinical and biomedical texts. In IEEE International Conference on Bioinformatics and Biomedicine, 2019.\n\nSilanon and Suvonvorn (2014) Kittasil Silanon and Nikom Suvonvorn. Finger-spelling recognition system using fuzzy finger shape and hand appearance features. In International Conference on Digital Information and Communication Technology and its Applications, 2014.\n\nTamkin et al. (2022) Alex Tamkin, Dat Nguyen, Salil Deshpande, Jesse Mu, and Noah Goodman. Active learning helps pretrained models learn the intended task. arXiv, 2022.\n\nXiao et al. (2017) Han Xiao, Kashif Rasul, and Roland Vollgraf. Fashion-mnist: a novel image dataset for benchmarking machine learning algorithms. arXiv, 2017.\n\nYin et al. (2021) Kayo Yin, Amit Moryossef, Julie Hochgesang, Yoav Goldberg, and Malihe Alikhani. Including signed languages in natural language processing. In Proceedings of the Annual Meeting of the Association for Computational Linguistics, 2021.\n\nA Background\n\nA.1 Sign language Recognition\n\nHand gesture recognition is essential for applications such as human-computer interaction, robot control, and sign language assistance systems (Natarajan et al., 2018; Silanon and Suvonvorn, 2014; Elakkiya, 2021). The purpose of sign language alphabet recognition is to detect labels from isolated or continuous Signs (Yin et al., 2021). Recognition of signs using computational models is a challenging problem because of a number of reasons. In some cases, one gesture can look very different from a different angle. Another challenge is the variations of how a sign is performed by different signers (Mercanoglu and Keles, 2020). Dinh et al. (2006) built ASL alphabet classifiers by a combination of 24 (only static) sign detectors with AdaBoost, then using threshold information given by each detector to suggest which gesture is the best match. Elakkiya and Selvamani (2015) used active learning to utilize the availability of the current training classifier for increasingly improving performance. They do not use certain acquisition functions to measure uncertainty but directly choose the sample which is not correctly classified by the current classifier.\n\nA.2 Active learning and Transfer active learning\n\nActive learning is a machine learning method in which a learning system can query a human expert interactively to label data with the desired output(Cohn et al., 1996). The main goal of active learning is that active learner performance is better than traditional supervised learning with the same labeled data. Therefore active learners can interactively pose queries during the training stage and are part of the human-in-the-loop paradigm. Transfer Learningâ€“Based Active Learning is a method to take advantage transfer learning for better active learning. Some papers tried to prove that pre-trained models are better active learners. (Hao et al., 2021) (Shelmanov et al., 2019)\n\nA.3 Experiment for higher resolution of data\n\nTo ensure resolution does not limit the knowledge that the pre-trained model can transfer, we resize data in all four corpora to 96Ã—96 and redo the same experiments. Because the ASL dataset used for the 28*28 experiment has a resolution of exactly 28*28, it cannot be used for resolution 96*96. We use a new ASL dataset from Kaggle with an original resolution of 400*400 for this experiment. Because the computation time grows rapidly with the resolution, we made some adjustments to save time. Firstly, we only repeated all experiments three times for transfer active learning. Secondly, because the Irish dataset is augmented with rotation, we sample 30%percent3030\\%30 % of the Irish dataset and use it for experiments. Figure 5 reports test accuracy for all pre-training configurations, including no pre-training and double-hand fingerspelling(Indian sign language). Similar to low-resolution results, the most obvious improvement is pre-train on GSL and fine-tuning on ISL. Figure 6 shows the accuracy per class(gap). Interestingly, we also observe improvements for non-sharing alphabets, e.g. â€™Dâ€™, â€™Qâ€™, â€™Râ€™ in ASL and â€™Pâ€™, â€™Qâ€™, â€™Xâ€™ in ISL. This shows that pre-training and fine-tuning can also help the model classify some non-shared alphabets."
    }
}
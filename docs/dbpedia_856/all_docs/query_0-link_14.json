{
    "id": "dbpedia_856_0",
    "rank": 14,
    "data": {
        "url": "https://arxiv.org/html/2403.07969v2",
        "read_more_link": "",
        "language": "en",
        "title": "KnowCoder: Coding Structured Knowledge into LLMs for Universal Information Extraction",
        "top_image": "",
        "meta_img": "",
        "images": [
            "https://arxiv.org/html/extracted/5469547/pic/logo.png",
            "https://arxiv.org/html/extracted/5469547/pic/logo.png",
            "https://arxiv.org/html/x1.png",
            "https://arxiv.org/html/x2.png",
            "https://arxiv.org/html/extracted/5469547/pic/logo.png",
            "https://arxiv.org/html/extracted/5469547/pic/logo.png",
            "https://arxiv.org/html/extracted/5469547/pic/logo.png",
            "https://arxiv.org/html/extracted/5469547/pic/logo.png",
            "https://arxiv.org/html/extracted/5469547/pic/schema_import_fig.png",
            "https://arxiv.org/html/extracted/5469547/pic/schema_ali_fig.png",
            "https://arxiv.org/html/extracted/5469547/pic/logo.png",
            "https://arxiv.org/html/extracted/5469547/pic/data_stat_pie.png"
        ],
        "movies": [],
        "keywords": [],
        "meta_keywords": [
            ""
        ],
        "tags": null,
        "authors": [],
        "publish_date": null,
        "summary": "",
        "meta_description": "",
        "meta_lang": "en",
        "meta_favicon": "",
        "meta_site_name": "",
        "canonical_link": null,
        "text": "HTML conversions sometimes display errors due to content that did not convert correctly from the source. This paper uses the following packages that are not yet supported by the HTML conversion tool. Feedback on these issues are not necessary; they are known and are being worked on.\n\nfailed: scalerel\n\nfailed: inconsolata\n\nAuthors: achieve the best HTML results from your LaTeX submissions by following these best practices.\n\nLicense: arXiv.org perpetual non-exclusive license\n\narXiv:2403.07969v2 [cs.LG] 14 Mar 2024\n\nKnowCoder: Coding Structured Knowledge into LLMs for\n\nUniversal Information Extraction\n\nZixuan Li , Yutao Zeng , Yuxin Zuo , Weicheng Ren ,\n\nWenxuan Liu , Miao Su , Yucan Guo , Yantao Liu , Xiang Li , Zhilei Hu , Long Bai ,\n\nWei Li , Yidan Liu , Pan Yang, Xiaolong Jin , Jiafeng Guo , Xueqi Cheng\n\nCAS Key Laboratory of Network Data Science and Technology,\n\nInstitute of Computing Technology, Chinese Academy of Sciences\n\n{lizixuan, jinxiaolong, guojiafeng}@ict.ac.cn\n\nhttps://ict-goknow.github.io/knowcoder/\n\nCo-first authors. Corresponding authors.\n\nAbstract\n\nIn this paper, we propose KnowCoder, a Large Language Model (LLM) to conduct Universal Information Extraction (UIE) via code generation. KnowCoder aims to develop a kind of unified schema representation that LLMs can easily understand and an effective learning framework that encourages LLMs to follow schemas and extract structured knowledge accurately. To achieve these, KnowCoder introduces a code-style schema representation method to uniformly transform different schemas into Python classes, with which complex schema information, such as constraints among tasks in UIE, can be captured in an LLM-friendly manner. We further construct a code-style schema library covering over 30,000 types of knowledge, which is the largest one for UIE, to the best of our knowledge. To ease the learning process of LLMs, KnowCoder contains a two-phase learning framework that enhances its schema understanding ability via code pretraining and its schema following ability via instruction tuning. After code pretraining on around 1.51.51.51.5B automatically constructed data, KnowCoder already attains remarkable generalization ability and achieves relative improvements by 49.8% F1, compared to LLaMA2, under the few-shot setting. After instruction tuning, KnowCoder further exhibits strong generalization ability on unseen schemas and achieves up to 12.5% and 21.9%, compared to sota baselines, under the zero-shot setting and the low resource setting, respectively. Additionally, based on our unified schema representations, various human-annotated datasets can simultaneously be utilized to refine KnowCoder, which achieves significant improvements up to 7.5% under the supervised setting.\n\nKnowCoder: Coding Structured Knowledge into LLMs for\n\nUniversal Information Extraction\n\nZixuan Li††thanks: Co-first authors.††thanks: Corresponding authors., Yutao Zeng , Yuxin Zuo , Weicheng Ren , Wenxuan Liu , Miao Su , Yucan Guo , Yantao Liu , Xiang Li , Zhilei Hu , Long Bai , Wei Li , Yidan Liu , Pan Yang, Xiaolong Jin , Jiafeng Guo , Xueqi Cheng CAS Key Laboratory of Network Data Science and Technology, Institute of Computing Technology, Chinese Academy of Sciences {lizixuan, jinxiaolong, guojiafeng}@ict.ac.cn https://ict-goknow.github.io/knowcoder/\n\n1 Introduction\n\nInformation Extraction (IE) aims to extract explicit and structured knowledge following the manually designed schemas. The IE schemas define high-level types of knowledge (i.e., concepts) and structures among them Hogan et al. (2021), which include various types of entities, relations, and events. To simultaneously extract various knowledge under different schemas via a single model, the Universal Information Extraction (UIE) task is proposed Lin et al. (2020a). Recently, Large Language Models (LLMs) have demonstrated general understanding abilities through large-scale pretraining, which drives their increasing utilization in UIE. However, their performance on UIE is still limited because of two main challenges: (1) the lack of a unified schema representation method that LLMs can easily understand; (2) the lack of an effective learning framework that encourages LLMs to accurately follow specific schemas for extracting structured knowledge.\n\nFor the first challenge, the existing UIE models first represent different schemas in a universal way, such as classification labels Lin et al. (2020a), keywords Gui et al. (2023), or a specifically-designed formal language Lu et al. (2022). These schema representation methods have three main restrictions: (1) ignoring information like taxonomies (e.g., “fairytale” is a subclass of “written work”) and constraints among concepts (e.g., “spouse” relation exists between two “human” entities); (2) classification labels or a specifically designed formal language is hard for LLMs to understand and follow; (3) designed for specific IE datasets and lacking a general schema library.\n\nTo solve these restrictions, in this paper, we propose a kind of code-style schema representation method, with which various types of knowledge are generally defined as Python classes. As shown in Figure 1, the class inheritance mechanism is adopted to describe the concept taxonomies. A mechanism of type hint is employed to model constraints among different concepts. The class comments are used to provide clear definitions of concepts. And, the class methods are used to post-process the results according to specific IE guidelines. Upon this method, we construct a comprehensive code-style schema library covering over 29,0002900029,00029 , 000 entity types, 900900900900 relation types, and 500500500500 event types based on Wikidata, the largest one for UIE, to the best of our knowledge, currently reported in the open literature.\n\nFor the second challenge, the existing learning framework for UIE directly conducts instruction tuning on LLMs to extract knowledge following specific and limited schemas Sainz et al. (2023); Wang et al. (2023b). The enormous concepts in the constructed schema library challenge the existing training framework. To help LLMs better understand and follow these schemas, we propose an effective two-phase framework containing a schema understanding phase and a schema following phase. The former improves the ability of LLMs to understand different concepts in schemas via large-scale code pretraining on the schema definition code and corresponding instance code. The latter advances their abilities to follow specific schemas in an IE task via instruction tuning. After code pretraining on around 1.5B automatically constructed data, KnowCoder already attains remarkable generalization ability and achieves NER improvements compared to the base model, LLaMA2, by 49.8% relative F1 point under the few-shot setting on NER. After instruction tuning on 1.5B automatically annotated data, KnowCoder experimentally demonstrates strong generalization ability on unseen schemas. Under the zero-shot setting, KnowCoder achieves average relative improvements up to 12.5% on the NER task. Under the low-resource setting, KnowCoder gets average relative improvements up to 21.9% on all the IE tasks. Additionally, based on our unified schema representation, various IE datasets can be simultaneously utilized to refine KnowCoder. After refinement, KnowCoder achieves consistent improvements across all IE tasks under the supervised setting, getting up to 7.5% improvement on the relation extraction task, respectively.\n\nIn general, the main contributions of this paper include:\n\n•\n\nWe propose a code-style schema representation method to uniformly represent different schemas for UIE. Using this method, we construct a large code-style schema library covering more than 30,0003000030,00030 , 000 types of knowledge.\n\n•\n\nWe propose an effective learning framework for LLMs in a two-phase manner, which first enhances the schema understanding through code pretraining and then boosts schema following via instruction tuning.\n\n•\n\nAfter training on billions of automatically annotated data and refining with human-annotated IE datasets, KnowCoder demonstrates superior performance on different IE tasks under the zero-shot, low-resource, and supervised settings.\n\n•\n\nThe constructed schema library, training data, code, and models are released for future research.\n\n2 KnowCoder Schema\n\nThe proposed schema representation method uses code, a language that LLMs easy to understand, to define schemas. Specifically, KnowCoder schema adopts a series of programming language features to comprehensively model schema information, including the concept taxonomies, the constraints among different concepts, the definition of concepts, and other extraction requirements. Besides, considering that previous schema representation methods are only designed for specific datasets and contain limited types of knowledge, we further construct a large-scale schema corpus containing a wide range of knowledge.\n\n2.1 Code-style Schema Representation Method\n\nThe code-style schema representation method comprises three basic classes, namely, ‘‘Entity’’, ‘‘Relation’’, and ‘‘Event’’. Based on the three basic classes, we represent all the concepts in the schemas by the corresponding classes. Then, the instances of each concept can be represented by the objects of the corresponding class. In the following, we will introduce four features of the proposed representation method.\n\nClass Inheritance.\n\nWe adopt the class inheritance mechanism to account for the taxonomies in the schemas. Specifically, we let class A inherit all the class members from class B if the corresponding concept A is the hyponym of concept B in the taxonomies. For a concept with multiple hypernyms, the hypernym concept with the most instances is selected. The class of an unseen concept can inherit from an existing class or directly from the basic class.\n\nClass comment.\n\nSimilar to Sainz et al. (2023), we adopt class comments to provide clear definitions of concepts. As shown in Figure 1, a class comment includes a natural language description that explains the corresponding concept and the examples of instances corresponding to that type. When there is an unseen concept, we use the description in its annotation guidelines and manually give out a few examples.\n\nType Hint.\n\nType hint is a formal solution to indicate the type of a value in the code. We adopt type hints in the initialization function of a class to define its constraints with other classes strictly. Thus, the constraints among the concepts in the schemas are modeled. As shown in Figure 1, taking the relation “PlaceOfBirth” for example, ‘‘def __init__(self, head_entity: Human, tail_entity: SpatialEntity)’’ denotes that the head entity must be a ‘‘Human’’ and the tail entity must be a ‘‘SpatialEntity’’.\n\nClass Method.\n\nA class method is bound to the class and not the object of the class. They are utilized to post-process the extracted instance results of a class. For example, some IE tasks may not consider the pronouns “he” and “she” as instances of the ‘‘Human’’ concept. To address this, a class method can be added to the ‘‘Human’’ class to filter out such pronouns from the extraction results, ensuring that the output aligns with the task’s unique criteria. Note that, class methods are manually designed for specific IE tasks based on their task constraints. We take a few IE datasets to demonstrate the effectiveness of class methods in our experiments, as shown in the Appendix C.\n\n2.2 Schema Library Construction\n\nWe construct the code-style schema library based on Wikidata . We select the concepts included in the existing IE datasets created from Wikidata, i.e., KELM Agarwal et al. (2021), UniversalNER Zhou et al. (2023), InstructIE Zhang et al. (2023), and LSEE Chen et al. (2017). We derive the constraints among concepts according to their co-occurrences. To construct the taxonomies, we extract the ‘‘SubclassOf’’ relations among these concepts from Wikidata. To obtain the description of a concept, we use its definition from Wikidata directly or generate its descriptions using GPT-4 if its definition in Wikidata is missing. Finally, the constructed schema library encompasses over 29,1772917729,17729 , 177 entity types, 876876876876 relation types, and 519519519519 event types. The detailed statistics of the schema are in Appendix H.\n\n3 Learning Framework of KnowCoder\n\nTo discriminate enormous concepts defined in schemas, we first let KnowCoder understand each concept through its definition and instances. Subsequently, we enhance KnowCoder to discriminate among a few concepts and extract corresponding knowledge. Thus, as shown in Figure 2, the proposed learning framework contains two phases, i.e., the schema understanding phase and the schema following phase. In the schema understanding phase, KnowCoder undergoes code pretraining to understand each concept in two manners: 1) Go through the class definition code of each concept. 2) Go through the instance codes of each concept. In the schema following phase, KnowCoder is finetuned using instruction tuning code, where multiple task-demanded concepts are given in the schemas, enhancing KnowCoder’s ability to follow schemas and generate instantiating code accordingly.\n\n3.1 Schema Understanding Phase\n\n3.1.1 Training Data Generation\n\nTo enhance KnowCoder’s schema understanding abilities, we construct a large-scale training dataset based on the schema library. As shown in the left part of Figure 2, the training data consists of two kinds of codes, i.e., schema definition codes and instance codes. The schema definition codes are generated based on the schema library, where we randomly sample a certain number of concepts (decided by the maximum sequence length) from the schema library to consist of a training sample. As the aim of the schema understanding phase is to understand each concept but not to discriminate various concepts, the instance code corresponding to a single concept contains three parts, i.e., a sentence containing instances of the given concept, an import clause to introduce the corresponding class of the given concept, and an instantiating clause to give out all the instances of the given concept in the sentence. The schema-instance codes are constructed based on KELM corpus Agarwal et al. (2021), which contains 15,628,4861562848615,628,48615 , 628 , 486 synthetic sentences to describe the structured knowledge from Wikidata. We do data cleaning for the corpus. The cleaning details are in Appendix G.\n\n3.1.2 Code Pretraining\n\nAfter obtaining the data, we apply regular code pretraining to make LLM understand the diverse concepts in the schemas. Given a training sample with length of L𝐿Litalic_L, X=x0,x1,…,xi,…,XL−1𝑋subscript𝑥0subscript𝑥1…subscript𝑥𝑖…subscript𝑋𝐿1X={x_{0},x_{1},...,x_{i},...,X_{L-1}}italic_X = italic_x start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT , italic_x start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , … , italic_x start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT , … , italic_X start_POSTSUBSCRIPT italic_L - 1 end_POSTSUBSCRIPT, the model attempts to predict every token xlsubscript𝑥𝑙x_{l}italic_x start_POSTSUBSCRIPT italic_l end_POSTSUBSCRIPT based on the x0,…,xl−1subscript𝑥0…subscript𝑥𝑙1{x_{0},...,x_{l-1}}italic_x start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT , … , italic_x start_POSTSUBSCRIPT italic_l - 1 end_POSTSUBSCRIPT, where l=0,…,L−1𝑙0…𝐿1l={0,...,L-1}italic_l = 0 , … , italic_L - 1. Some training details are as follows:\n\nSchema Importing.\n\nThe straightforward way to construct a pretraining sample is to directly give the whole schema definition for the corresponding instance code. However, this manner may cause the model to overfit the schema definition code because they are frequently repeated in every instance code. To address this problem, we separate the schema definition code from the instance code and use the “import” clause to introduce the corresponding schema definition to the instance code.\n\nThe position of the “import” clause is also critical for the LLMs to learn. We study two positions for the “import” clause, i.e., “Import-First” and “Sentence-First”. We adopt “Sentence-First” in the learning framework because it performs better than the others. The comparison results are in Appendix A.\n\n3.2 Schema Following Phase\n\n3.2.1 Training Data Generation\n\nTo enhance the schema following abilities of KnowCoder, we construct instruction tuning training data for UIE tasks. As shown in the middle part of Figure 2, a typical instruction tuning sample contains three parts of codes, i.e., instruction code T𝑇Titalic_T, input code I𝐼Iitalic_I, and output code O𝑂Oitalic_O.\n\nThe instruction code T𝑇Titalic_T comprises two snippets, i.e., schema definition and task description. The schema definition snippet includes definitions of some concepts selected from the former phase, which defines specific concepts to be extracted. The task description snippet includes a comment that contains a natural language description of an IE task. For example, the task description of Relation Extraction (RE) is “This is an object-oriented programming task: some Classes are defined above. Please instantiate all the corresponding Objects in the following sentence.”. The input I𝐼Iitalic_I contains the sentence to be extracted, which is denoted as a variable “sentence”, i.e., “sentence = …”. The output O𝑂Oitalic_O contains all the golden knowledge in the sentence, denoted as a list variable “results”, i.e., “results = […]”. We have conducted a performance comparison of different versions of the instructions, and the corresponding results are in Appendix D.\n\nWe construct the training corpus from three data sources. For Named Entity Extraction (NER), ChatGPT-annotated Pile corpus Zhou et al. (2023) is selected. For Relation Extraction (RE) and Event Extraction (EE), we adopt the data sources constructed in Gui et al. (2023) and LSEE Chen et al. (2017), respectively.\n\n3.2.2 Instruction Tuning\n\nThe objective of instruction tuning is to learn an LLM 𝐟:(I×T)→O:𝐟→𝐼𝑇𝑂\\mathbf{f}:(I\\times T)\\rightarrow Obold_f : ( italic_I × italic_T ) → italic_O. The LLM takes input code I𝐼Iitalic_I, and instruction code T𝑇Titalic_T as input. Subsequently, the LLM is tuned to generate every token in the output O𝑂Oitalic_O. Some training details are as follows:\n\nNegative Class Sampling.\n\nIn the constructed schema library, there are more than 30000300003000030000 concepts. It is challenging for the model to accommodate all the corresponding class definitions in a single prompt. Consequently, KnowCoder employs a negative class sampling strategy. For each training sample, in addition to the classes annotated in the sentence, we randomly sample several classes (20%percent2020\\%20 % number of the golden classes) from the remaining classes.\n\nFully negative Sample Construction.\n\nIn real-world scenarios, many sentences do not contain any knowledge of a specific IE task, called fully negative samples in this paper. However, the selected data sources neglect such samples. To address this problem, we randomly sample 5%percent55\\%5 % sentences from the data sources. For each sentence, we replace the golden classes with five random negative classes.\n\n3.3 Refinement\n\nAfter schema understanding and following, we obtain KnowCoder, an LLM that demonstrates strong generalization ability on unseen schemas. Additionally, based on our unified schema representation, KnowCoder can be further refined by various human-annotated datasets simultaneously. In this phase, we conduct instruction tuning based on the datasets used in previous work Wang et al. (2023b); Sainz et al. (2023).\n\nIn different IE datasets, concepts with the same name may follow different annotation guidelines. Take ‘‘PERSON’’ for example, in MultiNERD Tedeschi and Navigli (2022), entities do not include the pronouns, e.g., “he” and “she”, while ACE05 Walker and Consortium (2005) consider personal pronouns as ‘‘PERSON’’. To alleviate the problem, we add specific dataset information in the instructions to distinguish annotation guidelines for different datasets. For example, the instruction for the ACE05 dataset is “… Please instantiate all the corresponding Event Objects in the following sentence from DATASET ACE05.”\n\n4 Experiment Setup\n\nDatasets.\n\nWe conducted experiments using 33333333 specific domain Information Extraction (IE) datasets, including 23232323 datasets for Named Entity Extraction (NER), 8888 datasets for Relation Extraction (RE), 2222 datasets for Event Detection (ED) and Event Argument Extraction (EAE). The detailed statistics of these datasets are in Appendix H. Among these NER datasets, following Wang et al. (2023b); Zhou et al. (2023), we take 7777 datasets as the zero-shot benchmark, including 5555 datasets of different domains from CrossNER Liu et al. (2020), MIT-Movie Liu et al. (2019) and MIT-Restaurant Liu et al. (2019). For RE, we adopt GIDS Jat et al. (2018) as the zero-shot dataset. Following Sainz et al. (2023), we adopt CASIE Lu et al. (2021) as the zero-shot ED dataset.\n\nTo balance the evaluation coverage and costs, we introduce the KnowCoder benchmark, a composite derived from existing NER, RE, and EE datasets. Under the supervised setting, a sampling strategy was developed for NER and RE tasks to maintain the distributions of original datasets and ensure the broad coverage of knowledge types. Details on the proposed strategy and comprehensive benchmark information are available in Appendix F.\n\nMetrics.\n\nWe report the span-based offset Micro-F1 following previous methods Lu et al. (2022); Lin et al. (2020b). For NER, an entity is considered correct if the entity boundary and type are correctly predicted. For RE, a relation is considered correct if its triplet matches a golden annotation, including relation type, subject entity, and object entity. For ED, an event trigger is correct if its event type and trigger match a golden annotation. For the EAE task, given an event type, an argument is correct if the argument and its role type match a golden annotation.\n\n4.1 Implementation Details\n\nKnowCoder is finetuned based on LLaMA2-base-7B Touvron et al. (2023). We utilize the Megatron-LM framework Shoeybi et al. (2019) for schema understanding. We set the context length to 2048204820482048, the learning rate to 5×10−65superscript1065\\times 10^{-6}5 × 10 start_POSTSUPERSCRIPT - 6 end_POSTSUPERSCRIPT, the global batch size to 1111M tokens, and the maximum training step to 4500450045004500. For the schema following and refinement phases, we use LoRA Hu et al. (2021) for parameter-efficient fine-tuning. We set the lora rank and lora alpha parameters to 32323232 and 64646464, respectively. The warmup ratio is set to 0.030.030.030.03 and the dropout ratio is set to 0.10.10.10.1. The learning rates for these two phases are set to 3×10−43superscript1043\\times 10^{-4}3 × 10 start_POSTSUPERSCRIPT - 4 end_POSTSUPERSCRIPT. We limit the sequence length to 4096409640964096 and set the batch size to 256256256256. Detailed information about the training process is available in Appendix J. During the inference phase, we use greedy search and set the temperature to 00. The maximum output length is set to 640640640640.\n\n5 Results and Analyses\n\n5.1 Few-shot Evaluation After Schema Understanding\n\nConsidering that a pre-trained LLM cannot give proper results without given examples, we study the generalization ability of KnowCoder after the schema understanding phase, denoted as KnowCoder (SU. only), under the few-shot setting. Specifically, We utilize the first five samples from the training data as examples and report the NER F1 score in Table 1 across seven zero-shot NER datasets. The results demonstrate that KnowCoder (SU. only) outperforms LLaMA2-7B with an average relative improvement of 49.8%. Remarkably, KnowCoder (SU. only) gets an average F1 score of 46.3%percent46.346.3\\%46.3 % with only a few examples, which are comparable to InstructUIE refined using human-annotated datasets. These results strongly support the effectiveness of the schema understanding phase in enhancing model generalization and performance in NER tasks.\n\n○\n\n5.2 Zero-Shot Evaluation After Schema Following\n\nTo verify the generalization ability of KnowCoder, we conduct zero-shot experiments on 9 datasets across NER, RE, and ED tasks. In this setting, we employ KnowCoder after schema understanding and following to conduct extraction. KnowCoder is compared with two kinds of baselines. One is the LLM-based IE method that refined on human-annotated data, including InstructUIE Wang et al. (2023b), GoLLIE Sainz et al. (2023), and UniNER Zhou et al. (2023). The other is models without refinement, including Vicuna Chiang et al. (2023), ChatGPT, UniNER Zhou et al. (2023). The results of these three baselines are from Zhou et al. (2023). Note that KnowCoder is unfair when compared with methods after refinement.\n\nMain Results.\n\nThe results of zero-shot NER are in Table 2. It can be seen that KnowCoder surpasses baselines without refinement across four NER datasets, registering a relative performance enhancement of 12.5%. This improvement is attributed to KnowCoder’s training on a large-scale, automatically generated dataset within a two-phase learning framework, which enhances its generalization capabilities for NER, even surpassing methods refined with human-annotated data. The results of zero-shot RE and ED are in Table 3. For ED, KnowCoder’s performance is inferior to GoLLIE, a baseline model trained on high-quality, human-annotated data. This emphasizes that human-annotated datasets can enhance performance for more difficult IE tasks, such as ED. To further substantiate the point, we further refine KnowCoder with the ACE05 dataset, the same EE training data employed by GoLLIE. This refinement significantly improves zero-shot F1 performance to 72.0%percent72.072.0\\%72.0 % on the CASIE dataset. This represents a significant advancement over GoLLIE’s performance of 59.3%percent59.359.3\\%59.3 %, marking a relative improvement of 21.4%.\n\n5.3 Low Resource Evaluation After Schema Following\n\nTo further investigate the generalization ability of KnowCoder for IE tasks, we conduct low-resource experiments by fine-tuning KnowCoder with three different partitions of the original training sets (1/5/10% ratio) across four tasks. Following Lu et al. (2022), we adopt CoNLL03, CoNLL04, ACE05E⁢D𝐸𝐷{}_{ED}start_FLOATSUBSCRIPT italic_E italic_D end_FLOATSUBSCRIPT and ACE05E⁢A⁢E𝐸𝐴𝐸{}_{EAE}start_FLOATSUBSCRIPT italic_E italic_A italic_E end_FLOATSUBSCRIPT as the benchmarks for NER, RE, ED, and EAE tasks. LLaMA2 denotes directly fine-tuning LLaMA2 with these partial training data. The results are in Table 4. It can be shown that KnowCoder gets the highest average F1 scores across all IE tasks in low-resource settings at varying ratios. In ratio 1%percent11\\%1 %, KnowCoder gets the relative average improvement of 21.9% compared to UIE, which shows that KnowCoder has strong adaptability to downstream IE tasks after pretraining on large-scale data under the two-phase learning framework.\n\n5.4 Supervised Evaluation After Refinement\n\nUnder the supervised evaluation, KnowCoder is further refined with the IE datasets. We conduct supervised experiments on four IE tasks, including NER, RE, ED, and EAE. KnowCoder is compared with three kinds of methods. The first is the traditional UIE method Lou et al. (2023); Lu et al. (2022), which is based on relatively small language models (i.e., million-level parameters). The latter two are based on LLMs (i.e., ChatGPT, LLaMA2). They adopt the in-context learning Guo et al. (2023); Li et al. (2023); Ashok and Lipton (2023) and supervised fine-tuning paradigms Zhou et al. (2023); Wang et al. (2023b); Sainz et al. (2023), respectively. As some baselines only report results for specific IE tasks, we report the SOTA results of the above methods in each dataset, denoted as “SoTA” in the tables. As highlighted by Zhou et al. (2023), the evaluation script of InstructUIE Wang et al. (2023b) contains issues. Furthermore, the benchmark in Zhou et al. (2023) remains pending release. In the end, we have implemented these two baselines on KnowCoder benchmark using their released models.\n\nMain Results.\n\nThe results for NER, RE, EE (including ED and EAE) tasks are shown in Tables 5, 6 and 7, respectively. We can observe that: (1) KnowCoder outperforms the SOTA baselines on most datasets for NER, RE, ED, and EAE, respectively. Based on the code-style schemas, KnowCoder universally models IE tasks and effectively transfers IE abilities after conducting schema understanding, following, and refinement on large-scale training data. (2) In more challenging UIE tasks, such as RE, KnowCoder demonstrates impressive advancements up to the relative improvement of 8.6% compared to the SOTA baselines. KnowCoder achieves the performances of 73.9% for ED and 66% for EAE. This is the first time LLM-based UIE methods surpass smaller models like UIE in ED and EAE tasks. The code-style schemas and the learning framework enable a more precise definition and understanding of this complex structured knowledge, leading to a significant improvement. (4) UniNER Zhou et al. (2023) achieves comparable results to KnowCoder on NER. Nonetheless, KnowCoder surpasses UniNER in several respects. Primarily, UniNER is limited to extracting one type of entity per iteration, leading to a cost-time complexity. In contrast, KnowCoder can extract multiple entity types in a single iteration, enhancing efficiency. Additionally, UniNER relies on a text-style schema, making it hard to represent and extract relations and events effectively. Conversely, KnowCoder, as a UIE model, offers broader versatility and efficacy comparing to UniNER. (3) KnowCoder gets better results than baselines with code-style prompt Li et al. (2023); Guo et al. (2023); Sainz et al. (2023). This is because KnowCoder provides a more comprehensive schema representation method and conducts two-phase training to understand and follow these schemas.\n\n○ 7B\n\n○ 7B\n\n5.5 Ablation Study\n\nTo show how the schema following and understanding phases contribute to KnowCoder under the zero-shot setting, we further conduct ablation studies removing the schema understanding and following phase, denoted as KnowCoder (w.o. SU) and KnowCoder (w.o. SF), respectively. The results on seven zero-shot NER datasets are shown in Table 8. It can be seen that: (1) KnowCoder gets better results than KnowCoder (w.o. SF) on most NER datasets. It is because the schema understanding phase helps KnowCoder to understand concepts in the schema by training on definition and instance codes and increases its generalization ability. (2) Results of KnowCoder (w.o. SF) decrease extremely, which proves the importance of schema following. Due to the lack of in-context learning ability, a 7B model without instruction tuning is hard to understand instructions under the zero-shot setting, thus making it hard to finish the IE tasks.\n\n6 Related Work\n\n6.1 Universal Information Extraction\n\nUniversal information extraction aims to conduct different IE tasks via a single model. The existing UIE models first represent different schemas for IE tasks in a universal way. OneIE Lin et al. (2020a) represents schemas as classification labels, InstructUIE Wang et al. (2023b) uses keywords Gui et al. (2023); Lou et al. (2023) of concepts to represent schemas, and UIE Lu et al. (2022) uses a specifically-designed formal language to represent schemas. Based on such schema representations, these models adopt language models to understand the schemas and extract the corresponding structured knowledge.\n\n○\n\n6.2 Large Language Models for IE\n\nDue to the strong generation abilities of LLMs, they have been used in IE recently Xu et al. (2023). LLM-based IE methods can be divided into two categories: In-Context Learning (ICL) based methods and Supervised Finetuning (SFT) based methods. The ICL-based IE methods Li et al. (2023); Guo et al. (2023); Ashok and Lipton (2023); Wang et al. (2023a) make predictions only based on contexts augmented with a few examples. The SFT-based methods Wang et al. ; Gui et al. (2023); Wang et al. (2023b); Zhou et al. (2023); Xu et al. (2023); Sainz et al. (2023) use the annotated data to finetune LLMs.\n\nSome existing work uses code-style prompts to conduct IE. Most of them are ICL-based methods. Wang et al. (2022) uses the code-style prompt to conduct event argument extraction. Li et al. (2023) uses the code-style prompt to conduct the named entity extraction and relation extraction. Guo et al. (2023) proposes a reterive-argumented method to conduct the universal IE. These methods show relatively poor performance compared to SFT-based methods because of the lack of training to follow the schemas in the prompt. The most similar work with KnowCoder is GoLLIE, an SFT-based UIE method that gives out definitions of schemas as code comments. The difference between KnowCoder and GoLLIE is that KnowCoder designs a more comprehensive code-style schema representation method, including taxonomies, constraints, and class methods, and further constructs a large-scale schema library. Besides, GoLLIE conducts instruction tuning on human-annotated data, while KnowCoder contains a two-phase learning framework that enhances schema understanding and following ability via automatically annotated data.\n\nConclusion\n\nIn this paper, we introduced KnowCoder for UIE leveraging Large Language Models. KnowCoder is based on a code-style schema representation method and an effective two-phase learning framework. The code-style schema representation method uniformly transforms different schemas into Python classes, with which the UIE task can be converted to a code generation process. Based on the schema representation method, we constructed a comprehensive code-style schema library covering over 30,0003000030,00030 , 000 types of knowledge. To let LLMs understand and follow these schemas, we further proposed a two-phase learning framework that first enhances the schema comprehension ability and then boosts its schema following ability. After training on billions of automatically annotated data and refining with human-annotated IE datasets, KnowCoder demonstrates remarkable performance improvements on different IE tasks under the various evalution settings.\n\nLimitations\n\nThe schemas utilized in our approach are predominantly constructed from Wikidata, which occasionally results in some schemas lacking definitions or other relevant information. This necessitates the generation of additional data to supplement these missing elements. During the pretraining phase, we adopted a combination of automatic generation and distant supervision methods to amass a large corpus. However, this approach inevitably introduces a certain degree of noise. Furthermore, there remains room for improvement in terms of the richness and complexity of the current corpus. Further exploration of pretraining settings could also be beneficial in enhancing the zero-shot capabilities for relation and event-related tasks.\n\nReferences\n\nAgarwal et al. (2021) Oshin Agarwal, Heming Ge, Siamak Shakeri, and Rami Al-Rfou. 2021. Knowledge graph based synthetic corpus generation for knowledge-enhanced language model pre-training. In Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 3554–3565, Online. Association for Computational Linguistics.\n\nAshok and Lipton (2023) Dhananjay Ashok and Zachary C Lipton. 2023. Promptner: Prompting for named entity recognition. arXiv preprint arXiv:2305.15444.\n\nChen et al. (2017) Yubo Chen, Shulin Liu, Xiang Zhang, Kang Liu, and Jun Zhao. 2017. Automatically labeled data generation for large scale event extraction. In Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 409–419, Vancouver, Canada. Association for Computational Linguistics.\n\nChiang et al. (2023) Wei-Lin Chiang, Zhuohan Li, Zi Lin, Ying Sheng, Zhanghao Wu, Hao Zhang, Lianmin Zheng, Siyuan Zhuang, Yonghao Zhuang, Joseph E Gonzalez, et al. 2023. Vicuna: An open-source chatbot impressing gpt-4 with 90%* chatgpt quality. See https://vicuna. lmsys. org (accessed 14 April 2023).\n\nDerczynski et al. (2016) Leon Derczynski, Kalina Bontcheva, and Ian Roberts. 2016. Broad Twitter corpus: A diverse named entity recognition resource. In Proceedings of COLING 2016, the 26th International Conference on Computational Linguistics: Technical Papers, pages 1169–1179, Osaka, Japan. The COLING 2016 Organizing Committee.\n\nDerczynski et al. (2017) Leon Derczynski, Eric Nichols, Marieke Van Erp, and Nut Limsopatham. 2017. Results of the wnut2017 shared task on novel and emerging entity recognition. In Proceedings of the 3rd Workshop on Noisy User-generated Text, pages 140–147.\n\nDogan et al. (2014) Rezarta Islamaj Dogan, Robert Leaman, and Zhiyong Lu. 2014. Ncbi disease corpus: A resource for disease name recognition and concept normalization. Journal of biomedical informatics, 47:1–10.\n\nGuan (2022) Runwei Guan. 2022. Findvehicle and vehiclefinder: A ner dataset for a text-image cross-modal vehicle retrieval system.\n\nGui et al. (2023) Honghao Gui, Jintian Zhang, Hongbin Ye, and Ningyu Zhang. 2023. Instructie: A chinese instruction-based information extraction dataset. arXiv preprint arXiv:2305.11527.\n\nGuo et al. (2023) Yucan Guo, Zixuan Li, Xiaolong Jin, Yantao Liu, Yutao Zeng, Wenxuan Liu, Xiang Li, Pan Yang, Long Bai, Jiafeng Guo, et al. 2023. Retrieval-augmented code generation for universal information extraction. arXiv preprint arXiv:2311.02962.\n\nGurulingappa et al. (2012) Harsha Gurulingappa, Abdul Mateen Rajput, Angus Roberts, Juliane Fluck, Martin Hofmann-Apitius, and Luca Toldo. 2012. Development of a benchmark corpus to support the automatic extraction of drug-related adverse effects from medical case reports. Journal of Biomedical Informatics, 45(5):885–892. Text Mining and Natural Language Processing in Pharmacogenomics.\n\nHendrickx et al. (2010) Iris Hendrickx, Su Nam Kim, Zornitsa Kozareva, Preslav Nakov, Diarmuid Ó Séaghdha, Sebastian Padó, Marco Pennacchiotti, Lorenza Romano, and Stan Szpakowicz. 2010. Semeval-2010 task 8: Multi-way classification of semantic relations between pairs of nominals. In *SEMEVAL.\n\nHogan et al. (2021) Aidan Hogan, Eva Blomqvist, Michael Cochez, Claudia d’Amato, Gerard de Melo, Claudio Gutiérrez, Sabrina Kirrane, José Emilio Labra Gayo, Roberto Navigli, Sebastian Neumaier, Axel-Cyrille Ngonga Ngomo, Axel Polleres, Sabbir M. Rashid, Anisa Rula, Lukas Schmelzeisen, Juan F. Sequeda, Steffen Staab, and Antoine Zimmermann. 2021. Knowledge Graphs. Number 22 in Synthesis Lectures on Data, Semantics, and Knowledge. Springer.\n\nHu et al. (2021) Edward J Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu Chen. 2021. Lora: Low-rank adaptation of large language models. arXiv preprint arXiv:2106.09685.\n\nJat et al. (2018) Sharmistha Jat, Siddhesh Khandelwal, and Partha Pratim Talukdar. 2018. Improving distantly supervised relation extraction using word and entity based attention. ArXiv, abs/1804.06987.\n\nKim et al. (2003) Jin-Dong Kim, Tomoko Ohta, Yuka Tateisi, and Jun’ichi Tsujii. 2003. Genia corpus—a semantically annotated corpus for bio-textmining. Bioinformatics (Oxford, England), 19 Suppl 1:i180–2.\n\nKocaman and Talby (2020) Veysel Kocaman and David Talby. 2020. Biomedical named entity recognition at scale. In ICPR Workshops.\n\nKumar and Starly (2021) Aman Kumar and Binil Starly. 2021. “fabner”: information extraction from manufacturing process science domain literature using named entity recognition. Journal of Intelligent Manufacturing, 33:2393 – 2407.\n\nLi et al. (2016) Jiao Li, Yueping Sun, Robin J. Johnson, Daniela Sciaky, Chih-Hsuan Wei, Robert Leaman, Allan Peter Davis, Carolyn J. Mattingly, Thomas C. Wiegers, and Zhiyong Lu. 2016. Biocreative v cdr task corpus: a resource for chemical disease relation extraction. Database: The Journal of Biological Databases and Curation, 2016.\n\nLi et al. (2023) Peng Li, Tianxiang Sun, Qiong Tang, Hang Yan, Yuanbin Wu, Xuanjing Huang, and Xipeng Qiu. 2023. Codeie: Large code generation models are better few-shot information extractors. In Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), ACL 2023, Toronto, Canada, July 9-14, 2023, pages 15339–15353. Association for Computational Linguistics.\n\nLin et al. (2020a) Ying Lin, Heng Ji, Fei Huang, and Lingfei Wu. 2020a. A joint neural model for information extraction with global features. In Proceedings of the 58th annual meeting of the association for computational linguistics, pages 7999–8009.\n\nLin et al. (2020b) Ying Lin, Heng Ji, Fei Huang, and Lingfei Wu. 2020b. A joint neural model for information extraction with global features. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 7999–8009, Online. Association for Computational Linguistics.\n\nLiu et al. (2019) Yijin Liu, Fandong Meng, Jinchao Zhang, Jinan Xu, Yufeng Chen, and Jie Zhou. 2019. GCDT: A global context enhanced deep transition architecture for sequence labeling. CoRR, abs/1906.02437.\n\nLiu et al. (2020) Zihan Liu, Yan Xu, Tiezheng Yu, Wenliang Dai, Ziwei Ji, Samuel Cahyawijaya, Andrea Madotto, and Pascale Fung. 2020. Crossner: Evaluating cross-domain named entity recognition.\n\nLoshchilov and Hutter (2018) Ilya Loshchilov and Frank Hutter. 2018. Decoupled weight decay regularization. In International Conference on Learning Representations.\n\nLou et al. (2023) Jie Lou, Yaojie Lu, Dai Dai, Wei Jia, Hongyu Lin, Xianpei Han, Le Sun, and Hua Wu. 2023. Universal information extraction as unified semantic matching. AAAI.\n\nLu et al. (2021) Yaojie Lu, Hongyu Lin, Jin Xu, Xianpei Han, Jialong Tang, Annan Li, Le Sun, M. Liao, and Shaoyi Chen. 2021. Text2event: Controllable sequence-to-structure generation for end-to-end event extraction. ArXiv, abs/2106.09232.\n\nLu et al. (2022) Yaojie Lu, Qing Liu, Dai Dai, Xinyan Xiao, Hongyu Lin, Xianpei Han, Le Sun, and Hua Wu. 2022. Unified structure generation for universal information extraction. In Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 5755–5772, Dublin, Ireland. Association for Computational Linguistics.\n\nLuan et al. (2018) Yi Luan, Luheng He, Mari Ostendorf, and Hannaneh Hajishirzi. 2018. Multi-task identification of entities, relations, and coreference for scientific knowledge graph construction.\n\nMitchell et al. (2005) Alexis Mitchell, Stephanie Strassel, Shudong Huang, and Ramez Zakhary. 2005. Ace 2004 multilingual training corpus. Linguistic Data Consortium, Philadelphia, 1:1–1.\n\nopenbiocorpora (2015) openbiocorpora. 2015. openbiocorpora anatem.\n\nOuyang et al. (2022) Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, et al. 2022. Training language models to follow instructions with human feedback. Advances in Neural Information Processing Systems, 35:27730–27744.\n\nPan et al. (2017a) Xiaoman Pan, Boliang Zhang, Jonathan May, Joel Nothman, Kevin Knight, and Heng Ji. 2017a. Cross-lingual name tagging and linking for 282 languages. In Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 1946–1958, Vancouver, Canada. Association for Computational Linguistics.\n\nPan et al. (2017b) Xiaoman Pan, Boliang Zhang, Jonathan May, Joel Nothman, Kevin Knight, and Heng Ji. 2017b. Cross-lingual name tagging and linking for 282 languages. In Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 1946–1958, Vancouver, Canada. Association for Computational Linguistics.\n\nRiedel et al. (2010) Sebastian Riedel, Limin Yao, and Andrew McCallum. 2010. Modeling relations and their mentions without labeled text. In ECML/PKDD.\n\nRoth and tau Yih (2004) Dan Roth and Wen tau Yih. 2004. A linear programming formulation for global inference in natural language tasks. In Conference on Computational Natural Language Learning.\n\nSainz et al. (2023) Oscar Sainz, Iker García-Ferrero, Rodrigo Agerri, Oier Lopez de Lacalle, German Rigau, and Eneko Agirre. 2023. Gollie: Annotation guidelines improve zero-shot information-extraction. arXiv preprint arXiv:2310.03668.\n\nSang and Meulder (2003) Erik F. Tjong Kim Sang and Fien De Meulder. 2003. Introduction to the conll-2003 shared task: Language-independent named entity recognition.\n\nShoeybi et al. (2019) Mohammad Shoeybi, Mostofa Patwary, Raul Puri, Patrick LeGresley, Jared Casper, and Bryan Catanzaro. 2019. Megatron-lm: Training multi-billion parameter language models using model parallelism. arXiv preprint arXiv:1909.08053.\n\nTedeschi and Navigli (2022) Simone Tedeschi and Roberto Navigli. 2022. MultiNERD: A multilingual, multi-genre and fine-grained dataset for named entity recognition (and disambiguation). In Findings of the Association for Computational Linguistics: NAACL 2022, pages 801–812, Seattle, United States. Association for Computational Linguistics.\n\nTouvron et al. (2023) Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al. 2023. Llama 2: Open foundation and fine-tuned chat models. arXiv preprint arXiv:2307.09288.\n\nWalker and Consortium (2005) C. Walker and Linguistic Data Consortium. 2005. ACE 2005 Multilingual Training Corpus. LDC corpora. Linguistic Data Consortium.\n\n(43) Chenguang Wang, Xiao Liu, Zui Chen, Haoyun Hong, Jie Tang, Dawn Song, and UC Berkeley. Deepstruct: Pretraining of language models for structure prediction.\n\nWang et al. (2023a) Shuhe Wang, Xiaofei Sun, Xiaoya Li, Rongbin Ouyang, Fei Wu, Tianwei Zhang, Jiwei Li, and Guoyin Wang. 2023a. Gpt-ner: Named entity recognition via large language models. arXiv preprint arXiv:2304.10428.\n\nWang et al. (2023b) Xiao Wang, Weikang Zhou, Can Zu, Han Xia, Tianze Chen, Yuansen Zhang, Rui Zheng, Junjie Ye, Qi Zhang, Tao Gui, et al. 2023b. Instructuie: Multi-task instruction tuning for unified information extraction. arXiv preprint arXiv:2304.08085.\n\nWang et al. (2022) Xingyao Wang, Sha Li, and Heng Ji. 2022. Code4struct: Code generation for few-shot structured prediction from natural language. arXiv preprint arXiv:2210.12810.\n\nWeischedel et al. (2013) Ralph Weischedel, Martha Palmer, Mitchell Marcus, Eduard Hovy, Sameer Pradhan, Lance Ramshaw, Nianwen Xue, Ann Taylor, Jeff Kaufman, Michelle Franchini, et al. 2013. Ontonotes release 5.0 ldc2013t19. Linguistic Data Consortium, Philadelphia, PA, 23:170.\n\nXu et al. (2023) Derong Xu, Wei Chen, Wenjun Peng, Chao Zhang, Tong Xu, Xiangyu Zhao, Xian Wu, Yefeng Zheng, and Enhong Chen. 2023. Large language models for generative information extraction: A survey. arXiv preprint arXiv:2312.17617.\n\nZhang and Wang (2015) Dongxu Zhang and Dong Wang. 2015. Relation classification via recurrent neural network.\n\nZhang et al. (2023) Ningyu Zhang, Jintian Zhang, Xiaohan Wang, Honghao Gui, Kangwei Liu, Yinuo Jiang, Xiang Chen, Shengyu Mao, Shuofei Qiao, Yuqi Zhu, Zhen Bi, Jing Chen, Xiaozhuan Liang, Yixin Ou, Runnan Fang, Zekun Xi, Xin Xu, Lei Li, Peng Wang, Mengru Wang, Yunzhi Yao, Bozhong Tian, Yin Fang, Guozhou Zheng, and Huajun Chen. 2023. Knowlm technical report.\n\nZhou et al. (2023) Wenxuan Zhou, Sheng Zhang, Yu Gu, Muhao Chen, and Hoifung Poon. 2023. Universalner: Targeted distillation from large language models for open named entity recognition. arXiv preprint arXiv:2308.03279.\n\nAppendix A Analyses on Schema Importing\n\nTo get insight into how the data organization method contributes to the results of schema understanding, we compare the performance of KnowCoder training on different version data with different schema importing methods. Specifically, we generate three versions of training data, named “Import-First”, “Sentence-First”, and “Whole”. “Import-First” denotes that we import the class first and then give out the sentence. “Sentence-First” denotes that we import the class following the sentence, which is the version KnowCoder adopts. “Whole” denotes the version that we give out the class after the sentence with their whole definitions. We train the model under the same setting and report the micro F1 curve on the test set of seven zero-shot NER datasets. The results are shown in Figure 3. It can be seen that “Sentence-First” performs best. If the “import” clause is before the sentence, LLMs are trained to predict the specific class after ‘‘from Entities import’’ without giving any information. The “Whole” method makes the model overfitting to the definition code because they are repeated frequently.\n\nAppendix B Analyses on Class Name\n\nThe same concept may have different names in different IE datasets, and the concept name in downstream datasets may conflict with the name in KnowCoder schema. For example, ‘‘Human’’ in KnowCoder schema shares the same meaning as ‘‘Person’’ in ACE05. To eliminate conflicts among names of concepts in different schemas, we align the concept names in IE datasets to KnowCoder schema. Note that, for a fair comparison, we make sure the number of concepts in a dataset does not change during the alignment process. Figure 4 illustrates the F1 performance across all types under aligned and unaligned experimental settings. With average scores of 81.3781.3781.3781.37 and 81.3581.3581.3581.35, respectively, it can be inferred that aligning schemas does not significantly impact the model’s outcomes.\n\nAppendix C Analyses on Class Methods\n\nClass methods are utilized to post-process the extracted results generated by LLMs. Three cases of the used class methods are listed in Table 9. To demonstrate the effectiveness of class methods, We conduct experiments on five NER datasets, including ACE05, Broad Twitter, MIT Movie, MIT Restaurant, and Ncbi-disease. The results are shown in Table 10. It can be observed that KnowCoder gets an average F1 improvement of 1%percent11\\%1 %. By defining some class-specific extraction rules, class methods help KnowCoder to extract more precise results.\n\nAppendix D Analyses on Prompts\n\nTo validate the influence of different prompts on the results, Table 11 reports the performance of NER on ACE05 using prompts with two styles, i.e., Code and IE styles. It can be observed that results are similar (with a gap of 0.7%percent0.70.7\\%0.7 % of F1), which verifies the robustness of KnowCoder to different prompts. The code-style prompt is slightly better than the IE style, suggesting that code-style prompts can better stimulate the code generation capabilities of LLMs compared to the text-style prompt and thus benefit the IE tasks.\n\nAppendix E Analyses on Negative Sampling\n\nTo demonstrate how the negative class sampling and fully negative sample construction contribute to the results, we conduct experiments of removing the negative classes (denoted as w.o. NC) and fully negative samples (denoted as w.o. FNS), respectively. The macro average of F1 on seven zero-shot NER datasets is reported in Table 12. It can be seen that the performance of KnowCoder decreases without negative sampling, which proves the effectiveness of the negative class sampling and fully negative sample construction.\n\n○ 7B\n\nAppendix F KnowCoder Benchmark\n\nBenchmark Construction.\n\nConsidering the significant expenses associated with assessing all test sets for NER and RE tasks, we developed a sampling method to establish the KnowCoder Benchmark to balance evaluation expenses and precision. Our primary principle is ensuring the sampled subset retains the same distribution. Specifically, we randomly sampled a portion of samples from each type in the dataset with a scaling factor s𝑠sitalic_s. For NER and RE tasks, we set s𝑠sitalic_s to 14141414 and 4444, respectively. Assuming the original number of samples of the type in a dataset is x𝑥xitalic_x, the sampled number in the benchmark is:\n\nk=⌈x/s⌉,s≥1.formulae-sequence𝑘𝑥𝑠𝑠1\\displaystyle k=\\lceil x/s\\rceil,s\\geq 1.italic_k = ⌈ italic_x / italic_s ⌉ , italic_s ≥ 1 . (1)\n\nNote that we adopted the same sampling method for the empty samples in datasets. Moreover, a sample may be sampled multiple times because there may be more than one type of instance. Thus, we remove duplicate samples during the sampling process. Due to the smaller number and size of EAE and ED datasets, we used the complete dataset for evaluation.\n\nStatistics of the Benchmark.\n\nTable 13 summarizes the information on the benchmarks under the supervised setting for two tasks: NER and RE.\n\nBenchmark Significance.\n\nThe results reported in this paper are produced in the sampled benchmark with 42424242 as the base seed. To systematically assess how the generated benchmarks affect the reproducibility and consistency of the model’s effectiveness, we employ multiple rounds of experiments on benchmarks with distinct random seeds, i.e., 1111, 2222, and 42424242. Table 14 summarizes the average performance on NER and RE tasks. It can be observed that the performance variations of KnowCoder across different benchmarks are minor (85.1±0.2plus-or-minus85.10.285.1\\pm{0.2}85.1 ± 0.2 for NER and 71.9±0.5plus-or-minus71.90.571.9\\pm{0.5}71.9 ± 0.5 for RE). The results demonstrate that KnowCoder’s results reported in this paper are both consistent and reproducible.\n\nAppendix G Training Data Generation\n\nThe training data used in the schema understanding phase consists of two kinds of codes, i.e., schema definition codes and instance codes. In this section, we will give more details of the instance code generation process.\n\nThe instance code is generated based on the KELM corpus. The processing procedure mainly includes four steps: entity typing, entity code generation, relation code generation, and event code generation. The origin data of the KELM corpus does not annotate the types of entities. We obtain the mappings from entity names to entity types based on WikiData. Specifically, we find the corresponding WikiData ID for each entity in KELM and identify its types through the ‘‘InstanceOf’’ relations. For those entities without types, we filter them from training data. Then, we generate the entity code based on the typed KELM corpus. Finally, we clean the data by removing samples with the entity type “Wikimedia Disambiguation Page” and removing contents in brackets for entities and entity types. Based on the typed entities, we generate the relation code. Since KELM does not contain event codes, we consider relations to be events if they have sub-properties. We treat their relation types as event types, the sub-properties as corresponding role types, and the annotated mentions as arguments. Furthermore, we delete samples if the event role is one of “of”, “follows”, “followed by”, “point in time”, “country”.\n\nAppendix H Data Statistics\n\nStatistics of the Constructed Schema Library.\n\nThe schema library is constructed on KELM Agarwal et al. (2021), UniversalNER Zhou et al. (2023), InstructIE Zhang et al. (2023) and LSEE Chen et al. (2017). The detailed analysis of each task schema is shown in Table 16. Here, “#Type” denotes the total number of types, “#Type w/ desc.” indicates the count of types with descriptions, and “#Type w/o desc.” signifies the count of types without descriptions.\n\nStatistics of the Training Data.\n\nThe training data consists of three parts: schema understanding data, schema following data, and specific domain IE data. The schema understanding training data includes schema definition codes and instance codes. The schema definition codes are built based on the schema library, with statistical results shown in Table 16. Schema instance codes are constructed based on KELM Agarwal et al. (2021), with statistical results provided in Table 15. The schema following training data is constructed on UniversalNER Zhou et al. (2023), InstructIE Zhang et al. (2023) and LSEE Chen et al. (2017). The statistics of schema following training data are presented in Table 15.\n\nAdditionally, for specific domain Information Extraction (IE), we conduct experiments utilizing 33333333 datasets, comprising 23232323 datasets for the NER task, 8888 datasets for the RE task, and 2222 datasets for the ED and EAE tasks. Specifically, under the supervised setting, we employ 18181818 datasets for the NER task, including ACE04 Mitchell et al. (2005), ACE 2005 Walker and Consortium (2005), AnatEM openbiocorpora (2015), Broad Twitter Derczynski et al. (2016), bc2gm Kocaman and Talby (2020), bc5cdr Li et al. (2016), CoNLL03 Sang and Meulder (2003), DIANN Pan et al. (2017a), FabNERKumar and Starly (2021), FindVehicle Guan (2022), GENIA Kim et al. (2003), MIT Movie Liu et al. (2019) MIT Restaurant Liu et al. (2019) MultiNERD Tedeschi and Navigli (2022), ncbi-disease Dogan et al. (2014), Ontonotes5 Weischedel et al. (2013), WikiANN Pan et al. (2017b), and WNUT17 Derczynski et al. (2017). For the RE task, we utilize 8888 datasets under the supervised setting, including ACE 2005 Walker and Consortium (2005), ADE corpus Gurulingappa et al. (2012), CoNLL04 Roth and tau Yih (2004), GIDS Jat et al. (2018), kbp37 Zhang and Wang (2015), NYT Riedel et al. (2010), SciERC Luan et al. (2018), and semeval RE Hendrickx et al. (2010). For the ED and EAE tasks, ACE05 Walker and Consortium (2005) and CASIE Lu et al. (2021) are employed.\n\nUnder the zero-shot setting, we take 7777 datasets for the NER task, following Wang et al. (2023b); Zhou et al. (2023), which include 5555 CrossNER subsets (AI, literature, music, politics, science) Liu et al. (2020), MIT-Movie Liu et al. (2019) and MIT-Restaurant Liu et al. (2019). For the RE task, we adopt GIDS Jat et al. (2018) under the zero-shot setting. For the ED and EAE tasks, CASIE Lu et al. (2021) is adopted under the zero-shot setting, following Sainz et al. (2023).\n\nThe detailed statistic of each dataset is shown in Table 17. Here, “#Type” indicates the number of types, while “#Train”, “#Dev”, and “#Test” denote the number of sentences in the training, development, and test datasets, respectively. Figure 5 shows the overview of the datasets on specific domain IE by task and size. Note that the statistics for each dataset in the figure encompass the total number of train, dev, and test datasets.\n\nAppendix I Details of Result Post-processing\n\nAfter the output codes are generated, we obtain the extraction results based on some regular expressions. To ensure the prediction results are more standardized and credible, two extra post-processing operations are added.\n\nSuperclass Induction.\n\nFor the NER task, during the schema understanding phase, we have learned 29,1772917729,17729 , 177 entity schemas, while the test dataset only contains 391391391391 schemas. For specific categories, our model may provide more detailed answers which are not in the dataset schema. For example, when it comes to the entity “Harvard University”, our model tends to classify it as a ‘‘University’’, while the ground truth labels it as an ‘‘Organization’’. In such cases, we employ an upper-level recursive method to address this issue. Specifically, for the predicted entity, we perform Superclass Induction based on its position in the relationship tree in Wikidata. If the entity type of its upper-level concept matches the entity type in the ground truth, we consider the entity prediction to be correct.\n\nType and Text Filtering.\n\nFor NER, RE, and EE tasks, if the model predicts a type that is not defined in the dataset schema and cannot be derived through superclass induction, or if an argument appears in the EAE task that is not present in the schema, we filter out such cases when calculating metrics. Additionally, if the model predicts text that does not appear in the sentence, we also filter it out.\n\nAppendix J Implementation Details\n\nSchema Understanding Phase.\n\nThe model is trained using AdamW Loshchilov and Hutter (2018) optimizer with β1subscript𝛽1\\beta_{1}italic_β start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT = 0.90.90.90.9, β2subscript𝛽2\\beta_{2}italic_β start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT = 0.950.950.950.95, ϵitalic-ϵ\\epsilonitalic_ϵ = 10−8superscript10810^{-8}10 start_POSTSUPERSCRIPT - 8 end_POSTSUPERSCRIPT . We set the peak learning rate to 5×10−65superscript1065\\times 10^{-6}5 × 10 start_POSTSUPERSCRIPT - 6 end_POSTSUPERSCRIPT, and use a cosine learning rate schedule with warmup ratio of 0.10.10.10.1, and decay final learning rate down to 10%percent1010\\%10 % of the peak learning rate. To mitigate overfitting, we incorporated a weight decay of 0.10.10.10.1 and a gradient clipping of 1.01.01.01.0. We configure the context length to 2048204820482048 and the global batch size to 1111M tokens, with the maximum training step capped at 4500450045004500.\n\nSchema Following Phase.\n\nWe apply the LoRA Hu et al. (2021) method to all non-embedding linear layers for schema following. During this phase, we configure the LoRA rank and alpha parameters to 32323232 and 64646464, respectively, and set a dropout rate of 0.10.10.10.1 to prevent overfitting. We still use the AdamW optimizer along with a cosine learning rate scheduler as in the schema understanding phase. The model undergoes 510510510510K training samples, with a learning rate of 3×10−43superscript1043\\times 10^{-4}3 × 10 start_POSTSUPERSCRIPT - 4 end_POSTSUPERSCRIPT, a global batch size of 256256256256, and a warmup ratio of 0.030.030.030.03.\n\nRefinement Phase.\n\nIn the refinement phase, we employ a parameter configuration that is largely identical to the one used during the schema following phase. However, given the richer and more varied task-type data available during the refinement stage, we opt for a greater number of training iterations. Specifically, we conduct training over three epochs, cumulatively training on 1.91.91.91.9M samples.\n\nAppendix K Cases of KnowCoder Training Data\n\nHere, we outline the cases that we have picked out from the KnowCoder-Dataset.\n\nK.1 Instance Code in Schema Understanding Phase\n\nNER Task\n\n1\n\n2sentence=\"Lalita Yauhleuskaya competed at the 2008 Summer Olympics.\"\n\n3\n\n4fromEntitiesimportHuman\n\n5\n\n6results=[\n\n7Human(\"Lalita Yauhleuskaya\")\n\n8]\n\nRE Task\n\n1\n\n2sentence=\"Gzim Istrefi plays for Carlstad United BK.\"\n\n3\n\n4fromEntitiesimportHuman,AssociationFootballClub\n\n5fromRelationsimportMemberOfSportsTeam\n\n6\n\n7results=[\n\n8MemberOfSportsTeam(\n\n9Human(\"Gzim Istrefi\"),\n\n10AssociationFootballClub(\"Carlstad United BK\")\n\n11)\n\n12]\n\nEE Task\n\n1\n\n2sentence=\"Jamsilsaenae station is adjacent to Sports Complex station which is on the Seoul Subway Line 2. The Sports Complex station is in the direction of Inner Ring Road and is located near Gangnam station.\"\n\n3\n\n4fromEntitesimportEntity\n\n5fromEventsimportAdjacentStation\n\n6\n\n7results=[\n\n8AdjacentStation(\n\n9connecting_line=[Entity(\"Seoul Subway Line 2\")],\n\n10towards=[Entity(\"Gangnam station\")],\n\n11direction=[Entity(\"Inner Ring Road\")]\n\n12)\n\n13]\n\nK.2 Instruction-tuning Code in Schema Following Phase\n\nNER Task\n\n⬇ 1classEntity: 2\"\"\" 3Thebaseclassforallentities. 4\"\"\" 5def__init__(self,name:str): 6self.name=name 7 8classPerson(Entity): 9\"\"\" 10Description:beingthathascertaincapacitiesorattributesconstitutingpersonhood. 11Examples:patients,DonaldTrump,children,women,user,patient,Trump,PresidentTrump,BarackObama,people 12\"\"\" 13pass 14 15classNationality(SocialGroup): 16\"\"\" 17Description:Alegalidentificationofapersonininternationallaw,establishingthepersonasasubject,anational,ofasovereignstate. 18Examples:American,British,Americans,German,French,English,Japanese,Russian,Australian,Indian 19\"\"\" 20pass 21 22classTvShow(Entity): 23\"\"\" 24Description: 25Examples:GameofThrones,TheWalkingDead,AmericanIdol,ModernFamily,SaturdayNightLive,DoctorWho,House,TheTonightShow,MadMen,ArrestedDevelopment 26\"\"\" 27pass 28 29\"\"\" 30Thisisanobject-orientedprogrammingtask:someEntityClassesaredefinedabove.PleaseinstantiateallthecorrespondingEntityObjectsinthefollowingsentence. 31\"\"\" 32sentence=‘‘Ienjoyedtheseries‘ProfessionalMasterChef’ on television and I was struck by something the judges said when commenting about two of the semi-finalists. They had been highly impressed with the dishes the chefs had presented and Michel Roux Junior remarked that, despite their very obvious skill, neither chef exhibited any arrogance or conceit. Monica Galetti replied that they didn’tneedto,becausetheirworkspokeforthem.’’ ⬇ 1results=[ 2TvShow(\"Professional Master Chef\"), 3Person(\"Michel Roux Junior\"), 4Person(\"Monica Galetti\") 5]\n\nRE Task\n\n⬇ 1classEntity: 2\"\"\" 3Thebaseclassforallentities. 4\"\"\" 5def__init__(self,name:str): 6self.name=name 7 8classRelation: 9\"\"\" 10Thebaseclassforallrelations. 11\"\"\" 12def__init__(self,head_entity:Entity,tail_entity:Entity): 13self.head_entity=head_entity 14self.tail_entity=tail_entity 15 16classPlaceOfBirth(Relation): 17\"\"\" 18Description:Mostspecificknown(e.g.cityinsteadofcountry,orhospitalinsteadofcity)birthlocationofaperson,animalorfictionalcharacter. 19Examples:(Australian,London),(Muhammad,Mecca),(Augustus,Rome),(Tiberius,Rome),(Mozart,Salzburg),(CharlesII,London),(SimaZhao,China),(FredericktheGreat,Berlin),(JuliusCaesar,Rome),(QueenMyeongui,Goryeo) 20\"\"\" 21def__init__(self,head_entity:Entity,tail_entity:Entity): 22super().__init__(head_entity=head_entity,tail_entity=tail_entity) 23 24classPopulation(Relation): 25\"\"\" 26Description:Numberofpeopleinhabitingtheplace;numberofpeopleofsubject. 27Examples:(civilparish,201),(SaoPedro,201),(MachameKusini,13,572),(SaoJoao,201),(unincorporatedcommunity,15),(unincorporatedcommunity,94),(unincorporatedcommunity,25),(Mardekheh-yeKuchek,197),(PainHaluSara,701),(Marenj,1,055) 28\"\"\" 29def__init__(self,head_entity:Entity,tail_entity:Entity): 30super().__init__(head_entity=head_entity,tail_entity=tail_entity) 31 32classLocatedIn(Relation): 33\"\"\" 34Description: 35Examples:(NationalRegisterofHistoricPlaces,UnitedStates),(Ontario,Canada),(SaoPaulo,Brazil),(Victoria,Australia),(census-designatedplace,UnitedStates),(NewSouthWales,Australia),(California,UnitedStates),(Andes,Peru),(FAA,UnitedStates),(Norwegian,Norway) 36\"\"\" 37def__init__(self,head_entity:Entity,tail_entity:Entity): 38super().__init__(head_entity=head_entity,tail_entity=tail_entity) 39 40\"\"\" 41Thisisanobject-orientedprogrammingtask:someRelationClassesandrelatedEntityClassesaredefinedabove.PleaseinstantiateallthecorrespondingRelationObjectsinthefollowingsentence. 42\"\"\" 43sentence=‘‘KurushisamountainvillagelocatedintheDokuzparinskyDistrict,insouthernDagestan.Situatedat2480-2560mabovesealeveldependingonthesource,itisthehighestcontinuouslyinhabitedsettlementoftheGreaterCaucasusandofEuropeaswellasthesouthernmostsettlementinRussia.Asof2015,Kurushhadapopulationof813.’’ ⬇ 1results=[ 2LocatedIn(Entity(\"Kurush\"),Entity(\"Dokuzparinsky District\")), 3LocatedIn(Entity(\"Dokuzparinsky District\"),Entity(\"Dagestan\")), 4Population(Entity(\"Kurush\"),Entity(\"813\")) 5]\n\nED Task\n\n⬇ 1classEvent: 2\"\"\" 3Thebaseclassforallevents. 4\"\"\" 5def__init__(self,trigger:str,arg_names,*args): 6self.trigger=trigger 7self.arguments={} 8forarg_name,arg_valuesinzip(arg_names,args): 9self.arguments[arg_name]=arg_values 10 11classGroupMembership(Event): 12\"\"\" 13Description:Organization,clubormusicalgrouptowhichthesubjectbelongs. 14Examples:singer,music,musician,play,concert,performance,singing,sang,sung,sing, 15\"\"\" 16def__init__(self,trigger:str,*args): 17arg_names=[\"start\",\"role\",\"end\",\"group\",\"member\"] 18super().__init__(trigger=trigger,arg_names=arg_names,*args) 19 20classOlympicMedalHonor(Event): 21\"\"\" 22Description:ThehonorassociatedwithwinninganOlympicmedal. 23Examples:medal,gold,winner,win,silver,competition,bronze,victory,player,compete, 24\"\"\" 25def__init__(self,trigger:str,*args): 26arg_names=[\"event\",\"country\",\"medalist\",\"medal\",\"olympics\"] 27super().__init__(trigger=trigger,arg_names=arg_names,*args) 28 29classEducation(Event): 30\"\"\" 31Description:Educationalinstitutionattendedbysubject. 32Examples:school,professor,coach,graduate,student,study,master,education,pupil,lecturer, 33\"\"\" 34def__init__(self,trigger:str,*args): 35arg_names=[ 36\"start_date\", 37\"degree\", 38\"end_date\", 39\"institution\", 40\"student\", 41\"specialization\", 42\"major_field_of_study\", 43] 44super().__init__(trigger=trigger,arg_names=arg_names,*args) 45 46classMarriage(Event): 47\"\"\" 48Description:Thesubjecthastheobjectastheirspouse(husband,wife,partner,etc.). 49Examples:wife,married,husband,marriage,wedding,marry,couple,spouse,mistress,divorce, 50\"\"\" 51def__init__(self,trigger:str,*args): 52arg_names=[\"spouse\",\"location_of_ceremony\",\"type_of_union\",\"to\",\"from\"] 53super().__init__(trigger=trigger,arg_names=arg_names,*args) 54 55\"\"\" 56Thisisanobject-orientedprogrammingtask:someEventClassesaredefinedabove.PleaseinstantiateallthecorrespondingEventObjectsinthefollowingsentence. 57\"\"\" 58sentence=\"Thomas Lincoln on June 12, 1806 married Nancy Hanks in the Richard Berry home.\" ⬇ 1results=[ 2Marriage(\"married\") 3]\n\nEAE Task\n\n⬇ 1classEntity: 2\"\"\" 3Thebaseclassforallentities. 4\"\"\" 5def__init__(self,name:str): 6self.name=name 7 8classEvent: 9\"\"\" 10Thebaseclassforallevents. 11\"\"\" 12def__init__(self,trigger:str): 13self.trigger=trigger 14 15classEducation(Event): 16\"\"\" 17Description:Educationalinstitutionattendedbysubject. 18\"\"\" 19def__init__( 20self, 21trigger:str, 22start_date:List[Entity], 23degree:List[Entity], 24end_date:List[Entity], 25institution:List[Entity], 26student:List[Entity], 27specialization:List[Entity], 28major_field_of_study:List[Entity], 29): 30super().__init__(trigger=trigger) 31self.start_date=start_date 32self.degree=degree 33self.end_date=end_date 34self.institution=institution 35self.student=student 36self.specialization=specialization 37self.major_field_of_study=major_field_of_study 38 39\"\"\" 40Thisisanobject-orientedprogrammingtask:someEventClassesaredefinedabove.PleaseinstantiateallthecorrespondingEventObjectsinthefollowingsentence.Itisimportanttonotethatthetriggersoftheeventsareconfirmedasfollows:\"graduate\" is the trigger of event type \"Education\". 41\"\"\" 42sentence=\"Albert J. Herberger (born c. 1933) is a Vice Admiral of the United States Navy, and the first United States Merchant Marine Academy graduate to attain the rank.\" ⬇ 1results=[ 2Education( 3trigger=\"graduate\", 4institution=[Entity(\"United States Merchant Marine Academy\")], 5student=[Entity(\"Albert J. Herberger\")] 6) 7]"
    }
}
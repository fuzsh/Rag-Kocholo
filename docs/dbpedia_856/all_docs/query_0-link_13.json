{
    "id": "dbpedia_856_0",
    "rank": 13,
    "data": {
        "url": "https://crl.ucsd.edu/talks/pasttalks.php",
        "read_more_link": "",
        "language": "en",
        "title": "Past Talks",
        "top_image": "https://crl.ucsd.edu/favicon.ico",
        "meta_img": "https://crl.ucsd.edu/favicon.ico",
        "images": [
            "https://crl.ucsd.edu/talks/images/hickok-poeppel-2020.jpg"
        ],
        "movies": [],
        "keywords": [],
        "meta_keywords": [
            ""
        ],
        "tags": null,
        "authors": [],
        "publish_date": null,
        "summary": "",
        "meta_description": "",
        "meta_lang": "en",
        "meta_favicon": "/favicon.ico",
        "meta_site_name": "",
        "canonical_link": null,
        "text": "2023 2022 2021 2020 2019 2018 2017 2016 2015 2014 2013 2012 2011 2010 2009 2008 2007 2006 2005 2004 2003 2002 2001 2000 1999\n\n2024-05-24\n\nWhat’s in the speech signal? Identifying relevant cues for language differentiation\n\nReina Mizrahi\n\nDepartment of Cognitive Science at University of California, San Diego\n\n+ more\n\nFor decades, a question that has been at the center of bilingualism research is to what extent do bilingual’s two languages overlap during perception and production? In this talk, I aim to address different aspects of this question in two parts.\n\nIn the first part of this talk, I will review a series of experiments examining what speech cues are relevant for language categorization in 3- to 5-year-old children. In Experiment 1, we asked whether children of different language backgrounds (bilingual or monolingual) could utilize language as a cue for talker identification by measuring accuracy and fixation patterns to the target character. Experiments 2 and 3 investigate whether specific cues such as voice characteristics (Experiment 2) and comprehensibility (Experiment 3) are used to differentiate between speakers of different languages. Experiment 4 tests whether children can differentiate between two unknown languages, and lastly, in Experiment 5 we dissociate children’s use of language comprehensibility from their perceptual familiarity with language specific features (i.e., phonotactics and morphosyntax). experiments, we investigate whether different components of language, phonotactics, morphosyntax, and/or meaning, serve as meaningful cues for language differentiation in bilingual and monolingual children.\n\nIn part two, I present two experiments exploring the scope of cross-linguistic activation in Spanish-English bilingual adults, specifically whether language context and low-level features modulate levels of dual-language activation. I use the visual-world eye-tracking paradigm in these experiments as it has put forth some of the most compelling evidence for dual-language activation (Spivey & Marian, 1999; Marian & Spivey, 2003). In Experiment 1, I investigate how language context (mixed or single language) impacts cross-language activation. Further, to understand the level at which a global control mechanism might be involved (high vs. low level), Experiment 2 investigates if the accent (native vs. nonnative) of the word heard affects the degree of dual-language activation.\n\nFindings from both lines of research suggest that sound context, both phonotactics and morphosyntactic cues, play a stronger role in language categorization than other elements of speech, and than previously thought.\n\n2024-05-10\n\nSyntax drives default language selection in bilingual connected speech production\n\nJessie Quinn\n\nDepartment of Psychology at University of California, San Diego\n\n+ more\n\nIn this talk I will discuss data from a study which investigated the role of syntactic processing in driving bilingual language selection. In two experiments, 120 English-dominant Spanish-English bilinguals read aloud 18 paragraphs with language switches. In Experiment 1a, each paragraph included eight switch words on function targets (four that repeated in every paragraph), and Experiment 1b was a replication with eight additional switches on content words in each paragraph. Both experiments had three conditions: (a) normal, (b) nouns-swapped (in which nouns within consecutive sentences were swapped), and (c) random (in which words in each sentence were reordered randomly). In both experiments bilinguals produced intrusion errors, automatically translating language switch words by mistake, especially on function words (e.g., saying the day and stay awake instead of the day y stay awake). Intrusion rates did not vary across experiments even though switch rate was doubled in Experiment 1b relative to Experiment 1a. Bilinguals produced the most intrusions in normal paragraphs, slightly but significantly fewer intrusions in nouns-swapped paragraphs and there was a dramatic drop in intrusion rates in the random condition, even though the random condition elicited the most within-language errors. Bilinguals also demonstrated a common signature of inhibitory control in the form of reversed language dominance effects, which did not vary significantly across paragraph types. Finally, intrusions increased with switch word predictability (surprisal), but significant differences between conditions remained when controlling for predictability. These results demonstrate that bilingual language selection is driven by syntactic processing, which operates independently from other language control mechanisms, such as inhibition.\n\n2024-03-15\n\nLearning Kata Kolok - a case study of language socialization in a Balinese sign language\n\nHannah Lutzenberger\n\nDepartment of Communication and Cognition, Tilburg University\n\nDepartment of English Language and Linguistics, University of Birmingham\n\n+ more\n\nChildren learn language through social interaction, with conversation being their primary input to make them competent language users. Often (though not universally), parent-child interactions are characterized by altered input (Bunce et al., 2020): speaking parents may modify pitch or syntax and signing parents may sign slower or on the child’s body(Holzrichter & Meier, 2000; Pizer et al., 2011). In addition, child signing is often characterised by modifications: handshapes and locations may be substituted, and movements may be altered. Both child-directed signing and child signing has been little studied in non-Western contexts: In this presentation, I present work on child signing and preliminary insights into work in progress on how deaf children are socialized with the sign language Kata Kolok, a small sign language that is used in rural enclave in Bali, Indonesia. The sign language emerged due sustained congenital deafness and represents an exceptional acquisition setting for deaf children: they receive immediate and rich signing input from many hearing and deaf signers from birth (de Vos, 2012; Marsaja, 2008). Using longitudinal corpus recordings, I analyse phonological modifications children make to signs and document how child-directed signing shapes early social interactions, focusing on 1) child-directed signing strategies and 2) conversational constellations.\n\n2024-03-01\n\nWhat Can Language Models Tell Us About the N400?\n\nJames Michaelov\n\nDepartment of Cognitive Science at University of California, San Diego\n\n+ more\n\nWhile the idea that language comprehension involves prediction has been around since at least the 1960s, advances in natural language processing technology have made it more viable than ever to model this computationally. As language models have increased in size and power, performing better at an ever-wider array of natural language tasks, their predictions also increasingly appear to correlate with the N400, a neural signal that has been argued to index the extent to which a word is expected based on its preceding context. The predictions of contemporary large language models can not only be used to model the effects of certain types of stimuli on the amplitude of the N400 response, but in some cases appear to predict single-trial N400 amplitude better than traditional metrics such as cloze probability. With these results in mind, I will discuss how language models can be used to study human language processing, both as a deflationary tool, and as a way to support positive claims about the extent to which humans may use the statistics of language as the basis of prediction in language comprehension.\n\n2024-02-16\n\nGrad Data Blast\n\nMatthew Mcarthur-- Pandemic Disruptions and Socioeconomic Status: Examining their Effects on Early Vocabulary Development\n\nJessie Quinn-- Syntax Guides Bilingual Language Selection\n\nThomas Morton-- Dynamics of Sentence Planning: Repetition from Representation to Articulation\n\n2023-10-31\n\nListening effort in processing a non-dominant language: a dual task study of Heritage Spanish speakers\n\nZuzanna Fuchs\n\nAssistant Professor, University of Southern California, Department of Linguistics\n\n+ more\n\nStudies measuring listening effort in second-language learners (L2s) have found that these speakers exert more effort to process speech in their second language than do monolingual speakers of the language. The present study, conducted in collaboration with Christine Shea and John Muegge (Univ. of Iowa), investigates listening effort in adult Heritage Speakers (HSs) of Spanish in comparison with both Spanish-dominant control speakers and L2s of Spanish. Through this comparison, we ask whether previously observed increased listening effort in an L2 may be at least partly a result of the nature of the second language acquisition experience. Like L2s, HSs are non-dominant in the target language, but unlike L2s, their input to the non-dominant language was early and naturalistic. If the nature of the acquisition process of the non-dominant language determines listening effort, then HSs may pattern differently from L2s and more like the Spanish-dominant control group. To measure listening effort in these populations, we employ a dual task study that combines a non-linguistic motion-object tracking task with a picture-selection task targeting participants’ comprehension of subject and object relative clauses. Assuming that both tasks draw from the same finite pool of cognitive resources – in line with previous dual task studies –, listening effort is operationalized as participants’ (decreased) accuracy and/or (increased) response time on the non-linguistic task as the complexity of the linguistic stimulus increases. Of interest is whether this effect is similar across the three groups or whether they show differential increases in listening effort. With data collection underway, we offer tentative results.\n\n2023-10-24\n\nBenefits and Applications of Web-Based Early Language Assessment\n\nMatthew McArthur\n\nSan Diego State University\n\n+ more\n\nIn the rapidly evolving landscape of developmental research, online assessments have emerged as a promising alternative to traditional in-person evaluations. This presentation will discuss the following advantages of using this method. Firstly, online assessments guarantee standardization, ensuring uniform instructions and stimuli for all participants, thereby reducing potential examiner-introduced variability. They offer flexibility in scheduling, enabling participants to take the assessment at optimal times and settings, boosting participation rates and minimizing scheduling conflicts. They allow for real-time data collection, streamlining the research process by capturing, storing, and processing participant responses. With the right infrastructures, these platforms ensure robust data security through encryption and secure storage mechanisms, reducing potential breaches compared to traditional methods. They help facilitate longitudinal studies by simplifying re-assessment protocols and eliminating the need for repetitive in-person engagements. Online assessment is cost-effective as it allows researchers to reduce their purchases of physical resources. Lastly, the accessibility and reach of online assessments opens doors to diverse geographical and demographic pools, enriching sample diversity and size. This presentation will elaborate on these benefits, exploring the potential of online assessments in developmental research.\n\nFollowing a discussion of the broad advantages of online assessments, the Web-Based Computerized Comprehension Task (Web-CCT) will be introduced. This online language assessment is designed to measure children’s receptive vocabulary from the ages of 18 to 60 months. Unlike many existing measures, the Web-CCT is designed to be administered on a desktop, laptop, tablet, or smartphone without the need of a trained researcher. Beyond this, it can measure vocabulary earlier than other assessments, starting at 18 months. Preliminary psychometric analysis underscores its reliability and potential as an important tool for developmental research. Attendees will gain insights into its structure, functionalities, and how it is being used in research being conducted at the SDSU Infant and Child Development Lab.\n\n2023-05-23\n\nDuolingo: Building Fun and Effective Learning Environments\n\nBozena Pajak\n\nDuolingo\n\n+ more\n\nDuolingo is the leading mobile learning platform globally, offering courses in languages, math, and early literacy (and more in development!). In this talk, I will give an overview of the Duolingo app and the company's experimentation-focused approach to app development. I will also share specific examples of our in-app experiments and other research studies that have contributed to making Duolingo a unique learning product: one that engages and motivates learners while also teaching them effectively.\n\nPresenter Bio\n\nBozena Pajak is the Vice President of Learning and Curriculum at Duolingo. She has a Ph.D. in Linguistics from the University of California, San Diego, and received postdoctoral training in Brain and Cognitive Sciences at University of Rochester. Prior to joining Duolingo, Bozena was a Researcher and Lecturer in the Linguistics Department at Northwestern University. Her research investigated implicit learning and generalization of linguistic categories. At Duolingo, she has built a 40-person team of experts in learning and teaching, and is in charge of projects at the intersection of learning science, pedagogy, and product development.\n\n2023-05-16\n\nA history of our times\n\nTyler Marghetis\n\nAssistant Professor of Cognitive and Information Sciences, University of California, Merced\n\n+ more\n\nThis is a talk about Time. I start with the tension between, on the one hand, the global diversity in how people talk and think about time, and on the other, the sense of stability—even necessity—that we often assign to our own idiosyncratic conceptions. I then argue as follows. First, ways of talking and thinking about time are best analyzed, not as concepts within individual brains, but as heterogeneous systems distributed across brains, bodies, material artifacts, and cultural practices—that is, as “cognitive ecologies.” Second, within a cognitive ecology, mutual dependence is the rule rather than the exception. Third, since cognitive ecologies consist of such varied components as neural circuits and Twitter timelines, these ecologies exhibit change on multiple, nested timescales—timescales that range from the slow evolution by natural selection of innate biases in our brains and bodies, to the cultural evolution of language and other artifacts, to the rapid pace of situated communication and interaction. Fourth, these considerations explain the patterns in cross-cultural diversity, the stability of conceptions within communities, and the ways in which conceptions do, and do not, change over time. This argument is intended to be generic and to apply equally to our conceptions of other domains. I conclude that our conceptions of time—and number, and space—only make sense in light of their histories.\n\n2023-04-25\n\nLanguage generality in phonological encoding: Moving beyond Indo-European languages\n\nJohn Alderete\n\nSimon Fraser University\n\n+ more\n\nTheories of phonological encoding are centred on the selection and activation of phonological segments, and how these segments are organised in word and syllable structures in online processes of speech planning. The focus on segments, however, is due to an over-weighting of evidence from Indo-European languages, because languages outside this family exhibit strikingly different behaviour. We examine speech error, priming, and form encoding studies in Mandarin, Cantonese, and Japanese, and argue that these languages deepen our understanding of phonological encoding. These languages demonstrate the need for language particular differences in the first selectable (proximate) units of phonological encoding and the phonological units processed as word beginnings. Building on these results, an analysis of tone slips in Cantonese suggests that tone is processed concurrently with segments and sequentially assigned after segment encoding to fully encoded syllables.\n\n2023-04-18\n\nThe Neurophysiological Basis of Expectation-based Comprehension\n\nMatthew W. Crocker\n\nDept of Language Science & Technology, Saarland University, Germany\n\n+ more\n\nI will outline recent results from my lab supporting a single-stream model of expectation-based comprehension, under which event-related brain potentials directly index two core mechanisms of sentence comprehension. The N400 – modulated by both expectation and association, but not plausibility – indexes retrieval of each word from semantic memory (N400). The P600, by contrast, provides a continuous index of semantic integration difficulty as predicted by a comprehension-centric model of surprisal, characterizing the effort induced in recovering the unfolding sentence meaning (P600). The findings are also argued to present a strong challenge to multi-stream accounts.\n\n2023-03-14\n\nWhat L2 speakers can tell us about filler-gap dependencies\n\nGrant Goodall\n\nDepartment of Linguistics, UC San Diego\n\n+ more\n\nFiller-gap dependencies have long attracted attention because on the one hand, they are able to occur over long distances, while on the other hand, they are disallowed in certain specific environments, often for reasons that are still mysterious. Here I present a series of acceptability experiments on L2 speakers (done jointly with Boyoung Kim) that show that these speakers handle filler-gap dependencies in a way similar to L1 speakers, but not exactly the same, and that these L1-L2 differences can shed light on the nature of filler-gap dependencies.\n\nThe experiments concern three instances of filler-gap dependencies where L2 speakers turn out to behave in surprising ways:\n\n1) gap in a that-clause (“Who do you think [that Mary saw __ ]?”)\n\n2) gap inside an “island” structure (*”Who do you wonder [why Mary saw __ ]?”)\n\n3) gap as the subject of a that-clause (*”Who do you think [that __ saw Mary]?”)\n\nThe overall pattern of results in L1 and L2 suggest that the ability to put a gap in a that-clause needs to be learned (and does not follow automatically from the ability to construct a filler-gap dependency and to embed a that-clause) and that the problem with having a subject gap in a that-clause may be related to the processing difficulty associated with having a gap at the onset of a clause. At a larger level, the results show that serious exploration of L2 speakers can be an important new source of information regarding longstanding linguistic puzzles.\n\n2023-03-07\n\nBridging languages: Reconstructing “The Construction of Reality”\n\nMichael A. Arbib\n\nAdjunct Professor of Psychology, UCSD\n\nEmeritus University Professor, USC\n\n+ more\n\nThis talk presents the key arguments from a paper for the Special Issue on “The interdisciplinary language of science, philosophy and religious studies” of Rivista Italiana di Filosofia del LinguaggioVol. 17, N. 1/2023, edited by Giuseppe Tanzella-Nitti and Ivan Colagè. Feedback and criticism from people of diverse disciplines (or requests for copies of the submitted paper) will be gratefully received at arbib@usc.edu.\n\nThe search for a single interdisciplinary language for science, philosophy and religious studies is doomed to failure, and translation between a pair of languages for domains within these fields is often impossible. Crucially, interaction between domains is based on human interaction, whether directly or through documents or artefacts, and so we espouse the development of conversations between “speakers” of different domain languages. Thus we must understand the relation between the mind of the individual scholar and the emerging consensuses that define a domain. We start with the conversations that Mary Hesse and Michael Arbib developed in constructing their Gifford Lectures, The Construction of Reality. They extended a theory of “schemas in the head” to include “the social schemas of a community.” A key question for such conversations is this: “If a language is defined in part by its semantics, how can people using that language disagree?” Members of a specific community -- such as a group of scholars within the same domain – can possibly reach near agreement on the usage of terminology. Conversations between scholars in two domains thus requires that they develop or share a bridging language at the interface of their domains in which they may reach shared understandings of the terms each uses and thus reach shared conclusions or agree to disagree. This general account is then explored in relation to a case study: the bringing together of linguistics, psychology, and neuroscience in the cognitive neuroscience of linguistics.\n\n2023-02-21\n\nRelational climate conversations as catalysts of action: Strengths, limitations, and strategies\n\nJulia Coombs Fine\n\nCollege of St. Benedict & St. John's University\n\n+ more\n\nThough 58% of Americans are concerned or alarmed about the climate crisis (Leiserowitz et al. 2021), most people rarely discuss it with friends and family (Leiserowitz et al. 2019). One strategy for disrupting this socially constructed “climate of silence” (Geiger & Swim 2016) is to engage in relational climate conversations, i.e., conversations about climate issues that draw on and deepen existing interpersonal relationships. Relational climate conversations have shown promise as a means of shifting participants’ attitudes and prompting further reflection and discussion (Beery et al. 2019; Lawson et al. 2019; Goldberg et al. 2019; Galway et al. 2021), and are recommended by many organizations; however, few studies have yet examined the goals, audiences, content, interactional contexts, and specific outcomes of climate conversations.\n\nIn the first part of the talk, I discuss survey and interview data from 112 climate activists across the U.S., finding that activists mostly have climate conversations with like-minded, politically progressive friends and family members. While at first glance this finding might suggest an echo chamber effect that could exacerbate the political polarization of climate change, activists mention strategic reasons for choosing like-minded audiences and report that these conversations are moderately effective in moving audiences from passive concern to action (in the form of behavioral changes, political advocacy, and social movement participation). The second part of the talk discusses an in-progress study that more directly observes the content and outcomes of a series of dyadic climate conversations between activists and their non-activist friends, partners, and family members. Preliminary results suggest that the conversations are highly effective at influencing the non-activists to seek out more information and have further conversations with others, but are less effective at influencing them to take action. This result may be due to non-activists’ low perceived efficacy, emphasis on lifestyle changes versus political action, lack of time and resources, and lack of knowledge about what types of action are possible. Activists’ discursive strategies—mostly deferential, occasionally adversarial—provide clues as to how these barriers to action might be overcome in future refinements of climate conversation prompts.\n\n2023-02-14\n\nLanguage Structures as Explanations of Experience\n\nDavid Barner\n\nDepartment of Psychology, UCSD\n\n+ more\n\nMy research program investigates the nature and origin of human thought as it is expressed through language and other symbolic systems. To do this, my lab focuses on how humans use language to (1) represent abstract conceptual content, (2) engage in logical reasoning, and (3) solve problems of social coordination. In this talk, I focus on the first problem and discuss how humans encode concepts of color, number, and time in language. Against the view that word learning reduces to a kind of mapping problem (e.g., between words and perception), I will argue that a key function of language acquisition is to build new structures that serve to explain experience, rather than just reflect it. In particular, I will argue that our representations of concepts like time and number - and to a degree color - involve rich inferential structures that extend beyond the data that perception is capable of providing. For example, relations between time words like second, minute, and hour, drive children's learning about perceptual duration, rather than the opposite, and constitute the backbone for building \"theories\" of time. Similarly, relations between number words like \"twenty-seven\" and \"thirty-seven\" drive conceptual discovery about the nature of numbers, and provide the basis for children's early emerging belief that numbers are infinite. Drawing on cross-cultural studies and data from the historical record, I relate this idea to the cultural evolution of symbolic systems that humans use to measure trade and debt, and to the development of technologies like writing and primitive calculators, like the abacus.\n\nView the recorded talk on Zoom.\n\n2023-02-07\n\nUsing Lexical and Contextual Features of Spontaneous Speech Production to Predict Cognitive Impairment in Early-Stage Dementia\n\nRachel Ostrand\n\nIBM Research\n\n+ more\n\nDementia, and its precursor mild cognitive impairment, affects millions of people. Traditionally, assessment and diagnosis is performed via an extensive clinical battery, consisting of some or all of multiple hours of neuropsychological testing, neuroimaging such as MRI, blood draws, and genetic testing. This is expensive and burdensome for the participant, and thus is not a practical method for regular monitoring of an at-risk person's status. However, language production has recently been shown to contain properties which are predictive of even early-stage cognitive impairment. In this talk, I'll discuss my research investigating what properties of language production correlate with cognitive status, and I'll describe how I've built an automated pipeline for computing these properties from speech transcripts. In particular, I will focus on lexical and contextual features of language: features like word frequency, part of speech counts, and linguistic surprisal. I'll present several studies with patients of different degrees of impairment, and who responded to different types of speech elicitation prompts. Across studies, certain language features, largely those which capture some facet of semantic specificity and lexical retrieval difficulty, are highly correlated with participants' cognitive status. This suggests that language production, which can be collected easily and thus relatively frequently, could be used as a remote and ongoing metric for monitoring cognitive decline.\n\n2023-01-31\n\nWords and Signs: How are they processed in the minds of deaf signers?\n\nZed Sehyr\n\nSan Diego State University\n\n+ more\n\nMany congenitally deaf and hard-of-hearing people process language visually through signs and written words. My work investigates how the sensory-perceptual and linguistic experiences of deaf signers’ shape language processing. I first present two recent studies that identify a unique reading profile for skilled deaf readers using statistical modeling (Study 1) and event-related potentials (ERPs) (Study 2). Study 1 assessed reading comprehension in ~200 hearing and deaf adults (matched for reading skill) and revealed a) phonological ability predicted reading scores for hearing, but not deaf readers and b) orthographic skill (spelling and fingerspelling ability) as well as vocabulary size were critical to reading success for deaf readers. Study 2 further supported this unique reading profile by showing a more bilateral N170 response when deaf signers read single words. This result was attributed to reduced phonological mapping in left-hemisphere temporal regions for deaf compared to hearing readers. These studies clarify the long-standing controversy about the importance of phonological codes and highlight the need for further research into alternative routes to literacy for deaf readers.\n\nA separate line of my research is focused on sign processing and has led to the development of the largest publicly available lexical database for any signed language (ASL-LEX; https://asl-lex.org/). I will briefly describe this database, the novel resources it provides, and what we have learned about the phonological (form-based) organization of the American Sign Language (ASL) lexicon. We are also now developing a sister database that reveals the semantic (rather than phonological) organization of the ASL lexicon. To do so, we collected >100,000 semantic free associations from deaf fluent signers—the largest labeled dataset of ASL signs obtained to date. Analysis of phonological and semantic relations visualized using network graphs has uncovered widespread patterns of systematic non-arbitrary alignment between form and meaning (i.e., iconic networks). In my future work, I plan to leverage these linguistic insights, sign language datasets, and AI/machine learning to develop models for sign recognition.\n\nOverall, these research strands constitute important steps toward building theories of language processing inclusive of deaf communication (written and signed) that may also help guide clinical practice in characterizing and rehabilitating language deficits in deaf individuals.\n\n2022-11-29\n\nSeeing what it means: What we know about how readers use active vision to derive meaning from text\n\nElizabeth R. Schotter\n\nDepartment of Psychology, University of South Florida\n\n+ more\n\nSkilled reading – one of our most finely tuned cognitive skills – requires coordination of a multitude of basic cognitive processes (e.g., visual perception, allocation of attention, linguistic prediction/integration). Yet the apparent ease and automaticity of the reading process that literate adults subjectively experience belies this complexity. In this talk, I will present evidence that the efficiency of the reading process relies on the ability to read ahead – to allocate attention to words before our eyes reach them. Although this parafoveal preview provides a head start on several components, not all reading processes can occur in parafoveal vision. I will provide evidence for these claims from my past eye tracking research, as well as current work in my lab using event related potentials (ERPs) and co-registration of eye movements and ERPs during natural reading.\n\n2022-11-22\n\nA theory of structure building in speaking\n\nShota Momma\n\nDepartment of Linguistics\n\n+ more\n\nThere are many things we are not particularly good at, but one thing we are relatively good at is producing novel and structurally complex sentences that are more or less consistent with our grammatical knowledge. However, this impressive ability is notoriously hard to study scientifically, and existing theories of sentence production are not very good at capturing how speakers assemble structurally complex sentences that involve syntactically interesting phenomena, such as filler-gap dependencies. In this talk, I attempt to fill this gap. Based on various findings from our lab, I advance a theory of structure building in speaking based on a formal grammatical theory. This model is able to capture various syntactic phenomena. It also makes novel counter-intuitive predictions about one of the most well-studied phenomena in sentence production: structural priming. I present experimental evidence confirming those predictions.\n\n2022-11-15\n\nDirty pigs & Stubborn mules:\n\nPulling the curtain back on sign language and gesture\n\nBrandon Scates\n\nThe Underground, Brandon Scates and Bonnie Gough\n\n+ more\n\nThe current landscape of sign language research has not shed light on the magic of animal signs in visual languages. By a series of fortunate events, we now have the unique opportunity to explore the intricacies of this little-known subfield of sign language research. Following a brief history of the citation forms of animal signs and their effect on current usage, we demonstrate the database we have developed to highlight the variation and lack of variation in the American signing community.\n\nCross referencing of other global sign languages illustrates several ways that natural intuitions of the human mind led to the production of gestural forms from which the lexicon of many sign languages derive. While the data are not abundant, cross-linguistic comparisons open a window into the foundational understanding of human gesture practices.\n\n2022-11-08\n\nDo Large Language Models Know What Humans Know?\n\nSean Trott\n\nUC San Diego, Department of Cognitive Science & Computational Social Science\n\n+ more\n\nHumans can attribute mental states to others, a capacity known as Theory of Mind. However, it is unknown to what extent this ability results from an innate biological endowment or from experience accrued through child development, particularly exposure to language describing others’ mental states. One way to test the viability of the language exposure hypothesis is to assess whether models exposed to large quantities of human language (i.e., \"large language models\", or LLMs) develop evidence of Theory of Mind. We assessed GPT-3 (an LLM) on several Theory of Mind tasks, then compared the performance of GPT-3 to human performance on the same task. Both humans and GPT-3 displayed sensitivity to mental states in written text, though in two of the three tasks tested, GPT-3 did not perform as well as the humans. I conclude by discussing the implications for work on Theory of Mind as well as an empirical science of LLM capacities.\n\n2022-05-31\n\n(Re)activating the perception of space in language\n\nAlper Kumcu\n\nHacettepe University in Ankara, Turkey\n\n+ more\n\nGrounded-embodied views of language assert that language in the mind is not detached from perception in contrast to traditional, symbolic approaches. In support of this view, growing evidence shows that perception plays an important role in language processing. In particular, “sensorimotor simulation” (i.e., activations and re-activations of sensorimotor experiences triggered by linguistic stimuli) has important repercussions in several language-related tasks from semantic judgement to verbal memory. Throughout this talk, I will discuss a series of experiments in which we investigate how the perception of space can modulate how well we remember single words even though space is neither relevant nor necessary for successful retrieval. I will also address some methodological issues concerning norming words based on lexicosemantic and sensory variables. These studies overall corroborate the evidence that language is represented with sensorimotor experiences in mind, which, in turn, has certain consequences on language operations. I will discuss the results in the framework of grounded-embodied and extended views on memory and language.\n\nBio\n\nI am a language researcher at Hacettepe University in Ankara, Turkey. I received my PhD from the School of Psychology at the University of Birmingham in 2019 with the dissertation titled “Looking for language in space: Spatial simulations in memory for language” under the supervision of Dr Robin Thompson. My research explores the interaction between language, perception and memory through behavioural methods, eye movements, and more recently, corpus-based investigations and with a crosslinguistic perspective. Most of my work can be accessed at https://alperkumcu.github.io/.\n\nRelated papers\n\nKumcu, A., & Thompson, R. L. (2021). Remembering spatial words: Sensorimotor simulation affects verbal recognition memory. Quarterly Journal of Experimental Psychology. https://doi.org/10.1177/17470218211059011\n\nKumcu, A., & Thompson, R. L. (2020). Less imageable words lead to more looks to blank locations during memory retrieval. Psychological Research, 84, 667–684. https://doi.org/10.1007/s00426-018-1084-6\n\n2022-05-24\n\nVowel harmony functions, complexity, and interaction\n\nEric Baković\n\nUC San Diego, Department of Linguistics (joint work with Eric Meinhardt, Anna Mai, and Adam McCollum)\n\n+ more\n\nRecent work in formal language theory (Heinz & Lai 2013, Chandlee 2014, Jardine 2016, Heinz 2018, among many others) has aimed to classify phonological patterns in terms of the computational complexity of the functions required to express those patterns. Much attention has been focused on the significant boundary between two classes of functions: the non-deterministic functions, at the outer edge of the class of functions that can be described with finite-state transducers, and the more restrictive weakly deterministic functions, first identified and defined by Heinz & Lai (2013). The distinction between these two classes of functions is significant because it has been claimed that all of phonology (Heinz 2011), or at least all of segmental (= non-tonal) phonology (Jardine 2016), is subregular, meaning at most weakly deterministic.\n\nI have three goals in this talk within this context. The first goal is to illustrate distinctions among relevant classes of functions via the analysis of vowel harmony patterns from four languages (Turkish, Maasai, Tutrugbu, and Turkana). The second goal is to show that non-deterministic segmental phonological patterns do indeed exist, given the vowel harmony patterns of Tutrugbu and Turkana. The third goal is to provide a definition of weakly deterministic functions based on a notion of interaction familiar from ordering in rule-based phonology that -- unlike Heinz & Lai’s (2013) definition, which ours subsumes -- properly classifies the Turkana pattern as non-deterministic.\n\n2022-05-10\n\nThe neural response to speech is language specific and irreducible to speech acoustics\n\nAnna Mai\n\nUniversity of California, San Diego, Department of Linguistics\n\n+ more\n\nSpoken language comprehension requires the abstraction of linguistic information from the acoustic speech signal. Here we investigate the transition between auditory and linguistic processing of speech in the brain. Intracranial electroencephalography (EEG) was recorded while participants listened to conversational English speech. Through structured comparisons of the neural response to sounds in allophonic relationships with one another, sites that dissociated phonemic identity from acoustic similarity were identified. Mixed effects and Maximum Noise Entropy (MNE) models were then fit to account for the unique contributions of categorical phonemic information and spectrographic information to the neural response to speech. In lower frequency bands, phonemic category information was found to explain a greater proportion of the neural response variance than spectrographic information, and across all frequency bands, inclusion of stimulus covariance structure increased model prediction accuracy only when categorical phonemic information was available. Moreover, phonemic label information conferred no benefit to model fit when participants listened to speech in an unfamiliar language (Catalan). Thus, neural responses associated with categorical phonemic information are language specific and irreducible to speech acoustics.\n\n2022-04-12\n\nVisualizing lexical retrieval during speech production with ECoG\n\nAdam Morgan\n\nNYU Langone, Department of Neurology\n\n+ more\n\nDuring speech, humans retrieve a target word from among the 10s of 1000s of words in their mental lexicon quickly and with apparent ease. But this process involves many complex representations, transformations, and computations, which scientists are only beginning to understand at the neural level. Here, we employ direct neural recordings (ECoG) in awake neurosurgical patients to elucidate the neural instantiations of words’ (1) activation and (2) discrete stages of representation (conceptual, lemma, phonological, articulatory; Indefrey, 2011).\n\nFive neurosurgery patients repeatedly produced 6 nouns (dog, ninja, etc.) in a picture naming block while electrical potentials were measured directly from cortex. Subsequently, patients described depicted scenes involving the same 6 nouns engaged in transitive actions (e.g. “The dog tickled the ninja”).\n\nWe were able to predict above chance (p<0.05, permutation, accuracy ~=22%) which of the 6 nouns a subject was about to produce in the ~600ms leading to articulation using cross-validated multi-class classification. Accuracy increased leading up to production onset and then decreased, suggesting that the classifiers capture a neural process akin to lexical activation rather than signatures of articulatory processing (or early visual features which were removed from analysis). We tested generalizability by applying the same trained classifier to nouns produced in sentences, showing above-chance accuracy for the first noun in the sentence.\n\nNext, to test for discrete neural states corresponding to lexical stages, we employed a temporal generalizability approach: we trained classifiers on each time sample, then tested each of these on held-out trials, again from each time sample (following King & Dehaene, 2014; Gwilliams et al., 2020). In contrast with most prior approaches, which manipulate lexical features (e.g., animacy, phonology) and look for resulting differences in neural data, our data-driven approach identifies stable neural states during lexical retrieval without making assumptions about what these states encode or when/where they should appear in the brain. Results provide direct evidence for 2-4 distinct lexical states likely supporting conceptual, lemma, phonological, and articulatory representations. This is an important step towards linking neural codes to psycholinguistic constructs.\n\n2022-02-15\n\nListening Challenges of Multi-Talker Environments: Behavioral and Neural Mechanisms of Focused and Distributed Attention\n\nElana Zion Golumbic\n\nThe Gonda Center for Brain Research, Bar Ilan University, Israel\n\n+ more\n\nA primary challenge posed by many real-life settings is that of appropriately allocating attention to a desired speaker in noisy, multi-talker situations. Successfully accomplishing this feat depends on many factors, related both to the acoustic properties of the competing speech as well as on the listener's behavioral goals. In this talk I will discuss recent data from our lab, where we study the cognitive and neural mechanisms underlying the ability (and the challenges) of allocating attentional resources among competing speakers, under naturalistic conditions. We will discuss the effects of acoustic load, the type of attention required, and the employment of different ‘listening strategies’, as well as the factors contributing to individual differences in attentional abilities. We will also discuss the implication of our findings on the classic debate between ‘early’ and ‘late’ selection models of attention and what we have learned about the capacity for parallel processing of concurrent speech and potential ‘processing bottleneck’.\n\n2022-02-08\n\nThe Protolanguage Spectrum and Beyond\n\nMichael Arbib\n\nThis talk reports on a work in progress, the writing of a chapter for a book of essays in honor of Derek Bickerton: On the Evolution, Acquisition and Development of Syntax (Dany Adone & Astrid Gramatke, Eds.) to be published by Cambridge University Press.\n\n+ more\n\nRejecting Chomsky’s rejection of the notion of protolanguage, Section 1 briefly presents contrasting views on how protolanguage emerged in distinction from other animal communication systems, noting competing views on the roles of hand and voice.\n\nSection 2 contrasts two approaches to the transition to language, Bickerton’s compositionality approach and the contrasting holophrastic approach, but then suggests how elements of both accounts may have come together in yielding language. However, I will argue against Bickerton and Chomsky’s assertion that the Merge is the key to the transition, or that the transition was all-or-none. Instead, I recall the notion of a protolanguage spectrum, and enrich it by introducing the concept of a micro-protolanguage.\n\nThe rest of the paper explores to what extent the processes that supported the transition from protolanguage to language also support processes of language change, including grammaticalization and creolization.\n\nSection 3 summarizes some elements of the Heine-Kouteva theory of grammaticalization and seeks to assess whether, rather than depending on pre-existence of a language to grammaticalize, it can be recast in terms of more general social learning mechanisms that apply to action and cognition, not just within language, and to protolanguages.\n\nSection 4 then seeks a similar approach to pidgins and creoles. I suggest that the line is blurred between Bickerton’s Language Bioprogram Hypothesis and at least some versions of Chomsky’s ever-changing notion of Universal Grammar. I will thus suggest that we can approach the topic while avoiding any notion of a Universal Grammar if we replace the Language Bioprogram Hypothesis with a more general Bioprogram Hypothesis that exploits mechanisms in place in (proto)humans even at the time of the emergence of protolanguage.\n\n2022-02-01\n\nOn the properties of null subjects in Sign Languages: the case of French Sign Language (LSF)\n\nAngélique Jaber\n\nLLF, Paris, France; CNRS, Paris, France; Université de Paris, Paris, France; IJN, Paris, France; DEC, Paris, France; ENS, Paris, France, EHESS, Paris, France; PSL, Paris, France\n\nCaterina Donati\n\nLLF, Paris, France; CNRS, Paris, France; Université de Paris, Paris, France\n\nCarlo Geraci\n\nIJN, Paris, France; DEC, Paris, France; ENS, Paris, France; EHESS, Paris, France; CNRS, Paris, France; PSL, Paris, France\n\n+ more\n\nThe typology of subject omission in simple declarative sentences ranges from languages that simply do not allow it like English and French to languages that allow it as long as a minimum degree of topicality is guaranteed like Chinese and Japanese. In between, there are various languages in which subject omission is licensed, for example by rich agreement like in Italian and Spanish, or by a particular set of grammatical features like first and second person in Finnish, or tense like in Hebrew. In other languages subject omission is only limited to expletive sentences like in German. This rich typology observed in spoken languages is also attested across sign languages, with one important exception: there is no known sign language disallowing subject omission categorically. The goals of this presentation are twofold: first, we apply syntactic and semantic tests to assess the boundaries of subject omission in French Sign Language and characterize it within the typology; second, we discuss in light of some particular aspects of grammars in the visual modality this apparent anomaly of sign languages.\n\n2022-01-18\n\nConstraints on sound structure at multiple levels of analysis\n\nMatt Goldrick\n\nNorthwestern University (at UCSD for Jan. + Feb.)\n\n+ more\n\nAlthough understanding the mind/brain has been argued to require developing theories at multiple levels of analysis (Marr, 1982, et seq.), in practice specific research projects (my own included!) typically privilege explanations at a particular level of description. I'll discuss two recent projects from my lab that critically relied on understanding sound structure at different levels of explanation. The structured variation of external sandhi arises in part due to the nature of the psychological mechanisms that compute word forms in production. Psycholinguistic studies of implicit learning of phonotactic constraints yielded puzzles that can only be resolved by considering constraints on phonotactic grammars. Insights like these suggest researchers in the cognitive science of language may benefit from more careful attention to insights at multiple levels of analysis.\n\n2021-11-30\n\nElectrophysiological Insights into Figurative Language Comprehension\n\nSeana Coulson\n\nDepartment of Cognitive Science, UC San Diego\n\n+ more\n\nIn this talk I'll describe a number of studies using scalp-recorded ERPs to address how people understand metaphoric meanings as well as literal meanings that rely on other kinds of cognitive mappings.\n\n2021-11-02\n\nWorld Knowledge Influences Pronoun Resolution both Online and Offline\n\nCameron Jones\n\nDepartment of Cognitive Science, UC San Diego\n\n+ more\n\nUnderstanding language necessarily involves connecting linguistic input to knowledge about the world, but does world knowledge play a primarily elaborative role (fleshing out details of the core message) or can it influence the core propositional interpretation of the sentence itself. Across 3 experiments, we found evidence that knowledge about the physical world influences pronoun comprehension both offline (using comprehension questions) and online (using a self-paced reading paradigm). The results are consistent with the theory that non-linguistic world knowledge plays a constitutive role in language comprehension. An alternative explanation is that these decisions were driven instead by distributional word knowledge. We tested this by including surface statistics-based predictions of neural language models in regressions and found that physical plausibility explained variance on top of the neural language model predictions. This indicates that at least part of comprehenders' pronoun resolution judgments comes from knowledge about the world and not the word.\n\n2021-05-25\n\nAcquisition of plural classifier constructions in ASL: different learners, different errors\n\nNina Semushina\n\nDepartment of Linguistics, UC San Diego\n\n+ more\n\nClassifiers have been attested in most of the sign languages studied to date. They are morphologically complex predicates, where the handshape represents the class that the entity belongs to and may be combined with the depiction of the location/movement. Classifiers are used to mark plural number in ASL and other sign languages. Classifier handshapes are iconic, yet it has been shown that this iconicity does not help young children to learn classifiers fast and without errors.\n\nWhat happens when adult learners acquire the language? What errors can adults make and still be understood? In our talk, we will discuss three experiments investigating the use of plural classifiers predicates in American Sign Language (ASL) by deaf native ASL signers (L1), by deaf late first language learners (LL1), and by hearing second language learners (L2).\n\n2021-04-06\n\nAre word senses categorical or continuous?\n\nSean Trott\n\nDepartment of Cognitive Science, UC San Diego\n\n+ more\n\nTraditionally, the mental lexicon is likened to a dictionary, with ambiguous words mapping onto multiple \"entries\". But other researchers have argued that this model fails to capture more dynamic aspects of word meaning. In an alternative model, words are viewed as cues to a continuous state-space––such that \"senses\" simply constitute regular patterns or \"clusters\" in a word's context of use. Is there evidence for the psychological reality of discrete sense categories above and beyond these statistical regularities? Or is the notion of word senses primarily a convenient abstraction? In this talk, we attempt to answer these questions using a primed sensibility judgment paradigm.\n\n2021-03-30\n\nRethinking Bilingual Development and Disorder\n\nElizabeth D. Peña\n\nSchool of Education, University of California, Irvine\n\n+ more\n\nIn the U.S. one in five children has exposure to another language in their home or community. As such, patterns of language acquisition can be highly variable. An educational challenge in this population is how to distinguish between typical and atypical performance in L1 and L2 use. Comparison of bilingual children’s language to monolinguals may contribute to high rates of misidentification of DLD. On the other hand, assumptions of a “normal” bilingual delay may contribute to documented delays in identification and intervention. In this talk I will present data examining 1) whether bilingual children are at elevated risk for developmental language disorder (DLD); 2) how we can combine L1 and L2 performance to increase diagnostic accuracy for determining DLD in bilinguals; and 3) the nature of the “bilingual delay” using a person-based vs. a variable-based approach.\n\n2021-03-09\n\nPatterns and constraints in numerical symbols and words: a reckoning\n\nStephen Chrisomalis\n\nWayne State University\n\n+ more\n\nAmong the various modalities through which humans represent numbers, the lexical numeral systems associated with the world's languages and the symbolic numerical notations mainly associated with writing systems, are the two that have attracted the most scholarly attention. Drawing on work from my book, Reckonings: Numerals, Cognition, and History (Chrisomalis 2020), I show that these two sets of representations are both subject to cognitive constraints that structure their cross-cultural variability. The gulf between number systems of both types that are imaginable in principle, and those that are used in practice, is enormous. But number words and number symbols have radically different properties because they depend on different modalities and serve different functions. The constraints on one cannot be derived from the constraints on the other. Rather than seeing numerical notations as derivative of lexical numerals, these two systems interrelate in complex ways. Studies of numerical cognition must take account of the variability across these two modalities if they aim to more completely analyze the human number sense.\n\n2021-03-02\n\nHand-me-down syntax: Towards a top-down, phase-theoretic model of sentence generation\n\nDoug Merchant\n\nSan Diego State University\n\n+ more\n\nBeginning with the Minimalist Program (Chomsky, 1993, et seq.), syntactic derivations are viewed as step-by-step procedures through which incremental applications of Merge combine words into sentences. Such derivations are widely (if tacitly) assumed to proceed from the bottom-up, i.e., beginning with the most deeply embedded parts of a structure. In this talk, I contend that this assumption is an unnecessary artefact of older representational views of the grammar, and that its retention has stymied attempts to reconcile linguistic models of structure- building with psycholinguistic models of production. Having (hopefully) established this, I then survey a variety of arguments for top-down structure-building, with a focus on theory-external arguments such as the nature of temporary memory capacity (Cowan, 2001, 2015; Baddeley et al., 1987). The theory-internal evidence is somewhat scanter, but includes conflicting constituency in English and Japanese (Phillips, 1996, 2003; Chesi, 2007, 2015), as well as WH- movement in multiple questions in Bulgarian (Richards, 1999).\n\nNext, I consider top-down structure building in the context of phase theory (Chomsky, 2001, et seq.), in which derivations are divided into partially overlapping stages that are cyclically transferred to the interfaces. In such a system, a clause consists of a functional / discourse layer (CP/TP) above a lexical / propositional layer (vP/VP); these layers, which I argue are qualitatively different and key to understanding the dynamics of sentence generation, correlate directly with what Chomsky (2007, 2008) calls the duality of semantics. I then demonstrate how one might implement cyclic SPELLOUT in a top-down model of the grammar, one which (for independent reasons) also incorporates postsyntactic lexical insertion, presenting a full derivation. Finally, I briefly consider how island phenomena might be construed in a top- down system, and conclude.\n\n2021-02-23\n\nMapping the semantic structure of American Sign Language lexicon using the ASL-LEX Database\n\nZed Sevcikova Sehyr\n\nLaboratory for Language and Cognitive Neuroscience, San Diego State University\n\n+ more\n\nOne of the central assumptions about lexical organization is that the relationship between form and meaning is largely arbitrary. However, this assumption underrepresents characteristic traits of sign languages which contain varying degrees of iconic form-meaning mappings. What role does iconicity play in shaping the lexicon? The study builds on the large-scale data collection methods and visualization techniques that we developed for the ASL-LEX database (http://asl-lex.org/) in which phonological neighborhoods are displayed. The goal of this new study is to visualize and characterize the semantic structure of the ASL lexicon in order to identify areas of iconic and non-iconic systematicity. To accomplish this, we will use information derived from semantic association data from a group of deaf ASL signers, and techniques from network science. In this paradigm, deaf signers are given a cue sign (e.g., CAT) and are asked to produce three signs that immediately come to mind (e.g., DOG, PET, PLAY). A group of trained signers will tag these responses with glosses from the ASL-LEX database. From the data, we will then construct a semantic network graph where nodes represent signs, and edges connecting the nodes are weighted based on the strength of association. To test the feasibility of this paradigm, we conducted a small-scale pilot study with 80 ASL cue signs balanced for frequency and iconicity. Three ASL signers generated three semantic associates for each cue sign in ASL, resulting in a corpus 520 tagged signs. The preliminary results revealed that 335 signs (64%) were connected to at least one other sign in the lexicon with 411 edges, suggesting good overall interconnectivity among signs. Sign pairs like TEACH-STUDENT and TEACH-SCHOOL were strong semantic associates, while TEACH-LEARN were a weakly related pair in the lexicon. Some semantic neighborhoods overlapped with phonological neighborhoods, e.g., iconic signs BELIEVE, HOPE, THINK and KNOW were semantic neighbors which overlap by location (the head) and handshape. The completed study will be the largest dataset of its kind (with ~100,000 lexically-tagged ASL videos) and a valuable resource for further research or sign recognition technologies.\n\n2021-02-16\n\nBilingual language processing: Interactions between lexical retrieval and phonetic production\n\nMaria Gavino\n\nDepartment of Linguistics, Northwestern University\n\n+ more\n\nWhat is the relationship between lexical retrieval and phonetic production in bilingual language processing? Various factors related to bilingual language processing affect bilingual’s selection of context-appropriate words and speech sounds. One factor is whether bilinguals are using one (single context) or both (mixed context) languages. Increased language selection difficulty in mixed contexts (especially when the previous word is in a different language than the target word; i.e., switch context) slow down retrieval and increase accentedness. Another factor is whether a word has two highly distinct forms (non-cognates) or highly similar forms (cognates) for a concept in both languages. Increased cross language activation for cognates facilitate retrieval, but increase accentedness. In this project, 18 Spanish-English bilinguals named pictures of cognate and non-cognate words in single and mixed contexts in Spanish and English. Reaction time, voiced onset time, and vowel formants were analyzed. Results show that there are cognate facilitation effects, mixing, and switching costs for retrieval, but only consistent mixing costs for accentedness. The dissociation between these effects during lexical retrieval and phonetic production suggests continuing interactions between them after the initiation of the response.\n\n2021-02-09\n\nBeyond intentional proximity: Exploring self-motion to characterize effortful conversation\n\nCarson G. Miller Rigoli\n\nDepartment of Cognitive Science, UC San Diego\n\n+ more\n\nAs anyone who has attended an awkward reunion can attest, conversation is effortful and fatiguing. This fact has recently taken greater precedence in the speech and hearing sciences as researchers increasingly recognize the importance of understanding speech in the context of naturalistic conversation. In this presentation, I will provide a summary of recent work in ergonomics, motor control, and the speech and hearing sciences that suggests that the maintenance of effort in cognitive and perceptual tasks is intertwined with the control of our bodies. Self-motion, and postural variability in particular, is one area that offers to broaden our understanding of maintenance in conversation. I will present initial exploratory work which suggests that movement variability can be used as a marker of effort in naturalistic conversation. I will conclude by discussing why an understanding of the role of self-motion in conversation is particularly important in light of widespread adoption of video call technologies.\n\n2020-12-08\n\nReal-time brain response to an artificial language about time\n\nSeana Coulson\n\nDepartment of Cognitive Science, UC San Diego\n\nJoint work with Tessa Verhoef, Tyler Marghetis, and Esther Walker\n\n+ more\n\nHere we consider the connection between the cultural evolution of a language and the rapid processing response to that language in the brains of individual learners. In an artificial language learning task, participants were asked to communicate temporal concepts such as “day,” “year,” “before,” and “after” using movements of a cursor along a vertical bar. Via social interaction, dyads achieved above-chance performance on this communication task by exploiting an early bias to use the spatial extent of movement on the vertical bar to convey temporal duration. A later emerging strategy involved systematic mappings between the spatial location of the cursor and the temporal direction, i.e. past versus future. To examine how linguistic properties of a semiotic system relate to the demands of transmitting a language to a new generation, the language developed by one successful dyad in the communication game was used to seed an iterated language learning task. Through simulated cultural evolution, participants produced a ‘language’ with enough structure to convey compositional concepts such as ‘year before’. In the present study, EEG was recorded as healthy adults engaged in a guessing game to learn one of these emergent artificial languages. Results indicate a neural correlate of the cognitive bias to map spatial extent onto temporal duration, and suggest a dynamic dimension to iconicity.\n\n2020-12-01\n\nA meta-analysis of task-based differences in bilingual L1 and L2 language networks\n\nLindy Comstock\n\nDepartment of Psychology, UC San Diego\n\n+ more\n\nThe neural representation of a second language in bilinguals has been shown to be modulated by factors such as proficiency, age of acquisition, and frequency of use. Today, papers investigating bilingual L1 and L2 language networks number in the hundreds. Yet conclusive findings from fMRI studies as to whether the neural resources recruited by L1 and L2 can be reliably differentiated is complicated by small sample-sizes and the wide array of experimental task designs implemented in the field. One method to increase the statistical power and generalizability of findings in individual fMRI studies is to conduct a meta-analysis that averages across multiple studies. There are currently no less than seven meta-analyses devoted to various aspects of bilingual language use (Cargnelutti, Tomasino, & Fabbro, 2019; Indefrey, 2006; Liu & Cao, 2016; Luk, Green, Abutalebi, & Grady, 2012; Sebastian, Laird & Kiran, 2011; Sulpizio, Del Maschio, Fedeli, & Abutalebi, 2019; Tagarelli, Shattuck, Turkeltaub, & Ullman, 2019). However, the specific nature of many research questions in bilingual neuroimaging research and a relatively early shift in the literature away from simple language tasks to other paradigms such as language switching have resulted in a multiplicity of research designs that defy easy categorization or comparison. Thus, despite the proliferation of bilingual neuroimaging studies, the number of these that utilize the same task paradigm remain quite small and insufficient to investigate the intersection of multiple factors like AoA, proficiency, degree of exposure, and task. This talk will critique previous meta-analyses devoted to L1 and L2 processing and present the results of the first meta-analysis to group studies strictly by task with the goal to illustrate how task differences may contribute to previous results.\n\n2020-11-24\n\nDiscussion of Online Data Collection\n\nHosted by: Vic Ferreira\n\nDepartment of Psychology, UC San Diego\n\n+ more\n\nThis week we are doing things a little differently! With COVID, once again, getting worse and not better, many of us have been trying to move to online data collection for our experimental data collection (or alter our data collection to behavioral online experiments). Therefore, to help each other in this transition, we will be having a group discussion about online data collection methodologies. Please think about any questions you have regarding online data collection and any feedback or aid you could give to others.\n\nPlease fill out the Google form you received from the CRL Talks mailing list with questions you have or topics for discussion so we can scaffold the discussion!\n\nAlso, feel free to come and just listen. Hopefully, we will be sharing a lot of expertise and discussing the answer to some pressing questions, so this will definitely be one to not miss!\n\n2020-11-17\n\nWhy do human languages have homophones?\n\nSean Trott\n\nDepartment of Cognitive Science, UC San Diego\n\n+ more\n\nHuman languages are replete with ambiguity. This is most evident in homophony––where two or more words sound the same, but carry distinct meanings. For example, the wordform “bark” can denote either the sound produced by a dog or the protective outer sheath of a tree trunk. Why would a system evolved for efficient, effective communication display rampant ambiguity? Some accounts argue that ambiguity is actually a design feature of human communication systems, allowing languages to recycle their most optimal wordforms (those which are short, frequent, and phonotactically well-formed) for multiple meanings. We test this claim by constructing five series of artificial lexica matched for the phonotactics and distribution of word lengths found in five real languages (English, German, Dutch, French, and Japanese), and comparing both the quantity and concentration of homophony across the real and artificial lexica.\n\nSurprisingly, we find that the artificial lexica exhibit higher upper-bounds on homophony than their real counterparts, and that homophony is even more likely to be found among short, phonotactically plausible wordforms in the artificial than in the real lexica. These results suggest that homophony in real languages is not directly selected for, but rather, that it emerges as a natural consequence of other features of a language. In fact, homophony may even be selected against in real languages, producing lexica that better conform to other requirements of humans who need to use them.\n\nWe then ask whether the same is true of polysemy (in English), a form of lexical ambiguity in which the same wordform has two or more related meanings. Unlike homophony, we find that at least in English, wordforms are more polysemous than one would expect simply on account of their phonotactics and length. Combined, our findings suggest that these forms of ambiguity––homophony and polysemy––may face distinct selection pressures.\n\n2020-11-10\n\nConceptual combination and the emergence of privativity\n\nJoshua Martin\n\nDepartment of Linguistics, Harvard University\n\n+ more\n\nPrivativity is a linguistic phenomenon in which an instance of modification results in an output (usually, a modified noun phrase) disjoint from the input (the bare noun), e.g. the set denoted by fake door contains no members of the set denoted by door. So-called privative adjectives (fake, counterfeit, mock, etc.) are those which uniformly license this inference; that is, for a privative adjective A and any noun N, X is an AN -> X is not an N. This is contrasted with the standard intersective inference licensed by more basic property adjectives, where X is an AN -> X is an N. In this talk, I will introduce the topic of privative composition as a problem for current theories of formal semantics, which are unable to successfully predict the result of such modification, and argue that a successful theory of privative composition requires reference to enriched lexical meanings reflecting the structure of concepts. I will present some preliminary experimental data on variation in privative meanings, which motivate a reframing of the problem as one of 'privative compositionality', rather than of 'privative adjectives', divorcing the notion from a specific lexical class and treating it rather as an emergent phenomenon of the compositional process. A lexically enriched theory which treats privativity as a contingent byproduct of conceptual combination, rather than a grammaticalized process, I argue, is both a better account of the empirical picture and raises interesting questions about how conceptual structure is realized in semantic composition. Privativity is shown to have distinct syntactic reflexes, as well, and so serves as a case study for the interaction of conceptual meaning, semantic composition, and syntactic structure.\n\n2020-10-27\n\nMindfulness meditation engages a novel brain-based pain modulatory pathway\n\nFadel Zeidan\n\nDepartment of Anesthesiology, UC San Diego\n\n+ more\n\nMindfulness-based practices reliably reduce pain. Recent findings demonstrate that mindfulness-induced analgesia engages multiple, distinct mechanisms. In two separate neuroimaging studies, we found that mindfulness-based analgesia was associated with greater a) thalamic deactivation and b) prefrontal cortical (PFC) activation. We also discovered that mindfulness does not engage classical, endogenous opioidergic systems to reduce pain. We have developed a working hypothesis postulating that mindfulness-induced shifts in executive attention (non-reactive attention to the breath) facilitate pain-relief by engaging PFC-driven inhibition of the thalamus to reduce the elaboration of nociceptive information in somatosensory cortices. This presentation will provide a comprehensive delineation of the psychological, endogenous, autonomic, and neural mechanisms supporting mindfulness-based analgesia.\n\n2020-10-20\n\nMemory for stimulus sequences\n\nStefano Ghirlanda\n\nBrooklyn College and Stockholm University\n\n+ more\n\nHumans stand out among animals for their unique capacities in domains such as language, culture and imitation, but it is difficult to identify basic cognitive elements that are specifically human. In this talk, I will present evidence that, compared to humans, non-human animals have a more limited capacity to discriminate ordered sequences of stimuli. Collating data from over 100 experiments on stimulus sequence discrimination, I will show that animals commit pervasive and systematic errors, such as confusing a red–green sequence of lights with green–red and green–green sequences. These errors can persist after thousands of learning trials. Overall, the data are consistent with the assumption that non-human species represent stimulus sequences as unstructured collections of memory traces. This representation carries only approximate information about stimulus duration, recency, order, and frequency, yet it predicts non-human performance accurately. Lastly, I will present ongoing efforts to test sequence discrimination abilities in bonobos.\n\n2020-10-13\n\nThe Language Organ: Architecture and Development\n\nWilliam Matchin\n\nDepartment of Communication Sciences and Disorders, University of Southern Carolina\n\n+ more\n\nThe concepts of “the language organ” and “the language acquisition device” advanced by Chomsky and others in the tradition of Generative Grammar are controversial in part because their neurobiological instantiation is unclear. Here I address this by reviewing recent evidence from brain imaging and lesion-symptom mapping in aphasia. I propose a model for how the language organ develops in the brain in the context of the recent thesis of the architecture of the human language faculty and its evolution by Berwick & Chomsky, 2016. In this proposal, an innate syntactic computational system combines with domain-general sequencing systems, which become gradually specialized for speech externalization during language development.\n\n2020-06-02\n\nSelective Activation of Semantic Features\n\nAlyssa Truman\n\nDepartment of Cognitive Science, University of California, San Diego\n\n+ more\n\nThe question of whether activation of our conceptual system is fixed or flexible has been long debated. Spreading activation suggests that upon encountering a concept we activate other concepts that are related to it. However, this account tells us nothing about the dynamic nature of our semantic system. Do we activate all the relevant information about a concept, and the words related to it, each and every time we retrieve a concept? Or do we activate only the task or context relevant information about a concept, and the words related to it? In our study we wanted to assess what type of information is activated dynamically by looking at the modulation of ERPs for 3 types of word pairs: unrelated word pairs, related word pairs that share the feature that the subjects were required to attended to by task, and related word pairs that share a feature that subjects were not required to attend to. We found ERP modulation of the target word for attended-related vs unrelated word pairs in 3 time windows: 50-150ms, 350-450ms and 500-700ms. We did not find a statistically significant differences between unattended-related and unrelated word pairs in either task, however, we saw that the mean amplitudes of feature unattended target words were more positive than the mean amplitudes of unrelated target words in N400 time window. Our findings, taken together, point to a flexible view of semantic activation that is not automatic and is affected by context, suggesting strong top down activation of (task) relevant features.\n\n2020-05-26\n\nLearning Structural Alternations: What guides learners’ generalization?\n\nSin Hang Lau, Shota Momma, and Victor S. Ferreira\n\nDepartment of Psychology, University of California, San Diego\n\n+ more\n\nSpeakers can sometimes flexibly choose between multiple structures to describe the same event (e.g., prepositional vs. double object datives in English), but this flexibility is sometimes constrained (e.g., using only PD but not DO with “donate”). Additionally, typologically different languages have different degrees of flexibility. For example, languages that allow scrambling (e.g., Korean) have more flexible word orders than those that do not (e.g., English). The question we address here is whether learning these language-general patterns comes from simple exposure, versus being “helped” by some sort of internal bias.\n\nIn this talk, I present work that examines whether learners generalize structural alternations when learning a typologically different grammar, and if so, what guides their generalization. Three miniature novel-language learning experiments tested usage-based/statistical accounts against an internal bias account in both production and comprehension. Usage-based/statistical accounts predict that learners should largely constrain their production and acceptability of alternations to the verb that they learned the structures with and show limited generalization to other verbs. In contrast, an internal bias account predicts that learners who are exposed to scrambling should show a more liberal generalization pattern than those who are not, if learners indeed have tacit knowledge about the typologically relevant evidence that scrambling signals flexibility. The critical manipulation across our experiments was that half of the participants learned a grammar with no evidence of scrambling, whereas the other half saw scrambling. Our results revealed that the two groups showed different generalization patterns in both production and comprehension. Participants who saw no evidence of scrambling produced and accepted alternations with the verb that they learned the structures with, but to a smaller extent with novel verbs; whereas participants who saw scrambling produced and accepted alternations equally across all verbs. Our results suggest that learners do not merely track statistical patterns in the input but also use internal linguistically sophisticated biases to generalize structural alternations, supporting an internal bias account.\n\n2020-05-19\n\nDoes Inhibition Help Bilinguals Switch Languages?\n\nEvidence from Young and Older Bilinguals\n\nTamar H. Gollan, PhD\n\nProfessor of Psychiatry, UCSD\n\n+ more\n\nPeople love to invoke inhibition to explain all kinds of human behavior, but they also fight frequently and furiously over what inhibition is, how to measure it, and if it’s impaired in aging or not. This talk will briefly review some evidence for and against the Inhibitory Deficit Hypothesis, and then will explore reversed language dominance – a uniquely powerful signature of inhibition that can be found in bilinguals when they name pictures and are cued to switch back and forth between languages. Surprisingly, this sometimes leads bilinguals to speak more slowly in the language that is usually more proficient (i.e., dominant). Although this is arguably one of the most striking demonstrations of inhibitory control, it has rarely been studied in aging bilinguals. I will present a study that Alena Stasenko, Dan Kleinman, and I designed to reveal if inhibition transfers from one set of pictures to a new set of pictures in cued language-switching, and if such inhibition is impaired in aging or not. The results revealed strong evidence for global inhibitory control of the dominant language, and that older bilinguals either can’t or simply don’t apply control in the same way. This paints a picture that features inhibition as a basic mechanism of bilingual language selection, but other evidence we have suggests not necessarily in a simple “better-inhibition = better-bilinguals” manner. While the notion of inhibitory control as a unitary construct has been challenged repeatedly in heated debates across multiple sub-literatures in the field, I’m gonna try to convince you that dispensing with it is premature as it continues to lead to fruitful lines of investigation.\n\n2020-05-12\n\nTowards a functional anatomy of language: a 20 year retrospective\n\nwith Gregory Hickok and David Poeppel\n\nmoderated by William Matchin\n\n+ more\n\nA discussion with Gregory Hickok and David Poeppel to celebrate the 20th anniversary of their dual-stream model hosted by William Matchin.\n\nThis talk is on YouTube: https://www.youtube.com/watch?v=6GgeLbhXeCg.\n\n2020-05-05\n\nLap or Lab: The cross-domain entrainment effects from pure tones to speech perception\n\nTzu-Han Zoe Cheng and Sarah C. Creel\n\nUCSD, Department of Cognitive Science\n\n+ more\n\nTemporal context influences how humans perceive the durations of acoustic events. Recently, entrainment, a hypothesized process in which internal oscillators synchronize with the external temporal sequences, was proposed to be the underlying mechanism of timing estimation at short time durations. However, this approach has predominantly been tested on simple, music-like acoustic stimuli (e.g. pure tones). This raises the question of whether the same temporal context mechanisms are operative in other, more complex acoustic domains such as speech sound perception, which some have claimed to operate in modular fashion. In the current study, we first demonstrated an entrainment effect in restricted time range in our Experiment 1. Based on this finding, we extended the paradigm to spoken language. In Experiment 2, we investigated if an entrainment effect from a series of pure tones could influence syllable categorization (i.e. /ba/ vs. /pa/) and word (i.e. lap vs. lab) categorization, which were created by varying the durations of the sound. Our findings suggest that entrainment from pure tone contexts may influence speech sound categorization. Data from these and ongoing studies has the potential to reveal a general mechanism of short-duration entrainment that can explain temporal context effects on timing perception in acoustically diverse domains.\n\n2020-04-28\n\nPerception of ATR vowel contrasts by Akan speakers reveals evidence of near-merger\n\nSharon Rose1\n\nMichael Obiri-Yeboah1\n\nand Sarah Creel2\n\n1 UCSD Linguistics\n\n2 UCSD Cognitive Science\n\n+ more\n\nVowel systems in many African languages show contrasts for Advanced Tongue Root (ATR), in which the tongue root is retracted or advanced. These systems have been widely studied from phonological perspectives as they typically exhibit vowel harmony, where words contain only advanced (+ATR) vowels or non-advanced (-ATR) vowels. They have also been subject to numerous acoustic and articulatory (X-ray, ultrasound) studies. Yet, very few perceptual studies of ATR distinctions by speakers of ATR languages have been conducted. This is despite anecdotal reports that some ATR vowel contrasts are hard to perceive (Casali 2017) and that certain ‘weaker’ -ATR vowels tend to merge and neutralize in these systems. In this study, we investigate perception of ATR contrasts by Akan speakers, assessing whether phonemic contrast or phonetic similarity influence perception of ATR distinctions.\n\nAkan (Kwa language of Ghana) has ten vowels divided into two sets: +ATR [i e o u ɜ] and -ATR [ɪ ɛ ɔ ʊ a]. Nine vowels are phonemically contrastive, whereas [ɜ] is non-contrastive and created via vowel harmony. Two hypotheses are examined. The phonological hypothesis predicts that participants will rely on phonemic contrast and perform well on phonemically contrastive vowel pairs, but not those with allophonic [ɜ], which only occurs preceding +ATR vowels due to vowel harmony. The acoustic hypothesis predicts that participants will rely on acoustic similarity, and highly similar vowels will pose perceptual difficulties. The similar acoustic vowels are [e]/[ɪ], [o]/[ʊ] pairs, vowels which differ in two phonological features, height and ATR, but are phonemically contrastive.\n\nTwo experiments were run with a total of 82 subjects in Ghana. Experiment 1 was an AX discrimination task with monosyllables. Results showed that participants had a >90% accuracy rate at detecting all ATR contrasts, including the allophonic [a]/[ɜ] pair, and a >80% accuracy rate on all Height contrasts (ex. i vs. e). However, pairs contrasting both Height and ATR had only a 30% accuracy rate (still above a false-alarm baseline of 8%). These results support the acoustic hypothesis. Nevertheless, there were some factors in the experiment that make these results difficult to interpret on their own. The use of monosyllables may have impacted perception, as the +ATR vowels [e o] are rare in monosyllables. Vowel harmony may also affect perception differently in longer words. Experiment 2 was an ABX discrimination task with bisyllables, using stimuli with identical vowels (gebe / gɛbɛ / gɛbɛ) or non-identical vowels (gɛbe / gɛbɛ / gɛbɛ). Results were similar to those in Experiment 1: For identical stimuli, acoustically similar Height-and-ATR-mismatched pairs were still poorly discriminated (58%, where 50% is chance) compared to other pairs (84%). The same discrimination rate was found for non-identical pairs (58%). This poor discrimination further impacted how subjects perceived non-identical disharmonic words, as words that violated vowel harmony (ex. gɛbe) were perceived similarly to those that had different height (ex. gibe). Overall, these results suggest that Akan exhibits near merger, the situation whereby speakers differentiate sounds in production, but report that they are ‘the same’ in perception tests.\n\n2020-04-21\n\nMeaning identification using linguistic context in children with diverse language experiences\n\nAlyson D. Abel\n\nAssistant Professor, SDSU and SDSU/UCSD JDP-LCD; Director, Language Learning Lab\n\n+ more\n\nWhen learning a new word, the learner has to identify both the word form and its meaning, map the form and meaning and, over time, integrate this information in the mental lexicon. Much of the word learning literature has focused on acquisition of the word form with less attention paid to the process of meaning identification. Given the importance of robust semantic knowledge on a word’s representation in the mental lexicon, the lack of focused research on meaning identification is a significant gap. Additionally, much of the word learning literature has examined younger children, in the infant through preschool range. Younger children often use information in their environment, such as a physical referent and explicit instruction or guidance from parents, to help them identify a new word’s meaning. On the other hand, school-age children and adults rely more heavily on the information in the linguistic context for meaning identification. For children with diverse language experiences, including children raised in low socioeconomic status (SES) environments, children who are bilingual, and children with language impairment, the process of using linguistic context to identify a word’s meaning may be affected. In this talk, I will introduce an experimental meaning identification task, where participants are introduced to unfamiliar word forms in sentences that vary in their support for identifying the unfamiliar word’s meaning. Behavioral performance is assessed by whether they can identify the word’s meaning EEG is collected during the meaning identification task with analyses focused on the N400 ERP component. Behavioral and ERP data will be presented from four groups of school-age children, three groups of children with typical language development (monolingual middle-SES children, children from low SES homes, and bilingual children) and children with specific language impairment. Findings will be interpreted within a framework of how different language experiences shape behavioral and neural indices of meaning identification.\n\n2020-04-14\n\nCost-free language switching in connected speech? Syntactic Position matters\n\nChuchu Li\n\nDepartment of Psychiatry, UC San Diego\n\n+ more\n\nBilinguals spontaneously switch languages when conversing with other bilinguals in real life, although numerous laboratory studies have revealed robust language switching costs even when switching is voluntary or predictable. One reason is that some words are more accessible in the other language, and accessibility-driven switches can be cost-free in isolated word production (e.g., single picture naming; Kleinman & Gollan, 2016). The present study examined whether accessibility-driven language switching is costly in connected speech. We measured bilinguals’ speech onset latency as well as the production duration of each word before the switch word to monitor when switch costs occur. Two experiments consistently showed that lexical accessibility-driven switching sometimes is cost-free in connected speech, but it depends on where switches occur. Words trained to be lexically accessible enabled cost-free switching when produced across phrases (e.g., The butterfly moved above the mesa), but switch-costs returned when bilinguals had to produce language switches within a phrase (e.g., The butterfly and the mesa). This is probably because phrase is the default speech planning unit in sentence production (Smith & Wheeldon, 1999; Martin et al., 2010). In contrast to isolated word production, when bilinguals produce connected speech, they select a default language, that is, most of words are produced in this language. Our results suggest that default language selection operates over planning units, not whole language selection, in bilingual speech production.\n\n2020-04-07\n\nThe PASCAL Pen: Pressure Sensing for Cognition, Autonomic Function and Language\n\nCarson Miller Rigoli1 Eve Wittenberg2\n\n1 UC San Diego, Cognitive Science\n\n2 UC San Diego, Linguistics\n\n+ more\n\nThis talk will introduce PASCAL, a collaborative project to develop a handgrip-pressure sensitive pen for measuring cognitive workload and mental stress during written language production. Mentally challenging tasks – solving a tricky math problem, writing a difficult word, switching from one language into another, suppressing an urge to swear, even trying to lie effectively – impact not only cognitive processing 'in the head', but lead to a cascade of physiological responses throughout the body. Systematic effects of mental stress can be observed through changes in pupil dilation, cardiovascular activity and respiration. There is evidence indicating that cognitive tasks also lead to changes in muscle tension, for instance during handwriting. Handgrip pressure may therefore provide an additional measure to supplement pupillometry and galvanic skin response to understand cognitive workload in reading, writing, mathematics and other socially and academically critical tasks. Recent advances in soft-matter physics have made the construction of light-weight, flexible, and accurate pressure sensors more affordable than ever, allowing our collaborator Dr. Annie Colin and her lab at ESPCI Paris to create a set of pens with integrated pressure sensors built into their shafts. I will present some of the technical challenges in developing this tool, including issues related to study design, hardware design, data acquisition and analysis tools. I will also discuss preliminary findings from pilot data collected in our team's efforts to evaluate the validity of these pens as research tools in psycholinguistics and introduce plans for continued testing and opportunities for research with PASCAL.\n\n2020-03-10\n\nHow Information Structures Within Caregiver-Infant Interactions Support Early Language Learning\n\nGedeon Deák\n\nUC San Diego\n\n+ more\n\nLongstanding debates concerning the possible specialization of cognitive resources for primary language learning cannot be resolved without models of detectable information patterns (i.e., covariance statistics) in language models available to infants. Ultimately the debate requires an empirical account of whatever language-predictive information patterns infants attend to, encode, and remember.\n\nI will discuss this problem in general, and describe some research to sketch preliminary partial answers to these questions (in one language). Most results are from a longitudinal study of healthy English-learning infants and their mothers during the first year. Findings include: (1) moderate maternal longitudinal stability of language output, and modest relations to infants' later language competence; (2) sequential patterns in mothers' speech that could help infants partly predict the content of the next utterance; (3) non-random patterning of maternal speech content with infants' and mothers' concurrent manual and visual actions; (4) corpus analyses (by Lucas Chang) showing that age of acquisition of specific words is predicted by multiple levels of speech co-occurrence patterns, ranging from syntactic to discourse-level patterns. These results suggest that multiple sources of predictive information are available to scaffold infants' acquisition of words, phrases, and lexical relations. As time allows, I will describe ongoing efforts to expand our corpus, and to incorporate prosodic information as another channel of predictive patterning.\n\n2020-03-03\n\nSocial robots and language technologies\n\nJanet Wiles\n\nSchool of Information Technology and Electrical Engineering, The University of Queensland, Australia (currently on sabbatical in Cogsci UCSD)\n\n+ more\n\nThis talk will introduce research into social robots and language technologies at the ARC Centre of Excellence for the Dynamics of Language (CoEDL), a 7-year multi-university Australian collaborative research centre. I will present a series of case studies in technology design, based on the child-friendly robots in the ongoing Opal project (a sister project to UCSD’s RUBI project) and introduce some of our group’s analysis tools for interaction dynamics. Opie robots are primarily designed for physically-embodied social presence, enabling children and adults to touch, hold, or hug the robot, and to use its solid frame as a physical support. The key design issues start with safety of the users, which affects movement and speed; and safety of the robot from rough play by children. These capabilities enable different kinds of studies to those of commonly available commercial robots which are too fragile for rough use, or in danger of falling over, with potential for damage to themselves and a danger to children. The second design consideration for Opie robots is the speed of interaction, and how that can impact on human social engagement, which occurs in timescales of hundreds of milliseconds. Case studies will include Opie robots as story tellers in public spaces, such as science museums and technology showcases; robots as language assistants in classrooms and language centres; Lingodroids that evolve their own languages; and chatbots used for surveys and conversation. Insights from the development of multi-lingual robots include the critical role of embedding robots in communities and the extended nature of the robots’ influence within and beyond a classroom. The Opie real-world case studies enable us to reflect on fundamental questions about design decisions that affect when a robot is considered to be a social being; whether a robot could understand the grounded meanings of the words it uses; and practical questions about what is needed for a robot to be an effective learning companion in individual and group settings. The talk will conclude with a brief overview of two computational tools for analysing human-human and human-robot interactions, including conceptual recurrence analysis of turn-taking in conversations using Discursis and the timing of interactions using Calpy’s pausecode.\n\n2020-02-25\n\nAsk Not What Bilingualism Does for Cognition, Ask What Cognition Does for Bilingualism!\n\nAnat Prior\n\nFaculty of Education, University of Haifa\n\n+ more\n\nThe question of whether human language is a specialized skill (that operates as a module) separate from other cognitive abilities or not, has a long history in psychology. Recent studies suggest that bilinguals may rely on domain general mechanisms of attention and goal maintenance to prevent interference between their two languages, ultimately leading bilinguals to have an improved ability to manage interference in a broad range of nonlinguistic tasks relative to monolinguals. These findings imply that language is not specialized and is not separate from other aspects of cognition. However, group comparisons between monolinguals and bilinguals are problematic, as there are many associated group differences besides language use (e.g. cultural background, immigration status and others).\n\nI will present three lines of research that adopt a different approach to investigating if language is specialized or not. Instead of looking for cognitive advantages in bilinguals, these studies directly investigated how bilinguals might recruit general cognitive abilities in language processing. Study 1 did not find evidence that bilinguals recruit executive functions to manage language competition; In contrast, Study 2 demonstrated that bilinguals can use perceptual cues to facilitate language control, and Study 3 found that trilinguals can and do apply higher-order cognitive abilities to support their comprehension across languages. Taken together, results of these studies do not support a strict specialization of language, but rather suggest complex and reciprocal language-cognition relations.\n\n2020-02-18\n\nEarly language matters: Insights from Deaf with extremely late sign language onset\n\nQi Cheng\n\nUniversity of California, San Diego\n\n+ more\n\nDeaf individuals often have limited access to language during early life, and their delayed sign language onset can lead to later language deficits, especially at the morphs-syntactical level. Examining the effects of lacking early language in this population can help us better understand the role of early language in typical first language development. In this talk, I will focus on the syntactic development of American Sign Language among Deaf individuals with extremely late sign language onset, combining observations from three levels: longitudinal development, sentence processing strategies, and brain language pathways. Study 1 presents a longitudinal study of 4 Deaf late signers on their word order development during early stages. Study 2 uses a sentence-picture verification experiment to examine whether Deaf late signers robustly rely on word order to comprehend simple Subject-Verb-Object sentences, even when the sentence meaning is implausible. Study 3 looks at the connectivity patterns of major language pathways in the brain using diffusion tensor imaging. Altogether, the findings from these studies suggest profound effects of early language deprivation on language development at the mono-clausal level.\n\n*Sign language interpreters will be present*\n\n2020-02-11\n\nIs language control domain general?\n\nMathieu Declerck\n\nLaboratory for Language and Cognitive Neuroscience, San Diego State University, San Diego, USA\n\nDonders Institute for Brain, Cognition and Behaviour, Radboud University, Nijmegen, The Netherlands\n\n+ more\n\nOne of the major discussions in the bilingual literature revolves around whether language control, a process that makes sure that a bilingual uses the correct language even though both languages are typically activated, is domain general. In this talk, I will give an overview of the literature regarding domain-general language control, with a focus on studies that contrasted language switching, which allows for measures of language control, and task switching, which allows for measures of more domain-general non-linguistic control. I will also present the first ERP data comparing language and task switching.\n\n2020-02-04\n\nA predictive coding account of sociolinguistic stereotype formation\n\nLindy Comstock\n\nUCLA\n\n+ more\n\nThe practice of mixing languages (aka “code-switching”, Myers-Scotton, 1995) provides a unique opportunity to study sociolinguistic expectancy violations. Bilingual code-switching occurs most often among friends and family, representing a highly emotional expression of ingroup status. For the Mexican Latino (ML) heritage community in Los Angeles, refraining from code-switching with proficient Spanish speakers of this ethnic group may constitute a sociolinguistic expectancy violation, whereas monolinguals often censure code-switching, attributing bilinguals who code-switch a non-normative outgroup status. Expectancy violation theory states there are evaluative consequences to violating social expectations (Burgoon, 1993, 1995). When others reproduce our expectations, we evaluate them positively; when others confound our expectations, we perceive them negatively, irrespective of any real offense (Burgoon, 1990; Lemak, 2012). Predictive coding theory (Friston, 2009; Huang & Rao, 2011; Rao & Ballard, 1999) suggests these evaluations may persist, blinding us at least temporarily to new data that contradict our past assessment of an individual. Our ability to perceive changes in behavior once our predictions have become entrenched is called into question. By first training participants on stereotypes, then introducing behavior contradictory to those stereotypes in a language proficiency rating task, our study assesses which individuals will int"
    }
}
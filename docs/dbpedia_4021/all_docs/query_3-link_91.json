{
    "id": "dbpedia_4021_3",
    "rank": 91,
    "data": {
        "url": "https://arxiv.org/html/2404.08801v1",
        "read_more_link": "",
        "language": "en",
        "title": "Megalodon: Efficient LLM Pretraining and Inference with Unlimited Context Length",
        "top_image": "",
        "meta_img": "",
        "images": [],
        "movies": [],
        "keywords": [],
        "meta_keywords": [
            ""
        ],
        "tags": null,
        "authors": [],
        "publish_date": null,
        "summary": "",
        "meta_description": "",
        "meta_lang": "en",
        "meta_favicon": "",
        "meta_site_name": "",
        "canonical_link": null,
        "text": "Xuezhe Maπ Xiaomeng Yangμ∗ Wenhan Xiongμ Beidi Chenκ Lili Yuμ Hao Zhangδ Jonathan Mayπ Luke Zettlemoyerμ Omer Levyμ Chunting Zhouμ∗\n\nμAI at Meta πUniversity of Southern California\n\nκCarnegie Mellon University δUniversity of California San Diego Equal Contribution. Xiaomeng Yang’s work was done at AI at Meta. Correspondence to chuntinz@meta.com\n\nAbstract\n\nThe quadratic complexity and weak length extrapolation of Transformers limits their ability to scale to long sequences, and while sub-quadratic solutions like linear attention and state space models exist, they empirically underperform Transformers in pretraining efficiency and downstream task accuracy. We introduce Megalodon, an neural architecture for efficient sequence modeling with unlimited context length. Megalodon inherits the architecture of Mega (exponential moving average with gated attention), and further introduces multiple technical components to improve its capability and stability, including complex exponential moving average (CEMA), timestep normalization layer, normalized attention mechanism and pre-norm with two-hop residual configuration. In a controlled head-to-head comparison with Llama2, Megalodon achieves better efficiency than Transformer in the scale of 7 billion parameters and 2 trillion training tokens. Megalodon reaches a training loss of 1.70, landing mid-way between Llama2-7B (1.75) and 13B (1.67). The improvements of Megalodon over Transformers are robust throughout a range of benchmarks across different tasks and modalities.\n\nCode: https://github.com/XuezheMax/megalodon\n\n1 Introduction\n\nIn many real-world applications, such as multi-turn conversation, long-document comprehension, and video generation, large language models (LLMs) must efficiently process long sequential data, understand internal long-range dynamics, and generate coherent output. The Transformer architecture (Vaswani et al., 2017), despite its remarkable capabilities, faces challenges with quadratic computational complexity and limited inductive bias for length generalization, making it inefficient for long sequence modeling (Wang et al., 2024; Zhou et al., 2024). Even with recently proposed distributed attention solutions (Li et al., 2023b; Liu et al., 2024), computing a single training step of a 7B parameter model over a 1M-token sequence is more than 100 times slower than performing the equivalent computation using 256 separate sequences of 4K tokens each.\n\nWe introduce Megalodon, an improved Mega architecture (Ma et al., 2023), which harnesses the gated attention mechanism with the classical exponential moving average (EMA) (Hunter, 1986) approach (§2). To further improve the capability and efficiency of Megalodon on large-scale long-context pretraining, we propose multiple novel technical components. First, Megalodon introduces the complex exponential moving average (CEMA) component, which extends the multi-dimensional damped EMA in Mega to the complex domain (§3.1). Then, Megalodon proposes the timestep normalization layer, which generalizes the group normalization layer (Wu and He, 2018) to auto-regressive sequence modeling tasks to allow normalization along the sequential dimension (§3.2). To improve large-scale pretraining stability, Megalodon further proposes normalized attention (§3.3), together with pre-norm with two-hop residual configuration by modifying the widely-adopted pre- and post-normalization methods (§3.4). By simply chunking input sequences into fixed blocks, as is done in Mega-chunk (Ma et al., 2023), Megalodon achieves linear computational and memory complexity in both model training and inference.\n\nEmpirically, we demonstrate the potential of Megalodon as a general architecture for modeling long sequences, by evaluating its performance across multiple scales of language modeling, as well as downstream domain-specific tasks. Through a direct comparison with Llama2, while controlling for data and compute, Megalodon-7B significantly outperforms the state-of-the-art variant of Transformer used to train Llama2-7B (Touvron et al., 2023) on both training perplexity (Figure 1) and across downstream benchmarks (Table 1). Evaluation on long-context modeling, including perplexity in various context lengths up to 2M and long-context QA tasks in Scrolls (Parisotto et al., 2020) prove Megalodon’s ability to model sequences of unlimited length. Additional experimental results on small/medium-scale benchmarks, including LRA (Tay et al., 2021), ImageNet (Deng et al., 2009), Speech Commands (Warden, 2018), WikiText-103 (Merity et al., 2017) and PG19 (Rae et al., 2019), demonstrate the robust improvements of Megalodon across scales and modalities.\n\n2 Background: Moving Average Equipped Gated Attention (Mega)\n\nIn this section, we setup notations, briefly review the key components in the Mega architecture (Ma et al., 2023), and discuss the existing problems in Mega.\n\nFollowing the notations in Mega, we use 𝑿={𝐱1,𝐱2,…,𝐱n}∈ℝn×d𝑿subscript𝐱1subscript𝐱2…subscript𝐱𝑛superscriptℝ𝑛𝑑\\boldsymbol{X}=\\{\\mathbf{x}_{1},\\mathbf{x}_{2},\\ldots,\\mathbf{x}_{n}\\}\\in% \\mathbb{R}^{n\\times d}bold_italic_X = { bold_x start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , bold_x start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT , … , bold_x start_POSTSUBSCRIPT italic_n end_POSTSUBSCRIPT } ∈ blackboard_R start_POSTSUPERSCRIPT italic_n × italic_d end_POSTSUPERSCRIPT and 𝒀={𝐲1,𝐲2,…,𝐲n}∈ℝn×d𝒀subscript𝐲1subscript𝐲2…subscript𝐲𝑛superscriptℝ𝑛𝑑\\boldsymbol{Y}=\\{\\mathbf{y}_{1},\\mathbf{y}_{2},\\ldots,\\mathbf{y}_{n}\\}\\in% \\mathbb{R}^{n\\times d}bold_italic_Y = { bold_y start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , bold_y start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT , … , bold_y start_POSTSUBSCRIPT italic_n end_POSTSUBSCRIPT } ∈ blackboard_R start_POSTSUPERSCRIPT italic_n × italic_d end_POSTSUPERSCRIPT to denote the input and output sequences with length n𝑛nitalic_n, and assume the representations of the input and output sequences have the same dimension d𝑑ditalic_d.\n\n2.1 Multi-dimensional Damped EMA\n\nMega embeds an EMA component into the calculation of the attention matrix to incorporate inductive biases across the timestep dimension. Concretely, the multi-dimensional damped EMA first expands each dimension of the input sequence 𝑿𝑿\\boldsymbol{X}bold_italic_X individually into hℎhitalic_h dimensions via an expansion matrix 𝜷∈ℝd×h𝜷superscriptℝ𝑑ℎ\\boldsymbol{\\beta}\\in\\mathbb{R}^{d\\times h}bold_italic_β ∈ blackboard_R start_POSTSUPERSCRIPT italic_d × italic_h end_POSTSUPERSCRIPT, then applies damped EMA to the hℎhitalic_h-dimensional hidden space. Formally, for each dimension j∈{1,2,…,d}𝑗12…𝑑j\\in\\{1,2,\\ldots,d\\}italic_j ∈ { 1 , 2 , … , italic_d }:\n\n𝐮t(j)subscriptsuperscript𝐮𝑗𝑡\\displaystyle\\mathbf{u}^{(j)}_{t}bold_u start_POSTSUPERSCRIPT ( italic_j ) end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT =𝜷j⁢𝐱t,jabsentsubscript𝜷𝑗subscript𝐱𝑡𝑗\\displaystyle=\\boldsymbol{\\beta}_{j}\\mathbf{x}_{t,j}= bold_italic_β start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT bold_x start_POSTSUBSCRIPT italic_t , italic_j end_POSTSUBSCRIPT 𝐡t(j)subscriptsuperscript𝐡𝑗𝑡\\displaystyle\\mathbf{h}^{(j)}_{t}bold_h start_POSTSUPERSCRIPT ( italic_j ) end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT =𝜶j⊙𝐮t(j)+(1−𝜶j⊙𝜹j)⊙𝐡t−1(j)absentdirect-productsubscript𝜶𝑗subscriptsuperscript𝐮𝑗𝑡direct-product1direct-productsubscript𝜶𝑗subscript𝜹𝑗subscriptsuperscript𝐡𝑗𝑡1\\displaystyle=\\boldsymbol{\\alpha}_{j}\\odot\\mathbf{u}^{(j)}_{t}+(1-\\boldsymbol{% \\alpha}_{j}\\odot\\boldsymbol{\\delta}_{j})\\odot\\mathbf{h}^{(j)}_{t-1}= bold_italic_α start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT ⊙ bold_u start_POSTSUPERSCRIPT ( italic_j ) end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT + ( 1 - bold_italic_α start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT ⊙ bold_italic_δ start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT ) ⊙ bold_h start_POSTSUPERSCRIPT ( italic_j ) end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_t - 1 end_POSTSUBSCRIPT (1) 𝐲t,jsubscript𝐲𝑡𝑗\\displaystyle\\mathbf{y}_{t,j}bold_y start_POSTSUBSCRIPT italic_t , italic_j end_POSTSUBSCRIPT =𝜼jT⁢𝐡t(j)absentsubscriptsuperscript𝜼𝑇𝑗subscriptsuperscript𝐡𝑗𝑡\\displaystyle=\\boldsymbol{\\eta}^{T}_{j}\\mathbf{h}^{(j)}_{t}= bold_italic_η start_POSTSUPERSCRIPT italic_T end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT bold_h start_POSTSUPERSCRIPT ( italic_j ) end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT\n\nwhere 𝐮t(j)∈ℝhsubscriptsuperscript𝐮𝑗𝑡superscriptℝℎ\\mathbf{u}^{(j)}_{t}\\in\\mathbb{R}^{h}bold_u start_POSTSUPERSCRIPT ( italic_j ) end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ∈ blackboard_R start_POSTSUPERSCRIPT italic_h end_POSTSUPERSCRIPT is the expanded hℎhitalic_h-dimensional vector for the j𝑗jitalic_j-th dimension at timestep t𝑡titalic_t. 𝜶∈(0,1)d×h𝜶superscript01𝑑ℎ\\boldsymbol{\\alpha}\\in(0,1)^{d\\times h}bold_italic_α ∈ ( 0 , 1 ) start_POSTSUPERSCRIPT italic_d × italic_h end_POSTSUPERSCRIPT, 𝜹∈(0,1)d×h𝜹superscript01𝑑ℎ\\boldsymbol{\\delta}\\in(0,1)^{d\\times h}bold_italic_δ ∈ ( 0 , 1 ) start_POSTSUPERSCRIPT italic_d × italic_h end_POSTSUPERSCRIPT are the decaying and damping factors, respectively. 𝐡t(j)∈ℝhsubscriptsuperscript𝐡𝑗𝑡superscriptℝℎ\\mathbf{h}^{(j)}_{t}\\in\\mathbb{R}^{h}bold_h start_POSTSUPERSCRIPT ( italic_j ) end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ∈ blackboard_R start_POSTSUPERSCRIPT italic_h end_POSTSUPERSCRIPT is the EMA hidden state for the j𝑗jitalic_j-th dimension at timestep t𝑡titalic_t. 𝜼∈ℝd×h𝜼superscriptℝ𝑑ℎ\\boldsymbol{\\eta}\\in\\mathbb{R}^{d\\times h}bold_italic_η ∈ blackboard_R start_POSTSUPERSCRIPT italic_d × italic_h end_POSTSUPERSCRIPT is the projection matrix to map the hℎhitalic_h-dimensional hidden state back to 1111-dimensional output 𝐲t,j∈ℝsubscript𝐲𝑡𝑗ℝ\\mathbf{y}_{t,j}\\in\\mathbb{R}bold_y start_POSTSUBSCRIPT italic_t , italic_j end_POSTSUBSCRIPT ∈ blackboard_R.\n\n2.2 Moving Average Equipped Gated Attention\n\nIn the gated attention mechanism in Mega, the output from EMA (1) is used to compute the shared representation (Hua et al., 2022), because it encodes contextual information through EMA. Subsequently, Mega introduces the reset gate, the update gate , and computes the candidate activation with the update gate and the residual connection. The technical details are provided in Appendix A.\n\n2.3 Existing Problems in Mega\n\nTo reduce the quadratic complexity in the full attention mechanism, Mega simply split the sequences of queries, keys and values in (14-16) into chunks of length c𝑐citalic_c. The attention in (17) is individually applied to each chunk, yielding linear complexity O⁢(k⁢c2)=O⁢(n⁢c)𝑂𝑘superscript𝑐2𝑂𝑛𝑐O(kc^{2})=O(nc)italic_O ( italic_k italic_c start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT ) = italic_O ( italic_n italic_c ). Technically, the EMA sub-layer in Mega helps capture local contextual information near each token, mitigating the problem of losing contextual information beyond chunk boundaries in the chunk-wise attention.\n\nDespite the impressive successes of Mega, it still suffers its own problems: i) the performance of Mega with chunk-wise attention still fails behind the one with full attention, due to the limited expressiveness of the EMA sub-layer in Mega. ii) for different tasks and/or data types, there are architectural divergences in the final Mega architectures. For example, different normalization layers, normalization patterns (pre-norm vs. post-norm) and attention functions (f⁢(⋅)𝑓⋅f(\\cdot)italic_f ( ⋅ ) in (17)) are applied to different data types (see Ma et al. (2023) for details). iii) There are no empirical evidences showing that Mega is scalable for large-scale pretraining.\n\n3 Megalodon\n\nTo address the aforementioned problems of Mega, in this section we describe the novel technical advancements of Megalodon.\n\n3.1 CEMA: Extending Multi-dimensional Damped EMA to Complex Domain\n\nAs discussed in Ma et al. (2023), the EMA component can be regarded as a simplified state space model with diagonal state matrix. Directly inspired from Gu et al. (2022b), as almost all matrices diagonalize over the complex plane, a straight-forward idea to improve EMA capability is to extend to work over the complex number system ℂℂ\\mathbb{C}blackboard_C. We propose the complex exponential moving average (CEMA), which re-writes Eq. (1):\n\n𝐡t(j)subscriptsuperscript𝐡𝑗𝑡\\displaystyle\\mathbf{h}^{(j)}_{t}bold_h start_POSTSUPERSCRIPT ( italic_j ) end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT =𝜶j⁢(cos⁡θj+i⁢sin⁡θj)⊙𝐮t(j)+(1−𝜶j⊙𝜹j)⁢(cos⁡θj+i⁢sin⁡θj)⊙𝐡t−1(j)absentdirect-productsubscript𝜶𝑗subscript𝜃𝑗𝑖subscript𝜃𝑗subscriptsuperscript𝐮𝑗𝑡direct-product1direct-productsubscript𝜶𝑗subscript𝜹𝑗subscript𝜃𝑗𝑖subscript𝜃𝑗subscriptsuperscript𝐡𝑗𝑡1\\displaystyle=\\boldsymbol{\\alpha}_{j}(\\cos{\\theta_{j}+i\\sin{\\theta_{j}}})\\odot% \\mathbf{u}^{(j)}_{t}+(1-\\boldsymbol{\\alpha}_{j}\\odot\\boldsymbol{\\delta}_{j})(% \\cos{\\theta_{j}+i\\sin{\\theta_{j}}})\\odot\\mathbf{h}^{(j)}_{t-1}= bold_italic_α start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT ( roman_cos italic_θ start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT + italic_i roman_sin italic_θ start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT ) ⊙ bold_u start_POSTSUPERSCRIPT ( italic_j ) end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT + ( 1 - bold_italic_α start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT ⊙ bold_italic_δ start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT ) ( roman_cos italic_θ start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT + italic_i roman_sin italic_θ start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT ) ⊙ bold_h start_POSTSUPERSCRIPT ( italic_j ) end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_t - 1 end_POSTSUBSCRIPT 𝐲t,jsubscript𝐲𝑡𝑗\\displaystyle\\mathbf{y}_{t,j}bold_y start_POSTSUBSCRIPT italic_t , italic_j end_POSTSUBSCRIPT =Re⁢(𝜼jT⁢𝐡t(j))absentResubscriptsuperscript𝜼𝑇𝑗subscriptsuperscript𝐡𝑗𝑡\\displaystyle=\\mathrm{Re}(\\boldsymbol{\\eta}^{T}_{j}\\mathbf{h}^{(j)}_{t})= roman_Re ( bold_italic_η start_POSTSUPERSCRIPT italic_T end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT bold_h start_POSTSUPERSCRIPT ( italic_j ) end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ) (2)\n\nwhere 𝜶𝜶\\boldsymbol{\\alpha}bold_italic_α, 𝜹∈ℝd×h𝜹superscriptℝ𝑑ℎ\\boldsymbol{\\delta}\\in\\mathbb{R}^{d\\times h}bold_italic_δ ∈ blackboard_R start_POSTSUPERSCRIPT italic_d × italic_h end_POSTSUPERSCRIPT are the real number parameters same as in EMA. Different from EMA, 𝜼∈ℂd×h𝜼superscriptℂ𝑑ℎ\\boldsymbol{\\eta}\\in\\mathbb{C}^{d\\times h}bold_italic_η ∈ blackboard_C start_POSTSUPERSCRIPT italic_d × italic_h end_POSTSUPERSCRIPT in CEMA are complex numbers. θj∈ℝh,j∈{1,2,…,d}formulae-sequencesubscript𝜃𝑗superscriptℝℎ𝑗12…𝑑\\theta_{j}\\in\\mathbb{R}^{h},\\,\\,j\\in\\{1,2,\\ldots,d\\}italic_θ start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT ∈ blackboard_R start_POSTSUPERSCRIPT italic_h end_POSTSUPERSCRIPT , italic_j ∈ { 1 , 2 , … , italic_d } are the hℎhitalic_h arguments. To uniformly space the hℎhitalic_h arguments over the period 2⁢π2𝜋2\\pi2 italic_π, we parameterize θjsubscript𝜃𝑗\\theta_{j}italic_θ start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT as:\n\nθj,k=2⁢π⁢kh⁢ωj,∀k∈{1,2,…,h}formulae-sequencesubscript𝜃𝑗𝑘2𝜋𝑘ℎsubscript𝜔𝑗for-all𝑘12…ℎ\\theta_{j,k}=\\frac{2\\pi k}{h}\\omega_{j},\\quad\\forall k\\in\\{1,2,\\ldots,h\\}italic_θ start_POSTSUBSCRIPT italic_j , italic_k end_POSTSUBSCRIPT = divide start_ARG 2 italic_π italic_k end_ARG start_ARG italic_h end_ARG italic_ω start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT , ∀ italic_k ∈ { 1 , 2 , … , italic_h } (3)\n\nwhere the learnable parameter ω∈ℝd𝜔superscriptℝ𝑑\\omega\\in\\mathbb{R}^{d}italic_ω ∈ blackboard_R start_POSTSUPERSCRIPT italic_d end_POSTSUPERSCRIPT depicts the d𝑑ditalic_d base angles. By decaying the absolute value of each htsubscriptℎ𝑡h_{t}italic_h start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT, CEMA preserves the decaying structure in kernel weights, which is a key principle to the success of convolutional models on long sequence modeling (Li et al., 2023c).\n\n3.2 Timestep Normalization\n\nDespite the impressive performance of Layer Normalization combined with Transformer, it is obvious that layer normalization cannot directly reduce the internal covariate shift along the spatial dimension (a.k.a timestep or sequential dimension) (Ioffe and Szegedy, 2015). Group Normalization (Wu and He, 2018) normalizes hidden representations both along the timestep dimension and a subset of the feature dimension, which has obtained improvements over Layer Normalization on a range of computer vision tasks. However, it cannot be directly applied to Transformer on auto-regressive sequence modeling, due to the leakage of future information via the mean and variance across the timestep dimension.\n\nIn Megalodon, we extend Group Normalization to the auto-regressive case by computing the cumulative mean and variance. Formally, suppose an input sequence 𝑿={𝐱1,𝐱2,…,𝐱n}∈ℝn×d𝑿subscript𝐱1subscript𝐱2…subscript𝐱𝑛superscriptℝ𝑛𝑑\\boldsymbol{X}=\\{\\mathbf{x}_{1},\\mathbf{x}_{2},\\ldots,\\mathbf{x}_{n}\\}\\in% \\mathbb{R}^{n\\times d}bold_italic_X = { bold_x start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , bold_x start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT , … , bold_x start_POSTSUBSCRIPT italic_n end_POSTSUBSCRIPT } ∈ blackboard_R start_POSTSUPERSCRIPT italic_n × italic_d end_POSTSUPERSCRIPT, and k𝑘kitalic_k groups along the feature dimension with dg=d/ksubscript𝑑𝑔𝑑𝑘d_{g}=d/kitalic_d start_POSTSUBSCRIPT italic_g end_POSTSUBSCRIPT = italic_d / italic_k elements per group. Then, the mean and variance of the first group at timestep t∈{1,2,…,n}𝑡12…𝑛t\\in\\{1,2,\\ldots,n\\}italic_t ∈ { 1 , 2 , … , italic_n } are:\n\nμt=1t∗dg⁢∑i=1t∑j=1dgxi,j,σt2=1t∗dg⁢∑i=1t∑j=1dg(xi,j−μt)2formulae-sequencesubscript𝜇𝑡1𝑡subscript𝑑𝑔superscriptsubscript𝑖1𝑡superscriptsubscript𝑗1subscript𝑑𝑔subscript𝑥𝑖𝑗subscriptsuperscript𝜎2𝑡1𝑡subscript𝑑𝑔superscriptsubscript𝑖1𝑡superscriptsubscript𝑗1subscript𝑑𝑔superscriptsubscript𝑥𝑖𝑗subscript𝜇𝑡2\\mu_{t}=\\frac{1}{t*d_{g}}\\sum\\limits_{i=1}^{t}\\sum\\limits_{j=1}^{d_{g}}x_{i,j}% ,\\qquad\\sigma^{2}_{t}=\\frac{1}{t*d_{g}}\\sum\\limits_{i=1}^{t}\\sum\\limits_{j=1}^% {d_{g}}(x_{i,j}-\\mu_{t})^{2}italic_μ start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT = divide start_ARG 1 end_ARG start_ARG italic_t ∗ italic_d start_POSTSUBSCRIPT italic_g end_POSTSUBSCRIPT end_ARG ∑ start_POSTSUBSCRIPT italic_i = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_t end_POSTSUPERSCRIPT ∑ start_POSTSUBSCRIPT italic_j = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_d start_POSTSUBSCRIPT italic_g end_POSTSUBSCRIPT end_POSTSUPERSCRIPT italic_x start_POSTSUBSCRIPT italic_i , italic_j end_POSTSUBSCRIPT , italic_σ start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT = divide start_ARG 1 end_ARG start_ARG italic_t ∗ italic_d start_POSTSUBSCRIPT italic_g end_POSTSUBSCRIPT end_ARG ∑ start_POSTSUBSCRIPT italic_i = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_t end_POSTSUPERSCRIPT ∑ start_POSTSUBSCRIPT italic_j = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_d start_POSTSUBSCRIPT italic_g end_POSTSUBSCRIPT end_POSTSUPERSCRIPT ( italic_x start_POSTSUBSCRIPT italic_i , italic_j end_POSTSUBSCRIPT - italic_μ start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ) start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT (4)\n\nFigure 2 illustrates Layer Normalization and Timestep Normalization. To efficiently and precisely calculate the cumulative mean and variance in each timestep, we provide hardware-friendly implementation on modern hardware (GPU) (see Appendix B.1).\n\n3.3 Normalized Attention in Megalodon\n\nPrevious studies have investigated the saturation and instability issues in the original scaled dot-product attention (17). A number of novel techniques have emerged to modify the scaled dot-product attention, among which normalized attention mechanisms, such as (scaled-) cosine attention (Luo et al., 2018; Liu et al., 2022) and QK-normalization (Henry et al., 2020), have stood out for the simplicity and effectiveness.\n\nDirectly inspired from these normalized attention mechanisms, we propose the normalized attention mechanism specifically defined for Mega to improve its stability. Formally,\n\n𝑿′superscript𝑿′\\displaystyle\\boldsymbol{X}^{\\prime}bold_italic_X start_POSTSUPERSCRIPT ′ end_POSTSUPERSCRIPT =CEMA⁢(𝑿)absentCEMA𝑿\\displaystyle=\\mathrm{CEMA}(\\boldsymbol{X})\\qquad= roman_CEMA ( bold_italic_X ) ∈ℝn×dabsentsuperscriptℝ𝑛𝑑\\displaystyle\\qquad\\in\\mathbb{R}^{n\\times d}∈ blackboard_R start_POSTSUPERSCRIPT italic_n × italic_d end_POSTSUPERSCRIPT (5) 𝒁𝒁\\displaystyle\\boldsymbol{Z}bold_italic_Z =𝑿′⁢Wz+bz,𝒁′=𝒁‖𝒁‖formulae-sequenceabsentsuperscript𝑿′subscript𝑊𝑧subscript𝑏𝑧superscript𝒁′𝒁norm𝒁\\displaystyle=\\boldsymbol{X}^{\\prime}W_{z}+b_{z},\\quad\\boldsymbol{Z}^{\\prime}=% \\frac{\\boldsymbol{Z}}{\\|\\boldsymbol{Z}\\|}= bold_italic_X start_POSTSUPERSCRIPT ′ end_POSTSUPERSCRIPT italic_W start_POSTSUBSCRIPT italic_z end_POSTSUBSCRIPT + italic_b start_POSTSUBSCRIPT italic_z end_POSTSUBSCRIPT , bold_italic_Z start_POSTSUPERSCRIPT ′ end_POSTSUPERSCRIPT = divide start_ARG bold_italic_Z end_ARG start_ARG ∥ bold_italic_Z ∥ end_ARG ∈ℝn×zabsentsuperscriptℝ𝑛𝑧\\displaystyle\\qquad\\in\\mathbb{R}^{n\\times z}∈ blackboard_R start_POSTSUPERSCRIPT italic_n × italic_z end_POSTSUPERSCRIPT (6) 𝑸𝑸\\displaystyle\\boldsymbol{Q}bold_italic_Q =𝜿q⊙𝒁′+𝝁qabsentdirect-productsubscript𝜿𝑞superscript𝒁′subscript𝝁𝑞\\displaystyle=\\boldsymbol{\\kappa}_{q}\\odot\\boldsymbol{Z}^{\\prime}+\\boldsymbol{% \\mu}_{q}\\qquad= bold_italic_κ start_POSTSUBSCRIPT italic_q end_POSTSUBSCRIPT ⊙ bold_italic_Z start_POSTSUPERSCRIPT ′ end_POSTSUPERSCRIPT + bold_italic_μ start_POSTSUBSCRIPT italic_q end_POSTSUBSCRIPT ∈ℝn×zabsentsuperscriptℝ𝑛𝑧\\displaystyle\\qquad\\in\\mathbb{R}^{n\\times z}∈ blackboard_R start_POSTSUPERSCRIPT italic_n × italic_z end_POSTSUPERSCRIPT (7) 𝑲𝑲\\displaystyle\\boldsymbol{K}bold_italic_K =𝜿k⊙𝒁′+𝝁kabsentdirect-productsubscript𝜿𝑘superscript𝒁′subscript𝝁𝑘\\displaystyle=\\boldsymbol{\\kappa}_{k}\\odot\\boldsymbol{Z}^{\\prime}+\\boldsymbol{% \\mu}_{k}\\qquad= bold_italic_κ start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT ⊙ bold_italic_Z start_POSTSUPERSCRIPT ′ end_POSTSUPERSCRIPT + bold_italic_μ start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT ∈ℝn×zabsentsuperscriptℝ𝑛𝑧\\displaystyle\\qquad\\in\\mathbb{R}^{n\\times z}∈ blackboard_R start_POSTSUPERSCRIPT italic_n × italic_z end_POSTSUPERSCRIPT (8)\n\nwhere 𝑸𝑸\\boldsymbol{Q}bold_italic_Q and 𝑲𝑲\\boldsymbol{K}bold_italic_K are computed by using the normalized shared representation 𝒁′superscript𝒁′\\boldsymbol{Z}^{\\prime}bold_italic_Z start_POSTSUPERSCRIPT ′ end_POSTSUPERSCRIPT instead of 𝒁𝒁\\boldsymbol{Z}bold_italic_Z. Note that we remove the SiLU (Ramachandran et al., 2017) activation function ϕsilusubscriptitalic-ϕsilu\\phi_{\\mathrm{silu}}italic_ϕ start_POSTSUBSCRIPT roman_silu end_POSTSUBSCRIPT in (13), because the normalization on 𝒁𝒁\\boldsymbol{Z}bold_italic_Z has incorporated non-linearity into 𝒁′superscript𝒁′\\boldsymbol{Z}^{\\prime}bold_italic_Z start_POSTSUPERSCRIPT ′ end_POSTSUPERSCRIPT. Then the attention operation in (17) has been changed to:\n\n𝑶𝑶\\displaystyle\\boldsymbol{O}bold_italic_O =fsoftmax⁢(𝑸⁢𝑲T)⁢𝑽absentsubscript𝑓softmax𝑸superscript𝑲𝑇𝑽\\displaystyle=f_{\\mathrm{softmax}}\\left(\\boldsymbol{Q}{\\boldsymbol{K}}^{T}% \\right)\\boldsymbol{V}\\quad= italic_f start_POSTSUBSCRIPT roman_softmax end_POSTSUBSCRIPT ( bold_italic_Q bold_italic_K start_POSTSUPERSCRIPT italic_T end_POSTSUPERSCRIPT ) bold_italic_V ∈ℝn×vabsentsuperscriptℝ𝑛𝑣\\displaystyle\\qquad\\qquad\\in\\mathbb{R}^{n\\times v}∈ blackboard_R start_POSTSUPERSCRIPT italic_n × italic_v end_POSTSUPERSCRIPT (9)\n\nAs we use learnable 𝜿qsubscript𝜿𝑞\\boldsymbol{\\kappa}_{q}bold_italic_κ start_POSTSUBSCRIPT italic_q end_POSTSUBSCRIPT, 𝜿ksubscript𝜿𝑘\\boldsymbol{\\kappa}_{k}bold_italic_κ start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT in (7) and (8), we can remove the scaled term τ⁢(𝑿)𝜏𝑿\\tau(\\boldsymbol{X})italic_τ ( bold_italic_X ). In addition, we found that with the normalized attention, the softmax function fsoftmaxsubscript𝑓softmaxf_{\\mathrm{softmax}}italic_f start_POSTSUBSCRIPT roman_softmax end_POSTSUBSCRIPT obtains the best or at least comparable performance on different tasks and data modalities (see Appendix C). Hence, throughout this paper we use softmax as the default attention function.\n\n3.4 Pre-Norm with Two-hop Residual\n\nNormalization configurations are crucial in stably training deep architectures, and pre-normalization (Xiong et al., 2020) has become the default normalization configuration because of its better convergence properties than post-normalization in the original Transformer architecture (Vaswani et al., 2017). However, extensive studies have investigated the instability issue of pre-normalization when scaling up model size (Davis et al., 2021; Liu et al., 2022). Formally, a Transformer-based block in pre-noromalization can be formulated as (shown in Figure 3 (b)):\n\n𝒀^^𝒀\\displaystyle\\hat{\\boldsymbol{Y}}over^ start_ARG bold_italic_Y end_ARG =Attention⁢(Norm⁢(𝑿))+𝑿absentAttentionNorm𝑿𝑿\\displaystyle=\\mathrm{Attention}(\\mathrm{Norm}(\\boldsymbol{X}))+\\boldsymbol{X}= roman_Attention ( roman_Norm ( bold_italic_X ) ) + bold_italic_X 𝒀𝒀\\displaystyle\\boldsymbol{Y}bold_italic_Y =FFN⁢(Norm⁢(𝒀^))+𝒀^absentFFNNorm^𝒀^𝒀\\displaystyle=\\mathrm{FFN}(\\mathrm{Norm}(\\hat{\\boldsymbol{Y}}))+\\hat{% \\boldsymbol{Y}}= roman_FFN ( roman_Norm ( over^ start_ARG bold_italic_Y end_ARG ) ) + over^ start_ARG bold_italic_Y end_ARG =FFN⁢(Norm⁢(𝒀^))+Attention⁢(Norm⁢(𝑿))+𝑿absentFFNNorm^𝒀AttentionNorm𝑿𝑿\\displaystyle=\\mathrm{FFN}(\\mathrm{Norm}(\\hat{\\boldsymbol{Y}}))+\\mathrm{% Attention}(\\mathrm{Norm}(\\boldsymbol{X}))+\\boldsymbol{X}= roman_FFN ( roman_Norm ( over^ start_ARG bold_italic_Y end_ARG ) ) + roman_Attention ( roman_Norm ( bold_italic_X ) ) + bold_italic_X (10)\n\nwhere the output 𝒀𝒀\\boldsymbol{Y}bold_italic_Y is the sum of the input 𝑿𝑿\\boldsymbol{X}bold_italic_X and the output of each component in one block. Hence, the range and/or variance of 𝒀𝒀\\boldsymbol{Y}bold_italic_Y keeps increasing for deeper blocks, causing the instability issue. In the original Mega architecture, the update gate 𝝋𝝋\\boldsymbol{\\varphi}bold_italic_φ (19) is used for a gated residual connection (21) to mitigate this problem (Parisotto et al., 2020; Xu et al., 2020). However, the update gate 𝝋𝝋\\boldsymbol{\\varphi}bold_italic_φ introduces more model parameters and the instability issue still exists when scaling up model size to 7 billion.\n\nMegalodon introduces a new configuration named pre-norm with two-hop residual, which simply re-arranges the residual connections in each block (shown in Figure 3 (c):\n\n𝒀^^𝒀\\displaystyle\\hat{\\boldsymbol{Y}}over^ start_ARG bold_italic_Y end_ARG =Attention⁢(Norm⁢(𝑿))+𝑿absentAttentionNorm𝑿𝑿\\displaystyle=\\mathrm{Attention}(\\mathrm{Norm}(\\boldsymbol{X}))+\\boldsymbol{X}= roman_Attention ( roman_Norm ( bold_italic_X ) ) + bold_italic_X 𝒀𝒀\\displaystyle\\boldsymbol{Y}bold_italic_Y =FFN⁢(Norm⁢(𝒀^))+𝑿absentFFNNorm^𝒀𝑿\\displaystyle=\\mathrm{FFN}(\\mathrm{Norm}(\\hat{\\boldsymbol{Y}}))+\\boldsymbol{X}= roman_FFN ( roman_Norm ( over^ start_ARG bold_italic_Y end_ARG ) ) + bold_italic_X (11)\n\nwhere the input 𝑿𝑿\\boldsymbol{X}bold_italic_X is reused as the residual connection of the FFN layer. Since 𝒀^^𝒀\\hat{\\boldsymbol{Y}}over^ start_ARG bold_italic_Y end_ARG is directly followed by a normalization layer, we remove the update gate 𝝋𝝋\\boldsymbol{\\varphi}bold_italic_φ and use standard residual connection. The graphical architecture of a Megalodon sub-layer is visualized in Figure 3 (a). Note that the Timestep Normalization is only applied before the attention layer. Before the FFN layer, we still use Layer Normalization. The reasons are two-fold: i) Layer Normalization is faster than Timestep Normalization; ii) the output vector of each token from the attention layer is a mixture of vectors from contextual tokens via attention weights. Hence, normalizing the attention output along the feature dimension is similar to indirectly normalize along the timestep dimension.\n\n3.5 4-Dimensional Parallelism in Distributed LLM Pretraining\n\nEfficient distributed training algorithm is essential to train a large-scale language model, and several parallelization mechanisms have been introduced. The three most commonly used parallelism strategies are data, tensor (Shoeybi et al., 2019) and pipeline parallelism (Huang et al., 2019). However, the 3-dimensional parallelism is still insufficient to scale up the context length of LLMs (Li et al., 2023b; Liu et al., 2024).\n\nBenefiting from the chunk-wise attention in Megalodon, we can efficiently parallelize it along the new timestep/sequence dimension, which is orthogonal to all the aforementioned three parallelism dimensions. In Megalodon, the only communications between devices in one chunk-parallel group are the last hidden state of CEMA and the cumulative mean and variance of Timestep Normalization in each block. Using asynchronous communication, we can minimize the overhead of chunk parallelization by hiding the communication costs in the computation of other components inside the same block and/or other blocks.\n\n4 Experiments\n\nTo evaluate the scalability and efficiency of Megalodon on long-context sequence modeling, we scale up Megalodon to 7-billion model size and apply it to large-scale language model pretraining on 2 trillion tokens. We also conduct experiments on small/medium-scale sequence modeling benchmarks, including Long Range Arena (LRA) (Tay et al., 2021), raw speech classification on Speech Commands (Warden, 2018), image classification on ImageNet-1K (Deng et al., 2009), and language-modeling on WikiText-103 (Merity et al., 2017) and PG19 (Rae et al., 2019). Empirically, Megalodon significantly outperforms all the state-of-the-art baseline models on these tasks across various data modalities.\n\n4.1 LLM Pretraining\n\nArchitectural Details\n\nIn our Megalodon-7B model, we adopt most of architectural hyperparameters from Llama2-7B to ensure fair comparison: Megalodon-7B consists of 32 blocks, with feature dimension d=4096𝑑4096d=4096italic_d = 4096. Following Llama2, we use the SwiGLU activation function (Shazeer, 2020) in the feed-forward layer, and rotary positional embedding (RoPE, Su et al. (2021)). We set the attention chunk size c=4096𝑐4096c=4096italic_c = 4096, which is the same as the pretraining context length in Llama2. Benefiting from the attention gate (γ𝛾\\gammaitalic_γ in (18)), we use a much smaller number of attention heads h=4ℎ4h=4italic_h = 4 in Megalodon-7B, comparing to h=32ℎ32h=32italic_h = 32 in Llama2-7B. In addition, we apply pre-norm with two-hop residual (§3.4), using Timestep Normalization (§3.2) and Layer Normalization (Ba et al., 2016), while Llama2 models apply pre-normalization with RMSNorm (Zhang and Sennrich, 2019).\n\nData and Pretraining Details\n\nWe use the same mix of publicly available data from Llama2, ensuring that the model are trained on exactly the same 2-trillion tokens. We also use the same tokenizer as Llama2, whose vocabulary size is 32323232K.\n\nWe trained Megalodon-7B using the AdamW optimizer (Loshchilov and Hutter, 2019), with β1=0.9subscript𝛽10.9\\beta_{1}=0.9italic_β start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT = 0.9, β2=0.95subscript𝛽20.95\\beta_{2}=0.95italic_β start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT = 0.95, ϵ=1⁢e−8italic-ϵ1𝑒8\\epsilon=1e-8italic_ϵ = 1 italic_e - 8. The learning rate is 3.5⁢e−43.5𝑒43.5e-43.5 italic_e - 4 and cosine learning rate schedule is applied with warmup of 2500250025002500 steps. We use a weight decay of 0.10.10.10.1 and gradient clipping of 1.01.01.01.0, and no dropout is applied during training. The context length in pretraining is 32323232K (4 attention chunks). The global batch size is 4M tokens, and is distributed on 256 NVIDIA A100 GPUs (16K tokens per A100). We set data parallel size to 128, chunk parallel size to 2 and tensor parallel size to 1.\n\nData and Computation Efficiency\n\nWe evaluate the efficiency of Megalodon w.r.t both the data and computation perspectives. For data efficiency, we display the negative log-likelihood (NLL) for Megalodon-7B, Llama2-7B and Llama2-13B w.r.t processed tokens during training in Figure 1. Megalodon-7B obtains significantly better (lower) NLL than Llama2-7B under the same amount of training tokens, demonstrating better data efficiency. Moreover, Megalodon suffers less training spikes than the Transformer-based architecture in Llama2. Note that at the first 1/4 of the pretraining process (<500absent500<500< 500B tokens), the NLL of Megalodon-7B is slightly worse than Llama2-7B. We found that the main reason is that we increased the base θ𝜃\\thetaitalic_θ of RoPE from 10,0001000010,00010 , 000 in Llama2 to 100,000100000100,000100 , 000 in Megalodon, which slows down model convergence at the beginning of the pretraining process. At the end, Megalodon reaches a training loss of 1.70, landing mid-way between Llama2-7B (1.75) and Llama2-13B (1.67).\n\nFor computation efficiency, we conduct experiments of running Llama2-7B and Megalodon-7B using the same amount of computational resources and comparing their training speed under various context lengths. Specifically, we execute each experiment to train a model with global batch size 4M tokens distributed on 256 NVIDIA A100 GPUs (16K tokens per A100) and calculate the word/token per second (WPS) to measure the training speed. Figure 4 illustrates the average WPS per device of Llama2-7B and Megalodon-7B using 4K and 32K context lengths, respectively. For Llama2 models, we accelerate the computation of full attention with Flash-Attention V2 (Dao, 2024). Under 4K context length, Megalodon-7B is slightly slower (about 6%percent66\\%6 %) than Llama2-7B, due to the introduction of CEMA and Timestep Normalization. When we scale up context length to 32K, Megalodon-7B is significantly faster (about 32%percent3232\\%32 %) than Llama2-7B, demonstrating the computation efficiency of Megalodon for long-context pretraining. In addition, Megalodon-7B-32K, which utilizes chunk parallelism (§3.5), achieves about 94%percent9494\\%94 % utilization of Megalodon-7B-4K.\n\n4.2 Short-Context Evaluation on Academic Benchmarks\n\nWe compare Megalodon-7B to Llama2 models on standard academic benchmarks with short contexts (<4absent4<4< 4K tokens), closely following the settings in Llama2 (Touvron et al., 2023). The benchmarks are grouped into the categories listed below:\n\n•\n\nCommonsense Reasoning (0-shot): HellaSwag (Zellers et al., 2019), PIQA (Bisk et al., 2020), SIQA (Sap et al., 2019), WinoGrande (Sakaguchi et al., 2021), ARC-e and -c (Clark et al., 2018).\n\n•\n\nWorld Knowledge (5-shot): NaturalQuestions (NQ, Kwiatkowski et al. (2019)) and TriviaQA (TQA, Joshi et al. (2017)).\n\n•\n\nReading Comprehension (0-shot): BoolQ (Clark et al., 2019).\n\n•\n\nPopular aggregated results (5-shot): MMLU (Hendrycks et al., 2020).\n\nTable 1 summarizes the results of Megalodon and Llama2 on these academic benchmarks, together with other open-source base models, including MPT (MosaicML, 2023), RWKV (Peng et al., 2023), Mamba (Gu and Dao, 2023), Mistral (Jiang et al., 2023) and Gemma (Mesnard et al., 2024). Pretrained on the same 2T tokens, Megalodon-7B surpasses Llama2-7B across all the benchmarks. On some tasks, Megalodon-7B achieves comparable or even better performance with Llama2-13B. Note that Mistral-7B and Gemma-8B were pretrained on much larger datasets than Megalodon-7B, hence the results are not directly comparable.\n\n4.3 Long-Context Evaluation\n\nPerplexity over Long Sequences\n\nTo demonstrate the capability of Megalodon to make use of very long contexts to improve next-token prediction, we start by conducting the evaluation of valid perplexity on different context lengths. Concretely, we construct a validation dataset which consists of 1,920 selected books. Each of these books contains sequences with at least 2M tokens. The validation dataset is constructed by first randomly shuffling all the files and then concatenating them. Figure 5 shows the perplexity (PPL) of the validation dataset in various context lengths ranging from 4K to 2M. We observe that the PPL decreases monotonically with context length, validating the effectivenss and robustness of Megalodon on modeling extremely long sequences.\n\nLong-Context QA tasks in Scrolls\n\nNext, we evaluate Megalodon on long-context open-book question answering (QA) tasks in the Scrolls dataset (Shaham et al., 2022), including NarrativeQA (Kočiský et al., 2018), Qasper (Dasigi et al., 2021) and QMSum (Zhong et al., 2021). Following Xiong et al. (2023), we use a simple prompt {CONTEXT} Q: {QUESTION} A: for all the tasks, and evaluate 0-shot F1-score on NarrativeQA, 2-shot F1-score on Qasper and 1-shot geometric-ROUGE on QMSum. Table 2 lists the results of Megalodon-7B, together with other open-source long-context models in the scale of 7B, namely Xgen-7B-8K (Nijkamp et al., 2023), MPT-7B-8K (MosaicML, 2023), YaRN-7B-128k (Peng et al., 2024), Llama2-7B-4K (Touvron et al., 2023) and Llama2-7B-32K (Llama2-L, Xiong et al. (2023)). Megalodon-7B obtains the best F1 on NarrativeQA, and competitive results with Llama2-7B Long. It should be noticed that Llama2-7B Long extends the context length of Llama2-7B from 4K to 32K by continually pretraining it on additional 500B tokens from long-context data.\n\n4.4 Instruction Finetuning\n\nTo evaluation the generalization capability of Megalodon on instruction following and alignment, We finetune the base model of Megalodon-7B on a proprietary instruction-alignment data under a controlled setting. We did not apply any RLHF techniques to further finetune it. Table 3 summarizes the performance of chat models in 7B scale on MT-Bench . Megalodon exhibits superior performance on MT-Bench compared to Vicuna (Chiang et al., 2023), and comparable performance to Llama2-Chat, which utilizes RLHF for further alignment finetuning. We present some outputs from instruction finetuned Megalodon in Appendix D.\n\n4.5 Evaluation on Medium-Scale Benchmarks\n\nImageNet Classification\n\nTo evaluate Megalodon on image classification task, we conduct experiments on the Imagenet-1111K (Deng et al., 2009) dataset, which consists of 1.28M training images and 50K validation images from 1000 classes. We mostly follow DeiT’s approach of applying several data augmentation and regularization methods that facilitate the training process, and adopt most the hyperparameters from Ma et al. (2023). For classification task, we replace the timestep normalization with the standard group normalization method. Top-1 accuracy on the validation set is reported in Table 4 to assess various models. Megalodon obtains about 1.31.31.31.3% accuracy improvement over DeiT-B (Touvron et al., 2021), and 0.80.80.80.8%. improvement over Mega (Ma et al., 2023).\n\nAuto-regressive Language Modeling on PG-19\n\nWe also evaluate Megalodon on auto-regressive language modeling on the medium-scale PG19 (Rae et al., 2019) datasets. We use the same vocabulary from Block-Recurrent Transformer (Hutchins et al., 2022) and adopt most of its hyper-parameters to train a Megalodon model with 1.3B parameters. Table 5 illustrate the word-level perplexity (PPL) of Megalodon on PG-19, together with previous state-of-the-art models, including Compressive Transformer (Rae et al., 2020), Perceiver AR (Hawthorne et al., 2022), Block-Recurrent Transformer (Hutchins et al., 2022) and MegaByte (Yu et al., 2024). Megalodon significantly outperforms all the baselines.\n\n5 Conclusion\n\nWe have introduced Megalodon, an improved Mega architecture with multiple novel technical components, including complex exponential moving average (CEMA), the timestep normalization layer, normalized attention and pre-norm with two-hop residual configuration, to improve its capability, efficiency and scalability. Through a direct comparison with Llama2, Megalodon achieves impressive improvements on both training perplexity and across downstream benchmarks. Importantly, experimental results on long-context modeling demonstrate Megalodon’s ability to model sequences of unlimited length. Additional experiments on small/medium-scale benchmarks across different data modalities illustrate the robust improvements of Megalodon, which lead to a potential direction of future work to apply Megalodon for large-scale multi-modality pretraining.\n\nReferences\n\nBa et al. (2016) Jimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E Hinton. Layer normalization. arXiv preprint arXiv:1607.06450, 2016.\n\nBaevski and Auli (2018) Alexei Baevski and Michael Auli. Adaptive input representations for neural language modeling. In International Conference on Learning Representations, 2018.\n\nBisk et al. (2020) Yonatan Bisk, Rowan Zellers, Jianfeng Gao, Yejin Choi, et al. Piqa: Reasoning about physical commonsense in natural language. In Proceedings of the AAAI conference on artificial intelligence, pages 7432–7439, 2020.\n\nChiang et al. (2023) Wei-Lin Chiang, Zhuohan Li, Zi Lin, Ying Sheng, Zhanghao Wu, Hao Zhang, Lianmin Zheng, Siyuan Zhuang, Yonghao Zhuang, Joseph E. Gonzalez, Ion Stoica, and Eric P. Xing. Vicuna: An open-source chatbot impressing gpt-4 with 90%* chatgpt quality, March 2023. URL https://lmsys.org/blog/2023-03-30-vicuna/.\n\nClark et al. (2019) Christopher Clark, Kenton Lee, Ming-Wei Chang, Tom Kwiatkowski, Michael Collins, and Kristina Toutanova. Boolq: Exploring the surprising difficulty of natural yes/no questions. arXiv preprint arXiv:1905.10044, 2019.\n\nClark et al. (2018) Peter Clark, Isaac Cowhey, Oren Etzioni, Tushar Khot, Ashish Sabharwal, Carissa Schoenick, and Oyvind Tafjord. Think you have solved question answering? try arc, the ai2 reasoning challenge. arXiv preprint arXiv:1803.05457, 2018.\n\nCooley and Tukey (1965) James W Cooley and John W Tukey. An algorithm for the machine calculation of complex fourier series. Mathematics of computation, 19(90):297–301, 1965.\n\nDai et al. (2019) Zihang Dai, Zhilin Yang, Yiming Yang, Jaime G Carbonell, Quoc Le, and Ruslan Salakhutdinov. Transformer-xl: Attentive language models beyond a fixed-length context. In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 2978–2988, 2019.\n\nDao (2024) Tri Dao. Flashattention-2: Faster attention with better parallelism and work partitioning. In International Conference on Learning Representations (ICLR-2024), 2024.\n\nDasigi et al. (2021) Pradeep Dasigi, Kyle Lo, Iz Beltagy, Arman Cohan, Noah A. Smith, and Matt Gardner. A dataset of information-seeking questions and answers anchored in research papers. In Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (NAACL-2021), pages 4599–4610, Online, June 2021. Association for Computational Linguistics.\n\nDavis et al. (2021) Jared Q Davis, Albert Gu, Krzysztof Choromanski, Tri Dao, Christopher Re, Chelsea Finn, and Percy Liang. Catformer: Designing stable transformers via sensitivity analysis. In International Conference on Machine Learning, pages 2489–2499. PMLR, 2021.\n\nDeng et al. (2009) Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. Imagenet: A large-scale hierarchical image database. In 2009 IEEE conference on computer vision and pattern recognition, pages 248–255. Ieee, 2009.\n\nFu et al. (2023) Daniel Y Fu, Tri Dao, Khaled Kamal Saab, Armin W Thomas, Atri Rudra, and Christopher Re. Hungry hungry hippos: Towards language modeling with state space models. In The Eleventh International Conference on Learning Representations (ICLR-2023), 2023.\n\nGu and Dao (2023) Albert Gu and Tri Dao. Mamba: Linear-time sequence modeling with selective state spaces, 2023.\n\nGu et al. (2022a) Albert Gu, Karan Goel, and Christopher Ré. Efficiently modeling long sequences with structured state spaces. In International Conference on Learning Representations (ICLR-2022), 2022a.\n\nGu et al. (2022b) Albert Gu, Ankit Gupta, Karan Goel, and Christopher Ré. On the parameterization and initialization of diagonal state space models. arXiv preprint arXiv:2206.11893, 2022b.\n\nHanson and Pratt (1988) Stephen Hanson and Lorien Pratt. Comparing biases for minimal network construction with back-propagation. Advances in neural information processing systems, 1, 1988.\n\nHawthorne et al. (2022) Curtis Hawthorne, Andrew Jaegle, Cătălina Cangea, Sebastian Borgeaud, Charlie Nash, Mateusz Malinowski, Sander Dieleman, Oriol Vinyals, Matthew Botvinick, Ian Simon, et al. General-purpose, long-context autoregressive modeling with perceiver ar. In International Conference on Machine Learning, pages 8535–8558. PMLR, 2022.\n\nHendrycks et al. (2020) Dan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn Song, and Jacob Steinhardt. Measuring massive multitask language understanding. arXiv preprint arXiv:2009.03300, 2020.\n\nHenry et al. (2020) Alex Henry, Prudhvi Raj Dachapally, Shubham Shantaram Pawar, and Yuxuan Chen. Query-key normalization for transformers. In Findings of the Association for Computational Linguistics: EMNLP 2020, pages 4246–4253, 2020.\n\nHua et al. (2022) Weizhe Hua, Zihang Dai, Hanxiao Liu, and Quoc Le. Transformer quality in linear time. In International Conference on Machine Learning (ICML-2022), pages 9099–9117. PMLR, 2022.\n\nHuang et al. (2019) Yanping Huang, Youlong Cheng, Ankur Bapna, Orhan Firat, Dehao Chen, Mia Chen, HyoukJoong Lee, Jiquan Ngiam, Quoc V Le, Yonghui Wu, and zhifeng Chen. Gpipe: Efficient training of giant neural networks using pipeline parallelism. In Advances in Neural Information Processing Systems, volume 32. Curran Associates, Inc., 2019.\n\nHunter (1986) J Stuart Hunter. The exponentially weighted moving average. Journal of quality technology, 18(4):203–210, 1986.\n\nHutchins et al. (2022) DeLesley Hutchins, Imanol Schlag, Yuhuai Wu, Ethan Dyer, and Behnam Neyshabur. Block-recurrent transformers. Advances in neural information processing systems, 35:33248–33261, 2022.\n\nIoffe and Szegedy (2015) Sergey Ioffe and Christian Szegedy. Batch normalization: Accelerating deep network training by reducing internal covariate shift. In International conference on machine learning (ICML-2015), pages 448–456. pmlr, 2015.\n\nJiang et al. (2023) Albert Q Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bamford, Devendra Singh Chaplot, Diego de las Casas, Florian Bressand, Gianna Lengyel, Guillaume Lample, Lucile Saulnier, et al. Mistral 7b. arXiv preprint arXiv:2310.06825, 2023.\n\nJoshi et al. (2017) Mandar Joshi, Eunsol Choi, Daniel Weld, and Luke Zettlemoyer. Triviaqa: A large scale distantly supervised challenge dataset for reading comprehension. In Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers). Association for Computational Linguistics, 2017.\n\nKahan (1965) William Kahan. Pracniques: further remarks on reducing truncation errors. Communications of the ACM, 8(1):40, 1965.\n\nKočiský et al. (2018) Tomáš Kočiský, Jonathan Schwarz, Phil Blunsom, Chris Dyer, Karl Moritz Hermann, Gábor Melis, and Edward Grefenstette. The NarrativeQA reading comprehension challenge. Transactions of the Association for Computational Linguistics, 6:317–328, 2018.\n\nKrizhevsky et al. (2009) Alex Krizhevsky et al. Learning multiple layers of features from tiny images. Technical Report. University of Toronto, 2009.\n\nKwiatkowski et al. (2019) Tom Kwiatkowski, Jennimaria Palomaki, Olivia Redfield, Michael Collins, Ankur Parikh, Chris Alberti, Danielle Epstein, Illia Polosukhin, Jacob Devlin, Kenton Lee, et al. Natural questions: a benchmark for question answering research. Transactions of the Association for Computational Linguistics, 7:453–466, 2019.\n\nLi et al. (2023a) Bonan Li, Yinhan Hu, Xuecheng Nie, Congying Han, Xiangjian Jiang, Tiande Guo, and Luoqi Liu. Dropkey for vision transformer. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 22700–22709, June 2023a.\n\nLi et al. (2023b) Dacheng Li, Rulin Shao, Anze Xie, Eric P Xing, Joseph E Gonzalez, Ion Stoica, Xuezhe Ma, and Hao Zhang. Lightseq:: Sequence level parallelism for distributed training of long context transformers. In Workshop on Advancing Neural Network Training: Computational Efficiency, Scalability, and Resource Optimization (WANT@ NeurIPS 2023), 2023b.\n\nLi et al. (2023c) Yuhong Li, Tianle Cai, Yi Zhang, Deming Chen, and Debadeepta Dey. What makes convolutional models great on long sequence modeling? In International Conference on Learning Representations (ICLR-2023), 2023c.\n\nLinsley et al. (2018) Drew Linsley, Junkyung Kim, Vijay Veerabadran, Charles Windolf, and Thomas Serre. Learning long-range spatial dependencies with horizontal gated recurrent units. In S. Bengio, H. Wallach, H. Larochelle, K. Grauman, N. Cesa-Bianchi, and R. Garnett, editors, Advances in Neural Information Processing Systems, volume 31. Curran Associates, Inc., 2018.\n\nLiu et al. (2024) Hao Liu, Matei Zaharia, and Pieter Abbeel. Ring attention with blockwise transformers for near-infinite context. In International Conference on Learning Representations (ICLR-2024), 2024.\n\nLiu et al. (2022) Ze Liu, Han Hu, Yutong Lin, Zhuliang Yao, Zhenda Xie, Yixuan Wei, Jia Ning, Yue Cao, Zheng Zhang, Li Dong, et al. Swin transformer v2: Scaling up capacity and resolution. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 12009–12019, 2022.\n\nLoshchilov and Hutter (2019) Ilya Loshchilov and Frank Hutter. Decoupled weight decay regularization. In International Conference on Learning Representations, 2019.\n\nLuo et al. (2018) Chunjie Luo, Jianfeng Zhan, Xiaohe Xue, Lei Wang, Rui Ren, and Qiang Yang. Cosine normalization: Using cosine similarity instead of dot product in neural networks. In 27th International Conference on Artificial Neural Networks (ICANN-2018), pages 382–391. Springer, 2018.\n\nMa et al. (2021) Xuezhe Ma, Xiang Kong, Sinong Wang, Chunting Zhou, Jonathan May, Hao Ma, and Luke Zettlemoyer. Luna: Linear unified nested attention. Advances in Neural Information Processing Systems, 34:2441–2453, 2021.\n\nMa et al. (2023) Xuezhe Ma, Chunting Zhou, Xiang Kong, Junxian He, Liangke Gui, Graham Neubig, Jonathan May, and Luke Zettlemoyer. Mega: Moving average equipped gated attention. In The Eleventh International Conference on Learning Representations, 2023.\n\nMaas et al. (2011) Andrew Maas, Raymond E Daly, Peter T Pham, Dan Huang, Andrew Y Ng, and Christopher Potts. Learning word vectors for sentiment analysis. In Proceedings of the 49th annual meeting of the association for computational linguistics: Human language technologies, pages 142–150, 2011.\n\nMerity et al. (2017) Stephen Merity, Caiming Xiong, James Bradbury, and Richard Socher. Pointer sentinel mixture models. In International Conference on Learning Representations (ICLR-2017), 2017.\n\nMesnard et al. (2024) Thomas Mesnard, Cassidy Hardin, Robert Dadashi, Surya Bhupatiraju, Shreya Pathak, Laurent Sifre, Morgane Rivière, Mihir Sanjay Kale, Juliette Love, Pouya Tafti, Léonard Hussenot, Aakanksha Chowdhery, Adam Roberts, Aditya Barua, Alex Botev, Alex Castro-Ros, Ambrose Slone, Amélie Héliou, Andrea Tacchetti, Anna Bulanova, Antonia Paterson, Beth Tsai, Bobak Shahriari, Charline Le Lan, Christopher A. Choquette-Choo, Clément Crepy, Daniel Cer, Daphne Ippolito, David Reid, Elena Buchatskaya, Eric Ni, Eric Noland, Geng Yan, George Tucker, George-Christian Muraru, Grigory Rozhdestvenskiy, Henryk Michalewski, Ian Tenney, Ivan Grishchenko, Jacob Austin, James Keeling, Jane Labanowski, Jean-Baptiste Lespiau, Jeff Stanway, Jenny Brennan, Jeremy Chen, Johan Ferret, Justin Chiu, Justin Mao-Jones, Katherine Lee, Kathy Yu, Katie Millican, Lars Lowe Sjoesund, Lisa Lee, Lucas Dixon, Machel Reid, Maciej Mikuła, Mateo Wirth, Michael Sharman, Nikolai Chinaev, Nithum Thain, Olivier Bachem, Oscar Chang, Oscar Wahltinez, Paige Bailey, Paul Michel, Petko Yotov, Pier Giuseppe Sessa, Rahma Chaabouni, Ramona Comanescu, Reena Jana, Rohan Anil, Ross McIlroy, Ruibo Liu, Ryan Mullins, Samuel L Smith, Sebastian Borgeaud, Sertan Girgin, Sholto Douglas, Shree Pandya, Siamak Shakeri, Soham De, Ted Klimenko, Tom Hennigan, Vlad Feinberg, Wojciech Stokowiec, Yu hui Chen, Zafarali Ahmed, Zhitao Gong, Tris Warkentin, Ludovic Peran, Minh Giang, Clément Farabet, Oriol Vinyals, Jeff Dean, Koray Kavukcuoglu, Demis Hassabis, Zoubin Ghahramani, Douglas Eck, Joelle Barral, Fernando Pereira, Eli Collins, Armand Joulin, Noah Fiedel, Evan Senter, Alek Andreev, and Kathleen Kenealy. Gemma: Open models based on gemini research and technology, 2024.\n\nMosaicML (2023) MosaicML. Introducing mpt-7b: A new standard for open-source, commercially usable llms, 2023.\n\nNangia and Bowman (2018) Nikita Nangia and Samuel Bowman. Listops: A diagnostic dataset for latent tree learning. In Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Student Research Workshop, pages 92–99, 2018.\n\nNijkamp et al. (2023) Erik Nijkamp, Tian Xie, Hiroaki Hayashi, Bo Pang, Congying Xia, Chen Xing, Jesse Vig, Semih Yavuz, Philippe Laban, Ben Krause, Senthil Purushwalkam, Tong Niu, Wojciech Kryściński, Lidiya Murakhovs’ka, Prafulla Kumar Choubey, Alex Fabbri, Ye Liu, Rui Meng, Lifu Tu, Meghana Bhat, Chien-Sheng Wu, Silvio Savarese, Yingbo Zhou, Shafiq Joty, and Caiming Xiong. Xgen-7b technical report, 2023.\n\nParisotto et al. (2020) Emilio Parisotto, Francis Song, Jack Rae, Razvan Pascanu, Caglar Gulcehre, Siddhant Jayakumar, Max Jaderberg, Raphael Lopez Kaufman, Aidan Clark, Seb Noury, et al. Stabilizing transformers for reinforcement learning. In International conference on machine learning, pages 7487–7498. PMLR, 2020.\n\nPaszke et al. (2019) Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan, Trevor Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, et al. Pytorch: An imperative style, high-performance deep learning library. Advances in neural information processing systems, 32, 2019.\n\nPeng et al. (2023) Bo Peng, Eric Alcaide, Quentin Anthony, Alon Albalak, Samuel Arcadinho, Huanqi Cao, Xin Cheng, Michael Chung, Matteo Grella, Kranthi Kiran GV, et al. Rwkv: Reinventing rnns for the transformer era. arXiv preprint arXiv:2305.13048, 2023.\n\nPeng et al. (2024) Bowen Peng, Jeffrey Quesnelle, Honglu Fan, and Enrico Shippole. YaRN: Efficient context window extension of large language models. In International Conference on Learning Representations (ICLR-2024), 2024.\n\nPoli et al. (2023) Michael Poli, Stefano Massaroli, Eric Nguyen, Daniel Y Fu, Tri Dao, Stephen Baccus, Yoshua Bengio, Stefano Ermon, and Christopher Ré. Hyena hierarchy: Towards larger convolutional language models. In International conference on machine learning (ICML-2023). PMLR, 2023.\n\nRadev et al. (2013) Dragomir R Radev, Pradeep Muthukrishnan, Vahed Qazvinian, and Amjad Abu-Jbara. The acl anthology network corpus. Language Resources and Evaluation, 47(4):919–944, 2013.\n\nRae et al. (2019) Jack W Rae, Anna Potapenko, Siddhant M Jayakumar, Chloe Hillier, and Timothy P Lillicrap. Compressive transformers for long-range sequence modelling. arXiv preprint, 2019. URL https://arxiv.org/abs/1911.05507.\n\nRae et al. (2020) Jack W Rae, Anna Potapenko, Siddhant M Jayakumar, Chloe Hillier, and Timothy P Lillicrap. Compressive transformers for long-range sequence modeling. In International Conference on Learning Representations (ICLR-2020), 2020.\n\nRamachandran et al. (2017) Prajit Ramachandran, Barret Zoph, and Quoc V Le. Swish: a self-gated activation function. arXiv preprint arXiv:1710.05941, 7(1):5, 2017.\n\nSakaguchi et al. (2021) Keisuke Sakaguchi, Ronan Le Bras, Chandra Bhagavatula, and Yejin Choi. Winogrande: An adversarial winograd schema challenge at scale. Communications of the ACM, 64(9):99–106, 2021.\n\nSap et al. (2019) Maarten Sap, Hannah Rashkin, Derek Chen, Ronan LeBras, and Yejin Choi. Socialiqa: Commonsense reasoning about social interactions. arXiv preprint arXiv:1904.09728, 2019.\n\nShaham et al. (2022) Uri Shaham, Elad Segal, Maor Ivgi, Avia Efrat, Ori Yoran, Adi Haviv, Ankit Gupta, Wenhan Xiong, Mor Geva, Jonathan Berant, and Omer Levy. SCROLLS: Standardized CompaRison over long language sequences. In Yoav Goldberg, Zornitsa Kozareva, and Yue Zhang, editors, Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing (EMNLP-2022), pages 12007–12021, Abu Dhabi, United Arab Emirates, December 2022. Association for Computational Linguistics.\n\nShazeer (2020) Noam Shazeer. Glu variants improve transformer. arXiv preprint arXiv:2002.05202, 2020.\n\nShoeybi et al. (2019) Mohammad Shoeybi, Mostofa Patwary, Raul Puri, Patrick LeGresley, Jared Casper, and Bryan Catanzaro. Megatron-lm: Training multi-billion parameter language models using model parallelism. arXiv preprint arXiv:1909.08053, 2019.\n\nSu et al. (2021) Jianlin Su, Yu Lu, Shengfeng Pan, Bo Wen, and Yunfeng Liu. Roformer: Enhanced transformer with rotary position embedding. arXiv preprint arXiv:2104.09864, 2021.\n\nTay et al. (2020) Yi Tay, Mostafa Dehghani, Dara Bahri, and Donald Metzler. Efficient transformers: A survey. arXiv preprint arXiv:2009.06732, 2020.\n\nTay et al. (2021) Yi Tay, Mostafa Dehghani, Samira Abnar, Yikang Shen, Dara Bahri, Philip Pham, Jinfeng Rao, Liu Yang, Sebastian Ruder, and Donald Metzler. Long range arena : A benchmark for efficient transformers. In International Conference on Learning Representations, 2021. URL https://openreview.net/forum?id=qVyeW-grC2k.\n\nTay et al. (2022) Yi Tay, Mostafa Dehghani, Samira Abnar, Hyung Won Chung, William Fedus, Jinfeng Rao, Sharan Narang, Vinh Q. Tran, Dani Yogatama, and Donald Metzler. Scaling laws vs model architectures: How does inductive bias influence scaling?, 2022.\n\nTouvron et al. (2021) Hugo Touvron, Matthieu Cord, Matthijs Douze, Francisco Massa, Alexandre Sablayrolles, and Hervé Jégou. Training data-efficient image transformers & distillation through attention. In International Conference on Machine Learning, pages 10347–10357. PMLR, 2021.\n\nTouvron et al. (2023) Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al. Llama 2: Open foundation and fine-tuned chat models. arXiv preprint arXiv:2307.09288, 2023.\n\nVaswani et al. (2017) Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Ł ukasz Kaiser, and Illia Polosukhin. Attention is all you need. In Advances in Neural Information Processing Systems, volume 30. Curran Associates, Inc., 2017.\n\nWang et al. (2024) Xindi Wang, Mahsa Salmani, Parsa Omidi, Xiangyu Ren, Mehdi Rezagholizadeh, and Armaghan Eshaghi. Beyond the limits: A survey of techniques to extend the context length in large language models, 2024.\n\nWarden (2018) Pete Warden. Speech commands: A dataset for limited-vocabulary speech recognition. arXiv preprint arXiv:1804.03209, 2018.\n\nWelford (1962) B. P. Welford. Note on a method for calculating corrected sums of squares and products. Technometrics, 4(3):419–420, 1962.\n\nWu and He (2018) Yuxin Wu and Kaiming He. Group normalization. In Proceedings of the European conference on computer vision (ECCV-2018), pages 3–19, 2018.\n\nXiong et al. (2020) Ruibin Xiong, Yunchang Yang, Di He, Kai Zheng, Shuxin Zheng, Chen Xing, Huishuai Zhang, Yanyan Lan, Liwei Wang, and Tieyan Liu. On layer normalization in the transformer architecture. In International Conference on Machine Learning, pages 10524–10533. PMLR, 2020.\n\nXiong et al. (2023) Wenhan Xiong, Jingyu Liu, Igor Molybog, Hejia Zhang, Prajjwal Bhargava, Rui Hou, Louis Martin, Rashi Rungta, Karthik Abinav Sankararaman, Barlas Oguz, et al. Effective long-context scaling of foundation models. arXiv preprint arXiv:2309.16039, 2023.\n\nXu et al. (2020) Hongfei Xu, Qiuhui Liu, Deyi Xiong, and Josef van Genabith. Transformer with depth-wise lstm. arXiv preprint arXiv:2007.06257, 2020.\n\nYu et al. (2024) Lili Yu, Dániel Simig, Colin Flaherty, Armen Aghajanyan, Luke Zettlemoyer, and Mike Lewis. Megabyte: Predicting million-byte sequences with multiscale transformers. Advances in Neural Information Processing Systems, 36, 2024.\n\nZellers et al. (2019) Rowan Zellers, Ari Holtzman, Yonatan Bisk, Ali Farhadi, and Yejin Choi. Hellaswag: Can a machine really finish your sentence? In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics (ACL-2019). Association for Computational Linguistics, 2019.\n\nZhang and Sennrich (2019) Biao Zhang and Rico Sennrich. Root mean square layer normalization. Advances in Neural Information Processing Systems, 32, 2019.\n\nZhong et al. (2021) Ming Zhong, Da Yin, Tao Yu, Ahmad Zaidi, Mutethia Mutuma, Rahul Jha, Ahmed Hassan Awadallah, Asli Celikyilmaz, Yang Liu, Xipeng Qiu, and Dragomir Radev. QMSum: A new benchmark for query-based multi-domain meeting summarization. In Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (NAACL-2021), pages 5905–5921, Online, June 2021. Association for Computational Linguistics.\n\nZhou et al. (2024) Yongchao Zhou, Uri Alon, Xinyun Chen, Xuezhi Wang, Rishabh Agarwal, and Denny Zhou. Transformers can achieve length generalization but not robustly, 2024.\n\nAppendix: Megalodon: Efficient Long-Context LLM Pretraining and Inference with Unlimited Context Length\n\nAppendix A Background: Moving Average Equipped Gated Attention\n\nIn the gated attention mechanism in Mega, the output from EMA (1) is used to compute the shared representation (Hua et al., 2022) 𝒁𝒁\\boldsymbol{Z}bold_italic_Z:\n\n𝑿′superscript𝑿′\\displaystyle\\boldsymbol{X}^{\\prime}bold_italic_X start_POSTSUPERSCRIPT ′ end_POSTSUPERSCRIPT =EMA⁢(𝑿)absentEMA𝑿\\displaystyle=\\mathrm{EMA}(\\boldsymbol{X})\\qquad= roman_EMA ( bold_italic_X ) ∈ℝn×dabsentsuperscriptℝ𝑛𝑑\\displaystyle\\qquad\\in\\mathbb{R}^{n\\times d}∈ blackboard_R start_POSTSUPERSCRIPT italic_n × italic_d end_POSTSUPERSCRIPT (12) 𝒁𝒁\\displaystyle\\boldsymbol{Z}bold_italic_Z =ϕsilu⁢(𝑿′⁢Wz+bz)absentsubscriptitalic-ϕsilusuperscript𝑿′subscript𝑊𝑧subscript𝑏𝑧\\displaystyle=\\phi_{\\mathrm{silu}}(\\boldsymbol{X}^{\\prime}W_{z}+b_{z})\\qquad= italic_ϕ start_POSTSUBSCRIPT roman_silu end_POSTSUBSCRIPT ( bold_italic_X start_POSTSUPERSCRIPT ′ end_POSTSUPERSCRIPT italic_W start_POSTSUBSCRIPT italic_z end_POSTSUBSCRIPT + italic_b start_POSTSUBSCRIPT italic_z end_POSTSUBSCRIPT ) ∈ℝn×zabsentsuperscriptℝ𝑛𝑧\\displaystyle\\qquad\\in\\mathbb{R}^{n\\times z}∈ blackboard_R start_POSTSUPERSCRIPT italic_n × italic_z end_POSTSUPERSCRIPT (13)\n\nwhere 𝑿′superscript𝑿′\\boldsymbol{X}^{\\prime}bold_italic_X start_POSTSUPERSCRIPT ′ end_POSTSUPERSCRIPT can be regarded as the updated or contextual input, because it encodes contextual information through EMA. Then, the query and key sequences are computed by applying per-dimension scalars and offsets to 𝒁𝒁\\boldsymbol{Z}bold_italic_Z, and the value sequence is from the original 𝑿𝑿\\boldsymbol{X}bold_italic_X:\n\n𝑸𝑸\\displaystyle\\boldsymbol{Q}bold_italic_Q =𝜿q⊙𝒁+𝝁qabsentdirect-productsubscript𝜿𝑞𝒁subscript𝝁𝑞\\displaystyle=\\boldsymbol{\\kappa}_{q}\\odot\\boldsymbol{Z}+\\boldsymbol{\\mu}_{q}\\qquad= bold_italic_κ start_POSTSUBSCRIPT italic_q end_POSTSUBSCRIPT ⊙ bold_italic_Z + bold_italic_μ start_POSTSUBSCRIPT italic_q end_POSTSUBSCRIPT ∈ℝn×zabsentsuperscriptℝ𝑛𝑧\\displaystyle\\qquad\\in\\mathbb{R}^{n\\times z}∈ blackboard_R start_POSTSUPERSCRIPT italic_n × italic_z end_POSTSUPERSCRIPT (14) 𝑲𝑲\\displaystyle\\boldsymbol{K}bold_italic_K =𝜿k⊙𝒁+𝝁kabsentdirect-productsubscript𝜿𝑘𝒁subscript𝝁𝑘\\displaystyle=\\boldsymbol{\\kappa}_{k}\\odot\\boldsymbol{Z}+\\boldsymbol{\\mu}_{k}\\qquad= bold_italic_κ start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT ⊙ bold_italic_Z + bold_italic_μ start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT ∈ℝn×zabsentsuperscriptℝ𝑛𝑧\\displaystyle\\qquad\\in\\mathbb{R}^{n\\times z}∈ blackboard_R start_POSTSUPERSCRIPT italic_n × italic_z end_POSTSUPERSCRIPT (15) 𝑽𝑽\\displaystyle\\boldsymbol{V}bold_italic_V =ϕsilu⁢(𝑿⁢Wv+bv)absentsubscriptitalic-ϕsilu𝑿subscript𝑊𝑣subscript𝑏𝑣\\displaystyle=\\phi_{\\mathrm{silu}}(\\boldsymbol{X}W_{v}+b_{v})\\qquad= italic_ϕ start_POSTSUBSCRIPT roman_silu end_POSTSUBSCRIPT ( bold_italic_X italic_W start_POSTSUBSCRIPT italic_v end_POSTSUBSCRIPT + italic_b start_POSTSUBSCRIPT italic_v end_POSTSUBSCRIPT ) ∈ℝn×vabsentsuperscriptℝ𝑛𝑣\\displaystyle\\quad\\qquad\\in\\mathbb{R}^{n\\times v}∈ blackboard_R start_POSTSUPERSCRIPT italic_n × italic_v end_POSTSUPERSCRIPT (16)\n\nwhere 𝜿qsubscript𝜿𝑞\\boldsymbol{\\kappa}_{q}bold_italic_κ start_POSTSUBSCRIPT italic_q end_POSTSUBSCRIPT, 𝝁qsubscript𝝁𝑞\\boldsymbol{\\mu}_{q}bold_italic_μ start_POSTSUBSCRIPT italic_q end_POSTSUBSCRIPT, 𝜿ksubscript𝜿𝑘\\boldsymbol{\\kappa}_{k}bold_italic_κ start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT, 𝝁k∈ℝzsubscript𝝁𝑘superscriptℝ𝑧\\boldsymbol{\\mu}_{k}\\in\\mathbb{R}^{z}bold_italic_μ start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT ∈ blackboard_R start_POSTSUPERSCRIPT italic_z end_POSTSUPERSCRIPT are the learnable scalars and offsets of queries and keys, respectively. v𝑣vitalic_v is the expanded intermediate dimension for the value sequence. The output of attention is computed as follows:\n\n𝑶𝑶\\displaystyle\\boldsymbol{O}bold_italic_O =f⁢(𝑸⁢𝑲Tτ⁢(𝑿))⁢𝑽absent𝑓𝑸superscript𝑲𝑇𝜏𝑿𝑽\\displaystyle=f\\left(\\frac{\\boldsymbol{Q}{\\boldsymbol{K}}^{T}}{\\tau(% \\boldsymbol{X})}\\right)\\boldsymbol{V}\\quad= italic_f ( divide start_ARG bold_italic_Q bold_italic_K start_POSTSUPERSCRIPT italic_T end_POSTSUPERSCRIPT end_ARG start_ARG italic_τ ( bold_italic_X ) end_ARG ) bold_italic_V ∈ℝn×vabsentsuperscriptℝ𝑛𝑣\\displaystyle\\qquad\\qquad\\qquad\\in\\mathbb{R}^{n\\times v}∈ blackboard_R start_POSTSUPERSCRIPT italic_n × italic_v end_POSTSUPERSCRIPT (17)\n\nSubsequently, Mega introduces the reset gate 𝜸𝜸\\boldsymbol{\\gamma}bold_italic_γ, the update gate 𝝋𝝋\\boldsymbol{\\varphi}bold_italic_φ, and computes the candidate activation 𝑯^bold-^𝑯\\boldsymbol{\\hat{H}}overbold_^ start_ARG bold_italic_H end_ARG and final output 𝒀𝒀\\boldsymbol{Y}bold_italic_Y:\n\n𝜸𝜸\\displaystyle\\boldsymbol{\\gamma}bold_italic_γ =ϕsilu⁢(𝑿′⁢Wγ+bγ)absentsubscriptitalic-ϕsilusuperscript𝑿′subscript𝑊𝛾subscript𝑏𝛾\\displaystyle=\\phi_{\\mathrm{silu}}(\\boldsymbol{X}^{\\prime}W_{\\gamma}+b_{\\gamma})= italic_ϕ start_POSTSUBSCRIPT roman_silu end_POSTSUBSCRIPT ( bold_italic_X start_POSTSUPERSCRIPT ′ end_POSTSUPERSCRIPT italic_W start_POSTSUBSCRIPT italic_γ end_POSTSUBSCRIPT + italic_b start_POSTSUBSCRIPT italic_γ end_POSTSUBSCRIPT ) ∈ℝn×vabsentsuperscriptℝ𝑛𝑣\\displaystyle\\in\\mathbb{R}^{n\\times v}∈ blackboard_R start_POSTSUPERSCRIPT italic_n × italic_v end_POSTSUPERSCRIPT (18) 𝝋𝝋\\displaystyle\\boldsymbol{\\varphi}bold_italic_φ =ϕsigmoid⁢(𝑿′⁢Wφ+bφ)absentsubscriptitalic-ϕsigmoidsuperscript𝑿′subscript𝑊𝜑subscript𝑏𝜑\\displaystyle=\\phi_{\\mathrm{sigmoid}}(\\boldsymbol{X}^{\\prime}W_{\\varphi}+b_{% \\varphi})= italic_ϕ start_POSTSUBSCRIPT roman_sigmoid end_POSTSUBSCRIPT ( bold_italic_X start_POSTSUPERSCRIPT ′ end_POSTSUPERSCRIPT italic_W start_POSTSUBSCRIPT italic_φ end_POSTSUBSCRIPT + italic_b start_POSTSUBSCRIPT italic_φ end_POSTSUBSCRIPT ) ∈ℝn×dabsentsuperscriptℝ𝑛𝑑\\displaystyle\\in\\mathbb{R}^{n\\times d}∈ blackboard_R start_POSTSUPERSCRIPT italic_n × italic_d end_POSTSUPERSCRIPT (19) 𝑯^^𝑯\\displaystyle\\hat{\\boldsymbol{H}}over^ start_ARG bold_italic_H end_ARG =ϕsilu⁢(𝑿′⁢Wh+(𝜸⊙𝑶)⁢Uh+bh)absentsubscriptitalic-ϕsilusuperscript𝑿′subscript𝑊ℎdirect-product𝜸𝑶subscript𝑈ℎsubscript𝑏ℎ\\displaystyle=\\phi_{\\mathrm{silu}}(\\boldsymbol{X}^{\\prime}W_{h}+(\\boldsymbol{% \\gamma}\\odot\\boldsymbol{O})U_{h}+b_{h})= italic_ϕ start_POSTSUBSCRIPT roman_silu end_POSTSUBSCRIPT ( bold_italic_X start_POSTSUPERSCRIPT ′ end_POSTSUPERSCRIPT italic_W start_POSTSUBSCRIPT italic_h end_POSTSUBSCRIPT + ( bold_italic_γ ⊙ bold_italic_O ) italic_U start_POSTSUBSCRIPT italic_h end_POSTSUBSCRIPT + italic_b start_POSTSUBSCRIPT italic_h end_POSTSUBSCRIPT ) ∈ℝn×dabsentsuperscriptℝ𝑛𝑑\\displaystyle\\in\\mathbb{R}^{n\\times d}∈ blackboard_R start_POSTSUPERSCRIPT italic_n × italic_d end_POSTSUPERSCRIPT (20) 𝒀𝒀\\displaystyle\\boldsymbol{Y}bold_italic_Y =𝝋⊙𝑯^+(1−𝝋)⊙𝑿absentdirect-product𝝋bold-^𝑯direct-product1𝝋𝑿\\displaystyle=\\boldsymbol{\\varphi}\\odot\\boldsymbol{\\hat{H}}+(1-\\boldsymbol{% \\varphi})\\odot\\boldsymbol{X}= bold_italic_φ ⊙ overbold_^ start_ARG bold_italic_H end_ARG + ( 1 - bold_italic_φ ) ⊙ bold_italic_X ∈ℝn×dabsentsuperscriptℝ𝑛𝑑\\displaystyle\\in\\mathbb{R}^{n\\times d}∈ blackboard_R start_POSTSUPERSCRIPT italic_n × italic_d end_POSTSUPERSCRIPT (21)\n\nwith the update gate 𝝋𝝋\\boldsymbol{\\varphi}bold_italic_φ and the residual connection 𝑿𝑿\\boldsymbol{X}bold_italic_X.\n\nAppendix B Implementation Details\n\nB.1 Efficient Fused CUDA Operators Implementation\n\nFused Attention\n\nWe implemented a fused attention operator to improve the efficiency, mainly by fusing the causal mask, softmax function and dropout operation (if necessary). The fused implementation reduces the IO costs from global memory for the attention weight. For attention dropout, we adopt the dropout-before-softmax scheme in DropKey (Li et al., 2023a), which applies the dropout mask on the input attention matrix of the softmax function. Concretely, we fill the values of the attention matrix at dropout mask positions to −∞-\\infty- ∞ before feeding it into the softmax function. One important advantage of this dropout-before-softmax scheme comparing to the standard attention dropout is that the computation of the gradients in back-propagation is independent with the applied dropout mask.\n\nEfficient FFTConv\n\nWe also provide an efficient fused implementation of the FFTConv operator. Similar with the FlashConv in H3 (Fu et al., 2023), we fused the real number FFT (RFFT) and its inverse (IRFFT) and implemented the Cooley-Tukey FFT algorithm (Cooley and Tukey, 1965) in the CUDA shared memory. Our implementation is able to accommodate up to 16K tokens in the limited shared memory of A100.\n\nTimestep Normalization\n\nFor the TimestepNorm operator, we have an efficient implementation to improve both its speed and numerical stability. To compute the cumulative mean and variance for each of the timesteps, our implementation limits the parallel threads used for the timestep/sequence dimension. To improve numerical stability, we used the Welford algorithm (Welford, 1962) to compute the cumulative mean and variance and the Kahan Summation (Kahan, 1965) to reduce the numerical error from summation.\n\nB.2 Plus 1 Reparameterization in Normalization Layers\n\nIn the normalization methods, two learnable parameters γ𝛾\\gammaitalic_γ and β𝛽\\betaitalic_β are introduced to scale and shift the normalized value:\n\ny=γ⁢x−μσ+β𝑦𝛾𝑥𝜇𝜎𝛽y=\\gamma\\frac{x-\\mu}{\\sigma}+\\betaitalic_y = italic_γ divide start_ARG italic_x - italic_μ end_ARG start_ARG italic_σ end_ARG + italic_β (22)\n\nwhere μ𝜇\\muitalic_μ and σ2superscript𝜎2\\sigma^{2}italic_σ start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT are the mean and variance of the input x𝑥xitalic_x across the pre-defined dimensions. Initialization of γ𝛾\\gammaitalic_γ and β𝛽\\betaitalic_β is crucial for model performance and stability. The standard implementation of normalization layers, such as PyTorch (Paszke et al., 2019), initializes γ𝛾\\gammaitalic_γ and β𝛽\\betaitalic_β to vectors of ones and zeros, respectively, to preserve the mean and variance of the normalized inputs at the beginning of training.\n\nThis standard implementation, however, suffers a problem when weight decay regularization is applied to prevent overfitting (Hanson and Pratt, 1988). Technically, the weight decay regularization pushes the values of model parameters towards smaller magnitudes. In the context of normalization methods, weight decay pushes the values in γ𝛾\\gammaitalic_γ towards zero, which diverges from its initialization of one. This may prevent the model from learning the true scale of the data distribution, and may cause numerical stability issues as well.\n\nTo address this problem, we used the plus 1 reparameterization of the scale parameter γ𝛾\\gammaitalic_γ:\n\ny=(γ+1)⁢x−μσ+β𝑦𝛾1𝑥𝜇𝜎𝛽y=(\\gamma+1)\\frac{x-\\mu}{\\sigma}+\\betaitalic_y = ( italic_γ + 1 ) divide start_ARG italic_x - italic_μ end_ARG start_ARG italic_σ end_ARG + italic_β (23)\n\nwhere γ𝛾\\gammaitalic_γ is initialized to zero. Under weight decay, γ𝛾\\gammaitalic_γ remains centered around zero, resulting in a desirable scale of γ+1𝛾1\\gamma+1italic_γ + 1 around one.\n\nAppendix C Experiments on Small-Scale Benchmarks\n\nWe conducted small-scale experiments on five benchmarks across various data modalities, including text, audio and image. To demonstrate the robustness of the Megalodon architecture on different tasks and data types, we used a single unified architecture with minimal architectural divergence in all the experiments: softmax attention function, rotary postional embedding, pre-norm with two-hop residual, and timestep Normalization (Group Normalization for classification). We adopt (almost) all the architectural and training hyperparameters from the corresponding experiments of original Mega (Ma et al., 2023).\n\nC.1 Long Range Arena (LRA)\n\nLong Range Arena (LRA) benchmark (Tay et al., 2021) is designed for evaluating sequence models under the long-context scenario. They collect six tasks in this benchmark which are ListOps (Nangia and Bowman, 2018), byte-level text classification (Text; Maas et al. (2011)), byte-level document retrieval (Retrieval; Radev et al. (2013)), image classification on sequences of pixels (Image; Krizhevsky et al. (2009)), Pathfinder (Linsley et al., 2018) and its extreme long version (Path-X; Tay et al. (2021)). These tasks consist of input sequences ranging from 1K to 16K tokens and span across a variety of data types and modalities.\n\nTable 6 compares Megalodon against several baselines, including Transformer and its efficient variants, the state space model S4 (Gu et al., 2022a), and the original Mega model. Following Ma et al. (2023), we also evaluate Megalodon-chunk on each task, by setting the chunk size c=128𝑐128c=128italic_c = 128 for all the tasks, except Path-X where c=4096𝑐4096c=4096italic_c = 4096. With chunk-wise attention, Megalodon-chunk substantially outperforms Mega-chunk on all the six tasks. In addition, Megalodon significantly narrows the gap between chunk-wise attention and full attention.\n\nC.2 Raw Speech Classification\n\nTo evaluate the capability of Megalodon on the long-range modeling of speech signals, we apply Megalodon to classify raw speech (with length 16000), rather than using traditional preprocessing (e.g. convert to MFCC features). Following Ma et al. (2023), we perform speech classification on the SC10 subset of the Speech Commands dataset (Warden, 2018), and set attention chunk size c=1000𝑐1000c=1000italic_c = 1000. As shown in Table 7, our Megalodon model with 300K parameters achieves an accuracy of 98.14, which is significantly better than 97.50 from S4 and 96.92 from Mega with similar model size.\n\nC.3 Auto-regressive Language Modeling\n\nWe also evaluate Megalodon on auto-regressive language modeling on the small-scale WikiText-103 (Merity et al., 2017) datasets. Following Baevski and Auli (2018), we adopt adaptive softmax and input embeddings and use a vocabulary of 260K tokens for WikiText-103 and train a Megalodon model with about 250M parameters with attention chunk size c=2048𝑐2048c=2048italic_c = 2048. Table 8 illustrate the word-level perplexity (PPL) of Megalodon on WikiText-103, together with previous state-of-the-art models, including Transformer (Baevski and Auli, 2018), Transformer-XL (Dai et al., 2019), S4 (Gu et al., 2022a) and Mega (Ma et al., 2023). Megalodon significantly outperforms all the baselines on both the two datasets.\n\nAppendix D Model Outputs from Instruction-finetuned Megalodon"
    }
}
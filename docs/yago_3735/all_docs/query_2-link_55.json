{
    "id": "yago_3735_2",
    "rank": 55,
    "data": {
        "url": "https://www.forbes.com/sites/tiernanray/2019/09/02/separating-ai-from-the-nonsense/",
        "read_more_link": "",
        "language": "en",
        "title": "Separating AI From The Nonsense",
        "top_image": "https://imageio.forbes.com/specials-images/imageserve/5d6d519f673aa300083cc6cb/0x0.jpg?format=jpg&crop=1198,674,x30,y75,safe&height=900&width=1600&fit=bounds",
        "meta_img": "https://imageio.forbes.com/specials-images/imageserve/5d6d519f673aa300083cc6cb/0x0.jpg?format=jpg&crop=1198,674,x30,y75,safe&height=900&width=1600&fit=bounds",
        "images": [
            "https://secure.gravatar.com/avatar/ccce2e348dcc108bd0bc775762db2981?s=400&d=mm&r=g"
        ],
        "movies": [],
        "keywords": [],
        "meta_keywords": [
            ""
        ],
        "tags": null,
        "authors": [
            "Tiernan Ray"
        ],
        "publish_date": "2019-09-02T00:00:00",
        "summary": "",
        "meta_description": "Over sixty years ago, A Dartmouth professor named John McCarthy coined a term that would become an enormously successful, and problematic, marketing slogan: artificial intelligence.",
        "meta_lang": "en",
        "meta_favicon": "https://i.forbesimg.com/48X48-F.png",
        "meta_site_name": "Forbes",
        "canonical_link": "https://www.forbes.com/sites/tiernanray/2019/09/02/separating-ai-from-the-nonsense/",
        "text": "Over sixty years ago, A Dartmouth professor named John McCarthy coined a term that would become an enormously successful, and problematic, marketing slogan: artificial intelligence. The term AI is now ubiquitous in the technology industry. Its use bears little relation to the things with which McCarthy was concerned, and it’s often used to sex-up stuff that has little to with what practitioners of artificial intelligence care about.\n\nThe term as used today is illustrated by the stock photography images of glowing brains that accompany articles, and it evokes in the popular mind visions of the movie The Terminator, of robots that want to kill you or at least take away your job. The term often accompanies various anthropomorphized descriptions, so that AI seems to “know,” to “think.”\n\nAll of that is more or less nonsense and has little to do with the history and practice of artificial intelligence.\n\nMcCarthy was a mathematician by training, and he was interested in a number of problems, but creating a Terminator wasn’t one of them. In his proposal to the Rockefeller Foundation in 1955 to fund what would become the famous Dartmouth Summer Research Project on Artificial Intelligence, McCarthy proposed the problem of developing a computer programming language that didn’t specify in advance everything the computer should do. He was interested in human language as being in some ways an example of what was needed for a more sophisticated language. McCarthy produced one of the most enduring achievements of AI, the Lisp programming language.\n\nMcCarthy’s concerns were generally theoretical, not practical, but others in his circle were interested in ways to make computers do things that would be difficult to code line by line. Computers were primitive then, so everyone was motivated to find a more elegant way for them to operate.\n\nOne member of the Dartmouth project, Nathaniel Rochester, an IBM scientist, was basically interested in a smarter computer. As he put it in McCarthy’s proposal, “in writing programs, one sometimes must go at problems in a very laborious manner, whereas if the machine had just a little intuition, or could make reasonable guesses, the solution could be quite direct.”\n\nIn all cases, the researchers were taking inspiration from examples of human thought to formulate problems, but they were not literally trying to create an equivalent to the human mind. As McCarthy wrote in a book review in 1988, looking back on his inspiration, “we weren’t considering human behavior except as a clue to possible effective ways of doing tasks.” The whole point, he wrote, was “to get away from studying human behavior and to consider the computer as a tool for solving certain classes of problems.”\n\nAI was “created as a branch of computer science and not as a branch of psychology,” McCarthy explained.\n\nThe best general definition of AI comes from McCarthy’s peer, another Dartmouth project contributor, the MIT scientist Marvin Minsky. In 1958, Minsky wrote about the need to design or program “machines to work on problems for which the designer does not have, in advance, practical methods of solution.” In other words, the goal was computers whose design allows for a certain degree of freedom beyond rote specifications. The term AI was not that important to Minsky. In an interview in 1989, Minsky remarked, “I never used the word AI.” The term, he argued, was just a label placed after the fact on what was really something much simpler. “AI labs are the places where young people go if they want to make machines do things that they don't do yet and that are hard to do with the currently existing methods.”\n\nAt any point in time, said Minsky, “Only a few of the people are trying to make a thinking machine and other people are just trying far out, new kinds of software.”\n\nIn that way, “AI is just the more forward-looking part of computer science,” he said.\n\nThere’s been six decades-plus of different ways to go about that, and at times Minsky and McCarthy were at odds with other practitioners, including the “connectionist” scholars that created today’s deep learning forms of AI. Those debates sometimes touched on notions of human thought, but, basically, all those scientists were doing was exploring the limits of conventional ways of programming and designing computers.\n\nToday’s use of the term AI in industry often has nothing to do with that. If one digs into the details, many things that are claimed to be AI are nothing more than the application of statistics to large quantities of data to try and find patterns. That can be a worthwhile endeavor in itself, but it’s not necessarily very forward-looking. It’s more likely to be a novel application of well-developed methods within statistics and programming.\n\nAI will stick around in the language for the foreseeable future because it can be a useful catch-all phrase for techniques at the edge of computer science, what Minsky referred to as just trying stuff out until something works.\n\nJust remember that when you hear the term used in industry, it doesn’t actually denote a Terminator, and most of the time it doesn’t even mean cutting-edge computer science. Most of the time, it’s just hype."
    }
}
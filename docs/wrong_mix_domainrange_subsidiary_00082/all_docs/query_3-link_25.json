{
    "id": "wrong_mix_domainrange_subsidiary_00082_3",
    "rank": 25,
    "data": {
        "url": "https://ar5iv.labs.arxiv.org/abs/2305.14293",
        "read_more_link": "",
        "language": "en",
        "title": "[2305.14293] WebIE: Faithful and Robust Information Extraction on the Web",
        "top_image": "https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png",
        "meta_img": "https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png",
        "images": [
            "https://ar5iv.labs.arxiv.org/html/2305.14293/assets/x1.png",
            "https://ar5iv.labs.arxiv.org/html/2305.14293/assets/figures/fact_extraction_guidance.png",
            "https://ar5iv.labs.arxiv.org/html/2305.14293/assets/figures/fact_extraction_UI.png",
            "https://ar5iv.labs.arxiv.org/html/2305.14293/assets/figures/translation_UI_1.png",
            "https://ar5iv.labs.arxiv.org/html/2305.14293/assets/figures/translation_UI_2.png",
            "https://ar5iv.labs.arxiv.org/assets/ar5iv.png"
        ],
        "movies": [],
        "keywords": [],
        "meta_keywords": [
            ""
        ],
        "tags": null,
        "authors": [],
        "publish_date": null,
        "summary": "",
        "meta_description": "Extracting structured and grounded fact triples from raw text is a fundamental task in Information Extraction (IE).\nExisting IE datasets are typically collected from Wikipedia articles, using hyperlinks to link entitie…",
        "meta_lang": "en",
        "meta_favicon": "",
        "meta_site_name": "ar5iv",
        "canonical_link": "https://ar5iv.labs.arxiv.org/html/2305.14293",
        "text": "WebIE: Faithful and Robust Information Extraction on the Web\n\nChenxi Whitehouse ∗ Work conducted as Research Intern at Amazon Alexa AI. Clara Vania Alham Fikri Aji ∗⁣∗** Now at Mohamed Bin Zayed University of Artificial Intelligence (MBZUAI), Abu Dhabi, UAE.\n\nChristos Christodoulopoulos Andrea Pierleoni\n\nAbstract\n\nExtracting structured and grounded fact triples from raw text is a fundamental task in Information Extraction (IE). Existing IE datasets are typically collected from Wikipedia articles, using hyperlinks to link entities to the Wikidata knowledge base. However, models trained only on Wikipedia have limitations when applied to web domains, which often contain noisy text or text that does not have any factual information. We present WebIE, the first large-scale, entity-linked closed IE dataset consisting of 1.6M sentences automatically collected from the English Common Crawl corpus. WebIE also includes negative examples, i.e. sentences without fact triples, to better reflect the data on the web. We annotate ∼similar-to\\scriptstyle\\mathtt{\\sim}21K triples from WebIE through crowdsourcing and introduce mWebIE, a translation of the annotated set in four other languages: French, Spanish, Portuguese, and Hindi. We evaluate the in-domain, out-of-domain, and zero-shot cross-lingual performance of generative IE models and find models trained on WebIE show better generalisability. We also propose three training strategies that use entity linking as an auxiliary task. Our experiments show that adding Entity-Linking objectives improves the faithfulness of our generative IE models.\n\n1 Introduction\n\nInformation Extraction (IE) is the task of extracting structured information from unstructured text, usually in the form of triples <subject, relation, object>. It is essential for many Natural Language Processing applications such as knowledge base population, question answering, faithful summarisation, and fake news detection (Trisedya et al., 2019; Huguet Cabot and Navigli, 2021; Narayan et al., 2021; Whitehouse et al., 2022).\n\nTypically, two pieces of information are needed for training closed IE systems: (i) the entities mentioned in the text and (ii) the relations that exist between each pair of entities. Obtaining such information requires expensive annotations, therefore most existing IE datasets, such as WikiNRE (Trisedya et al., 2019) or REBEL (Huguet Cabot and Navigli, 2021), are built using Wikipedia, as entity information is available through hyperlinks and relation information can be automatically extracted via distant supervision (DS) approach (Mintz et al., 2009) using a knowledge base (KB) such as Wikidata. The DS approach assumes that if two entities are connected through a relation in a KB, then the sentences that mention both entities together express the relation.\n\nWhile models trained only on this fact-rich domain have shown to be useful for IE applications, they have limited capacity when applied to extracting information in other web domains, which often contains noisy text or text without any factual information. Take AllenAI’s C4 dataset, an open-sourced version of Google’s C4 Raffel et al. (2020) dataset based on Common Crawl, as an example. Our analysis using the DS approach reveals that less than 15% of the sentences contain triples (§2.1), whereas we observe that a state-of-the-art (SOTA) generative IE model, GenIE Josifoski et al. (2022), which is trained on REBEL, the largest IE dataset to date (which includes only positive examples), tends to generate triples for every sentence, resulting in a high rate of false positives and issues with hallucination.\n\nTo address these issues and facilitate future work on IE on the web, we present WebIE, the first large-scale, entity-linked closed IE dataset collected from web sources. The WebIE dataset is collected from the 200 most frequent URL domains from the C4 dataset. First, we use ReFinED Ayoola et al. (2022), a state-of-the-art Entity Linking (EL) model to identify mention spans of the entities and link them to Wikidata. We then apply the DS approach to extract triples and use a Natural Language Inference (NLI) model to filter out triples not expressed by the sentence. We also include negative examples, i.e., sentences without any factual information, to better reflect the data on the web. Our final dataset consists of 1.6M sentences, and we annotate a subset of ∼similar-to\\scriptstyle\\mathtt{\\sim}21K triples through crowdsourcing. The annotated set is exclusively used as part of the test set to allow more reliable evaluation. Finally, we introduce mWebIE, which contains human-corrected translations of the annotated version of WebIE in four languages: French, Spanish, Portuguese, and Hindi.\n\nPrevious works have shown that compared to discriminative pipelines which often suffer from accumulative errors due to separate Entity Linking and Relation Extraction (RE) steps Mesquita et al. (2019); Trisedya et al. (2019); Josifoski et al. (2022), generative models achieve superior performance in many closed IE tasks. Therefore we primarily benchmark WebIE with generative, transformer-based encoder-decoder models, BART Lewis et al. (2020) and mBART Tang et al. (2021). The latter is used to evaluate the zero-shot cross-lingual transfer performance on mWebIE.\n\nWe further propose three training strategies (§3.2) that use entity linking as an auxiliary task for generative IE, namely joint generation with the linked-entity prompt (Entity-Prompt), multi-task learning with distinguished artificial prompt tokens (Artificial-Prompt), and training with an additional task-specific language model (LM) head (2LM-Heads). We find that training with EL as an auxiliary task overall leads to better and more faithful IE results. An illustration of these training strategies is provided in Figure 1.\n\nOur experiments show that compared to models trained only on Wikipedia datasets, models also trained on WebIE are more robust and generalisable, achieving a new SOTA performance on REBEL (§5) and competitive zero-shot performance on WikiNRE. We demonstrate that WebIE serves as a complementary dataset to existing datasets based on Wikipedia, and show that including negative examples is crucial for addressing false positives in generative IE.\n\nOur main contributions are as follows: (1) We present (m)WebIE, the first large-scale, entity-linked IE dataset on the web, where a subset is further annotated by humans and translated into four other languages; (2) We propose and study the effectiveness of using entity linking as an auxiliary task for generative IE with various training strategies; (3) Our comprehensive experiments demonstrate that models trained on WebIE exhibit better generalisability in Information Extraction on the web domain, including competitive zero-shot performance on IE tasks on Wikipedia.\n\n2 (m)WebIE\n\nIn this section, we provide a detailed explanation of the dataset collection process for (m)WebIE.\n\n2.1 Collecting WebIE\n\nData Preprocessing\n\nWe start with the English portion of the AllenAI’s C4 dataset and keep the most frequent 200 URL domains. We randomly sample 1M documents and use SpaCy for sentence segmentation. Sentences with fewer than 10 words are removed, resulting in ∼similar-to\\scriptstyle\\mathtt{\\sim}20M sentences.\n\nEntity Linking and DS Dataset\n\nNext, we run ReFinED (Ayoola et al., 2022), a state-of-the-art EL model on the sentences to identify entity spans and link them to their corresponding Wikidata ID. Besides named entities, ReFinED also extracts numerical entities that do not have Wikidata ID. In this work, we only consider numerical entities that express dates, and map them to the corresponding year for simplicity. Some examples of ReFinED processed output are included in Appendix B.\n\nAfter obtaining the entity-linked sentences, we apply the DS paradigm to retrieve the set of relations that exist between each pair of entities in each sentence using Wikidata (September 2022 dump) as our KB and build a DS dataset. After the above steps, we obtain WebIE DS dataset consisting of 21.2M entities and 4.8M triples.\n\nEntailment Filtering\n\nOne major drawback of the DS approach is that the triples extracted may or may not be expressed by the source sentence (Riedel et al., 2010). Following previous work on obtaining a cleaner version of the DS dataset (Huguet Cabot and Navigli, 2021; Vania et al., 2022), we apply an NLI model, nli-deberta-v3-large, that is trained on SNLI Bowman et al. (2015) and MultiNLI Williams et al. (2018), to filter out triples that do not entail the sentence.\n\nEach source sentence is treated as the premise and we use manually created templates (similar to Vania et al. (2022)) to convert a DS triple to one or more hypotheses. We then obtain the entailment probability score for each premise-hypothesis pair and take the maximum score for cases with multiple converted hypotheses. We set the threshold to be 0.7, similar to Huguet Cabot and Navigli (2021), and only keep triples with an entailment score above the threshold. We retain 2.1M triples (44% of the previous DS triples, see Table 1) after this filtering process.\n\nNegative Examples\n\nAfter the DS creation and NLI filtering steps, only less than 10% of the original sentences contain triples. To train models for extracting facts from the web and alleviate false positives, we include two kinds of negative examples in WebIE: (i) sentences with one or zero entities, and (ii) sentences with two or more entities, but without any factual information (i.e., no relation between the entities). We randomly sample negative instances covering both cases evenly and add them to WebIE. In the end, WebIE consists of 1.6M sentences, where 50% are negative examples. A summary of the statistics of WebIE with a comparison with other datasets is shown in Table 1. The dataset is randomly split into train/validation/test sets using a 90/5/5 split.\n\n2.2 Human Annotation\n\nExisting IE datasets, such as REBEL, are often automatically annotated using the DS approach, hence the labels can be noisy. To allow more reliable evaluation of WebIE, we randomly sample ∼similar-to\\scriptstyle\\mathtt{\\sim}21K triples from the most frequent 200 relations and annotate them with MTurk. Given a sentence, each HIT (Human Intelligence Task) is designed to verify if a DS triple is correctly expressed in the sentence. First, the annotators are asked to verify if the head entity (subject) and tail entity (object) are linked correctly. For each entity, we provide its Wikipedia title and link to its Wikidata page as additional context. After that, the annotators are asked to verify if the triple relation is correctly inferred from the sentence. Here, we provide the relation descriptions and example use cases of each relation. We ask three MTurk workers to annotate each DS triple and take the majority vote as the final label for each triple. A triple is considered valid if both entities are linked to the correct Wikidata entities and the relation is inferred by the sentence. An annotation interface is shown in Appendix C.\n\nTo ensure the annotation quality, we set qualifications with additional requirements for MTurk workers (see Appendix C for details). The agreement among the three annotators is high: 99.4% for the head entities, 99.2% for the tail entities, and 76.1% for the relations have all three annotators agreeing on the same label. After the majority vote, 92.1% of the triples are labelled as inferred and therefore kept as valid triples.\n\n2.3 Multilingual WebIE\n\nTo enable zero-shot cross-lingual transfer evaluation on WebIE, we further extend the annotated subset, with additional negative examples, to four other languages: French, Spanish, Portuguese, and Hindi. First, we use a neural machine translation model, the distilled 1.3B variant of NLLB-200 Costa-jussà et al. (2022), to translate the English sentences into the target languages. We then use MTurk to verify the translation and add entity span information in the translated sentences. We provide the English sentence (with the entity spans highlighted) and its translation, and first, ask the annotators to correct the translation. After that, MTurk workers are asked to mark the corresponding entity spans in the target language. We ask two annotators to complete the aforementioned HIT, and an additional worker to select the better translation, which is used in our final dataset. To obtain translations with higher quality, we restrict the region of the workers to countries where the target language is the official language. The final mWebIE consists of 9K instances in each language, which corresponds to roughly 90% of the 21K annotated triples.\n\n3 Generative Information Extraction\n\nThis section describes the training strategies that we use for benchmarking (m)WebIE.\n\n3.1 Sentence-to-Triples Generation\n\nWe use BART and mBART for all of our experiments. Given a sentence s𝑠s as input, we train the model to autoregressively generate the linearised triples t𝑡t as an output. Following the practice from Huguet Cabot and Navigli (2021) and Josifoski et al. (2022), we linearise a triple tisubscript𝑡𝑖t_{i} by converting it into ‘‘<sub> head entity label <rel> relation <obj> tail entity label <et>’’, where the tags in brackets represent subject, relation, object, and the end of triple, respectively. Head/tail entity label refers to the Wikipedia title that the mention span in the sentence is mapped to, which also has a one-to-one correspondence with the Wikidata ID.\n\nFor each sentence, we order its linearised triples accordingly to the order in which they appear in the input sentence; first by the order of the appearance of the head entity, and then by the order of the tail entity (for cases when the head entities are the same). The conditional probability of generating t𝑡t is formulated as p​(t|s)=∏t=0Np​(ti|t<i,s)𝑝conditional𝑡𝑠superscriptsubscriptproduct𝑡0𝑁𝑝conditionalsubscript𝑡𝑖subscript𝑡absent𝑖𝑠p(t|s)=\\prod_{t=0}^{N}p(t_{i}|t_{<i},s). We use the standard cross-entropy loss and maximise the output sequence likelihood with teacher forcing Sutskever et al. (2014). An example of input and output can be seen in the top left of Figure 1.\n\n3.2 Entity-Linking as an Auxiliary Task\n\nThe standard linearised triples output only contains the label of the entity and not the span. As a result, it may be difficult to trace back from which input span an entity is generated, especially in the case when the model hallucinates (e.g., by generating an entity that is not mentioned in the sentence). To encourage models to generate faithful and interpretable output, we also experiment with models that are jointly optimised for generating triples and EL. The goal of the EL task is to identify and extract entity spans from the input sentence and link them to their corresponding KB entities. We posit that adding the EL task as an additional training objective will teach the model to put attention to the input spans when generating the output. We experiment with the following three approaches.\n\nEntity-Prompt\n\nNarayan et al. (2021, 2022) have shown that generation with entity-chain planning, i.e. generating the desired entities first before the actual output, is effective in improving the faithfulness and controlling hallucinations in text generation tasks such as abstractive summarisation. For generative IE tasks, EL can be used as an intermediate plan to ground the generation of the linearised triples. We define the Entity-Linking target in the format of ‘‘Mention Span1 # Entity Label1 | Mention Span2 # Entity Label2 | …’’, where the entity spans are ordered as they appear in the text. We then prepend the EL target to the linearised triples target, using special symbols as separators, i.e., ‘‘[ENTITY] Entity-Linking target [TRIPLE] Linearised Triples Target’’, where ‘‘[ENTITY]’’ is the start symbol before generating the EL output, and ‘‘[TRIPLE]’’ is the start symbol before generating the linearised triples. Given an input sentence, we essentially train the decoder to first generate the EL chain and then generate the triples, conditioned on both the input sentence and the EL output.\n\nArtificial-Prompt\n\nArtificial Prompt tokens are symbols placed in front of the input sequence, which has previously been explored in areas such as neural machine translation to distinguish the language of the target output translation Johnson et al. (2017), visual question answering for joint answer and explanation generation Whitehouse et al. (2023), etc. We adapt this approach for jointly training our models for Entity Linking and generative IE. Specifically, we use an artificial prompt token <#el#> at the beginning of the input sentence when training for the Entity-Linking target, and use <#tri#> for the linearised output target. Training instances for both tasks are mixed and randomly shuffled for training.\n\n2LM-Heads\n\nFinally, inspired by Gontier et al. (2022), the third approach that we experiment with is the addition of a second language model (LM) head in the decoder, which is initialised with the same weights as the first (standard) LM head. The first LM head is optimised for generating the linearised triples while the second LM head is optimised for the EL task, thus each training instance has two different target outputs. During training, the input sentence is fed to the encoder once, and different target outputs are given to the same decoder. Each task-specific LM head is then responsible for generating output targeted for it. The training loss is then formulated as a weighted sum of the losses from both tasks: ℒ=α​ℒ​IE+(1−α)​ℒ​ELℒ𝛼ℒIE1𝛼ℒEL\\mathcal{L}=\\alpha\\mathcal{L}\\textsubscript{IE}+(1-\\alpha)\\mathcal{L}\\textsubscript{EL}.\n\n3.3 Inference with a Constraint Trie\n\nIn addition to standard beam search decoding, we experiment with constraint decoding by restricting the generated output to be valid Wikipedia titles and Wikidata relations using a prefix Trie, following the ideas proposed in GENRE Cao et al. (2021) and GenIE Josifoski et al. (2022). We use two constraint Tries: an entity Trie and a relation Trie. The entity Trie is built using all Wikipedia titles (as the entity labels), and the relation Trie is built using all Wikidata relation property labels. We refer the readers to Cao et al. (2021) for more details on constructing the Trie.\n\nWe use four special symbols, <sub>, <rel>, <obj> and <et> to define the state of the generation. We apply both constraint Tries as follows. We adopt the constraint Trie so that, in the very first decoding state, the model is allowed to either (i) return an empty string for a negative example, or (ii) generate <sub>, which is the start symbol for generating a triple. If the <sub> symbol is generated, then we generate the head entity using the entity Trie, i.e., only valid entities will be considered. Once the generation of the head entity is completed, the model proceeds to generate <rel> (i.e., the start symbol for generating relation string) and then subsequently generate allowed tokens from the relation Trie which is built from the relations in Wikidata. After that, the model generates <obj> and the tail entity, in the same manner, using the entity Trie. After generating the full triple (indicated by <et> generated after the tail entity), the decoder can either stop the generation or start a new iteration for generating the next triple.\n\nFor the Entity-Prompt models, since the entity mention spans are text from the input sentences and usually are not the same as the entity labels in Wikidata, we propose a partial constraint generation approach. Specifically, we start the standard beam search for the EL target output and only activate the Trie constraints after that when generating the linearised triples.\n\n4 Experiments\n\nIn this section, we explain the datasets used in the experiments and the detailed modelling setup.\n\n4.1 Dataset\n\nIn addition to our proposed WebIE dataset, we also use the following datasets for our experiments.\n\nWikiNRE\n\n(Trisedya et al., 2019) is an IE dataset based on Wikipedia which is automatically constructed by aligning Wikipedia sentences to Wikidata triples using the DS approach. The authors apply a coreference resolution model Clark and Manning (2016) to obtain sentences with implicit entity names, and use a paraphrase detection model Ganitkevitch et al. (2013); Grycner and Weikum (2016) to filter out sentences that do not express the DS triples. In our experiments, we only use WikiNRE for zero-shot evaluation.\n\nREBEL\n\n(Huguet Cabot and Navigli, 2021) is a large-scale IE dataset constructed automatically from Wikipedia abstracts. Using the Wikipedia hyperlinks in the abstracts, as well as numerical values and dates, they map the entity spans to their corresponding Wikidata entities. They then use the DS approach to identify triples in each sentence. To filter out false positives, the authors use an NLI model by concatenating the entities and the relation as the hypothesis. In our experiment, we use the REBEL dataset that is sub-sampled by Josifoski et al. (2022), where 857 relations are considered. Both WikiNRE and REBEL do not contain negative examples and are not annotated by humans.\n\n4.2 Models\n\nWe experiment with BART using two settings: Bartplm with the pre-trained weights from Lewis et al. (2020), and Bartrand, using the same configuration and architecture but randomly initialised weights. Across the two settings, Josifoski et al. (2022) find that Bartrand generates better results than Bartplm on REBEL. For mWebIE, we experiment with the mBART-50 model (for simplicity we refer to it as mBART in this paper).\n\nTo compare models trained on different datasets, we train both Bartplm and Bartrand on REBEL (r), WebIE (w), and both datasets together (r+w). We evaluate the performance of the generated triples by parsing the linearised output to a list of triples and comparing it to the gold label to calculate precision, recall, and F1 scores. For WebIE, we also calculate the accuracy of the prediction of negative instances, where a prediction is considered correct if the model accurately generates empty strings rather than hallucinating triples.\n\nFor training with EL as an auxiliary task, we primarily experiment with the Bartrand. We prepare the training instances as described in §3.2, and train separate models on REBEL and on WebIE. For the 2LM-Heads, we conduct experiments with different values of the α𝛼\\alpha parameter in the combined loss function, specifically, we set it to 0.5 and 0.75.\n\nWe use 8 GPUs, each with 32G VRAM, for all experiments. We set the batch size to 8 and accumulate gradient batches to 32. We follow the hyperparameters settings from Josifoski et al. (2022) and set the learning rate to 3​e−53superscript𝑒53e^{-5}, weight decay to 0.01, and warmup steps to 5K. We train for up to 30 epochs with early stopping (patience 10), validate twice per epoch, and take the last checkpoint for evaluation. Training one epoch takes ∼similar-to\\scriptstyle\\mathtt{\\sim}1.5 hours for BART and ∼similar-to\\scriptstyle\\mathtt{\\sim}2 hours for mBART.\n\n5 Results and Analysis\n\nWe now present the main results of (m)WebIE and compare different training strategies.\n\n5.1 Main Results\n\nTable 2 shows our benchmarking results on WebIE. We report results with the constraint Trie in decoding since it overall achieves better results. Contrary to the findings from Josifoski et al. (2022), we find that BART models with pre-trained weights are better than initialised weights. Constraint Trie decoding benefits REBEL, WikiNRE, and the recall performance of WebIE, but may compromise the precision since the models are also trained to handle negative examples.\n\nModels trained on both REBEL and WebIE (r+w) obtain overall better F1 scores on the two datasets compared to models trained on each dataset separately. Similar performance can also be observed in the zero-shot performance on WikiNRE. Models trained solely on the REBEL dataset (Wikipedia-domain) show poor generalisability on WebIE and always generate false positives thus resulting in 0% accuracy for negative instances in WebIE. This indicates that Wikipedia-domain data only is not adequate for training robust models for the web, and the absence of negative examples in these datasets leads to a prominent issue of hallucination when applied to the web.\n\nBartplm (r+w) also achieves a new state-of-the-art F1 score of 71.87 on REBEL, surpassing the performance of 68.93 from GenIE Josifoski et al. (2022) and 70.74 from KnowGL Rossiello et al. (2023), the latter of which trains with additional information including entity type. The results demonstrate the benefit of WebIE, which contributes to the generalisability of the models.\n\n5.2 Cross-lingual Transfer with mBART\n\nWe train mBART on the training set of WebIE and evaluate the zero-shot cross-lingual transfer on mWebIE. Similar to prior experiments, results in Table 3 show that constraint Trie decoding obtains higher performance than standard decoding.\n\nFor English, mBART achieves higher overall performance than Bartplm (see Table 2). The zero-shot results reveal that Hindi has a significant decline in performance compared to the other three non-English languages, French, Spanish, and Portuguese. Since these three languages utilise the Latin script as in English, which may result in an overlap of entity surface forms. In contrast, the transfer is more difficult for Hindi as it employs a different writing system. Manual analysis indicates that mBART tends to produce a high rate of false negatives in Hindi examples, where the correct extraction mostly occurs when the entities in the sentences share similar surface forms with the English counterparts.\n\n5.3 Results with Additional EL Training\n\nTable 4 shows the results of training with Entity-Linking as an auxiliary task. For REBEL, the best results are achieved with the 2LM-Heads approach, where the α𝛼\\alpha parameter is set to 0.75. For WebIE with negative examples, all EL training models achieve better F1 performance than Bartrand, with Entity-Prompt particularly resulting in better recall. This shows the benefit of joint training with EL to improve the faithfulness of web domain data. Artificial-Prompt achieves the best precision in WebIE but does not show significant differences in performance compared to Bartrand. Nevertheless, all three approaches provide better interpretability, i.e., the information of the mention spans in the text that contributes to the IE prediction.\n\nEntity-Prompt and Artificial-Prompt do not require additional architectural adaptation over the standard model. Entity-Prompt also does not introduce training overhead, whereas the other two models may require twice the training time. 2LM-Heads offers the flexibility of adapting the weighted combination of the main task and the auxiliary task by adjusting α𝛼\\alpha in the joint loss formula, which allows more emphasis on the main target.\n\n6 Related Work\n\nIE Datasets\n\nThe term Information Extraction has been used for different tasks in the literature. Most existing IE datasets are collected from Wikipedia articles aligned with Wikidata, including sentence-level IE datasets such as REBEL, WikiNRE, FewRel Han et al. (2018), T-REx Elsahar et al. (2018); document-level Relation Extraction datasets, e.g., DocRED Yao et al. (2019), CodRED Yao et al. (2021). SMiLER Seganti et al. (2021) is a multilingual sentence-level IE dataset that is also based on Wikipedia, covering 14 languages and 36 relations. These sentence-level IE datasets typically do not contain negative examples.\n\nDatasets such as TACRED Zhang et al. (2017), RE-TACRED Stoica et al. (2021), and WebRED (Ormandi et al., 2021) have negative relation examples but they are not linked to knowledge bases. Our proposed dataset WebIE is distinct from the existing datasets in that it is on the web domain, entity-linked, and with negative examples.\n\nIE Approaches\n\nIE approaches can be classified into two categories: pipeline systems with discriminative models, and sequence-to-sequence systems with generative models. Pipeline models typically include separate modules for Named Entity Recognition (NER), Entity Linking and Relation Extraction Chaganty et al. (2017); Yamada et al. (2020). Systems that jointly train NER, EL, and RE, have also been explored, taking advantage of the information shared among the tasks Ji et al. (2020); Eberts and Ulges (2020).\n\nIn recent years, generative IE has gained a lot of attention. Nayak and Ng (2020) utilise an LTSM model and propose a pointer network-based decoding. More recent approaches, e.g. as introduced in REBEL and GenIE, train a transformer-based encoder-decoder model with standard maximum-likelihood objectives to convert sentences to linearised output. KnowGL Rossiello et al. (2023) improves upon REBEL with additional entity type information added to the linearised output. Our work extends GenIE and experiments with three different approaches where we incorporate explicit EL information as an auxiliary task with adapted constraint Trie decoding.\n\n7 Conclusions\n\nWe present (m)WebIE, the first large-scale, entity-linked closed IE dataset on the web. A subset of the dataset is further annotated by humans and translated into four other languages, French, Spanish, Portuguese, and Hindi, via crowdsourcing.\n\nWe benchmark WebIE with generative models and compare the models trained on WebIE and REBEL (Wikipedia-domain). Our results show that models trained on WebIE have competitive zero-shot performance when applied to REBEL and WikiNRE, whereas models trained only on REBEL have 0% accuracy on the negative examples in WebIE. This highlights the importance of including negative examples for training more robust models and reducing hallucination in generative IE on the web. Models trained on both REBEL and WebIE achieve the best performance on both datasets, as well as zero-shot results on WikiNRE, showing that WebIE serves as a complementary dataset to existing Wikipedia-domain datasets.\n\nInvestigating the approaches with Entity Linking as an auxiliary task, we find that adding an additional task-specific LM head achieves the overall best performance for REBEL, and the Entity-Prompt approach shows the most significant improvement on WebIE, particularly benefiting recall. We primarily benchmark transformer-based encoder-decoder models on WebIE, but future work could also explore pipeline frameworks and larger language models for few-shot performance.\n\nLimitations\n\nWe identify several limitations in this work: (i) False negatives. Our current automatic triple extraction pipeline is built using the DS approach followed by filtering using an NLI model. However, Wikidata is not complete (Tan et al., 2022). While some triples may not be completely available in WebIE, we expect models trained on this dataset can still discover new triples that do not exist in Wikidata. (ii) Limited relations in annotation. The human annotation is only conducted on the most frequent 200 relations. (iii) Limited languages in mWebIE. As discussed in §2.3 and Appendix C, the languages in mWebIE are limited to official languages from geographical regions where there is a reasonable amount of MTurk workers to accept the job. An alternative solution would be to use professional translators, especially for low-resource languages. (iv) Fixed dataset. Facts might change in the world (and Wikidata). This can lead to a degraded real-world performance if a system relies exclusively on WebIE for evaluation when the dataset is not updated accordingly.\n\nAcknowledgements\n\nWe would like to thank Jens Lehmann for the helpful feedback on the paper draft, and Balkarn Hayre for helping with the MTurk experiments. We also thank the anonymous reviewers for their valuable comments that improved the paper.\n\nReferences\n\nAyoola et al. (2022) Tom Ayoola, Shubhi Tyagi, Joseph Fisher, Christos Christodoulopoulos, and Andrea Pierleoni. 2022. ReFinED: An efficient zero-shot-capable approach to end-to-end entity linking. In Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies: Industry Track, pages 209--220, Hybrid: Seattle, Washington + Online. Association for Computational Linguistics.\n\nBowman et al. (2015) Samuel R. Bowman, Gabor Angeli, Christopher Potts, and Christopher D. Manning. 2015. A large annotated corpus for learning natural language inference. In Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages 632--642, Lisbon, Portugal. Association for Computational Linguistics.\n\nCao et al. (2021) Nicola De Cao, Gautier Izacard, Sebastian Riedel, and Fabio Petroni. 2021. Autoregressive entity retrieval. In International Conference on Learning Representations.\n\nChaganty et al. (2017) Arun Chaganty, Ashwin Paranjape, Percy Liang, and Christopher D. Manning. 2017. Importance sampling for unbiased on-demand evaluation of knowledge base population. In Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing, pages 1038--1048, Copenhagen, Denmark. Association for Computational Linguistics.\n\nClark and Manning (2016) Kevin Clark and Christopher D. Manning. 2016. Improving coreference resolution by learning entity-level distributed representations. In Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 643--653, Berlin, Germany. Association for Computational Linguistics.\n\nCosta-jussà et al. (2022) Marta R Costa-jussà, James Cross, Onur Çelebi, Maha Elbayad, Kenneth Heafield, Kevin Heffernan, Elahe Kalbassi, Janice Lam, Daniel Licht, Jean Maillard, et al. 2022. No language left behind: Scaling human-centered machine translation. arXiv preprint arXiv:2207.04672.\n\nEberts and Ulges (2020) Markus Eberts and Adrian Ulges. 2020. Span-based joint entity and relation extraction with transformer pre-training. ECAI, page 2006–2013.\n\nElsahar et al. (2018) Hady Elsahar, Pavlos Vougiouklis, Arslen Remaci, Christophe Gravier, Jonathon Hare, Frederique Laforest, and Elena Simperl. 2018. T-REx: A large scale alignment of natural language with knowledge base triples. In Proceedings of the Eleventh International Conference on Language Resources and Evaluation (LREC 2018), Miyazaki, Japan. European Language Resources Association (ELRA).\n\nGanitkevitch et al. (2013) Juri Ganitkevitch, Benjamin Van Durme, and Chris Callison-Burch. 2013. PPDB: The paraphrase database. In Proceedings of the 2013 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 758--764, Atlanta, Georgia. Association for Computational Linguistics.\n\nGontier et al. (2022) Nicolas Gontier, Siva Reddy, and Christopher Pal. 2022. Does entity abstraction help generative transformers reason? Transactions on Machine Learning Research.\n\nGrycner and Weikum (2016) Adam Grycner and Gerhard Weikum. 2016. POLY: Mining relational paraphrases from multilingual sentences. In Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, pages 2183--2192, Austin, Texas. Association for Computational Linguistics.\n\nHan et al. (2018) Xu Han, Hao Zhu, Pengfei Yu, Ziyun Wang, Yuan Yao, Zhiyuan Liu, and Maosong Sun. 2018. FewRel: A large-scale supervised few-shot relation classification dataset with state-of-the-art evaluation. In Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 4803--4809, Brussels, Belgium. Association for Computational Linguistics.\n\nHuguet Cabot and Navigli (2021) Pere-Lluís Huguet Cabot and Roberto Navigli. 2021. REBEL: Relation extraction by end-to-end language generation. In Findings of the Association for Computational Linguistics: EMNLP 2021, pages 2370--2381, Punta Cana, Dominican Republic. Association for Computational Linguistics.\n\nJi et al. (2020) Bin Ji, Jie Yu, Shasha Li, Jun Ma, Qingbo Wu, Yusong Tan, and Huijun Liu. 2020. Span-based joint entity and relation extraction with attention-based span-specific and contextual semantic representations. In Proceedings of the 28th International Conference on Computational Linguistics, pages 88--99, Barcelona, Spain (Online). International Committee on Computational Linguistics.\n\nJohnson et al. (2017) Melvin Johnson, Mike Schuster, Quoc V. Le, Maxim Krikun, Yonghui Wu, Zhifeng Chen, Nikhil Thorat, Fernanda Viégas, Martin Wattenberg, Greg Corrado, Macduff Hughes, and Jeffrey Dean. 2017. Google’s multilingual neural machine translation system: Enabling zero-shot translation. Transactions of the Association for Computational Linguistics, 5:339--351.\n\nJosifoski et al. (2022) Martin Josifoski, Nicola De Cao, Maxime Peyrard, Fabio Petroni, and Robert West. 2022. GenIE: Generative information extraction. In Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 4626--4643, Seattle, United States. Association for Computational Linguistics.\n\nLewis et al. (2020) Mike Lewis, Yinhan Liu, Naman Goyal, Marjan Ghazvininejad, Abdelrahman Mohamed, Omer Levy, Veselin Stoyanov, and Luke Zettlemoyer. 2020. BART: Denoising sequence-to-sequence pre-training for natural language generation, translation, and comprehension. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 7871--7880, Online. Association for Computational Linguistics.\n\nMesquita et al. (2019) Filipe Mesquita, Matteo Cannaviccio, Jordan Schmidek, Paramita Mirza, and Denilson Barbosa. 2019. KnowledgeNet: A benchmark dataset for knowledge base population. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), pages 749--758, Hong Kong, China. Association for Computational Linguistics.\n\nMintz et al. (2009) Mike Mintz, Steven Bills, Rion Snow, and Daniel Jurafsky. 2009. Distant supervision for relation extraction without labeled data. In Proceedings of the Joint Conference of the 47th Annual Meeting of the ACL and the 4th International Joint Conference on Natural Language Processing of the AFNLP, pages 1003--1011, Suntec, Singapore. Association for Computational Linguistics.\n\nNarayan et al. (2022) Shashi Narayan, Gonçalo Simões, Yao Zhao, Joshua Maynez, Dipanjan Das, Michael Collins, and Mirella Lapata. 2022. A well-composed text is half done! composition sampling for diverse conditional generation. In Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 1319--1339, Dublin, Ireland. Association for Computational Linguistics.\n\nNarayan et al. (2021) Shashi Narayan, Yao Zhao, Joshua Maynez, Gonçalo Simões, Vitaly Nikolaev, and Ryan McDonald. 2021. Planning with learned entity prompts for abstractive summarization. Transactions of the Association for Computational Linguistics, 9:1475--1492.\n\nNayak and Ng (2020) Tapas Nayak and Hwee Tou Ng. 2020. Effective modeling of encoder-decoder architecture for joint entity and relation extraction. Proceedings of the AAAI Conference on Artificial Intelligence, 34(05):8528--8535.\n\nOrmandi et al. (2021) Robert Ormandi, Mohammad Saleh, Erin Winter, and Vinay Rao. 2021. Webred: Effective pretraining and finetuning for relation extraction on the web. arXiv preprint arXiv:2102.09681.\n\nRaffel et al. (2020) Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J. Liu. 2020. Exploring the limits of transfer learning with a unified text-to-text transformer. Journal of Machine Learning Research, 21(140):1--67.\n\nRiedel et al. (2010) Sebastian Riedel, Limin Yao, and Andrew McCallum. 2010. Modeling relations and their mentions without labeled text. In Machine Learning and Knowledge Discovery in Databases, pages 148--163, Berlin, Heidelberg. Springer Berlin Heidelberg.\n\nRossiello et al. (2023) Gaetano Rossiello, Md. Faisal Mahbub Chowdhury, Nandana Mihindukulasooriya, Owen Cornec, and Alfio Gliozzo. 2023. Knowgl: Knowledge generation and linking from text. In Proceedings of the AAAI Conference on Artificial Intelligence.\n\nSeganti et al. (2021) Alessandro Seganti, Klaudia Firląg, Helena Skowronska, Michał Satława, and Piotr Andruszkiewicz. 2021. Multilingual entity and relation extraction dataset and model. In Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics: Main Volume, pages 1946--1955, Online. Association for Computational Linguistics.\n\nStoica et al. (2021) George Stoica, Emmanouil Antonios Platanios, and Barnabas Poczos. 2021. Re-tacred: Addressing shortcomings of the tacred dataset. Proceedings of the AAAI Conference on Artificial Intelligence, 35(15):13843--13850.\n\nSutskever et al. (2014) Ilya Sutskever, Oriol Vinyals, and Quoc V Le. 2014. Sequence to sequence learning with neural networks. In Advances in Neural Information Processing Systems, volume 27. Curran Associates, Inc.\n\nTan et al. (2022) Qingyu Tan, Lu Xu, Lidong Bing, Hwee Tou Ng, and Sharifah Mahani Aljunied. 2022. Revisiting DocRED - addressing the false negative problem in relation extraction. In Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing, pages 8472--8487, Abu Dhabi, United Arab Emirates. Association for Computational Linguistics.\n\nTang et al. (2021) Yuqing Tang, Chau Tran, Xian Li, Peng-Jen Chen, Naman Goyal, Vishrav Chaudhary, Jiatao Gu, and Angela Fan. 2021. Multilingual translation from denoising pre-training. In Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021, pages 3450--3466, Online. Association for Computational Linguistics.\n\nTrisedya et al. (2019) Bayu Distiawan Trisedya, Gerhard Weikum, Jianzhong Qi, and Rui Zhang. 2019. Neural relation extraction for knowledge base enrichment. In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 229--240, Florence, Italy. Association for Computational Linguistics.\n\nVania et al. (2022) Clara Vania, Grace Lee, and Andrea Pierleoni. 2022. Improving distantly supervised document-level relation extraction through natural language inference. In Proceedings of the Third Workshop on Deep Learning for Low-Resource Natural Language Processing, pages 14--20, Hybrid. Association for Computational Linguistics.\n\nWhitehouse et al. (2023) Chenxi Whitehouse, Tillman Weyde, and Pranava Madhyastha. 2023. Towards a unified model for generating answers and explanations in visual question answering. In Findings of the Association for Computational Linguistics: EACL 2023, pages 1648--1660, Dubrovnik, Croatia. Association for Computational Linguistics.\n\nWhitehouse et al. (2022) Chenxi Whitehouse, Tillman Weyde, Pranava Madhyastha, and Nikos Komninos. 2022. Evaluation of fake news detection with knowledge-enhanced language models. In Proceedings of the International AAAI Conference on Web and Social Media, volume 16, pages 1425--1429.\n\nWilliams et al. (2018) Adina Williams, Nikita Nangia, and Samuel Bowman. 2018. A broad-coverage challenge corpus for sentence understanding through inference. In Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers), pages 1112--1122, New Orleans, Louisiana. Association for Computational Linguistics.\n\nYamada et al. (2020) Ikuya Yamada, Akari Asai, Hiroyuki Shindo, Hideaki Takeda, and Yuji Matsumoto. 2020. LUKE: Deep contextualized entity representations with entity-aware self-attention. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 6442--6454, Online. Association for Computational Linguistics.\n\nYao et al. (2021) Yuan Yao, Jiaju Du, Yankai Lin, Peng Li, Zhiyuan Liu, Jie Zhou, and Maosong Sun. 2021. CodRED: A cross-document relation extraction dataset for acquiring knowledge in the wild. In Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 4452--4472, Online and Punta Cana, Dominican Republic. Association for Computational Linguistics.\n\nYao et al. (2019) Yuan Yao, Deming Ye, Peng Li, Xu Han, Yankai Lin, Zhenghao Liu, Zhiyuan Liu, Lixin Huang, Jie Zhou, and Maosong Sun. 2019. DocRED: A large-scale document-level relation extraction dataset. In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 764--777, Florence, Italy. Association for Computational Linguistics.\n\nZhang et al. (2017) Yuhao Zhang, Victor Zhong, Danqi Chen, Gabor Angeli, and Christopher D. Manning. 2017. Position-aware attention and supervised data improve slot filling. In Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing, pages 35--45, Copenhagen, Denmark. Association for Computational Linguistics.\n\nAppendix A Additional Results\n\nWe show the full results in Table 5 for Bartrand and Bartplm trained on REBEL and WebIE, using both beam search with and without constraint Trie decoding.\n\nWe show in Table 6 the results for non-English languages for mWebIE when specifying the source language and using the default (English) for the mBART tokenizer. These results are from beam search without constraint Trie. We can see that specifying the source language mostly harms the performance (except French), especially for Portuguese. We hypothesise that due to the model being trained solely on English as the source token, mBART may have difficulty handling other languages.\n\nAppendix B Examples of ReFinED Output\n\nWe show examples of the sentences processed by ReFinED in Table 7. For each input sentence, ReFinED identifies the set of entities in that sentence, and outputs mention span, Wikidata id, and Wikipedia title for each entity. For our experiments, we use the wikipedia_model_with_numbers model with wikipedia entity set.\n\nAppendix C MTurk Annotation Details\n\nIn this section, we describe the detailed settings for annotating (m)WebIEwith MTurk.\n\nC.1 WebIE\n\nThe first annotation task (HIT) is to verify the correctness of the triples automatically created from the DS approach and filtered by the NLI model. The guidance and the interface are shown in Figure 2 and Figure 3, respectively.\n\nIn each HIT, we provide a sentence with its entities highlighted (head entity in blue and tail entity in green) and the URL of the web page which the sentence is extracted from. For the first EL annotation job, we provide both links to the Wikipedia and Wikidata pages. Annotators are asked to choose if the highlighted spans are linked correctly to the KB. Next, the annotators are asked to verify if a relation (highlighted in orange) can be inferred from the sentence. We provide the description of the relation and an example use case to facilitate the annotation.\n\nEach triple is annotated by three workers, and we pay $0.2 per HIT. We hire MTurk workers with Masters Qualification and set additional requirements including (i) having done 2,000 HITs and (ii) having a job approval rate ≥\\geq99%.\n\nC.2 mWebIE\n\nFigure 4 and Figure 5 illustrates the interface for correcting machine-translated sentence and identifying corresponding entities in them. As it is challenging to find qualified crowd workers for the translation task, we set the geographical regions for each language to the countries where the language is one of the official languages. We find that only India and countries in America have an adequate number of MTurk workers, which highly restricts the options for our target languages. In the end, the countries we set for the target languages are as follows: Portuguese: AO, BR, CV, ST, GW, GQ, MZ; Spanish: ES, MX, CO, PE, CL; CA for French, and IN for Hindi. It was also necessary to remove the Masters Qualification requirement for MTurk workers (except Hindi) to find adequate annotators. We then conduct pilot annotations, where we deliberately introduce errors in the reference machine translation to verify if the workers under our requirement settings are able to correct them.\n\nWe provide the English sentence paired with the original machine-translated sentence for the actual HIT. The English sentence is highlighted with its entity spans, and we instruct the workers to correct the translation while ensuring that the entities are correctly translated. After confirming the translation, workers are then asked to highlight the corresponding entities in the target language (in green). For negative sentences without entity spans, the longest noun phrases were highlighted instead to prevent workers from simply copying the reference translations. We pay $0.35 per HIT for positive sentences and $0.25 for negative sentences (since most sentences in negative examples have only one highlighted entity/noun phrase and it is considered an easier task).\n\nTwo MTurk workers are asked for the translation task, and an additional worker was asked to select the better translation, for which $0.10 per HIT was paid.\n\nAppendix D Domains in WebIE\n\nThe 200 URL domains included in WebIE are shown in Table 8.\n\nAppendix E Relations in the Annotated Set\n\nTable 9 shows the details of the 200 relations that are covered in the human-annotated set of WebIE."
    }
}
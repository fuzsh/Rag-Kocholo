by Jessica P. B. Hansen and Elizabeth Stokoe

“You’re on mute!” has become one of the most regularly uttered phrases in video-mediated communication, as much of our everyday and working life moved online during the coronavirus pandemic. And remember the point at which Zoom, Teams, and other platforms started informing us — via captions and audio describing — that “recording” was “in progress”? What if the next generation of platforms told us “The participants can see your presenter notes”? or “Your image is ten times larger than life-size in this meeting”?

In a recent UK television interview on the BBC’s The Graham Norton Show, actor Cate Blanchett described attending a movie premiere by Zoom and the moment she realized that her face was giant on a cinema screen behind a row of physically present cast and crew members:

Along with discussions about participants turning their camera on or off, or the strange distraction of virtual backgrounds and disappearing limbs, one of the most common complaints about remote encounters is the apparent lack of ‘body language’ they afford, compared to being co-present and in person. In this article, however, we flip the issue around to consider how video-meetings sometimes give us far too much access to people’s faces and bodies.

The issue of ‘who can see or access what’ runs in multiple directions. For example, while online participants may be unaware of how they are represented on someone else’s screen, in-person participants may not know what aspects of their physical environment can be accessed by those attending remotely.

Much is already written about the pivot to remote interaction and what counts as best practice for hybrid meetings. Many articles describe the best lighting, the right technology, how to organize an agenda, and how to run inclusive sessions or enable audience participation. However, far less attention has been paid about how to handle the way we get represented on each other’s computer or projector screens — and how much control we have over these representations.

For instance, how do we know whether someone is using a ‘speaker’ or ‘gallery’ view’? Is everyone participating using the same view? Is our image the same size or much larger than other participants? Are we giant or tiny? Do multiple screens mean that we think we’re looking at the camera while, to recipients, appear to be looking at nobody? Can people see that you are on Facebook because your screen is reflected in your glasses? Can people see your eyes glaze over because you are not at the back of a room but on a screen and the size of the 50 Foot Woman or the Wizard of Oz?

A related issue is whether participants have ‘self-view’ on or off. While many have commented on the strangeness and ‘fatigue’ of seeing one’s own face while interacting with others, when this is not the case in person, there are important reasons for seeing your face when online. ‘Self-view’ is about far more than our faces — it is about knowing and seeing what we can convey within the boundaries of our digital windows. This includes artefacts and aspects of our physical environment that we may show to camera or what we choose to hide by using a background setting or screen. Or we can turn the camera off altogether to reduce access to ourselves in ways we cannot do when in person.

Given that our faces and bodies may be “subjected to a different scrutiny while online” — and given that we may be relatively unaware or not in control of how we are represented on someone else’s screen — in this article we consider what might help to manage these new interactional matters. Before recommending five etiquette tips, we take a quick look at the science behind video-mediated meetings and other encounters.

Here comes the science

We tend to compare all forms of interaction and evaluate in-person as the ‘gold standard’ and remote communication as impoverished. However, it is important to keep in mind that humans have always used whatever resources are available to interact with subtlety and speed. On the telephone, for instance, we use voice and intonation but not gestures or aspects of our physical environment. This is why we say things like, “I’m just walking upstairs” (if we sound ‘out of breath’) or “my partner is just gesturing something at me” (if we are distracted by them, mid-conversation). Similarly, in written communication, a complex lexicon of emoticons, emojis, gifs, and other non-lexical resources has evolved to stand in for the way intonation modifies and conveys what our words are doing when we speak.

The challenge of creating effective interactional environments for remote collaboration is not new. The issue of how “to ensure that participants have compatible views of each other’s domains” and “common frames of reference” has been researched for at least twenty years preceding the pandemic across what some have termed the ‘fractured ecologies’ that remote communication creates.

Imagine you are presenting at an online event. You choose a specific portion of your screen that you want the audience to see. But, while presenting, you may have access only to your notes or the presentation slides, and not be able to see the audience’s video displays, chat or emojis. Does the audience know what you have access to, in terms of their participation? While someone might ask a question in the chat, you may still pursue responses from the audience, leaving their question unanswered — and, for that person, possibly feeling ignored. Meanwhile, you do not continue with your presentation until someone unmutes and responds verbally.

In scenarios like these, neither the presenter nor the audience knows what each other can see, hear, or has access to — and it is easy to make the wrong assumption. And this can lead to further assumptions, like the audience is not engaged, or the presenter is being rude.

The ‘fracturing’ of video-mediated environments gets even more interesting when different languages and interpreting are involved. For example, in video-mediated interpreting, an interpreter enables communication between people who do not share a language. Video technologies are increasingly used to provide interpreting services across a wide range of settings, such as in asylum court hearings and in medical consultations, and can be done in various ways depending on institutional practices and the distribution of participants.

In our research, we collected and analysed recordings of doctor-patient consultations that were enabled by an interpreter. The medical professionals and their patients were situated together in a hospital, while interpreters participated remotely. One of the striking things about these interactions was that although technology enabled a ‘face-to-face’ experience, the participants did not always use the technology in ways that ensured they each knew what the other parties could access and ‘see’:

In one case, the doctor was partially visible to the interpreter while the patient and their next-of-kin, also present at the consultation, were not visually available to the interpreter. When the doctor asked the patient, “how you doing”, the Norwegian second person singular pronoun “du” (you) caused an obvious problem — who was the “you” referring to? The interpreter could see that the doctor was gazing at someone, but not who. This lack of equal visual access for all participants and awareness of who can see what can create friction in even the most mundane moments of interaction — the ‘how are yous’ at the start. But the medical professionals and interpreters rarely discussed the practical matter of visual access, even after problems occurred.

As with all technology, however, human beings are pretty good at navigating complexity and developing and adapting practices to do things in the environments they create — from workplace and institutional settings to our personal and family lives. For instance, a teacher in an online Norwegian Sign Language class developed practices for addressing and referring to individual pupils by pointing in directions related to where on their screen the students were positioned. At first, the teacher’s pointing gestures were combined with other resources to identify pupils, such as using their names. However, across the course of the class — and even though the students could not see how their images were distributed across the teacher’s screen — the direction of points became mutually understood.

Other research has shown how people position their bodies and gestures with nuance and skill across many settings, from telemedicine and video-mediated physiotherapy, to the staying-connected conversations that migrant workers have with their young children. Even very young children can use the affordances of technology to interact with others:
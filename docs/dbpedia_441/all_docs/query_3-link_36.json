{
    "id": "dbpedia_441_3",
    "rank": 36,
    "data": {
        "url": "https://concepts.4angle.com/edward/Claude_Shannon:_information_theory",
        "read_more_link": "",
        "language": "en",
        "title": "Claude Shannon: information theory Concepts",
        "top_image": "",
        "meta_img": "",
        "images": [
            "https://concepts.4angle.com/book_cover/006943.jpg",
            "https://concepts.4angle.com/book_cover/001773.jpg",
            "https://concepts.4angle.com/book_cover/003947.jpg",
            "https://concepts.4angle.com/book_cover/003314.jpg",
            "https://concepts.4angle.com/book_cover/002753.jpg",
            "https://concepts.4angle.com/book_cover/005945.jpg",
            "https://concepts.4angle.com/book_cover/003219.jpg",
            "https://concepts.4angle.com/book_cover/006562.jpg",
            "https://concepts.4angle.com/book_cover/005369.jpg",
            "https://concepts.4angle.com/book_cover/005843.jpg",
            "https://concepts.4angle.com/book_cover/006554.jpg",
            "https://concepts.4angle.com/book_cover/003588.jpg",
            "https://concepts.4angle.com/book_cover/007020.jpg",
            "https://concepts.4angle.com/book_cover/004421.jpg",
            "https://concepts.4angle.com/book_cover/006503.jpg",
            "https://concepts.4angle.com/book_cover/004019.jpg",
            "https://concepts.4angle.com/book_cover/003404.jpg",
            "https://concepts.4angle.com/book_cover/001593.jpg",
            "https://concepts.4angle.com/book_cover/002520.jpg",
            "https://concepts.4angle.com/book_cover/004379.jpg",
            "https://concepts.4angle.com/book_cover/003458.jpg",
            "https://concepts.4angle.com/book_cover/003775.jpg",
            "https://concepts.4angle.com/book_cover/004981.jpg",
            "https://concepts.4angle.com/book_cover/003644.jpg",
            "https://concepts.4angle.com/book_cover/005092.jpg",
            "https://concepts.4angle.com/book_cover/002891.jpg",
            "https://concepts.4angle.com/book_cover/003381.jpg",
            "https://concepts.4angle.com/book_cover/001929.jpg",
            "https://concepts.4angle.com/book_cover/005846.jpg",
            "https://concepts.4angle.com/book_cover/002976.jpg",
            "https://concepts.4angle.com/book_cover/003898.jpg",
            "https://concepts.4angle.com/book_cover/007088.jpg",
            "https://concepts.4angle.com/book_cover/000419.jpg",
            "https://concepts.4angle.com/book_cover/006519.jpg",
            "https://concepts.4angle.com/book_cover/001891.jpg",
            "https://concepts.4angle.com/book_cover/003385.jpg",
            "https://concepts.4angle.com/book_cover/004453.jpg",
            "https://concepts.4angle.com/book_cover/002765.jpg",
            "https://concepts.4angle.com/book_cover/005751.jpg",
            "https://concepts.4angle.com/book_cover/007410.jpg",
            "https://concepts.4angle.com/book_cover/003180.jpg",
            "https://concepts.4angle.com/book_cover/001783.jpg",
            "https://concepts.4angle.com/book_cover/003464.jpg",
            "https://concepts.4angle.com/book_cover/006666.jpg",
            "https://concepts.4angle.com/book_cover/004783.jpg",
            "https://concepts.4angle.com/book_cover/004434.jpg",
            "https://concepts.4angle.com/book_cover/003178.jpg",
            "https://concepts.4angle.com/book_cover/004842.jpg",
            "https://concepts.4angle.com/book_cover/006552.jpg",
            "https://concepts.4angle.com/book_cover/005083.jpg",
            "https://concepts.4angle.com/book_cover/005666.jpg",
            "https://concepts.4angle.com/book_cover/006945.jpg",
            "https://concepts.4angle.com/book_cover/002615.jpg",
            "https://concepts.4angle.com/book_cover/004098.jpg",
            "https://concepts.4angle.com/book_cover/004591.jpg",
            "https://concepts.4angle.com/book_cover/005631.jpg",
            "https://concepts.4angle.com/book_cover/004166.jpg",
            "https://concepts.4angle.com/book_cover/003734.jpg",
            "https://concepts.4angle.com/book_cover/005394.jpg",
            "https://concepts.4angle.com/book_cover/006629.jpg",
            "https://concepts.4angle.com/book_cover/005294.jpg",
            "https://concepts.4angle.com/book_cover/005663.jpg",
            "https://concepts.4angle.com/book_cover/000361.jpg",
            "https://concepts.4angle.com/book_cover/000933.jpg",
            "https://concepts.4angle.com/book_cover/007470.jpg",
            "https://concepts.4angle.com/book_cover/000863.jpg",
            "https://concepts.4angle.com/book_cover/005251.jpg",
            "https://concepts.4angle.com/book_cover/000038.jpg",
            "https://concepts.4angle.com/book_cover/006892.jpg",
            "https://concepts.4angle.com/book_cover/001820.jpg",
            "https://concepts.4angle.com/book_cover/003742.jpg",
            "https://concepts.4angle.com/book_cover/003002.jpg",
            "https://concepts.4angle.com/book_cover/006984.jpg",
            "https://concepts.4angle.com/book_cover/002152.jpg",
            "https://concepts.4angle.com/book_cover/005540.jpg",
            "https://concepts.4angle.com/book_cover/003503.jpg",
            "https://concepts.4angle.com/book_cover/006667.jpg",
            "https://concepts.4angle.com/book_cover/002530.jpg",
            "https://concepts.4angle.com/book_cover/004397.jpg",
            "https://concepts.4angle.com/book_cover/002571.jpg",
            "https://concepts.4angle.com/book_cover/006617.jpg",
            "https://concepts.4angle.com/book_cover/007378.jpg",
            "https://concepts.4angle.com/book_cover/004621.jpg",
            "https://concepts.4angle.com/book_cover/004872.jpg",
            "https://concepts.4angle.com/book_cover/000054.jpg",
            "https://concepts.4angle.com/book_cover/006806.jpg",
            "https://concepts.4angle.com/book_cover/005027.jpg",
            "https://concepts.4angle.com/book_cover/005312.jpg",
            "https://concepts.4angle.com/book_cover/005957.jpg",
            "https://concepts.4angle.com/book_cover/001615.jpg",
            "https://concepts.4angle.com/book_cover/001803.jpg",
            "https://concepts.4angle.com/book_cover/001957.jpg",
            "https://concepts.4angle.com/book_cover/005477.jpg",
            "https://concepts.4angle.com/book_cover/002922.jpg",
            "https://concepts.4angle.com/book_cover/004262.jpg",
            "https://concepts.4angle.com/book_cover/003191.jpg",
            "https://concepts.4angle.com/book_cover/003001.jpg",
            "https://concepts.4angle.com/book_cover/001371.jpg",
            "https://concepts.4angle.com/book_cover/004103.jpg",
            "https://concepts.4angle.com/book_cover/006712.jpg",
            "https://concepts.4angle.com/book_cover/006882.jpg",
            "https://concepts.4angle.com/book_cover/003698.jpg",
            "https://concepts.4angle.com/book_cover/004203.jpg",
            "https://concepts.4angle.com/book_cover/001964.jpg",
            "https://concepts.4angle.com/book_cover/004710.jpg",
            "https://concepts.4angle.com/book_cover/002182.jpg",
            "https://concepts.4angle.com/book_cover/001753.jpg",
            "https://concepts.4angle.com/book_cover/006301.jpg",
            "https://concepts.4angle.com/book_cover/001885.jpg",
            "https://concepts.4angle.com/book_cover/006740.jpg",
            "https://concepts.4angle.com/book_cover/002463.jpg",
            "https://concepts.4angle.com/book_cover/001921.jpg",
            "https://concepts.4angle.com/book_cover/003696.jpg",
            "https://concepts.4angle.com/book_cover/003456.jpg",
            "https://concepts.4angle.com/book_cover/000278.jpg",
            "https://concepts.4angle.com/book_cover/000850.jpg",
            "https://concepts.4angle.com/book_cover/003133.jpg"
        ],
        "movies": [],
        "keywords": [],
        "meta_keywords": [
            ""
        ],
        "tags": null,
        "authors": [],
        "publish_date": null,
        "summary": "",
        "meta_description": "",
        "meta_lang": "en",
        "meta_favicon": "",
        "meta_site_name": "",
        "canonical_link": null,
        "text": "Fortune's Formula: The Untold Story of the Scientific Betting System That Beat the Casinos and Wall Street\n\nby William Poundstone\n\nPublished 18 Sep 2006\n\nSeveral of its scientists, notably Billy Kluver, collaborated with the New York avant-garde: John Cage, Robert Rauschenberg, Nam June Paik, Andy Warhol, David Tudor, and others, some of whom lived and worked steps away from Bell Labs’ Manhattan building on West Street. Many of these artists were acquainted with at least the name of Claude Shannon and the conceptual gist of his theory. To people like Cage and Rauschenberg, who were exploring how minimal a work of music or art may be, information theory appeared to have something to say—even if no one was ever entirely sure what. Shannon came to feel that information theory had been over-sold. In a 1956 editorial he gently derided the information theory “bandwagon.” People who did not understand the theory deeply were seizing on it as a trendy metaphor and overstating its relevance to fields remote from its origin.\n\n…\n\nPART ONE Entropy Claude Shannon LIFE IS A GAMBLE. There are few sure things, least of all in the competitive world of academic recruitment. Claude Shannon was as close to a sure thing as existed. That is why the Massachusetts Institute of Technology was prepared to do what was necessary to lure Shannon away from AT&T’s Bell Labs, and why the institute was delighted when Shannon became a visiting professor in 1956. Shannon had done what practically no one else had done since the Renaissance. He had single-handedly invented an important new science. Shannon’s information theory is an abstract science of communication that lies behind computers, the Internet, and all digital media.\n\n…\n\nWeaver’s essay presented information theory as a humanistic discipline—perhaps misleadingly so. Strongly influenced by Shannon, media theorist Marshall McLuhan coined the term “information age” in Understanding Media (1964). Oracular as some of his pronouncements were, McLuhan spoke loud and clear with that concise coinage. It captured the way the electronic media (still analog in the 1960s) were changing the world. It implied, more presciently than McLuhan could have known, that Claude Shannon was a prime mover in that revolution. There were earnest attempts to apply information theory to semantics, linguistics, psychology, economics, management, quantum physics, literary criticism, garden design, music, the visual arts, and even religion.\n\nThe Most Human Human: What Talking With Computers Teaches Us About What It Means to Be Alive\n\nby Brian Christian\n\nPublished 1 Mar 2011\n\nA Mathematical Theory of Communication It seems, at first glance, that information theory—the science of data transmission, data encryption, and data compression—would be mostly a question of engineering, having little to do with the psychological and philosophical questions that surround the Turing test and AI. But these two ships turn out to be sailing quite the same seas. The landmark paper that launched information theory is Claude Shannon’s 1948 “A Mathematical Theory of Communication,” and as it happens, this notion of scientifically evaluating “communication” binds information theory and the Turing test to each other from the get-go.\n\n…\n\nFor more, see Hofstadter’s I Am a Strange Loop. 56 Benjamin Seider, Gilad Hirschberger, Kristin Nelson, and Robert Levenson, “We Can Work It Out: Age Differences in Relational Pronouns, Physiology, and Behavior in Marital Conflict,” Psychology and Aging 24, no. 3 (September 2009), pp. 604–13. 10. High Surprisal 1 Claude Shannon, “A Mathematical Theory of Communication,” Bell System Technical Journal 27 (1948), pp. 379–423, 623–56. 2 average American teenager: Katie Hafner, “Texting May Be Taking a Toll,” New York Times, May 25, 2009. 3 The two are in fact related: For more information on the connections between Shannon (information) entropy and thermodynamic entropy, see, e.g., Edwin Jaynes, “Information Theory and Statistical Mechanics,” Physical Review 106, no. 4, (May 1957), pp. 620–30; and Edwin Jaynes, “Information Theory and Statistical Mechanics II,” Physical Review 108, no. 2 (October 1957), pp. 171–90. 4 Donald Barthelme, “Not-Knowing,” in Not-Knowing: The Essays and Interviews of Donald Barthelme, edited by Kim Herzinger (New York: Random House, 1997). 5 Jonathan Safran Foer, Extremely Loud and Incredibly Close (Boston: Houghton Mifflin, 2005). 6 The cloze test comes originally from W.\n\n…\n\nEpilogue: The Unsung Beauty of the Glassware Cabinet Acknowledgments Notes The beautiful changes as a forest is changed By a chameleon’s tuning his skin to it; As a mantis, arranged On a green leaf, grows Into it, makes the leaf leafier … –RICHARD WILBUR I think metaphysics is good if it improves everyday life; otherwise forget it. –ROBERT PIRSIG As President, I believe that robotics can inspire young people to pursue science and engineering. And I also want to keep an eye on those robots, in case they try anything. –BARACK OBAMA 0. Prologue Claude Shannon, artificial intelligence pioneer and founder of information theory, met his wife, Mary Elizabeth, at work. This was Bell Labs in Murray Hill, New Jersey, the early 1940s. He was an engineer, working on wartime cryptography and signal transmission. She was a computer. 1. Introduction: The Most Human Human I wake up five thousand miles from home in a hotel room with no shower: for the first time in fifteen years, I take a bath.\n\nThe Chip: How Two Americans Invented the Microchip and Launched a Revolution\n\nby T. R. Reid\n\nPublished 18 Dec 2007\n\nHe published another seminal paper, “A Mathematical Theory of Communication,” that launched an even more important new academic discipline known as information theory; today information theory is fundamental not only in electronics and computer science but also in linguistics, sociology, and numerous other fields. You could argue that Claude Shannon was the Alexander Graham Bell of the cellular phone, because mobile communications would be impossible without the basic formulas of information theory that Shannon devised. In 1949, Shannon published a monograph—once again, the first one ever written on the topic—called “Programming a Computer for Playing Chess.”\n\n…\n\nTo this day, the capacity of computers and other digital devices is still measured in bits; if a personal computer is rated at 64 megabits, that means it comes with enough random-access memory to store 64 million bits, or distinct pieces of information. The term is now used by digital designers everywhere, many of whom have probably never heard of Claude Shannon. Shannon wouldn’t mind that, though. He was not one to blow his own horn. During the years he taught information theory at MIT, he never mentioned that he was the creator of the academic discipline his students were studying, and seemed somewhat embarrassed when diligent students figured out that their prof was the progenitor. Early in 2001, Bell Labs set up an exhibit in Shannon’s honor, noting how many of his twentieth-century ideas have become part and parcel of daily life in the new century.\n\n…\n\nThere is also interesting Booleana in Mary Everest Boole, A Boolean Anthology (Association of Teachers of Mathematics, 1972). Dover Press deserves our gratitude for keeping in print a paperback version of George Boole’s masterpiece, The Laws of Thought (New York: Dover Publications, 1951). There is as yet no biography of Claude Shannon, but a reader might be interested in the book that launched the burgeoning field of information theory—that is, Claude E. Shannon, The Mathematical Theory of Communication (Champaign: University of Illinois Press, 1949). Computer history is just now emerging as an academic discipline of its own, and there will no doubt be some fine books written on the work of von Neumann, Turing, and other computer pioneers.\n\nThe Scandal of Money\n\nby George Gilder\n\nPublished 23 Feb 2016\n\nLike the electromagnetic spectrum, which bears all the messages of the Internet to and from your smartphone or computer, it must be rooted in the absolute speed of light, the ultimate guarantor of the integrity of time. Dominating our own era and revealing in fundamental ways the nature of money is the information theory of Kurt Gödel, John von Neumann, Alan Turing, and Claude Shannon. Information theory tells us that information is not order but disorder, not the predictable regularity that contains no news, but the unexpected modulation, the surprising bits. But human creativity and surprise depend upon a matrix of regularities, from the laws of physics to the stability of money.4 Information theory has impelled the global ascendancy of information technology. From worldwide webs of glass and light to a boom in biotech based on treating life itself as chiefly an information system, a new system of the world is transforming our lives.\n\n…\n\nThe Stanford physicist and Nobel laureate Robert Laughlin has derided the elaborate efforts of scientists to find significance in the intrinsically transitory forms that arise on their computers during phase changes, such as bubbles in water on the brink of a boil.7 These computational figments have an analogue here in the outside traders’ search for momentary correlations. As Claude Shannon knew, in principle a creative pattern of data points—reflecting long and purposeful preparation and invention—is indistinguishable from a random pattern. Both are high entropy. Parsing of random patterns for transitory correlations fails to yield new knowledge. You cannot meaningfully study the ups and downs of the market with an oscilloscope. You need a microscope, exploring inside the cells of individual companies. Currency values should be stable. In information theory terms, they should function as low-entropy carriers for high-entropy creations.\n\n…\n\nBecause we use it to prioritize most of our activities, register and endow our accomplishments of learning and invention, and organize the life-sustaining work of our society, money is more than a mere payments system. It expresses a system of the world. That is why I link it to the information theory of Kurt Gödel, Alan Turing, and Claude Shannon. Each of these thinkers attempted to define his philosophy in utilitarian and determinist mathematics. Addressing pure logic as math, Gödel concluded that even arithmetic cannot constitute a complete and coherent system. All logical schemes have to move beyond self-referential circularity and invoke axioms outside themselves.\n\nA World Without Email: Reimagining Work in an Age of Communication Overload\n\nby Cal Newport\n\nPublished 2 Mar 2021\n\nWhen I was writing my master’s thesis at MIT in the electrical engineering and computer science department (the field Shannon created from scratch with his 1937 work), we heard about Shannon’s spectacular student efforts. In retrospect, I’m not sure if this was supposed to motivate us or demoralize us. 2. For a more complete treatment of Claude Shannon, I recommend Jimmy Soni and Rob Goodman’s fascinating 2017 biography, which was the source for much of the summary that follows: A Mind at Play: How Claude Shannon Invented the Information Age (New York: Simon & Schuster, 2017). 3. Information theorists would traditionally use the word code instead of protocol in this instance, but for the sake of clarity in the discussion we’re having here, I’m going to use protocol—as in a set of communication rules agreed on in advance—as it sidesteps the colloquial associations people hold with respect to the word code. 4.\n\n…\n\nWhether you’re deploying complex automation or just following handcrafted procedures, these processes will reduce your dependence on the hyperactive hive mind workflow and reward you with extra cognitive energy and mental peace. Make automatic what you can reasonably make automatic, and only then worry about what to do with what remains. Chapter 6 The Protocol Principle The Invention of Information Claude Shannon is one of the most important figures in twentieth-century science, yet few outside the specialized fields he helped innovate know his name. Perhaps his largest intellectual leap was his 1937 MIT master’s thesis, which he submitted at the age of twenty-one and, among other contributions, laid the foundation for all of digital electronics.1 But it’s toward another of his most famous works that I’ll turn our attention now, as it will prove useful in our quest to move beyond the hyperactive hive mind workflow.\n\n…\n\nYou might think that the gains here are small—how hard is it to send some emails?—but if you’re like me, you’ll likely be surprised by the feeling of a burden being lifted when you eliminate all these ongoing scheduling conversations, which have a way of nibbling at the borders of your concentration, driving you again and again back into the hive mind chatter. Claude Shannon’s framework underscores this reality. Meeting-scheduling protocols induce a small extra inconvenience cost, as you have to set up the system, and your correspondents now have to select times from a website instead of simply shooting back a short email reply in the moment. But the cognitive cycles saved are so substantial that there’s no comparison: the average cost of these meeting-scheduling protocols is significantly lower than what’s required by the status quo of energy-minimizing email ping-pong.\n\nA Man for All Markets\n\nby Edward O. Thorp\n\nPublished 15 Nov 2016\n\nThorp’s method is as follows: He cuts to the chase in identifying a clear edge (that is something that in the long run puts the odds in his favor). The edge has to be obvious and uncomplicated. For instance, calculating the momentum of a roulette wheel, which he did with the first wearable computer (and with no less a coconspirator than the great Claude Shannon, father of information theory), he estimated a typical edge of roughly 40 percent per bet. But that part is easy, very easy. It is capturing the edge, converting it into dollars in the bank, restaurant meals, interesting cruises, and Christmas gifts to friends and family—that’s the hard part. It is the dosage of your betting—not too little, not too much—that matters in the end.\n\n…\n\nTo protect myself from this happening with my work on blackjack, I settled on Proceedings of the National Academy of Sciences, as it was the quickest to publish of any journal I knew, taking as little as two or three months, and was also very prestigious. This required a member of the academy to approve and forward my work, so I sought out the only mathematics member of the academy at MIT, Claude Shannon. Claude was famous for the creation of information theory, which is crucial for modern computing, communications, and much more. The department secretary arranged a short appointment with a reluctant Shannon at noon. However, she warned me that Shannon was going to be in for only a few minutes, that I shouldn’t expect more, and that he didn’t spend time on topics or people that didn’t interest him.\n\n…\n\nDuring the long ride back I wondered how my research into the mathematical theory of a game might change my life. In the abstract, life is a mixture of chance and choice. Chance can be thought of as the cards you are dealt in life. Choice is how you play them. I chose to investigate blackjack. As a result, chance offered me a new set of unexpected opportunities. Ever since my first meeting with Claude Shannon in September, we had been working on the roulette project approximately twenty hours a week. Meanwhile, I was teaching courses, doing research in pure mathematics, attending department functions, writing up my blackjack research, and adjusting to being a new father. Following a roulette work session at the Shannons’, Claude asked me at dinner if I thought anything would ever top this in my life.\n\nNine Algorithms That Changed the Future: The Ingenious Ideas That Drive Today's Computers\n\nby John MacCormick and Chris Bishop\n\nPublished 27 Dec 2011\n\nSo it is not altogether surprising that the two major events triggering the creation of error-correcting codes both occurred in the research laboratories of the Bell Telephone Company. The two heroes of our story, Claude Shannon and Richard Hamming, were both researchers at Bell Labs. Hamming we have met already: it was his annoyance at the weekend crashes of a company computer that led directly to his invention of the first error-correcting codes, now known as Hamming codes. However, error-correcting codes are just one part of a larger discipline called information theory, and most computer scientists trace the birth of the field of information theory to a 1948 paper by Claude Shannon. This extraordinary paper, entitled “The Mathematical Theory of Communication,” is described in one biography of Shannon as “the Magna Carta of the information age.”\n\n…\n\nTHE ORIGINS OF COMPRESSION ALGORITHMS The same-as-earlier trick described in this chapter—one of the main compression methods used in ZIP files—is known to computer scientists as the LZ77 algorithm. It was invented by two Israeli computer scientists, Abraham Lempel and Jacob Ziv, and published in 1977. To trace the origins of compression algorithms, however, we need to delve three decades further back into scientific history. We have already met Claude Shannon, the Bell Labs scientist who founded the field of information theory with his 1948 paper. Shannon was one of the two main heroes in our story of error-correcting codes (chapter 5), but he and his 1948 paper also figure importantly in the emergence of compression algorithms. This is no coincidence. In fact, error-correcting codes and compression algorithms are two sides of the same coin.\n\n…\n\nMathematicians will greatly enjoy Thompson's delightful book, but it definitely assumes the reader has a healthy dose of college math. Dewdney's book (see above) has two interesting chapters on coding theory. The two quotations about Shannon on pages 77-78 are taken from a brief biography by N. J. A. Sloane and A. D. Wyner, appearing in Claude Shannon: Collected Papers edited by Sloane and Wyner (1993). Pattern recognition (chapter 6). Bishop's lectures (see above) have some interesting material that nicely complements this chapter. The geographical data about political donations is taken from the Fundrace project of the Huffington Post. All the handwritten digit data is taken from a dataset provided by Yann LeCun, of New York University's Courant Institute, and his collaborators.\n\nThinking Machines: The Inside Story of Artificial Intelligence and Our Race to Build the Future\n\nby Luke Dormehl\n\nPublished 10 Aug 2016\n\n‘But most of this is probably used in remembering visual impressions, and other comparatively wasteful ways. One might reasonably hope to be able to make some real progress [towards Artificial Intelligence] with a few million digits [of computer memory].’ The third of AI’s forefathers was a man named Claude Shannon, known today as the father of ‘information theory’. Born in 1916 – making him the youngest of the three – Shannon’s big contribution to computing related to the way in which transistors work. Transistors are the billions of tiny switches that make up a computer. An algorithm is the sequence of instructions that tells a computer what to do by switching these transistors on and off.\n\n…\n\nCompared with the unreliable memory of humans, a machine capable of accessing thousands of items in the span of microseconds had a clear advantage. There are entire books written about the birth of modern computing, but three men stand out as laying the philosophical and technical groundwork for the field that became known as Artificial Intelligence: John von Neumann, Alan Turing and Claude Shannon. A native of Hungary, von Neumann was born in 1903 into a Jewish banking family in Budapest. In 1930, he arrived at Princeton University as a maths teacher and, by 1933, had established himself as one of six professors in the new Institute for Advanced Study in Princeton: a position he stayed in until the day he died.\n\n…\n\nIn the summer of 1956 – when Elvis Presley was scandalising audiences with his hip gyrations, Marilyn Monroe married playwright Arthur Miller, and President Dwight Eisenhower authorised ‘In God we trust’ as the US national motto – AI’s first official conference took place. A rolling six-week workshop, bringing together the smartest academics from a broad range of disciplines, the event unfolded on the sprawling 269-acre estate of Dartmouth College in Hanover, New England. Along with Claude Shannon, two of the organisers were young men named John McCarthy and Marvin Minsky, both of whom became significant players in the growing field of Artificial Intelligence. ‘The study [of AI] is to proceed on the basis of the conjecture that every aspect of learning or any other feature of intelligence can be so precisely described that a machine can be made to simulate it,’ they wrote.\n\nThe Dream Machine: J.C.R. Licklider and the Revolution That Made Computing Personal\n\nby M. Mitchell Waldrop\n\nPublished 14 Apr 2001\n\nAnd yet even in the midst of all that, Claude Shannon's long-delayed opus on information theory exploded like a bomb. His analysis of communication was breathtaking in scope, masterful in execution-and, for most people, totally unexpected. \"It was like a bolt out of the blue, a really unique thing,\" recalls his Bell Labs colleague John Pierce. \"I don't know of any other theory that came in a complete form like that, with very few antecedents or history.\" \"It was a revelation,\" agrees Oliver Selfridge. \"Around MIT the reaction was 'Brilliant! Why didn't I think of that?' Information theory gave us a whole con- ceptual vocabulary, as well as a technical vocabulary.\"\n\n…\n\nNEW KINDS OF PEOPLE 75 Miller kept on reading. And by the time he was finished, he now says, he knew that his life had changed. THE CONJURER Legend has it that Claude Shannon published \"A Mathematical Theory of Com- munication\" in 1948 only because his boss at Bell Labs finally badgered him into it. And whatever the truth of that story, the point is that no one who knew Shannon has any trouble believing it. \"He wrote beautiful papers-when he wrote,\" says Robert Fano, who became a leader of MIT's information-theory group in the 1950s and still has a reveren- tial photograph of Shannon hanging in his office. \"And he gave beautiful talks- when he gave a talk.\n\n…\n\nIndeed, his meet- ing with Chomsky was as pivotal as the moment eight years earlier when he'd picked up that July 1948 issue of the Bell Systems Technical Journal containing Claude Shannon's article-and all the more so because Chomsky's message was to be strongly reinforced just a short time later. Miller remembers the day very clearly: Tuesday, September 11, 1956, the second day of the second international conference on information theory. Actually, the whole conference was good. Held in MIT's Sloan Building, right on the river- front, it included talks by Jerry Wiesner, Bob Fano, Peter Elias, Oliver Selfridge, Walter Rosenblith, and even Shannon himself.\n\nTuring's Vision: The Birth of Computer Science\n\nby Chris Bernhardt\n\nPublished 12 May 2016\n\nAfter Turing completed his Ph.D., von Neumann offered him a position as his assistant, but Turing decided not to accept and instead returned to England. During the time that Turing was working on his Ph.D. another breakthrough paper was written. This was on logic and switching circuits and was written by Claude Shannon. Claude Shannon In 1936, Claude Shannon graduated from the University of Michigan with two undergraduate degrees; one in electrical engineering and one in mathematics. He then went to M.I.T. for graduate school. At M.I.T. he worked on an early analog computer. This work led him to consider switches and digital computing.\n\n…\n\nHis first bombe was operational in the spring of 1940.7 In 1942, after the United States joined the war, Turing went to America to work with cryptologists there, helping with the design of American bombes. While in America, he visited Bell Labs, which was then in lower Manhattan, where he met Claude Shannon. In addition to working on cryptology, they discussed their work on computing.8 At the end of the war there were a large number of bombes working in both the US and England. Being able to read Germany’s messages enabled the allies to locate German U-boats and the routes of ships. It helped with the choice of when and where to attack.\n\n…\n\nRejewsksi, when asked about this, is said to have replied that he couldn’t think of a better name. 8. Both Shannon and Turing were interested in using ideas from probability to extract information from data. Shannon would later extend some of his wartime work and write the groundbreaking paper “A Mathematical Theory of Communication” that is one of the foundations of Information Theory. Turing wrote several articles on the application of probability to cryptography. These were classified and only now are being made available to the public (two papers were declassified in 2012. They are available at http://www.nationalarchives.gov.uk). 9. The English use the term valve where the Americans use vacuum tube. 10.\n\nThe Deep Learning Revolution (The MIT Press)\n\nby Terrence J. Sejnowski\n\nPublished 27 Sep 2018\n\nWhen you make a cell phone call, your voice is encoded into bits and transmitted over radio waves and digital transmission lines to a receiver, where the digital signals are decoded and converted to sounds. Information theory puts bounds on the capacity of the communications channel (figure 15.2), and codes have been devised that approach the Shannon limit. Despite the many forms of information in the world, there is a way to measure precisely how much of it is in a data set. The unit of information is a “binary bit,” which can take on a value of 1 or 0. A “byte” is eight bits. 220 Chapter 15 Figure 15.1 Claude Shannon around 1963 in front of a telephone switching network. He worked at AT&T Bell Laboratories when he invented information theory. From Alfred Eisenstaedt/The LIFE Picture Collection/Getty Images.\n\n…\n\nThe National Security Agency uses machine learning to sift through all of the data it has been collecting everywhere. The economy is going digital, and programming skills are in great demand at many companies. As the world shifts from an industrial to an information economy, education and job training will have to adapt. This already is having a profound impact on the world. Information Theory In 1948, Claude Shannon (figure 15.1) at the AT&T Bell Laboratories in Murray Hill, New Jersey, proposed a remarkably simple but subtle theory for information to understand signal transmission through noisy phone lines.1 Shannon’s theory drove the digital communications revolution that gave rise to cell phones, digital television, and the Internet.\n\n…\n\nThe latest growth spurt has been fueled by the widespread availability of big data, and the story of NIPS has been one of preparing for this day to come. III Technological and Scientific Impact Timeline 1971—Noam Chomsky publishes “The Case against B. F. Skinner” in the New York Review of Books, an essay that steered a generation of cognitive scientists away from learning. 1982—Claude Shannon publishes the seminal book A Mathematical Theory of Communication, which laid the foundation for modern digital communication. 1989—Carver Mead publishes Analog VLSI and Neural Systems, founding the field of neuromorphic engineering, which builds computer chips inspired by biology. 2002—Stephen Wolfram publishes A New Kind of Science, which explored the computational capabilities of cellular automata, algorithms that are even simpler than neural networks but still capable of powerful computing. 2005—Sebastian Thrun’s team wins the DARPA Grand Challenge for an autonomous vehicle. 2008—Tobias Delbrück develops a highly successful spiking retina chip called the “Dynamic Vision Sensor” (DVS) that uses asynchronous spikes rather than synchronous frames used in current digital cameras. 2013—U.S.\n\nCrypto: How the Code Rebels Beat the Government Saving Privacy in the Digital Age\n\nby Steven Levy\n\nPublished 15 Jan 2002\n\nBut he never did. (Worse, he came to confuse this book with another book published at that time, David Kahn’s The Codebreakers, which delayed his reading of the more important work.) Similarly, one day at Mitre, a colleague moving out of his office gave Diffie a 1949 paper by Claude Shannon. The legendary father of information theory had been teaching at MIT since 1956, but Diffie had never met him, a slight, introverted professor who lived a quiet family life, pursuing a variety of interests from reading science fiction to listening to jazz. (Presumably, by the time Shannon had reached his sixties, he had put aside the unicycle he had once mastered.)\n\n…\n\nThe answer was cryptography. Though Tuchman had a background in information theory, he had never specifically done any crypto work. But he soon found out about the system that the guys in IBM research at Yorktown Heights had cooked up. He ventured down to Watson Labs one day and heard Feistel speak about Lucifer. He immediately set up a lunch with Feistel and Alan Konheim. The first thing Tuchman asked Feistel was where he had gotten the ideas for Lucifer. Feistel, in his distinctive German accent, mentioned the early papers of Claude Shannon. “The Shannon paper reveals all,” he said. Meanwhile, Tuchman’s colleague Karl Meyer was exploring whether Lucifer might be a good fit for an expanded version of the Lloyd’s Cashpoint system.\n\n…\n\nThough Hellman didn’t work directly with Horst Feistel, the German-born cryptographer worked nearby in the building, and sometimes the two of them would sit together at lunch, where the older man would describe some of the classical cryptosystems and some of the means of breaking them. Hellman left IBM in 1970, accepting a post as assistant professor at MIT. At that time Peter Elias, who had worked closely with Claude Shannon, was just stepping down as the head of the electronic engineering department. Elias’s talks with Hellman drew the young academic deeper into crypto, and for the first time he began thinking about making it the focus of his research. “Partially, it was the magician aspect, being able to impress people with magic tricks,” he now explains.\n\nWhy Information Grows: The Evolution of Order, From Atoms to Economies\n\nby Cesar Hidalgo\n\nPublished 1 Jun 2015\n\nWhat is surprising to most people, however, is that information is meaningless, even though the meaningless nature of information, much like its physicality, is often misunderstood. In 1949 Claude Shannon and Warren Weaver published a short book entitled The Mathematical Theory of Communication. In its first section, Weaver described the conceptual aspects of information. In the second section, Shannon described the mathematics of what we now know as information theory. For information theory to be properly understood, Shannon and Weaver needed to detach the word information from its colloquial meaning. Weaver made this distinction early on his essay: “The word information, in this theory, is used in a special sense that must not be confused with its ordinary usage.\n\n…\n\nThis is another way of saying that the $2.5 million worth of value was stored not in the car’s atoms but in the way those atoms were arranged.3 That arrangement is information.4 So the value of the Bugatti is connected to physical order, which is information, even though people still debate what information is.5 According to Claude Shannon, the father of information theory, information is a measure of the minimum volume of communication required to uniquely specify a message. That is, it’s the number of bits we need to communicate an arrangement, like the arrangement of atoms that made the Bugatti. To grasp Shannon’s definition of information firmly, however, it is better to start with something simpler than a Bugatti.\n\n…\n\nMathematicians continued to formalize the idea of information, but they framed their efforts in the context of communication technologies, transcending the efforts to decipher intercepted messages. The mathematicians who triumphed became known as the world’s first information theorists or cyberneticists. These pioneers included Claude Shannon, Warren Weaver, Alan Turing, and Norbert Wiener. In the 1950s and 1960s the idea of information took science by storm. Information was welcomed in all academic fields as a powerful concept that cut across scientific boundaries. Information was neither microscopic nor macroscopic.3 It could be inscribed sparsely on clay tablets or packed densely in a strand of DNA.\n\nWonderland: How Play Made the Modern World\n\nby Steven Johnson\n\nPublished 15 Nov 2016\n\nHe was, instead, a computer scientist from MIT named Edward Thorp, who had come to Vegas not to break the bank but rather to test a brand-new device: the very first wearable computer ever designed. Thorp had an accomplice at the roulette table, standing unobserved at the other end, pretending not to know his partner. He would have been unrecognizable to the average casino patron, but he was in fact one of the most important minds of the postwar era: Claude Shannon, the father of information theory and one of the key participants in the invention of digital computers. Thorp had begun thinking about beating the odds at roulette as a graduate student in physics at UCLA in 1955. Unlike card games like blackjack or poker where strategy could make a profound difference in outcomes, roulette was supposed to be a game of pure chance; the ball was equally likely to end up on any number on the wheel.\n\n…\n\nTuring’s speculations form a kind of origin point for two parallel paths that would run through the rest of the century: building intelligence into computers by teaching them to play chess, and studying humans playing chess as a way of understanding our own intelligence. Those interpretative paths would lead to some extraordinary breakthroughs: from the early work on cybernetics and game theory from people like Claude Shannon and John von Neumann, to machines like IBM’s Deep Blue that could defeat grandmasters with ease. In cognitive science, the litany of insights that derived from the study of chess could almost fill an entire textbook, insights that have helped us understand the human capacity for problem solving, pattern recognition, visual memory, and the crucial skill that scientists call, somewhat awkwardly, chunking, which involves grouping a collection of ideas or facts into a single “chunk” so that they can be processed and remembered as a unit.\n\n…\n\nImpressed by Thorp’s blackjack system, Shannon inquired whether Thorp was working on anything else “in the gambling area.” Dormant for five years, Thorp’s roulette investigation was suddenly reawakened as the two men began a furious year of activity, seeking a predictable pattern in the apparent randomness of the roulette wheel. Claude Shannon with an electronic mouse In his old, rambling wooden house outside of Cambridge, Shannon had created a basement exploratorium that would have astounded Merlin and Babbage. Thorp later described it as a “gadgeteer’s paradise”: It had perhaps a hundred thousand dollars (about six hundred thousand 1998 dollars) worth of electronic, electrical and mechanical items.\n\nAstounding: John W. Campbell, Isaac Asimov, Robert A. Heinlein, L. Ron Hubbard, and the Golden Age of Science Fiction\n\nby Alec Nevala-Lee\n\nPublished 22 Oct 2018\n\nCampbell even reached out to Wiener himself, writing that his former professor would “be greatly interested” in dianetics “as suggesting a new direction of development of the work from the cybernetics side,” and concluding, “Further study of dianetics will be of immense aid in your projects.” He also contacted his neighbor, the mathematician Claude Shannon, who had founded the field of information theory at Bell Labs. Shannon encouraged Warren McCulloch to meet Hubbard: “If you read science fiction as avidly as I do you’ll recognize him as one of the best writers in that field. . . . [He] has been doing some very interesting work lately in using a modified hypnotic technique for therapeutic purposes. . . .\n\n…\n\nAt that point, Hubbard took over, claiming to have spent eleven years on observations of “the medicine man of the Goldi people of Manchuria, the shamans of North Borneo, Sioux medicine men, the cults of Los Angeles, and modern psychology. . . . Odds and ends like these, countless odds and ends.” The article reflected this hodgepodge of influences, comparing the analytic mind to “a well-greased Univac,” alluding to demon circuits, and quoting Claude Shannon and Warren McCulloch. Hubbard provided no real description of the therapy itself, but he concluded with what might have been the motto of the postwar Astounding: “Up there are the stars. Down in the arsenal is an atom bomb. Which one is it going to be?” The issue also carried an advertisement for Dianetics: The Modern Science of Mental Health, which was scheduled to be released by Hermitage House on April 19.\n\n…\n\nIt was exactly the attitude that had infuriated him in Parker, and his refusal to be pinned down undermined any attempts to investigate the subject seriously. Many were skeptical, and when a fan asked Campbell whether the Hieronymus Machine was a hoax, like Asimov’s articles about thiotimoline, the editor seemed horrified by the implication. There were inquiries from Bell Aircraft and the RAND Corporation, and Claude Shannon offered to test it, although the timing never worked out. Campbell soon moved on to other causes, and Hieronymus himself felt that the editor had set back acceptance of his work by a century. The symbolic machine, he said, functioned because the ink conducted lines of force, but when it came to serious research, it wasn’t worth “a tinker’s damn.”\n\nWhen Things Start to Think\n\nby Neil A. Gershenfeld\n\nPublished 15 Feb 1999\n\nSince then computer chess has been studied by a who's who of computing pioneers who took it to be a defining challenge for what came to be known as Artificial Intelligence. It was thought that if a machine could win at chess it would have to draw on fundamental insights into how humans think. Claude Shannon, the inventor of Information Theory, which provides the foundation for modern digital communications, designed a simple chess program in 1949 and was able to get it running to play endgames. The first program that could play a full game of chess was developed at IBM in 1957, and an MIT computer won the first BIT BELIEFS + 129 tournament match against a human player in 1967.\n\n…\n\nThere is a disconnect between the breathless pronouncements of cyber gurus and the experience of ordinary people left perpetually upgrading hardware to meet the demands of new software, or wondering where their files have gone, or trying to understand why they can't connect to the network. The revolution so far has been for the computers, not the people. Digital data of all kinds, whether an e-mail message or a movie, is encoded as a string of O's and 1's because of a remarkable discovery by Claude Shannon and John von Neumann in the 1940s. Prior to their work, it was obvious that engineered systems degraded with time and use. A tape recording sounds worse after it is duplicated, a photocopy is less satisfactory than an original, a telephone call becomes more garbled the farther it has to travel.\n\n…\n\nAn important step came in 1929 when Leo Szilard reduced the problem to its essence with a single molecule that could be on either side of a partition. While he wasn't able to solve the demon paradox, this introduced the notion of a \"bit\" of information. Szilard's one-bit analysis of Maxwell's demon provided the inspiration for Claude Shannon's theory of information in 1948. Just as the steam engine powered the Industrial Revolution, electronic communications was powering an information revolution. And just as finding the capacity of a steam engine was a matter of some industrial import, the growing demand for communications links required an understanding of how many messages could be sent through a wire.\n\nFrom Counterculture to Cyberculture: Stewart Brand, the Whole Earth Network, and the Rise of Digital Utopianism\n\nby Fred Turner\n\nPublished 31 Aug 2006\n\nIn his book Cybernetics; or, Control and Communication in the Animal and the Machine, he deﬁned cybernetics as a ﬁeld focused on “the study of messages as a means of controlling machinery and society,” with machinery seeming to include, by analogy at least, biological organisms. For Wiener, the world, like the anti-aircraft predictor, was composed of systems linked by, and to some extent made out of, messages. Drawing on Claude Shannon’s information theory (published in 1948, but likely familiar to Wiener much earlier), Wiener deﬁned messages as “forms of pattern and organization.”37 Like Shannon’s information, Wiener’s messages were surrounded by “noise,” yet they somehow maintained their integrity. So too did organisms and machines: incorporating and responding to feedback through structural mechanisms, Wiener explained, both kept themselves in a state of homeostasis.\n\n…\n\nCybernetics, as the theory of control mechanisms in technology and nature and founded on the concepts of information and feedback, is but a part of a general theory of systems; cybernetic systems are a special case, however important, of systems showing self-regulation.” Bertalanffy, General System Theory, 3. For Bertalanffy, cybernetics was only one root of systems theory, albeit an important one. Others included the servomechanisms of the nineteenth century, Claude Shannon’s information theory, Von Neumann and Morgenstern’s game theory, and the increasing need in the post–World War II world to monitor and control large systems for social functions such as trafﬁc and ﬁnance. For a critical analysis of the relationship between cybernetics and other systems theories, see Lilienfeld, Rise of Systems Theory. 44.\n\n…\n\nSirius), 164 Goldstein, Emmanuel, 168, 169 Gore, Al, 219 Graham, Bill, 66 graphical user interface, 111 Grateful Dead, 13, 65, 66, 166 Great Society, 26 Greenblatt, Richard, 136 Greenspan, Alan, 215 Griesemer, James, 72 [ 319 ] Grooms, Red, 48 Gullichsen, Eric, 163 Gurdjieff, Georges Ivanovitch, Meetings with Remarkable Men, 187 hacker ethic, 134 –35, 136 hackers, 117, 132 –35, 133 Hackers’ Conference, 132, 137–38, 168, 169, 171, 219, 249, 254 hacking, as a free-speech issue, 169 Hafner, Katie, 143, 145, 221, 252 Hagel, John, Net Gain, 234 Haight-Ashbury, 32, 48, 66 – 67 Hapgood, Fred, 221–22 “happenings,” 48, 49, 67, 269n14 “hardware hackers,” 133 Harman, Willis, 61, 185, 274n12 Harper’s Magazine, 167 Hart, Pam, 117 Harvard Business Review, analysis of Out of Control, 204 –5 Harvey, David, 242 Hawken, Paul, 128, 185, 188 Hayles, Katherine, 26, 122 Hedgepeth, William, 77 Hefner, Christy, 211 Heims, Steve, 26, 122 Helmreich, Stefan, 198 Hermosillo, Carmen, 155 Herring, Susan, 152 Hertzfeld, Andy, 135 heterarchy, 156 Hewlett-Packard, 138 Hickman, Berry, 96 High Frontiers (’zine), 164 Hillis, Danny, 182, 183, 189 hippies, 32 Hiroshima, 16 Hitt, Jack, 167 Hofmann, Albert, 164 Hog Farm commune, 110 Holm, Richard, 44 Homebrew Computer Club, 70, 102, 106, 114 homeostat, 26, 146, 178 Horowitz, Ed, 208 Hoyt, Brad, 193 HTML code, 222 Hudson Institute, 186 Hudson Review, 47 human-machine collaboration, 108 –9, 111 hyperlinks, 213 [ 320 ] Index I Ching, 65, 82, 93 I. M. Pei, 178 Industry Standard, 207 information: economic paradox of, 136 –37; free dissemination of, 137 Information Processing Techniques Ofﬁce, 108 Information Superhighway, 219 information system, material world imagined as, 15 information theory: and American art, 268n13; of Claude Shannon, 265n43; and microbiology, 43 – 44 Information Week, 131 Innis, Harold, 52, 269n21 Institute for Advanced Study, 185 Intel, 212 Intercontinental Ballistic Missile, 24 interdisciplinary migration, 58 International Federation for Advanced Study (IFAS), 61 Internet, 247; growth of, 160, 214; as infrastructure and symbol of new economic era, 7; as the New Millennium, 232 –36; privatization of backbone, 213; as symbol of a post-Fordist economic order, 202; utopian claims surrounding the emergence of the, 1–3, 33 Internet stocks, 214, 232 Inuit, 53 IT-290, 60 Jackson, Charles, 211 Jennings, Lois, 70 Jerome, Judson, 32 Jobs, Steve, 133, 138 Johnson, Lyndon, 26 Joselit, David, 46 journalism, shaping of public perceptions, 253 Joy, Bill, 220 juxtaposition, 84 Kahn, Herman, 130, 181, 186, 197 Kahn, Lloyd, 94, 95, 97 Kanter, Rosabeth Moss, 76, 271n9 Kapor, Mitch, 171–72, 218 Kaprow, Allan, 46, 48, 58, 67 Katz, Bruce, 211, 277n1 Kay, Alan, 111–13, 117, 177, 246 Kay, Lily, 44 Kelly, Kevin, 3, 16, 131–32; account of “vivisystems,” 200; commercial sphere as a site of social change, 202 –3; “computational metaphor,” 216; concept of “hive mind,” 201, 202, 204; doctrine of cyberevolutionism, 204; editorial model, 195; editor of Signal, 196; editor of Whole Earth Review, 177, 195 –96; as executive director of Wired, 7, 206, 209, 212, 217; and ﬁrst Hackers’ Conference, 195; forum on hacking on the WELL, 168 –70; and Gilder, 223; and hacking community, 135; Internet as symbol of post-Fordist economy, 202; longing to return to an egalitarian world, 248; as network entrepreneur, 194 –99; “New Rules for the New Economy,” 15, 234 –35; Out of Control: The Rise of Neo-Biological Civilization, 176, 195, 199 –206; response to 1987 conference on artiﬁcial life, 199; review of Electric Word, 211; underplayed the work of embodied labor, 204; and the WELL, 148; on WELL design goals, 143 Keniston, Kenneth, 31; The Young Radicals, 261– 62 Kennedy, Alison (aka Queen Mu), 163 Kennedy, John F., 229, 271n10 Kent State University killings, 98, 118 Kepler’s bookstore, 70 Kerouac, Jack, 62 Kerr, Clark, 11, 12 Kesey, Ken: and geodesic dome, 94; leadership of Merry Pranksters, 63, 65, 67; and LSD, 61, 63; notion of Acid Test, 65; One Flew Over the Cuckoo’s Nest, 59 – 60, 64; rejection of agonistic politics, 64; subject of CIA experimental drug protocols, 60 – 61; and the Supplement, 81; and Trips Festival, 66; at Vietnam Day in 1965, 98 Keyworth, George, 222; “Magna Carta for the Knowledge Age,” 228 –30 Kleiner, Art, 131, 132, 135, 185 Kline, David, 287n37 Korzybski, Alfred, 62 Kravitz, Henry, 211 Kubrick, Stanley, 186 Kuhr, Barbara, 211, 285n2 Lama Foundation, 75, 76, 94, 97, 109, 119 Lampson, Butler, 111 Langton, Christopher, 198 Language Technology (magazine), 211 Lanier, Jaron, 163, 165, 172, 195 laser printer, 111 Index Last Whole Earth Catalog, 70, 81, 98, 112, 118 Learning Conferences, 181– 84 Leary, Timothy, 51, 163, 164, 165 legitimacy exchange, 25 –26, 84, 85, 88, 95, 250 Lehr, Stan, 210 Levy, Steven, 137, 139, 195; Hackers: Heroes of the Computer Revolution, 132 –35 Leyden, Peter, 233 –34 Libertarianism, 210, 249, 259, 287n49 Libre commune, 81, 94, 96, 109 Licklider, Joseph C.\n\nThe Fractalist\n\nby Benoit Mandelbrot\n\nPublished 30 Oct 2012\n\nTurning to me, he continued, “It seems that you don’t believe me!” I responded, “Of course I do. I tailored it to my needs, and am delighted that they also fit yours.” In a serious vein, liaising was a good opportunity to scout for Ph.D. topics. At Caltech, I had read the seed papers in which Claude Shannon founded information theory, and I badly wanted to know more. A get-together in London on this topic attracted me greatly, so I asked if I could attend. The air force obliged and sent me there. It was my first scientific conference. An Extended Sentence? The end of my twelve months of duty was approaching, and I was counting days.\n\n…\n\nTherefore, RLE was often filled with either the aroma of a traditional chocolate factory or the stench of a rendering plant that boiled carrion into pure white soap. I took all this as constant confirmation that the process of creation is intrinsically messy and suffers more from soulless order than from surrounding physical decay. Controversial Balance Between Conjecture and Proof Claude Shannon (1916–2001) was the intellectual leader whose wartime work, published in 1948, created information theory and provided RLE with an intellectual backbone. His work on noiseless channels was a point of departure for the theory of word frequencies presented in my Ph.D. thesis. But far more impressive was his noisy channel theorem. Actually, it was not a theorem at all, only a brilliant conjecture—in a style that is controversial, and of which I eventually became a very active supplier.\n\n…\n\nThe timing was ideal because several new developments that had been “bottled up” by war conditions were being revealed in a kind of fireworks I saw on no other occasion. My restless curiosity led me to read works that were widely discussed when they appeared: Mathematical Theory of Communication by Claude Shannon, Cybernetics, or Control and Communication in the Animal and the Machine by Norbert Wiener, and Theory of Games and Economic Behavior by John von Neumann and Oskar Morgenstern. Except for a fleeting thought that I might return to mathematics in 1949 via the University of Chicago, I was beginning to think that the examples of Wiener and von Neumann might guide me to an idea big enough to make me, in some way, the Delbrück of a new field.\n\nThe End of College: Creating the Future of Learning and the University of Everywhere\n\nby Kevin Carey\n\nPublished 3 Mar 2015\n\nBill was enacted, the director of the national Office of Scientific Research and Development, Vannevar Bush, sent a report to President Truman titled Science: The Endless Frontier. Bush had a doctorate in electrical engineering from MIT, where he had served as a scientist and administrator. He and his colleagues had made important contributions to the emerging development of computer science; his student Claude Shannon helped develop the information theory that sits at the heart of modern computing. Science, Bush said, was a source of great good for humanity. Penicillin and other medical advances had saved countless lives. “In 1939 millions of people were employed in industries which did not even exist at the close of the last war—radio, air conditioning, rayon and other synthetic fibers, and plastics. . . .\n\n…\n\nSix months later, Simon attended a conference at Dartmouth College, where he and a small group of scientists gave a name to this new field of research: artificial intelligence. The study of the human mind and the exploding power of information technology were coming together, and the smartest people in the world were in the middle of the action. Among the Dartmouth participants was Claude Shannon, a former student of Vannevar Bush at MIT and one of the fathers of modern information theory. In addition to creating the blueprint for the Cold War university, Bush had also seen the future technological revolution. In a 1945 Atlantic article titled “As We May Think,” Bush observed that various fields of manufacturing and computing were on trajectories of improvement that would soon lead to changes that, although they might be unknowable in the specific, were highly predictable in general.\n\n…\n\nUntil, that is, the summer of 2011, when a computer science professor named Sebastian Thrun had a kind of inspiration. Thrun was born in Germany and trained in computer science and statistics at the University of Bonn. His specialty was artificial intelligence, continuing the project first outlined at the Dartmouth conference by Herbert Simon, Claude Shannon, and others back in 1956. Carnegie Mellon hired Thrun as a professor in 1995, and he spent most of the next decade in Pittsburgh working at the intersection of computer science, statistics, and machines. Artificial intelligence had come in and out of fashion over the years. Initial hopes for replicating the human mind in silicon had proved highly optimistic as the parallel march of neuroscience and cognitive psychology revealed how fantastically complicated human cognition truly was.\n\nThe Myth of Artificial Intelligence: Why Computers Can't Think the Way We Do\n\nby Erik J. Larson\n\nPublished 5 Apr 2021\n\nTo win at chess, it is not enough to apply the rules; you have to know which rules to select in the first place. Turing saw chess as a handy (and no doubt entertaining) way to think about machines and the possibility of giving them intuition. Across the Atlantic, the founder of modern information theory, Turing’s colleague and friend Claude Shannon at Bell Labs, was also thinking about chess. He later built one of the first chess-playing computers, an extension of work he had done earlier on a proto-computer called the “differential analyzer,” which could convert certain problems in calculus into mechanical procedures.1 THE SIMPLIFICATION OF INTELLIGENCE BEGINS Chess fascinated Turing and his colleagues in part because it seemed that a computer could be programmed to play it, without the human programmer needing to know everything in advance.\n\n…\n\nThe shift away from linguistics and rule-based approaches to data-driven or “empirical” methods seemed to liberate AI from those early, cloudy days of work on machine translation, when seemingly endless problems with capturing meaning and context plagued engineering efforts. In fact, machine translation itself was later cracked by a group of IBM researchers using a statistical (that is, not grammar-based) approach that was essentially an ingenious application of Claude Shannon’s early work on information theory. Called the “noisy channel” approach, it viewed sentences from a source language (say, French) and a target language (say, English) as an information exchange in which bad translations constituted a form of noise—making it the system’s task to reduce the noise in the translation channel between source and target sentences.\n\n…\n\nBletchley, meanwhile, also proved a haven for thinking about computation: Bombes were machines, and they ran programs to solve problems that humans, by themselves, could not. INTUITIVE MACHINES? NO. For Turing, Bletchley played a major role in crystallizing his ideas about the possibility of intelligent machines. Like his colleagues Jack Good and Claude Shannon, Turing saw the power and utility of their “brain games” as cryptanalysts during the war: they could decipher messages that were otherwise completely opaque to the military. The new methods of computation were not just interesting for considering automated chess-playing. Computation could, quite literally, sink warships.\n\nThe Fourth Age: Smart Robots, Conscious Computers, and the Future of Humanity\n\nby Byron Reese\n\nPublished 23 Apr 2018\n\nIn addition to the computer’s memory, there might also be external storage to hold data and information not currently needed. Throw in input and output devices, and one has a von Neumann setup. If, when you were reading that, your brain mapped it to your computer’s CPU, memory, hard drive, keyboard, and monitor, then move to the head of the class. Finally, in 1949, Claude Shannon wrote a paper entitled “Programming a Computer for Playing Chess” in which he described a way to reduce chess to a series of calculations that could be performed on a computer. While this may not sound like it should earn Shannon one of the four spots on the Mount Rushmore of computer history, for the first time, in a practical and realistic way, computers were thought of as not just machines to perform mathematical calculations.\n\n…\n\nHas the Fourth Age begun yet? Well, when just a few humans learned to farm, or a few isolated places developed writing, did that mark the beginning of a new age or the beginning of the end of an old one? It matters very little where we draw the line. Whether it already happened decades ago when Claude Shannon explained how a computer could be programmed to play chess, or whether it will happen in a few years when a computer can carry on a complex conversation with a human using natural language, this kind of hairsplitting is not particularly meaningful. Let’s say that the transition began no earlier than 1950 and will complete no later than 2050.\n\n…\n\nThe goal was to “find how to make machines use language, form abstractions and concepts, solve [the] kinds of problems now reserved for humans, and improve themselves.” To do this, he put together a group of four computer scientists who were already thinking about machines that could think: himself, Marvin Minsky, Nathaniel Rochester, and Claude Shannon, whom we met earlier when we discussed the first chess-playing computer program. Their proposal then added a very, shall we say, “optimistic” prediction, especially given that it was 1955: “We think that a significant advance can be made in one or more of these problems if a carefully selected group of scientists work on it together for a summer.”\n\nThe Road Ahead\n\nby Bill Gates , Nathan Myhrvold and Peter Rinearson\n\nPublished 15 Nov 1995\n\nThis is why a computer's capacity to compress digital data, store or transmit it, then expand it back into its original form is so useful and will become more so. Quickly, here's how the computer accomplishes these feats. It goes back to Claude Shannon, the mathematician who in the 1930s recognized how to express information in binary form. During World War II, he began developing a mathematical description of information and founded a field that later became known as information theory. Shannon defined information as the reduction of uncertainty. By this definition, if you already know it is Saturday and someone tells you it is Saturday, you haven't been given any information.\n\n…\n\nIt is hard to sort out the paternity of the modern computer, because much of the thinking and work was done in the United States and Britain during World War II under the cloak of wartime secrecy. Three major contributors were Alan Turing, Claude Shannon, and John von Neumann. In the mid-1930s, Alan Turing, like Babbage a superlative Cambridge-trained British mathematician, proposed what is known today as a Turing machine. It was his version of a completely general-purpose calculating machine that could be instructed to work with almost any kind of information. In the late 1930s, when Claude Shannon was still a student, he demonstrated that a machine executing logical instructions could manipulate information. His insight, the subject of his master's thesis, was about how computer circuits—closed for true and open for false—could perform logical operations, using the number 1 to represent \"true\" and 0 to represent \"false.\"\n\n…\n\nBy this definition, if you already know it is Saturday and someone tells you it is Saturday, you haven't been given any information. On the other hand, if you're not sure of the day and someone tells you it is Saturday, you've been given information, because your uncertainty has been reduced. Shannon's information theory eventually led to other break-throughs. One was effective data-compression, vital to both computing and communications. On the face of it what he said is obvious: Those parts of data that don't provide unique information are redundant and can be eliminated. Headline writers leave out nonessential words, as do people paying by the word to send a telegraph message or place a classified advertisement.\n\nIntertwingled: The Work and Influence of Ted Nelson (History of Computing)\n\nby Douglas R. Dechow\n\nPublished 2 Jul 2015\n\nXanadu is a daring design that presented awesome challenges: how to link to evolving documents, how to track changes to a document, how to manipulate linkages, how to organize archival storage, how to name the target of a link, how to track micro-copyright royalties, how to organize the physical storage of a universe of discourse and how to scale storage and processing around the world. Many people are still skeptical of the need for bi-directional links. I am one who suspects links might only occasionally need to be bi-directional, or that a pair of one-way links could simulate a bi-directional link. Claude Shannon’s popular demonstration of his computer-controlled maze-navigating mouse was essential to the success of his project. Shannon’s demonstration appeared as a segment in the television show, Time Machine: Robots [2]. Shannon went to a lot of trouble to prepare a tabletop maze and to eliminate any arm or cord connecting the mouse to the computer.\n\n…\n\nOpen Access This chapter is distributed under the terms of the Creative Commons Attribution Noncommercial License, which permits any noncommercial use, distribution, and reproduction in any medium, provided the original author(s) and source are credited. References 1. Nelson TH (1974) Computer lib: you can and must understand computers now/dream Machines. Hugo’s Book Service, Chicago 2. Time Machine: Robots. Robots. History Channel. Aug.–Sept. 2000. Television. See segment on Claude, Shannon. 1952. Theseus Maze-Solving Mouse. (begins at 9:16 in the video). http://​youtu.​be/​KmURvu4x0Do Part III Hypertext and Ted Nelson-Influenced Research © The Author(s) 2015 Douglas R. Dechow and Daniele C. Struppa (eds.)IntertwingledHistory of Computing10.1007/978-3-319-16925-5_9 9.\n\n…\n\nFrom Nelson, Computer Lib/Dream Machines (Courtesy of Theodor Holm Nelson) The problem of the relationship between coding and thinking has always been central to the work of Theodor Holm Nelson, and a key aspect of his influence both inside and outside computer fields has been his unwavering insistence on the epistemological consequences of this relationship, often discussed under the rubric he calls “systems humanism.” While there is every reason to read Nelson as a figure in the modern history of information theory and design, there are as many reasons to read him in the stream of the contemporary humanities. More concretely, there are excellent reasons to consider Nelson’s work—from his earliest efforts such as the literary journal, Nothing, through to his visionary samizdat manifesto, Computer Lib/Dream Machines, and his recent work reconceptualizing the spreadsheet—as a guide to the universe of paper as it is to that of the screen.\n\nArtificial Intelligence: A Guide for Thinking Humans\n\nby Melanie Mitchell\n\nPublished 14 Oct 2019\n\nIn graduate school in the mathematics department at Princeton, McCarthy had met a fellow student, Marvin Minsky, who shared his fascination with the potential of intelligent computers. After graduating, McCarthy had short-lived stints at Bell Labs and IBM, where he collaborated, respectively, with Claude Shannon, the inventor of information theory, and Nathaniel Rochester, a pioneering electrical engineer. Once at Dartmouth, McCarthy persuaded Minsky, Shannon, and Rochester to help him organize “a 2 month, 10 man study of artificial intelligence to be carried out during the summer of 1956.”1 The term artificial intelligence was McCarthy’s invention; he wanted to distinguish this field from a related effort called cybernetics.2 McCarthy later admitted that no one really liked the name—after all, the goal was genuine, not “artificial,” intelligence—but “I had to call it something, so I called it ‘Artificial Intelligence.’”3 The four organizers submitted a proposal to the Rockefeller Foundation asking for funding for the summer workshop.\n\n…\n\nIn particular, the best-known reinforcement-learning successes have been in the domain of game playing. Applying reinforcement learning to games is the topic of the next chapter. 9 Game On Since the earliest days of AI, enthusiasts have been obsessed with creating programs that can beat humans at games. In the late 1940s, both Alan Turing and Claude Shannon, two founders of the computer age, wrote programs to play chess before there were even computers that could run their code. In the decades that followed, many a young game fanatic has been driven to learn to program in order to get computers to play their favorite game, whether it be checkers, chess, backgammon, Go, poker, or, more recently, video games.\n\n…\n\nLike Samuel’s checkers player before it, Deep Blue’s defeat of Kasparov spurred a significant increase in IBM’s stock price.16 This defeat also generated considerable consternation in the media about the implications for superhuman intelligence as well as doubts about whether humans would still be motivated to play chess. But in the decades since Deep Blue, humanity has adapted. As Claude Shannon wrote presciently in 1950, a machine that can surpass humans at chess “will force us either to admit the possibility of mechanized thinking or to further restrict our concept of thinking.”17 The latter happened. Superhuman chess playing is now seen as something that doesn’t require general intelligence.\n\nHedge Fund Market Wizards\n\nby Jack D. Schwager\n\nPublished 24 Apr 2012\n\nFor example, the basic strategy indicates standing with 16 if the dealer’s card is between two and six, and hitting otherwise. The basic strategy does not involve any card counting. 7The opening paragraph in the Wikipedia entry for Claude Shannon provides the following synopsis: Claude Elwood Shannon (April 30, 1916–February 24, 2001) was an American mathematician, electronics engineer, and cryptographer known as “the father of information theory.” Shannon is famous for having founded information theory with one landmark paper published in 1948. But he is also credited with founding both digital computer and digital circuit design theory in 1937, when, as a 21-year-old master’s student at MIT, he wrote a thesis demonstrating that electrical application of Boolean algebra could construct and resolve any logical, numerical relationship.\n\n…\n\nTrack records such as Thorp’s prove conclusively that it is possible to beat the market and that the large group of economists who insist otherwise are choosing to believe theory over evidence.2 The contention that it is possible to beat the markets, however, does not say anything about the difficulty of the task. In fact, it is the difficulty in beating the market (the vast majority of market participants fail to do so) that helps create the illusion that markets are efficient. Thorp’s career encompasses an extraordinary number of first achievements: He co-developed (along with Claude Shannon) the first wearable computer that could be used to win at roulette. He developed the first blackjack betting strategy that provided a positive edge to the player, which he divulged in his global best seller, Beat the Dealer. The book changed the way casinos operate. Thorp along with Sheen Kassouf developed the first known systematic approach to trading warrants and other convertible securities (e.g., options, convertible bonds, convertible preferred stocks) by hedging them with offsetting stock positions, an approach they detailed in their book, Beat the Market.3 He was the first to formulate an option-pricing model that was equivalent to the Black-Scholes model.\n\n…\n\nThe best way to do that was to get it published in the National Academy of Sciences, but you had to find a member who would submit the paper for you or else they wouldn’t take it. I researched the Cambridge area where I was located and found that there were two members. One member was an algebraist at Harvard who wouldn’t have any idea what I was talking about and probably wouldn’t have cared if he did. The other member was Claude Shannon at MIT. Shannon was a joint professor of mathematics and engineering, and one of only two Distinguished Professors at MIT. I went to his secretary and asked if I could get an appointment. She said, “He might see you for five minutes, but he doesn’t talk to people if he is not interested. So don’t expect more than a very brief interview.”\n\nSkin in the Game: Hidden Asymmetries in Daily Life\n\nby Nassim Nicholas Taleb\n\nPublished 20 Feb 2018\n\nFor, in the quarter millennia since an initial formulation of decision making under uncertainty by the mathematician Jacob Bernoulli, one that has since become standard, almost all people involved in the field have made the severe mistake of missing the effect of the difference between ensemble and time.fn1 Everyone? Not quite: every economist maybe, but not everyone: the applied mathematicians Claude Shannon and Ed Thorp, and the physicist J. L. Kelly of the Kelly Criterion got it right. They also got it in a very simple way. The father of insurance mathematics, the Swedish applied mathematician Harald Cramér, also got the point. And, more than two decades ago, practitioners such as Mark Spitznagel and myself built our entire business careers around it.\n\n…\n\nUnless one is a genius, that is, has the clarity of mind to see through the mud, or has a sufficiently profound command of probability theory to cut through the nonsense. Now, certifiably, Murray Gell-Mann is a genius (and, likely, Peters). Gell-Mann discovered the subatomic particles he himself called quarks (which got him the Nobel). Peters said that when he presented the idea to Gell-Mann, “he got it instantly.” Claude Shannon, Ed Thorp, J. L. Kelly, and Harald Cramér are, no doubt, geniuses—I can personally vouch for Thorp, who has an unmistakable clarity of mind combined with a depth of thinking that juts out in conversation. These people could get it without skin in the game. But economists, psychologists, and decision theorists have no geniuses among them (unless one counts the polymath Herb Simon, who did some psychology on the side), and odds are they never will.\n\n…\n\nAll these risks add up, and the attitude of the subject reflects them all. Ruin is indivisible and invariant to the source of randomness that may cause it. Another common error in the psychology literature concerns what is called “mental accounting.” The Thorp, Kelly, and Shannon school of information theory requires that, for an investment strategy to be ergodic and eventually capture the return of the market, agents increase their risks as they are winning, but contract after losses, a technique called “playing with the house money.” In practice, it is done by threshold, for ease of execution, not complicated rules: you start betting aggressively whenever you have a profit, never when you have a deficit, as if a switch was turned on or off.\n\nThe Cultural Logic of Computation\n\nby David Golumbia\n\nPublished 31 Mar 2009\n\nWeaver’s combinatoric argument fails to address Wiener’s chief points, namely that human language is able to manage ambiguity and approximation in a way quite different from the way that computers handle symbols. The persistent belief that philosophical skeptics must be wrong about the potential for machine translation is characteristic of computational thinking from the 1950s to the present. Only Claude Shannon himself—again, a dedicated scientist and engineer with limited experience in the study of language—is accorded authority by Weaver, so that “only Shannon himself, at this stage, can be a good judge of the possibilities in this direction”; remarkably, Weaver suggests that “a book written in Chinese is simply a book written in English which was coded into the ‘Chinese Code’ ” (22).\n\n…\n\nWhile it is no doubt inevitable that forms of long-distance communication would develop in any plausible human history, I am not persuaded that the exact forms of the telephone, telegraph, etc., are metaphysically necessary. Computation hovers provocatively between invention and discovery. Perhaps some of the most extreme computer scientists (Ray Kurzweil, Stephen Wolfram, Claude Shannon, Konrad Zuse) believe that digital computation in particular is a fundamental part of the physical universe; certainly there is at least some interesting evidence to support this view. At the same time, it seems equally if not more plausible that a wide range of calculating, quasilogical, and simulative mechanisms exist in the physical world, and that digital computation is simply one means of replicating some of these phenomena, perhaps the means that is most available to us for cultural reasons—in other words, we found digital computation because our society is already so oriented toward binarisms, hierarchy, and instrumental rationality.\n\n…\n\nPaul Edwards reports on a telling moment in the history of these laboratories: [George] Miller himself marks the year 1956, when he returned to Harvard, as the great transition. In that year his studies of language, information theory, The Cultural Logic of Computation p 36 and behavior crystallized into a new research paradigm. In an unpublished essay, Miller recounts his experience of the second Symposium on Information Theory, held at MIT on September 10–12, 1956. There he had his ﬁrst realization, “more intuitive than rational, that human experimental psychology, theoretical linguistics, and the computer simulation of cognitive processes were all pieces from a larger whole.”\n\nShape: The Hidden Geometry of Information, Biology, Strategy, Democracy, and Everything Else\n\nby Jordan Ellenberg\n\nPublished 14 May 2021\n\nON can’t go to just any bigram; what follows has to be a bigram starting with N. (The most common follow-up, Norvig’s tables tell us, is NS, which happens 14.7% of the time, followed by NT at 11.3%.) This gives a yet more refined picture of the structure of English text. It was the engineer and mathematician Claude Shannon who first realized that the Markov chain could be used not only to analyze text, but to generate it. Suppose you want to produce a passage of text with the same statistical properties as written English, and it starts with ON. Then you can use a random number generator to select the next letter; there should be a 14.7% chance it is S, an 11.3% chance it is T, and so on.\n\n…\n\nBut all the outputs somehow do sound like they come from the book you’re reading, which, let me tell you, is somewhat unsettling for the human being writing the book, even when the sentences make no literal sense at all, as in this GPT-3 output: If you’re familiar with the concept of Bayes’ theorem, then this should be easy for you. If there’s a 50% chance that the next word will be “the” and a 50% chance that it’ll be “geometry,” then the probability that the next word is either “the geometry” or “graupel” is (50/50)2 = 0. There’s a really big difference between this problem and Shannon’s text machine. Imagine a Claude Shannon with a much bigger library, trying to produce English sentences using this method, starting with five hundred words of what you’ve just read. He looks through his books until he finds one where those exact words appear in that exact order, so that he can record what word comes next. But of course he doesn’t find one!\n\n…\n\nNim with two piles of two stones is a loss. Connect Four is a win. (Pretty dispiriting, sis!) But we don’t know whether chess is a win, a loss, or a draw. We may never know. The tree of chess has many, many leaves. We don’t know exactly how many, but it’s more than an eight-foot robot can contemplate, that’s for sure. Claude Shannon, who we last saw generating faux English text with a Markov chain, also wrote one of the first papers to take machine chess seriously; he thought the number of leaves was on the order of 1 with 120 zeroes after it, a hundred million trillion googols. That’s more than the number of . . . okay, actually, it’s more than the number of anything in the universe, and it is certainly not a number of things you’re going to comb through one by one and write little W’s, L’s, and D’s next to.\n\nFrom eternity to here: the quest for the ultimate theory of time\n\nby Sean M. Carroll\n\nPublished 15 Jan 2010\n\nThere’s nothing wrong with that; after all, Boltzmann and Gibbs were proposing definitions to supercede Clausius’s perfectly good definition of entropy, which is still used today under the rubric of “thermodynamic” entropy. After quantum mechanics came on the scene, John von Neumann proposed a formula for entropy that is specifically adapted to the quantum context. As we’ll discuss in the next chapter, Claude Shannon suggested a definition of entropy that was very similar in spirit to Gibbs’s, but in the framework of information theory rather than physics. The point is not to find the one true definition of entropy; it’s to come up with concepts that serve useful functions in the appropriate contexts. Just don’t let anyone bamboozle you by pretending that one definition or the other is the uniquely correct meaning of entropy.\n\n…\n\nRather, they provide ways that we could appear to violate the Second Law, if we didn’t properly account for the crucial role played by information. The information collected and processed by the Demon must somehow be accounted for in any consistent story of entropy. The concrete relationship between entropy and information was developed in the 1940s by Claude Shannon, an engineer/mathematician working for Bell Labs.153 Shannon was interested in finding efficient and reliable ways of sending signals across noisy channels. He had the idea that some messages carry more effective information than others, simply because the message is more “surprising” or unexpected.\n\n…\n\nSee also Prigogine (1955), Kauffman (1993), and Avery (2003). 160 A good recent book is Nelson (2007). 161 He would have been even more wary in modern times; a Google search on “free energy” returns a lot of links to perpetual-motion schemes, along with some resources on clean energy. 162 Informally speaking, the concepts of “useful” and “useless” energy certainly predate Gibbs; his contribution was to attach specific formulas to the ideas, which were later elaborated on by German physicist Hermann von Helmholtz. In particular, what we are calling the “useless” energy is (in Helmholtz’s formulation) simply the temperature of the body times its entropy. The free energy is then the total internal energy of the body minus that quantity. 163 In the 1950s, Claude Shannon built “The Ultimate Machine,” based on an idea by Marvin Minsky. In its resting state, the machine looked like a box with a single switch on one face. If you were to flip the switch, the box would buzz loudly. Then the lid would open and a hand would reach out, flipping the switch back to its original position, and retreat back into the box, which became quiet once more.\n\nA Devil's Chaplain: Selected Writings\n\nby Richard Dawkins\n\nPublished 1 Jan 2004\n\nRather than engage in further recriminations and disputes about exactly what happened at the time of the interview, I shall try to redress the matter now in constructive fashion by answering the original question, the ‘Information Challenge’, at adequate length – the sort of length you can achieve in a proper article. The technical definition of ‘information’ was introduced by the American engineer Claude Shannon in 1948. An employee of the Bell Telephone Company, Shannon was concerned to measure information as an economic commodity. It is costly to send messages along a telephone line. Much of what passes in a message is not information: it is redundant. You could save money by recoding the message to remove the redundancy.\n\n…\n\nYou could spend a lifetime reading in this ancient library and die unsated by the wonder of it. 1 See ‘Unfinished Correspondence with a Darwinian Heavyweight’ (pp. 256–62). 2 The producers never deigned to send me a copy: I completely forgot about it until an American colleague called it to my attention. 3 See Barry Williams, ‘Creationist deception exposed’, the Skeptic 18 (1998), 3, pp. 7–10, for an account of how my long pause (trying to decide whether to throw them out) was made to look like hesitant inability to answer the question, followed by an apparently evasive answer to a completely different question. 4 It is important not to blame Shannon for my verbal and intuitive way of expressing what I think of as the essence of his idea. Mathematical readers should go straight to the original, C. Shannon and W. Weaver, The Mathematical Theory of Communication (University of Illinois Press, 1949). Claude Shannon, by the way, had an imaginative sense of humour. He once built a box with a single switch on the outside. If you threw the switch, the lid of the box slowly opened, a mechanical hand appeared, reached down and switched off the box. It then put itself away and the lid closed. As Arthur C. Clarke said, ‘There is something unspeakably sinister about a machine that does nothing – absolutely nothing – except switch itself off.’ 5 These round figures are all decimal approximations.\n\n…\n\nWhen the prior uncertainty is some mixture of alternatives that are not equiprobable, Shannon’s formula becomes a slightly more elaborate weighted average, but it is essentially similar. By the way, Shannon’s weighted average is the same formula as physicists have used, since the nineteenth century, for entropy. The point has interesting implications but I shall not pursue them here.8 That’s enough background on information theory. It is a theory which has long held a fascination for me, and I have used it in several of my research papers over the years. Let’s now think how we might use it to ask whether the information content of genomes increases in evolution. First, recall the three-way distinction between total information capacity, the capacity that is actually used, and the true information content when stored in the most economical way possible.\n\nChaos: Making a New Science\n\nby James Gleick\n\nPublished 18 Oct 2011\n\nThe patterns revealed a stretching and folding that led back to the horseshoe map of Smale. THE MOST CHARACTERISTICALLY Santa Cruzian imprint on chaos research involved a piece of mathematics cum philosophy known as information theory, invented in the late 1940s by a researcher at the Bell Telephone Laboratories, Claude Shannon. Shannon called his work “The Mathematical Theory of Communication,” but it concerned a rather special quantity called information, and the name information theory stuck. The theory was a product of the electronic age. Communication lines and radio transmissions were carrying a certain thing, and computers would soon be storing this same thing on punch cards or magnetic cylinders, and the thing was neither knowledge nor meaning.\n\n…\n\nIsaac Newton has more than a cameo: he seems to be the antihero of chaos, or the god to be overthrown. I discovered only later, reading his notebooks and letters, how wrong I’d been about him. And for twenty years I’ve been pursuing a thread that began with something Rob Shaw told me, about chaos and information theory, as invented by Claude Shannon. Chaos is a creator of information—another apparent paradox. This thread connects with something Bernardo Hubemian said: that he was seeing complex behaviors emerge unexpectedly in information networks. Something was dawning, and we’re finally starting to see what it is. James Gleick Key West February 2008 Notes on Sources and Further Reading THIS BOOK DRAWS on the words of about two hundred scientists, in public lectures, in technical writing, and most of all in interviews conducted from April 1984 to December 1986.\n\n…\n\nBecause information was stored in binary on-off switches newly designated as bits, bits became the basic measure of information. From a technical point of view, information theory became a handle for grasping how noise in the form of random errors interfered with the flow of bits. It gave a way of predicting the necessary carrying capacity of communication lines or compact disks or any technology that encoded language, sounds, or images. It offered a theoretical means of reckoning the effectiveness of different schemes for correcting errors—for example, using some bits as checks on others. It put teeth into the crucial notion of “redundancy.” In terms of Shannon’s information theory, ordinary language contains greater than fifty percent redundancy in the form of sounds or letters that are not strictly necessary to conveying a message.\n\nWhole Earth: The Many Lives of Stewart Brand\n\nby John Markoff\n\nPublished 22 Mar 2022\n\nNegroponte’s Architecture Machine Group, an early effort at an advanced computer system to prototype human-computer interaction, was also a descendent of the RLE. The Media Lab’s intellectual tradition was reminiscent of what Brand had first come in contact with as a college student when he encountered cybernetics through Wiener and Warren McCulloch and information theory from the writings of Claude Shannon and Robert Fano. At MIT, visitor and host discussed the idea of coauthoring a book, but Brand had already learned enough about Negroponte, who had a large ego and a forceful personality, to realize that would be problematic. Instead, he negotiated a contract that gave him access to the Media Lab and in return agreed to pay special attention to Negroponte’s research.\n\n…\n\nA little more than a decade later Brand would also become close to Bateson, and a conversation between them, titled “Both Sides of the Necessary Paradox,” appeared first in Harper’s Magazine before being reprinted in Brand’s first book, II Cybernetic Frontiers, along with his Rolling Stone article about computer hacker culture. Years later, Brand would return to the thinking of several others he encountered during this time, among them cyberneticist Warren McCulloch, and the MIT engineers Claude Shannon and Robert Fano, who pioneered theories regarding the measurement and transmission of information. For the moment, however, he was just another freelancer without takers. * * * He had fallen in love with the idea of becoming a biologist when he read Cannery Row and its portrayal of Ed Ricketts.\n\n…\n\nSpencer, 216 Brown, Jerry, 98, 225–27, 230, 231, 348 Brussell, Mae, 215 Burning Man festival, 109, 328 Burrows, George Lord (great-grandfather), 8–9 Burrows, Lorenzo (great-great-grandfather), 7 Butler, Katy, 247–48 butterfly effect, 361 C Caffe Trieste, 48, 74 California, University of, at Berkeley, 25, 135, 302 California Museum of Science and Industry, 91 California Water Atlas, 227 Callahan, Michael, 94, 106–7, 137 Calthorpe, Peter, 129, 246, 256, 302, 305, 306, 307, 318, 341 Cannery Row (Steinbeck), 18, 46, 243 Cape Breton Island, Canada, Jennings and SB’s home on, 199–200, 207–9, 218, 234–35 Capra, Frank, 28 Capra, Fritjof, 295, 297 Carlston, Doug, 271, 325, 328–29 Carroll, Jon, 357 Cassady, Neal, 69, 126, 131–32, 143 Center for Advanced Study in the Behavioral Sciences, 45–46, 208 Chappell, Walter, 116, 119 Chernobyl nuclear disaster, 282 Chicago 8, trial of, 177, 188 Chippewas, 7 choice, freedom of, 42–43 Church, George, 360 Churchill, Winston, 291 CIA, 202, 298 City Lights Books, 37, 50, 74 Clear Creek, 205 climate change, 338–39 SB and, 4, 338, 339, 342, 347, 348–50, 357, 361 Schwartz and, 338–40, 342 Clock Library, 314, 325, 326, 327–28, 329, 362 Coate, John, 309 coevolution, 46, 217, 219, 222, 223, 232 CoEvolution Quarterly, 6, 85, 175, 208, 221–22, 227, 229, 230, 236, 240–41, 251, 254, 255–56, 257, 289 Black Panther–edited issues of, 229 Butler’s article on Baker scandal in, 247–48 demise of, 261 Gaia hypothesis story in, 230, 349 Kelly’s cover article in, 254 Kleiner’s cyberspace article in, 240 as money-losing venture, 233, 240 O’Neill’s space colonies story in, 231–32 provocative viewpoint of, 228–29 SB’s idiosyncratic editorial style at, 229, 249 SB’s separation from, 271 Schweickart’s “No Frames, No Boundaries” reprinted in, 225 Co-Existence Bagel Shop, 37, 48, 74 Collyns, Napier, 277, 295 Commoner, Barry, 206 commune movement, 139–40, 154, 177 communications technology, 133, 241, 282, 290, 298 Community Memory, 265 complex systems, learning by, 274, 277, 279–80, 284, 289 computer conferencing, 151, 240, 251–52, 263, 264, 265, 266 computer networks, see cyberspace; internet computers, computing: counterculture and, 185–86 Engelbart’s “bootstrapping” vision of, 151, 153 hobbyists in, 147, 158, 196, 198, 213, 230, 266–67 personal, see personal computers predicted exponential increase in powers of, 152–53 SB’s growing interest in, 145–46, 168, 266, 280 conservationism, of SB, 4, 340 Contact Is the Only Love (Stern), 93, 136 counterculture, 2, 10, 71, 75, 143, 146, 176, 177–78, 202, 216, 228 computing community and, 185–86 demise of, 181, 241 political-psychedelic divide in, 145, 152 SB’s negative reassessment of, 297 Trips Festival as catalyst for, 127–28, 130 Whole Earth Catalog in, 173–74 Counterculture Green (Kirk), 135 Coyote, Peter, 127, 225, 226, 287–88, 297, 358, 361 Creative Initiative Foundation, 41 Creative Philanthropy seminar, SB’s organizing of, 250 creativity, LSD and, 72, 76–77 Crooks family, 65–66 Crosby, David, 189–90 Crumb, R., 215, 228 Curwen, Darcy, 22, 23 cybernetics, 2, 4, 169, 208, 213, 216–17, 226, 273 Cybernetics (Wiener), 169, 226 cyberspace, 54, 84, 212, 240, 254–55, 258, 261, 279, 298–99 anonymity and pseudonymity in, 266 dangers of, 293, 315 dystopian aspects of, 308, 310–11 gold rush mentality and, 293, 323 impact of, 295–96 SB and, 4, 251–52, 282, 293 see also internet D Dalton, Richard, 261 Daumal, René, 186 Deadheads, 265–66 Defense Department, US, 315, 338 de Geus, Arie, 274, 285, 289 Delattre, Pierre, 47, 74 deserts, SB’s attraction to, 108–10, 114 Desert Solitaire (Abbey), 181 desktop publishing, 164–65 Detroit Free Press, 10, 172–73 Dick Cavett Show, SB’s appearance on, 192 Diehl, Digby, 200 Diggers, 206 Direct Medical Knowledge, 326, 333, 342 DiRuscio, Jim, 324–25 Divine Right’s Trip (Norman), 193, 223 DNA Direct, 343 Dome Cookbook (Baer), 162, 180 Doors of Perception, The (Huxley), 28 dot-com bubble, 296, 326, 333–34, 348 Doubleday, 253, 256, 257 Drop City (commune), 154, 162 “Drugs and the Arts” panel (SUNY Buffalo), 177 Duffy, Frank, 319 Durkee, Aurora, 180 Durkee, Barbara and Stephen, 61, 105, 119–20, 137, 139, 162, 177, 179, 180, 186, 229 Garnerville studio of, 60, 66, 69, 105, 106–7 SB’s friendship with, 51–52, 59–60, 66, 67, 133 in USCO, 106–7 Dvorak, John, 259, 260 Dymax, 147–48 Dynabook, 212 Dyson, Esther, 315, 325 E Eames, Charles, 44–45, 96, 113 Earth, viewed from space: SB’s campaign for photograph of, 134–35, 164 SB’s revelatory vision of, 1, 6, 362 Earth Day (1970), 182, 190, 364 Edson, Joanna, 75–76 Edson, John, 14, 18–20, 38–39, 75 education: intersection of technology and, 144, 145 see also alternative education movement; learning, act of Education Automation (Fuller), 169 Education Innovations Faire, 149 Ehrlich, Paul, 46, 177, 341, 360–61, 364 SB as influenced by, 28, 45, 47, 187, 188, 206, 222–23 EIES (Electronic Information Exchange System), 240, 251–52, 264, 266 Electric Kool-Aid Acid Test, The (Wolfe), 5, 88, 111, 121, 125, 170, 181 “Electric Kool-Aid Management Consultant, The” (Fortune profile of SB), 297 Electronic Frontier Foundation, 325 endangered species, 2, 360–61 Engelbart, Douglas, 83, 138, 146, 151–53, 158, 186–87, 230, 292, 361 “Mother of All Demos” by, 171–72 oNLine System of, 151, 156, 197, 212 SB influenced by, 150, 153, 185, 364 English, Bill, 160, 171, 185, 203, 211 English, Roberta, 160 Eno, Brian, 305, 306, 314, 319, 320, 325, 327, 336, 342, 353, 354 “Environmental Heresies” (Brand), 341–42 environmental movement, 2, 71, 159, 204 activist approach to, 181–82, 187, 188, 201–2, 297, 347 conservation vs. preservation in, 340 SB’s break with, 246, 336, 341, 347 SB’s role in, 4, 9–10, 180, 181–82, 201, 202, 204–7, 284, 347 Esalen Institute, 71–72, 84, 138, 176, 185 Esquire, 88, 146, 183, 247, 250 Essential Whole Earth Catalog, 286 Evans, Dave, 146, 156, 180, 185–87, 212 Exploratorium, 194–98 extinct species, revival of, 359–60 F Fadiman, Jeff, 38, 44, 59, 62–63, 64 Fadiman, Jim, 72–73, 77–78, 80, 84, 89, 97, 98, 101 Fall Joint Computer Conference (San Francisco; 1968), 171–72 Fano, Robert, 46, 273 Fariña, Mimi, 141, 237 Farm (commune), 257 Ferlinghetti, Lawrence, 50, 71 Field, Eric, 44, 53 Fillmore Auditorium (San Francisco), 125–26, 128, 130 filter bubbles, 279, 308 Fluegelman, Andrew, 220, 221–22, 269 Foer, Franklin, 5 Foreign Policy, 356 Fort Benning, SB at, 53–58 Fort Dix, SB at, 58–63, 64, 65–68 Fortune, 297, 339 “Four Changes” (Snyder), 187 Francis, Sharon, 105, 112 Frank, Delbert, 86–87 Frank, Robert, 179, 188, 199–200, 218 Fraunhofer, Joseph Ritter von, 108 Free Speech Movement, 135, 175 From Bauhaus to Our House (Wolfe), 304–5 “Fruits of a Scholar’s Paradise” (Brand; unpublished), 45–46, 208 Fukushima nuclear disaster, 355–58 Fuller, Buckminster, 134, 147, 162, 169, 175, 176, 217 SB influenced by, 132, 138, 146, 150, 168–69, 222, 243–44, 363–64 Fulton, Katherine, 318 futurists, 262, 273 SB as, 258–59, 280, 323 G Gaia hypothesis, 230, 349 games, SB’s interest in, 84, 120, 129–30, 149, 210, 211, 217, 219–21, 236–37 Gandhi, Mohandas K., 53 Garcia, Jerry, 128, 158 Garnerville, N.Y., Durkee/USCO studio at, 60, 66, 69, 105, 106–7, 136, 154 Gaskin, Stephen, 257 Gause, Gregory, 46 GBN, see Global Business Network genetic engineering, 341, 344, 360–61 geodesic domes, 176, 217 Georgia-Pacific, 9, 29 Gerbode Valley, Calif., 219–21, 236–37 Getty Museum, 329 Gibbons, Euell, 138 Gibson, William, 262, 294, 315 Gilman, Nils, 297–98 Gilmore, John, 325, 352 Ginsberg, Allen, 33–34, 50, 69, 77, 94, 177, 237 Global Business Network (GBN), 291, 295–300, 311, 313, 335, 340 Brand and Schwartz as co-founders of, 291–92 climate change scenario of, 338–39 SB consulting position at, 296, 298, 305, 314, 315–16, 323–24, 343, 354 globalization, 295–96, 346 global warming, see climate change GMO foods, 2, 344, 347, 357 Godwin, Mike, 308 Golden Gate National Recreation Area, 237, 360 Gone (Kirkland), 359 Gottlieb, Lou, 140 government, SB’s evolving view of, 166, 227, 348 Graham, Bill, 124, 125, 128, 130–31, 143 Grand Canyon, SB’s visit to, 19–20 Grateful Dead, 24, 123, 125, 126, 130, 131, 141, 158, 160, 189, 265–66 Great Basin National Park, 329–30 “Great Bus Race, The,” SB at, 181 Gregorian, Vartan, 27 Griffin, Susan, 295, 297 Griffith, Saul, 349–50 Gross, Cathleen, 286, 289 Grossman, Henry, 63 H hackers, hacker culture, 25, 84, 147–48, 150, 261, 267, 268–69, 273, 293, 294 SB’s Rolling Stone article on, 46, 211–13, 217, 250 Hackers: Heroes of the Computer Revolution (Levy), 266–67, 268, 270 Hackers Conference, 266–70, 326 Haight-Ashbury (San Francisco neighborhood), 74, 75, 128 Halpern, Sue, 241–42 Harman, Willis, 41, 42, 72, 73, 77, 273 Harner, Michael, 101, 118, 129 Harper’s Magazine, 46, 213, 228 SB’s Bateson profile in, 216–17 Whole Earth Epilog proposal of, 218, 219, 222 Harris, David, 149, 162, 299 Harvey, Brian, 268–69 Hawken, Paul, 247–48, 250, 281, 286, 290, 299, 332, 333, 334 Hayden Planetarium, 91, 92, 105 Hayes, Denis, 351 Healy, Mary Jean, 205, 206, 207 Heard, Gerald, 41–42, 84 Hells Angels, 120 Herbert, Anne, 230–31, 241, 255 Hershey, Hal, 183 Hertsgaard, Mark, 357 Hertzfeld, Andy, 267 Hewlett, William, 156 Hewlett-Packard, 147 Hickel, Walter, 206 Higgins Lake, Mich., Brand family camp at, 7, 8, 9, 10–11, 21, 30, 209, 289–90, 326, 327 Hillis, Danny, 289, 301, 305, 315, 336 Long Now Clock and, 313–14, 316–17, 325–26, 327, 328, 329, 330, 333, 362, 363 Thinking Machines founded by, 279–80, 288 Hippies, Indians, and the Fight for Red Power (Smith), 118 Hoagland, Edward, 201 Hoffer, Eric, 32 Hoffman, Abbie, 177–78, 214, 299 Hog Farm commune, 159, 181, 186, 188, 202, 205, 206, 220 Homebrew Computer Club, 147, 158, 198, 230, 266–67 Homo Ludens (Huizinga), 220 Hopcroft, David, 275 Hopi Indians, 100, 139, 205 Horvitz, Robert, 6 House Committee on Education and Labor, SB at hearing of, 190–91 Household Earth, see Life Forum How Buildings Learn (SB’s UC Berkeley seminar), 302 How Buildings Learn (BBC documentary), 320 How Buildings Learn (Brand), 291, 300–301, 304–7, 310, 312, 317–19, 323, 324, 331 How to Be Rich Well (SB book proposal), 344–46 Hubbard, Al, 42, 77, 273 Huerfano Valley, Colo., 139–40 Huizinga, Johan, 220 human potential movement, 71, 73, 84 humans: freedom of choice of, 42–43 as morally responsible for care of natural world, 42, 347, 349, 360, 361 SB’s speculations about fate of, 38–39 Human Use of Human Beings, The (Wiener), 160 Hunger Show (Life-Raft Earth), 187–88, 189, 203, 263 Huxley, Aldous, 28, 33, 41, 72, 144, 226 hypertext, concept of, 172, 230, 292, 293 I IBM, 91, 92, 96, 108, 211 I Ching, 89–90, 117, 153, 197, 253 Idaho, University of, 21 identity, fake, cyberspace and, 266 II Cybernetic Frontiers (Brand), 46, 213, 217, 221 Iktomi (Ivan Drift), 96–97 Illich, Ivan, 196 Independent, 353 information, personalization of, 279 information sharing, 180 information technology, 299–300, 315 information theory, 273 “Information wants to be free,” 270, 299, 301 information warfare, 315 In Our Time (Hemingway), 11 Institute for International Relations (IIR), 27, 34, 35, 37 Institute for the Future, 315 intelligence augmentation (IA), 83, 185, 187 International Federation of Internal Freedom, 89 International Foundation for Advanced Study, LSD experiments at, 42, 72, 73, 76–82, 273 internet, 146, 151, 279, 293, 314, 316, 326 ARPANET as forerunner of, 212 impact of, 295–96, 323 libertarianism and, 5, 348 see also cyberspace Internet Archive, 330, 332 Internet of Things, 279 Interval Research Corporation, 321–23 “Is Environmentalism Dead?”\n\nThe Internet of Us: Knowing More and Understanding Less in the Age of Big Data\n\nby Michael P. Lynch\n\nPublished 21 Mar 2016\n\nBut it is also true that in other respects we know less, that the walls of our digital life make real objective knowledge harder to come by, and that the Internet has promoted a more passive, more deferential way of knowing.13 Like our imaginary neuromedians, we are in danger of depending too much on one way of accessing the world and letting our other senses dull. Socrates on the Way to Larissa Data is not the same thing as information. As the founding father of information theory, Claude Shannon, put it back in the 1940s, data signals are noisy, and if you want to filter out what’s meaningful from those signals, you have to filter out at least some of that noise. For Shannon the noise was literal. His groundbreaking work concerned how to extract discernible information from the signals sent across telephone lines.14 But the moral is entirely general: bits of code aren’t themselves information; information is what we extract from those bits.\n\n…\n\nS., 159–60 Bush, George W., advisor to, 86 calculators, 153 Caldarelli, Guido, 112 cameras, 89, 94 security, 91, 97 Cancer Ward (Solzhenitsyn), 185 capitalism, changes in, 140–41, 144 Cartesian coordinates, 175 Cartesian foundationalism, 126–29, 131 Catanzaro, Michele, 112 Cavell, Stanley, 10 CCTV, 91 CD-ROMs, 8 censorship, 42, 65, 66, 134, 144 self-, 97 Chabris, Christopher, 30 Chalmers, David, 115 chat rooms, 118 chess, 165 children: and hands-on experience, 174 understanding in, 177 China: iPhone production in, 77–78, 139 oppressive government policies of, 81 truth deleted from Internet in, 65, 66 Chuck, 167–68, 173 Church, power held by, 133–34 CIA, 100 data searched by, 99 Clark, Andy, 115 climate change, 56, 100, 124, 144, 185, 198 cloud: data storage on, 23 data trail on, 9 in information sharing, 4–5 coherentism, 130 see also fabric metaphor, for structure of beliefs Collaborative Commons, 140–41 educational model of, 151–52 as first world resource, 144 collective responsibility, 118–19 College Humor (website), 24 common point of view, 48 communication: isolation and, 41–42 neuromedia and, 113–14 of standards, 39–40 computers, xvii brains and, 19 games, 19–20, 191 reality simulated in, see SIMs self-awareness in, 116, 193 Concept of Mind, The (Ryle), 168 conclusions, jumping to, 29–30, 58 Condorcet Jury Theorum, 120–21 confirmation bias, 54–55, 56 defined, 51–52 Congress, U.S., NSA vs., 99 consequences, 172 conservatives, 43 context, in data analysis, 161 continuum hypothesis, defined, 197 “cookies,” tracking by, 90 Copernicus, Nicolaus, 34 corporate responsibility, 118 corporations, as “people,” 200 correlation analyses, 158–59 Coursera, 150 cover-ups, 83 creationism, 49 crea"
    }
}
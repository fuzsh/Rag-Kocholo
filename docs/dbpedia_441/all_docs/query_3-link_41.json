{
    "id": "dbpedia_441_3",
    "rank": 41,
    "data": {
        "url": "http://adami.natsci.msu.edu/blog/2014/6/25/whose-entropy-is-it-anyway-part-1-boltzmann-shannon-and-gibbs-",
        "read_more_link": "",
        "language": "en",
        "title": "Whose entropy is it anyway? (Part 1: Boltzmann, Shannon, and Gibbs ) — Chris Adami",
        "top_image": "http://static1.squarespace.com/static/53308268e4b00147ec53e8b4/t/53ab3727e4b01f59562cf30d/1403729745016/?format=1500w",
        "meta_img": "http://static1.squarespace.com/static/53308268e4b00147ec53e8b4/t/53ab3727e4b01f59562cf30d/1403729745016/?format=1500w",
        "images": [
            "http://static1.squarespace.com/static/53308268e4b00147ec53e8b4/t/552d1634e4b08fa7d30cc330/1429018164966/masthead-helmet-white.png",
            "http://static1.squarespace.com/static/54592123e4b018e772712817/t/5526b41de4b0bba1b085b845/1428599837436/masthead-helmet-black.png",
            "https://images.squarespace-cdn.com/content/v1/53308268e4b00147ec53e8b4/1398277948044-G13IIQIQ9GYVNAHGU0GK/image-asset.jpeg",
            "https://images.squarespace-cdn.com/content/v1/53308268e4b00147ec53e8b4/1403729735734-2X4GVJYWNHZ2FAHS7IAS/image-asset.png",
            "https://images.squarespace-cdn.com/content/v1/53308268e4b00147ec53e8b4/1403729773501-MHK9APXBRETDGTE1MPJX/image-asset.jpeg",
            "https://images.squarespace-cdn.com/content/v1/53308268e4b00147ec53e8b4/1403729862945-7GK8ALZENPK7E4T227ZC/P7140522.JPG",
            "https://images.squarespace-cdn.com/content/v1/53308268e4b00147ec53e8b4/1403729876792-WX6NHHWGTHDJZLLWO1SB/image-asset.png",
            "https://images.squarespace-cdn.com/content/v1/53308268e4b00147ec53e8b4/1403729932006-6RSUZ346STWUGOQRXJQH/image-asset.png",
            "https://images.squarespace-cdn.com/content/v1/53308268e4b00147ec53e8b4/1403729963876-E6UFDKTSA1P1JY5BZZHO/image-asset.jpeg",
            "https://images.squarespace-cdn.com/content/v1/53308268e4b00147ec53e8b4/1403729988723-52XJ2H6QS4SUYYY9QWR6/Screen+Shot+2014-06-25+at+4.53.40+PM.png",
            "https://images.squarespace-cdn.com/content/v1/53308268e4b00147ec53e8b4/1403730011154-9286BCF2ISON213LYX2C/image-asset.png",
            "https://images.squarespace-cdn.com/content/v1/53308268e4b00147ec53e8b4/1403730026375-NCTS4D3DN8WF0NGDFXZS/image-asset.jpeg",
            "https://images.squarespace-cdn.com/content/v1/53308268e4b00147ec53e8b4/1438957648155-JCVLTFNA74VT21FZ9CSF/image-asset.png",
            "http://static1.squarespace.com/static/53308268e4b00147ec53e8b4/t/552d1a27e4b0c4575fde06bc/1429019175733/msu-wordmark-white.png"
        ],
        "movies": [],
        "keywords": [],
        "meta_keywords": [
            ""
        ],
        "tags": null,
        "authors": [
            "Kim Ward"
        ],
        "publish_date": "2014-06-25T00:00:00",
        "summary": "",
        "meta_description": "Note: this post was slated to appear on May 31, 2014, but events outside of my control (such as grant submission deadlines, and parties at my house) delayed its issuance.   The word \"entropy\" is used a lot, isn't it? OK, not in your average conversation, but it is a staple of conversations",
        "meta_lang": "en",
        "meta_favicon": "https://images.squarespace-cdn.com/content/v1/53308268e4b00147ec53e8b4/1395839100150-UWE4CY30FHO1ZW42X9W4/favicon.ico",
        "meta_site_name": "Chris Adami",
        "canonical_link": "http://adami.natsci.msu.edu/blog/2014/6/25/whose-entropy-is-it-anyway-part-1-boltzmann-shannon-and-gibbs-",
        "text": "Note: this post was slated to appear on May 31, 2014, but events outside of my control (such as grant submission deadlines, and parties at my house) delayed its issuance.\n\nThe word \"entropy\" is used a lot, isn't it? OK, not in your average conversation, but it is a staple of conversations between some scientists, but certainly all nerds and geeks. You have read my introduction to information theory I suppose (and if not, go ahead and start here, right away!) But in my explanations of Shannon's entropy concept, I only obliquely referred to another \"entropy\": that which came before Shannon: the thermodynamic entropy concept of Boltzmann and Gibbs. The concept was originally discussed by Clausius, but because he did not give a formula, I will just have to ignore him here.\n\nWhy do these seemingly disparate concepts have the same name? How are they related? And what does this tell us about the second law of thermodynamics?\n\nThis is the blog post (possibly a series) where I try to throw some light on that relationship. I suspect that what follows below isn't very original (otherwise I probably should have written it up in a paper), but I have to admit that I didn't really check. I did write about some of these issues in an article that was published in a Festschrift on the occasion of the 85th birthday of Gerry Brown, who was my Ph.D. co-advisor and a strong influence on my scientific career. He passed away a year ago to this day, and I have not yet found a way to remember him properly. Perhaps a blog post on the relationship between thermodynamics and information theory is appropriate, as it bridges a subject Gerry taught often (Thermodynamics) with a subject I have come to love: the concept of information. But face it: a book chapter doesn't get a lot of readership. Fortunately, you can read it on arxiv here, and I urge you to because it does talk about Gerry in the introduction.\n\nBefore we get to the relationship between Shannon's entropy and Boltzmann's, how did they end up being called by the same name? After all, one is a concept within the realm of physics, the other from electrical engineering. What gives?\n\nThe one to blame for this confluence is none other than John von Neumann, the mathematician, physicist, engineer, computer scientist (perhaps Artificial Life researcher, sometimes moonlighting as an economist). It is difficult to appreciate the genius that was John von Neumann, not the least because there aren't many people who are as broadly trained as he was. For me, the quote that fills me with awe comes from another genius who I've had the priviledge to know well, the physicist Hans Bethe. I should write a blog post about my recollections of our interactions, but there is already a write-up in the book memorializing Hans's life. While I have never asked Hans directly about his impressions of von Neumann (how I wish that I had!), he is quoted as saying (in the 1957 LIFE magazine article commemorating von Neumann's death: \"I have sometimes wondered whether a brain like von Neumann's does not indicate a species superior to man\".\n\nThe reason why I think that this quite a statement, is that I think Bethe's brain was in itself very unrepresentative of our species, and perhaps indicated an altogether different kind.\n\nSo, the story goes (as told by Myron Tribus in his 1971 article \"Energy and Information\")that when Claude Shannon had figured out his channel capacity theorem, he consulted von Neumann (both at Princeton at the time) about what he should call the \"-p log p\" value of the message to be sent over a channel. von Neumann supposedly replied:\n\n\"You should call it entropy, for two reasons. In the first place your uncertainty function has been used in statistical mechanics under that name. In the second place, and more importantly, no one knows what entropy really is, so in a debate you will always have the advantage.”\n\nThe quote is also reprinted in the fairly well-known book \"Maxwell's Demon: Entropy, Information, and Computing\", edited by Leff and Rex. Indeed, von Neumann had defined a quantity just like that as early as 1927 in the context of quantum mechanics (I'll get to that). So he knew exactly what he was talking about.\n\nLet's assume that this is an authentic quote. I can see how it could be authentic, because the thermodynamic concept of entropy (due to the Austrian physicist Ludwig Boltzmann) can be quite, let's say, challenging. I'm perfectly happy to report that I did not understand it for the longest time, in fact not until I understood Shannon's entropy, and perhaps not until I understood quantum entropy.\n\nNow Eq. (3) does precisely look like Shannon's, which you can check by comparing to Eq. (1) in the post \"What is Information? (Part 3: Everything is conditional)\". Thus, it is Gibbs's entropy that is like Shannon's, not Boltzmann's. But before I discuss this subtlety, ponder this:\n\nAt first sight, this similarity between Boltzmann's and Shannon's entropy appears ludicrous. Boltzmann was concerned with the dynamics of gases (and many-particle systems in general). Shannon wanted to understand whether you can communicate accurately over noisy channels. These appear to be completely unrelated endeavors. Except they are not, if you move far enough away from the particulars. Both, in the end, have to do with measurement.\n\nIf you want to communicate over a noisy channel, the difficult part is on the receiving end (even though you quickly find out that in order to be able to receive the message in its pristine form, you also have to do some work at the sender's end). Retrieving a message from a noisy channel requires that you or I make accurate measurements that can distinguish the signal from the noise.\n\nIf you want to characterize the state of a many-particle system, you have to do something other than measure the state of every particle (because that would be impossible). You'll have to develop a theory that allows us to quantify the state given a handful of proxy variables, such as energy, temperature, and pressure. This is, fundamentally, what thermodynamics is all about. But before you can think about what to measure in order to know the state of your system, you have to define what it is you don't know. This is Boltzmann's entropy: how much you don't know about the many-particle system.\n\nIn Shannon's channel, a message is simply a set of symbols that can encode meaning (they can refer to something). But before it has any meaning, it is just a vessel that can carry information. How much information? This is what's given by Shannon's entropy. Thus, the Shannon entropy quantifies how much information you could possibly send across the channel (per use of the channel), that is, entropy is potential information.\n\nOf course, Boltzmann entropy is also potential information: If you knew the state of the many-particle system precisely, then the Boltzmann entropy would vanish. You (being an ardent student of thermodynamics) already know what is required to make a thermodynamical entropy vanish: the temperature of the system must be zero. This, incidentally, is the content of the third law of thermodynamics.\n\n\"The third law?\", I hear some of you exclaim. \"What about the second?\"\n\nYes, what about this so-called Second Law?\n\nTo be continued, with special emphasis on the Second Law, in Part 2."
    }
}
{
    "id": "dbpedia_3902_3",
    "rank": 51,
    "data": {
        "url": "https://mediatheoryjournal.org/2020/11/09/ekin-erkan-promethean-philosophy/",
        "read_more_link": "",
        "language": "en",
        "title": "Ekin Erkan: Promethean Philosophy",
        "top_image": "https://i0.wp.com/mediatheoryjournal.org/wp-content/uploads/2024/01/e7c33-nyu-presentation-image.jpg?fit=580%2C557&ssl=1",
        "meta_img": "https://i0.wp.com/mediatheoryjournal.org/wp-content/uploads/2024/01/e7c33-nyu-presentation-image.jpg?fit=580%2C557&ssl=1",
        "images": [
            "https://mediatheoryjournal.org/wp-content/uploads/2024/03/pexels-photo-1484759.jpeg",
            "https://mediatheoryjournal.org/wp-content/uploads/2024/03/mtj_logo.png",
            "https://mediatheoryjournal.org/wp-content/uploads/2024/01/e7c33-nyu-presentation-image.jpg",
            "https://i0.wp.com/mediatheoryjournal.org/wp-content/uploads/2024/01/e7c33-nyu-presentation-image.jpg?resize=580%2C557&ssl=1",
            "https://mediatheoryjournal.org/wp-content/uploads/2024/03/trent-logo.jpg",
            "https://mediatheoryjournal.org/wp-content/uploads/2024/03/uvsq-logo-1.jpg",
            "https://mediatheoryjournal.org/wp-content/uploads/2024/03/doaj-logo.jpg",
            "https://mediatheoryjournal.org/wp-content/uploads/2024/07/Radical-OA-Collective.jpg"
        ],
        "movies": [],
        "keywords": [],
        "meta_keywords": [
            ""
        ],
        "tags": null,
        "authors": [],
        "publish_date": "2020-11-09T00:00:00",
        "summary": "",
        "meta_description": "Editor’s Note This article was retracted, corrected and republished on 2021-11-12 following a post-publication complaint which led to a review that determined the published text contained a small number of sentences that i) had unacceptable similarities to other work and ii) were unreferenced. Because the journal’s boards were not in a position to determine if…",
        "meta_lang": "en",
        "meta_favicon": "https://i0.wp.com/mediatheoryjournal.org/wp-content/uploads/2024/01/homepageimage_en_us.png?fit=32%2C12&ssl=1",
        "meta_site_name": "Media Theory",
        "canonical_link": "https://mediatheoryjournal.org/2020/11/09/ekin-erkan-promethean-philosophy/",
        "text": "Editor’s Note\n\nThis article was retracted, corrected and republished on 2021-11-12 following a post-publication complaint which led to a review that determined the published text contained a small number of sentences that i) had unacceptable similarities to other work and ii) were unreferenced. Because the journal’s boards were not in a position to determine if this was accidental or intentional, and because the scope of the impacted text was considered insufficient to warrant either retraction or complete removal, the decision was taken to republish a corrected version of the article. The disputed passages have now been removed from the version of record.\n\nA Promethean Philosophy of External Technologies, Empiricism, & the Concept:\n\nSecond-Order Cybernetics, Deep Learning, and Predictive Processing\n\nEKIN ERKAN\n\nCUNY Graduate Center, USA\n\nAbstract\n\nBeginning with a survey of the shortcoming of theories of organology/media-as-externalization of mind/body—a philosophical-anthropological tradition that stretches from Plato through Ernst Kapp and finds its contemporary proponent in Bernard Stiegler—I propose that the phenomenological treatment of media as an outpouching and extension of mind qua intentionality is not sufficient to counter the ‘black-box’ mystification of today’s deep learning’s algorithms. Focusing on a close study of Simondon’s On the Existence of Technical Objects and Individuation, I argue that the process-philosophical work of Gilbert Simondon, with its critique of Norbert Wiener’s first-order cybernetics, offers a precursor to the conception of second-order cybernetics (as endorsed by Francisco Varela, Humberto Maturana, and Ricardo B. Uribe) and, specifically, its autopoietic treatment of information. It has been argued by those such as Frank Pasquale that neuro-inferential deep learning systems premised on predictive patterning, such as AlphaGo Zero, have a veiled logic and, thus, are ‘black boxes’. In detailing a philosophical-historical approach to demystify predictive patterning/processing and the logic of such deep learning algorithms, this paper attempts to shine a light on such systems and their inner workings à la Simondon.\n\nKeywords\n\ndeep learning; externalization; Simondon; Carnap; second-order cybernetics; Predictive Processing\n\nThis article was penned shortly before Bernard Stiegler passed and was constructed after a series of exchanges between myself and Stiegler. Although the article seeks to problematize some of Stiegler’s thought regarding the philosophy of technology, specifically in regards to his theorizing neural networks and deep learning, I ought to underscore that Stiegler was a mentor and friend who always encouraged my debating and challenging his work. Thus, this article is dedicated to the life and memory of Bernard.\n\nIntroduction: Media as Externalization\n\nThe philosopher of media Bernard Stiegler is one of the most recent thinkers to propose that media objects are the externalization of something more primal and transcendental, whether it be intention, language, or cognition. Stiegler considers that the implications of the Promethean program, where collective self-mastery and active participation are imbricated within the remaking of mankind in the world, has become exacerbated in the era of digital technologies and networks. Stiegler’s normatively negative position—violently poised against the possibility of a transhumanist telos of further enlightenment—is perhaps what distinguishes him most from the philosophical genealogy of technicity. Before proceeding with a critique of Stiegler, however, we must first contextualize that his thought does not exist in a vacuum and that his is only the most recent instantiation within a rich history of philosophical anthropology and the philosophy of technology that sees the human as a ‘not-yet-finished’ creature capable of developing itself by way of self-constituting prostheses—linguistic/semantic and technological—artificializing its environment and automating its ecology. Russian philosopher Nikolai Fedorov, an early champion of radical life extension and a precursor of transhumanism, identified this same deficiency as what moors mankind to the creative drive, the impetus to explore foreign lands and celestial spaces with the index of domination in mind. Musing on the history of human invention and reinvention, Fedorov remarks that:\n\nthe spread of humanity over the planet was accompanied by the creation of new (artificial) organs and coverings. The purpose of humanity is to change all that is natural, a free gift of nature, into what is created by work. Outer space, expansion beyond the limits of the planet, demands precisely such radical change. The great feat of courage now confronting humanity requires the highest martial virtues such as daring and self-sacrifice, while excluding that which is most horrible in war—taking the lives of people like oneself (Fedorov, 1990: 96).\n\nFor Fedorov, this ‘underdetermination’ of overspecialization, the Promethean anthropological-philosophical treatise, grants a level of behavioral leeway and cognitive lability to mankind.[1] On one hand, Stiegler follows from this tradition, for he sees such invention as inexorable from the human project but, on the other hand, Stiegler also argues that there are catastrophic implications riddling its most recent actualization in the realm of ‘the digital’ (or what he calls ‘digital tertiary retentions’). For Stiegler, digital reinvention is steeped with the phenomenological erasure of creative technical activity, where technē is no longer the project of human rationality[2] but the engine of positive feedback loops that overdetermine and undercut creativity, forecasting mankind’s future into anomie and isolation. Such is the murmur of Stiegler’s techno-pessimism.[3]\n\nStiegler’s fatalistic treatment thus recalls what Friedrich Kittler perhaps all too dismissively denounced as the ‘old thesis’ of media—the understanding of media artefacts as extensions of human systems/organs.[4] Yet this ‘old thesis’ is not merely [5]relegated to media theorists such as Stiegler nor obscure transhumanist thinkers such as Fedorov. In fact, it finds its philosophical roots in the birth of philosophy, which arguably spouts forth from the consideration of the human organism as the inheritor of lack: a remainder that is underdetermined, underspecialized, and underdeveloped physiologically and adaptively.[6] This ‘lack’, the posthumanism that haunts the human project, is precisely what innumerable philosophers have argued motivates mankind’s invention of artificial technologies, artefacts, and languages, systematizing a recursive scaffolding of self-reference and development.\n\nThis rendering has a long lineage beginning with Plato’s Meno. In the Meno, Socrates insists that one can know nothing of virtue intrinsically and that knowledge transpires dialectically, a byproduct of dialogue and interrogation vis-à-vis sociability and deduction. For Socrates, knowledge is an offspring of the nature of virtue revealing itself. Consequently, Socrates calls upon an uneducated slave from Meno’s retinue, querying whether this boy knows how to calculate the double of the area of a square. As the boy draws a square in the sand, followed by diagonal lines, Socrates claims that the slave ‘spontaneously’ recovers the solution to this problem. Exteriorizing the calculation, technesis involves the synthesis of the hypomnesic inscription within the inorganic domain (the sand), which results in the recursive logic of recollection—mechanical reproduction invites knowledge into a dance with artificially reconstructed memory.[7] Socrates, noting how knowledge springs forth from the inscribed calculation concludes by noting:\n\nMeno, can you see where our friend here has got to on his journey towards recollection? At first, he didn’t know which line would produce the figure with an area of 8 square feet––just as he doesn’t yet know the answer now either; but he still thought he knew the answer then, and he was answering confidently, as if he had knowledge. He didn’t think he was stuck before, but now he appreciates that he is stuck and he also doesn’t think he knows what in fact he doesn’t know (Waterfield, 2005: 84a-84b, 120).\n\nThe Socratic elenchus here asserts that the correct process of questioning (by oneself or an external agent) elicits the recollection of relevant truths and is markedly dialectical—that is, despite the fact that it is Socrates who proposes the definitions or ideas for discourse, he does not examine them or grant them truth-value until he gains a measure of agreement from his interlocutor (in this case the slave-boy and Meno). In the Meno, this process is synthesized by way of the inscribed formula, finding its nexus in artefaction. The formulation of artefaction, as the offspring of a dialectical intellectual process, is normatively positive[8]—occupying the space of calculation-cum-inscription, that provides for recollection of that which was dormant: knowledge, which becomes prosthetic. The Hegelian philosopher of technology Ernst Kapp (1808-1896) called this process ‘organ-projection’, German anthropologist Paul Alsberg (1883-1965) termed it ‘body-liberation’, mathematician Alfred J. Lotka (1880-1949) deemed it ‘exosomatic evolution’,[9] and André Leroi-Gourhan (1911-1986) called it ‘exteriorization’. Despite the various verbiage, the central principle remains the same: the human and its function(s) becomes delaminated from any particularized specializations and adaptations, such that mankind configures an existence for itself by way of inventing technologies. In doing so, mankind externalizes its bodily functions, automating its means of survival by way of artificializing its environment so as to supplement its biotic deficiency.\n\nFor the sake of this paper, we will focus on the recursive logic at play here, and thus limit ourselves to two strands of thought: dialectical and non-dialectical. The first camp ensnares a collective of dialectically-minded philosophers, who—reaffirming the humanist Hegelian project of the Dasein of Geist as progressing via an asymptotic telos—imply that technological evolution is necessarily carved around lines of succession, wherein ‘technological progression’ finds itself always countered by the intervention of negativity. In addition to Kapp, Alsberg, Lotka, and Leroi-Gourhan this genealogy includes Alfred Espinas (1844-1922), Georges Canguilhem (1904-1995), Arnold Gehlen (1904-1976), Heinrich Popitz (1925-2002), Raymond Ruyer (1902-1987), Bernard Stiegler (1952-2020), and Marc Azéma (b. 1967),[10] many of whom are often regarded as philosophers of ‘organology’. Before more closely examining the Hegelian core of this project, let us clarify what, exactly, being a thinker of organology entails, as it invokes a particular conception of reflective thought that recalls a central pillar of the first-order cybernetics of Wiener, McCulloch, and Shannon: positive feedback within a closed system. This is an important schema to keep in play as we review the Hegelian notion of self-externalization.\n\nOrganology\n\n‘Organology’, as a philosophical system, concerns itself with how recursive forms occur in nature and, thus, has an inherent ecological penchant. It invokes an epistemological and differential rendering of synthetic division—that which is inorganic is differentiated from that which is organic, such that one can direct the reflective judgment towards the object of thought, deracinating consciousness from the object of consciousness. Organology is a synthetic thought ‘that not only integrates but also searches for a new epistemology that creates a new loop’ (Hui, 2019: 25). As evidenced by the 1947 paper ‘Machine and Organism’, Canguilhem was the first thinker to use the term ‘organology’;[11] in refusing the mechanist purview, Canguilhem purposed this term in order to consider the relation between organism and machine within a dialectical purview while reverting the Cartesian epistemology (of mechanizing life). Where, according to Descartes, the body and its movements are governed by mechanical rules, for Canguilhem they are governed by something far more fundamental and primary—an exigent vitalism that is always translated by way of representeds. Canguilhem rejected the parallelism between organisms and machines—organisms are not necessarily adaptive and, thus, can display pathological behaviors; machines (specifically, Canguilhem seems to have had the automata of first-order cybernetics in mind) on the other hand, are guided by a telos—fixed pre-programmed goals. Where organisms are able to self-organize and self-repair, machines cannot repurpose their goals, programs, or directives; Canguilhem remarks that ‘[i]t can easily be said that there is more purpose in the machine than in the organism, because the purpose of the machine is rigid, univocal, univalent. A machine cannot replace another machine’ (2008: 89). Although this was not necessarily the case, as Norbert Wiener’s work on homeostasis and adaptive response was formulated around the concepts of emergency, error, and shock (modelled around Walter Bradford Cannon’s work on fight-or-flight response),[12] for Canguilhem—and the philosophy of organology thereafter—science and technology are understood to be circular, with the body and its nomological laws reinforcing something more primary: life.[13] This gives a clue as to why, exactly, Canguilhem’s ‘organology’, and, in turn, the project of organology writ large is dialectical. Rather than constituting a supersensible world, the nomological laws of nature are understood as making explicit something that is implicit already in ordinary empirical descriptions of how things are.\n\nCanguilhem’s rethinking does not reduce the machine as the equivalent of the human but, instead, conceives of the human-machine as an organic whole, set into dialectical motion. Canguilhem’s conception was inspired by many other thinkers, including Kurt Goldstein and his theory of holism, the exteriorization theory of Leroi-Gourhan,[14] and Kapp’s organprojektion.[15] Organology finds its most markedly Hegelian articulation with Kapp, who coined the phrase ‘philosophy of technology’ in 1877. Kapp was a Hegelian philosopher true and true, whose work sought to demonstrate technics as the synthesized projection of organs, tools understood as biophysical hardware, with the ineluctable human canalizing itself by way of its technology. For Kapp such tools mechanically reconstruct organic form, serving as indices of morphological residue. As a Hegelian, Kapp’s image of thought finds the human (and the human project) reasserting its primacy, entailed in dialectical becoming. Kapp’s understanding of the tool illustrates the Hegelian core of media-as-externalization, wherefore the universal and singular forms are entangled in totality, each containing the determination of the other within it and, thus, the two are wrapped into absolutely one totality, their oneness the diremption of itself in the free reflective shine of this duality.[16] Having now reviewed organology, let us now implore this Hegelian core.\n\nSpirit, the Rational Object, and Definite Conception\n\nIn Hegel’s Phenomenology of Spirit (1807), the notion ‘alienation’ is moored to two German terms that Hegel utilizes: ‘Entfremdung’ and ‘Entäusserung’. Although both have often been translated as ‘alienation’, in parsing this heteronomy with a sharpened conceptual scalpel at hand we ought to note that ‘Entfremdung’ more closely refers to estrangement as the process or state whereby consciousness is separated from one or more of the aspects required for consciousness to fully understand itself. ‘Entäusserung’, on the other hand, is the process where consciousness externalizes itself in an object-ified form and, by way of the object, develops a more adequate understanding of itself. The former is linked to alienation and the latter artefaction, as (self)-externalization is the way consciousness learns that it is not purely a subject and has an ontological structure that not only incorporates a relation to objectivity but depends on this relation. In turn, consciousness is purposed and re-purposed, deracinated from the subject as it is distributed among a community in the form of concrete content—instrumentalized vide the form of work and the objects of labor. Thinking, the profoundest aspect of Spirit with its highest activity being to comprehend itself, unspools by way of its operations, which direct themselves towards determinate activity, the aims of finitude. Thus, we see how cognitive activity is directed not towards interiority but a determinate actuality. For Hegel, the nature of Spirit must particularize itself to become true and this is achieved by way of movement towards externalization: “consciousness is essentially this process—not a remaining static in the immediate natural state but a passage through a process in which what is eternal or true, as its essence, becomes its object or purpose” (Hegel, 1990: 21).\n\nFor Hegel, the activity of object-ification transfers and converts empty objectivity into a manifestation of being in-and-for itself, i.e., self-determination. For “[a]s soon as the universal is externalised, it takes on a particular character. In isolation, the inward dimension of the Idea would remain a lifeless abstraction, and it is only by means of activity that it acquires real existence” (Hegel, 1975: 79). Spirit abandons its original condition and discovers itself through what it performs, translating inner essence into reality by way of externalizing the universal concept and, thus, attaining a ‘real’ existence. For Hegel customs, laws, institutions, and symbols of ancient nations were vessels of speculative ideas and products of Spirit but the true fruit of Spirit never comes first; the speculative Idea is externalized, it is always the manifestation point of rationality upon worldly existence, where the potentiality in consciousness, volition, and action finds itself inorganically excised through its determinate object. Spirit, for Hegel, is not abstract, because “it is consciousness, but it is also the object of consciousness—for it is in the nature of the spirit to have itself as its object. The spirit, then, is capable of thought, and its thought is that of a being which itself exists, and which thinks that it exists and how it exists” (Hegel, 1990: 45)\n\nRay Brassier, in a recent conference chaired by Paul B. Preciado titled “The Parliament of Bodies: Communism will be the collective management of alienation” asks whether this heteronomy between ‘Entfremdung’ and ‘Entaüsserung’ implies synonymy? Spirit’s self-externalization—that is, collective self-consciousness’ self-externalization—is undoubtedly constitutive, but there is a marked difference between how Spirit realizes its freedom and those ways by which it becomes bound or subjected to a foreign agency or power, which is only, itself, an alienated or estranged form. That is, following Brassier’s account, all estrangement is externalization but not all externalization is estrangement. Drawing from Adorno’s reading of Hegel, Brassier proffers that this account evidences the dialectical interplay between Spirit’s independence and dependence, wherefore Spirit frees itself form its subjection to nature, achieving spiritual independence/autonomy, and in doing so moves towards culture as a kind of “second nature” to which it then becomes subjected. It becomes dependent on societal institutions, customs, and norms in a manner by which Spirit’s freedom is significantly diminished. At once, naturalness or instinct is replicated within Spirit, manifesting within it in an estranged form while, institutions, customs, and norms begin to function as if they were nature. The anthropologist Arnold Gehlen would later similarly refer to reified second nature but by way of mankind’s deficiency, using the term “ersatz organs” when describing technologies and institutions alike, with both of them compensating for the unfinished or lacking human; nonetheless, in both accounts in furnishing institutions qua norms culture becomes man’s “second nature.”\n\nAccordingly, for Hegel every self-consciousness denaturalization engenders an unconscious re-naturalization, repressing Spirit. First there is subjection to necessity and then emancipation by way of generating another form of subjection. As Ludwig Feuerbach, a key disciple of Hegel, remarks: “Man—this is the mystery of religion—projects his being into objectivity, and then again makes himself an object to this projected image of himself thus converted into a subject; he thinks of himself as an object to himself, but as the object of an object, of another being than himself” (Feuerbach [1841] 2008: 181).\n\nObjectification thus yields the object to which the object-ifier is objecti-fied internally. This is the processual movement of alienation as double-objectification in Feuerbach and the young Marx.[17] A naturalized scenography divulges itself where humans are necessarily self-externalizing, i.e., producers by nature. The termination of subjection is not the reinstatement of interiority—externalization is not the externalization of a pre-existing originary substance or the index of a vital source but, rather, a constant process of amendment, with this process generated because of the constituent non-identity of humans as self-transforming producers. Given this picture of Spirit, we are encouraged to see self-externalization as resulting in either a state that is alienated or un-alienated depending on the circumstances in question. Stiegler, however, collapses all norms and exercises of freedom into alienation by way of mechanical compulsion, such that we cannot measure the discrepancy between realized and unrealized collective human freedom, for our metaphysical collective Spirit is always being outpouched by way of a processual unfolding.[18]\n\nRecall how, in the Science of Logic ([1816] 2010), the universal and singular form totality, the concept passing into concrete existence which is, itself, free and is none other than the ‘I’ or pure self-consciousness. The ‘I’ is the pure concept itself, the concept that has come into determinate existence and finds itself instantiated into all manmade hardware.[19] In Philosophy of Nature ([1830] 1970), Hegel constructs concepts that define Being by way of a tripartite model—the mechanical, the chemical, and the organic—demonstrating that these are instantiated into our productive experience of the world in equal part, with the living body sustaining this ‘contradiction’ (Hegel, 1970: 10).[20] For Kapp, Hegelian production as such is conceptually compounded into machine- or tool-construction and machine- or tool-use, with this ‘contradiction’ elaborated upon by way of technical objects (which exteriorize, or ‘project’ the organs of the body). Via Absolute Spirit, the eternal idea in and of itself keeps itself concentrated and reproduced, continually regenerating itself and enjoying its eternal status.[21] By demonstrating an inner affinity between the constructed tool and the human organ, Kapp’s notion of organprojektion (‘organ projection’) facilitates a constructive affinity that is drive-based, as artefactual technesis results in the human being’s made partial, or divided, such that the artefact serves as a means of reproduction. While contemporary Hegelians such as Robert Brandom (2007) take an interest in how language’s reproduction is equipollent to mankind’s sapience, Kapp designates technical objects as similarly primordial and biological—for Kapp, the human impulse to manufacture tools and machines is steeped in the well of cognitive activity from which language is drawn, with tool-construction similarly understood as reproductive activity fettered to reason and knowledge-production. With the organ-cum-tool or -machine now partially separable from the entirety of Being, mankind’s Being is defined and derived through the attributes, usage, and complexity of practicability.\n\nKapp’s cyclic and augmentive-physiognomic description of artefaction reveals his Hegelian plexus, with the ontological paradox of dialectical historicity as premised on an open Whole that is irremediably ruptured by its own absolute negativity. However, as the aforementioned description makes clear, his understanding of technical objects is normatively positive, with tools archiving consciousness. Another dialectical philosopher of organology, Raymond Ruyer, reserves consciousness as a kind of ‘absolute’ form with non-localizable zones of indetermination. According to Ruyer, animal species produce phenotypically externalized technologies—‘spiders weave webs, beavers build dams, birds construct nests’ (Smith, 2017: 121). In contrast to animals, humans engage in the quasi-finalist endeavor of fabricating technical artefacts. Here, Ruyer recollects Kapp’s morphologically-directed and evolutionary understanding of artefaction—the ‘hammer externalizes the forearm and fist in wood and iron; clothing externalizes the skin; a baby’s bottle externalizes the mother’s breast; a kitchen stove externalizes the stomach; and so on’ (Smith, 2017: 121). Directly citing Leroi-Gourhan’s work while simultaneously calling to mind Kapp, Ruyer also notes that our ‘externalized organs’ become detachable, removable and disparate from our body—therefore, our ‘bodily organs’ are, themselves, uniquely technical artefacts.\n\nWith Canguilhem, Kapp, and Ruyer, Being has its foundation in the question of eternal metahistorical identification, which begins with the Absolute Idealism of Hegel’s teleological arrow but is further continued by the account of post-mechanist natural science, where Being is explicitly related to self-hood and presupposes ‘a whole of which subjects and objects are parts’ (Hölderlin, 1988: 124-126). Given form by the reflective judgment, which imposes limit conditions upon cognition’s freedom, Being materializes and internal emergence is extended through technologies. However, none of these aforementioned thinkers of organology were alive to speak of digitally embedded and networked technologies; Stiegler remains heir to organology, although he wields the throne as a pessimist of necessitarian alienation, collapsing technesis writ large.\n\nAlienated and unalienated states do have a place of distinction for Hegel, however. Hegel’s insistence on phenomenological immanence amounts to the claim that we cannot arbitrarily create a criterion of unalienated Spirit based on what our current historical conception of what freedom is and, in turn, rules out any appeal or reference to an unalienated originary state. Thus, there is no such originary unalienated state, it is the stuff of mythology. This is the critical constraint of historical immanence, such that any differential criterion must be internal and immanent to the shape of self-consciousness that is held under consideration. The problem is that the historical sequence is not empirically given, so we do not have access to it by way of our perceptual history. For Hegel, try as we will, we cannot get at alienation by threading historical facts together into a progressing narrative that unfolds from past to present. Deliberately and self-consciously constructing the retrospective preconditions for our current self-conception as free or unalienated is a matter of retrogressing to a previous unfreedom or alienation (in order to then discern our current measure of freedom or unalienation). As Brassier says, “it entails that we are never in complete possession of the resources through which we confidently distinguish between alienating and non-alienating automatisms among contemporary customs, institutions, or norms” (2020). The processual model of alienation reifies the moments in what is, for Hegel, an indivisible movement, where compulsion and freedom coincide. Estrangement cannot be the return of repression within spirit so long as this return is understood as the reiteration of a proceeding state and we are never in complete possession of the resources through which we could confidently distinguish between normatively positive and negative automatisms (as well as the technologies underpinning them). To acquire such resources we must retrospectively reconstruct the indivisible movement between what coincides, and this is but an impossible task. Estrangement and de-estrangement, compulsion and freedom, coincide for Hegel such that estrangement is not a repetition of compulsion within the attempt to undo compulsion so long as this repetition is understood as the reiteration of an initial or preceding state. Alienation proper, in both Marx and Hegel, is externalization as de-estrangement-cum-estrangement; the prospect of an unalienated state emerges by retrospection and, as a consequence, there can be no narrative about overcoming the need to overcome so long as self-externalization is compelled by the need to dominate external nature. Alienation will always generate a surplus of estrangement, a series of compulsive automatisms that prevent us from realizing our freedom.[22] As such, we are necessarily directed towards constructing our machines—it is how we concede to Spirit, convincing Spirit of rationality’s vim. In order to create an incision to solve the two unique problems that emerge here, the problem of phenomenological immanence (with Hegel) vs. the problem of naturalized metaphysical lossage (with Stiegler) we will eventually take a more systems-theory oriented approach that emphasizes how homeostasis and autopoiesis figure into the introduction of novelty, where any system’s equilibrium is directed by way of technical objects and their entanglements. First, however, let us more exactingly scrutinize Stiegler.\n\nStiegler’s Fatalism and Digital Media Artefacts\n\nIs Stiegler’s program truly one of Prometheanism? If Prometheanism is the rejection of predetermined limits on action and self-transformation, then the parallel between rationalism and Prometheanism suggests that if action is constrained by thought and self-transformation is constrained by self-understanding, then the rejection on limits on one entails the rejection of limiting the other (Brassier, 2014). That is, Prometheanism proposes that all reasons are ‘artificial’ and rejects limits on artificialization, enjoining the wholesale reengineering of intelligence to our techno-environmentally embedded ecology according to a more rational program of self-invention than Stiegler’s prescription of quasi-theological limits to artificialization.\n\nKapp sparsely appears in Stiegler’s pages as a direct source and Canguilhem even less so. Nonetheless, for Stiegler, as was the case for Kapp and Canguilhem, tools are imbricated in habitus. For Stiegler the relationship between tool and cognition is reflexive: ‘[t]he interior is constituted in exteriorization’ (1998: 141), such that the tool (re)invents the human just as much as the human invents its tools. Stiegler’s conception of Being recalls Hegel by way of its plasticity: Being here is progressive not only in the sense that information, as it is exteriorized beginning with early lithic technologies, accompanies hominid evolution but also in how media are related to neural development. Thus, Stiegler’s engagement with externalization, like his predecessors, is preoccupied with registers of cognition. For Stiegler, like the flint tool, tools of writing are understood as epiphylogenetic vectors that advance corticalization; although stone tools were not crafted with the intention of storing memory, for Stiegler they inadvertently do so, inciting a series of learned motor actions (e.g., flexing the elbow in the act of using a hammer). Despite retrofitting Kapp and company’s conception of media-as-externalization, however, Stiegler’s sociological argument re: technics elicits an admixture of Derridean ontology with Husserlian phenomenology, where the externalization of technology imbricates mind and intention through successive feedback loops. Consider, for instance, how for Stiegler the true advent of externalized memory (‘mnemotechnics’) finds its pinnacle with the invention of writing and demarcation; according to Stiegler, writing stabilizes language (vis-à-vis time-consciousness) in both the brain and the discursive world.[23] Unlike writing, however, Stiegler regards those digital technologies that store memory, such as smart-phones and tablets, as software agents that deprive the user’s freedom, making choices for the user in advance (Stiegler & Rouvroy, 2016). For Stiegler, with analog media after writing, but especially with digital media, we see not only the exteriorization of intention, mind, and memory but also its loss—instead of expanding the capacities of the human mind and brain, a loss occurs, outsourcing the mind without emancipating the human agent. If we are to take Stiegler at his word, digital artefaction’s relationship with technē ought to be understood as normatively negative, as it results in the deprivation of human agency (what Stiegler terms ‘noesis’) and alienation:\n\nThis exteriorization that constitutes the principle of ‘liberation’ is always also an ‘alienation’: it leads to an offsetting of neganthropic possibilities to exosomatized organs that amounts to a kind of dependence, which is the basis of proletarianization as the loss of knowledge (of how to live, work, and conceptualize), that is, as entropy. This is nothing new (Stiegler, 2019: 50).\n\nIn a recently translated book, The Age of Disruption (2019), Stiegler makes it his mission to:\n\n…show, by combining Foucault’s analysis with that of Sloterdijk, that what the latter describes as a process of disinhibition is made possible by the tensions and contradictions that occur over the course of the successive doubly epokhal redoublings that unfold from the late Middle Ages to the Renaissance, the classical age, the first industrial revolution, the advent of Taylorism (which is also the advent of consumerism, the culture industry, and marketing), and, finally digital technology—agent of the contemporary disruption (ibid., 111).\n\nThe Age of Disruption is only the most recent articulation of this project, but since it is the most recently translated into English,[24] it includes the most novel references to digital technologies, including GPS vehicle tracking systems and smart watches. While references to actual technologies are sparse, it seems that Stiegler has no laudatory words for recent developments such as fitness wearables and location tracking apps that harvest data and metadata, or which engage in social media sharing and life-logging infrastructure. For Stiegler, these technologies are far from the positive self-archivization or self-curation of labor that those such as Gordon Bell and Jim Gemmell identify with becoming ‘the librarian archivist, cartographer, and curator of your life’ (2009: 5); for Stiegler such embedded wearables further disembody one’s autonomy by way of pushing us into an objectified mass, each of us becoming an unwitting member of the cognitariat.[25]\n\nMark B.N. Hansen’s critique of Stiegler takes his normatively negative positioning of phenomenology as its object. For Hansen, Stiegler’s phenomenological system is tunnel-visioned; Hansen’s project takes issue with this picture of consciousness that undermines what also makes networks and microtemporal digital devices useful—their possibility of shaping the future by way of honing in on the operational present (in a way that we, deprived of digital technologies, are restrained from accessing). Unlike Stiegler, Hansen sees digital media as also engaged in the channeling of agency, and thus understands the ‘causal efficacy’ of digital media as normatively positive (2015: 197). For Hansen, twenty-first century media in particular can enhance our cognitive, perceptual, and sensory agency precisely because they put us into functional cooperation with other cognitive, perceptual, and sensory agents that not only follow protocols of their own, but that, most crucially, operate environmentally—‘independently of and autonomously from our directly experienced, conscious agency’ (ibid., 183).\n\nHansen’s critique primarily cites Deleuze’s machine ontology and Whitehead’s process philosophical system to account for that which Stiegler’s occludes. From Deleuze, Hansen recuperates how ‘media might impact experience without being channeled through delimited, higher-order processes’, thus understanding media as ends in themselves (36). From Whitehead, Hansen reconceives of how perceptions can be understood from within the material universe where causality reigns, moving beyond a strictly perception-centered account to one where perceptions are, themselves, caused by the very same kind of shift that causes all events in the universe’s becoming (48). Moving through causal processes in cyclic fashion, Hansen, too, is a thinker of dialectical organology, using Deleuze and Whitehead’s process philosophy to understand the non-perceptual world through the aperture of perception.[26]\n\nAnother feature of Hansen’s project is his conception of ‘feed-forward’ systems, which presents propensity as causally directed by future efficacy, such that continuous data-gathering, microcomputational processing and predictive analytics make a new episteme possible. According to Hansen, our contemporary conception of technē must be understood as an ‘enveloping of virtualities offered to the body, which constitutes the fundamental anchor point for present and future technological evolutions, and which induces an automatized and fluid relation to the milieu’ (Hansen in Grusin, 2010: 117-118). Consequently, Hansen’s resolutely speculative ontologising of predictive processing is predicated on an ecological functor of equipossibility and the independence of outcomes.\n\nAccording to Hansen, Stiegler’s conception of technicity does not properly apply to today’s enactive field of predictive calculation. That is, Stiegler’s model of time-consciousness and anticipation (what Stiegler calls ‘protension’) operates in relation to a static source of inert possibilities. Hansen remarks that predictive probabilities, on the other hand, do not project past data into the future but generatively integrate present data:\n\n[w]hatever else it betokens, twenty-first-century media centrally involve a massive expansion in, as well as a fundamental differentiation—a ‘heterogenesis’—of, the interface between human being and sensory environment. Consciousness’s Being […] becomes, in and through the operationality of twenty-first-century media, a functional […] processual, relationship. With this functionalisation, moreover, the relationship between aboutness and being, between data as access to sensibility and data as sensibility, undergoes a certain reconceptualisation through its anchoring in temporalisation: aboutness is linked to being in an incessant oscillation, where each act of access onto sensibility creates a new unit of sensibility that itself calls forth a new act of access that creates a new unit of sensibility’ (Hansen, 2015: 17).\n\nHansen is, effectively, interested in the ‘becoming’ of homeostatic systems that self-regulate, a becoming that Stiegler reduces to an ellipsis of lossage, a telos of asymptotic reduction whereby creativity, potential, and freedom find themselves overturned to disruption and anomie (Stiegler: 2019). In addition to Hansen’s critique, however, and perhaps more detrimentally—particularly when we consider how Stiegler conflates the conditions for genesis of language with the conditions of its reproduction when he writes about linguistic systems—we can remark that Stiegler negotiates metaphysics from within. There is no distinction in terms of complexity between the analog and the digital,[27] just an arbitrary line between that which is fundamentally tethered to organic existence and preternatural existence, which is negatively determined. That is, Stiegler produces a quasi-materialist ontology that positively addresses the Hegelian Objective Spirit as the inscription of object-ified knowledge, according to which ‘objectified knowledge is the consequence of the epiphylogenetic formation that arose in life with the practical activity of human beings’ (Stiegler, 2020: 179). Yet Stiegler’s explanatory ontologization of grammatization is akin to a half-formed functionalism that engages solely on the level of empirical content—it characterizes a system as functional in nature by treating its function in terms of stimulus-response dispositions. In short, Stiegler does not differentiate conceptual activity from non-conceptual activity. For the early Hegelian theorists of externalization/projection, the Dasein of Geist supervened upon ontogenesis but, for Stiegler, there is no differential identity of acquisition.\n\nSimondon and Constructivist Cybernetics: On Second-Order Systems\n\nHow, then, can we extract a philosophy of differential media without reducing the objects of mediation to (externalized) appearances? Hansen is, indeed, correct to point out that what Stiegler elides is a process-philosophical understanding of causality. Instead of Deleuze or Whitehead, however, I propose a return to Simondon—specifically, that which Stiegler misses in Simondon. Stiegler is, as I seek to demonstrate, a crude reader of Simondon and in some sense a traitor to Simondon. First of all, Simondon is utterly opposed to Heidegger,[28] as Simondon actively dismisses the study of technical objects in terms of the relations that can be established with them.[29] Simondon’s project is marked by a sociological stripe: to reclaim technical operations from the teeth of (Capitalist) work and prod forth specialization and a place for a philosophy of technics, which is markedly scientific. This means separating technical activity and knowledge from the doctrine of work, Capital, and the criterion of productivity by encouraging a relationship between man and technical objects where one is not only the owner of their machines and means of productions but imbricated in their maintenance, adjustment, engineering, knowledge, and further invention.\n\nSimondon offers a unique stripe of process philosophy of the ‘transindividual’, which Stiegler refers to in name only but does not make conceptually central to his system. Deleuze’s machine ontology, at least as it treats ‘actualization’ (the co-determination between beings and technical artefacts), is tethered to Simondon so rather than jumping to Deleuze in order to analyze today’s digital protocols, my study—which agrees with Hansen that process philosophy has immense instrumental value to understand ‘feedforward systems’—turns to Simondon. First of all, what distinguishes Simondon from the previous dialectical thinkers of organology is that Simondon is a thinker of genesis above all else. This is why he will serve as a valuable resource when we consider the roots of deep learning’s algorithmic infrastructure given case studies such as AlphaGo Zero, which has been admired by philosophers and technicians alike due to how it generates inventive winning moves that even master players such as Lee Sedol and Ke Jie could not predict (e.g., the infamous ‘move 37’).\n\nFurthermore, Simondon’s philosophical program is non-dialectical. This also sets him aside from the aforementioned Hegelian genealogy. For Simondon, genesis’ role is to deal with something that dialectics elides, as Simondon’s conception of individuation does not correspond to the appearance of the negative. Rather, Simondon considers the role of genesis as something immanent to conceptual negativity; Simondon, as a thinker of the existence of potentials, is primarily preoccupied with the cause of incompatibility and the non-stability of pre-individual genesis—that which precedes the genesis of an object. Given his penchant for complex systems, we will also make the case for Simondon as a thinker of second-order cybernetics (the conceptual predecessor to deep learning’s logic of predictive inference). Alongside genesis, Simondon is concerned with transductive relations: how subjects and objects engage in mutual structuration such that identity is metastable (much like second order cybernetics and W. Ross Ashby’s conception of allostatic behavior). Unlike Hegel, for Simondon the negative appears initially as an ontogenetic incompatibility, but, after examination, reveals itself to, in actuality, merely illuminate a wellspring of potentials. Thus, Simondon’s notion of the ‘negative’ is not a substantial or regulative negative—it is not a stage or phase. Unlike Hegel, Kapp, Leroi-Gourhan, or Stiegler, the process of individuation for Simondon is not a synthesis or return to unity but a phase-shift of Being based on a pre-individual center of potentialized incompatibility.\n\nSimondon’s description of communication as such is critical to consider in formulating a rigorous philosophy of becoming that applies to machines and their systems of equilibrium, which I argue is a highly pertinent concept when thinking of predictive patterning in deep learning, a development born out of second order cybernetics’ critique of first-order cybernetics. Simondon’s On the Mode of Existence of Technical Objects (2017) is composed of three sections. Part I, entitled Genesis and Evolution of Technical Objects, is devoted to the study of the machine itself and closely considers the terms of what computer scientist John Hart[30] terms ‘intrinsic machine reality’—that is, Simondon parses the principles of the nature of the technical object while supplementing this study with corresponding examples. Part II, Man and the Technical Object, launches a critique of mechanology and first-order cybernetics, particularly Norbert Wiener’s mechanist paradigm. Simondon’s conception of technical objects opposes the conception of automata according to a human resemblance-model or the conflation of natural and technical objects, preferring, instead, an analysis by way of functional organization. In the final section, Part III, Genesis of Technicality, Simondon most lucidly articulates his normative program concerning the convergence between technics, the machine, and philosophy.\n\nWhy is Simondon a thinker of environmental synergies? In order to make the normative value of ‘technics’ clear—for Simondon, ‘technics’, as a philosophical term, invokes normativity in a way that ‘technology’ does not—Simondon recalls a curious incident. Simondon notes that ‘about ten years ago’ a group of science university students attempted a small profanation that illuminates the normative value of technics outside of the practical consequences they generally appear to entail. The science students disrupted the clock atop the Paris Observatory, causing an uproar (2017: 229). Simondon highlights this as an example of the inherence of technicity’s values surpassing utility, critiquing the Heideggerian conception of technics as separate utensils understood solely through their relational status as ontic objects (i.e., by way of their use-value), shining a light on the necessarily networked nature of technics—such is the reticulated structuration, which can change in intensity but never disappears (even though its ‘readiness-to-hand’ is modulated by way of its breaking).[31] The reticulated nature of technics is thus well illustrated by the image of the Paris Observatory clock domineering over the perturbed Parisians.[32]\n\nAccording to Simondon, ‘technics’ considers the coordinated and mutually organized relation at the level of machines, man, and culture. For Simondon, ‘the true nature of man is not to be a tool bearer—and thus a competitor of the machine, but man’s nature is that of the inventor of technical and living objects capable of resolving problems of compatibility between machines within an ensemble’ (2017: xvi). In rendering machines compatible with a particular function, man opens up the mutually contingent flow of information, a signal of transindividual exchange.[33] Man’s assiduous invention and reinvention also invites technicity to unmask the pretensions of rationality, with Simondon’s concept of invention shepherding the long-term progressive undertaking of a program of self-betterment and self-improvement rather than the blind effect of contingent causes.\n\nIn Genesis and Evolution of Technical Objects, Simondon pursues a review of various technical objects and how the technical object, in itself, is characterized by sundry functional processes and modalities of genesis. Simondon separates the genesis of the technical object, by which it becomes a proper object of study, into a tripartite and non-dialectical cast: the element, the individual, and the ensemble (20). Much of Simondon’s research is devoted to the erudition of particular machines, their evolutional lineage(s), and their functional assemblies—Simondon undergoes this pursuit with impressive rigor. From Coolidge and Crookes tubes to thermo-dynamic machines, anode-cathode synergies, semi-conductors, and vacuum valves, Simondon carves a pure schema of functioning, examining technical objects and their conductance by way of and through the phenomena from which their fecundity foments. Of particular interest to Simondon is concretization, the process by which any technical object is posited within an intermediate place between the natural object and its scientific representation. This process is separate from invention, the latter dealing with our genealogical heritage and natural kinds. Rather than the shift from idea to physical artefact, concretization deals with internal self-coherence (cycles of refinement), the ‘way technical objects shift their internal composition as they iterate from one version or instance of a schema to the next’ (Rieder, 2020: 61-62). According to Simondon’s system, the primitive technical object is distinguished from the constitution of a natural system—the primitive technical object is not a natural/physical system but the ‘physical translation of an intellectual system’ and, thus, a bundle of applications (Simondon, 2017: 49). Opposed to the primitive technical object is the concrete technical object, which produces a mapping of causes and effects by way of internal coherence (homeostasis), a closure of functional resolve that designates the technical object’s evolution and processes of human intervention. Simondon provides the example of an artificial plant/flower grown in a greenhouse, which yields petals—‘a double flower’—and is unable to bear fruit due to processes of human grafting (49). By way of artificialization, the interventive rendering of a natural object into a complex system of thermal and hydraulic (i.e., autopoietic) regulations via machinic coherence, we see how biological capacities and their functions are outpouched to a mechanic framing system and, simultaneously, are specialized in cyclic fashion.\n\nIn Man and the Technical Object, Simondon’s criticism of Wiener’s project brings forth considerations similar to that of cybernetic constructivism, endo-modelling, and action-oriented neuroeconomic patterning—the tenets of second order cybernetics which we will further elaborate on soon. Written in the 1950s, this section finds Simondon critiquing Wiener’s 1950 publication, The Human Use of Human Beings. Conceptualizing information as an entity distinct from the substrate(s) carrying it and likening self-regulating technical objects to living beings, Wiener here proclaims that:\n\n‘[i]f the feedback system is itself controlled—if, in other words, its own entropic tendencies are checked by still other controlling mechanisms—and kept within limits sufficiently stringent, this [disastrous instability] will not occur, and the existence of the feedback will increase the stability of performance of the gun. In other words, the performance will become less dependent on the frictional load; or what is the same thing, on the drag created by the stiffness of the grease. Something very similar to this occurs in human action’ (1954: 25).\n\nAccording to Wiener, man (and the animal) have a kinaesthetic sense that, like the machine, is subject to varied external environmental effects related to informational exchange by way of negative feedback. In turn, contingency acquires meaning by way of reference to recursion. This is precisely why Wiener compliments any mention of homeostatic processes with internal regulation, finding a parallel in the nervous system and computing machines wherefore computing machines and human/animal nervous systems are conceived of by way of reference to physiological processes of detection, an ‘all-or-none’ principle of measuring transmitted intensity (Wiener, 1965: 120). Following the prototypic probabilistic laws of internal regulation and Leibnizian universalism, Wiener’s cybernetics conceives of information by way of measurement as a degree of organization. For Simondon, however, technical objects must be studied in their evolution in order for one to discern the process of concretization. Simondon is not satisfied with Wiener’s conception of individuation by way of circumscribing probabilistic events. While Simondon’s conception of individuation is equipollent to an understanding of information concerned with that which precedes differentiation and facilitates an operation in a system, Simondon also harbors an ulterior concern: the intra-individual psychical problematic of perception and affectivity, which leads to the collective level of the transindividual.\n\nSince Simondon’s target is first-order cybernetics, with reference to Wiener in particular, it makes a great deal of sense that criticisms similar to those launched by Simondon in the late 1950s would be prodded forth by those second-order cyberneticists (of the 1960s and 70s) who endorsed the endo-model. The endo-model provides for an understanding of models within systems, as opposed to exo-models, or models of systems (i.e., models that we, as observers, construct of systems). The endo-model is a stable and simulated internal control system used to model anticipatory control dynamics between mode, system, and environment. This endo-model also introduces environmentally coupled relations of entailment, force, and influence, wherefore the phenomena of circular causality, self-reference, and self-production dovetail via autopoiesis. Unlike Wiener’s interest in regulation by way of negative feedback, autopoietic systems—as delineated by Francisco Varela, Humberto Maturana, and Ricardo B. Uribe’s work in biology and supplemented by Milan Zeleny’s work on the economy of self-sustainable systems—deal with the structural knowledge that makes a system capable of producing/reproducing its unique self-organization. Unlike Wiener’s cybernetics, second-order cybernetics’ autopoiesis necessitates a physical boundary that separates the system from its surroundings; in Simondon, we see a similar treatment of technical objects and their associated milieu(s), i.e., a similar segregation of subjects from objects. As a theory of self-reference, autopoiesis takes those processes generated through a closed organization of production and designates that this same organization is regenerated through the interactions within a system’s own products (or components).\n\nErnst von Glasersfeld, Heinz von Foerster, and Maturana’s elaboration of machines as autopoietic or self-creating systems articulates Simondon’s concern in distinguishing how transindividual systems can be considered as distinct from a transmission model where knowledge is directly thought of and conceived as being a representation of an external objective or mind-independent reality (as in the case of Wiener). According to Glasersfeld and Foerster’s functional constructivism there is a systematic interface from which all of our knowledge about the world springs forth. We do not have access to the description of any environment but, instead, features—particular patterns with which we construct possible orderings of functional relationships (systems of categories, feature spaces, objects, states/state transitions, etc.). That is, we do not empirically recognize the given objects of our environment but are involved in constructing them by way of the informational regularities that becomes systemized, presented as they are at the interface of our cognitive system (Foerster & Glasersfeld, 1999). Glasersfeld’s emphasis on observationally-constructed consensual domains finds its compliment in Maturana’s notion of structural coupling(s) between system and environment, where consensual domains of interaction and communication are ecologically ordered between objects, events, and classes (Maturana & Varela, 1980). Moving from a reflective judgment and the moment of the ‘I’ to the ‘not-I’, communication is no longer about registration of information or its transmission, but instead a circular coupling between the ‘I’ and its environment in which each reentering (homeostasis) is indicated by the integration of reflection. For Simondon, this means a structural coupling between subject and totality and technical objects facilitate such transindividuation.\n\nAccording to the research program of second-order cybernetics, there is reflexive acknowledgement between structure and behavior regardless of whatever a priori predications observers adopt. For such second-order cyberneticians it is critical to recognize that although there are such a prioris, observers become reflectively aware of them vis-à-vis the epistemological, evolutionary, and ontogenetic development of systems (which observe and ‘converse’ with one another). This turn is often captured by the umbrella term ‘radical constructivism’, which endorses Jean Piaget’s conception of knowledge as an adaptive function, echoing Simondon’s appraisal of Wiener’s cybernetic program for being mistakenly founded upon the ‘intention of command and communication in the living being and the machine’ while eliding detailed considerations as to ‘how one can put individuals representing homeostatic functions at the heads of [organizational] groups’ (Simondon, 2017: 160). Glasersfeld, Foerster, Valera, and Maturana take up similar concerns regarding the conception of knowledge and its adaptive function, wherein ‘cognitive efforts have the purpose of helping us cope in the world of experience, rather than the traditional goal of furnishing an ‘objective’ representation of a world as it might ‘exist’ apart from us and our experience’ (von Glasersfeld, 1991).\n\nDespite the founding fathers of cybernetics such as Wiener, McCulloch, Bateson, and co. emphasized autonomy and self-organization vis-a-vis the self-organization and subjectivity of modelling, they were not all first-order reductionists. Ashby’s constructivist turn is particularly important for the move away from first-order cybernetics—where a system is understood as passive and objectively given, such that it can be manipulated—into second-order-cybernetics, where the system is seen as cast in engagement with the observer. According to Ashby’s ‘constructivism’, the observer, too, is a cybernetic dictum. In Ashby’s framework, there are two critical principles that have become most important for homeostatic self-regulating systems:\n\nHomeostasis of Internal Essential Variables (Principle of Ultrastability); according to this principle variables move beyond specific viability limits and, as a consequence, are part of adaptive processes that are triggered by and re-parameterize a system until it reaches a new equilibrium, at which point homeostasis is restored. In physiology, one can think of such essential variables as corresponding to blood pressure, heart rate, blood sugar levels. Such systems embody two levels of feedback at least: i) a first-order feedback that homeostatically regulates essential variables, like a thermostat, and a second-order feedback that allostatically reorganizes a system’s input–output relations when first-order feedback fails (Ashby, 1952).\n\nLaw of Requisite Variety; a successful control system must be capable of entering at least as many states as the system being controlled (i.e., ‘only variety in R can force down the variety due to D’ [Ashby, 1956: 206-207]).\n\nAshby’s constructivist description provides a bridge between first- and second-order cybernetics, providing a paradigm highly useful for those contemporary models of neuro-inferential ‘perceptual control theory’ and ‘inference to the best prediction’ in Bayesian modeling.[34] Ashby’s principles imply a minimum level of complexity for a successful controller (or, to recall Simondon, an agent of ‘individuation’), which is determined by the causal complexity of the environmental states that constitute potential perturbations to a system’s essential variables.\n\nFor Simondon, such ‘error-correction’ is inherent to how we deal with technical objects as inherently machinic, for they necessarily wield with equal force a rapport to man and systems–relational characteristics (i.e., analogical structure and analogical human dynamism). These two internal characteristics of the technical object simply cannot be understood if the technical object is conflated with the tool, ‘which then makes it lose its individuality and therefore its own value; as Piaget has remarkably shown based on archaeological and ethnographic considerations, the tool is deprived of its own individuality because it is grafted onto another individualized organism’s body part and because its function is to extend, reinforce, and protect but not replace the latter’ (Simondon, 2020b, 417).[35] For Simondon, machines are not externalizations (of mind, memory, or intentionality) but endowed with differential relational situatedness in their own right. Simondon’s critique of Wiener is at the level of cognition—including perception and action—and invention, with an interest in how systems maintain the homeostasis of essential variables and of internal organization.\n\nMore generally, Simondon’s critique of cybernetics is rooted in his refusal to consider the machine as an isolated system. His concern is not with autonomous machines but with machines that are steeped in an exchange between machine and environment. Thus, Simondon marks a difference between machines that operate with and by their own process and those machines that are able to modify their own process according to environmental variation. Such machines are distinguished from those like the thermo-siphon, which operates simply by means of registering or reacting to information (2017: 266). Simondon’s interest is in systems that engage in a proper convergence of economic constraints and technical requirements, where the machine maintains its stable functioning (for as long as possible) while exchanging with its environment (i.e., reciprocal modification [2017: 21]). This is directly in contrast to how Wiener articulates man without reference to interiority by way of environmental homeostasis; Wiener conceives of the social system as ‘an organization like the individual, that it is bound together by a system of communication’ (1961: 24), where man and machine solely exchange information with their environment without being mutually constituted by way of adequation and relative positions of observational instability. Elsewhere, Simondon critiques not only Wiener but ‘Shannon, Fischer [sic], Hartley’ (2020b: 686)[36] and the given relationship of information as solely a relationship between an emitter and receiver. This centers our debate over whether information should be considered an explanatory tool—an epistemic heuristic that does not refer back to a basic constituent of nature—or fundamentally ontological. Thus, this position ultimately concerns whether information can be said to be ultimately fundamental when understanding the ‘objective modal structure of the world’, which reiterates our earlier concerns about the relation between structure and matter.\n\nFor Simondon, in order for culture to incorporate technical objects one has to discover an adequate intermediary between the majority and minority status of technical objects. With the former, social relations are brought into relation while, in the case of the latter, the object is conceived as a matter of utility. As a consequence, the ‘secret and stable aspect of such a technics is thus not only a product of social conditions; it produces the structure of groups as much as this structure of the group conditions it. It is possible that every technics must to a certain extent contain a coefficient of intuition and instinct necessary for establishing the appropriate communication between man and the technical being’ (Simondon, 2017: 109). Such a divergence between man and technical being corresponds to the discrepancy between any (perceptible) model of empirical predicates and its categorization, making the claim that such a chasm is not arbitrary. Rather, this fracture answers to the meta-theoretic explanatory aims between objects’ components/differential parts and the mind-dependence of sensory episodes: Simondon’s conception of communication accounts for the inferential-causal dependency between the presence of physical objects with given qualities and a corresponding sensory experience, and the qualitative homogeneity attributed to such experiences. However, such a framework cannot explain the distinction between ostensible and veridical perception, and thus the possibility of error when making perceptual reports concerning the sensory determinations supervenes upon communication, which is always privy to error-corrections—Simondon’s understanding of homeostasis, a becoming-equilibrium. Unlike tools taken in isolation, with machines and organisms, Being is thus conceived of as the structure of all structures, the absolutely primordial and comprehensive structure (Puntel, 2010: 439).[37]\n\nFor Simondon, the individual (or Being) is the reality of any constituting relation rather than occupying the interiority of a constituted term; thus ‘the intrinsic, the interiority of the individual, would not exist without the ongoing relational operation that the ongoing individuation is’ (2020a: 50). Unlike the aforementioned dialectical treatment of being, when it comes to the level of the individual Simondon understands Being as the value of a relation—at the level of individuation, the individual can be considered ‘a being with an interiority relative to which an exteriority exists’ (50). Thus, it is not the externalized media (the liberation of organs, mind, or memory) that defines Being, but the system that this media, digital or analog, communicates across.\n\nConsider the model of crystallization which, for Simondon, allows him to establish a transcendental field for individuation without positing a substantialist division between different modes of existence, such as ‘physical beings’ (e.g., crystal) and living beings (e.g., plants). For Simondon, there is an ontological difference between physical and living beings: the former is characterized by carrying its associated milieu with it. For instance, the “crystal stops growing when you remove it from its aqueous solution” while the “plant may stop growing if you do not water it, but it carries a good deal of water with it, which allows for greater regulation of the relation between its inner and external milieus. It even puts water into circulation with its ecology’ (Lamarre, 2019: 109). For Simondon, the transcendental movement of evolution is not teleological or dialectical but deals with structure and reorganized material flows along transindividual relations, recombining aspects. Although some, such as Steven Shaviro, may draw the distinction that in Varela’s ‘autopoiesis, the emphasis is on a continuity that is created and preserved in and through continual change and interaction with the environment, whereas in individuation and [Whitehead’s] concrescence, the emphasis is on the production of novelty, the entity’s continual redefinition, or becoming’ (Shaviro, 2009: 112), for Simondon the relational system maintains itself through dynamic interaction with its environment, re-creating the very processes that produce it. An entity not only reconstitutes itself by actualizing potentials that preexist in a metastable environment but negotiates this reconstitution with recursive homeostasis. This recursive logic is inseparable from the unspooling of novel systems, moored by homeostatic thresholds.\n\nIt is critical for Simondon that the technical object does not facilitate its own operation(s) but that there is an integration between the machine and the technical. Simondon often makes mention of ‘the virtual’, remarking that ‘[t]he living thing has the faculty to modify itself according to the virtual: this faculty is the sense of time, which the machine does not have because it does not live’ (2017: 157). It is important to note that Simondon’s conception of the virtual is distinct from Deleuze’s. Simondon is not interested in the possibility of modification of process. He is, instead, describing a real process and this real process is happening in actuality, i.e., the process of individuation.[38] Consequently, Simondon is interested in the media object as a threshold of intensity and quality—here, Simondon is working against the hylomorphism of pure matter and pure form/the alliance of form and matter, which he deems a contradiction. For Simondon, the physical object is a bundle of differential relations, an organization of thresholds and of levels which are maintained and transposed via a myriad of variegated situations, with the object’s properties cohering and undergoing variations as it moves from one state to another (2020a: 264-265).\n\nTo better understand this, we ought to situate Simondon within the history of philosophy. Simondon’s philosophy begins with the Aristotelian classic model whereby we have matter, devoid of shape, quality, or determined characteristics, and we have forms, which are a priori and characterize matter by giving them form (while being unique in kind as distinct causes). Simondon is actively working against this Aristotelian model of hylomorphism: for Simondon, there are no a priori forms that are imposed onto undetermined matter—matter is determined itself through the process of individuation. Simondon begins from matter rather than a priori forms, as forms emerge through a difference of matter by which matter is crossed, like a (quantum/differential) field, by a difference of potential. This is also analogous with our empirical observations—Simondon thus remarks that, if ‘to perceive consists in increasing the information of the system formed by the subject and the field in which it is oriented, the conditions of perception are analogous to those of every stable structuration: a metastable state must precede perception’ (2020a: 269). This ‘objective field’, which Simondon often makes reference to and describes as a primitive, neutral, and ‘magical’ stage, is the arena upon which man and mediation find themselves fundamentally structured through circuits of exchange qua being—that is, ‘[m]an finds himself linked to a universe experienced as a milieu’ (2017: 177).[39]\n\nFor Simondon, the objectivation of mediation has a correlative in the subjectivation of mediation, which is object-ified and objectivized by way of the technical object that tethers man and the world to one another. The field of reference here, the ‘magical stage’, precedes mediation and, thus, precedes subjectivization or objectivization, neither fragmented nor universalized; it ‘is only the simplest and most fundamental of structurations of the milieu of a living being: the birth of a network of privileged points of exchange between the being and the milieu’ (2017: 177). This conception of a differential stage that precedes mediation and mediation’s becoming-subjectivized or -objectivized is similar to an electric field of electrostatic vectors and it is no coincidence that Simondon often recalls such electrical fields often in his writing.\n\nFor Simondon, these two ideas—matter and field/neutral foundation—are related to one another, as demonstrated in his references to the (preindividual) ‘found’ [fond] as the background upon which a form/figure may emerge:\n\n…there would not only be a genesis of technicity, but also a genesis on the basis of technicity, through the splitting of an original technicity into figure and ground [fond], the ground corresponding to the functions of totality that are independent of each application of technical gestures, whereas the figure, made of definite and particular schemas, specifies each technique as a manner of acting. The deepest reality [réalité de fond] of technics constitutes theoretical knowledge, whereas the particular schemas give us praxis’ (2017: 171).\n\nFor Simondon, matter is like a field with differences in potential; forms emerge in matter and they are not given a priori. Form is not a priori imposed onto undifferentiated matter but, instead, forms emerge through a process of individuation.[40] For Simondon, given the technical operation that gives rise to an object with form and matter, the dynamism of this operation simply cannot be represented by the matter-form pair. Unlike the Hegelian dialectic based on the notion of identity and non-identity—the opposition between ‘A’ and ‘not-A’, or ‘I’ and ‘not-I’—Simondon’s work on relations seeks to explain the genesis of phenomenal reality (including the objects populating it) and the subject by way of a dialectic founded on the idea of the differential, rather than contradiction.[41] Difference, as indeterminate and preindividual variation, is not the limit between two given identities but a non-phenomenal condition of all identity engendered as a determined object of and for thought. Simondon’s differential relations place media in relation with their spatio-temporal relations of realization; this kind of infinitesimal difference accounts for genesis outside of the self, understanding the real/actual non-phenomenal conditions of the process of individuation of thought. As with Deleuzean difference, Simondon’s difference can be linked to the model of calculus, which maps the processes by which thought determines itself on the basis of that which is non-phenomenal, on the basis of the differential (the dx). As a process of actualization or becoming-differenciated,[42] individualization is always and necessarily a genuine creation. Progressing by way of spatio–temporal dynamism, a differentiated idea is differenciated—it is a process of individuation in the sense of dramatizing the idea, producing an ‘indistinct differential relation in the Idea to incarnate itself in a distinct quality and a distinguished intensity’, with the result being that ‘the individual finds itself attached to a pre-individual half which is not the impersonal within it so much as the reservoir of its singularities’ (Deleuze, 1996: 245, 246). Railing against hylomorphism, Simondon remarks that ‘[t]he form and the matter of the hylomorphic schema are an abstract form and an abstract matter’ (2020a: 22), where:\n\n[t]he hylomorphic schema is thus a couple in which the two terms are clear and the relation obscure […. And] represents the transposition into philosophical thought of the technical operation reduced to work, and taken as a universal paradigm of the genesis of beings. It is indeed a technical experience, but a very incomplete technical experience that is at the basis of this paradigm. The generalized use of the hylomorphic schema in philosophy introduces an obscurity that comes from the insufficiency of this schemas technical basis’ (2017: 248).\n\nFor Simondon, as in the example of a brick, definite Being does not result from the combination of unspecified matter with unspecified form (where form is imposed upon the brick). Simondon criticizes this Aristotelian hylomorphic model that presupposes a priori forms—for Simondon, there are only forms insofar as they are co-determined by a process. Thus we do not need to turn to Whitehead or Deleuze for processual construction of subjectivism that emerges bottom-up.[43]\n\nSimondon’s considerations of practical reason recall C.S. Peirce, where we have knowledge which depends on actions/decisions that are taken. Similarly, knowledge for Simondon is a know-how—to learn how to cope with environment, to make accurate predictions related to this environment, and so on. Knowledge as such provides us with a means of selecting important information in order to react in appropriate ways and to achieve a specific goal. For Simondon, the process of individuation is not so different from this idea, such that decisions taken or the knowledge of an environment that one has is not absolutely true but, instead, true relative to an appropriate solution to a specific problem. Accordingly, Simondon provides us with an intersubjective conception of environment; the creation of knowledge necessitates the most efficient way of actuating a goal.[44]\n\nWe also ought to note that all communication for Simondon is prelinguistic. Communication as such is exacted by way of individuation; during this process of individuation, there is a sort of negotiation by way of action. As it involves the actant, action always forms a rapport with a discrete part/element such that the other part/element has to act in response in order to preserve its own equilibrium. Little by little, both elements ‘learn’ how to interact; signification is the selection of or the capacity of recognizing specific actions or affects coming from the environment or milieu. Thus, Simondon remarks that:\n\n[t]he emergence of the distinction between figure and ground is indeed the result of a state of tension, of the incompatibility of the system with itself from what one could call the oversaturation of the system; but structuration is not the discovery of the lowest level of equilibrium: stable equilibrium, in which all potential would be actualized, would correspond to the death of any possibility of further transformation; whereas living systems, those which precisely manifest (2017: 177).\n\nSimondon’s description of communication provides us with a rich philosophy of becoming. Consider how this structuration would apply to, say, bacterium; bacterium are not ‘intelligent’ (i.e., sapient) and do not have a language but, nonetheless, are able to move within and through their environment in order to find food by discerning pertinent signs (e.g., difference in sugar in water). Thus, the bacterium selects different intensity from a signal or environ that is significant for it, matching exteroceptive and interoceptive inputs; the bacterium does not visually ‘see’ anything but, by way of differential patterning, becomes sensible to difference(s) in degrees in relation to the substance it is interested in, which allows for it to move effectively and knowledgably—to move with reason. This top-down analytical approach allots a kind of prediction-error. This is what Simondon means when he designates signification before language (i.e., it is ‘prelinguistic’), where intensity is precisely that which allows for reaction. In their being non-representational, deep learning neural networks are, similarly, prelinguistic. Consider how deep learning systems like AlphaGo Zero use previous data as a signal-processing heuristic to predict future variances.[45] Simondon’s notion of transindividuation, as applied to the bacterium, is not terribly different from how deep learning algorithms like AlphaGo Zero pluck certain pertinent data to ‘train’ themselves by way of patterned-reinforcement learning.[46] Just as sugar serves as the basic observational empirical data for the bacterium, for predictive patterning there exist protocol statements/propositions. With machine learning, the probability of any hypothesis is not determined a priori but determined by way of conditions and implementations. Similarly, for Simondon transduction is never exterior to the terms that it brings forth, patterning through the inherent tensions in a domain (2020a: xxiii).[47]\n\nOften, the roots for this kind of paradigm are linked to Carnap’s confirmation theory, which provides an outline of the acquisition of knowledge from experience. Specifically, in The Logical Structure of the World ([1928] 2003), Carnap defines an explicit computational procedure for extracting knowledge from elementary experiences. For Carnap, Simondon, and the kind of inferential machine learning logic that we are interested in—i.e., a Bayesian statistical logic that cannot be explained away with the extension of mind, intention, or memory, but navigates a differential space—objects deal with and restrict themselves to structural properties.[48] That is, objects are objects of form, where objectivity is structured by way of intersubjectivity, the dimensions of form structuring and holding together individual strands of content. As a consequence, the sole reason objects of knowledge are possible is because despite ‘the material of the individual streams of experience is complete diverging … certain structural features agree of all streams of experience … objects of knowledge are not content, but form, and they can be represented as structural entities’ (Carnap, [1928] 2003: §66). This hierarchy allots deep and varied empirical Bayes in a prediction error landscape where prior probabilities are ‘empirical’ in that they are learned and pulled down from higher levels, such that they do not have to be extracted de novo from the any current input. In terms of a bidirectional multilevel hierarchy, the upper layers of the network form ‘top-down’ predictions upon the basis of which data coming from levels below is assessed, where past certain thresholds of tolerance error signals ‘trickle up’ in a feedforward manner, triggering the system to revise its predictive models. This reliance on higher levels means that processing at one level depends on processing at higher levels. Such priors are bifurcated into hyperparameters (re: expectations of means) and hyperpriors (re: expectations of precisions).[49]\n\nThe pluralism of an open-ended form of practical reason is fundamental in this model of technical- /machine-communication insofar as it is reason-guided; a framework is not chosen by an individual but revealed preferences, a social process of some kind moving through a regulative ideal (Carus, 2017: 178). For Simondon, individuation has its own signal and signification, which are rendered functional to the being’s relation with its environment. How, then, do we go from here to (object-)language? First and foremost, we have to consider the operative importance for this process of selection by way of intensity or degrees, which allows for the distinction of further definition(s) by way of signification. Machine language depends on specific individuation. This same process is the case with logic or any formal mode of thought, but this occurs in opposite fashion. Concepts arise by way of a specific interaction with formal language and any specific process of individuation gives origin to concepts, which are useful for coping with a specific environment:\n\n‘[t]echnical ensembles are characterized by the fact that in them a relation between technical objects takes shape at the level of the margin of indeterminacy of each technical object’s way of functioning. This relation between technical objects is of a problematic type, insofar as it puts indeterminacies into correlation, and for this reason it cannot be taken on by the objects themselves; it cannot be calculated, nor be the result of a calculation; it must be thought, posed as a problem by a living being and for a living being’ (Carus, 2017: 157).\n\nSimondon’s idea of indetermination corresponds to our previous discussion of potential and prediction-error. For Simondon, any individuated being has a functional relationship with its environment whereby its relation(s) can change. This indeterminacy means that change/becoming in technical objects cannot be tied down to a doctrine of representation/the image; for Simondon, the concept of information is important for any ‘calculating machine’ because calculations cannot observationally ‘feel through’ the environment (much like the aforementioned bacterium), at least not by way of the same kind of sensorial feeling(s) that humans and most animals possess (using qualia/epiphenomena/sensa). Nonetheless, machines are sensible to information and this sensitivity is what Simondon is interested in via indetermination; for Simondon, communication, data, and information are all entangled in the world of actuality. Recall that, for Simondon, information is a general term for the intensities which cause reaction and affection; in computing machines, this is a process of information coming into a coupling with respect to the machine, causing it to act or react in some way. Similarly, Carnap’s conceptual engineering of constructivism is anti-representationalist, as the fruitfulness of a concept motivates its adoption into certain contexts where said application had not previously been used. While we will not be following Carnap into the woods of symbolic logic, the bridge between Simondon and Carnap has been constructed simply to assert that, in addition to evincing a picture of second-order cybernetics—invoking principles of autopoiesis by challenging Wiener’s conception of information and homeostasis—Simondon’s conception of individuation by way of transduction (like Carnap’s problematization of inductive logic) deals with real relations between individuals. Unlike Stiegler, who does not work with any measure of measurable loss in his cynicism towards digital networks (for Stiegler, ‘neganthropy’ corresponds to an idealist noesis), Simondon’s concretization—the engine of individuation—asserts that we are interested in specific results, which can only be understood by way of a system’s changes (and, invariably, its equilibrium).\n\nFrom Second-Order Systems to Deep Learning\n\nLet us now turn to the final section of our study—deep learning through the Simondonian schema of technics, individuation, and concretization. A particularly pertinent approach to understand the higher-order representational abilities of sentient and sapient cognitive behavior is the predictive processing paradigm (hereafter: PP), which explains how cognitive systems generate ‘representations’ of various phenomena on the basis of incoming information by way of a procedure called ‘prediction error minimization’ (Hohwy, 2013). Parsing deep learning by way of Anil K. Seth’s Bayesian modelling of the cybernetic brain and his work on PP, we shall endeavor through some of Simondon’s work on ecologies of influence, with a particular interest in objects (esp. networked digital media objects vide computation), infrastructural operations, and formal planes of reference.\n\nAccording to certain cultural commentators, the epochal cultural development of ‘deep learning’ invokes a shift from linear Cartesian thinking towards intentional obfuscatory ‘black box’ de-spatialisation. Frank Pasquale remarks that, rather than intelligibility, fields such as ‘actuarial finance’—increasingly privy to high-speed trading via AI —cultivate a sense of ‘secrecy’, establishing a barrier between ‘hidden content’ and penetrability, where the former is associated with the machine’s and machine learning’s top-down mereological parsing of Big Data sets and unauthorized access to its endogenous encoding on behalf of the ‘user’ (Pasquale, 2016: 16). Similarly, Cathy O’Neil likens data-based mathematical complex computation to ‘Gods’—impenetrable and ‘opaque, their workings invisible to all but the highest priests in their domain: mathematicians and computer scientists’ (O’Neil, 2017: 3).[50]\n\nBroadly speaking, Big Data and its computational methods have bolstered the long-term construction of our datafied algorithmic reality; the future implications are drastic, with implications including predictive policing, hiring practices, credit assessment, and criminal justice. Thanks to the widespread cast of today’s digital networks and the resulting availability of metadata and data we see why the neural metaphor that was once popular with early cybernetics researchers may make sense to invoke once more.[51] Like the connective conditioning of synaptic connections between neurons that have previously fired together,[52] Big Data allows for activity-dependent plasticity where causal lines are quickly cemented and, subsequently, capitalized upon by marketing campaigns. Jointed research in reinforcement learning has also resulted in a milestone for AI research. Using the logics of Hebbian associative learning and Monte Carlo ‘tree-search’ algorithms,[53] ‘deep learning’ and its Generative Adversarial Networks are predicated upon a scaffold that probabilistically retrofits output; that is, given samples are drawn from two competing models fighting against each other, with a generative model capturing data distribution and a (competing) discriminative model distinguishing training samples while, simultaneously, generating fake data samples. Given large sets of sampling data, training data has likewise burgeoned, resulting in the strengthened self-reinforcing process that Bernhard Rieder calls ‘computerization’,[54] thrusting deep learning research into celestial zenith.\n\nThe ‘Monte Carlo’ framework is deserving of closer examination. Christened ‘Monte Carlo’ after the gambling mecca, this analytical calculating method (developed during and shortly after WWII) utilizes simulative recall-and-precision while implementing random numbers (a la roulette) so as to simulate stochastic processes that had, until that point, been too complex to calculate. As an early example of analytical virtualization (and, arguably, virtual reality) the Monte Carlo method ‘came to constitute an alternative reality—in some cases a preferred one—on which ‘experimentation’ could be conducted’ (Galison, 1996: 119) By modeling a sequence of random scattering on a computer, the Monte Carlo sampling method ushered in a new epistemological plane for extracting information from physical measurements and equations. Its scheme of representation presupposes an ecology composed of discrete entities that interact through irreducibly stochastic processes. The Monte Carlo method, borrowing from both experimental and theoretical domains, negotiates the traditional categories of experimentation and theory; Peter Galison identifies this shift in the calculating machine as a development from ‘computer-as-tool to computer-as-nature’ (1996: 121).\n\nSuch technological changes, ushering in epistemic evolutions, necessitate paradigm shifts in how we understand technicity; this is a specific feature inherent to Simondon’s concept of concretization, the engine of conceptual technicity which attunes network objects to the sensitivity of local circumstances. Previous to Monte Carlo and neural networks was von Neumann architecture—von Neumann’s work on infinite random generators allowed for the sequential repetition of a finite cycle of numbers ad infinitum within a finite machine. Instead of treating cognition (esp. memory and intention) as a process that is extended by and through technical systems, convoluted neural nets constitutively ‘remodel’ themselves iteratively and reactively, proffering unique outputs which human players have, so far, been unable to predict. AlphaGo Zero, the Google DeepMind neural network that recently defeated the world’s highest-ranking Go player, Ke Jie, and 18-time world Go champion Lee Sedol, is perhaps the most celebrated example. AlphaGo Zero’s novelty is a byproduct of using the aheuristic ‘tree search’ pattern recognition,[55] simulation, and backpropagation to probabilistically account for simulative scenarios: by iteratively building partial search-inputs with which to update its ‘weights’—or the default values of selection—such neural nets are able to start at a root node and recursively create non-terminal values that are revised according to ‘backpropagation’, or how simulated error scenarios unfold as a reactive gradient of layering. Multilayer ‘feedforward’ neural networks such as AlphaGo are not only based on binary classification or principles of registration but on an ecology of data which is elliptically remodeled according to a hierarchical scaffold, constructing an evolutionary closed-loop evaluation mechanism that optimizes according to the current action in question.\n\nAdvances in Bayesian networks and statistics are inseparable from these developments. The Bayesian approach calculates probabilities for various hypotheses in terms of degrees of certainty and ‘is ideally suited for situations where one might want to begin calculating probabilities for different hypotheses (e.g., concerning class membership) even if very few observations are available, continuously updating and refining prior probabilities as more evidence comes in’ (Rieder, 2020: 245). Bayesian networks are acyclical, directed graphs, illuminating model dependencies with nodes connected through the latticework of probabilities.[56] In the case of machine learning, Bayesian classification allows for the capacity of algorithmic techniques that move between application domains where there are ‘habitats’—data sets and classification characteristics—where one set of techniques burgeons and another diminishes. With deep neural networks, this involves modeling complex non-linear relationships between input-variables and output-classes distributing calculations over parallel hardware. This technique is i) perceptual, and ii) apperceptive: at once a model based on image-detection, the empirical frame of registration and recognition found in first-order cybernetics; but also dealing with internal-classification based on advanced graphics processing where the most salient features are weighted according to df-idf frequency.[57]\n\nThis sets the stage for an epochal dilemma re: contemporary computation’s internal logic. Were we to take syntax-based classical deductive computing, such as the Universal Turing Machine, as an ideal model (plucking neural nets from the mid-to-late twentieth century), we could characterize the parsing of information as a linear spatial procedure—one composed of sifting through data by moving forward and backward as information is divided into procedural units and consequential steps. However, ‘deep learning’ algorithms, as recently exemplified by advancements in reinforcement-learning AI (such as AlphaGo Zero), seem to ‘experience’ data opportunistically. Such deep learning software are able to decisively re-integrate evaluative metrics that deviate from a sample-proportion.\n\nSybille Krämer and Horst Bredekamp, describing contemporary advances in computation at the level of cultural techniques, remark upon the long-term effects of computerization, noting that:\n\n…cultural techniques are promoting the achievements of intelligence through the senses and the externalizing operationalization of thought processes. Cognition does not remain locked up in any invisible interiority; on the contrary, intelligence and spirit advance to become a kind of distributive, and hence collective, phenomenon that is determined by the hands-on contact humans have with things and symbolic and technical artefacts’ (2013: 26-27).\n\nHere, Krämer and Bredekamp describe calculus as a ‘mechanism of forgetting’ (26), exacerbated by how computation implements operative signs and mathematical competence sans-reflection. The duo thus liken computation qua calculus to Stiegler’s and Derrida’s reification of the hypomnēmata; the two see the register of computation as externalizing cognition from the invisible interiority of the minds of individuals by way of distribution, shooting forth these grand prostheses which not only make perceptible that which was cognitively invisible but operationalize dissociation into previously unforeseen heights. Stiegler, Krämer, and Bredekamp recycle a pessimistic tale that ensnares the planetary apparatus of computational becoming where the sole means of recovery amounts to re-interiorizing that which has been computationally exteriorized.[58] Recall that the mission of Hegel’s phenomenological propaedeutic investigation into the structure of thinking, raising it to the ideal of a ‘presuppositionless science’, was to determine that even appeals to sensory immediacy at the ground of experience are always already mediated by the concept. But this concept finds a sea change in such thinkers of digitality, for whom a new concept of networked misfortune has taken anchor.\n\nHas the automation of intelligence left the biological human organism behind, with the human organism reduced to the reproductive organ of the machine phylum, thus fulfilling the role of a pollinator for machine reproduction? This fear far predates computerization. This is, in fact, what Samuel Butler, in his books Darwin among the Machines (1863) and Erewhon: or, Over the Range (1872), prognosticated with the intensive spatium of impersonally structured thought.[59]André Leroi-Gourhan carries such fears of the organized inorganic into the terrain of self-obsolescence in Gesture and Speech, foreseeing the biotic human as a fossil in the technosphere, with the human motor brain exteriorized in the last instance and, accordingly, captured by autonomously locomoting robots, total mechanicity commemorating automation’s final hour.[60] Today, such fears of transhumanism run rampant haunt the popular imagination, the incubus of a fully automated future aggravated by conspiracies of synthetic pathogens and weaponized pandemics. Compounded with Pasquale’s ‘black-boxing’, these fears paint a horror-stricken scenography of humans not only tethered to our machinic overlords but utterly unable to comprehend their masters’ whims—outflanked and outpaced, a picture of man succumbing to ‘parasitism-as-atrophy-of-individuation’ (Moynihan, 2020: 85). Nonetheless, mustering philo-sophical concepts alongside historical-archeological technical pursuits shows that such is not the case (at least not yet)—there is still a great deal of contingent genealogical residue between mankind and even the most advanced deep learning algorithmic logic.\n\nFunctioning like brains, deep learning algorithms engage in predictive inference, working through the causes of sensory- or data-inputs by minimizing prediction-error(s). Anil K. Seth has been one of the foremost thinkers in philosophy of mind and researchers of PP to show how the predictive perception of sensorimotor contingencies originates not in the Helmholtzian principle of perception-as-interference but in 20th century cybernetic principles that emphasize homeostasis and predictive control (i.e., second order cybernetics). Seth, drawing on Ashby’s work on homeostasis, builds on the Law of Requisite Variety by arguing that the nature of a controller capable of suppressing perturbations imposed by an external system must instantiate a (virtual) model of that system. Interestingly, the paradigm of PP not only allots us with a description of algorithmic intelligence’s patterning-based derivability but, in turn, how neuro-inferential mentality occurs by encoding such patterning. This Bayesian model of mind anchors rationality as a formal-computational reformulation of inductive reasoning, whereby cognition is a process of directing order out of"
    }
}
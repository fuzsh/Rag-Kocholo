{
    "id": "dbpedia_3038_2",
    "rank": 21,
    "data": {
        "url": "https://www.oreilly.com/library/view/programming-pig-2nd/9781491937082/ch01.html",
        "read_more_link": "",
        "language": "en",
        "title": "Programming Pig, 2nd Edition [Book]",
        "top_image": "https://www.oreilly.com/library/cover/9781491937082/1200w630h/",
        "meta_img": "https://www.oreilly.com/library/cover/9781491937082/1200w630h/",
        "images": [
            "https://cdn.oreillystatic.com/images/sitewide-headers/oreilly_logo_mark_red.svg",
            "https://www.oreilly.com/api/v2/epubs/9781491937082/files/assets/pig2_0101.png",
            "https://cdn.oreillystatic.com/oreilly/images/app-store-logo.png",
            "https://cdn.oreillystatic.com/oreilly/images/google-play-logo.png",
            "https://cdn.oreillystatic.com/oreilly/images/roku-tv-logo.png",
            "https://cdn.oreillystatic.com/oreilly/images/amazon-appstore-logo.png",
            "https://cdn.oreillystatic.com/images/sitewide-headers/oreilly_logo_mark_red.svg",
            "https://cdn.oreillystatic.com/oreilly/images/report-software-architecture-patterns-553x420.jpg",
            "https://cdn.oreillystatic.com/oreilly/images/laptop-flat-topics-ml-1124x638.png"
        ],
        "movies": [],
        "keywords": [],
        "meta_keywords": [
            ""
        ],
        "tags": null,
        "authors": [
            "Alan Gates",
            "Daniel Dai"
        ],
        "publish_date": null,
        "summary": "",
        "meta_description": "Chapter 1. What Is Pig? Pig provides an engine for executing data flows in parallel on Apache Hadoop. It includes a language, Pig Latin, for expressing these data flows. …  - Selection from Programming Pig, 2nd Edition [Book]",
        "meta_lang": "en",
        "meta_favicon": "//www.oreilly.com/favicon.ico",
        "meta_site_name": "O’Reilly Online Learning",
        "canonical_link": "https://www.oreilly.com/library/view/programming-pig-2nd/9781491937082/ch01.html",
        "text": "Comparing Query and Data Flow Languages\n\nAfter a cursory look, people often say that Pig Latin is a procedural version of SQL. Although there are certainly similarities, there are more differences. SQL is a query language. Its focus is to allow users to form queries. It lets users describe what question they want answered, but not how they want it answered. In Pig Latin, on the other hand, the user describes exactly how to process the input data.\n\nAnother major difference is that SQL is oriented around answering one question. When users want to do several data operations together, they must either write separate queries, storing the intermediate data into temporary tables, or use subqueries inside the query to do the earlier steps of the processing. However, many SQL users find subqueries confusing and difficult to form properly. Also, using subqueries creates an inside-out design where the first step in the data pipeline is the innermost query.\n\nPig, however, is designed with a long series of data operations in mind, so there is no need to write the data pipeline in an inverted set of subqueries or to worry about storing data in temporary tables. This is illustrated in Examples 1-1 and 1-2.\n\nConsider a case where a user wants to group one table on a key and then join it with a second table. Because joins happen before grouping in a SQL query, this must be expressed either as a subquery or as two queries with the results stored in a temporary table. Example 1-1 will use a temporary table, as that is more readable.\n\nExample 1-1. Group then join in SQL\n\nCREATE TEMP TABLE t1 AS SELECT customer, sum(purchase) AS total_purchases FROM transactions GROUP BY customer; SELECT customer, total_purchases, zipcode FROM t1, customer_profile WHERE t1.customer = customer_profile.customer;\n\nIn Pig Latin, on the other hand, this looks like Example 1-2.1\n\nExample 1-2. Group then join in Pig Latin\n\n-- Load the transactions file, group it by customer, and sum their total purchases txns = load 'transactions' as (customer, purchase); grouped = group txns by customer; total = foreach grouped generate group, SUM(txns.purchase) as tp; -- Load the customer_profile file profile = load 'customer_profile' as (customer, zipcode); -- Join the grouped and summed transactions and customer_profile data answer = join total by group, profile by customer; -- Write the results to the screen dump answer;\n\nFurthermore, SQL and Pig were designed to live in different environments. SQL is designed for the RDBMS environment, where data is normalized and schemas and proper constraints are enforced (that is, there are no nulls in places they do not belong, etc.). Pig is designed for the Hadoop data-processing environment, where schemas are sometimes unknown or inconsistent. Data may not be properly constrained, and it is rarely normalized. As a result of these differences, Pig does not require data to be loaded into tables first. It can operate on data as soon as it is copied into HDFS.\n\nAn analogy with human languages and cultures might help. My wife and I (Alan) have been to France together a couple of times. I speak very little French. But because English is the language of commerce (and probably because Americans and the British like to vacation in France), there is enough English spoken in France for me to get by. My wife, on the other hand, speaks French. She has friends there to visit. She can talk to people we meet. She can explore the parts of France that are not on the common tourist itinerary. Her experience of France is much deeper than mine because she can speak the native language.\n\nSQL is the English of data processing. It has the nice feature that everyone and every tool knows it, which means the barrier to adoption is very low. Our goal is to make Pig Latin the native language of parallel data-processing systems such as Hadoop. It may take some learning, but it will allow users to utilize the power of Hadoop much more fully.\n\nMapReduce’s “Hello World”\n\nConsider a simple MapReduce application that counts the number of times each word appears in a given text. This is the hello world program of MapReduce. In this example the map phase will read each line in the text, one at a time. It will then split out each word into a separate string, and, for each word, it will output the word and a 1 to indicate it has seen the word one time. The shuffle phase will use the word as the key, hashing the records to reducers. The reduce phase will then sum up the number of times each word was seen and write that together with the word as output. Let’s consider the case of the nursery rhyme “Mary Had a Little Lamb.” Our input will be:\n\nMary had a little lamb its fleece was white as snow and everywhere that Mary went the lamb was sure to go\n\nLet’s assume that each line is sent to a different map task. In reality, each map is assigned much more data than this, but this simple example will be easier to follow. The data flow through MapReduce is shown in Figure 1-1.\n\nOnce the map phase is complete, the shuffle phase will collect all records with the same word onto the same reducer. For this example we assume that there are two reducers: all words that start with A–L are sent to the first reducer, and words starting with M–Z are sent to the second reducer. The reducers will then output the summed counts for each word.\n\nWhen Pig is running MapReduce as the execution engine, it compiles the Pig Latin scripts that users write into a series of one or more MapReduce jobs that it then executes. See Example 1-3 for a Pig Latin script that will do a word count of “Mary Had a Little Lamb.”\n\nExample 1-3. Pig counts Mary and her lamb\n\n-- Load input from the file named Mary, and call the single -- field in the record 'line'. input = load 'mary' as (line); -- TOKENIZE splits the line into a field for each word. -- flatten will take the collection of records returned by -- TOKENIZE and produce a separate record for each one, calling the single -- field in the record word. words = foreach input generate flatten(TOKENIZE(line)) as word; -- Now group them together by each word. grpd = group words by word; -- Count them. cntd = foreach grpd generate group, COUNT(words); -- Print out the results. dump cntd;\n\nThere is no need to be concerned with map, shuffle, and reduce phases when using Pig. It will manage decomposing the operators in your script into the appropriate MapReduce phases.\n\nHow Pig Differs from MapReduce\n\nEarlier, we made the claim that a goal of the Pig team is to make Pig Latin the native language of parallel data-processing environments such as Hadoop. But does MapReduce really not provide enough? Why is Pig necessary?\n\nPig offers users several advantages over using MapReduce directly. Pig Latin provides all of the standard data-processing operations, such as join, filter, group by, order by, union, etc. MapReduce provides the group by operation directly (in the shuffle and reduce phases), and it provides the order by operation indirectly through the way it implements the grouping. Filtering and projection can be implemented trivially in the map phase. But other operators—particularly join—are not provided and must instead be written by the user.\n\nPig furnishes some complex, nontrivial implementations of these standard data operations. For example, because the number of records per key in a dataset is rarely evenly distributed, the data sent to the reducers is often skewed. That is, one reducer may get 10 or more times the data as other reducers. Pig has join and order by operators that will handle this case and (in some cases) rebalance the reducers. But these took the Pig team months to write, and rewriting them in MapReduce would be time consuming.\n\nIn MapReduce, the data processing inside the map and reduce phases is opaque to the system. This means that MapReduce has no opportunity to optimize or check the user’s code. Pig, on the other hand, can analyze a Pig Latin script and understand the data flow that the user is describing. That means it can do early error checking (did the user try to add a string field to an integer field?) and optimizations (can these two grouping operations be combined?).\n\nMapReduce does not have a type system. This is intentional, and it gives users the flexibility to use their own data types and serialization frameworks. But the downside is that this further limits the system’s ability to check users’ code for errors both before and during runtime.\n\nAll of these points mean that Pig Latin is much lower-cost to write and maintain than Java code for MapReduce. Consider the following (very unscientific) experiment, where we wrote the same operation in Pig Latin and MapReduce. Given one file with user data and one with click data for a website, the Pig Latin script in Example 1-4 will find the five pages most visited by users between the ages of 18 and 25.\n\nExample 1-4. Finding the top five URLs\n\nUsers = load 'users' as (name, age); Fltrd = filter Users by age >= 18 and age <= 25; Pages = load 'pages' as (user, url); Jnd = join Fltrd by name, Pages by user; Grpd = group Jnd by url; Smmd = foreach Grpd generate group, COUNT(Jnd) as clicks; Srtd = order Smmd by clicks desc; Top5 = limit Srtd 5; store Top5 into 'top5sites';\n\nThe first line of this program loads the file users and declares that this data has two fields: name and age. It assigns the name of Users to the input. The second line applies a filter to Users that passes through records with an age between 18 and 25, inclusive. All other records are discarded. Now the data has only records of users in the age range we are interested in. The results of this filter are named Fltrd.\n\nThe second load statement loads pages and names it Pages. It declares its schema to have two fields, user and url.\n\nThe line Jnd = join joins together Fltrd and Pages using Fltrd.name and Pages.user as the key. After this join we have found all the URLs each user has visited.\n\nThe line Grpd = group collects records together by URL, so for each value of url, such as pignews.com/frontpage, there will be one record with a collection of all records that have that value in the url field. The next line then counts how many records are collected together for each URL. So after this line we now know, for each URL, how many times it was visited by users aged 18–25.\n\nThe next thing to do is to sort this from most visits to least. The line Srtd = order sorts on the count value from the previous line and places it in desc (descending) order. Thus, the largest value will be first. Finally, we need only the top five pages, so the last line limits the sorted results to only five records. The results of this are then stored back to HDFS in the file top5sites.\n\nIn Pig Latin this comes to nine lines of code and took about 15 minutes to write and debug. The same code in MapReduce (omitted here for brevity) came out to about 170 lines of code and took four hours to get working. The Pig Latin will similarly be easier to maintain, as future developers can easily understand and modify this code.\n\nThere is, of course, a cost to all this. It is possible to develop algorithms in MapReduce that cannot be done easily in Pig. And the developer gives up a level of control. A good engineer can always, given enough time, write code that will outperform a generic system. So, for less common algorithms or extremely performance-sensitive ones, MapReduce is still the right choice. Basically, this is the same situation as choosing to code in Java versus a scripting language such as Python. Java has more power, but due to its lower-level nature, it requires more development time than scripting languages. Developers will need to choose the right tool for each job."
    }
}
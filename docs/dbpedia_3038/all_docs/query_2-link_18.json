{
    "id": "dbpedia_3038_2",
    "rank": 18,
    "data": {
        "url": "https://pig.apache.org/docs/r0.15.0/start.html",
        "read_more_link": "",
        "language": "en",
        "title": "Getting Started",
        "top_image": "",
        "meta_img": "",
        "images": [
            "https://pig.apache.org/docs/r0.15.0/images/hadoop-logo.jpg",
            "https://pig.apache.org/docs/r0.15.0/images/pig-logo.gif",
            "https://pig.apache.org/docs/r0.15.0/skin/images/rc-b-l-15-1body-2menu-3menu.png",
            "https://pig.apache.org/docs/r0.15.0/skin/images/pdfdoc.gif"
        ],
        "movies": [],
        "keywords": [],
        "meta_keywords": [
            ""
        ],
        "tags": null,
        "authors": [],
        "publish_date": null,
        "summary": "",
        "meta_description": "",
        "meta_lang": "",
        "meta_favicon": "",
        "meta_site_name": "",
        "canonical_link": null,
        "text": "Pig Setup\n\nRequirements\n\nMandatory\n\nUnix and Windows users need the following:\n\nHadoop 0.23.X, 1.X or 2.X - http://hadoop.apache.org/common/releases.html (You can run Pig with different versions of Hadoop by setting HADOOP_HOME to point to the directory where you have installed Hadoop. If you do not set HADOOP_HOME, by default Pig will run with the embedded version, currently Hadoop 1.0.4.)\n\nJava 1.7 - http://java.sun.com/javase/downloads/index.jsp (set JAVA_HOME to the root of your Java installation)\n\nOptional\n\nPython 2.7 - https://www.python.org (when using Streaming Python UDFs)\n\nAnt 1.8 - http://ant.apache.org/ (for builds)\n\nDownload Pig\n\nTo get a Pig distribution, do the following:\n\nDownload a recent stable release from one of the Apache Download Mirrors (see Pig Releases).\n\nUnpack the downloaded Pig distribution, and then note the following:\n\nThe Pig script file, pig, is located in the bin directory (/pig-n.n.n/bin/pig). The Pig environment variables are described in the Pig script file.\n\nThe Pig properties file, pig.properties, is located in the conf directory (/pig-n.n.n/conf/pig.properties). You can specify an alternate location using the PIG_CONF_DIR environment variable.\n\nAdd /pig-n.n.n/bin to your path. Use export (bash,sh,ksh) or setenv (tcsh,csh). For example:\n\n$ export PATH=/<my-path-to-pig>/pig-n.n.n/bin:$PATH\n\nTest the Pig installation with this simple command: $ pig -help\n\nBuild Pig\n\nTo build pig, do the following:\n\nCheck out the Pig code from SVN: svn co http://svn.apache.org/repos/asf/pig/trunk\n\nBuild the code from the top directory: ant\n\nIf the build is successful, you should see the pig.jar file created in that directory.\n\nValidate the pig.jar by running a unit test: ant test\n\nIf you are using Hadoop 0.23.X or 2.X, please add -Dhadoopversion=23 in your ant command line in the previous steps\n\nRunning Pig\n\nYou can run Pig (execute Pig Latin statements and Pig commands) using various modes.\n\nLocal Mode Tez Local Mode Mapreduce Mode Tez Mode Interactive Mode yes experimental yes yes Batch Mode yes experimental yes yes\n\nExecution Modes\n\nPig has two execution modes or exectypes:\n\nLocal Mode - To run Pig in local mode, you need access to a single machine; all files are installed and run using your local host and file system. Specify local mode using the -x flag (pig -x local).\n\nTez Local Mode - To run Pig in tez local mode. It is similar to local mode, except internally Pig will invoke tez runtime engine. Specify Tez local mode using the -x flag (pig -x tez_local).\n\nNote: Tez local mode is experimental. There are some queries which just error out on bigger data in local mode.\n\nMapreduce Mode - To run Pig in mapreduce mode, you need access to a Hadoop cluster and HDFS installation. Mapreduce mode is the default mode; you can, but don't need to, specify it using the -x flag (pig OR pig -x mapreduce).\n\nTez Mode - To run Pig in Tez mode, you need access to a Hadoop cluster and HDFS installation. Specify Tez mode using the -x flag (-x tez).\n\nYou can run Pig in either mode using the \"pig\" command (the bin/pig Perl script) or the \"java\" command (java -cp pig.jar ...).\n\nExamples\n\nThis example shows how to run Pig in local and mapreduce mode using the pig command.\n\n/* local mode */ $ pig -x local ... /* Tez local mode */ $ pig -x tez_local ... /* mapreduce mode */ $ pig ... or $ pig -x mapreduce ... /* Tez mode */ $ pig -x tez ...\n\nInteractive Mode\n\nYou can run Pig in interactive mode using the Grunt shell. Invoke the Grunt shell using the \"pig\" command (as shown below) and then enter your Pig Latin statements and Pig commands interactively at the command line.\n\nExample\n\nThese Pig Latin statements extract all user IDs from the /etc/passwd file. First, copy the /etc/passwd file to your local working directory. Next, invoke the Grunt shell by typing the \"pig\" command (in local or hadoop mode). Then, enter the Pig Latin statements interactively at the grunt prompt (be sure to include the semicolon after each statement). The DUMP operator will display the results to your terminal screen.\n\ngrunt> A = load 'passwd' using PigStorage(':'); grunt> B = foreach A generate $0 as id; grunt> dump B;\n\nLocal Mode\n\n$ pig -x local ... - Connecting to ... grunt>\n\nTez Local Mode\n\n$ pig -x tez_local ... - Connecting to ... grunt>\n\nMapreduce Mode\n\n$ pig -x mapreduce ... - Connecting to ... grunt> or $ pig ... - Connecting to ... grunt>\n\nTez Mode\n\n$ pig -x tez ... - Connecting to ... grunt>\n\nBatch Mode\n\nYou can run Pig in batch mode using Pig scripts and the \"pig\" command (in local or hadoop mode).\n\nExample\n\nThe Pig Latin statements in the Pig script (id.pig) extract all user IDs from the /etc/passwd file. First, copy the /etc/passwd file to your local working directory. Next, run the Pig script from the command line (using local or mapreduce mode). The STORE operator will write the results to a file (id.out).\n\n/* id.pig */ A = load 'passwd' using PigStorage(':'); -- load the passwd file B = foreach A generate $0 as id; -- extract the user IDs store B into 'id.out'; -- write the results to a file name id.out\n\nLocal Mode\n\n$ pig -x local id.pig\n\nTez Local Mode\n\n$ pig -x tez_local id.pig\n\nMapreduce Mode\n\n$ pig id.pig or $ pig -x mapreduce id.pig\n\nTez Mode\n\n$ pig -x tez id.pig\n\nPig Scripts\n\nUse Pig scripts to place Pig Latin statements and Pig commands in a single file. While not required, it is good practice to identify the file using the *.pig extension.\n\nYou can run Pig scripts from the command line and from the Grunt shell (see the run and exec commands).\n\nPig scripts allow you to pass values to parameters using parameter substitution.\n\nYou can include comments in Pig scripts:\n\nFor multi-line comments use /* …. */\n\nFor single-line comments use --\n\n/* myscript.pig My script is simple. It includes three Pig Latin statements. */ A = LOAD 'student' USING PigStorage() AS (name:chararray, age:int, gpa:float); -- loading data B = FOREACH A GENERATE name; -- transforming data DUMP B; -- retrieving results\n\nScripts and Distributed File Systems\n\nPig supports running scripts (and Jar files) that are stored in HDFS, Amazon S3, and other distributed file systems. The script's full location URI is required (see REGISTER for information about Jar files). For example, to run a Pig script on HDFS, do the following:\n\n$ pig hdfs://nn.mydomain.com:9020/myscripts/script.pig\n\nPig Latin Statements\n\nPig Latin statements are the basic constructs you use to process data using Pig. A Pig Latin statement is an operator that takes a relation as input and produces another relation as output. (This definition applies to all Pig Latin operators except LOAD and STORE which read data from and write data to the file system.) Pig Latin statements may include expressions and schemas. Pig Latin statements can span multiple lines and must end with a semi-colon ( ; ). By default, Pig Latin statements are processed using multi-query execution.\n\nPig Latin statements are generally organized as follows:\n\nA LOAD statement to read data from the file system.\n\nA series of \"transformation\" statements to process the data.\n\nA DUMP statement to view results or a STORE statement to save the results.\n\nNote that a DUMP or STORE statement is required to generate output.\n\nIn this example Pig will validate, but not execute, the LOAD and FOREACH statements.\n\nA = LOAD 'student' USING PigStorage() AS (name:chararray, age:int, gpa:float); B = FOREACH A GENERATE name;\n\nIn this example, Pig will validate and then execute the LOAD, FOREACH, and DUMP statements.\n\nA = LOAD 'student' USING PigStorage() AS (name:chararray, age:int, gpa:float); B = FOREACH A GENERATE name; DUMP B; (John) (Mary) (Bill) (Joe)\n\nLoading Data\n\nUse the LOAD operator and the load/store functions to read data into Pig (PigStorage is the default load function).\n\nWorking with Data\n\nPig allows you to transform data in many ways. As a starting point, become familiar with these operators:\n\nUse the FILTER operator to work with tuples or rows of data. Use the FOREACH operator to work with columns of data.\n\nUse the GROUP operator to group data in a single relation. Use the COGROUP, inner JOIN, and outer JOIN operators to group or join data in two or more relations.\n\nUse the UNION operator to merge the contents of two or more relations. Use the SPLIT operator to partition the contents of a relation into multiple relations.\n\nStoring Intermediate Results\n\nPig stores the intermediate data generated between MapReduce jobs in a temporary location on HDFS. This location must already exist on HDFS prior to use. This location can be configured using the pig.temp.dir property. The property's default value is \"/tmp\" which is the same as the hardcoded location in Pig 0.7.0 and earlier versions.\n\nStoring Final Results\n\nUse the STORE operator and the load/store functions to write results to the file system (PigStorage is the default store function).\n\nNote: During the testing/debugging phase of your implementation, you can use DUMP to display results to your terminal screen. However, in a production environment you always want to use the STORE operator to save your results (see Store vs. Dump).\n\nDebugging Pig Latin\n\nPig Latin provides operators that can help you debug your Pig Latin statements:\n\nUse the DUMP operator to display results to your terminal screen.\n\nUse the DESCRIBE operator to review the schema of a relation.\n\nUse the EXPLAIN operator to view the logical, physical, or map reduce execution plans to compute a relation.\n\nUse the ILLUSTRATE operator to view the step-by-step execution of a series of statements.\n\nShortcuts for Debugging Operators\n\nPig provides shortcuts for the frequently used debugging operators (DUMP, DESCRIBE, EXPLAIN, ILLUSTRATE). These shortcuts can be used in Grunt shell or within pig scripts. Following are the shortcuts supported by pig\n\n\\d alias - shourtcut for DUMP operator. If alias is ignored last defined alias will be used.\n\n\\de alias - shourtcut for DESCRIBE operator. If alias is ignored last defined alias will be used.\n\n\\e alias - shourtcut for EXPLAIN operator. If alias is ignored last defined alias will be used.\n\n\\i alias - shourtcut for ILLUSTRATE operator. If alias is ignored last defined alias will be used.\n\n\\q - To quit grunt shell\n\nPig Properties\n\nPig Tutorial\n\nThe Pig tutorial shows you how to run Pig scripts using Pig's local mode, mapreduce mode and Tez mode (see Execution Modes).\n\nTo get started, do the following preliminary tasks:\n\nMake sure the JAVA_HOME environment variable is set the root of your Java installation.\n\nMake sure your PATH includes bin/pig (this enables you to run the tutorials using the \"pig\" command).\n\n$ export PATH=/<my-path-to-pig>/pig-0.14.0/bin:$PATH\n\nSet the PIG_HOME environment variable:\n\n$ export PIG_HOME=/<my-path-to-pig>/pig-0.14.0\n\nCreate the pigtutorial.tar.gz file:\n\nMove to the Pig tutorial directory (.../pig-0.14.0/tutorial).\n\nRun the \"ant\" command from the tutorial directory. This will create the pigtutorial.tar.gz file.\n\nCopy the pigtutorial.tar.gz file from the Pig tutorial directory to your local directory.\n\nUnzip the pigtutorial.tar.gz file.\n\n$ tar -xzf pigtutorial.tar.gz\n\nA new directory named pigtmp is created. This directory contains the Pig Tutorial Files. These files work with Hadoop 0.20.2 and include everything you need to run Pig Script 1 and Pig Script 2.\n\nRunning the Pig Scripts in Local Mode\n\nTo run the Pig scripts in local mode, do the following:\n\nMove to the pigtmp directory.\n\nExecute the following command (using either script1-local.pig or script2-local.pig).\n\n$ pig -x local script1-local.pig\n\nOr if you are using Tez local mode:\n\n$ pig -x tez_local script1-local.pig\n\nReview the result files, located in the script1-local-results.txt directory.\n\nThe output may contain a few Hadoop warnings which can be ignored:\n\n2010-04-08 12:55:33,642 [main] INFO org.apache.hadoop.metrics.jvm.JvmMetrics - Cannot initialize JVM Metrics with processName=JobTracker, sessionId= - already initialized\n\nRunning the Pig Scripts in Mapreduce Mode or Tez Mode\n\nTo run the Pig scripts in mapreduce mode, do the following:\n\nMove to the pigtmp directory.\n\nCopy the excite.log.bz2 file from the pigtmp directory to the HDFS directory.\n\n$ hadoop fs –copyFromLocal excite.log.bz2 .\n\nSet the PIG_CLASSPATH environment variable to the location of the cluster configuration directory (the directory that contains the core-site.xml, hdfs-site.xml and mapred-site.xml files):\n\nexport PIG_CLASSPATH=/mycluster/conf\n\nIf you are using Tez, you will also need to put Tez configuration directory (the directory that contains the tez-site.xml):\n\nexport PIG_CLASSPATH=/mycluster/conf:/tez/conf\n\nNote: The PIG_CLASSPATH can also be used to add any other 3rd party dependencies or resource files a pig script may require. If there is also a need to make the added entries take the highest precedence in the Pig JVM's classpath order, one may also set the env-var PIG_USER_CLASSPATH_FIRST to any value, such as 'true' (and unset the env-var to disable).\n\nSet the HADOOP_CONF_DIR environment variable to the location of the cluster configuration directory:\n\nexport HADOOP_CONF_DIR=/mycluster/conf\n\nExecute the following command (using either script1-hadoop.pig or script2-hadoop.pig):\n\n$ pig script1-hadoop.pig\n\nOr if you are using Tez:\n\n$ pig -x tez script1-hadoop.pig\n\nReview the result files, located in the script1-hadoop-results or script2-hadoop-results HDFS directory:\n\n$ hadoop fs -ls script1-hadoop-results $ hadoop fs -cat 'script1-hadoop-results/*' | less\n\nPig Tutorial Files\n\nThe contents of the Pig tutorial file (pigtutorial.tar.gz) are described here.\n\nFile\n\nDescription\n\npig.jar\n\nPig JAR file\n\ntutorial.jar\n\nUser defined functions (UDFs) and Java classes\n\nscript1-local.pig\n\nPig Script 1, Query Phrase Popularity (local mode)\n\nscript1-hadoop.pig\n\nPig Script 1, Query Phrase Popularity (mapreduce mode)\n\nscript2-local.pig\n\nPig Script 2, Temporal Query Phrase Popularity (local mode)\n\nscript2-hadoop.pig\n\nPig Script 2, Temporal Query Phrase Popularity (mapreduce mode)\n\nexcite-small.log\n\nLog file, Excite search engine (local mode)\n\nexcite.log.bz2\n\nLog file, Excite search engine (mapreduce)\n\nThe user defined functions (UDFs) are described here.\n\nPig Script 1: Query Phrase Popularity\n\nThe Query Phrase Popularity script (script1-local.pig or script1-hadoop.pig) processes a search query log file from the Excite search engine and finds search phrases that occur with particular high frequency during certain times of the day.\n\nThe script is shown here:\n\nRegister the tutorial JAR file so that the included UDFs can be called in the script.\n\nREGISTER ./tutorial.jar;\n\nUse the PigStorage function to load the excite log file (excite.log or excite-small.log) into the “raw” bag as an array of records with the fields user, time, and query.\n\nraw = LOAD 'excite.log' USING PigStorage('\\t') AS (user, time, query);\n\nCall the NonURLDetector UDF to remove records if the query field is empty or a URL.\n\nclean1 = FILTER raw BY org.apache.pig.tutorial.NonURLDetector(query);\n\nCall the ToLower UDF to change the query field to lowercase.\n\nclean2 = FOREACH clean1 GENERATE user, time, org.apache.pig.tutorial.ToLower(query) as query;\n\nBecause the log file only contains queries for a single day, we are only interested in the hour. The excite query log timestamp format is YYMMDDHHMMSS. Call the ExtractHour UDF to extract the hour (HH) from the time field.\n\nhoured = FOREACH clean2 GENERATE user, org.apache.pig.tutorial.ExtractHour(time) as hour, query;\n\nCall the NGramGenerator UDF to compose the n-grams of the query.\n\nngramed1 = FOREACH houred GENERATE user, hour, flatten(org.apache.pig.tutorial.NGramGenerator(query)) as ngram;\n\nUse the DISTINCT operator to get the unique n-grams for all records.\n\nngramed2 = DISTINCT ngramed1;\n\nUse the GROUP operator to group records by n-gram and hour.\n\nhour_frequency1 = GROUP ngramed2 BY (ngram, hour);\n\nUse the COUNT function to get the count (occurrences) of each n-gram.\n\nhour_frequency2 = FOREACH hour_frequency1 GENERATE flatten($0), COUNT($1) as count;\n\nUse the GROUP operator to group records by n-gram only. Each group now corresponds to a distinct n-gram and has the count for each hour.\n\nuniq_frequency1 = GROUP hour_frequency2 BY group::ngram;\n\nFor each group, identify the hour in which this n-gram is used with a particularly high frequency. Call the ScoreGenerator UDF to calculate a \"popularity\" score for the n-gram.\n\nuniq_frequency2 = FOREACH uniq_frequency1 GENERATE flatten($0), flatten(org.apache.pig.tutorial.ScoreGenerator($1));\n\nUse the FOREACH-GENERATE operator to assign names to the fields.\n\nuniq_frequency3 = FOREACH uniq_frequency2 GENERATE $1 as hour, $0 as ngram, $2 as score, $3 as count, $4 as mean;\n\nUse the FILTER operator to remove all records with a score less than or equal to 2.0.\n\nfiltered_uniq_frequency = FILTER uniq_frequency3 BY score > 2.0;\n\nUse the ORDER operator to sort the remaining records by hour and score.\n\nordered_uniq_frequency = ORDER filtered_uniq_frequency BY hour, score;\n\nUse the PigStorage function to store the results. The output file contains a list of n-grams with the following fields: hour, ngram, score, count, mean.\n\nSTORE ordered_uniq_frequency INTO '/tmp/tutorial-results' USING PigStorage();\n\nPig Script 2: Temporal Query Phrase Popularity\n\nThe Temporal Query Phrase Popularity script (script2-local.pig or script2-hadoop.pig) processes a search query log file from the Excite search engine and compares the occurrence of frequency of search phrases across two time periods separated by twelve hours.\n\nThe script is shown here:\n\nRegister the tutorial JAR file so that the user defined functions (UDFs) can be called in the script.\n\nREGISTER ./tutorial.jar;\n\nUse the PigStorage function to load the excite log file (excite.log or excite-small.log) into the “raw” bag as an array of records with the fields user, time, and query.\n\nraw = LOAD 'excite.log' USING PigStorage('\\t') AS (user, time, query);\n\nCall the NonURLDetector UDF to remove records if the query field is empty or a URL.\n\nclean1 = FILTER raw BY org.apache.pig.tutorial.NonURLDetector(query);\n\nCall the ToLower UDF to change the query field to lowercase.\n\nclean2 = FOREACH clean1 GENERATE user, time, org.apache.pig.tutorial.ToLower(query) as query;\n\nBecause the log file only contains queries for a single day, we are only interested in the hour. The excite query log timestamp format is YYMMDDHHMMSS. Call the ExtractHour UDF to extract the hour from the time field.\n\nhoured = FOREACH clean2 GENERATE user, org.apache.pig.tutorial.ExtractHour(time) as hour, query;\n\nCall the NGramGenerator UDF to compose the n-grams of the query.\n\nngramed1 = FOREACH houred GENERATE user, hour, flatten(org.apache.pig.tutorial.NGramGenerator(query)) as ngram;\n\nUse the DISTINCT operator to get the unique n-grams for all records.\n\nngramed2 = DISTINCT ngramed1;\n\nUse the GROUP operator to group the records by n-gram and hour.\n\nhour_frequency1 = GROUP ngramed2 BY (ngram, hour);\n\nUse the COUNT function to get the count (occurrences) of each n-gram.\n\nhour_frequency2 = FOREACH hour_frequency1 GENERATE flatten($0), COUNT($1) as count;\n\nUse the FOREACH-GENERATE operator to assign names to the fields.\n\nhour_frequency3 = FOREACH hour_frequency2 GENERATE $0 as ngram, $1 as hour, $2 as count;\n\nUse the FILTERoperator to get the n-grams for hour ‘00’\n\nhour00 = FILTER hour_frequency2 BY hour eq '00';\n\nUses the FILTER operators to get the n-grams for hour ‘12’\n\nhour12 = FILTER hour_frequency3 BY hour eq '12';\n\nUse the JOIN operator to get the n-grams that appear in both hours.\n\nsame = JOIN hour00 BY $0, hour12 BY $0;\n\nUse the FOREACH-GENERATE operator to record their frequency.\n\nsame1 = FOREACH same GENERATE hour_frequency2::hour00::group::ngram as ngram, $2 as count00, $5 as count12;\n\nUse the PigStorage function to store the results. The output file contains a list of n-grams with the following fields: ngram, count00, count12.\n\nSTORE same1 INTO '/tmp/tutorial-join-results' USING PigStorage();"
    }
}
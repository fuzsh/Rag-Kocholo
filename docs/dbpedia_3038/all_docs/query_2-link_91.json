{
    "id": "dbpedia_3038_2",
    "rank": 91,
    "data": {
        "url": "https://www.tutorialspoint.com/apache_pig/apache_pig_quick_guide.htm",
        "read_more_link": "",
        "language": "en",
        "title": "Quick Guide",
        "top_image": "https://www.tutorialspoint.com/images/tp_logo_436.png",
        "meta_img": "https://www.tutorialspoint.com/images/tp_logo_436.png",
        "images": [
            "https://www.tutorialspoint.com/apache_pig/images/apache-pig-mini-logo.jpg",
            "https://www.tutorialspoint.com/apache_pig/images/apache_pig_architecture.jpg",
            "https://www.tutorialspoint.com/apache_pig/images/data_model.jpg",
            "https://www.tutorialspoint.com/apache_pig/images/home_page.jpg",
            "https://www.tutorialspoint.com/apache_pig/images/apache_pig_releases.jpg",
            "https://www.tutorialspoint.com/apache_pig/images/mirror_site.jpg",
            "https://www.tutorialspoint.com/apache_pig/images/pig_release.jpg",
            "https://www.tutorialspoint.com/apache_pig/images/index.jpg",
            "https://www.tutorialspoint.com/apache_pig/images/local_mode_output.jpg",
            "https://www.tutorialspoint.com/apache_pig/images/mapreduce_mode_output.jpg",
            "https://www.tutorialspoint.com/apache_pig/images/select_export.jpg",
            "https://www.tutorialspoint.com/apache_pig/images/click_export.jpg",
            "https://www.tutorialspoint.com/apache_pig/images/jar_export.jpg",
            "https://www.tutorialspoint.com/static/images/logo-footer.svg",
            "https://www.tutorialspoint.com/static/images/googleplay.svg",
            "https://www.tutorialspoint.com/static/images/appstore.svg"
        ],
        "movies": [],
        "keywords": [],
        "meta_keywords": [
            ""
        ],
        "tags": null,
        "authors": [],
        "publish_date": null,
        "summary": "",
        "meta_description": "Apache Pig - Quick Guide - Apache Pig is an abstraction over MapReduce. It is a tool/platform which is used to analyze larger sets of data representing them as data flows. Pig is generally used with Hadoop; we can perform all the data manipulation operations in Hadoop using Apache Pig.",
        "meta_lang": "en",
        "meta_favicon": "https://www.tutorialspoint.com/images/favicon.ico",
        "meta_site_name": "",
        "canonical_link": "https://www.tutorialspoint.com/apache_pig/apache_pig_quick_guide.htm",
        "text": "Apache Pig - Quick Guide\n\nApache Pig - Overview\n\nWhat is Apache Pig?\n\nApache Pig is an abstraction over MapReduce. It is a tool/platform which is used to analyze larger sets of data representing them as data flows. Pig is generally used with Hadoop; we can perform all the data manipulation operations in Hadoop using Apache Pig.\n\nTo write data analysis programs, Pig provides a high-level language known as Pig Latin. This language provides various operators using which programmers can develop their own functions for reading, writing, and processing data.\n\nTo analyze data using Apache Pig, programmers need to write scripts using Pig Latin language. All these scripts are internally converted to Map and Reduce tasks. Apache Pig has a component known as Pig Engine that accepts the Pig Latin scripts as input and converts those scripts into MapReduce jobs.\n\nWhy Do We Need Apache Pig?\n\nProgrammers who are not so good at Java normally used to struggle working with Hadoop, especially while performing any MapReduce tasks. Apache Pig is a boon for all such programmers.\n\nUsing Pig Latin, programmers can perform MapReduce tasks easily without having to type complex codes in Java.\n\nApache Pig uses multi-query approach, thereby reducing the length of codes. For example, an operation that would require you to type 200 lines of code (LoC) in Java can be easily done by typing as less as just 10 LoC in Apache Pig. Ultimately Apache Pig reduces the development time by almost 16 times.\n\nPig Latin is SQL-like language and it is easy to learn Apache Pig when you are familiar with SQL.\n\nApache Pig provides many built-in operators to support data operations like joins, filters, ordering, etc. In addition, it also provides nested data types like tuples, bags, and maps that are missing from MapReduce.\n\nFeatures of Pig\n\nApache Pig comes with the following features −\n\nRich set of operators − It provides many operators to perform operations like join, sort, filer, etc.\n\nEase of programming − Pig Latin is similar to SQL and it is easy to write a Pig script if you are good at SQL.\n\nOptimization opportunities − The tasks in Apache Pig optimize their execution automatically, so the programmers need to focus only on semantics of the language.\n\nExtensibility − Using the existing operators, users can develop their own functions to read, process, and write data.\n\nUDF’s − Pig provides the facility to create User-defined Functions in other programming languages such as Java and invoke or embed them in Pig Scripts.\n\nHandles all kinds of data − Apache Pig analyzes all kinds of data, both structured as well as unstructured. It stores the results in HDFS.\n\nApache Pig Vs MapReduce\n\nListed below are the major differences between Apache Pig and MapReduce.\n\nApache Pig Vs SQL\n\nListed below are the major differences between Apache Pig and SQL.\n\nIn addition to above differences, Apache Pig Latin −\n\nAllows splits in the pipeline.\n\nAllows developers to store data anywhere in the pipeline.\n\nDeclares execution plans.\n\nProvides operators to perform ETL (Extract, Transform, and Load) functions.\n\nApache Pig Vs Hive\n\nBoth Apache Pig and Hive are used to create MapReduce jobs. And in some cases, Hive operates on HDFS in a similar way Apache Pig does. In the following table, we have listed a few significant points that set Apache Pig apart from Hive.\n\nApplications of Apache Pig\n\nApache Pig is generally used by data scientists for performing tasks involving ad-hoc processing and quick prototyping. Apache Pig is used −\n\nTo process huge data sources such as web logs.\n\nTo perform data processing for search platforms.\n\nTo process time sensitive data loads.\n\nApache Pig – History\n\nIn 2006, Apache Pig was developed as a research project at Yahoo, especially to create and execute MapReduce jobs on every dataset. In 2007, Apache Pig was open sourced via Apache incubator. In 2008, the first release of Apache Pig came out. In 2010, Apache Pig graduated as an Apache top-level project.\n\nApache Pig - Architecture\n\nThe language used to analyze data in Hadoop using Pig is known as Pig Latin. It is a highlevel data processing language which provides a rich set of data types and operators to perform various operations on the data.\n\nTo perform a particular task Programmers using Pig, programmers need to write a Pig script using the Pig Latin language, and execute them using any of the execution mechanisms (Grunt Shell, UDFs, Embedded). After execution, these scripts will go through a series of transformations applied by the Pig Framework, to produce the desired output.\n\nInternally, Apache Pig converts these scripts into a series of MapReduce jobs, and thus, it makes the programmer’s job easy. The architecture of Apache Pig is shown below.\n\nApache Pig Components\n\nAs shown in the figure, there are various components in the Apache Pig framework. Let us take a look at the major components.\n\nParser\n\nInitially the Pig Scripts are handled by the Parser. It checks the syntax of the script, does type checking, and other miscellaneous checks. The output of the parser will be a DAG (directed acyclic graph), which represents the Pig Latin statements and logical operators.\n\nIn the DAG, the logical operators of the script are represented as the nodes and the data flows are represented as edges.\n\nOptimizer\n\nThe logical plan (DAG) is passed to the logical optimizer, which carries out the logical optimizations such as projection and pushdown.\n\nCompiler\n\nThe compiler compiles the optimized logical plan into a series of MapReduce jobs.\n\nExecution engine\n\nFinally the MapReduce jobs are submitted to Hadoop in a sorted order. Finally, these MapReduce jobs are executed on Hadoop producing the desired results.\n\nPig Latin Data Model\n\nThe data model of Pig Latin is fully nested and it allows complex non-atomic datatypes such as map and tuple. Given below is the diagrammatical representation of Pig Latin’s data model.\n\nAtom\n\nAny single value in Pig Latin, irrespective of their data, type is known as an Atom. It is stored as string and can be used as string and number. int, long, float, double, chararray, and bytearray are the atomic values of Pig. A piece of data or a simple atomic value is known as a field.\n\nExample − ‘raja’ or ‘30’\n\nTuple\n\nA record that is formed by an ordered set of fields is known as a tuple, the fields can be of any type. A tuple is similar to a row in a table of RDBMS.\n\nExample − (Raja, 30)\n\nBag\n\nA bag is an unordered set of tuples. In other words, a collection of tuples (non-unique) is known as a bag. Each tuple can have any number of fields (flexible schema). A bag is represented by ‘{}’. It is similar to a table in RDBMS, but unlike a table in RDBMS, it is not necessary that every tuple contain the same number of fields or that the fields in the same position (column) have the same type.\n\nExample − {(Raja, 30), (Mohammad, 45)}\n\nA bag can be a field in a relation; in that context, it is known as inner bag.\n\nExample − {Raja, 30, {9848022338, raja@gmail.com,}}\n\nMap\n\nA map (or data map) is a set of key-value pairs. The key needs to be of type chararray and should be unique. The value might be of any type. It is represented by ‘[]’\n\nExample − [name#Raja, age#30]\n\nRelation\n\nA relation is a bag of tuples. The relations in Pig Latin are unordered (there is no guarantee that tuples are processed in any particular order).\n\nApache Pig - Installation\n\nThis chapter explains the how to download, install, and set up Apache Pig in your system.\n\nPrerequisites\n\nIt is essential that you have Hadoop and Java installed on your system before you go for Apache Pig. Therefore, prior to installing Apache Pig, install Hadoop and Java by following the steps given in the following link −\n\nhttps://www.tutorialspoint.com/hadoop/hadoop_enviornment_setup.htm\n\nDownload Apache Pig\n\nFirst of all, download the latest version of Apache Pig from the following website − https://pig.apache.org/\n\nStep 1\n\nOpen the homepage of Apache Pig website. Under the section News, click on the link release page as shown in the following snapshot.\n\nStep 2\n\nOn clicking the specified link, you will be redirected to the Apache Pig Releases page. On this page, under the Download section, you will have two links, namely, Pig 0.8 and later and Pig 0.7 and before. Click on the link Pig 0.8 and later, then you will be redirected to the page having a set of mirrors.\n\nStep 3\n\nChoose and click any one of these mirrors as shown below.\n\nStep 4\n\nThese mirrors will take you to the Pig Releases page. This page contains various versions of Apache Pig. Click the latest version among them.\n\nStep 5\n\nWithin these folders, you will have the source and binary files of Apache Pig in various distributions. Download the tar files of the source and binary files of Apache Pig 0.15, pig0.15.0-src.tar.gz and pig-0.15.0.tar.gz.\n\nInstall Apache Pig\n\nAfter downloading the Apache Pig software, install it in your Linux environment by following the steps given below.\n\nStep 1\n\nCreate a directory with the name Pig in the same directory where the installation directories of Hadoop, Java, and other software were installed. (In our tutorial, we have created the Pig directory in the user named Hadoop).\n\n$ mkdir Pig\n\nStep 2\n\nExtract the downloaded tar files as shown below.\n\n$ cd Downloads/ $ tar zxvf pig-0.15.0-src.tar.gz $ tar zxvf pig-0.15.0.tar.gz\n\nStep 3\n\nMove the content of pig-0.15.0-src.tar.gz file to the Pig directory created earlier as shown below.\n\n$ mv pig-0.15.0-src.tar.gz/* /home/Hadoop/Pig/\n\nConfigure Apache Pig\n\nAfter installing Apache Pig, we have to configure it. To configure, we need to edit two files − bashrc and pig.properties.\n\n.bashrc file\n\nIn the .bashrc file, set the following variables −\n\nPIG_HOME folder to the Apache Pig’s installation folder,\n\nPATH environment variable to the bin folder, and\n\nPIG_CLASSPATH environment variable to the etc (configuration) folder of your Hadoop installations (the directory that contains the core-site.xml, hdfs-site.xml and mapred-site.xml files).\n\nexport PIG_HOME = /home/Hadoop/Pig export PATH = $PATH:/home/Hadoop/pig/bin export PIG_CLASSPATH = $HADOOP_HOME/conf\n\npig.properties file\n\nIn the conf folder of Pig, we have a file named pig.properties. In the pig.properties file, you can set various parameters as given below.\n\npig -h properties\n\nThe following properties are supported −\n\nLogging: verbose = true|false; default is false. This property is the same as -v switch brief=true|false; default is false. This property is the same as -b switch debug=OFF|ERROR|WARN|INFO|DEBUG; default is INFO. This property is the same as -d switch aggregate.warning = true|false; default is true. If true, prints count of warnings of each type rather than logging each warning. Performance tuning: pig.cachedbag.memusage=<mem fraction>; default is 0.2 (20% of all memory). Note that this memory is shared across all large bags used by the application. pig.skewedjoin.reduce.memusagea=<mem fraction>; default is 0.3 (30% of all memory). Specifies the fraction of heap available for the reducer to perform the join. pig.exec.nocombiner = true|false; default is false. Only disable combiner as a temporary workaround for problems. opt.multiquery = true|false; multiquery is on by default. Only disable multiquery as a temporary workaround for problems. opt.fetch=true|false; fetch is on by default. Scripts containing Filter, Foreach, Limit, Stream, and Union can be dumped without MR jobs. pig.tmpfilecompression = true|false; compression is off by default. Determines whether output of intermediate jobs is compressed. pig.tmpfilecompression.codec = lzo|gzip; default is gzip. Used in conjunction with pig.tmpfilecompression. Defines compression type. pig.noSplitCombination = true|false. Split combination is on by default. Determines if multiple small files are combined into a single map. pig.exec.mapPartAgg = true|false. Default is false. Determines if partial aggregation is done within map phase, before records are sent to combiner. pig.exec.mapPartAgg.minReduction=<min aggregation factor>. Default is 10. If the in-map partial aggregation does not reduce the output num records by this factor, it gets disabled. Miscellaneous: exectype = mapreduce|tez|local; default is mapreduce. This property is the same as -x switch pig.additional.jars.uris=<comma seperated list of jars>. Used in place of register command. udf.import.list=<comma seperated list of imports>. Used to avoid package names in UDF. stop.on.failure = true|false; default is false. Set to true to terminate on the first error. pig.datetime.default.tz=<UTC time offset>. e.g. +08:00. Default is the default timezone of the host. Determines the timezone used to handle datetime datatype and UDFs. Additionally, any Hadoop property can be specified.\n\nVerifying the Installation\n\nVerify the installation of Apache Pig by typing the version command. If the installation is successful, you will get the version of Apache Pig as shown below.\n\n$ pig –version Apache Pig version 0.15.0 (r1682971) compiled Jun 01 2015, 11:44:35\n\nApache Pig - Execution\n\nIn the previous chapter, we explained how to install Apache Pig. In this chapter, we will discuss how to execute Apache Pig.\n\nApache Pig Execution Modes\n\nYou can run Apache Pig in two modes, namely, Local Mode and HDFS mode.\n\nLocal Mode\n\nIn this mode, all the files are installed and run from your local host and local file system. There is no need of Hadoop or HDFS. This mode is generally used for testing purpose.\n\nMapReduce Mode\n\nMapReduce mode is where we load or process the data that exists in the Hadoop File System (HDFS) using Apache Pig. In this mode, whenever we execute the Pig Latin statements to process the data, a MapReduce job is invoked in the back-end to perform a particular operation on the data that exists in the HDFS.\n\nApache Pig Execution Mechanisms\n\nApache Pig scripts can be executed in three ways, namely, interactive mode, batch mode, and embedded mode.\n\nInteractive Mode (Grunt shell) − You can run Apache Pig in interactive mode using the Grunt shell. In this shell, you can enter the Pig Latin statements and get the output (using Dump operator).\n\nBatch Mode (Script) − You can run Apache Pig in Batch mode by writing the Pig Latin script in a single file with .pig extension.\n\nEmbedded Mode (UDF) − Apache Pig provides the provision of defining our own functions (User Defined Functions) in programming languages such as Java, and using them in our script.\n\nInvoking the Grunt Shell\n\nYou can invoke the Grunt shell in a desired mode (local/MapReduce) using the −x option as shown below.\n\nEither of these commands gives you the Grunt shell prompt as shown below.\n\ngrunt>\n\nYou can exit the Grunt shell using ‘ctrl &plus; d’.\n\nAfter invoking the Grunt shell, you can execute a Pig script by directly entering the Pig Latin statements in it.\n\ngrunt> customers = LOAD 'customers.txt' USING PigStorage(',');\n\nExecuting Apache Pig in Batch Mode\n\nYou can write an entire Pig Latin script in a file and execute it using the –x command. Let us suppose we have a Pig script in a file named sample_script.pig as shown below.\n\nSample_script.pig\n\nstudent = LOAD 'hdfs://localhost:9000/pig_data/student.txt' USING PigStorage(',') as (id:int,name:chararray,city:chararray); Dump student;\n\nNow, you can execute the script in the above file as shown below.\n\nNote − We will discuss in detail how to run a Pig script in Bach mode and in embedded mode in subsequent chapters.\n\nApache Pig - Grunt Shell\n\nAfter invoking the Grunt shell, you can run your Pig scripts in the shell. In addition to that, there are certain useful shell and utility commands provided by the Grunt shell. This chapter explains the shell and utility commands provided by the Grunt shell.\n\nNote − In some portions of this chapter, the commands like Load and Store are used. Refer the respective chapters to get in-detail information on them.\n\nShell Commands\n\nThe Grunt shell of Apache Pig is mainly used to write Pig Latin scripts. Prior to that, we can invoke any shell commands using sh and fs.\n\nsh Command\n\nUsing sh command, we can invoke any shell commands from the Grunt shell. Using sh command from the Grunt shell, we cannot execute the commands that are a part of the shell environment (ex − cd).\n\nSyntax\n\nGiven below is the syntax of sh command.\n\ngrunt> sh shell command parameters\n\nExample\n\nWe can invoke the ls command of Linux shell from the Grunt shell using the sh option as shown below. In this example, it lists out the files in the /pig/bin/ directory.\n\ngrunt> sh ls pig pig_1444799121955.log pig.cmd pig.py\n\nfs Command\n\nUsing the fs command, we can invoke any FsShell commands from the Grunt shell.\n\nSyntax\n\nGiven below is the syntax of fs command.\n\ngrunt> sh File System command parameters\n\nExample\n\nWe can invoke the ls command of HDFS from the Grunt shell using fs command. In the following example, it lists the files in the HDFS root directory.\n\ngrunt> fs –ls Found 3 items drwxrwxrwx - Hadoop supergroup 0 2015-09-08 14:13 Hbase drwxr-xr-x - Hadoop supergroup 0 2015-09-09 14:52 seqgen_data drwxr-xr-x - Hadoop supergroup 0 2015-09-08 11:30 twitter_data\n\nIn the same way, we can invoke all the other file system shell commands from the Grunt shell using the fs command.\n\nUtility Commands\n\nThe Grunt shell provides a set of utility commands. These include utility commands such as clear, help, history, quit, and set; and commands such as exec, kill, and run to control Pig from the Grunt shell. Given below is the description of the utility commands provided by the Grunt shell.\n\nclear Command\n\nThe clear command is used to clear the screen of the Grunt shell.\n\nSyntax\n\nYou can clear the screen of the grunt shell using the clear command as shown below.\n\ngrunt> clear\n\nhelp Command\n\nThe help command gives you a list of Pig commands or Pig properties.\n\nUsage\n\nYou can get a list of Pig commands using the help command as shown below.\n\ngrunt> help Commands: <pig latin statement>; - See the PigLatin manual for details: http://hadoop.apache.org/pig File system commands:fs <fs arguments> - Equivalent to Hadoop dfs command: http://hadoop.apache.org/common/docs/current/hdfs_shell.html Diagnostic Commands:describe <alias>[::<alias] - Show the schema for the alias. Inner aliases can be described as A::B. explain [-script <pigscript>] [-out <path>] [-brief] [-dot|-xml] [-param <param_name>=<pCram_value>] [-param_file <file_name>] [<alias>] - Show the execution plan to compute the alias or for entire script. -script - Explain the entire script. -out - Store the output into directory rather than print to stdout. -brief - Don't expand nested plans (presenting a smaller graph for overview). -dot - Generate the output in .dot format. Default is text format. -xml - Generate the output in .xml format. Default is text format. -param <param_name - See parameter substitution for details. -param_file <file_name> - See parameter substitution for details. alias - Alias to explain. dump <alias> - Compute the alias and writes the results to stdout. Utility Commands: exec [-param <param_name>=param_value] [-param_file <file_name>] <script> - Execute the script with access to grunt environment including aliases. -param <param_name - See parameter substitution for details. -param_file <file_name> - See parameter substitution for details. script - Script to be executed. run [-param <param_name>=param_value] [-param_file <file_name>] <script> - Execute the script with access to grunt environment. -param <param_name - See parameter substitution for details. -param_file <file_name> - See parameter substitution for details. script - Script to be executed. sh <shell command> - Invoke a shell command. kill <job_id> - Kill the hadoop job specified by the hadoop job id. set <key> <value> - Provide execution parameters to Pig. Keys and values are case sensitive. The following keys are supported: default_parallel - Script-level reduce parallelism. Basic input size heuristics used by default. debug - Set debug on or off. Default is off. job.name - Single-quoted name for jobs. Default is PigLatin:<script name> job.priority - Priority for jobs. Values: very_low, low, normal, high, very_high. Default is normal stream.skippath - String that contains the path. This is used by streaming any hadoop property. help - Display this message. history [-n] - Display the list statements in cache. -n Hide line numbers. quit - Quit the grunt shell.\n\nhistory Command\n\nThis command displays a list of statements executed / used so far since the Grunt sell is invoked.\n\nUsage\n\nAssume we have executed three statements since opening the Grunt shell.\n\ngrunt> customers = LOAD 'hdfs://localhost:9000/pig_data/customers.txt' USING PigStorage(','); grunt> orders = LOAD 'hdfs://localhost:9000/pig_data/orders.txt' USING PigStorage(','); grunt> student = LOAD 'hdfs://localhost:9000/pig_data/student.txt' USING PigStorage(',');\n\nThen, using the history command will produce the following output.\n\ngrunt> history customers = LOAD 'hdfs://localhost:9000/pig_data/customers.txt' USING PigStorage(','); orders = LOAD 'hdfs://localhost:9000/pig_data/orders.txt' USING PigStorage(','); student = LOAD 'hdfs://localhost:9000/pig_data/student.txt' USING PigStorage(',');\n\nset Command\n\nThe set command is used to show/assign values to keys used in Pig.\n\nUsage\n\nUsing this command, you can set values to the following keys.\n\nquit Command\n\nYou can quit from the Grunt shell using this command.\n\nUsage\n\nQuit from the Grunt shell as shown below.\n\ngrunt> quit\n\nLet us now take a look at the commands using which you can control Apache Pig from the Grunt shell.\n\nexec Command\n\nUsing the exec command, we can execute Pig scripts from the Grunt shell.\n\nSyntax\n\nGiven below is the syntax of the utility command exec.\n\ngrunt> exec [–param param_name = param_value] [–param_file file_name] [script]\n\nExample\n\nLet us assume there is a file named student.txt in the /pig_data/ directory of HDFS with the following content.\n\nStudent.txt\n\n001,Rajiv,Hyderabad 002,siddarth,Kolkata 003,Rajesh,Delhi\n\nAnd, assume we have a script file named sample_script.pig in the /pig_data/ directory of HDFS with the following content.\n\nSample_script.pig\n\nstudent = LOAD 'hdfs://localhost:9000/pig_data/student.txt' USING PigStorage(',') as (id:int,name:chararray,city:chararray); Dump student;\n\nNow, let us execute the above script from the Grunt shell using the exec command as shown below.\n\ngrunt> exec /sample_script.pig\n\nOutput\n\nThe exec command executes the script in the sample_script.pig. As directed in the script, it loads the student.txt file into Pig and gives you the result of the Dump operator displaying the following content.\n\n(1,Rajiv,Hyderabad) (2,siddarth,Kolkata) (3,Rajesh,Delhi)\n\nkill Command\n\nYou can kill a job from the Grunt shell using this command.\n\nSyntax\n\nGiven below is the syntax of the kill command.\n\ngrunt> kill JobId\n\nExample\n\nSuppose there is a running Pig job having id Id_0055, you can kill it from the Grunt shell using the kill command, as shown below.\n\ngrunt> kill Id_0055\n\nrun Command\n\nYou can run a Pig script from the Grunt shell using the run command\n\nSyntax\n\nGiven below is the syntax of the run command.\n\ngrunt> run [–param param_name = param_value] [–param_file file_name] script\n\nExample\n\nLet us assume there is a file named student.txt in the /pig_data/ directory of HDFS with the following content.\n\nStudent.txt\n\n001,Rajiv,Hyderabad 002,siddarth,Kolkata 003,Rajesh,Delhi\n\nAnd, assume we have a script file named sample_script.pig in the local filesystem with the following content.\n\nSample_script.pig\n\nstudent = LOAD 'hdfs://localhost:9000/pig_data/student.txt' USING PigStorage(',') as (id:int,name:chararray,city:chararray);\n\nNow, let us run the above script from the Grunt shell using the run command as shown below.\n\ngrunt> run /sample_script.pig\n\nYou can see the output of the script using the Dump operator as shown below.\n\ngrunt> Dump; (1,Rajiv,Hyderabad) (2,siddarth,Kolkata) (3,Rajesh,Delhi)\n\nNote − The difference between exec and the run command is that if we use run, the statements from the script are available in the command history.\n\nPig Latin – Basics\n\nPig Latin is the language used to analyze data in Hadoop using Apache Pig. In this chapter, we are going to discuss the basics of Pig Latin such as Pig Latin statements, data types, general and relational operators, and Pig Latin UDF’s.\n\nPig Latin – Data Model\n\nAs discussed in the previous chapters, the data model of Pig is fully nested. A Relation is the outermost structure of the Pig Latin data model. And it is a bag where −\n\nA bag is a collection of tuples.\n\nA tuple is an ordered set of fields.\n\nA field is a piece of data.\n\nPig Latin – Statemets\n\nWhile processing data using Pig Latin, statements are the basic constructs.\n\nThese statements work with relations. They include expressions and schemas.\n\nEvery statement ends with a semicolon (;).\n\nWe will perform various operations using operators provided by Pig Latin, through statements.\n\nExcept LOAD and STORE, while performing all other operations, Pig Latin statements take a relation as input and produce another relation as output.\n\nAs soon as you enter a Load statement in the Grunt shell, its semantic checking will be carried out. To see the contents of the schema, you need to use the Dump operator. Only after performing the dump operation, the MapReduce job for loading the data into the file system will be carried out.\n\nExample\n\nGiven below is a Pig Latin statement, which loads data to Apache Pig.\n\ngrunt> Student_data = LOAD 'student_data.txt' USING PigStorage(',')as ( id:int, firstname:chararray, lastname:chararray, phone:chararray, city:chararray );\n\nPig Latin – Data types\n\nGiven below table describes the Pig Latin data types.\n\nNull Values\n\nValues for all the above data types can be NULL. Apache Pig treats null values in a similar way as SQL does.\n\nA null can be an unknown value or a non-existent value. It is used as a placeholder for optional values. These nulls can occur naturally or can be the result of an operation.\n\nPig Latin – Arithmetic Operators\n\nThe following table describes the arithmetic operators of Pig Latin. Suppose a = 10 and b = 20.\n\nPig Latin – Comparison Operators\n\nThe following table describes the comparison operators of Pig Latin.\n\nPig Latin – Type Construction Operators\n\nThe following table describes the Type construction operators of Pig Latin.\n\nPig Latin – Relational Operations\n\nThe following table describes the relational operators of Pig Latin.\n\nApache Pig - Reading Data\n\nIn general, Apache Pig works on top of Hadoop. It is an analytical tool that analyzes large datasets that exist in the Hadoop File System. To analyze data using Apache Pig, we have to initially load the data into Apache Pig. This chapter explains how to load data to Apache Pig from HDFS.\n\nPreparing HDFS\n\nIn MapReduce mode, Pig reads (loads) data from HDFS and stores the results back in HDFS. Therefore, let us start HDFS and create the following sample data in HDFS.\n\nThe above dataset contains personal details like id, first name, last name, phone number and city, of six students.\n\nStep 1: Verifying Hadoop\n\nFirst of all, verify the installation using Hadoop version command, as shown below.\n\n$ hadoop version\n\nIf your system contains Hadoop, and if you have set the PATH variable, then you will get the following output −\n\nHadoop 2.6.0 Subversion https://git-wip-us.apache.org/repos/asf/hadoop.git -r e3496499ecb8d220fba99dc5ed4c99c8f9e33bb1 Compiled by jenkins on 2014-11-13T21:10Z Compiled with protoc 2.5.0 From source with checksum 18e43357c8f927c0695f1e9522859d6a This command was run using /home/Hadoop/hadoop/share/hadoop/common/hadoop common-2.6.0.jar\n\nStep 2: Starting HDFS\n\nBrowse through the sbin directory of Hadoop and start yarn and Hadoop dfs (distributed file system) as shown below.\n\ncd /$Hadoop_Home/sbin/ $ start-dfs.sh localhost: starting namenode, logging to /home/Hadoop/hadoop/logs/hadoopHadoop-namenode-localhost.localdomain.out localhost: starting datanode, logging to /home/Hadoop/hadoop/logs/hadoopHadoop-datanode-localhost.localdomain.out Starting secondary namenodes [0.0.0.0] starting secondarynamenode, logging to /home/Hadoop/hadoop/logs/hadoop-Hadoopsecondarynamenode-localhost.localdomain.out $ start-yarn.sh starting yarn daemons starting resourcemanager, logging to /home/Hadoop/hadoop/logs/yarn-Hadoopresourcemanager-localhost.localdomain.out localhost: starting nodemanager, logging to /home/Hadoop/hadoop/logs/yarnHadoop-nodemanager-localhost.localdomain.out\n\nStep 3: Create a Directory in HDFS\n\nIn Hadoop DFS, you can create directories using the command mkdir. Create a new directory in HDFS with the name Pig_Data in the required path as shown below.\n\n$cd /$Hadoop_Home/bin/ $ hdfs dfs -mkdir hdfs://localhost:9000/Pig_Data\n\nStep 4: Placing the data in HDFS\n\nThe input file of Pig contains each tuple/record in individual lines. And the entities of the record are separated by a delimiter (In our example we used “,”).\n\nIn the local file system, create an input file student_data.txt containing data as shown below.\n\n001,Rajiv,Reddy,9848022337,Hyderabad 002,siddarth,Battacharya,9848022338,Kolkata 003,Rajesh,Khanna,9848022339,Delhi 004,Preethi,Agarwal,9848022330,Pune 005,Trupthi,Mohanthy,9848022336,Bhuwaneshwar 006,Archana,Mishra,9848022335,Chennai.\n\nNow, move the file from the local file system to HDFS using put command as shown below. (You can use copyFromLocal command as well.)\n\n$ cd $HADOOP_HOME/bin $ hdfs dfs -put /home/Hadoop/Pig/Pig_Data/student_data.txt dfs://localhost:9000/pig_data/\n\nVerifying the file\n\nYou can use the cat command to verify whether the file has been moved into the HDFS, as shown below.\n\n$ cd $HADOOP_HOME/bin $ hdfs dfs -cat hdfs://localhost:9000/pig_data/student_data.txt\n\nOutput\n\nYou can see the content of the file as shown below.\n\n15/10/01 12:16:55 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable 001,Rajiv,Reddy,9848022337,Hyderabad 002,siddarth,Battacharya,9848022338,Kolkata 003,Rajesh,Khanna,9848022339,Delhi 004,Preethi,Agarwal,9848022330,Pune 005,Trupthi,Mohanthy,9848022336,Bhuwaneshwar 006,Archana,Mishra,9848022335,Chennai\n\nThe Load Operator\n\nYou can load data into Apache Pig from the file system (HDFS/ Local) using LOAD operator of Pig Latin.\n\nSyntax\n\nThe load statement consists of two parts divided by the “=” operator. On the left-hand side, we need to mention the name of the relation where we want to store the data, and on the right-hand side, we have to define how we store the data. Given below is the syntax of the Load operator.\n\nRelation_name = LOAD 'Input file path' USING function as schema;\n\nWhere,\n\nrelation_name − We have to mention the relation in which we want to store the data.\n\nInput file path − We have to mention the HDFS directory where the file is stored. (In MapReduce mode)\n\nfunction − We have to choose a function from the set of load functions provided by Apache Pig (BinStorage, JsonLoader, PigStorage, TextLoader).\n\nSchema − We have to define the schema of the data. We can define the required schema as follows −\n\n(column1 : data type, column2 : data type, column3 : data type);\n\nNote − We load the data without specifying the schema. In that case, the columns will be addressed as $01, $02, etc… (check).\n\nExample\n\nAs an example, let us load the data in student_data.txt in Pig under the schema named Student using the LOAD command.\n\nStart the Pig Grunt Shell\n\nFirst of all, open the Linux terminal. Start the Pig Grunt shell in MapReduce mode as shown below.\n\n$ Pig –x mapreduce\n\nIt will start the Pig Grunt shell as shown below.\n\n15/10/01 12:33:37 INFO pig.ExecTypeProvider: Trying ExecType : LOCAL 15/10/01 12:33:37 INFO pig.ExecTypeProvider: Trying ExecType : MAPREDUCE 15/10/01 12:33:37 INFO pig.ExecTypeProvider: Picked MAPREDUCE as the ExecType 2015-10-01 12:33:38,080 [main] INFO org.apache.pig.Main - Apache Pig version 0.15.0 (r1682971) compiled Jun 01 2015, 11:44:35 2015-10-01 12:33:38,080 [main] INFO org.apache.pig.Main - Logging error messages to: /home/Hadoop/pig_1443683018078.log 2015-10-01 12:33:38,242 [main] INFO org.apache.pig.impl.util.Utils - Default bootup file /home/Hadoop/.pigbootup not found 2015-10-01 12:33:39,630 [main] INFO org.apache.pig.backend.hadoop.executionengine.HExecutionEngine - Connecting to hadoop file system at: hdfs://localhost:9000 grunt>\n\nExecute the Load Statement\n\nNow load the data from the file student_data.txt into Pig by executing the following Pig Latin statement in the Grunt shell.\n\ngrunt> student = LOAD 'hdfs://localhost:9000/pig_data/student_data.txt' USING PigStorage(',') as ( id:int, firstname:chararray, lastname:chararray, phone:chararray, city:chararray );\n\nFollowing is the description of the above statement.\n\nNote − The load statement will simply load the data into the specified relation in Pig. To verify the execution of the Load statement, you have to use the Diagnostic Operators which are discussed in the next chapters.\n\nApache Pig - Storing Data\n\nIn the previous chapter, we learnt how to load data into Apache Pig. You can store the loaded data in the file system using the store operator. This chapter explains how to store data in Apache Pig using the Store operator.\n\nSyntax\n\nGiven below is the syntax of the Store statement.\n\nSTORE Relation_name INTO ' required_directory_path ' [USING function];\n\nExample\n\nAssume we have a file student_data.txt in HDFS with the following content.\n\n001,Rajiv,Reddy,9848022337,Hyderabad 002,siddarth,Battacharya,9848022338,Kolkata 003,Rajesh,Khanna,9848022339,Delhi 004,Preethi,Agarwal,9848022330,Pune 005,Trupthi,Mohanthy,9848022336,Bhuwaneshwar 006,Archana,Mishra,9848022335,Chennai.\n\nAnd we have read it into a relation student using the LOAD operator as shown below.\n\ngrunt> student = LOAD 'hdfs://localhost:9000/pig_data/student_data.txt' USING PigStorage(',') as ( id:int, firstname:chararray, lastname:chararray, phone:chararray, city:chararray );\n\nNow, let us store the relation in the HDFS directory “/pig_Output/” as shown below.\n\ngrunt> STORE student INTO ' hdfs://localhost:9000/pig_Output/ ' USING PigStorage (',');\n\nOutput\n\nAfter executing the store statement, you will get the following output. A directory is created with the specified name and the data will be stored in it.\n\n2015-10-05 13:05:05,429 [main] INFO org.apache.pig.backend.hadoop.executionengine.mapReduceLayer. MapReduceLau ncher - 100% complete 2015-10-05 13:05:05,429 [main] INFO org.apache.pig.tools.pigstats.mapreduce.SimplePigStats - Script Statistics: HadoopVersion PigVersion UserId StartedAt FinishedAt Features 2.6.0 0.15.0 Hadoop 2015-10-0 13:03:03 2015-10-05 13:05:05 UNKNOWN Success! Job Stats (time in seconds): JobId Maps Reduces MaxMapTime MinMapTime AvgMapTime MedianMapTime job_14459_06 1 0 n/a n/a n/a n/a MaxReduceTime MinReduceTime AvgReduceTime MedianReducetime Alias Feature 0 0 0 0 student MAP_ONLY OutPut folder hdfs://localhost:9000/pig_Output/ Input(s): Successfully read 0 records from: \"hdfs://localhost:9000/pig_data/student_data.txt\" Output(s): Successfully stored 0 records in: \"hdfs://localhost:9000/pig_Output\" Counters: Total records written : 0 Total bytes written : 0 Spillable Memory Manager spill count : 0 Total bags proactively spilled: 0 Total records proactively spilled: 0 Job DAG: job_1443519499159_0006 2015-10-05 13:06:06,192 [main] INFO org.apache.pig.backend.hadoop.executionengine .mapReduceLayer.MapReduceLau ncher - Success!\n\nVerification\n\nYou can verify the stored data as shown below.\n\nStep 1\n\nFirst of all, list out the files in the directory named pig_output using the ls command as shown below.\n\nhdfs dfs -ls 'hdfs://localhost:9000/pig_Output/' Found 2 items rw-r--r- 1 Hadoop supergroup 0 2015-10-05 13:03 hdfs://localhost:9000/pig_Output/_SUCCESS rw-r--r- 1 Hadoop supergroup 224 2015-10-05 13:03 hdfs://localhost:9000/pig_Output/part-m-00000\n\nYou can observe that two files were created after executing the store statement.\n\nStep 2\n\nUsing cat command, list the contents of the file named part-m-00000 as shown below.\n\n$ hdfs dfs -cat 'hdfs://localhost:9000/pig_Output/part-m-00000' 1,Rajiv,Reddy,9848022337,Hyderabad 2,siddarth,Battacharya,9848022338,Kolkata 3,Rajesh,Khanna,9848022339,Delhi 4,Preethi,Agarwal,9848022330,Pune 5,Trupthi,Mohanthy,9848022336,Bhuwaneshwar 6,Archana,Mishra,9848022335,Chennai\n\nApache Pig - Diagnostic Operators\n\nThe load statement will simply load the data into the specified relation in Apache Pig. To verify the execution of the Load statement, you have to use the Diagnostic Operators. Pig Latin provides four different types of diagnostic operators −\n\nDump operator\n\nDescribe operator\n\nExplanation operator\n\nIllustration operator\n\nIn this chapter, we will discuss the Dump operators of Pig Latin.\n\nDump Operator\n\nThe Dump operator is used to run the Pig Latin statements and display the results on the screen. It is generally used for debugging Purpose.\n\nSyntax\n\nGiven below is the syntax of the Dump operator.\n\ngrunt> Dump Relation_Name\n\nExample\n\nAssume we have a file student_data.txt in HDFS with the following content.\n\n001,Rajiv,Reddy,9848022337,Hyderabad 002,siddarth,Battacharya,9848022338,Kolkata 003,Rajesh,Khanna,9848022339,Delhi 004,Preethi,Agarwal,9848022330,Pune 005,Trupthi,Mohanthy,9848022336,Bhuwaneshwar 006,Archana,Mishra,9848022335,Chennai.\n\nAnd we have read it into a relation student using the LOAD operator as shown below.\n\ngrunt> student = LOAD 'hdfs://localhost:9000/pig_data/student_data.txt' USING PigStorage(',') as ( id:int, firstname:chararray, lastname:chararray, phone:chararray, city:chararray );\n\nNow, let us print the contents of the relation using the Dump operator as shown below.\n\ngrunt> Dump student\n\nOnce you execute the above Pig Latin statement, it will start a MapReduce job to read data from HDFS. It will produce the following output.\n\n2015-10-01 15:05:27,642 [main] INFO org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.MapReduceLauncher - 100% complete 2015-10-01 15:05:27,652 [main] INFO org.apache.pig.tools.pigstats.mapreduce.SimplePigStats - Script Statistics: HadoopVersion PigVersion UserId StartedAt FinishedAt Features 2.6.0 0.15.0 Hadoop 2015-10-01 15:03:11 2015-10-01 05:27 UNKNOWN Success! Job Stats (time in seconds): JobId job_14459_0004 Maps 1 Reduces 0 MaxMapTime n/a MinMapTime n/a AvgMapTime n/a MedianMapTime n/a MaxReduceTime 0 MinReduceTime 0 AvgReduceTime 0 MedianReducetime 0 Alias student Feature MAP_ONLY Outputs hdfs://localhost:9000/tmp/temp580182027/tmp757878456, Input(s): Successfully read 0 records from: \"hdfs://localhost:9000/pig_data/ student_data.txt\" Output(s): Successfully stored 0 records in: \"hdfs://localhost:9000/tmp/temp580182027/ tmp757878456\" Counters: Total records written : 0 Total bytes written : 0 Spillable Memory Manager spill count : 0Total bags proactively spilled: 0 Total records proactively spilled: 0 Job DAG: job_1443519499159_0004 2015-10-01 15:06:28,403 [main] INFO org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.MapReduceLau ncher - Success! 2015-10-01 15:06:28,441 [main] INFO org.apache.pig.data.SchemaTupleBackend - Key [pig.schematuple] was not set... will not generate code. 2015-10-01 15:06:28,485 [main] INFO org.apache.hadoop.mapreduce.lib.input.FileInputFormat - Total input paths to process : 1 2015-10-01 15:06:28,485 [main] INFO org.apache.pig.backend.hadoop.executionengine.util.MapRedUtil - Total input paths to process : 1 (1,Rajiv,Reddy,9848022337,Hyderabad) (2,siddarth,Battacharya,9848022338,Kolkata) (3,Rajesh,Khanna,9848022339,Delhi) (4,Preethi,Agarwal,9848022330,Pune) (5,Trupthi,Mohanthy,9848022336,Bhuwaneshwar) (6,Archana,Mishra,9848022335,Chennai)\n\nApache Pig - Describe Operator\n\nThe describe operator is used to view the schema of a relation.\n\nSyntax\n\nThe syntax of the describe operator is as follows −\n\ngrunt> Describe Relation_name\n\nExample\n\nAssume we have a file student_data.txt in HDFS with the following content.\n\n001,Rajiv,Reddy,9848022337,Hyderabad 002,siddarth,Battacharya,9848022338,Kolkata 003,Rajesh,Khanna,9848022339,Delhi 004,Preethi,Agarwal,9848022330,Pune 005,Trupthi,Mohanthy,9848022336,Bhuwaneshwar 006,Archana,Mishra,9848022335,Chennai.\n\nAnd we have read it into a relation student using the LOAD operator as shown below.\n\ngrunt> student = LOAD 'hdfs://localhost:9000/pig_data/student_data.txt' USING PigStorage(',') as ( id:int, firstname:chararray, lastname:chararray, phone:chararray, city:chararray );\n\nNow, let us describe the relation named student and verify the schema as shown below.\n\ngrunt> describe student;\n\nOutput\n\nOnce you execute the above Pig Latin statement, it will produce the following output.\n\ngrunt> student: { id: int,firstname: chararray,lastname: chararray,phone: chararray,city: chararray }\n\nApache Pig - Explain Operator\n\nThe explain operator is used to display the logical, physical, and MapReduce execution plans of a relation.\n\nSyntax\n\nGiven below is the syntax of the explain operator.\n\ngrunt> explain Relation_name;\n\nExample\n\nAssume we have a file student_data.txt in HDFS with the following content.\n\n001,Rajiv,Reddy,9848022337,Hyderabad 002,siddarth,Battacharya,9848022338,Kolkata 003,Rajesh,Khanna,9848022339,Delhi 004,Preethi,Agarwal,9848022330,Pune 005,Trupthi,Mohanthy,9848022336,Bhuwaneshwar 006,Archana,Mishra,9848022335,Chennai.\n\nAnd we have read it into a relation student using the LOAD operator as shown below.\n\ngrunt> student = LOAD 'hdfs://localhost:9000/pig_data/student_data.txt' USING PigStorage(',') as ( id:int, firstname:chararray, lastname:chararray, phone:chararray, city:chararray );\n\nNow, let us explain the relation named student using the explain operator as shown below.\n\ngrunt> explain student;\n\nOutput\n\nIt will produce the following output.\n\n$ explain student; 2015-10-05 11:32:43,660 [main] 2015-10-05 11:32:43,660 [main] INFO org.apache.pig.newplan.logical.optimizer .LogicalPlanOptimizer - {RULES_ENABLED=[AddForEach, ColumnMapKeyPrune, ConstantCalculator, GroupByConstParallelSetter, LimitOptimizer, LoadTypeCastInserter, MergeFilter, MergeForEach, PartitionFilterOptimizer, PredicatePushdownOptimizer, PushDownForEachFlatten, PushUpFilter, SplitFilter, StreamTypeCastInserter]} #----------------------------------------------- # New Logical Plan: #----------------------------------------------- student: (Name: LOStore Schema: id#31:int,firstname#32:chararray,lastname#33:chararray,phone#34:chararray,city# 35:chararray) | |---student: (Name: LOForEach Schema: id#31:int,firstname#32:chararray,lastname#33:chararray,phone#34:chararray,city# 35:chararray) | | | (Name: LOGenerate[false,false,false,false,false] Schema: id#31:int,firstname#32:chararray,lastname#33:chararray,phone#34:chararray,city# 35:chararray)ColumnPrune:InputUids=[34, 35, 32, 33, 31]ColumnPrune:OutputUids=[34, 35, 32, 33, 31] | | | | | (Name: Cast Type: int Uid: 31) | | | | | |---id:(Name: Project Type: bytearray Uid: 31 Input: 0 Column: (*)) | | | | | (Name: Cast Type: chararray Uid: 32) | | | | | |---firstname:(Name: Project Type: bytearray Uid: 32 Input: 1 Column: (*)) | | | | | (Name: Cast Type: chararray Uid: 33) | | | | | |---lastname:(Name: Project Type: bytearray Uid: 33 Input: 2 Column: (*)) | | | | | (Name: Cast Type: chararray Uid: 34) | | | | | |---phone:(Name: Project Type: bytearray Uid: 34 Input: 3 Column: (*)) | | | | | (Name: Cast Type: chararray Uid: 35) | | | | | |---city:(Name: Project Type: bytearray Uid: 35 Input: 4 Column: (*)) | | | |---(Name: LOInnerLoad[0] Schema: id#31:bytearray) | | | |---(Name: LOInnerLoad[1] Schema: firstname#32:bytearray) | | | |---(Name: LOInnerLoad[2] Schema: lastname#33:bytearray) | | | |---(Name: LOInnerLoad[3] Schema: phone#34:bytearray) | | | |---(Name: LOInnerLoad[4] Schema: city#35:bytearray) | |---student: (Name: LOLoad Schema: id#31:bytearray,firstname#32:bytearray,lastname#33:bytearray,phone#34:bytearray ,city#35:bytearray)RequiredFields:null #----------------------------------------------- # Physical Plan: #----------------------------------------------- student: Store(fakefile:org.apache.pig.builtin.PigStorage) - scope-36 | |---student: New For Each(false,false,false,false,false)[bag] - scope-35 | | | Cast[int] - scope-21 | | | |---Project[bytearray][0] - scope-20 | | | Cast[chararray] - scope-24 | | | |---Project[bytearray][1] - scope-23 | | | Cast[chararray] - scope-27 | | | |---Project[bytearray][2] - scope-26 | | | Cast[chararray] - scope-30 | | | |---Project[bytearray][3] - scope-29 | | | Cast[chararray] - scope-33 | | | |---Project[bytearray][4] - scope-32 | |---student: Load(hdfs://localhost:9000/pig_data/student_data.txt:PigStorage(',')) - scope19 2015-10-05 11:32:43,682 [main] INFO org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.MRCompiler - File concatenation threshold: 100 optimistic? false 2015-10-05 11:32:43,684 [main] INFO org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.MultiQueryOp timizer - MR plan size before optimization: 1 2015-10-05 11:32:43,685 [main] INFO org.apache.pig.backend.hadoop.executionengine.mapReduceLayer. MultiQueryOp timizer - MR plan size after optimization: 1 #-------------------------------------------------- # Map Reduce Plan #-------------------------------------------------- MapReduce node scope-37 Map Plan student: Store(fakefile:org.apache.pig.builtin.PigStorage) - scope-36 | |---student: New For Each(false,false,false,false,false)[bag] - scope-35 | | | Cast[int] - scope-21 | | | |---Project[bytearray][0] - scope-20 | | | Cast[chararray] - scope-24 | | | |---Project[bytearray][1] - scope-23 | | | Cast[chararray] - scope-27 | | | |---Project[bytearray][2] - scope-26 | | | Cast[chararray] - scope-30 | | | |---Project[bytearray][3] - scope-29 | | | Cast[chararray] - scope-33 | | | |---Project[bytearray][4] - scope-32 | |---student: Load(hdfs://localhost:9000/pig_data/student_data.txt:PigStorage(',')) - scope 19-------- Global sort: false ----------------\n\nApache Pig - Illustrate Operator\n\nThe illustrate operator gives you the step-by-step execution of a sequence of statements.\n\nSyntax\n\nGiven below is the syntax of the illustrate operator.\n\ngrunt> illustrate Relation_name;\n\nExample\n\nAssume we have a file student_data.txt in HDFS with the following content.\n\n001,Rajiv,Reddy,9848022337,Hyderabad 002,siddarth,Battacharya,9848022338,Kolkata 003,Rajesh,Khanna,9848022339,Delhi 004,Preethi,Agarwal,9848022330,Pune 005,Trupthi,Mohanthy,9848022336,Bhuwaneshwar 006,Archana,Mishra,9848022335,Chennai.\n\nAnd we have read it into a relation student using the LOAD operator as shown below.\n\ngrunt> student = LOAD 'hdfs://localhost:9000/pig_data/student_data.txt' USING PigStorage(',') as ( id:int, firstname:chararray, lastname:chararray, phone:chararray, city:chararray );\n\nNow, let us illustrate the relation named student as shown below.\n\ngrunt> illustrate student;\n\nOutput\n\nOn executing the above statement, you will get the following output.\n\ngrunt> illustrate student; INFO org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.PigMapOnly$M ap - Aliases being processed per job phase (AliasName[line,offset]): M: student[1,10] C: R: --------------------------------------------------------------------------------------------- |student | id:int | firstname:chararray | lastname:chararray | phone:chararray | city:chararray | --------------------------------------------------------------------------------------------- | | 002 | siddarth | Battacharya | 9848022338 | Kolkata | ---------------------------------------------------------------------------------------------\n\nApache Pig - Group Operator\n\nThe GROUP operator is used to group the data in one or more relations. It collects the data having the same key.\n\nSyntax\n\nGiven below is the syntax of the group operator.\n\ngrunt> Group_data = GROUP Relation_name BY age;\n\nExample\n\nAssume that we have a file named student_details.txt in the HDFS directory /pig_data/ as shown below.\n\nstudent_details.txt\n\n001,Rajiv,Reddy,21,9848022337,Hyderabad 002,siddarth,Battacharya,22,9848022338,Kolkata 003,Rajesh,Khanna,22,9848022339,Delhi 004,Preethi,Agarwal,21,9848022330,Pune 005,Trupthi,Mohanthy,23,9848022336,Bhuwaneshwar 006,Archana,Mishra,23,9848022335,Chennai 007,Komal,Nayak,24,9848022334,trivendram 008,Bharathi,Nambiayar,24,9848022333,Chennai\n\nAnd we have loaded this file into Apache Pig with the relation name student_details as shown below.\n\ngrunt> student_details = LOAD 'hdfs://localhost:9000/pig_data/student_details.txt' USING PigStorage(',') as (id:int, firstname:chararray, lastname:chararray, age:int, phone:chararray, city:chararray);\n\nNow, let us group the records/tuples in the relation by age as shown below.\n\ngrunt> group_data = GROUP student_details by age;\n\nVerification\n\nVerify the relation group_data using the DUMP operator as shown below.\n\ngrunt> Dump group_data;\n\nOutput\n\nThen you will get output displaying the contents of the relation named group_data as shown below. Here you can observe that the resulting schema has two columns −\n\nOne is age, by which we have grouped the relation.\n\nThe other is a bag, which contains the group of tuples, student records with the respective age.\n\n(21,{(4,Preethi,Agarwal,21,9848022330,Pune),(1,Rajiv,Reddy,21,9848022337,Hydera bad)}) (22,{(3,Rajesh,Khanna,22,9848022339,Delhi),(2,siddarth,Battacharya,22,984802233 8,Kolkata)}) (23,{(6,Archana,Mishra,23,9848022335,Chennai),(5,Trupthi,Mohanthy,23,9848022336 ,Bhuwaneshwar)}) (24,{(8,Bharathi,Nambiayar,24,9848022333,Chennai),(7,Komal,Nayak,24,9848022334, trivendram)})\n\nYou can see the schema of the table after grouping the data using the describe command as shown below.\n\ngrunt> Describe group_data; group_data: {group: int,student_details: {(id: int,firstname: chararray, lastname: chararray,age: int,phone: chararray,city: chararray)}}\n\nIn the same way, you can get the sample illustration of the schema using the illustrate command as shown below.\n\n$ Illustrate group_data;\n\nIt will produce the following output −\n\n------------------------------------------------------------------------------------------------- |group_data| group:int | student_details:bag{:tuple(id:int,firstname:chararray,lastname:chararray,age:int,phone:chararray,city:chararray)}| ------------------------------------------------------------------------------------------------- | | 21 | { 4, Preethi, Agarwal, 21, 9848022330, Pune), (1, Rajiv, Reddy, 21, 9848022337, Hyderabad)}| | | 2 | {(2,siddarth,Battacharya,22,9848022338,Kolkata),(003,Rajesh,Khanna,22,9848022339,Delhi)}| -------------------------------------------------------------------------------------------------\n\nGrouping by Multiple Columns\n\nLet us group the relation by age and city as shown below.\n\ngrunt> group_multiple = GROUP student_details by (age, city);\n\nYou can verify the content of the relation named group_multiple using the Dump operator as shown below.\n\ngrunt> Dump group_multiple; ((21,Pune),{(4,Preethi,Agarwal,21,9848022330,Pune)}) ((21,Hyderabad),{(1,Rajiv,Reddy,21,9848022337,Hyderabad)}) ((22,Delhi),{(3,Rajesh,Khanna,22,9848022339,Delhi)}) ((22,Kolkata),{(2,siddarth,Battacharya,22,9848022338,Kolkata)}) ((23,Chennai),{(6,Archana,Mishra,23,9848022335,Chennai)}) ((23,Bhuwaneshwar),{(5,Trupthi,Mohanthy,23,9848022336,Bhuwaneshwar)}) ((24,Chennai),{(8,Bharathi,Nambiayar,24,9848022333,Chennai)}) (24,trivendram),{(7,Komal,Nayak,24,9848022334,trivendram)})\n\nGroup All\n\nYou can group a relation by all the columns as shown below.\n\ngrunt> group_all = GROUP student_details All;\n\nNow, verify the content of the relation group_all as shown below.\n\ngrunt> Dump group_all; (all,{(8,Bharathi,Nambiayar,24,9848022333,Chennai),(7,Komal,Nayak,24,9848022334 ,trivendram), (6,Archana,Mishra,23,9848022335,Chennai),(5,Trupthi,Mohanthy,23,9848022336,Bhuw aneshwar), (4,Preethi,Agarwal,21,9848022330,Pune),(3,Rajesh,Khanna,22,9848022339,Delhi), (2,siddarth,Battacharya,22,9848022338,Kolkata),(1,Rajiv,Reddy,21,9848022337,Hyd erabad)})\n\nApache Pig - Cogroup Operator\n\nThe COGROUP operator works more or less in the same way as the GROUP operator. The only difference between the two operators is that the group operator is normally used with one relation, while the cogroup operator is used in statements involving two or more relations.\n\nGrouping Two Relations using Cogroup\n\nAssume that we have two files namely student_details.txt and employee_details.txt in the HDFS directory /pig_data/ as shown below.\n\nstudent_details.txt\n\n001,Rajiv,Reddy,21,9848022337,Hyderabad 002,siddarth,Battacharya,22,9848022338,Kolkata 003,Rajesh,Khanna,22,9848022339,Delhi 004,Preethi,Agarwal,21,9848022330,Pune 005,Trupthi,Mohanthy,23,9848022336,Bhuwaneshwar 006,Archana,Mishra,23,9848022335,Chennai 007,Komal,Nayak,24,9848022334,trivendram 008,Bharathi,Nambiayar,24,9848022333,Chennai\n\nemployee_details.txt\n\n001,Robin,22,newyork 002,BOB,23,Kolkata 003,Maya,23,Tokyo 004,Sara,25,London 005,David,23,Bhuwaneshwar 006,Maggy,22,Chennai\n\nAnd we have loaded these files into Pig with the relation names student_details and employee_details respectively, as shown below.\n\ngrunt> student_details = LOAD 'hdfs://localhost:9000/pig_data/student_details.txt' USING PigStorage(',') as (id:int, firstname:chararray, lastname:chararray, age:int, phone:chararray, city:chararray); grunt> employee_details = LOAD 'hdfs://localhost:9000/pig_data/employee_details.txt' USING PigStorage(',') as (id:int, name:chararray, age:int, city:chararray);\n\nNow, let us group the records/tuples of the relations student_details and employee_details with the key age, as shown below.\n\ngrunt> cogroup_data = COGROUP student_details by age, employee_details by age;\n\nVerification\n\nVerify the relation cogroup_data using the DUMP operator as shown below.\n\ngrunt> Dump cogroup_data;\n\nOutput\n\nIt will produce the following output, displaying the contents of the relation named cogroup_data as shown below.\n\n(21,{(4,Preethi,Agarwal,21,9848022330,Pune), (1,Rajiv,Reddy,21,9848022337,Hyderabad)}, { }) (22,{ (3,Rajesh,Khanna,22,9848022339,Delhi), (2,siddarth,Battacharya,22,9848022338,Kolkata) }, { (6,Maggy,22,Chennai),(1,Robin,22,newyork) }) (23,{(6,Archana,Mishra,23,9848022335,Chennai),(5,Trupthi,Mohanthy,23,9848022336 ,Bhuwaneshwar)}, {(5,David,23,Bhuwaneshwar),(3,Maya,23,Tokyo),(2,BOB,23,Kolkata)}) (24,{(8,Bharathi,Nambiayar,24,9848022333,Chennai),(7,Komal,Nayak,24,9848022334, trivendram)}, { }) (25,{ }, {(4,Sara,25,London)})\n\nThe cogroup operator groups the tuples from each relation according to age where each group depicts a particular age value.\n\nFor example, if we consider the 1st tuple of the result, it is grouped by age 21. And it contains two bags −\n\nthe first bag holds all the tuples from the first relation (student_details in this case) having age 21, and\n\nthe second bag contains all the tuples from the second relation (employee_details in this case) having age 21.\n\nIn case a relation doesn’t have tuples having the age value 21, it returns an empty bag.\n\nApache Pig - Join Operator\n\nThe JOIN operator is used to combine records from two or more relations. While performing a join operation, we declare one (or a group of) tuple(s) from each relation, as keys. When these keys match, the two particular tuples are matched, else the records are dropped. Joins can be of the following types −\n\nSelf-join\n\nInner-join\n\nOuter-join − left join, right join, and full join\n\nThis chapter explains with examples how to use the join operator in Pig Latin. Assume that we have two files namely customers.txt and orders.txt in the /pig_data/ directory of HDFS as shown below.\n\ncustomers.txt\n\n1,Ramesh,32,Ahmedabad,2000.00 2,Khilan,25,Delhi,1500.00 3,kaushik,23,Kota,2000.00 4,Chaitali,25,Mumbai,6500.00 5,Hardik,27,Bhopal,8500.00 6,Komal,22,MP,4500.00 7,Muffy,24,Indore,10000.00\n\norders.txt\n\n102,2009-10-08 00:00:00,3,3000 100,2009-10-08 00:00:00,3,1500 101,2009-11-20 00:00:00,2,1560 103,2008-05-20 00:00:00,4,2060\n\nAnd we have loaded these two files into Pig with the relations customers and orders as shown below.\n\ngrunt> customers = LOAD 'hdfs://localhost:9000/pig_data/customers.txt' USING PigStorage(',') as (id:int, name:chararray, age:int, address:chararray, salary:int); grunt> orders = LOAD 'hdfs://localhost:9000/pig_data/orders.txt' USING PigStorage(',') as (oid:int, date:chararray, customer_id:int, amount:int);\n\nLet us now perform various Join operations on these two relations.\n\nSelf - join\n\nSelf-join is used to join a table with itself as if the table were two relations, temporarily renaming at least one relation.\n\nGenerally, in Apache Pig, to perform self-join, we will load the same data multiple times, under different aliases (names). Therefore let us load the contents of the file customers.txt as two tables as shown below.\n\ngrunt> customers1 = LOAD 'hdfs://localhost:9000/pig_data/customers.txt' USING PigStorage(',') as (id:int, name:chararray, age:int, address:chararray, salary:int); grunt> customers2 = LOAD 'hdfs://localhost:9000/pig_data/customers.txt' USING PigStorage(',') as (id:int, name:chararray, age:int, address:chararray, salary:int);\n\nSyntax\n\nGiven below is the syntax of performing self-join operation using the JOIN operator.\n\ngrunt> Relation3_name = JOIN Relation1_name BY key, Relation2_name BY key ;\n\nExample\n\nLet us perform self-join operation on the relation customers, by joining the two relations customers1 and customers2 as shown below.\n\ngrunt> customers3 = JOIN customers1 BY id, customers2 BY id;\n\nVerification\n\nVerify the relation customers3 using the DUMP operator as shown below.\n\ngrunt> Dump customers3;\n\nOutput\n\nIt will produce the following output, displaying the contents of the relation customers.\n\n(1,Ramesh,32,Ahmedabad,2000,1,Ramesh,32,Ahmedabad,2000) (2,Khilan,25,Delhi,1500,2,Khilan,25,Delhi,1500) (3,kaushik,23,Kota,2000,3,kaushik,23,Kota,2000) (4,Chaitali,25,Mumbai,6500,4,Chaitali,25,Mumbai,6500) (5,Hardik,27,Bhopal,8500,5,Hardik,27,Bhopal,8500) (6,Komal,22,MP,4500,6,Komal,22,MP,4500) (7,Muffy,24,Indore,10000,7,Muffy,24,Indore,10000)\n\nInner Join\n\nInner Join is used quite frequently; it is also referred to as equijoin. An inner join returns rows when there is a match in both tables.\n\nIt creates a new relation by combining column values of two relations (say A and B) based upon the join-predicate. The query compares each row of A with each row of B to find all pairs of rows which satisfy the join-predicate. When the join-predicate is satisfied, the column values for each matched pair of rows of A and B are combined into a result row.\n\nSyntax\n\nHere is the syntax of performing inner join operation using the JOIN operator.\n\ngrunt> result = JOIN relation1 BY columnname, relation2 BY columnname;\n\nExample\n\nLet us perform inner join operation on the two relations customers and orders as shown below.\n\ngrunt> coustomer_orders = JOIN customers BY id, orders BY customer_id;\n\nVerification\n\nVerify the relation coustomer_orders using the DUMP operator as shown below.\n\ngrunt> Dump coustomer_orders;\n\nOutput\n\nYou will get the following output that will the contents of the relation named coustomer_orders.\n\n(2,Khilan,25,Delhi,1500,101,2009-11-20 00:00:00,2,1560) (3,kaushik,23,Kota,2000,100,2009-10-08 00:00:00,3,1500) (3,kaushik,23,Kota,2000,102,2009-10-08 00:00:00,3,3000) (4,Chaitali,25,Mumbai,6500,103,2008-05-20 00:00:00,4,2060)\n\nNote −\n\nOuter Join: Unlike inner join, outer join returns all the rows from at least one of the relations. An outer join operation is carried out in three ways −\n\nLeft outer join\n\nRight outer join\n\nFull outer join\n\nLeft Outer Join\n\nThe left outer Join operation returns all rows from the left table, even if there are no matches in the right relation.\n\nSyntax\n\nGiven below is the syntax of performing left outer join operation using the JOIN operator.\n\ngrunt> Relation3_name = JOIN Relation1_name BY id LEFT OUTER, Relation2_name BY customer_id;\n\nExample\n\nLet us perform left outer join operation on the two relations customers and orders as shown below.\n\ngrunt> outer_left = JOIN customers BY id LEFT OUTER, orders BY customer_id;\n\nVerification\n\nVerify the relation outer_left using the DUMP operator as shown below.\n\ngrunt> Dump outer_left;\n\nOutput\n\nIt will produce the following output, displaying the contents of the relation outer_left.\n\n(1,Ramesh,32,Ahmedabad,2000,,,,) (2,Khilan,25,Delhi,1500,101,2009-11-20 00:00:00,2,1560) (3,kaushik,23,Kota,2000,100,2009-10-08 00:00:00,3,1500) (3,kaushik,23,Kota,2000,102,2009-10-08 00:00:00,3,3000) (4,Chaitali,25,Mumbai,6500,103,2008-05-20 00:00:00,4,2060) (5,Hardik,27,Bhopal,8500,,,,) (6,Komal,22,MP,4500,,,,) (7,Muffy,24,Indore,10000,,,,)\n\nRight Outer Join\n\nThe right outer join operation returns all rows from the right table, even if there are no matches in the left table.\n\nSyntax\n\nGiven below is the syntax of performing right outer join operation using the JOIN operator.\n\ngrunt> outer_right = JOIN customers BY id RIGHT, orders BY customer_id;\n\nExample\n\nLet us perform right outer join operation on the two relations customers and orders as shown below.\n\ngrunt> outer_right = JOIN customers BY id RIGHT, orders BY customer_id;\n\nVerification\n\nVerify the relation outer_right using the DUMP operator as shown below.\n\ngrunt> Dump outer_right\n\nOutput\n\nIt will produce the following output, displaying the contents of the relation outer_right.\n\n(2,Khilan,25,Delhi,1500,101,2009-11-20 00:00:00,2,1560) (3,kaushik,23,Kota,2000,100,2009-10-08 00:00:00,3,1500) (3,kaushik,23,Kota,2000,102,2009-10-08 00:00:00,3,3000) (4,Chaitali,25,Mumbai,6500,103,2008-05-20 00:00:00,4,2060)\n\nFull Outer Join\n\nThe full outer join operation returns rows when there is a match in one of the relations.\n\nSyntax\n\nGiven below is the syntax of performing full outer join using the JOIN operator.\n\ngrunt> outer_full = JOIN customers BY id FULL OUTER, orders BY customer_id;\n\nExample\n\nLet us perform full outer join operation on the two relations customers and orders as shown below.\n\ngrunt> outer_full = JOIN customers BY id FULL OUTER, orders BY customer_id;\n\nVerification\n\nVerify the relation outer_full using the DUMP operator as shown below.\n\ngrun> Dump outer_full;\n\nOutput\n\nIt will produce the following output, displaying the contents of the relation outer_full.\n\n(1,Ramesh,32,Ahmedabad,2000,,,,) (2,Khilan,25,Delhi,1500,101,2009-11-20 00:00:00,2,1560) (3,kaushik,23,Kota,2000,100,2009-10-08 00:00:00,3,1500) (3,kaushik,23,Kota,2000,102,2009-10-08 00:00:00,3,3000) (4,Chaitali,25,Mumbai,6500,103,2008-05-20 00:00:00,4,2060) (5,Hardik,27,Bhopal,8500,,,,) (6,Komal,22,MP,4500,,,,) (7,Muffy,24,Indore,10000,,,,)\n\nUsing Multiple Keys\n\nWe can perform JOIN operation using multiple keys.\n\nSyntax\n\nHere is how you can perform a JOIN operation on two tables using multiple keys.\n\ngrunt> Relation3_name = JOIN Relation2_name BY (key1, key2), Relation3_name BY (key1, key2);\n\nAssume that we have two files namely employee.txt and employee_contact.txt in the /pig_data/ directory of HDFS as shown below.\n\nemployee.txt\n\n001,Rajiv,Reddy,21,programmer,003 002,siddarth,Battacharya,22,programmer,003 003,Rajesh,Khanna,22,programmer,003 004,Preethi,Agarwal,21,programmer,003 005,Trupthi,Mohanthy,23,programmer,003 006,Archana,Mishra,23,programmer,003 007,Komal,Nayak,24,teamlead,002 008,Bharathi,Nambiayar,24,manager,001\n\nemployee_contact.txt\n\n001,9848022337,Rajiv@gmail.com,Hyderabad,003 002,9848022338,siddarth@gmail.com,Kolkata,003 003,9848022339,Rajesh@gmail.com,Delhi,003 004,9848022330,Preethi@gmail.com,Pune,003 005,9848022336,Trupthi@gmail.com,Bhuwaneshwar,003 006,9848022335,Archana@gmail.com,Chennai,003 007,9848022334,Komal@gmail.com,trivendram,002 008,9848022333,Bharathi@gmail.com,Chennai,001\n\nAnd we have loaded these two files into Pig with relations employee and employee_contact as shown below.\n\ngrunt> employee = LOAD 'hdfs://localhost:9000/pig_data/employee.txt' USING PigStorage(',') as (id:int, firstname:chararray, lastname:chararray, age:int, designation:chararray, jobid:int); grunt> employee_contact = LOAD 'hdfs://localhost:9000/pig_data/employee_contact.txt' USING PigStorage(',') as (id:int, phone:chararray, email:chararray, city:chararray, jobid:int);\n\nNow, let us join the contents of these two relations using the JOIN operator as shown below.\n\ngrunt> emp = JOIN employee BY (id,jobid), employee_contact BY (id,jobid);\n\nVerification\n\nVerify the relation emp using the DUMP operator as shown below.\n\ngrunt> Dump emp;\n\nOutput\n\nIt will produce the following output, displaying the contents of the relation named emp as shown below.\n\n(1,Rajiv,Reddy,21,programmer,113,1,9848022337,Rajiv@gmail.com,Hyderabad,113) (2,siddarth,Battacharya,22,programmer,113,2,9848022338,siddarth@gmail.com,Kolka ta,113) (3,Rajesh,Khanna,22,programmer,113,3,9848022339,Rajesh@gmail.com,Delhi,113) (4,Preethi,Agarwal,21,programmer,113,4,9848022330,Preethi@gmail.com,Pune,113) (5,Trupthi,Mohanthy,23,programmer,113,5,9848022336,Trupthi@gmail.com,Bhuwaneshw ar,113) (6,Archana,Mishra,23,programmer,113,6,9848022335,Archana@gmail.com,Chennai,113) (7,Komal,Nayak,24,teamlead,112,7,9848022334,Komal@gmail.com,trivendram,112) (8,Bharathi,Nambiayar,24,manager,111,8,9848022333,Bharathi@gmail.com,Chennai,111)\n\nApache Pig - Cross Operator\n\nThe CROSS operator computes the cross-product of two or more relations. This chapter explains with example how to use the cross operator in Pig Latin.\n\nSyntax\n\nGiven below is the syntax of the CROSS operator.\n\ngrunt> Relation3_name = CROSS Relation1_name, Relation2_name;\n\nExample\n\nAssume that we have two files namely customers.txt and orders.txt in the /pig_data/ directory of HDFS as shown below.\n\ncustomers.txt\n\n1,Ramesh,32,Ahmedabad,2000.00 2,Khilan,25,Delhi,1500.00 3,kaushik,23,Kota,2000.00 4,Chaitali,25,Mumbai,6500.00 5,Hardik,27,Bhopal,8500.00 6,Komal,22,MP,4500.00 7,Muffy,24,Indore,10000.00\n\norders.txt\n\n102,2009-10-08 00:00:00,3,3000 100,2009-10-08 00:00:00,3,1500 101,2009-11-20 00:00:00,2,1560 103,2008-05-20 00:00:00,4,2060\n\nAnd we have loaded these two files into Pig with the relations customers and orders as shown below.\n\ngrunt> customers = LOAD 'hdfs://localhost:9000/pig_data/customers.txt' USING PigStorage(',') as (id:int, name:chararray, age:int, address:chararray, salary:int); grunt> orders = LOAD 'hdfs://localhost:9000/pig_data/orders.txt' USING PigStorage(',') as (oid:int, date:chararray, customer_id:int, amount:int);\n\nLet us now get the cross-product of these two relations using the cross operator on these two relations as shown below.\n\ngrunt> cross_data = CROSS customers, orders;\n\nVerification\n\nVerify the relation cross_data using the DUMP operator as shown below.\n\ngrunt> Dump cross_data;\n\nOutput\n\nIt will produce the following output, displaying the contents of the relation cross_data.\n\n(7,Muffy,24,Indore,10000,103,2008-05-20 00:00:00,4,2060) (7,Muffy,24,Indore,10000,101,2009-11-20 00:00:00,2,1560) (7,Muffy,24,Indore,10000,100,2009-10-08 00:00:00,3,1500) (7,Muffy,24,Indore,10000,102,2009-10-08 00:00:00,3,3000) (6,Komal,22,MP,4500,103,2008-05-20 00:00:00,4,2060) (6,Komal,22,MP,4500,101,2009-11-20 00:00:00,2,1560) (6,Komal,22,MP,4500,100,2009-10-08 00:00:00,3,1500) (6,Komal,22,MP,4500,102,2009-10-08 00:00:00,3,3000) (5,Hardik,27,Bhopal,8500,103,2008-05-20 00:00:00,4,2060) (5,Hardik,27,Bhopal,8500,101,2009-11-20 00:00:00,2,1560) (5,Hardik,27,Bhopal,8500,100,2009-10-08 00:00:00,3,1500) (5,Hardik,27,Bhopal,8500,102,2009-10-08 00:00:00,3,3000) (4,Chaitali,25,Mumbai,6500,103,2008-05-20 00:00:00,4,2060) (4,Chaitali,25,Mumbai,6500,101,2009-20 00:00:00,4,2060) (2,Khilan,25,Delhi,1500,101,2009-11-20 00:00:00,2,1560) (2,Khilan,25,Delhi,1500,100,2009-10-08 00:00:00,3,1500) (2,Khilan,25,Delhi,1500,102,2009-10-08 00:00:00,3,3000) (1,Ramesh,32,Ahmedabad,2000,103,2008-05-20 00:00:00,4,2060) (1,Ramesh,32,Ahmedabad,2000,101,2009-11-20 00:00:00,2,1560) (1,Ramesh,32,Ahmedabad,2000,100,2009-10-08 00:00:00,3,1500) (1,Ramesh,32,Ahmedabad,2000,102,2009-10-08 00:00:00,3,3000)-11-20 00:00:00,2,1560) (4,Chaitali,25,Mumbai,6500,100,2009-10-08 00:00:00,3,1500) (4,Chaitali,25,Mumbai,6500,102,2009-10-08 00:00:00,3,3000) (3,kaushik,23,Kota,2000,103,2008-05-20 00:00:00,4,2060) (3,kaushik,23,Kota,2000,101,2009-11-20 00:00:00,2,1560) (3,kaushik,23,Kota,2000,100,2009-10-08 00:00:00,3,1500) (3,kaushik,23,Kota,2000,102,2009-10-08 00:00:00,3,3000) (2,Khilan,25,Delhi,1500,103,2008-05-20 00:00:00,4,2060) (2,Khilan,25,Delhi,1500,101,2009-11-20 00:00:00,2,1560) (2,Khilan,25,Delhi,1500,100,2009-10-08 00:00:00,3,1500) (2,Khilan,25,Delhi,1500,102,2009-10-08 00:00:00,3,3000) (1,Ramesh,32,Ahmedabad,2000,103,2008-05-20 00:00:00,4,2060) (1,Ramesh,32,Ahmedabad,2000,101,2009-11-20 00:00:00,2,1560) (1,Ramesh,32,Ahmedabad,2000,100,2009-10-08 00:00:00,3,1500) (1,Ramesh,32,Ahmedabad,2000,102,2009-10-08 00:00:00,3,3000)\n\nApache Pig - Union Operator\n\nThe UNION operator of Pig Latin is used to merge the content of two relations. To perform UNION operation on two relations, their columns and domains must be identical.\n\nSyntax\n\nGiven below is the syntax of the UNION operator.\n\ngrunt> Relation_name3 = UNION Relation_name1, Relation_name2;\n\nExample\n\nAssume that we have two files namely student_data1.txt and student_data2.txt in the /pig_data/ directory of HDFS as shown below.\n\nStudent_data1.txt\n\n001,Rajiv,Reddy,9848022337,Hyderabad 002,siddarth,Battacharya,9848022338,Kolkata 003,Rajesh,Khanna,9848022339,Delhi 004,Preethi,Agarwal,9848022330,Pune 005,Trupthi,Mohanthy,9848022336,Bhuwaneshwar 006,Archana,Mishra,9848022335,Chennai.\n\nStudent_data2.txt\n\n7,Komal,Nayak,9848022334,trivendram. 8,Bharathi,Nambiayar,9848022333,Chennai.\n\nAnd we have loaded these two files into Pig with the relations student1 and student2 as shown below.\n\ngrunt> student1 = LOAD 'hdfs://localhost:9000/pig_data/student_data1.txt' USING PigStorage(',') as (id:int, firstname:chararray, lastname:chararray, phone:chararray, city:chararray); grunt> student2 = LOAD 'hdfs://localhost:9000/pig_data/student_data2.txt' USING PigStorage(',') as (id:int, firstname:chararray, lastname:chararray, phone:chararray, city:chararray);\n\nLet us now merge the contents of these two relations using the UNION operator as shown below.\n\ngrunt> student = UNION student1, student2;\n\nVerification\n\nVerify the relation student using the DUMP operator as shown below.\n\ngrunt> Dump student;\n\nOutput\n\nIt will display the following output, displaying the contents of the relation student.\n\n(1,Rajiv,Reddy,9848022337,Hyderabad) (2,siddarth,Battacharya,9848022338,Kolkata) (3,Rajesh,Khanna,9848022339,Delhi) (4,Preethi,Agarwal,9848022330,Pune) (5,Trupthi,Mohanthy,9848022336,Bhuwaneshwar) (6,Archana,Mishra,9848022335,Chennai) (7,Komal,Nayak,9848022334,trivendram) (8,Bharathi,Nambiayar,9848022333,Chennai)\n\nApache Pig - Split Operator\n\nThe SPLIT operator is used to split a relation into two or more relations.\n\nSyntax\n\nGiven below is the syntax of the SPLIT operator.\n\ngrunt> SPLIT Relation1_name INTO Relation2_name IF (condition1), Relation2_name (condition2),\n\nExample\n\nAssume that we have a file named student_details.txt in the HDFS directory /pig_data/ as shown below.\n\nstudent_details.txt\n\n001,Rajiv,Reddy,21,9848022337,Hyderabad 002,siddarth,Battacharya,22,9848022338,Kolkata 003,Rajesh,Khanna,22,9848022339,Delhi 004,Preethi,Agarwal,21,9848022330,Pune 005,Trupthi,Mohanthy,23,9848022336,Bhuwaneshwar 006,Archana,Mishra,23,9848022335,Chennai 007,Komal,Nayak,24,9848022334,trivendram 008,Bharathi,Nambiayar,24,9848022333,Chennai\n\nAnd we have loaded this file into Pig with the relation name student_details as shown below.\n\nstudent_details = LOAD 'hdfs://localhost:9000/pig_data/student_details.txt' USING PigStorage(',') as (id:int, firstname:chararray, lastname:chararray, age:int, phone:chararray, city:chararray);\n\nLet us now split the relation into two, one listing the employees of age less than 23, and the other listing the employees having the age between 22 and 25.\n\nSPLIT student_details into student_details1 if age<23, student_details2 if (22<age and age>25);\n\nVerification\n\nVerify the relations student_details1 and student_details2 using the DUMP operator as shown below.\n\ngrunt> Dump student_details1; grunt> Dump student_details2;\n\nOutput\n\nIt will produce the following output, displaying the contents of the relations student_details1 and student_details2 respectively.\n\ngrunt> Dump student_details1; (1,Rajiv,Reddy,21,9848022337,Hyderabad) (2,siddarth,Battacharya,22,9848022338,Kolkata) (3,Rajesh,Khanna,22,9848022339,Delhi) (4,Preethi,Agarwal,21,9848022330,Pune) grunt> Dump student_details2; (5,Trupthi,Mohanthy,23,9848022336,Bhuwaneshwar) (6,Archana,Mishra,23,9848022335,Chennai) (7,Komal,Nayak,24,9848022334,trivendram) (8,Bharathi,Nambiayar,24,9848022333,Chennai)\n\nApache Pig - Filter Operator\n\nThe FILTER operator is used to select the required tuples from a relation based on a condition.\n\nSyntax\n\nGiven below is the syntax of the FILTER operator.\n\ngrunt> Relation2_name = FILTER Relation1_name BY (condition);\n\nExample\n\nAssume that we have a file named student_details.txt in the HDFS directory /pig_data/ as shown below.\n\nstudent_details.txt\n\n001,Rajiv,Reddy,21,9848022337,Hyderabad 002,siddarth,Battacharya,22,9848022338,Kolkata 003,Rajesh,Khanna,22,9848022339,Delhi 004,Preethi,Agarwal,21,9848022330,Pune 005,Trupthi,Mohanthy,23,9848022336,Bhuwaneshwar 006,Archana,Mishra,23,9848022335,Chennai 007,Komal,Nayak,24,9848022334,trivendram 008,Bharathi,Nambiayar,24,9848022333,Chennai\n\nAnd we have loaded this file into Pig with the relation name student_details as shown below.\n\ngrunt> student_details = LOAD 'hdfs://localhost:9000/pig_data/student_details.txt' USING PigStorage(',') as (id:int, firstname:chararray, lastname:chararray, age:int, phone:chararray, city:chararray);\n\nLet us now use the Filter operator to get the details of the students who belong to the city Chennai.\n\nfilter_data = FILTER student_details BY city == 'Chennai';\n\nVerification\n\nVerify the relation filter_data using the DUMP operator as shown below.\n\ngrunt> Dump filter_data;\n\nOutput\n\nIt will produce the following output, displaying the contents of the relation filter_data as follows.\n\n(6,Archana,Mishra,23,9848022335,Chennai) (8,Bharathi,Nambiayar,24,9848022333,Chennai)\n\nApache Pig - Distinct Operator\n\nThe DISTINCT operator is used to remove redundant (duplicate) tuples from a relation.\n\nSyntax\n\nGiven below is the syntax of the DISTINCT operator.\n\ngrunt> Relation_name2 = DISTINCT Relatin_name1;\n\nExample\n\nAssume that we have a file named student_details.txt in the HDFS directory /pig_data/ as shown below.\n\nstudent_details.txt\n\n001,Rajiv,Reddy,9848022337,Hyderabad 002,siddarth,Battacharya,9848022338,Kolkata 002,siddarth,Battacharya,9848022338,Kolkata 003,Rajesh,Khanna,9848022339,Delhi 003,Rajesh,Khanna,9848022339,Delhi 004,Preethi,Agarwal,9848022330,Pune 005,Trupthi,Mohanthy,9848022336,Bhuwaneshwar 006,Archana,Mishra,9848022335,Chennai 006,Archana,Mishra,9848022335,Chennai\n\nAnd we have loaded this file into Pig with the relation name student_details as shown below.\n\ngrunt> student_details = LOAD 'hdfs://localhost:9000/pig_data/student_details.txt' USING PigStorage(',') as (id:int, firstname:chararray, lastname:chararray, phone:chararray, city:chararray);\n\nLet us now remove the redundant (duplicate) tuples from the relation named student_details using the DISTINCT operator, and store it as another relation named distinct_data as shown below.\n\ngrunt> distinct_data = DISTINCT student_details;\n\nVerification\n\nVerify the relation distinct_data using the DUMP operator as shown below.\n\ngrunt> Dump distinct_data;\n\nOutput\n\nIt will produce the following output, displaying the contents of the relation distinct_data as follows.\n\n(1,Rajiv,Reddy,9848022337,Hyderabad) (2,siddarth,Battacharya,9848022338,Kolkata) (3,Rajesh,Khanna,9848022339,Delhi) (4,Preethi,Agarwal,9848022330,Pune) (5,Trupthi,Mohanthy,9848022336,Bhuwaneshwar) (6,Archana,Mishra,9848022335,Chennai)\n\nApache Pig - Foreach Operator\n\nThe FOREACH operator is used to generate specified data transformations based on the column data.\n\nSyntax\n\nGiven below is the syntax of FOREACH operator.\n\ngrunt> Relation_name2 = FOREACH Relatin_name1 GENERATE (required data);\n\nExample\n\nAssume that we have a file named student_details.txt in the HDFS directory /pig_data/ as shown below.\n\nstudent_details.txt\n\n001,Rajiv,Reddy,21,9848022337,Hyderabad 002,siddarth,Battacharya,22,9848022338,Kolkata 003,Rajesh,Khanna,22,9848022339,Delhi 004,Preethi,Agarwal,21,9848022330,Pune 005,Trupthi,Mohanthy,23,9848022336,Bhuwaneshwar 006,Archana,Mishra,23,9848022335,Chennai 007,Komal,Nayak,24,9848022334,trivendram 008,Bharathi,Nambiayar,24,9848022333,Chennai\n\nAnd we have loaded this file into Pig with the relation name student_details as shown below.\n\ngrunt> student_details = LOAD 'hdfs://localhost:9000/pig_data/student_details.txt' USING PigStorage(',') as (id:int, firstname:chararray, lastname:chararray,age:int, phone:chararray, city:chararray);\n\nLet us now get the id, age, and city values of each student from the relation student_details and store it into another relation named foreach_data using the foreach operator as shown below.\n\ngrunt> foreach_data = FOREACH student_details GENERATE id,age,city;\n\nVerification\n\nVerify the relation foreach_data using the DUMP operator as shown below.\n\ngrunt> Dump foreach_data;\n\nOutput\n\nIt will produce the following output, displaying the contents of the relation foreach_data.\n\n(1,21,Hyderabad) (2,22,Kolkata) (3,22,Delhi) (4,21,Pune) (5,23,Bhuwaneshwar) (6,23,Chennai) (7,24,trivendram) (8,24,Chennai)\n\nApache Pig - Order By\n\nThe ORDER BY operator is used to display the contents of a relation in a sorted order based on one or more fields.\n\nSyntax\n\nGiven below is the syntax of the ORDER BY operator.\n\ngrunt> Relation_name2 = ORDER Relatin_name1 BY (ASC|DESC);\n\nExample\n\nAssume that we have a file named student_details.txt in the HDFS directory /pig_data/ as shown below.\n\nstudent_details.txt\n\n001,Rajiv,Reddy,21,9848022337,Hyderabad 002,siddarth,Battacharya,22,9848022338,Kolkata 003,Rajesh,Khanna,22,9848022339,Delhi 004,Preethi,Agarwal,21,9848022330,Pune 005,Trupthi,Mohanthy,23,9848022336,Bhuwaneshwar 006,Archana,Mishra,23,9848022335,Chennai 007,Komal,Nayak,24,9848022334,trivendram 008,Bharathi,Nambiayar,24,9848022333,Chennai\n\nAnd we have loaded this file into Pig with the relation name student_details as shown below.\n\ngrunt> student_details = LOAD 'hdfs://localhost:9000/pig_data/student_details.txt' USING PigStorage(',') as (id:int, firstname:chararray, lastname:chararray,age:int, phone:chararray, city:chararray);\n\nLet us now sort the relation in a descending order based on the age of the student and store it into another relation named order_by_data using the ORDER BY operator as shown below.\n\ngrunt> order_by_data = ORDER student_details BY age DESC;\n\nVerification\n\nVerify the relation order_by_data using the DUMP operator as shown below.\n\ngrunt> Dump order_by_data;\n\nOutput\n\nIt will produce the following output, displaying the contents of the relation order_by_data.\n\n(8,Bharathi,Nambiayar,24,9848022333,Chennai) (7,Komal,Nayak,24,9848022334,trivendram) (6,Archana,Mishra,23,9848022335,Chennai) (5,Trupthi,Mohanthy,23,9848022336,Bhuwaneshwar) (3,Rajesh,Khanna,22,9848022339,Delhi) (2,siddarth,Battacharya,22,9848022338,Kolkata) (4,Preethi,Agarwal,21,9848022330,Pune) (1,Rajiv,Reddy,21,9848022337,Hyderabad)\n\nApache Pig - Limit Operator\n\nThe LIMIT operator is used to get a limited number of tuples from a relation.\n\nSyntax\n\nGiven below is the syntax of the LIMIT operator.\n\ngrunt> Result = LIMIT Relation_name required number of tuples;\n\nExample\n\nAssume that we have a file named student_details.txt in the HDFS directory /pig_data/ as shown below.\n\nstudent_details.txt\n\n001,Rajiv,Reddy,21,9848022337,Hyderabad 002,siddarth,Battacharya,22,9848022338,Kolkata 003,Rajesh,Khanna,22,9848022339,Delhi 004,Preethi,Agarwal,21,9848022330,Pune 005,Trupthi,Mohanthy,23,9848022336,Bhuwaneshwar 006,Archana,Mishra,23,9848022335,Chennai 007,Komal,Nayak,24,9848022334,trivendram 008,Bharathi,Nambiayar,24,9848022333,Chennai\n\nAnd we have loaded this file into Pig with the relation name student_details as shown below.\n\ngrunt> student_details = LOAD 'hdfs://localhost:9000/pig_data/student_details.txt' USING PigStorage(',') as (id:int, firstname:chararray, lastname:chararray,age:int, phone:chararray, city:chararray);\n\nNow, let’s sort the relation in descending order based on the age of the student and store it into another relation named limit_data using the ORDER BY operator as shown below.\n\ngrunt> limit_data = LIMIT student_details 4;\n\nVerification\n\nVerify the relation limit_data using the DUMP operator as shown below.\n\ngrunt> Dump limit_data;\n\nOutput\n\nIt will produce the following output, displaying the contents of the relation limit_data as follows.\n\n(1,Rajiv,Reddy,21,9848022337,Hyderabad) (2,siddarth,Battacharya,22,9848022338,Kolkata) (3,Rajesh,Khanna,22,9848022339,Delhi) (4,Preethi,Agarwal,21,9848022330,Pune)\n\nApache Pig - Eval Functions\n\nApache Pig provides various built-in functions namely eval, load, store, math, string, bag and tuple functions.\n\nEval Functions\n\nGiven below is the list of eval functions provided by Apache Pig.\n\nApache Pig - Load & Store Functions\n\nThe Load and Store functions in Apache Pig are used to determine how the data goes ad comes out of Pig. These functions are used with the load and store operators. Given below is the list of load and store functions available in Pig.\n\nApache Pig - Bag & Tuple Functions\n\nGiven below is the list of Bag and Tuple functions.\n\nApache Pig - String Functions\n\nWe have the following String functions in Apache Pig.\n\nApache Pig - Date-time Functions\n\nApache Pig provides the following Date and Time functions −\n\nApache Pig - Math Functions\n\nWe have the following Math functions in Apache Pig −\n\nApache Pig - User Defined Functions\n\nIn addition to the built-in functions, Apache Pig provides extensive support for User Defined Functions (UDF’s). Using these UDF’s, we can define our own functions and use them. The UDF support is provided in six programming languages, namely, Java, Jython, Python, JavaScript, Ruby and Groovy.\n\nFor writing UDF’s, complete support is provided in Java and limited support is provided in all the remaining languages. Using Java, you can write UDF’s involving all parts of the processing like data load/store, column transformation, and aggregation. Since Apache Pig has been written in Java, the UDF’s written using Java language work efficiently compared to other languages.\n\nIn Apache Pig, we also have a Java repository for UDF’s named Piggybank. Using Piggybank, we can access Java UDF’s written by other users, and contribute our own UDF’s.\n\nTypes of UDF’s in Java\n\nWhile writing UDF’s using Java, we can create and use the following three types of functions −\n\nFilter Functions − The filter functions are used as conditions in filter statements. These functions accept a Pig value as input and return a Boolean value.\n\nEval Functions − The Eval functions are used in FOREACH-GENERATE statements. These functions accept a Pig value as input and return a Pig result.\n\nAlgebraic Functions − The Algebraic functions act on inner bags in a FOREACHGENERATE statement. These functions are used to perform full MapReduce operations on an inner bag.\n\nWriting UDF’s using Java\n\nTo write a UDF using Java, we have to integrate the jar file Pig-0.15.0.jar. In this section, we discuss how to write a sample UDF using Eclipse. Before proceeding further, make sure you have installed Eclipse and Maven in your system.\n\nFollow the steps given below to write a UDF function −\n\nOpen Eclipse and create a new project (say myproject).\n\nConvert the newly created project into a Maven project.\n\nCopy the following content in the pom.xml. This file contains the Maven dependencies for Apache Pig and Hadoop-core jar files.\n\n<project xmlns = \"http://maven.apache.org/POM/4.0.0\" xmlns:xsi = \"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation = \"http://maven.apache.org/POM/4.0.0http://maven.apache .org/xsd/maven-4.0.0.xsd\"> <modelVersion>4.0.0</modelVersion> <groupId>Pig_Udf</groupId> <artifactId>Pig_Udf</artifactId> <version>0.0.1-SNAPSHOT</version> <build> <sourceDirectory>src</sourceDirectory> <plugins> <plugin> <artifactId>maven-compiler-plugin</artifactId> <version>3.3</version> <configuration> <source>1.7</source> <target>1.7</target> </configuration> </plugin> </plugins> </build> <dependencies> <dependency> <groupId>org.apache.pig</groupId> <artifactId>pig</artifactId> <version>0.15.0</version> </dependency> <dependency> <groupId>org.apache.hadoop</groupId> <artifactId>hadoop-core</artifactId> <version>0.20.2</version> </dependency> </dependencies> </project>\n\nSave the file and refresh it. In the Maven Dependencies section, you can find the downloaded jar files.\n\nCreate a new class file with name Sample_Eval and copy the following content in it.\n\nimport java.io.IOException; import org.apache.pig.EvalFunc; import org.apache.pig.data.Tuple; import java.io.IOException; import org.apache.pig.EvalFunc; import org.apache.pig.data.Tuple; public class Sample_Eval extends EvalFunc<String>{ public String exec(Tuple input) throws IOException { if (input == null || input.size() == 0) return null; String str = (String)input.get(0); return str.toUpperCase(); } }\n\nWhile writing UDF’s, it is mandatory to inherit the EvalFunc class and provide implementation to exec() function. Within this function, the code required for the UDF is written. In the above example, we have return the code to convert the contents of the given column to uppercase.\n\nAfter compiling the class without errors, right-click on the Sample_Eval.java file. It gives you a menu. Select export as shown in the following screenshot.\n\nOn clicking export, you will get the following window. Click on JAR file.\n\nProceed further by clicking Next> button. You will get another window where you need to enter the path in the local file system, where you need to store the jar file.\n\nFinally click the Finish button. In the specified folder, a Jar file sample_udf.jar is created. This jar file contains the UDF written in Java.\n\nUsing the UDF\n\nAfter writing the UDF and generating the Jar file, follow the steps given below −\n\nStep 1: Registering the Jar file\n\nAfter writing UDF (in Java) we have to register the Jar file that contain the UDF using the Register operator. By registering the Jar file, users can intimate the location of the UDF to Apache Pig.\n\nSyntax\n\nGiven below is the syntax of the Register operator.\n\nREGISTER path;\n\nExample\n\nAs an example let us register the sample_udf.jar created earlier in this chapter.\n\nStart Apache Pig in local mode and register the jar file sample_udf.jar as shown below.\n\n$cd PIG_HOME/bin $./pig –x local REGISTER '/$PIG_HOME/sample_udf.jar'\n\nNote − assume the Jar file in the path − /$PIG_HOME/sample_udf.jar\n\nStep 2: Defining Alias\n\nAfter registering the UDF we can define an alias to it using the Define operator.\n\nSyntax\n\nGiven below is the syntax of the Define operator.\n\nDEFINE alias {function | [`command` [input] [output] [ship] [cache] [stderr] ] };\n\nExample\n\nDefine the alias for sample_eval as shown below.\n\nDEFINE sample_eval sample_eval();\n\nStep 3: Using the UDF\n\nAfter defining the alias you can use the UDF same as the built-in functions. Suppose there is a file named emp_data in the HDFS /Pig_Data/ directory with the following content.\n\n001,Robin,22,newyork 002,BOB,23,Kolkata 003,Maya,23,Tokyo 004,Sara,25,London 005,David,23,Bhuwaneshwar 006,Maggy,22,Chennai 007,Robert,22,newyork 008,Syam,23,Kolkata 009,Mary,25,Tokyo 010,Saran,25,London 011,Stacy,25,Bhuwaneshwar 012,Kelly,22,Chennai\n\nAnd assume we have loaded this file into Pig as shown below.\n\ngrunt> emp_data = LOAD 'hdfs://localhost:9000/pig_data/emp1.txt' USING PigStorage(',') as (id:int, name:chararray, age:int, city:chararray);\n\nLet us now convert the names of the employees in to upper case using the UDF sample_eval.\n\ngrunt> Upper_case = FOREACH emp_data GENERATE sample_eval(name);\n\nVerify the contents of the relation Upper_case as shown below.\n\ngrunt> Dump Upper_case; (ROBIN) (BOB) (MAYA) (SARA) (DAVID) (MAGGY) (ROBERT) (SYAM) (MARY) (SARAN) (STACY) (KELLY)\n\nApache Pig - Running Scripts\n\nHere in this chapter, we will see how how to run Apache Pig scripts in batch mode.\n\nComments in Pig Script\n\nWhile writing a script in a file, we can include comments in it as shown below.\n\nMulti-line comments\n\nWe will begin the multi-line comments with '/*', end them with '*/'.\n\n/* These are the multi-line comments In the pig script */\n\nSingle –line comments\n\nWe will begin the single-line comments with '--'.\n\n--we can write single line comments like this.\n\nExecuting Pig Script in Batch mode\n\nWhile executing Apache Pig statements in batch mode, follow the steps given below.\n\nStep 1\n\nWrite all the required Pig Latin statements in a single file. We can write all the Pig Latin statements and commands in a single file and save it as .pig file.\n\nStep 2\n\nExecute the Apache Pig script. You can execute the Pig script from the shell (Linux) as shown below.\n\nYou can execute it from the Grunt shell as well using the exec command as shown below.\n\ngrunt> exec /sample_script.pig\n\nExecuting a Pig Script from HDFS\n\nWe can also execute a Pig script that resides in the HDFS. Suppose there is a Pig script with the name Sample_script.pig in the HDFS directory named /pig_data/. We can execute it as shown below.\n\n$ pig -x mapreduce hdfs://localhost:9000/pig_data/Sample_script.pig\n\nExample\n\nAssume we have a file student_details.txt in HDFS with the following content.\n\nstudent_details.txt\n\n001,Rajiv,Reddy,21,9848022337,Hyderabad 002,siddarth,Battacharya,22,9848022338,Kolkata 003,Rajesh,Khanna,22,9848022339,Delhi 004,Preethi,Agarwal,21,9848022330,Pune 005,Trupthi,Mohanthy,23,9848022336,Bhuwaneshwar 006,Archana,Mishra,23,9848022335,Chennai 007,Komal,Nayak,24,9848022334,trivendram 008,Bharathi,Nambiayar,24,9848022333,Chennai\n\nWe also have a sample script with the name sample_script.pig, in the same HDFS directory. This file contains statements performing operations and transformations on the student relation, as shown below.\n\nstudent = LOAD 'hdfs://localhost:9000/pig_data/student_details.txt' USING PigStorage(',') as (id:int, firstname:chararray, lastname:chararray, phone:chararray, city:chararray); student_order = ORDER student BY age DESC; student_limit = LIMIT student_order 4; Dump student_limit;\n\nThe first statement of the script will load the data in the file named student_details.txt as a relation named student.\n\nThe second statement of the script will arrange the tuples of the relation in descending order, based on age, and store it as student_order.\n\nThe third statement of the script will store the first 4 tuples of student_order as student_limit.\n\nFinally the fourth statement will dump the content of the relation student_limit.\n\nLet us now execute the sample_script.pig as shown below.\n\n$./pig -x mapreduce hdfs://localhost:9000/pig_data/sample_script.pig\n\nApache Pig gets executed and gives you the output with the following content.\n\n(7,Komal,Nayak,24,9848022334,trivendram) (8,Bharathi,Nambiayar,24,9848022333,Chennai) (5,Trupthi,Mohanthy,23,9848022336,Bhuwaneshwar) (6,Archana,Mishra,23,9848022335,Chennai) 2015-10-19 10:31:27,446 [main] INFO org.apache.pig.Main - Pig script completed in 12 minutes, 32 seconds and 751 milliseconds (752751 ms)"
    }
}
{
    "id": "wrong_mix_domainrange_subsidiary_00029_3",
    "rank": 49,
    "data": {
        "url": "https://www.mckinsey.com/featured-insights/mckinsey-explainers/what-is-ai",
        "read_more_link": "",
        "language": "en",
        "title": "What is AI (artificial intelligence)?",
        "top_image": "https://www.mckinsey.com/~/media/mckinsey/featured%20insights/mckinsey%20explainers/what%20is%20ai/what-is-ai-1270595233-standard-1536x1536-april-2024.jpg",
        "meta_img": "https://www.mckinsey.com/~/media/mckinsey/featured%20insights/mckinsey%20explainers/what%20is%20ai/what-is-ai-1270595233-standard-1536x1536-april-2024.jpg",
        "images": [
            "https://www.mckinsey.com/~/media/mckinsey/featured%20insights/mckinsey%20explainers/what%20is%20ai/what-is-ai-1270595233-hero-1536x864-april-2024.jpg?cq=50&mw=767&cpy=Center",
            "https://www.mckinsey.com/~/media/mckinsey/featured%20insights/mckinsey%20explainers/mckinseyexplainers-flat-thumb-1536x1536.jpg?cq=50&mh=145&car=16:9&cpy=Center",
            "https://www.mckinsey.com/~/media/mckinsey/featured%20insights/mckinsey%20explainers/what%20is%20ai/whatisai-ex2.svgz?cq=50&cpy=Center",
            "https://www.mckinsey.com/~/media/mckinsey/featured%20insights/mckinsey%20explainers/what%20is%20ai/whatisai-ex1.svgz?cq=50&cpy=Center",
            "https://www.mckinsey.com/~/media/mckinsey/featured%20insights/mckinsey%20explainers/what%20is%20ai/what-is-ai-1270595233-standard-1536x1536-april-2024.jpg?cq=50&mh=145&car=16:9&cpy=Center",
            "https://www.mckinsey.com/~/media/mckinsey/business%20functions/mckinsey%20digital/our%20insights/ten%20unsung%20digital%20and%20ai%20ideas%20shaping%20business/thumb-gettyimages-1448016556.jpg?cq=50&mw=767&car=16:9&cpy=Center",
            "https://www.mckinsey.com/~/media/mckinsey/business%20functions/strategy%20and%20corporate%20finance/our%20insights/driving%20innovation%20with%20generative%20ai/itsr-innovation-genai-standard-536x1536.jpg?cq=50&mw=767&car=16:9&cpy=Center",
            "https://www.mckinsey.com/~/media/mckinsey/business%20functions/risk/our%20insights/as%20gen%20ai%20advances%20regulators%20and%20risk%20functions%20rush%20to%20keep%20pace/gen-ai-s-advance-has-regulators-1158276881-thumb-1536x1536.jpg?cq=50&mw=767&car=16:9&cpy=Center"
        ],
        "movies": [],
        "keywords": [],
        "meta_keywords": [
            ""
        ],
        "tags": null,
        "authors": [],
        "publish_date": "2023-04-24T16:41:00+00:00",
        "summary": "",
        "meta_description": "In this McKinsey Explainer, we define what AI is, and look at how rapid advances in Artificial Intelligence are reshaping almost every aspect of global society.",
        "meta_lang": "en",
        "meta_favicon": "/favicon.ico",
        "meta_site_name": "McKinsey & Company",
        "canonical_link": "https://www.mckinsey.com/featured-insights/mckinsey-explainers/what-is-ai",
        "text": "Humans and machines: a match made in productivityÂ heaven. Our species wouldnât have gotten very far without our mechanized workhorses. From the wheel that revolutionized agriculture to the screw that held together increasingly complex construction projects to the robot-enabled assembly lines of today, machines have made life as we know it possible. And yet, despite their seemingly endless utility, humans have long feared machinesâmore specifically, the possibility that machines might someday acquire human intelligenceÂ and strike out on their own.\n\nBut we tend to view the possibility of sentient machines with fascination as well as fear. This curiosity has helped turn science fiction into actual science. Twentieth-century theoreticians, like computer scientist and mathematician Alan Turing, envisioned a future where machines could perform functions faster than humans. The work of Turing and others soon made this a reality. Personal calculators became widely available in the 1970s, and by 2016, the US census showed that 89 percent of American households had a computer. Machinesâsmart machines at thatâare now just an ordinary part of our lives and culture.\n\nThose smart machines are also getting faster and more complex. Some computers have now crossed the exascale threshold, meaning they can perform as many calculations in a single second as an individual could in 31,688,765,000 years. And beyond computation, which machines have long been faster at than we have, computers and other devices are now acquiring skills and perception that were once unique to humans and a few other species.\n\nAI is a machineâs ability to perform the cognitive functions we associate with human minds, such as perceiving, reasoning, learning, interacting with the environment, problem-solving, and even exercising creativity. Youâve probably interacted with AI even if you donât realize itâvoice assistants like Siri and Alexa are founded on AI technology, as are some customer service chatbots that pop up to help you navigate websites.\n\nApplied AIâsimply, artificial intelligence applied to real-world problemsâhas serious implications for the business world. By using artificial intelligence, companies have the potential to make business more efficient and profitable. But ultimately, the value of AI isnât in the systems themselves. Rather, itâs in how companies use these systems to assist humansâand their ability to explain to shareholders and the public what these systems doâin a way that builds trust and confidence.\n\nFor more about AI, its history, its future, and how to apply it in business, read on.\n\nLearn more about QuantumBlack, AI by McKinsey.\n\nWhat is machine learning?\n\nMachine learning is a form of artificial intelligence that can adapt to a wide range of inputs, including large sets of historical data, synthesized data, or human inputs. (Some machine learning algorithms are specialized in training themselves to detect patterns; this is called deep learning. See Exhibit 1.) These algorithms can detect patterns and learn how to make predictions and recommendations by processing data, rather than by receiving explicit programming instruction. Some algorithms can also adapt in response to new data and experiences to improve over time.\n\nThe volume and complexity of data that is now being generated, too vast for humans to process and apply efficiently, has increased the potential of machine learning, as well as the need for it. In the years since its widespread deployment, which began in the 1970s, machine learning has had an impact on a number of industries, including achievements in medical-imaging analysisÂ and high-resolution weather forecasting.\n\nWhat is deep learning?\n\nDeep learning is a more advanced version of machine learning that is particularly adept at processing a wider range of data resources (text as well as unstructured data including images), requires even less human intervention, and can often produce more accurate results than traditional machine learning. Deep learning uses neural networksâbased on the ways neurons interact in the human brainâto ingest data and process it through multiple neuron layers that recognize increasingly complex features of the data. For example, an early layer might recognize something as being in a specific shape; building on this knowledge, a later layer might be able to identify the shape as a stop sign. Similar to machine learning, deep learning uses iteration to self-correct and improve its prediction capabilities. For example, once it âlearnsâ what a stop sign looks like, it can recognize a stop sign in a new image.\n\nLearn more about QuantumBlack, AI by McKinsey.\n\nWhat is generative AI?\n\nGenerative AI (gen AI) is an AI model that generates content in response to a prompt. Itâs clear that generative AI tools like ChatGPT and DALL-E (a tool for AI-generated art) have the potential to change how a range of jobsÂ are performed. Much is still unknown about gen AIâs potential, but there are some questions we can answerâlike how gen AI models are built, what kinds of problems they are best suited to solve, and how they fit into the broader category of AI and machine learning.\n\nFor more on generative AI and how it stands to affect business and society, check out our Explainer âWhat is generative AI?â\n\nWhat is the history of AI?\n\nThe term âartificial intelligenceâ was coined in 1956Â by computer scientist John McCarthy for a workshop at Dartmouth. But he wasnât the first to write about the concepts we now describe as AI. Alan Turing introduced the concept of the âimitation gameâ in a 1950 paper. Thatâs the test of a machineâs ability to exhibit intelligent behavior, now known as the âTuring test.â He believed researchers should focus on areas that donât require too much sensing and action, things like games and language translation. Research communities dedicated to concepts like computer vision, natural language understanding, and neural networks are, in many cases, several decades old.\n\nMIT physicist Rodney Brooks shared details on the four previous stages of AI:\n\nSymbolic AI (1956). Symbolic AI is also known as classical AI, or even GOFAI (good old-fashioned AI). The key concept here is the use of symbols and logical reasoning to solve problems. For example, we know a German shepherd is a dog, which is a mammal; all mammals are warm-blooded; therefore, a German shepherd should be warm-blooded.\n\nThe main problem with symbolic AI is that humans still need to manually encode their knowledge of the world into the symbolic AI system, rather than allowing it to observe and encode relationships on its own. As a result, symbolic AI systems struggle with situations involving real-world complexity. They also lack the ability to learn from large amounts of data.\n\nSymbolic AI was the dominant paradigm of AI research until the late 1980s.\n\nNeural networks (1954, 1969, 1986, 2012). Neural networks are the technology behind the recent explosive growth of gen AI. Loosely modeling the ways neurons interact in the human brain, neural networks ingest data and process it through multiple iterations that learn increasingly complex features of the data. The neural network can then make determinations about the data, learn whether a determination is correct, and use what it has learned to make determinations about new data. For example, once it âlearnsâ what an object looks like, it can recognize the object in a new image.\n\nNeural networks were first proposed in 1943 in an academic paper by neurophysiologist Warren McCulloch and logician Walter Pitts. Decades later, in 1969, two MIT researchers mathematically demonstrated that neural networks could perform only very basic tasks. In 1986, there was another reversal, when computer scientist and cognitive psychologist Geoffrey Hinton and colleagues solved the neural network problem presented by the MIT researchers. In the 1990s, computer scientist Yann LeCun made major advancements in neural networksâ use in computer vision, while JÃ¼rgen Schmidhuber advanced the application of recurrent neural networks as used in language processing.\n\nIn 2012, Hinton and two of his students highlighted the power of deep learning. They applied Hintonâs algorithm to neural networks with many more layers than was typical, sparking a new focus on deep neural networks. These have been the main AI approaches of recent years.\n\nTraditional robotics (1968). During the first few decades of AI, researchers built robots to advance research. Some robots were mobile, moving around on wheels, while others were fixed, with articulated arms. Robots used the earliest attempts at computer vision to identify and navigate through their environments or to understand the geometry of objects and maneuver them. This could include moving around blocks of various shapes and colors. Most of these robots, just like the ones that have been used in factories for decades, rely on highly controlled environments with thoroughly scripted behaviors that they perform repeatedly. They have not contributed significantly to the advancement of AI itself.\n\nBut traditional robotics did have significant impact in one area, through a process called âsimultaneous localization and mappingâ (SLAM). SLAM algorithms helped contribute to self-driving cars and are used in consumer products like vacuum cleaning robots and quadcopter drones. Today, this work has evolved into behavior-based robotics, also referred to as haptic technology because it responds to human touch.\n\nBehavior-based robotics (1985). In the real world, there arenât always clear instructions for navigation, decision making, or problem-solving. Insects, researchers observed, navigate very well (and are evolutionarily very successful) with few neurons. Behavior-based robotics researchers took inspiration from this, looking for ways robots could solve problems with partial knowledge and conflicting instructions. These behavior-based robots are embedded with neural networks.\n\nLearn more aboutÂ QuantumBlack, AI by McKinsey.\n\nWhat is artificial general intelligence?\n\nThe term âartificial general intelligenceâ (AGI) was coined to describe AI systems that possess capabilities comparable to those of a human. In theory, AGI could someday replicate human-like cognitive abilities including reasoning, problem-solving, perception, learning, and language comprehension. But letâs not get ahead of ourselves: the key word here is âsomeday.â Most researchers and academics believe we are decades away from realizing AGI; some even predict we wonât see AGI this century, or ever. Rodney Brooks, an MIT roboticist and cofounder of iRobot, doesnât believe AGI will arrive until the year 2300.\n\nThe timing of AGIâs emergence may be uncertain. But when it does emergeâand it likely willâitâs going to be a very big deal, in every aspect of our lives. Executives should begin working to understand the path to machines achieving human-level intelligence now and making the transition to a more automated world.\n\nFor more on AGI, including the four previous attempts at AGI, read our Explainer.\n\nWhat is narrow AI?\n\nNarrow AI is the application of AI techniques to a specific and well-defined problem, such as chatbots like ChatGPT, algorithms that spot fraud in credit card transactions, and natural-language-processing engines that quickly process thousands of legal documents. Most current AI applications fall into the category of narrow AI. AGI is, by contrast, AI thatâs intelligent enough to perform a broad range of tasks.\n\nLearn more about QuantumBlack, AI by McKinsey.\n\nHow is the use of AI expanding?\n\nAI is a big story for all kinds of businesses, but some companies are clearly moving ahead of the pack. Our state of AI in 2022 survey showed that adoption of AI models has more than doubled since 2017âand investment has increased apace. Whatâs more, the specific areas in which companies see value from AI have evolved, from manufacturing and risk to the following:\n\nmarketing and sales\n\nproduct and service development\n\nstrategy and corporate finance\n\nOne group of companies is pulling ahead of its competitors. Leaders of these organizations consistently make larger investments in AI, level up their practices to scale faster, and hire and upskill the best AI talent. More specifically, they link AI strategy to business outcomes and âindustrializeâ AI operations by designing modular data architecture that can quickly accommodate new applications.\n\nWhat are the limitations of AI models? How can these potentially be overcome?\n\nWe have yet to see the longtail effect of gen AI models. This means there are some inherent risks involved in using themâboth known and unknown.\n\nThe outputs gen AI models produce may often sound extremely convincing. This is by design. But sometimes the information they generate is just plain wrong. Worse, sometimes itâs biased (because itâs built on the gender, racial, and other biases of the internet and society more generally).\n\nIt can also be manipulated to enable unethical or criminal activity. Since gen AI models burst onto the scene, organizations have become aware of users trying to âjailbreakâ the modelsâthat means trying to get them to break their own rules and deliver biased, harmful, misleading, or even illegal content. Gen AI organizations are responding to this threat in two ways: for one thing, theyâre collecting feedback from users on inappropriate content. Theyâre also combing through their databases, identifying prompts that led to inappropriate content, and training the model against these types of generations.\n\nBut awareness and even action donât guarantee that harmful content wonât slip the dragnet. Organizations that rely on gen AI models should be aware of the reputational and legal risks involved in unintentionally publishing biased, offensive, or copyrighted content.\n\nThese risks can be mitigated, however, in a few ways. âWhenever you use a model,â says McKinsey partner Marie El Hoyek, âyou need to be able to counter biasesÂ and instruct it not to use inappropriate or flawed sources, or things you donât trust.â How? For one thing, itâs crucial to carefully select the initial data used to train these models to avoid including toxic or biased content. Next, rather than employing an off-the-shelf gen AI model, organizations could consider using smaller, specialized models. Organizations with more resources could also customize a general model based on their own data to fit their needs and minimize biases.\n\nItâs also important to keep a human in the loop (that is, to make sure a real human checks the output of a gen AI model before it is published or used) and avoid using gen AI models for critical decisions, such as those involving significant resources or human welfare.\n\nIt canât be emphasized enough that this is a new field. The landscape of risks and opportunities is likely to continue to change rapidly in the coming years. As gen AI becomes increasingly incorporated into business, society, and our personal lives, we can also expect a new regulatory climate to take shape. As organizations experimentâand create valueâwith these tools, leaders will do well to keep a finger on the pulse of regulation and risk.\n\nLearn more about QuantumBlack, AI by McKinsey.\n\nWhat is the AI Bill of Rights?\n\nThe Blueprint for an AI Bill of Rights, prepared by the US government in 2022, provides a framework for how government, technology companies, and citizens can collectively ensure more accountable AI. As AI has become more ubiquitous, concerns have surfacedÂ about a potential lack of transparency surrounding the functioning of gen AI systems, the data used to train them, issues of bias and fairness, potential intellectual property infringements, privacy violations, and more. The Blueprint comprises five principles that the White House says should âguide the design, use, and deployment of automated systems to protect [users] in the age of artificial intelligence.â They are as follows:\n\nThe right to safe and effective systems. Systems should undergo predeployment testing, risk identification and mitigation, and ongoing monitoring to demonstrate that they are adhering to their intended use.\n\nProtections against discrimination by algorithms. Algorithmic discrimination is when automated systems contribute to unjustified different treatment of people based on their race, color, ethnicity, sex, religion, age, and more.\n\nProtections against abusive data practices, via built-in safeguards. Users should also have agency over how their data is used.\n\nThe right to know that an automated system is being used, and a clear explanation of how and why it contributes to outcomes that affect the user.\n\nThe right to opt out, and access to a human who can quickly consider and fix problems.\n\nAt present, more than 60 countries or blocs have national strategies governing the responsible use of AIÂ (Exhibit 2). These include Brazil, China, the European Union, Singapore, South Korea, and the United States. The approaches taken vary from guidelines-based approaches, such as the Blueprint for an AI Bill of Rights in the United States, to comprehensive AI regulations that align with existing data protection and cybersecurity regulations, such as the EUâs AI Act, due in 2024.\n\nThere are also collaborative efforts between countries to set out standards for AI use. The USâEU Trade and Technology Council is working toward greater alignment between Europe and the United States. The Global Partnership on Artificial Intelligence, formed in 2020, has 29 members including Brazil, Canada, Japan, the United States, and several European countries.\n\nEven though AI regulations are still being developed, organizations should act now to avoid legal, reputational, organizational, and financial risks. In an environment of public concern, a misstep could be costly. Here are four no-regrets, preemptive actions organizations can implement today:\n\nTransparency. Create an inventory of models, classifying them in accordance with regulation, and record all usage across the organization that is clear to those inside and outside the organization.\n\nGovernance. Implement a governance structure for AI and gen AI that ensures sufficient oversight, authority, and accountability both within the organization and with third parties and regulators.\n\nData, model, and technology management.\n\nData management. Proper data management includes awareness of data sources, data classification, data quality and lineage, intellectual property, and privacy management.\n\nModel management. Organizations should establish principles and guardrails for AI development and use them to ensure all AI models uphold fairness and bias controls.\n\nCybersecurity and technology management. Establish strong cybersecurity and technology to ensure a secure environment where unauthorized access or misuse is prevented.\n\nIndividual rights. Make users aware when they are interacting with an AI system, and provide clear instructions for use.\n\nHow can organizations scale up their AI efforts from ad hoc projects to full integration?\n\nMost organizations are dipping a toe into the AI poolânot cannonballing. Slow progress toward widespread adoption is likely due to cultural and organizational barriers. But leaders who effectively break down these barriers will be best placed to capture the opportunities of the AI era. Andâcruciallyâcompanies that canât take full advantage of AI are already being sidelined by those that can, in industries like auto manufacturing and financial services.\n\nTo scale up AI, organizations can make three major shifts:\n\nMove from siloed work to interdisciplinary collaboration. AI projects shouldnât be limited to discrete pockets of organizations. Rather, AI has the biggest impact when itâs employed by cross-functional teams with a mix of skills and perspectives, enabling AI to address broad business priorities.\n\nEmpower frontline data-based decision making. AI has the potential to enable faster, better decisions at all levels of an organization. But for this to work, people at all levels need to trust the algorithmsâ suggestions and feel empowered to make decisions. (Equally, people should be able to override the algorithm or make suggestions for improvement when necessary.)\n\nAdopt and bolster an agile mindset. The agile test-and-learn mindset will help reframe mistakes as sources of discovery, allaying the fear of failure and speeding up development.\n\nLearn more about QuantumBlack, AI by McKinsey, and check out AI-related job opportunities if youâre interested in working at McKinsey.\n\nPop quiz\n\nArticles referenced:\n\nâAs gen AI advances, regulatorsâand risk functionsârush to keep pace,â December 21, 2023, Andreas Kremer, Angela Luget, Daniel Mikkelsen, Henning Soller, Malin Strandell-Jansson, and Sheila Zingg\n\nâWhat is generative AI?,â January 19, 2023\n\nâTech highlights from 2022âin eight charts,â December 22, 2022\n\nâGenerative AI is here: How tools like ChatGPT could change your business,â December 20, 2022, Michael Chui, Roger Roberts, and Lareina YeeÂ\n\nâThe state of AI in 2022âand a half decade in review,â December 6, 2022, Michael Chui, Bryce Hall, Helen Mayhew, Alex Singla, and Alex SukharevskyÂ\n\nâWhy businesses need explainable AIâand how to deliver it,â September 29, 2022, Liz Grennan, Andreas Kremer, Alex Singla, and Peter Zipparo\n\nâWhy digital trust truly matters,â September 12, 2022, Jim Boehm, Liz Grennan, Alex Singla, and Kate Smaje\n\nâMcKinsey Technology Trends Outlook 2023,â July 20, 2023, Michael Chui, Mena Issler, Roger Roberts, and Lareina YeeÂ\n\nâAn AI power play: Fueling the next wave of innovation in the energy sector,â May 12, 2022, Barry Boswell, Sean Buckley, Ben Elliott, Matias Melero, and Micah SmithÂ\n\nâScaling AI like a tech native: The CEOâs role,â October 13, 2021, Jacomo Corbo, David Harvey, Nicolas Hohn, Kia Javanmardian, and Nayur Khan\n\nâWhat the draft European Union AI regulations mean for business,â August 10, 2021, Misha Benjamin, Kevin Buehler, Rachel Dooley, and Peter Zipparo\n\nâWinning with AI is a state of mind,â April 30, 2021, Thomas Meakin, Jeremy Palmer, Valentina Sartori, and Jamie Vickers\n\nâBreaking through data-architecture gridlock to scale AI,â January 26, 2021, Sven Blumberg, Jorge Machado, Henning Soller, and Asin TavakoliÂ\n\nâAn executiveâs guide to AI,â November 17, 2020, Michael Chui, Brian McCarthy, and Vishnu Kamalnath\n\nâExecutiveâs guide to developing AI at scale,â October 28, 2020, Nayur Khan, Brian McCarthy, and Adi Pradhan\n\nâAn executive primer on artificial general intelligence,â April 29, 2020, Federico Berruti, Pieter Nel, and Rob Whiteman\n\nâThe analytics academy: Bridging the gap between human and artificial intelligence,â McKinsey Quarterly, September 25, 2019, Solly Brown, Darshit Gandhi, Louise Herring, and Ankur PuriÂ\n\nThis article was updated in April 2024; it was originally published in April 2023."
    }
}
{
    "id": "correct_foundationPlace_00107_2",
    "rank": 81,
    "data": {
        "url": "https://archive.org/stream/ninthtextretriev5002voor/ninthtextretriev5002voor_djvu.txt",
        "read_more_link": "",
        "language": "en",
        "title": "Full text of \"The ninth text REtrieval conference (TREC",
        "top_image": "",
        "meta_img": "",
        "images": [
            "https://archive.org/services/img/etree",
            "https://archive.org/services/img/librivoxaudio",
            "https://archive.org/services/img/metropolitanmuseumofart-gallery",
            "https://archive.org/services/img/clevelandart",
            "https://archive.org/services/img/internetarcade",
            "https://archive.org/services/img/consolelivingroom",
            "https://archive.org/images/book-lend.png",
            "https://archive.org/images/widgetOL.png",
            "https://archive.org/services/img/tv",
            "https://archive.org/services/img/911",
            "https://analytics.archive.org/0.gif?kind=track_js&track_js_case=control&cache_bust=589337205",
            "https://analytics.archive.org/0.gif?kind=track_js&track_js_case=disabled&cache_bust=590145966"
        ],
        "movies": [],
        "keywords": [],
        "meta_keywords": [
            ""
        ],
        "tags": null,
        "authors": [],
        "publish_date": null,
        "summary": "",
        "meta_description": "",
        "meta_lang": "en",
        "meta_favicon": "https://archive.org/images/glogo.jpg",
        "meta_site_name": "",
        "canonical_link": "https://archive.org/details/ninthtextretriev5002voor",
        "text": "Full text of \"The ninth text REtrieval conference (TREC-9)\"\n\nSee other formats\n\nNAT'L INST. OF STAND & TECf MIST PUBLICATIONS NIST Special Publication 500-249 Information Technology: The Ninth Text REtrieval Conference (TREC-9) E. M. Voorhees and D. K. Harman Editors NIST CENTENNIAL Nisr National Institute of Standards and Technology Technology Administration, U.S. Department of Commerce rhe National Institute of Standards and Technology was established in 1988 by Congress to \"assist industry in the development of technology . . . needed to improve product quality, to modernize manufacturing processes, to ensure product reliability . . . and to facilitate rapid commercialization ... of products based on new scientific discoveries.\" NIST, originally founded as the National Bureau of Standards in 1901, works to strengthen U.S. industry's competitiveness; advance science and engineering; and improve public health, safety, and the environment. One of the agency's basic functions is to develop, maintain, and retain custody of the national standards of measurement, and provide the means and methods for comparing standards used in science, engineering, manufacturing, commerce, industry, and education with the standards adopted or recognized by the Federal Government. As an agency of the U.S. Commerce Department's Technology Administration, NIST conducts basic and applied research in the physical sciences and engineering, and develops measurement techniques, test methods, standards, and related services. The Institute does generic and precompetitive work on new and advanced technologies. NIST's research facilities are located at Gaithersburg, MD 20899, and at Boulder, CO 80303. Major technical operating units and their principal activities are listed below. For more information contact the Publications and Program Inquiries Desk, 301-975-3058. Office of the Director • National Quality Program • International and Academic Affairs Technology Services • Standards Services • Technology Partnerships • Measurement Services • Information Services Advanced Technology Program • Economic Assessment • Information Technology and Applications • Chemistry and Life Sciences • Materials and Manufacturing Technology • Electronics and Photonics Technology Manufacturing Extension Partnership Program • Regional Programs • National Programs • Program Development Electronics and Electrical Engineering Laboratory • Microelectronics • Law Enforcement Standards • Electricity • Semiconductor Electronics • Radio-Frequency Technology' • Electromagnetic Technology1 • Optoelectronics' Materials Science and Engineering Laboratory • Intelligent Processing of Materials • Ceramics • Materials Reliability1 • Polymers • Metallurgy • NIST Center for Neutron Research Chemical Science and Technology Laboratory • Biotechnology • Physical and Chemical Properties2 • Analytical Chemistry • Process Measurements • Surface and Microanalysis Science Physics Laboratory • Electron and Optical Physics • Atomic Physics • Optical Technology • Ionizing Radiation • Time and Frequency' • Quantum Physics' Manufacturing Engineering Laboratory • Precision Engineering • Manufacturing Metrology • Intelligent Systems • Fabrication Technology • Manufacturing Systems Integration Building and Fire Research Laboratory • Applied Economics • Structures • Building Materials • Building Environment • Fire Safety Engineering • Fire Science Information Technology Laboratory • Mathematical and Computational Sciences2 • Advanced Network Technologies • Computer Security • Information Access • Convergent Information Systems • Information Services and Computing • Software Diagnostics and Conformance Testing • Statistical Engineering 'At Boulder, CO 80303. 2Some elements at Boulder, CO. NIST Special Publication 500-249 Information Technology: The Ninth Text REtrieval Conference (TREC-9) E.M. Voorhees and D.K. Harman Editors Information Technology Laboratory National Institute of Standards and Technology Gaithersburg, MD 20890-8980 October 2001 U.S. Department of Commerce Donald L. Evans, Secretary Technology Administration Karen H. Brown, Acting Under Secretary of Commerce for Technologx National Institute of Standards and Technology Karen H. Brown, Acting Director Reports on Information Technology The Information Technology Laboratory (ITL) at the National Institute of Standards and Technology (NIST) stimulates U.S. economic growth and industrial competitiveness through technical leadership and collaborative research in critical infrastructure technology, including tests, test methods, reference data, and forward-looking standards, to advance the development and productive use of information technology. To overcome barriers to usability, scalability, interoperability, and security in information systems and networks, ITL programs focus on a broad range of networking, security, and advanced information technologies, as well as the mathematical, statistical, and computational sciences, This Special Publication 500-series reports on ITL's research in tests and test methods for information technology, and its collaborative activities with industry, government, and academic organizations. Certain commercial entities, equipment, or materials may be identified in this document in order to describe an experimental procedure or concept adequately. Such identification is not intended to imply recommendation or endorsement by the National Institute of Standards and Technology, nor is it intended to imply that the entities, materials, or eauipment are necessarily the best available for the purpose. National Institute of Standards and Technology Special Publication 500-249 Natl. Inst. Stand. Technol. Spec. Publ. 500-249 946 pages (October 2001) CODEN: NSPUE2 U.S. GOVERNMENT PRINTING OFFICE WASHINGTON: 2001 For sale by the Superintendent of Documents, U.S. Government Printing Office Internet: bookstore.gpo.gov — Phone: (202) 5 12-1800 — Fax: (202) 5 12-2250 Mail: Stop SSOP, Washington, DC 20402-0001 Foreword This report constitutes the proceedings of the ninth Text REtrieval Conference (TREC-9) held in Gaithersburg, Maryland, November 13-16, 2000. The conference was co-sponsored by the National Institute of Standards and Technology (NIST), the Defense Advanced Research Projects Agency (DARPA), and the Advanced Research and Development Agency (ARDA). Approximately 175 people attended the conference, including representatives from seventeen different countries. The conference was the ninth in an on-going series of workshops to evaluate new technologies for text retrieval and related information-seeking tasks. Sixty- nine groups submitted retrieval results to one or more of the workshop's tracks. The workshop included plenary sessions, discussion groups, a poster session, and demonstra- tions. Because the participants in the workshop drew on their personal experiences, they sometimes cited specific vendors and commercial products. The inclusion or omission of a particular company or product implies neither endorsement nor criticism by NIST. Any opinions, findings, and conclusions or recommendations expressed in the individual papers are the authors' own and do not necessarily relect those of the sponsors. The sponsorship of the U.S. Department of Defense is gratefully acknowledged, as is the tremendous work of the program committee and the track coordinators. Ellen Voorhees, Donna Harman October 3, 2001 TREC-9 Program Committee Ellen Voorhees, NIST, chair James Allan, University of Massachusetts at Amherst Nick Belkin, Rutgers University Chris Buckley, Sabir Research, Inc. Jamie Callan, Carnegie Mellon University Susan Dumais, Microsoft Donna Harman, NIST David Hawking, CSIRO Bill Hersh, Oregon Health Sciences Institute Darryl Howard, U.S. Department of Defense David Hull, WhizzBang Labs John Prange, U.S. Department of Defense Steve Robertson, Microsoft Amit Singhal, AT&T Labs-Research Karen Sparck Jones, University of Cambridge, UK Tomek Strzalkowski, State University of New York, Albany Ross Wilkinson, CSIRO iii TABLE OF CONTENTS Index of TREC-9 Papers by Task/Track xi Abstract xix PAPERS Overview of the Ninth Text Retrieval Conference (TREC-9) 1 E. Voorhees, D. Harman, National Institute of Standards and Technology (NIST) TREC-9 Cross-Language Information Retrieval (English- Chinese) Overview 15 F. Gey, A. Chen, University of California, Berkeley The TREC-9 Filtering Track Final Report 25 S. Robertson, Microsoft Research D. A. Hull, WhizzBang Labs The TREC-9 Interactive Track Report 41 W. Hersh, Oregon Health Sciences University Paul Over, National Institute of Standards and Technology Query Expansion Seen Through Return Order of Relevant Documents 51 W. Liggett, NIST C. Buckley, SablR Research, Inc. Overview of the TREC-9 Question Answering Track 71 E. Voorhees, NIST The TREC-9 Query Track 81 Chris Buckley, Sabir Research, Inc. Overview of the TREC-9 Web Track 87 D. Hawking, CSIRO Mathematical and Information Sciences AT&T at TREC-9 103 A.Singhal, M. Kaszkiel, AT&T Labs-Research TREC-9 Cross-lingual Retrieval at BBN 106 J. Xu, R. Weischedel, BBN Technologies Spoken Document Retrieval for TREC-9 at Cambridge University 117 S.E. Johnson, P. Jourlin, K. Sparck Jones, P.C. Woodland, Cambridge University kNN at TREC-9 127 T. Ault, Y. Yang, Carnegie Mellon University v YFilter at TREC-9 135 Y. Zhang, J. Callan, Carnegie Mellon University Passive Feedback Collection- An Attempt to Debunk the Myth of Clickthroughs 141 C. Vogt, Chapman University TREC-9 CLIR at CUHK: Disambiguation by Similarity Values Between Adjacent Words 151 H. Jin, K-F Wong, The Chinese University of Hong Kong Syntactic Clues and Lexical Resources in Question-Answering 157 K.C. Litkowski, CL Research ACSys/CSIRO TREC-9 Experiments 167 D. Hawking, CSIRO Mathematics and Information Sciences The Mirror DBMS at TREC-9 171 A.P. de Vries, CWI, Amsterdam, The Netherlands Dublin City University Experiments in Connectivity Analysis for TREC-9 179 C. Gurrin, A.F. Smeaton, Dublin City University FDU at TREC-9: CLIR, Filtering and QA Tasks 189 L. Wu, X-j Huang, Y. Guo, B. Liu, Y. Zhang, Fudan University Fujitsu Laboratories TREC-9 Report 203 I. Namba, Fujitsu Laboratories, Ltd. Hummingbird's Fulcrum SearchServer at TREC-9 209 S. Tomlinson, T. Blackwell, Hummingbird English-Chinese Information Retrieval at IBM 223 M. Franz, J.S. McCarley, W-J Zhu, IBM T.J. Watson Research Center IBM's Statistical Question Answering System 229 A. Ittycheriah, M. Franz, W-J Zhu, A. Ratnaparkhi, IBM T.J. Watson Research Center R.J. Mammone, Rutgers University One Search Engine or Two for Question-Answering 235 J. Prager, E. Brown, IBM T.J. Watson Research Center D. R. Radev, University of Michigan K. Czuba, Carnegie-Mellon University ITT TREC-9 - Entity Based Feedback with Fusion 241 A. Chowdhury, S. Beitzel, E. Jensen, M. Sai-lee, D. Grossman, O. Frieder, Illinois Institute of Technology M.C. McCabe, U.S. Government D. Holmes, NCR Corporation VI A Simple Question Answering System 249 R. J. Cooper, S.M. Ruger, Imperial College of Science, Technology and Medicine T raining Context-Sensitive Neural Networks with Few Relevant Examples for the TREC-9 Routing 257 M. Strieker, Informatique-CDC and ESPCI F. Vichot, F. Wolinski, Informatique-CDC G. Dreyfus, ESPCI Mercure at trec9: Web and Filtering tasks 263 M. Abchiche, M. Boughanem, T. Dkaki, J. Mothe, C. Soule Dupuy, M. Tmar, IRIT-SIG The HAIRCUT System at TREC-9 273 P. McNamee, J. Mayfield, C. Piatko, The Johns Hopkins University, APL Reflections on \"Aboutness\" TREC-9 Evaluation Experiments at Justsystem 281 S. Fujita, Justsystem Corporation Kasetsart University TREC-9 Experiments 289 P. Narasetsathapom, A. Rungsawang, Kasetsart University, Bangkok, Thailand Experiments on the TREC-9 Filtering Track 295 K. Hoashi, K. Matsumoto, N. Inoue, K. Hashimoto, KDD R&D Laboratories, Inc. T. Hasegawa, K. Shirai, Waseda University TREC-9 Experiments at KAIST: QA, CLIR and Batch Filtering 303 K-S Lee, J-H Oh, JX Huang, J-H Kim, K-S Choi, Korea Advanced Institute of Science and Technology Question Answering Considering Semantic Categories and Co-Occurrence Density 317 S-M Kim, D-H Baek, S-B Kim H-C Rim Korea University QALC-The Question-Answering System of LIMSI-CNRS 325 O. Ferret, B. Grau, M. Hurault-Plantet, G. Dlouz, C. Jacquemin, LIMSI-CNRS N. Masson, P. Lecuyer, Bertin Technologies The LIMSI SDR System for TREC-9 335 J.-L. Gauvain, L. Lamel, C. Barras, G. Adda, Y. de Kercardio, LIMSI-CNRS TREC-9 CLIR Experiments at MSRCN 343 J. Gao, E. Xun, M. Zhou, C. Huang, Microsoft Research China J-Y Nie, Universite de Montreal J. Zhang, Y. Su, Tsinghua University China vii Question Answering Using a Large NLP System 355 D. Elworthy, Microsoft Research Ltd. Microsoft Cambridge at TREC-9: Filtering Track 361 S.E. Robertson, S. Walker, Microsoft Research Ltd., UK Another Sys Called Qanda 369 E. Breck, J. Burger, L. Ferro, W. Greiff, M. Light, I. Mani, J. Rennie, The MITRE Corporation CINDOR TREC-9 English-Chinese Evaluation 379 M.E. Ruiz, S. Rowe, M. Forrester, P. Sheridan, MNIS-TextWise Labs Description of NTU QA and CLIR Systems in TREC-9 389 C-J Lin, W-C Lin, H-H Chen, National Taiwan University NTT DATA TREC-9 Question Answering Track Report 399 T. Takaki, NTT Data Corporation Further Analysis of Whether Batch and User Evaluations Give the Same Results with a Question-Answering Task 407 W. Hersh, A. Turpin, L. Sacherek, D. Olson, S. Price, B. Chan, D. Kraemer, Oregon Health Sciences University TREC-9 Cross Language, Web and Question-Answering Track Experiments Using PIRCS 417 K.L. Kwok, L. Grunfeld, N. Dinstl, M. Chan, Queens College, CUNY Structuring and Expanding Queries in the Probabilistic Model 427 Y. Ogawa, H. Mano, M. Narita, S. Honma, RICOH Co., Ltd. Melbourne TREC-9 Experiments 437 D. D'Souza, M. Fuller, J. Thorn, P. Vines, J. Zobel, RMIT University O. de Kretser, University of Melbourne R. Wilkinson, M. Wu, CSIRO, Division of Mathematics and Information Science Logical Analysis of Data in the TREC-9 Filtering Track 453 E. Boros, P.B. Kantor, D.J. Neu, Rutgers University Support for Question- Answering in Interactive Information Retrieval: Rutgers' TREC-9 Interactive Track Experience 463 N.J. Belkin, A. Keller, D. Kelly, J. Perez-Carballo, C. Sikora, Y. Sun, Rutgers University Sabir Research at TREC-9 475 C. Buckley, J. Walz, Sabir Research FALCON: Boosting Knowledge for Answer Engines 479 S. Harabagiu, D. Moldovan, M. Pasca, R. Mihalcea, M. Surdeanu, R. Bunescu, R. Girju, V. Rus, P. Morarescu, Southern Methodist University viii Halfway to Question Answering 489 W.A. Woods, S. Green, P. Martin, A. Houston, Sun Microsystems Laboratories Question Answering: CNLP at the TREC-9 Question Answering Track 501 A.Diekema, X. Liu, J. Chen, H. Wang, N. McCracken, O. Yilmazel, E. D. Liddy, Syracuse University A Semantic Approach to Question Answering Systems 511 J.L. Vicedo, A. Ferrandez, Universidad de Alicante English-Chinese Cross-Language IR Using Bilingual Dictionaries 517 A. Chen, H. Jiang, University of California at Berkeley F. Gey, University of California at Berkeley (UC DATA) Question Answering, Relevance Feedback and Summarisation: TREC-9 Interactive Track Report 523 N. Alexander, C. Brown, J. Jose, I. Ruthven, A. Tombros, University of Glasgow Filters and Answers: The University of Iowa TREC-9 Results 533 E. Catona, D. Eichmann, P. Srinivasan, University of Iowa TREC-9 Experiments at Maryland: Interactive CLIR 543 D.W. Oard, G-A Levow, C.I. Cabezas, University of Maryland INQUERY and TREC-9 551 J. Allan, M.E. Connell, W.B. Croft, F-F Feng, D. Fisher, X. Li, University of Massachusetts Goal-Driven Answer Extraction 563 M. Laszlo, L. Kosseim, G. Lapalme, Universite de Montreal The System RELIEFS: A New Approach for Information Filtering 573 C. Brouard and J-Y Nie, Universite de Montreal Report on the TREC-9 Experiment: Link-based Retrieval and Distributed Collections 579 J. Savoy, Y. Rasolofo, Universite de Neuchatel Incrementality, Half-life, and Threshold Optimization for Adaptive Document Filtering 589 A.Arampatzis, J. Beney, C.H.A. Koster, T.P. van der Weide, University of Nijmegen Information Space Based on HTML Structure 601 G. Newby, University of North Carolina, Chapel Hill Web Document Retrieval Using Passage Retrieval, Connectivity Information, and Automatic Link Weighting--TREC-9 Report 611 F. Crivellari, M. Melucci, University of Padova (Italy) ix The PISAB Question Answering System... G. Attardi, C. Burrini, Universita di Pisa - Italy 621 The Thisl SDR System at TREC-9 627 S. Renals, D. Abberley, University of Sheffield University of Sheffield TREC-9 Q&A System 635 S. Scott, R. Gaizauskas, University of Sheffield Sheffield Interactive Experiment at TREC-9 645 M. Beaulieu, H. Fowkes, H. Joho, University of Sheffield Question Answering in Webclopedia 655 E. Hovy, L. Gerber, U. Hermjakob, M. Junk, C-Y Lin, University of Southern California TNO-UT at TREC-9: How Different are Web Documents? 665 W. Kraaij, TNO-TPD T. Westerveld, University of Twente, CTIT Question Answering by Passage Selection (MultiText Experiments for TREC-9) 673 C.L.A. Clarke, G.V. Cormack, D.I.E. Kisman, T.R. Lynam, University of Waterloo Appendix A TREC-9 Results A-l Task/Track Runs Lists A-2 Common Evaluation Measures A-15 Cross-language track results A-20 Filtering track results A-58 Interactive track results A-88 Query track results A-95 QA track results A-100 Spoken document retrieval track results A-178 Web track results A-210 X INDEX OF TREC-9 PAPERS BY TASK/TRACK Cross-Language BBN Technologies TREC-9 Cross Lingual Retrieval at BBN 106 Chinese University at Hong Kong TREC-9 CLIR at CUHK Disambiguation by Similarity Values Between Adjacent Words 151 Fudan University FDU at TREC-9: CLIR, Filtering and QA Tasks 189 IBM T.J. Watson Research Center English-Chinese Information Retrieval at IBM 223 Johns Hopkins University, APL The HAIRCUT System at TREC-9 273 Korea Advanced Institute of Science and Technology TREC-9 Experiments at KAIST: QA, CLIR and Batch Filtering 303 Microsoft Research China, Tsinghua University China, and the Universite de Montreal TREC-9 CLIR Experiments at MSRCN 343 MNIS-TextWise Labs CINDOR Trec-9 English-Chinese Evaluation 379 National Taiwan University Description of NTU QA and CLIR Systems in TREC-9 389 Queens College, CUNY TREC-9 Cross Language, Web and Question- Answering Track Experiments Using PIRCS ! 419 RMIT University Melbourne TREC-9 Experiments 437 University of California at Berkeley English-Chinese Cross-Language IR Using Bilingual Dictionaries 517 TREC-9 Cross-Language Information Retrieval (English-Chinese) Overview 15 XI University of Maryland TREC-9 Experiments at Maryland: Interactive CLIR 543 University of Massachusetts INQUERY and TREC-9 551 Filtering Carnegie Mellon University kNN at TREC-9 ...127 YFilter at TREC-9 135 Fudan University FDU at TREC-9: CLIR, Filtering and QA Tasks 189 Informatique-CDC, ESPCI Training Context-Sensitive Neural Networks with Few Relevant Examples for the TREC-9 Routing 257 IRIT-SIG Mercure at TREC-9: Web and Filtering Tasks 263 KDD R&D Laboratories, Inc., Waseda University Experiments on the TREC-9 Filtering Track 295 Korea Advanced Institute of Science and Technology TREC-9 Experiments at KAIST: QA, CLIR and Batch Filtering 303 Microsoft Research, WhizzBang Labs The TREC-9 Filtering Track Final Report 25 Microsoft Research Ltd., UK Microsoft Cambridge at TREC-9: Filtering Track 361 Queens College, CUNY TREC-9 Cross Language, Web and Question- Answering Track Experiments Using PIRCS 419 Rutgers University Logical Analysis of Data in the TREC-9 Filtering Track 453 Universite de Montreal The System RELIEFS: A New Approach for Information Filtering 573 xii University of Iowa Filters and Answers: The University of Iowa TREC-9 Results 533 University of Nijmegen Incrementality, Half-life, and Threshold Optimization for Adaptive Document Filtering 589 Interactive Chapman University Passive Feedback Collection— An Attempt to Debunk the Myth of Clickthroughs 141 National Institute of Standards and Technology and the Oregon Health Sciences University TREC-9 Interactive Track Report 41 Oregon Health Sciences University Further Analysis of Whether Batch and User Evaluations Give the Same Results with a Question-Answering Task 407 RMIT Melbourne TREC-9 Experiments 437 Rutgers University Support for Question- Answering in Interactive Information Retrieval: Rutgers' TREC-9 Interactive Track Experience 463 University of Glasgow Question Answering, Relevance Feedback and Summarisation: Trec-9 Interactive Track Report 523 University of Sheffield Sheffield Interactive Experiment at TREC-9 645 Query Hummingbird Hummingbird's Fulcrum SearchServer at TREC-9 211 Microsoft Research Ltd., UK Microsoft Cambridge at TREC-9: Filtering Track 361 National Institute of Standards and Technology Query Expansion Seen Through Return Order of Relevant Documents 51 xiii SablR Research, Inc. Query Expansion Seen Through Return Order of Relevant Documents 51 The TREC-9 Query Track 81 Sun Microsystems Laboratories Halfway to Question Answering 489 University of Massachusetts INQUERYand TREC-9 .....551 Question Answering CL Research Syntactic Clues and Lexical Resources in Question- Answering 157 Fudan University FDU at TREC-9: CLLR, Filtering and QA Tasks 189 IBM T.J. Watson Research Center One Search Engine or Two for Question-Answering 235 IBM's Statistical Question Answering System 231 Imperial College of Science, Technology and Medicine A Simple Question Answering System 251 Korea Advanced Institute of Science and Technology TREC-9 Experiments at KAIST: QA, CLIR and Batch Filtering 303 Korea University Question Answering Considering Semantic Categories and Co-Occurence Density 317 LIMSI-CNRS QALC~The Question-Answering System of LIMSI-CNRS 325 Microsoft Research Ltd. Question Answering Using a Large NLP System 355 The MITRE Corporation Another Sys Called Qanda 369 National Institute of Standards and Technology Overview of the TREC-9 Question Answering Track 71 National Taiwan University Description of NTU QA and CLER Systems in TREC-9 389 xiv NTT Data Corporation NTT DATA TREC-9 Question Answering Track Report 399 Queens College, CUNY TREC-9 Cross Language, Web and Question-Answering Track Experiments Using PIRCS 419 Southern Methodist University FALCON: Boosting Knowledge for Answer Engines 479 Sun Microsystems Laboratories Halfway to Question Answering 489 Syracuse University Question Answering: CNLP at the TREC-9 Question Answering Track 501 Universidad de Alicante A Semantic Approach to Question Answering Systems 511 Universita di Pisa - Italy The PISAB Question Answering System 621 Universite de Montreal Goal-Driven Answer Extraction 563 University of Iowa Filters and Answers: The University of Iowa TREC-9 Results 533 University of Massachusetts INQUERY and TREC-9 551 University of Sheffield University of Sheffield TREC-9 Q&A System 635 University of Southern California Question Answering in Webclopedia 655 University of Waterloo, CTIT Question Answering by Passage Selection (MultiText Experiments for TREC-9) 673 Spoken Document Retrieval Cambridge University Spoken Document Retrieval for TREC-9 at Cambridge University 117 XV LIMSI The LIMSI SDR System for TREC-9 335 University of Sheffield The Thisl SDR System at TREC-9 627 Web AT&T Labs-Research AT&T at TREC-9 103 CSIRO Mathematics and Information Sciences ACSys/CSIRO TREC-9 Experiments 167 Melbourne TREC-9 Experiments 437 Overview of the TREC-9 Web Track 87 CWI, Amsterdam The Mirror DBMS at TREC-9 171 Dublin City University Dublin City University Experiments in Connectivity Analysis for TREC-9 179 Fujitsu Laboratories, Ltd. Fujitsu Laboratories TREC-9 Report 203 Hummingbird Hummingbird's Fulcrum SearchServer at TREC-9 211 Illinois Institute of Technology HT TREC-9-Entity Based Feedback with Fusion 241 IRIT-SIG Mercure at trec9: Web and Filtering Tasks 263 Johns Hopkins University, APL The HAIRCUT System at TREC-9 273 Justsystem Corporation Reflections on \"Aboutness\" TREC-9 Evaluation Experiments at Justsystem 281 Queens College TREC-9 Cross Language, Web and Question-Answering Track Experiments Using PIRCS 419 xvi RICOH Co., Ltd. Structuring and Expanding Queries in the Probablistic Model 427 RMIT University Melbourne TREC-9 Experiments 437 SablR Research, Inc. SablR Research at TREC-9 475 TNO-TPD and Univ. of Twente TNO-UT at TREC-9: How Different are Web Documents? 665 University of Bangkok, Thailand Kasetsart University TREC-9 Experiments 289 Universite de Neuchatel Report on the TREC-9 Experiment: Link-based Retrieval an Distributed Collections 579 University of North Carolina, Chapel Hill Information Space Based on HTML Structure 601 University of Padova, Italy Web Document Retrieval Using Passage Retrieval, Connectivity Information and Automatic Link Weighting— TREC-9 Report 611 University of Waterloo, CTIT Question Answering by Passage Selection (MultiText Experiments for TREC-9) 673 xvii Abstract This report constitutes the proceedings of the ninth Text REtrieval Conference (TREC-9) held in Gaithersburg, Maryland, November 13-16, 2000. The conference was co-sponsored by the National Institute of Standards and Technology (NIST), the Defense Advanced Research Projects Agency (DARPA), and the Advanced Research and Development Agency (ARDA). Sixty-nine groups including participants from seventeen different countries were represented. TREC-9 is the latest in a series of workshops designed to foster research in text retrieval and related technologies. The previous eight TRECs each had an \"ad hoc\" main task through which eight large test collections were built. In recognition that sufficient infrastructure exists to support researchers interested in this traditional retrieval task, the ad hoc main task was discontinued in TREC-9 so that more TREC resources could be focused on building evaluation infrastructure for other retrieval tasks (called \"tracks\"). The seven tracks included in TREC-9 were web retrieval, cross-language retrieval, spoken document retrieval, query analysis, question answering, interactive retrieval, and filtering. The conference included paper sessions and discussion groups. This proceedings includes papers from most of the participants (some groups did not submit papers), track reports that define the problem addressed by the track plus summarize the main track results, and tables of individual group results. The TREC-9 proceedings web site also contains system descriptions that detail the timing and storage requirements of the different runs. xix Overview of the Ninth Text REtrieval Conference (TREC-9) Ellen M. Voorhees, Donna Harman National Institute of Standards and Technology Gaithersburg, MD 20899 1 Introduction The ninth Text REtrieval Conference (TREC-9) was held at the National Institute of Standards and Tech- nology (NIST) on November 13-16, 2000. The conference was co-sponsored by NIST, the Information Technology Office of the Defense Advanced Research Projects Agency (DARPA/ITO), and the Advanced Research and Development Activity (ARDA) office of the Department of Defense. TREC-9 is the latest in a series of workshops designed to foster research in text retrieval. The workshop series has four goals: • to encourage research in text retrieval based on large test collections; • to increase communication among industry, academia, and government by creating an open forum for the exchange of research ideas; • to speed the transfer of technology from research labs into commercial products by demonstrating substantial improvements in retrieval methodologies on real-world problems; and • to increase the availability of appropriate evaluation techniques for use by industry and academia, including development of new evaluation techniques more applicable to current systems. The previous eight TRECs each had an \"ad hoc\" main task through which eight large test collections were built [17]. In recognition that sufficient infrastructure exists to support researchers interested in this traditional retrieval task, the ad hoc main task was discontinued in TREC-9 so that more TREC resources could be focused on building evaluation infrastructure for other retrieval tasks (called \"tracks\"). The seven tracks included in TREC-9 were Cross-Language Retrieval, Filtering, Interactive Retrieval, Query Analysis, Question Answering, Spoken Document Retrieval, and Web Retrieval. Table 1 lists the groups that participated in TREC-9. Sixty-nine groups including participants from 17 different countries were represented. The diversity of the participating groups ensures that TREC represents many different approaches to text retrieval. This paper serves as an introduction to the research described in detail in the remainder of the volume. The next section provides a summary of the retrieval background knowledge that is assumed in the other papers. Section 3 presents a short description of each track — a more complete description of a track can be found in that track's overview paper in the proceedings. The final section looks forward to future TREC conferences. 2 Text Retrieval Text retrieval, also called information retrieval or document retrieval, is concerned with locating documents that are relevant to a user's information need. Traditionally, the emphasis in text retrieval research has been to provide access to natural language texts where the set of documents to be searched is large and topically diverse. Since the documents are free text and not specially structured for access by computers, standard database technologies are not effective solutions to the problem. The prototypical retrieval task is a researcher doing a literature search in a library. In this environment the retrieval system knows the set of documents to be searched (the library's holdings), but cannot anticipate 1 Table 1: Organizations participating in TREC-9 Australian National University/CSIRO NTT DATA Corporation Alicante University Oregon Health Sciences University AT&T Labs Research Pam Wood BBN Technologies Queens College, CUNY Chapman University New Mexico State University Chinese University of Kong Kong RICOH Co., Ltd. CL Research RMIT University/CSIRO Carnegie Mellon University (2 groups) Rutgers University (2 groups) Conexor Oy Sabir Research CWI, The Netherlands Seoul National University Dipartimento di iniormatica, Pisa Sheffield /Cambridge/ SoftSound/ICSI Dublin City University Southern Methodist University Fudan University State University of New York at Buffalo Fujitsu Laboratories, Ltd. Sun Microsystems Hummingbird Communications Syracuse University IBM T. J. Watson Research Center (2 groups) Trans-EZ Inc. IIT/AAT/NCR TwentyOne Imperial College University of Alberta Inf ormatique- C D C TT • - , r /--*, ■»■/• ■ t~> l l University of California, Berkeley IRIT/SIG University of Cambridge Johns Hopkins University University of Glasgow Justsystem Corporation University of Iowa KAIST TT\" *. f Tt K 1 1 1 1 T\"\\ 1 University of Maryland, College Park Katholieke Universiteit Nijmegen University of Massachusetts KDD R&D Laboratories/ Waseda University University of Melbourne Korea University Universite de Montreal LiMSI (2 groups) University ot INortn Carolina, L/iiapel rliil Microsoft (2 groups) Universite de Neuchatel MITRE University of Padova MNIS-TextWise Labs University of Sheffield MuliText Project USC-ISI National Taiwan University Xerox Research Centre Europe NeurOK, LLC the particular topic that will be investigated. We call this an ad hoc retrieval task, reflecting the arbitrary subject of the search and its short duration. Other examples of ad hoc searches are web surfers using Internet search engines, lawyers performing patent searches or looking for precedences in case law, and analysts searching archived news reports for particular events. A retrieval system's response to an ad hoc search is generally a list of documents ranked by decreasing similarity to the query. In a document routing or filtering task, the topic of interest is known and stable, but the document collection is constantly changing [3]. For example, an analyst who wishes to monitor a news feed for items on a particular subject requires a solution to a filtering task. The filtering task generally requires a retrieval system to make a binary decision whether to retrieve each document in the document stream as the system sees it. The retrieval system's response in the filtering task is therefore an unordered set of documents (accumulated over time) rather than a ranked list. Text retrieval has traditionally focused on returning documents that contain answers to questions rather than returning the answers themselves. This emphasis is both a reflection of retrieval systems' heritage as library reference systems and an acknowledgement of the difficulty of question answering. However, for certain types of questions, users would much prefer the system to answer the question than be forced to 2 <num> Number: 451 <title> What is a Bengals cat? <desc> Description: Provide information on the Bengal cat breed. <narr> Narrative: Item should include any information on the Bengal cat breed, including description, origin, characteristics, breeding program, names of breeders and catteries carrying bengals. References which discuss bengal clubs only are not relevant. Discussions of bengal tigers are not relevant. Figure 1: A sample TREC-9 topic from the web track. wade through a list of documents looking for the specific answer. To encourage research on systems that return answers instead of document lists, TREC introduced a question answering task in 1999. 2.1 Test collections Text retrieval has a long history of using retrieval experiments on test collections to advance the state of the art [5, 10, 13], and TREC continues this tradition. A test collection is an abstraction of an operational retrieval environment that provides a means for researchers to explore the relative benefits of different retrieval strategies in a laboratory setting. Test collections consist of three parts: a set of documents, a set of information needs (called topics in TREC), and relevance judgments, an indication of which documents should be retrieved in response to which topics. 2.1.1 Documents The document set of a test collection should be a sample of the kinds of texts that will be encountered in the operational setting of interest. It is important that the document set reflect the diversity of subject matter, word choice, literary styles, document formats, etc. of the operational setting for the retrieval results to be representative of the performance in the real task. Frequently, this means the document set must be large. The TREC test collections created in previous years' ad hoc main tasks used about 2 gigabytes of text (between 500,000 and 1,000,000 documents). The document sets used in various tracks have been smaller and larger depending on the needs of the track and the availability of data. The TREC document sets consist mostly of newspaper or newswire articles, though there are also some government documents (the Federal Register, patent applications) and computer science abstracts ( Computer Selects by Ziff-Davis publishing) included. High-level structures within each document are tagged using SGML, and each document is assigned an unique identifier called the DOCNO. In keeping of the spirit of realism, the text was kept as close to the original as possible. No attempt was made to correct spelling errors, sentence fragments, strange formatting around tables, or similar faults. 2.1.2 Topics TREC distinguishes between a statement of information need (the topic) and the data structure that is actually given to a retrieval system (the query). The TREC test collections provide topics to allow a wide range of query construction methods to be tested and also to include a clear statement of what criteria make a document relevant. The format of a topic statement has evolved since the beginning of TREC, but it has been stable for the past several years. A topic statement generally consists of four sections: an identifier, a title, a description, and a narrative. An example topic taken from this year's web track is shown in figure 1. The different parts of the TREC topics allow researchers to investigate the effect of different query lengths on retrieval performance. The \"titles\" in topics 301-450 were specially designed to allow experiments with very short queries; those title fields consist of up to three words that best describe the topic. (The title field 3 was used differently in topics 451-500, this year's web track topics, as described below.) The description field is a one sentence description of the topic area. The narrative gives a concise description of what makes a document relevant. Participants are free to use any method they wish to create queries from the topic statements. TREC distinguishes among two major categories of query construction techniques, automatic methods and manual methods. An automatic method is a means of deriving a query from the topic statement with no manual intervention whatsoever; a manual method is anything else. The definition of manual query construction methods is very broad, ranging from simple tweaks to an automatically derived query, through manual construction of an initial query, to multiple query reformulations based on the document sets retrieved. Since these methods require radically different amounts of (human) effort, care must be taken when comparing manual results to ensure that the runs are truly comparable. TREC topic statements are created by the same person who performs the relevance assessments for that topic (the assessor). Usually, each assessor comes to NIST with ideas for topics based on his or her own interests, and searches the document collection using NIST's PRISE system to estimate the likely number of relevant documents per candidate topic. The NIST TREC team selects the final set of topics from among these candidate topics based on the estimated number of relevant documents and balancing the load across assessors. This standard procedure for topic creation was changed for topics 451-500. These topics were to be used in the web track, and participants were concerned that the queries that users type into current web search engines are quite different from standard TREC topic statements. However, if participants were given only the literal queries submitted to a web search engine, they would not know the criteria by which documents would be judged. As a compromise, standard TREC topic statements were retrofitted around actual web queries. NIST obtained the log of queries that were submitted to the Excite search engine on December 20, 19991. A sample of queries that were deemed acceptable for use in a government-sponsored evaluation was given to the assessors. Each assessor selected a query from the sample and developed a description and narrative for that query. The assessors were instructed that the original query might well be ambiguous (e.g., \"cats\"), and they were to develop a description and narrative that were consistent with any one interpretation of the original (e.g., \"Where is the musical Cats playing?\"). They then searched the web document collection to estimate the likely number of relevant documents for that topic. The \"title\" field of topics 451-500 contains the literal query that was the seed of the topic. Unlike other TREC topics and the description and narrative fields of these topics, the title field contains all of the spelling and grammatical errors of the original Excite query. 2.1.3 Relevance judgments The relevance judgments are what turns a set of documents and topics into a test collection. Given a set of relevance judgments, the retrieval task is then to retrieve all of the relevant documents and none of the irrelevant documents. TREC almost always uses binary relevance judgments — either a document is relevant to the document or it is not. To define relevance for the assessors, the assessors are told to assume that they are writing a report on the subject of the topic statement. If they would use any information contained in the document in the report, then the (entire) document should be marked relevant, otherwise it should be marked irrelevant. The assessors are instructed to judge a document as relevant regardless of the number of other documents that contain the same information. Relevance is inherently subjective. Relevance judgments are known to differ across judges and for the same judge at different times [11]. Furthermore, a set of static, binary relevance judgments makes no provision for the fact that a real user's perception of relevance changes as he or she interacts with the retrieved documents. Despite the idiosyncratic nature of relevance, test collections are useful abstractions because the comparative effectiveness of different retrieval methods is stable in the face of changes to the relevance judgments [15]. The relevance judgments in early retrieval test collections were complete. That is, a relevance decision was made for every document in the collection for every topic. The size of the TREC document sets makes complete judgments utterly infeasible— with 800,000 documents, it would take over 6500 hours to judge the entire document set for one topic, assuming each document could be judged in just 30 seconds. Instead, :Jack Xu of Excite released this log on his ftp site at ftp.excite.com/pub/jack. 4 TREC uses a technique called pooling [12] to create a subset of the documents (the \"pool\") to judge for a topic. Each document in the pool for a topic is judged for relevance by the topic author. Documents that are not in the pool are assumed to be irrelevant to that topic. The judgment pools are created as follows. When participants submit their retrieval runs to NIST, they rank their runs in the order they prefer them to be judged. NIST chooses a number of runs to be merged into the pools, and selects that many runs from each participant respecting the preferred ordering. For each selected run, the top X documents (usually, X = 100) per topic are added to the topics' pools. Since the retrieval results are ranked by decreasing similarity to the query, the top documents are the documents most likely to be relevant to the topic. Many documents are retrieved in the top X for more than one run, so the pools are generally much smaller the theoretical maximum of X x the-number-of-selected-runs documents (usually about 1/3 the maximum size). The use of pooling to produce a test collection has been questioned because unjudged documents are assumed to be not relevant. Critics argue that evaluation scores for methods that did not contribute to the pools will be deflated relative to methods that did contribute because the non-contributors will have highly ranked unjudged documents. Zobel demonstrated that the quality of the pools (the number and diversity of runs contributing to the pools and the depth to which those runs are judged) does affect the quality of the final collection [20]. He also found that the TREC collections were not biased against unjudged runs. In this test, he evaluated each run that contributed to the pools using both the official set of relevant documents published for that collection and the set of relevant documents produced by removing the relevant documents uniquely retrieved by the run being evaluated. For the TREC-5 ad hoc collection, he found that using the unique relevant documents increased a run's 11 point average precision score by an average of 0.5 %. The maximum increase for any run was 3.5 %. The average increase for the TREC-3 ad hoc collection was somewhat higher at 2.2 %. A similar investigation of the TREC-8 ad hoc collection showed that every automatic run that had a mean average precision score of at least .1 had a percentage difference of less than 1 % between the scores with and without that group's uniquely retrieved relevant documents [16]. That investigation also showed that the quality of the pools is significantly enhanced by the presence of recall-oriented manual runs, an effect noted by the organizers of the NTCIR (NACSIS Test Collection for evaluation of Information Retrieval systems) workshop who performed their own manual runs to supplement their pools [9]. While the lack of any appreciable difference in the scores of submitted runs is not a guarantee that all relevant documents have been found, it is very strong evidence that the test collection is reliable for comparative evaluations of retrieval runs. Indeed, the differences in scores resulting from incomplete pools observed here are smaller than the differences that result from using different relevance assessors [15]. 2.2 Evaluation Retrieval runs on a test collection can be evaluated in a number of ways. In TREC, all ad hoc tasks (i.e., all tasks that involve returning a ranked list of documents) are evaluated using the trec_eval package written by Chris Buckley of Sabir Research [4]. This package reports about 85 different numbers for a run, including recall and precision at various cut-off levels plus single-valued summary measures that are derived from recall and precision. Precision is the proportion of retrieved documents that are relevant, while recall is the proportion of relevant documents that are retrieved. A cut-off level is a rank that defines the retrieved set; for example, a cut-off level of ten defines the retrieved set as the top ten documents in the ranked list. The trec_eval program reports the scores as averages over the set of topics where each topic is equally weighted. (The alternative is to weight each relevant document equally and thus give more weight to topics with more relevant documents. Evaluation of retrieval effectiveness historically weights topics equally since all users are assumed to be equally important.) Precision reaches its maximal value of 1.0 when only relevant documents are retrieved, and recall reaches its maximal value (also 1.0) when all the relevant documents are retrieved. Note, however, that these theoretical maximum values are not obtainable as an average over a set of topics at a single cut-off level because different topics have different numbers of relevant documents. For example, a topic that has fewer than ten relevant documents will have a precision score less than one after ten documents are retrieved regardless of how the documents are ranked. Similarly, a topic with more than ten relevant documents 5 must have a recall score less than one after ten documents are retrieved. At a single cut-off level, recall and precision reflect the same information, namely the number of relevant documents retrieved. At varying cut-off levels, recall and precision tend to be inversely related since retrieving more documents will usually increase recall while degrading precision and vice versa. Of all the numbers reported by trec.eval, the recall-precision curve and mean (non-interpolated) average precision are the most commonly used measures to describe TREC retrieval results. A recall-precision curve plots precision as a function of recall. Since the actual recall values obtained for a topic depend on the number of relevant documents, the average recall-precision curve for a set of topics must be interpolated to a set of standard recall values. The particular interpolation method used is given in Appendix A, which also defines many of the other evaluation measures reported by trec.eval. Recall-precision graphs show the behavior of a retrieval run over the entire recall spectrum. Mean average precision is the single-valued summary measure used when an entire graph is too cum- bersome. The average precision for a single topic is the mean of the precision obtained after each relevant document is retrieved (using zero as the precision for relevant documents that are not retrieved). The mean average precision for a run consisting of multiple topics is the mean of the average precision scores of each of the individual topics in the run. The average precision measure has a recall component in that it re- flects the performance of a retrieval run across all relevant documents, and a precision component in that it weights documents retrieved earlier more heavily than documents retrieved later. Geometrically, mean average precision is the area underneath a non-interpolated recall-precision curve. The (reformatted) output of trec.eval for each submitted run is given in Appendix A. In addition to the ranked results, participants are also asked to submit data that describes their system features and timing figures to allow a primitive comparison of the amount of effort needed to produce the corresponding retrieval results. These system descriptions are not included in the printed version of the proceedings due to their size, but they are available on the TREC web site (http://trec.nist.gov). 3 TREC-9 Tracks TREC's track structure was begun in TREC-3 (1994). The tracks serve several purposes. First, tracks act as incubators for new research areas: the first running of a track often defines what the problem really is, and a track creates the necessary infrastructure (test collections, evaluation methodology, etc.) to support research on its task. The tracks also demonstrate the robustness of core retrieval technology in that the same techniques are frequently appropriate for a variety of tasks. Finally, the tracks make TREC attractive to a broader community by providing tasks that match the research interests of more groups. Table 2 lists the different tasks that were in each TREC, the number of groups that submitted runs to that task, and the total number of groups that participated in each TREC. The tasks within the tracks offered for a given TREC have diverged as TREC has progressed. This has helped fuel the growth in the number of participants, but has also created a smaller common base of experience among participants since each participant tends to submit runs to fewer tracks. This section describes the tasks performed in the TREC-9 tracks. See the track reports elsewhere in this proceedings for a more complete description of each track. 3.1 The Web track The purpose of the web track was to build a test collection that more closely mimics the retrieval envi- ronment of the World Wide Web. In creating such a collection, a variety of web retrieval strategies were also investigated. The task in the track was a traditional ad hoc retrieval task where the documents were a collection of web pages. The web track was coordinated by David Hawking and his colleagues at CSIRO and the Australian National University. They obtained a snapshot of the web from 1997 from the Internet Archive, and produced several subsets of that spidering. A 10 gigabyte subset known as WTlOg was used for the main task in the web track [1]. There was also a separate large task in the web track that used the 100 gigabyte VLC2/WT100g collection and a set of 10,000 queries selected from Electronic Monk and AltaVista query logs. See the web track report in these proceedings for more details about the large web task. 6 Table 2: Number of participants per task and total number of distinct participants in each TREC. Task TREC 1 2 3 4 5 6 7 s q Ad Hoc 18 24 26 23 28 31 42 41 Routing 16 25 25 15 21 Interactive 3 11 2 9 8 7 a Spanish 4 10 7 Confusion 4 5 Database Merging 3 3 Filtering 4 7 10 12 14 15 Chinese 9 12 NLP 4 2 Speech 13 10 10 3 Cross-Language 13 9 13 16 High Precision 5 4 Very Large Corpus 7 6 Query 2 5 6 Question Answering 20 28 Web 17 23 Total participants 22 31 33 36 38 51 56 66 69 The topics used in the main web task were TREC topics 451-500. As described earlier, these topics were created especially for the track. Three-way relevance judgments (not relevant, relevant, and highly relevant) were used. In addition, assessors were asked to select the best document from among all the documents in the pool for each topic. While the official results of the task were scored by conflating the relevant and highly relevant categories, the additional information collected during assessing can be used to develop other evaluation schemes for web retrieval. Twenty-three groups submitted 105 runs to the main task of the web track. Twelve of the runs used manual query construction techniques, 40 of the runs were automatic runs that used only the original Excite query, and the remaining 53 runs were automatic runs that used some other part of the topic in addition to the Excite query. Runs using the description field of the topic were always more effective than the corresponding run using only the original Excite query. (Remember that the description field corrected the spelling errors of the original query. Seven topics had some sort of error in the original Excite query, five of which were serious. Examples of serious errors are the one- word queries \"nativityscenes\" and \"angioplast7\".) The order of systems ranked by average effectiveness differed depending on whether the evaluation used both relevant and highly relevant documents or highly relevant documents only. This finding implies that retrieving highly relevant documents is a different task from retrieving generally relevant documents, at least to the extent that different retrieval techniques should be used. More research is needed to determine precisely which techniques work better for which task and why. The motivation for distinguishing between highly relevant and generally relevant documents is a wide- spread belief that web users will be better served by systems that retrieve highly relevant documents. Taking this reasoning a step further, some have argued that web search engines should actually be evaluated on their ability to retrieve the very best page. However, the results of the web track demonstrate that using the best page as the single relevant document is too unstable to be a reliable evaluation strategy. Once the web track assessing was complete, NIST gave the set of relevant documents (both highly relevant and generally relevant) to two additional assessors and asked them to select the best page. In all cases the definition of best was left up to the assessor. There was significant disagreement among the assessors as to the best page for a topic: all three assessors disagreed with one another for 17 of the 50 topics, and of the 13 topics for which all three assessors picked the same page, 5 topics had three or fewer pages in its relevant set. Furthermore, unlike traditional retrieval system evaluation [15], best document evaluation is not robust against changes caused by using different assessors to select best documents. The Kendall tau correlation 7 between system rankings produced by using different assessors averaged only about .75 when using best document evaluation as compared to over .9 for traditional evaluation. 3.2 The Cross-Language (CLIR) track The CLIR task is an ad hoc retrieval task in which the documents are in one language and the topics are in a different language. The goal of the track is to facilitate research on systems that are able to retrieve relevant documents regardless of the language a document happens to be written in. The TREC-9 cross-language track used Chinese documents and English topics. A Chinese version of the topics was also developed so that cross-language retrieval performance could be compared with the equivalent monolingual performance. The document set was approximately 250 megabytes of news articles taken from the Hong Kong Com- mercial Daily, the Hong Kong Daily News, and Takungpao. The documents were made available for use in TREC by Wisers, Ltd. Twenty-five topics were developed by NIST assessors. The assessment pools were created using each group's first choice cross-language run and first-choice monolingual run (if any), using the top 50 documents from each run. Fifty-two runs from 15 different groups were submitted to the track. Thirteen of the runs were monolin- gual runs. Only one run (a cross-language run) was a manual run. The effectiveness of cross-language runs is frequently reported as a percentage of monolingual effective- ness. In the CLIR track, the cross-language run submitted by the BBN group was not only better than their monolingual run (as measured by mean average precision), it was better than all the submitted monolingual runs. While BBN has since produced a monolingual run that is better than the best of their cross- language runs [19], the effectiveness of English to Chinese cross-language retrieval remains high. 3.3 The Spoken Document Retrieval (SDR) track The SDR track fosters research on retrieval methodologies for spoken documents (i.e., recordings of speech). The task in the track is an ad hoc task in which the documents are transcriptions of audio signals. The SDR track has run in several TRECs and the track has had the same general structure each year. Participants worked with different versions of transcripts of news broadcasts to judge the effects of errors in the transcripts on retrieval performance. The reference transcripts were manually produced and assumed to be perfect. (For TREC-9 the reference transcripts were a combination of human reference transcripts, closed captioning transcripts, and automatically-combined (using NIST's ROVER algorithm) automatic transcripts.) The baseline transcripts were produced by one automatic speech recognizer and made available to all participants. There was one TREC-9 baseline transcript, which was the \"B2\" transcript used in the TREC-8 track. This transcript was produced using NIST's installation of the BBN Rough 'N Ready BYBLOS speech recognizer. The recognizer transcripts were produced by the participants' own recognizer systems. The recognizer transcripts of the different participants were made available to one another so that participants could perform retrieval runs against their own recognizer transcripts as well as others' recognizer transcripts ( cross- recognizer runs). The different versions of the transcripts allowed participants to observe the effect of recognizer errors on their retrieval strategy. The different recognizer runs provide a comparison of how different recognition strategies affect retrieval. The document collection used in TREC-9 was the audio portion of the TDT-2 News Corpus as collected by the Linguistic Data Consortium (LDC). This corpus contains 557 hours of audio representing 1,064 news shows, which were segmented into approximately 21,500 documents. Two different versions of 50 new topics (numbers 124-173) were created for the track. The \"standard\" version of the topics was a one sentence description, while the \"terse\" version of the topics was just a few words. As in the TREC-8 track, the TREC-9 track focused on the unknown boundary condition, that is, retrieving documents from the audio stream when the system was not given the story boundaries. Three groups participated in the track, submitting a total of 64 runs. Overall, the retrieval results were excellent: the systems could find relevant passages produced by a variety of recognizers on the full unsegmented news broadcasts, using either the terse or longer standard queries. Indeed, for each of the participants, retrieval from the transcripts created by their own recognizer was comparable to the retrieval from the human reference transcripts. 8 Table 3: Runsets submitted to the TREC-9 query track. Participant Runsets Description Hummingbird Microsoft Sabir Research Sun Microsystems Univ. of Massachusetts Univ. of Melbourne hum* ok9u Sab* SUN, SUNl IN7* UoMd, UoMl 7 variants of SearchServer Okapi run with no query expansion 3 variants of SMART 2 variants of Nova 3 variants of INQUERY 2 variants of MG 3.4 The Query track The task in the query track was an ad hoc task using old TREC document and topic sets. The focus in the track was not on absolute retrieval effectiveness, but on the variability of topic performance. A variety of research (for example, see [2]) has shown that the difference in retrieval effectiveness for a given retrieval system on different topics is much greater on average than the difference in retrieval effectiveness between systems for the same topic. The development of query-specific processing strategies has been hampered as a result because the available topic sets (of size 50 for most TREC collections) are too small to isolate the effects caused by different topics. The query track was designed as a means for creating a large set of different queries for an existing TREC topic set as a first step toward query-specific processing. Six groups participated in the TREC-9 query track, with each group running each of 43 different querysets using one or more variants of their retrieval system. A queryset consists of one query for each of 50 topics (TREC topics 51-100) where each query is from the same category of queries. Three different query categories were used. 1. Short: 2-4 words selected by reading the topic statement. 2. Sentence: a sentence — normally less than one line — developed after reading the topic statement and possibly some relevant documents. 3. Sentence- Rel: a sentence developed after reading a handful of relevant documents. The topic statement was not used for this category of query. Relevant documents from TREC disk 2 were used to construct the queries. Twenty-one of the querysets were developed for the TREC-8 query track, and the remaining 22 querysets were developed for the TREC-9 track. A runset is the result of running one version of a retrieval system on all 43 querysets against the documents on TREC disk 1. Eighteen runsets were submitted, for a total of 774 (18 x 43) runs submitted to the track. Table 3 gives a short description of each runset, while Table 4 gives the average, minimum, and maximum mean average precision score computed over the 43 different querysets. As can be seen from wide range of scores for each system, the individual query formulations again had a pronounced effect on retrieval performance. 3.5 The Question Answering (QA) track The purpose of the question answering track was to encourage research into systems that return actual answers, as opposed to ranked lists of documents, in response to a question. Participants received a set of fact-based, short-answer questions and searched a large document set to extract (or construct) an answer to each question. Participants returned a ranked list of five [document-id, answer- string] pairs per question such that each answer string was believed to contain an answer to the question and the document supported that answer. Answer strings were limited to either 50 bytes or 250 bytes depending on the run type. Each question was guaranteed to have an answer in the collection. An individual question received a score equal to the reciprocal of the rank at which the first correct response was returned, or 0 if none of the five responses 9 Table 4: Average, minimum, and maximum Mean Average Precision scores for each query track system computed over the 43 different querysets submitted to the track. Each queryset contains queries created for TREC topics 51-100. Average Minimum Maximum Average Minimum Maximum TN7a L<i (a D 1 7QQ n i m 7 TToMl \\J LUVll U. ±OOj 0 D812 f) 20QQ IN7e 0.2288 0.1456 0.3168 hum4 0.1713 0.0907 0.2580 IN7p 0.1848 0.1095 0.2593 humA 0.1741 0.0912 0.2482 SUN 0.0572 0.0062 0.1247 humB 0.1732 0.0908 0.2420 Sunl 0.0677 0.0268 0.1152 humD 0.1771 0.0897 0.2508 Saba 0.1924 0.1089 0.2665 huml 0.1736 0.0910 0.2418 Sabe 0.2516 0.1481 0.3202 humK 0.1713 0.0863 0.2425 Sabm 0.2321 0.1347 0.3057 humV 0.1648 0.0788 0.2453 UoMd 0.1612 0.1010 0.2091 ok9u 0.1917 0.0963 0.2608 contained a correct answer. The score for a submission was then the mean of the individual questions' reciprocal ranks. Question answering systems were given no credit for retrieving multiple (different) correct answers, or for recognizing that they did not know the answer. There were several changes between the TREC-9 track and the initial running of the track in TREC-8. In TREC-9, the document set was larger, consisting of all the news articles on TREC disks 1-5. The test set of questions was also much larger, consisting of 693 questions rather than 200. A more substantial difference was the way in which the questions were created. Instead of using questions created especially for the track, which tended to be back-formulations of a sentence in some document in the collection, questions were selected from query logs. Some questions were taken from a log of questions that had been submitted to Encarta and were made available to NIST by Microsoft. Other questions were created by NIST staff using the Excite log from which the web track queries were selected for suggestions. In all cases, the questions were created without reference to the document set. Once the questions were created, NIST assessors searched the document set to find which questions had answers in the test document set. Five hundred questions were selected from among the candidate questions that had an answer in the document set. In a separate pass, NIST assessors were given a subset of the questions (but not their answers) and were asked to create equivalent, re-worded questions. For example, the question \"How tall is the Empire State building?\" might be re-worded as \"How high is the Empire State building?\" , \"What is the height of the Empire State building?\" , \"The Empire State Building is how tall?\", etc. A total of 193 question variants were added to the set of 500 to make the final question set of 693 questions. Human assessors read each string and decided whether the answer string contained an answer to the question. If not, the response was judged as incorrect. If so, the assessor decided whether the answer was supported by the document returned with the string. If the answer was not supported by that document, the response was judged as \"Not Supported\". If it was supported, the response was judged as correct. The official scoring for the track treated Not Supported answers as incorrect. Twenty-eight groups submitted 78 runs to the track, with 34 runs using the 50-byte-limit and 44 runs using the 250- byte-limit. The best performing system, from Southern Methodist University, was able to extract a correct answer about 65 % of the time by integrating multiple natural language processing techniques with abductive reasoning [6]. While the 65 % score is a slightly worse result than the TREC-8 scores in absolute terms, it represents a very significant improvement in question answering systems. The TREC-9 task was considerably harder than the TREC-8 task because of the switch to \"real\" questions (which tend to be far more ambiguous than the questions constructed for the TREC-8 task). The SMU system found an answer about a third again as often as the next best system (66 % of the questions vs. 42 % of the questions). 10 3.6 The Interactive track The interactive track was one of the first tracks to be introduced into TREC. Since its inception, the high- level goal of the track has been the investigation of searching as an interactive task by examining the process as well as the outcome. One of the main problems with studying interactive behavior of retrieval systems is that both searchers and topics generally have a much larger effect on search results than does the retrieval system used. The task in the TREC-9 track was a question answering task. Two different types of questions were used: • find any n Xs (for example, \"Name 3 US Senators on committees regulating the nuclear industry.\" ) • compare two specific Xs (for example, \"Do more people graduate with an MBA from Harvard Business School or MIT Sloan?\") Human searchers were given a maximum of 5 minutes to find the answer to a question and support that answer with a set of documents. A total of 8 questions was used in the track. The document set was the set of documents used in the question answering track (the news articles from TREC disks 1-5). The track defined an experimental framework that specified a minimum number of searchers, the order in which searchers were assigned questions, and the set of data to be collected. This framework did not provide for a comparison of systems across sites, but did allow groups to estimate the effect of their own experimental manipulation free and clear of the main (additive) effects of searcher and topic. Six groups participated in the interactive track, with some groups performing more than the minimum number of searches. Of the 829 responses submitted to TREC across all topics and groups, 309 (37 %) found no correct answer, suggesting that the 5-minute time limit made the task challenging. The percentage of no answer found was roughly the same for the two types of questions (36 % for the find any n questions and 39 % for the comparison questions). 3.7 The Filtering track The filtering task is to retrieve just those documents in a document stream that match the user's interest as represented by the query. The main focus of the track was an adaptive filtering task. In this task, a filtering system starts with just a query derived from the topic statement (for TREC-9, the system also received a few (< 5) relevant documents), and processes documents one at a time in date order. If the system decides to retrieve a document, it obtains the relevance judgment for it, and can modify the query based on the judgment if desired. Two other, simpler tasks were also part of the track. In the batch filtering task, the system is given a topic and a (relatively large) set of known relevant documents. The system creates a query from the topic and known relevant documents, and must then decide whether or not to retrieve each document in the test portion of the collection. In the routing task, the system again builds a query from a topic statement and a set of relevant documents, but then uses the query to rank the test portion of the collection. Ranking the collection by similarity to the query (routing) is an easier problem than making a binary decision as to whether a document should be retrieved (batch filtering) because the latter requires a threshold that is difficult to set appropriately. The document set for the TREC-9 filtering task was the OHSUMED test collection [8]. The test docu- ments were the documents from 1988-1991, while a set of documents from 1987 were available for training (if the particular task allowed training). There were three topic sets: the queries from the OHSUMED test collection, a set of almost 5000 MeSH headings that were treated as topic statements, and a subset of 500 MeSH headings. Since the track used an existing test collection, no relevance judgments were made at NIST for the track. Research into appropriate evaluation methods for filtering runs (which do not produce a ranked list and therefore cannot be evaluated by the usual IR evaluation measures) has been a major thrust of the filtering track. The earliest filtering tracks used linear utility functions as the evaluation metric. With a linear utility function, a system is rewarded some number of points for retrieving a relevant document and penalized a different number of points for retrieving an irrelevant document. Utility functions are attractive because 11 they directly reflect the experience of a user of the filtering system. Unfortunately, there are drawbacks to the functions as evaluation measures. Utilities do not average well because the best possible score for each topic is a function of the number of relevant documents for that topic, and the worst possible score is essentially unbounded. Thus topics that have many relevant documents will dominate an average, and a single poorly performing topic can eclipse all other topics. Furthermore, it is difficult to know how to set the relative worth of relevant and irrelevant documents. For example, one of the utility functions used in the TREC-8 track rewarded systems with three points for retrieving a relevant document and penalized systems two points for retrieving an irrelevant document. This actually defines a very difficult filtering task: using this utility function, the average behavior of each of the TREC-8 filtering systems was worse than the baseline of retrieving no documents at all. The TREC-9 track used a bounded utility function as one measure and introduced a new \"precision-oriented\" measure, T9P. The bounded utility function rewarded systems two points for a relevant document and penalized systems one point for an irrelevant document, and in addition set a limit on the worst possible score a topic could receive. The idea of the precision-oriented measure was to penalize systems for not retrieving a sufficient number of documents, which for TREC-9 was 50 documents. Thus, j,gp _ number of relevant retrieved max(number retrieved, 50) Seventy-five runs from 14 different groups were submitted to the filtering track. Forty-one of the runs were adaptive filtering runs, 19 runs were batch filtering runs, and 15 runs were routing runs. Unlike TREC-8, several TREC-9 adaptive filtering systems obtained good average utilities while retrieving an adequate number of documents. In addition, the adaptive filtering scores were relatively close to the scores for the much easier routing task, as demonstrated by comparing the scores from the new T9P measure to the average of precision at 50 documents retrieved for the routing runs. 4 The Future The next TREC, TREC 2001, will see a few changes in the tracks that are offered. The spoken document track has met its goals and will therefore be discontinued. A new track that will focus on content-based access to digital video will continue TREC's interest in multimedia retrieval. The query track will cease to be a track and evolve into a \"station.\" That is, the TREC web site will serve as a repository for both query statements and retrieval results produced from those queries. This will allow the research of the track to continue without the time pressures of the TREC deadlines. The remaining tracks will continue, though the specific task involved in the track will likely change. The web track will include so-called navigational topics [14] in addition to the informational topics TREC has traditionally used. The cross-language track will focus on retrieving Arabic documents using English, French, or Arabic topics. In addition to the fact-based, short-answer questions used in the first two years of the question answering track, the 2001 track will feature a pilot study using questions that require information from multiple documents to be combined to form a correct answer. The task in the interactive track will most likely involve observing subjects using the live web to accomplish a specific task. The observations made in the track will be used to inform the definition of a task with a metrics-based evaluation plan for the following year's track, as suggested by the SIGIR Workshop on Interactive Retrieval at TREC and Beyond [7] held after SIGIR 2000. The filtering track will continue to focus on adaptive filtering using a new collection of data released by Reuters (see http://about.reuters.com/resecirchaiidstandards/corpus/). References [1] Peter Bailey, Nick Craswell, and David Hawking. Engineering a multi-purpose test collection for web retrieval experiments. Information Processing and Management. To appear. [2] David Banks, Paul Over, and Nien-Fan Zhang. Blind men and elephants: Six approaches to TREC data. Information Retrieval, 1:7-34, 1999. [3] Nicholas J. Belkin and W. Bruce Croft. Information filtering and information retrieval: Two sides of the same coin? Communications of the ACM, 35(12):29-38, December 1992. 12 Chris Buckley. trec_eval IR evaluation package. Available from ftp://ftp.cs.cornell.edu/pub/ smart. C. W. Cleverdon, J. Mills, and E. M. Keen. Factors determining the performance of indexing systems. Two volumes, Cranfield, England, 1968. S. Harabagiu, D. Moldovan, M. Pasca, R. Mihalcea, M. Surdeanu, R. Bunescu, R. Girju, V. Rus, and P. Morarescu. FALCON: Boosting knowledge for answer engines. In Voorhees and Harman [18]. William Hersh and Paul Over. SIGIR workshop on interactive retrieval at TREC and beyond. SIGIR Forum, 34(l):24-27, Spring 2000. W.R. Hersh, C. Buckley, T.J. Leone, and D.H. Hickam. OHSUMED: An interactive retrieval evalua- tion and new large test collection for research. In W. Bruce Croft and C.J. van Rijsbergen, editors, Proceedings of the 11th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval, pages 192-201, 1994. Noriko Kando, Kazuko Kuriyama, Toshihiko Nozue, Koji Eguchi, Hiroyuki Kato, and Souichiro Hidaka. Overview of IR tasks at the first NTCIR workshop. In Proceedings of the First NTCIR Workshop on Research in Japanese Text Retrieval and Term Recognition, pages 11-44, 1999. G. Salton, editor. The SMART Retrieval System: Experiments in Automatic Document Processing. Prentice-Hall, Inc. Englewood Cliffs, New Jersey, 1971. Linda Schamber. Relevance and information behavior. Annual Review of Information Science and Technology, 29:3-48, 1994. K. Sparck Jones and C. van Rijsbergen. Report on the need for and provision of an \"ideal\" information retrieval test collection. British Library Research and Development Report 5266, Computer Laboratory, University of Cambridge, 1975. Karen Sparck Jones. Information Retrieval Experiment. Butterworths, London, 1981. Bob Travis and Andrei Broder. The need behind the query: Web search vs classic information retrieval, http : //www. inf onortics . com/ searchengines/ shOl/ slides-01/ shOlpro .html. Ellen M. Voorhees. Variations in relevance judgments and the measurement of retrieval effectiveness. Information Processing and Management, 36:697-716, 2000. Ellen M. Voorhees and Donna Harman. Overview of the eighth Text REtrieval Conference (TREC-8). In E.M. Voorhees and D.K. Harman, editors, Proceedings of the Eighth Text REtrieval Conference (TREC-8), pages 1-24, 2000. NIST Special Publication 500-246. Electronic version available at http: / / tree . nist . gov/pubs . html. Ellen M. Voorhees and Donna Harman. Overview of the sixth Text REtrieval Conference (TREC-6). Information Processing and Management, 36(l):3-35, January 2000. E.M. Voorhees and D.K. Harman, editors. Proceedings of the Ninth Text REtreival Conference (TREC-9), 2001. J. Xu and R. Weischedel. TREC-9 cross-lingual retrieval at BBN. In Voorhees and Harman [18]. Justin Zobel. How reliable are the results of large-scale information retrieval experiments? In W. Bruce Croft, Alistair Moffat, C.J. van Rijsbergen, Ross Wilkinson, and Justin Zobel, editors, Proceedings of the 21st Annual International ACM SIGIR Conference on Research and Development in Information Retrieval, pages 307-314, Melbourne, Australia, August 1998. ACM Press, New York. 13 TREC-9 Cross-Language Information Retrieval (English - Chinese) Overview Fredric Gey and Aitao Chen UC DATA and SIMS University of California, Berkeley e-mail: gey@ucdata.berkeley.edu, aitao@sims.berkeley.edu Abstract Sixteen groups participated in the TREC-9 cross-language information retrieval track which focussed on retrieving Chinese language documents in response to 25 English queries. A variety of CLIR approaches were tested and a rich set of experiments performed which measured the utility of various resources such as machine translation and parallel corpora, as well as pre- and post- translation query expansion using pseudo-relevance feedback. 1 Introduction For TREC-9 the cross-language information retrieval task was to utilize English queries against Chinese documents. This aspect of multilingual information access at TREC-9 was the seventh year in which non-English document retrieval was tested and evaluated, and the fourth year for which cross-language information retrieval has been experimented with. In TREC-3, retrieval of 25 queries against a Mexican newspaper corpus was tested by four groups. Spanish language retrieval was evaluated in TREC-3, TREC-4 (another 25 queries for the same Mexican corpus), and TREC-5 (where an European Spanish corpus was used). In TREC-5 a Chinese language track was introduced using both newspaper (People's Daily) and newswire (XinHua) sources from People's Republic of China and 25 Chinese queries with an English translation supplied. The TREC-5 corpus was represented with the GB character set for the simplified Chinese language of PRC. Chinese monolingual experiments on this collection were done in TREC-5 and TREC-6 and sparked serious research into Chinese text segmentation methods using dictionary methods as well as statistical methods using measures such as mutual information. Compar- isons have been made with simple overlapping bigram segmentation methods for monolingual Chinese retrieval. TREC conferences TREC-6, TREC-7 and TREC-8 has cross language tracks which focussed upon European languages (English, French, German, and later Italian). Following TREC-8 the venue for evaluating European language retrieval moved to Europe with the Cross-Language Evaluation Forum (CLEF) first held in Lisbon in September 2000 [9]. 2 Task Description As in past TREC cross-language information retrieval evaluations, the task for each group was to match topics in one language (English in this case) against documents in another language (Chinese) and return a ranked list of the top 1000 documents associated with each topic. Multiple runs were allowed for each group but one run using only the title and description field was required. Evaluation then proceeded by pooling ranks and manual examination of the pools by human judges who decide upon the relevance or irrelevance of each document in the pool. Once relevance judgments were established the usual measures of recall and precision could be computed upon the ranked list of each entry. 15 2.1 Topics Twenty-five topics in English (numbers CH55-CH79) were created at NIST. Two typical topics are Topic 56 (human rights violations) and Topic 79 (livestock in China): <top> <num> Number: CH56 <title> human rights violations <desc> Description: What human rights violations have occurred in countries outside of China according to the Chinese press. <narr> Narrative: Reports of human rights violations in China are not relevant. </top> <top> <num> Number: CH79 <title> livestock in China <desc> Description: What kinds of livestock are being raised in China? <narr> Narrative: A document that discusses livestock farming in China, but is not specific about the kind of livestock is not relevant. </top> These topics demonstrate two kinds of difficulty. For topic CH56, the limitation of relevant human rights violations to 'countries outside China' is one of discrimination between human rights news stories concerned within China and those whose focus is other than China. Topic CH79 illustrates the use of a general term (livestock) while requesting specificity (e.g. pigs) within the documents returned. 2.2 Documents The corpus for TREC-9's CLIR evaluation consisted of 126,937 documents (188 megabytes in size) with newspaper sources from Hong Kong for the periods 1998-1999. In distinction from the earlier TREC Chinese corpus, these sources were written in the richer traditional Chinese character set, encoded in the BIG5 encoding. In particular the source documents came from: • Hong Kong Commercial Daily (Aug 11, 1998 - Jul 31, 1999) • Hong Kong Daily News (Feb 1, 1999 - Jul 31, 1999) • Ta kung pao (Oct 21, 1998 - Mar 4, 1999) 3 Participants and General Approach Sixteen groups participated in the TREC-9 Chinese evaluation, listed here in alphabetical order: 16 BBN Technologies Fudan University (PRC) IBM T.J. Watson Research Center Johns Hopkins University (Applied Physics Laboratory) Korea Advanced Institute of Science and Technology Microsoft Research, China MNIS-TextWise Labs National Taiwan University Queens College, CUNY RMIT University (Australia) Telecordia Technologies, Inc. The Chinese University of Hong Kong Trans-EZ Inc. University of California at Berkeley University of Maryland University of Massachusetts The majority of approaches utilized word or phrase translation from English to Chinese by lookup in bilingual dictionaries or word lists. A number of groups used the Linguistic Data Consortium's English- Mandarin word list of approximately 120,000 pairs of words. Other dictionaries included the CETA (Chinese-English Translation Assistance) dictionary and the KingSoft online bilingual dictionary as well as local (proprietary) dictionaries. Other approaches (in particular BBN) made use of statistical association models to create bilingual dictionaries from the alignment of parallel English-Chinese Corpora. Corpora used for development of resources or pre/post query expansion included: • LDC parallel Hong Kong SAR Law • LDC parallel Hong Kong SAR News • Academica Sinica Balanced Corpus (ASBC) • TREC-6 (People's Daily and Xinhua News Agency) • Foreign Broadcast Information Service (FBIS) data • Bilingual data harvested from the WWW • Other local (proprietary) mono and bilingual corpora In addition a few commercial machine translation software packages were used and coupled with other resources. Extensive experimentation was done by some groups with query expansion, both before query trans- lation from English to Chinese and after translation using blind feedback from the top ranked documents of an initial retrieval. 17 4 Experimental and methodological details by group This section provides a summary of experiments run and methodological approaches tested by the eight groups with best-performing English-Chinese crosslingual runs, according to the official results (see Results section below). Experiments and approaches which seem unique are given more description in this section. Readers are directed to the individual papers for more detail. 4.1 BBN BBN [12] extended the hidden Markov model (HMM) for monolingual retrieval to cross-language retrieval by incorporating into the model the word translation probabilities. Two manually created lexicons (i.e, the LDC wordlist and CETA) and two parallel corpora (i.e, the Hong Kong News and Hong Kong Law) were used to translate English query words into Chinese. The parallel texts were first aligned at the sentence level iteratively using WEAVER, a statistical machine translation toolkit developed at Carnegie Mellon University. Then WEAVER was applied to the sentence-aligned parallel texts to estimate word translation probabilities. When the translation resources were used individually, the Hong Kong News corpus yielded the best performance, probably because of similarity in copies covered in the test documents and the Hong Kong News corpus. However when all four translation resources were combined, the overall precision was substantially better. Unlike many participating groups of the cross-language track, BBN did not attempt phrasal translation and disambiguation of translation terms. A number of retrieval runs were performed to test the impact of query expansion for three levels of query length. The results showed over 10% improvement for overall precision for both pre-translation and post-translation query expansion when either one was applied alone. But when both pre- and post-translation expansion were applied, the post-translation expansion did not further improve the overall precision execept for the short queries consisting of only titles. In their official monolingual run, the Chinese text was segmented into words using the built-in segmentor in BBN's IdentiFinder. The monolingual performance was slightly lower than the best cross-language retrieval performance. However later it was found that when the bigrams and unigrams were used in indexing, the monolingual performance increaded from .2888 to .3779. 4.2 Microsoft Research, China Microsoft Research China group used a slightly modified version of SMART as their retrieval system [4]. A series of experiments were carried out to test the impact on retrieval performance using different indexing units and their combination. The results show that combining word-indexing and character- indexing works well for Chinese monolingual retrieval. They also used NLWin, a natural language processing system developed by Microsoft, to identify multi-word phrases and unknown words in the Chinese texts. They developed a co-occurrence based method to disambiguate translation terms, and a phrase detection and translation technique which improved the retrieval performance over the primitive dictionary-based translation. The phrases were identified using NLWin, and complex phrases were translated into Chinese based on a statistical model that maximizes the probability of phrase translation patterns and bigram probabilities estimated from a bigram language model trained on a large Chinese corpus. An interesting feature of the phrasal translation was that the Chinese words were put in the approriate order which may differ from the order of the source English words. About 125 MB of Chinese and English parallel texts were automatically mined from the Internet. A statistical translation model which is a variant of the IBM model was applied to the sentence-aligned parallel text to estimate word translation probabilities. When translation disambiguation and phrasal translations were augmented by statistical translation, the cross-language retrieval performance was as good as that obtained using IBM HomePage Dictionary 2000, a commercial Engish- Chinese machine translation system. The best cross- language retrieval run MSRCN2 combined bilingual lexicon, parallel texts mined from the Internet, and the machine translation system. Both pre- and post-translation query expansion were tried, however the pre-translation query expansion did not improve the overall precision. 18 4.3 Fudan University, China The document scoring function used by Fudan University group was based on the maximum likelihood ratio formula developed by MIT. A number of rule-based named entity extractors were used to identify words that are not in the segmentation dictionary. In addition, the occurrence frequency and mu- tual information between characters of unidentified strings were used to identify unknown words. The translation resources used for query translation consist of three dictionaries: a general English-Chinese dictionary, a technical terminology dictionary, and an idiom dictionary. The translated queries were further expanded using a Chinese thesaurus of nearly 70,000 entries. The best cross-language run was the one without pseudo relevance feedback [11]. 4.4 Chinese University of Hong Kong The CUHK group translated the queries into Chinese by considering two adjacent words each time. Among all possible translation pairs found in a bilingual dictionary, the one with the hightest similarity was chosen as the final translation for the source adjacent words. They experimented with pre- and post-translation query expansion using Rocchio relevance feedback within the SMART retrieval system. The pre-translation query expansion improved the overall precision from .1862 to .2642, an increase of 42% over the baseline run. However the post-translation did not gain any further improvement [5]. 4.5 Queens College, New York The Queens College group used a commercial machine translation software named HuaJian and the LDC wordlist augmented with an additional 6,000 translation pairs extracted from the Hong Kong Laws corpus to translate the queries. For dictionary lookup, up to six translation terms were kept for each query term. An equal weight was assigned to each translation terms of the same source query term. The final result for a cross-language run was produced by combining the result using the MT-translated queries and the result using the dictionary-translated queries. The best cross-language run named HxD combined MT and augmented LDC wordlist translations with pre- and post-translation query expansion and Chinese collection enrichment [6]. 4.6 University of Massachusetts The University of Massachusetts group used their INQUERY system with Local Context Analysis (LCA) technique for query expansion. The Chinese queries were translated into English by looking up multi- word phrases or words in a bilingual dictionary built by merging two Chinese-English dictionaries with an English-Chinese dictionary. Multiple translations were retained and treated as synonyms. Both pre- and post-translation query expansion using LCA were tried. The post-translation query expansion gained very little improvement [1]. 4.7 IBM Research The IBM group used a character-based statistical model to translate the English queries into Chinese, and a word-based statistical model supplemented with the LDC dictionary to translate the Chinese documents into English, both models being trained on Hong Kong News and Hong Kong Law parallel corpora. Their official run was a merging of the results from three runs based statistical query translation, commercial MT-based query translation, and statistical document translation [3]. 4.8 Korea Advanced Institute for Science and Technology (KAIST) The KAIST group experimented with query translations using bilingual dictionaries and machine trans- lation systems. The cross-language run using two bilingual dictionaries of 50,000 and 15,000 entries respectively outperformed the one using two machine translation systems. They observed that some of the proper names in the queries are spelled out in Chinese Pinyin (e.g., Daya Wan) and attempted to 19 obtain the Chinese names based on a Chinese pinyin table and the occurrence statistics of the characters in the Chinese collection [7]. 5 Relevance judgments and pool contributions In order to create a pool of documents for each topic for human evaluation of relevance, each participating group was invited to nominate a single entry run from the monolingual and/or cross-lingual tasks to be included in the judgments. This produced 39 cross-lingual runs and 13 monolingual runs. All but one of the runs was automatic. The top 50 ranked documents were taken from each nominated run and added to the pool to be evaluated. As usual duplicated documents from runs with overlap are removed to produce a unique list of documents for each topic. The resulting document pools had mean size of 598 documents (39 percent of the maximum pool size) to be read by the judges. The relevant documents over the pools came from the component run-types as follows: thirteen percent were only found by monolingual runs, twenty-eight percent came from crosslingual runs only, while the remaining 59 percent were found in both monolingual and crosslingual runs. Figure 1 shows the number of unique documents contributed by site. 50 „ 45 I 40 5 35 % 30 J 25 £ 20 § 15 1 10 * 5 0 □ Both ■ Cross □ Mono t <* ^ X ;V: >• c S A- *. \" ^ ? * S 12 o J i S *>* to Kj <j <c £ <? if-/* * TREC-9 CUR Track Overview Figure 1: Unique Relevant Documents by Site The only groups which contributed more than 20 relevant documents to the pool were Fudan Uni- versity [11] and Berkeley [2]. 6 Results Figure 2 displays the recall-precision graph for the top eight best-performing crosslingual sites (the figure shows only the best run from each site). Although some groups seem to have clearly outperformed 20 others, readers are cautioned that the evaluation only covered twenty-five queries, and it is unlikely that sufficient statistical signficance could be attained to confirm the rankings. The team from BBN outperformed all others with their hybrid combination of methods using a hidden markov ranking model, parallel corpora, and query expansion. Crosslingual results (top 8 sites) 1 -r BBN9XLA — * — msrcnl fdut9xl2 * 0 0.2 0.4 0.6 0.8 1 * narrative not used Recall 2 Figure 2: TREC-9 Cross-Language Retrieval Results The reporting in the graph mixes run modes, since three runs used only title and description, while the others used the narrative portion of the topic as well. The monolingual results for the top eight sights are displayed in figure 3. While they show very- little difference between sites, two sites, Johns Hopkins - apl9xmon [8] and Text Wise - TWmono [10] clearly performed significantly better on their monolingual than crosslingual runs. On the other hand, the best precision at 0.0 recall of 0.7079 from Berkeley (BRKCCA1) exceeded the best official CLIR run of 0.6078 by BBN. Finally, the overall average precision of the best official monolingual run (apl9xmon - 0.3085) trails the best crosslingual run (bbn9xla - 0.3485). BBN noted this discrepancy and attributed it in part to lack of query expansion in other bigram-based methods (BBN's official monolingual run used word-based indexing). BBN later implemented a bigram/unigram based monolingual algorithm with query expansion and achieved an overall monolingual precision of 0.3779 [12]. 7 Summary and Outlook The TREC-9 crosslingual information retreival task focussed this year on English-Chinese retrieval. Ma- jor experiments were undertaken using combinations of machine-readable dictionaries, machine transla- tion software, and parallel corpora of news stories, legal documents, and bilingual sites mined from the WWW. These were coupled with a variety of pre and post query expansion techniques. Many partic- ipating groups ran experiments which showed the contribution to overall precision of each component in the combination. The best performance was achieved by combining many of these techniques and by 21 Monolingual results (top 8 sites) Figure 3: TREC-9 Monolingual Chinese Results extensive use of the supportive resources. In 2001 the TREC cross-language track will move from Chinese experiments to retrieving from a collection Arabic documents using either English or French queries. However, CLIR experiments with Chinese collections will continue with the NTCIR evaluation organized and hosted by the National Institute of Informatics of Japan (http://research.nii.ac.jp/ntcir/workshop/work-en.html). We are grateful to Paul Over of NIST who supplied the basic information contained in our figures. His original Powerpoint presentation provided the basic outline from which this overview was written. References [1] J. Allan, M. Connell, W. B. Croft, F.-F. Feng, D. Fisher, , and X. Li. Inquery at trec-9. In D. K. Harman and E. Voorhees, editors, in this volume, 2001. [2] A. Chen, H. Jiang, and F. Gey. English-chinese cross-language ir using bilingual dictionaries. In D. K. Harman and E. Voorhees, editors, in this volume, 2001. [3] M. Franz, S. McCarly, and W.-J. Zhu. English-chinese information retrieval at ibm. In D. K. Harman and E. Voorhees, editors, in this volume, 2001. [4] J. Gao, J.-Y. Nie, J. Zhang, E. Xun, Y. Su, M. Zhou, and C. Huang. Trec-9 clir experiments at msrcn. In D. K. Harman and E. Voorhees, editors, in this volume, 2001. [5] H. Jin and K.-F. Wong. Trec-9 clir at cuhk, disabmiguation by similarity values between adjacent words. In D. K. Harman and E. Voorhees, editors, in this volume, 2001. [6] K. Kwok, L. Grunfeld, N. Dinstl, and M. Chan. Trec-9 cross-language, web and question answering track experiments using pircsi. In D. K. Harman and E. Voorhees, editors, in this volume, 2001. 22 [7] K.-S. Lee, J.-H. Oh, J.-X. Huang, J.-H. Kim, and K.-S. Choi. Trec-9 experiments at kaist: Qa, clir and batch filtering. In D. K. Harman and E. Voorhees, editors, in this volume, 2001. [8] P. McNamee, J. Mayfield, and C. Piatko. The haircut system at trec-9. In D. K. Harman and E. Voorhees, editors, in this volume, 2001. [9] C. Peters, editor. Springer, Lecture Notes in Computer Science Series No. 2069, 2001. [10] M. Ruiz, S. Rowe, M. Forrester, and P. She"
    }
}
{
    "id": "dbpedia_1852_0",
    "rank": 14,
    "data": {
        "url": "https://www.science.gov/topicpages/m/matched%2Bcontrolled%2Bcohort",
        "read_more_link": "",
        "language": "en",
        "title": "matched controlled cohort: Topics by Science.gov",
        "top_image": "",
        "meta_img": "",
        "images": [
            "https://www.science.gov/scigov/desktop/en/images/SciGov_logo.png"
        ],
        "movies": [],
        "keywords": [],
        "meta_keywords": [
            ""
        ],
        "tags": null,
        "authors": [],
        "publish_date": null,
        "summary": "",
        "meta_description": "",
        "meta_lang": "en",
        "meta_favicon": "",
        "meta_site_name": "",
        "canonical_link": null,
        "text": "Nested case-control studies: should one break the matching?\n\nPubMed\n\nBorgan, Ãrnulf; Keogh, Ruth\n\n2015-10-01\n\nIn a nested case-control study, controls are selected for each case from the individuals who are at risk at the time at which the case occurs. We say that the controls are matched on study time. To adjust for possible confounding, it is common to match on other variables as well. The standard analysis of nested case-control data is based on a partial likelihood which compares the covariates of each case to those of its matched controls. It has been suggested that one may break the matching of nested case-control data and analyse them as case-cohort data using an inverse probability weighted (IPW) pseudo likelihood. Further, when some covariates are available for all individuals in the cohort, multiple imputation (MI) makes it possible to use all available data in the cohort. In the paper we review the standard method and the IPW and MI approaches, and compare their performance using simulations that cover a range of scenarios, including one and two endpoints.\n\nHospital Readmissions in a Community-based Sample of Homeless Adults: a Matched-cohort Study.\n\nPubMed\n\nSaab, Dima; Nisenbaum, Rosane; Dhalla, Irfan; Hwang, Stephen W\n\n2016-09-01\n\nHospital readmission rates are a widely used quality indicator that may be elevated in disadvantaged populations. The objective of this study was to compare the hospital readmission rate among individuals experiencing homelessness with that of a low-income matched control group, and to identify risk factors associated with readmission within the group experiencing homelessness. We conducted a 1:1 matched cohort study comparing 30-day hospital readmission rates between homeless patients and low-income controls matched on age, sex and primary reason for admission. Multivariate analyses using generalized estimating equations were used to assess risk factors associated with 30-day readmission in the homeless cohort. This study examined a cohort of 1,165 homeless adults recruited at homeless shelters and meal programs in Toronto, Ontario, between 6 December 2004 and 20 December 2005. The primary outcome was the occurrence of an unplanned medical or surgical readmission within 30Â days of discharge from hospital. Between 6 December 2004 and 31 March 2009, homeless participants (Nâ=â203) had 478 hospitalizations and a 30-day readmission rate of 22.2Â %, compared to 300 hospitalizations and a readmission rate of 7.0Â % among matched controls (ORâ=â3.79, 95Â % CI 1.93-7.39). In the homeless cohort, having a primary care physician (ORâ=â2.65, 95Â % CI 1.05-6.73) and leaving against medical advice (ORâ=â1.96, 95Â % CI 0.99-3.86) were associated with an increased risk of 30-day readmission. Homeless patients had nearly four times the odds of being readmitted within 30-days as compared to low-income controls matched on age, sex and primary reason for admission to hospital. Further research is needed to evaluate interventions to reduce readmissions among this patient population.\n\nFeasibility of reusing time-matched controls in an overlapping cohort.\n\nPubMed\n\nDelcoigne, BÃ©nÃ©dicte; Hagenbuch, Niels; Schelin, Maria Ec; Salim, Agus; LindstrÃ¶m, Linda S; Bergh, Jonas; Czene, Kamila; Reilly, Marie\n\n2018-06-01\n\nThe methods developed for secondary analysis of nested case-control data have been illustrated only in simplified settings in a common cohort and have not found their way into biostatistical practice. This paper demonstrates the feasibility of reusing prior nested case-control data in a realistic setting where a new outcome is available in an overlapping cohort where no new controls were gathered and where all data have been anonymised. Using basic information about the background cohort and sampling criteria, the new cases and prior data are \"aligned\" to identify the common underlying study base. With this study base, a Kaplan-Meier table of the prior outcome extracts the risk sets required to calculate the weights to assign to the controls to remove the sampling bias. A weighted Cox regression, implemented in standard statistical software, provides unbiased hazard ratios. Using the method to compare cases of contralateral breast cancer to available controls from a prior study of metastases, we identified a multifocal tumor as a risk factor that has not been reported previously. We examine the sensitivity of the method to an imperfect weighting scheme and discuss its merits and pitfalls to provide guidance for its use in medical research studies.\n\nRisk and protective factors for meningococcal disease in adolescents: matched cohort study.\n\nPubMed\n\nTully, Joanna; Viner, Russell M; Coen, Pietro G; Stuart, James M; Zambon, Maria; Peckham, Catherine; Booth, Clare; Klein, Nigel; Kaczmarski, Ed; Booy, Robert\n\n2006-02-25\n\nTo examine biological and social risk factors for meningococcal disease in adolescents. Prospective, population based, matched cohort study with controls matched for age and sex in 1:1 matching. Controls were sought from the general practitioner. Six contiguous regions of England, which represent some 65% of the country's population. 15-19 year olds with meningococcal disease recruited at hospital admission in six regions (representing 65% of the population of England) from January 1999 to June 2000, and their matched controls. Blood samples and pernasal and throat swabs were taken from case patients at admission to hospital and from cases and matched controls at interview. Data on potential risk factors were gathered by confidential interview. Data were analysed by using univariate and multivariate conditional logistic regression. 144 case control pairs were recruited (74 male (51%); median age 17.6). 114 cases (79%) were confirmed microbiologically. Significant independent risk factors for meningococcal disease were history of preceding illness (matched odds ratio 2.9, 95% confidence interval 1.4 to 5.9), intimate kissing with multiple partners (3.7, 1.7 to 8.1), being a university student (3.4, 1.2 to 10) and preterm birth (3.7, 1.0 to 13.5). Religious observance (0.09, 0.02 to 0.6) and meningococcal vaccination (0.12, 0.04 to 0.4) were associated with protection. Activities and events increasing risk for meningococcal disease in adolescence are different from in childhood. Students are at higher risk. Altering personal behaviours could moderate the risk. However, the development of further effective meningococcal vaccines remains a key public health priority.\n\nRisk and protective factors for meningococcal disease in adolescents: matched cohort study\n\nPubMed Central\n\nTully, Joanna; Viner, Russell M; Coen, Pietro G; Stuart, James M; Zambon, Maria; Peckham, Catherine; Booth, Clare; Klein, Nigel; Kaczmarski, Ed; Booy, Robert\n\n2006-01-01\n\nObjective To examine biological and social risk factors for meningococcal disease in adolescents. Design Prospective, population based, matched cohort study with controls matched for age and sex in 1:1 matching. Controls were sought from the general practitioner. Setting Six contiguous regions of England, which represent some 65% of the country's population. Participants 15-19 year olds with meningococcal disease recruited at hospital admission in six regions (representing 65% of the population of England) from January 1999 to June 2000, and their matched controls. Methods Blood samples and pernasal and throat swabs were taken from case patients at admission to hospital and from cases and matched controls at interview. Data on potential risk factors were gathered by confidential interview. Data were analysed by using univariate and multivariate conditional logistic regression. Results 144 case control pairs were recruited (74 male (51%); median age 17.6). 114 cases (79%) were confirmed microbiologically. Significant independent risk factors for meningococcal disease were history of preceding illness (matched odds ratio 2.9, 95% confidence interval 1.4 to 5.9), intimate kissing with multiple partners (3.7, 1.7 to 8.1), being a university student (3.4, 1.2 to 10) and preterm birth (3.7, 1.0 to 13.5). Religious observance (0.09, 0.02 to 0.6) and meningococcal vaccination (0.12, 0.04 to 0.4) were associated with protection. Conclusions Activities and events increasing risk for meningococcal disease in adolescence are different from in childhood. Students are at higher risk. Altering personal behaviours could moderate the risk. However, the development of further effective meningococcal vaccines remains a key public health priority. PMID:16473859\n\nHealth expenditures among high-risk patients after gastric bypass and matched controls.\n\nPubMed\n\nMaciejewski, Matthew L; Livingston, Edward H; Smith, Valerie A; Kahwati, Leila C; Henderson, William G; Arterburn, David E\n\n2012-07-01\n\nTo determine whether bariatric surgery is associated with reduced health care expenditures in a multisite cohort of predominantly older male patients with a substantial disease burden. Retrospective cohort study of bariatric surgery. Outpatient, inpatient, and overall health care expenditures within Department of Veterans Affairs (VA) medical centers were examined via generalized estimating equations in the propensity-matched cohorts. Bariatric surgery programs in VA medical centers. Eight hundred forty-seven veterans who were propensity matched to 847 nonsurgical control subjects from the same 12 VA medical centers. Bariatric surgical procedures. Health expenditures through December 2006. Outpatient, inpatient, and total expenditures trended higher for bariatric surgical cases in the 3 years leading up to the procedure and then converged back to the lower expenditure levels of nonsurgical controls in the 3 years after the procedure. Based on analyses of a cohort of predominantly older men, bariatric surgery does not appear to be associated with reduced health care expenditures 3 years after the procedure.\n\nMatched cohort comparison of endovascular abdominal aortic aneurysm repair with and without EndoAnchors.\n\nPubMed\n\nMuhs, Bart E; Jordan, William; Ouriel, Kenneth; Rajaee, Sareh; de Vries, Jean-Paul\n\n2018-06-01\n\nThe objective of this study was to examine whether prophylactic use of EndoAnchors (Medtronic, Santa Rosa, Calif) contributes to improved outcomes after endovascular aneurysm repair (EVAR) of abdominal aortic aneurysms through 2Â years. The Aneurysm Treatment Using the Heli-FX Aortic Securement System Global Registry (ANCHOR) subjects who received prophylactic EndoAnchors during EVAR were considered for this analysis. Imaging data of retrospective subjects who underwent EVAR at ANCHOR enrolling institutions were obtained to create a control sample. Nineteen baseline anatomic measurements were used to perform propensity score matching, yielding 99 matched pairs. Follow-up imaging of the ANCHOR and control cohorts was then compared to examine outcomes through 2Â years, using Kaplan-Meier survival analysis. Freedom from type Ia endoleak was 97.0%Â Â± 2.1% in the ANCHOR cohort and 94.1%Â Â± 2.5% in the control cohort through 2Â years (PÂ = .34). The 2-year freedom from neck dilation in the ANCHOR and control cohorts was 90.4%Â Â± 5.6% and 87.3%Â Â± 4.3%, respectively (PÂ = .46); 2-year freedom from sac enlargement was 97.0%Â Â± 2.1% and 94.0%Â Â± 3.0%, respectively (PÂ = .67). No device migration was observed. Aneurysm sac regression was observed in 81.1%Â Â± 9.5% of ANCHOR subjects through 2Â years compared with 48.7%Â Â± 5.9% of control subjects (PÂ = .01). Cox regression analysis found an inverse correlation between number of hostile neck criteria met and later sac regression (PÂ = .05). Preoperative neck thrombus circumference and infrarenal diameter were also variables associated with later sac regression, although not to a significant degree (PÂ = .10 and PÂ = .06, respectively). Control subjects with thrombus were significantly less likely to experience later sac regression than those without thrombus (6% and 43%, respectively; PÂ = .001). In ANCHOR subjects, rate of regression was not significantly different in subjects with or without thrombus (33% and 36\n\nMicrosurgery Versus Stereotactic Radiosurgery for Brain Arteriovenous Malformations: A Matched Cohort Study.\n\nPubMed\n\nChen, Ching-Jen; Ding, Dale; Wang, Tony R; Buell, Thomas J; Ilyas, Adeel; Ironside, Natasha; Lee, Cheng-Chia; Kalani, M Yashar; Park, Min S; Liu, Kenneth C; Sheehan, Jason P\n\n2018-05-12\n\nMicrosurgery (MS) and stereotactic radiosurgery (SRS) remain the preferred interventions for the curative treatment of brain arteriovenous malformations (AVM), but their relative efficacy remains incompletely defined. To compare the outcomes of MS to SRS for AVMs through a retrospective, matched cohort study. We evaluated institutional databases of AVM patients who underwent MS and SRS. MS-treated patients were matched, in a 1:1 ratio based on patient and AVM characteristics, to SRS-treated patients. Statistical analyses were performed to compare outcomes data between the 2 cohorts. The primary outcome was defined as AVM obliteration without a new permanent neurological deficit. The matched MS and SRS cohorts were each comprised of 59 patients. Both radiological (85 vs 11 mo; PÂ <Â .001) and clinical (92 vs 12 mo; PÂ <Â .001) follow-up were significantly longer for the SRS cohort. The primary outcome was achieved in 69% of each cohort. The MS cohort had a significantly higher obliteration rate (98% vs 72%; PÂ =Â .001), but also had a significantly higher rate of new permanent deficit (31% vs 10%; PÂ =Â .011). The posttreatment hemorrhage rate was significantly higher for the SRS cohort (10% for SRS vs 0% for MS; PÂ =Â .027). In subgroup analyses of ruptured and unruptured AVMs, no significant differences between the primary outcomes were observed. For patients with comparable AVMs, MS and SRS afford similar rates of deficit-free obliteration. Nidal obliteration is more frequently achieved with MS, but this intervention also incurs a greater risk of new permanent neurological deficit.\n\nAnesthesia and Poliomyelitis: A Matched Cohort Study.\n\nPubMed\n\nVan Alstine, Luke W; Gunn, Paul W; Schroeder, Darrell R; Hanson, Andrew C; Sorenson, Eric J; Martin, David P\n\n2016-06-01\n\nPoliomyelitis is a viral infectious disease caused by 1 of the 3 strains of poliovirus. The World Health Organization launched an eradication campaign in 1988. Although the number of cases of poliomyelitis has drastically declined, eradication has not yet been achieved, and there are a substantial number of survivors of the disease. Survivors of poliomyelitis present a unique set of challenges to the anesthesiologist. The scientific literature regarding the anesthetic management of survivors of poliomyelitis, however, is limited and primarily experiential in nature. Using a retrospective, matched cohort study, we sought to more precisely characterize the anesthetic implications of poliomyelitis and to determine what risks, if any, may be present for patients with a history of the disease. Using the Mayo Clinic Life Sciences System Data Discovery and Query Builder, study subjects were identified as those with a history of paralytic poliomyelitis who had undergone major surgery at Mayo Clinic Rochester between 2005 and 2009. For each case, 2 sex- and age-matched controls that underwent the same surgical procedure during the study period were randomly selected from a pool of possible controls. Medical records were manually interrogated with respect to demographic variables, comorbid conditions, operative and anesthetic course, and postoperative course. We analyzed 100 cases with 2:1 matched controls and found that the peri- and postoperative courses were very similar for both groups of patients. Pain scores, postanesthesia care unit admission, length of postanesthesia care unit stay, intensive care unit admission, length of intensive care unit stay, and initial extubation location were not significantly different between the 2 groups. Looking at pulmonary complications in our primary outcome, there was no significant difference between the 2 groups (17% vs 14% for polio versus control, respectively; conditional logistic regression odds ratio = 1.5; 95% confidence\n\nDegenerative changes in adolescent spines: a comparison of motocross racers and age-matched controls.\n\nPubMed\n\nDaniels, David J; Luo, T David; Puffer, Ross; McIntosh, Amy L; Larson, A Noelle; Wetjen, Nicholas M; Clarke, Michelle J\n\n2015-03-01\n\nMotocross racing is a popular sport; however, its impact on the growing/developing pediatric spine is unknown. Using a retrospective cohort model, the authors compared the degree of advanced degenerative findings in young motocross racers with findings in age-matched controls. Patients who had been treated for motocross-related injury at the authors' institution between 2000 and 2007 and had been under 18 years of age at the time of injury and had undergone plain radiographic or CT examination of any spinal region were eligible for inclusion. Imaging was reviewed in a blinded fashion by 3 physicians for degenerative findings, including endplate abnormalities, loss of vertebral body height, wedging, and malalignment. Acute pathological segments were excluded. Spine radiographs from age-matched controls were similarly reviewed and the findings were compared. The motocross cohort consisted of 29 riders (mean age 14.7 years; 82% male); the control cohort consisted of 45 adolescents (mean age 14.3 years; 71% male). In the cervical spine, the motocross cohort had 55 abnormalities in 203 segments (average 1.90 abnormalities/patient) compared with 20 abnormalities in 213 segments in the controls (average 0.65/patient) (p = 0.006, Student t-test). In the thoracic spine, the motocross riders had 51 abnormalities in 292 segments (average 2.04 abnormalities/patient) compared with 25 abnormalities in 299 segments in the controls (average 1.00/patient) (p = 0.045). In the lumbar spine, the motocross cohort had 11 abnormalities in 123 segments (average 0.44 abnormalities/patient) compared with 15 abnormalities in 150 segments in the controls (average 0.50/patient) (p = 0.197). Increased degenerative changes in the cervical and thoracic spine were identified in adolescent motocross racers compared with age-matched controls. The long-term consequences of these changes are unknown; however, athletes and parents should be counseled accordingly about participation in motocross\n\nMatched cohort study of topical tranexamic acid in cementless primary total hip replacement.\n\nPubMed\n\nSanz-Reig, Javier; Mas Martinez, Jesus; Verdu RomÃ¡n, Carmen; Morales Santias, Manuel; MartÃ­nez Gimenez, Enrique; Bustamante Suarez de Puga, David\n\n2018-03-29\n\nTranexamic acid has been shown to be effective in reducing blood loss after total hip replacement. The purpose of this study was to prospectively assess the effectiveness of topical TXA use to reduce blood loss after primary total hip replacement and to compare these outcomes with those of a matched control group from a similar cohort that did not have received tranexamic acid. This is a prospective matched control study to assess the effect of a 2 g topical tranexamic acid in 50Â mL physiological saline solution in total hip replacement. Primary outcomes were hemoglobin and hematocrit drop, and total blood loss. Secondary outcomes were transfusion rates, length of hospital stay, deep vein thrombosis, and pulmonary embolism events. We could match 100 patients to a control group. There were no statistical significantly differences between the two groups. The hemoglobin and hematocrit postoperative values were significantly higher in topical tranexamic acid group than in control group (Pâ<â0.001). The mean total blood loss was 769 in topical tranexamic acid group and 1163 in control group with significant differences (Pâ=â0.001), which meant 34% reduction in total blood loss. Length of stay was lower in topical tranexamic acid group. The risk of deep vein thrombosis and pulmonary events did not increase. A single dose of 2 g tranexamic acid in 50Â mL physiological saline solution topical administration was effective and safe in reducing bleeding in patients undergoing unilateral primary non-cemented total hip replacement compared to a matched control group.\n\nMortality and the relationship of somatic comorbidities to mortality in schizophrenia. A nationwide matched-cohort study.\n\nPubMed\n\nBitter, I; Czobor, P; Borsi, A; FehÃ©r, L; Nagy, B Z; Bacskai, M; Rakonczai, P; Hegyi, R; NÃ©meth, T; Varga, P; Gimesi-OrszÃ¡gh, J; Fadgyas-Freyler, P; Sermon, J; TakÃ¡cs, P\n\n2017-09-01\n\nWe conducted a matched-cohort study to assess mortality in schizophrenia and the relationship of mortality with comorbid somatic conditions and suicide attempts. A full-population register-based prospective matched-cohort study was performed including all eligible patients with schizophrenia in Hungary between 01/01/2005 and 31/12/2013. Control subjects were individually matched to patients with schizophrenia at a 5:1 ratio. The principal outcome measure was death due to any reason. A non-parametric approach was used for descriptive statistical purposes, the Kaplan-Meier model for survival analysis, and the Cox proportional-hazards regression model for inferential statistics. Patients with schizophrenia (n=65,169) had substantially higher risk of all-cause mortality than the control subjects (n=325,435) (RR=2.4; P<0.0001). Comorbidities and suicide attempts were associated with significantly increased mortality in both groups. As compared to the controls, 20-year old males with schizophrenia had a shorter life expectancy by 11.5years, and females by 13.7years; the analogous numbers for 45-year old schizophrenics were 8.1 and 9.6years, respectively. A significant mortality gapÂ -Â mainly associated with somatic comorbiditiesÂ -Â was detected between patients with schizophrenia and individually matched controls. Improved medical training to address the disparity in mortality, and many other factors including lack of resources, access to and model of medical care, lifestyle, medication side effects, smoking, stigma, need for early intervention and adequate health care organization could help to better address the physical health needs of patients with schizophrenia. Copyright Â© 2017 The Author(s). Published by Elsevier Masson SAS.. All rights reserved.\n\nPerformance of Disease Risk Score Matching in Nested Case-Control Studies: A Simulation Study.\n\nPubMed\n\nDesai, Rishi J; Glynn, Robert J; Wang, Shirley; Gagne, Joshua J\n\n2016-05-15\n\nIn a case-control study, matching on a disease risk score (DRS), which includes many confounders, should theoretically result in greater precision than matching on only a few confounders; however, this has not been investigated. We simulated 1,000 hypothetical cohorts with a binary exposure, a time-to-event outcome, and 13 covariates. Each cohort comprised 2 subcohorts of 10,000 patients each: a historical subcohort and a concurrent subcohort. DRS were estimated in the historical subcohorts and applied to the concurrent subcohorts. Nested case-control studies were conducted in the concurrent subcohorts using incidence density sampling with 2 strategies-matching on age and sex, with adjustment for additional confounders, and matching on DRS-followed by conditional logistic regression for 9 outcome-exposure incidence scenarios. In all scenarios, DRS matching yielded lower average standard errors and mean squared errors than did matching on age and sex. In 6 scenarios, DRS matching also resulted in greater empirical power. DRS matching resulted in less relative bias than did matching on age and sex at lower outcome incidences but more relative bias at higher incidences. Post-hoc analysis revealed that the effect of DRS model misspecification might be more pronounced at higher outcome incidences, resulting in higher relative bias. These results suggest that DRS matching might increase the statistical efficiency of case-control studies, particularly when the outcome is rare. Â© The Author 2016. Published by Oxford University Press on behalf of the Johns Hopkins Bloomberg School of Public Health. All rights reserved. For permissions, please e-mail: journals.permissions@oup.com.\n\nUsing full-cohort data in nested case-control and case-cohort studies by multiple imputation.\n\nPubMed\n\nKeogh, Ruth H; White, Ian R\n\n2013-10-15\n\nIn many large prospective cohorts, expensive exposure measurements cannot be obtained for all individuals. Exposure-disease association studies are therefore often based on nested case-control or case-cohort studies in which complete information is obtained only for sampled individuals. However, in the full cohort, there may be a large amount of information on cheaply available covariates and possibly a surrogate of the main exposure(s), which typically goes unused. We view the nested case-control or case-cohort study plus the remainder of the cohort as a full-cohort study with missing data. Hence, we propose using multiple imputation (MI) to utilise information in the full cohort when data from the sub-studies are analysed. We use the fully observed data to fit the imputation models. We consider using approximate imputation models and also using rejection sampling to draw imputed values from the true distribution of the missing values given the observed data. Simulation studies show that using MI to utilise full-cohort information in the analysis of nested case-control and case-cohort studies can result in important gains in efficiency, particularly when a surrogate of the main exposure is available in the full cohort. In simulations, this method outperforms counter-matching in nested case-control studies and a weighted analysis for case-cohort studies, both of which use some full-cohort information. Approximate imputation models perform well except when there are interactions or non-linear terms in the outcome model, where imputation using rejection sampling works well. Copyright Â© 2013 John Wiley & Sons, Ltd.\n\nCollagen fragment biomarkers as serological biomarkers of lean body mass â a biomarker pilot study from the DAHANCA25B cohort and matched controls\n\nPubMed Central\n\nNedergaard, Anders; Dalgas, Ulrik; Primdahl, Hanne; Johansen, JÃ¸rgen; Overgaard, Jens; Overgaard, Kristian; Henriksen, Kim; Karsdal, Morten Asser; LÃ¸nbro, Simon\n\n2015-01-01\n\nBackground Loss of muscle mass and function is an important complication to ageing and a range of pathologies, including, but not restricted to, cancer, organ failures, and sepsis. A number of interventions have been proposed ranging from exercise to anabolic pharmacological therapy, with varying success. Easily applicable serological biomarkers of lean and/or muscle mass and change therein would benefit monitoring of muscle mass during muscle atrophy as well as during recovery. We set out to validate if novel peptide biomarkers derived from Collagen III and VI were markers of lean body mass (LBM) or change therein in head and neck cancer patients in the Danish Head and Neck Cancer Group(DAHANCA) 25B cohort subjected to resistance training as well as in an age-matched and gender-matched control group. Methods Blood samples and dual X-ray absorptiometry data were measured at baseline, after 12 and 24âweeks in 41 HNSCC subjects of the DAHANCA 25B cohort of subjects recovering from neck and head cancer (stages provided in TableÂ 1), and at baseline only in 21 healthy age-matched and gender-matched controls. Serum from blood was analyzed for the ProC3, IC6, and C6M peptide biomarkers and LBM were derived from the dual X-ray absorptiometry scans. Results We were not able to show any correlation between biomarkers and LBM or C6M and anabolic response to exercise in recovering head and neck cancer patients. However, we did find that the biomarkers IC6, IC6/C6M, and ProC3 are biomarkers of LBM in the control group subjects (R2/P of 0.249/0.035, 0.416/0.007 and 0.178 and Pâ=â0.057, respectively), Conclusion In conclusion, the IC6, ProC3, and IC6/C6M biomarkers are indeed biomarkers of LBM in healthy individuals of both genders, but not in HNSCC patients. PMID:26673155\n\nCongenital lumbar spinal stenosis: a prospective, control-matched, cohort radiographic analysis.\n\nPubMed\n\nSingh, Kern; Samartzis, Dino; Vaccaro, Alexander R; Nassr, Ahmad; Andersson, Gunnar B; Yoon, S Tim; Phillips, Frank M; Goldberg, Edward J; An, Howard S\n\n2005-01-01\n\nDegenerative lumbar spinal stenosis manifests primarily after the sixth decade of life as a result of facet hypertrophy and degenerative disc disease. Congenital stenosis, on the other hand, presents earlier in age with similar clinical findings but with multilevel involvement and fewer degenerative changes. These patients may have subtle anatomic variations of the lumbar spine that may increase the likelihood of thecal sac compression. However, to the authors' knowledge, no quantitative studies have addressed various radiographic parameters of symptomatic, congenitally stenotic individuals to normal subjects. To radiographically quantify and compare the anatomy of the lumbar spine in symptomatic, congenitally stenotic individuals to age- and sex-matched, asymptomatic, nonstenotic controlled individuals. A prospective, control-matched, cohort radiographic analysis. Axial and sagittal magnetic resonance imaging (MRI) and lateral, lumbar, plain radiographs of 20 surgically treated patients who were given a clinical diagnosis of congenital lumbar stenosis by the senior author were randomized with images of 20, asymptomatic age- and sex-matched subjects. MRIs and lateral, lumbar, plain radiographs were independently quantitatively assessed by two individuals. Measurements obtained from the axial MRIs included: midline anterior-posterior (AP) vertebral body diameter, vertebral body width, midline AP canal diameter, canal width, spinal canal cross-sectional area, pedicle length, and pedicle width. From the sagittal MRIs, the following measurements were calculated: AP vertebral body diameter, vertebral body height, and AP canal diameter at the mid-vertebral level. On the lateral, lumbar, plain radiograph (L3 level), the AP diameters of the vertebral body spinal canal were measured. The images of these 40 individuals were then randomized and distributed in a blinded fashion to five separate spine surgeons who graded the presence and severity of congenital stenosis\n\nKnee manipulation under anaesthetic following total knee arthroplasty: a matched cohort design.\n\nPubMed\n\nDzaja, I; Vasarhelyi, E M; Lanting, B A; Naudie, D D; Howard, J L; Somerville, L; McCalden, R W; MacDonald, S J\n\n2015-12-01\n\nThe purpose of this study was to compare clinical outcomes of total knee arthroplasty (TKA) after manipulation under anaesthesia (MUA) for post-operative stiffness with a matched cohort of TKA patients who did not requre MUA. In total 72 patients (mean age 59.8 years, 42 to 83) who underwent MUA following TKA were identified from our prospective database and compared with a matched cohort of patients who had undergone TKA without subsequent MUA. Patients were evaluated for range of movement (ROM) and clinical outcome scores (Western Ontario and McMaster Universities Arthritis Index, Short-Form Health Survey, and Knee Society Clinical Rating System) at a mean follow-up of 36.4 months (12 to 120). MUA took place at a mean of nine weeks (5 to 18) after TKA. In patients who required MUA, mean flexion deformity improved from 10Â° (0Â° to 25Â°) to 4.4Â° (0Â° to 15Â°) (p < 0.001), and mean range of flexion improved from 79.8Â° (65Â° to 95Â°) to 116Â° (80Â° to 130Â°) (p < 0.001). There were no statistically significant differences in ROM or functional outcome scores at three months, one year, or two years between those who required MUA and those who did not. There were no complications associated with manipulation. At most recent follow-up, patients requiring MUA achieved equivalent ROM and clinical outcome scores when compared with a matched control group. While other studies have focused on ROM after manipulation, the current study adds to current literature by supplementing this with functional outcome scores. Â©2015 The British Editorial Society of Bone & Joint Surgery.\n\nMemory and phonological awareness in children with Benign Rolandic Epilepsy compared to a matched control group.\n\nPubMed\n\nNorthcott, Ellen; Connolly, Anne M; Berroya, Anna; McIntyre, Jenny; Christie, Jane; Taylor, Alan; Bleasel, Andrew F; Lawson, John A; Bye, Ann M E\n\n2007-06-01\n\nIn a previous study we demonstrated children with Benign Rolandic Epilepsy have normal intelligence and language ability. However, difficulties in verbal and visual memory and aspects of phonological awareness were found compared to normative data. To address the methodological limitations related to the use of normative data, we compared the same cohort of children with Benign Rolandic Epilepsy to a matched control group. Controls (n=40) matched on age and gender to the Benign Rolandic Epilepsy cohort underwent neuropsychological assessment. The life functioning of the control group was assessed using a modified version of the Quality of Life in Childhood Epilepsy Questionnaire (QOLCE). The study confirmed the previous findings of memory and phonological awareness difficulties. In addition, the children with Benign Rolandic Epilepsy had significantly lower IQ scores than the matched control group. Paired sample t-tests showed that on 8 of 11 QOLCE scales, children with Benign Rolandic Epilepsy were rated by parents as having poorer life functioning compared to matched controls, including lower parental ratings on the subscales of memory and language. Benign Rolandic Epilepsy has an excellent seizure prognosis, but this study further emphasizes potential cognitive difficulties. Using an age and gender matched control group, the previous findings of memory and phonological awareness difficulties were validated. These problems in cognition were also identified by parents of children with Benign Rolandic Epilepsy as problematic and impacting upon the child's quality of life.\n\nDo Stem Taper Microgrooves Influence Taper Corrosion in Total Hip Arthroplasty? A Matched Cohort Retrieval Study.\n\nPubMed\n\nArnholt, Christina M; MacDonald, Daniel W; Underwood, Richard J; Guyer, Eric P; Rimnac, Clare M; Kurtz, Steven M; Mont, Michael A; Klein, Gregg R; Lee, Gwo-Chin; Chen, Antonia F; Hamlin, Brian R; Cates, Harold E; Malkani, Arthur L; Kraay, Matthew J\n\n2017-04-01\n\nPrevious studies identified imprinting of the stem morphology onto the interior head bore, leading researchers to hypothesize an influence of taper topography on mechanically assisted crevice corrosion. The purpose of this study was to analyze whether microgrooved stem tapers result in greater fretting corrosion damage than smooth stem tapers. A matched cohort of 120 retrieved head-stem pairs from metal-on-polyethylene bearings was created controlling for implantation time, flexural rigidity, apparent length of engagement, and head size. There were 2 groups of 60 heads each, mated with either smooth or microgrooved stem tapers. A high-precision roundness machine was used to measure and categorize the surface morphology. Fretting corrosion damage at the head-neck junction was characterized using the Higgs-Goldberg scoring method. Fourteen of the most damaged heads were analyzed for the maximum depth of material loss and focused ion beam cross-sectioned to view oxide and base metal. Fretting corrosion damage was not different between the 2 cohorts at the femoral head (PÂ = .14, Mann-Whitney) or stem tapers (PÂ = .35). There was no difference in the maximum depths of material loss between the cohorts (PÂ = .71). Cross-sectioning revealed contact damage, signs of micro-motion, and chromium-rich oxide layers in both cohorts. Microgroove imprinting did not appear to have a different effect on the fretting corrosion behavior. The results of this matched cohort retrieval study do not support the hypothesis that taper surfaces with microgrooved stems exhibit increased inÂ vivo fretting corrosion damage or material release. Copyright Â© 2016 Elsevier Inc. All rights reserved.\n\nIs schizophrenia associated with an increased risk of chronic kidney disease? A nationwide matched-cohort study.\n\nPubMed\n\nTzeng, Nian-Sheng; Hsu, Yung-Ho; Ho, Shinn-Ying; Kuo, Yu-Ching; Lee, Hua-Chin; Yin, Yun-Ju; Chen, Hong-An; Chen, Wen-Liang; Chu, William Cheng-Chung; Huang, Hui-Ling\n\n2015-01-27\n\nThe impact of schizophrenia on vital diseases, such as chronic kidney disease (CKD), has not as yet been verified. This study aims to establish whether there is an association between schizophrenia and CKD. A nationwide matched cohort study. Taiwan's National Health Insurance Research Database. A total of 2338 patients with schizophrenia, and 7014 controls without schizophrenia (1:3), matched cohort for sex, age group, geography, urbanisation and monthly income, between 1 January 2003 and 31 December 2007, based on the International Classifications of Disease Ninth Edition (ICD-9), Clinical Modification codes. After making adjustments for confounding risk factors, a Cox proportional hazards model was used to compare the risk of developing CKD during a 3-year follow-up period from the index date. Of the 2338-subject case cohort, 163 (6.97%) developed a CKD, as did 365 (5.20%) of the 7014 control participants. Cox proportional hazards regression analysis revealed that patients with schizophrenia were more likely to develop CKD (HR=1.36, 95% CI 1.13 to 1.63; p<0.001). After adjusting for gender, age group, hypertension, diabetes mellitus, hyperlipidaemia, heart disease and non-steroid anti-inflammatory drugs (NSAIDs) usage, the HR for patients with schizophrenia was 1.25 (95% CI 1.04 to 1.50; p<0.05). Neither typical nor atypical antipsychotics was associated an increased risk of CKD in patients with schizophrenia. The findings from this population-based retrospective cohort study suggest that schizophrenia is associated with a 25% increase in the risk of developing CKD within only a 3-year follow-up period. Published by the BMJ Publishing Group Limited. For permission to use (where not already granted under a licence) please go to http://group.bmj.com/group/rights-licensing/permissions.\n\nLong-term survival following in-hospital cardiac arrest: A matched cohort studyâ\n\nPubMed Central\n\nFeingold, Paul; Mina, Michael J.; Burke, Rachel M.; Hashimoto, Barry; Gregg, Sara; Martin, Greg S.; Leeper, Kenneth; Buchman, Timothy\n\n2016-01-01\n\nBackground Each year, 200,000 patients undergo an in-hospital cardiac arrest (IHCA), with approximately 15â20% surviving to discharge. Little is known, however, about the long-term prognosis of these patients after discharge. Previous efforts to describe out-of-hospital survival of IHCA patients have been limited by small sample sizes and narrow patient populations Methods A single institution matched cohort study was undertaken to describe mortality following IHCA. Patients surviving to discharge following an IHCA between 2008 and 2010 were matched on age, sex, race and hospital admission criteria with non-IHCA hospital controls and follow-up between 9 and 45 months. KaplanâMeier curves and Cox PH models assessed differences in survival. Results Of the 1262 IHCAs, 20% survived to hospital discharge. Of those discharged, survival at 1 year post-discharge was 59% for IHCA patients and 82% for controls (p < 0.0001). Hazard ratios (IHCA vs. controls) for mortality were greatest within the 90 days following discharge (HR = 2.90, p < 0.0001) and decreased linearly thereafter, with those surviving to one year post-discharge having an HR for mortality below 1.0. Survival after discharge varied amongst IHCA survivors. When grouped by discharge destination, out of hospital survival varied; in fact, IHCA patients discharged home without services demonstrated no survival difference compared to their non-IHCA controls (HR 1.10, p = 0.72). IHCA patients discharged to long-term hospital care or hospice, however, had a significantly higher mortality compared to matched controls (HR 3.91 and 20.3, respectively; p < 0.0001). Conclusion Among IHCA patients who survive to hospital discharge, the highest risk of death is within the first 90 days after discharge. Additionally, IHCA survivors overall have increased long-term mortality vs. controls. Survival rates were varied widely with different discharge destinations, and those discharged to home, skilled nursing facilities or to\n\nDo Stem Taper Microgrooves Influence Taper Corrosion in Total Hip Arthroplasty? A Matched Cohort Retrieval Study\n\nPubMed Central\n\nArnholt, Christina M.; MacDonald, Daniel W.; Underwood, Richard; Guyer, Eric P.; Rimnac, Clare M.; Kurtz, Steven M.; Mont, Michael A.; Klein, Gregg; Lee, Gwo-Chin; Chen, Antonia F.; Hamlin, Brian; Cates, Harold; Malkani, Arthur; Kraay, Matthew\n\n2017-01-01\n\nBackground Previous studies identified imprinting of the stem morphology onto the interior head bore, leading researchers to hypothesize an influence of taper topography on mechanically assisted crevice corrosion (MACC). The purpose of this study was to analyze whether micro-grooved stem tapers result in greater fretting corrosion damage than smooth stem tapers. Methods A matched cohort of 120 retrieved head-stem pairs from metal-on-polyethylene bearings was created controlling for implantation time, flexural rigidity, apparent length of engagement, and head size. There were two groups of 60 heads each, mated with either smooth or micro-grooved stem tapers. A high precision roundness machine was used to measure and categorize the surface morphology. Fretting corrosion damage at the head/neck junction was characterized using the Higgs-Goldberg scoring method. Fourteen of the most damaged heads, were analyzed for the maximum depth of material loss and focused ion beam (FIB) cross-sectioned to view oxide and base metal. Results Fretting corrosion damage was not different between the two cohorts at the femoral head (p = 0.14, Mann Whitney) or stem tapers (p = 0.35). There was no difference in the maximum depths of material loss between the cohorts (p = 0.71). Cross sectioning revealed contact damage, signs of micro-motion, and chromium rich oxide layers in both cohorts. Micro-groove imprinting did not appear to have a different effect on the fretting corrosion behavior. Conclusion The results of this matched cohort retrieval study do not support the hypothesis that taper surfaces with micro-grooved stems exhibit increased in vivo fretting corrosion damage or material release. PMID:28111124\n\nMoving to a Highly Walkable Neighborhood and Incidence of Hypertension: A Propensity-Score Matched Cohort Study.\n\nPubMed\n\nChiu, Maria; Rezai, Mohammad-Reza; Maclagan, Laura C; Austin, Peter C; Shah, Baiju R; Redelmeier, Donald A; Tu, Jack V\n\n2016-06-01\n\nThe impact of moving to a neighborhood more conducive to utilitarian walking on the risk of incident hypertension is uncertain. Our study aimed to examine the effect of moving to a highly walkable neighborhood on the risk of incident hypertension. A population-based propensity-score matched cohort study design was used based on the Ontario population from the Canadian Community Health Survey (2001-2010). Participants were adults â¥ 20 years of age who moved from a low-walkability neighborhood (defined as any neighborhood with a Walk Score < 90) to either a high- (Walk Score â¥ 90) or another low-walkability neighborhood. The incidence of hypertension was assessed by linking the cohort to administrative health databases using a validated algorithm. Propensity-score matched Cox proportional hazard models were used. Annual health examination was used as a control event. Among the 1,057 propensity-score matched pairs there was a significantly lower risk of incident hypertension in the low to high vs. the low to low-walkability groups [hazard ratio = 0.46; 95% CI, 0.26, 0.81, p < 0.01]. The crude hypertension incidence rates were 18.0 per 1,000 person-years (95% CI: 11.6, 24.8) among the low- to low-walkability movers compared with 8.6 per 1,000 person-years (95% CI: 5.3, 12.7) among the low- to high-walkability movers (p < 0.001). There were no significant differences in the hazard of annual health examination between the two mover groups. Moving to a highly walkable neighborhood was associated with a significantly lower risk of incident hypertension. Future research should assess whether specific attributes of walkable neighborhoods (e.g., amenities, density, land-use mix) may be driving this relationship. Chiu M, Rezai MR, Maclagan LC, Austin PC, Shah BR, Redelmeier DA, Tu JV. 2016. Moving to a highly walkable neighborhood and incidence of hypertension: a propensity-score matched cohort study. Environ Health Perspect 124:754-760;âhttp://dx.doi.org/10.1289/ehp\n\nAnalysis of Clinical Cohort Data Using Nested Case-control and Case-cohort Sampling Designs. A Powerful and Economical Tool.\n\nPubMed\n\nOhneberg, K; Wolkewitz, M; Beyersmann, J; Palomar-Martinez, M; Olaechea-Astigarraga, P; Alvarez-Lerma, F; Schumacher, M\n\n2015-01-01\n\nSampling from a large cohort in order to derive a subsample that would be sufficient for statistical analysis is a frequently used method for handling large data sets in epidemiological studies with limited resources for exposure measurement. For clinical studies however, when interest is in the influence of a potential risk factor, cohort studies are often the first choice with all individuals entering the analysis. Our aim is to close the gap between epidemiological and clinical studies with respect to design and power considerations. Schoenfeld's formula for the number of events required for a Cox' proportional hazards model is fundamental. Our objective is to compare the power of analyzing the full cohort and the power of a nested case-control and a case-cohort design. We compare formulas for power for sampling designs and cohort studies. In our data example we simultaneously apply a nested case-control design with a varying number of controls matched to each case, a case cohort design with varying subcohort size, a random subsample and a full cohort analysis. For each design we calculate the standard error for estimated regression coefficients and the mean number of distinct persons, for whom covariate information is required. The formula for the power of a nested case-control design and the power of a case-cohort design is directly connected to the power of a cohort study using the well known Schoenfeld formula. The loss in precision of parameter estimates is relatively small compared to the saving in resources. Nested case-control and case-cohort studies, but not random subsamples yield an attractive alternative for analyzing clinical studies in the situation of a low event rate. Power calculations can be conducted straightforwardly to quantify the loss of power compared to the savings in the num-ber of patients using a sampling design instead of analyzing the full cohort.\n\nOptimal case-control matching in practice.\n\nPubMed\n\nCologne, J B; Shibata, Y\n\n1995-05-01\n\nWe illustrate modern matching techniques and discuss practical issues in defining the closeness of matching for retrospective case-control designs (in which the pool of subjects already exists when the study commences). We empirically compare matching on a balancing score, analogous to the propensity score for treated/control matching, with matching on a weighted distance measure. Although both methods in principle produce balance between cases and controls in the marginal distributions of the matching covariates, the weighted distance measure provides better balance in practice because the balancing score can be poorly estimated. We emphasize the use of optimal matching based on efficient network algorithms. An illustration is based on the design of a case-control study of hepatitis B virus infection as a possible confounder and/or effect modifier of radiation-related primary liver cancer in atomic bomb survivors.\n\nImpact of organised mammography screening on breast cancer mortality in a caseâcontrol and cohort study\n\nPubMed Central\n\nHeinÃ¤vaara, Sirpa; Sarkeala, Tytti; Anttila, Ahti\n\n2016-01-01\n\nBackground: The usefulness of caseâcontrol studies has been questioned. Our aim was to evaluate the long-term effect of screening on breast cancer mortality within the population-based mammography programme in Finland using a caseâcontrol design, and to compare the analyses with the earlier cohort study. Methods: The cases were women invited to screening, diagnosed and died from breast cancer in 1992â2011 while being 50â84 years at death. We chose 10 controls for each case with non-restrictive eligibility criteria. Our data included 1907 cases and 18â978 matched controls. We analysed associations between the screening participation and the risk of breast cancer death using the conditional Cox proportional hazards model. The effect estimates were corrected for self-selection bias. Results: An overall effect of screening was 0.67 (95% confidence interval (CI): 0.49â0.90), and that remained unchanged over time. Analyses with matching criteria comparable to the cohort study yielded an effect (0.70, 95% CI: 0.49â1.00) in 1992â2003 similar to that of the previous cohort analysis (0.72, 95% CI: 0.56â0.88). Conclusions: Organised mammography screening decreases mortality from breast cancer by 33% among the participants. If made comparable, a caseâcohort study can yield effect estimates similar to a cohort study. PMID:27010748\n\nMortality in patients with diabetes mellitus and Addison's disease: a nationwide, matched, observational cohort study.\n\nPubMed\n\nChantzichristos, Dimitrios; Persson, Anders; Eliasson, BjÃ¶rn; Miftaraj, Mervete; FranzÃ©n, Stefan; Bergthorsdottir, Ragnhildur; GudbjÃ¶rnsdottir, Soffia; Svensson, Ann-Marie; Johannsson, Gudmundur\n\n2017-01-01\n\nOur hypothesis was that patients with diabetes mellitus obtain an additional risk of death if they develop Addison's disease (AD). Nationwide, matched, observational cohort study cross-referencing the Swedish National Diabetes Register with Inpatient, Cancer and Cause of Death Registers in patients with diabetes (type 1 and 2) and AD and matched controls with diabetes. Clinical characteristics at baseline, overall, and cause-specific mortality were assessed. The relative risk of death was assessed using a Cox proportional hazards regression model. Between January 1996 and December 2012, 226 patients with diabetes and AD were identified and matched with 1129 controls with diabetes. Median (interquartile range) follow-up was 5.9 (2.7-8.6) years. When patients with diabetes were diagnosed with AD, they had an increased frequency of diabetes complications, but both medical history of cancer and coronary heart disease did not differ compared with controls. Sixty-four of the 226 patients with diabetes and AD (28%) died, while 112 of the 1129 controls (10%) died. The estimated relative risk increase (hazard ratio) in overall mortality in the diabetes and AD group was 3.89 (95% confidence interval 2.84-5.32) compared with controls with diabetes. The most common cause of death was cardiovascular in both groups, but patients with diabetes and AD showed an increased death rate from diabetes complications, infectious diseases and unknown causes. Patients with the rare combination of diabetes and AD showed a markedly increased mortality and died more frequently from infections and unknown causes than patients with diabetes alone. Improved strategy for the management of this combination of metabolic disorders is needed. Â© 2017 European Society of Endocrinology.\n\nIncidence of type 2 diabetes after bariatric surgery: population-based matched cohort study.\n\nPubMed\n\nBooth, Helen; Khan, Omar; Prevost, Toby; Reddy, Marcus; Dregan, Alex; Charlton, Judith; Ashworth, Mark; Rudisill, Caroline; Littlejohns, Peter; Gulliford, Martin C\n\n2014-12-01\n\nThe effect of currently used bariatric surgical procedures on the development of diabetes in obese people is not well defined. We aimed to assess the effect of bariatric surgery on development of type 2 diabetes in a large population of obese individuals. We did a matched cohort study of adults (age 20â100 years) identified from a UK-wide database of family practices, who were obese (BMI â¥30 kg/m2) and did not have diabetes. We enrolled 2167 patients who had undergone bariatric surgery between Jan 1, 2002, and April 30, 2014, and matched them--according to BMI, age, sex, index year, and HbA1c--with 2167 controls who had not had surgery. Procedures included laparoscopic gastric banding (n=1053), gastric bypass (795), and sleeve gastrectomy (317), with two procedures undefined. The primary outcome was development of clinical diabetes, which we extracted from electronic health records. Analyses were adjusted for matching variables, comorbidity, cardiovascular risk factors, and use of antihypertensive and lipid-lowering drugs. During a maximum of 7 years of follow-up (median 2Â·8 years [IQR 1Â·3â4Â·5]), 38 new diagnoses of diabetes were made in bariatric surgery patients and 177 were made in controls. By the end of 7 years of follow-up, 4Â·3% (95% CI 2Â·9â6Â·5) of bariatric surgery patients and 16Â·2% (13Â·3â19Â·6) of matched controls had developed diabetes. The incidence of diabetes diagnosis was 28Â·2 (95% CI 24Â·4â32Â·7) per 1000 person-years in controls and 5Â·7 (4Â·2â7Â·8) per 1000 person-years in bariatric surgery patients; the adjusted hazard ratio was 0Â·20 (95% CI 0Â·13â0Â·30, p<0Â·0001). This estimate was robust after varying the comparison group in sensitivity analyses, excluding gestational diabetes, or allowing for competing mortality risk. Bariatric surgery is associated with reduced incidence of clinical diabetes in obese participants without diabetes at baseline for up to 7 years after the procedure. UK National Institute for Health\n\nCarbapenem-resistant Enterobacteriaceae colonization and infection in critically ill patients: a retrospective matched cohort comparison with non-carriers.\n\nPubMed\n\nDickstein, Y; Edelman, R; Dror, T; Hussein, K; Bar-Lavie, Y; Paul, M\n\n2016-09-01\n\nTo examine whether carbapenem-resistant Enterobacteriaceae (CRE) carriage is associated with incidence of clinical infection as a means of assessing whether the morbidity and mortality associated with these bacteria are mediated by underlying conditions or intrinsic properties of CRE. This retrospective matched cohort study compared the incidence of invasive infections in CRE-colonized patients and matched non-carriers in the intensive care unit (ICU). The primary outcome was infection caused by CRE of the same species as the colonizing strain among CRE carriers, and infections caused by carbapenem-sensitive strains of the same organism in non-carriers. Hospital discharge and death were considered as competing events. Competing-risks hazard analysis was performed for the entire cohort and for a nested cohort matched by Acute Physiology and Chronic Health Evaluation (APACHE) II scores, stratified by matching. In total, 146 CRE carriers were compared with 292 non-carriers. Patients were well matched for most risk factors for Enterobacteriaceae infection, including age, renal failure, previous invasive infection, previous hospitalization, APACHE II score, length of mechanical ventilation, length of hospitalization and CRE carriage. On regression analysis, colonization with CRE was independently associated with Enterobacteriaceae infection {cause-specific hazard ratio (CSHR) 2.06 [95% confidence interval (CI) 1.03-4.09]}. On regression analysis of the APACHE-II-matched cohort (N=284), colonization with CRE remained significantly associated with Enterobacteriaceae infection [CSHR 3.32 (95% CI 1.31-8.43)]. Colonization with CRE was associated with at least a two-fold increased risk of infection by the colonizing strain amongst ICU patients. Copyright Â© 2016 The Healthcare Infection Society. Published by Elsevier Ltd. All rights reserved.\n\nMagnetic resonance imaging based morphologic evaluation of the pineal gland for suspected pineoblastoma in retinoblastoma patients and age-matched controls.\n\nPubMed\n\nPham, Thi Thai Hien; Siebert, Eberhard; Asbach, Patrick; Willerding, Gregor; Erb-Eigner, Katharina\n\n2015-12-15\n\nThe purpose of this study was to evaluate the morphologic magnetic resonance imaging (MRI) characteristics of the pineal gland in retinoblastoma (Rb) patients without and with pineoblastoma in comparison to age-matched controls to improve early identification of pineoblastomas (trilateral retinoblastoma, TRb). 80 patients with retinoblastoma and 80 age-matched controls who had undergone brain MRI were included in this retrospective institutional review board approved cohort study. Two readers analyzed the following MR characteristics of the pineal gland: signal intensity on T1- and T2-weighted images, enhancement pattern, delineation of the gland, presence of cystic component, size of pineal gland and size of pineal cysts, respectively. A third reader assessed all images for the presence or absence of pineoblastoma. 3 patients were positive (TRb cohort) and 77 negative for pineoblastoma (non-TRb cohort). The mean maximum diameter of the pineal gland was 6.4mm in Rb patients and 6.3mm in age-matched controls. The mean volume of the pineal gland in Rb patients was 93.1mm(3) and was 87.6mm(3) in age-matched controls. Considering all available MRI scans the mean maximum diameter of the pineal gland in TRb patients was 11.2mm and the mean volume in TRb patients was 453.3mm(3). The third reader identified pineoblastomas with a sensitivity of 100% (3 of 3) and a specificity of 94% (72 of 77). Our non-TRb patients did not show significant differences in the size of the pineal gland and pineal gland cysts compared to age-matched controls. The presented data can serve as a reference for the volume of normal pineal glands and pineal cysts in the diagnostic work-up of Rb patients with suspected pineoblastoma. Copyright Â© 2015 Elsevier B.V. All rights reserved.\n\nSequential cohort design applying propensity score matching to analyze the comparative effectiveness of atorvastatin and simvastatin in preventing cardiovascular events.\n\nPubMed\n\nHelin-Salmivaara, Arja; Lavikainen, Piia; Aarnio, Emma; Huupponen, Risto; Korhonen, Maarit Jaana\n\n2014-01-01\n\nSequential cohort design (SCD) applying matching for propensity scores (PS) in accrual periods has been proposed to mitigate bias caused by channeling when calendar time is a proxy for strong confounders. We studied the channeling of patients according to atorvastatin and simvastatin initiation in Finland, starting from the market introduction of atorvastatin in 1998, and explored the SCD PS approach to analyzing the comparative effectiveness of atorvastatin versus simvastatin in the prevention of cardiovascular events (CVE). Initiators of atorvastatin or simvastatin use in the 45-75-year age range in 1998-2006 were characterized by their propensity of receiving atorvastatin over simvastatin, as estimated for 17 six-month periods. Atorvastatin (10 mg) and simvastatin (20 mg) initiators were matched 1â¶1 on the PS, as estimated for the whole cohort and within each period. Cox regression models were fitted conventionally, and also for the PS matched cohort and the periodically PS matched cohort, to estimate the hazard ratios (HR) for CVEs. Atorvastatin (10 mg) was associated with a 11%-12% lower incidence of CVE in comparison with simvastatin (20 mg). The HR estimates were the same for a conventional Cox model (0.88, 95% confidence interval 0.85-0.91), for the analysis in which the PS was used to match across all periods and the Cox model was adjusted for strong confounders (0.89, 0.85-0.92), and for the analysis in which PS matching was applied within sequential periods (0.88, 0.84-0.92). The HR from a traditional PS matched analysis was 0.80 (0.77-0.83). The SCD PS approach produced effect estimates similar to those obtained in matching for PS within the whole cohort and adjusting the outcome model for strong confounders, but at the cost of efficiency. A traditional PS matched analysis without further adjustment in the outcome model produced estimates further away from unity.\n\nImpact of nutrition support on clinical outcome and cost-effectiveness analysis in patients at nutritional risk: AÂ prospective cohort study with propensity score matching.\n\nPubMed\n\nZhang, Hui; Wang, Yang; Jiang, Zhu-Ming; Kondrup, Jens; Fang, Hai; Andrews, Martha; Nolan, Marie T; Mu, Shao-Yu; Zhang, Jun; Yu, Kang; Lu, Qian; Kang, Wei-Ming\n\n2017-05-01\n\nThere is a lack of evidence regarding the economic effects of nutrition support in patients at nutritional risk. The aim of this study was to perform a cost-effectiveness analysis by comparing an adequate nutrition support cohort with a no-support cohort. A prospective observational study was performed in the surgical and medical gastroenterology wards. We identified patients at nutritional risk and the provision of nutrition support by the staff, unaware of the risk status, was recorded. Cost data were obtained from each patient's statement of accounts, and effectiveness was measured by the rate of infectious complication. To control for potential confounding variables, the propensity score method with matching was carried out. The incremental cost-effectiveness ratio was calculated based on the matched population. We screened 3791 patients, and 440 were recruited for the analysis. Patients in the nutrition support cohort had a lower incidence of infectious complications than those in the no-support cohort (9.1 versus 18.1%; PÂ =Â 0.007). This result was similar in the 149 propensity matched pairs (9.4 versus 24.2%; PÂ <Â 0.001). The median hospital length of stay was significantly reduced among the matched nutrition support patients (13 versus 15Â d; PÂ <Â 0.001). The total costs were similar among the matched pairs (US $6219 versus $6161). The incremental cost-effectiveness analysis suggested that nutrition support cost US $392 per patient prevented from having infectious complications. Nutrition support was associated with fewer infectious complications and shorter length of stay in patients at nutritional risk. The incremental cost-effectiveness ratio indicated that nutrition support had not increased costs significantly. Copyright Â© 2016 Elsevier Inc. All rights reserved.\n\nMatch injuries in amateur Rugby Union: a prospective cohort study - FICS Biennial Symposium Second Prize Research Award.\n\nPubMed\n\nSwain, Michael S; Lystad, Reidar P; Henschke, Nicholas; Maher, Christopher G; Kamper, Steven J\n\n2016-01-01\n\nThe majority of Rugby Union (rugby) players participate at the amateur level. Knowledge of player characteristics and injury risks is predominantly ascertained from studies on professional or junior athletes in rugby. The objectives of the current study are to: (1) describe the health-related quality of life (HRQoL) and physical characteristics of a cohort of amateur rugby players; (2) describe the incidence, severity and mechanism of match injuries in amateur rugby, and; (3) explore factors associated with rates of match injury in this population. Participants (nâ=â125) from one amateur men's rugby club were followed in a one-season (2012) prospective cohort study. Match injury and match time exposure data were collected. A participant match exposure log was maintained. Baseline variables collected include: participant's age, playing experience, position of play, the SF-36v2 health survey, height and weight. Injury incidence rates (IIRs) per 1000 match-hours exposure were calculated. Injury sub-groups were compared by calculating rate ratios of two IIRs. Poisson mixed-effects generalised linear modelling was used to explore relationships between IIRs and baseline predictors. A total of 129 injuries occurred during a combined period of 2465 match-hours of exposure. The overall IIR was 52.3 (43.7-62.2) /1000 match-hours exposure. Moderate-severe injuries (>1Â week time-loss from play) comprised 36Â % of all injuries. Tackling was the most common mechanism of injury, the head/face was the most common body region of injury and sprain/ligament injuries were the most common injury type. Fewer years of rugby participation, lower BMI and lower SF-36v2 mental component summary score were associated with higher IIR in amateur rugby. Age, player position i.e., backs versus forwards and SF-36v2 physical component summary score were not associated with injury incidence. Amateur rugby players report similar HRQoL as the general population. We found amateur players had a\n\nThe Risk of Achilles or Biceps Tendon Rupture in New Statin Users: A Propensity Score-Matched Sequential Cohort Study.\n\nPubMed\n\nSpoendlin, Julia; Layton, J Bradley; Mundkur, Mallika; Meier, Christian; Jick, Susan S; Meier, Christoph R\n\n2016-12-01\n\nCase reports and pharmacovigilance data reported cases of tendon ruptures in statin users, but evidence from observational studies is scarce and inconclusive. We aimed to assess the association between new statin use and tendon rupture. We performed a propensity score (PS)-matched sequential cohort study, using data from the Clinical Practice Research Datalink. Patients aged â¥45Â years with at least one new statin prescription between 1995 and 2014 were PS-matched within 2-year entry blocks to patients without a statin prescription during the block. We followed patients until they had a recorded Achilles or biceps tendon rupture, completed 5Â years of follow-up, or were censored for change in exposure status or another censoring criterion. We calculated hazard ratios (HRs) with 95Â % confidence intervals (CIs), applying Cox proportional hazard analyses in the overall cohort (crude and multivariable) and in the PS-matched cohort. We performed subgroup analyses by sex, age, treatment duration, and statin dose. We observed a crude HR of 1.32 (95Â % CI 1.21-1.44) in the overall cohort, which attenuated after multivariable adjustment (HR 1.02, 95Â % CI 0.92-1.12) and after PS-matching (HR 0.95, 95Â % CI 0.84-1.08). Crude HRs were higher in women than in men, but remained around null in both sexes after multivariable adjustment and PS-matching. Subgroup analyses by age, treatment duration, and statin dose revealed null results across all subgroups. The results of this cohort study suggest that statin use does not increase the risk of tendon rupture, irrespective of gender, age, statin dose, or treatment duration.\n\nQuantifying the hospitalised morbidity and mortality attributable to traumatic injury using a population-based matched cohort in Australia\n\nPubMed Central\n\nMitchell, Rebecca J; Cameron, Cate M; McClure, Rod\n\n2016-01-01\n\nObjectives To quantify the 12-month hospitalised morbidity and mortality attributable to traumatic injury using a population-based matched cohort in Australia. Setting New South Wales, Queensland and South Australia, Australia. Participants Individuals â¥18â years who had an injury-related hospital admission in 2009 formed the injured cohort. The non-injured comparison cohort was randomly selected from the electoral roll and was matched 1:1 on age, gender and postcode of residence at the date of the index injury admission of their matched counterpart. Primary outcome measures Using linked emergency department presentation, hospital admission and mortality records from 1 January 2008 to 31 December 2010 for both the injured and non-injured cohorts, 12-month mortality and pre-index and post-index injury hospital service use was examined. Adjusted rate ratios and attributable risk were calculated. Results There were 167â 600 individuals injured in 2009 and admitted to hospital in New South Wales, South Australia or Queensland with a matched comparison. The injured cohort had 3 times higher proportion of having â¥1 comorbidity preinjury, higher preinjury hospital service use, and a higher 12-month mortality compared with a non-injured comparison group. The injured cohort had 2.20 (95% CI 2.12 to 2.28) times higher rate of hospital admissions in the 12â months post the index injury admission compared with the non-injured comparison cohort. Injury was a likely contributory factor in at least 55% of hospitalisations within 12â months of the index injury hospitalisation. Conclusions Individuals who had an injury-related hospitalisation had higher mortality and are hospitalised at increased rates for many months postinjury. While comorbid conditions are significant, they do not account for the differences in outcomes. This study contributes to informing research efforts on better quantifying the attributable burden of hospitalised injury-related disability and\n\nImpact Evaluation of a System-Wide Chronic Disease Management Program on Health Service Utilisation: A Propensity-Matched Cohort Study.\n\nPubMed\n\nBillot, Laurent; Corcoran, Kate; McDonald, Alina; Powell-Davies, Gawaine; Feyer, Anne-Marie\n\n2016-06-01\n\nThe New South Wales Health (NSW Health) Chronic Disease Management Program (CDMP) delivers interventions to adults at risk of hospitalisation for five target chronic conditions that respond well to ambulatory care: diabetes, hypertension, chronic obstructive pulmonary disease, congestive heart failure, and coronary artery disease. The intervention consists of two main components: (1) care coordination across sectors (acute, ambulatory, and community care from both public and private sectors) and clinical specialties, facilitated by program care coordinators, and (2) health coaching including management of lifestyle risk factors and medications and self-management. These components were broadly prescribed by the head office of NSW Health, which funded the program, and were implemented by regional health services (local health districts) in ways that best suited their own history, environment, workforce, and patient need. We used a propensity-matched cohort study to evaluate health service utilisation after enrolment in the CDMP. The evaluation cohort included 41,303 CDMP participants enrolled between 1 January 2011 and 31 December 2013 who experienced at least one hospital admission or emergency department (ED) presentation for a target condition in the 12 mo preceding enrolment. Potential controls were selected from patients not enrolled in the CDMP but experiencing at least one hospital admission or ED presentation over the same period. Each CDMP patient in the evaluation cohort was matched to one control using 1:1 propensity score matching. The primary outcome was avoidable hospitalisations. Secondary outcomes included avoidable readmissions, avoidable bed days, unplanned hospitalisations, unplanned readmissions, unplanned bed days, ED presentations, and all-cause death. The primary analysis consisted of 30,057 CDMP participants and 30,057 matched controls with a median follow-up of 15 mo. Of those, 25,638 (85.3%) and 25,597 (85.2%) were alive by the end of\n\nMultiple imputation of missing data in nested case-control and case-cohort studies.\n\nPubMed\n\nKeogh, Ruth H; Seaman, Shaun R; Bartlett, Jonathan W; Wood, Angela M\n\n2018-06-05\n\nThe nested case-control and case-cohort designs are two main approaches for carrying out a substudy within a prospective cohort. This article adapts multiple imputation (MI) methods for handling missing covariates in full-cohort studies for nested case-control and case-cohort studies. We consider data missing by design and data missing by chance. MI analyses that make use of full-cohort data and MI analyses based on substudy data only are described, alongside an intermediate approach in which the imputation uses full-cohort data but the analysis uses only the substudy. We describe adaptations to two imputation methods: the approximate method (MI-approx) of White and Royston () and the \"substantive model compatible\" (MI-SMC) method of Bartlett et al. (). We also apply the \"MI matched set\" approach of Seaman and Keogh () to nested case-control studies, which does not require any full-cohort information. The methods are investigated using simulation studies and all perform well when their assumptions hold. Substantial gains in efficiency can be made by imputing data missing by design using the full-cohort approach or by imputing data missing by chance in analyses using the substudy only. The intermediate approach brings greater gains in efficiency relative to the substudy approach and is more robust to imputation model misspecification than the full-cohort approach. The methods are illustrated using the ARIC Study cohort. Supplementary Materials provide R and Stata code. Â© 2018, The International Biometric Society.\n\nEffect of Predraft Ulnar Collateral Ligament Reconstruction on Future Performance in Professional Baseball: A Matched Cohort Comparison.\n\nPubMed\n\nCamp, Christopher L; Conte, Stan; D'Angelo, John; Fealy, Stephen A; Ahmad, Christopher S\n\n2018-05-01\n\nIn recent years, there has been a dramatic rise in the annual number of ulnar collateral ligament (UCL) reconstructions performed in amateur baseball pitchers. Accordingly, increasing numbers of players are entering professional baseball having already undergone the procedure; however, the effect of prior UCL reconstruction on future success remains unknown. (1) To provide an epidemiologic report on baseball players who undergo UCL reconstruction before being selected in the Major League Baseball (MLB) Draft, (2) to define the outcomes in terms of statistical performance, and (3) to compare these results with those of matched controls (ie, non-UCL reconstruction). Cohort study; Level of evidence, 3. The MLB Amateur Draft Database was queried to identify all drafted pitchers who underwent UCL reconstruction before being drafted. For each pitcher drafted from 2005 to 2014 with prior UCL reconstruction, 3 healthy controls with no history of elbow surgery were randomly identified for matched analysis. A number of demographic and performance comparisons were made between these groups. A total of 345 pitchers met inclusion criteria. The annual number of pitchers undergoing predraft UCL reconstructions rose steadily from 2005 to 2016 ( P < .001). For matched control analysis, 252 pitchers with a UCL reconstruction and a minimum 2-year follow-up (drafted between 2005 and 2014) were matched to 756 controls (non-UCL reconstruction). As compared with the non-UCL reconstruction group, pitchers who underwent predraft UCL reconstruction reached the MLB level with greater frequency (20% vs 12%, P = .003), and their MLB statistical performances were similar for all measures. Compared with all other pitchers drafted during that period, players who had a predraft UCL reconstruction demonstrated an increased likelihood of reaching progressive levels of play (Full Season A, AA, and MLB) within a given time frame ( P < .05 for all). The number of UCL reconstructions performed in\n\nAnesthesia Technique and Mortality after Total Hip or Knee Arthroplasty: A Retrospective, Propensity Score-matched Cohort Study.\n\nPubMed\n\nPerlas, Anahi; Chan, Vincent W S; Beattie, Scott\n\n2016-10-01\n\nThis propensity score-matched cohort study evaluates the effect of anesthetic technique on a 30-day mortality after total hip or knee arthroplasty. All patients who had hip or knee arthroplasty between January 1, 2003, and December 31, 2014, were evaluated. The principal exposure was spinal versus general anesthesia. The primary outcome was 30-day mortality. Secondary outcomes were (1) perioperative myocardial infarction; (2) a composite of major adverse cardiac events that includes cardiac arrest, myocardial infarction, or newly diagnosed arrhythmia; (3) pulmonary embolism; (4) major blood loss; (5) hospital length of stay; and (6) operating room procedure time. A propensity score-matched-pair analysis was performed using a nonparsimonious logistic regression model of regional anesthetic use. We identified 10,868 patients, of whom 8,553 had spinal anesthesia and 2,315 had general anesthesia. Ninety-two percent (n = 2,135) of the patients who had general anesthesia were matched to similar patients who did not have general anesthesia. In the matched cohort, the 30-day mortality rate was 0.19% (n = 4) in the spinal anesthesia group and 0.8% (n = 17) in the general anesthesia group (risk ratio, 0.42; 95% CI, 0.21 to 0.83; P = 0.0045). Spinal anesthesia was also associated with a shorter hospital length of stay (5.7 vs. 6.6 days; P < 0.001). The results of this observational, propensity score-matched cohort study suggest a strong association between spinal anesthesia and lower 30-day mortality, as well as a shorter hospital length of stay, after elective joint replacement surgery.\n\nDoes co-payment for inhaler devices affect therapy adherence and disease outcomes? A historical, matched cohort study.\n\nPubMed\n\nVoorham, Jaco; Vrijens, Bernard; van Boven, Job Fm; Ryan, Dermot; Miravitlles, Marc; Law, Lisa M; Price, David B\n\n2017-01-01\n\nAdherence to asthma and chronic obstructive pulmonary disease (COPD) treatment has been shown to depend on patient-level factors, such as disease severity, and medication-level factors, such as complexity. However, little is known about the impact of prescription charges - a factor at the health care system level. This study used real-life data to investigate whether co-payment affects adherence (implementation and persistence) and disease outcomes in patients with asthma or COPD. A matched, historical cohort study was carried out using two UK primary care databases. The exposure was co-payment for prescriptions, which is required for most patients in England but not in Scotland. Two comparison cohorts were formed: one comprising patients registered at general practices in England and the other comprising patients registered in Scotland. Patients aged 20-59 years with asthma, or 40-59 years with COPD, who were initiated on fluticasone propionate/salmeterol xinafoate, were included, matched to patients in the opposite cohort, and followed up for 1 year following fluticasone propionate/salmeterol xinafoate initiation. The primary outcome was good adherence, defined as medication possession ratio â¥80%, and was analyzed using conditional logistic regression. Secondary outcomes included exacerbation rate. There were 1,640 patients in the payment cohort, ie, England (1,378 patients with asthma and 262 patients with COPD) and 619 patients in the no-payment cohort, ie, Scotland (512 patients with asthma and 107 patients with COPD). The proportion of patients with good adherence was 34.3% and 34.9% in the payment and no-payment cohorts, respectively, across both disease groups. In a multivariable model, no difference in odds of good adherence was found between the cohorts (odds ratio, 1.04; 95% confidence interval, 0.85-1.27). There was also no difference in exacerbation rate. There was no difference in adherence between matched patients registered in England and Scotland\n\nSelf-harm hospitalised morbidity and mortality risk using a matched population-based cohort design.\n\nPubMed\n\nMitchell, Rebecca J; Cameron, Cate M\n\n2018-03-01\n\nPrior and repeated self-harm hospitalisations are common risk factors for suicide. However, few studies have accounted for pre-existing comorbidities and prior hospital use when quantifying the burden of self-harm. The aim is to quantify hospitalisation in the 12âmonths preceding and re-hospitalisation and mortality risk in the 12âmonths post a self-harm hospitalisation. A population-based matched cohort using linked hospital and mortality data for individuals â©¾18âyears from four Australian jurisdictions. A non-injured comparison cohort was matched on age, gender and residential postcode. Twelve-month pre- and post-index self-harm hospitalisations and mortality were examined. The 11,597 individuals who were hospitalised following self-harm in 2009 experienced 21% higher health service use in the 12âmonths pre and post the index admission and a higher mortality rate (2.9% vs 0.3%) than their matched counterparts. There were 133 (39.0%) deaths within 2âweeks of hospital discharge and 342 deaths within 12âmonths of the index hospitalisation in the self-harm cohort. Adjusted rate ratios for hospital readmission were highest for females (2.86; 95% confidence interval: [2.33, 2.52]) and individuals aged 55-64âyears (3.96; 95% confidence interval: [2.79, 5.64]). Improved quantification of the burden of self-harm-related hospital use can inform resource allocation for intervention and after-care services for individuals at risk of repeated self-harm. Better assessment of at-risk self-harm behaviour, appropriate referrals and improved post-discharge care, focusing on care continuity, are needed.\n\nExamining HIV Viral Load in a Matched Cohort of HIV Positive Individuals With and Without Psoriasis.\n\nPubMed\n\nWu, Jashin J; Gilbert, Kathleen E; Batech, Michael; Manalo, Iviensan F; Towner, William J; Raposo, Rui AndrÃ© Saraiva; Nixon, Douglas F; Liao, Wilson\n\n2017-04-01\n\nBACKGROUND: HIV-associated psoriasis is well-documented. Genetic, cellular, and cytokine profiles have been used as evidence to suggest psoriasis activates antiviral pathways. There has been a lack of epidemiologic evidence investigating whether psoriasis patients have lower HIV viral counts compared to non-psoriasis patients.\n\nOBJECTIVE: Compare the viral load set point of HIV positive patients with and without psoriasis.\n\nMETHODS: A retrospective matched cohort study of HIV positive patients with and without psoriasis using the Kaiser Permanente Southern California Health Plan database.\n\nRESULTS: We identified 101 HIV-positive psoriasis cases; 19 met inclusion criteria and were matched with 3-5 control patients; 94 total patients were analyzed. The mean age was 41.4 (12.07) years and 83% were male. Overall, the median log of the viral load of cases was slightly higher than controls (4.3 vs 4.2; P less than 0.01).\n\nCONCLUSIONS: The serum viral load set point of patients with HIV and psoriasis was slightly higher than the viral load set point of HIV patients without psoriasis.\n\nJ Drugs Dermatol. 2017;16(4):372-377.\n\n.\n\nRisk of total knee arthroplasty after operatively treated tibial plateau fracture: a matched-population-based cohort study.\n\nPubMed\n\nWasserstein, David; Henry, Patrick; Paterson, J Michael; Kreder, Hans J; Jenkinson, Richard\n\n2014-01-15\n\nThe aims of operative treatment of displaced tibial plateau fractures are to stabilize the injured knee to restore optimal function and to minimize the risk of posttraumatic arthritis and the eventual need for total knee arthroplasty. The purpose of our study was to define the rate of subsequent total knee arthroplasty after tibial plateau fractures in a large cohort and to compare that rate with the rate in the general population. All patients sixteen years of age or older who had undergone surgical treatment of a tibial plateau fracture from 1996 to 2009 in the province of Ontario, Canada, were identified from administrative health databases with use of surgeon fee codes. Each member of the tibial plateau fracture cohort was matched to four individuals from the general population according to age, sex, income, and urban/rural residence. The rates of total knee arthroplasty at two, five, and ten years were compared by using time-to-event analysis. A separate Cox proportional hazards model was used to explore the influence of patient, provider, and surgical factors on the time to total knee arthroplasty. We identified 8426 patients (48.5% female; median age, 48.9 years) who had undergone fixation of a tibial plateau fracture and matched them to 33,698 controls. The two, five, and ten-year rates of total knee arthroplasty in the plateau fracture and control cohorts were 0.32% versus 0.29%, 5.3% versus 0.82%, and 7.3% versus 1.8%, respectively (p < 0.0001). After adjustment for comorbidity, plateau fracture surgery was found to significantly increase the likelihood of total knee arthroplasty (hazard ratio [HR], 5.29 [95% confidence interval, 4.58, 6.11]; p < 0.0001). Higher rates of total knee arthroplasty were also associated with increasing age (HR, 1.03 [1.03, 1.04] per year over the age of forty-eight; p < 0.0001), bicondylar fracture (HR, 1.53 [1.26, 1.84]; p < 0.0001), and greater comorbidity (HR, 2.17 [1.70, 2.77]; p < 0.001). Ten years after tibial plateau\n\nEffect of telephone health coaching (Birmingham OwnHealth) on hospital use and associated costs: cohort study with matched controls.\n\nPubMed\n\nSteventon, Adam; Tunkel, Sarah; Blunt, Ian; Bardsley, Martin\n\n2013-08-06\n\nTo test the effect of a telephone health coaching service (Birmingham OwnHealth) on hospital use and associated costs. Analysis of person level administrative data. Difference-in-difference analysis was done relative to matched controls. Community based intervention operating in a large English city with industry. 2698 patients recruited from local general practices before 2009 with heart failure, coronary heart disease, diabetes, or chronic obstructive pulmonary disease; and a history of inpatient or outpatient hospital use. These individuals were matched on a 1:1 basis to control patients from similar areas of England with respect to demographics, diagnoses of health conditions, previous hospital use, and a predictive risk score. Telephone health coaching involved a personalised care plan and a series of outbound calls usually scheduled monthly. Median length of time enrolled on the service was 25.5 months. Control participants received usual healthcare in their areas, which did not include telephone health coaching. Number of emergency hospital admissions per head over 12 months after enrolment. Secondary metrics calculated over 12 months were: hospital bed days, elective hospital admissions, outpatient attendances, and secondary care costs. In relation to diagnoses of health conditions and other baseline variables, matched controls and intervention patients were similar before the date of enrolment. After this point, emergency admissions increased more quickly among intervention participants than matched controls (difference 0.05 admissions per head, 95% confidence interval 0.00 to 0.09, P=0.046). Outpatient attendances also increased more quickly in the intervention group (difference 0.37 attendances per head, 0.16 to 0.58, P<0.001), as did secondary care costs (difference Â£175 per head, Â£22 to Â£328, P=0.025). Checks showed that we were unlikely to have missed reductions in emergency admissions because of unobserved differences between intervention and\n\nPsychosocial Health of Disease-Free Breast Cancer Survivors Compared with Matched Non-cancer Controls.\n\nPubMed\n\nPark, Boyoung; Lee, Moo Hyun; Kong, Sun-Young; Lee, Eun Sook\n\n2018-04-05\n\nThe present study investigated the psychosocial health of disease-free breast cancer survivors who receive health examinations compared to matched non-cancer controls in a community setting. We used baseline data from the Health Examinee cohort, which is composed of subjects participating in health. The disease-free breast cancer survivors were defined as those who were â¥2 years from initial diagnosis of breast cancer who had completed treatment. Females without a history of cancer were randomly selected at 1:4 ratio by 5-year age groups, education, and household income as a comparison group. We analyzed results from the Psychosocial Well-being Index-Short Form (PWI-SF) as a psychosocial health measurement. A total of 347 survivors of breast cancer and 1,388 matched controls were included. Total scores on the PWI-SF were lower in breast cancer survivors than matched non-cancer controls (p=0.006), suggesting a lower level of psychosocial stress in breast cancer survivors. In comparison to the control group, prevalence of drinking, smoking and obesity were lower, while exercising for â¥150 min/wk was higher in breast cancer survivors (p < 0.05). These findings suggest that breast cancer survivors have better health behaviors than their non-cancer controls. After adjusting for other sociodemographic variables, breast cancer survivors were 36% less likely to be included in the stress group (odds ratio, 0.64; 95% confidence interval, 0.42 to 0.98). The disease-free breast cancer survivors resuming daily life demonstrated better psychosocial health status compared to matched non-cancer controls.\n\nThe economic impact of Marfan syndrome: a non-experimental, retrospective, population-based matched cohort study\n\nPubMed Central\n\n2014-01-01\n\nBackground Marfan syndrome is a rare disease of the connective tissues, affecting multiple organ systems. Elevated morbidity and mortality in these patients raises the issue of costs for sickness funds and society. To date, there has been no study analysing the costs of Marfan syndrome from a sickness fund and societal perspective. Objective To estimate excess health resource utilisation, direct (non-)medical and indirect costs attributable to Marfan syndrome from a healthcare payer and a societal perspective in Germany in 2008. Methods A retrospective matched cohort study design is applied, using claims data. For isolating the causal effect of Marfan syndrome on excess costs, a genetic matching algorithm was used to reduce differences in observable characteristics between Marfan syndrome patients and the control group. 892 patients diagnosed with Marfan syndrome (ICD-10 Q87.4) were matched from a pool of 26,645 control individuals. After matching, we compared health resource utilisation and costs. Results From the sickness fund perspective, an average Marfan syndrome patient generates excess annual costs of â¬2496 compared with a control individual. From the societal perspective, excess annual costs amount to â¬15,728. For the sickness fund, the strongest cost drivers are inpatient treatment and care by non-physicians. From the sickness fund perspective, the third (25â41 years) and first (0â16 years) age quartiles reveal the greatest surplus in total costs. Marfan syndrome patients have 39% more physician contacts, a 153% longer average length of hospital stay, 119% more inpatient stays, 33% more prescriptions, 236% more medical imaging and 20% higher average prescription costs than control individuals. Depending on the prevalence, the economic impact from the sickness fund perspective ranges between â¬24.0 million and â¬61.4 million, whereas the societal economic impact extends from â¬151.3 million to â¬386.9 million. Conclusions Relative to its low\n\nThe economic impact of Marfan syndrome: a non-experimental, retrospective, population-based matched cohort study.\n\nPubMed\n\nAchelrod, Dmitrij; Blankart, Carl Rudolf; Linder, Roland; von Kodolitsch, Yskert; Stargardt, Tom\n\n2014-06-23\n\nMarfan syndrome is a rare disease of the connective tissues, affecting multiple organ systems. Elevated morbidity and mortality in these patients raises the issue of costs for sickness funds and society. To date, there has been no study analysing the costs of Marfan syndrome from a sickness fund and societal perspective. To estimate excess health resource utilisation, direct (non-)medical and indirect costs attributable to Marfan syndrome from a healthcare payer and a societal perspective in Germany in 2008. A retrospective matched cohort study design is applied, using claims data. For isolating the causal effect of Marfan syndrome on excess costs, a genetic matching algorithm was used to reduce differences in observable characteristics between Marfan syndrome patients and the control group. 892 patients diagnosed with Marfan syndrome (ICD-10 Q87.4) were matched from a pool of 26,645 control individuals. After matching, we compared health resource utilisation and costs. From the sickness fund perspective, an average Marfan syndrome patient generates excess annual costs of â¬2496 compared with a control individual. From the societal perspective, excess annual costs amount to â¬15,728. For the sickness fund, the strongest cost drivers are inpatient treatment and care by non-physicians. From the sickness fund perspective, the third (25-41 years) and first (0-16 years) age quartiles reveal the greatest surplus in total costs. Marfan syndrome patients have 39% more physician contacts, a 153% longer average length of hospital stay, 119% more inpatient stays, 33% more prescriptions, 236% more medical imaging and 20% higher average prescription costs than control individuals. Depending on the prevalence, the economic impact from the sickness fund perspective ranges between â¬24.0 million and â¬61.4 million, whereas the societal economic impact extends from â¬151.3 million to â¬386.9 million. Relative to its low frequency, Marfan syndrome requires high healthcare\n\nA Matched Cohort Study of Patients With End-Stage Heart Failure from Anthracycline-Induced Cardiomyopathy Requiring Advanced Cardiac Support.\n\nPubMed\n\nThomas, Garry R; McDonald, Michael A; Day, Jennifer; Ross, Heather J; Delgado, Diego H; Billia, Filio; Butany, Jagdish W; Rao, Vivek; Amir, Eitan; Bedard, Philippe L; Thavendiranathan, Paaladinesh\n\n2016-11-15\n\nAnthracycline-induced cardiomyopathy (AIC) may progress to end-stage heart failure requiring mechanical circulatory support or orthotopic heart transplantation (OHT). Previous studies have described important clinical differences between AIC and nonischemic cardiomyopathy (NIC) cohorts requiring these advanced interventions. Therefore, we sought to extend this literature by comparing echocardiographic parameters, treatment strategies, and the prognosis between matched patients from these cohorts. This is a retrospective matched cohort study. All patients who received a ventricular assist device or OHT at a large Canadian center were reviewed (nÂ = 421; 1988 to 2015) and subjects with clinical and pathologic evidence of AIC were included (nÂ = 17, 4.0%). A comparison cohort with idiopathic NIC from the same database, matched 3:1 for age, gender, ethnicity, and year of heart failure onset was selected. The Mann-Whitney rank-sum and Fisher's exact tests were used for comparisons. Patients with AIC were predominantly women (70.6%) with heart failure diagnosed at age 40.2 Â± 15.8 and 8.3 Â± 8.9Â years after anthracycline treatment. Compared with NIC, no differences were seen in co-morbidities, echocardiographic measures, the proportion of patients receiving a defibrillator, ventricular assist device, or OHT, the incidence of graft failure, and all-cause mortality. In contrast to other studies, AIC was not associated with a higher incidence of right ventricular dysfunction. A greater proportion of patients with AIC developed cancer (recurrence or new primary) post-OHT (21.4% vs 2.3%, pÂ = 0.042). In conclusion, we demonstrate that when matched cohortsÂ of patients with end-stage heart failure secondary to AIC and idiopathic NIC are compared, they are similar with respect to co-morbidities, degree of ventricular dysfunction, and advanced therapeutics used. The prognosis with OHT is also similar. Copyright Â© 2016 Elsevier Inc. All rights reserved.\n\nAcupuncture decreased the risk of coronary heart disease in patients with fibromyalgia in Taiwan: a nationwide matched cohort study.\n\nPubMed\n\nWu, Mei-Yao; Huang, Ming-Cheng; Chiang, Jen-Huai; Sun, Mao-Feng; Lee, Yu-Chen; Yen, Hung-Rong\n\n2017-02-28\n\nThe aim of this study was to understand whether acupuncture can decrease the risk of coronary heart disease (CHD) in patients with fibromyalgia. Using data from the Taiwanese National Health Insurance Research Database, we performed a propensity score-matched cohort study to analyze patients with fibromyalgia diagnosed between 1 January 2000 and 31 December 2010. Patients who received acupuncture treatment, beginning with their initial date of fibromyalgia diagnosis and extending to 31 December 2010, were regarded as the acupuncture cohort. The no-acupuncture cohort comprised patients who never received acupuncture through 31 December 2010. A Cox regression model was used to adjust for age, sex, comorbidities, and drugs used. The HRs of the acupuncture and no-acupuncture cohorts were compared. After performing a 1:1 propensity score match, 58,899 patients in both cohorts were identified. Baseline characteristics were similar in both cohorts. The cumulative incidence of CHD was significantly lower in the acupuncture cohort (log-rank test, pâ<â0.001). In the follow-up period, 4389 patients in the acupuncture cohort (17.44 per 1000 person-years) and 8133 patients in the no-acupuncture cohort (38.36 per 1000 person-years) developed CHD (adjusted HR 0.43, 95% CI 0.41-0.45). The beneficial effect of acupuncture on the incidence of CHD was independent of age, sex, comorbidities, and statins used. Our study confirmed that acupuncture reduced the risk of CHD in patients with fibromyalgia in Taiwan. Further clinical and mechanistic studies are warranted.\n\nSingle-Anesthetic Versus Staged Bilateral Total Hip Arthroplasty: A Matched Cohort Study.\n\nPubMed\n\nHoudek, Matthew T; Wyles, Cody C; Watts, Chad D; Wagner, Eric R; Sierra, Rafael J; Trousdale, Robert T; Taunton, Michael J\n\n2017-01-04\n\nThere is debate regarding the role of single-anesthetic versus staged bilateral total hip arthroplasty (THA) for patients with end-stage bilateral osteoarthritis. Studies have shown that single-anesthetic bilateral THA is associated with systemic complications, but there are limited data comparing patient outcomes in a matched setting of bilateral THA. We identified 94 patients (188 hips) who underwent single-anesthetic bilateral THA. Fifty-seven percent of the patients were male. Patients had a mean age of 52.2 years and body mass index of 27.1 kg/m. They were matched 1:1 on the basis of sex, age (Â±1 year), and year of surgery (Â±3 years) to a cohort of patients undergoing staged bilateral THA. In the staged group, there was <1 year between procedures (range, 5 days to 10 months). Mean follow-up was 4 years for each group. Patients in the single-anesthetic group experienced shorter total operating room time and length of stay. There was no difference (hazard ratio [HR] = 0.73, p = 0.50) in the overall revision-free survival in patients undergoing single-anesthetic or staged bilateral THA. The risks of reoperation (HR = 0.69, p = 0.40), complications (HR = 0.83, p = 0.48), and mortality (HR = 0.47, p = 0.10) were similar. Single-anesthetic bilateral THA reduced the total cost of care (by 27%, p = 0.0001). In this matched cohort analysis, single-anesthetic bilateral THA was not associated with an increased risk of revision, reoperation, or postoperative complications, while decreasing cost. In our experience, single-anesthetic bilateral THA is a safe procedure that, for certain patients, offers an excellent means to deal with bilateral hip osteoarthritis. Therapeutic Level III. See Instructions for Authors for a complete description of levels of evidence.\n\nCancer survivorship and opioid prescribing rates: A population-based matched cohort study among individuals with and without a history of cancer.\n\nPubMed\n\nSutradhar, Rinku; Lokku, Armend; Barbera, Lisa\n\n2017-11-01\n\nLittle is known about opioid prescribing among individuals who have survived cancer. Our aim is to examine a predominantly socio-economically disadvantaged population for differences in opioid prescribing rates among cancer survivors compared with matched controls without a prior diagnosis of cancer. This was a retrospective population-wide matched cohort study. Starting in 2010, individuals residing in Ontario, Canada, who were 18 to 64 years of age and at least 5 years p"
    }
}
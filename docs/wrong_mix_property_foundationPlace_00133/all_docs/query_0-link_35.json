{
    "id": "wrong_mix_property_foundationPlace_00133_0",
    "rank": 35,
    "data": {
        "url": "https://arxiv.org/html/2401.00562v1",
        "read_more_link": "",
        "language": "en",
        "title": "Ruhr Hand Motion Catalog of Human Center-Out Transport Trajectories in 3D Task-Space Captured by a Redundant Measurement System Data Descriptor",
        "top_image": "",
        "meta_img": "",
        "images": [
            "https://arxiv.org/html/extracted/5324580/img/ages_histogram.png",
            "https://arxiv.org/html/extracted/5324580/img/handedness_histogram.png",
            "https://arxiv.org/html/x1.jpg",
            "https://arxiv.org/html/x2.jpg",
            "https://arxiv.org/html/extracted/5324580/img/Kamerapositionen_crop.jpg",
            "https://arxiv.org/html/extracted/5324580/img/times_white.png",
            "https://arxiv.org/html/extracted/5324580/img/1_R_processed_comparison_rec5.png"
        ],
        "movies": [],
        "keywords": [],
        "meta_keywords": [
            ""
        ],
        "tags": null,
        "authors": [],
        "publish_date": null,
        "summary": "",
        "meta_description": "",
        "meta_lang": "en",
        "meta_favicon": "",
        "meta_site_name": "",
        "canonical_link": null,
        "text": "License: CC BY-NC-SA 4.0\n\narXiv:2401.00562v1 [q-bio.QM] 31 Dec 2023\n\nRuhr Hand Motion Catalog of Human Center-Out Transport Trajectories in 3D Task-Space Captured by a Redundant Measurement System\n\nData Descriptor\n\nTim Sziburis Susanne Blex Tobias Glasmachers Ioannis Iossifidis\n\nAbstract\n\nNeurological conditions are a major source of movement disorders. Motion modelling and variability analysis have the potential to identify pathology but require profound data. We introduce a systematic dataset of 3D center-out task-space trajectories of human hand transport movements in a natural setting. The transport tasks of this study consist of grasping a cylindric object from a unified start position and transporting it to one of nine target locations in unconstrained operational space. The measurement procedure is automatized to record ten trials per target location. With that, the dataset consists of 90 movement trajectories for each hand of 31 participants without known movement disorders. The participants are aged between 21 and 78 years, covering a wide range. Data are recorded redundantly by both an optical tracking system and an IMU sensor. As opposed to the stationary capturing system, the IMU can be considered as a portable, low-cost and energy-efficient alternative to be implemented on embedded systems.\n\nBackground & Summary\n\nThe experimental observation and analysis of reaching tasks is an elementary means to develop a systematic understanding of movement generation. As a specific form of a reaching task (also referred to as aiming task), a transport (or transportation) task can be understood as the combination of a first reach-to-grasp movement, the grasping of the object itself, followed by transporting the object and reaching towards a target, before positioning it at the target location. In this study, we measured the movement trajectories during the time of transporting the grasped object, i. e. without consideration of grasping activity.\n\nOver the last decades, there have been various studies in the discipline of neuroscience comprising different variants and phases of reaching task experiments. Studies with human participants include analyses of kinematic coordination and muscle activity, such as with regard to trial consistency and coupling between distal and proximal kinematic chain trajectories[1], or with specific focus on three-dimensional target positioning[2]. Furthermore, there exist studies about hand selection strategy [3], decision-making[4], and motor sequence learning [5]. Reaching experiments were also conducted with non-human primates: Georgopolous et al. described the activation of specific direction-coded neural populations during two-dimensional[6] and three-dimensional[7, 8] reaching tasks.\n\nNevertheless, datasets of the experiments are sparsely publicly available, do not specifically target object transportation, or inquire 2D movements with touchscreens or 3D movements in virtual reality.\n\nA collection of published datasets is gathered in the Database for Reaching Experiments and Models (DREAM) from Northwestern University[9]. In this database, several datasets from reaching tasks are published in combination with specific tools and models, also with special focus on neural activity[10]. Conducted with different modalities of recording, it comprises datasets of reaching studies on:\n\n•\n\nobservations of reaching tasks without primary motor cortex population vectors pointing in hand motion direction, measuring neural activity in monkeys[11],\n\n•\n\nprobabilistic models during sensorimotor learning via optical finger tracking in the plane [12],\n\n•\n\nneural tuning of movement space in environments of differing complexity, measured by grasping a robotic arm exerting perturbing forces to change movement direction[13],\n\n•\n\ngeneralization of dynamics learning to novel directions by interpolation, horizontally measured by optical encoders of a robotic handle also exerting forces [14],\n\n•\n\nmovement timing and speed–accuracy trade-off (Fitt’s law), measured with a stylus on a tablet[15],\n\n•\n\nnervous system motor adaptation to errors, proposing a non-linear model, recorded by a robotic manipulandum[16],\n\n•\n\ninfluences of perturbations, measured by moving a horizontally restricted robot arm[17],\n\n•\n\nforce production and generalization with differing movement amplitudes while moving a robotic manipulandum[18],\n\n•\n\nmulti-sensory integration while moving a haptic device in a horizontal manner and also measuring electrooculography[19],\n\n•\n\neffects of motor-learning on somatosensory plasticity, experiments conducted with a planar robotic arm handle with optical encoders and force-torque sensors for measurements[20],\n\n•\n\ntuning curve stability of neural representation of limb motion during center-out reaching of monkeys, measured from primary motor cortex[21],\n\n•\n\nchanges of neural functional connectivity after motor learning, measured by a planar robotic handle together with MRI scans [22],\n\n•\n\nneuroprosthetic control by recording a stylus attached to a robot in combination with eye- and head-tracking as well as electromyography [23],\n\n•\n\nthe effect of uncertainty on the generalization of visuomotor rotations by optical tracking a finger used for two-dimensional cursor control [24], and\n\n•\n\nmotion target and trajectory decoding of center-out and random target tasks from neural recordings of monkeys, moving and tracking a two-link manipulandum [25].\n\nHowever, none of these studies specifically examines the object transport after grasping, which is the target of this study.\n\nOther experiments involving data from actual transport tasks with healthy participants consisted of studies regarding force trajectories[26], correction motion in the presence of moving targets[27], or age-dependent motor coordination[28]. Several transport motion analyses with participants suffering from diseases captured data in comparison to a healthy control group. These included the examination of force control in Huntington’s disease [29], movement dexterity of Parkinson’s disease patients[30], or eye-hand coordination under hemiparetic cerebral palsy[31]. However, these studies targeted specific pre-selected movement properties that were supposedly characteristic for the individual use case and lack the systematic variation of the set-up such as regarding target positioning. Therefore, the data from these studies, if available, would be lacking detailed and systematic insights into the general processes and methodology behind movement generation.\n\nWith specific regard to systematic hand transportation experiments with varying parameters, Grimme et al. (2012)[32] recorded natural human arm motion for two target directions of transporting an object over varying distances as well as obstacle heights and positions. While this first experiment was conducted with ten healthy participants, a second experiment with five participants (again, without known movement disorders) also included further obstacle configurations of one target direction. They analyzed specific invariants and properties of such motion, including isochrony, planarity and the decomposition of movements into a transport and a lift/descend primitive. Based on extended datasets gathered in further experiments[33], they furthermore describe an obstacle-dependent primitive. A subset of that data was published[34].\n\nBesides general public availability, our novel dataset provides systematicity with regard to target positioning and consists of methodologically captured hand transport movements in 3D. Furthermore, to our knowledge, it is the first dataset comprising transport trajectory recordings from both hands with target-randomized trial settings. The center-out setting of the task can provide information on direction-dependent motion components and factors. The general set-up makes it relevant for movement modelling as well as for comparison with later studies on movement impairments.\n\nNone of the aforementioned studies and data utilized more than one system for parallel measurement, since optical systems are considered as precise state-of-the-art techniques for motion capture. However, they are usually not suitable for embedded applications. Instead, portable sensors such as inertial measurement units (IMUs) might be favorable, for example in medical diagnosis scenarios requiring flexible applicability and mobility.\n\nThus, our presented dataset provides synchronized capturing from both systems to evaluate the feasibility of the portable system.\n\nMethods\n\nParticipation\n\nThe experiments were conducted in November and December 2022 in the eHealth laboratory of the Ruhr West University of Applied Sciences. Experimental data was recorded from thirty-one participants without known movement disorders. After getting introduced to the experimental procedure and providing informed consent, the participants filled a questionnaire with the following basic information: • age, • gender, • body height, • general physical condition, and • former experience with motion capture experiments.\n\nFurthermore, they conducted the 10-item Edinburgh Handedness Inventory[35] in order to calculate their handedness score. The following aspects were addressed: • writing, • painting, • throwing, • scissors, • tooth brush, • knife (without fork), • spoon, • broom (upper hand), • lighting a match, • opening the lid of a box, • preferred foot, and • preferred eye.\n\nThe participants, 11 female and 20 male, were in a range of age between 21 and 78. Figure 1 shows histograms of age and handedness of the study participants.\n\nExperimental Design\n\nTransport trajectories were measured by means of two motion capture systems in parallel, while participants moved a cylindrical solid body from a start position to one of nine target positions. The cylindrical object had a diameter of 5 cm and a height of 2.5 cm. Being individually 3D-printed, the cylinder provided the possibility to lock the used sensor object by latches, see Figure 2.\n\nIn this way, a single inertial measurement unit (IMU) sensor was attached to the cylindrical transport object and recorded movement data. Additionally, six cameras were positioned in the room which recorded two reflection elements.\n\nThese reflective spheres were glued at the top of the IMU sensor diagonally so that at least one of them was always visible from any perspective angle. Both systems were time-synchronized via NTP. The cameras were positioned in the room in a way that occlusions caused by the participants’ movements were avoided. Furthermore, the positions of the cameras, their aperture, focus, and zoom settings were adjusted so that the tracking object was recognizable from all cameras and that no reflection disturbances from other sources appeared. For this reason, the windows of the laboratory were shaded, and ceiling lighting was switched on to guarantee identical illumination conditions for all participants.\n\nThe IMU data communication base station was positioned at one corner of the table and aligned with its edges, not disturbing or distracting the participants. The experimental location in the laboratory room was selected in a way that magnetic field perturbations influencing the IMU sensor were minimized.\n\nParticipants were seated on a chair in front of a table, which were both adapted to their individual body dimensions (see also section Data Acquisition Workflow). The table was made out of wood to further reduce electromagnetic influences. Chair and table positions were fixed in the room, adjustments were possible via height and tabletop position, respectively. On the table, the start position and the target positions were marked by circles, see Figure 3. The targets were equidistantly positioned on a semicircle, with the start position in the center and a radius of 25 cm. The start and target circles themselves had a radius of 6 cm. The center of the start circle was positioned at 6 cm from the edge of the table. A 3D-printed docking block in the form of a short segment of a circle was glued to the printed starting circle (blue block in Figure 3). With that, an identical start position could be guaranteed for all trials.\n\nFor displaying target cues, a projector was positioned at the bottom and in front of the table, not observable by the participants so that they experienced no distraction. The projection was displayed on a canvas further in front and directly visible. Auditory start and stop signals were played via a loudspeaker integrated in the projector.\n\nTo avoid rhythmic movement patterns and specific time dependencies such as anticipatory behavior[36], random delays were introduced between the visual target cue and the acoustic start signal.\n\nEach participant performed two sessions, one for each hand. Half of the participants started with the left-hand session, the other with the right-hand session. Both sessions were performed one after another, separated by a small break of some minutes.\n\nCombining the transport cylinder with the IMU sensor and the two reflective elements for optical tracking, the overall transport object had a weight of 41 g in total.\n\nInstrumentation\n\nA state-of-the-art optical motion capture system (Vicon Nexus 2.14) comprised of six infrared cameras (Vicon Vero 1.3, with 1.3 MP resolution, maximum frame rate 250 Hz) was utilized to provide precise position reference data. These movements were measured by the continuous optical tracking of two reflection elements attached to the cylindric transport object. From the infrared recordings of the cameras that were able to detect the reflection elements in each frame, the individual three-dimensional positions were calculated in a world frame and combined to a course of positional data. The capturing rate was configured to the maximum possible frame rate of 250 Hz. The camera positioning is visualized in Figure 4.\n\nThe optical system with the cameras and the measurement computer were interconnected via RJ45 Ethernet ports. The measurement PC for the optical system was running a 64-bit Microsoft Windows 10 operating system with a 10 Gbit Ethernet controller.\n\nThe second measurement system, based on drift-corrected accelerometer data of a single state-of-the-art inertial measurement unit (IMU) from an Xsens MTw Awinda system, was used in parallel to evaluate the quality of a portable sensor set-up with embedded applicability. The following movement data were recorded by the IMU: accelerations (accelerometer), angular velocities (gyroscope), and orientations (magnetometer).\n\nFor capturing the IMU data, the Xsens SDK from the Xsens MT Software Suite 2021.4 was utilized. Since an Xsens MTw system was used as opposed to the more recent MTi systems, the closed-source API with proprietary libraries had to be called instead of the open-source API. A C++ program was written which incorporated the communication with the Xsens Awinda v2 base station, addressing the single sensor (MTw2), as well as initiating the measurement and eventually recording the data. Both the base station and the sensor were running the most recent firmware (4.6.0).\n\nThe base station was connected via USB to a measurement laptop running a 64-bit Debian GNU/Linux 11 \"Bullseye\". In order to take advantage of real-time capabilities, a specific kernel, namely Liquorix 6.0 with preemptive scheduling was loaded.\n\nBased on an example for the utilization of the Xsens SDK, the recording program was extended by a QT graphical user interface to allow the configuration of communication channel, frame rate and connected sensor. Moreover, this was applied to configure the experimental control and finally conduct the experiments, including visualizations and acoustic signals. Besides the actual data recordings and their exact start and stop timestamps for all trials, this program calculated and stored the randomized order of targets to reach as well as the durations of all randomization time periods.\n\nData Acquisition Workflow\n\nThe participants were introduced to the study and received information on the experimental procedure as well as the purpose of the measurements. After they had the possibility to request clarification and details, they were asked to sign an informed consent form. If everything was clear, they also filled the questionnaire described in section Participation.\n\nFirst, the wooden table and chair were adjusted for each individual participant in a way that they were sitting in an upright position. Furthermore, it was made sure that there was an angle of 90° inside of the elbow while positioning their hand palms on target 1 (the outer left) and target 9 (the outer right). Each participant’s configuration (chair height and tabletop position) was noted.\n\nAfter that, a calibration of the Vicon Nexus optical motion capture system was initiated via the active wand calibration method according to the official instructions from the manufacturer, i. e moving the wand evenly in and over all directions of the experimental space until each camera reached a count of 2000 calibration frames. Until a self-set world error threshold could be reached, the procedure was repeated. This was followed by masking the infrared cameras via the Vicon Nexus software in order to eliminate any potentially remaining externally disturbing reflections out of the particular region of measurements.\n\nAfter successful calibration, the origin of the coordinate frame was set for the Vicon system by means of positioning the calibration wand horizontally on the table with the origin in the center of the starting circle.\n\nWe made sure that the battery of the IMU sensor was charged to at least 90% and then continued with magnetic field mapping via the corresponding software tool from the MT Software Suite 2021.4 (Magnetic Field Mapper). For this, the IMU sensor, attached to the cylindrical transport object, had to be uniformly rotated around any possible axes, until the criteria for 3D calibration stated in the Xsens manual were fulfilled (for details, see section Technical Validation).\n\nBefore seating, the participants were asked to remove all electronic devices and metallic objects from their pockets and from their clothing to minimize electromagnetic disturbances. The task space was systematically adjusted to their individual body dimensions. As soon as they were correctly seated, they had the possibility to start a prototypical test trial to familiarize themselves with the visual projection, the timing characteristics with randomization, and the start and stop sounds.\n\nThe recording procedure was automatized. The experimental sequence with the corresponding time spans per trial is depicted in Figure 5.\n\nEach participant performed two experimental sessions, one for each hand. In each transportation task, they grasped the object and moved it from the start to a target position on the labeled table. They were instructed to perform this movement as fast and as precise as possible.\n\nWhile the start position was always the same, the target position randomly varied between nine possibilities and was visually announced before an acoustic start signal appeared. The time between visual cue and acoustic start signal was randomized between one and two seconds. For each target, ten trials were asked to be performed for each hand. Conducted as a double-blind study, the appearing targets were neither known to the participants nor to the experimentators beforehand. After finishing a trial, participants had to stay still until an acoustic stop signal appeared, signalling to move the object back to the starting position again. Between the two sessions, a longer break was scheduled according to the individual needs. Furthermore, the participants could always introduce additional breaks between the trials if needed. However, nobody requested to make use of this possibility. One recording session usually took about 30 minutes (15 minutes per hand).\n\nData Processing\n\nWhile the IMU data capturing program recorded the data on a per-trial-basis and not during intermediate time periods, the Vicon system recorded the optical reflection data continuously from the start of the experimental session to its end. After finishing, this data stream was automatically cut into per-trial data via the stored IMU recording timestamps.\n\nFor both systems, a rotation was introduced to match the trajectory profiles. The trajectories were rotated in a way that the straight connection line between start and target maps onto the y-axis, calculated individually for each trial.\n\nSince the time synchronization via NTP could have been subject to network delays, an additional fine alignment of time synchronization was introduced. This precise overall alignment for each trial could be realized via shifting the time courses to match the middle points of position on the y-axes.\n\nVelocity and acceleration data of the optical system were calculated by differentiation, while for the IMU-based system velocity and position data were integrated from the acceleration after appropriate preprocessing. A fourth-order Butterworth low-pass filter was applied to all data to remove high-frequency noise from the trajectories (cut-off frequency 25 Hz).\n\nOptical Data\n\nThe optical system utilized the infrared reflection recordings from all cameras which were able to perceive at least one marker in order to reconstruct the position course of the transported object in a world coordinate frame. For this, both markers had to be labeled in the Vicon Nexus software. If this assignment was not possible in a continuous manner, for example due to occlusions or the instantaneous appearance of further reflection signals, the labelling had to be performed for each self-contained sequence. If one optical marker was occluded by a participant’s limb or head, the missing position of this marker in single frames could be padded by pattern fill: Since the two markers had a fixed position relative to each other, the pattern of movement could be inferred from the non-occluded marker. This was automatically manageable in the Vicon software.\n\nIf in single frames both markers appeared to be not visible, also automatic spline fills were possible, interpolating the previous and the subsequent visible marker positions. This was only applied if there were not more than five missing frames. In the seldom case of a larger gap, the particular trial was disregarded.\n\nFor calculating the courses of velocity and acceleration from the positional data, the numerical derivatives were derived via finite differences. A central differencing method using three points was applied.\n\nIMU Data\n\nThe IMU data from the Xsens sensor was saved in the proprietary Xsens format and converted to CSV files. One file was recorded per trial. Furthermore, for each session (left or right hand for each participant) the randomized delays between visual cue and acoustic start signal, the order of targets appearing, and the individual start and stop timestamps of each trial were stored.\n\nGenerally, IMU data is often used for orientation estimation. However, for the presented dataset, only the accelerometer data were used for position determination. Though, measures to compensate for sensor drift needed to be introduced. In preliminary tests, the gyroscope and magnetometer data were incorporated as well to calculate estimated corrections of the accelerations by these additionally measured values and the application of an extended Kalman filter. At the cost of computational requirements, this did not show any improvement of the acceleration data or, after integration, velocity and position estimation, respectively. For this reason, and to apply the least possible preprocessing but as much as necessary, the extended Kalman filter approach was eventually not applied. The finally conducted steps are described in the following.\n\nFirst, for the acceleration data, the free acceleration was calculated by the embedded processing of the IMU sensor itself. For this, the gravitational acceleration was subtracted from the measured acceleration data. The gravitational acceleration can be measured during resting state without any external movement.\n\nFor acceleration drift compensation, movement initiation was defined as the time point when an empirically determined acceleration noise threshold combining all three dimensions of the acceleration vector in the form of Euclidean norm is exceeded. For this, the remaining noise acceleration during resting state after the end of the movement was utilized, precisely the per-component maximum of the last ten samples.\n\nThe acceleration values during initiation and ending resting state (phases under acceleration threshold) were corrected by subtracting the averaged noise activity. The averaged acceleration sensor drift rate was then calculated between movement initiation and end. With this drift rate, the intermediate acceleration measurements at each time point during movement could be corrected by the relative proportion of overall drift.\n\nTo implement the integration of acceleration to obtain velocity, an approach related to drift correction was followed, called zero velocity update (ZUPT), originally proposed for navigation and survey applications[37]. Following this method, the acceleration time course was split into continuous segments of under-or-equal-threshold and over-threshold activity, again depending on the same empirically determined acceleration threshold. For all samples, a correction of velocity due to drifting sensor data took place, while during the phases of under-threshold activity the velocity was reset to zero. This could prevent the integrative accumulation of drift errors. First, the velocity was calculated kinematically as if there was no drift:\n\nvt+1,p⁢r⁢e⁢d=vt+at⋅d⁢tsubscript𝑣𝑡1𝑝𝑟𝑒𝑑subscript𝑣𝑡⋅subscript𝑎𝑡𝑑𝑡v_{t+1,pred}=v_{t}+a_{t}\\cdot dtitalic_v start_POSTSUBSCRIPT italic_t + 1 , italic_p italic_r italic_e italic_d end_POSTSUBSCRIPT = italic_v start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT + italic_a start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ⋅ italic_d italic_t\n\nFor the first sample of an under-threshold phase, the velocity drift rate between the current and the last sample of the previous under-threshold region was calculated via dividing their velocity difference by time (sample count). Ideally, at these samples, the acceleration activity would be zero if there was no drift. Within the under-threshold phases (particularly including the very beginning of the movement, where there exists no previous under-threshold phase), the velocity was set to zero. With the drift rate, the predicted velocity values of the over-threshold activity phase laying directly in between the under-threshold phases were corrected by subtracting the corresponding drift in order to guarantee continuity.\n\nIn principle, zero acceleration could also mean constant velocity instead of zero velocity. However, this was practically not relevant due to highly variable acceleration courses and the accelerometer sensitivity to close-to-constant velocities. This led to accelerometer responses even to low deviations such as noise.\n\nFinally, with the zero-velocity-updated integration of acceleration to obtain the course of velocity, the position course is kinematically computed based on these corrected velocity values:\n\nst+1=st+vt⋅d⁢t+12⋅a⋅d⁢t2subscript𝑠𝑡1subscript𝑠𝑡⋅subscript𝑣𝑡𝑑𝑡⋅12𝑎𝑑superscript𝑡2s_{t+1}=s_{t}+v_{t}\\cdot dt+\\frac{1}{2}\\cdot a\\cdot dt^{2}italic_s start_POSTSUBSCRIPT italic_t + 1 end_POSTSUBSCRIPT = italic_s start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT + italic_v start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ⋅ italic_d italic_t + divide start_ARG 1 end_ARG start_ARG 2 end_ARG ⋅ italic_a ⋅ italic_d italic_t start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT\n\nData Records\n\nAll data can be made available upon request to the authors after publication. An analytical study of the movement data is in progress.\n\nAfter extracting the data archive, the captured records can be found in the form of CSV files, structured in three main directories (the session ID [SesID] is composed of the subject number in the interval [1, 31], sorted by age, and the recorded hand in {L, R}):\n\n1.\n\nmeasurement for the recorded data without further processing (for Vicon, the data is just split into single files and preprocessed by Nexus software):\n\n•\n\nordered list of all targets in the session:\n\n[SesID]_targets.txt,\n\n•\n\nordered lists of recording timestamps for all trials’ start and end times in the session:\n\n[SesID]_timestampsRecStart.txt and\n\n[SesID]_timestampsRecStop.txt, respectively\n\n(format YYYY-MM-DD-hh-mm-ss.sss), as well as\n\n•\n\nordered list of the added randomized time delays for all recording trials in the session:\n\n[SesID]_timeDelays.txt, in milliseconds,\n\n•\n\npositions for Vicon data,\n\nCSV file [SesID]_V_measurement_rec[0,89]:\n\n–\n\nframe number (1 columns),\n\n–\n\n{x,y,z} positions of marker 1 [m]delimited-[]𝑚[m][ italic_m ] (3 columns),\n\n–\n\n{x,y,z} positions of marker 2 [m]delimited-[]𝑚[m][ italic_m ] (3 columns).\n\n•\n\nIMU values for Xsens data,\n\nCSV file [SesID]_X_measurement_rec[0,89]:\n\n–\n\n{x,y,z} acceleration [m⁢s−2]delimited-[]𝑚superscript𝑠2[ms^{-2}][ italic_m italic_s start_POSTSUPERSCRIPT - 2 end_POSTSUPERSCRIPT ] (3 columns),\n\n–\n\n{x,y,z} free acceleration [m⁢s−2]delimited-[]𝑚superscript𝑠2[ms^{-2}][ italic_m italic_s start_POSTSUPERSCRIPT - 2 end_POSTSUPERSCRIPT ] (3 columns),\n\n–\n\ngyroscope values (angular velocities [r⁢a⁢d−1𝑟𝑎superscript𝑑1rad^{-1}italic_r italic_a italic_d start_POSTSUPERSCRIPT - 1 end_POSTSUPERSCRIPT], 3 columns),\n\n–\n\nmagnetometer values (quaternion, 4 columns),\n\n2.\n\noriginal for the measurement data processed by:\n\n•\n\nfiltering as described above,\n\n•\n\ncorrection for acceleration data (drift, ZUPT),\n\n•\n\ntime alignment as mentioned above, as well as\n\n•\n\ndifferentiation of positional Vicon data to derive velocity and acceleration,\n\nCSV file [SesID]_V_original_rec[0,89]:\n\n–\n\ntime [s] (1 column),\n\n–\n\n{x,y,z} positions [m]delimited-[]𝑚[m][ italic_m ] (3 columns),\n\n–\n\n{x,y,z} velocities [m⁢s−1]delimited-[]𝑚superscript𝑠1[ms^{-1}][ italic_m italic_s start_POSTSUPERSCRIPT - 1 end_POSTSUPERSCRIPT ] (3 columns),\n\n–\n\n{x,y,z} accelerations [m⁢s−2]delimited-[]𝑚superscript𝑠2[ms^{-2}][ italic_m italic_s start_POSTSUPERSCRIPT - 2 end_POSTSUPERSCRIPT ] (3 columns),\n\n•\n\nand integration for IMU accelerometer data to calculate velocity and position,\n\nCSV file [SesID]_X_original_rec[0,89]:\n\n–\n\ntime [s] (1 column),\n\n–\n\n{x,y,z} positions [m]delimited-[]𝑚[m][ italic_m ] (3 columns),\n\n–\n\n{x,y,z} velocities [m⁢s−1]delimited-[]𝑚superscript𝑠1[ms^{-1}][ italic_m italic_s start_POSTSUPERSCRIPT - 1 end_POSTSUPERSCRIPT ] (3 columns),\n\n–\n\n{x,y,z} accelerations [m⁢s−2]delimited-[]𝑚superscript𝑠2[ms^{-2}][ italic_m italic_s start_POSTSUPERSCRIPT - 2 end_POSTSUPERSCRIPT ] (3 columns).\n\n3.\n\nprocessed for the final processed data, i. e. after applying to the original data:\n\n•\n\nrotation to map the straight positional start-target-line onto the y-axis for each trial,\n\n•\n\nleading to the same CSV columns as in the case of the original data,\n\nCSV files: [SesID]_V_processed_rec[0,89], and\n\n[SesID]_X_processed_rec[0,89],\n\n4.\n\nQuestionnaire for the anonymized answers to the questions asked beforehand, including the EHI.\n\nWithin the directories measurement, original and processed, there is a folder structure naming the number of the participant, the hand recorded (left/right), and the capturing system used (Xsens/Vicon), e. g. for subject 23: 23/L/X, 23/L/V, 23/R/X, and 23/R/V.\n\nExample Data Record Structure\n\nAs an example, Table 1 illustrates the directory and file structure of the extracted dataset archive for subject number 1. Subject numbers in the interval [1, 31] can be chosen. The number of lines of the movement data files depend on the individual recordings and the particular measurement system (250 Hz capturing rate for Vicon, 100 Hz for Xsens).\n\nExample Trajectory of Processed Recording\n\nAn exemplary trajectory of the processed data can be seen in Figure 6, i. e. after filtering, drift correction, and fine alignment. This depicts one trial’s position, velocity and acceleration courses over time [s] for the x-, y-, and z-dimensions of subject 1 (trial 5). While the first column shows the data from the optical system and the second column that from the IMU system, the third visualizes the overlay of both systems.\n\nTechnical Validation\n\nThe Vicon Nexus motion capture system mentioned as optical reference system was calibrated with the Wand calibration method recommended by the manufacturer. 2000 calibration frames were gathered per camera, until a maximum world error of 0.17 mm per camera could be guaranteed by the software. To further reduce errors and provide redundancy, two reflection elements were attached to the cylindrical object with a fixed distance to each other. This made sure that occlusions of both markers at the same time could be avoided. When both markers were detected, the transport object’s position was determined by the mean of the two markers’ positions, further minimizing noise effects. In the case that for a short time, only one marker was visible to sufficiently many cameras to determine its position, the positional course of the other could be reconstructed by the software’s functionality of pattern filling for single frames, orienting at the motion behavior of the visible marker.\n\nAs IMU sensors are known to be prone to disturbances, at first a pilot study was conducted in an electrically shielded lab to avoid the effect of electrostatic discharge. This analysis could not reveal any apparent difference in data quality when compared to the standard, non-shielded laboratory environment where the optical system was available. Inside the non-shielded laboratory, magnetic field mappings were conducted by means of the Xsens Magnetic Field Mapper software to find the most suitable location for the experimental setup and to calibrate the sensor with respect to the locally prevailing magnetic field. The software provided different criteria for the quality of calibration by deriving a model mapping the measured magnetic field to an ideal sphere with center at zero and a vector magnitude of 1: average of the magnetic norm close to 1, standard deviation of the norm and maximum error with respect to a norm of 1 as low as possible, no large spikes in spatial distribution of difference, and residuals of the corrected magnetic field vector following a Gaussian distribution[38].\n\nThe laboratory location with least temporal and spatial disturbances during the calibration process with the Magnetic Field Mapper was chosen for the experiments. Moreover, before each experimental session, further magnetic field mappings were undertaken to consider the effect of static disturbances, i. e. to predict and compensate for deterministic, constant magnetic field errors. As suggested by the manual from the manufacturer for 3D calibration, the IMU sensor was rotated in as many orientation angles as possible at the same location (no translational velocity) with an approximately constant angular velocity during about three minutes of magnetic field mapping.\n\nAdditionally, before each session, it was made sure that the battery of the IMU sensor was always fully charged with at least 90% so that voltage-descent-dependent effects can be excluded as sources of errors. The wireless IMU communication with the real-time Linux kernel to avoid data loss of single frames due to process scheduling was examined in prototypical tests. These showed that all data could be successfully transferred at a rate of 100 Hz. A fully reliable communication could be established via the Xsens wireless channel 25. In comparison to other possible channels, pilot experiments showed that this wireless channel could dependably minimize interference with other 802.11 devices (WiFi, Bluetooth).\n\nUsage Notes\n\nThe dataset comprises 31 subjects. The recruitment of participants by personal request does not warrant a representative selection compared to the general population.\n\nFor the motion trajectories, some trials to single targets have to be disregarded due to bad quality. In the case of the optical motion capture, this stems from occlusions that could not be fully avoided in some cases, particularly regarding both infrared reflection markers at the same time. In the case of the IMU system, the accelerometer sensor is sensitive to the exerted velocity. While fast movements, including abrupt changes of accelerations when moving the cylndrical object back onto the table, had no effect on data quality, the effect of sensor drift for slow movements could not be sufficiently compensated. However, since the paradigm was that movements had to be executed as fast and precise as possible, too slow movements appeared only in few cases. These individual trials have to be disregarded, too.\n\nCode and Data Availability\n\nFor recording and processing the optical motion capture data, no custom code was in use. The standard processing pipelines from Vicon Nexus 2.14 for reconstructing, marker labeling, pattern filling of single missing frames, and data export were applied.\n\nThe developed custom code for conducting the overall experiment and recording IMU data is based on an example from the Xsens MT Software Suite 2021.4 software development kit with a graphical QT user interface.\n\nThe source code for preprocessing the IMU data includes filtering, drift correction, and integration of acceleration data. In the case of the optical data, custom code for filtering and numerical differentiation of the position data was in use.\n\nFinally, further custom code for processing both the optical- and the IMU-based data comprises synchronization by time offset adjustment and rotation for data comparability.\n\nAll custom code can be made available upon request to the authors after publication.\n\nReferences\n\n[1] Lacquaniti, F. & Soechting, J. Coordination of arm and wrist motion during a reaching task. \\JournalTitleThe Journal of Neuroscience 2, 399–408, DOI: 10.1523/JNEUROSCI.02-04-00399.1982 (1982).\n\n[2] Vandenberghe, A., Levin, O., De Schutter, J., Swinnen, S. & Jonkers, I. Three-dimensional reaching tasks: Effect of reaching height and width on upper limb kinematics and muscle activity. \\JournalTitleGait & Posture 32, 500–507, DOI: 10.1016/j.gaitpost.2010.07.009 (2010).\n\n[3] Stins, J. F., Kadar, E. E. & Costall, A. A kinematic analysis of hand selection in a reaching task. \\JournalTitleLaterality: Asymmetries of Body, Brain and Cognition 6, 347–367, DOI: 10.1080/713754421 (2001).\n\n[4] Chapman, C. S. et al. Reaching for the unknown: Multiple target encoding and real-time decision-making in a rapid reach task. \\JournalTitleCognition 116, 168–176, DOI: 10.1016/j.cognition.2010.04.008 (2010).\n\n[5] Moisello, C. et al. The serial reaction time task revisited: a study on motor sequence learning with an arm-reaching task. \\JournalTitleExperimental Brain Research 194, 143–155, DOI: 10.1007/s00221-008-1681-5 (2009).\n\n[6] Georgopoulos, A. P., Kalaska, J. F., Caminiti, R. & Massey, J. T. On the relations between the direction of two-dimensional arm movements and cell discharge in primate motor cortex. \\JournalTitleThe Journal of Neuroscience 2, 1527–1537, DOI: 10.1523/JNEUROSCI.02-11-01527.1982 (1982).\n\n[7] Georgopoulos, A. P., Schwartz, A. B. & Kettner, R. E. Neuronal Population Coding of Movement Direction. \\JournalTitleScience 233, 1416–1419, DOI: 10.1126/science.3749885 (1986).\n\n[8] Georgopoulos, A. P., Kettner, R. E. & Schwartz, A. B. Primate motor cortex and free arm movements to visual targets in three- dimensional space. II. Coding of the direction of movement by a neuronal population. \\JournalTitleThe Journal of Neuroscience 8, 2928–2937, DOI: 10.1523/JNEUROSCI.08-08-02928.1988 (1988).\n\n[9] Walker, B. & Körding, K. P. The Database for Reaching Experiments and Models. \\JournalTitlePLoS ONE 8, e78747, DOI: 10.1371/journal.pone.0078747 (2013).\n\n[10] Körding, K. P. & Walker, B. Database for reaching experiments and models (dream). NERSC https://crcns.org/data-sets/movements/dream/downloading-dream (2014).\n\n[11] Scott, S. H., Gribble, P. L., Graham, K. M. & Cabel, D. W. Dissociation between hand motion and population vectors from neural activity in motor cortex. \\JournalTitleNature 413, 161–165, DOI: 10.1038/35093102 (2001).\n\n[12] Körding, K. P. & Wolpert, D. M. Bayesian integration in sensorimotor learning. \\JournalTitleNature 427, 244–247, DOI: 10.1038/nature02169 (2004).\n\n[13] Thoroughman, K. A. & Taylor, J. A. Rapid Reshaping of Human Motor Generalization. \\JournalTitleJournal of Neuroscience 25, 8948–8953, DOI: 10.1523/JNEUROSCI.1771-05.2005 (2005).\n\n[14] Mattar, A. A. G. & Ostry, D. J. Modifiability of Generalization in Dynamics Learning. \\JournalTitleJournal of Neurophysiology 98, 3321–3329, DOI: 10.1152/jn.00576.2007 (2007).\n\n[15] Young, S. J., Pratt, J. & Chau, T. Target-Directed Movements at a Comfortable Pace: Movement Duration and Fitts’s Law. \\JournalTitleJournal of Motor Behavior 41, 339–346, DOI: 10.3200/JMBR.41.4.339-346 (2009).\n\n[16] Wei, K. & Körding, K. P. Relevance of Error: What Drives Motor Adaptation? \\JournalTitleJournal of Neurophysiology 101, 655–664, DOI: 10.1152/jn.90545.2008 (2009).\n\n[17] Wei, K., Wert, D. & Körding, K. P. The Nervous System Uses Nonspecific Motor Learning in Response to Random Perturbations of Varying Nature. \\JournalTitleJournal of Neurophysiology 104, 3053–3063, DOI: 10.1152/jn.01025.2009 (2010).\n\n[18] Mattar, A. A. G. & Ostry, D. J. Generalization of Dynamics Learning Across Changes in Movement Amplitude. \\JournalTitleJournal of Neurophysiology 104, 426–438, DOI: 10.1152/jn.00886.2009 (2010).\n\n[19] Burns, J. K. & Blohm, G. Multi-Sensory Weights Depend on Contextual Noise in Reference Frame Transformations. \\JournalTitleFrontiers in Human Neuroscience 4, DOI: 10.3389/fnhum.2010.00221 (2010).\n\n[20] Ostry, D. J., Darainy, M., Mattar, A. A. G., Wong, J. & Gribble, P. L. Somatosensory Plasticity and Motor Learning. \\JournalTitleJournal of Neuroscience 30, 5384–5393, DOI: 10.1523/JNEUROSCI.4571-09.2010 (2010).\n\n[21] Stevenson, I. H. et al. Statistical assessment of the stability of neural movement representations. \\JournalTitleJournal of Neurophysiology 106, 764–774, DOI: 10.1152/jn.00626.2010 (2011).\n\n[22] Vahdat, S., Darainy, M., Milner, T. E. & Ostry, D. J. Functionally Specific Changes in Resting-State Sensorimotor Networks after Motor Learning. \\JournalTitleJournal of Neuroscience 31, 16907–16915, DOI: 10.1523/JNEUROSCI.2737-11.2011 (2011).\n\n[23] Corbett, E. A., Körding, K. P. & Perreault, E. J. Real-time fusion of gaze and EMG for a reaching neuroprosthesis. In 2012 Annual International Conference of the IEEE Engineering in Medicine and Biology Society, 739–742, DOI: 10.1109/EMBC.2012.6346037 (IEEE, San Diego, CA, 2012).\n\n[24] Fernandes, H. L., Stevenson, I. H. & Körding, K. P. Generalization of Stochastic Visuomotor Rotations. \\JournalTitlePLoS ONE 7, e43016, DOI: 10.1371/journal.pone.0043016 (2012).\n\n[25] Flint, R. D., Lindberg, E. W., Jordan, L. R., Miller, L. E. & Slutzky, M. W. Accurate decoding of reaching movements from field potentials in the absence of spikes. \\JournalTitleJournal of Neural Engineering 9, 046006, DOI: 10.1088/1741-2560/9/4/046006 (2012).\n\n[26] Hejduková, B. et al. Grip and Load Force Coordination during a Manual Transport Movement: Findings in Healthy Participants. \\JournalTitleMotor Control 6, 282–293, DOI: 10.1123/mcj.6.3.282 (2002).\n\n[27] Danion, F. & Sarlegna, F. R. Can the Human Brain Predict the Consequences of Arm Movement Corrections When Transporting an Object? Hints from Grip Force Adjustments. \\JournalTitleJournal of Neuroscience 27, 12839–12843, DOI: 10.1523/JNEUROSCI.3110-07.2007 (2007).\n\n[28] Huntley, A. H., Zettel, J. L. & Vallis, L. A. Older adults exhibit altered motor coordination during an upper limb object transport task requiring a lateral change in support. \\JournalTitleHuman Movement Science 52, 133–142, DOI: 10.1016/j.humov.2017.01.014 (2017).\n\n[29] Quinn, L., Reilmann, R., Marder, K. & Gordon, A. Altered movement trajectories and force control during object transport in Huntington’s disease. \\JournalTitleMovement Disorders 16, 469–480, DOI: 10.1002/mds.1108 (2001).\n\n[30] Hejduková, B. et al. Manual transport in Parkinson’s disease: Manual Transport in PD. \\JournalTitleMovement Disorders 18, 565–572, DOI: 10.1002/mds.10402 (2003).\n\n[31] Verrel, J., Bekkering, H. & Steenbergen, B. Eye–hand coordination during manual object transport with the affected and less affected hand in adolescents with hemiparetic cerebral palsy. \\JournalTitleExperimental Brain Research 187, 107–116, DOI: 10.1007/s00221-008-1287-y (2008).\n\n[32] Grimme, B., Lipinski, J. & Schöner, G. Naturalistic arm movements during obstacle avoidance in 3D and the identification of movement primitives. \\JournalTitleExperimental Brain Research 222, 185–200, DOI: 10.1007/s00221-012-3205-6 (2012).\n\n[33] Grimme, B. Analysis and identification of elementary invariants as building blocks of human arm movements. doctoralthesis, International Graduate School of Biosciences, Ruhr-Universität Bochum (2015).\n\n[34] Raket, L. L. & Grimme, B. Bochum movement data. Github https://github.com/larslau/Bochum_movement_data (2016).\n\n[35] Oldfield, R. C. The assessment and analysis of handedness: The Edinburgh inventory. \\JournalTitleNeuropsychologia 9, 97–113, DOI: 10.1016/0028-3932(71)90067-4 (1971).\n\n[36] Tsunoda, Y. & Kakei, S. Anticipation of future events improves the ability to estimate elapsed time. \\JournalTitleExperimental Brain Research 214, 323–334, DOI: 10.1007/s00221-011-2821-x (2011).\n\n[37] Ball, W. E. & Voorhees, G. D. Adjustment of inertial survey system errors. \\JournalTitleThe Canadian Surveyor 32, 453–463, DOI: 10.1139/tcs-1978-0042 (1978).\n\n[38] XsensTechnologiesB.V. Magnetic calibration manual. https://www.xsens.com/hubfs/Downloads/Manuals/MT_Magnetic_Calibration_Manual.pdf (2019).\n\nAcknowledgements\n\nThis work was supported by the Ministry of Economics, Innovation, Digitization and Energy of the State of North Rhine-Westphalia and the European Union, grants GE-2-2-023A (REXO) and IT-2-2-023 (VAFES).\n\nAuthor Contributions Statement\n\nConceptualization: T.S., S.B., T.G., I.I.\n\nFunding acquisition: I.I.\n\nData recording: T.S., S.B.\n\nInvestigation: T.S., S.B.\n\nWriting – original draft: T.S., S.B.\n\nCompeting Interests\n\nThe authors declare no competing interests."
    }
}
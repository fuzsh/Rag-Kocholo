{
    "id": "wrong_mix_property_foundationPlace_00133_0",
    "rank": 51,
    "data": {
        "url": "https://elifesciences.org/articles/52465",
        "read_more_link": "",
        "language": "en",
        "title": "Navigating the garden of forking paths for data exclusions in fear conditioning research",
        "top_image": "https://elifesciences.org/assets/images/social/icon-600x600@1.52e87ee6.png",
        "meta_img": "https://elifesciences.org/assets/images/social/icon-600x600@1.52e87ee6.png",
        "images": [
            "https://elifesciences.org/assets/patterns/img/patterns/organisms/elife-logo-xs@1x.f9b97bde.png",
            "https://elifesciences.org/assets/patterns/img/icons/corresponding-author@1x.075234c5.png",
            "https://iiif.elifesciences.org/lax/52465%2Felife-52465-fig1-v2.tif/full/617,/0/default.jpg",
            "https://iiif.elifesciences.org/lax/52465%2Felife-52465-fig2-v2.tif/full/617,/0/default.jpg",
            "https://iiif.elifesciences.org/lax/52465%2Felife-52465-fig3-v2.tif/full/617,/0/default.jpg",
            "https://iiif.elifesciences.org/lax/52465%2Felife-52465-fig4-v2.tif/full/617,/0/default.jpg",
            "https://iiif.elifesciences.org/lax/52465%2Felife-52465-fig5-v2.tif/full/617,/0/default.jpg",
            "https://iiif.elifesciences.org/lax/52465%2Felife-52465-fig6-v2.tif/full/617,/0/default.jpg",
            "https://iiif.elifesciences.org/lax/52465%2Felife-52465-fig7-v2.tif/full/617,/0/default.jpg",
            "https://elifesciences.org/assets/patterns/img/icons/orcid.e0a7f9de.png",
            "https://elifesciences.org/assets/patterns/img/icons/orcid.e0a7f9de.png",
            "https://elifesciences.org/assets/patterns/img/icons/orcid.e0a7f9de.png",
            "https://elifesciences.org/assets/patterns/img/icons/orcid.e0a7f9de.png",
            "https://elifesciences.org/assets/patterns/img/icons/orcid.e0a7f9de.png",
            "https://elifesciences.org/assets/patterns/img/icons/orcid.e0a7f9de.png",
            "https://elifesciences.org/assets/patterns/img/icons/orcid.e0a7f9de.png",
            "https://elifesciences.org/assets/patterns/img/icons/orcid.e0a7f9de.png",
            "https://elifesciences.org/assets/patterns/img/icons/orcid.e0a7f9de.png",
            "https://elifesciences.org/assets/patterns/img/patterns/organisms/elife-logo-xs@1x.f9b97bde.png",
            "https://elifesciences.org/assets/images/investors/hhmi@1x.739f96c8.png",
            "https://elifesciences.org/assets/images/investors/wellcome@1x.b38198be.png",
            "https://elifesciences.org/assets/images/investors/max@1x.71f18cc3.png",
            "https://elifesciences.org/assets/images/investors/kaw@1x.d11d4f95.png",
            "https://elifesciences.org/assets/patterns/img/patterns/molecules/carbon-neutral.706efe6d.svg"
        ],
        "movies": [],
        "keywords": [],
        "meta_keywords": [
            ""
        ],
        "tags": null,
        "authors": [
            "Tina B Lonsdorf",
            "Maren Klingelhöfer-Jens",
            "Marta Andreatta",
            "Tom Beckers",
            "Anastasia Chalkia",
            "Anna Gerlicher",
            "Valerie L Jentsch",
            "Shira Meir Drexler",
            "Gaetan Mertens",
            "Jan Richter"
        ],
        "publish_date": "2019-12-16T00:00:00",
        "summary": "",
        "meta_description": "Exclusion of participants in tasks with a learning element can introduce substantial bias and needs to be carefully considered and transparently reported and justified.",
        "meta_lang": "en",
        "meta_favicon": "/assets/favicons/apple-touch-icon-57x57.c2b7763c.png",
        "meta_site_name": "eLife",
        "canonical_link": "https://elifesciences.org/articles/52465",
        "text": "Thirty-two percent (i.e., 16 records) of the records in our systematic literature search included a definition and exclusion of ‘non-responders’, with percentages of participants excluded as a result of non-responding ranging between 0% and 14% (see Figure 6A). A single study (Chauret et al., 2014; Oyarzún et al., 2012) reported % ‘non-learners’ and % ‘non-responders’ as a single value (see Appendix 1—table 2). The definitions differed in: i) the stimulus type(s) used to define ‘non-responding’ (CS+ reinforced, CS+ unreinforced, all CS+s, CS–, US), ii) the SCR minimum amplitude criterion used to define a responder (varying between 0.01 µS and 0.05 µS; visual inspection), and iii) the percentage of trials for which these criteria have to be met (see Figure 6B and Appendix 1—table 2), as well as a combination thereof.\n\nFigure 6\n\n‘Non-responding’ was most commonly defined as not showing a sufficient number of responses to the conditioned stimuli (CS+ and CS–), less frequently by the absence of responses to the US or any stimulus (CS+, CS- or US), and in two cases by the absence of responses to the CS+ or context (CXT+) specifically (see Figure 6B). Not surprisingly, the percentage of excluded participants differed substantially depending on the stimulus type used to define ‘non-responding’ (CS based, 0–10%; CS+/CXT+ based, 10–11%; US based, 0–4%; CS and US based, 11–14%; pre-experimental test based, 5%; Figure 6A).\n\nDespite these differences in the stimulus types used to define ‘non-responding’ in the first place, studies differed widely in the amplitude cutoff criterion to be exceeded in order to qualify as a response (see Figure 6B) as well as in the percentage of trials in which this cutoff had to be met (see Appendix 1—table 2).\n\nThe question of what (physiological) ‘non-responders’ during fear acquisition training are and how to identify them might be elucidated by investigating the number of ‘non-responses’ across trial types (CS and US) across data sets, and whether ‘non-responding’ to the US predicts ‘non-responding’ to the CS or vice versa. As expected from Figure 6A, the number of ‘non-responses’ to the US was low (as was also the case in our data [10%, Data set 1]), while the number of ‘non-responses’ to the CS (48.29%) was substantially higher – in particular for the CS– (58.6%; CS+ ‘non-responses’: 37.9%, see Figure 7A). This pattern, exemplarily illustrated here in one data set is representative of a larger number of data sets (see Appendix 4, table 1 for details). Furthermore, in our data (Data set 1), all individuals that did not react to the US in more than two thirds of the US trials also showed no responses to the CS (n = 3 of N = 119). To summarize, this provides the first evidence that ‘non-responding’ to the US may predict ‘non-responding’ to the CS but not vice versa. Furthermore, our data also suggest a positive correlation between the number of ‘non-responses’ to the US and the number of ‘non-responses’ to the CS (see Figure 7B for statistics).\n\nFigure 7\n\nOperationalizational heterogeneity is illustrated by every single record in our systematic literature search (covering a six months period) that employed definitions of ‘non-learners’ using a set of idiosyncratic criteria. The true number of definitions in the field applied over decades will be even substantially larger. In the records included here, 6–52% of participants were excluded (disregarding one study reporting percentages of ‘non-learners’ and ‘non-responders’ together with 74%; cf. Figure 2A), which substantially exceed the percentages recently put forward for ‘non-learning’ exclusions (Marin et al., 2019) that were suggested to lie between 4% (Chauret et al., 2014) and 19% (Oyarzún et al., 2012).\n\nIf several thousand analytical pipelines can be applied, the likelihood of false positives is high (Munafò et al., 2017) and the temptation of their opportunistic (ab)use must be considered a threat. Hence, a constructive discussion on where to go from here and how to not get lost in the garden of forking paths is important. This being said, we do acknowledge that certain research questions or the use of different recording equipment (robust lab equipment vs. novel mobile devices such as smartwatches) may potentially require distinct data-processing pipelines and the exclusion of certain observations (Silberzahn et al., 2018; Simmons et al., 2011), and hence it is not desirable to propose rigid and fixed rules for generic adoption. Procedural differences, in particular the inclusion of outcome measures that require certain triggers to elicit a response (such as startle responses or ratings) have also been shown to impact on the learning process itself (Sjouwerman et al., 2016). Rather, we call for a reconsideration of methods in the field and want to raise awareness to the pitfalls of adopting exclusion criteria from previously published work without critical evaluation of whether these apply meaningfully to one’s own research. Furthermore, we want to promote the adoption of transparent reporting of data processing, recording and analyses and strive to suggest standards in the field to reduce heterogeneity based on idiosyncratic customs rather than methodological and theoretical considerations (see Box 1).\n\nBox 1.\n\nList of reporting details, potential difficulties and recommendations when excluding ‘non-learners’ (performance-based exclusion) and/or ‘non-responders’ with a focus on SCRs.\n\nPlease note that this Box can be annotated online.\n\n(A) General reporting detailsWhat to report?Why is this considered important?What can go wrong or be ambiguous?Recommendations on how to proceedDetails on data recording and response quantification pipeline• because differences in data recording and quantification (i.e., response scoring) can make a substantial difference• report recording equipment and all settings used (e.g., filter)\n\n• report software used for response quantification\n\n• report precise details of response quantificationMinimal response criterion (μS) to define a valid SCR• to define valid responses• minimally detectable amplitude (e.g., 0.01, 0.02, 0.03, 0.05 µS, etc.) may be sample- and equipment-specific\n\n• no clear recommendations (existing guidelines provide a range of 0.01 to 0.05 µS) because this is influenced by noise level and equipment• test different minimal response criteria in the data set and define the cutoff empirically. In our experience (Data set 1), a cutoff was easily determined empirically by visually inspecting responses at different cutoffs (e.g., <0.01 µS, between 0.01 µS and 0.02 µS) and by evaluating their discrimination from noiseWhether the first CS+ and/or the first CS– trial is included or not, and information on trial sequence• no learning can be evident in the first trial, as the first US may occur at the earliest at the end of the CS+ and hence after the scoring window for the CS+-induced SCR\n\n• if the first trial is a CS–, no learning can have taken place as the US has not been presented yet\n\n• inclusion of the first trial (or the first trials in partial reinforcement protocols) may thus artificially reduce CS+/CS– discrimination• in fully randomized partial reinforcement protocols, US presentations may cluster in the first or last half of the acquisition training, which will impact on CS+/CS–discrimination in SCRs• careful experimental design with respect to trial-sequences (in particular in partial reinforcement protocols)\n\n• report whether the first trial for both CS+ and CS– is excluded because it may induce noise and bias CS+/CS– discrimination towards non-discrimination and as the first trial is sensitive to trial sequence effectsPrecise number of trials considered (if applicable for each trial type including reinforced and non-reinforced CS+ trials in case of partial reinforcement)• often difficult/ambiguous to infer this information from the 'Materials and methods' section of a reporta\n\n• number of trials that the ‘last half’ or ‘full phase’ refers to is contingent on experimental design and hence ambiguous and imprecise (see Figure 2B)• precision in reporting rather than relying on the reader making the right inferences\n\n• specify clearly the number of trials per stimulus type that are comprised in the ‘last half’ or ‘full phase’\n\n• provide a justification (theoretical and/or empirical) for this decisionbDetails of whether results were based on raw or transformed data• typically, transformations are required to allow interpretation of the reported results and to meet the assumptions of commonly statistical models• report details of transformation (e.g., logarithmized [log/LN], range-corrected, square-root) including the number of trials considered (for each stimulus type) and the sequence of transformations applied and specific formula (e.g., for range-correction)\n\n• provide justification for any applied transformation (e.g., violation of assumption of normal distribution of residuals)Precise number of excluded participants and specific reasons• often difficult/ambiguous to infer this information from the 'Materials and methods' section of a reporta• different researchers have different opinions on what ‘exclusion’ is (e.g., having individuals discontinue after a first experimental day based on performance should be considered and reported as exclusion)• report a breakdown of specific reasons for exclusions with respective n’s(B) Specific reporting details for exclusion of ‘non-learners’What to report?Why is this considered important?What can go wrong or be ambiguous?Recommendations on how to proceedCS+/CS– discrimination is calculated on the basis of raw SCR or transformed (e.g., logarithmized [log/LN], range-corrected, square-root) scores• the same criteria lead to different proportions of excluded individuals when applying them to raw or transformed data (see Figure 3A and B)• exact details of transformations (optimally calculation formulas) need to be included for full transparency and reproducibilityMinimal differential (CS+ vs. CS) cutoff for ‘non-learning’ in μS• different cutoffs lead to very different proportions of individuals excluded (see Figure 3)• exact details on cutoffs need to be included for full transparency and reproducibilityOn what outcome measures is ‘non-learning’ determined?• ‘non-learners’ do not necessarily converge across different outcome measures (Appendix 3, Figure 4—figure supplement 1)• all outcome measures recorded need to be reported• ‘non-learning’ should not be based on a single outcome measure or a clear justification needs to be provided as to why a single measure is considered meaningfulIf ‘non-learning’ is determined by responding during fear acquisition training, which trial types and number of trials per trial type were considered?• depending on the criteria employed, the same individual may be classified as ‘learner’ or ‘non-learner’ (see Figure 4)• classification as ‘non-learner’ should be based on differential scores (CS+ vs. CS–), and the number of trials included for this calculation should be clearly justified. Providing a generally valid recommendation regarding the number of trials to be included is difficult because it critically depends on experimental design choicesIf ‘non-learning ‘criteria are used, do they differ from criteria that the researcher or the research group used in previous publications? If yes, why were the criteria changed?• provide explicit justifications on why different criteria were used previously and presently• report differences between present and previous criteria used including references and justificationsDid ‘non-learners’ really fail to learn?• important as a manipulation check but note that the absence of a statistically significant CS+/CS– discrimination effect in a group on average cannot be taken to imply that all individuals in this group do not show meaningful CS+/CS–discrimination• individuals classified as ‘non-learners’ may in fact show significant CS+/CS– discrimination in SCRs (see Appendix 2) or in other outcome measures (see Figure 3—figure supplement 1 and Appendix 4) and hence fail the manipulation check• do the groups classified as ‘non-learners’ and ‘learners’ differ significantly in discrimination, and do ‘non-learners’ really not discriminate in SCRs and other outcome measures? Report the data on this group graphically and/or statistically in the supplementary material (do not report the full sample with and without exclusions only)Are results contingent on the exclusion of ‘non-learners’?• important to allow for transparency and to evaluate the impact of the results• it is not clearly defined when results differ meaningfully when excluding and including ‘non-learners’• provide results with and without exclusion of ‘non-learners’\n\n• additional analyses can be provided as supplementary material. When results are not contingent on the exclusion of ‘non-learners’, it is sufficient to mention this briefly in the results of the main manuscript (e.g., results are not contingent on the exclusion of ‘non-learners’)\n\n• if the results of the main analyses and hence the main conclusions change when ‘non-learners’ are excluded, this needs to be included in the main manuscript , and the implications need to be adequately discussed. Please note that this does not necessarily invalidate findings but can refine themDescriptive statistics for excluded ‘non-learners’• important to allow for transparency and evaluation of the potential sample biases introduced• report sex, age, anxiety levels, awareness(C) Specific reporting details for exclusions of ‘non-responders’What to report?Why is this considered important?What can go wrong or be ambiguous?Recommendations on how to proceedWhether ‘non-responses’ are calculated on the basis of raw SCR or transformed (e.g., logarithmized [log/LN], range-corrected, square-root) scores• the same criteria lead to different proportions of excluded individuals when applying to raw or transformed data (see Figure 3A and B)• exact details of transformations (optimally calculation formulas) need to be included for full transparency and reproducibilityMinimal cutoff for ‘non-responses’ in μS• it is often difficult/ambiguous to infer this information from the 'Materials and methods' section of a reporta\n\n• higher cutoffs could unnecessarily reduce the sample size• exact details on cutoffs need to be included for full transparency and reproducibilityWas ‘non-responding’ determined in a pre-experimental phase such as forced-breathing or US calibration?• determining ‘non-responding’ during a pre-experimental phase may help to detect malfunctioning of the equipment and allow this to be corrected prior to data acquisition\n\n• classification of ‘non-responders’ independent of the experimental task and its specifications (e.g., number of US presentations)• electrodes may detach between the pre-experimental phase and fear acquisition training• report details of pre-experimental phase\n\n• classification in SCR ‘non-responders’ should be based on a pre-experimental phase if no US presentations occur during the experiment, such as in case of threat of shock experiments, observational conditioning, extinction or return of fear testsIf ‘non-responding’ is determined by responding during fear acquisition training, what trial types are considered?• frequency of ‘non-responding’ differs substantially between different stimuli (CS and US) but also between CS+ and CS– (see Figure 7A)• ‘non-responding’ to the US may be due to technical failure (i.e., no US was administered)• classification in SCR ‘non-responders’ should not be based on SCRS elicited by CS (CS+, CS– or both), but should be based on US responding\n\n• a question on the estimated number of US presented during fear acquisition training (and all other phases) may serve as a manipulation checkDescriptive statistics for excluded ‘non-responder’• important to allow for transparency and evaluation of the potential sample biases introduced• report sex, age, anxiety levels, awareness\n\nYet, there are many other critical considerations worth discussing beyond the heterogeneous criteria used to define ‘non-learners’ and their impact on the outcome of statistical tests:\n\nFirst, ‘performance-based exclusion of participants’ is often based on a single outcome measure (typically SCRs), despite multiple measures being recorded (for exceptions see Ahmed and Lovibond, 2019; Belleau et al., 2018; Oyarzún et al., 2012). Importantly, ‘fear learning’ cannot be reliably inferred by means of SCRs, because SCRs capture arousal-related processes and can only be used as a proxy to infer ‘fear learning’ as fear is closely linked to arousal (Hamm and Weike, 2005). Relatedly, the fact that physiological proxies of ‘fear’ do not map onto ‘fear’ itself has been discussed extensively (LeDoux, 2012; LeDoux, 2014).\n\nSecond, but related, individuals that fail to show CS+/CS– discrimination in SCRs may show substantial discrimination, as an indicator of successful learning, in other outcome measures such as ratings of fear, US expectancy or fear potentiated startle (Hamm and Weike, 2005; Marin et al., 2019), as illustrated here for fear ratings (see Figure 4—figure supplement 1 and Appendix 3—table 1).\n\nThird, a common justification for excluding ‘non-learners’ is that it is not possible to investigate extinction- or return-of-fear-related phenomena in individuals who ‘did not learn’. To our knowledge, there is some evidence (Craske et al., 2008; Plendl and Wotjak, 2010; Prenoveau et al., 2013) that this theoretical assumption does not necessarily hold true, (i.e., CS+/CS– discrimination during fear acquisition training does not necessarily predict CS+/CS– discrimination during other experimental phases) (Gerlicher et al., 2019). An empirical investigation of this, however, would go beyond this manuscript’s scope.\n\nFourth, we provided empirical evidence that those classified as a group of ‘non-learners’ in SCRs in the literature (sometimes referred to as ‘outliers’) on the basis of the identified definitions in fact displayed significant CS+/CS– discrimination when applied to our own data. An exception to this was using cut offs in differential responding of <0.05 µS (note, however, that a non-significant CS+/CS– discrimination effect in the group of ‘non-learners’ as a whole cannot be taken as evidence that all individuals in this group do not in fact display meaningful or statistically significant CS+/CS– discrimination). Hence, in addition to the many conceptual problems we raised here, the operationalization of ‘non-learning’ in the field failed its critical manipulation check given that those classified as ‘non-learners’ show clear evidence of learning as a group (i.e., CS+/CS– discrimination, see Appendix 2—table 1).\n\nFifth, we illustrate a concerning sample bias that is introduced by performance-based participant exclusion. CS+/CS– discrimination in SCRs during fear acquisition training has been linked to a number of individual difference factors (Lonsdorf and Merz, 2017) and, naturally, selecting participants on the basis of SCR CS+/CS– discrimination will also select them on the basis of these individual differences (illustrated by our case example on trait anxiety, Figure 5). In our case example, we illustrate that excluding ‘non-learners’ biases the sample towards low anxiety scores, which hampers the generalizability and replicability of findings: i) the effect may only exist in low-anxiety individuals but not in the general population, and ii) as fear acquisition is a clinically relevant paradigm, pre-selection in favor of low-anxiety individuals might represent a threat to the clinical translation of the findings. Many studies in the field of fear conditioning research aim to develop behavioral or pharmacological manipulations to enhance treatment effects or aim to study mechanisms that are relevant for clinical fear and anxiety. Hence, it is highly problematic that these studies may exclude individuals who show response patterns that mimic responses typically observed in anxiety patients when excluding ‘non-learners’. In fact, patients suffering from anxiety disorders have been shown to be characterized by generalization of fear from the CS+ to the CS– (Duits et al., 2015).\n\nSixth, as illustrated by our case example (Figure 3), high CS+/CS– discrimination cutoffs generally favor individuals with high SCR amplitudes despite potentially identical ratios between CS+ and CS– amplitudes, which may introduce a sampling bias for individuals characterized by high arousal levels that probably have biological underpinnings. Relatedly, future studies need to empirically address which criteria for SCR transformation and exclusions are more or less sensitive to baseline differences (for an example from startle responding see Bradford et al., 2015; Grillon and Baas, 2002).\n\nIn summary, in light of the many (potential) problems associated with performance-based exclusion of participants, we forcefully echo Marin et al.’s conclusion that one needs \"to be cautious when excluding SCR non-learners and to consider the potential implications of such exclusion when interpreting the findings from studies of conditioned fear\" (Marin et al., 2019, abstract). Routinely, excluding participants who are intentionally or unintentionally characterized by specific individual differences represents a major threat to generalizability, replicability and potentially clinical translation of findings, as results might be contingent on a specific sub-sample and specific sample characteristics. This is also true when researchers are interested in the study of general processes. Furthermore, by excluding these individuals from further analyses, we may miss the opportunity to understand why some individuals do not show discrimination between the CS+ and the CS– in SCRs (or other outcome measures) or whether this lack of discrimination is maintained across subsequent experimental phases. It can be speculated that this lack of discrimination may carry meaningful information – at least for a subsample.\n\nIn addition to ‘non-learners’, ‘non-responders’ are also often excluded during fear conditioning research. We showed that the definition of ‘non-responders’, like that of ‘non-learners’, varies widely across studies. Heterogeneity in definitions manifests in different cutoff criteria for what is considered a valid response, the number of trials and the stimulus type(s) considered (Appendix 1—table 2, Figure 6). Surprisingly, most definitions are based on CS responses (i.e., SCRs to the CS+ and/or CS–) and only few are based on US responses. This highlights a potentially problematic overlap between ‘non-learners’ and ‘non-responders’: ‘non-responding’ to the CS (i.e., CS+ and CS– or CS+ only) is not necessarily indicative of physiological ‘non-responding’ – especially if high cutoffs are used. In fact, ‘non-responding’ to the CS may, or at least in some cases, reflect the absence of learning-based patterns in physiological responding – which may carry important information. Having observed the striking differences in percentages of ‘non-responses’ to the US (10%) and CS (48%) observed in our data (see Figure 7 and Appendix 4—table 1), we suggest that physiological ‘non-responding’ cannot and should not be determined on the basis of the absence of responding to the CS.\n\nMore globally, the group of ‘non-responders’, as defined by the criteria identified here, probably lumps together several sub-groups: individuals (1) for whom technical problems resulted in no valid SCRs, (2) who fell asleep or did not pay attention, (3) who cognitively learned the CS+/US contingencies but did not express the expected corresponding responses in SCRs, and (4) who were attentive to the experiment but did not learn the contingencies (i.e., unaware participants) and hence did not show the expected SCR patterns (Tabbert et al., 2011).\n\nIn summary, although excluding physiological ‘non-responders’ makes sense (in terms of a manipulation check and independent of the hypothesis), we consider defining ‘non-responders’ on the basis of the absence of SCRs to the CS as problematic (dependent on the hypothesis). We suggest that physiological SCR ‘non-responders’ should be defined on the basis of US responses during fear acquisition training or to strong stimuli during pre-conditioning phases such as US calibration, startle habituation or forced breathing (reliably eliciting strong SCRs). If ‘non-responding’ to the US (during fear acquisition training) is used, it is difficult to suggest a universally valid cutoff with respect to the number or percentage of required valid US responses, because this critically depends on a number of variables such as hardware and sampling rate used. It remains an open question for future work whether data quality of novel mobile devices (e.g., smartwatches) for the acquisition of SCRs differs from traditional, robust lab-based recordings and how this would impact on the frequency of exclusions based on SCRs. Appendix 4 suggests that the cutoff may typically range between 1/3 and 2/3 of valid responses but may be data-set specific. US-based criteria are of course not trivial in multiple-day experiments, in which certain experimental days do not involve the presentation of US or involve few temporally clustered US presentations (i.e., reinstatement), or in paradigms not involving direct exposure to the US (i.e., observational or instructional learning; Haaker et al., 2017). In these cases, the other options listed above are strongly preferred to CS based criteria.\n\nIn this work, we have comprehensively illustrated and argued that most of the current definitions employed to define ‘non-learners’ and ‘non-responders’ have to be considered as theoretically and empirically problematic. It is not sufficient, however, to raise awareness to these problems and the practical question of ‘Where do we go from here?’ remains to be addressed. What can we do to avoid getting lost in the garden of forking paths of exclusion criteria? Here, we would like to offer several solutions to improve practices in the field, which we expect to foster robustness, replicability and potentially clinical translation of findings: (1) transparency in reporting, (2) adopting open science practices, (3) increasing the level and quality of reporting and (4) graphical data presentation, (5) manipulation checks, and (6) fostering critical evaluation. We refer to see Box 1 for specific recommendations.\n\nMore precisely, transparency can be enhanced ‘if observations are eliminated, authors must also report what the statistical results are if those observations are included’, as suggested by Simmons and colleagues, nearly a decade ago (Simmons et al., 2011, Table 2). Here, we echo this call that this recommendation should be implemented routinely in data reporting pipelines when employing performance-based participant exclusions (‘non-learners’) in fear conditioning research. We also call for a transparent and adequate reporting in the results (in brief) and discussion section rather than providing this information exclusively in the appendix. This being said, it is important to point out that should a finding turn out to be contingent on the exclusion of ‘non-learners’, this does not necessarily invalidate this finding. On the contrary, it may further specify the finding or hint to possible mechanisms and/or boundary conditions – yet inferences on boundary conditions should be made carefully (Hardwicke and Shanks, 2016). Relatedly, adopting an open science culture will facilitate transparent reporting of exclusion criteria (Nosek et al., 2015) and will minimize the risk of exploiting heterogeneous definitions in the field. Registered reports (Hardwicke and Ioannidis, 2018), publicly available data including those from excluded participants and pre-registration (Munafò et al., 2017) of definitions and analysis pipelines (Ioannidis, 2014), as well as openly accessible lab-specific standard operational protocols (SOPs), may also be helpful.\n\nWe acknowledge, however, that transparent reporting and particularly pre-registration of exclusion criteria is not trivial in light of the unsatisfactory quality and level of detail in reporting in the field of fear conditioning research. It was striking that the compilation of exclusion criteria (‘non-learners’ and ‘non-responders’, see Appendix 1—tables 1 and 2) employed in the records included in our systematic literature search required extensive personal exchange with the authors because the definitions provided were often insufficient, ambiguous or incorrect. It is our responsibility as authors, reviewers and editors to improve these reporting standards to an acceptable level. As a guidance, Box 1 provides a compilation of reporting details that we consider important to include in both pre-registered protocols and publications (an editable online version of Box 1 is available to allow for further development, see Box caption).\n\nOur recommendations to improve the level of reporting details and transparency extends to the graphical illustration of results, which should optimally allow for a complete presentation of data (Weissgerber et al., 2015) without risking obscuring important patterns, providing detailed distributional information rather than merely presenting summary statistics (see Weissgerber et al., 2015 for a discussion). Such visualization options include, for instance, scatterplots, box plots, histograms, violin plots as well as their combination (see also Figure 5) in so called ‘rain cloud plots’ (see Allen et al., 2018 for a tutorial in R, Matlab and Phyton) and utilizing colors or color gradients to visualize different groups of individuals (for instance ‘learners’ and ‘non-learners’) or discrimination scores. This will provide readers with the opportunity to evaluate the presented results and conclusions independently and comprehensively.\n\nFinally, if criteria for ‘non-learners’ or ‘non-responders’ are employed to exclude participants from data analyses (or continuation of the experiment), we recommend that a sanity or manipulation check should be performed to determine whether – for instance - ‘non-learners’ really did not learn (i.e., really do not show significant CS+/CS– discrimination). We have empirically illustrated that most definitions of ‘non-learners’ fail this manipulation check (Appendix 2—table 1). Yet, it may not be feasible in all cases to determine such statistics, as these may not be appropriate for small samples and correspondingly small sub-groups of ‘non-learners’. Relatedly, we urge authors to justify adequately all details of the exclusion criteria (if applied) – both theoretically and practically. Furthermore, we encourage authors, reviewers and editors alike to critically evaluate whether exclusions and applied criteria are warranted in the first place and appropriate in the specific context (vs. mere adopting published or previously employed criteria) and whether these exclusion criteria are transparently reported and discussed if results hinge on them (Steegen et al., 2016).\n\nFurthermore, future work should empirically address the question of how to best define ‘non-learning’ in particular in light of different outcome measures in fear conditioning studies, which capture different aspects of defensive responding (Jentsch et al., 2020; Lonsdorf et al., 2017)."
    }
}
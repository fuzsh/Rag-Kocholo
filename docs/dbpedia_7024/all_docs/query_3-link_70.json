{
    "id": "dbpedia_7024_3",
    "rank": 70,
    "data": {
        "url": "https://dl.acm.org/doi/10.1145/3579371.3589049",
        "read_more_link": "",
        "language": "en",
        "title": "ArchGym: An Open-Source Gymnasium for Machine Learning Assisted Architecture Design",
        "top_image": "https://dl.acm.org/cms/asset/a36b81f7-6db5-4429-8c65-96c1c9362fc8/3579371.cover.jpg",
        "meta_img": "https://dl.acm.org/cms/asset/a36b81f7-6db5-4429-8c65-96c1c9362fc8/3579371.cover.jpg",
        "images": [
            "https://dl.acm.org/specs/products/acm/releasedAssets/images/acm-dl-logo-white-1ecfb82271e5612e8ca12aa1b1737479.png",
            "https://dl.acm.org/doi/10.1145/specs/products/acm/releasedAssets/images/acm-logo-1-ad466e729c8e2a97780337b76715e5cf.png",
            "https://dl.acm.org/userimages/na101/home/literatum/publisher/acm/classification/LinkedImages/reproducibility-types/artifacts_available_v101/icon-small_202009300323.png",
            "https://dl.acm.org/userimages/na101/home/literatum/publisher/acm/classification/LinkedImages/reproducibility-types/artifacts_evaluated_functional_v101/icon-small_202009300323.png",
            "https://dl.acm.org/userimages/na101/home/literatum/publisher/acm/classification/LinkedImages/reproducibility-types/results_reproduced_v101/icon-small_202009300324.png",
            "https://dl.acm.org/pb-assets/icons/DOs/default-profile-1543932446943.svg",
            "https://dl.acm.org/pb-assets/icons/DOs/default-profile-1543932446943.svg",
            "https://dl.acm.org/pb-assets/icons/DOs/default-profile-1543932446943.svg",
            "https://dl.acm.org/pb-assets/icons/DOs/default-profile-1543932446943.svg",
            "https://dl.acm.org/pb-assets/icons/DOs/default-profile-1543932446943.svg",
            "https://dl.acm.org/pb-assets/icons/DOs/default-profile-1543932446943.svg",
            "https://dl.acm.org/pb-assets/icons/DOs/default-profile-1543932446943.svg",
            "https://dl.acm.org/pb-assets/icons/DOs/default-profile-1543932446943.svg",
            "https://dl.acm.org/pb-assets/icons/DOs/default-profile-1543932446943.svg",
            "https://dl.acm.org/pb-assets/icons/DOs/default-profile-1543932446943.svg",
            "https://dl.acm.org/pb-assets/icons/DOs/default-profile-1543932446943.svg",
            "https://dl.acm.org/specs/products/acm/releasedAssets/images/footer-logo1-45ae33115db81394d8bd25be65853b77.png",
            "https://dl.acm.org/specs/products/acm/releasedAssets/images/Default_image_lazy-0687af31f0f1c8d4b7a22b686995ab9b.svg",
            "https://dl.acm.org/pb-assets/icons/DOs/default-profile-1543932446943.svg",
            "https://dl.acm.org/pb-assets/icons/DOs/default-profile-1543932446943.svg",
            "https://dl.acm.org/userimages/na101/home/literatum/publisher/acm/classification/LinkedImages/reproducibility-types/artifacts_available_v101/icon-large_202009300323.png",
            "https://dl.acm.org/userimages/na101/home/literatum/publisher/acm/classification/LinkedImages/reproducibility-types/artifacts_evaluated_functional_v101/icon-large_202009300323.png",
            "https://dl.acm.org/userimages/na101/home/literatum/publisher/acm/classification/LinkedImages/reproducibility-types/results_reproduced_v101/icon-large_202009300324.png",
            "https://dl.acm.org/specs/products/acm/releasedAssets/images/loader-7e60691fbe777356dc81ff6d223a82a6.gif",
            "https://dl.acm.org/pb-assets/icons/DOs/default-profile-1543932446943.svg",
            "https://dl.acm.org/pb-assets/icons/DOs/default-profile-1543932446943.svg",
            "https://dl.acm.org/pb-assets/icons/DOs/default-profile-1543932446943.svg",
            "https://dl.acm.org/pb-assets/icons/DOs/default-profile-1543932446943.svg",
            "https://dl.acm.org/pb-assets/icons/DOs/default-profile-1543932446943.svg",
            "https://dl.acm.org/pb-assets/icons/DOs/default-profile-1543932446943.svg",
            "https://dl.acm.org/pb-assets/icons/DOs/default-profile-1543932446943.svg",
            "https://dl.acm.org/pb-assets/icons/DOs/default-profile-1543932446943.svg",
            "https://dl.acm.org/pb-assets/icons/DOs/default-profile-1543932446943.svg",
            "https://dl.acm.org/pb-assets/icons/DOs/default-profile-1543932446943.svg",
            "https://dl.acm.org/pb-assets/icons/DOs/default-profile-1543932446943.svg",
            "https://dl.acm.org/specs/products/acm/releasedAssets/images/acm-logo-dl-8437178134fce530bc785276fc316cbf.png",
            "https://dl.acm.org/specs/products/acm/releasedAssets/images/acm-logo-3-10aed79f3a6c95ddb67053b599f029af.png"
        ],
        "movies": [],
        "keywords": [],
        "meta_keywords": [
            ""
        ],
        "tags": null,
        "authors": [
            "Aleksandra Faust Google Research",
            "Mountain View",
            "USA https:",
            "orcid.org",
            "Srivatsan Krishnan Harvard University",
            "Amir Yazdanbakhsh Google Research",
            "Brain Team",
            "Shvetank Prakash Harvard University",
            "Jason Jabbour Harvard University",
            "Ikechukwu Uchendu Harvard University"
        ],
        "publish_date": null,
        "summary": "",
        "meta_description": "",
        "meta_lang": "en",
        "meta_favicon": "/pb-assets/head-metadata/apple-touch-icon-1574252172393.png",
        "meta_site_name": "ACM Conferences",
        "canonical_link": "https://dl.acm.org/doi/10.1145/3579371.3589049",
        "text": "Abstract\n\nMachine learning (ML) has become a prevalent approach to tame the complexity of design space exploration for domain-specific architectures. While appealing, using ML for design space exploration poses several challenges. First, it is not straightforward to identify the most suitable algorithm from an ever-increasing pool of ML methods. Second, assessing the trade-offs between performance and sample efficiency across these methods is inconclusive. Finally, the lack of a holistic framework for fair, reproducible, and objective comparison across these methods hinders the progress of adopting ML-aided architecture design space exploration and impedes creating repeatable artifacts. To mitigate these challenges, we introduce ArchGym, an open-source gymnasium and easy-to-extend framework that connects a diverse range of search algorithms to architecture simulators. To demonstrate its utility, we evaluate ArchGym across multiple vanilla and domain-specific search algorithms in the design of a custom memory controller, deep neural network accelerators, and a custom SoC for AR/VR workloads, collectively encompassing over 21K experiments. The results suggest that with an unlimited number of samples, ML algorithms are equally favorable to meet the user-defined target specification if its hyperparameters are tuned thoroughly; no one solution is necessarily better than another (e.g., reinforcement learning vs. Bayesian methods). We coin the term \"hyperparameter lottery\" to describe the relatively probable chance for a search algorithm to find an optimal design provided meticulously selected hyperparameters. Additionally, the ease of data collection and aggregation in ArchGym facilitates research in ML-aided architecture design space exploration. As a case study, we show this advantage by developing a proxy cost model with an RMSE of 0.61% that offers a 2,000-fold reduction in simulation time. Code and data for ArchGym is available at https://bit.ly/ArchGym.\n\nReferences\n\n[1]\n\n[n. d.]. Scikit-opt. https://scikit-opt.github.io/\n\n[2]\n\n[n. d.]. Scikit-Optimize. https://scikit-optimize.github.io/\n\n[3]\n\nMartín Abadi, Paul Barham, Jianmin Chen, Zhifeng Chen, Andy Davis, Jeffrey Dean, Matthieu Devin, Sanjay Ghemawat, Geoffrey Irving, Michael Isard, et al. 2016. Tensorflow: a system for large-scale machine learning. In USENIX Symposium on Operating Systems Design and Implementation (OSDI), Vol. 16. Savannah, GA, USA, 265--283.\n\n[4]\n\nMichael Ahn, Anthony Brohan, Noah Brown, Yevgen Chebotar, Omar Cortes, Byron David, Chelsea Finn, Chuyuan Fu, Keerthana Gopalakrishnan, Karol Hausman, Alex Herzog, Daniel Ho, Jasmine Hsu, Julian Ibarz, Brian Ichter, Alex Irpan, Eric Jang, Rosario Jauregui Ruano, Kyle Jeffrey, Sally Jesmonth, Nikhil J Joshi, Ryan Julian, Dmitry Kalashnikov, Yuheng Kuang, Kuang-Huei Lee, Sergey Levine, Yao Lu, Linda Luu, Carolina Parada, Peter Pastor, Jornell Quiambao, Kanishka Rao, Jarek Rettinghouse, Diego Reyes, Pierre Sermanet, Nicolas Sievers, Clayton Tan, Alexander Toshev, Vincent Vanhoucke, Fei Xia, Ted Xiao, Peng Xu, Sichun Xu, Mengyuan Yan, and Andy Zeng. 2022. Do As I Can, Not As I Say: Grounding Language in Robotic Affordances. arXiv preprint arXiv:2204.01691 (2022).\n\n[5]\n\nDario Amodei, Girish Sastry Danny Hernandez, Jack Clark, Greg Brockman, and Ilya Sutskever. 2018. AI and Compute. https://openai.com/blog/ai-and-compute/\n\n[6]\n\nYehia Arafa, Abdel-Hameed A Badawy, Gopinath Chennupati, Nandakishore Santhi, and Stephan Eidenbenz. 2019. Ppt-gpu: Scalable gpu performance modeling. IEEE Computer Architecture Letters (2019), 55--58.\n\n[7]\n\nDominik Bauer, Timothy Patten, and Markus Vincze. 2021. Reagent: Point Cloud Registration Using Imitation and Reinforcement Learning. In Conference on Computer Vision and Pattern Recognition (CVPR).\n\n[8]\n\nJames Bergstra and Yoshua Bengio. 2012. Random Search for Hyper-Parameter Optimization. Journal of Machine Learning Research 13, 10 (2012), 281--305. http://jmlr.org/papers/v13/bergstra12a.html\n\n[9]\n\nKshitij Bhardwaj, Marton Havasi, Yuan Yao, David M. Brooks, José Miguel Hernández Lobato, and Gu-Yeon Wei. 2019. Determining Optimal Coherency Interface for Many-Accelerator SoCs Using Bayesian Optimization. IEEE Computer Architecture Letters (CAL) (2019).\n\n[10]\n\nBehzad Boroujerdian, Ying Jing, Devashree Tripathy, Amit Kumar, Lavanya Subramanian, Luke Yen, Vincent Lee, Vivek Venkatesan, Amit Jindal, Robert Shearer, et al. 2023. FARSI: An early-stage design space exploration framework to tame the domain-specific system-on-chip complexity. ACM Transactions on Embedded Computing Systems 22, 2 (2023), 1--35.\n\n[11]\n\nLeo Breiman. 2001. Random Forests. Machine learning 45, 1 (2001), 5--32.\n\n[12]\n\nTom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. 2020. Language models are few-shot learners. Advances in neural information processing systems (NeuRIPS 33 (2020), 1877--1901.\n\n[13]\n\nJacob Buckman, Danijar Hafner, George Tucker, Eugene Brevdo, and Honglak Lee. 2018. Sample-Efficient Reinforcement Learning with Stochastic Ensemble Value Expansion. In Conference on Neural Information Processing Systems (NeurIPS.\n\n[14]\n\nEdvinas Byla and Wei Pang. 2019. DeepSwarm: Optimising Convolutional Neural Networks using Swarm Intelligence. In UK Workshop on Computational Intelligence.\n\n[15]\n\nTrevor E Carlson, Wim Heirman, and Lieven Eeckhout. 2011. Sniper: Exploring the level of abstraction for scalable and accurate parallel multi-core simulation. In Proceedings of 2011 International Conference for High Performance Computing, Networking, Storage and Analysis. 1--12.\n\n[16]\n\nLeandro Nunes de Castro. 2007. Fundamentals of Natural Computing (Chapman & Hall/Crc Computer and Information Sciences).\n\n[17]\n\nHongming Chen, Ola Engkvist, Yinhai Wang, Marcus Olivecrona, and Thomas Blaschke. 2018. The Rise of Deep Learning in Drug Discovery. Drug discovery today (2018).\n\n[18]\n\nYu-Hsin Chen, Joel Emer, and Vivienne Sze. 2016. Eyeriss: A Spatial Architecture for Energy-Efficient Dataflow for Convolutional Neural Networks. In The 42nd Annual International Symposium on Computer Architecture (ISCA).\n\n[19]\n\nAakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam Roberts, Paul Barham, Hyung Won Chung, Charles Sutton, Sebastian Gehrmann, Parker Schuh, Kensen Shi, Sasha Tsvyashchenko, Joshua Maynez, Abhishek Rao, Parker Barnes, Yi Tay, Noam Shazeer, Vinodkumar Prabhakaran, Emily Reif, Nan Du, Ben Hutchinson, Reiner Pope, James Bradbury, Jacob Austin, Michael Isard, Guy Gur-Ari, Pengcheng Yin, Toju Duke, Anselm Levskaya, Sanjay Ghemawat, Sunipa Dev, Henryk Michalewski, Xavier Garcia, Vedant Misra, Kevin Robinson, Liam Fedus, Denny Zhou, Daphne Ippolito, David Luan, Hyeontaek Lim, Barret Zoph, Alexander Spiridonov, Ryan Sepassi, David Dohan, Shivani Agrawal, Mark Omernick, Andrew M. Dai, Thanumalayan Sankaranarayana Pillai, Marie Pellat, Aitor Lewkowycz, Erica Moreira, Rewon Child, Oleksandr Polozov, Katherine Lee, Zongwei Zhou, Xuezhi Wang, Brennan Saeta, Mark Diaz, Orhan Firat, Michele Catasta, Jason Wei, Kathy Meier-Hellstern, Douglas Eck, Jeff Dean, Slav Petrov, and Noah Fiedel. 2022. PaLM: Scaling Language Modeling with Pathways.\n\n[20]\n\nNVIDIA Corporation. 2022. NVIDIA H100 Tensor Core GPU. https://www.nvidia.com/en-us/data-center/h100/\n\n[21]\n\nChris Cummins, Bram Wasti, Jiadong Guo, Brandon Cui, Jason Ansel, Sahir Gomez, Somya Jain, Jia Liu, Olivier Teytaud, Benoit Steiner, et al. 2022. Compilergym: Robust, performant compiler optimization environments for ai research. In 2022 IEEE/ACM International Symposium on Code Generation and Optimization (CGO). IEEE, 92--105.\n\n[22]\n\nMiguel de Prado, Andrew Mundy, Rabia Saeed, Maurizo Denna, Nuria Pazos, and Luca Benini. 2020. Automated design space exploration for optimized deployment of dnn on arm cortex-a cpus. IEEE Transactions on Computer-Aided Design of Integrated Circuits and Systems 40, 11 (2020), 2293--2305.\n\n[23]\n\nJonas Degrave, Federico Felici, Jonas Buchli, Michael Neunert, Brendan Tracey, Francesco Carpanese, Timo Ewalds, Roland Hafner, Abbas Abdolmaleki, Diego de las Casas, Craig Donner, Leslie Fritz, Cristian Galperti, Andrea Huber, James Keeling, Maria Tsimpoukelli, Jackie Kay, Antoine Merle, Jean-Marc Moret, Seb Noury, Federico Pesamosca, David Pfau, Olivier Sauter, Cristian Sommariva, Stefano Coda, Basil Duval, Ambrogio Fasoli, Pushmeet Kohli, Koray Kavukcuoglu, Demis Hassabis, and Martin Riedmiller. 2022. Magnetic Control of Tokamak Plasmas Through Deep Reinforcement Learning. Nature (2022).\n\n[24]\n\nPrafulla Dhariwal, Christopher Hesse, Oleg Klimov, Alex Nichol, Matthias Plappert, Alec Radford, John Schulman, Szymon Sidor, Yuhuai Wu, and Peter Zhokhov. 2017. OpenAI Baselines.\n\n[25]\n\nLieven Eeckhout, Robert H. Bell Jr., Bastiaan Stougie, Koen De Bosschere, and Lizy K. John. 2004. Control Flow Modeling in Statistical Simulation for Accurate and Efficient Processor Design Studies. In Proceedings of the 31st Annual International Symposium on Computer Architecture (München, Germany) (ISCA '04). IEEE Computer Society, USA, 350.\n\n[26]\n\nAhmed T Elthakeb, Prannoy Pilligundla, Fatemehsadat Mireshghallah, Amir Yazdanbakhsh, and Hadi Esmaeilzadeh. 2020. ReLeQ: A Reinforcement Learning Approach for Deep Quantization of Neural Networks. IEEE Micro (2020).\n\n[27]\n\nLasse Espeholt, Raphaël Marinier, Piotr Stanczyk, Ke Wang, and Marcin Michalski. 2019. Seed RL: Scalable and Efficient Deep-RL with Accelerated Central Inference. arXiv preprint arXiv:1910.06591 (2019).\n\n[28]\n\nMatthias Feurer and Frank Hutter. 2019. Hyperparameter optimization. Automated machine learning: Methods, systems, challenges (2019), 3--33.\n\n[29]\n\nJeremy Fowers, Kalin Ovtcharov, Michael Papamichael, Todd Massengill, Ming Liu, Daniel Lo, Shlomi Alkalay, Michael Haselman, Logan Adams, Mahdi Ghandi, et al. 2018. A configurable cloud-scale DNN processor for real-time AI. In 2018 ACM/IEEE 45th Annual International Symposium on Computer Architecture (ISCA). IEEE, 1--14.\n\n[30]\n\nYiheng Gao and Benjamin Carrion Schafer. 2021. Effective high-level synthesis design space exploration through a novel cost function formulation. In 2021 IEEE International Symposium on Circuits and Systems (ISCAS). IEEE, 1--5.\n\n[31]\n\nSimon Goss, Serge Aron, Jean-Louis Deneubourg, and Jacques Marie Pasteels. 1989. Self-Organized Shortcuts in the Argentine Ant. Naturwissenschaften (1989).\n\n[32]\n\nSergio Guadarrama, Anoop Korattikara, Oscar Ramirez, Pablo Castro, Ethan Holly, Sam Fishman, Ke Wang, Ekaterina Gonina, Neal Wu, Efi Kokiopoulou, Luciano Sbaiz, Jamie Smith, Gábor Bartók, Jesse Berent, Chris Harris, Vincent Vanhoucke, and Eugene Brevdo. 2018. TF-Agents: A library for Reinforcement Learning in TensorFlow. https://github.com/tensorflow/agents. https://github.com/tensorflow/agents [Online; accessed 25-June-2019].\n\n[33]\n\nGagan Gupta, Tony Nowatzki, Vinay Gangadhar, and Karthikeyan Sankaralingam. 2017. Kickstarting semiconductor innovation with open source hardware. Computer 50, 6 (2017), 50--59.\n\n[34]\n\nTuomas Haarnoja, Aurick Zhou, Pieter Abbeel, and Sergey Levine. 2018. Soft actor-critic: Off-policy maximum entropy deep reinforcement learning with a stochastic actor. In International conference on machine learning. PMLR, 1861--1870.\n\n[35]\n\nCharles R Harris, K Jarrod Millman, Stéfan J Van Der Walt, Ralf Gommers, Pauli Virtanen, David Cournapeau, Eric Wieser, Julian Taylor, Sebastian Berg, Nathaniel J Smith, et al. 2020. Array programming with NumPy. Nature 585, 7825 (2020), 357--362.\n\n[36]\n\nMilad Hashemi, Kevin Swersky, Jamie Smith, Grant Ayers, Heiner Litz, Jichuan Chang, Christos Kozyrakis, and Parthasarathy Ranganathan. 2018. Learning memory access patterns. In International Conference on Machine Learning. PMLR, 1919--1928.\n\n[37]\n\nKartik Hegde, Po-An Tsai, Sitao Huang, Vikas Chandra, Angshuman Parashar, and Christopher W Fletcher. 2021. Mind mappings: enabling efficient algorithm-accelerator mapping space search. In Proceedings of the 26th ACM International Conference on Architectural Support for Programming Languages and Operating Systems. 943--958.\n\n[38]\n\nDan Hendrycks and Kevin Gimpel. 2017. A Baseline for Detecting Misclassified and Out-of-Distribution Examples in Neural Networks. In International Conference on Learning Representations.\n\n[39]\n\nMatthew W. Hoffman, Bobak Shahriari, John Aslanides, Gabriel Barth-Maron, Nikola Momchev, Danila Sinopalnikov, Piotr Stańczyk, Sabela Ramos, Anton Raichuk, Damien Vincent, Léonard Hussenot, Robert Dadashi, Gabriel Dulac-Arnold, Manu Orsini, Alexis Jacq, Johan Ferret, Nino Vieillard, Seyed Kamyar Seyed Ghasemipour, Sertan Girgin, Olivier Pietquin, Feryal Behbahani, Tamara Norman, Abbas Abdolmaleki, Albin Cassirer, Fan Yang, Kate Baumli, Sarah Henderson, Abe Friesen, Ruba Haroun, Alex Novikov, Sergio Gómez Colmenarejo, Serkan Cabi, Caglar Gulcehre, Tom Le Paine, Srivatsan Srinivasan, Andrew Cowie, Ziyu Wang, Bilal Piot, and Nando de Freitas. 2020. ACME: A Research Framework for Distributed Reinforcement Learning. arXiv preprint arXiv:2006.00979 (2020).\n\n[40]\n\nRamsey Hourani, Ravi Jenkal, W Rhett Davis, and Winser Alexander. 2009. Automated Design Space Exploration for DSP Applications. Journal of Signal Processing Systems (2009).\n\n[41]\n\nQijing Huang, Charles Hong, John Wawrzynek, Mahesh Subedar, and Yakun Sophia Shao. 2022. Learning A Continuous and Reconstructible Latent Space for Hardware Accelerator Design. In 2022 IEEE International Symposium on Performance Analysis of Systems and Software (ISPASS). IEEE, 277--287.\n\n[42]\n\nShengyi Huang, Rousslan Fernand Julien Dossa, Chang Ye, and Jeff Braga. 2021. CleanRL: High-quality Single-file Implementations of Deep Reinforcement Learning Algorithms. arXiv preprint arXiv:2111.08819 (2021).\n\n[43]\n\nEngin Ipek, Onur Mutlu, José F. Martínez, and Rich Caruana. 2008. Self-Optimizing Memory Controllers: A Reinforcement Learning Approach. In Proceedings of the 35th Annual International Symposium on Computer Architecture (ISCA '08). IEEE Computer Society, USA, 39--50.\n\n[44]\n\nTang Jie and Pieter Abbeel. 2010. On a connection between importance sampling and the likelihood ratio policy gradient. Advances in Neural Information Processing Systems 23 (2010).\n\n[45]\n\nHadi S. Jomaa, Josif Grabocka, and Lars Schmidt-Thieme. 2019. Hyp-RL : Hyperparameter Optimization by Reinforcement Learning. arXiv preprint arXiv:1906.11527 (2019).\n\n[46]\n\nNorman P. Jouppi, George Kurian, Sheng Li, Peter Ma, Rahul Nagarajan, Lifeng Nai, Nishant Patil, Suvinay Subramanian, Andy Swing, Brian Towles, Cliff Young, Xiang Zhou, Zongwei Zhou, and David Patterson. 2023. TPU v4: An Optically Reconfigurable Supercomputer for Machine Learning with Hardware Support for Embeddings. arXiv:2304.01433 [cs.AR]\n\n[47]\n\nNorman P. Jouppi, Cliff Young, Nishant Patil, David Patterson, Gaurav Agrawal, Raminder Bajwa, Sarah Bates, Suresh Bhatia, Nan Boden, Al Borchers, Rick Boyle, Pierre luc Cantin, Clifford Chao, Chris Clark, Jeremy Coriell, Mike Daley, Matt Dau, Jeffrey Dean, Ben Gelb, Tara Vazir Ghaemmaghami, Rajendra Gottipati, William Gulland, Robert Hagmann, C. Richard Ho, Doug Hogberg, John Hu, Robert Hundt, Dan Hurt, Julian Ibarz, Aaron Jaffey, Alek Jaworski, Alexander Kaplan, Harshit Khaitan, Andy Koch, Naveen Kumar, Steve Lacy, James Laudon, James Law, Diemthu Le, Chris Leary, Zhuyuan Liu, Kyle Lucke, Alan Lundin, Gordon MacKean, Adriana Maggiore, Maire Mahony, Kieran Miller, Rahul Nagarajan, Ravi Narayanaswami, Ray Ni, Kathy Nix, Thomas Norrie, Mark Omernick, Narayana Penukonda, Andy Phelps, and Jonathan Ross. 2017. In-Datacenter Performance Analysis of a Tensor Processing Unit. In Proceedings of the 44th annual international symposium on computer architecture. 1--12.\n\n[48]\n\nJohn Jumper, Richard Evans, Alexander Pritzel, Tim Green, Michael Figurnov, Olaf Ronneberger, Kathryn Tunyasuvunakool, Russ Bates, Augustin Žídek, Anna Potapenko, Alex Bridgland, Clemens Meyer, Simon A. A. Kohl, Andrew J. Ballard, Andrew Cowie, Bernardino Romera-Paredes, Stanislav Nikolov, Rishub Jain, Jonas Adler, Trevor Back, Stig Petersen, David Reiman, Ellen Clancy, Michal Zielinski, Martin Steinegger, Michalina Pacholska, Tamas Berghammer, Sebastian Bodenstein, David Silver, Oriol Vinyals, Andrew W. Senior, Koray Kavukcuoglu, Pushmeet Kohli, and Demis Hassabis. 2021. Highly Accurate Protein Structure Prediction with AlphaFold. Nature (2021).\n\n[49]\n\nMatthias Jung, Christian Weis, and Norbert Wehn. 2015. DRAMSys: A Flexible DRAM Subsystem Design Space Exploration Framework. IPSJ Transactions on System LSI Design Methodology (2015).\n\n[50]\n\nEunsuk Kang, Ethan Jackson, and Wolfram Schulte. 2010. An Approach for Effective Design Space Exploration. In Monterey Workshop.\n\n[51]\n\nSheng-Chun Kao, Geonhwa Jeong, and Tushar Krishna. 2020. Confuciux: Autonomous hardware resource assignment for dnn accelerators using reinforcement learning. In 2020 53rd Annual IEEE/ACM International Symposium on Microarchitecture (MICRO). IEEE, 622--636.\n\n[52]\n\nSheng-Chun Kao and Tushar Krishna. 2020. Gamma: Automating the hw mapping of dnn models on accelerators via genetic algorithm. In Proceedings of the 39th International Conference on Computer-Aided Design. 1--9.\n\n[53]\n\nTejas S. Karkhanis and James E. Smith. 2004. A First-Order Superscalar Processor Model. In Proceedings of the 31st Annual International Symposium on Computer Architecture (München, Germany) (ISCA '04). IEEE Computer Society, USA, 338.\n\n[54]\n\nSam Kaufman, Phitchaya Phothilimthana, Yanqi Zhou, Charith Mendis, Sudip Roy, Amit Sabne, and Mike Burrows. 2021. A learned performance model for tensor processing units. Proceedings of Machine Learning and Systems 3 (2021), 387--400.\n\n[55]\n\nSamuel Kaufman, Phitchaya Mangpo Phothilimthana, and Mike Burrows. 2019. Learned TPU cost model for XLA tensor programs. In Proc. Workshop ML Syst. NeurIPS. 1--6.\n\n[56]\n\nSrivatsan Krishnan, Zishen Wan, Kshitij Bhardwaj, Paul Whatmough, Aleksandra Faust, Sabrina Neuman, Gu-Yeon Wei, David Brooks, and Vijay Janapa Reddi. 2022. Automatic Domain-Specific SoC Design for Autonomous Unmanned Aerial Vehicles. In 2022 55th IEEE/ACM International Symposium on Microarchitecture (MICRO). IEEE, 300--317.\n\n[57]\n\nAviral Kumar, Amir Yazdanbakhsh, Milad Hashemi, Kevin Swersky, and Sergey Levine. 2022. Data-Driven Offline Optimization For Architecting Hardware Accelerators. In International Conference on Learning Representations (ICLR).\n\n[58]\n\nHyoukjun Kwon, Prasanth Chatarasi, Vivek Sarkar, Tushar Krishna, Michael Pellauer, and Angshuman Parashar. 2020. Maestro: A data-centric approach to understand reuse, performance, and hardware cost of dnn mappings. IEEE micro 40, 3 (2020), 20--29.\n\n[59]\n\nSergey Levine, Aviral Kumar, George Tucker, and Justin Fu. 2020. Offline Reinforcement Learning: Tutorial, Review, and Perspectives on Open Problems. CoRR abs/2005.01643 (2020). arXiv:2005.01643 https://arxiv.org/abs/2005.01643\n\n[60]\n\nChaojian Li, Zhongzhi Yu, Yonggan Fu, Yongan Zhang, Yang Zhao, Haoran You, Qixuan Yu, Yue Wang, Cong Hao, and Yingyan Lin. 2021. {HW}-{NAS}-Bench: Hardware-Aware Neural Architecture Search Benchmark. In International Conference on Learning Representations.\n\n[61]\n\nEric Liang, Richard Liaw, Robert Nishihara, Philipp Moritz, Roy Fox, Ken Goldberg, Joseph Gonzalez, Michael Jordan, and Ion Stoica. 2018. RLlib: Abstractions for distributed reinforcement learning. In International Conference on Machine Learning. PMLR, 3053--3062.\n\n[62]\n\nTimothy P. Lillicrap, Jonathan J. Hunt, Alexander Pritzel, Nicolas Heess, Tom Erez, Yuval Tassa, David Silver, and Daan Wierstra. 2015. Continuous Control with Deep Reinforcement Learning. arXiv preprint arXiv:1509.02971 (2015).\n\n[63]\n\nTing-Ru Lin, Drew Penney, Massoud Pedram, and Lizhong Chen. 2019. Optimizing Routerless Network-on-Chip Designs: An Innovative Learning-based Framework. arXiv preprint arXiv:1905.04423 (2019).\n\n[64]\n\nTing-Ru Lin, Drew Penney, Massoud Pedram, and Lizhong Chen. 2020. A deep reinforcement learning framework for architectural exploration: A routerless NoC case study. In 2020 IEEE International Symposium on High Performance Computer Architecture (HPCA). IEEE, 99--110.\n\n[65]\n\nJason Lowe-Power, Abdul Mutaal Ahmad, Ayaz Akram, Mohammad Alian, Rico Amslinger, Matteo Andreozzi, Adrià Armejach, Nils Asmussen, Srikant Bharadwaj, Gabe Black, Gedare Bloom, Bobby R. Bruce, Daniel Rodrigues Carvalho, Jerónimo Castrillón, Lizhong Chen, Nicolas Derumigny, Stephan Diestelhorst, Wendy Elsasser, Marjan Fariborz, Amin Farmahini Farahani, Pouya Fotouhi, Ryan Gambord, Jayneel Gandhi, Dibakar Gope, Thomas Grass, Bagus Hanindhito, Andreas Hansson, Swapnil Haria, Austin Harris, Timothy Hayes, Adrian Herrera, Matthew Horsnell, Syed Ali Raza Jafri, Radhika Jagtap, Hanhwi Jang, Reiley Jeyapaul, Timothy M. Jones, Matthias Jung, Subash Kannoth, Hamidreza Khaleghzadeh, Yuetsu Kodama, Tushar Krishna, Tommaso Marinelli, Christian Menard, Andrea Mondelli, Tiago Mück, Omar Naji, Krishnendra Nathella, Hoa Nguyen, Nikos Nikoleris, Lena E. Olson, Marc S. Orr, Binh Pham, Pablo Prieto, Trivikram Reddy, Alec Roelke, Mahyar Samani, Andreas Sandberg, Javier Setoain, Boris Shingarov, Matthew D. Sinclair, Tuan Ta, Rahul Thakur, Giacomo Travaglini, Michael Upton, Nilay Vaish, Ilias Vougioukas, Zhengrong Wang, Norbert Wehn, Christian Weis, David A. Wood, Hongil Yoon, and Éder F. Zulian. 2020. The gem5 Simulator: Version 20.0+. arXiv preprint arXiv:2007.03152 (2020).\n\n[66]\n\nWesley J Maddox, Maximilian Balandat, Andrew G Wilson, and Eytan Bakshy. 2021. Bayesian optimization with high-dimensional outputs. Advances in neural information processing systems 34 (2021), 19274--19287.\n\n[67]\n\nPeter S Magnusson, Magnus Christensson, Jesper Eskilson, Daniel Forsgren, Gustav Hallberg, Johan Hogberg, Fredrik Larsson, Andreas Moestedt, and Bengt Werner. 2002. Simics: A full system simulation platform. Computer 35, 2 (2002), 50--58.\n\n[68]\n\nStefano Markidis, Steven Wei Der Chien, Erwin Laure, Ivy Bo Peng, and Jeffrey S Vetter. 2018. Nvidia tensor core programmability, performance & precision. In 2018 IEEE international parallel and distributed processing symposium workshops (IPDPSW). IEEE, 522--531.\n\n[69]\n\nCharith Mendis, Alex Renda, Saman Amarasinghe, and Michael Carbin. 2019. Ithemal: Accurate, portable and fast basic block throughput estimation using deep neural networks. In International Conference on machine learning. PMLR, 4505--4515.\n\n[70]\n\nAzalia Mirhoseini, Anna Goldie, Mustafa Yazgan, Joe Wenjie Jiang, Ebrahim Songhori, Shen Wang, Young-Joon Lee, Eric Johnson, Omkar Pathak, Azade Nazi, et al. 2021. A Graph Placement Methodology for Fast Chip Design. Nature (2021).\n\n[71]\n\nVolodymyr Mnih, Koray Kavukcuoglu, David Silver, Alex Graves, Ioannis Antonoglou, Daan Wierstra, and Martin Riedmiller. 2013. Playing Atari with Deep Reinforcement Learning. arXiv preprint arXiv:1312.5602 (2013).\n\n[72]\n\nJonas Močkus. 1975. On Bayesian Methods for Seeking the Extremum. In Optimization techniques IFIP technical conference. Springer.\n\n[73]\n\nVittoriano Muttillo, Paolo Giammatteo, Giuseppe Fiorilli, and Luigi Pomante. 2020. An OpenMP Parallel Genetic Algorithm for Design Space Exploration of Heterogeneous Multi-processor Embedded Systems. In PARMA-DITAM.\n\n[74]\n\nVu Nguyen. 2019. Bayesian optimization for accelerating hyper-parameter tuning. In 2019 IEEE second international conference on artificial intelligence and knowledge engineering (AIKE). IEEE, 302--305.\n\n[75]\n\nDerek B Noonburg and John P Shen. 1994. Theoretical modeling of superscalar processor performance. In Proceedings of the 27th annual international symposium on Microarchitecture. 52--62.\n\n[76]\n\nMark Oskin, Frederic T. Chong, and Matthew Farrens. 2000. HLS: Combining Statistical and Symbolic Simulation to Guide Microprocessor Designs. In Proceedings of the 27th Annual International Symposium on Computer Architecture (Vancouver, British Columbia, Canada) (ISCA '00). Association for Computing Machinery, New York, NY, USA, 71--82.\n\n[77]\n\nDavid Paper and David Paper. 2021. TensorFlow Datasets. State-of-the-Art Deep Learning Models in TensorFlow: Modern Machine Learning in the Google Colab Ecosystem (2021).\n\n[78]\n\nAngshuman Parashar, Priyanka Raina, Yakun Sophia Shao, Yu-Hsin Chen, Victor A Ying, Anurag Mukkara, Rangharajan Venkatesan, Brucek Khailany, Stephen W Keckler, and Joel Emer. 2019. Timeloop: A systematic approach to dnn accelerator evaluation. In 2019 IEEE international symposium on performance analysis of systems and software (ISPASS). IEEE, 304--315.\n\n[79]\n\nAdam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan, Trevor Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, et al. 2019. Pytorch: An imperative style, high-performance deep learning library. Advances in neural information processing systems 32 (2019).\n\n[80]\n\nAntonin Raffin, Ashley Hill, Maximilian Ernestus, Adam Gleave, Anssi Kanervisto, and Noah Dormann. 2019. Stable Baselines3.\n\n[81]\n\nSabela Ramos, Sertan Girgin, Léonard Hussenot, Damien Vincent, Hanna Yakubovich, Daniel Toyama, Anita Gergely, Piotr Stanczyk, Raphael Marinier, Jeremiah Harmsen, et al. 2021. RLDS: an Ecosystem to Generate, Share and Use Datasets in Reinforcement Learning. arXiv preprint arXiv:2111.02767 (2021).\n\n[82]\n\nRavishankar Rao, Mark H Oskin, and Frederic T Chong. 2002. Hlspower: Hybrid statistical modeling of the superscalar power-performance design space. In High Performance Computing---HiPC 2002: 9th International Conference Bangalore, India, December 18--21, 2002 Proceedings 9. Springer, 620--629.\n\n[83]\n\nJeff Rasley, Samyam Rajbhandari, Olatunji Ruwase, and Yuxiong He. 2020. Deep-speed: System optimizations enable training deep learning models with over 100 billion parameters. In Proceedings of the 26th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining. 3505--3506.\n\n[84]\n\nBrandon Reagen, José Miguel Hernández-Lobato, Robert Adolf, Michael Gelbart, Paul Whatmough, Gu-Yeon Wei, and David Brooks. 2017. A case for efficient accelerator design space exploration via bayesian optimization. In 2017 IEEE/ACM International Symposium on Low Power Electronics and Design (ISLPED). IEEE, 1--6.\n\n[85]\n\nRajarshi Roy, Jonathan Raiman, Neel Kant, Ilyas Elkin, Robert Kirby, Michael Siu, Stuart Oberman, Saad Godil, and Bryan Catanzaro. 2021. Prefixrl: Optimization of parallel prefix circuits using deep reinforcement learning. In 2021 58th ACM/IEEE Design Automation Conference (DAC). IEEE, 853--858.\n\n[86]\n\nAnanda Samajdar, Jan Moritz Joseph, Matthew Denton, and Tushar Krishna. 2021. AIRCHITECT: Learning Custom Architecture Design and Mapping Space. arXiv preprint arXiv:2108.08295 (2021).\n\n[87]\n\nBenjamin Carrion Schafer. 2017. Parallel high-level synthesis design space exploration for behavioral ips of exact latencies. ACM Transactions on Design Automation of Electronic Systems (TODAES) 22, 4 (2017), 1--20.\n\n[88]\n\nJohn Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. 2017. Proximal Policy Optimization Algorithms. CoRR abs/1707.06347 (2017). arXiv:1707.06347 http://arxiv.org/abs/1707.06347\n\n[89]\n\nKiran Seshadri, Berkin Akin, James Laudon, Ravi Narayanaswami, and Amir Yazdanbakhsh. 2022. An Evaluation of Edge TPU Accelerators for Convolutional Neural Networks. In 2022 IEEE International Symposium on Workload Characterization (IISWC). IEEE, 79--91.\n\n[90]\n\nJaime Sevilla, Lennart Heim, Anson Ho, Tamay Besiroglu, Marius Hobbhahn, and Pablo Villalobos. 2022. Compute Trends Across Three Eras of Machine Learning. arXiv preprint arXiv:2202.05924 (2022).\n\n[91]\n\nTimothy Sherwood, Erez Perelman, Greg Hamerly, and Brad Calder. 2002. Automatically Characterizing Large Scale Program Behavior. In Proceedings of the 10th International Conference on Architectural Support for Programming Languages and Operating Systems (San Jose, California) (ASPLOS X). Association for Computing Machinery, New York, NY, USA, 45--57.\n\n[92]\n\nZhan Shi, Chirag Sakhuja, Milad Hashemi, Kevin Swersky, and Calvin Lin. 2020. Learned Hardware/Software Co-Design of Neural Accelerators. arXiv preprint arXiv:2010.02075 (2020).\n\n[93]\n\nGhassan Shobaki, Vahl Scott Gordon, Paul McHugh, Theodore Dubois, and Austin Kerbow. 2022. Register-Pressure-Aware instruction scheduling using ant colony optimization. ACM Transactions on Architecture and Code Optimization (TACO) 19, 2 (2022), 1--23.\n\n[94]\n\nGreg Snider. 2001. Spacewalker: Automated Design space Exploration for Embedded Computer Systems. HP Labs Palo Alto HPL-2001-220 (2001).\n\n[95]\n\nLukas Steiner, Matthias Jung, Felipe S Prado, Kirill Bykov, and Norbert Wehn. 2020. DRAMSys4. 0: a fast and cycle-accurate systemC/TLM-based DRAM simulator. In Embedded Computer Systems: Architectures, Modeling, and Simulation: 20th International Conference, SAMOS 2020, Samos, Greece, July 5--9, 2020, Proceedings 20. Springer, 110--126.\n\n[96]\n\nRichard S Sutton and Andrew G Barto. 2018. Reinforcement Learning: An Introduction. MIT press.\n\n[97]\n\nOndřej Sỳkora, Phitchaya Mangpo Phothilimthana, Charith Mendis, and Amir Yazdanbakhsh. 2022. GRANITE: A Graph Neural Network Model for Basic Block Throughput Estimation. In 2022 IEEE International Symposium on Workload Characterization (IISWC). IEEE, 14--26.\n\n[98]\n\nSynopsys. 2022. Deliver Better, Faster, Cheaper Semiconductors with DSO.ai. https://www.synopsys.com/implementation-and-signoff/ml-ai-design/dso-ai.html\n\n[99]\n\nThierry Tambe, En-Yu Yang, Glenn G Ko, Yuji Chai, Coleman Hooper, Marco Donato, Paul N Whatmough, Alexander M Rush, David Brooks, and Gu-Yeon Wei. 2021. 9.8 A 25mm2 SoC for IoT Devices with 18ms Noise-Robust Speech-to-Text Latency via Bayesian Speech Denoising and Attention-Based Sequence-to-Sequence DNN Speech Recognition in 16nm FinFET. In 2021 IEEE International Solid-State Circuits Conference (ISSCC), Vol. 64. 158--160.\n\n[100]\n\nHanrui Wang, Kuan Wang, Jiacheng Yang, Linxiao Shen, Nan Sun, Hae-Seung Lee, and Song Han. 2020. GCN-RL circuit designer: Transferable transistor sizing with graph neural networks and reinforcement learning. In 2020 57th ACM/IEEE Design Automation Conference (DAC). IEEE, 1--6.\n\n[101]\n\nHanrui Wang, Jiacheng Yang, Hae-Seung Lee, and Song Han. 2018. Learning to design circuits. arXiv preprint arXiv:1812.02734 (2018).\n\n[102]\n\nZirui Wang, Shangzhe Wu, Weidi Xie, Min Chen, and Victor Adrian Prisacariu. 2021. NeRF-: Neural Radiance Fields without Known Camera Parameters. arXiv preprint arXiv:2102.07064 (2021).\n\n[103]\n\nJian Weng, Animesh Jain, Jie Wang, Leyuan Wang, Yida Wang, and Tony Nowatzki. 2021. UNIT: Unifying tensorized instruction compilation. In 2021 IEEE/ACM International Symposium on Code Generation and Optimization (CGO). IEEE, 77--89.\n\n[104]\n\nYannan Nellie Wu, Po-An Tsai, Angshuman Parashar, Vivienne Sze, and Joel S Emer. 2021. Sparseloop: An analytical, energy-focused design space exploration methodology for sparse tensor accelerators. In 2021 IEEE International Symposium on Performance Analysis of Systems and Software (ISPASS). IEEE, 232--234.\n\n[105]\n\nRoland E Wunderlich, Thomas F Wenisch, Babak Falsafi, and James C Hoe. 2003. SMARTS: Accelerating microarchitecture simulation via rigorous statistical sampling. In Proceedings of the 30th annual international symposium on Computer architecture. 84--97.\n\n[106]\n\nFeng Xia, Jiaying Liu, Hansong Nie, Yonghao Fu, Liangtian Wan, and Xiangjie Kong. 2019. Random walks: A review of algorithms and applications. IEEE Transactions on Emerging Topics in Computational Intelligence 4, 2 (2019), 95--107.\n\n[107]\n\nLi Yang and Abdallah Shami. 2020. On hyperparameter optimization of machine learning algorithms: Theory and practice. Neurocomputing 415 (2020), 295--316.\n\n[108]\n\nDan Zhang, Safeen Huda, Ebrahim Songhori, Kartik Prabhu, Quoc Le, Anna Goldie, and Azalia Mirhoseini. 2022. A full-stack search technique for domain optimized deep learning accelerators. In Proceedings of the 27th ACM International Conference on Architectural Support for Programming Languages and Operating Systems. 27--42.\n\n[109]\n\nJun Zhang, Henry Shu-Hung Chung, Alan Wai-Lun Lo, and Tao Huang. 2008. Extended ant colony optimization algorithm for power electronic circuit design. IEEE Transactions on power Electronics 24, 1 (2008), 147--162.\n\n[110]\n\nMatthew M Ziegler, Hung-Yi Liu, George Gristede, Bruce Owens, Ricardo Nigaglioni, and Luca P Carloni. 2016. A synthesis-parameter tuning system for autonomous design-space exploration. In 2016 Design, Automation & Test in Europe Conference & Exhibition (DATE). IEEE, 1148--1151.\n\nCited By\n\nView all\n\nWang JGe MDing BXu QChen SKang YNicePIM: Design Space Exploration for Processing-In-Memory DNN Accelerators With 3-D Stacked-DRAMIEEE Transactions on Computer-Aided Design of Integrated Circuits and Systems10.1109/TCAD.2023.334260543:5(1456-1469)\n\nIndex Terms\n\nArchGym: An Open-Source Gymnasium for Machine Learning Assisted Architecture Design\n\nComputer systems organization\n\nArchitectures\n\nComputing methodologies\n\nMachine learning\n\nLearning paradigms\n\nReinforcement learning\n\nMachine learning algorithms\n\nMachine learning approaches\n\nBio-inspired approaches\n\nRecommendations\n\nMachine Learning in Adversarial Game Using Flight Chess\n\nMINES '11: Proceedings of the 2011 Third International Conference on Multimedia Information Networking and Security\n\nGame playing is a perfect domain of the study of machine learning for its simplicity that allows the researchers to focus on the learning problems themselves and ignore marginal factors. Many learning techniques derived from games have been applied ...\n\nAvalanche RL: A Continual Reinforcement Learning Library\n\nImage Analysis and Processing – ICIAP 2022\n\nAbstract\n\nContinual Reinforcement Learning (CRL) is a challenging setting where an agent learns to interact with an environment that is constantly changing over time (the stream of experiences). In this paper, we describe Avalanche RL, a library for ...\n\nA Survey of Machine Learning for Computer Architecture and Systems\n\nIt has been a long time that computer architecture and systems are optimized for efficient execution of machine learning (ML) models. Now, it is time to reconsider the relationship between ML and systems and let ML transform the way that computer ...\n\nInformation & Contributors\n\nInformation\n\nPublished In\n\n1225 pages\n\nISBN:9798400700958\n\nDOI:10.1145/3579371\n\nChair:\n\nYan Solihin,\n\nGeneral Chair:\n\nMark Heinrich\n\nUniversity of Central Florida\n\nCopyright © 2023 ACM.\n\nPermission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than the author(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. Request permissions from [email protected].\n\nPublisher\n\nAssociation for Computing Machinery\n\nNew York, NY, United States\n\nPublication History\n\nPublished: 17 June 2023\n\nPermissions\n\nRequest permissions for this article.\n\nCheck for updates\n\nBadges\n\nAuthor Tags\n\nmachine learning\n\nmachine learning for computer architecture\n\nmachine learning for system\n\nreinforcement learning\n\nbayesian optimization\n\nopen source\n\nbaselines\n\nreproducibility\n\nQualifiers\n\nResearch-article\n\nFunding Sources\n\nIARPA\n\nConference\n\nISCA '23\n\nAcceptance Rates\n\nOverall Acceptance Rate 543 of 3,203 submissions, 17%\n\nContributors\n\nOther Metrics\n\nBibliometrics & Citations\n\nBibliometrics\n\nArticle Metrics\n\n1\n\nTotal Citations\n\nView Citations\n\n2,731\n\nTotal Downloads\n\nDownloads (Last 12 months)1,966\n\nDownloads (Last 6 weeks)143\n\nOther Metrics\n\nCitations\n\nCited By\n\nView all\n\nWang JGe MDing BXu QChen SKang YNicePIM: Design Space Exploration for Processing-In-Memory DNN Accelerators With 3-D Stacked-DRAMIEEE Transactions on Computer-Aided Design of Integrated Circuits and Systems10.1109/TCAD.2023.334260543:5(1456-1469)\n\nView Options\n\nView options\n\nPDF\n\nView or Download as a PDF file.\n\nPDF\n\neReader\n\nView online with eReader.\n\neReader\n\nGet Access\n\nLogin options\n\nCheck if you have access through your login credentials or your institution to get full access on this article.\n\nSign in\n\nFull Access\n\nMedia\n\nFigures\n\nOther\n\nTables\n\nShare\n\nShare\n\nShare this Publication link\n\nCopied!\n\nCopying failed.\n\nShare on social media\n\nAffiliations\n\nSrivatsan Krishnan\n\nHarvard University, Cambridge, Massachusetts, USA\n\nAmir Yazdanbakhsh\n\nGoogle Research, Brain Team, Mountain View, USA\n\nShvetank Prakash\n\nHarvard University, Cambridge, USA\n\nJason Jabbour\n\nHarvard University, Cambridge, USA\n\nIkechukwu Uchendu\n\nHarvard University, Cambridge, USA\n\nSusobhan Ghosh\n\nHarvard University, Cambridge, USA\n\nBehzad Boroujerdian\n\nUniversity of Texas/Harvard University, Austin, USA\n\nDaniel Richins\n\nThe University of Texas, Austin, USA\n\nDevashree Tripathy\n\nIIT Bhubaneswar/Harvard University, Bhubaneswar, USA\n\nAleksandra Faust\n\nGoogle Research, Mountain View, USA\n\nVijay Janapa Reddi\n\nHarvard University, Cambridge, USA\n\nRequest permissions Authors Info & Affiliations"
    }
}
{
    "id": "dbpedia_2556_2",
    "rank": 84,
    "data": {
        "url": "https://patents.google.com/patent/WO2012122293A1/de",
        "read_more_link": "",
        "language": "en",
        "title": "WO2012122293A1 - Augmented reality mission generators - Google Patents",
        "top_image": "",
        "meta_img": "",
        "images": [],
        "movies": [],
        "keywords": [],
        "meta_keywords": [
            ""
        ],
        "tags": null,
        "authors": [],
        "publish_date": "2011-03-07T00:00:00",
        "summary": "",
        "meta_description": "",
        "meta_lang": "en",
        "meta_favicon": "",
        "meta_site_name": "",
        "canonical_link": "https://patents.google.com/patent/WO2012122293A1/de",
        "text": "Augmented reality mission generators Download PDF\n\nInfo\n\nPublication number\n\nWO2012122293A1\n\nWO2012122293A1 PCT/US2012/028109 US2012028109W WO2012122293A1 WO 2012122293 A1 WO2012122293 A1 WO 2012122293A1 US 2012028109 W US2012028109 W US 2012028109W WO 2012122293 A1 WO2012122293 A1 WO 2012122293A1\n\nAuthority\n\nWO\n\nWIPO (PCT)\n\nPrior art keywords\n\nmission\n\ngenerator\n\ndata\n\ntemplate\n\nmobile device\n\nPrior art date\n\n2011-03-07\n\nApplication number\n\nPCT/US2012/028109\n\nOther languages\n\nEnglish (en)\n\nFrench (fr)\n\nInventor\n\nBrian Elan Lee\n\nMichael Sean STEWART\n\nJames Stewartson\n\nOriginal Assignee\n\nFourth Wall Studios, Inc.\n\nPriority date (The priority date is an assumption and is not a legal conclusion. Google has not performed a legal analysis and makes no representation as to the accuracy of the date listed.)\n\n2011-03-07\n\nFiling date\n\n2012-03-07\n\nPublication date\n\n2012-09-13\n\n2012-03-07 Application filed by Fourth Wall Studios, Inc. filed Critical Fourth Wall Studios, Inc.\n\n2012-09-13 Publication of WO2012122293A1 publication Critical patent/WO2012122293A1/en\n\nLinks\n\nEspacenet\n\nGlobal Dossier\n\nPatentScope\n\nDiscuss\n\nClassifications\n\nA—HUMAN NECESSITIES\n\nA63—SPORTS; GAMES; AMUSEMENTS\n\nA63F—CARD, BOARD, OR ROULETTE GAMES; INDOOR GAMES USING SMALL MOVING PLAYING BODIES; VIDEO GAMES; GAMES NOT OTHERWISE PROVIDED FOR\n\nA63F13/00—Video games, i.e. games using an electronically generated display having two or more dimensions\n\nA63F13/70—Game security or game management aspects\n\nA63F13/79—Game security or game management aspects involving player-related data, e.g. identities, accounts, preferences or play histories\n\nA—HUMAN NECESSITIES\n\nA63—SPORTS; GAMES; AMUSEMENTS\n\nA63F—CARD, BOARD, OR ROULETTE GAMES; INDOOR GAMES USING SMALL MOVING PLAYING BODIES; VIDEO GAMES; GAMES NOT OTHERWISE PROVIDED FOR\n\nA63F13/00—Video games, i.e. games using an electronically generated display having two or more dimensions\n\nA63F13/60—Generating or modifying game content before or while executing the game program, e.g. authoring tools specially adapted for game development or game-integrated level editor\n\nA63F13/65—Generating or modifying game content before or while executing the game program, e.g. authoring tools specially adapted for game development or game-integrated level editor automatically by game devices or servers from real world data, e.g. measurement in live racing competition\n\nA—HUMAN NECESSITIES\n\nA63—SPORTS; GAMES; AMUSEMENTS\n\nA63F—CARD, BOARD, OR ROULETTE GAMES; INDOOR GAMES USING SMALL MOVING PLAYING BODIES; VIDEO GAMES; GAMES NOT OTHERWISE PROVIDED FOR\n\nA63F13/00—Video games, i.e. games using an electronically generated display having two or more dimensions\n\nA63F13/20—Input arrangements for video game devices\n\nA63F13/21—Input arrangements for video game devices characterised by their sensors, purposes or types\n\nA63F13/216—Input arrangements for video game devices characterised by their sensors, purposes or types using geographical information, e.g. location of the game device or player using GPS\n\nA—HUMAN NECESSITIES\n\nA63—SPORTS; GAMES; AMUSEMENTS\n\nA63F—CARD, BOARD, OR ROULETTE GAMES; INDOOR GAMES USING SMALL MOVING PLAYING BODIES; VIDEO GAMES; GAMES NOT OTHERWISE PROVIDED FOR\n\nA63F13/00—Video games, i.e. games using an electronically generated display having two or more dimensions\n\nA63F13/20—Input arrangements for video game devices\n\nA63F13/21—Input arrangements for video game devices characterised by their sensors, purposes or types\n\nA63F13/217—Input arrangements for video game devices characterised by their sensors, purposes or types using environment-related information, i.e. information generated otherwise than by the player, e.g. ambient temperature or humidity\n\nA—HUMAN NECESSITIES\n\nA63—SPORTS; GAMES; AMUSEMENTS\n\nA63F—CARD, BOARD, OR ROULETTE GAMES; INDOOR GAMES USING SMALL MOVING PLAYING BODIES; VIDEO GAMES; GAMES NOT OTHERWISE PROVIDED FOR\n\nA63F13/00—Video games, i.e. games using an electronically generated display having two or more dimensions\n\nA63F13/20—Input arrangements for video game devices\n\nA63F13/21—Input arrangements for video game devices characterised by their sensors, purposes or types\n\nA63F13/213—Input arrangements for video game devices characterised by their sensors, purposes or types comprising photodetecting means, e.g. cameras, photodiodes or infrared cells\n\nA—HUMAN NECESSITIES\n\nA63—SPORTS; GAMES; AMUSEMENTS\n\nA63F—CARD, BOARD, OR ROULETTE GAMES; INDOOR GAMES USING SMALL MOVING PLAYING BODIES; VIDEO GAMES; GAMES NOT OTHERWISE PROVIDED FOR\n\nA63F13/00—Video games, i.e. games using an electronically generated display having two or more dimensions\n\nA63F13/30—Interconnection arrangements between game servers and game devices; Interconnection arrangements between game devices; Interconnection arrangements between game servers\n\nA63F13/33—Interconnection arrangements between game servers and game devices; Interconnection arrangements between game devices; Interconnection arrangements between game servers using wide area network [WAN] connections\n\nA63F13/332—Interconnection arrangements between game servers and game devices; Interconnection arrangements between game devices; Interconnection arrangements between game servers using wide area network [WAN] connections using wireless networks, e.g. cellular phone networks\n\nA—HUMAN NECESSITIES\n\nA63—SPORTS; GAMES; AMUSEMENTS\n\nA63F—CARD, BOARD, OR ROULETTE GAMES; INDOOR GAMES USING SMALL MOVING PLAYING BODIES; VIDEO GAMES; GAMES NOT OTHERWISE PROVIDED FOR\n\nA63F13/00—Video games, i.e. games using an electronically generated display having two or more dimensions\n\nA63F13/80—Special adaptations for executing a specific game genre or game mode\n\nA63F13/822—Strategy games; Role-playing games\n\nA—HUMAN NECESSITIES\n\nA63—SPORTS; GAMES; AMUSEMENTS\n\nA63F—CARD, BOARD, OR ROULETTE GAMES; INDOOR GAMES USING SMALL MOVING PLAYING BODIES; VIDEO GAMES; GAMES NOT OTHERWISE PROVIDED FOR\n\nA63F13/00—Video games, i.e. games using an electronically generated display having two or more dimensions\n\nA63F13/90—Constructional details or arrangements of video game devices not provided for in groups A63F13/20 or A63F13/25, e.g. housing, wiring, connections or cabinets\n\nA63F13/92—Video game devices specially adapted to be hand-held while playing\n\nA—HUMAN NECESSITIES\n\nA63—SPORTS; GAMES; AMUSEMENTS\n\nA63F—CARD, BOARD, OR ROULETTE GAMES; INDOOR GAMES USING SMALL MOVING PLAYING BODIES; VIDEO GAMES; GAMES NOT OTHERWISE PROVIDED FOR\n\nA63F2300/00—Features of games using an electronically generated display having two or more dimensions, e.g. on a television screen, showing representations related to the game\n\nA63F2300/20—Features of games using an electronically generated display having two or more dimensions, e.g. on a television screen, showing representations related to the game characterised by details of the game platform\n\nA63F2300/209—Features of games using an electronically generated display having two or more dimensions, e.g. on a television screen, showing representations related to the game characterised by details of the game platform characterized by low level software layer, relating to hardware management, e.g. Operating System, Application Programming Interface\n\nA—HUMAN NECESSITIES\n\nA63—SPORTS; GAMES; AMUSEMENTS\n\nA63F—CARD, BOARD, OR ROULETTE GAMES; INDOOR GAMES USING SMALL MOVING PLAYING BODIES; VIDEO GAMES; GAMES NOT OTHERWISE PROVIDED FOR\n\nA63F2300/00—Features of games using an electronically generated display having two or more dimensions, e.g. on a television screen, showing representations related to the game\n\nA63F2300/40—Features of games using an electronically generated display having two or more dimensions, e.g. on a television screen, showing representations related to the game characterised by details of platform network\n\nA63F2300/406—Transmission via wireless network, e.g. pager or GSM\n\nA—HUMAN NECESSITIES\n\nA63—SPORTS; GAMES; AMUSEMENTS\n\nA63F—CARD, BOARD, OR ROULETTE GAMES; INDOOR GAMES USING SMALL MOVING PLAYING BODIES; VIDEO GAMES; GAMES NOT OTHERWISE PROVIDED FOR\n\nA63F2300/00—Features of games using an electronically generated display having two or more dimensions, e.g. on a television screen, showing representations related to the game\n\nA63F2300/60—Methods for processing data by generating or executing the game program\n\nA63F2300/69—Involving elements of the real world in the game world, e.g. measurement in live races, real video\n\nA—HUMAN NECESSITIES\n\nA63—SPORTS; GAMES; AMUSEMENTS\n\nA63F—CARD, BOARD, OR ROULETTE GAMES; INDOOR GAMES USING SMALL MOVING PLAYING BODIES; VIDEO GAMES; GAMES NOT OTHERWISE PROVIDED FOR\n\nA63F2300/00—Features of games using an electronically generated display having two or more dimensions, e.g. on a television screen, showing representations related to the game\n\nA63F2300/80—Features of games using an electronically generated display having two or more dimensions, e.g. on a television screen, showing representations related to the game specially adapted for executing a specific type of game\n\nA63F2300/807—Role playing or strategy games\n\nA—HUMAN NECESSITIES\n\nA63—SPORTS; GAMES; AMUSEMENTS\n\nA63F—CARD, BOARD, OR ROULETTE GAMES; INDOOR GAMES USING SMALL MOVING PLAYING BODIES; VIDEO GAMES; GAMES NOT OTHERWISE PROVIDED FOR\n\nA63F2300/00—Features of games using an electronically generated display having two or more dimensions, e.g. on a television screen, showing representations related to the game\n\nA63F2300/80—Features of games using an electronically generated display having two or more dimensions, e.g. on a television screen, showing representations related to the game specially adapted for executing a specific type of game\n\nA63F2300/8082—Virtual reality\n\nDefinitions\n\nU.S. pat. publ. no. 2006/0223635 to Rosenberg takes simulated gaming a step further by combing simulated gaming objects and events with the real-world. A display can present simulated objects on a display.\n\nRosenberg fails to appreciate the dynamic nature of the real -world and that each game player can have their game play experience.\n\nU.S. pat. publ. no. 2007/0281765 to Mullen discusses systems and methods for location based games. Although Mullen contemplates using the physical location of the user to correspond to a virtual location of a virtual character, Mullen fails to contemplate the use of ambient environmental information apart from location information when generating the game.\n\nU.S. pat. publ. no. 2011/0081973 to Hall discusses a different location based game, but also fails to contemplate the use of ambient environmental information apart from location information when generating the game.\n\nan augmented reality platform can be constructed to generate augmented reality missions for users.\n\na mission can be generated, possibly from a template, based on a user's environment or data collected about the user's environment.\n\nMission objects can have their attributes populated based on the environment data. For example, all red cars local to the user can become mission objects. As the missions are based on a user's environment, two users could experience quite different missions even though the missions are generated from the same template.\n\nthe inventive subject matter provides apparatus, systems and methods in which one can provide augmented or mixed reality experiences to users.\n\nOne of the many aspects of the inventive subject matter includes an augmented reality (AR) gaming system capable of generating one or more AR missions.\n\nAn AR mission can be presented to a user via a mobile device (e.g., portable computer, media player, cell phone, vehicle, game system, sensor, etc.) where the user can interact with the mission via the mobile device, or other interactive devices.\n\na mobile device e.g., portable computer, media player, cell phone, vehicle, game system, sensor, etc.\n\nAR missions can be generated via an AR mission generator that includes a mission database storing one or more AR mission templates and an AR mission engine coupled with the database.\n\nthe AR mission engine can obtain environmental data apart from location information (e.g., GPS coordinates) from one or more remote sensing devices, including the user's mobile device, where the environmental data comprises a digital representation of a scene.\n\nthe AR mission engine can combine information derived from the digital representation of the scene with an AR mission template to construct a quest (i.e., an instantiated mission) for the user.\n\nthe AR mission engine can select a mission template from the database based on the environmental data and the location of the user's mobile device, and then populate the mission template with AR objects (e.g., objectives, rewards, goals, etc.) to flush out the mission.\n\nAR objects e.g., objectives, rewards, goals, etc.\n\nthe attributes of the AR objects can also be populated based on the environmental data.\n\nFIG. 1 is a schematic of an augmented reality system having an augmented reality mission generator. Detailed Description\n\ncomputing devices comprise a processor configured to execute software instructions stored on a tangible, non-transitory computer readable storage medium (e.g., hard drive, solid state drive, RAM, flash, ROM, etc.).\n\nthe software instructions preferably configure the computing device to provide the roles, responsibilities, or other functionality as discussed below with respect to the disclosed apparatus.\n\nthe various servers, systems, databases, or interfaces exchange data using standardized protocols or algorithms, possibly based on SMS, MMS, HTTP, HTTPS, AES, public -private key exchanges, web service APIs, known financial transaction protocols, or other electronic information exchanging methods.\n\nData exchanges preferably are conducted over a packet-switched network, the Internet, LAN, WAN, VPN, PAN, or other type of packet switched network.\n\nthe disclosed techniques provide many advantageous technical effects including providing an augmented reality infrastructure capable of configuring one or more mobile devices to present a mixed reality interactive environment to users.\n\nthe mixed reality environment, and accompany missions can be constructed from external data obtained from sensors that are external to the infrastructure.\n\na mission can be populated with information obtained from satellites, GoogleÂ® StreetViewTM, third party mapping information, security cameras, kiosks, televisions or television stations, set top boxes, weather stations, radios or radio stations, web sites, cellular towers, or other data sources.\n\nCoupled to is intended to include both direct coupling (in which two elements that are coupled to each other contact each other) and indirect coupling (in which at least one additional element is located between the two elements). Therefore, the terms âcoupled toâ and âcoupled withâ are used synonymously.\n\ninventive subject matter provides many example embodiments of the inventive subject matter. Although each embodiment represents a single combination of inventive elements, the inventive subject matter is considered to include all possible combinations of the disclosed elements. Thus if one embodiment comprises elements A, B, and C, and a second embodiment comprises elements B and D, then the inventive subject matter is also considered to include other remaining combinations of A, B, C, or D, even if not explicitly disclosed.\n\nFIG. 1 presents an overview of one embodiment of an augmented or mixed reality environment 100 where a user can obtain one or more missions from an AR mission generator 1 10.\n\neach user can utilize a mobile device 102 to obtain sensor data from one or more sensor(s) 104 related to a scene 120 or the user's environment that is separate from a user's location information.\n\na user's mobile device 102 can exchange the collected environmental data or a digital representation of the scene 120 with the AR mission generator 1 10.\n\nData exchanges preferably are conducted over a network 130, which could include, for example, cell networks, mesh networks, Internet, LANs, WANs, VPNs, PANs, or other types of networks or combinations thereof.\n\nthe AR mission generator 1 10 can generate one or more missions for the user, at least in part based on the obtained environment data.\n\nthe mobile device 102 could also transmit location information such as GPS coordinates and/or cellular triangulation information to the AR mission generator 110.\n\nthe mobile device 102 is presented as a smart phone, which represents one of many different types of devices that can integrate into the overall AR environment 100.\n\nMobile devices can include, for example, smart phones and other wireless telephones, laptops, netbooks, tablet PCs, and other mobile computers, vehicles, sensors (e.g., a camera), media players, personal digital assistants, MP3 or other media players, watches, and gaming platforms.\n\nOther types of devices can include electronic picture frames, desktop computers, appliances (e.g., STB, kitchen appliances, etc), kiosks, non-mobile sensors, media players, game consoles, televisions, or other types of devices.\n\nPreferred devices have a\n\na communication link and offer a presentation system (e.g., display, speakers, vibrators, etc.) for presenting AR data to the user.\n\na presentation system e.g., display, speakers, vibrators, etc.\n\nEnvironmental data or a digital representation of the scene 120 can include data from multiple sources or sensors.\n\na sensor 122 e.g., a camera\n\nContemplated sensors can include, for example, microphones, magnetometers, accelerometers, biosensors, still and video cameras, weather sensors, optical sensors, or other types of sensors.\n\nthe types of data used to form a digital representation of the scene can cover a wide range of modalities including image data, audio data, haptic data, or other modalities.\n\nadditional data can include weather data, location data, orientation data, movement data, biometrics data, or other types of data.\n\nthe AR mission generator 110 can include one or more modules or components configured to support the roles or responsibilities of the AR mission generator 110.\n\nthe AR mission generator 1 10 can include an AR mission template database 1 12 and an AR mission engine 1 14.\n\nthe AR mission template database 1 12 and AR mission engine 114 are shown as local to the AR mission generator 110, it is contemplated that one or both of the AR mission template database 112 and AR mission engine 1 14 can be separate from, and located locally or remotely with respect to, the AR mission generator 1 10.\n\nthe AR mission template database 1 12 can store a plurality of AR mission template objects where each mission template object comprises attributes or metadata describing characteristics of a mission.\n\nthe mission template objects can be stored as an XML file or other serialized format.\n\na mission template object can include a wide spectrum of information including, for example, name / ID of mission, a type of mission (e.g., dynamic, chain, etc.), goals, supporting objects, rewards, narratives, digital assets (e.g., video, audio, etc), mission requirements (e.g., required weapons, achievements, user level, number of players, etc.), location requirements (e.g., indoors or outdoors), conditions, programmatic instructions, links to other missions, or other information that can be used to instantiate a mission.\n\na type of mission e.g., dynamic, chain, etc.\n\ngoals e.g., supporting objects, rewards, narratives, digital assets (e.g., video, audio, etc)\n\nmission requirements e.g., required weapons, achievements, user level, number of players, etc.\n\nlocation requirements e.g., indoors or outdoors\n\nconditions programmatic instructions, links to other missions, or other information that can be used to instantiate a mission.\n\nthe AR mission generator 110 is illustrated as being remote relative to the scene 120 or mobile device 102. However, it is specifically contemplated that some or all of the features of the mission generator 1 10, AR mission engine 114 and/or AR mission template database 1 12, for example, can be integrated into the mobile device 102. In such\n\ninformation can be exchanged through an application program interface (API) or other suitable interface.\n\nAPI application program interface\n\nthe AR mission engine 114 or other components can comprise a distal computing server, a distributed computing platform, or even an AR computing platform.\n\nthe AR mission engine 1 14 is preferably configured to obtain environmental data from the user's mobile device 102, about the scene 120 proximate to the mobile device 102. Based on the environmental data, the AR mission engine 114 can determine the AR mission engine 114 .\n\nScene characteristics can include user identification and capabilities of the mobile device 102 including, for example, available sensors 104, screen size, processor speed, available memory, presence of a camera or other imaging sensor. Scene characteristics can also include weather conditions, visual images, location information, orientation, captured audio, presence and type of real-world objects, or other types of characteristics.\n\nthe AR mission engine 1 14 can compare the characteristics to the requirements, attributes, or conditions associated with the stored AR mission template objects to select a mission template. Once selected or otherwise obtained, the AR mission engine 114 can instantiate a mission for the user from the selected mission template object. It is contemplated that the AR mission generator 1 10 can configured the mobile device 102 to present the generated mission.\n\na mission template object includes a defined grammar having verbs that define user actions with respect to one or more AR objects associated with a mission.\n\nan AR mission template object might have several verbs that define a mission with respect to the user's actions.\n\nContemplated verbs include, for example, read, view, deliver, fire (e.g., a weapon, etc.), upgrade, collect, converse, travel, or other actions.\n\nthe AR objects associated with a mission template can also be stored as a template, or rather as AR object templates.\n\nthe selected AR mission template object can be populated based on the environmental data.\n\na user could be in a shopping mall and log in to the AR mission generator 110 via their mobile phone to obtain a mission.\n\nthe AR mission engine 1 14 recognizes from the user's location (e.g., based on GPS coordinates) that the user is in a mall, and selects a mission that requires the user to collect objects.\n\nthe AR mission engine 1 14 instantiates AR objects as mannequins, and the mission requires that the user travels around the mall photographing mannequins (e.g., collecting the AR objects) to complete the mission.\n\nthe mobile device 102 could be configured to identify the mannequins, or other object of interest, by its associated features such as by using image recognition software.\n\nPopulating attributes or features of a mission or associated AR objects can also be achieved through object recognition.\n\nthe AR mission engine 1 14, perhaps in the mobile device 102 can recognize real-world objects in the scene 120 and use the objects' attributes to populate attributes of the one or more AR objects 124.\n\nthe attributes can be simply observed or looked-up from a database based on object recognition algorithms (e.g., SIFT, vSLAM, Viper, etc.).\n\na user may capture a picture of a scene having a plurality of trees.\n\nthe trees can be recognized by the AR mission engine, and AR objects can be generated based upon the trees' attributes (e.g., size, leave color, distance from mobile device, etc.).\n\nthe AR objects associated with a mission can range across a full spectrum of objects from completely real-world objects through completely virtual objects.\n\nExemplary AR objects can include, for example, a mission objective, a reward, an award point, a currency, a relationship, a virtual object, a real-world object, a promotion, a coupon, or other types of objects.\n\nthe AR objects can be integrated into the real-world via mobile device 102. For example, as the user pans and tilts their mobile device 102, the AR objects associated with the mission could be superimposed (overlaid) on the captured scene 120 while also maintaining their proper location and orientation with respect to real-world objects within the scene 120. Superimposing images of AR objects on a real-world image can be accomplished by many techniques.\n\na mission can be customized for a specific user based on the user's specific environment. Still, the missions can be efficiently based on just a few types of mission templates.\n\nOne especially interesting type of mission is a dynamic mission that can be fully customizable for the user. Dynamic missions can be a single one-off mission constructed in real-time if desired based on the obtained environmental data. While completion of a dynamic mission may not advance a story, users may obtain rewards for completing the mission including, for example, points, levels, currency, weapons, and experience. Examples of dynamic missions include shooting ten boars, collective five coins, going on a night patrol, finding a treasure, and so forth.\n\nAnother interesting type of mission is a chain mission that can be linked with preceding or succeeding missions to form a story arch. Chain mission can be constructed with more thought to create a greater level of immersion for the user.\n\nmissions have been presented as a single player platform. However, one should appreciate that missions can also comprise multi-player missions requiring two or more users. When multiple users are involved, new types of interactions can occur. Some multi-player missions might require cooperative objectives, while other multi-player missions might comprise counter objectives for the players where the players oppose or compete against each other. Because of the AR nature of the missions, it is contemplated that players could be in a variety of disparate locations while interacting with one another. An exemplary mission having counter objectives could be to infiltrate an enemy's base or to defend a fort.\n\nmissions are associated with game play. Still, missions can bridge across many markets beyond game play. Other types of missions can be constructed as an exercise program, an advertising campaign, or even following an alternative navigation route home. By constructing various types of missions for a user, the user can be enticed to discover new businesses or opportunities, possibly commercial opportunities.\n\nContemplated AR systems can include an analysis engine that correlates player attributes against mission objectives. Collecting and tracking of such information can be advantageous to businesses when targeting promotions or missions to players or other individuals.\n\nambient environmental data separate from the mobile device's location can be received.\n\nAn AR mission generator can select an AR mission template from a mission database coupled to the AR mission generator. It is contemplated that the AR mission template can be selected based at least in part upon the ambient environmental data.\n\na mission can be generated using the AR mission generator and the selected AR mission template, where the mission is based on at least a portion of the ambient\n\na mobile device can be configured via the AR mission generator to present the generated mission to a user.\n\nLandscapes\n\nEngineering & Computer Science (AREA)\n\nMultimedia (AREA)\n\nHuman Computer Interaction (AREA)\n\nEnvironmental & Geological Engineering (AREA)\n\nBiodiversity & Conservation Biology (AREA)\n\nGeneral Business, Economics & Management (AREA)\n\nComputer Security & Cryptography (AREA)\n\nLife Sciences & Earth Sciences (AREA)\n\nBusiness, Economics & Management (AREA)\n\nEcology (AREA)\n\nEnvironmental Sciences (AREA)\n\nRadar, Positioning & Navigation (AREA)\n\nComputer Networks & Wireless Communication (AREA)\n\nUser Interface Of Digital Computer (AREA)\n\nProcessing Or Creating Images (AREA)\n\nPCT/US2012/028109 2011-03-07 2012-03-07 Augmented reality mission generators WO2012122293A1 (en)\n\nApplications Claiming Priority (2)\n\nApplication Number Priority Date Filing Date Title US201161450052P 2011-03-07 2011-03-07 US61/450,052 2011-03-07\n\nPublications (1)\n\nPublication Number Publication Date WO2012122293A1 true WO2012122293A1 (en) 2012-09-13\n\nFamily\n\nID=45976510\n\nFamily Applications (1)\n\nApplication Number Title Priority Date Filing Date PCT/US2012/028109 WO2012122293A1 (en) 2011-03-07 2012-03-07 Augmented reality mission generators\n\nCountry Status (2)\n\nCountry Link US (1) US20120231887A1 (de) WO (1) WO2012122293A1 (de)\n\nCited By (1)\n\n* Cited by examiner, â Cited by third party Publication number Priority date Publication date Assignee Title US9104236B2 (en) 2013-08-22 2015-08-11 International Business Machines Corporation Modifying information presented by an augmented reality device\n\nFamilies Citing this family (43)\n\n* Cited by examiner, â Cited by third party Publication number Priority date Publication date Assignee Title US7565008B2 (en) 2000-11-06 2009-07-21 Evryx Technologies, Inc. Data capture and identification system and process US8224078B2 (en) 2000-11-06 2012-07-17 Nant Holdings Ip, Llc Image capture and identification system and process US7680324B2 (en) 2000-11-06 2010-03-16 Evryx Technologies, Inc. Use of image-derived information as search criteria for internet and other search engines US7899243B2 (en) 2000-11-06 2011-03-01 Evryx Technologies, Inc. Image capture and identification system and process US9310892B2 (en) 2000-11-06 2016-04-12 Nant Holdings Ip, Llc Object information derived from object images AU2008282886B2 (en) 2007-07-27 2012-11-29 Pls Iv, Llc Content publishing systems and methods EP2193825B1 (de) * 2008-12-03 2017-03-22 Alcatel Lucent Mobile Vorrichtung fÃ¼r Anwendungen mit erweiteter RealitÃ¤t US9573064B2 (en) * 2010-06-24 2017-02-21 Microsoft Technology Licensing, Llc Virtual and location-based multiplayer gaming WO2013078345A1 (en) * 2011-11-21 2013-05-30 Nant Holdings Ip, Llc Subscription bill service, systems and methods US20130281202A1 (en) * 2012-04-18 2013-10-24 Zynga, Inc. Method and apparatus for providing game elements in a social gaming environment US9174128B2 (en) * 2012-04-26 2015-11-03 Zynga Inc. Dynamic quests in game US9539498B1 (en) 2012-07-31 2017-01-10 Niantic, Inc. Mapping real world actions to a virtual world associated with a location-based game US9604131B1 (en) 2012-07-31 2017-03-28 Niantic, Inc. Systems and methods for verifying player proximity within a location-based game US9226106B1 (en) 2012-07-31 2015-12-29 Niantic, Inc. Systems and methods for filtering communication within a location-based game US9669293B1 (en) 2012-07-31 2017-06-06 Niantic, Inc. Game data validation US9669296B1 (en) 2012-07-31 2017-06-06 Niantic, Inc. Linking real world activities with a parallel reality game US9128789B1 (en) 2012-07-31 2015-09-08 Google Inc. Executing cross-cutting concerns for client-server remote procedure calls US9782668B1 (en) 2012-07-31 2017-10-10 Niantic, Inc. Placement of virtual elements in a virtual world associated with a location-based parallel reality game US9621635B1 (en) 2012-07-31 2017-04-11 Niantic, Inc. Using side channels in remote procedure calls to return information in an interactive environment US9338622B2 (en) * 2012-10-04 2016-05-10 Bernt Erik Bjontegard Contextually intelligent communication systems and processes US8968099B1 (en) 2012-11-01 2015-03-03 Google Inc. System and method for transporting virtual objects in a parallel reality game US20140128161A1 (en) * 2012-11-06 2014-05-08 Stephen Latta Cross-platform augmented reality experience US20140201205A1 (en) * 2013-01-14 2014-07-17 Disney Enterprises, Inc. Customized Content from User Data EP2994830A4 (de) * 2013-05-08 2017-04-19 Square Enix Holdings Co., Ltd. Informationsverarbeitungsvorrichtung, steuerungsverfahren und programm US10463953B1 (en) 2013-07-22 2019-11-05 Niantic, Inc. Detecting and preventing cheating in a location-based game US9545565B1 (en) 2013-10-31 2017-01-17 Niantic, Inc. Regulating and scoring player interactions within a virtual world associated with a location-based parallel reality game WO2015167549A1 (en) * 2014-04-30 2015-11-05 Longsand Limited An augmented gaming platform US9861894B2 (en) * 2015-09-29 2018-01-09 International Business Machines Corporation Dynamic personalized location and contact-aware games US10115234B2 (en) * 2016-03-21 2018-10-30 Accenture Global Solutions Limited Multiplatform based experience generation CN109564351A (zh) * 2016-06-06 2019-04-02 åçº³å å¼å¨±ä¹å ¬å¸ æ··åç°å®ç³»ç» US10384130B2 (en) * 2016-08-05 2019-08-20 AR Sports LLC Fantasy sport platform with augmented reality player acquisition WO2018160081A1 (en) * 2017-03-02 2018-09-07 Motorola Solutions, Inc. Method and apparatus for gathering visual data using an augmented-reality application GB2573477A (en) * 2017-03-02 2019-11-06 Motorola Solutions Inc Method and apparatus for gathering visual data using an augmented-reality application US11638869B2 (en) * 2017-04-04 2023-05-02 Sony Corporation Information processing device and information processing method US10717005B2 (en) * 2017-07-22 2020-07-21 Niantic, Inc. Validating a player's real-world location using activity within a parallel reality game US10872533B1 (en) 2017-09-29 2020-12-22 DroneUp, LLC Multiplexed communications of telemetry data, video stream data and voice data among piloted aerial drones via a common software application CN108245881A (zh) * 2017-12-29 2018-07-06 æ­¦æ±å¸é©¬éæ¬§ç½ç»æéå ¬å¸ åºäºarçä¸ç»´æ¼æ¿æ¨¡åæ­å»ºç³»ç» US20210263484A1 (en) * 2018-07-05 2021-08-26 Themissionzone, Inc. Systems and methods for manipulating the shape and behavior of a physical space US11410488B2 (en) * 2019-05-03 2022-08-09 Igt Augmented reality virtual object collection based on symbol combinations US10873951B1 (en) 2019-06-04 2020-12-22 Motorola Solutions, Inc. Method and device to minimize interference in a converged LMR/LTE communication device US11210857B2 (en) 2019-09-26 2021-12-28 The Toronto-Dominion Bank Systems and methods for providing an augmented-reality virtual treasure hunt US11574423B2 (en) 2021-01-29 2023-02-07 Boomanity Corp. A Delaware Corporation Augmented reality (AR) object communication and interaction system and method US11941558B2 (en) * 2021-04-08 2024-03-26 Raytheon Company Intelligence preparation of the battlefield (IPB) collaborative time machine with real-time options\n\nCitations (13)\n\n* Cited by examiner, â Cited by third party Publication number Priority date Publication date Assignee Title GB2385238A (en) * 2002-02-07 2003-08-13 Hewlett Packard Co Using virtual environments in wireless communication systems US20040041788A1 (en) 2002-08-28 2004-03-04 Lockheed Martin Corporation Interactive virtual portal US6771294B1 (en) 1999-12-29 2004-08-03 Petri Pulli User interface US6951515B2 (en) 1999-06-11 2005-10-04 Canon Kabushiki Kaisha Game apparatus for mixed reality space, image processing method thereof, and program storage medium US6972734B1 (en) 1999-06-11 2005-12-06 Canon Kabushiki Kaisha Mixed reality apparatus and mixed reality presentation method US20060223635A1 (en) 2005-04-04 2006-10-05 Outland Research method and apparatus for an on-screen/off-screen first person gaming experience US20070104348A1 (en) 2000-11-06 2007-05-10 Evryx Technologies, Inc. Interactivity via mobile image recognition US20070281765A1 (en) 2003-09-02 2007-12-06 Mullen Jeffrey D Systems and methods for location based games and employment of the same on locaton enabled devices WO2009016186A2 (de) * 2007-07-31 2009-02-05 Jochen Hummel Verfahren zur computerunterstÃ¼tzten erzeugung einer interaktiven dreidimensionalen virtuellen realitÃ¤t US7564469B2 (en) 2005-08-29 2009-07-21 Evryx Technologies, Inc. Interactivity with a mixed reality EP2098271A2 (de) * 2008-02-21 2009-09-09 Palo Alto Research Center Incorporated Ortsbewusste Spieleplattform mit gemischter RealitÃ¤t US20110081973A1 (en) 2005-11-30 2011-04-07 Hall Robert J Geogame for mobile device US20110319148A1 (en) 2010-06-24 2011-12-29 Microsoft Corporation Virtual and location-based multiplayer gaming\n\nFamily Cites Families (13)\n\n* Cited by examiner, â Cited by third party Publication number Priority date Publication date Assignee Title GB2405010A (en) * 2002-05-13 2005-02-16 Cons Global Fun Unltd Llc Method and system for interacting with simulated phenomena US20050009608A1 (en) * 2002-05-13 2005-01-13 Consolidated Global Fun Unlimited Commerce-enabled environment for interacting with simulated phenomena US20030232649A1 (en) * 2002-06-18 2003-12-18 Gizis Alexander C.M. Gaming system and method US8585476B2 (en) * 2004-11-16 2013-11-19 Jeffrey D Mullen Location-based games and augmented reality systems US20060223637A1 (en) * 2005-03-31 2006-10-05 Outland Research, Llc Video game system combining gaming simulation with remote robot control and remote robot feedback US8355410B2 (en) * 2007-08-17 2013-01-15 At&T Intellectual Property I, L.P. Location-based mobile gaming application and method for implementing the same using a scalable tiered geocast protocol US8506404B2 (en) * 2007-05-07 2013-08-13 Samsung Electronics Co., Ltd. Wireless gaming method and wireless gaming-enabled mobile terminal GB2449694B (en) * 2007-05-31 2010-05-26 Sony Comp Entertainment Europe Entertainment system and method US9901828B2 (en) * 2010-03-30 2018-02-27 Sony Interactive Entertainment America Llc Method for an augmented reality character to maintain and exhibit awareness of an observer US8251819B2 (en) * 2010-07-19 2012-08-28 XMG Studio Sensor error reduction in mobile device based interactive multiplayer augmented reality gaming through use of one or more game conventions US8425295B2 (en) * 2010-08-17 2013-04-23 Paul Angelos BALLAS System and method for rating intensity of video games US8267793B2 (en) * 2010-08-17 2012-09-18 Samsung Electronics Co., Ltd. Multiplatform gaming system US20120122570A1 (en) * 2010-11-16 2012-05-17 David Michael Baronoff Augmented reality gaming experience\n\n2012\n\n2012-03-07 US US13/414,491 patent/US20120231887A1/en not_active Abandoned\n\n2012-03-07 WO PCT/US2012/028109 patent/WO2012122293A1/en active Application Filing\n\nPatent Citations (13)\n\n* Cited by examiner, â Cited by third party Publication number Priority date Publication date Assignee Title\n\nCited By (2)\n\n* Cited by examiner, â Cited by third party Publication number Priority date Publication date Assignee Title US9104236B2 (en) 2013-08-22 2015-08-11 International Business Machines Corporation Modifying information presented by an augmented reality device US9104235B2 (en) 2013-08-22 2015-08-11 International Business Machines Corporation Modifying information presented by an augmented reality device\n\nAlso Published As\n\nPublication number Publication date US20120231887A1 (en) 2012-09-13\n\nSimilar Documents\n\nPublication Publication Date Title\n\nLegal Events\n\nDate Code Title Description"
    }
}
{
    "id": "correct_foundationPlace_00049_1",
    "rank": 43,
    "data": {
        "url": "https://github.com/Rudrabha/Wav2Lip",
        "read_more_link": "",
        "language": "en",
        "title": "Rudrabha/Wav2Lip: This repository contains the codes of \"A Lip Sync Expert Is All You Need for Speech to Lip Generation In the Wild\", published at ACM Multimedia 2020. For HD commercial model, please ",
        "top_image": "https://opengraph.githubassets.com/31781a65be6664a621d4d4bbf5bb57a74191cf8460aac7cb8d612434e1292e2a/Rudrabha/Wav2Lip",
        "meta_img": "https://opengraph.githubassets.com/31781a65be6664a621d4d4bbf5bb57a74191cf8460aac7cb8d612434e1292e2a/Rudrabha/Wav2Lip",
        "images": [
            "https://camo.githubusercontent.com/46dcacf6259055870176803c3d157ddc303af7eca0ef0c931d34a5ad0d753d51/68747470733a2f2f696d672e736869656c64732e696f2f656e64706f696e742e7376673f75726c3d68747470733a2f2f70617065727377697468636f64652e636f6d2f62616467652f612d6c69702d73796e632d6578706572742d69732d616c6c2d796f752d6e6565642d666f722d7370656563682f6c69702d73796e632d6f6e2d6c727332",
            "https://camo.githubusercontent.com/72ad5609da16abb1a53d85e0b972c1f5b058ace972e11acf6c19a3f4bea6c5be/68747470733a2f2f696d672e736869656c64732e696f2f656e64706f696e742e7376673f75726c3d68747470733a2f2f70617065727377697468636f64652e636f6d2f62616467652f612d6c69702d73796e632d6578706572742d69732d616c6c2d796f752d6e6565642d666f722d7370656563682f6c69702d73796e632d6f6e2d6c727333",
            "https://camo.githubusercontent.com/cf3cdd34ff551b5fb4197bf8b7812309432e8cb3f1ecc336db91b242e377d776/68747470733a2f2f696d672e736869656c64732e696f2f656e64706f696e742e7376673f75726c3d68747470733a2f2f70617065727377697468636f64652e636f6d2f62616467652f612d6c69702d73796e632d6578706572742d69732d616c6c2d796f752d6e6565642d666f722d7370656563682f6c69702d73796e632d6f6e2d6c7277",
            "https://camo.githubusercontent.com/32342fcb7adb751bca1112605f3f2ed10b5d406338e6b4d3bc4b085fbe751f63/68747470733a2f2f64726976652e676f6f676c652e636f6d2f75633f6578706f72743d766965772669643d31576e3068506d706f3447526243494a523854663230416b7a646931716a6a4739",
            "https://avatars.githubusercontent.com/u/8796788?s=64&v=4",
            "https://avatars.githubusercontent.com/u/35054375?s=64&v=4",
            "https://avatars.githubusercontent.com/u/15241507?s=64&v=4",
            "https://avatars.githubusercontent.com/u/35267863?s=64&v=4",
            "https://avatars.githubusercontent.com/u/13085375?s=64&v=4",
            "https://avatars.githubusercontent.com/u/2056896?s=64&v=4",
            "https://avatars.githubusercontent.com/u/21982975?s=64&v=4",
            "https://avatars.githubusercontent.com/u/25949019?s=64&v=4"
        ],
        "movies": [],
        "keywords": [],
        "meta_keywords": [
            ""
        ],
        "tags": null,
        "authors": [],
        "publish_date": null,
        "summary": "",
        "meta_description": "This repository contains the codes of \"A Lip Sync Expert Is All You Need for Speech to Lip Generation In the Wild\", published at ACM Multimedia 2020. For HD commercial model, please try out Sync Labs  - GitHub - Rudrabha/Wav2Lip: This repository contains the codes of \"A Lip Sync Expert Is All You Need for Speech to Lip Generation In the Wild\", published at ACM Multimedia 2020. For HD commercial model, please try out Sync Labs",
        "meta_lang": "en",
        "meta_favicon": "https://github.com/fluidicon.png",
        "meta_site_name": "GitHub",
        "canonical_link": "https://github.com/Rudrabha/Wav2Lip",
        "text": "Wav2Lip: Accurately Lip-syncing Videos In The Wild\n\nWav2Lip is hosted for free at Sync Labs\n\nAre you looking to integrate this into a product? We have a turn-key hosted API with new and improved lip-syncing models here: https://synclabs.so/\n\nFor any other commercial / enterprise requests, please contact us at pavan@synclabs.so and prady@synclabs.so\n\nTo reach out to the authors directly you can reach us at prajwal@synclabs.so, rudrabha@synclabs.so.\n\nThis code is part of the paper: A Lip Sync Expert Is All You Need for Speech to Lip Generation In the Wild published at ACM Multimedia 2020.\n\nðŸ“‘ Original Paper ðŸ“° Project Page ðŸŒ€ Demo âš¡ Live Testing ðŸ“” Colab Notebook Paper Project Page Demo Video Interactive Demo Colab Notebook /Updated Collab Notebook\n\nWeights of the visual quality disc has been updated in readme!\n\nLip-sync videos to any target speech with high accuracy ðŸ’¯. Try our interactive demo.\n\nâœ¨ Works for any identity, voice, and language. Also works for CGI faces and synthetic voices.\n\nComplete training code, inference code, and pretrained models are available ðŸ’¥\n\nOr, quick-start with the Google Colab Notebook: Link. Checkpoints and samples are available in a Google Drive folder as well. There is also a tutorial video on this, courtesy of What Make Art. Also, thanks to Eyal Gruss, there is a more accessible Google Colab notebook with more useful features. A tutorial collab notebook is present at this link.\n\nðŸ”¥ ðŸ”¥ Several new, reliable evaluation benchmarks and metrics [evaluation/ folder of this repo] released. Instructions to calculate the metrics reported in the paper are also present.\n\nAll results from this open-source code or our demo website should only be used for research/academic/personal purposes only. As the models are trained on the LRS2 dataset, any form of commercial use is strictly prohibited. For commercial requests please contact us directly!\n\nPython 3.6\n\nffmpeg: sudo apt-get install ffmpeg\n\nInstall necessary packages using pip install -r requirements.txt. Alternatively, instructions for using a docker image is provided here. Have a look at this comment and comment on the gist if you encounter any issues.\n\nFace detection pre-trained model should be downloaded to face_detection/detection/sfd/s3fd.pth. Alternative link if the above does not work.\n\nGetting the weights\n\nModel Description Link to the model Wav2Lip Highly accurate lip-sync Link Wav2Lip + GAN Slightly inferior lip-sync, but better visual quality Link Expert Discriminator Weights of the expert discriminator Link Visual Quality Discriminator Weights of the visual disc trained in a GAN setup Link\n\nLip-syncing videos using the pre-trained models (Inference)\n\nYou can lip-sync any video to any audio:\n\npython inference.py --checkpoint_path <ckpt> --face <video.mp4> --audio <an-audio-source>\n\nThe result is saved (by default) in results/result_voice.mp4. You can specify it as an argument, similar to several other available options. The audio source can be any file supported by FFMPEG containing audio data: *.wav, *.mp3 or even a video file, from which the code will automatically extract the audio.\n\nTips for better results:\n\nExperiment with the --pads argument to adjust the detected face bounding box. Often leads to improved results. You might need to increase the bottom padding to include the chin region. E.g. --pads 0 20 0 0.\n\nIf you see the mouth position dislocated or some weird artifacts such as two mouths, then it can be because of over-smoothing the face detections. Use the --nosmooth argument and give it another try.\n\nExperiment with the --resize_factor argument, to get a lower-resolution video. Why? The models are trained on faces that were at a lower resolution. You might get better, visually pleasing results for 720p videos than for 1080p videos (in many cases, the latter works well too).\n\nThe Wav2Lip model without GAN usually needs more experimenting with the above two to get the most ideal results, and sometimes, can give you a better result as well.\n\nPreparing LRS2 for training\n\nOur models are trained on LRS2. See here for a few suggestions regarding training on other datasets.\n\nLRS2 dataset folder structure\n\ndata_root (mvlrs_v1) â”œâ”€â”€ main, pretrain (we use only main folder in this work) | â”œâ”€â”€ list of folders | â”‚ â”œâ”€â”€ five-digit numbered video IDs ending with (.mp4)\n\nPlace the LRS2 filelists (train, val, test) .txt files in the filelists/ folder.\n\nPreprocess the dataset for fast training\n\npython preprocess.py --data_root data_root/main --preprocessed_root lrs2_preprocessed/\n\nAdditional options like batch_size and the number of GPUs to use in parallel to use can also be set.\n\nPreprocessed LRS2 folder structure\n\npreprocessed_root (lrs2_preprocessed) â”œâ”€â”€ list of folders | â”œâ”€â”€ Folders with five-digit numbered video IDs | â”‚ â”œâ”€â”€ *.jpg | â”‚ â”œâ”€â”€ audio.wav\n\nThere are two major steps: (i) Train the expert lip-sync discriminator, (ii) Train the Wav2Lip model(s).\n\nTraining the expert discriminator\n\nYou can download the pre-trained weights if you want to skip this step. To train it:\n\npython color_syncnet_train.py --data_root lrs2_preprocessed/ --checkpoint_dir <folder_to_save_checkpoints>\n\nTraining the Wav2Lip models\n\nYou can either train the model without the additional visual quality discriminator (< 1 day of training) or use the discriminator (~2 days). For the former, run:\n\npython wav2lip_train.py --data_root lrs2_preprocessed/ --checkpoint_dir <folder_to_save_checkpoints> --syncnet_checkpoint_path <path_to_expert_disc_checkpoint>\n\nTo train with the visual quality discriminator, you should run hq_wav2lip_train.py instead. The arguments for both files are similar. In both cases, you can resume training as well. Look at python wav2lip_train.py --help for more details. You can also set additional less commonly-used hyper-parameters at the bottom of the hparams.py file.\n\nTraining on datasets other than LRS2\n\nTraining on other datasets might require modifications to the code. Please read the following before you raise an issue:\n\nYou might not get good results by training/fine-tuning on a few minutes of a single speaker. This is a separate research problem, to which we do not have a solution yet. Thus, we would most likely not be able to resolve your issue.\n\nYou must train the expert discriminator for your own dataset before training Wav2Lip.\n\nIf it is your own dataset downloaded from the web, in most cases, needs to be sync-corrected.\n\nBe mindful of the FPS of the videos of your dataset. Changes to FPS would need significant code changes.\n\nThe expert discriminator's eval loss should go down to ~0.25 and the Wav2Lip eval sync loss should go down to ~0.2 to get good results.\n\nWhen raising an issue on this topic, please let us know that you are aware of all these points.\n\nWe have an HD model trained on a dataset allowing commercial usage. The size of the generated face will be 192 x 288 in our new model.\n\nPlease check the evaluation/ folder for the instructions.\n\nLicense and Citation\n\nThis repository can only be used for personal/research/non-commercial purposes. However, for commercial requests, please contact us directly at rudrabha@synclabs.so or prajwal@synclabs.so. We have a turn-key hosted API with new and improved lip-syncing models here: https://synclabs.so/ The size of the generated face will be 192 x 288 in our new models. Please cite the following paper if you use this repository:\n\n@inproceedings{10.1145/3394171.3413532, author = {Prajwal, K R and Mukhopadhyay, Rudrabha and Namboodiri, Vinay P. and Jawahar, C.V.}, title = {A Lip Sync Expert Is All You Need for Speech to Lip Generation In the Wild}, year = {2020}, isbn = {9781450379885}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3394171.3413532}, doi = {10.1145/3394171.3413532}, booktitle = {Proceedings of the 28th ACM International Conference on Multimedia}, pages = {484â€“492}, numpages = {9}, keywords = {lip sync, talking face generation, video generation}, location = {Seattle, WA, USA}, series = {MM '20} }\n\nParts of the code structure are inspired by this TTS repository. We thank the author for this wonderful code. The code for Face Detection has been taken from the face_alignment repository. We thank the authors for releasing their code and models. We thank zabique for the tutorial collab notebook.\n\nAwesome Readme Templates\n\nAwesome README\n\nHow to write a Good readme"
    }
}
{
    "id": "correct_foundationPlace_00041_1",
    "rank": 34,
    "data": {
        "url": "https://github.com/uncbiag/Awesome-Foundation-Models",
        "read_more_link": "",
        "language": "en",
        "title": "Models: A curated list of foundation models for vision and language tasks",
        "top_image": "https://opengraph.githubassets.com/c73d5ffe0767a961a0e996772894630fbf1c9388f49f82f552b9d355ab7b6987/uncbiag/Awesome-Foundation-Models",
        "meta_img": "https://opengraph.githubassets.com/c73d5ffe0767a961a0e996772894630fbf1c9388f49f82f552b9d355ab7b6987/uncbiag/Awesome-Foundation-Models",
        "images": [
            "https://camo.githubusercontent.com/715ee701c8a9a0dbe30aac69ed79f5712a6542f5a482a3940084ce76d494a779/68747470733a2f2f617765736f6d652e72652f62616467652e737667",
            "https://camo.githubusercontent.com/6da5c586c33e37bea29aabb84b9f81a9a2988a6ba46847ee38e67ecca4d5d631/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f73746172732f63616d627269616e2d6d6c6c6d2f63616d627269616e2e7376673f7374796c653d736f6369616c266c6162656c3d53746172",
            "https://camo.githubusercontent.com/3765fb9e128e21970fc41a6364a7c4387b856cfa2651316161c6cb515b708ba8/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f73746172732f6170706c652f6d6c2d346d2e7376673f7374796c653d736f6369616c266c6162656c3d53746172",
            "https://camo.githubusercontent.com/1e56f6103ee4f33c9f956e5dac7b91197d90365ba753548d21b95935cf3553e7/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f73746172732f4e582d41492f766973696f6e2d6c73746d2e7376673f7374796c653d736f6369616c266c6162656c3d53746172",
            "https://camo.githubusercontent.com/3c523551955ab5e2411335b627c6053cbc55657adac801802b971ad5ac813b7f/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f73746172732f4f70656e4d6573684c61622f4d657368584c2e7376673f7374796c653d736f6369616c266c6162656c3d53746172",
            "https://camo.githubusercontent.com/1a9c86ee44806457fd9eba9b41d230cd7a8397dc56b9a9272fcc2cb09b436308/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f73746172732f70726f762d67696761706174682f70726f762d67696761706174682e7376673f7374796c653d736f6369616c266c6162656c3d53746172",
            "https://camo.githubusercontent.com/85cd121596fc5e2ba2f4a08c87ed9276ce7cbf8e1380f74438805d153cc2d7bf/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f73746172732f6f63746f2d6d6f64656c732f6f63746f2e7376673f7374796c653d736f6369616c266c6162656c3d53746172",
            "https://camo.githubusercontent.com/661f1b854bef7cea8088868d7aa7314615e3998285cd62c10a2685e7c863e079/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f73746172732f416c7068612d564c4c4d2f4c756d696e612d5432582e7376673f7374796c653d736f6369616c266c6162656c3d53746172",
            "https://camo.githubusercontent.com/82f9b15f7af95ad9b8088b40d007c83579b7bfd2039c675af87860194a5167a4/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f73746172732f6d6963726f736f66742f756e696c6d2e7376673f7374796c653d736f6369616c266c6162656c3d53746172",
            "https://camo.githubusercontent.com/4f498d9c425c264de9981dbdc889d1ac1e43495e29eb329af4fbb8aa7e497760/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f73746172732f72656b612d61692f72656b612d766962652d6576616c2e7376673f7374796c653d736f6369616c266c6162656c3d53746172",
            "https://camo.githubusercontent.com/54eabf3bb30f181357af2dea9f8244986afbfd42323cc85c6cf98af1718f037e/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f73746172732f4b696e645869616f6d696e672f70796b616e2e7376673f7374796c653d736f6369616c266c6162656c3d53746172",
            "https://camo.githubusercontent.com/5c6a38c8a2f2ec74905a7db3dd519a220b360639c0566120c981316662a5bc22/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f73746172732f4f70656e47564c61622f496e7465726e564c2e7376673f7374796c653d736f6369616c266c6162656c3d53746172",
            "https://camo.githubusercontent.com/6c0faf194c34d68da3db9d35aa0b3148a296ecfd4f5c137e198ceb1d844b674e/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f73746172732f4f70656e47564c61622f496e7465726e566964656f322e7376673f7374796c653d736f6369616c266c6162656c3d53746172",
            "https://camo.githubusercontent.com/6f96a117ce53c458fee77fbba3a2b11811b992fb2975e0142d5692c9b0f27f04/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f73746172732f666f69766f737061722f41726332466163652e7376673f7374796c653d736f6369616c266c6162656c3d53746172",
            "https://camo.githubusercontent.com/2c2391c53c2d25f27a3a1c1e48707f6ef53f40f2bc8789e97c1ea27d008cbe63/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f73746172732f756e63626961672f756e694772616449434f4e2e7376673f7374796c653d736f6369616c266c6162656c3d53746172",
            "https://camo.githubusercontent.com/051c5d330c72ba60514e505ba0cc6dcd165b1a727ec5abc5ba5f4ef14f26aba9/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f73746172732f4d65697475616e2d4175746f4d4c2f566973696f6e4c4c614d412e7376673f7374796c653d736f6369616c266c6162656c3d53746172",
            "https://camo.githubusercontent.com/0d6cee5ce74ded449eac1dafdeca8af56a35fb65b9b16ee483d5391180ed9edc/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f73746172732f68616f2d61692d6c61622f436f6e73697374656e63795f4c4c4d2e7376673f7374796c653d736f6369616c266c6162656c3d53746172",
            "https://camo.githubusercontent.com/7139f8e09f9f51e71eca5ffafb2ddf3bee6f86cd6629cef0b98cfc6335378ee9/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f73746172732f6c61796572646966667573696f6e2f4c61796572446966667573652e7376673f7374796c653d736f6369616c266c6162656c3d53746172",
            "https://camo.githubusercontent.com/67bbc51e5ef90958bdd28e23e597f8873df3972de1302dcc0242c3c282de27fe/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f73746172732f4e55532d4850432d41492d4c61622f4e657572616c2d4e6574776f726b2d446966667573696f6e2e7376673f7374796c653d736f6369616c266c6162656c3d53746172",
            "https://camo.githubusercontent.com/c59d2571956db10043375b45053f358f2fece45d32d01b764e0b8a82b015dcf5/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f73746172732f77686c7a792f4669542e7376673f7374796c653d736f6369616c266c6162656c3d53746172",
            "https://camo.githubusercontent.com/d3806df7a807b941e1f43a6fecd29bec7a733db98c7c84b24f5ba454435ae544/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f73746172732f4d65697475616e2d4175746f4d4c2f4d6f62696c65564c4d2e7376673f7374796c653d736f6369616c266c6162656c3d53746172",
            "https://camo.githubusercontent.com/0487f9832941f074ad69cc25b3b3aac35dd28ac07fca65a49a17481406d4d33f/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f73746172732f41494c61622d4356432f594f4c4f2d576f726c642e7376673f7374796c653d736f6369616c266c6162656c3d53746172",
            "https://camo.githubusercontent.com/40c0047ef746c23e8554bae6172b3d37957ddfb591a5201e0b0a11c38e83806d/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f73746172732f5374616e666f72642d41494d492f436865586167656e742e7376673f7374796c653d736f6369616c266c6162656c3d53746172",
            "https://camo.githubusercontent.com/97bd39515779a4136dac8cb032ff8efae5bf4262949ccc736a26ee7da459a75e/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f73746172732f4c696865596f756e672f44657074682d416e797468696e672e7376673f7374796c653d736f6369616c266c6162656c3d53746172",
            "https://camo.githubusercontent.com/2ccfbd3d3140b96d1e23e0b9e3edd4f58b57401cf9b3afe241e703f1473e550d/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f73746172732f77696c6c69736d612f5369542e7376673f7374796c653d736f6369616c266c6162656c3d53746172",
            "https://camo.githubusercontent.com/df5ad77b3dd92db1265d055b0262314b6584dc4ac159133a16f25f6f62f924e3/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f73746172732f496e7374616e7449442f496e7374616e7449442e7376673f7374796c653d736f6369616c266c6162656c3d53746172",
            "https://camo.githubusercontent.com/a5038d4807b0382de4280a8a5acc7b78edd441fd31c4afb3f2d49880246113e5/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f73746172732f68616f7469616e2d6c69752f4c4c6156412e7376673f7374796c653d736f6369616c266c6162656c3d53746172",
            "https://camo.githubusercontent.com/02878499ca49ed733d145e8246ffcd7432c0ee4bd08f3c822548e59e832f7945/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f73746172732f55582d4465636f6465722f5365676d656e742d45766572797468696e672d457665727977686572652d416c6c2d41742d4f6e63652e7376673f7374796c653d736f6369616c266c6162656c3d53746172",
            "https://camo.githubusercontent.com/43d024594642e4d80f3d00facc0ecab1c33bebdfb8f51f7310a120e32ee96eb9/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f73746172732f66616365626f6f6b72657365617263682f7365676d656e742d616e797468696e672e7376673f7374796c653d736f6369616c266c6162656c3d53746172",
            "https://camo.githubusercontent.com/c70cb2737c6f4f482a81b5e30183db02f146d7e4886e1281e5caa07e7206a9a2/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f73746172732f62616169766973696f6e2f5061696e7465722e7376673f7374796c653d736f6369616c266c6162656c3d53746172",
            "https://camo.githubusercontent.com/63edc1b576900e1c32f957cee609dde66d0e570be5f1c4e7e375918a994149bb/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f73746172732f7a68656e79757731362f556e694465746563746f722e7376673f7374796c653d736f6369616c266c6162656c3d53746172",
            "https://camo.githubusercontent.com/60627383d0448a98732e2ad5835ab8ed17b53575950da49f8e0baed0f04a7655/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f73746172732f676f6f676c652d72657365617263682f746578742d746f2d746578742d7472616e736665722d7472616e73666f726d65722e7376673f7374796c653d736f6369616c266c6162656c3d53746172",
            "https://camo.githubusercontent.com/a5038d4807b0382de4280a8a5acc7b78edd441fd31c4afb3f2d49880246113e5/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f73746172732f68616f7469616e2d6c69752f4c4c6156412e7376673f7374796c653d736f6369616c266c6162656c3d53746172",
            "https://camo.githubusercontent.com/b500188d37caaf41934242d388bcfc3a1c3085b46bf16e132545b96635f27a25/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f73746172732f566973696f6e2d434149522f4d696e694750542d342e7376673f7374796c653d736f6369616c266c6162656c3d53746172",
            "https://camo.githubusercontent.com/43d024594642e4d80f3d00facc0ecab1c33bebdfb8f51f7310a120e32ee96eb9/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f73746172732f66616365626f6f6b72657365617263682f7365676d656e742d616e797468696e672e7376673f7374796c653d736f6369616c266c6162656c3d53746172",
            "https://camo.githubusercontent.com/c70cb2737c6f4f482a81b5e30183db02f146d7e4886e1281e5caa07e7206a9a2/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f73746172732f62616169766973696f6e2f5061696e7465722e7376673f7374796c653d736f6369616c266c6162656c3d53746172",
            "https://camo.githubusercontent.com/98272cee9df6c548528c68c8d922be7951405b21afa2a694d9cdd06db1c0a552/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f73746172732f646966662d7573696f6e2f417765736f6d652d446966667573696f6e2d4d6f64656c732e7376673f7374796c653d736f6369616c266c6162656c3d53746172",
            "https://camo.githubusercontent.com/a0ad91aa93afce5a07031a7523741976a1a248bcafb73085503abf0280fa96b3/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f73746172732f4368656e4873696e672f417765736f6d652d566964656f2d446966667573696f6e2d4d6f64656c732e7376673f7374796c653d736f6369616c266c6162656c3d53746172",
            "https://camo.githubusercontent.com/a42bd3dcd3715912a058cedddf676e6cb5d6198a8e92260c27262fc801a4a328/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f73746172732f536961744d4d4c61622f417765736f6d652d446966667573696f6e2d4d6f64656c2d42617365642d496d6167652d45646974696e672d4d6574686f64732e7376673f7374796c653d736f6369616c266c6162656c3d53746172",
            "https://camo.githubusercontent.com/f5b87c7b9f2b150acb348ee46871aa6c733085e86fd04b4c2af958afb38c4d12/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f73746172732f6177616973726175662f417765736f6d652d43562d466f756e646174696f6e616c2d4d6f64656c732e7376673f7374796c653d736f6369616c266c6162656c3d53746172",
            "https://camo.githubusercontent.com/93ddec9c24e25520b49b98abd0a6ad2a7c5c6867a7bddc05ee56b101fd6668d8/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f73746172732f4a69616e696e672d5169752f417765736f6d652d4865616c7468636172652d466f756e646174696f6e2d4d6f64656c732e7376673f7374796c653d736f6369616c266c6162656c3d53746172",
            "https://camo.githubusercontent.com/1c288d4b3f4527d1e27d69a0984c3c25c9418e8815d7db3650393355d48e1a38/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f73746172732f6a756e3077616e616e2f617765736f6d652d6c617267652d6d756c74696d6f64616c2d6167656e74732e7376673f7374796c653d736f6369616c266c6162656c3d53746172",
            "https://camo.githubusercontent.com/368da236ca5d475fecd653697831c8156975dd21ca5055c782cb57f28993821d/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f73746172732f436f6d70757465722d566973696f6e2d696e2d7468652d57696c642f4356696e575f52656164696e67732e7376673f7374796c653d736f6369616c266c6162656c3d53746172",
            "https://avatars.githubusercontent.com/u/57234075?s=64&v=4",
            "https://avatars.githubusercontent.com/u/40518028?s=64&v=4"
        ],
        "movies": [],
        "keywords": [],
        "meta_keywords": [
            ""
        ],
        "tags": null,
        "authors": [],
        "publish_date": null,
        "summary": "",
        "meta_description": "A curated list of foundation models for vision and language tasks - uncbiag/Awesome-Foundation-Models",
        "meta_lang": "en",
        "meta_favicon": "https://github.com/fluidicon.png",
        "meta_site_name": "GitHub",
        "canonical_link": "https://github.com/uncbiag/Awesome-Foundation-Models",
        "text": "A foundation model is a large-scale pretrained model (e.g., BERT, DALL-E, GPT-3) that can be adapted to a wide range of downstream applications. This term was first popularized by the Stanford Institute for Human-Centered Artificial Intelligence. This repository maintains a curated list of foundation models for vision and language tasks. Research papers without code are not included.\n\nTowards Vision-Language Geo-Foundation Model: A Survey (Nanyang Technological University)\n\nAn Introduction to Vision-Language Modeling (from Meta)\n\nThe Evolution of Multimodal Model Architectures (from Purdue University)\n\nEfficient Multimodal Large Language Models: A Survey (from Tencent)\n\nFoundation Models for Video Understanding: A Survey (from Aalborg University)\n\nIs Sora a World Simulator? A Comprehensive Survey on General World Models and Beyond (from GigaAI)\n\nProspective Role of Foundation Models in Advancing Autonomous Vehicles (from Tongji University)\n\nParameter-Efficient Fine-Tuning for Large Models: A Comprehensive Survey (from Northeastern University)\n\nA Review on Background, Technology, Limitations, and Opportunities of Large Vision Models (from Lehigh University)\n\nLarge Multimodal Agents: A Survey (from CUHK)\n\nThe Uncanny Valley: A Comprehensive Analysis of Diffusion Models (from Mila)\n\nReal-World Robot Applications of Foundation Models: A Review (from University of Tokyo)\n\nFrom GPT-4 to Gemini and Beyond: Assessing the Landscape of MLLMs on Generalizability, Trustworthiness and Causality through Four Modalities (from Shanghai AI Lab)\n\nFoundational Models in Medical Imaging: A Comprehensive Survey and Future Vision (from SDSU)\n\nMultimodal Foundation Models: From Specialists to General-Purpose Assistants (from Microsoft)\n\nTowards Generalist Foundation Model for Radiology (from SJTU)\n\nFoundational Models Defining a New Era in Vision: A Survey and Outlook (from MBZ University of AI)\n\nTowards Generalist Biomedical AI (from Google)\n\nA Systematic Survey of Prompt Engineering on Vision-Language Foundation Models (from Oxford)\n\nLarge Multimodal Models: Notes on CVPR 2023 Tutorial (from Chunyuan Li, Microsoft)\n\nA Survey on Multimodal Large Language Models (from USTC and Tencent)\n\nVision-Language Models for Vision Tasks: A Survey (from Nanyang Technological University)\n\nFoundation Models for Generalist Medical Artificial Intelligence (from Stanford)\n\nA Comprehensive Survey on Pretrained Foundation Models: A History from BERT to ChatGPT\n\nA Comprehensive Survey of AI-Generated Content (AIGC): A History of Generative AI from GAN to ChatGPT\n\nVision-language pre-training: Basics, recent advances, and future trends\n\nOn the Opportunities and Risks of Foundation Models (This survey first popularizes the concept of foundation model; from Standford)\n\nPapers by Date\n\n[06/24] Cambrian-1: A Fully Open, Vision-Centric Exploration of Multimodal LLMs (from NYU)\n\n[06/13] 4M-21: An Any-to-Any Vision Model for Tens of Tasks and Modalities (from EPFL and Apple)\n\n[06/10] Merlin: A Vision Language Foundation Model for 3D Computed Tomography (from Stanford. Code will be available.)\n\n[06/06] Vision-LSTM: xLSTM as Generic Vision Backbone (from LSTM authors)\n\n[05/31] MeshXL: Neural Coordinate Field for Generative 3D Foundation Models (from Fudan)\n\n[05/22] Attention as an RNN (from Mila & Borealis AI)\n\n[05/22] GigaPath: A whole-slide foundation model for digital pathology from real-world data (from Nature)\n\n[05/21] BiomedParse: a biomedical foundation model for biomedical image parsing (from Microsoft)\n\n[05/20] Octo: An Open-Source Generalist Robot Policy (from UC Berkeley)\n\n[05/17] Observational Scaling Laws and the Predictability of Language Model Performance (fro Standford)\n\n[05/14] Understanding the performance gap between online and offline alignment algorithms (from Google)\n\n[05/09] Lumina-T2X: Transforming Text into Any Modality, Resolution, and Duration via Flow-based Large Diffusion Transformers (from Shanghai AI Lab)\n\n[05/08] You Only Cache Once: Decoder-Decoder Architectures for Language Models\n\n[05/06] Advancing Multimodal Medical Capabilities of Gemini (from Google)\n\n[05/07] xLSTM: Extended Long Short-Term Memory (from Sepp Hochreiter, the author of LSTM.)\n\n[05/03] Vibe-Eval: A hard evaluation suite for measuring progress of multimodal language models\n\n[04/30] KAN: Kolmogorov-Arnold Networks (Promising alternatives of MLPs. from MIT)\n\n[04/26] How Far Are We to GPT-4V? Closing the Gap to Commercial Multimodal Models with Open-Source Suites (InternVL 1.5. from Shanghai AI Lab)\n\n[04/14] TransformerFAM: Feedback attention is working memory (from Google. Efficient attention.)\n\n[04/10] Leave No Context Behind: Efficient Infinite Context Transformers with Infini-attention (from Google)\n\n[04/02] Octopus v2: On-device language model for super agent (from Stanford)\n\n[04/02] Mixture-of-Depths: Dynamically allocating compute in transformer-based language models (from Google)\n\n[03/22] InternVideo2: Scaling Video Foundation Models for Multimodal Video Understanding (from Shanghai AI Lab)\n\n[03/18] Arc2Face: A Foundation Model of Human Faces (from Imperial College London)\n\n[03/14] MM1: Methods, Analysis & Insights from Multimodal LLM Pre-training (30B parameters. from Apple)\n\n[03/09] uniGradICON: A Foundation Model for Medical Image Registration (from UNC-Chapel Hill)\n\n[03/05] Scaling Rectified Flow Transformers for High-Resolution Image Synthesis (Stable Diffusion 3. from Stability AI)\n\n[03/01] Learning and Leveraging World Models in Visual Representation Learning (from Meta)\n\n[03/01] VisionLLaMA: A Unified LLaMA Interface for Vision Tasks (from Meituan)\n\n[02/28] CLLMs: Consistency Large Language Models (from SJTU)\n\n[02/27] Transparent Image Layer Diffusion using Latent Transparency (from Standford)\n\n[02/22] MobileLLM: Optimizing Sub-billion Parameter Language Models for On-Device Use Cases (from Meta)\n\n[02/21] Beyond A∗: Better Planning with Transformers via Search Dynamics Bootstrapping (from Meta)\n\n[02/20] Neural Network Diffusion (Generating network parameters via diffusion models. from NUS)\n\n[02/20] VideoPrism: A Foundational Visual Encoder for Video Understanding (from Google)\n\n[02/19] FiT: Flexible Vision Transformer for Diffusion Model (from Shanghai AI Lab)\n\n[02/06] MobileVLM V2: Faster and Stronger Baseline for Vision Language Model (from Meituan)\n\n[01/30] YOLO-World: Real-Time Open-Vocabulary Object Detection (from Tencent and HUST)\n\n[01/23] Lumiere: A Space-Time Diffusion Model for Video Generation (from Google)\n\n[01/22] CheXagent: Towards a Foundation Model for Chest X-Ray Interpretation (from Stanford)\n\n[01/19] Depth Anything: Unleashing the Power of Large-Scale Unlabeled Data (from TikTok)\n\n[01/16] SiT: Exploring Flow and Diffusion-based Generative Models with Scalable Interpolant Transformers (from NYU)\n\n[01/15] InstantID: Zero-shot Identity-Preserving Generation in Seconds (from Xiaohongshu)\n\nBioCLIP: A Vision Foundation Model for the Tree of Life (CVPR 2024 best student paper)\n\nMamba: Linear-Time Sequence Modeling with Selective State Spaces (Mamba appears to outperform similarly-sized Transformers while scaling linearly with sequence length. from CMU)\n\nFoundationPose: Unified 6D Pose Estimation and Tracking of Novel Objects (from NVIDIA)\n\nTracking Everything Everywhere All at Once (from Cornell, ICCV 2023 best student paper)\n\nFoundation Models for Generalist Geospatial Artificial Intelligence (from IBM and NASA)\n\nLLaMA 2: Open Foundation and Fine-Tuned Chat Models (from Meta)\n\nInternLM-XComposer: A Vision-Language Large Model for Advanced Text-image Comprehension and Composition (from Shanghai AI Lab)\n\nThe All-Seeing Project: Towards Panoptic Visual Recognition and Understanding of the Open World (from Shanghai AI Lab)\n\nMeta-Transformer: A Unified Framework for Multimodal Learning (from CUHK and Shanghai AI Lab)\n\nRetentive Network: A Successor to Transformer for Large Language Models (from Microsoft and Tsinghua University)\n\nNeural World Models for Computer Vision (PhD Thesis of Anthony Hu from University of Cambridge)\n\nRecognize Anything: A Strong Image Tagging Model (a strong foundation model for image tagging. from OPPO)\n\nTowards Visual Foundation Models of Physical Scenes (describes a first step towards learning general-purpose visual representations of physical scenes using only image prediction as a training criterion; from AWS)\n\nLIMA: Less Is More for Alignment (65B parameters, from Meta)\n\nPaLM 2 Technical Report (from Google)\n\nIMAGEBIND: One Embedding Space To Bind Them All (from Meta)\n\nVisual Instruction Tuning (LLaVA, from U of Wisconsin-Madison and Microsoft)\n\nSEEM: Segment Everything Everywhere All at Once (from University of Wisconsin-Madison, HKUST, and Microsoft)\n\nSAM: Segment Anything (the first foundation model for image segmentation; from Meta)\n\nSegGPT: Segmenting Everything In Context (from BAAI, ZJU, and PKU)\n\nImages Speak in Images: A Generalist Painter for In-Context Visual Learning (from BAAI, ZJU, and PKU)\n\nUniDector: Detecting Everything in the Open World: Towards Universal Object Detection (CVPR, from Tsinghua and BNRist)\n\nUnmasked Teacher: Towards Training-Efficient Video Foundation Models (from Chinese Academy of Sciences, University of Chinese Academy of Sciences, Shanghai AI Laboratory)\n\nVisual Prompt Multi-Modal Tracking (from Dalian University of Technology and Peng Cheng Laboratory)\n\nToward Building General Foundation Models for Language, Vision, and Vision-Language Understanding Tasks (from ByteDance)\n\nEVA-CLIP: Improved Training Techniques for CLIP at Scale (from BAAI and HUST)\n\nEVA-02: A Visual Representation for Neon Genesis (from BAAI and HUST)\n\nEVA-01: Exploring the Limits of Masked Visual Representation Learning at Scale (CVPR, from BAAI and HUST)\n\nLLaMA: Open and Efficient Foundation Language Models (A collection of foundation language models ranging from 7B to 65B parameters; from Meta)\n\nThe effectiveness of MAE pre-pretraining for billion-scale pretraining (from Meta)\n\nBloombergGPT: A Large Language Model for Finance (50 billion parameters; from Bloomberg)\n\nBLOOM: A 176B-Parameter Open-Access Multilingual Language Model (this work was coordinated by BigScience whose goal is to democratize LLMs.)\n\nFLIP: Scaling Language-Image Pre-training via Masking (from Meta)\n\nBLIP-2: Bootstrapping Language-Image Pre-training with Frozen Image Encoders and Large Language Models (from Saleforce Research)\n\nGPT-4 Technical Report (from OpenAI)\n\nVisual ChatGPT: Talking, Drawing and Editing with Visual Foundation Models (from Microsoft Research Asia)\n\nUNINEXT: Universal Instance Perception as Object Discovery and Retrieval (a unified model for 10 instance perception tasks; CVPR, from ByteDance)\n\nInternVideo: General Video Foundation Models via Generative and Discriminative Learning (from Shanghai AI Lab)\n\nInternImage: Exploring Large-Scale Vision Foundation Models with Deformable Convolutions (CVPR, from Shanghai AI Lab)\n\nBridgeTower: Building Bridges Between Encoders in Vision-Language Representation Learning (from Harbin Institute of Technology and Microsoft Research Asia)\n\nBEVT: BERT Pretraining of Video Transformers (CVPR, from Shanghai Key Lab of Intelligent Information Processing)\n\nFoundation Transformers (from Microsoft)\n\nA Generalist Agent (known as Gato, a multi-modal, multi-task, multi-embodiment generalist agent; from DeepMind)\n\nFIBER: Coarse-to-Fine Vision-Language Pre-training with Fusion in the Backbone (from Microsoft, UCLA, and New York University)\n\nFlamingo: a Visual Language Model for Few-Shot Learning (from DeepMind)\n\nMetaLM: Language Models are General-Purpose Interfaces (from Microsoft)\n\nPoint-E: A System for Generating 3D Point Clouds from Complex Prompts (efficient 3D object generation using a text-to-image diffusion model; from OpenAI)\n\nImage Segmentation Using Text and Image Prompts (CVPR, from University of Göttingen)\n\nUnifying Flow, Stereo and Depth Estimation (A unified model for three motion and 3D perception tasks; from ETH Zurich)\n\nPaLI: A Jointly-Scaled Multilingual Language-Image Model (from Google)\n\nVideoMAE: Masked Autoencoders are Data-Efficient Learners for Self-Supervised Video Pre-Training (NeurIPS, from Nanjing University, Tencent, and Shanghai AI Lab)\n\nSLIP: Self-supervision meets Language-Image Pre-training (ECCV, from UC Berkeley and Meta)\n\nGLIPv2: Unifying Localization and VL Understanding (NeurIPS'22, from UW, Meta, Microsoft, and UCLA)\n\nGLIP: Grounded Language-Image Pre-training (CVPR, from UCLA and Microsoft)\n\nBLIP: Bootstrapping Language-Image Pre-training for Unified Vision-Language Understanding and Generation (from Salesforce Research)\n\nNUWA-Infinity: Autoregressive over Autoregressive Generation for Infinite Visual Synthesis (from Microsoft)\n\nPaLM: Scaling Language Modeling with Pathways (from Google)\n\nCoCa: Contrastive Captioners are Image-Text Foundation Models (from Google)\n\nParti: Scaling Autoregressive Models for Content-Rich Text-to-Image Generation (from Google)\n\nA Unified Sequence Interface for Vision Tasks (from Google Research, Brain Team)\n\nImagen: Photorealistic Text-to-Image Diffusion Models with Deep Language Understanding (from Google)\n\nStable Diffusion: High-Resolution Image Synthesis with Latent Diffusion Models (CVPR, from Stability and Runway)\n\nBeyond the Imitation Game: Quantifying and extrapolating the capabilities of language models (BIG-Bench: a 204-task extremely difficult and diverse benchmark for LLMs, 444 authors from 132 institutions)\n\nCRIS: CLIP-Driven Referring Image Segmentation (from University of Sydney and OPPO)\n\nMasked Autoencoders As Spatiotemporal Learners (extension of MAE to videos; NeurIPS, from Meta)\n\nMasked Autoencoders Are Scalable Vision Learners (CVPR 2022, from FAIR)\n\nInstructGPT: Training language models to follow instructions with human feedback (trained with humans in the loop; from OpenAI)\n\nA Unified Sequence Interface for Vision Tasks (NeurIPS 2022, from Google)\n\nDALL-E2: Hierarchical Text-Conditional Image Generation with CLIP Latents (from OpenAI)\n\nRobust and Efficient Medical Imaging with Self-Supervision (from Google, Georgia Tech, and Northwestern University)\n\nVideo Swin Transformer (CVPR, from Microsoft Research Asia)\n\nOFA: Unifying Architectures, Tasks, and Modalities Through a Simple Sequence-to-Sequence Learning Framework (ICML 2022. from Alibaba.)\n\nMask2Former: Masked-attention Mask Transformer for Universal Image Segmentation (CVPR 2022, from FAIR and UIUC)\n\nFLAVA: A Foundational Language And Vision Alignment Model (CVPR, from Facebook AI Research)\n\nTowards artificial general intelligence via a multimodal foundation model (Nature Communication, from Renmin University of China)\n\nFILIP: Fine-Grained Interactive Language-Image Pre-Training (ICLR, from Huawei and HKUST)\n\nSimVLM: Simple Visual Language Model Pretraining with Weak Supervision (ICLR, from CMU and Google)\n\nGLIDE: Towards Photorealistic Image Generation and Editing with Text-Guided Diffusion Models (from OpenAI)\n\nUnifying Vision-and-Language Tasks via Text Generation (from UNC-Chapel Hill)\n\nALIGN: Scaling Up Visual and Vision-Language Representation Learning With Noisy Text Supervision (PMLR, from Google)\n\nUniT: Multimodal Multitask Learning with a Unified Transformer (ICCV, from FAIR)\n\nWenLan: Bridging Vision and Language by Large-Scale Multi-Modal Pre-Training (This paper presents the first large-scale Chinese multimodal pre-training model called BriVL; from Renmin University of China)\n\nCodex: Evaluating Large Language Models Trained on Code (a GPT language model finetuned on public code from GitHub, from OpenAI and Anthropic AI)\n\nFlorence: A New Foundation Model for Computer Vision (from Microsoft)\n\nDALL-E: Zero-Shot Text-to-Image Generation (from OpenAI)\n\nCLIP: Learning Transferable Visual Models From Natural Language Supervision (from OpenAI)\n\nMultimodal Few-Shot Learning with Frozen Language Models (NeurIPS, from DeepMind)\n\nSwin Transformer: Hierarchical Vision Transformer using Shifted Windows (ICCV, from Microsoft Research Asia)\n\nAn Image is Worth 16x16 Words: Transformers for Image Recognition at Scale (The first Vision Transfomer with pure self-attention blocks; ICLR, from Google)\n\nGPT-3: Language Models are Few-Shot Learners (175B parameters; permits in-context learning compared with GPT-2; from OpenAI)\n\nUNITER: UNiversal Image-TExt Representation Learning (from Microsoft)\n\nT5: Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer (from Google)\n\nGPT-2: Language Models are Unsupervised Multitask Learners (1.5B parameters; from OpenAI)\n\nLXMERT: Learning Cross-Modality Encoder Representations from Transformers (EMNLP, from UNC-Chapel Hill)\n\nBERT: Pre-training of Deep Bidirectional Transformers for Language Understanding (from Google AI Language)\n\nGPT: Improving Language Understanding by Generative Pre-Training (from OpenAI)\n\nAttention Is All You Need (NeurIPS, from Google and UoT)\n\nGPT-4 Technical Report (from OpenAI)\n\nGPT-3: Language Models are Few-Shot Learners (175B parameters; permits in-context learning compared with GPT-2; from OpenAI)\n\nGPT-2: Language Models are Unsupervised Multitask Learners (1.5B parameters; from OpenAI)\n\nGPT: Improving Language Understanding by Generative Pre-Training (from OpenAI)\n\nLLaMA 2: Open Foundation and Fine-Tuned Chat Models (from Meta)\n\nLLaMA: Open and Efficient Foundation Language Models (models ranging from 7B to 65B parameters; from Meta)\n\nT5: Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer (from Google)\n\nLLaVA: Visual Instruction Tuning (from University of Wisconsin-Madison)\n\nMiniGPT-4: Enhancing Vision-Language Understanding with Advanced Large Language Models (from KAUST)\n\nMMT-Bench: A Comprehensive Multimodal Benchmark for Evaluating Large Vision-Language Models Towards Multitask AGI (from Shanghai AI Lab, 2024)\n\nBLINK: Multimodal Large Language Models Can See but Not Perceive (multimodal benchmark. from University of Pennsylvania, 2024)\n\nCAD-Estate: Large-scale CAD Model Annotation in RGB Videos (RGB videos with CAD annotation. from Google 2023)\n\nImageNet: A Large-Scale Hierarchical Image Database (vision benchmark. from Stanford, 2009)\n\nFLIP: Scaling Language-Image Pre-training via Masking (from Meta)\n\nBLIP-2: Bootstrapping Language-Image Pre-training with Frozen Image Encoders and Large Language Models (proposes a generic and efficient VLP strategy based on off-the-shelf frozen vision and language models. from Salesforce Research)\n\nBLIP: Bootstrapping Language-Image Pre-training for Unified Vision-Language Understanding and Generation (from Salesforce Research)\n\nSLIP: Self-supervision meets Language-Image Pre-training (ECCV, from UC Berkeley and Meta)\n\nGLIP: Grounded Language-Image Pre-training (CVPR, from UCLA and Microsoft)\n\nALIGN: Scaling Up Visual and Vision-Language Representation Learning With Noisy Text Supervision (PMLR, from Google)\n\nRegionCLIP: Region-Based Language-Image Pretraining\n\nCLIP: Learning Transferable Visual Models From Natural Language Supervision (from OpenAI)\n\nFoundationPose: Unified 6D Pose Estimation and Tracking of Novel Objects (from NVIDIA)\n\nSEEM: Segment Everything Everywhere All at Once (from University of Wisconsin-Madison, HKUST, and Microsoft)\n\nSAM: Segment Anything (the first foundation model for image segmentation; from Meta)\n\nSegGPT: Segmenting Everything In Context (from BAAI, ZJU, and PKU)\n\nGreen AI (introduces the concept of Red AI vs Green AI)\n\nThe Lottery Ticket Hypothesis: Finding Sparse, Trainable Neural Networks (the lottery ticket hypothesis, from MIT)\n\nTowards AGI in Computer Vision: Lessons Learned from GPT and Large Language Models (from Huawei)\n\nManaging Extreme AI Risks amid Rapid Progress (from Science, May 2024)\n\nRelated Awesome Repositories"
    }
}
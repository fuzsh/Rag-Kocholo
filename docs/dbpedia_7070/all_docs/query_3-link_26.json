{
    "id": "dbpedia_7070_3",
    "rank": 26,
    "data": {
        "url": "https://www.ncbi.nlm.nih.gov/pmc/articles/PMC10587626/",
        "read_more_link": "",
        "language": "en",
        "title": "Evaluating Visual Acuity in the American Academy of Ophthalmology IRIS® Registry",
        "top_image": "https://www.ncbi.nlm.nih.gov/corehtml/pmc/pmcgifs/pmc-card-share.jpg?_=0",
        "meta_img": "https://www.ncbi.nlm.nih.gov/corehtml/pmc/pmcgifs/pmc-card-share.jpg?_=0",
        "images": [
            "https://www.ncbi.nlm.nih.gov/coreutils/uswds/img/favicons/favicon-57.png",
            "https://www.ncbi.nlm.nih.gov/coreutils/uswds/img/icon-dot-gov.svg",
            "https://www.ncbi.nlm.nih.gov/coreutils/uswds/img/icon-https.svg",
            "https://www.ncbi.nlm.nih.gov/coreutils/nwds/img/logos/AgencyLogo.svg",
            "https://www.ncbi.nlm.nih.gov/corehtml/pmc/pmcgifs/logo-ops.gif",
            "https://www.ncbi.nlm.nih.gov/pmc/articles/PMC10587626/bin/gr1.jpg",
            "https://www.ncbi.nlm.nih.gov/pmc/articles/PMC10587626/bin/gr2.jpg",
            "https://www.ncbi.nlm.nih.gov/pmc/articles/PMC10587626/bin/gr3.jpg",
            "https://www.ncbi.nlm.nih.gov/pmc/articles/PMC10587626/bin/gr4.jpg"
        ],
        "movies": [],
        "keywords": [],
        "meta_keywords": [
            ""
        ],
        "tags": null,
        "authors": [
            "Arthur Brant",
            "Natasha Kolomeyer",
            "Jeffrey L. Goldberg",
            "Julia Haller",
            "Cecilia S. Lee",
            "Aaron Y. Lee",
            "Alice C. Lorch",
            "Joan W. Miller",
            "Leslie Hyman",
            "Suzann Pershing"
        ],
        "publish_date": null,
        "summary": "",
        "meta_description": "To describe visual acuity data representation in the American Academy of Ophthalmology Intelligent Research in Sight (IRIS) Registry and present a data-cleaning strategyReliability and validity study.Patients with visual acuity records from 2018 in the ...",
        "meta_lang": "en",
        "meta_favicon": "https://www.ncbi.nlm.nih.gov/coreutils/nwds/img/favicons/favicon.ico",
        "meta_site_name": "PubMed Central (PMC)",
        "canonical_link": "https://www.ncbi.nlm.nih.gov/pmc/articles/PMC10587626/",
        "text": "Results\n\nStudy Sample, Visits, and Characteristics\n\nThe Rome data set contains 187 805 383 (136 158 960 with nonmissing logMAR) VA records among 22 329 528 (22 067 228 with nonmissing logMAR) unique patients in 2018, including duplicate measurements (same value) and multiple measurements (different values) on the same date. The Chicago data set contains 168 920 049 (168 530 763 with nonmissing logMAR) VA records among 23 001 531 (22 821 606 with nonmissing logMAR) unique patients in 2018. Measurement characteristics, including laterality and measurements associated with refraction or pinhole are shown in and .\n\nTable 1\n\nIndividual VA Records in 2018Unique Patient Visit Dates in 2018Unique Patients in 2018N (%)Mean (SD) §Median (IQR) §N (%)Mean (SD) §Median (IQR) §N (%)Mean (SD) §Median (IQR) §All136 158 960 (100.0)0.28 (0.45)0.18 (0.00–0.30)46 480 974 (100.0)0.29 (0.39)0.18 (0.05–0.35)22 067 228 (100.0)0.22 (0.32)0.14 (0.05–0.28)Laterality Right67 126 234 (49.3)0.27 (0.45)0.18 (0.00–0.30)43 961 059 (94.6)0.28 (0.47)0.16 (0.00–0.30)21 649 430 (98.1)0.22 (0.39)0.11 (0.00–0.27) Left67 165 803 (49.3)0.28 (0.45)0.18 (0.00–0.30)43 961 589 (94.6)0.29 (0.47)0.18 (0.05–0.30)21 678 382 (98.2)0.22 (0.39)0.12 (0.00–0.28) Both971 833 (0.7)0.13 (0.25)0.00 (0.00–0.18)838 048 (1.8)0.11 (0.21)0.00 (0.00–0.18)622 633 (2.8)0.10 (0.19)0.00 (0.00–0.15) Unspecified895,090 (0.7)0.35 (0.53)0.18 (0.10–0.40)529 021 (1.1)0.33 (0.44)0.20 (0.09–0.40)272 585 (1.2)0.28 (0.39)0.18 (0.05–0.33)Type CVA52 208 797 (38.3)0.26 (0.43)0.10 (0.00–0.30)25 930 757 (55.8)0.26 (0.36)0.15 (0.05–0.33)14 045 257 (63.7)0.21 (0.31)0.14 (0.05–0.27) UCVA32 744 110 (24.1)0.37 (0.51)0.18 (0.10–0.48)17 016 153 (36.6)0.38 (0.46)0.24 (0.10–0.48)8 292 934 (37.6)0.34 (0.41)0.23 (0.10–0.43) Unspecified51 206 053 (37.6)0.24 (0.41)0.10 (0.00–0.30)22 501 329 (48.4)0.24 (0.36)0.14 (0.02–0.30)13 469 176 (61.0)0.19 (0.30)0.10 (0.00–0.24)Method Distance89 653 937 (65.9)0.28 (0.44)0.18 (0.00–0.30)34 443 898 (74.1)0.29 (0.38)0.18 (0.07–0.36)16 438 363 (74.5)0.23 (0.31)0.14 (0.05–0.30) Near∥0 (0)†0 (0)†0 (0)2 Unspecified46 505 023 (34.2)0.27 (0.46)0.10 (0.00–0.30)17 530 837 (37.7)0.28 (0.42)0.16 (0.05–0.35)9 076 245 (41.1)0.22 (0.35)0.13 (0.03–0.28)Refraction6 618 549 (4.9)0.13 (0.27)0.00 (0.00–0.18)3 449 817 (7.4)0.13 (0.23)0.05 (0.00–0.18)2 321 081 (10.5)0.11 (0.21)0.04 (0.00–0.14)Pinhole16 283 667 (12.0)0.22 (0.29)0.18 (0.00–0.30)10 246 326 (22.0)0.24 (0.27)0.18 (0.07–0.30)6 321 618 (28.7)0.20 (0.25)0.14 (0.03–0.30)\n\nTable 2\n\nIndividual VA Records in 2018Unique Patient Visit DatesUnique Patients in 2018N (%)Mean (SD)§Median (IQR)§N (%)Mean (SD)§Median (IQR)§N (%)Mean (SD)§Median (IQR)§All168 530 763 (100.0)0.26 (0.41)0.10 (0.00–0.30)49 707 013 (100.0)0.28 (0.36)0.18 (0.05–0.35)22 821 606 (100.0)0.21 (0.29)0.13 (0.05–0.27)Laterality Right79 417 999 (47.1)0.26 (0.41)0.10 (0.00–0.30)46 379 658 (93.3)0.27 (0.43)0.15 (0.03–0.30)22 195 382 (97.3)0.21 (0.36)0.1 (0.00–0.25) Left79 617 069 (47.2)0.26 (0.41)0.10 (0.00–0.30)46 498 340 (93.5)0.27 (0.44)0.15 (0.03–0.30)22 313 880 (97.8)0.21 (0.36)0.1 (0.00–0.26) Both1 980 667 (1.2)0.11 (0.24)0.00 (0.00–0.10)1 518 329 (3.1)0.11 (0.21)0.0 (0.00–0.10)1 124 163 (4.9)0.09 (0.20)0.0 (0.00–0.10) Unspecified7 515 028 (4.5)0.28 (0.44)0.18 (0.00–0.30)3 529 975 (7.1)0.28 (0.39)0.18 (0.05–0.35)1 807 181 (7.9)0.22 (0.33)0.13 (0.03–0.28)Type CVA65 888 408 (39.1)0.24 (0.40)0.10 (0.00–0.30)28 946 271 (58.2)0.25 (0.33)0.14 (0.05–0.30)15 243 584 (66.8)0.19 (0.28)0.11 (0.03–0.24) UCVA39 355 985 (23.4)0.35 (0.47)0.18 (0.10–0.48)18 516 852 (37.3)0.37 (0.43)0.24 (0.10–0.47)8 879 231 (38.9)0.32 (0.39)0.21 (0.09–0.41) Unspecified63 286 370 (37.6)0.23 (0.38)0.10 (0.00–0.30)24 436 629 (49.2)0.23 (0.33)0.14 (0.03–0.30)14 338 148 (62.8)0.18 (0.28)0.1 (0.00–0.24)Method Distance100 696 572 (59.8)0.28 (0.42)0.18 (0.00–0.30)36 829 432 (74.1%)0.29 (0.35)0.18 (0.07–0.36)17 115 049 (75.0)0.22 (0.29)0.14 (0.05–0.29) Near∥13 763 728 (8.2)0.11 (0.17)0.00 (0.00–0.18)5 977 881 (12.0)0.10 (0.15)0.0 (0.00–0.15)4 372 042 (19.2)0.09 (0.14)0.0 (0.00–0.12) Unspecified54 070 463 (32.1)0.27 (0.44)0.10 (0.00–0.30)19 258 887 (38.7)0.28 (0.39)0.16 (0.05–0.35)9 757 086 (42.8)0.22 (0.32)0.13 (0.03–0.28)Refraction8 868 286 (5.7)0.12 (0.26)0.00 (0.0–0.18)4 277 090 (8.6)0.12 (0.23)0.05 (0.00–0.18)2 969 897 (13.0)0.10 (0.21)0.0 (0.00–0.14)Pinhole18 648 406 (11.1)0.22 (0.28)0.18 (0.00–0.30)11 523 973 (23.2)0.24 (0.26)0.18 (0.07–0.30)6 923 268 (30.3)0.20 (0.24)0.14 (0.03–0.300)\n\nThere were 47 135 899 unique patient visit dates with associated VA data in 2018 in the Rome data set, and 49 968 974 corresponding unique patient visit dates in the Chicago data set. We identified an unexpectedly wide spread to the number of VA records per visit, ranging from 1 to 59 in Rome and from 1 to 57 in Chicago, with an average of 4.04 in Rome (3.38 in Chicago) and a standard deviation (SD) of 2.39 in Rome (2.31 in Chicago).\n\nNotably, 1.4% (n = 663 451) of visit dates in Rome contained > 10 VA measurements, including duplicate entries. In total, 10.8% of all Rome records (n = 20 248 080) shared identical metadata (patient ID, date, eye laterality, VA correction, VA method, and refraction/pinhole/NI flags) with another VA record, and 2.7% shared identical metadata and logMAR with another record. In Chicago, 1.6% (n = 817 867) of visit dates contained > 10 VA measurements and only 1.4% of all records (n = 2 381 998) shared identical metadata (patient ID, date, eye laterality, VA correction, VA method, and refraction/pinhole/NI flags) with another VA record. More visit dates contained > 10 records in Chicago despite Chicago containing fewer duplicate records.\n\nVA Values\n\nRome\n\nQualitative VA values are reported using standard terminology for Snellen measurements and also assigned numeric values for logMAR and modified logMAR in the Rome data set (logMAR values in ). Both logMAR and modified logMAR values ranged from −0.1 to 4.0, with 92.5% of Rome values falling between 0 and 1, with the exception of 999, which denotes unspecified ( ). Snellen values ranged from 20/10 to no light perception (NLP), and modified Snellen and modified logMAR values demonstrated good concordance (Table S3, available at www.ophthalmologyscience.org). Count fingers was assigned logMAR = 2, HM logMAR = 3, LP logMAR = 3, and NLP logMAR = 4.\n\nThe mean (SD) of logMAR and modified logMAR were the same (0.28 and 0.45, respectively). Only 0.7% of records had different values for logMAR and modified logMAR for a patient on a given visit date, with the average difference being −0.00051 (SD, 0.00953). Median VA was notably better (lower logMAR) than mean VA, reflecting a distribution of VA among patients that skews toward better vision ( ; ).\n\nThe mean spread in VA measurements per patient within each eye (difference between best and worst VA measurements) was 0.17 logMAR across all assessment corrections in 2018, but was 0.06 logMAR for CVA measurements ( ). For patient visit dates with both right and left eye VA documented, the mean (SD) difference in VA between the 2 eyes was 0.24 (0.49) logMAR. However, in aggregate the mean and median VA values for right and left eyes were similar across the data set.\n\nChicago\n\nIn Chicago, CF was assigned logMAR = 1.9, HM logMAR = 2.3, LP logMAR = 2.7, and NLP logMAR = 4. The vast majority of VA records (99.8%, or 168 530 763) were associated with a logMAR value. Chicago logMAR values were similar to recorded values in Rome; 98.9% of Chicago logMAR values ranged from −0.1 to 4.0 ( ), 94.8% between 0 and 1. Median VA was better (lower logMAR) than mean VA ( ; ) and mean spread in VA measurements per patient within each eye (difference between best and worst VA measurements) was 0.19 logMAR across all assessment corrections in 2018 and 0.08 logMAR within only corrected VA measurements ( ). For patient visit dates with both right and left eye VA documented, the mean (SD) difference in VA between the 2 eyes was 0.23 (0.45) logMAR. However, in aggregate the mean and median VA values for right and left eyes were similar across the Chicago data set, as in Rome.\n\nVA Assessment Correction\n\nVisual acuity correction was classified as CVA, UCVA, or unspecified. Corrected VA refers to habitual correction (usually noted as “cc”). Out of all VA records in 2018, in both Rome and Chicago, a plurality was CVA, followed by slightly fewer unspecified, and the smallest fraction were UCVA. Measurement characteristics are shown in , , , and .\n\nTable 4\n\nMethodTypeAll (100.0%)CVA (31.33%)UCVA (18.96%)Unspecified (59.71%)Distance89 653 937 (48.3%)36 398 894 (62.6%)26 094 692 (74.2%)27 160 351 (29.4%)Near11 066 330 (6.0%)5 950 459 (10.2%)2 444 336 (6.9%)2 671 535 (2.9%)Unspecified84 915 304 (45.7%)15 809 903 (27.2%)6 649 418 (18.9%)62 455 983 (67.7%)\n\nTable 5\n\nMethodTypeAll (100.00%)CVA (39.06%)UCVA (23.41%)Unspecified (37.53%)Distance100 696 572 (59.6%)40 131 958 (60.8%)28 966 865 (73.3%)31 597 749 (49.8%)Near14 153 014 (8.4%)7 186 123 (10.9%)2 993 807 (7.6%)3 973 084 (6.3%)Unspecified54 070 463 (32.0%)18 669 813 (28.3%)7 578 178 (19.2%)27 822 472 (43.9%)\n\nIn aggregate, the mean of nonmissing modified logMAR measurements was similar for CVA and unspecified VA measurements in both Rome and Chicago (approximately 0.25 for each, corresponding to approximately 20/40 Snellen VA). Uncorrected VA was worse (mean, 0.35). Median logMAR was the same in both Rome and Chicago.\n\nNotably, among patient visit dates where both CVA and UCVA were recorded for the same eye, CVA was better than UCVA in the majority (> 60%) of cases in both Rome and Chicago. It was tied in approximately 20–25% of cases and recorded instead of UCVA in 10% to 15% of cases. ( ). Among patient visit dates where both CVA and unspecified VA were recorded for the same eye, CVA was most commonly tied or slightly worse than unspecified VA ( ).\n\nTable 6\n\nRome\n\nN (%)Chicago\n\nN (%)CVA versus UCVA CVA better than UCVA3 332 460 (64.3)5 943 414 (62.3) CVA same as UCVA1 239 915 (23.9)2 176 533 (22.8) CVA worse than UCVA614 146 (11.8)1 417 713 (14.9)CVA versus unspecified correction CVA better than unspecified187 433 (14.1)6 482 994 (17.8) CVA same as unspecified9 428 438 (41.8)14 368 639 (39.5) CVA worse than unspecified9 931 952 (44.0)15 542 319 (42.7)Distance versus unspecified distance/near Distance better than unspecified5 554 632 (37.6)7 167 775 (34.5) Distance same as unspecified4 025 703 (27.2)6 259 216 (30.1) Distance worse than unspecified5 195 903 (35.2)7 358 935 (35.40)Refraction-associated versus unrefracted Refraction better than unrefracted5 139 578 (50.7)10 200 264 (50.3) Refraction same as unrefracted4 293 141 (42.4)8 220 001 (40.6) Refraction worse than unrefracted702 198 (6.9)1 849 873 (9.1)Pinhole versus nonpinhole Pinhole better than nonpinhole13 866 597 (68.8)17 629 140 (62.2) Pinhole same as nonpinhole4 445 754 (22.1)6 800 762 (24.0) Pinhole worse than nonpinhole1 844 468 (9.2)3 912 848 (13.8)\n\nVA Assessment Method\n\nVisual acuity measurements were documented in the registry as obtained at distance, near, or unspecified. Among all 2018 nonmissing logMAR VA records, the majority were documented as distance measurements, followed by unspecified. Most CVA and UCVA measurements were documented at distance, however, the majority of unspecified correction measurements were also unspecified method (distance versus near) ( , ).\n\nThere were no logMAR or modified logMAR values for “near” in Rome; in these cases, modified logMAR and Snellen were always 999, indicating all near logMAR data were missing. However, the near value field was populated with Jaeger notation, with 48.0% (n = 10 228 548) having missing data, and among nonmissing data, the most frequent record was Jaeger 1 (J1; 54.2%, n = 6 001 698). Values ranged from J1 to J16, and 80.2% of values were either J1, J2, or J3. In Chicago, near measurements with associated logMAR values ranged from 0 to 1, with 2.8% (n = 389 286) recorded as 999. One hundred percent (14 153 014) of near VA records were associated with specific labeled Jaeger notation VAs. Of these near Jaeger records in Chicago, 0% were associated with distant or unspecified VAs. Jaeger values in Chicago also ranged from J1 to J16; 55.4% (7 835 559) of values were J1 and 80.7% (11 426 085) of values were either J1, J2, or J3.\n\nIn aggregate, distance VA measurements were similar to unspecified VA measurements. When comparing patient visit dates where both a distance and unspecified VA measurement were recorded in the same eye, the relationship between distance and unspecified VA was (approximately one-third in each category, for both Rome and Chicago) ( ). However, the difference between mean distance and mean unspecified measurements across all 2018 nonmissing records was small (only 0.02 logMAR in Rome and 0.01 logMAR in Chicago).\n\nVA Measurements Obtained with Refraction\n\nA “refraction” flag was defined as true or false for each VA measurement in both Rome and Chicago. Among all 2018 VA measurements, approximately 5% were associated with a refraction flag ( , ). Notably, among patient visit dates where both refraction-associated and unrefracted VA measurements were recorded for the same eye in Rome, refraction-associated VA was better than unrefracted VA in over half of cases in both Rome and Chicago, and it was seldom worse ( ). Unrefracted VA measurements could include corrected as well as uncorrected or unspecified VA measurements.\n\nVA Measurements Associated with Pinhole Assessment\n\nA “pinhole” flag was defined as true or false for each VA measurement in Rome and Chicago. Among all 2018 VA measurements, > 11% were obtained with pinhole ( , ). Pinhole-associated VA was better than without pinhole in > 60% of cases in both Rome and Chicago ( ).\n\nA “no improvement” (NI) flag was also defined as true or false for each VA measurement in Rome. However, a total of 1788 VA measurements (< 0.01%) were labeled as NI, all with logMAR of 2, 3, or 4. The NI flag was not included in Chicago.\n\nVA Refinement Algorithm\n\nTo develop a data-cleaning algorithm, we sought to identify representative corrected distance VA in the better-seeing eye at the patient level in 2018, as the closest representation of best-corrected VA in actual clinical practice (other than limiting analysis only to VA measurements associated with a refraction). We first excluded VA measurements obtained at near. Rome contained 0 values and Chicago’s 8.2% of near values were outliers (almost 3 × lower logMAR) compared with the distance values. These would introduce bias toward better vision in the proportion of patients with near VA values. Additionally, despite uncorrected values comprising of approximately 23% to 24% of records, we also excluded these measurements because their average logMAR was 0.11 less than corrected VA and would underestimate patients’ habitual vision. The algorithm retained refraction-associated and pinhole-flagged VA measurements. Unspecified VA measurements were evaluated and found to yield similar VA results as for CVA measurements. Because these accounted for the plurality of VA correction and method measurements and distance was assumed to be the default method of VA assessment, unspecified VA measurements were also retained. The PostgreSQL code for this filter can be found in the Supplemental Appendix (available at www.ophthalmologyscience.org) for both Rome and Chicago data sets. The code: (1) required each VA to be of type corrected/unspecified or a refraction-associated VA; (2) required each VA measurement method to be distance or unspecified; and (3) did not filter on pinhole or NI.\n\nTo validate our filter, we compared results to those with no filter. Approximately 19% of records were filtered in both Rome and Chicago data sets (n = 16 619 111 out of 88 692 944 in Rome, and 18 739 599 out of 98 262 368 in Chicago). Application of the filter reduced the impact of high-outlier VA measurements; on average in Rome after filtering, the mean logMAR measurement per visit date per eye was 0.25, whereas the average maximum logMAR measurement per visit date per eye was 0.33. In Chicago, on average the mean logMAR measurement per visit date per eye was 0.24 with our filter, compared with 0.32 when based on the maximum logMAR measurement per visit date per eye. Similarly, the SD of VA measurements was empirically narrower with filter application (0.47 prefilter vs. 0.23 postfilter in Rome, and 0.43 prefilter vs. 0.41 postfilter in Chicago).\n\nTo estimate representative VA in the better-seeing eye, we limited to the best VA measurement on each visit date in 2018 for each eye, condensed VA measurements to the mean of best per visit date values for each eye in 2018, and identified the better of the 2 eyes’ mean VA measurements.\n\nDiscussion\n\nThe IRIS Registry offers a myriad of research potential and has already enabled a variety of research questions, ranging from prevalence of rare disease7 to treatment patterns9 and surgical complication rates.5 The inclusion of VA measurements, which are not available from other large national clinical data sources such as insurance claims, is essential for studying visual impact and outcomes, which are of critical importance in ophthalmology. However, analysis and interpretation of VA data requires careful consideration of data limitations and metadata fields. To date, studies have used variable methodologies for cleaning VA data, and there is a lack of published data to help researchers and readers.8,9 Furthermore, there are 2 versions of the database: Rome, which was available to researchers through 2021, and Chicago, which became available mid-2021. It is important to understand how VA information is recorded and interpreted in IRIS Registry-based analyses; analyses may benefit from development of targeted algorithms to evaluate specific VA information.\n\nAs expected for a database derived from clinical practice, we found considerable evidence for inconsistencies in VA data. For example, an appreciable fraction of visit dates had duplicate VA records. Among Rome VA records, 10.8% shared identical metadata, which vastly outnumbered the 1.4% of Chicago records with duplicate metadata. On the other hand, Rome had slightly fewer visit dates with > 10 VA records (1.4%) compared with 1.6% in Chicago. In Rome, when both logMAR and modified logMAR were present, the difference between the 2 was negligible. In Chicago, only logMAR was present.\n\nAlthough we anticipated CVA to generally be better than UCVA when measurements were obtained on the same day, we were surprised to find CVA measurements worse than UCVA 12% of the time and worse than unspecified VA measurements 44% of the time in Rome (15% and 43% of the time in Chicago, respectively), when measurements were obtained on the same day. This perhaps represents documentation error, ingestion errors, difficult refraction exams, CVA collected after only autorefraction, confounders, and/or refraction performed late in a patient’s visit or after other examination, testing, or treatment. There may also be differences in VA assessment method (e.g., a visit may have CVA recorded without pinhole refinement and UCVA recorded based on pinhole) and we suspect that many VA measurements recorded as unspecified actually reflect CVA, given similarities in aggregate characteristics of the 2 groups. Particularly because most unspecified correction measurements also had unspecified distance versus near measurement method, it is likely that some EHR systems may not retain correction status and measurement method in structured data fields. Furthermore, relationships between data may not always be as expected. For example, for visit dates that had both refraction-associated and non-refraction VA measurements or pinhole and nonpinhole measurements on the same day, respectively, 7% (Rome) and 9% (Chicago) of nonrefracted VA measurements were better than refracted and 9% (Rome) and 14% (Chicago) of the nonpinhole VAs were better than pinhole.\n\nOf note, there were no major differences between the Rome and Chicago databases for 2018 VA data aggregate statistics. Chicago average VAs were slightly lower than in Rome, likely because: (1) near VAs were also associated with logMARs and the average near logMAR was 0.11, significantly lower than the overall average logMAR of 0.26; and (2) HM and LP were assigned logMAR values of 2.3 and 2.7 in Chicago versus 3.0 for each in Rome, causing a leftward shift in the mean Chicago logMAR.\n\nRome contained more VA records; however, Chicago contained more VA records with nonmissing VA data. Chicago also contained slightly more visit dates and patients. Mean, SD, and median logMAR for all laterality, correction, distance, pinhole, and refraction measurements were nearly identical between Rome and Chicago, as expected, supporting the fidelity of the IRIS Registry data set. Chicago contained fewer duplicate values but still had a nontrivial number of duplicate records that required data cleaning. The frequency of atypical pairings (i.e., some unrefracted VA better than refracted VA in the same eye on the same day, or some nonpinhole values better than pinhole values on the same day) were similar between Rome and Chicago.\n\nBased on these caveats, we propose applying selective algorithms to filter VA values based on study purpose. For example, because most analyses of visual outcomes emphasize corrected and distance VA, we suggest selecting the best logMAR value from a filtered subset of CVA/unspecified measurements obtained at distance/unspecified, including refractions and not filtering on pinhole or NI flags. Unspecified VA measurements were very similar to CVA in aggregate. We suggest excluding UCVA (or evaluating UCVA measurements on a case-by-case basis) because when UCVA and CVA were not identical, UCVA was worse than CVA 84% (Rome) and 81% (Chicago) of the time. We also suggest excluding near VA measurements because in Rome all corresponding logMARs are null and in Chicago Jaeger charts may demonstrate too much variability (empirically demonstrating much lower vision than distance values).11,12 And we suggest neither including nor excluding on the presence of pinhole and NI flags due to clinical relevance/measurement consistency as well as record sparsity (pinhole flags were present in 12% [Rome] and 11% [Chicago], and NI flags present in < 1% [Rome] and 0% [Chicago]). Issues of data duplicates or misclassification can be addressed by selecting the best VA measurement in the filtered subset and flattening an arbitrary number of VA measurements to a single entry per visit date per eye.\n\nWhere possible, depending on the research question, we suggest reducing the impact of data entry errors or outliers by measuring VA per eye over a span of time rather than at a single time point. For example, running a data selection algorithm per date per eye, identifying the best VA measurement in each eye per visit date (e.g., corrected or unspecified), and retaining the mean of these values. Unlike median, utilizing mean values: (1) considers measurements that are better or worse, giving weight to notable decline or improvement in vision; (2) smoothes out the impact of each individual measurement; and (3) is less susceptible to outliers than minimum or maximum filters. We recently used this approach for an epidemiologic study assessing for blindness over a calendar year (Brant, unpublished data). Approaches may be more complex for research questions evaluating clinical outcomes or changes over time that require reliable repeat measurements, necessitating more careful validation of VA measurements. Currently IRIS Registry VA data are specific to a given date, but not a particular visit, information that may be difficult to glean even from EHR review (e.g., multiple measurements on the same date could represent encounters with different providers in a multispecialty group, or appointments for a provider visit and imaging/testing, respectively). However, for most analyses, data scientists will need to map VA measurements into a composite value for analysis. We expect that most IRIS research questions would benefit from availability of standardized filtered eye-level VA data in addition to the more detailed, unfiltered VA data. We have shared our algorithm, recognizing that site-specific implementation may yield inconsistencies. Current Verana Health efforts to improve the reliability of IRIS Registry VA data may support the eventual integration of standardized filtered VA in the IRIS Registry research extracts.\n\nThe etiology of data errors and inconsistencies in IRIS Registry VA measurements is likely multifactorial, reflecting many of the expected challenges inherent to data reflecting actual clinical practice, including ophthalmic technician variability, data entry errors, patient inconsistency, and ingestion pipeline bugs or value classification errors. Current work to integrate data extraction and transformation under a single entity, the same registry vendor as for data analysis (Verana Health), will improve transparency and reliability of data ingestion and transformation going forward. Although large data sets enable smoothing of noise, if the distribution of data is shifted due to underlying biases, smoothing and aggregation algorithms may still propagate the bias. Because the IRIS Registry contains data for most United States ophthalmology patients, most studies have large sample size, and analysis of thousands or millions of patient records lessens the impact of individual entry errors. However, to further minimize the influence of erroneous data and strengthen data interpretability, the IRIS Registry data set is actively curated with implementation of continuous enhancements to data-cleaning methods, algorithms to filter outliers and strengthen data interpretability, and application of clinical logic to understand results. In addition, researchers can and should employ specific analysis methods to more specifically infer the VA records of interest for a particular research question. We recommend that researchers prefilter and aggregate VAs using an approach as described above. Our suggested approach further ameliorates the influence of outliers.\n\nIn summary, the IRIS Registry offers the unique opportunity to study vision for millions of United States patients. Given the complexity of documentation, VA data are subject to expected challenges. Although these considerations do not eclipse the overall unprecedented strengths of the data set to answer research questions, particular care and attention is required to design studies and clean the data to ensure consistency and reliability of analyses. We propose a data filtering strategy and present an example algorithm for excluding outliers and aggregating duplicate data, informed by the underlying caveats of the data set. We recommend that future researchers standardize and document their methodology to improve accuracy and reproducibility of IRIS Registry analyses. This is a topic that warrants careful attention, and because VA coding, logic, and content are likely to evolve in the future, it also warrants iterative review."
    }
}
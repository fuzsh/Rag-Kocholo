{
    "id": "dbpedia_6687_2",
    "rank": 73,
    "data": {
        "url": "https://patents.google.com/patent/US11393361B1/en",
        "read_more_link": "",
        "language": "en",
        "title": "US11393361B1 - Braille reader system using deep learning framework - Google Patents",
        "top_image": "",
        "meta_img": "",
        "images": [
            "https://patentimages.storage.googleapis.com/1a/57/ec/5c2d968b917b53/US11393361-20220719-D00000.png",
            "https://patentimages.storage.googleapis.com/8f/89/1c/e7512d5b374956/US11393361-20220719-D00001.png",
            "https://patentimages.storage.googleapis.com/f3/ea/49/0ae38b35579811/US11393361-20220719-D00002.png",
            "https://patentimages.storage.googleapis.com/8b/cc/6e/d70aac98613da7/US11393361-20220719-D00003.png",
            "https://patentimages.storage.googleapis.com/75/9a/e0/09c9403e7b604a/US11393361-20220719-D00004.png",
            "https://patentimages.storage.googleapis.com/45/b4/bf/9e1e804513e58b/US11393361-20220719-D00005.png",
            "https://patentimages.storage.googleapis.com/e8/ae/d0/1d385b2011bec3/US11393361-20220719-D00006.png",
            "https://patentimages.storage.googleapis.com/b8/a4/7e/e6bbbcce8a7b6c/US11393361-20220719-D00007.png",
            "https://patentimages.storage.googleapis.com/a1/b4/d7/55fb445e1a2837/US11393361-20220719-D00008.png",
            "https://patentimages.storage.googleapis.com/0a/4b/b5/283805742fd253/US11393361-20220719-D00009.png",
            "https://patentimages.storage.googleapis.com/4d/e9/a5/b2a83074de9d73/US11393361-20220719-D00010.png"
        ],
        "movies": [],
        "keywords": [],
        "meta_keywords": [
            ""
        ],
        "tags": null,
        "authors": [],
        "publish_date": "2021-12-10T00:00:00",
        "summary": "",
        "meta_description": "A device, method, and system for converting printed Braille dots to speech. A Braille image of the printed Braille dots is captured by a digital camera mounted on a 3D ring case. Data processing and one or more image recognition operations are performed by a microprocessor to match the Braille image to a textural character corresponding to the Braille image. The textural character is converted to an audio waveform. The audio waveform is transmitted to a speaker. The speaker generates a sound representative of a spoken word corresponding to the textural character.",
        "meta_lang": "en",
        "meta_favicon": "",
        "meta_site_name": "",
        "canonical_link": "https://patents.google.com/patent/US11393361B1/en",
        "text": "Braille reader system using deep learning framework Download PDF\n\nInfo\n\nPublication number\n\nUS11393361B1\n\nUS11393361B1 US17/668,964 US202217668964A US11393361B1 US 11393361 B1 US11393361 B1 US 11393361B1 US 202217668964 A US202217668964 A US 202217668964A US 11393361 B1 US11393361 B1 US 11393361B1\n\nAuthority\n\nUS\n\nUnited States\n\nPrior art keywords\n\nbraille\n\nimage\n\narabic\n\nsegments\n\nmicroprocessor\n\nPrior art date\n\n2021-12-10\n\nLegal status (The legal status is an assumption and is not a legal conclusion. Google has not performed a legal analysis and makes no representation as to the accuracy of the status listed.)\n\nActive\n\nApplication number\n\nUS17/668,964\n\nInventor\n\nGhazanfar LATIF\n\nJaafar Alghazo\n\nShurouq Alufaisan\n\nWafa Albur\n\nShaikha Alsedrah\n\nCurrent Assignee (The listed assignees may be inaccurate. Google has not performed a legal analysis and makes no representation or warranty as to the accuracy of the list.)\n\nPrince Mohammad Bin Fahd University\n\nOriginal Assignee\n\nPrince Mohammad Bin Fahd University\n\nPriority date (The priority date is an assumption and is not a legal conclusion. Google has not performed a legal analysis and makes no representation as to the accuracy of the date listed.)\n\n2021-12-10\n\nFiling date\n\n2022-02-10\n\nPublication date\n\n2022-07-19\n\n2022-02-10 Application filed by Prince Mohammad Bin Fahd University filed Critical Prince Mohammad Bin Fahd University\n\n2022-02-10 Priority to US17/668,964 priority Critical patent/US11393361B1/en\n\n2022-02-10 Assigned to Prince Mohammad Bin Fahd University reassignment Prince Mohammad Bin Fahd University ASSIGNMENT OF ASSIGNORS INTEREST (SEE DOCUMENT FOR DETAILS). Assignors: ALBUR, WAFA, ALSEDRAH, SHAIKHA, ALUFAISAN, SHUROUQ, ALGHAZO, JAAFAR, LATIF, GHAZANFAR\n\n2022-07-19 Application granted granted Critical\n\n2022-07-19 Publication of US11393361B1 publication Critical patent/US11393361B1/en\n\nStatus Active legal-status Critical Current\n\n2042-02-10 Anticipated expiration legal-status Critical\n\nLinks\n\nUSPTO\n\nUSPTO PatentCenter\n\nUSPTO Assignment\n\nEspacenet\n\nGlobal Dossier\n\nDiscuss\n\nImages\n\nClassifications\n\nG—PHYSICS\n\nG09—EDUCATION; CRYPTOGRAPHY; DISPLAY; ADVERTISING; SEALS\n\nG09B—EDUCATIONAL OR DEMONSTRATION APPLIANCES; APPLIANCES FOR TEACHING, OR COMMUNICATING WITH, THE BLIND, DEAF OR MUTE; MODELS; PLANETARIA; GLOBES; MAPS; DIAGRAMS\n\nG09B21/00—Teaching, or communicating with, the blind, deaf or mute\n\nG09B21/001—Teaching or communicating with blind persons\n\nG09B21/007—Teaching or communicating with blind persons using both tactile and audible presentation of the information\n\nG—PHYSICS\n\nG09—EDUCATION; CRYPTOGRAPHY; DISPLAY; ADVERTISING; SEALS\n\nG09B—EDUCATIONAL OR DEMONSTRATION APPLIANCES; APPLIANCES FOR TEACHING, OR COMMUNICATING WITH, THE BLIND, DEAF OR MUTE; MODELS; PLANETARIA; GLOBES; MAPS; DIAGRAMS\n\nG09B21/00—Teaching, or communicating with, the blind, deaf or mute\n\nG09B21/001—Teaching or communicating with blind persons\n\nG09B21/008—Teaching or communicating with blind persons using visual presentation of the information for the partially sighted\n\nG—PHYSICS\n\nG09—EDUCATION; CRYPTOGRAPHY; DISPLAY; ADVERTISING; SEALS\n\nG09B—EDUCATIONAL OR DEMONSTRATION APPLIANCES; APPLIANCES FOR TEACHING, OR COMMUNICATING WITH, THE BLIND, DEAF OR MUTE; MODELS; PLANETARIA; GLOBES; MAPS; DIAGRAMS\n\nG09B21/00—Teaching, or communicating with, the blind, deaf or mute\n\nG09B21/001—Teaching or communicating with blind persons\n\nG09B21/006—Teaching or communicating with blind persons using audible presentation of the information\n\nDefinitions\n\nthe present disclosure is directed to a Braille reader system including a deep learning framework which converts printed Braille dots to speech.\n\nBraille is a form of written language used by visually impaired persons to read a document consisting of raised dots.\n\ncharacters or words are represented by patterns of raised dots that are felt with the fingertips.\n\nBraille script is comprised of Braille cells, each representing a character or a word.\n\na Braille sheet is a paper embossed with raised dots that are arranged in Braille cells.\n\na Braille cell is represented by a collection of 6 raised dots arranged in two columns of 3 dots each.\n\nBraille script may be presented on Braille paper, on a refreshable Braille display device with dots or round-tipped pins raised through holes in a flat surface, by a Braille input device, and on public notices and signboards.\n\nVarious computer vision and image processing techniques may be used to recognize the characters or words from scanned documents and images of Braille script and convert the recognized characters or words to speech. Such techniques may assist a visually disabled person, an instructor or trainer in reading the Braille scripts.\n\na contact type image sensor that can be worn on a finger and configured to superimpose multiple images obtained from the contact type image sensor to obtain a composited image, convert the composited image into a Braille pattern, and convert the Braille pattern into a corresponding voice was described in JP2011070530A, âContact type image sensor and image recognition deviceâ, incorporated herein by reference in its entirety.\n\nthis reference has a drawback that the contact type image sensor comes in contact with the Braille pattern instead of the finger of the user. This drawback may hinder the learning experience of the blind person, as the tactile feedback from touching the Braille dots serves to orient the hand towards the next Braille word.\n\na mobile communication terminal to convert Braille points into voice by taking a picture of Braille points through a camera and recognize the Braille image to output the Braille image as a voice was described in KR2007057351A, âMobile communication terminal for converting Braille points into a voice, especially concerned with recognizing the Braille points to output the recognized Braille points as the voiceâ, incorporated herein by reference in its entirety.\n\nKR2007057351A Mobile communication terminal for converting Braille points into a voice, especially concerned with recognizing the Braille points to output the recognized Braille points as the voice\n\na semantic segmentation model trained by using a convolutional neural network and Braille images are input into the semantic segmentation model which obtains a semantic segmentation result diagram of the Braille image was described in CN110298236A, incorporated herein by reference in its entirety. However, this reference outputs images of the Braille pattern which are not readable by a blind person.\n\nDCNN deep convolutional neural network\n\na system for converting printed Braille dots to speech includes a 3D ring case and a digital camera mounted in the 3D ring case.\n\nthe digital camera is configured to capture a Braille image of the printed Braille dots.\n\nthe system further includes a rechargeable battery and a speaker.\n\nthe system further includes a microprocessor operatively connected to the rechargeable battery, the digital camera, and the speaker.\n\nthe microprocessor is configured to perform data processing and one or more image recognition operations which match the Braille image to a textural character corresponding to the Braille image.\n\nthe microprocessor is further configured to convert the textural character to an audio waveform and transmit the audio waveform to the speaker.\n\nthe speaker is configured to receive the audio waveform and generate a sound representative of a spoken word corresponding to the textural character.\n\na method for converting printed Braille dots to speech includes capturing a Braille image of the printed Braille dots. The method further includes matching the Braille image to a textural character corresponding to the Braille image. Data processing and one or more image recognition operations are performed to match the Braille image to the textural character. The method further includes converting the textural character to an audio waveform and transmitting the audio waveform to a speaker. The method further includes generating, by the speaker, a sound representative of a spoken word corresponding to the textural character.\n\na method for converting printed Braille dots to speech includes directing a lens of a digital camera towards Braille text including printed Braille dots.\n\nthe digital camera is confined in a 3D ring case.\n\nthe method further includes compressing a push button to capture a Braille image of the printed Braille dots.\n\nthe method further includes matching the Braille image to a textural character corresponding to the Braille image.\n\na microprocessor is configured to perform data processing and one or more image recognition operations to match the Braille image to the textural character.\n\nthe method further includes converting the textural character to an audio waveform and transmitting the audio waveform to a speaker.\n\nthe method further includes generating, by the speaker, a sound representative of a spoken word corresponding to the textural character.\n\nFIG. 1 is a schematic diagram of a Braille reading device, according to certain embodiments.\n\nFIG. 2 is a schematic diagram of a 3D ring case, according to certain embodiments.\n\nFIG. 3A is a schematic diagram of a microprocessor housing, where (a) is a base and (b) is a lid of the microprocessor housing, according to certain embodiments.\n\nFIG. 3B is a schematic diagram of a microprocessor housing, according to certain embodiments.\n\nFIG. 4 is a process flow diagram illustrating a conversion of printed Braille dots to speech, according to certain embodiments.\n\nFIG. 5 is an exemplary process flow of a method for converting printed Braille dots to speech, according to certain embodiments.\n\nFIG. 6 is an exemplary process flow of a method for converting printed Braille dots to speech, according to certain embodiments.\n\nFIG. 7 is an illustration of a non-limiting example of details of computing hardware used in the computing system, according to certain embodiments.\n\nFIG. 8 is an exemplary schematic diagram of a data processing system used within the computing system, according to certain embodiments.\n\nFIG. 9 is an exemplary schematic diagram of a processor used with the computing system, according to certain embodiments.\n\nthe terms âapproximately,â âapproximate,â âabout,â and similar terms generally refer to ranges that include the identified value within a margin of 20%, 10%, or preferably 5%, and any values therebetween.\n\naspects of this disclosure are directed to a system, device, and method for converting printed Braille dots to speech.\n\nthe present disclosure discloses a device to be worn by a user.\n\nthe device includes a camera to capture images of printed Braille dots.\n\nthe device includes a microprocessor that processes the captured images of printed Braille dots to determine textural characters or words corresponding to the images of printed Braille dots.\n\nFIG. 1 depicts a schematic diagram of a Braille reading device 100 for reading Braille scripts, according to exemplary aspects of the present disclosure.\n\nthe Braille reading device 100 includes a three-dimensional (3D) ring case 102 , a microprocessor housing 104 , a digital camera 106 , a microprocessor board 108 , a switch 110 , a flexible flat cable (FFC) 112 , a speaker 114 , a strap 115 , and a pair of connecting wires 116 .\n\n3D three-dimensional\n\nthe 3D ring case 102 includes a first ring 203 and a second ring 204 .\n\nthe second ring is positioned below the first ring.\n\nthe digital camera 106 is arranged in the first ring.\n\nthe 3D ring case 102 is worn on a finger of a user by inserting the finger of the user in the second ring.\n\nthe switch 110 is arranged on the 3D ring case 102 .\n\nthe 3D ring case 102 is explained in further detail with reference to FIG. 2 .\n\nthe microprocessor housing 104 may include a base 303 and a lid 305 .\n\nthe base holds the microprocessor board 108 .\n\nthe lid may be placed on top of the base to enclose components arranged on the base.\n\na top side of the lid includes a cut-out portion to receive the FFC 112 .\n\nthe microprocessor board 108 enclosed in the microprocessor housing 104 , is connected to the digital camera 106 by the FFC 112 passing through the cut-out portion.\n\nthe base includes the strap 115 (shown as 315 , FIG. 3B ) for the user to wear the microprocessor housing 104 on the wrist of the user.\n\nthe lid further includes another cut-out portion on a sidewall (or a lateral side) to hold the speaker 114 .\n\nthe speaker 114 may be arranged within the microprocessor housing 104 .\n\nthe microprocessor housing 104 is explained in further detail with reference to FIG. 3A and FIG. 3B .\n\nthe Braille reading device 100 further includes a battery (not shown) to power one or more of the digital camera 106 , the microprocessor board 108 , and the speaker 114 .\n\nthe battery may be arranged in the microprocessor housing 104 .\n\nthe battery may be a rechargeable battery.\n\nthe microprocessor housing 104 may include a power interface that may be connected to a power charging adapter to charge the rechargeable battery.\n\nthe battery may be replaced with a new battery when the battery runs out of power.\n\nthe pair of connecting wires 116 connects the switch 110 to the microprocessor board 108 .\n\nthe pair of connecting wires 116 passes through the cut-out portion on the top side of the lid to connect the switch 110 to the microprocessor board 108 enclosed in the microprocessor housing 104 .\n\nthe microprocessor board 108 may include General-Purpose Input/Output (GPIO) pins.\n\nGPIO General-Purpose Input/Output\n\nthe GPIO pins provide a physical interface between the microprocessor board 108 and external components.\n\nOne end of the pair of connecting wires 116 is connected to the switch 110 , and the other end of the pair of connecting wires 116 is connected to the GPIO pins.\n\nthe switch 110 activates the Braille reading device 100 .\n\nthe user may wear the 3D ring case 102 on a finger of either hand and wear the microprocessor housing 104 on the wrist of the same hand.\n\nthe user may scan the Braille script by sensing printed Braille dots with the finger.\n\nthe user may press the switch 110 to activate the Braille reading device 100 to identify one or more characters or words corresponding to the printed Braille dots sensed by the finger.\n\nthe digital camera 106 captures a Braille image of the Braille dots.\n\nthe Braille image is transmitted to the microprocessor board 108 over the FFC 112 .\n\nthe microprocessor board 108 comprises a microprocessor that performs one or more image processing techniques or algorithms to process the Braille image for image recognition.\n\nthe microprocessor may input the processed Braille image to a trained deep learning based convolutional neural network (CNN) model.\n\nthe trained deep learning based CNN model is trained on a dataset of images of Braille scripts.\n\nthe trained deep learning based CNN model accepts the processed Braille image, extract features from the processed Braille image, and classifies them into respective classes to recognize one or more characters or words corresponding to the Braille image.\n\nthe microprocessor generates an audio waveform corresponding to the recognized characters or words and transmits the audio waveform to the speaker 114 .\n\nthe speaker 114 outputs the sound corresponding to the audio waveform.\n\nthe microprocessor board 108 may be one of a Raspberry Pi series or other similar single-board computers.\n\nthe microprocessor board 108 may include the processor, the GPIO pins to provide a physical interface between the microprocessor board 108 and external components, memory module such as a random access memory (RAM), one or more HDMI ports, camera serial interface (CSI) to connect the digital camera 106 through the FFC 112 , one or more USB ports to connect peripheral devices and power supply, one or more micro USB ports, and one or more wireless connectivity modules, such as Bluetooth and Wi-Fi and secure digital (SD) card slot.\n\nRAM random access memory\n\nCSI camera serial interface\n\nUSB ports to connect peripheral devices and power supply\n\nmicro USB ports to connect peripheral devices and power supply\n\nwireless connectivity modules such as Bluetooth and Wi-Fi and secure digital (SD) card slot.\n\nthe Braille reading device 100 may be communication-enabled using various wired and wireless connectivity protocols, such as Wi-Fi and Personal Area Network.\n\nthe processor of the Braille reading device 100 may load the trained deep learning based CNN model from a host machine or a server.\n\nFIG. 2 depicts a schematic diagram of a 3D ring case 202 , according to exemplary aspects of the present disclosure.\n\nthe 3D ring case 202 is designed to be worn on a finger of the user and to accommodate the digital camera 106 and the switch 110 .\n\nthe 3D ring case 202 corresponds to the 3D ring case 102 of FIG. 1 .\n\nthe 3D ring case 202 includes a first opening 203 , a second opening 204 below the first opening 203 , and a switch 206 .\n\nthe digital camera 106 may be arranged in the first opening 203 .\n\nthe 3D ring case 202 may be worn on a finger of the user by inserting the finger of the user in the second opening 204 .\n\nthe switch 206 may be arranged on either of the lateral sides of the 3D ring case 202 .\n\nfirst opening 203 and the second opening 204 as shown in FIG. 2 are round in shape, one or both the first opening 203 and the second opening 204 may be of any other shape depending on design requirements.\n\nshape and size of the first opening 203 may be designed based on shape and size of the digital camera 106 to be arranged in the first opening 203 .\n\nthe 3D ring case 202 may be custom designed to provide a slot, in place of the switch 206 , for accommodating the switch 110 on the 3D ring case 202 .\n\nthe 3D ring case 202 may be custom designed to have dimensions in accordance with requirements and components of Braille reading device 100 .\n\nthe 3D ring case 202 may be printed by a 3D printer.\n\nthe 3D ring case 202 may be designed in a plurality of sizes for the second opening 204 .\n\nthe 3D ring case 202 may be designed for three different sizes, i.e., small, medium, and large size of the second opening 204 , which can be selected to fit a finger size of a user.\n\nthe 3D ring case 202 may be designed such that size of the second opening 204 may be adjusted to fit size of the finger of the user.\n\nthe 3D ring case 202 may be custom designed in accordance with physical appearance preferences of different users.\n\nFIG. 3A depicts a schematic diagram of a microprocessor housing 304 , where (a) is a base and (b) is a lid of the microprocessor housing, according to exemplary aspects of the present disclosure.\n\nthe microprocessor housing 304 is designed to accommodate the microprocessor board 108 , the speaker 114 , and the battery.\n\nthe microprocessor housing 304 corresponds to the microprocessor housing 104 of FIG. 1 .\n\nthe microprocessor housing 300 includes a base 303 and a lid 305 .\n\nthe base 303 is designed to hold the microprocessor board 108 and the battery.\n\nthe base 303 includes a strap 315 .\n\nthe strap 315 is used to wear the microprocessor housing 304 on a wrist of the user as shown in FIG. 3B .\n\nthe lid 305 includes a plurality of cutout portions 308 on sidewalls 306 (or lateral sides).\n\nthe plurality of cut-out portions 308 may enable air flow in the microprocessor housing 304 .\n\nAt least one of the plurality of cut-out portions 308 is used to arrange the speaker 114 .\n\na top side of the lid 305 includes a cut-out portion 310 .\n\nthe FFC 112 passes through the cut-out portion 310 to connect the digital camera 106 to the microprocessor board 108 .\n\nthe plurality of cut-out portions 308 and the cut-out portion 310 as shown in FIG. 3A are round in shape, the plurality of cut-out portions 308 and the cut-out portion 310 may be of any other shape depending on design requirements.\n\nthe base 303 may include coupling means, such as fasteners, hooks, clips, and buckles to couple the strap 315 with the base 303 .\n\nthe coupling means may be arranged on exterior sides or exterior bottom of the base 303 .\n\nthe base 303 may include slots on opposite side and the strap 315 may pass through the slots.\n\nthe microprocessor housing 304 may be custom designed to have dimensions in accordance with requirements and components of Braille reading device 100 .\n\nthe microprocessor housing 304 may be printed by a 3D printer.\n\nthe microprocessor housing 304 may be designed to have a slim profile such that the microprocessor housing 304 can be hidden under sleeve of garment worn by the user.\n\nthe microprocessor housing 304 may be designed in the shape of one or more cartoon characters, superhero characters, animals, or may be custom designed to any desired shape and size, so as to appeal to children learning Braille.\n\nFIG. 4 depicts a process flow diagram 400 illustrating conversion of Braille dots to speech, according to exemplary aspects of the present disclosure.\n\na user may wear the Braille reading device 100 .\n\nthe user may wear the microprocessor housing 104 on a wrist and the 3D ring case 102 on a finger.\n\nthe user may scan a Braille script 402 with the finger to sense Braille dots.\n\nthe Braille script 402 may be presented on one or more of: a Braille paper or textbook with printed dots, a refreshable Braille display device with dots or round-tipped pins raised through holes in a flat surface, a Braille input device, and public notices, bathroom doors, and signboards and the like.\n\nthe user may wear the 3D ring case 102 on the index finger of a hand.\n\nthe user may press the switch 110 with the thumb of the same hand.\n\nthe user may wear the 3D ring case 102 in any finger on either hand, and use any finger on either hand to press the switch 110 .\n\nthe user may wear the 3D ring case 102 on the same finger with which the user senses the Braille dots.\n\nOne or more of focal length, position, and optical angle of the digital camera 106 may be appropriately adjusted to capture images of the Braille dots sensed by the user.\n\nthe user may identify the location of the Braille dots. After the user identifies the location of the Braille dots, the user actuates the digital camera 106 . In an aspect of the present disclosure, after sensing the Braille dots, the user may move the finger behind the Braille dots such that the finger does not overlap the Braille dots and the Braille dots are in the field of view of the digital camera 106 . In an aspect of the present disclosure, optical axis and/or field of view of the digital camera 106 may be oriented to capture Braille images of the Braille dots in one or more directions with respect to the finger of the user. The digital camera 106 may capture Braille images of the Braille dots above the finger and on the right and left side of the finger.\n\nthe microprocessor may be configured to adjust the optical axis and/or field of view of the digital camera 106 based on user preferences.\n\nthe user may manually adjust the optical axis and/or field of view of the digital camera 106 by adjusting position and/or orientation of the digital camera 106 in the first opening 203 .\n\nthe digital camera 106 transmits the Braille image to the microprocessor included in the microprocessor board 108 over the FFC 112 .\n\nthe digital camera 106 may begin to capture the Braille images at predetermined time intervals.\n\nthe user may press the switch 110 again to stop capturing the Braille images.\n\nthe microprocessor may be configured to capture the Braille images at the predetermined time intervals.\n\nthe predetermined time interval may be adjusted based on reading speed and/or proficiency of the user.\n\nthe microprocessor may apply one or more digital image processing and/or computer vision techniques or algorithms to process the Braille image.\n\nthe Braille script 402 is comprised of a plurality of Braille cells, each representing a character or a word.\n\na Braille cell is represented by a collection of 6 raised dots arranged in two columns, each having three dots.\n\nthe Braille image captured by the digital camera 106 may include one or more Braille cells, each representing a character or a word.\n\nthe microprocessor processes the Braille image by applying an image segmentation process 404 to the Braille image. Applying the image segmentation process 404 to the Braille image segments the Braille image into a plurality of segments. The plurality of segments in the segmented Braille image distinguishes individual Braille cells in the Braille image.\n\nthe image segmentation process 404 may also distinguish the Braille cells from the background in the captured Braille image.\n\none or more image segmentation algorithms may be used to distinguish the Braille cells from the background and to distinguish individual Braille cells across a single line and/or multiple lines in the Braille image.\n\nthe microprocessor may further process the Braille image by applying an image noise reduction process 406 to the segmented Braille image.\n\nthe quality of the Braille image captured by the digital camera 106 may be degraded due to various factors, such as a non-uniform ambient illumination, a low resolution imaging sensor, and an impulse noise.\n\nthe image noise reduction process 406 may apply one or more image enhancement and denoising techniques to remove noise and enhance the quality of the segmented Braille image.\n\nthe output of the image noise reduction process 406 is a denoised segmented Braille image.\n\nthe microprocessor may further process the Braille image by applying an image resizing process 408 to the denoised segmented Braille image.\n\nthe plurality of segments in the denoised segmented Braille image may be appropriately resized for recognition of character or word corresponding to each of the plurality of Braille cells.\n\nOne or more image interpolation and resizing techniques may be used to resize the plurality of denoised segments in the Braille image to generate a resized Braille image.\n\nthe microprocessor performs an image recognition process 410 on the resized Braille image.\n\nthe resized Braille image is input to the trained deep learning based CNN model.\n\nthe image recognition process 410 is performed to recognize characters or words corresponding to each of the plurality of Braille cells, i.e., the plurality of segments.\n\nthe deep learning based CNN model may be trained on a dataset of Braille scripts.\n\nthe dataset of Braille scripts which train the deep learning based CNN model may include a plurality of labeled images of Braille scripts in one or more languages.\n\nthe plurality of labeled images of Braille scripts may include alphabets, numerals, and text.\n\ndataset of Braille scripts may include one or more of Arabic Braille numerals, Arabic Braille alphabet, Arabic Braille text, Arabic numerals, Arabic alphabet, Arabic text, English Braille numerals, English Braille alphabet, English Braille text, English numerals, English alphabet and English text.\n\nthe plurality of labeled images of Braille scripts generates a plurality of Braille script recognition classes.\n\nthe dataset of Braille scripts is not limited to English or Arabic and may also include numerals, alphabet and text in any language in which Braille is used.\n\nthe resized Braille image is input to the trained deep learning based CNN model to recognize characters or words corresponding to the Braille image.\n\nthe trained deep learning based CNN model is a sequence of convolutional layer and pooling layer. The important features of the Braille image are kept in the convolution layers and intensified in the pooling layers and kept over the network, while discarding all the unnecessary information.\n\nthe convolutional layers and pooling layers are consecutively connected to extract features of the resized Braille image to generate a features map.\n\na flattening function may then be applied to the features map.\n\nthe flattened features map may be passed through a neural network of fully connected layers or dense layers followed by an output layer.\n\nthe Braille script recognition class is determined for the resized Braille image input to the trained deep learning based CNN model. The image recognition is complete, and the text corresponding to the captured Braille image is determined.\n\nthe processor may generate or extract an audio waveform of the text corresponding to the captured Braille image and transmit the audio waveform to the speaker 114 .\n\nthe speaker 114 may generate a sound corresponding to the audio waveform of the text corresponding to the captured Braille image.\n\nthe microprocessor may be configured to change the sound corresponding to the audio waveform. For example, the user may select the sound to be a man's voice, a woman's voice, a child's voice, a cartoon character's voice, and any other voice from a selection of voice options.\n\nthe microprocessor may connect to a computing device, such as a mobile phone or a server computing device to configure the sound of the Braille reading device 100 .\n\nthe Braille reading device 100 may pair with a mobile computing device running an application corresponding to the Braille reading device 100 .\n\nthe user may adjust one or more configurations of the Braille reading device 100 via the application running on the mobile computing device.\n\nthe application may provide a user interface to present the learning progress of the user.\n\na display device may be connected to the digital camera 106 . At least one of a written text or a picture representative of the text corresponding to the captured Braille image may be displayed on the display device.\n\nthe trained deep learning based CNN model may be implemented using open source Python libraries, such as TensorFlow, Keras, NumPy, and OpenCV.\n\nthe microprocessor may be configured to run a programming code written in a programming language, such as R and Python. Running the programming code may cause the microprocessor to run the deep learning based CNN model to start the recognition process of the captured Braille image, and play an audio representation of the recognized text corresponding to the captured Braille image.\n\nthe trained deep learning based CNN model may be programmed on a host machine and configured on the microprocessor. The microprocessor may connect with the host machine to receive any updates related to the trained deep learning based CNN model and/or digital image processing.\n\naspects of the present disclosure may enable a user, such as a visually impaired person or any other person reading or learning the Braille script to understand Braille scripts without assistance of a trainer.\n\nthe user may sense the Braille dots and activate the digital camera 106 to capture the Braille image.\n\nthe processor may process the captured Braille image and access the trained deep learning based CNN model to recognize text corresponding to the Braille image. Sounds of the recognized text may be played by the speaker 114 and/or displayed on a display screen.\n\nthe Braille reading device 100 of the present disclosure provides real-time learning experience for the user with minimal or no assistance.\n\nthe deep learning based CNN model may be trained on a dataset of Arabic braille numerals, alphabet, and text. Approximately 50,500 images may be collected for the dataset consisting of 10 classes for numerals, 28 classes for alphabet, and 60 classes of words in the Arabic language. Similar concept for training the deep learning based CNN model may be applied for other languages, thus, making the Braille reading device 100 compatible with any other language.\n\nthe microprocessor may load the trained deep learning based CNN model for any other language to recognize text corresponding to Braille in that language.\n\nFIG. 5 shows an exemplary process flow 500 of the present invention illustrating a method for converting printed Braille dots to speech, according to exemplary aspects of the present disclosure.\n\nthe method includes capturing a Braille image of the printed Braille dots.\n\nthe method includes matching the Braille image to a textural character corresponding to the Braille image.\n\nData processing and one or more image recognition operations may be performed to match the Braille image to the textural character.\n\nthe method includes converting the textural character to an audio waveform.\n\nthe method includes transmitting the audio waveform to the speaker 114 .\n\nthe method includes generating a sound representative of a spoken word corresponding to the textural character.\n\nthe speaker 114 generates the sound representative of the spoken word.\n\nFIG. 6 shows an exemplary process flow 600 of the present invention illustrating a method for converting printed Braille dots to speech, according to exemplary aspects of the present disclosure.\n\nthe method includes directing a lens of the digital camera 106 towards Braille text including printed Braille dots.\n\nthe digital camera is confined in the 3D ring case 102 .\n\nthe method includes compressing the switch 110 to capture a Braille image of the printed Braille dots.\n\nthe method includes matching the Braille image to a textural character corresponding to the Braille image.\n\nthe microprocessor is configured to perform data processing and one or more image recognition operations to match the Braille image to the textural character.\n\nthe method includes converting the textural character to an audio waveform.\n\nthe method includes transmitting the audio waveform to the speaker 114 .\n\nthe method includes generating a sound representative of a spoken word corresponding to the textural character.\n\nthe speaker 114 generates the sound representative of the spoken word.\n\nthe first embodiment is illustrated with respect to FIGS. 1-9 .\n\nthe first embodiment describes a system for converting printed Braille dots to speech.\n\nthe system comprises a 3D ring case 102 , 200 , a digital camera 106 mounted in the 3D ring case 102 , 200 , wherein the digital camera 106 is configured to capture a Braille image of the printed Braille dots, a rechargeable battery, a speaker 114 , a microprocessor operatively connected to the rechargeable battery, the digital camera 106 and the speaker 114 , the microprocessor configured to perform data processing and one or more image recognition operations which match the Braille image to a textural character corresponding to the Braille image, convert the textural character to an audio waveform, and transmit the audio waveform to the speaker 114 , and wherein the speaker 114 is configured to receive the audio waveform and generate a sound representative of a spoken word corresponding to the textural character.\n\nthe printed Braille dots are configured to represent an Arabic Braille textural character and the speaker 114 is configured to output a sound representative of an Arabic spoken word.\n\nthe textural character is configured to include one or more letters, one or more numbers, or one or more words.\n\nthe one or more letters include an Arabic letter or an English letter\n\nthe one or more numbers include an Arabic number or an English number\n\nthe one or more words include an Arabic word or an English word.\n\nthe system further comprises a display connected to the digital camera 106 , wherein the display is configured to display one of a written word or a picture representative of the word corresponding to the Braille image.\n\nthe rechargeable battery is a lithium battery.\n\nthe digital camera 106 is connected to the microprocessor by a serial bus configured to transmit data signals representative of the Braille image from the digital camera 106 to the microprocessor.\n\nthe 3D ring case 102 , 200 includes a first ring configured to receive the digital camera 106 , a second ring configured to be worn on a finger of a user, wherein the second ring is arranged below the first ring, and a switch configured to actuate the digital camera 106 to capture the Braille image.\n\nthe system further comprises a microprocessor housing.\n\nthe microprocessor housing includes a base configured to hold the microprocessor and the rechargeable battery, the base having a first elastic wristband connector and a second elastic wristband connector, a lid configured with sidewalls having a first plurality of cut outs configured for air flow and a second cut out configured to hold the speaker 114 , wherein a top of the lid has a third cut out configured to receive the serial bus, and wherein the base includes a plurality of slots around its periphery and the sidewalls include a plurality of tabs, wherein each tab is configured to align with a respective slot when the lid and the base are joined together.\n\nthe system further comprises a serial bus port located on the microprocessor; a power port configured to connect the rechargeable battery to the microprocessor, a first output pin configured to connect to a first wire, wherein the first wire is connected to a power input of the push button switch, a second output pin configured to connect a second wire, wherein the second wire is connected to a power output of the digital camera 106 , and a third output pin configured to connect to the speaker 114 .\n\nthe microprocessor is configured to perform the data processing by segmenting the Braille image into a plurality of segments, reducing image noise from the plurality of segments to generate a plurality of denoised segments, and resizing the plurality of denoised segments to generate a plurality of resized denoised segments.\n\nthe microprocessor is configured to perform one or more image recognition operations on the plurality of resized denoised segments to match the Braille image to a textural character corresponding to the Braille image by: training a deep learning based convolutional neural network on a dataset of Braille scripts, the Braille scripts including one or more of Arabic Braille numerals, Arabic Braille alphabet, Arabic Braille text, Arabic numerals, Arabic alphabet, Arabic text, English Braille numerals, English Braille alphabet, English Braille text, English numerals, English alphabet and English text to generate a plurality of Braille script recognition classes; applying the plurality of resized denoised segments to the deep learning based convolutional neural network, matching each of the plurality of resized denoised segments to a Braille script recognition class, and retrieving the audio waveform associated with the Braille script recognition class.\n\nthe second embodiment is illustrated with respect to FIGS. 1-9 .\n\nthe second embodiment describes a method for converting printed Braille dots to speech.\n\nthe method comprising capturing a Braille image of the printed Braille dots, matching, by performing data processing and one or more image recognition operations, the Braille image to a textural character corresponding to the Braille image, converting the textural character to an audio waveform, transmitting the audio waveform to a speaker 114 , and generating, by the speaker 114 , a sound representative of a spoken word corresponding to the textural character.\n\nthe method further comprising segmenting the Braille image into a plurality of segments, reducing image noise from the plurality of segments to generate a plurality of denoised segments, and resizing the plurality of denoised segments to generate a plurality of resized denoised segments.\n\nthe method further comprising training a deep learning based convolutional neural network on a dataset of Braille scripts, the Braille scripts including one or more of Arabic Braille numerals, Arabic Braille alphabet, Arabic Braille text, Arabic numerals, Arabic alphabet, Arabic text, English Braille numerals, English Braille alphabet, English Braille text, English numerals, English alphabet and English text to generate a plurality of Braille script recognition classes; applying the plurality of resized denoised segments to the deep learning based convolutional neural network, and matching each of the plurality of resized denoised segments to a Braille script recognition class, and retrieving the audio waveform associated with the Braille script recognition class.\n\nthe third embodiment is illustrated with respect to FIGS. 1-9 .\n\nthe third embodiment describes a method for converting printed Braille dots to speech.\n\nthe method comprising directing a lens of a digital camera 106 confined in a 3D ring case 102 , 200 towards Braille text including printed Braille dots, compressing a push button to capture a Braille image of the printed Braille dots, matching, by a microprocessor configured to perform data processing and one or more image recognition operations, the Braille image to a textural character corresponding to the Braille image, converting the textural character to an audio waveform, transmitting the audio waveform to a speaker 114 , and generating, by the speaker 114 , a sound representative of a spoken word corresponding to the textural character.\n\nthe method further comprising segmenting the Braille image into a plurality of segments, reducing image noise from the plurality of segments to generate a plurality of denoised segments, and resizing the plurality of denoised segments to generate a plurality of resized denoised segments.\n\nthe method further comprising training a deep learning based convolutional neural network on a dataset of Braille scripts, the Braille scripts including one or more of Arabic Braille numerals, Arabic Braille alphabet, Arabic Braille text, Arabic numerals, Arabic alphabet, Arabic text, English Braille numerals, English Braille alphabet, English Braille text, English numerals, English alphabet and English text to generate a plurality of Braille script recognition classes, applying the plurality of resized denoised segments to the deep learning based convolutional neural network, matching each of the plurality of resized denoised segments to a Braille script recognition class, and retrieving the audio waveform associated with the Braille script recognition class.\n\nthe Braille scripts including one or more of Arabic Braille numerals, Arabic Braille alphabet, Arabic Braille text, Arabic numerals, Arabic alphabet, Arabic text, English Braille numerals, English Braille alphabet, English Braille text, English numerals, English alphabet and English text to generate a plurality of Braille script recognition classes, applying the plurality of\n\nthe method further comprising transmitting, to the speaker, a sound representative of an Arabic spoken word corresponding to an Arabic textural character associated with Arabic Braille printed dots.\n\nthe method further comprising displaying, on the digital camera 106 , one of a written word or a picture representative of the textural character corresponding to the Braille image.\n\nFIG. 7 is an illustration of a non-limiting example of details of computing hardware used in the computing system, according to exemplary aspects of the present disclosure.\n\na controller 700 is described is representative of the system 600 of FIG. 6 in which the controller is a computing device which includes a CPU 701 which performs the processes described above/below.\n\nthe process data and instructions may be stored in memory 702 .\n\nThese processes and instructions may also be stored on a storage medium disk 704 such as a hard drive (HDD) or portable storage medium or may be stored remotely.\n\nHDD hard drive\n\nthe claims are not limited by the form of the computer-readable media on which the instructions of the inventive process are stored.\n\nthe instructions may be stored on CDs, DVDs, in FLASH memory, RAM, ROM, PROM, EPROM, EEPROM, hard disk or any other information processing device with which the computing device communicates, such as a server or computer.\n\nclaims may be provided as a utility application, background daemon, or component of an operating system, or combination thereof, executing in conjunction with CPU 701 , 703 and an operating system such as Microsoft Windows 7, Microsoft Windows 10, UNIX, Solaris, LINUX, Apple MAC-OS, and other systems known to those skilled in the art.\n\nan operating system such as Microsoft Windows 7, Microsoft Windows 10, UNIX, Solaris, LINUX, Apple MAC-OS, and other systems known to those skilled in the art.\n\nCPU 701 or CPU 703 may be a Xenon or Core processor from Intel of America or an Opteron processor from AMD of America, or may be other processor types that would be recognized by one of ordinary skill in the art.\n\nthe CPU 701 , 703 may be implemented on an FPGA, ASIC, PLD or using discrete logic circuits, as one of ordinary skill in the art would recognize.\n\nCPU 701 , 703 may be implemented as multiple processors cooperatively working in parallel to perform the instructions of the inventive processes described above.\n\nthe computing device in FIG. 7 also includes a network controller 706 , such as an Intel Ethernet PRO network interface card from Intel Corporation of America, for interfacing with network 760 .\n\nthe network 760 can be a public network, such as the Internet, or a private network such as an LAN or WAN network, or any combination thereof and can also include PSTN or ISDN sub-networks.\n\nthe network 760 can also be wired, such as an Ethernet network, or can be wireless such as a cellular network including EDGE, 3G and 4G wireless cellular systems.\n\nthe wireless network can also be WiFi, Bluetooth, or any other wireless form of communication that is known.\n\nthe computing device further includes a display controller 708 , such as a NVIDIA GeForce GTX or Quadro graphics adaptor from NVIDIA Corporation of America for interfacing with display 710 , such as a Hewlett Packard HPL2445w LCD monitor.\n\na general purpose I/O interface 712 interfaces with a keyboard and/or mouse 714 as well as a touch screen panel 716 on or separate from display 710 .\n\nGeneral purpose I/O interface also connects to a variety of peripherals 718 including printers and scanners, such as an OfficeJet or DeskJet from Hewlett Packard.\n\na sound controller 720 is also provided in the computing device such as Sound Blaster X-Fi Titanium from Creative, to interface with speakers/microphone 722 thereby providing sounds and/or music.\n\nthe general purpose storage controller 724 connects the storage medium disk 704 with communication bus 726 , which may be an ISA, EISA, VESA, PCI, or similar, for interconnecting all of the components of the computing device.\n\ncommunication bus 726 may be an ISA, EISA, VESA, PCI, or similar, for interconnecting all of the components of the computing device.\n\na description of the general features and functionality of the display 710 , keyboard and/or mouse 714 , as well as the display controller 708 , storage controller 724 , network controller 706 , sound controller 720 , and general purpose I/O interface 712 is omitted herein for brevity as these features are known.\n\ncircuitry configured to perform features described herein may be implemented in multiple circuit units (e.g., chips), or the features may be combined in circuitry on a single chipset, as shown on FIG. 8 .\n\nFIG. 8 shows a schematic diagram of a data processing system, according to certain embodiments, for performing the functions of the exemplary embodiments.\n\nthe data processing system is an example of a computer in which code or instructions implementing the processes of the illustrative embodiments may be located.\n\ndata processing system 800 employs a hub architecture including a north bridge and memory controller hub (NB/MCH) 825 and a south bridge and input/output (I/O) controller hub (SB/ICH) 820 .\n\nthe central processing unit (CPU) 830 is connected to NB/MCH 825 .\n\nthe NB/MCH 825 also connects to the memory 845 via a memory bus, and connects to the graphics processor 850 via an accelerated graphics port (AGP).\n\nAGP accelerated graphics port\n\nthe NB/MCH 825 also connects to the SB/ICH 820 via an internal bus (e.g., a unified media interface or a direct media interface).\n\nthe CPU Processing unit 830 may contain one or more processors and even may be implemented using one or more heterogeneous processor systems.\n\nFIG. 9 shows one implementation of CPU 830 .\n\nthe instruction register 938 retrieves instructions from the fast memory 940 . At least part of these instructions are fetched from the instruction register 938 by the control logic 936 and interpreted according to the instruction set architecture of the CPU 830 . Part of the instructions can also be directed to the register 932 .\n\nthe instructions are decoded according to a hardwired method, and in another implementation the instructions are decoded according to a microprogram that translates instructions into sets of CPU configuration signals that are applied sequentially over multiple clock pulses.\n\nthe instructions are executed using the arithmetic logic unit (ALU) 934 that loads values from the register 932 and performs logical and mathematical operations on the loaded values according to the instructions.\n\nthe results from these operations can be feedback into the register and/or stored in the fast memory 940 .\n\nthe instruction set architecture of the CPU 830 can use a reduced instruction set architecture, a complex instruction set architecture, a vector processor architecture, a very large instruction word architecture.\n\nthe CPU 830 can be based on the Von Neuman model or the Harvard model.\n\nthe CPU 830 can be a digital signal processor, an FPGA, an ASIC, a PLA, a PLD, or a CPLD.\n\nthe CPU 830 can be an x86 processor by Intel or by AMD; an ARM processor, a Power architecture processor by, e.g., IBM; a SPARC architecture processor by Sun Microsystems or by Oracle; or other known CPU architecture.\n\nthe data processing system 800 can include that the SB/ICH 820 is coupled through a system bus to an I/O Bus, a read only memory (ROM) 856 , universal serial bus (USB) port 864 , a flash binary input/output system (BIOS) 868 , and a graphics controller 858 .\n\nPCI/PCIe devices can also be coupled to SB/ICH 888 through a PCI bus 862 .\n\nthe PCI devices may include, for example, Ethernet adapters, add-in cards, and PC cards for notebook computers.\n\nthe Hard disk drive 860 and CD-ROM 866 can use, for example, an integrated drive electronics (IDE) or serial advanced technology attachment (SATA) interface.\n\nthe I/O bus can include a super I/O (SIO) device.\n\nthe hard disk drive (HDD) 860 and optical drive 866 can also be coupled to the SB/ICH 820 through a system bus.\n\na keyboard 870 , a mouse 872 , a parallel port 878 , and a serial port 876 can be connected to the system bus through the I/O bus.\n\nOther peripherals and devices that can be connected to the SB/ICH 820 using a mass storage controller such as SATA or PATA, an Ethernet port, an ISA bus, a LPC bridge, SMBus, a DMA controller, and an Audio Codec.\n\ncircuitry described herein may be adapted based on changes on battery sizing and chemistry, or based on the requirements of the intended back-up load to be powered.\n\nLandscapes\n\nEngineering & Computer Science (AREA)\n\nHealth & Medical Sciences (AREA)\n\nAudiology, Speech & Language Pathology (AREA)\n\nGeneral Health & Medical Sciences (AREA)\n\nBusiness, Economics & Management (AREA)\n\nPhysics & Mathematics (AREA)\n\nEducational Administration (AREA)\n\nEducational Technology (AREA)\n\nGeneral Physics & Mathematics (AREA)\n\nTheoretical Computer Science (AREA)\n\nUser Interface Of Digital Computer (AREA)\n\nAbstract\n\nA device, method, and system for converting printed Braille dots to speech. A Braille image of the printed Braille dots is captured by a digital camera mounted on a 3D ring case. Data processing and one or more image recognition operations are performed by a microprocessor to match the Braille image to a textural character corresponding to the Braille image. The textural character is converted to an audio waveform. The audio waveform is transmitted to a speaker. The speaker generates a sound representative of a spoken word corresponding to the textural character.\n\nDescription\n\nCROSS-REFERENCE TO RELATED APPLICATIONS\n\nThe present application claims priority to U.S. Provisional Application No. 63/288,224, filed Dec. 10, 2021, the entire contents which is incorporated by reference herein in its entirety for all purposes.\n\nBACKGROUND Technical Field\n\nThe present disclosure is directed to a Braille reader system including a deep learning framework which converts printed Braille dots to speech.\n\nDescription of Related Art\n\nThe âbackgroundâ description provided herein is for the purpose of generally presenting the context of the disclosure. Work of the presently named inventors, to the extent it is described in this background section, as well as aspects of the description which may not otherwise qualify as prior art at the time of filing, are neither expressly or impliedly admitted as prior art against the present invention.\n\nBraille is a form of written language used by visually impaired persons to read a document consisting of raised dots. In Braille, characters or words are represented by patterns of raised dots that are felt with the fingertips. Braille script is comprised of Braille cells, each representing a character or a word. A Braille sheet is a paper embossed with raised dots that are arranged in Braille cells. A Braille cell is represented by a collection of 6 raised dots arranged in two columns of 3 dots each. Braille script may be presented on Braille paper, on a refreshable Braille display device with dots or round-tipped pins raised through holes in a flat surface, by a Braille input device, and on public notices and signboards.\n\nVarious computer vision and image processing techniques may be used to recognize the characters or words from scanned documents and images of Braille script and convert the recognized characters or words to speech. Such techniques may assist a visually disabled person, an instructor or trainer in reading the Braille scripts.\n\nVarious solutions have been developed in recent years for the recognition of Braille scripts. A contact type image sensor that can be worn on a finger and configured to superimpose multiple images obtained from the contact type image sensor to obtain a composited image, convert the composited image into a Braille pattern, and convert the Braille pattern into a corresponding voice was described in JP2011070530A, âContact type image sensor and image recognition deviceâ, incorporated herein by reference in its entirety. However, this reference has a drawback that the contact type image sensor comes in contact with the Braille pattern instead of the finger of the user. This drawback may hinder the learning experience of the blind person, as the tactile feedback from touching the Braille dots serves to orient the hand towards the next Braille word.\n\nA mobile communication terminal to convert Braille points into voice by taking a picture of Braille points through a camera and recognize the Braille image to output the Braille image as a voice was described in KR2007057351A, âMobile communication terminal for converting Braille points into a voice, especially concerned with recognizing the Braille points to output the recognized Braille points as the voiceâ, incorporated herein by reference in its entirety. However, it is difficult for a blind person to take a picture of the Braille pattern with a camera, as the individual cannot orient the camera without vision.\n\nA semantic segmentation model trained by using a convolutional neural network and Braille images are input into the semantic segmentation model which obtains a semantic segmentation result diagram of the Braille image was described in CN110298236A, incorporated herein by reference in its entirety. However, this reference outputs images of the Braille pattern which are not readable by a blind person.\n\nFurther, a deep convolutional neural network (DCNN) model that takes pre-processed Braille images as input to recognize Braille cells has been proposed. (See: Abdulmalik Alsalman, Amani Alsalman, Abdu Gumaei, and Suheer Ali Al-Hadhrami, âA Deep Learning-Based Recognition Approach for the Conversion of Multilingual Braille Imagesâ, Article in Computers, Materials and Continua, March 2021, DOI: 10.32604/cmc.2021.015614, incorporated herein by reference in its entirety). However, this reference does not convert the Braille pattern to speech so is not useful for a blind person.\n\nEach of the aforementioned references suffers from one or more drawbacks hindering their adoption. Accordingly, it is one object of the present disclosure to provide a Braille reader system to capture and process Braille images for real time recognition of characters and words corresponding to Braille cells sensed by a finger of a user, and outputs speech.\n\nSUMMARY\n\nIn an exemplary embodiment, a system for converting printed Braille dots to speech is disclosed. The system includes a 3D ring case and a digital camera mounted in the 3D ring case. The digital camera is configured to capture a Braille image of the printed Braille dots. The system further includes a rechargeable battery and a speaker. The system further includes a microprocessor operatively connected to the rechargeable battery, the digital camera, and the speaker. The microprocessor is configured to perform data processing and one or more image recognition operations which match the Braille image to a textural character corresponding to the Braille image. The microprocessor is further configured to convert the textural character to an audio waveform and transmit the audio waveform to the speaker. The speaker is configured to receive the audio waveform and generate a sound representative of a spoken word corresponding to the textural character.\n\nIn another exemplary embodiment, a method for converting printed Braille dots to speech is disclosed. The method includes capturing a Braille image of the printed Braille dots. The method further includes matching the Braille image to a textural character corresponding to the Braille image. Data processing and one or more image recognition operations are performed to match the Braille image to the textural character. The method further includes converting the textural character to an audio waveform and transmitting the audio waveform to a speaker. The method further includes generating, by the speaker, a sound representative of a spoken word corresponding to the textural character.\n\nIn another exemplary embodiment, a method for converting printed Braille dots to speech is disclosed. The method includes directing a lens of a digital camera towards Braille text including printed Braille dots. The digital camera is confined in a 3D ring case. The method further includes compressing a push button to capture a Braille image of the printed Braille dots. The method further includes matching the Braille image to a textural character corresponding to the Braille image. A microprocessor is configured to perform data processing and one or more image recognition operations to match the Braille image to the textural character. The method further includes converting the textural character to an audio waveform and transmitting the audio waveform to a speaker. The method further includes generating, by the speaker, a sound representative of a spoken word corresponding to the textural character.\n\nThe foregoing general description of the illustrative embodiments and the following detailed description thereof are merely exemplary aspects of the teachings of this disclosure, and are not restrictive.\n\nBRIEF DESCRIPTION OF THE DRAWINGS\n\nA more complete appreciation of this disclosure and many of the attendant advantages thereof will be readily obtained as the same becomes better understood by reference to the following detailed description when considered in connection with the accompanying drawings, wherein:\n\nFIG. 1 is a schematic diagram of a Braille reading device, according to certain embodiments.\n\nFIG. 2 is a schematic diagram of a 3D ring case, according to certain embodiments.\n\nFIG. 3A is a schematic diagram of a microprocessor housing, where (a) is a base and (b) is a lid of the microprocessor housing, according to certain embodiments.\n\nFIG. 3B is a schematic diagram of a microprocessor housing, according to certain embodiments.\n\nFIG. 4 is a process flow diagram illustrating a conversion of printed Braille dots to speech, according to certain embodiments.\n\nFIG. 5 is an exemplary process flow of a method for converting printed Braille dots to speech, according to certain embodiments.\n\nFIG. 6 is an exemplary process flow of a method for converting printed Braille dots to speech, according to certain embodiments.\n\nFIG. 7 is an illustration of a non-limiting example of details of computing hardware used in the computing system, according to certain embodiments.\n\nFIG. 8 is an exemplary schematic diagram of a data processing system used within the computing system, according to certain embodiments.\n\nFIG. 9 is an exemplary schematic diagram of a processor used with the computing system, according to certain embodiments.\n\nDETAILED DESCRIPTION\n\nIn the drawings, like reference numerals designate identical or corresponding parts throughout the several views. Further, as used herein, the words âa,â âanâ and the like generally carry a meaning of âone or more,â unless stated otherwise.\n\nFurthermore, the terms âapproximately,â âapproximate,â âabout,â and similar terms generally refer to ranges that include the identified value within a margin of 20%, 10%, or preferably 5%, and any values therebetween.\n\nAspects of this disclosure are directed to a system, device, and method for converting printed Braille dots to speech. The present disclosure discloses a device to be worn by a user. The device includes a camera to capture images of printed Braille dots. The device includes a microprocessor that processes the captured images of printed Braille dots to determine textural characters or words corresponding to the images of printed Braille dots.\n\nFIG. 1 depicts a schematic diagram of a Braille reading device 100 for reading Braille scripts, according to exemplary aspects of the present disclosure.\n\nAccording to aspects of the present disclosure, the Braille reading device 100 includes a three-dimensional (3D) ring case 102, a microprocessor housing 104, a digital camera 106, a microprocessor board 108, a switch 110, a flexible flat cable (FFC) 112, a speaker 114, a strap 115, and a pair of connecting wires 116.\n\nReferring to FIG. 2, the 3D ring case 102 includes a first ring 203 and a second ring 204. The second ring is positioned below the first ring. The digital camera 106 is arranged in the first ring. The 3D ring case 102 is worn on a finger of a user by inserting the finger of the user in the second ring. The switch 110 is arranged on the 3D ring case 102. The 3D ring case 102 is explained in further detail with reference to FIG. 2.\n\nReferring to FIG. 1 and FIG. 3A, the microprocessor housing 104 may include a base 303 and a lid 305. The base holds the microprocessor board 108. The lid may be placed on top of the base to enclose components arranged on the base. A top side of the lid includes a cut-out portion to receive the FFC 112. The microprocessor board 108, enclosed in the microprocessor housing 104, is connected to the digital camera 106 by the FFC 112 passing through the cut-out portion. The base includes the strap 115 (shown as 315, FIG. 3B) for the user to wear the microprocessor housing 104 on the wrist of the user. The lid further includes another cut-out portion on a sidewall (or a lateral side) to hold the speaker 114. In an aspect of the present disclosure, the speaker 114 may be arranged within the microprocessor housing 104. The microprocessor housing 104 is explained in further detail with reference to FIG. 3A and FIG. 3B.\n\nReferring back to FIG. 1, the Braille reading device 100 further includes a battery (not shown) to power one or more of the digital camera 106, the microprocessor board 108, and the speaker 114. The battery may be arranged in the microprocessor housing 104. In an aspect of the present disclosure, the battery may be a rechargeable battery. The microprocessor housing 104 may include a power interface that may be connected to a power charging adapter to charge the rechargeable battery. In an aspect of the present disclosure, the battery may be replaced with a new battery when the battery runs out of power.\n\nIn an aspect of the present disclosure, the speaker 114 may be an external speaker or a headphone that may connect to the microprocessor board 108 via one or more audio interfaces. The one or more audio interfaces may include a wired audio interface or a wireless audio interface. The wired audio interface may include high definition multimedia interface (HDMI), a 3.5 mm audio jack, and a universal serial bus (USB) interface. The wireless audio interface may include Bluetooth or Wireless Fidelity (Wi-Fi) connection. The speaker 114 may be powered by the battery of the Braille reading device 100, by an in-built battery in the speaker 114, or an external AC or DC power supply.\n\nThe pair of connecting wires 116 connects the switch 110 to the microprocessor board 108. The pair of connecting wires 116 passes through the cut-out portion on the top side of the lid to connect the switch 110 to the microprocessor board 108 enclosed in the microprocessor housing 104. In an aspect of the present disclosure, the microprocessor board 108 may include General-Purpose Input/Output (GPIO) pins. The GPIO pins provide a physical interface between the microprocessor board 108 and external components. One end of the pair of connecting wires 116 is connected to the switch 110, and the other end of the pair of connecting wires 116 is connected to the GPIO pins. The switch 110 activates the Braille reading device 100.\n\nIn operation, the user may wear the 3D ring case 102 on a finger of either hand and wear the microprocessor housing 104 on the wrist of the same hand. To read or learn Braille script, the user may scan the Braille script by sensing printed Braille dots with the finger. The user may press the switch 110 to activate the Braille reading device 100 to identify one or more characters or words corresponding to the printed Braille dots sensed by the finger. When the user presses the switch 110, the digital camera 106 captures a Braille image of the Braille dots. The Braille image is transmitted to the microprocessor board 108 over the FFC 112. The microprocessor board 108 comprises a microprocessor that performs one or more image processing techniques or algorithms to process the Braille image for image recognition. Further, the microprocessor may input the processed Braille image to a trained deep learning based convolutional neural network (CNN) model. The trained deep learning based CNN model is trained on a dataset of images of Braille scripts. The trained deep learning based CNN model accepts the processed Braille image, extract features from the processed Braille image, and classifies them into respective classes to recognize one or more characters or words corresponding to the Braille image. The microprocessor generates an audio waveform corresponding to the recognized characters or words and transmits the audio waveform to the speaker 114. The speaker 114 outputs the sound corresponding to the audio waveform.\n\nIn a non-limiting example, the microprocessor board 108 may be one of a Raspberry Pi series or other similar single-board computers. The microprocessor board 108 may include the processor, the GPIO pins to provide a physical interface between the microprocessor board 108 and external components, memory module such as a random access memory (RAM), one or more HDMI ports, camera serial interface (CSI) to connect the digital camera 106 through the FFC 112, one or more USB ports to connect peripheral devices and power supply, one or more micro USB ports, and one or more wireless connectivity modules, such as Bluetooth and Wi-Fi and secure digital (SD) card slot.\n\nIn an aspect of the present disclosure, the Braille reading device 100 may be communication-enabled using various wired and wireless connectivity protocols, such as Wi-Fi and Personal Area Network. The processor of the Braille reading device 100 may load the trained deep learning based CNN model from a host machine or a server.\n\nFIG. 2 depicts a schematic diagram of a 3D ring case 202, according to exemplary aspects of the present disclosure.\n\nAccording to aspects of the present disclosure the 3D ring case 202 is designed to be worn on a finger of the user and to accommodate the digital camera 106 and the switch 110. The 3D ring case 202 corresponds to the 3D ring case 102 of FIG. 1. The 3D ring case 202 includes a first opening 203, a second opening 204 below the first opening 203, and a switch 206. The digital camera 106 may be arranged in the first opening 203. The 3D ring case 202 may be worn on a finger of the user by inserting the finger of the user in the second opening 204. The switch 206 may be arranged on either of the lateral sides of the 3D ring case 202.\n\nAlthough the first opening 203 and the second opening 204 as shown in FIG. 2 are round in shape, one or both the first opening 203 and the second opening 204 may be of any other shape depending on design requirements. For example, shape and size of the first opening 203 may be designed based on shape and size of the digital camera 106 to be arranged in the first opening 203.\n\nIn an aspect of the present disclosure, the 3D ring case 202 may be custom designed to provide a slot, in place of the switch 206, for accommodating the switch 110 on the 3D ring case 202. The 3D ring case 202 may be custom designed to have dimensions in accordance with requirements and components of Braille reading device 100. In an aspect of the present disclosure, the 3D ring case 202 may be printed by a 3D printer.\n\nIn an aspect of the present disclosure, the 3D ring case 202 may be designed in a plurality of sizes for the second opening 204. In a non-limiting example, the 3D ring case 202 may be designed for three different sizes, i.e., small, medium, and large size of the second opening 204, which can be selected to fit a finger size of a user. In an aspect of the present disclosure, the 3D ring case 202 may be designed such that size of the second opening 204 may be adjusted to fit size of the finger of the user. In an aspect of the present disclosure, the 3D ring case 202 may be custom designed in accordance with physical appearance preferences of different users.\n\nFIG. 3A depicts a schematic diagram of a microprocessor housing 304, where (a) is a base and (b) is a lid of the microprocessor housing, according to exemplary aspects of the present disclosure.\n\nAccording to aspects of the present disclosure the microprocessor housing 304 is designed to accommodate the microprocessor board 108, the speaker 114, and the battery. The microprocessor housing 304 corresponds to the microprocessor housing 104 of FIG. 1. The microprocessor housing 300 includes a base 303 and a lid 305. The base 303 is designed to hold the microprocessor board 108 and the battery. The base 303 includes a strap 315. The strap 315 is used to wear the microprocessor housing 304 on a wrist of the user as shown in FIG. 3B.\n\nThe lid 305 includes a plurality of cutout portions 308 on sidewalls 306 (or lateral sides). The plurality of cut-out portions 308 may enable air flow in the microprocessor housing 304. At least one of the plurality of cut-out portions 308 is used to arrange the speaker 114. A top side of the lid 305 includes a cut-out portion 310. The FFC 112 passes through the cut-out portion 310 to connect the digital camera 106 to the microprocessor board 108.\n\nThe base 303 further includes a plurality of slots 312 and the sidewalls 306 include a plurality of tabs 314. Each tab 314 aligns with a corresponding slot 312 when the lid 305 and the base 303 are joined together.\n\nAlthough the plurality of cut-out portions 308 and the cut-out portion 310 as shown in FIG. 3A are round in shape, the plurality of cut-out portions 308 and the cut-out portion 310 may be of any other shape depending on design requirements.\n\nFIG. 3B depicts a schematic diagram of a microprocessor housing 304, according to exemplary aspects of the present disclosure. The base 303 includes the strap 315. In an aspect of the present disclosure, the strap 315 may be an elastic band which may be worn on the wrist of the user by stretching the elastic band and slipping (or sliding) the hand through the elastic band. In another aspect of the present disclosure, the strap 315 may include a first elastic wristband connector and a second elastic wristband connector. The first elastic wristband connector and the second elastic wristband connector may be coupled together to wear the microprocessor housing 304 on a wrist of the user. In an aspect of the present disclosure, the base 303 may include coupling means, such as fasteners, hooks, clips, and buckles to couple the strap 315 with the base 303. The coupling means may be arranged on exterior sides or exterior bottom of the base 303. In an aspect of the present disclosure, the base 303 may include slots on opposite side and the strap 315 may pass through the slots.\n\nIn an aspect of the present disclosure, the microprocessor housing 304 may be custom designed to have dimensions in accordance with requirements and components of Braille reading device 100. In an aspect of the present disclosure, the microprocessor housing 304 may be printed by a 3D printer.\n\nIn an aspect of the present disclosure, the microprocessor housing 304 may be designed to have a slim profile such that the microprocessor housing 304 can be hidden under sleeve of garment worn by the user. In an aspect of the present disclosure, the microprocessor housing 304 may be designed in the shape of one or more cartoon characters, superhero characters, animals, or may be custom designed to any desired shape and size, so as to appeal to children learning Braille.\n\nFIG. 4 depicts a process flow diagram 400 illustrating conversion of Braille dots to speech, according to exemplary aspects of the present disclosure. As shown in FIG. 4, a user may wear the Braille reading device 100. The user may wear the microprocessor housing 104 on a wrist and the 3D ring case 102 on a finger. The user may scan a Braille script 402 with the finger to sense Braille dots. The Braille script 402 may be presented on one or more of: a Braille paper or textbook with printed dots, a refreshable Braille display device with dots or round-tipped pins raised through holes in a flat surface, a Braille input device, and public notices, bathroom doors, and signboards and the like.\n\nIn an aspect of the present disclosure, the user may wear the 3D ring case 102 on the index finger of a hand. For ease of operation, the user may press the switch 110 with the thumb of the same hand. In an aspect of the present disclosure, the user may wear the 3D ring case 102 in any finger on either hand, and use any finger on either hand to press the switch 110.\n\nIn an aspect of the present invention, the user may wear the 3D ring case 102 on the same finger with which the user senses the Braille dots. One or more of focal length, position, and optical angle of the digital camera 106 may be appropriately adjusted to capture images of the Braille dots sensed by the user.\n\nBy sensing the Braille dots with the finger, the user may identify the location of the Braille dots. After the user identifies the location of the Braille dots, the user actuates the digital camera 106. In an aspect of the present disclosure, after sensing the Braille dots, the user may move the finger behind the Braille dots such that the finger does not overlap the Braille dots and the Braille dots are in the field of view of the digital camera 106. In an aspect of the present disclosure, optical axis and/or field of view of the digital camera 106 may be oriented to capture Braille images of the Braille dots in one or more directions with respect to the finger of the user. The digital camera 106 may capture Braille images of the Braille dots above the finger and on the right and left side of the finger. In an aspect of the present disclosure, the microprocessor may be configured to adjust the optical axis and/or field of view of the digital camera 106 based on user preferences. In an aspect of the present disclosure, the user may manually adjust the optical axis and/or field of view of the digital camera 106 by adjusting position and/or orientation of the digital camera 106 in the first opening 203.\n\nThe user presses the switch 110 to actuate the digital camera 106 to capture a Braille image of the Braille dots. The digital camera 106 transmits the Braille image to the microprocessor included in the microprocessor board 108 over the FFC 112.\n\nIn an aspect of the present disclosure, when the user presses the switch 110, the digital camera 106 may begin to capture the Braille images at predetermined time intervals. The user may press the switch 110 again to stop capturing the Braille images. The microprocessor may be configured to capture the Braille images at the predetermined time intervals. The predetermined time interval may be adjusted based on reading speed and/or proficiency of the user.\n\nThe microprocessor may apply one or more digital image processing and/or computer vision techniques or algorithms to process the Braille image. The Braille script 402 is comprised of a plurality of Braille cells, each representing a character or a word. A Braille cell is represented by a collection of 6 raised dots arranged in two columns, each having three dots. The Braille image captured by the digital camera 106 may include one or more Braille cells, each representing a character or a word.\n\nThe microprocessor processes the Braille image by applying an image segmentation process 404 to the Braille image. Applying the image segmentation process 404 to the Braille image segments the Braille image into a plurality of segments. The plurality of segments in the segmented Braille image distinguishes individual Braille cells in the Braille image. The image segmentation process 404 may also distinguish the Braille cells from the background in the captured Braille image. In an aspect of the present disclosure, one or more image segmentation algorithms may be used to distinguish the Braille cells from the background and to distinguish individual Braille cells across a single line and/or multiple lines in the Braille image.\n\nThe microprocessor may further process the Braille image by applying an image noise reduction process 406 to the segmented Braille image. The quality of the Braille image captured by the digital camera 106 may be degraded due to various factors, such as a non-uniform ambient illumination, a low resolution imaging sensor, and an impulse noise. The image noise reduction process 406 may apply one or more image enhancement and denoising techniques to remove noise and enhance the quality of the segmented Braille image. The output of the image noise reduction process 406 is a denoised segmented Braille image.\n\nThe microprocessor may further process the Braille image by applying an image resizing process 408 to the denoised segmented Braille image. The plurality of segments in the denoised segmented Braille image may be appropriately resized for recognition of character or word corresponding to each of the plurality of Braille cells. One or more image interpolation and resizing techniques may be used to resize the plurality of denoised segments in the Braille image to generate a resized Braille image.\n\nThe microprocessor performs an image recognition process 410 on the resized Braille image. To perform the image recognition process 410, the resized Braille image is input to the trained deep learning based CNN model. The image recognition process 410 is performed to recognize characters or words corresponding to each of the plurality of Braille cells, i.e., the plurality of segments.\n\nIn an aspect of the present disclosure, the deep learning based CNN model may be trained on a dataset of Braille scripts. The dataset of Braille scripts which train the deep learning based CNN model may include a plurality of labeled images of Braille scripts in one or more languages. The plurality of labeled images of Braille scripts may include alphabets, numerals, and text. For example, dataset of Braille scripts may include one or more of Arabic Braille numerals, Arabic Braille alphabet, Arabic Braille text, Arabic numerals, Arabic alphabet, Arabic text, English Braille numerals, English Braille alphabet, English Braille text, English numerals, English alphabet and English text. The plurality of labeled images of Braille scripts generates a plurality of Braille script recognition classes. The dataset of Braille scripts is not limited to English or Arabic and may also include numerals, alphabet and text in any language in which Braille is used.\n\nThe resized Braille image is input to the trained deep learning based CNN model to recognize characters or words corresponding to the Braille image. The trained deep learning based CNN model is a sequence of convolutional layer and pooling layer. The important features of the Braille image are kept in the convolution layers and intensified in the pooling layers and kept over the network, while discarding all the unnecessary information. The convolutional layers and pooling layers are consecutively connected to extract features of the resized Braille image to generate a features map. A flattening function may then be applied to the features map. The flattened features map may be passed through a neural network of fully connected layers or dense layers followed by an output layer. In the output layer, the Braille script recognition class is determined for the resized Braille image input to the trained deep learning based CNN model. The image recognition is complete, and the text corresponding to the captured Braille image is determined.\n\nThe processor may generate or extract an audio waveform of the text corresponding to the captured Braille image and transmit the audio waveform to the speaker 114. The speaker 114 may generate a sound corresponding to the audio waveform of the text corresponding to the captured Braille image. In an aspect of the present disclosure, the microprocessor may be configured to change the sound corresponding to the audio waveform. For example, the user may select the sound to be a man's voice, a woman's voice, a child's voice, a cartoon character's voice, and any other voice from a selection of voice options.\n\nIn an aspect of the present disclosure, the microprocessor may connect to a computing device, such as a mobile phone or a server computing device to configure the sound of the Braille reading device 100. In an aspect of the present disclosure, the Braille reading device 100 may pair with a mobile computing device running an application corresponding to the Braille reading device 100. The user may adjust one or more configurations of the Braille reading device 100 via the application running on the mobile computing device. In an aspect of the present disclosure, the application may provide a user interface to present the learning progress of the user.\n\nIn an aspect display of the present disclosure, a display device may be connected to the digital camera 106. At least one of a written text or a picture representative of the text corresponding to the captured Braille image may be displayed on the display device.\n\nIn an aspect of the present disclosure, the trained deep learning based CNN model may be implemented using open source Python libraries, such as TensorFlow, Keras, NumPy, and OpenCV.\n\nIn an aspect of the present disclosure, the microprocessor may be configured to run a programming code written in a programming language, such as R and Python. Running the programming code may cause the microprocessor to run the deep learning based CNN model to start the recognition process of the captured Braille image, and play an audio representation of the recognized text corresponding to the captured Braille image. In an aspect of the present disclosure, the trained deep learning based CNN model may be programmed on a host machine and configured on the microprocessor. The microprocessor may connect with the host machine to receive any updates related to the trained deep learning based CNN model and/or digital image processing.\n\nAspects of the present disclosure may enable a user, such as a visually impaired person or any other person reading or learning the Braille script to understand Braille scripts without assistance of a trainer. The user may sense the Braille dots and activate the digital camera 106 to capture the Braille image. The processor may process the captured Braille image and access the trained deep learning based CNN model to recognize text corresponding to the Braille image. Sounds of the recognized text may be played by the speaker 114 and/or displayed on a display screen. Thus, the Braille reading device 100 of the present disclosure provides real-time learning experience for the user with minimal or no assistance.\n\nIn an aspect of the present disclosure, the deep learning based CNN model may be trained on a dataset of Arabic braille numerals, alphabet, and text. Approximately 50,500 images may be collected for the dataset consisting of 10 classes for numerals, 28 classes for alphabet, and 60 classes of words in the Arabic language. Similar concept for training the deep learning based CNN model may be applied for other languages, thus, making the Braille reading device 100 compatible with any other language. The microprocessor may load the trained deep learning based CNN model for any other language to recognize text corresponding to Braille in that language.\n\nFIG. 5 shows an exemplary process flow 500 of the present invention illustrating a method for converting printed Braille dots to speech, according to exemplary aspects of the present disclosure.\n\nAt step 502, the method includes capturing a Braille image of the printed Braille dots.\n\nAt step 504, the method includes matching the Braille image to a textural character corresponding to the Braille image. Data processing and one or more image recognition operations may be performed to match the Braille image to the textural character.\n\nAt step 506, the method includes converting the textural character to an audio waveform.\n\nAt step 508, the method includes transmitting the audio waveform to the speaker 114.\n\nAt step 510, the method includes generating a sound representative of a spoken word corresponding to the textural character. The speaker 114 generates the sound representative of the spoken word.\n\nFIG. 6 shows an exemplary process flow 600 of the present invention illustrating a method for converting printed Braille dots to speech, according to exemplary aspects of the present disclosure.\n\nAt step 602, the method includes directing a lens of the digital camera 106 towards Braille text including printed Braille dots. The digital camera is confined in the 3D ring case 102.\n\nAt step 604, the method includes compressing the switch 110 to capture a Braille image of the printed Braille dots.\n\nAt step 606, the method includes matching the Braille image to a textural character corresponding to the Braille image. The microprocessor is configured to perform data processing and one or more image recognition operations to match the Braille image to the textural character.\n\nAt step 608, the method includes converting the textural character to an audio waveform.\n\nAt step 610, the method includes transmitting the audio waveform to the speaker 114.\n\nAt step 612, the method includes generating a sound representative of a spoken word corresponding to the textural character. The speaker 114 generates the sound representative of the spoken word.\n\nThe first embodiment is illustrated with respect to FIGS. 1-9. The first embodiment describes a system for converting printed Braille dots to speech. The system comprises a 3D ring case 102, 200, a digital camera 106 mounted in the 3D ring case 102, 200, wherein the digital camera 106 is configured to capture a Braille image of the printed Braille dots, a rechargeable battery, a speaker 114, a microprocessor operatively connected to the rechargeable battery, the digital camera 106 and the speaker 114, the microprocessor configured to perform data processing and one or more image recognition operations which match the Braille image to a textural character corresponding to the Braille image, convert the textural character to an audio waveform, and transmit the audio waveform to the speaker 114, and wherein the speaker 114 is configured to receive the audio waveform and generate a sound representative of a spoken word corresponding to the textural character.\n\nThe printed Braille dots are configured to represent an Arabic Braille textural character and the speaker 114 is configured to output a sound representative of an Arabic spoken word.\n\nThe textural character is configured to include one or more letters, one or more numbers, or one or more words.\n\nThe one or more letters include an Arabic letter or an English letter, the one or more numbers include an Arabic number or an English number, and the one or more words include an Arabic word or an English word.\n\nThe system further comprises a display connected to the digital camera 106, wherein the display is configured to display one of a written word or a picture representative of the word corresponding to the Braille image.\n\nThe rechargeable battery is a lithium battery.\n\nThe digital camera 106 is connected to the microprocessor by a serial bus configured to transmit data signals representative of the Braille image from the digital camera 106 to the microprocessor.\n\nThe 3D ring case 102, 200 includes a first ring configured to receive the digital camera 106, a second ring configured to be worn on a finger of a user, wherein the second ring is arranged below the first ring, and a switch configured to actuate the digital camera 106 to capture the Braille image.\n\nThe system further comprises a microprocessor housing. The microprocessor housing includes a base configured to hold the microprocessor and the rechargeable battery, the base having a first elastic wristband connector and a second elastic wristband connector, a lid configured with sidewalls having a first plurality of cut outs configured for air flow and a second cut out configured to hold the speaker 114, wherein a top of the lid has a third cut out configured to receive the serial bus, and wherein the base includes a plurality of slots around its periphery and the sidewalls include a plurality of tabs, wherein each tab is configured to align with a respective slot when the lid and the base are joined together.\n\nThe system further comprises a serial bus port located on the microprocessor; a power port configured to connect the rechargeable battery to the microprocessor, a first output pin configured to connect to a first wire, wherein the first wire is connected to a power input of the push button switch, a second output pin configured to connect a second wire, wherein the second wire is connected to a power output of the digital camera 106, and a third output pin configured to connect to the speaker 114.\n\nThe microprocessor is configured to perform the data processing by segmenting the Braille image into a plurality of segments, reducing image noise from the plurality of segments to generate a plurality of denoised segments, and resizing the plurality of denoised segments to generate a plurality of resized denoised segments.\n\nThe microprocessor is configured to perform one or more image recognition operations on the plurality of resized denoised segments to match the Braille image to a textural character corresponding to the Braille image by: training a deep learning based convolutional neural network on a dataset of Braille scripts, the Braille scripts including one or more of Arabic Braille numerals, Arabic Braille alphabet, Arabic Braille text, Arabic numerals, Arabic alphabet, Arabic text, English Braille numerals, English Braille alphabet, English Braille text, English numerals, English alphabet and English text to generate a plurality of Braille script recognition classes; applying the plurality of resized denoised segments to the deep learning based convolutional neural network, matching each of the plurality of resized denoised segments to a Braille script recognition class, and retrieving the audio waveform associated with the Braille script recognition class.\n\nThe second embodiment is illustrated with respect to FIGS. 1-9. The second embodiment describes a method for converting printed Braille dots to speech. The method comprising capturing a Braille image of the printed Braille dots, matching, by performing data processing and one or more image recognition operations, the Braille image to a textural character corresponding to the Braille image, converting the textural character to an audio waveform, transmitting the audio waveform to a speaker 114, and generating, by the speaker 114, a sound representative of a spoken word corresponding to the textural character.\n\nThe method further comprising segmenting the Braille image into a plurality of segments, reducing image noise from the plurality of segments to generate a plurality of denoised segments, and resizing the plurality of denoised segments to generate a plurality of resized denoised segments.\n\nThe method further comprising training a deep learning based convolutional neural network on a dataset of Braille scripts, the Braille scripts including one or more of Arabic Braille numerals, Arabic Braille alphabet, Arabic Braille text, Arabic numerals, Arabic alphabet, Arabic text, English Braille numerals, English Braille alphabet, English Braille text, English numerals, English alphabet and English text to generate a plurality of Braille script recognition classes; applying the plurality of resized denoised segments to the deep learning based convolutional neural network, and matching each of the plurality of resized denoised segments to a Braille script recognition class, and retrieving the audio waveform associated with the Braille script recognition class.\n\nThe third embodiment is illustrated with respect to FIGS. 1-9. The third embodiment describes a method for converting printed Braille dots to speech. The method comprising directing a lens of a digital camera 106 confined in a 3D ring case 102, 200 towards Braille text including printed Braille dots, compressing a push button to capture a Braille image of the printed Braille dots, matching, by a microprocessor configured to perform data processing and one or more image recognition operations, the Braille image to a textural character corresponding to the Braille image, converting the textural character to an audio waveform, transmitting the audio waveform to a speaker 114, and generating, by the speaker 114, a sound representative of a spoken word corresponding to the textural character.\n\nThe method further comprising segmenting the Braille image into a plurality of segments, reducing image noise from the plurality of segments to generate a plurality of denoised segments, and resizing the plurality of denoised segments to generate a plurality of resized denoised segments.\n\nThe method further comprising training a deep learning based convolutional neural network on a dataset of Braille scripts, the Braille scripts including one or more of Arabic Braille numerals, Arabic Braille alphabet, Arabic Braille text, Arabic numerals, Arabic alphabet, Arabic text, English Braille numerals, English Braille alphabet, English Braille text, English numerals, English alphabet and English text to generate a plurality of Braille script recognition classes, applying the plurality of resized denoised segments to the deep learning based convolutional neural network, matching each of the plurality of resized denoised segments to a Braille script recognition class, and retrieving the audio waveform associated with the Braille script recognition class.\n\nThe method further comprising transmitting, to the speaker, a sound representative of an Arabic spoken word corresponding to an Arabic textural character associated with Arabic Braille printed dots.\n\nThe method further comprising displaying, on the digital camera 106, one of a written word or a picture representative of the textural character corresponding to the Braille image.\n\nFIG. 7 is an illustration of a non-limiting example of details of computing hardware used in the computing system, according to exemplary aspects of the present disclosure. In FIG. 7, a controller 700 is described is representative of the system 600 of FIG. 6 in which the controller is a computing device which includes a CPU 701 which performs the processes described above/below. The process data and instructions may be stored in memory 702. These processes and instructions may also be stored on a storage medium disk 704 such as a hard drive (HDD) or portable storage medium or may be stored remotely.\n\nFurther, the claims are not limited by the form of the computer-readable media on which the instructions of the inventive process are stored. For example, the instructions may be stored on CDs, DVDs, in FLASH memory, RAM, ROM, PROM, EPROM, EEPROM, hard disk or any other information processing device with which the computing device communicates, such as a server or computer.\n\nFurther, the claims may be provided as a utility application, background daemon, or component of an operating system, or combination thereof, executing in conjunction with CPU 701, 703 and an operating system such as Microsoft Windows 7, Microsoft Windows 10, UNIX, Solaris, LINUX, Apple MAC-OS, and other systems known to those skilled in the art.\n\nThe hardware elements in order to achieve the computing device may be realized by various circuitry elements, known to those skilled in the art. For example, CPU 701 or CPU 703 may be a Xenon or Core processor from Intel of America or an Opteron processor from AMD of America, or may be other processor types that would be recognized by one of ordinary skill in the art. Alternatively, the CPU 701, 703 may be implemented on an FPGA, ASIC, PLD or using discrete logic circuits, as one of ordinary skill in the art would recognize. Further, CPU 701, 703 may be implemented as multiple processors cooperatively working in parallel to perform the instructions of the inventive processes described above.\n\nThe computing device in FIG. 7 also includes a network controller 706, such as an Intel Ethernet PRO network interface card from Intel Corporation of America, for interfacing with network 760. As can be appreciated, the network 760 can be a public network, such as the Internet, or a private network such as an LAN or WAN network, or any combination thereof and can also include PSTN or ISDN sub-networks. The network 760 can also be wired, such as an Ethernet network, or can be wireless such as a cellular network including EDGE, 3G and 4G wireless cellular systems. The wireless network can also be WiFi, Bluetooth, or any other wireless form of communication that is known.\n\nThe computing device further includes a display controller 708, such as a NVIDIA GeForce GTX or Quadro graphics adaptor from NVIDIA Corporation of America for interfacing with display 710, such as a Hewlett Packard HPL2445w LCD monitor. A general purpose I/O interface 712 interfaces with a keyboard and/or mouse 714 as well as a touch screen panel 716 on or separate from display 710. General purpose I/O interface"
    }
}
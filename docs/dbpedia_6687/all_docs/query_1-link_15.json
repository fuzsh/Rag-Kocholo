{
    "id": "dbpedia_6687_1",
    "rank": 15,
    "data": {
        "url": "https://www.academia.edu/83760655/Sign_Language_Translation_Approach_to_Sinhalese_Language",
        "read_more_link": "",
        "language": "en",
        "title": "Sign Language Translation Approach to Sinhalese Language",
        "top_image": "http://a.academia-assets.com/images/open-graph-icons/fb-paper.gif",
        "meta_img": "http://a.academia-assets.com/images/open-graph-icons/fb-paper.gif",
        "images": [
            "https://a.academia-assets.com/images/academia-logo-redesign-2015-A.svg",
            "https://a.academia-assets.com/images/academia-logo-redesign-2015.svg",
            "https://a.academia-assets.com/images/single_work_splash/adobe.icon.svg",
            "https://0.academia-photos.com/attachment_thumbnails/89001306/mini_magick20220728-1-1g49b0y.png?1659006099",
            "https://0.academia-photos.com/225750314/82883144/71495160/s65_prasad.wimalaratne.jpeg",
            "https://a.academia-assets.com/images/loswp/related-pdf-icon.png",
            "https://a.academia-assets.com/images/loswp/related-pdf-icon.png",
            "https://a.academia-assets.com/images/loswp/related-pdf-icon.png",
            "https://a.academia-assets.com/images/loswp/related-pdf-icon.png",
            "https://a.academia-assets.com/images/loswp/related-pdf-icon.png",
            "https://a.academia-assets.com/images/loswp/related-pdf-icon.png",
            "https://a.academia-assets.com/images/loswp/related-pdf-icon.png",
            "https://a.academia-assets.com/images/loswp/related-pdf-icon.png",
            "https://a.academia-assets.com/images/loswp/related-pdf-icon.png",
            "https://a.academia-assets.com/images/loswp/related-pdf-icon.png",
            "https://a.academia-assets.com/images/loaders/paper-load.gif",
            "https://a.academia-assets.com/images/loswp/related-pdf-icon.png",
            "https://a.academia-assets.com/images/loswp/related-pdf-icon.png",
            "https://a.academia-assets.com/images/loswp/related-pdf-icon.png",
            "https://a.academia-assets.com/images/loswp/related-pdf-icon.png",
            "https://a.academia-assets.com/images/loswp/related-pdf-icon.png",
            "https://a.academia-assets.com/images/loswp/related-pdf-icon.png",
            "https://a.academia-assets.com/images/loswp/related-pdf-icon.png",
            "https://a.academia-assets.com/images/loswp/related-pdf-icon.png",
            "https://a.academia-assets.com/images/loswp/related-pdf-icon.png",
            "https://a.academia-assets.com/images/loswp/related-pdf-icon.png",
            "https://a.academia-assets.com/images/loswp/related-pdf-icon.png",
            "https://a.academia-assets.com/images/loswp/related-pdf-icon.png",
            "https://a.academia-assets.com/images/loswp/related-pdf-icon.png",
            "https://a.academia-assets.com/images/loswp/related-pdf-icon.png",
            "https://a.academia-assets.com/images/loswp/related-pdf-icon.png",
            "https://a.academia-assets.com/images/loswp/related-pdf-icon.png",
            "https://a.academia-assets.com/images/loswp/related-pdf-icon.png",
            "https://a.academia-assets.com/images/loswp/related-pdf-icon.png",
            "https://a.academia-assets.com/images/loswp/related-pdf-icon.png",
            "https://a.academia-assets.com/images/loswp/related-pdf-icon.png",
            "https://a.academia-assets.com/images/loswp/related-pdf-icon.png",
            "https://a.academia-assets.com/images/loswp/related-pdf-icon.png",
            "https://a.academia-assets.com/images/loswp/related-pdf-icon.png",
            "https://a.academia-assets.com/images/loswp/related-pdf-icon.png",
            "https://a.academia-assets.com/images/loswp/related-pdf-icon.png",
            "https://a.academia-assets.com/images/loswp/related-pdf-icon.png",
            "https://a.academia-assets.com/images/loswp/related-pdf-icon.png",
            "https://a.academia-assets.com/images/loswp/related-pdf-icon.png",
            "https://a.academia-assets.com/images/loswp/related-pdf-icon.png",
            "https://a.academia-assets.com/images/loswp/related-pdf-icon.png",
            "https://a.academia-assets.com/images/loswp/related-pdf-icon.png",
            "https://a.academia-assets.com/images/loswp/related-pdf-icon.png",
            "https://a.academia-assets.com/images/loswp/related-pdf-icon.png",
            "https://a.academia-assets.com/images/loswp/related-pdf-icon.png",
            "https://a.academia-assets.com/images/loswp/related-pdf-icon.png",
            "https://a.academia-assets.com/images/loswp/related-pdf-icon.png",
            "https://a.academia-assets.com/images/loswp/related-pdf-icon.png",
            "https://a.academia-assets.com/images/loswp/related-pdf-icon.png",
            "https://a.academia-assets.com/images/loswp/related-pdf-icon.png",
            "https://a.academia-assets.com/images/loswp/related-pdf-icon.png"
        ],
        "movies": [],
        "keywords": [],
        "meta_keywords": [
            ""
        ],
        "tags": null,
        "authors": [
            "Prasad Wimalaratne",
            "independent.academia.edu"
        ],
        "publish_date": "2022-07-26T00:00:00",
        "summary": "",
        "meta_description": "Sign language is used for communication between deaf persons while Sinhalese language is used by normal hearing persons whose first language is Sinhalese in Sri Lanka. This research focuses on an approach for a real-time translation from Sri Lankan",
        "meta_lang": "",
        "meta_favicon": "",
        "meta_site_name": "",
        "canonical_link": "https://www.academia.edu/83760655/Sign_Language_Translation_Approach_to_Sinhalese_Language",
        "text": "A Sign Language is a visual language that uses a system of manual, facial and body movements as the means of communication. Sign language is not an universal language, and different sign languages are used in different countries, like the many spoken languages all over the world. Sign languages that exist around the world are usually identified by the country where they are used such as Ethiopian Sign Language (EthSL). Sign language is the basic alternative communication method between hearing impaired people and several dictionaries of words have been defined to make this communication possible. Even if it is widely used in the hearing impaired community, they struggle to communicate with hearing people due to the language barrier. Due to this communication gap hearing impaired people encounter so many problems in their daily life since they are living with the people who communicate with spoken languages. Unfortunately, few people have knowledge of sign language in our daily life. In general, interpreters can help us to communicate with these challengers, but they only can be found in Government Agencies. Moreover, it is expensive to employ interpreter on personal behalf and inconvenient when privacy is required. Consequently, it is very important to develop a system which fills the communication gap between the hearing impaired and hearing people. Many researches are conducted on the recognition of EthSL but mostly they are limited to recognition of isolated words and highly affected by lighting and complex background. The goal of this study is to recognize Continuous Signs in EthSL In this research, we use Three Dimensional (3D) depth information from hand motions and body joints generated from Microsoft Kinect. After extracting manual and non-manual sign language features, the system stores them in a gesture dictionary. A Random Forest algorithm is used to match gestures with those stored in the dictionary and later converted to Amharic text. we proposed language model providing simple solution with inverted indexing concept to reorder topic-comment pattern and the subject topic pattern of EthSL sentence to subjectobject-verb sentence pattern of spoken Amahric language. We also address challenges such as Movement Epenthesis (ME) and sign segmentation which appears in continuous sign language recognition by analyzing hand movement pauses. The performance of the system was measured in two categories at: word, and sentence level and we got system accuracy of 84.4% at word level and 60% at sentence level. Keywords: Sign Language, Random Forest, Recognition, Continuous Translation\n\nIn this thesis, we propose a human-computer interaction platform for the hearing impaired, that would be used in hospitals and banks. In order to develop such a system, we collected BosphorusSign, a Turkish Sign Language corpus in health and finance domains, by consulting sign language linguists, native users and domain specialists. Using a subset of the collected corpus, we have designed a prototype system, which we called HospiSign, that is aimed to help the Deaf in their hospital visits. The HospiSign platform guides its users through a tree-based activity diagram by asking specific questions and requiring the users to answer from the given options. In order to recognize signs that are given as answers to the interaction platform, we proposed using hand position, hand shape, hand movement and upper body pose features to represent signs. To model the temporal aspect of the signs we used Dynamic Time Warping and Temporal Templates. The classification of the signs are done using k-Nearest Neighbors and Random Decision Forest classifiers. We conducted experiments on a subset of BosphorusSign and evaluated the effectiveness of the system in terms of features, temporal modeling techniques and classification methods. In our experiments, the combination of hand position and hand movement features yielded the highest recognition performance while both of the temporal modeling and classification methods gave competitive results. Moreover, we investigated the effects of using a tree-based activity diagram and found the approach to not only increase the recognition performance, but also ease the adaptation of the users to the system. Furthermore, we investigated domain adaptation and facial landmark localization techniques and examined their applicability to the gesture and sign language recognition tasks.\n\nThis paper investigated studentsâ€™ achievement for learning American Sign Language (ASL), using two different methods. There were two groups of samples. The first experimental group (Group A) was the game-based learning for ASL, using Kinect. The second control learning group (Group B) was the traditional face-to-face learning method, generally used in sign language skill training for students with hearing impairments. This study was separated into two phases. In Phase I: 3D trajectory matching measurement algorithm, the Euclidean distance algorithm was employed to present the similarity between teacher and student datasets. Then, Phase II: Effectiveness of the Game-Based Learning system, showed the proposed framework of the game-based learning for sign language. Moreover, knowledge of sign language together with the corresponding actions were captured from three sign language experts using the knowledge engineering method. Then, the proposed game-based system would be analysed to provide students immediate feedbacks and suggestions based on the knowledge transfer from ASL experts. In the experiment, the students (N=31) were divided into two groups. The first group, Group A (N=17), learnt with the game-based learning while the second group, Group B (N=14), learnt with the traditional face to face learning method. The study result showed a significant difference (p<0.05) on the mean score of the post-tests for both Group A and Group B. It also presented that the game-based learning approach provided a better performance of ASL vocabularies than the traditional face-to-face learning approach. Finally, in the section of Discussion and Conclusion, the effectiveness and the future work opportunity of the proposed game-based learning system were discussed for the improvement of sign language actions.\n\nAcross the world, several millions of people use sign language as their main way of communication with their society, daily they face a lot of obstacles with their families, teachers, neighbours, employers. According to the most recent statistics of World Health Organization, there are 360 million persons in the world with disabling hearing loss i.e. (5.3% of the world's population), around 13 million in the Middle East. Hence, the development of automated systems capable of translating sign languages into words and sentences becomes a necessity. We propose a model to recognize both of static gestures like numbers, letters, ...etc and dynamic gestures which includes movement and motion in performing the signs. Additionally, we propose a segmentation method in order to segment a sequence of continuous signs in real time based on tracking the palm velocity and this is useful in translating not only pre-segmented signs but also continuous sentences. We use an affordable and compact device called Leap Motion controller, which detects and tracks the hands' and fingers' motion and position in an accurate manner. The proposed model applies several machine learning algorithms as Support Vector Machine (SVM), K-Nearest Neighbour (KNN), Artificial Neural Network (ANN) and Dynamic Time Wrapping (DTW) depending on two different features sets. This research will increase the chance for the Arabic hearing-impaired and deaf persons to communicate easily using Arabic Sign language(ArSLR). The proposed model works as an interface between hearing-impaired and normal persons who are not familiar with Arabic sign language, overcomes the gap between them and it is also valuable for social respect. The proposed model is applied on Arabic signs with 38 static gestures (28 letters, numbers (1:10) and 16 static words) and 20 dynamic gestures. Features selection process is maintained and we get two different features sets. For static gestures, KNN model dominates other models for both of palm features set and bone features set with accuracy 99 and 98% respectively. For dynamic gestures, DTW model dominates other models for both palm features set and bone features set with accuracy 97.4% and 96.4% respectively."
    }
}
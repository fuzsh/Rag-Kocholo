{
    "id": "dbpedia_6687_2",
    "rank": 6,
    "data": {
        "url": "https://arxiv.org/html/1906.02358v22",
        "read_more_link": "",
        "language": "en",
        "title": "Survey on Publicly Available Sinhala Natural Language Processing Tools and Research",
        "top_image": "",
        "meta_img": "",
        "images": [],
        "movies": [],
        "keywords": [],
        "meta_keywords": [
            "Sinhala",
            "Natural Language Processing",
            "Resource Poor Language"
        ],
        "tags": null,
        "authors": [],
        "publish_date": null,
        "summary": "",
        "meta_description": "",
        "meta_lang": "en",
        "meta_favicon": "",
        "meta_site_name": "",
        "canonical_link": null,
        "text": "Nisansa de Silva Nisansa de Silva is with the Department of Computer Science & Engineering, University of Moratuwa.\n\nE-mail: nisansa@cse.mrt.ac.lk Manuscript revised: May 1, 2024.\n\nAbstract\n\n\\justify\n\nSinhala is the native language of the Sinhalese people who make up the largest ethnic group of Sri Lanka. The language belongs to the globe-spanning language tree, Indo-European. However, due to poverty in both linguistic and economic capital, Sinhala, in the perspective of Natural Language Processing tools and research, remains a resource-poor language which has neither the economic drive its cousin English has nor the sheer push of the law of numbers a language such as Chinese has. A number of research groups from Sri Lanka have noticed this dearth and the resultant dire need for proper tools and research for Sinhala natural language processing. However, due to various reasons, these attempts seem to lack coordination and awareness of each other. The objective of this paper is to fill that gap of a comprehensive literature survey of the publicly available Sinhala natural language tools and research so that the researchers working in this field can better utilize the contributions of their peers. As such, we shall be uploading this paper to arXiv and perpetually update it periodically as a living research article to reflect the advances made in the field. This manuscript is at version 5.4.05.4.05.4.05.4.0.\n\nIndex Terms:\n\nSinhala, Natural Language Processing, Resource Poor Language\n\n1 Introduction\n\nSinhala language, being the native language of the Sinhalese people [2, 3, 4], who make up the largest ethnic group of the island country of Sri Lanka, enjoys being reported as the mother tongue (L1) of approximately 16 million people [5, 6]. When both L1 and L2 speakers are counted, 79.7%percent79.779.7\\%79.7 % of the total Sri Lankan population are literate in Sinhala [7]. A strong correlation between the island of Sri Lanka and the usage of Sinhala has been observed [8] . This implies two things: 1) the majority of the Sinhala linguistic sources are located in the geographical area of Sri Lanka, and 2) the majority of the available linguistic sources in Sri Lanka are in Sinhala.\n\nTo give a brief linguistic background for the purpose of aligning the Sinhala language with the baseline of English, primarily it should be noted that the Sinhala language belongs to the same Indo-European language tree [9, 10]. However, unlike English, which is part of the Germanic branch, Sinhala belongs to the Indo-Aryan branch. Further, Sinhala, unlike English, which borrowed the Latin alphabet, has its own writing system, which is a descendant of the Indian Brahmi script [11, 12, 13, 14, 15, 16, 17]. By extension, this makes Sinhala Script a member of the Aramaic family of scripts [18, 19]. Thus by inheritance, the Sinhala writing system is abugida (alphasyllabary), which to say, that consonant-vowel sequences are written as single units [20]. We show the evolutionary eras of the Sinhala script in Appendix A. It should be noted that the modern Sinhala language has loanwords from languages such as Tamil, English, Portuguese, and Dutch due to various historical reasons [21]. Regardless of the rich historical array of literature spanning several millennia (starting between 3r‚Å¢dsuperscript3ùëüùëë3^{rd}3 start_POSTSUPERSCRIPT italic_r italic_d end_POSTSUPERSCRIPT to 2n‚Å¢dsuperscript2ùëõùëë2^{nd}2 start_POSTSUPERSCRIPT italic_n italic_d end_POSTSUPERSCRIPT century BCE [22, 23]), modern natural language processing tools for the Sinhala language are scarce [24, 25]. Further, Sinhala is a diglossic language [26] which has two variations: the literary form and the spoken form.\n\nNatural Language Processing (NLP) is a broad area covering all computational processing and analysis of human languages. To achieve this end, NLP systems operate at different levels [27, 28, 29]. A graphical representation of NLP layers and application domains are shown in Figure 1. On one hand, according to Liddy [28], these systems can be categorized into the following layers; phonological, morphological, lexical, syntactic, semantic, discourse, and pragmatic. The phonological layer deals with the interpretation of language sounds. As such, it consists of mainly speech-to-text and text-to-speech systems. In cases where one is working with the written text of the language rather than speech, it is possible to replace this layer with tools which handle Optical Character Recognition (OCR) and language rendering standards (such as Unicode [30]). The morphological layer analyses words at their smallest units of meaning. As such, analysis on word lemmas and prefix-suffix-based inflection are handled in this layer. Lexical layer handles individual words. Therefore tasks such as Part of Speech (PoS) tagging happens here. The next layer, syntactic, takes place at the phrase and sentence level where grammatical structures are utilized to obtain meaning. Semantic layer attempts to derive the meanings from the word level to the sentence level. Starting with Named Entity Recognition (NER) at the word level and working its way up by identifying the contexts they are set in until arriving at overall meaning. The discourse layer handles meaning in textual units larger than a sentence. In this, the function of a particular sentence maybe contextualized within the document it is set in. Finally, the pragmatic layer handles contexts read into contents without having to be explicitly mentioned [27, 28]. Some forms of anaphora (coreference) resolution [31, 32, 33, 34, 35] fall into this application.\n\nOn the other hand, Wimalasuriya and Dou [29] categorize NLP tools and research by utility. They introduce three categories with increasing complexity; Information Retrieval (IR), Information Extraction (IE), and Natural Language Understanding (NLU). Information Retrieval covers applications, which search and retrieve information which are relevant to a given query. For pure IR, tools and methods up-to and including the syntactic layer in the above analysis are used. Information Extraction, on the other hand, extracts structured information. The difference between IR and IE is the fact that IR does not change the structure of the documents in question. Be them structured, semi-structured, or unstructured, all IR does is fetching them as they are. In comparison, IE, takes semi-structured or unstructured text and puts them in a machine readable structure. For this, IE utilizes all the layers used by IR and the semantic layer. Natural Language Understanding is purely the idea of cognition. Most NLU tasks fall under AI-hard category and remain unsolved [27]. However, with varying accuracy, some NLU tasks such as machine translation are being attempted. The pragmatic layer of the above analysis belongs to the NLU tasks while the discourse layer straddles information extraction and natural language understanding [27].\n\nThe objective of this paper is to serve as a comprehensive survey on the state of natural language processing resources for the Sinhala language. The initial structure and content of this survey are heavily influenced by the preliminary surveys carried out by de Silva [24] and Wijeratne et al. [27]. However, our hope is to host this survey at arXiv as a perpetually evolving, living research article [37, 38] which continuously gets updated as new research and tools for Sinhala language are created and made publicly available. We also discuss how the non-compliance of policies to put data and code online [39], after the research is concluded and the paper is published, has negatively impacted the growth and sustainability of Sinhala NLP. Hence, it is our hope that this work will help future researchers who are engaged in Sinhala NLP research to conduct their literature surveys efficiently and comprehensively. For the success of this survey, we shall also consider the Sri Lankan NLP tools repository, lknlp. This manuscript is at version 5.4.05.4.05.4.05.4.0. The latest version of the manuscript can be obtained from arXiv or ResearchGate .\n\nFigure 6 in Appendix B shows the most prolific researchers in the domain of Sinhala NLP. The nodes contain the name of the researcher along with the total number of Sinhala NLP papers that the researcher has authored. The edges between the two researchers are labelled with the number of Sinhala NLP papers the relevant pair of researchers have co-authored. When selecting authors, we have applied a threshold of 3 Sinhala NLP publications. Given that the objective of the visualization is to portray the cooperation between researchers, we have also added the strongest edge that connects each researcher to the rest of the researchers in the graph. The few isolated nodes are researchers who have authored at least 3 Sinhala NLP publications but do not have any coauthored papers with anyone else on the graph. We have also added labels to clusters in cases where all or the majority of researchers in those clusters have the same affiliation. It is observable that the cluster from the Department of Computer Science & Engineering, University of Moratuwa is the most prolific in Sinhala NLP research.\n\nFigure 7 in Appendix B shows the probability of studies from the institutions to which the most prolific authors from Figure 6 are affiliated citing institutions of the same set. We observe both interesting and disturbing trends which we discuss in the figure caption. Figure 8 in Appendix B shows the mapping between authors with at least 10 papers in the Sinhala NLP domain and their research interests denoted by the subsection titles of the Section 3 of this paper. If two or more authors on the diagram have co-authored a paper, it gets counted for each of the authors separately without any bias on the author order listed on the publication. For example, the paper Building a wordnet for Sinhala [40] is counted for both Nisansa de Silva and Gihan Dias. If a single paper contributes to more than one research area, that paper is counted for all of the research areas to which it contributes. For example, the paper Sinhala Text Classification: Observations from the Perspective of a Resource Poor Language [24] which introduces a new Sinhala text classification data set is counted for both Data Sets (Section 3.2) and Text Classification (Section 3.10).\n\nThe remainder of this survey is organized as follows; Section 2 introduces some important properties and conventions of the Sinhala language which are important for the development and understanding of Sinhala NLP. Section 3 discusses the various tools and research available for Sinhala NLP. In this section we would discuss both pure Sinhala NLP tools and research as well as hybrid Sinhala-English work. We will also discuss research and tools which contributes to Sinhala NLP either along with or by the help of Tamil, the other official language of Sri Lanka. Section 4 gives a brief introduction to the primary language sources used by the studies discussed in this work. Finally, Section 5, concludes the survey.\n\n2 Properties of the Sinhala Language\n\nBefore moving on to discussing Sinhala NLP resources, we shall give a brief introduction to some of the important properties of Sinhala language, which impact the development of Sinhala NLP resources. Sinhala grammar has two forms: written (literary) and spoken. These forms differ from each-other in their core grammatical structures [41, 1, 42]. The written form strictly adheres to the SOV (Subject, Object, and Verb) configuration [43, 44]. Further, in the written form, subject-verb agreement is enforced [45] such that, in order to be grammatically correct, the subject and the verb must agree in terms of: gender (male/female), number (singular/plural) and person (1st/2nd/3rd). However, in spoken Sinhala, the SOV order can be neglected [46] and male singular 3rd person verb can be used for all nouns [45]. Sinhala is also a head-final language, where the complements and modifiers would appear before their heads [47] this is similar to that of English and dissimilar to that of French. In total, according to Abhayasinghe [48], there are 25 types of simple sentence structures in Sinhala. Similar to many Indo Aryan languages, animacy plays a major role in Sinhala grammar in syntactic and semantic roles [49, 50, 51]. Comparative studies done by Noguchi [52] and by Miyagishi [53, 42] have found that animacy extends its influence from phrase level to sentence level in Sinhala (e.g., Usage of post-positions [43, 54]). On this matter, Table I explains grammatical cases and inflections of animate common nouns while Table II explains grammatical cases and inflections of inanimate common nouns. We provide a comparative analysis of parsing the very simple English sentence ‚ÄúI eat a red apple‚Äù and its Sinhala, Hindi, and French translations in Fig 2. English and French parsing was done using the Stanford Parser. Hindi parsing was done using the IIIT-Hyderabad Parser and the study by Singh et al. [55].\n\nHerath et al. [56, 57] argue that pure Sinhala words did not have suffixes and that adding suffixes was incorporated to Sinhala after 12th century BC with the influx of Sanskrit words. With this, they declare Sinhala to have to following types of words:\n\n1.\n\nSuffixes\n\n2.\n\nNouns\n\n3.\n\nCases\n\n4.\n\nVerbs\n\n5.\n\nConjunctions and articles\n\n6.\n\nAdjectives\n\n7.\n\nDemonstratives, Interrogatives, and negatives\n\n8.\n\nParticles and prefixes\n\nThey further divide nouns into five groups: material, agentive, common, abstract, and proper. In addition to these, they also introduce compound nouns.We show the noun categorization proposed by Herath et al. [56] in Table III.\n\nHerath et al. [57] categorize Sinhala suffixes along the attributes of: gender, number, definiteness, case, and conjunctive. They further claim that there are 3 types of suffixes: Suf1 adds gender, number, and definiteness; Suf2 adds case; and Suf3 adds conjunctive. Conjunctive is claimed to be equivalent to too and and in English. We show an extension of the suffix structure proposed by Herath et al. [57] in Table IV. In their analysis on register variation (vocabulary) of 60 languages, Li et al. [58] observes that Sinhala exhibits homogeneity between 0.5 and 1.0 in the three corpora considered: CC (macro-web register - Common Crawl), TW (social media register - Twitter), and WK (Wikipedia register - March 2020).\n\n3 Sinhala NLP resources\n\nIn this section we generally follow the structure shown in Figure 1 for sectioning. However, in addition to that, we also discuss topics such as available corpora, other data sets, dictionaries, and WordNets. We focus on NLP tools and research rather than the mechanics of language script handling [59, 60, 61, 62, 63, 64]. One of the earliest attempts on Sinhala NLP was done by Herath et al. [65]. However, progress on that project has been minimal due to the limitations of their time. The later work by Nandasara [66] has not caught much of the advances done up to the time of its publication. Given that it was a decade old by the time the first edition of this survey was compiled, we observe the existence of many new discoveries in Sinhala NLP which have not been taken into account by it. A review on some challenges and opportunities of using Sinhala in computer science was done by Nandasara and Mikami [67]. At this point, it is worth noting that the largest number of studies in Sinhala NLP has been on optical character recognition (OCR) rather than on higher levels of the hierarchy shown in Figure 1. On the other hand, the most prolific single project of Sinhala NLP we have observed so far is an attempt to create an end-to-end Sinhala-to-English translator [68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 20, 79, 80, 81, 82, 83, 84, 85]. The sinling on GitHub contains a collection of tools for handling Sinhala NLP including a Tokenizer, Stemmer, PoS Tagger, Morphological joiner, and a Morphological splitter. The package is available for Python via pypi.\n\nTamil, the other official language of Sri Lanka is also a resource-poor language. However, due to the existence of larger populations of Tamil speakers worldwide, including but not limited to economic powerhouses such as India, there are more research and tools available for Tamil NLP tasks [27]. Therefore, it is rational to notice that Sinhala and Tamil NLP endeavours can help each other. Especially, given the above fact, that these are official languages of Sri Lanka, results in the generation of parallel data sets in the form of official government documents and local news items. A number of researchers make use of this opportunity. We shall be discussing those applications in this paper as well. Further, there have been some fringe implementations, which bridge Sinhala with other languages such as Japanese [23, 86, 87, 88, 10].\n\n3.1 Corpora\n\nFor any language, the key for NLP applications and implementations is the existence of adequate corpora. On this matter, a relatively substantial Sinhala text corpus was created by Upeksha et al. [89, 90] by web crawling. It was later extended by adding Jathaka Stories and more web-crawled news articles . Later a smaller Sinhala news corpus was created by de Silva [24]. Both of the above corpora are publicly available. However, none of these come close to the massive capacity and range of the existing English corpora.\n\nA word corpus of approximately 35,000 entries was developed by Weerasinghe et al. [91]. But it does not seem to be online anymore. Guzm√°n et al. [92] provided two monolingual corpora for Sinhala. Those were a 155k+ sentences of filtered Sinhala Wikipedia and 5178k+ sentences of Sinhala common crawl . Wijeratne and de Silva [93] have publicly released a massive corpus of text and stop words taken from a decade of Sinhala Facebook posts. While the stop word extraction of Wijeratne and de Silva [93] algorithmic, Lakmal et al. [94] has introduced a manually curated stop word list . A parallel corpus of Sinhala and English was collected by Ba√±√≥n et al. [95] containing 217,407 sentences and available to download from their website . However, the later audit by Caswell et al. [96] raised issues on the quality of that data set. A parallel corpus of aligned Sinhala-English documents and sentences obtained from crawling the web was released by Sachintha et al. [97]. The study by Warusawithana et al. [98] created a refined version of the OpenSLR-52 speech corpus for Sinhala by Kjartansson et al. [99] . Dhananjaya et al. [101] created sin-cc-15M corpus , which they claim, at the time of their publication to be the largest monolingual Sinhala corpus. A text corpus collected from the Sihala blog Kalaya along with the relevant code is available on Github . A Sinhala NEWS corpus of more than 500,000 articles collected from various online free news websites was released by Hettiarachchi et al. [102].\n\nAs for Sinhala-Tamil corpora, Hameed et al. [103] claim to have built a sentence-aligned Sinhala-Tamil parallel corpus and Mohamed et al. [104] claim to have built a word aligned Sinhala-Tamil parallel corpus. However, at the time of writing this paper, neither of them was publicly available. A very small Sinhala-Tamil aligned parallel corpus created by Farhath et al. [105] using order papers of government of Sri Lanka is available to download . Vasantharajan and Thayasivam [106] used Printed Character Recognition (PCR) to create a large-scale Tamil-Sinhala-English parallel corpus which they claimed to be available on Github . Their follow-up work [107] on adapting the Tesseract engine to handle non-Unicode (legacy fonts) in pdf documents resulted in an improved version of the corpus which also can be found on Github .\n\n3.2 Data Sets\n\nSpecific data sets for Sinhala, as expected, is scarce. However, a Sinhala PoS tagged data set [108, 109, 110] is available to download from github . Further, a Sinhala NER data set created by Manamini et al. [111] is also available to download from github . Liyanage et al. [112] analyzed Sinhala fastText and Word2Vec in the context of cross-lingual embedding spaces.\n\nFacebook has released FastText [113, 114, 115] models for the Sinhala language trained using the Wikipedia corpus. They are available as both text models and binary files . Using the above models by Facebook, Lakmal et al. [94] have created an extended FastText model trained on Wikipedia, News, and official government documents. The binary file of the trained model is available to be downloaded. Herath et al. [116], Herath and Medagoda [117] have compiled a report on the Sinhala lexicon for the purpose of establishing a basis for NLP applications. A comparative analysis of Sinhala word embedding has been conducted by Lakmal et al. [94]. A similar study was then conducted by Silva [118] analyising the progress that had been made in NLP in the intervening years. A dataset consisting of 3576 Sinhala documents drawn from Sri Lankan news websites and tagged (CREDIBLE, FALSE, PARTIAL or UNCERTAIN) was published by Jayawickrama et al. [119] A benchmark data set for Sinhala spell correction was created by Sonnadara et al. [120] which they put online on Github . De Saa and Ranathunga [121] has released a data set for Sinhala hate speech detection which consists of comments from youtube. Another data set consisting of Sinhala hate speech comments pulled from Facebook is available on Kaggle but it does not have an accompanying paper. Similarly, Perera et al. [122] have released a hate annotated dataset of 1600+ annotated Sinhala tweets. A text-to-speech data set with 3300 Sinhala sentences with 7.5 hours of recordings was released by the Path Nirvana Foundation. The open parallel corpus (OPUS) by Tiedemann [123] curates the largest collection of parallel data between Sinhala and other languages. Zhang et al. [124] sampled data from OPUS for 100 languages (including Sinhala) and created the OPUS100 data set as a benchmark for neural machine translation (NMT). Thier code is also available on GitHub . Further, Tiedemann [125] created an NMT benchmark named Tatoeba MT Challenge using OPUS data based on Tatoeba . The English-Sinhala data set can also be directly downloaded from a link on Github .\n\nA benchmark Sinhala-English translation data set named FLORES was created by [92]. This includes a 600k+ Sinhala-English subtitle pairs initially collected by [126], 45k+ Sinhala-English sentence pairs from GNOME , KDE , and Ubuntu . They further provided two monolingual corpora for Sinhala. Those were a 155k+ sentences of filtered Sinhala Wikipedia and 5178k+ sentences of Sinhala common crawl . In addition to the data set, they also have made their code publicly available. This work was then extended by NLLB Team et al. [127] of which the data and code is available on github under the No Language Left Behind (NLLB) project . Very importantly, they also provide a toxicity data set for Sinhala which can be used for hate speech detection in or by translation or otherwise.\n\nJenarthanan et al. [128] introduced the ACTSEA dataset which contains Sinhala and Tamil tweets annotated with emotions. They have 318,308 Sinhala tweets annotated. Dhananjaya et al. [101] have made publicly available three data sets developed for their study: 1) They have taken the corpus created by Sachintha et al. [97] and derived a Sinhala News source classification data set, They have taken the corpus created by de Silva [24] and derived a Sinhala News source classification data set, and 3) They have taken the corpus created by Upeksha et al. [89] and derived a Sinhala writing style classification data set. A data set of Sinhala swear and/or obscene words is publicly available in both Unicode and Singlish formats. Fernando et al. [129] has created aligned corpora for Sinhala‚ÄìEnglish, Tamil‚ÄìEnglish, and Sinhala‚ÄìTamil language pairs. The corpora are available on github as well as the code for their document and sentence alignment task. Buddhika et al. [130] introduce a crowd sourcing tool that they named Voicer to collect speech data. They claim that the tool is open source and that they had created a Sinhala Speech corpus of 10 hours with 39 different sentences in the banking domain. Neither the code nor the data is publicly linked in the research paper. However, a subsequent work by Hellarawa and Thayasivam [131] uses this data set. Thus, it can be assumed that this data set may be available if the authors are contacted.\n\nRanasinghe et al. [132] created a benchmark dataset they named SOLD: Sinhala Offensive Language Dataset which contains 10,000 posts from Twitter annotated both at sentence-level and token-level with the two classes offensive and not offensive. In the same paper, they also introduce the dataset SemiSOLD which contains 145,000 Sinhala tweets annotated with the same classes but with a semi-supervised approach. The relevant code is also available on Github . Sinhala is included in the 2800+ language metadata set composed by van Esch et al. [133]. This can be used for comparative analysis of Sinhala against other languages in the data set. Ruder et al. [134] created the multi-language dataset named XTREME-UP which contains Sinhala data sets for OCR and Transliteration tasks. Pratap et al. [135] created the The Massively Multilingual Speech (MMS) data set which contains Sinhala for the Spoken Language Identification (LID) task. A data set and a model for the Language Identification (LID) task including Sinhala were created by Burchell et al. [136]. A large Sinhala-English dictionary with 1,368,41613684161,368,4161 , 368 , 416 unfiltered and 195,255195255195,255195 , 255 filtered En-Sipairs has been made publicly available on github by Wickramasinghe and De Silva [137]. In their follow-up work [138] they used that dictionary data to obtain Sinhala-English embedding alignment. The relevant code and data can be accessed on GitHub . Nguyen et al. [139] released CulturaX a cleaned dataset for training Large Language Models. It contains 753,655 Sinhala documents with 880,289,097 tokens. However, it should be noted that this only amounts to 0.01% of their multilingual data set. In comparison, Tamil has a 0.07% share, Hindi has a 0.27% share, and English has a 45.13% share. Kudugunta et al. [140] released MADLAD-400 a manually audited, general domain, large document level dataset spanning 419 languages. For Sinhala, they report that they have 788K noisy docs, 349.2K clean docs, 22.1M noisy sentences, 16M clean sentences, 3.4B noisy tokens, and 1.9B clean tokens. Further, for the translation tasks they report that they have 7363378 sentence pairs where one of the languages in each pair is Sinhala.\n\nXL-sum Hasan et al. [141] and M3LS Verma et al. [142] are text summarizing datasets which contains Sinhala data collected from the BBC website . Respectively, they have 3,414 and 10,148 Sinhala text documents and their summaries. Being a multi-modal dataset, M3LS additionally contains the relevant images collected from the same source. A dynamic word-level Sinhala Sign Language video dataset of 50 classes has been published by Charuka et al. [143]. It contains 1110 videos. The OSCAR dataset [144, 145] contains web mined Sinhala data consisting of 172,755,385 words in 301,066 documents.\n\nThe Dakshina dataset by [146] contains Native Sinhala script data (200k train and 28k validation sentences) along with Romanized (Singlish) data (10k sentences) from Wikipedia. It also contains a Romanization Lexicon (25k train and 5k test). Ranathunga et al. [147] have publicly released their data, code, and models for Sinhala-English translation. Very importantly, this includes a human-cleaned portion of the NLLB dataset [127] in En-Si .\n\nSingh et al. [148] introduced the Aya Data set, a human-curated multilingual instruction fine-tuning (IFT) data set for large language models (LLM). They also introduced Aya Collection, 44 instruction-style datasets that were created by transforming existing NLP datasets into pairs of prompt and completion and then using NMT on them to obtain data in multiple languages. Both of these data sets include Sinhala. Further, they also released Aya Evaluation suite to measure multilingual open-ended generation quality.\n\n3.3 Dictionaries\n\nA necessary component for the purpose of bridging Sinhala and English resources are English-Sinhala dictionaries. The earliest and most extensive Sinhala-English dictionary available for consumption was by Malalasekera [149]. However, this dictionary is locked behind copyright laws and is not available for public research and development. This copyright issue is shared with other printed dictionaries [150, 151, 152, 153, 154, 155] as well. The dictionary by Kulatunga [156] is publicly available for usage through an online web interface but does not provide API access or means to directly access the data set. The largest publicly available English-Sinhala dictionary data set is from a discontinued FireFox plug-in EnSiTip [157] which bears a more than passing resemblance to the above dictionary by Kulatunga [156]. Hettige and Karunananda [71] claim to to have created a lexicon to help in their attempt to create a system capable of English-to-Sinhala machine translation. A review on the requirements for English-Sinhala smart bilingual dictionary was conducted by Samarawickrama and Hettige [158]. The study by Wickramasinghe and De Silva [137] introduced a large Sinhala-English dictionary which has 1,368,41613684161,368,4161 , 368 , 416 unfiltered and 195,255195255195,255195 , 255 filtered En-Sipairs. Both their code and the two versions of the dictionary are publicly available on github . A Sinhala-Sinhala dictionary named Wahara was built and is publicly maintained by the University of Moratuwa.\n\nThere exists the government-sponsored trilingual dictionary [159], which matches Sinhala, English, and Tamil. However, other than a crude web interface on the ministry website, there is no efficient API or any other way for a researcher to access the data on this dictionary. Weerasinghe and Dias [160] have created a multilingual place name database for Sri Lanka which may function both as a dictionary and a resource for certain NER tasks.\n\n3.4 WordNets\n\nWordNets [161] are extremely powerful and act as a versatile component of many NLP applications. They encompass a number of linguistic properties which exist between the words in the lexicon of the language including but not limited to: hyponymy, hypernymy, synonymy, and meronymy. Their uses range from simple gazetteer listing applications [29] to information extraction based on semantic similarity [162, 163] or semantic oppositeness [164]. An attempt has been made to build a Sinhala Wordnet by Wijesiri et al. [40]. For a time it was hosted on [165] but it too is now defunct and all the data and applications are lost other than what Arukgoda et al. [166] have cloned to use in their application uploaded on github . However, even at its peak, due to the lack of volunteers for the crowd-sourced methodology of populating the WordNet, it was at best an incomplete product. Another effort to build a Sinhala Wordnet was initiated by Welgama et al. [167] independently from above; but it too have stopped progression even before achieving the completion level of Wijesiri et al. [40].\n\n3.5 Morphological Analyzers\n\nAs shown in Fig 1, morphological analysis is a ground level necessary component for natural language processing. Given that Sinhala is a highly inflected language [46, 45, 24], a proper morphological analysis process is vital. The earliest attempt on Sinhala morphological analysis we have observed are the studies by Herath et al. [56, 57]. They are more of an analysis of Sinhala morphology rather than a working tool. As such we discussed the observations and conclusions of these works at Section 2. It is also worth to note that these works predates the introduction of Sinhala unicode and thus use a transliteration of Sinhala in the Latin alphabet.\n\nThe next attempt by Herath et al. [168] creates a modular unit structure for morphological analysis of Sinhala. Much later, as a step on their efforts to create a system with the ability to do English-to-Sinhala machine translation, Hettige and Karunananda [68] claim to have created a morphological analyzer (void of any public data or code), which links to their studies of a Sinhala parser [69] and computational grammar [20]. Hettige et al. [80] further propose a multi-agent System for morphological analysis. Welgama et al. [169] attempted to evaluate machine learning approaches for Sinhala morphological analysis. Yet another independent attempt to create a morphological parser for Sinhala verbs was carried out by Fernando and Weerasinghe [170]. Later, another study, which was restricted to morphological analysis of Sinhala verbs was conducted by Dilshani and Dias [171]. There was no indication on whether this work was continued to cover other types of words. Further, other than this singular publication, no data or tools were made publicly accessible. Nandathilaka et al. [172] proposed a rule based approach for Sinhala lemmatizing. The work by Welgama et al. [173] claim to have set a set of gold standard definitions for the morphology of Sinhala Words; but given that their results are not publicly available, further usage or confirmation of these claims cannot not be done. The table V provides a comparative summery of the discussion above. The combined study introduced a rule-based stemmer [174] and a tokenizer [175] for Sinhala. A later work by Kumarasinghe et al. [176] named SinMorphy used a comprehensive vocabulary of Sinhala words to conduct rule-based morphological analysis on Sinhala. Ekanayaka et al. [177] compared the effectiveness of using RNN, LSTM, and GRU for Sinhala morphological analysis of Sinhala and Sinhlish deatasets and found the BiGRU gives the highest accuracy of 87.96%.\n\n3.6 Part of Speech Taggers\n\nThe next step after morphological analysis is Part of Speech (PoS) tagging. The PoS tags differ in number and functionality from language to language. Therefore, the first step in creating an effective PoS tagger is to identifying the PoS tag set for the language. This work has been accomplished by Fernando et al. [108] and Dilshani et al. [109]. Expanding on that, Fernando et al. [108] has introduced an SVM Based PoS Tagger for Sinhala and then Fernando and Ranathunga [110] give an evaluation of different classifiers for the task of Sinhala PoS tagging. While here it is obvious that there has been some follow up work after the initial foundation, it seems, all of that has been internal to one research group at one institution as neither the data nor the tools of any of these findings have been made available for the use of external researchers. Several attempts to create a stochastic PoS tagger for Sinhala has been done with the studies by Herath and Weerasinghe [178], Jayaweera and Dias [179], and Jayasuriya and Weerasinghe [180] being the most notable.\n\nWithin a single group which did one of the above stochastic studies [179], yet another set of studies was carried out to create a Sinhala PoS tagger starting with the foundation of Jayaweera and Dias [181] which then extended to a Hidden Markov Model (HMM) based approach [182] and an analysis of unknown words [183, 184]. Further, this group presented a comparison of few Sinhala PoS taggers that are available to them [185]. A RESTFul PoS tagging web service created by Jayaweera and Dias [186] using the above research can still be accessed via POST and GET. A hybrid PoS tagger for Sinhala language was proposed by Gunasekara et al. [187]. The study by Kothalawala et al. [188] discussed the data availability problem in NLP with a Sinhala POS tagging experiment among others. Withanage and Silva [189] proposed a stochastic POS tagger based on a small 10,000 word corpus drawn from Facebook and Twitter. Wijerathna [190] used Support Vector Machines (SVM) to tag Sinhala text with 30 the PoS tags that were proposed by Fernando et al. [108]. The study by Sathsarani et al. [191] compared rule-based and stochastic models against deep learning models in the task of Sinhala PoS tagging.\n\n3.7 Parsers\n\nThe PoS tagged data then needs to be handed over to a parser. This is an area which is not completely solved even in English due to various inherent ambiguities in natural languages. However, in the case of English, there are systems which provide adequate results [192] even if not perfect yet. The Sinhala state of affairs, is that, the first parser for the Sinhala language was proposed by Hettige and Karunananda [69] with a model for grammar [20]. The study by Liyanage et al. [46] is concentrated on the same given that they have worked on formalizing a computational grammar for Sinhala. While they do report reasonable results, yet again, do not provide any means for the public to access the data or the tools that they have developed. Kanduboda [45] have worked on Sinhala differential object markers relevant for parsing.\n\nThe first attempt at a Sinhala parser, as mentioned above, was by Hettige and Karunananda [69] where they created prototype Sinhala morphological analyzer and a parser as part of their larger project to build an end-to-end translator system. The function of the parser is based on three dictionaries: Base Dictionary, Rule Dictionary, and Concept Dictionary. They are built as follows:\n\n‚Ä¢\n\nThe Base Dictionary: prakurthi (base words), nipatha (prepositions), upasarga (prefixes), and vibakthi (Irregular Verbs).\n\n‚Ä¢\n\nThe Rule Dictionary: inflection rules used to generate various forms of verbs and nouns from the base words.\n\n‚Ä¢\n\nThe Concept Dictionary: synonyms and antonyms for the words found in the base dictionary.\n\nParsers are, in essence, a computational representation of the grammar of a natural language. As such, in building Sinhala parsers, it is crucial to create a computational model for Sinhala grammar. The first such attempt was taken by Hettige and Karunananda [20] with special consideration given to Morphology and the Syntax of the Sinhala language as an extension to their earlier work [69]. Here, it is worthy to note that, unlike in their earlier attempt [69], where they explicitly mentioned that they are building a parser, in this study [20], they use the much conservative claim of building a computational grammar. Under Morphology, they again handled Sinhala inflection. Their system is based on a Finite State Transducer (FST) and Context-Free Grammar (CFG) where they they modeled 85 rules for nouns and 18 rules for verbs. The specific implementation is more partial to a rule-based composer rather than parser. It is also worthy to note that this system could only handle simple sentences which only contained the following 8 constituents: Attributive Adjunct of Subject, Subject, Attributive Adjunct of Object, Object, Attributive Adjunct of Predicate, Attributive Adjunct of the Complement of Predicate, Complement of Predicate, and Predicate. With these, they propose the following grammar rules for Sinhala:\n\nS = Subject Akkyanaya Subject = SimpleSubject | ComplexSubject ComplexSubject = SimpleSubject ConSub SimpleSubject = Noun | Adjective Noun ConSub = Conjunction SimpleSubject Akkyanaya = VerbP | Object VerbP Object = SimpleObject | ComplexObject ComplexObject = Conjunction SimpleObject SimpleObject = Noun | Adjective Noun VerbP = Verb | Adverb Verb\n\nThe later work by Liyanage et al. [46] also involves formalizing a computational grammar for Sinhala. They claim that Sinhala can have any order of words in practice. However, they do not note that this is happening because practices of the spoken language, which does not share the strong SOV conventions of the written language, are slowly seeping into written text. However, they do make note of how Sinhala grammar is modeled as a head-final language [47]. They propose the Sinhala Noun Phrase (N‚Å¢PùëÅùëÉNPitalic_N italic_P) to be defined as shown in equation 1 where N‚Å¢NùëÅùëÅNNitalic_N italic_N is a noun which can be of types: common noun (NùëÅNitalic_N), pronoun (P‚Å¢r‚Å¢NùëÉùëüùëÅPrNitalic_P italic_r italic_N) or proper noun (P‚Å¢r‚Å¢o‚Å¢p‚Å¢NùëÉùëüùëúùëùùëÅPropNitalic_P italic_r italic_o italic_p italic_N). The adjectival phrase (A‚Å¢D‚Å¢J‚Å¢Pùê¥ùê∑ùêΩùëÉADJPitalic_A italic_D italic_J italic_P) is then defined as as shown in equation 2 where: D‚Å¢e‚Å¢tùê∑ùëíùë°Detitalic_D italic_e italic_t is a Determiner, A‚Å¢d‚Å¢jùê¥ùëëùëóAdjitalic_A italic_d italic_j is the adjective, and D‚Å¢e‚Å¢gùê∑ùëíùëîDegitalic_D italic_e italic_g is an optional operator Degrees which can be used to intensify the meaning of the adjective in cases where the adjective is qualitative. While they note that according to Gunasekara [193], there has to three classes of adjectives (qualitative, quantitative, and demonstrative), they do not implement this distinction in their system. Similarly, they propose Sinhala Verb Phrase (V‚Å¢PùëâùëÉVPitalic_V italic_P) to be defined as shown in equation 3 where VùëâVitalic_V is a single verb. They here note that they are ignoring compound verbs and auxiliary verbs in their grammar. The adverbial phrases (A‚Å¢D‚Å¢V‚Å¢Pùê¥ùê∑ùëâùëÉADVPitalic_A italic_D italic_V italic_P) are then recursively defined as as shown in equation 4.\n\nN‚Å¢P=[A‚Å¢D‚Å¢J‚Å¢P]‚Å¢[N‚Å¢N]ùëÅùëÉdelimited-[]ùê¥ùê∑ùêΩùëÉdelimited-[]ùëÅùëÅNP=[ADJP][NN]italic_N italic_P = [ italic_A italic_D italic_J italic_P ] [ italic_N italic_N ] (1)\n\nA‚Å¢D‚Å¢J‚Å¢P=[[D‚Å¢e‚Å¢t]‚Å¢[[D‚Å¢e‚Å¢g]‚Å¢[A‚Å¢d‚Å¢j]]]ùê¥ùê∑ùêΩùëÉdelimited-[]delimited-[]ùê∑ùëíùë°delimited-[]delimited-[]ùê∑ùëíùëîdelimited-[]ùê¥ùëëùëóADJP=\\bigg{[}[Det]\\Big{[}[Deg][Adj]\\Big{]}\\bigg{]}italic_A italic_D italic_J italic_P = [ [ italic_D italic_e italic_t ] [ [ italic_D italic_e italic_g ] [ italic_A italic_d italic_j ] ] ] (2)\n\nV‚Å¢P=[A‚Å¢D‚Å¢V‚Å¢P]‚Å¢[V]ùëâùëÉdelimited-[]ùê¥ùê∑ùëâùëÉdelimited-[]ùëâVP=[ADVP][V]italic_V italic_P = [ italic_A italic_D italic_V italic_P ] [ italic_V ] (3)\n\nA‚Å¢D‚Å¢V‚Å¢P=[[N‚Å¢P]‚Å¢[[A‚Å¢D‚Å¢V‚Å¢P]‚Å¢[[D‚Å¢e‚Å¢g]‚Å¢[A‚Å¢D‚Å¢V]]]]ùê¥ùê∑ùëâùëÉdelimited-[]delimited-[]ùëÅùëÉdelimited-[]delimited-[]ùê¥ùê∑ùëâùëÉdelimited-[]delimited-[]ùê∑ùëíùëîdelimited-[]ùê¥ùê∑ùëâADVP=\\Bigg{[}[NP]\\bigg{[}[ADVP]\\Big{[}[Deg][ADV]\\Big{]}\\bigg{]}\\Bigg{]}italic_A italic_D italic_V italic_P = [ [ italic_N italic_P ] [ [ italic_A italic_D italic_V italic_P ] [ [ italic_D italic_e italic_g ] [ italic_A italic_D italic_V ] ] ] ] (4)\n\nSimilar to Hettige and Karunananda [20], the work by Liyanage et al. [46] also builds a CFG for Sinhala covering 10 out of the 25 types of simple sentence structures in Sinhala reported by Abhayasinghe [48]. This parser is unable to parse sentences where inanimate subjects do not consider the number. Further, sentences which contain, compound verbs, auxiliary verbs, present participles, or past participles cannot be handled by this parser. If the verbs have imperative mood or negation those too cannot be handled by this. Non-verbal sentences which end with adjectives, oblique nominals, locative predicates, adverbials, or any other language entity which is not a verb cannot be handled by this parser.\n\nThe study by Kanduboda [45] covers not the whole of Sinhala parsing but analyzes a very specific property of Sinhala observed by Aissen [194] which states that it is possible to notice Differential Object Marking (DOM) in Sinhala active sentences. Kanduboda [45] define this as the choice of /wa/ and /ta/ object markers. They further observe three unique aspects of DOM in Sinhala: (a) it is only observed in active sentences which contain transitive verbs, (b) it can occur with accusative marked nouns but not with any other cases, (c) it exists only if the sentence has placed an animate noun in the accusative position. They do a statistical analysis and provide a number of short gazetteer lists as appendixes. However, they observe that further work has to be done for this particular language rule in Sinhala given that they found some examples which proved to be exceptions to the general model which they proposed.\n\n3.8 Named Entity Recognition Tools\n\nAs shown in Fig 1, once the text is properly parsed, it has to be processed using a Named Entity Recognition (NER) system. The first attempt of Sinahla NER was done by Dahanayaka and Weerasinghe [195]. Given that they were conducting the first study for Sinhala NER, they based their approach on NER research done for other languages. In this, they gave prominent notice to that of Indic languages. On that matter, they were the first to make the interesting observation that NER for Indic languages (including, but not limited to Sinhala) is more difficult than that of English by the virtue of the absence of a capitalization mechanic. Following prior work done on other languages, they used Conditional Random Fields (CRF) as their main model and compared it against a baseline of a Maximum Entropy (ME) model. However, they only use the candidate word, Context Words around the candidate word, and a simple analysis of Sinhala suffixes as their features.\n\nThe follow up work by Senevirathne et al. [196] kept the CRF model with all the previous features but did not report comparative analysis with an ME model. The innovation introduced by this work is a richer set of features. In addition to the features used by Dahanayaka and Weerasinghe [195], they introduced, Length of the Word as a threshold feature. They also introduced First Word feature after observing certain rigid grammatical rules of Sinhala. A feature of clue Words in the form of a subset of Context Words feature was first proposed by this work. Finally, they introduced a feature for Previous Map which is essentially the NE value of the preceding word. Some of these feature extractions are done with the help of a rule-based post-processor which utilizes context-based word lists.\n\nThe third attempt at Sinhala NER was by Manamini et al. [111] who dubbed their system Ananya. They inherit the CRF model and ME baseline from the work of Dahanayaka and Weerasinghe [195]. In addition to that, they take the enhanced feature list of Senevirathne et al. [196] and enrich it further more. They introduce a Frequency of the Word feature based on the assumption that most commonly occurring words are not NEs. Thus, they model this as a Boolean value with a threshold applied on the word frequency. They extend the First Word feature proposed by Senevirathne et al. [196] to a First Word/ Last Word of a Sentence feature noting that Sinhala grammar is of SOV configuration. They introduce a (PoS) Tag feature and a gazetteer lists based feature keeping in line with research done on NER in other languages. They formally introduce clue Words, which was initially proposed as a sub-feature by Dahanayaka and Weerasinghe [195], as an independent feature. Utilizing the fact that they have the ME model unlike Dahanayaka and Weerasinghe [195], they introduce a complementary feature to Previous Map named Outcome Prior, which uses the underlying distribution of the outcomes of the ME model. Finally, they introduce a Cutoff Value feature to handle the over-fitting problem.\n\nThe table VI provides a comparative summary of the discussion above. It should be noted that all three of these models only tag NEs of types: person names, location names and organization names. The Ananya system by Manamini et al. [111] is available to download at GitHub . The data and code for the approaches by Dahanayaka and Weerasinghe [195] and by Senevirathne et al. [196] are not accessible to the public. Azeez and Ranathunga [197] proposed a fine-grained NER model for Sinhala building on their earlier work on NER [111] and PoS tagging [110]. Anuruddha [198] proposed a method based on reinforcement learning for Sinhala NER. A Sinhala NER system restricted to the sports domain was introduced by Wijesinghe and Tissera [199], where they attempted to utilise classical machine learning models. The work by Mallikarachchi et al. [200] used support vector machines to detect Sinhala named entities of types person, location and, organization.\n\n3.9 Semantic Similarity\n\nA Sinhala semantic similarity measure has been developed for short sentences by Kadupitiya et al. [201]. This work has been then extended by Kadupitiya et al. [202] for the application use case of short answer grading. Data and tools for these projects are not publicly available. A sentence similarity measurement which uses Siamese neural networks was developed by Nilaxan and Ranathunga [203] where they demonstrate their results for Sinhala and Tamil. A cross-lingual document similarity measurement using the use-case of Sinhala and English was developed by Isuranga et al. [204].\n\n3.10 Text Classification\n\nText classification is a popular application on the semantic layer of the NLP stack. A very basic Sinhala text classification using Na√Øve Bayes Classifier, Zipf‚Äôs Law Behavior, and SVMs was attempted by Gallege [205]. A smaller implementation of Sinhala news classification has been attempted by de Silva [24]. As mentioned in Section 3.2, their news corpus is publicly available . Another attempt at Sinhala text classification using six popular rule-based algorithms was done by Lakmali and Haddela [206]. Even though they talk about building a corpus named SinNG5, they do not indicate of means for others to obtain the said corpus. Another study by Kumari and Haddela [207] utilizes the SinNG5 corpus as the data set for their attempt to use LIME [208] for human interpretability of Sinhala document classification. However, they too do not provide access corpus. Nanayakkara and Ranathunga [209] have implemented a system which uses corpus-based similarity measures for Sinhala text classification. Gunasekara and Haddela [210, 211] claim to have created a context-aware stop word extraction method for Sinhala text classification based on simple TF-IDF. An LSTM based textual entailment system for Sinhala was proposed by Jayasinghe and Sirts [212]. Demotte and Ranathunga [213] proposed a dual-state capsule network architecture for text classification where they demonstrated their methodology on the Sinhala data set established by Senevirathne et al. [214]. The attempt by Sameemdeen and Selvanthan [215] considers three classical machine learning algorithms (Na√Øve Bayes,SVM,and KNN) and then goes on to briefly discusses the pros and cons of previous attempts by: Nanayakkara and Ranathunga [209], Gunasekara and Haddela [210], Lakmali and Haddela [206], and Buddhika et al. [216]. Then they propose active learning [217, 218] as an alternative. However, no experimental results of how active learning would improve Sinhala text classification are given. The study by Bandara et al. [219] proposed and ontology-based approach for Sinhala fake news detection. However, their literature survey did not cover seminal papers in OBIE such as the work by Wimalasuriya and Dou [29]. This has impacted the overall methodology that was presented. A novel, domain-idependant, and domain-adaptive text classification framework named AdaptText for Sinhala text classification was proposed by Kodithuwakku and Hettiarachchi [220]. A simple TF-IDF based Sinhala text classification system was proposed by Koralage [221]. The study by [222] used Genetic Algorithm on Lucene search queries to obtain interpretable classification models for Sinhala text documents. The approach proposed by Rathnayake et al. [223] uses adapter-based [224, 225, 226, 227, 228, 229] fine-tuning on XLM-R [230], for code-mixed and code-switched text. Kirindage and Godewithana [231] used LDA [232] to develop Sinhala news topic hierarchies and categorize Sinhala news documents using the said topic hierarchies. Hettigoda [233] collected English-Sinhala code-mixed comments from Facebook pages belonging to Clothing industrial online businesses in Sri Lanka and classified them in to the classes: Inquiries, Maybe Inquiries, and Not Inquiries. A comparative analysis on BERT based models for Sinhala text classification was conducted by Dhananjaya et al. [101]. Both their code and data (sin-cc-15M corpus , Sinhala News source classification data set , Sinhala News category classification data set , and Sinhala writing style classification data set ) are publicly available. The study by Wijayarathna and Jayalal [234] used classical machine learning techniques including Random Forest to classify fake news in Sinhala on Twitter. Chathuranga and Ranathunga [235] used capsule-based methods recommended by Senevirathne et al. [214] to classify Sinhala-English code-mixed data. The work by Weerasiri et al. [236] compared Word2Vec, FastText and Doc2vec [237] against TF-IDF for Sinhala news document classification. The study by Caldera et al. [238] used stacked LSTM to classify Sinhala and Singlish text discussing COVID-19 on Youtube and Twitter.\n\n3.11 Sentiment Analysis\n\nA simple MLP-based method to classify sentiments in Sinhala text was initially proposed by Medagoda [239] based on their prior work [240]. A word2vec based tool for sentiment analysis of Sinhala news comments is available. A methodology for constructing a sentiment lexicon for Sinhala Language in a semi-automated manner based on a given corpus was proposed by Chathuranga et al. [241]. Demotte et al. [242] proposed a sentiment analysis system based on sentence-state LSTM Networks for Sinhala news comments. In the subsequent work [243], they used word similarity to generate a Sinhala semantic lexicon. They followed this up with a further study [214] which discussed a number of other deep learning models such as RNN and Bi-LSTM in the domain of Sinhala sentiment analysis. Jayasuriya et al. [244] proposed a method to classify Sinhala posts in the domain of sports into positive and negative class sentiments. Ranathunga and Liyanage [245] claimed that using word embedding models as semantic features can compensate for the lack of well developed language-specific linguistic or language resources in the case of analysing sentiment of Sinhala news comments. Jayasuriya et al. [246] conducted a comparative study between word N-grams and character N-grams in the task of semantic classification of Sinhala content in social media. Which they soon followed up with an ensemble approach [247]. The work by Karunarathne [248] used word embedding to analyse the sentiment of manually annotated Sinhala Tweets. Abeyratne and Jayaratne [249] conducted a multi model analysis on classifying Sinhala songs by emotion.\n\nThe work by Jayawickrama et al. [250, 251] used the data set released by Wijeratne and de Silva [93] to predict the reactions induced by Sinhala Facebook posts. They then extended the work [252] and compared the results obtained with their data set against that of Senevirathne et al. [214]. Aththanayaka and Naleer [253] used Random forest, Support vector machines, and Multinomial Na√Øve Bayes models to analyse sentiment in Sinhala-English code-mixed text from social media. The ACTSEA dataset for Sinhala sentiment analysis was introduced by Jenarthanan et al. [128]. The dataset contains 318,308 Sinhala tweets annotated with emotions. The study by Dhananjaya et al. [254] uses a sentiment lexicon of a high‚Äêresource language to fine-tune Pre‚Äêtrained multilingual language models (PMLMs) such as mBERT and XLM‚ÄêR on an intermediate task which in turn is then used on the sentiment classification task of Sinhala.\n\n3.12 Hate Speech Detection\n\nGiven the low-resource nature of the Sinhala language, there does not seem to be a distinction between the Hate Speech Detection task, Offensive Speech Detection task, and Inappropriate Speech Detection task in the literature for Sinhala. More often, the terms are used interchangeably even though there is a clear distinction between the definitions of them in the literature focused on high-resource languages [255]. Given the lack of differentiation in the Sinhala literature, we too list studies conducted on one or multiple of those tasks in this section. As mentioned in the Section 3.2, a large annotated data set for Sinhala hate speech detection was created by NLLB Team et al. [127].\n\nA machine learning approach to detect hate speech in Sinhala was proposed by De Silva [256]. A feature model and a data set for Sinhala hate speech detection for youtube was proposed by De Saa and Ranathunga [121]. Sandaruwan et al. [257] have attempted to identify abusive Sinhala comments in social media using text mining and machine learning techniques. A cyberbullying comment classification study for Sinhala was conducted by [258] where they used classical machine learning algorithms. The study by Hettiarachchi et al. [259] used classical machine learning methods to detect hate speech in Romanized Sinhala social media posts. While the basic idea is the same, they have avoided mentioning transliteration in their paper. The study by Samarasinghe et al. [260] proposed using CNNs for detecting hate speech in Sinhala text. Kariyawasam [261] proposed a machine learning approach for identifying toxic Sinhala language on social media. Guruge et al. [262] used an ensemble of Na√Øve Bayes, Support Vector Machine, XGBoost, MLP, and AdaBoost to detect hate speech in 49019 Tweets they collected from February of 2021 to April of 2021. Sandaruwan et al. [263] collected 3000 Sinhala comments corpus and conducted Multinomial Na√Øve Bayes to detect hate speech. Sheran [264] claims to have used machine learning to detect hate speech written in Sinhala or Singlish on social media. A deep learning approach for detecting hate speech in Sinhala tweets was explored by Munasinghe and Thayasivam [265]. The study by Shalinda and Munasinghe [266] utilized classical machine learning techniques such as Linear Support vector classifier, Random Forest Classification, SGD classifier, Logistic Regression, XGBoost classifier and multinomial Naive Bayes classifier on both Sinhala and Singlish (Romanized Sinhala) to identify hate words. They report that the SGD classifier over TF-IDF with uni-grams and bi-grams gives them the highest accuracy. Gamage et al. [267] conducted a comparative analysis on a number of embedding systems as well as classical frequency-based methods for Sinhala hate speech detection. The study by Fernando et al. [268] also claims to use machine learning and deep learning to detect hate speech in Sinhala. The study by Perera et al. [269] predicted Sinhala hate speech using user behaviour on twitter by applying ensemble Machine Learning to classify them by the five big personality traits. The follow-up study by Perera et al. [122] analyses Sinhala hate speech propagation on Twitter. They have also published a dataset of 1600+ annotated Sinhala tweets. Rajapaksha et al. [270] used deep learning to identify trending periods of hate topics on Twitter in Sinhala. Arachchi et al. [271] used a web-based tool to translate between Sinhala and English to detect inappropriate word usage in Sinhala. The study by Ehelepola [272] analyzed Sinhala text from social media and e-commerce sites for hate speech. The work by Dikwatta et al. [273] used supervised algorithms to detect Sinhala hate speech in text from image posts on the Internet (memes). Wickramaarachchi et al. [274] used LSTM on BART to compare the title and description of Sinhala youtube videos to their audio in order to determine if they contain hate speech.\n\nAfter the release of the SOLD dataset by Ranasinghe et al. [132] for the hate speech detection task in Sinhala, a number of subsequent works have been conducted using it. Fernando and Deng [275] used traditional machine learning algorithms to predict Sinhala hate speech. Ranasinghe and Zampieri [276] compared the results of pre-trained mT5 on Sinhala tweets from the SOLD dataset against results for German, Spanish, Hindi, Korean, and Marathi. The work by Bestgen [277] compared the result of classical machine learning algorithms on simple n-gram features for the hate speech detection task on the SOLD dataset for Sinhala against results for Assamese and Bengali. Ranasinghe et al. [278] used the SOLD dataset for the Hate Speech And Offensive Content (HASOC) identification task they introduced. In completing the task, the study by Narayan et al. [279] compared the effectiveness of a number of pre-trained deep learning models for Sinhala hate speech detection. They showed that XLM Roberta Base achieves a macro-f1 of 83.49% over the LSTM baseline of 75.30%. For the same task, Ojo et al. [280] used mBERT to identify hate and offensive content in Sinhala.\n\n3.13 Fake News Detection\n\nA dataset consisting of Sinhala documents drawn from Sri Lankan news websites was published by Jayawickrama et al. [119] along with the benchmark misinformation classification models. A hybrid approach to detect Sinhala fake news on Social media was proposed by Wijayarathna and Jayalal [281] where the text content of the post is checked against credible sources and the authenticity of the user account posting the relevant post is evaluated by a rule-based points allocation schema. Wijayarathna and Jayalal [282] collected a set of 120 fake news tweets and 250 non-fake news tweets which they then converted to vectors by taking the fasttext vector for words and averaging. The vector representations of the tweets were then compared to predict whether the news containing it is real or fake. The study by Udurawana et al. [283] proposes to use an accuracy score (obtained by analysing the news content) and a credibility score (obtained by using a scoring mechanism) to detect fake news in Sinhala text. They also incorporate a module that classifies on the basis of passive aggressiveness. The study by Adihetti and Jayalal [284] used autoencoders to detect Sinhala fake news from social media posts. The study by Wickramaarachchi et al. [274] detected fake content in Sinhala youtube videos by comparing their title and description to the audio using LSTM on BART.\n\n3.14 Word Sense Disambiguation\n\nThere have been multiple attempts to do word sense disambiguation (WSD) [285, 286, 287, 288, 289] for Sinhala. For this, Arukgoda et al. [166] have proposed a system named Aruth based on the Lesk Algorithm[290]. An online tool , an API of the algorithm, and code along with data on github are available. For the same task, Marasinghe et al. [291] have proposed a system based on probabilistic modeling. A dialogue act recognition system which utilizes simple classification algorithms has been proposed by Palihakkara et al. [292]. A word sense disambiguation tool named Sinsense was introduced by Subasingha [293]. They used cross-lingual sense disambiguation where English sense disambiguation was used to obtain Sinhala sense disambiguation. However, neither their tool nor their full research is publicly available.\n\n3.15 Text Summarizing\n\nA deterministic process flow for automatic Sinhala text summarizing was proposed by Welgama [294]. The study by Wimalasuriya [295], which has the same name as the above work by Welgama [294], uses the graph-based TextRank algorithm for automatic Sinhala text summarizing. The use case of Sinhala Text summarization for government gazettes was explored by Jayawardane [296]. The study by Rathnayake et al. [297] compared the results of extractive and abstractive summarization on Sinhala textbooks. The study by Jahan and Wijesekara [298] compared the abstractive summarizing methods of TF-IDF and Text-Rank for Sinhala using ROUGE as the evaluation score. They concluded TF-IDF to be the superior choice. Hasan et al. [141] introduced the XL-sum dataset which includes 3,414 Sinhala documents and their summaries collected from Sinhala BBC website . The M3LS multi-modal dataset introduced by Verma et al. [142] contains 10,148 Sinhala documents, relevant images, and their summaries collected from Sinhala BBC website . Further, they claim that MT5 [299] obtains the best ROUGE-1 and ROUGE-L f scores for their Sinhala data set.\n\n3.16 Other Semantic Tools\n\nApplications of the semantic layer are more advanced than the ones below it in Figure 1. But even with the obvious lack of resources and tools, a number of attempts have been made on semantic-level applications for the Sinhala Language. The earliest attempt on semantic analysis was done by Herath et al. [300] using their earlier work which dealt with Sinhala morphological analysis [56].\n\n3.17 Phonological Tools\n\nOn the case of phonological layer, a report on Sinhala phonetics and phonology was published by Wasala and Gamage [301]. Wickramasinghe et al. [302] discussed the practical issues in developing Sinhala Text-to-Speech and Speech Recognition systems. The Massively Multilingual Speech (MMS) data set created by Pratap et al. [135] has Sinhala data for the Spoken Language Identification (LID) task. The meta-study by Al-Fraihat et al. [303] compared the status of Sinhala speech recognition research against 17 other languages.\n\n3.17.1 Text-to-Speech\n\nBased on the earlier work by Weerasinghe et al. [304], Wasala et al. [305] have developed methods for Sinhala grapheme-to-phoneme conversion along with a set of rules for schwa epenthesis. This work was then extended by Nadungodage et al. [306]. Weerasinghe et al. [307] developed a Sinhala text-to-speech system. However, it is not publicly accessible. They internally extended it to create a system capable of helping a mute person achieve synthesized real-time interactive voice communication in Sinhala [308]. A rule based approach for automatic segmentation of a small set of Sinhala text into syllables was proposed by Kumara et al. [309]. An ew prosodic phrasing method to help with Sinhala Text-to-Speech process was proposed by Bandara et al. [310, 311, 312]. Sodimana et al. [313] proposed a text normalization methodology for Sinhala text-to-speech systems. Further, Sodimana et al. [314] formalized a step-by-step process for building text-to-speech voices for Sinhala. Both Jayamanna [315] and Mishangi [316] have created Sinhala document readers for visually impaired persons to be used on Android devices. An OCR and Text-to-Speech system for Sinhala named Bhashitha was proposed by De Zoysa et al. [317]. The works by Lakmal et al. [318] and Senarathna et al. [319] adapted MaryTTS [320] to synthesize Sinhala speech. The study by Jayawardhana et al. [321] used Deep Voice [322] for Sinhala and English TTS. Gamage et al. [323] included a Sinhala text-to-speech module as one of the three modules present in their currency recognition system. The study by Madhusha et al. [324] claims to have created a mobile app with Sinhala Text-to-Speech and OCR to read books for visually impaired students. Anuradha and Thelijjagoda [325] proposed a machine translation system to convert Sinhala and English Braille documents into voice. A separate group has done work on Sinhala text-to-speech systems independent to above [326].\n\n3.17.2 Speech-to-Text\n\nOn the converse, Nadungodage et al. [327] have done a series of work on Sinhala speech recognition with special notice given to Sinhala being a resource poor language. This project divides its focus on: continuity [328], active learning [329], and speaker adaptation [330]. A Sinhala speech recognition for voice dialing which is speaker independent was proposed by Amarasingha and Gamini [331] and on the other end, a Sinhala speech recognition methodology for interactive voice response systems, which are accessed through mobile phones was proposed by Manamperi et al. [332]. A Sinhala speech to Unicode text converter for the disaster relief domain was proposed by Prasangini and Nagahamulla [333]. Priyadarshani [334] proposes a method for speaker dependant speech recognition based on their previous work on: dynamic time warping for recognizing isolated Sinhala words [335], genetic algorithms [336], and syllable segmentation method utilizing acoustic envelopes [337]. The method proposed by Gunasekara and Meegama [338] utilizes an HMM model for Sinhala speech-to-text. A Sinhala speech recognizer supporting bi-directional conversion between Unicode Sinhala and phonetic English was proposed by Punchimudiyanse and Meegama [339]. The work by Karunanayake et al. [340] transfer learns CNNs for transcribing free-form Sinhala and Tamil speech data sets for the purpose of classification. Dilshan [341] conducted a study for the specific use case of transcribing number sequences in continuous Sinhala speech. Gamage et al. [342] explored the use of combinational acoustic models such as Deep Neural Network - Hidden Markov Model (DNN-HMM) [343] and Subspace Gaussian Mixture Model (SGMM) [343] in Sinhala speech recognition. In the follow-up work, Gamage et al. [344] extended that work with end-to-end Lattice-Free Maximum Mutual Information (e2e LF-MMI) model [345] which is claimed to be a viable solution for low resource language speech recognition by Carmantini et al. [346]. However, it was shown that the new model slightly underperforms compared to the state-of-the-art result. Karunathilaka et al. [347] explore Sinhala speech recognition using deep learning models such as: pre-trained DNN, DNN, TDNN, TDNN+LSTM. The first half of the study by Arafath [348] dealt with recognizing Sinhala speech using LSTMs. Gamage et al. [323] included a Sinhala speech recognition module as one of the three modules present in their currency recognition system. Time-delay neural network architectures (including multistream CNN architecture) were used for acoustic modeling of Sinhala Automatic Speech Recognition (ASR) by Warusawithana et al. [349]. They have used the Kaldi speech recognition toolkit [350] for training the ASR models. As part of their child [sic] cognitive ability assessment model, Kahawanugoda et al. [351], proposed a Sinhala speech recognition system. The study by Azir et al. [352] attempts to identify number sequences spoken in Sinhala. TacoSi introduced by Arachchige and Weerasinghe [353] is based on Tacotron [354] and has been evaluated with 10 human evaluators to determine its text-to-speech quality. Nanayakkara [355], Nanayakkara and Weerasinghe [356] used DeepSpeech by Mozilla for Sinhala speech recognition. Gunarathne et al. [357] used an earlier version of the CMUSphinx toolkit to transcribe Sinhala speech to text. The later work by Akesh and Meegama [358] also used CMUSphinx toolkit on features extracted from Mel-frequency cepstral coefficients (MFCC) [359, 360] to automatically generate Sinhala subtitles from Speech.\n\n3.17.3 Speech-to-Speech\n\nLayansan et al. [361] created a speech-to-speech translation system for Sinhala on the Android platform. The system developed by Rajapakshe et al. [362] is also speech-to-speech in the sense that, it is a chatbot for scheduling medical appointments and giving medical advice where the front end contains speech recognition and voice synthesizer components that interfaced with a chatbot component in the back end.\n\n3.17.4 Speech-to-Intent\n\nThe work by Karunanayake et al. [363] used English phoneme-Based Automatic Speech Recognition (ASR) for intent identification in Sinhala and Tamil. Ignatius and Thayasivam [364] proposed a speaker-invariant speech-to-intent classification model with i-vector based speaker normalization, which was then evaluated on Sinhala, and Tamil speech intent data sets. The later work by Yadav et al. [365] used pre-trained embeddings for Sinhala speech intent classification. Hellarawa and Thayasivam [131] proposes a BiLSTM-based ASR system for intent classification which they have tested on the banking domain Sinhala speech dataset created by Buddhika et al. [130]. For this, they report an accuracy of 98.53%.\n\n3.17.5 Speech classification\n\nThe Sinhala speech classification system proposed by Buddhika et al. [216] does so without converting the speech-to-text. However, they report that this approach only works for specific domains with well-defined limited vocabularies. The work by Dinushika et al. [366] uses automatic speech recognition of Sinhala for speech command classification. Extending that, Kavmini et al. [367] presented a Sinhala speech command classification system which can be used for downstream applications. The voice assistant system created by Senarathne et al. [368] is capable of handling Sinhala voice commands. The work by Welarathna et al. [369] used CNNs to classify emotions (sad, disgust, surprise, neutral, happy, calm, fear, and angry) of Sinhala speech by Autistic children. Sundarapperuma [370] created a speech emotion detection system for Sinhala.\n\n3.17.6 Lip Synchronization\n\nThe study by Weerathunga et al. [371] worked on lip synchronization for Sinhala speech where videos of people speaking Sinhala were mapped to a visemes alphabet created by them. Further of this line of study, Wakkumbura et al. [372] came up with Phoneme-Viseme mapping for Sinhala speech that they intended to be used for future applications of robotics.\n\n3.18 Optical Character Recognition Applications\n\nWhile it is not necessarily a component of the NLP stack shown in Fig 1, which follows the definition by Liddy [28], it is possible to swap out the bottom most phonological layer of the stack in favour of an Optical Character Recognition (OCR) and text rendering layer.\n\n3.18.1 Printed Text\n\nThe XTREME-UP data set created by Ruder et al. [134] contains a Sinhala data set for the OCR task. The data was obtained from book transcriptions. An attempt for Sinhala OCR system has been taken by Rajapakse et al. [373] before any other work has been done on the topic. Much later, a linear symmetry-based approach was proposed by Premaratne and Bigun [374, 375]. They then used hidden Markov model-based optimization on the recognized Sinhala script [376]. Similarly, Hewavitharana et al. [377] used hidden Markov models for off-line Sinhala character recognition. Herath et al. [116], Herath and Medagoda [117] developed a prepossessing engine based on a template matching for printed Sinhala OCR. Statistical approaches with histogram projections for Sinhala character recognition is proposed by Hewavitharana and Kodikara [378], by Ajward et al. [379], and by Madushanka et al. [380]. Karunanayaka et al. [381] also did off-line Sinhala character recognition with a use case for postal city name recognition. A separate group had attempted Sinhala OCR [382] mainly involving the nearest-neighbor method [383, 384]. A study by Ediriweera [385] uses dictionaries to correct errors in Sinhala OCR. An early attempt for Sinhala OCR by Dias et al. [386] has been extended to be online and made available to use via desktops [387] and hand-held devices [388] with the ability to recognize handwriting. A simple neural network based approach for Sinhala OCR was utilized by Rimas et al. [389]. A fuzzy-based model for identifying printed Sinhala characters was proposed by Gunarathna et al. [390]. Premachandra et al. [391] proposes a simple back-propagation artificial neural network with hand crafted features for Sinhala character recognition. Another neural network with specialized feature extraction for Sinhala character recognition was proposed by Jayamaha and Naleer [392]. On the matter of neural networks and feature extraction, a feature selection process for Sinhala OCR was proposed by Kumara and Ragel [393]. Jayawickrama et al. [394] worked on Sinhala printed characters with special focus on handling diacritic vowels. However, they opted to refer to diacritic vowels as modifiers in their work. Gunawardhana and Ranathunga [395] proposed a limited approach to recognize Sinhala letters on Facebook images. A CNN-based methodology to improve printed Sinhala character OCR was proposed by Liyanage [396]. Printed Character Recognition (PCR) was used by Vasantharajan and Thayasivam [106] to create a large-scale Tamil-Sinhala-English parallel corpus. A meta-study on the effects of text genre, image resolution, and algorithmic complexity needed for Sinhala OCR from books and newspapers was conducted by Anuradha et al. [397]. Anuradha et al. [398] used Tesseract 3 [399] for Sinhala OCR. The later study by [400] improved the accuracy of Tesseract OCR engine on Sinhala from 53.22% to 86.16% for the data set they tested on. Maduranga and Jayalal [401] used Artificial Neural Network (ANN) based on Universe of Discourse and Self Organization Map methods to recognize multi-style printed Sinhala characters. The study by De Silva et al. [402], even though ostensibly presented as translation research, is just invoking Google Cloud service for translation. Their novelty in Sinhala NLP lies in the facility provided to OCR the text from photos and documents. An OCR and Text-to-Speech system for Sinhala named Bhashitha was proposed by De Zoysa et al. [317]. A study on Sinhala text extraction from social media images (memes) was conducted by Samarajeewa and Ranathunga [403]. They specifically handled the character-touching issue. The study by Walawage and Ranathunga [404] attempted to devise a feature set to separately identify Sinhala and English text on social media images (memes). The study by de Silva and Liyanage [405] used Convolutional Spiking Neural Networks to extract Sinhala text from YouTube thumbnails. Chanda et al. [406] proposed a Gaussian kernel SVM based method for word-wise Sinhala, Tamil, and English script identification. The work by Vasantharajan et al. [107] adapted the Tesseract engine to handle non-Unicode (legacy fonts) in pdf documents to create a Tamil-Sinhala-English parallel corpus.\n\n3.18.2 Handwritten Text\n\nFernando et al. [407] claim to have created a database for handwriting recognition research in the Sinhala language and further claims that the data set is available at the National Science Foundation (NSF) of Sri Lanka. However, the paper provides no URLs and we were not able to find the data set on the NSF website either. The work by Karunanayaka et al. [408] is focused on noise reduction and skew correction of Sinhala handwritten words. A genetic algorithm-based approach for non-cursive Sinhala handwritten script recognition was proposed by Jayasekara and Udawatta [409]. Nilaweera et al. [410] compare projection and wavelet-based techniques for recognizing handwritten Sinhala script. Silva and Kariyawasam [411] worked on segmenting Sinhala handwritten characters with special focus on handling diacritic vowels. A comparative study of few available Sinhala handwriting recognition methods was done by Silva et al. [412]. Silva et al. [413] uses contour tracing for isolated characters in handwritten Sinhala text. A Sinhala handwriting OCR system which utilizes zone-based feature extraction has been proposed by Dharmapala et al. [414]. The study by Walawage and Ranathunga [415] and its follow up study by Walawage [416] specifically focus on segmentation of overlapping and touching Sinhala handwritten characters. Silva and Jayasundere [417] focused on recognizing character modifiers in Sinhala handwriting. The similarly named studies by Mariyathas et al. [418] and Wasalthilake and Kartheeswaran [419], both utilize CNN to recognize Sinhala handwriting; as does the study by Weerasinghe [420]. Ifhaam and Jayalal [421] used genetic algorithms to recognize Sinhala handwritten postal addresses for postal sorting. A segmentation-based approach that utilizes an n-gram model to recognize and validate Sinhala words written on touch screens was proposed by Mahesh and Priyankara [422]. They used a CNN classifier and were able to classify 19 different Sinhala characters. The study by Rowel et al. [423] frames their work as an E-Learning platform for hearing impaired children. However, their research does not contain any work done towards Sinhala sign language to be included in Section 3.23. What they do have is an OCR system that they claim to recognize letters and digits. Even there, we are only given an example of a recognised digit. Whether or not their system can recognize Sinhala letters is not explicitly discussed. As part of their child [sic] cognitive ability assessment model, Kahawanugoda et al. [351], proposed a Sinhala handwriting letter recognition system. The study by Withana and Rupasinghe [424] used Sinhala handwritten text classification to detect Dyslexia and Dysgraphia. The study by eka [425] has used CNN to recognize Sinhala handwritten text.\n\n3.18.3 Ancient Text\n\nSummarizing optically recognized old Sinhala text for the purpose of archival search and preservation was explored by Rathnasena et al. [426]. The work of Peiris [427] also focused on OCR for ancient Sinhala inscriptions. Building upon the architecture proposed by Ruwanmini et al. [428], a neural network-based method for recognizing ancient Sinhala inscriptions was proposed by Karunarathne et al. [429]. The study by Wickramarathna and Ranathunga [430] created a system to recognize Brahmi characters, correct errors, and generate Sinhala meanings. Heenkenda and Fernando [431] used Inception-v3 [432], VGG-19 [433], and ResNet-50 [434] to classify Sinhala inscriptions to historical time periods.\n\n3.19 Translators\n\nA meta-study on the viability of machine translators replacing English to Sinhala human translators was conducted by Dilshani and Senevirathna [435]. However, this study only involves 100 combined and complex English sentences translated to Sinhala by human translators as well as MT software. Given that reason and the fact that they seem to only used Google translate and Akura Sinhala dictionary app for comparison, the conclusions of this study may not be generalized. Another meta-study on the impact of pre-trained multilingual sequence-to-sequence models on low-resource language translation has been conducted by Lee et al. [436]; while they consider Sinhala as one of the examples, they do not go much into the specific impact due to the general nature of the paper. The meta-analysis by Ramadasa et al. [437] attempts to evaluate the goodness of the Google Sinhala-to-English translation by using the Google cloud API to translate Sinhala to English and then analysing the accuracy of the Sentiment Analysis task and the Named Entity Recognition task on the translated text. The meta-analysis by Das et al. [438] compared the results of translating English to 15 Indic languages including Sinhala using statistical translation methods. They used datasets from OPUS [123] for model building and utilized Flores-200 for fine-tuning. The NMT for Indic languages study conducted by Sheshadri et al. [439] discusses the Sinhala language translation in the abstract and conclusion but the paper itself focuses more on the languages spoken in India. Nevertheless, it puts Sinhala into a regional linguistic perspective. The meta-study by Bapna et al. [440] discusses the task of building clean, web-mined datasets for a number of languages including Sinhala for the task of machine translation. This discussion was continued by Jones et al. [441] who discussed the bilingual lexica (BILEXs) in the context of a number of languages including Sinhala.\n\nAs mentioned in Section 3.1, the study by [96] raised questions on the quality of the existing Sinhala-English corpora. In a follow-up study by Ranathunga et al. [147], it was pointed out that using an automated ranking based on a similarity measure on web-mined corpora and using the resultant top samples can yield better translation models for English-Sinhala and Sinhala-Tamil. These models sometimes were better performing than a model trained on the full dataset. They further showed that using human labour to clean web-mined parallel corpora only gives marginal benefits over automated ranking and filtering. Thus they concluded that using expensive human labour for this task might not be efficient. Sen et al. [442] on the other hand attempted to improve the quality of the Sinhala-English parallel corpora using fuzzy string matching where they tried to match the English translation of the given Sinhala sentence to the English sentence in the dataset pair.\n\nA study on the viability of using Google Translate for the legal domain English-Sinhala and Sinhala-English translation was conducted by [443]. They used human experts to extensively analyse the end result of the translation with many concrete examples of legal phrases that needed to be translated. The multilingual lexicon for low-resource machine translation dataset GATITOS introduced by Jones et al. [444] does not include Sinhala among their 170 languages set. However, they use Sinhala data from other sources and include several observations for Sinhala in this task. They observe that PanLex [445] and GATITOS improve results in the En-Si direction for smaller models (Transformer Big, 475M) but weaken the results in the Si-En direction. For bigger models (Transformer 1.6B), they do not observe an improvement or degradation of En-Si or Si-En direction.\n\n3.19.1 Sinhala-English Non-NMT\n\nA series of work has been done by a group towards English to Sinhala translation as mentioned in some of the above subsections. This work includes; building a morphological analyzer [68], lexicon databases [71], a transliteration system [72], an evaluation model [77], a computational model of grammar [20], and a multi-agent solution [83]. After working on human-assisted machine translation [73], Hettige and Karunananda [76, 78] have attempted to establish a theoretical basics for English to Sinhala machine translation. A very simplistic web based translator was proposed [74, 75]. The same group have worked on a Sinhala ontology generator for the purpose of machine translation [82] and a phrase level translator [84] based on the previous work on a multi-agent system for translation [81]. Further, an application of the English to Sinhala translator on the use case of selected text for reading was implemented [79]. They later continued their work on multi-agent English to Sinhala translation with the AGR organizational model [85].\n\nAnother group independently attempted English-to-Sinhala machine translation [446] with a statistical approach [447]. Wijerathna et al. [448] and De Silva et al. [449] have proposed simple rule based translators. An example-based method applied on the English-Sinhala sentence aligned government domain corpus was proposed by Silva and Weerasinghe [450]. A translator based on a look-up system was proposed by Vidanaralage et al. [451]. In a preprint, Joseph et al. [452] proposes an evolutionary algorithm for Sinhala to English translation with a basis of Point-wise Mutual Information (PMI) and claims that the code will be shared once the paper is accepted. However, they do not report any quantitative results to be compared and the reported qualitative results are also superficial. Pushpananda et al. [453] utilized statistical machine translation to translate between Sinhala and Tamil. Fernando et al. [454] tries to solve the Out of vocabulary (OOV) problem for Sinhala in the context of Sinhala-English-Tamil statistical machine translation. The approach proposed by Rajitha et al. [455] uses statistical machine translation and transliteration to align Sinhala and English documents.\n\n3.19.2 Sinhala-English NMT\n\nFonseka et al. [456] used Byte Pair Encoding (BPE) for English to Sinhala neural machine translation. As another solution to the OOV problem, an analysis of subword techniques to improve English to Sinhala Neural Machine Translation (NMT) was conducted by Naranpanawa et al. [457]. A data augmentation method to expand bilingual lexicon terms based on case markers for the purpose of solving the OOV problem in the domain of NMT was proposed by Fernando et al. [458] which they later extended further [459]. Epaliyana et al. [460] proposed iterative filtering and data selection be used to improve Sinhala-English NMT. Perera et al. [461] used English Part-of-Speech (PoS) tags to improve English to Sinhala NMT. Lin et al. [462] used a model based on fairseq [463] to improve machine translation between English and Sinhala. The second half of the study by Arafath [348] dealt with translating Sinhala speech to other languages. Kugathasan and Sumathipala [464] proposed an NMT system for Sinhala-English Code-Mixed text using the standardized Sinhala Code-Mixed text they proposed earlier [465]. Nguyen et al. [466] introduced a new fine-tuning objective LAgSwAV (Language-Agnostic Constraint for SwAV loss), using which they obtained 5.4 BLEU for English-Sinhala. Later, this method was further discussed by Nguyen [467]. Attigala and Weerasinghe [468] conducted an analysis on the effectiveness of ChatGPT [469] in translating Sinhala songs to English. Utsa et al. [470] used transfer learning and back translation as well as focal loss of the Sinhala-English dataset from FLoRes [92] and reported better results than mBART [471].\n\n3.19.3 Singlish to Sinhala (Transliteration)\n\nThe XTREME-UP data set created by Ruder et al. [134] contains a Sinhala data set for the transliteration task. The early work by Goonetilleke et al. [472] attempted Sinhala transliteration through the Latin alphabet. However, their work does not use the word transliteration and instead focuses on the predictive aspect. Priyadarshani et al. [473] used statistical machine learning for transliteration of names between Sinhala, Tamil, and English. A rule-based system on trigrams was proposed by Liwera and Ranathunga [474] for Singlish to Sinhala transliteration of social media text. A Singlish to Sinhala converter which uses an LSTM was proposed by De Silva [475]. A rule-based approach for the same was proposed by de Silva and Ahangama [476]. The study by Nanayakkara et al. [477] introduced an English-to-Sinhala transliteration system. Sumanathilaka et al. [478, 479] proposed a Trie [480] data structure-based algorithm for word suggestion in Sinhala transliterations. An extended analysis of the same work was presented in a later work [481]. Amarasekara et al. [482] proposed a rule-based method supported by N-gram analysis and a corpus dataset to transliterate Singlish tweets to Sinhala. The study by Rajapaksha et al. [483] claims to have created a translation system, however, what they have created is a transliteration system between Singlish and Sinhala. While they also claim to have trained ASR and TTS systems, the paper does not identify them by name or citation. The same is true for the data sets they have used to train the various modules in their pipeline. The studies by Kumaravithana et al. [484] and Jayawardhana et al. [485] claim to have built Sinhala-English translators but in actuality are just invoking Google API for their translation tasks.\n\nThe work by Kirov et al. [486] used the Dakshina [146] dataset and compared the accuracy of Singlish to Sinhala transliteration with vanilla LSTM, vanilla Transformers, mT5 [299], ByT5 [487], and a non-neural finite-state transducer (FST) based on work by Bisani and Ney [488]. They discuss in detail how the zero-width joiner (ZWJ, U+200D) character cause issues in this task and Sinhala rendering as a whole. Their code is available on GitHub . Khiu et al. [489] fine-tuned mBART on Sinhala Government Corpus and Bible corpus to predict the NMT performance dependence on domain similarity.\n\n3.19.4 Singlish and English (Translation via Transliteration)\n\nAn LSTM-based sequence-to-sequence model was used by Sandaruwan et al. [490] for Singlish to English NMT task. Nalinka et al. [491] used simple transformers with positional embedding to translate Singlish into English. The study by De Silva et al. [492] claims to have achieved Singlish to English translation by simple stemming.\n\n3.19.5 Between Sinhala and Non-English Languages\n\nMost of the cross-Sinhala and Tamil work has been done in the domain of machine translation. A neural machine translation for Sinhala and Tamil languages was initiated by Tennage et al. [493, 494]. Then they further enhanced it with transliteration and byte pair encoding [495] and used synthetic training data to handle the rare word problem [496]. This project produced Si-Ta [497] a machine translation system of Sinhala and Tamil official documents. In the statistical machine translation front, Farhath et al. [498] worked on integrating bilingual lists. The attempts by Weerasinghe [499] and Sripirakas et al. [500] were also focused on statistical machine translation while Jeyakaran and Weerasinghe [501] attempted a kernel regression method. A yet another attempt was made by Pushpananda et al. [502] which they later extended with some quality improvements [503]. An attempt at real-time direct translation between Sinhala and Tamil was done by Rajpirathap et al. [504]. Dilshani et al. [505] have done a study on the linguistic divergence of Sinhala and Tamil languages with respect to machine translation. Mokanarangan [506] claims to have built a named entity translator between Sinhala and Tamil for official government documents. But this work is locked behind an institutional repository wall and thus is not accessible by other researchers. Arukgoda et al. [507] studied the possibility of using deep learning techniques to improve Sinhala-Tamil translation which they further improved later [508]. Pramodya et al. [509] compared Transformers, Recurrent Neural Networks, and Statistical Machine Translation (SMT) in the context of Tamil to Sinhala machine translation. The work by Nissanka et al. [510] used monolingual word embedding to improve NMT between Sinhala and Tamil. Thillainathan et al. [511] uses pre-trained mBART [471] models for six directional translations between Sinhala, Tamil, and English. Yashothara and Uthayasanker [512] discussed the use of the Hierarchical Phrase-Based Model for Tamil to Sinhala and Sinhala to Tamil translations. Pramodya [513] presented a comparison of SMT [453] and NMT models for Sinhala-Tamil translation. For NMT they have used base transformer, turned transformer, and mT5 [299]. The work by Su et al. [514] which compared 8 Parameter-efficient fine-tuning (PEFT) methods on the Si-Ta language pair using the Government corpus [454] and NLLB [127] reports that Houlsby adapter [224], with a 33.34 SacreBLEU [515] score, to be the best PEFT method. However, they also note that the Pfeiffer adapter [226] runs the fastest at 52.59 hours with a reasonable score of 31.24. (Comparatively, the Houlsby adapter runs for 78.65 hours).\n\nThere have been attempts to link Sinhala NLP with Japanese by Herath et al. [23, 87, 86], Thelijjagoda et al. [88], Thelijjagoda et al. [516], and Kanduboda [10]. Jayasinghe [517] discusses the importance of translation between Sinhala and the liturgical language of Buddhism, Pali [518, 519, 520]. [521] attempted to use a dictionary-based machine-translation method for this task. The study by Anuradha and Thelijjagoda [325] uses machine translation on the unique application of converting Sinhala and English Braille documents, which they have run OCR on, into voice.\n\n3.20 Spelling and Grammar\n\nThe open-source data driven approach proposed by Wasala et al. [522, 523] claims to be able to check and correct spelling errors in Sinhala. The approach by Jayalatharachchi et al. [524] attempts to obtain synergy between two algorithms for the same purpose. These efforts [522, 524] were then extended by Subhagya et al. [525]. A rule-based Sinhala spell checker named SinSpell based on Hunspell was introduced by Liyanapathirana et al. [526]. They have also made the tool available online for use. The study by Sithamparanathan and Uthayasanker [527] extended the Generic Environment for context-aware spell correction to handle Sinhala and Tamil. Sonnadara et al. [120] created a benchmark data set for Sinhala spell correction along with a neural model. A multi agent-based spell checker, named LaSi Spell for Sinhala spell checking was introduced by Samarawickrama et al. [528]. The study by Udagedara et al. [529] specifically solved the problem of spell-checking Sri Lankan names and addresses. A system named Erroff was proposed by Sudesh et al. [530] to correct real-word errors in Sinhala text.\n\nA model for detecting grammatical mistakes in Sinhala was developed by Pabasara and Jayalal [531]. They followed this up with a grammatical error detection and correction model [532]. Gunasekara et al. [533] used annotation projection for semantic role labelling for Sinhala. A Sinhala grammar checker based on Hidden Markov models was developed by Fernando and Arudchelvam [534]. Widyaratna [535] used a sequence-to-sequence model with attention, which is generally used for translation tasks, to translate sentences with common grammatical errors to their corrected counterparts. The work by Jayasuriya et al. [536] used a rule-based approach along with Google Translation to correct grammar is Sinhala text. A rule-based system to convert Sinhala sentences from active voice to passive voice while correcting grammatical errors was proposed by Ilukkumbura and Rupasinghe [537]. The study by Goonawardena et al. [538] a rule-based system to spell-check Sinhala text as well as detect and correct grammatical errors. Navoda et al. [539] also claim to have created an automated tool to check Sinhala spelling and grammar.\n\n3.21 Chat Bots\n\nA simple Sinhala chat bot which utilizes a small knowledge base has been proposed by Hettige and Karunananda [70]. A study on the effect of word embeddings on a Sinhala chatbot was conducted by Gamage et al. [540] where they used, the fasttext model trained by Facebook [113, 114, 115], on a RASA [541] chat bot. A Sinhala chat bot for train information was proposed by Harshani [542]. Similarly, the tool proposed by Chandrasena et al. [543] serves as a chat bot-based recommendation system for Sri Lankan traditional dancers. The chat bot discussed by Kumanayake [544] has the very specific purpose of answering user inquiries about the degree programs at University of Ruhuna. Avishka et al. [545] used off the shelf RASA NLU Engine [541] and Microsoft Bot Network [546] to set up a generic Sinhala chat bot architecture. They demonstrated the effectiveness of their architecture by creating a food ordering chat bot. A web-based code-less chat bot development platform for Sinhala was proposed by Dissanayake et al. [547]. Further, they claimed that their system can handle OOV tokens as well as Sinhala-English code-switching. The work by Dasanayaka and Warnajith [548] used a deep learning Intent Mapping (IM) model to map consumer responses in their Sinhala chat bot framework. Rajapakshe et al. [362] proposed a Sinhala conversational interface for the purpose of scheduling medical appointments and giving medical advice. The chat bot component was in the back end while the front end contained speech recognition and voice synthesizer components.\n\n3.22 News and/or Social Media Recommendation\n\nA trending topic detection model for Sinhala tweets using simple clustering and ranking algorithms was proposed by Jayasekara and Ahangama [549]. Sandamini et al. [550] proposed a post recommendation system, which supports Singlish, for social media. Tennakoon and Gamlath [551] proposes a hybrid system which uses skip-gram and collaborative Filtering on Multi-Layer Perceptron for recommending categorized Sinhala news articles. Tennakoon et al. [552] then extends the the system to also be able identify grey sheep users while preforming the aforementioned hybrid recommendation using LDA [232] and SVM. Following the above work, a news aggregator with news categorization, comment filtering, and two types of recommendation systems was proposed by Malsha et al. [553]. Madhushika et al. [554] analysed Twitter trending topics to understand how Sinhala Twitter data affects news dissemination on mass media. They proposed calculating a news value to a tweet which can be utilized to sort tweets by their news-worthiness in order to give better recommendations.\n\n3.23 Sinhala Sign Language\n\nMeyler [555] claim that there is no such thing as a Sinhala Sign Language or a Tamil Sign Language and there is only Sri Lankan Sign Language. Further, [556] claim that SSL was derived from British Sign Language (BSL). However, the literature uses Sri Lankan"
    }
}
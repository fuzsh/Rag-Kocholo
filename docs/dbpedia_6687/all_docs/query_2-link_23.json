{
    "id": "dbpedia_6687_2",
    "rank": 23,
    "data": {
        "url": "https://www.mdpi.com/1424-8220/22/5/1836",
        "read_more_link": "",
        "language": "en",
        "title": "Characterization of English Braille Patterns Using Automated Tools and RICA Based Feature Extraction Methods",
        "top_image": "https://pub.mdpi-res.com/sensors/sensors-22-01836/article_deploy/html/images/sensors-22-01836-g001-550.jpg?1646039004",
        "meta_img": "https://pub.mdpi-res.com/sensors/sensors-22-01836/article_deploy/html/images/sensors-22-01836-g001-550.jpg?1646039004",
        "images": [
            "https://pub.mdpi-res.com/img/design/mdpi-pub-logo-black-small1.svg?da3a8dcae975a41c?1723640743",
            "https://pub.mdpi-res.com/img/design/mdpi-pub-logo-black-small1.svg?da3a8dcae975a41c?1723640743",
            "https://pub.mdpi-res.com/img/journals/sensors-logo.png?8600e93ff98dbf14",
            "https://pub.mdpi-res.com/bundles/mdpisciprofileslink/img/unknown-user.png?1723640743",
            "https://pub.mdpi-res.com/bundles/mdpisciprofileslink/img/unknown-user.png?1723640743",
            "https://www.mdpi.com/bundles/mdpisciprofileslink/img/unknown-user.png",
            "https://www.mdpi.com/bundles/mdpisciprofileslink/img/unknown-user.png",
            "https://pub.mdpi-res.com/img/design/orcid.png?0465bc3812adeb52?1723640743",
            "https://www.mdpi.com/bundles/mdpisciprofileslink/img/unknown-user.png",
            "https://pub.mdpi-res.com/img/design/orcid.png?0465bc3812adeb52?1723640743",
            "https://www.mdpi.com/bundles/mdpisciprofileslink/img/unknown-user.png",
            "https://pub.mdpi-res.com/img/design/orcid.png?0465bc3812adeb52?1723640743",
            "https://www.mdpi.com/bundles/mdpisciprofileslink/img/unknown-user.png",
            "https://pub.mdpi-res.com/img/design/orcid.png?0465bc3812adeb52?1723640743",
            "https://www.mdpi.com/img/table.png",
            "https://www.mdpi.com/sensors/sensors-22-01836/article_deploy/html/images/sensors-22-01836-g001-550.jpg",
            "https://www.mdpi.com/sensors/sensors-22-01836/article_deploy/html/images/sensors-22-01836-g001.png",
            "https://www.mdpi.com/sensors/sensors-22-01836/article_deploy/html/images/sensors-22-01836-g002-550.jpg",
            "https://www.mdpi.com/sensors/sensors-22-01836/article_deploy/html/images/sensors-22-01836-g002.png",
            "https://www.mdpi.com/sensors/sensors-22-01836/article_deploy/html/images/sensors-22-01836-g003-550.jpg",
            "https://www.mdpi.com/sensors/sensors-22-01836/article_deploy/html/images/sensors-22-01836-g003.png",
            "https://www.mdpi.com/sensors/sensors-22-01836/article_deploy/html/images/sensors-22-01836-g004-550.jpg",
            "https://www.mdpi.com/sensors/sensors-22-01836/article_deploy/html/images/sensors-22-01836-g004.png",
            "https://www.mdpi.com/sensors/sensors-22-01836/article_deploy/html/images/sensors-22-01836-g005-550.jpg",
            "https://www.mdpi.com/sensors/sensors-22-01836/article_deploy/html/images/sensors-22-01836-g005.png",
            "https://www.mdpi.com/img/table.png",
            "https://www.mdpi.com/img/table.png",
            "https://www.mdpi.com/img/table.png",
            "https://www.mdpi.com/img/table.png",
            "https://www.mdpi.com/img/table.png",
            "https://www.mdpi.com/img/table.png",
            "https://pub.mdpi-res.com/img/design/mdpi-pub-logo-white-small.png?71d18e5f805839ab?1723640743"
        ],
        "movies": [],
        "keywords": [],
        "meta_keywords": [
            ""
        ],
        "tags": null,
        "authors": [
            "Sana Shokat",
            "Rabia Riaz",
            "Sanam Shahla Rizvi",
            "Inayat Khan",
            "Anand Paul",
            "Sanam Shahla"
        ],
        "publish_date": "2022-02-25T00:00:00",
        "summary": "",
        "meta_description": "Braille is used as a mode of communication all over the world. Technological advancements are transforming the way Braille is read and written. This study developed an English Braille pattern identification system using robust machine learning techniques using the English Braille Grade-1 dataset. English Braille Grade-1 dataset was collected using a touchscreen device from visually impaired students of the National Special Education School Muzaffarabad. For better visualization, the dataset was divided into two classes as class 1 (1–13) (a–m) and class 2 (14–26) (n–z) using 26 Braille English characters. A position-free braille text entry method was used to generate synthetic data. N = 2512 cases were included in the final dataset. Support Vector Machine (SVM), Decision Trees (DT) and K-Nearest Neighbor (KNN) with Reconstruction Independent Component Analysis (RICA) and PCA-based feature extraction methods were used for Braille to English character recognition. Compared to PCA, Random Forest (RF) algorithm and Sequential methods, better results were achieved using the RICA-based feature extraction method. The evaluation metrics used were the True Positive Rate (TPR), True Negative Rate (TNR), Positive Predictive Value (PPV), Negative Predictive Value (NPV), False Positive Rate (FPR), Total Accuracy, Area Under the Receiver Operating Curve (AUC) and F1-Score. A statistical test was also performed to justify the significance of the results.",
        "meta_lang": "en",
        "meta_favicon": "https://pub.mdpi-res.com/img/mask-icon-128.svg?c1c7eca266cd7013?1723640743",
        "meta_site_name": "MDPI",
        "canonical_link": "https://www.mdpi.com/1424-8220/22/5/1836",
        "text": "1\n\nDepartment of Computer Science and IT, University of Azad Jammu and Kashmir, Muzaffarabad 13100, Pakistan\n\n2\n\nRaptor Interactive (Pty) Ltd., Eco Boulevard, Witch Hazel Ave, Centurion 0157, South Africa\n\n3\n\nDepartment of Computer Science, University of Buner, Buner 19290, Pakistan\n\n4\n\nThe School of Computer Science and Engineering, Kyungpook National University, Daegu 41566, Korea\n\n*\n\nAuthor to whom correspondence should be addressed.\n\nSensors 2022, 22(5), 1836; https://doi.org/10.3390/s22051836\n\nSubmission received: 12 November 2021 / Revised: 18 February 2022 / Accepted: 22 February 2022 / Published: 25 February 2022\n\n(This article belongs to the Special Issue Big Data Analytics in Internet of Things Environment)\n\nAbstract\n\n:\n\nBraille is used as a mode of communication all over the world. Technological advancements are transforming the way Braille is read and written. This study developed an English Braille pattern identification system using robust machine learning techniques using the English Braille Grade-1 dataset. English Braille Grade-1 dataset was collected using a touchscreen device from visually impaired students of the National Special Education School Muzaffarabad. For better visualization, the dataset was divided into two classes as class 1 (1–13) (a–m) and class 2 (14–26) (n–z) using 26 Braille English characters. A position-free braille text entry method was used to generate synthetic data. N = 2512 cases were included in the final dataset. Support Vector Machine (SVM), Decision Trees (DT) and K-Nearest Neighbor (KNN) with Reconstruction Independent Component Analysis (RICA) and PCA-based feature extraction methods were used for Braille to English character recognition. Compared to PCA, Random Forest (RF) algorithm and Sequential methods, better results were achieved using the RICA-based feature extraction method. The evaluation metrics used were the True Positive Rate (TPR), True Negative Rate (TNR), Positive Predictive Value (PPV), Negative Predictive Value (NPV), False Positive Rate (FPR), Total Accuracy, Area Under the Receiver Operating Curve (AUC) and F1-Score. A statistical test was also performed to justify the significance of the results.\n\n1. Introduction\n\nVisual impairment is defined as a loss of the ability to see that cannot be fixed with conventional procedures such as medication or glasses. In a report presented by the World Health Organization (WHO), 2.2 billion people all over the world suffer from near or distance vision problems [1]. Enormous efforts are required to assess the impact of illness on individuals and society. Recent advances in quantitative measures of the quality of life, life expectancy, the financial impact of disease and its treatment have allowed us to calculate the effects of illness and assist in future research to improve public health. Over 120 diseases and conditions have been thoroughly reviewed in terms of disability-adjusted life years (DALYs), quality-adjusted life years (QALYs), quality of life and financial measures [2]. Visual impairment harms the well-being of children and adults. School-aged children with vision impairments may have lower levels of academic achievement [3].\n\nTo educate visually impaired people, Louis Braille invented a language known as Braille, also known as night writing, which is a language introduced by Louis Braille specifically for visually impaired people [4]. Braille is composed of six dots. Using these different dots patterns, the visually impaired can read and write different characters. Previously, Braille was read and written using a slate and stylus. The world these days is equipped with the latest technologies that make people’s lifestyles more comfortable. Touchscreen devices are used more commonly; between 2009 and 2014, mobile screen readers’ popularity rose exponentially from 12% to 82% [5]. The use of touchscreen-based devices helped visually impaired people to live their lives in a better way. Touchscreens have shown remarkable developments in recent years. It encourages users to perform a wide range of tasks [6]. This includes e-learning, medical diagnosis, performing household chores, gaming, online shopping, etc. Touchscreens are easy to carry and simple to deal with, thus making them a leading tool used in our daily lives. People with visual impairments are an integral part of every community.\n\nTo live a better life, visually impaired people also adopt touchscreen devices for smoothly carrying out their daily activities [7]. Many famous applications are available for visually impaired users to help improve their living conditions, such as: “Look Tel”—the money identification mobile application, “Kurzweil-National Federation of the Blind Reader (KNFB) Reader” that reads any text aloud [8], “TapTapSee” that identifies objects using photos [9], “Color ID Free” that discovers the names of the colors around you [10] and “Be My Eyes”—the one that helps visually impaired people in real time [11]. Georgios et al. analyzed different assistive tools to highlight visually impaired people’s issues using these applications [12]. Braille, also known as night writing, is a language introduced by Louis Braille specifically for visually impaired people [13,14]. There are several touchscreen-based Braille applications like Braille Easy [14], Single Tap Braille [5], Braille Ecran [15], Braille Enter [16], Braille Sketch [17], etc. available for people with visual impairments. Visually impaired people face serious usability and accessibility issues with these applications. Research is progressing to develop practical and more friendly devices for visually impaired people. Recently, machine learning techniques have been widely used for converting Braille into natural language and vice versa. Currently, a lot of work is done for converting English into Braille [18,19,20,21,22,23,24,25]. However, these conversions are based on handwritten scanned Braille sheets. These procedures do not reduce the burden of visually impaired users.\n\nBraille to English character conversion has not been explored considerably; to the best of our knowledge, no current application for the visually impaired recognizes a position-free touchscreen-based Braille data entry in real time and provides both texts as well as voice feedback. Therefore, there is a strong need for an application that is both accessible to and usable for visually impaired users. The application proposed by the authors of Reference [19] is unique, because visually impaired users can tap anywhere on the screen and are not required to find the precise location of the dots. This work is the extension of a previous study conducted by Sana et al. The English Braille dataset used in this study was collected using the application proposed by Sana et al. In this research, the input storage mechanism was changed. Previously, Braille’s image was saved and used by the authors for character prediction. Here, the coordinate value of each dot entered is saved in a text file. The authors manually validated the numerical dataset acquired using the touchscreen application. Anomalies were removed before processing. Using this new dataset, Braille to English character recognition was made using machine learning techniques like DT, SVM and KNN combined with RICA- and PCA-based feature extraction methods. The Random Forest and Sequential methods were also implemented for a comparative analysis. Figure 1 represents the schematic diagram for the research conducted.\n\nThe following are the main contributions of this research paper:\n\n(a)\n\nCollection of the English Braille Grade 1 dataset from visually impaired students of Muzaffarabad Special Education School, using the position-free Braille input application developed by Sana et al. This dataset was collected, as no such prior dataset exists that gathers Braille input from visually impaired users directly on touchscreen devices in real time.\n\n(b)\n\nA novel backend storage mechanism for Braille characters entered using the touchscreen-based application.\n\n(c)\n\nPrediction of the English alphabet for the corresponding Braille characters using Decision Tree (DT), Support Vector Machine (SVM), K-Nearest Neighbor (KNN) and Random Forest (RF) with RICA- and PCA-based feature extraction methods.\n\n(d)\n\nEvaluation and comparison of the proposed mechanism with previous studies and using other techniques like the Random Forest and Sequential methods.\n\nThis manuscript is organized as follows: Section 2 provides a literature review, Section 3 presents the materials and methods, Section 4 contains the detailed results and comparative analysis, Section 5 contains the discussion and Section 6 contains the conclusions and future recommendations.\n\n2. Literature Review\n\nBraille is a language invented by Louis Braille, and it is still used worldwide as the standard communication tool for people with visual impairment. Braille is written by punching dots on paper and read by gliding the fingers over the raised dots. A Braille cell is defined by combining six dot patterns of 3 × 2 matrixes [26]. A couple of decades ago, efforts began to develop machines that could assist and speed up the writing process. Touchscreen devices, invented in 1965, have become an important part of everyday life. People can easily interact with touchscreens without using any other tools. Touchscreens enable developers to create interfaces tailored to the audience’s specific needs. A wide range of touchscreen-based applications are available to assist visually impaired people in the accomplishment of their daily life tasks with ease, like Braille Tap and Nav Tap [27], V-Braille [28], TypeIn Braille [29], Braille Touch [9,30], TapTapSee [9], Braille Play [31], Edge Braille [32], Braille Ecran [15], BeSpecular [33], Color Teller [34], KNFBReader [8], Text to Speech Synthesizer [35] and so on. Since Braille is a popular language for the visually impaired, converting Braille to other languages has become an important study area. Many researchers have used Braille in natural language conversion mechanisms by considering it an important research topic. Several techniques have been used in multiple studies to achieve better results for Braille in natural language predictions. Machine learning and Deep Learning techniques with different feature extraction methods were used in these studies. A study carried out by Hassan et al., a feed-forward neural network with 400 inputs in the input layer and 190 hidden neurons in the hidden layer, was designed to convert English books into Braille text. For this purpose, small and capital English alphabets, 10 numbers and space were used with and without noise removal. The neural networks achieved satisfactory results for converting text into Braille. The network was built with a simple structure that included several layers and neurons in each layer. Noisy input patterns were introduced to the network, including the noise of a standard 0.2 to all characters, resulting in one or two characters being detected incorrectly each time the program was run [18]. An automated value thresholding algorithm was used for accurately converting English text into Braille. After feeding English text to the system, every word was read and converted into Braille characters. With the advent of this method, visually impaired people can easily read novels, books, etc. Flexibility, low cost and portability were the major advantages of the system, but this system can only be used for reading purposes. Using this method, reading books becomes easier for visually impaired users [20]. Similarly, Braille character segmentation was performed using image processing techniques. Then, Support Vector Machine (SVM) with the Histogram of Oriented Gradients (HOG) feature extraction method was applied to translate English Braille characters. The Histogram of Oriented Gradients, or HOG, is a feature descriptor used to extract features from image data. It is generally used for image/object detection tasks. For this purpose, Megha et al. proposed another SVM technique to include a preprocessing step like noise removal and contrast enhancement on Braille images. Preprocessing was performed at each step to predict Braille to English characters accurately. A better performance achieved 96% accuracy using this technique. However, this study also worked only for scanned documents [23]. In a study by Raghunandan et al., an algorithm was designed that converts text to Braille and Braille to text. Raw data was captured in the form of images. After image acquisition and character segmentation, ASCII characters are converted to Braille text using newly designed algorithms. This method is also limited in the scanned-based input method [24]. The HAAR feature extraction method, along with SVM, was used by Li et al., using scanned Braille sheets for Braille to English conversion. The two-dimensional HAAR wavelet is often utilized in digital image processing as a feature extraction approach. Various versions of 2D HAAR wavelets detect edges in different orientations. Original, as well as cropped, images were used for training using the SVM. English characters were recognized successfully [36]. English characters were converted to the Braille system. Input characters were read using scanned files. Word segmentation was performed, and blank spaces were removed. The Braille database was accessed, the output was matched and the Braille characters were raised on a Braille pad [25]. English and Hindi text to Braille translation was performed. In a study carried out by Singh et al., English and Hindi words were read by the system. After removing spaces from the words and breaking them into the corresponding characters, they were matched using a lookup table [19]. A similar study was carried out to convert Scanned Braille English, Hindi and Tamil into text. For this purpose, 20 scanned Braille sheets were used. Ten were English Braille documents, five were Tamil Braille and five were Hindi Braille. Image enhancement and necessary noise removal were performed for converting Braille into text. A 100% accuracy was achieved by two English Braille documents and one Tamil Braille document. More than 97% accuracy was achieved for the rest of the documents [22]. An image-based Braille character recognition for English and Sinhala was performed using SVM with a Histogram of Oriented Gradient (HOG) feature extraction method. Characters were extracted after applying preprocessing steps and segmentation techniques. More than 99% accuracy was achieved for converting English- and Sinhala-based Braille characters [23]. The math to speech translation system made learning easy for people with visual disabilities. This tool helps non-native speakers and visually impaired people to solve mathematical equations easily [37,38,39].\n\nBraille has been converted into other languages like Urdu [40,41], Arabic [42,43,44], Hindi [19,45], Bengali [46,47] Tamil [22], Sinhala [23], Kannada [48], ODIA [49], Chinese [50,51], Korean [52] and Gujarati [53,54,55].\n\nIn an earlier study conducted by the authors, a new touchscreen-based position-free application was developed that takes Braille input and converts it into equivalent Urdu characters. The English Braille dataset for this study was collected using the same application. This dataset was collected from the “National Special Education School”, Manak Payyan Muzaffarabad, Pakistan. Each student was requested to enter English Braille codes using the position-free application touchscreen device. This application was less tiring, because users had to tap only active dots for each character. This application can only recognize its equivalent English character based on the active dots entered. Every “X” and “Y” coordinate value of the tap dot is stored and further processed for character prediction.\n\nIn this research, the input storage mechanism was changed. Previously, Braille’s image was saved and used by the authors for character predictions. Here, the coordinate value of each dot entered was saved in a text file. The author manually validated the numerical dataset acquired using the touchscreen application. Anomalies were completely removed before processing. For Braille to English character recognition, machine learning techniques such as DT, SVM, KNN and RF are combined with the RICA- and PCA-based feature extraction methods. These techniques are simple to use to give better results, even with a small dataset.\n\n3. Materials and Methods\n\n3.1. Dataset Collection\n\nA position-free touchscreen-based braille input application was designed and developed by the authors in Reference [56]. Using this application, a new dataset of English Braille Patterns was collected from the students of the National Special Education School (NSEC) Manak Payyan, Muzaffarabad, Pakistan, as shown in Figure 2. The age of the students ranged between 12 and 19 years. Visually impaired students entered each English letter by tapping their fingers on the touchscreen. English Braille alphabets comprise different arrangements of six dots. Our application facilitates the users by requiring tapping of only the active dots for each character. “x”- and “y”-coordinate values of the tapped dots against one alphabet are stored with comma separators in a “.txt file”. The inactive dots are filled with a “0” value. All the comma separators are removed, and finally, after removing all the redundant data, a .csv file comprised of 2512 characters is made for further processing.\n\n3.2. English Braille Character Recognition\n\nThis study used robust supervised machine learning techniques like SVM, KNN and Decision Trees to predict English Braille characters correctly. Even with small datasets, these algorithms are well-known for better text prediction [49]. These machine learning techniques were combined with the RICA-based feature extraction method, as it helps improve accuracy, reduce the risk of overfitting the model and speeds up the training process. The RICA (Reconstruction Independent Component Analysis method) also reduces the dimensionality by taking the input data as a mixture of independent components and correctly identifying each one by eliminating unnecessary noise.\n\nThe steps included for English Braille alphabet prediction are as follows:\n\nStep 1: A “.csv” file is given as input for a machine learning application.\n\nStep 2: Input data is divided into test and train data.\n\nStep 3: 5-K cross-validation for (k = 5) was used.\n\nStep 4: All three models are trained using training data.\n\nStep 5: New models are generated for each applied algorithm.\n\nStep 6: Performance of the newly built models are predicted using test data.\n\nStep 7: English Braille alphabets are predated.\n\n3.3. Feature Extraction\n\nThe first stage in classification is the extraction of features according to the type of problem. Previously, various feature extraction techniques used Braille to text predictions. Jha and Parvathi extracted the HOG feature for Braille to Hindi text conversion, and later, they were used by the SVM classifier [57]. In the same way, Li, Xeng and Zu used conventional extraction methods to identify Braille characters using KNN and the Naïve Bayes classifier [58]. Additionally, Li, Yan and Zhang employed HOG extractor methods using SVM to translate Braille to Odia, Sinhala and English [23,49]. The current study uses the reconstruction Independent Component Analysis (RICA)- and PCA-based feature extraction techniques with SVM, DT and KNN for Braille to English alphabet text translation.\n\n3.3.1. RICA Feature Extraction Method\n\nRICA is not a supervised machine learning method; therefore, it does not require class label information. The RICA algorithm was implemented to address the shortcomings and disadvantages of the Independent Component Analysis (ICA) algorithm. This approach yielded more positive results than ICA. Many algorithms for learning sparse features have been introduced in the last few years. The sparse filter is being used to separate many natural signals and man-made ones, and this feature plays a significant role in various machine learning techniques.\n\nThe unlabeled data is indicated as the input.\n\n{ y i } i = 1 n , y i ∈ ℝ m\n\n(1)\n\nThe problem of standard ICA [59] optimization for calculating independent components can be described mathematically as\n\nX min 1 n ∑ i = 1 n h X y i\n\n(2)\n\nSubject to … … … . XX U = I\n\nwhere h(.) is a variation penalty function, “ X ∈ S L × m ” is a matrix-vector, L is the count of the vectors and “I” represents the identity matrix. Besides, XX U = I is used to prevent the vectors in “X” from degenerating. A smooth penalty function can be used for this purpose, i.e., h . = log cos h . [60]. On the other hand, the traditional Independent Component Analysis is blocked by some restrictions relevant to orthonormality from learning on an overcomplete basis. As a result, the deficiency mentioned above prevented ICA from scaling up to high-dimensional data. Therefore, soft reconstruction costs are used in RICA to cover orthonormality constraints in ICA. After this substitution, RICA filtering can be expressed with the following unconstrained problem:\n\nX min λ n ∑ i = 1 n | | X U X y i − y i | | 2 2 + ∑ i = 1 n ∑ k = 1 l h X k y i\n\n(3)\n\nIn the above equation, π > 0 indicates the tradeoff between sparsity and reconstruction error rate. RICA can learn sparse representations when X is over-completed after exchanging orthonormality constraints with reconstruction costs, even on unwhitened data in this way. However, penalty h can yield sparse, but not invariant, representations [61]. RICA [62] therefore swapped it with an additional pooling penalty defined by L 2 , which, at the same time, helps to facilitate pooling features into associated features in the cluster. In addition, L 2 pooling also encourages sparsity for the learning of features. L 2 [63,64] pooling stands for a two-layered network in the first layer . 2 with square nonlinearity and nonlinearity in the second layer . 2 square root.\n\nh X y i = ∑ k = 1 L ε + H k . X y i ⊙ ( X y i )\n\n(4)\n\nIn the pooling matrix H ∈ P L × L by which H k reflects a row of the pooling matrix set to uniform weights, i.e., 1 for each component in the matrix H, element-wise multiplication is defined by a ⨀, and ε > 0 is a small average, as well as an element-wise multiplication. The RICA is a linear approach that only explores sparse representation in the real data space. The RICA method cannot use the relation between class label knowledge and sample training.\n\n3.3.2. Principal Component Analysis\n\nPCA is one of the popular dimensionality reduction feature extraction methods. It is used for data exploration and predictive model development. It decreases the dataset’s dimensionality by merging several features into a few. While creating new features, PCA keeps the majority of the variance. PCA allows us to find correlations and patterns in a dataset to be converted into a dataset with relatively fewer dimensions while retaining all important data. PCA is the foundation of a projection-based multivariate data analysis. The most common application of PCA is to represent a multivariate data table as a smaller number of variables (summary indices) so that trends, jumps, clusters and outliers can be observed. PCA is a powerful technique that can evaluate datasets with missing values, categorical data and erroneous measurements. Its goal is to extract the most significant facts from the data and describe it as a set of summary indices known as principal components. The major advantage of using PCA on the dataset are removal of the correlation between the components, expediating the algorithm performance, resolving the problem of overfitting in the model and better visualization. However, due to feature reduction, there are chances of information loss [65].\n\n3.4. Classification\n\nClassification is the fundamental process for categorizing two or many classes according to the extracted features. Several machine learning methods are supervised, unsupervised, Ensemble Reinforced, Deep Learning and Neural Networks. Previously, most researchers used supervised Braille to text conversion learning techniques. Therefore, we used machine learning techniques such as Decision Trees, KNN and SVM based on the RICA extraction method [66].\n\n3.4.1. Decision Trees\n\nDecision Trees are a predictive method used in machine learning. DT works well for both categorical and continuous data. Decision Trees require less effort to prepare data than other decision techniques. This is a standard classifier for machine learning, since it does not involve many computations [67]. Decision Tree classifiers have a tree-like structure, splitting the dataset into various subsets. This classifier trains the model by imposing basic rules on training data while making decisions [68]. Then, the model is used to predict and identify the targeted values by reading the dataset according to its classes [69].\n\nMathematically, a Decision Tree classifier can be formed using the following equations:\n\nX ¯ = { X 1 , X 2 , X 3 , … … … … … . X m } T\n\n(5)\n\nX i = x 1 , x 2 , x 3 , … … . x ij , … … … x in\n\n(6)\n\nS = { S 1 , S 2 , … … … S i , … … . . S m }\n\n(7)\n\nIn this analysis, we divided the test and training data with a ratio of 70:30. Using the training data is to construct a model, and test data is used to verify the model’s validity. This study used Decision Trees to predict Braille to English text using a multiclass approach. The default parameters were used to change the trees’ decisions.\n\n3.4.2. K-Nearest Neighbor\n\nKNN is the most common and straightforward nonparametric technique employed in machine learning for regression and classification models. KNN is a simple algorithm that works well with smaller datasets, and only two parameters are required for implementation: the value of K and the distance function. The KNN algorithm does not require training before making predictions, and new data can be added without affecting the algorithm’s accuracy. The samples’ differences are determined using the following Euclidian Distance formula [67]:\n\nEU a , b = ∑ i = 1 n a i − b i 2\n\n(8)\n\nwhere a and b represent the number of samples, and a i − b i is the ith feature dimensions of the samples, and n denotes the total number of features dimension.\n\nThe Output value with the KNN classifier depends on the number of neighbors closest to it. If K = 1, the value means the object can be categorized and allocated to the nearest neighbor of that single class [68]. In this study, we used KNN to classify Braille into English text. We selected K = 3 in this analysis, with the Euclidean distance and equal weight.\n\n3.4.3. Random Forest\n\nRandom Forest is a supervised machine learning algorithm widely used in classification and regression problems. Random Forest is capable of handling both continuous and categorical data. It constructs Decision Trees from various samples and uses their majority vote for classification and average for regression, thus eliminating overfitting. It works well, even when the data contains null or missing values. Random Forest selects observations at random, builds a Decision Tree and uses the average result. It does not rely on any formulas. Different parameters are used for enhancing the predictive power and speed. Hyperparameters like n_estimators, max_features, mini-sample_leaf and n_jobs, random_state and oob_score are used to increase the predictive power and speed, respectively. Random Forest is more complex and requires more training time [70].\n\n3.4.4. Support Vector Machine\n\nSVM is the most well-known machine learning classification algorithm for pattern identification and character recognition. SVM is a powerful classification technique for supervised data. It has been applied successfully to many applications, including Computer Vision, Biomedical Imaging and Speech Recognition, which require efficiently dealing with linear and nonlinear dimension data [71]. SVM is good at handling outliers and requires less training time and effort. To obtain better classification, SVM builds a hyperplane in high-dimensional areas. A classifier can achieve good efficiency if the hyperplane has a high functional margin [72]. A more significant margin reduces the risk of generalized mistakes. SVM finds the hyperplane, which provides the training data with the most significant minimum distance. SVM can produce a better, more generic performance. SVM is a double classifier that converts data into a hyperplane that depends on data of a higher dimension.\n\nLet us consider hyperplane x. If w + b = 0, w is normal.\n\nThe representation of linearly separable data is as given below:\n\nx i , y i , x i ϵ R N d , y i ϵ − 1 , 1 , i = 1 , 2 , … … … . N\n\n(9)\n\nwhere y i is the label of a two-fold class.\n\nWhen we optimize the margin by maximizing the value of the objective function, E = ║w║2 gives\n\nx i . w + b ≥ 1 , for y i = + 1\n\n(10)\n\nx i . w + b ≤ 1 , for y i = − 1\n\n(11)\n\nBy eliminating the inconsistencies of the above equations, we now have\n\n( x i . b + b ) y i ≥ 1 , for all i\n\n(12)\n\nIf a dataset cannot be separated linearly, a slack variable “Ξi” is used to recognize classification errors. Thus, the objective function in this scenario is defined as\n\nE = 1 2 ║ w ║ 2 + C ∑ i L Ξ i\n\n(13)\n\nSubject to\n\n( x i . b + b ) y i ≥ 1 − ξ i , for all i\n\n(14)\n\nHere, “C” and “L”. respectively, describe hyperparameters and cost functions. Cost functions for identifying outliers are used. The dual formulation with L(Ξi) = Ξi is\n\nα = max α ( ∑ i , j α i α j y i y j x i x j )\n\n(15)\n\nSubject to\n\n0 ≤ ∝ α i ≤ C , ∑ i α i y j = 0\n\n(16)\n\nHere,\n\nα = α 1 , α 2 , α 3 … … … . α i , ω 0 = ∑ i α i x i y i\n\n(17)\n\nA kernel trick is being used to accommodate separable data, which are not linear [73]. The nonlinear mapping function is transformed from the input space into a higher dimensional feature space. The dot product of x i , y i is replaced by functions to apply this kernel trick to two classes. The most popular kernels: Polynomial, Gaussian and Radial-based functions are given in Table 1.\n\nThe dual formation of a nonlinear case is shown as\n\nα * = max α ( ∑ i α i + ∑ i , j α i α j y i y j , K x i . y j )\n\n(18)\n\nSubject to\n\n0 ≤ α i ≤ C , ∑ i α i y j = 0\n\n(19)\n\nGrid search is the famous assessment metric used for evaluating SVM. The appropriate parameters are carefully selected by setting the grid range and phase size. Only one parameter, the “C” constant of soft margins, is used in a linear kernel, whereas SVM Gaussian kernel and SVM fine Gaussian kernel have two training parameters that cost “C” and Gamma to control the degree of nonlinearity. We used the RICA and PCA feature extraction methods with train test splits of 70:30 and 80:20, respectively. We applied default parameters to the Polynomial kernel.\n\n3.4.5. Sequential Model\n\nThe Sequential model is one of the simplest linear function neural network models. This model is suitable for a simple stack of layers with one input and one output tensor. In this model, not all the nodes are connected to other nodes of other layers. It handles input or output data sequences in text streams, audio clips, video clips, time series data and other types of sequential data. It comprises a convolution layer, nonlinear activation layer, pooling layer and a fully connected layer [74].\n\n4. Results\n\nEnglish Braille alphabet recognition is performed using SVM, KNN and Decision Trees with the RICA-based feature extraction method. The performance metrics used for the evaluation are the True Positive Rate (TPR), True Negative Rate (TNR), False Positive Rate (FPR), Positive Predicted Value (PPV), Negative Predicted Value (NPV), Total Accuracy, Area Under the curve (AUC) and F1-Score.\n\nFigure 3a,b shows AUC values for category-1 (class a–class m) and category-2 (class n–class z) using DT. The highest performance achieved from category-1 (class a–class m) by using the RICA feature extraction method is for Braille classes a, c, d, h, i, j and k with TA (100%), TPR (100%), TNR (100%), AUC = 1 and F1-Score = 1. Following that, other classes like b and f achieved accuracy of 99.87% and 99.60%, AUC of 0.99 and 0.97 and F1-Score 0.99 and 0.94, respectively. As shown in Figure 3b, classes p, u and w from category-2 (n–z) have the best performances with TA (100%), TPR (100%), TNR (100%), AUC (1) and F1-Score = 1. Following them are classes q, s, t, v, y and z with TA (99.87%); TPR (100%, 94.74%, 100%, 96.15% and 97.44%); TNR (96.30%, 100%, 96.55%, 97.14%, 100% and 100%) and AUC (0.99, 0.97, 0.99, 0.99, 0.98 and 0.98). As shown in Table 2. the highest separation (AUC = 1) was achieved for the classes a, c, d, h, i, j, k, p, u and w. The AUC value of classes b, f, g, q, t and v is greater than 99.5%, indicating good classification.\n\nThe highest accuracy achieved for category-1 (a-m) using KNN was for classes a, i, j and k, with TA (100%), TPR (100%), TNR (100%), AUC (1) and F1- Score achieved also equal to 1. As shown in Figure 4a, classes d, l and m have TA (99.87%); TPR (100%, 94.74% and 100%); TNR (99.86%, 100% and 99.87%) and AUC (0.99, 0.97 and 0.99), respectively. As shown in Figure 4b, the classes p, u and y achieve the highest TA of 100%, AUC and F1-Score equal to 1. For category-2 (n–z), followed by q, v, w and x with TA (99.87%); TPR (100%, 96.97%, 96.67% and 100%); TNR (99.86%, 100%, 100% and 99.86%); AUC (0.99, 0.98, 0.98 and 0.99) and F1-Score (0.98, 0.98, 0.98 and 0.97). The overall results indicated that English Braille characters such as a, i, j, k, p, u and y achieved the highest AUC value of 1, indicating 100% separation. Using KNN on the extracted feature set yielded AUC values greater than 0.99 for English Braille characters such as b, c, d, m, q and x. KNN also showed promising results for recognizing Urdu Braille characters. It has also improved the results for English Braille alphabet recognition. Table 3 exhibits detailed KNN results for all the characters.\n\nFurthermore, SVM was used to evaluate performances of different classes. Category-1 (a–m) achieved the highest accuracy with TA, TPR and TNR (100%) and the highest separation AUC of 1 and F1-Score also 1 for classes a, c, d, h, i, j and k. Followed by b and l with TA (99.87%), TPR (100%), TNR (99.86%) and AUC (0.99), as shown in Figure 5a. The highest performance for category-2 (n–z) was achieved for classes n, p, t, u, v, w, y and z with the TA, TPR and TNR (100%) and AUC (1) and F1-Score also 1. As shown in Figure 5b, classes q and r with TA (99.87%), TPR (100%), TNR (99.86%) and AUC (0.99) are followed by classes o and s with TA (99.73%), TPR (94.74% and 88.89%), TNR (99.86% and 100%), AUC (0.97 and 0.94) and F1-Score (0.95 and 0.94), respectively. Among all classes, a, c, d, h, i, j, k, n, p, t, u, v, w, y and z have the greatest separation of 1. Table 4 presents the SVM classifier’s detailed performance for English Braille character recognition.\n\nFor comparison purposes, we used PCA-based feature extraction methods, Random Forest and the Simple neural network. Among all the results, classifiers used with the RICA-based feature extraction method showed the best performances. Using Random Forest with five n_folds, a max_depth of 10 and Trees value of 10, the mean accuracy achieved was 75.61%. The results revealed by implementing the Sequential model using activation functions ReLu and Softmax, a batch size of five and 2000 epochs accuracy achieved was 93.51%, with a loss of 0.1736. With PCA feature extraction, the accuracy achieved for SVM, KNN and DT were 86.32%, 75.40% and 70.02%, respectively. All performances of SVM were better than DT and KNN using both feature extraction methods. SVM achieved a TA of (99.86%), TPR of (98.23%), TNR of (99.91%), PPV of (97.94%), NPV of (99.94%), FPR of (0.0009%) and F1-Score = 0.980. Followed by DT, which shows the TA (99.79%), TPR (96.91%), TNR (99.89%), PPV (97.22%), NPV (99.89%), FPR (0.0011) and F1-score (0.970), followed by KNN with TA (99.50%), TPR (95.32%), TNR (99.70%), PPV (93.70%), NPV (99.79%), FPR (0.0030%) and F1-Score (0.939), as shown in Table 5.\n\nTwo hypotheses were built to measure the significance between the results achieved using RICA- and PCA-based feature extraction method on different classification techniques. A t-test was applied to calculate the p-value.\n\nHypothesis 1 (Null Hypothesis).\n\nThere is no difference in the results of the RICA-based features extraction method using DT, KNN, SVM and RF.\n\nHypothesis 1 (Alternative Hypothesis).\n\nThere is a difference between the results of the RICA-based feature extraction method using DT, KNN, SVM and RF.\n\nHypothesis 2 (Null Hypothesis).\n\nThere is no difference in the results of the PCA-based features extraction method using DT, KNN, SVM and RF.\n\nHypothesis 2 (Alternative Hypothesis).\n\nThere is a difference between the results of the PCA-based feature extraction method using DT, KNN, SVM and RF.\n\nThe results of the RICA-based feature extraction method showed that there is a significant difference among the values of DT vs. KNN and DT vs. RF, as the p-value was less than 0.05, but no significant difference was found between SVM and DT. For the PCA-based feature extraction method, a significant difference was observed in all three comparisons. The results are shown in Table 6.\n\n5. Discussion\n\nBraille to natural language conversion is essential for people with visual impairment. This can help to improve their living quality. English is the most common medium used for communication all over the world. Various studies have been carried out for converting Braille into English. In the literature, several studies have converted Braille to English or vice versa. However, those studies usually used the conventional methods for text conversion. Previous studies used handwritten scanned Braille sheets a thes input, and then, those scanned sheets were converted into other languages. Padmavathi et al. used English, Hindi and Tamil handwritten scanned sheets for Braille conversion [22].\n\nSimilarly, Perera et al. used scanned Braille sheets using a histogram of gradient features and SVM [23]. Most of the studies used conventional methods like scanned-based input with different character prediction techniques like Deep Learning; Image classification and other machine learning techniques like SVM, KNN, Artificial Neural Network (ANN), etc. These classification techniques were used with different feature extraction methods for character recognition, as shown in Table 7. Whereas we used touchscreen-based real-time Braille input for Braille to natural language conversion, the focus of this study was to provide users with a position-free Braille input method using touchscreen-based android devices to provide an improved Braille to English Language conversion mechanism that would be more user-friendly and easily accessible.\n\nFor the prediction of Braille to English characters, DT, SVM and KNN, along with RICA- and PCA-based feature extraction, were used. The English Braille dataset was collected from visually impaired students at the National Special Education School using a previously developed position-free touchscreen-based application. SVM, KNN and DT showed better performances using the RICA feature extraction method. The major findings were achieved using DT for the highest TA for characters: a, c, d, h, i, j, k, p, u and w, yielding TA = 100%, AUC = 1 and F1-Score = 1. Characters like b, m, q, s, t, v, y and z were next, with TA = 99.87%; AUC of 0.999, 0.976, 0.999, 0.974, 0.999, 0.999, 0.981 and 0.985 and F1-Scores 0.99, 0.98, 0.97, 0.98, 0.99, 0.98 and 0.99, respectively. The maximum accuracy was achieved with the KNN classifier for characters a, i, j, k, p, u and y, with TA, TPR and TNR = 100%, AUC = 1 and F1-Score = 1. This was followed by characters d, l, m, q, v, w and x with TA = 99.87%; TPR = 100%, 99.74%, 100%, 100%, 96.97%, 96.97%, and 100%; TNR > 99%; AUC 0.999, 0.974, 0.999, 0.985, 0.985 and 0.999 and F1-Score 0.98, 0.97, 0.96, 0.98, 0.98, 0.98 and 0.97, respectively. Similarly, the SVM classifier achieved the best performance for classes a, c, d, h, i, j, k, n, p, t, u, v, w, y and z with TA, TPR, TNR = 100% and with F1-Score = 1, and the highest separation of 1 was achieved. Classes b, l, q and r were next, with TA = 99.87%; TPR = 100%; TNR = 99.86%, 99.86%, 100% and 99.96%; AUC 0.999, 0.999, 1 and 0.999 and F1-Scores 0.99, 0.97, 0.98 and 0.98, respectively. The total accuracy achieved using PCA with SVM, KNN and DT were 86.32%, 75.40% and 70.02%, respectively. Other results achieved were Precision (88.12%, 79.56% and 72.01%); Recall (86.32%, 75.45% and 68.04%) and F1- Score (0.86, 0.75 and 0.71). For comparison purposes, Random Forest with the RICA and PCA and Sequential models has also been employed, and they achieved accuracies of 80%, 90.02% and 93.51%, respectively.\n\nTable 7. Comparative analysis with previous studies.\n\nPaper TitleInput MethodSupported LanguageBraille to TextText to BrailleTechniques UsedFeature Extraction/Algorithms/OthersAccuracyReferenceBraille Messenger: Adaptive Learning Based Non-Visual Touchscreen Text Input for the Blind Community Using BrailleGesture-Based Touchscreen InputEnglishYesNoKNNBayesian Touch Distance97.4%[73]NillNewly Proposed Static Mathematical Algorithm94.86%Conversion of Braille to Text in\n\nEnglish, Hindi, And Tamil LanguagesHand-Written Scanned Braille SheetsEnglishYesNoNillImage Segmentation Technique99.4%[22]Optical Braille Recognition with HAAR Wavelet Features and Support-Vector\n\nMachineHand-Written Scanned Braille SheetsEnglishYesNoSVMHAAR Feature Extraction MethodReduced classification error to 10[36]Optical Braille Recognition Based on Histogram of Oriented Gradient Features and Support-Vector MachineHand-Written Scanned Braille SheetEnglishYesNoSVMHOG Feature Extraction Method99% [23]Robust Braille recognition system using image preprocessing and feature extraction algorithmsHand-Written Braille Scanned SheetEnglishYesNoImage Processing TechniquesEdge Detection, Image Projection, and Image Segmentation100%[75]Braille Identification System Using Artificial Neural NetworksHand-Written Braille Scanned SheetEnglishYesNoArtificial Neural NetworkBack Propagation Algorithm85%[76]Conversion Of English Characters Into Braille Using Neural NetworkHand-Written English Scanned SheetEnglishNoYesNeural NetworkNoise with 0.1 std showed no errors[18]Designing Of English Text To Braille\n\nConversion System: A SurveyHand-Written English Scanned SheetEnglishNoYesMicrocontrollerAccurate mapping of English to Braille text[20]Efficient Approach for English Braille to Text\n\nConversionHand-Written English Scanned SheetEnglishYesNoSVM Image Enhancement, Noise Reduction, Contrast\n\nEnhancement and Image Dilation96%[21]The Methods Used in Text to Braille\n\nConversion and Vice VersaImage Taken from CameraEnglishNoYesRaspberry PIAccurate output was achieved[24]Automated Conversion of English and Hindi Text to Braille RepresentationHand-Written Scanned SheetsEnglishNoYesUsing Lookup TablesEnglish To Braille characters were accurately mapped[19]Application of Deep Learning to Classification of Braille Dot for Restoration of Old Braille BooksHand-Written Braille Scanned SheetsBrailleDeep LearningImage Enhancement and Restoration Techniques98%[77]A Recurrent Neural Network Approach to Image\n\nCaptioning in Braille for Blind-Deaf PeopleEnglish Captions of Images Taken from Camera EnglishDeep Recurrent Neural NetworkBLEU-4 Score\n\nOf 0.24 is achieved[78]Smart Braille Recognition SystemBraille Images Taken from Camera EnglishYesNoBayesianCentroid Features100%[79]KNN100%Classification\n\nTree80.76%SVM67.9%Proposed SchemesTouch-Screen Based Input MethodEnglishYesNoSVMRICA Feature Extraction99.86%KNN99.50%DT99.79%RF90.02%SVMPCA Feature Extraction86%KNN75%DT70.02%RF80%Sequential Method93.51%\n\n6. Conclusions and Future Work\n\nBraille is used as an important means of communication for people with low or no vision. There are approximately over 150 million Braille users worldwide. Braille is becoming increasingly accessible to blind or visually challenged persons with the growing use of technology. Numerous studies have been carried out for Braille to English conversion. The majority of these conversions are carried out with scanned sheets as the input. This research collected a new English Braille dataset using touchscreen devices. The authors used the Android application developed to collect Braille English datasets in their previous research. For visually impaired users, the application is less tiring and less complicated. Machine learning techniques such as SVM with polynomial kernels, KNN = 3 and Decision Trees with default parameters are combined with RICA- and PCA-based feature extraction methods for English alphabet recognition. For training and testing, the dataset was split into 70:30 and 80:20, respectively. Precision, Recall, F1-Score and Accuracy were used as the evaluation metrics using PCA SVM, KNN and DT accuracies of 86.32%, 75.40% and 70.02%, whereas better results were obtained using RICA with SVM, KNN and DT. The SVM classifier outperformed all others, achieving an accuracy of 99.85%. KNN and DT achieved 99.50% and 99.79% accuracy, respectively. For comparing these results with other techniques, Random Forest with the RICA and PCA and Sequential methods were used, and they achieved an accuracy of 90.01% and 80%, respectively.\n\nThis work was limited to only Grade 1 Braille for the English language. This work can be enhanced by increasing the number of datasets not only for Grade 1 but also for Grade 2 and contracted Braille. Currently, the results of the proposed models are not satisfactory when implemented on mobile devices with limited computation power; thus, we tested these results only for computers. The future plan includes converting these models into lighter versions to work appropriately on Android devices. This study used RICA- and PCA-based feature extraction methods for English Braille character prediction with robust machine learning techniques on the Grade 1 Braille dataset. This work can be enhanced to the Braille English dataset of Grade 2. Deep learning techniques like CNN, GoogLeNet and Transfer Learning will improve the results for mobile devices.\n\nAuthor Contributions\n\nConceptualization, R.R.; Formal analysis, S.S. and S.S.R.; Funding acquisition, A.P.; Methodology, S.S. and A.P.; Project administration, R.R.; Resources, A.P.; Supervision, R.R. and S.S.R.; Validation, I.K.; Visualization, I.K.; Writing—original draft, S.S. and Writing—review and editing, S.S.R. and I.K. All authors have read and agreed to the published version of the manuscript.\n\nFunding\n\nThis research work was supported by the National Research Foundation of Korea (NRF) grants funded by the Korean government under reference number (2020R1A2C1012196).\n\nInformed Consent Statement\n\nInformed consent was obtained from all subjects involved in the study.\n\nConflicts of Interest\n\nThe authors declare that there are no conflicts of interest regarding the publication of this paper.\n\nAbbreviations\n\nANN Artificial Neural NetworkASCIIAmerican Standard Code for Information InterchangeAUCArea under the CurveDTDecision TreeFDRFalse Discovery RateFNFalse NegativeFNRFalse Negative RateFPFalse PositiveFPRFalse Positive RateGPU’sGraphics Processing UnitHOGHistogram of Oriented GradientsKNNK-Nearest NeighborNPVNegative Predicted ValuePCAPrincipal Component AnalysisPPVPositive Predicted ValueRFRandom ForestROCReceiver Operating CharacteristicsSVMSupport Vector MachineSnSensitivitySpSpecificityTNRTrue Negative RateTPRTrue Positive Rate\n\nReferences\n\nWHO. Blindness and Vision Impairment; WHO: Geneva, Switzerland, 2019. [Google Scholar]\n\nJared Smith WebAIM: Screen Reader User Survey #5 Results. Available online: http://webaim.org/projects/screenreadersurvey5/ (accessed on 10 September 2020).\n\nMiyauchi, H. A Systematic Review on Inclusive Education of Students with Visual Impairment. Educ. Sci. 2020, 10, 346. [Google Scholar] [CrossRef]\n\nWorks, B. History of Braille. Available online: https://brailleworks.com/braille-resources/history-of-braille/#:~:text=Braille (accessed on 12 August 2020).\n\nAlnfiai, M.; Sampalli, S. SingleTapBraille: Developing a Text Entry Method Based on Braille Patterns Using a Single Tap. Procedia Comput. Sci. 2016, 94, 248–255. [Google Scholar] [CrossRef] [Green Version]\n\nAlnfiai, M.; Sampalli, S. Improved SingleTapBraille: Developing a Single Tap Text Entry Method Based on Grade 1 and 2 Braille Encoding. J. Ubiquitous Syst. Pervasive Netw. 2017, 9, 23–31. [Google Scholar] [CrossRef]\n\nNational Institutes of Health. National Eye Institute LookTel—Instant Recognition Apps for Persons with Low Vision or Blindness. Available online: https://www.nih.gov/about-nih/what-we-do/nih-almanac/national-eye-institute-nei (accessed on 12 September 2020).\n\nReader, K. KNFB Reader App Features the Best OCR. Available online: https://www.knfbreader.com/ (accessed on 10 February 2020).\n\nCloudSight TapTapSee—Blind and Visually Impaired Assistive Technology. Image Recognition API. Available online: https://taptapseeapp.com/ (accessed on 2 September 2020).\n\nAl-Doweesh, S.A.; Al-Hamed, F.A.; Al-Khalifa, H.S. What Color? A Real-Time Color Identification Mobile Application for Visually Impaired People. In Proceedings of the International Conference on Human-Computer Interaction, 22–24 June 2014; Springer: Toronto, ON, Canada, 2014; pp. 203–208. [Google Scholar]\n\nWiberg, H.J. Be My Eyes—See the World Together. Available online: https://www.bemyeyes.com/ (accessed on 2 July 2019).\n\nKouroupetroglou, G.; Pino, A.; Riga, P. A Methodological Approach for Designing and Developing Web-Based Inventories of Mobile Assistive Technology Applications. Multimed. Tools Appl. 2017, 76, 5347–5366. [Google Scholar] [CrossRef]\n\nLouis Braille and the Night Writer. Available online: https://www.historytoday.com/louis-braille-and-night-writer (accessed on 18 September 2020).\n\nŠepić, B.; Ghanem, A.; Vogel, S. BrailleEasy: One-Handed Braille Keyboard for Smartphones. Stud. Health Technol. Inform. 2015, 217, 1030–1035. [Google Scholar] [CrossRef]\n\nSiqueira, J.; Soares, A.d.M.F.A.; Silva, C.R.G.; de Oliveira Berretta, L.; Ferreira, C.B.R.; Felix, I.M.; Luna, M.M. BrailleÉcran: A Braille Approach to Text Entry on Smartphones. In Proceedings of the 2016 IEEE 40th Annual Computer Software and Applications Conference (COMPSAC), Atlanta, GA, USA, 10–14 June 2016; pp. 608–609. [Google Scholar]\n\nAlnfiai, M.; Sampalli, S. BrailleEnter: A Touch Screen Braille Text Entry Method for the Blind. Procedia Comput. Sci. 2017, 109, 257–264. [Google Scholar] [CrossRef]\n\nLi, M.; Fan, M.; Truong, K.N. BrailleSketch. In Proceedings of the 19th International ACM SIGACCESS Conference on Computers and Accessibility, Baltimore, MD, USA, 20 October–1 November 2017; ACM: New York, NY, USA, 2017; pp. 12–21. [Google Scholar]\n\nHassan, M.Y.; Mohammed, A.G. Conversion of English Characters into Braille Using Neural Network 1. Iraqi J. Comput. Commun. Control Syst. Eng. 2011, 11, 30–37. [Google Scholar]\n\nSingh, M.; Bhatia, P. Automated Conversion of English and Hindi Text to Braille Representation. Int. J. Comput. Appl. 2010, 4, 25–29. [Google Scholar] [CrossRef]\n\nDharme, V.S.; Karmore, S.P. Designing of English Text to Braille Conversion System: A Survey. In Proceedings of the 2015 International Conference on Innovations in Information, Embedded and Communication Systems (ICIIECS), Coimbatore, India, 19–20 March 2015; pp. 1–6. [Google Scholar]\n\nGadag, M.; Udayashankara, V. Efficient Approach for English Braille to Text Conversion. Int. J. Adv. Res. Electr. Electron. Instrum. Eng. 2016, 5, 3343–3348. [Google Scholar] [CrossRef]\n\nPadmavathi, S.; Reddy, S.S.; Meenakshy, D. Conversion of Braille to Text in English, Hindi and Tamil Languages. Int. J. Comput. Sci. Eng. Appl. 2013, 3, 19–32. [Google Scholar] [CrossRef]\n\nPerera, T.D.S.H.; Wanniarachchi, W.K.I.L.I. Optical Braille Recognition Based on Histogram of Oriented Gradient Features and Support-Vector Machine. Int. J. Eng. Sci. 2018, 8, 19192–19195. [Google Scholar]\n\nRaghunadan, A.; MR, A. The Methods Used in Text to Braille Conversion and Vice Versa. Int. J. Innov. Res. Comput. Commun. Eng. 2017, 5, 8198–8205. [Google Scholar] [CrossRef]\n\nKumar, P.R. Braille Language Converter for Visually Impaired People. Int. J. Intellect. Adv. Res. Eng. Comput. 2018, 6, 2229–2232. [Google Scholar]\n\nDevi, G.G. Braille Document Recongnition Languages—A Review. In Proceedings of the 2018 Fourth International Conference on Advances in Electrical, Electronics, Information, Communication and Bio-Informatics (AEEICB), Chennai, India, 27–28 February 2018; pp. 1–4. [Google Scholar]\n\nGuerreiro, T.; Lagoá, P.; Santana, P.; Gonçalves, D.; Jorge, J. NavTap and BrailleTap: Non-Visual Texting Interfaces. In Proceedings of the Rehabilitation Engineering and Assistive Technology Society of North America Conference (Resna), Arlington, VA, USA, 27 June 2008; pp. 1–10. Available online: https://www.researchgate.net/profile/Tiago-Guerreiro-5/publication/267550620_NavTap_and_BrailleTap_NavTap_and_BrailleTap_Non-Visual_Texting_Interfaces/links/09e4151066a4ee9c12000000/NavTap-and-BrailleTap-NavTap-and-BrailleTap-Non-Visual-Texting-Interfaces.pdf (accessed on 20 August 2021).\n\nJayant, C.; Acuario, C.; Johnson, W.A.; Hollier, J.; Ladner, R.E. VBraille: Haptic Braille Perception Using a Touch-Screen and Vibration on Mobile Phones. In Proceedings of the 12th International ACM SIGACCESS Conference on Computers and Accessibility, ASSETS 2010, Orlando, FL, USA, 25–27 October 2010; ACM: New York, NY, USA; pp. 295–296. [Google Scholar]\n\nMascetti, S.; Bernareggi, C.; Belotti, M. TypeInBraille: A Braille-Based Typing Application for Touchscreen Devices. In Proceedings of the 13th International ACM SIGACCESS conference on Computers and Accessibility, Dundee Scotland, UK, 24–26 October 2011; ACM: New York, NY, USA; pp. 295–296. [Google Scholar]\n\nFrey, B.; Southern, C.; Romero, M. BrailleTouch: Mobile Texting for the Visually Impaired. In Universal Access in Human-Computer Interaction; Springer: Berlin/Heidelberg, Germany, 2011; pp. 19–25. [Google Scholar]\n\nMilne, L.R.; Bennett, C.L.; Ladner, R.E.; Azenkot, S. BraillePlay: Educational Smartphone Games for Blind Children. In Proceedings of the 16th International ACM SIGACCESS Conference on Computers & Accessibility, Rochester, NY, USA, 20–22 October 2014; ACM: New York, NY, USA, 2014; pp. 137–144. [Google Scholar]\n\nMattheiss, E.; Regal, G.; Schrammel, J.; Garschall, M.; Tscheligi, M. EdgeBraille: Braille-Based Text Input for Touch Devices. J. Assist. Technol. 2015, 9, 147–158. [Google Scholar] [CrossRef]\n\nBeSpecular. Available online: https://www.bespecular.com/ (accessed on 20 February 2020).\n\nKacorri, H.; Kitani, K.M.; Bigham, J.P.; Asakawa, C. People with Visual Impairment Training Personal Object Recognizers: Feasibility and Challenges. In Proceedings of the 2017 CHI Conference on Human Factors in Computing Systems, Denver, CO, USA, 6–11 May 2017; ACM: New York, NY, USA; pp. 5839–5849. [Google Scholar]\n\nPriyadarsini, S.; Ajit, P.; Nayak, K.; Champati, S. A Survey on Speech Synthesis Techniques in Indian Languages. Multimed. Syst. 2020, 26, 453–478. [Google Scholar] [CrossRef]\n\nLi, J.; Yan, X.; Zhang, D. Optical Braille Recognition with Haar Wavelet Features and Support-Vector Machine. In Proceedings of the International Conference on Computer, Mechatronics, Control and Electronic Engineering, Changchun, China, 24–26 August 2010; pp. 64–67. [Google Scholar]\n\nBier, A.; Sroczyński, Z. Rule Based Intelligent System Verbalizing Mathematical Notation. Multimed. Tools Appl. 2019, 78, 28089–28110. [Google Scholar] [CrossRef] [Green Version]\n\nMaćkowski, M.; Brzoza, P.; Żabka, M.; Spinczyk, D. Multimedia Platform for Mathematics’ Interactive Learning Accessible to Blind People. Multimed. Tools Appl. 2018, 77, 6191–6208. [Google Scholar] [CrossRef] [Green Version]\n\nYook, J.; Kim, K.; Son, B.C.; Park, S. A Translating Program Usability Analysis of Alternative Multimedia Mathematics Materials for the Blind. Multimed. Tools Appl. 2021, 80, 34643–34659. [Google Scholar] [CrossRef]\n\nFahiem, M.A. A. A Deterministic Turing Machine for Context Sensitive Translation of Braille Codes to Urdu Text. In Combinatorial Image Analysis; Springer: Berlin/Heidelberg, Germany, 2008; pp. 342–351. [Google Scholar]\n\nIqbal, M.Z.; Shahid, S.; Naseem, M. Interactive Urdu Braille Learning System for Parents of Visually Impaired Students. In Proceedings of the 19th International ACM SIGACCESS Conference on Computers and Accessibility, Baltimore, MD, USA, 20 October–1 November 2017; ACM: New York, NY, USA, 2017; pp. 327–328. [Google Scholar]\n\nAl-Salman, A.; AlOhali, Y.; AlKanhal, M.; AlRajih, A. An Arabic Optical Braille Recognition System. In Proceedings of the New Trends in ICT and Accessibility–Proceedings of the 1st International Conference in Information and Communication Technology and Accessibility, Hammamet, Tunisia, 12–14 April 2007. [Google Scholar]\n\nAl-Shamma, S.D.; Fathi, S. Arabic Braille Recognition and Transcription into Text and Voice. In Proceedings of the 2010 5th Cairo International Biomedical Engineering Conference, Cairo, Egypt, 16–18 December 2010; pp. 227–231. [Google Scholar]\n\nAl-Salman, A.S. A Bi-Directional Bi-Lingual Translation Braille-Text System. J. King Saud Univ.-Comput. Inf. Sci. 2008, 20, 13–29. [Google Scholar] [CrossRef] [Green Version]\n\nBeg, U.; Parvathi, K.; Jha, V. Text Translation of Scanned Hindi Document to Braille via Image Processing. Indian J. Sci. Technol. 2017, 10, 1–8. [Google Scholar] [CrossRef]\n\nNahar, L.; Jaafar, A.; Ahamed, E.; Kaish, A.B.M.A. Design of a Braille Learning Application for Visually Impaired Students in Bangladesh. Assist. Technol. 2015, 27, 172–182. [Google Scholar] [CrossRef] [PubMed]\n\nAbir, T.R.; Ahmed, T.S.B.; Rahman, M.D.T.; Jafreen, S. Handwritten Bangla Character Recognition to Braille Pattern Conversion Using Image Processing and Machine Learning. Ph.D. Thesis, Brac University, Dhaka, Bangladesh, 2018. [Google Scholar]\n\nUmarani, M.V.; Sheddi, R.P. A Review of Kannada Text to Braille Conversion. Int. J. Eng. Sci. Comput. 2018, 8, 15953–15956. [Google Scholar]\n\nJha, V.; Parvathi, K. Machine Learning Based Braille Transliteration of Odia Language. Int. J. Innov. Technol. Explor. Eng. 2020, 9, 1866–1871. [Google Scholar] [CrossRef]\n\nWang, X.; Yang, Y.; Liu, H.; Qian, Y. Chinese-Braille Translation Based on Braille Corpus. Int. J. Adv. Pervasive Ubiquitous Comput. 2016, 8, 56–63. [Google Scholar] [CrossRef] [Green Version]\n\nWang, X.; Zhong, J.; Cai, J.; Liu, H.; Qian, Y. CBConv: Service for Automatic Conversion of Chinese Characters into Braille with High Accuracy. In Proceedings of the 21st International ACM SIGACCESS Conference on Computers and Accessibility, Pittsburgh, PA, USA, 28–30 October 2019; ACM: New York, NY, USA, 2019; pp. 566–568. [Google Scholar]\n\nLee, S.; Jung, S.; Song, H. CNN-Based Drug Recognition and Braille Embosser System for the Blind. J. Comput. Sci. Eng. 2018, 12, 149–156. [Google Scholar] [CrossRef]\n\nParekh, H.; Shah, S.; Patel, F.; Vyas, H. Gujarati Braille Text Recognition: A Review. Int. J. Comput. Sci. Commun. 2015, 7, 19–24. [Google Scholar] [CrossRef]\n\nJariwala, N.; Patel, B. A System for the Conversion of Digital Gujarati Text-to-Speech for Visually Impaired People. In Speech and Language Processing for Human-Machine Communications; Advances in Intelligent Systems and Computing; Springer: Singapore, 2018; Volume 664, pp. 67–75. ISBN 9789811066252. [Google Scholar]\n\nVyas, H.A.; Virparia, P.V. Transliteration of Braille Character to Gujarati Text the Application. Int. J. Comput. Sci. Eng. 2019, 7, 701–705. [Google Scholar] [CrossRef]\n\nShokat, S.; Riaz, R.; Rizvi, S.S.; Abbasi, A.M.; Abbasi, A.A.; Kwon, S.J. Deep Learning Scheme for Character Prediction with Position-Free Touch Screen-Based Braille Input Method. Hum.-Cent. Comput. Inf. Sci. 2020, 10, 41. [Google Scholar] [CrossRef]\n\nJha, V.; Parvathi, K. Braille Transliteration of Hindi Handwritten Texts Using Machine Learning for Character Recognition. Int. J. Sci. Technol. Res. 2019, 8, 1188–1193. [Google Scholar] [CrossRef]\n\nLi, T.; Zeng, X.; Xu, S. A Deep Learning Method for Braille Recognition. In Proceedings of the 2014 International Conference on Computational Intelligence and Communication Networks, Bhopal, India, 14–16 November 2014; pp. 1092–1095. [Google Scholar]\n\nHyvärinen, A.; Oja, E. Independent Component Analysis: Algorithms and Applications. Neural Netw. 2000, 13, 411–430. [Google Scholar] [CrossRef] [Green Version]\n\nXiao, Y.; Zhu, Z.; Zhao, Y.; Wei, Y.; Wei, S. Kernel Reconstruction ICA for Sparse Representation. IEEE Trans. Neural Netw. Learn. Syst. 2015, 26, 1222–1232. [Google Scholar] [CrossRef]\n\nHyvärinen, A.; Hurri, J.; Hoyer, P.O. Natural Image Statistics: A Probabilistic Approach to Early Computational Vision; Springer: London, UK, 2009; Volume 39, ISBN 978-1-84882-490-4. [Google Scholar]\n\nLe, Q.V.; Karpenko, A.; Ngiam, J.; Ng, A.Y. ICA with Reconstruction Cost for Efficient Overcomplete Feature Learning. In Proceedings of the Advances in Neural Information Processing Systems, Granada, Spain, 12–15 December 2011; pp. 1017–1025. [Google Scholar]\n\nLe, Q.V. Building High-Level Features Using Large Scale Unsupervised Learning. In Proceedings of the 2013 IEEE International Conference on Acoustics, Speech and Signal Processing, Vancouver, BC, Canada, 26–31 May 2013; pp. 8595–8598. [Google Scholar]\n\nBoureau, Y.-L.; Ponce, J.; LeCun, Y. A Theoretical Analysis of Feature Pooling in Visual Recognition. In Proceedings of the 27th International Conference on Machine Learning (ICML-10), Haifa, Israel, 21–24 June 2010; pp. 111–118. [Google Scholar]\n\nOdhiambo Omuya, E.; Onyango Okeyo, G.; Waema Kimwele, M. Feature Selection for Classification Using Principal Component Analysis and Information Gain. Expert Syst. Appl. 2021, 174, 114765. [Google Scholar] [CrossRef]\n\nRasheed, I.; Gupta, V.; Banka, H.; Kumar, C. Urdu Text Classification: A Comparative Study Using Machine Learning Techniques. In Proceedings of the 2018 Thirteenth International Conference on Digital Information Management (ICDIM), Berlin, Germany, 24–26 September 2018; pp. 274–278. [Google Scholar]\n\nShaikhina, T.; Lowe, D.; Daga, S.; Briggs, D.; Higgins, R.; Khovanova, N. Decision Tree and Random Forest Models for Outcome Prediction in Antibody Incompatible Kidney Transplantation. Biomed. Signal Process. Control 2019, 52, 456–462. [Google Scholar] [CrossRef]\n\nHussain, L.; Awan, I.A.; Aziz, W.; Saeed, S.; Ali, A.; Zeeshan, F.; Kwak, K.S. Detecting Congestive Heart Failure by Extracting Multimodal Features and Employing Machine Learning Techniques. BioMed Res. Int. 2020, 2020, 4281243. [Google Scholar] [CrossRef]\n\nTaylor, K.; Silver, L. Smartphone Ownership Is Growing Rapidly around the World, but Not Always Equally|Pew Research Center. 2019. Available online: https://www.Pewresearch.Org/Global/2019/02/05/Smartphone-Ownership-Is-Growing-Rapidly-Around-the-World-But-Not-Always-Equally/ (accessed on 20 September 2021).\n\nPranckevičius, T.; Marcinkevičius, V. Comparison of Naïve Bayes, Random Forest, Decision Tree, Support Vector Machines, and Logistic Regression Classifiers for Text Reviews Classification. Balt. J. Mod. Comput. 2017, 5, 221–232. [Google Scholar] [CrossRef]\n\nTanveer, M.; Sharma, A.; Suganthan, P.N. General Twin Support Vector Machine with Pinball Loss Function. Inf. Sci. 2019, 494, 311–327. [Google Scholar] [CrossRef]\n\nGammerman, A.; Vovk, V.; Boström, H.; Carlsson, L. Conformal and Probabilistic Prediction with Applications: Editorial. Mach. Learn. 2019, 108, 379–380. [Google Scholar] [CrossRef] [Green Version]\n\nUdapola, H.; Liyanage, S.R. Braille Messenger: Adaptive Learning Based Non-Visual Touch Screen Input for the Blind Community Using Braille. In Proceedings of the International Conference on Innovations in Info-Business and Technology, Colombo, Sri Lanka, 9–10 September 2017; pp. 1–11. [Google Scholar]\n\nZhong, M.; Zhou, Y.; Chen, G. Sequential Model Based Intrusion Detection System for Iot Servers Using Deep Learning Methods. Sensors 2021, 21, 1113. [Google Scholar] [CrossRef]\n\nTaha, H.M. Robust Braille Recognition System Using Image Preprocessing and Feature Extraction Algorithms. Ph.D. Thesis, Universiti Tun Hussien Onn Malaysia, Parit Raja, Malaysia, 2014. [Google Scholar]\n\nWaleed, M. Braille Identification System Using Artificial Neural Networks. Tikrit J. Pure Sci. 2017, 22, 140–145. [Google Scholar]\n\nKawabe, H.; Shimomura, Y.; Nambo, H.; Seto, S. Application of Deep Learning to Classification of Braille Dot for Restoration of Old Braille Books. In Proceedings of the International Conference on Management Science and Engineering Management; Xu, J., Cooke, F.L., Gen, M., Ahmed, S.E., Eds.; Lecture Notes on Multidisciplinary Industrial Engineering; Springer International Publishing: Cham, Switzerland, 2019; pp. 913–926. ISBN 978-3-319-93350-4. [Google Scholar]\n\nZaman, S.; Abrar, M.A.; Hassan, M.M.; Islam, A.N.M.N. A Recurrent Neural Network Approach to Image Captioning in Braille for Blind-Deaf People. In Proceedings of the 2019 IEEE International Conference on Signal Processing, Information, Communication & Systems (SPICSCON), Dhaka, Bangladesh, 28–30 November 2019; pp. 49–53. [Google Scholar]\n\nValsan, K.S.; Stella, S.J. Smart Braille Recognition System. Int. J. Res. Appl. Sci. Eng. Technol. 2017, 5, 2452–2460. [Google Scholar] [CrossRef]\n\nFigure 1. Schematic Diagram.\n\nFigure 2. Visually Impaired student entering Braille patterns using a touchscreen device.\n\nFigure 3. (a) Category-1 (a–m) using DT and (b) category-2 (n–z) using DT.\n\nFigure 4. (a) Category-1 (a–m) using KNN and (b) category-2 (n–z) using KNN.\n\nFigure 5. (a)Category-1 (a–m) using SVM and (b) category-2 (n–z) using SVM.\n\nTable 1. SVM kernel descriptions.\n\nKernel TypeClassification MethodMathematical DescriptionLinear KernelLinear SVM K x i , y i = x i , y i Polynomial KernelQuadratic SVM K x i , y i = 1 + x i , y i 2 Cubic SVM K x i , y i = 1 + x i , y i 3 Gaussian Radial Base FunctionFine Gaussian SVM K x i , y i = exp | | x i − y i 2 | | 2 σ 2 , σ = 0.75 Medium Gaussian SVM K x i , y i exp | | x i − y i 2 | | 2 σ 2 , σ = 3 Course Gaussian SVM K x i , y i = exp | | x i − y i 2 | | 2 σ 2 , σ = 12\n\nTable 2. Performance metrics showing the results obtained for the Decision Tree classifier.\n\nSerial NumberEnglish CharactersTPR (%)TNR (%)PPV (%)NPV (%)FPR (%)Total\n\nAccuracy (%)AUCF1-Score1a1001001001000.001001.001.002b10099.8697.141000.1499.870.990.993c1001001001000.001001.001.004d1001001001000.001001.001.005e83.8799.8696.3099.310.1499.200.910.906f96.1599.7292.5999.860.2899.600.970.947g10099.4588.891000.5599.470.990.948h1001001001000.001001.001.009i1001001001000.001001.001.0010j1001001001000.001001.001.0011k1001001001000.001001.001.0012l90.4899.7390.4899.730.2799.470.950.9013m95.8399.4585.1999.860.5599.340.970.9014n96.7799.5890.9199.860.4299.470.980.9415o84.2110010099.590.0099.600.920.9116p1001001001000.001001.001.0017q10099.8696.301000.1499.870.990.9818r96.1599.8696.1599.860.1499.730.980.9619s94.7410010099.860.0099.870.970.9720t10099.8696.551000.1499.870.990.9821u1001001001000.001001.001.0022v10099.8697.141000.1499.870.990.9923w1001001001000.001001.001.0024x87.8810010099.450.0099.470.930.9425y96.1510010099.860.0099.870.980.9826z97.4410010099.860.0099.870.980.9926z97.4410010099.860.0099.870.980.99\n\nTable 3. Performance metrics showing the results obtained for the KNN classifier.\n\nSerial\n\nNumberEnglish\n\nCharactersTPR\n\n(%)TNR\n\n(%)PPV\n\n(%)NPV\n\n(%)FPR\n\n(%)Total\n\nAccuracy (%)AUCF1-Score1a1001001001000.001001.001.002b10099.5891.891000.4299.600.990.963c10099.3280.001000.6899.340.990.894d10099.8696.671000.1499.870.990.985e55.8199.8696.0097.390.1497.340.770.716f96.1598.3567.5799.861.6598.270.970.797g10097.3662.751002.6497.480.980.778h96.4399.3184.3899.860.6999.200.980.909i1001001001000.001001.001.0010j1001001001000.001001.001.0011k1001001001000.001001.001.0012l94.7410010099.860.0099.870.970.9713m10099.8792.311000.1399.870.990.9614n96.3099.1781.2599.860.8399.070.970.8815o85.7110010099.590.0099.600.930.9216p1001001001000.001001.001.0017q10099.8696.431000.1499.870.990.9818r96.1599.7292.5999.860.2899.600.980.9419s88.8910010099.730.0099.730.940.9420t89.6610010099.590.0099.600.950.9521u1001001001000.001001.001.0022v96.9710010099.860.0099.870.980.9823w96.9710010099.860.0099.870.980.9824x10099.8694.441000.1499.870.990.9725y1001001001000.001001.001.0026z84.6210010099.170.0099.200.920.92\n\nTable 4. Performance metrics showing the results obtained for the SVM classifier.\n\nSerial NumberEnglish CharactersTPR\n\n(%)TNR\n\n(%)PPV\n\n(%)NPV\n\n(%)FPR\n\n(%)Total\n\nAccuracy (%)AUCF1-Score1a1001001001000.001001.001.002b10099.8697.301000.1499.870.990.993c1001001001000.001001.001.004d1001001001000.001001.001.005e92.8610010099.720.0099.730.960.966f10099.7392.001000.2799.730.990.967g10099.1685.371000.8499.200.990.928h1001001001000.001001.001.009i1001001001000.001001.001.0010j1001001001000.001001.001.0011k1001001001000.001001.001.0012l10099.8695.001000.1499.870.990.9713m92.5999.5989.2999.720.4199.340.960.9114n1001001001000.001001.001.0015o94.7499.8694.7499.860.1499.730.970.9516p1001001001000.001001.001.0017q10099.8696.431000.1499.870.990.9818r10099.8696.301000.1499.870.990.9819s88.8910010099.730.0099.730.940.9420t1001001001000.001001.001.0021u1001001001000.001001.001.0022v1001001001000.001001.001.0023w1001001001000.001001.001.0024x84.8510010099.310.0099.340.920.9225y1001001001000.001001.001.0026z1001001001000.001001.001.00\n\nTable 5. Overall results achieved using Decision Trees, SVM and KNN with the RICA and PCA feature extraction methods.\n\nClassifierFeature Extraction MethodPrecision (%)Recall (%)F1-ScoreAccuracy (%)DTRICA97.2296.910.97099.79KNN93.7095.320.93999.50SVM97.9498.230.98099.86RF90.1290.340.90490.02DTPCA72.0168.040.7170.02KNN79.5675.450.7675.40SVM88.1286.320.8686.32RF80.079.00.7980.0\n\nTable 6. p-values for DT, SVM, KNN and RF using the RICA- and PCA-based feature extraction methods.\n\nClassifierFeature Extraction Methodp-ValueDT vs. KNNRICA0.001SVM vs. DT0.000DT vs. RF0.021KNN vs. DTPCA0.024DT vs. SVM0.031RF vs. DT0.03\n\nPublisher’s Note: MDPI stays neutral with regard to jurisdictional claims in published maps and institutional affiliations.\n\n© 2022 by the authors. Licensee MDPI, Basel, Switzerland. This article is an open access article distributed under the terms and conditions of the Creative Commons Attribution (CC BY) license (https://creativecommons.org/licenses/by/4.0/).\n\nShare and Cite\n\nMDPI and ACS Style\n\nShokat, S.; Riaz, R.; Rizvi, S.S.; Khan, I.; Paul, A. Characterization of English Braille Patterns Using Automated Tools and RICA Based Feature Extraction Methods. Sensors 2022, 22, 1836. https://doi.org/10.3390/s22051836\n\nAMA Style\n\nShokat S, Riaz R, Rizvi SS, Khan I, Paul A. Characterization of English Braille Patterns Using Automated Tools and RICA Based Feature Extraction Methods. Sensors. 2022; 22(5):1836. https://doi.org/10.3390/s22051836\n\nChicago/Turabian Style\n\nShokat, Sana, Rabia Riaz, Sanam Shahla Rizvi, Inayat Khan, and Anand Paul. 2022. \"Characterization of English Braille Patterns Using Automated Tools and RICA Based Feature Extraction Methods\" Sensors 22, no. 5: 1836. https://doi.org/10.3390/s22051836\n\nNote that from the first issue of 2016, this journal uses article numbers instead of page numbers. See further details here.\n\nArticle Metrics\n\nNo\n\nNo\n\nArticle Access Statistics\n\nFor more information on the journal statistics, click here.\n\nMultiple requests from the same IP address are counted as one view."
    }
}
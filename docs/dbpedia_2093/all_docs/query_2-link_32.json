{
    "id": "dbpedia_2093_2",
    "rank": 32,
    "data": {
        "url": "https://blog.hum.works/posts/multi-armed-bandits",
        "read_more_link": "",
        "language": "en",
        "title": "Forget A/B Testing - Use Multi-Armed Bandits For Smarter Campaign Optimization",
        "top_image": "https://content.hum.works/humworks/assets/ifw642rely8googk?key=gallery_1",
        "meta_img": "https://content.hum.works/humworks/assets/ifw642rely8googk?key=gallery_1",
        "images": [
            "https://content.hum.works/humworks/assets/ifw642rely8googk?key=mid_3",
            "https://content.hum.works/uploads/humworks/originals/e4704ad7-bc22-44ed-b9e2-94a377b84891.jpg",
            "https://content.hum.works/uploads/humworks/originals/42ae03c8-ad17-4f55-b2d9-efbf85115e9c.jpg",
            "https://content.hum.works/humworks/assets/ooto0xkswtcgk4o0?key=mid_2",
            "https://content.hum.works/humworks/assets/db20aqmw0w004sgc?key=mid_2",
            "https://content.hum.works/humworks/assets/oi8byafys004w0ck?key=mid_2",
            "https://content.hum.works/humworks/assets/k47y4576ehwwwcoo?key=mid_2",
            "https://content.hum.works/humworks/assets/o2xxy1opc2s4wos8?key=mid_2",
            "https://content.hum.works/humworks/assets/h5rmypt9tzc4cc80?key=mid_2"
        ],
        "movies": [],
        "keywords": [],
        "meta_keywords": [
            ""
        ],
        "tags": null,
        "authors": [
            "Niall Little"
        ],
        "publish_date": "2024-03-06T14:59:42",
        "summary": "",
        "meta_description": "Automatically send more visitors to the top-performing experience sooner to boost conversions and revenue. Learn how multi-armed bandit algorithms are taking digital publishers further when it comes to optimizing audience engagement.",
        "meta_lang": "en",
        "meta_favicon": "/static/images/favicon.ico",
        "meta_site_name": "",
        "canonical_link": "https://blog.hum.works/posts/multi-armed-bandits",
        "text": "For publishers and media companies struggling to cut through the digital noise, optimizing the performance of digital campaigns is one of the most essential – and most challenging! – components to reaching audiences, maximizing engagement, and driving results.\n\nTraditional A/B testing requires large sample sizes to deliver statistically significant results - a luxury many publishers simply don't have when they’re delivering campaigns to highly specialized audiences. So while you’re collecting data about which message resonates more with your audience, you’re also feeding traffic to less effective options until a single winner is declared.\n\nWhen every impression matters, you can’t afford to waste time or opportunities sending a significant percentage of your audience a sub-optimal experience.\n\nEnter: The multi-armed bandit approach (MAB). A smarter, more efficient way to maximize campaign performance.\n\nWhat is a Multi-Armed Bandit Algorithm?\n\nUnlike traditional A/B testing, MAB uses machine learning algorithms to dynamically shift traffic towards better-performing variants. Where A/B testing is about determining a winner, MAB is about getting you as many conversions as possible for your campaign.\n\nImagine you have 50 tokens to use in an arcade, and you want to win as many tickets as possible.\n\nIf you took an A/B testing approach, you’d need to play each of the three games in the arcade ten times to determine which game you are the best at. Only after you’d played each game ten times would you spend your 20 remaining tokens on the game that netted you the most tickets.\n\nWith MAB, you throw out the notion that you need to spend 30 tickets upfront to find a winner. Instead, you play at random for a bit. You might play a game or two of Zombie Snatcher, then move on to Skee-Ball. But if you notice after 3 or 4 games that Zombie Snatcher seems rigged, while you consistently win several tickets on Skee-Ball, you stop spending your effort and your tickets on Zombie Snatcher.\n\nYou might come back to it later, just to make sure you weren’t unlucky at the start. And you’d keep an eye on your wins at Skee-Ball to make sure things don’t fall off after a stroke of early luck. Overall though, in just a few games you’ve found that your tokens are better spent on Skee-Ball than on Zombie Snatcher.\n\nYou didn’t waste 30 tokens up front figuring that out. You noticed quickly and adjusted.\n\nAdapting to Evolving Trends\n\nMAB dynamically shifts more impressions to the better-performing campaign as users interact with your campaign in real-time, meaning more of your readers will see the best campaign.\n\nHow does MAB choose a top performing variant? That depends on overall conversion rate, the difference in conversion rates across variants, and the number of variants being tested.\n\nWhen you launch a campaign with multiple variants, impressions will be served to variant A and variant B (and C, D, or E!) in an even split.\n\nThe algorithm updates the odds ratios with every single impression and conversion, reacting as more and more users interact with it. As the number of impressions increases, the impact that a single impression can have on the balance between variants decreases and the campaign begins to push more people towards the better-converting campaign.\n\nMulti-Armed Bandits in Hum\n\nHum now includes the ability to create up to 10 variants for Live Engagement Campaigns.\n\nMarketers are able to test multiple elements simultaneously – including prompt positioning, headline and body copy, images, and calls to action – to find the most effective combination.\n\nHum uses real-time user behavior to optimize delivery of the variants within each campaign, meaning your campaigns will:\n\nAutomatically funnel more traffic towards the more effective experience, maximizing the number of audience members who will see the top-performing campaign.\n\nReact to changes in audience dynamics if variant preference changes over time.\n\nAllow marketers to use variants on campaigns with lower traffic than might be needed for traditional A/B testing.\n\nAs with all Live Engagement Campaigns in Hum, you can set multi-armed bandit campaigns to show to specific audience segments and/or on specific pages. And Hum’s reporting shows you variant performance over time, so marketers can do detailed analyses of campaign performance and develop best-practices for other, future campaigns.\n\n(Hum still can’t improve your odds of winning Zombie Snatcher, but we’re working on it.)"
    }
}
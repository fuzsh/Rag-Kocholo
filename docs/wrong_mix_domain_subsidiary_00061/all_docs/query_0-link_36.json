{
    "id": "wrong_mix_domain_subsidiary_00061_0",
    "rank": 36,
    "data": {
        "url": "https://docs.gitlab.com/ee/development/migration_style_guide.html",
        "read_more_link": "",
        "language": "en",
        "title": "Migration Style Guide",
        "top_image": "https://docs.gitlab.com/favicon.ico?v=2",
        "meta_img": "https://docs.gitlab.com/favicon.ico?v=2",
        "images": [
            "https://docs.gitlab.com/assets/images/gitlab-logo-header.svg",
            "https://docs.gitlab.com/assets/images/gitlab-logo.svg",
            "https://docs.gitlab.com/assets/images/by-sa.svg",
            "https://dc.ads.linkedin.com/collect/?pid=30694&fmt=gif"
        ],
        "movies": [],
        "keywords": [],
        "meta_keywords": [
            ""
        ],
        "tags": null,
        "authors": [],
        "publish_date": null,
        "summary": "",
        "meta_description": "GitLab product documentation.",
        "meta_lang": "en",
        "meta_favicon": "/favicon.ico?v=2",
        "meta_site_name": "",
        "canonical_link": "https://docs.gitlab.com/ee/development/migration_style_guide.html",
        "text": "Choose an appropriate migration type\n\nHow long a migration should take\n\nDecide which database to target\n\nCreate a regular schema migration\n\nRegular schema migrations to add new models\n\nSchema Changes\n\nAvoiding downtime\n\nReversibility\n\nAtomicity and transaction\n\nHeavy operations in a single transaction\n\nTemporarily turn off the statement timeout limit\n\nDisable transaction-wrapped migration\n\nNaming conventions\n\nTruncate long index names\n\nBest practice\n\nMigration helpers and versioning\n\nRetry mechanism when acquiring database locks\n\nTransactional migrations\n\nMultiple changes on the same table\n\nChanging default value for a column\n\nCreating a new table when we have two foreign keys\n\nUsage with non-transactional migrations\n\nWhen to use the helper method\n\nHow the helper method works\n\nLock-retry methodology at the SQL level\n\nRemoving indexes\n\nDisabling an index\n\nAdding indexes\n\nTesting for existence of indexes\n\nAdding foreign-key constraints\n\nNOT NULL constraints\n\nAdding Columns With Default Values\n\nRemoving the column default for non-nullable columns\n\nChanging the column default\n\nUpdating an existing column\n\nRemoving a foreign key constraint\n\nDropping a database table\n\nDropping a sequence\n\nTruncate a table\n\nSwapping primary key\n\nInteger column type\n\nStrings and the Text data type\n\nStoring JSON in database\n\nEncrypted attributes\n\nTesting\n\nData migration\n\nModifying existing data\n\nUsing application code in migrations (discouraged)\n\nExample: Add a column my_column to the users table\n\nHigh traffic tables\n\nMilestone\n\nAutovacuum wraparound protection\n\nMigration Style Guide\n\nWhen writing migrations for GitLab, you have to take into account that these are run by hundreds of thousands of organizations of all sizes, some with many years of data in their database.\n\nIn addition, having to take a server offline for an upgrade small or big is a big burden for most organizations. For this reason, it is important that your migrations are written carefully, can be applied online, and adhere to the style guide below.\n\nMigrations are not allowed to require GitLab installations to be taken offline ever. Migrations always must be written in such a way to avoid downtime. In the past we had a process for defining migrations that allowed for downtime by setting a DOWNTIME constant. You may see this when looking at older migrations. This process was in place for 4 years without ever being used and as such we’ve learned we can always figure out how to write a migration differently to avoid downtime.\n\nWhen writing your migrations, also consider that databases might have stale data or inconsistencies and guard for that. Try to make as few assumptions as possible about the state of the database.\n\nDon’t depend on GitLab-specific code since it can change in future versions. If needed copy-paste GitLab code into the migration to make it forward compatible.\n\nChoose an appropriate migration type\n\nThe first step before adding a new migration should be to decide which type is most appropriate.\n\nThere are currently three kinds of migrations you can create, depending on the kind of work it needs to perform and how long it takes to complete:\n\nRegular schema migrations. These are traditional Rails migrations in db/migrate that run before new application code is deployed (for GitLab.com before Canary is deployed). This means that they should be relatively fast, no more than a few minutes, so as not to unnecessarily delay a deployment.\n\nOne exception is a migration that takes longer but is absolutely critical for the application to operate correctly. For example, you might have indices that enforce unique tuples, or that are needed for query performance in critical parts of the application. In cases where the migration would be unacceptably slow, however, a better option might be to guard the feature with a feature flag and perform a post-deployment migration instead. The feature can then be turned on after the migration finishes.\n\nMigrations used to add new models are also part of these regular schema migrations. The only differences are the Rails command used to generate the migrations and the additional generated files, one for the model and one for the model’s spec.\n\nPost-deployment migrations. These are Rails migrations in db/post_migrate and are run independently from the GitLab.com deployments. Pending post migrations are executed on a daily basis at the discretion of release manager through the post-deploy migration pipeline. These migrations can be used for schema changes that aren’t critical for the application to operate, or data migrations that take at most a few minutes. Common examples for schema changes that should run post-deploy include:\n\nClean-ups, like removing unused columns.\n\nAdding non-critical indices on high-traffic tables.\n\nAdding non-critical indices that take a long time to create.\n\nThese migrations should not be used for schema changes that are critical for the application to operate. Making such schema changes in a post-deployment migration have caused issues in the past, for example this issue. Changes that should always be a regular schema migration and not be executed in a post-deployment migration include:\n\nCreating a new table, example: create_table.\n\nAdding a new column to an existing table, example: add_column.\n\nnote\n\nPost-deployment migration is often abbreviated as PDM.\n\nBatched background migrations. These aren’t regular Rails migrations, but application code that is executed via Sidekiq jobs, although a post-deployment migration is used to schedule them. Use them only for data migrations that exceed the timing guidelines for post-deploy migrations. Batched background migrations should not change the schema.\n\nUse the following diagram to guide your decision, but keep in mind that it is just a tool, and the final outcome will always be dependent on the specific changes being made:\n\ngraph LR A{Schema<br/>changed?} A -->|Yes| C{Critical to<br/>speed or<br/>behavior?} A -->|No| D{Is it fast?} C -->|Yes| H{Is it fast?} C -->|No| F[Post-deploy migration] H -->|Yes| E[Regular migration] H -->|No| I[Post-deploy migration<br/>+ feature flag] D -->|Yes| F[Post-deploy migration] D -->|No| G[Background migration]\n\nAlso refer to Migration type to use when choosing which migration type to use when adding a database index.\n\nHow long a migration should take\n\nIn general, all migrations for a single deploy shouldn’t take longer than 1 hour for GitLab.com. The following guidelines are not hard rules, they were estimated to keep migration duration to a minimum.\n\nMigration Type Recommended Duration Notes Regular migrations <= 3 minutes A valid exception are changes without which application functionality or performance would be severely degraded and which cannot be delayed. Post-deployment migrations <= 10 minutes A valid exception are schema changes, since they must not happen in background migrations. Background migrations > 10 minutes Since these are suitable for larger tables, it’s not possible to set a precise timing guideline, however, any single query must stay below 1 second execution time with cold caches.\n\nDecide which database to target\n\nGitLab connects to two different Postgres databases: main and ci. This split can affect migrations as they may run on either or both of these databases.\n\nRead Migrations for Multiple databases to understand if or how a migration you add should account for this.\n\nCreate a regular schema migration\n\nTo create a migration you can use the following Rails generator:\n\nThis generates the migration file in db/migrate.\n\nRegular schema migrations to add new models\n\nTo create a new model you can use the following Rails generator:\n\nThis will generate:\n\nthe migration file in db/migrate\n\nthe model file in app/models\n\nthe spec file in spec/models\n\nSchema Changes\n\nChanges to the schema should be committed to db/structure.sql. This file is automatically generated by Rails when you run bundle exec rails db:migrate, so you typically should not edit this file by hand. If your migration is adding a column to a table, that column is added at the bottom. Do not reorder columns manually for existing tables as this causes confusion to other people using db/structure.sql generated by Rails.\n\nWhen your local database in your GDK is diverging from the schema from main it might be hard to cleanly commit the schema changes to Git. In that case you can use the scripts/regenerate-schema script to regenerate a clean db/structure.sql for the migrations you’re adding. This script applies all migrations found in db/migrate or db/post_migrate, so if there are any migrations you don’t want to commit to the schema, rename or remove them. If your branch is not targeting the default Git branch, you can set the TARGET environment variable.\n\nThe scripts/regenerate-schema script can create additional differences. If this happens, use a manual procedure where <migration ID> is the DATETIME part of the migration file.\n\nAfter a table has been created, it should be added to the database dictionary, following the steps mentioned in the database dictionary guide.\n\nAvoiding downtime\n\nThe document “Avoiding downtime in migrations” specifies various database operations, such as:\n\ndropping and renaming columns\n\nchanging column constraints and types\n\nadding and dropping indexes, tables, and foreign keys\n\nmigrating integer primary keys to bigint\n\nand explains how to perform them without requiring downtime.\n\nReversibility\n\nYour migration must be reversible. This is very important, as it should be possible to downgrade in case of a vulnerability or bugs.\n\nNote: On GitLab production environments, if a problem occurs, a roll-forward strategy is used instead of rolling back migrations using db:rollback. On self-managed instances we advise users to restore the backup which was created before the upgrade process started. The down method is used primarily in the development environment, for example, when a developer wants to ensure their local copy of structure.sql file and database are in a consistent state when switching between commits or branches.\n\nIn your migration, add a comment describing how the reversibility of the migration was tested.\n\nSome migrations cannot be reversed. For example, some data migrations can’t be reversed because we lose information about the state of the database before the migration. You should still create a down method with a comment, explaining why the changes performed by the up method can’t be reversed, so that the migration itself can be reversed, even if the changes performed during the migration can’t be reversed:\n\nMigrations like this are inherently risky and additional actions are required when preparing the migration for review.\n\nAtomicity and transaction\n\nBy default, migrations are a single transaction: it’s opened at the beginning of the migration, and committed after all steps are processed.\n\nRunning migrations in a single transaction makes sure that if one of the steps fails, none of the steps are executed, leaving the database in a valid state. Therefore, either:\n\nPut all migrations in one single-transaction migration.\n\nIf necessary, put most actions in one migration and create a separate migration for the steps that cannot be done in a single transaction.\n\nFor example, if you create an empty table and need to build an index for it, you should use a regular single-transaction migration and the default rails schema statement: add_index. This operation is a blocking operation, but it doesn’t cause problems because the table is not yet used, and therefore it does not have any records yet.\n\nHeavy operations in a single transaction\n\nWhen using a single-transaction migration, a transaction holds a database connection for the duration of the migration, so you must make sure the actions in the migration do not take too much time. In general, transactions must execute quickly. To that end, observe the maximum query time limit for each query run in the migration.\n\nIf your single-transaction migration takes long to finish, you have several options. In all cases, remember to select the appropriate migration type depending on how long a migration takes\n\nSplit the migration into multiple single-transaction migrations.\n\nUse multiple transactions by using disable_ddl_transaction!.\n\nKeep using a single-transaction migration after adjusting statement and lock timeout settings. If your heavy workload must use the guarantees of a transaction, you should check your migration can execute without hitting the timeout limits. The same advice applies to both single-transaction migrations and individual transactions.\n\nStatement timeout: the statement timeout is configured to be 15s for GitLab.com’s production database but creating an index often takes more than 15 seconds. When you use the existing helpers including add_concurrent_index, they automatically turn off the statement timeout as needed. In rare cases, you might need to set the timeout limit yourself by using disable_statement_timeout.\n\nTemporarily turn off the statement timeout limit\n\nThe migration helper disable_statement_timeout enables you to temporarily set the statement timeout to 0 per transaction or per connection.\n\nYou use the per-connection option when your statement does not support running inside an explicit transaction, like CREATE INDEX CONCURRENTLY.\n\nIf your statement does support an explicit transaction block, like ALTER TABLE ... VALIDATE CONSTRAINT, the per-transaction option should be used.\n\nUsing disable_statement_timeout is rarely needed, because the most migration helpers already use them internally when needed. For example, creating an index usually takes more than 15 seconds, which is the default statement timeout configured for GitLab.com’s production database. The helper add_concurrent_index creates an index inside the block passed to disable_statement_timeout to disable the statement timeout per connection.\n\nIf you are writing raw SQL statements in a migration, you may need to manually use disable_statement_timeout. Consult the database reviewers and maintainers when you do.\n\nDisable transaction-wrapped migration\n\nYou can opt out of running your migration as a single transaction by using disable_ddl_transaction!, an ActiveRecord method. The method might be called in other database systems, with different results. At GitLab we exclusively use PostgreSQL. You should always read disable_ddl_transaction! as meaning:\n\n“Do not execute this migration in a single PostgreSQL transaction. I’ll open PostgreSQL transaction(s) only when and if I need them.”\n\nWhen should you use disable_ddl_transaction!? In most cases, the existing RuboCop rules or migration helpers can detect if you should be using disable_ddl_transaction!. Skip disable_ddl_transaction! if you are unsure whether to use it or not in your migration, and let the RuboCop rules and database reviews guide you.\n\nUse disable_ddl_transaction! when PostgreSQL requires an operation to be executed outside an explicit transaction.\n\nThe most prominent example of such operation is the command CREATE INDEX CONCURRENTLY. PostgreSQL allows the blocking version (CREATE INDEX) to be run inside a transaction. Unlike CREATE INDEX, CREATE INDEX CONCURRENTLY must be performed outside a transaction. Therefore, even though a migration may run just one statement CREATE INDEX CONCURRENTLY, you should disable disable_ddl_transaction!. It’s also the reason why the use of the helper add_concurrent_index requires disable_ddl_transaction! CREATE INDEX CONCURRENTLY is more of the exception than the rule.\n\nUse disable_ddl_transaction! when you need to run multiple transactions in a migration for any reason. Most of the time you would be using multiple transactions to avoid running one slow transaction.\n\nFor example, when you insert, update, or delete (DML) a large amount of data, you should perform them in batches. Should you need to group operations for each batch, you can explicitly open a transaction block when processing a batch. Consider using a batched background migration for any reasonably large workload.\n\nUse disable_ddl_transaction! when migration helpers require them. Various migration helpers need to run with disable_ddl_transaction! because they require a precise control on when and how to open transactions.\n\nA foreign key can be added inside a transaction, unlike CREATE INDEX CONCURRENTLY. However, PostgreSQL does not provide an option similar to CREATE INDEX CONCURRENTLY. The helper add_concurrent_foreign_key instead opens its own transactions to lock the source and target table in a manner that minimizes locking while adding and validating the foreign key.\n\nAs advised earlier, skip disable_ddl_transaction! if you are unsure and see if any RuboCop check is violated.\n\nUse disable_ddl_transaction! when your migration does not actually touch PostgreSQL databases or does touch multiple PostgreSQL databases.\n\nFor example, your migration might target a Redis server. As a rule, you cannot interact with an external service inside a PostgreSQL transaction.\n\nA transaction is used for a single database connection. If your migrations are targeting multiple databases, such as both ci and main database, follow Migrations for multiple databases.\n\nNaming conventions\n\nNames for database objects (such as tables, indexes, and views) must be lowercase. Lowercase names ensure that queries with unquoted names don’t cause errors.\n\nWe keep column names consistent with ActiveRecord’s schema conventions.\n\nCustom index and constraint names should follow the constraint naming convention guidelines.\n\nTruncate long index names\n\nPostgreSQL limits the length of identifiers, like column or index names. Column names are not usually a problem, but index names tend to be longer. Some methods for shortening a name that’s too long:\n\nPrefix it with i_ instead of index_.\n\nSkip redundant prefixes. For example, index_vulnerability_findings_remediations_on_vulnerability_remediation_id becomes index_vulnerability_findings_remediations_on_remediation_id.\n\nInstead of columns, specify the purpose of the index, such as index_users_for_unconfirmation_notification.\n\nThe timestamp portion of a migration filename determines the order in which migrations are run. It’s important to maintain a rough correlation between:\n\nWhen a migration is added to the GitLab codebase.\n\nThe timestamp of the migration itself.\n\nA new migration’s timestamp should never be before the previous hard stop. Migrations are occasionally squashed, and if a migration is added whose timestamp falls before the previous hard stop, a problem like what happened in issue 408304 can occur.\n\nFor example, if we are currently developing against GitLab 16.0, the previous hard stop is 15.11. 15.11 was released on April 23rd, 2023. Therefore, the minimum acceptable timestamp would be 20230424000000.\n\nBest practice\n\nWhile the above should be considered a hard rule, it is a best practice to try to keep migration timestamps to within three weeks of the date it is anticipated that the migration will be merged upstream, regardless of how much time has elapsed since the last hard stop.\n\nTo update a migration timestamp:\n\nMigrate down the migration for the ci and main databases:\n\nrake db:migrate:down:main VERSION=<timestamp> rake db:migrate:down:ci VERSION=<timestamp>\n\nDelete the migration file.\n\nRecreate the migration following the migration style guide.\n\nMigration helpers and versioning\n\nVarious helper methods are available for many common patterns in database migrations. Those helpers can be found in Gitlab::Database::MigrationHelpers and related modules.\n\nIn order to allow changing a helper’s behavior over time, we implement a versioning scheme for migration helpers. This allows us to maintain the behavior of a helper for already existing migrations but change the behavior for any new migrations.\n\nFor that purpose, all database migrations should inherit from Gitlab::Database::Migration, which is a “versioned” class. For new migrations, the latest version should be used (which can be looked up in Gitlab::Database::Migration::MIGRATION_CLASSES) to use the latest version of migration helpers.\n\nIn this example, we use version 2.1 of the migration class:\n\nDo not include Gitlab::Database::MigrationHelpers directly into a migration. Instead, use the latest version of Gitlab::Database::Migration, which exposes the latest version of migration helpers automatically.\n\nRetry mechanism when acquiring database locks\n\nWhen changing the database schema, we use helper methods to invoke DDL (Data Definition Language) statements. In some cases, these DDL statements require a specific database lock.\n\nExample:\n\nExecuting this migration requires an exclusive lock on the users table. When the table is concurrently accessed and modified by other processes, acquiring the lock may take a while. The lock request is waiting in a queue and it may also block other queries on the users table once it has been enqueued.\n\nMore information about PostgreSQL locks: Explicit Locking\n\nFor stability reasons, GitLab.com has a short statement_timeout set. When the migration is invoked, any database query has a fixed time to execute. In a worst-case scenario, the request sits in the lock queue, blocking other queries for the duration of the configured statement timeout, then failing with canceling statement due to statement timeout error.\n\nThis problem could cause failed application upgrade processes and even application stability issues, since the table may be inaccessible for a short period of time.\n\nTo increase the reliability and stability of database migrations, the GitLab codebase offers a method to retry the operations with different lock_timeout settings and wait time between the attempts. Multiple shorter attempts to acquire the necessary lock allow the database to process other statements.\n\nLock retries are controlled by two different helpers:\n\nenable_lock_retries!: enabled by default for all transactional migrations.\n\nwith_lock_retries: enabled manually for a block within non-transactional migrations.\n\nTransactional migrations\n\nRegular migrations execute the full migration in a transaction. lock-retry mechanism is enabled by default (unless disable_ddl_transaction!).\n\nThis leads to the lock timeout being controlled for the migration. Also, it can lead to retrying the full migration if the lock could not be granted within the timeout.\n\nOccasionally a migration may need to acquire multiple locks on different objects. To prevent catalog bloat, ask for all those locks explicitly before performing any DDL. A better strategy is to split the migration, so that we only need to acquire one lock at the time.\n\nMultiple changes on the same table\n\nWith the lock-retry methodology enabled, all operations wrap into a single transaction. When you have the lock, you should do as much as possible inside the transaction rather than trying to get another lock later. Be careful about running long database statements within the block. The acquired locks are kept until the transaction (block) finishes and depending on the lock type, it might block other database operations.\n\nChanging default value for a column\n\nNote that changing column defaults can cause application downtime if a multi-release process is not followed. See avoiding downtime in migrations for changing column defaults for details.\n\nCreating a new table when we have two foreign keys\n\nOnly one foreign key should be created per transaction. This is because the addition of a foreign key constraint requires a SHARE ROW EXCLUSIVE lock on the referenced table, and locking multiple tables in the same transaction should be avoided.\n\nFor this, we need three migrations:\n\nCreating the table without foreign keys (with the indices).\n\nAdd foreign key to the first table.\n\nAdd foreign key to the second table.\n\nCreating the table:\n\nAdding foreign key to projects:\n\nWe can use the add_concurrent_foreign_key method in this case, as this helper method has the lock retries built into it.\n\nAdding foreign key to users:\n\nUsage with non-transactional migrations\n\nOnly when we disable transactional migrations using disable_ddl_transaction!, we can use the with_lock_retries helper to guard an individual sequence of steps. It opens a transaction to execute the given block.\n\nA custom RuboCop rule ensures that only allowed methods can be placed within the lock retries block.\n\nThe RuboCop rule generally allows standard Rails migration methods, listed below. This example causes a RuboCop offense:\n\nWhen to use the helper method\n\nYou can only use the with_lock_retries helper method when the execution is not already inside an open transaction (using PostgreSQL subtransactions is discouraged). It can be used with standard Rails migration helper methods. Calling more than one migration helper is not a problem if they’re executed on the same table.\n\nUsing the with_lock_retries helper method is advised when a database migration involves one of the high-traffic tables.\n\nExample changes:\n\nadd_foreign_key / remove_foreign_key\n\nadd_column / remove_column\n\nchange_column_default\n\ncreate_table / drop_table\n\nThe with_lock_retries method cannot be used within the change method, you must manually define the up and down methods to make the migration reversible.\n\nHow the helper method works\n\nIterate 50 times.\n\nFor each iteration, set a pre-configured lock_timeout.\n\nTry to execute the given block. (remove_column).\n\nIf LockWaitTimeout error is raised, sleep for the pre-configured sleep_time and retry the block.\n\nIf no error is raised, the current iteration has successfully executed the block.\n\nFor more information check the Gitlab::Database::WithLockRetries class. The with_lock_retries helper method is implemented in the Gitlab::Database::MigrationHelpers module.\n\nIn a worst-case scenario, the method:\n\nExecutes the block for a maximum of 50 times over 40 minutes.\n\nMost of the time is spent in a pre-configured sleep period after each iteration.\n\nAfter the 50th retry, the block is executed without lock_timeout, just like a standard migration invocation.\n\nIf a lock cannot be acquired, the migration fails with statement timeout error.\n\nThe migration might fail if there is a very long running transaction (40+ minutes) accessing the users table.\n\nLock-retry methodology at the SQL level\n\nIn this section, we provide a simplified SQL example that demonstrates the use of lock_timeout. You can follow along by running the given snippets in multiple psql sessions.\n\nWhen altering a table to add a column, AccessExclusiveLock, which conflicts with most lock types, is required on the table. If the target table is a very busy one, the transaction adding the column may fail to acquire AccessExclusiveLock in a timely fashion.\n\nSuppose a transaction is attempting to insert a row into a table:\n\nAt this point Transaction 1 acquired RowExclusiveLock on my_notes. Transaction 1 could still execute more statements prior to committing or aborting. There could be other similar, concurrent transactions that touch my_notes.\n\nSuppose a transactional migration is attempting to add a column to the table without using any lock retry helper:\n\nTransaction 2 is now blocked because it cannot acquire AccessExclusiveLock on my_notes table as Transaction 1 is still executing and holding the RowExclusiveLock on my_notes.\n\nA more pernicious effect is blocking the transactions that would normally not conflict with Transaction 1 because Transaction 2 is queueing to acquire AccessExclusiveLock. In a normal situation, if another transaction attempted to read from and write to the same table my_notes at the same time as Transaction 1, the transaction would go through since the locks needed for reading and writing would not conflict with RowExclusiveLock held by Transaction 1. However, when the request to acquire AccessExclusiveLock is queued, the subsequent requests for conflicting locks on the table would block although they could be executed concurrently alongside Transaction 1.\n\nIf we used with_lock_retries, Transaction 2 would instead quickly timeout after failing to acquire the lock within the specified time period and allow other transactions to proceed:\n\nThe lock retry helper would repeatedly try the same transaction at different time intervals until it succeeded.\n\nNote that SET LOCAL scopes the parameter (lock_timeout) change to the transaction.\n\nRemoving indexes\n\nIf the table is not empty when removing an index, make sure to use the method remove_concurrent_index instead of the regular remove_index method. The remove_concurrent_index method drops indexes concurrently, so no locking is required, and there is no need for downtime. To use this method, you must disable single-transaction mode by calling the method disable_ddl_transaction! in the body of your migration class like so:\n\nYou can verify that the index is not being used with Thanos:\n\nNote that it is not necessary to check if the index exists prior to removing it, however it is required to specify the name of the index that is being removed. This can be done either by passing the name as an option to the appropriate form of remove_index or remove_concurrent_index, or by using the remove_concurrent_index_by_name method. Explicitly specifying the name is important to ensure the correct index is removed.\n\nFor a small table (such as an empty one or one with less than 1,000 records), it is recommended to use remove_index in a single-transaction migration, combining it with other operations that don’t require disable_ddl_transaction!.\n\nDisabling an index\n\nDisabling an index is not a safe operation.\n\nAdding indexes\n\nBefore adding an index, consider if one is necessary. The Adding Database indexes guide contains more details to help you decide if an index is necessary and provides best practices for adding indexes.\n\nTesting for existence of indexes\n\nIf a migration requires conditional logic based on the absence or presence of an index, you must test for existence of that index using its name. This helps avoids problems with how Rails compares index definitions, which can lead to unexpected results.\n\nFor more details, review the Adding Database Indexes guide.\n\nAdding foreign-key constraints\n\nWhen adding a foreign-key constraint to either an existing or a new column also remember to add an index on the column.\n\nThis is required for all foreign-keys, for example, to support efficient cascading deleting: when a lot of rows in a table get deleted, the referenced records need to be deleted too. The database has to look for corresponding records in the referenced table. Without an index, this results in a sequential scan on the table, which can take a long time.\n\nHere’s an example where we add a new column with a foreign key constraint. Note it includes index: true to create an index for it.\n\nWhen adding a foreign-key constraint to an existing column in a non-empty table, we have to employ add_concurrent_foreign_key and add_concurrent_index instead of add_reference.\n\nIf you have a new or empty table that doesn’t reference a high-traffic table, we recommend that you use add_reference in a single-transaction migration. You can combine it with other operations that don’t require disable_ddl_transaction!.\n\nYou can read more about adding foreign key constraints to an existing column.\n\nNOT NULL constraints\n\nSee the style guide on NOT NULL constraints for more information.\n\nAdding Columns With Default Values\n\nWith PostgreSQL 11 being the minimum version in GitLab, adding columns with default values has become much easier and the standard add_column helper should be used in all cases.\n\nBefore PostgreSQL 11, adding a column with a default was problematic as it would have caused a full table rewrite.\n\nRemoving the column default for non-nullable columns\n\nIf you have added a non-nullable column, and used the default value to populate existing data, you need to keep that default value around until at least after the application code is updated. You cannot remove the default value in the same migration, as the migrations run before the model code is updated and models will have an old schema cache, meaning they won’t know about this column and won’t be able to set it. In this case it’s recommended to:\n\nAdd the column with default value in a standard migration.\n\nRemove the default in a post-deployment migration.\n\nThe post-deployment migration happens after the application restarts, ensuring the new column has been discovered.\n\nChanging the column default\n\nOne might think that changing a default column with change_column_default is an expensive and disruptive operation for larger tables, but in reality it’s not.\n\nTake the following migration as an example:\n\nMigration above changes the default column value of one of our largest tables: namespaces. This can be translated to:\n\nIn this particular case, the default value exists and we’re just changing the metadata for request_access_enabled column, which does not imply a rewrite of all the existing records in the namespaces table. Only when creating a new column with a default, all the records are going be rewritten.\n\nFor the reasons mentioned above, it’s safe to use change_column_default in a single-transaction migration without requiring disable_ddl_transaction!.\n\nUpdating an existing column\n\nTo update an existing column to a particular value, you can use update_column_in_batches. This splits the updates into batches, so we don’t update too many rows at in a single statement.\n\nThis updates the column foo in the projects table to 10, where some_column is 'hello':\n\nIf a computed update is needed, the value can be wrapped in Arel.sql, so Arel treats it as an SQL literal. It’s also a required deprecation for Rails 6.\n\nThe below example is the same as the one above, but the value is set to the product of the bar and baz columns:\n\nIn the case of update_column_in_batches, it may be acceptable to run on a large table, as long as it is only updating a small subset of the rows in the table, but do not ignore that without validating on the GitLab.com staging environment - or asking someone else to do so for you - beforehand.\n\nRemoving a foreign key constraint\n\nWhen removing a foreign key constraint, we need to acquire a lock on both tables that are related to the foreign key. For tables with heavy write patterns, it’s a good idea to use with_lock_retries, otherwise you might fail to acquire a lock in time. You might also run into deadlocks when acquiring a lock, because ordinarily the application writes in parent,child order. However, removing a foreign key acquires the lock in child,parent order. To resolve this, you can explicitly acquire the lock in parent,child, for example:\n\nDropping a database table\n\nDropping a database table is uncommon, and the drop_table method provided by Rails is generally considered safe. Before dropping the table, consider the following:\n\nIf your table has foreign keys on a high-traffic table (like projects), then the DROP TABLE statement is likely to stall concurrent traffic until it fails with statement timeout error.\n\nTable has no records (feature was never in use) and no foreign keys:\n\nUse the drop_table method in your migration.\n\nTable has records but no foreign keys:\n\nRemove the application code related to the table, such as models, controllers and services.\n\nIn a post-deployment migration, use drop_table.\n\nThis can all be in a single migration if you’re sure the code is not used. If you want to reduce risk slightly, consider putting the migrations into a second merge request after the application changes are merged. This approach provides an opportunity to roll back.\n\nTable has foreign keys:\n\nRemove the application code related to the table, such as models, controllers, and services.\n\nIn a post-deployment migration, remove the foreign keys using the with_lock_retries helper method. In another subsequent post-deployment migration, use drop_table.\n\nThis can all be in a single migration if you’re sure the code is not used. If you want to reduce risk slightly, consider putting the migrations into a second merge request after the application changes are merged. This approach provides an opportunity to roll back.\n\nRemoving the foreign key on the projects table using a non-transactional migration:\n\nDropping the table:\n\nDropping a sequence\n\nDropping a sequence is uncommon, but you can use the drop_sequence method provided by the database team.\n\nUnder the hood, it works like this:\n\nRemove a sequence:\n\nRemove the default value if the sequence is actually used.\n\nExecute DROP SEQUENCE.\n\nRe-add a sequence:\n\nCreate the sequence, with the possibility of specifying the current value.\n\nChange the default value of the column.\n\nA Rails migration example:\n\nTruncate a table\n\nTruncating a table is uncommon, but you can use the truncate_tables! method provided by the database team.\n\nUnder the hood, it works like this:\n\nFinds the gitlab_schema for the tables to be truncated.\n\nIf the gitlab_schema for the tables is included in the connection’s gitlab_schemas, it then executes the TRUNCATE statement.\n\nIf the gitlab_schema for the tables is not included in the connection’s gitlab_schemas, it does nothing.\n\nSwapping primary key\n\nSwapping the primary key is required to partition a table as the partition key must be included in the primary key.\n\nYou can use the swap_primary_key method provided by the database team.\n\nUnder the hood, it works like this:\n\nDrop the primary key constraint.\n\nAdd the primary key using the index defined beforehand.\n\nInteger column type\n\nBy default, an integer column can hold up to a 4-byte (32-bit) number. That is a max value of 2,147,483,647. Be aware of this when creating a column that holds file sizes in byte units. If you are tracking file size in bytes, this restricts the maximum file size to just over 2GB.\n\nTo allow an integer column to hold up to an 8-byte (64-bit) number, explicitly set the limit to 8-bytes. This allows the column to hold a value up to 9,223,372,036,854,775,807.\n\nRails migration example:\n\nStrings and the Text data type\n\nSee the text data type style guide for more information.\n\nBy default, Rails uses the timestamp data type that stores timestamp data without time zone information. The timestamp data type is used by calling either the add_timestamps or the timestamps method.\n\nAlso, Rails converts the :datetime data type to the timestamp one.\n\nExample:\n\nInstead of using these methods, one should use the following methods to store timestamps with time zones:\n\nadd_timestamps_with_timezone\n\ntimestamps_with_timezone\n\ndatetime_with_timezone\n\nThis ensures all timestamps have a time zone specified. This, in turn, means existing timestamps don’t suddenly use a different time zone when the system’s time zone changes. It also makes it very clear which time zone was used in the first place.\n\nStoring JSON in database\n\nThe Rails 5 natively supports JSONB (binary JSON) column type. Example migration adding this column:\n\nBy default hash keys will be strings. Optionally you can add a custom data type to provide different access to keys.\n\nWhen using a JSONB column, use the JsonSchemaValidator to keep control of the data being inserted over time.\n\nAdditionally, you can expose the keys in a JSONB column as ActiveRecord attributes. Do this when you need complex validations, or ActiveRecord change tracking. This feature is provided by the jsonb_accessor gem, and does not replace JsonSchemaValidator.\n\nYou can now use restricted_visibility_levels as an ActiveRecord attribute:\n\nEncrypted attributes\n\nDo not store attr_encrypted attributes as :text in the database; use :binary instead. This uses the bytea type in PostgreSQL and makes storage more efficient:\n\nWhen storing encrypted attributes in a binary column, we need to provide the encode: false and encode_iv: false options to attr_encrypted:\n\nTesting\n\nSee the Testing Rails migrations style guide.\n\nData migration\n\nPrefer Arel and plain SQL over usual ActiveRecord syntax. In case of using plain SQL, you need to quote all input manually with quote_string helper.\n\nExample with Arel:\n\nExample with plain SQL and quote_string helper:\n\nIf you need more complex logic, you can define and use models local to a migration. For example:\n\nWhen doing so be sure to explicitly set the model’s table name, so it’s not derived from the class name or namespace.\n\nBe aware of the limitations when using models in migrations.\n\nModifying existing data\n\nIn most circumstances, prefer migrating data in batches when modifying data in the database.\n\nWe introduced a new helper each_batch_range which facilitates the process of iterating over a collection in a performant way. The default size of the batch is defined in the BATCH_SIZE constant.\n\nSee the following example to get an idea.\n\nPurging data in batch:\n\nThe first argument is the table being modified: 'ci_pending_builds'.\n\nThe second argument calls a lambda which fetches the relevant dataset selected (the default is set to .all): scope: ->(table) { table.ref_protected }.\n\nThe third argument is the batch size (the default is set in the BATCH_SIZE constant): of: BATCH_SIZE.\n\nHere is an example MR illustrating how to use our new helper.\n\nUsing application code in migrations (discouraged)\n\nThe use of application code (including models) in migrations is generally discouraged. This is because the migrations stick around for a long time and the application code it depends on may change and break the migration in future. In the past some background migrations needed to use application code in order to avoid copying hundreds of lines of code spread across multiple files into the migration. In these rare cases it’s critical to ensure the migration has good tests so that anyone refactoring the code in future will learn if they break the migration. Using application code is also discouraged for batched background migrations , the model needs to be declared in the migration.\n\nUsually you can avoid using application code (specifically models) in a migration by defining a class that inherits from MigrationRecord (see examples below).\n\nIf using are using a model (including defined in the migration), you should first clear the column cache using reset_column_information.\n\nIf using a model that leverages single table inheritance (STI), there are special considerations.\n\nThis avoids problems where a column that you are using was altered and cached in a previous migration.\n\nExample: Add a column my_column to the users table\n\nIt is important not to leave out the User.reset_column_information command, to ensure that the old schema is dropped from the cache and ActiveRecord loads the updated schema information.\n\nThe underlying table is modified and then accessed via ActiveRecord.\n\nNote that this also needs to be used if the table is modified in a previous, different migration, if both migrations are run in the same db:migrate process.\n\nThis results in the following. Note the inclusion of my_column:\n\nIf you skip clearing the schema cache (User.reset_column_information), the column is not used by ActiveRecord and the intended changes are not made, leading to the result below, where my_column is missing from the query.\n\nHigh traffic tables\n\nHere’s a list of current high-traffic tables.\n\nDetermining what tables are high-traffic can be difficult. Self-managed instances might use different features of GitLab with different usage patterns, thus making assumptions based on GitLab.com not enough.\n\nTo identify a high-traffic table for GitLab.com the following measures are considered. Note that the metrics linked here are GitLab-internal only:\n\nRead operations\n\nNumber of records\n\nSize is greater than 10 GB\n\nAny table which has some high read operation compared to current high-traffic tables might be a good candidate.\n\nAs a general rule, we discourage adding columns to high-traffic tables that are purely for analytics or reporting of GitLab.com. This can have negative performance impacts for all self-managed instances without providing direct feature value to them.\n\nMilestone\n\nBeginning in GitLab 16.6, all new migrations must specify a milestone, using the following syntax:\n\nAdding the correct milestone to a migration enables us to logically partition migrations into their corresponding GitLab minor versions. This:\n\nSimplifies the upgrade process.\n\nAlleviates potential migration ordering issues that arise when we rely solely on the migration’s timestamp for ordering.\n\nAutovacuum wraparound protection\n\nThis is a special autovacuum run mode for PostgreSQL and it requires a ShareUpdateExclusiveLock on the table that it is vacuuming. For larger tables this could take hours and the lock can conflict with most DDL migrations that try to modify the table at the same time. Because the migrations will not be able to acquire the lock in time, they will fail and block the deployments.\n\nThe post-deploy migration (PDM) pipeline can check and halt its execution if it detects a wraparound prevention vacuum process on one of the tables. For this to happen we need to use the complete table name in the migration name. For example add_foreign_key_between_ci_builds_and_ci_job_artifacts will check for vacuum on ci_builds and ci_job_artifacts before executing the migrations."
    }
}
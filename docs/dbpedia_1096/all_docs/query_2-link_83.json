{
    "id": "dbpedia_1096_2",
    "rank": 83,
    "data": {
        "url": "https://www.history-and-philosophy-of-physics.com/events.html",
        "read_more_link": "",
        "language": "en",
        "title": "Events",
        "top_image": "https://www.history-and-philosophy-of-physics.com/uploads/8/5/4/4/8544232/schedule-final-final_2.jpg",
        "meta_img": "https://www.history-and-philosophy-of-physics.com/uploads/8/5/4/4/8544232/schedule-final-final_2.jpg",
        "images": [
            "https://www.history-and-philosophy-of-physics.com/uploads/8/5/4/4/8544232/schedule-final-final_2.jpg"
        ],
        "movies": [],
        "keywords": [],
        "meta_keywords": [
            ""
        ],
        "tags": null,
        "authors": [],
        "publish_date": null,
        "summary": "",
        "meta_description": "DE / EN",
        "meta_lang": "en",
        "meta_favicon": "",
        "meta_site_name": "History and Philosophy of Physics",
        "canonical_link": "https://www.history-and-philosophy-of-physics.com/events.html",
        "text": "Shortly after the development of general relativity, it would become clear that Einstein’s general covariance based approach does not reduce inertia to mass interactions. While Einstein subsequently treated “Mach’s principle” as a selection criterion for models of his theory, there is an alternative research program that implements this idea explicitly in its foundation. What I have called the Reissner-Sciama hypothesis combines Mach’s hypothesis concerning the material origin of inertia with Einstein’s equivalence hypothesis (which suggests the unity of gravity and inertia), in such a way that gravity is identified as a necessary side-effect of the relativity of inertia; thus hypothetically explaining the reason for the existence of the gravitational force. A corollary of this hypothesis is that the gravitational “constant” becomes a dynamical variable determined by the cosmic structure.\n\nThe key papers I will draw from are Reissner (1915) and Sciama (1953). The first of these provides the best historical articulation of the hypothesis, whereas the second best illustrates the idea mathematically. In the last part of my talk, I will analyse these ideas from the philosophical perspective, discuss what it means for gravity to arise necessarily, and explore the broader implications that this sort of reasoning has for cosmology.\n\nTo answer the question of whether machines can make scientific discoveries, two sub-questions need to be answered: (1) What is required to make a scientific discovery? (2) Are machines capable of meeting these requirements? After some remarks on the role of scientific progress and the significance of machines in science, I show how to conceive of scientific discoveries as structured processes with the three indispensable structural features of finding, acceptance, and knowledge (cf. Michel 2022). By elaborating on each of these features, I identify several requirements for artificial discoverers. Turning then to an objection raised by Green (2022), I address two crucial issues: First, the question of whether machines can perform speech acts (cf. Green & Michel 2022), and in particular declarative speech acts. Second, the question of how different institutionalized publication cultures realize what I call acceptance mechanisms. With this in mind, I show how Green’s objection can be met once we distinguish between degrees of acceptance in certain ways. I close with a diagnosis of what to make of the idea of artificial discoverers in science.\n\nThis talk introduces a new approach to understanding quantum mechanics (QM) called ‘pure-inferential quantum mechanics.’ The animating thought behind this approach is that, while theories can be about the world without describing it, we would be just as wrong to assume, as anti-realists such as QBists do, that quantum states never describe the world, as we would be to assume, as realists such as Everettians do, that they always do. I marshal some resources from pragmatist philosophy of language to delineate the circumstances under which QM should be understood as descriptive from those under which it should not be. By constructing a novel inferentialist-pragmatist interpretation of QM, I demonstrate, pace Healey, that the inferentialist about QM has the resources to ground an autonomous, descriptive interpretation of QM in its inferential profile, under the appropriate circumstances. I argue that, if we should be inferentialists about QM (and there are good reasons why we should be), then we should be pure-inferentialists.\n\nOur notions of space and time underwent a radical change just over a 100 years ago. Through general relativity, gravity ceased to be a force and became a manifestation of space-time geometry. This paradigm shift opened unforeseen perspectives in our understanding of the physical universe: possibility of ripples in space-time geometries that manifest themselves as gravitational waves; of black holes, representing geometries that trap not only matter but also light; and of the Big Bang, the primordial explosion marking the birth of the space-time continuum itself. However, through black holes and the big bang we also learnt that Einstein’s equations predict the presence of space-time singularities: rugged edges where the space-time continuum tears and all of classical physics comes to an abrupt halt. These singularities are the gates to physics beyond Einstein –i.e., to unification of general relativity with quantum physics. Construction of this desired theory of quantum gravity is a truly challenging task because it requires an entirely new syntax to formulate concepts that are sufficiently adequate to describe the extreme universe. We now need the quantum analog of Riemannian geometry that serves as the syntax for general relativity. After a brief discussion of why several distinct approaches are being pursued, I will focus on loop quantum gravity, based on a specific theory of Riemannian quantum geometry. I will explain how it leads to quantum space-times that extend Einstein’s classical continuum beyond its singularities. While this conceptual framework is rather abstract and involves novel mathematics, it also leads to predictions that can be tested observationally.\n\n​Ken Wharton and I have proposed a mechanism for quantum entanglement. The key ingredient is the familiar statistical phenomenon of collider bias, or Berkson's bias. In the language of causal models, a collider is a variable causally influenced by two or more other variables. Conditioning on a collider typically produces non-causal associations between its contributing causes, even if they are actually independent. It is easy to show that this phenomenon can produce associations analogous to Bell correlations, in suitable post-selected ensembles. It is also straightforward that such collider artefacts may become real connections, resembling causality, if a collider is 'constrained' (e.g., by a future boundary condition). We consider the time-reversed analogues of these points in the context of retrocausal models of QM. Retrocausality yields a collider at the source of an EPR-Bell particle pair, and in this case constraint of the collider is possible by normal methods of experimental preparation. It follows that connections resembling causality may be expected to emerge across such colliders, from one branch of the experiment to the other. Our hypothesis is that this constrained retrocausal collider bias is the origin of entanglement. This talk will be based on an explanation of the idea for general audiences available at https://arxiv.org/abs/2212.06986, itself based on a suggestion we first made in arXiv:2101.05370v4.\n\n​​The central motivation of the Many-Worlds Interpretation (MWI) of quantum mechanics is to view quantum mechanics as a faithful representation of (all) physical reality. In this talk, I exposit the core of the reasoning of adherents of the MWI from this commitment to the conclusion that physical reality consists of a simultaneous multitude of branching, approximately classical worlds. Given a commitment to realism about quantum mechanics, this reasoning is plausible, but ultimately unsound. In particular, evidentially motivated realism about unitary, continuous evolution of quantum states implies the falsity of two premises essential to this reasoning: (i) that wavefunctions in general consist of superpositions of components each of which would on their own deterministically evolve into one unique corresponding measurement state in a given experimental context; (ii) that unitary, continuous evolution of quantum states is universal. In the bulk of the talk, I accordingly advance an immanent critique of the MWI: evidentially motivated realism about unitary, continuous evolution of quantum states not only fails to support the MWI, but positively rules out the existence of any such worlds. This critique in turn implies a few positive upshots for the foundations of quantum physics, including that quantum measurements of position must be (NB: not “cause”) discontinuous changes of quantum state or “quantum jumps” and that the wavefunctions of non-relativistic quantum mechanics must be effective representations of (a plurality of possible) dynamics of a specific class of physical systems. ​\n\n​When the first image of the shadow of a black hole was released in 2019, the result was presented by the Event Horizon Telescope (EHT) Collaboration as a ‘direct’ image. Yet the meaning of ‘directness’ was not always clear. In this talk, I present joint work with Emilie Skulberg analysing the many notions of `directness' that emerged with respect to this image in peer-reviewed literature, science communication, and interviews with members of the EHT Collaboration. I will compare and contrast the EHT case to other case studies including: the LIGO-Virgo `direct detection' of gravitational waves, the observation of s-stars by the Genzel and Ghez groups, and the detection of gravitational waves using the Hulse-Taylor binary. In doing so, I will tease apart different notions of directness and the extent to which these connect to matters of epistemology and prestige. The goal will be to spell out the ways in which the EHT images are (and are not) direct and what (if anything) ascriptions of directness tell us about the nature of an astronomical observation.\n\n​Wolfgang Pauli is considered to be one of the greatest minds of the twentieth century, a genius of nearly unparalleled depth and scope. His critical insight was legendary, and his judgment unassailable. Indeed, Pauli was long held as the living 'conscience of physics'. However, this high acclaim has not extended to Pauli's extensive historical and philosophical research. The criticism of Pauli's philosophical thought is often tied to his friendship with Carl Jung, and their decades-long collaboration on Jung's analytic psychology (which included a shared interest in parapsychology and the paranormal). While it is true that Pauli's thought tended toward mysticism, his historical and philosophical studies spanned over two millennia in the history of ideas, and went far beyond the confines of Jungian psychology. And though Pauli's collaboration with Jung has long garnered significant interest within the history of psychology and the philosophy of mind, there has been little discussion of his broader philosophy of science. This lack of detailed historical and philosophical study has obscured some of the fundamental insights that Pauli sought to draw from his decades of research in the foundations of physics.\n\nIn this talk, I will present some of the lesser known aspects of Pauli's philosophy. In particular, I will focus on Pauli's reading of Plato, as it emerges from both his work with Jung and his broader correspondence in the late 40s and early 50s. For Pauli, Plato is not only the progenitor of a distinct philosophy, but is also representative of a key stage in the historical development of philosophical thought. The rich interplay between Pauli's reading of Plato and the history of Platonism sets the course for his understanding of the expression of archetypical forms of thought throughout history. This expression is guided by what Pauli terms a cosmic harmony which underwrites our understanding of nature. Through a discussion of Pauli's Platonism and his appeal to a cosmic harmony in nature, I will try to outline some of the contours of his later philosophical thought and his hopes for future physics.\n\n-----\n\n​This talk will focus on the development of new mathematical methods during the 1960s that allowed for new ways of understanding the solution space of the Einstein equations. The focus will be on the classification scheme for vacuum solutions first developed by Aleksei Petrov in 1954 and then applied to the question of how to give an coordinate-independent definition of the presence of gravitational radiation by Felix Pirani in 1957. I will review Pirani’s definition and rationale for proposing his definition, and then discuss Penrose’s 1960 re-derivation and elaboration of the Petrov classification in the context of his spinor formulation of GR, and his criticism of Pirani’s definition of gravitational radiation. Starting from there, I will review the emerging discussion of how the different Petrov classes should be interpreted, and thus how the solution space of the Einstein equations could be understood, indeed how it could be used as a map of spacetimes and their interpretation. I shall argue that present-day philosophy of physics is still very far from having harvested all the conceptual treasures that originated from this debate in the 1950s and 1960s.\n\nIn the last years, a complex historical debate has emerged on the causes, origins, and manifestations of the process dubbed the ‘renaissance’ of Einstein’s theory of gravitation starting. After a thirty-year period of stagnation of the theory, known as its low-water-mark phase, the renaissance marks the return of general relativity to the mainstream of physics after the mid-1950s. The talk aims at presenting the main results of our analysis of the renaissance of general relativity based on the conceptual and methodological framework of socio-epistemic networks. This framework defines a three-layered taxonomy of knowledge networks: the social network (the collection of relations involving individuals and institutions), the semiotic network (the collection of the material representations of knowledge, e.g., citation networks), and the semantic network (the collection of knowledge elements and their relations, including concepts, topics, research agendas, or methods). On the basis of this multi-layer network analysis of the general relativity research landscape between 1925 and 1970, I will argue that the renaissance process should be understood as a two-step reconfiguration of research agendas resulting initially from the interplay of social and epistemic factors. A first phase of theoretical renaissance, driven by social transformations, occurred between the mid-1950s and the early 1960s, and transformed the general theory of relativity to a bona fide physical theory. The second phase of this process, which can be called the astrophysical turn, was an experiment-driven shift toward relativistic astrophysics and physical cosmology, and was strongly related to discoveries in the astrophysical domain in the 1960s. I will conclude by showing early results built on this approach to characterize the conceptual transformation characterizing the first phase as well as to capture the role played by the Brans-Dicke theory as catalyzer of research trends in the second phase of the astrophysical turn.\n\nScientific realists argue that empirically successful theories latch on to unobservable features of reality. But it is often thought that conventional theories of particle physics do not deserve realist commitment, despite their outstanding empirical success. Recently, a number of \"effective\" realisms have argued that we should distinguish between the low- and high-energy claims of particle theory and that we can and should be realist about the former but not the latter. I argue that this proposal conflicts with physical folklore, according to which the claims of realistic particle theories cannot be relativized to energy scales. I use this conflict to distinguish two forms of effective realism. One form of effective realism indeed conflicts with the kernel of truth in the physical folklore, making it inapplicable to the theories that enjoy empirical success. The second form of effective realism is compatible with realistic theories, but requires a substantial revision of the terms of the realist debate. I will indicate why I think the second form is nevertheless the more promising.\n\nIn this talk, I will clarify and adjudicate a controversy that arose within the astrophysics community concerning whether or not the first LIGO detection, “GW150914”, was the first “direct detection” of gravitational waves. To do so, I provide an analysis according to which there is an epistemically significant distinction between direct and indirect detections in this context. Roughly, our justification for trusting a direct detection depends mainly on the reliability of instruments that are under our control, rather than on the reliability of our models of separate target systems. In contrast, indirect detections rely on confidence in such models. Having argued for this account of the direct/indirect distinction, I close by considering its application to LIGO’s other scientific role: observing black holes. This helps to illustrate some of the key epistemic challenges of gravitational-wave astrophysics (and indeed astrophysics broadly) in contrast to science with controlled experiments.\n\n​(For those who are interested, the most recent version of the corresponding paper can be downloaded from the research section of my website.)\n\nPhilosophers of science who engage with astrophysics often portray it as a distinctively observational science, since its targets are too distant to experiment upon. The lack of experiment in astrophysics is usually taken either (pessimistically) to imply that the field as a whole is epistemically handicapped, or (optimistically) to explain and justify the use of simulations in place of experiments in the pursuit of astrophysical knowledge. This characterization misses some fascinating features of the epistemology of laboratory astrophysics, which investigates the nature of celestial objects and processes using terrestrial experiments. Laboratory astrophysics provides an illuminating testbed for understanding the tradeoffs between experimental and observational methods that scientists face in practice. Drawing on a variety of cases—from axion searches to accelerator-based nuclear astrophysics to high-energy laser confinement experiments—I will argue that the subtleties of the epistemology of laboratory astrophysics are best appreciated by attending to the relationship between the research target and the causal production of the data. Ultimately, this framework actually illuminates more continuity between the epistemic situation in astrophysics and in other fields of empirical research than is often appreciated, in both the obstacles and the opportunities that arise. ​​\n\nSeveral philosophers have advocated an eliminativist position regarding gravitational energy and conservation principles applied to it. We cannot directly characterize the energy carried by the gravitational field with a local quantity analogous to what is used in other field theories: we cannot define a gravitational energy-momentum tensor that assigns local properties to spacetime points and can be integrated over volumes to characterize energy-momentum flows. Because of the equivalence principle, we can always choose a locally freely falling frame, and by so doing locally transform away the gravitational field. The eliminitavists take these features to imply that there is no such thing as ``gravitational energy'' or integral conservation laws governing it, and that efforts to resurrect such a notion illustrate how misleading it can be to treat general relativity as analogous to other field theories.\n\nIn this talk I will consider how quasi-local definitions of energy and conservation laws based on them support a response to the eliminativists, and in particular concerns about whether such proposals depend on ``background structure'' in a problematic sense. Quasi-local energy and conservation laws depend on background structure --- we need a way to designate some motions as ``freely falling,'' so that energy-momentum transfers can be measured via departures from these trajectories. But I will argue that these background structures can justifiably be introduced within particular modeling contexts. The challenge regarding gravitational energy then has a different character: namely that there are many conflicting proposals for how to define quasi-local energy, and it is not clear whether they deliver consistent verdicts.\n\nIn 1906, Walter Kaufmann published the results of his highly anticipated experiments on the velocity-dependency of the electron's charge-to-mass ratio. The interest mainly stemmed from Kaufmann's promise that these experiments would be so precise as to allow for a decision between two fundamental approaches to physics at the time: the theory of special relativity and the electromagnetic world view. And the results were clear, according to Kaufmann: the relativistic approach had to be considered as a failure.\n\nIn this talk, I will focus on one particular aspect of these experiments and the attempts to replicate them, namely the use of photographic plates to capture electrons and measure their velocity and charge-to-mass ratio. I will argue, more specifically, that by analyzing how these photographic plates were produced and handled, one can discern different conceptualizations of the relation between objectivity, precision, and error. By means of the work of Kathryn Olesko, Lorraine Daston and Kelley Wilder, I will then argue that these different conceptualizations can be traced back to different local measurement contexts.\n\nIn the final part of my talk, I will then reflect on what this could mean for how we study approaches such as the electromagnetic world view, following a debate between Suman Seth and Shaul Katzir. I will suggest, more specifically, that they are not to be seen as a set of explicit theoretical commitments, but rather as a way of going about in either theoretical or experimental practice, and that how this way is given content is, in part, influenced by the local context in which a scientist is active.\n\nThe history of Albert Einstein’s relativity theory is often told through a fixed set of places: the Bern Patent office, the Zurich Notebooks, and the celebrated presentation of the general theory in Berlin in November 1915. Marginal to this progression of intellectual triumphs is Prague, where Einstein worked at the German University as professor of theoretical physics from April 1911 to August 1912. While in Prague, Einstein for the first time devoted significant attention to expanding his earlier ideas about how to generalize special relativity as a theory of gravity. The so-called “static theory,” which he developed in the Bohemian capital, has been widely considered a failure (when it is considered at all), and serious analysis of his intellectual trajectory typically begins with his subsequent collaboration with Marcel Grossmann in Zurich. This presentation places the static theory within the Prague context to show the many ways in which the development and interpretation of relativity was shaped by his comparatively brief residence in the third city of the Habsburg Empire. In addition to gravitation, the presentation will discuss Einstein’s personal and philosophical connections with his Prague predecessor Ernst Mach and Prague successor Philipp Frank, as well as later Czech interpretations of relativity theory in the decades following Einstein’s departure.\n\nMeasuring the value of the Hubble constant, i.e., the rate at which the universe expands at a given time, has always been a topic of controversy. As early as the 1970’s, Sandage et de Vaucouleurs have been arguing about what the adequate methodology should be for such a delicate experimental measure. Should astronomers focus only their best indicators, e.g. the Cepheids, and improve the precision of this measurement based on a unique object to the best possible? Or should they “spread the risks” , i.e., multiply the indicators and methodologies before averaging over their results? Should a robust agreement across several uncertain measures weigh more than a single 1% precision measurement? This disagreement, I argue, stems from a misconception of what managing the uncertainties associated with such experimental measures require. Astrophysical measurements such as the measure of the Hubble constant, indeed, require a methodology that permits both to reduce the known uncertainties and to track the unknown unknowns. In other words, the methods advocated by Sandage and de Vaucouleurs are both needed, but do not serve the same purpose. Based on the lessons drawn from the Hubble crisis, I suggest a methodological guide for identifying, quantifying and reducing uncertainties in astrophysical measurements.\n\nCombining empirical results in an epistemically responsible way requires significant care and attention to details of the provenance of those results. Indeed, the epistemic utility of joint constraints hangs crucially on how they were combined. Cosmologists want to study dark energy by combining data from different probes (the cosmic microwave background, supernovae, gravitational lensing, galaxy clustering, and baryon acoustic oscillations) in order to produce joint constraints on the dark energy equation of state parameter. The details involved in each of these probes are different. They use different instruments, techniques, and modeling assumptions. Until relatively recently, the way that cosmologists have deployed these results together was by processing the data from these diverse probes in parallel and then combining the results at the very end. But, as the Dark Energy Survey (DES) collaboration has pointed out, we have good reason to think that this approach to combining results could be epistemically problematic since the physical processes from which these various results derive are not independent of one another. Therefore, a more sophisticated way to combine them is to take into account correlations between the results from the different probes. Is this second approach enough to ensure that resources and assumptions that have gone into each are not interacting in a way that is problematically affecting the use to which the cosmologists want to put these results? More broadly, what is required in order to combine diverse results in an epistemically responsible way?\n\nCombining empirical results in an epistemically responsible way requires significant care and attention to details of the provenance of those results. Indeed, the epistemic utility of joint constraints hangs crucially on how they were combined. Cosmologists want to study dark energy by combining data from different probes (the cosmic microwave background, supernovae, gravitational lensing, galaxy clustering, and baryon acoustic oscillations) in order to produce joint constraints on the dark energy equation of state parameter. The details involved in each of these probes are different. They use different instruments, techniques, and modeling assumptions. Until relatively recently, the way that cosmologists have deployed these results together was by processing the data from these diverse probes in parallel and then combining the results at the very end. But, as the Dark Energy Survey (DES) collaboration has pointed out, we have good reason to think that this approach to combining results could be epistemically problematic since the physical processes from which these various results derive are not independent of one another. Therefore, a more sophisticated way to combine them is to take into account correlations between the results from the different probes. Is this second approach enough to ensure that resources and assumptions that have gone into each are not interacting in a way that is problematically affecting the use to which the cosmologists want to put these results? More broadly, what is required in order to combine diverse results in an epistemically responsible way?"
    }
}
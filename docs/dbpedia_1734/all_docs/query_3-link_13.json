{
    "id": "dbpedia_1734_3",
    "rank": 13,
    "data": {
        "url": "https://hevodata.com/learn/kafka-for-data-ingestion/",
        "read_more_link": "",
        "language": "en",
        "title": "Kafka for Data Ingestion Simplified 101 - Learn",
        "top_image": "https://res.cloudinary.com/hevo/images/f_webp,q_auto/v1685906866/hevo-learn-1/kafka-for-data-ingestion-featured-image/kafka-for-data-ingestion-featured-image.png?_i=AA",
        "meta_img": "https://res.cloudinary.com/hevo/images/f_webp,q_auto/v1685906866/hevo-learn-1/kafka-for-data-ingestion-featured-image/kafka-for-data-ingestion-featured-image.png?_i=AA",
        "images": [
            "https://res.cloudinary.com/hevo/image/upload/v1684393936/hevo-logos-v2/logo-horizontal/black/logo_cqvfsz.svg",
            "https://res.cloudinary.com/hevo/image/upload/v1650395814/hevo-website/icons/blog-icon_dyyxou.svg",
            "https://res.cloudinary.com/hevo/image/upload/v1679319423/hevo-website/icons/Learn_dmj8xm.svg",
            "https://res.cloudinary.com/hevo/image/upload/v1650395854/hevo-website/icons/success-story-icon_j5dwiy.svg",
            "https://res.cloudinary.com/hevo/image/upload/v1650395917/hevo-website/icons/ebook-icon_gpvq1j.svg",
            "https://res.cloudinary.com/hevo/image/upload/v1650597746/hevo-website/icons/play-orange_mymqko.svg",
            "https://res.cloudinary.com/hevo/image/upload/v1650395882/hevo-website/icons/webinar-icon_p4m0gm.svg",
            "https://res.cloudinary.com/hevo/image/upload/v1650395944/hevo-website/icons/docs-icon_umhq5b.svg",
            "https://res.cloudinary.com/hevo/image/upload/v1650395967/hevo-website/icons/api-icon_ntmmor.svg",
            "https://res.cloudinary.com/hevo/images/f_webp,q_auto/v1685953143/hevo-learn-1/Install-Kafka-on-Mac-Thumbnail/Install-Kafka-on-Mac-Thumbnail.png?_i=AA",
            "https://res.cloudinary.com/hevo/image/upload/v1714480442/hevo-learn/Screenshot_2024-04-30_at_6.02.47_PM_mpc4wf.png",
            "https://res.cloudinary.com/hevo/images/w_768,h_408/f_webp,q_auto/v1722512564/hevo-learn-1/Fivetran_vs_airbyte_zorirq/Fivetran_vs_airbyte_zorirq.png?_i=AA",
            "https://res.cloudinary.com/hevo/images/w_768,h_432/f_webp,q_auto/v1721038626/hevo-learn-1/Iceberg_vs_Paraquet_byuju8/Iceberg_vs_Paraquet_byuju8.png?_i=AA",
            "https://res.cloudinary.com/hevo/images/w_768,h_434/f_webp,q_auto/v1721034843/hevo-learn-1/Data_Lakes_iok8mo/Data_Lakes_iok8mo.png?_i=AA",
            "https://res.cloudinary.com/hevo/image/upload/v1684394045/hevo-logos-v2/logo-horizontal/white/logo_zrn99q.svg",
            "https://res.cloudinary.com/hevo/image/upload/f_auto,q_auto,h_20/v1621528401/hevo-website/investors-footer/chiratae_j0fr47.png",
            "https://res.cloudinary.com/hevo/image/upload/f_auto,q_auto,h_20/v1621528401/hevo-website/investors-footer/qualgro_attcoq.png",
            "https://res.cloudinary.com/hevo/image/upload/f_auto,q_auto,h_20/v1621528401/hevo-website/investors-footer/sequoia_bshgwe.png",
            "https://res.cloudinary.com/hevo/image/upload/v1661155017/hevo-website/news/TechCrunch_sy4cjz.png",
            "https://res.cloudinary.com/hevo/image/upload/v1661154933/hevo-website/news/Yourstory_x1amqo.png",
            "https://res.cloudinary.com/hevo/image/upload/v1661155209/hevo-website/news/The_Economics_Times_qwi4ay.png",
            "https://res.cloudinary.com/hevo/image/upload/f_auto,q_auto,w_120/v1621528581/hevo-website/security/HIPAA_ib55zc.png",
            "https://res.cloudinary.com/hevo/image/upload/f_auto,q_auto,w_120/v1694596315/hevo-website/security/GDPR_nhkboe.png",
            "https://res.cloudinary.com/hevo/image/upload/f_auto,q_auto,w_120/v1621528581/hevo-website/security/HIPAA_ib55zc.png",
            "https://res.cloudinary.com/hevo/image/upload/f_auto,q_auto,w_120/v1694596315/hevo-website/security/CCPA_n0wham.png",
            "https://res.cloudinary.com/hevo/image/upload/v1663058308/hevo-blog/icons/close-white_aakthq_cwg6pm.svg",
            "https://res.cloudinary.com/hevo/image/upload/v1645798343/hevo-blog/close-grey_a4wzsp.svg",
            "https://res.cloudinary.com/hevo/image/upload/v1663051656/hevo-blog/icons/btn-secondary-loading_ifbmq0.svg",
            "https://www.facebook.com/tr?id=3042478099207429&ev=PageView&noscript=1",
            "https://px.ads.linkedin.com/collect/?pid=208339&fmt=gif"
        ],
        "movies": [],
        "keywords": [],
        "meta_keywords": [
            ""
        ],
        "tags": null,
        "authors": [
            "Raj Verma"
        ],
        "publish_date": "2023-05-10T12:55:00+05:30",
        "summary": "",
        "meta_description": "Kafka for Data Ingestion a reliable event-based system that can handle large volumes of data with low latency and high scalability. Read on to learn more.",
        "meta_lang": "en",
        "meta_favicon": "https://res.cloudinary.com/hevo/images/w_32,h_32,c_fill/f_webp,q_auto/v1685872133/cropped-Hevo-Brand-@2x_1730446e4fb/cropped-Hevo-Brand-@2x_1730446e4fb.png?_i=AA",
        "meta_site_name": "Learn | Hevo",
        "canonical_link": "https://hevodata.com/learn/kafka-for-data-ingestion/",
        "text": "Organizations today are overflowing with data. The amount of data produced every day is truly staggering. A study by Forbes[1] a few years ago confirmed the Data Explosion.\n\nIt was found in the study that nearly 2.5 Quintillion Bytes of data are generated every day!\n\nBecause of this Data Explosion, it has become seemingly difficult to capture, process, and store big or complex datasets. You see, having a large amount of data isn’t a problem, but extracting useful information out of it is.\n\nThankfully, there exist systems and technologies capable enough not only to store this data efficiently but also to elevate the value of the generated data. One such technology is Kafka for Data Ingestion.\n\nOrganizations today have access to a wide stream of data. Apache Kafka, a popular Data Processing Service is used by over 30% of Fortune 500 companies to develop real-time data feeds.\n\nKafka is a fault-tolerant distributed event store platform that exhibits high resiliency and eases the process of data ingestion. But before getting started with Kafka for Data Ingestion, let’s discuss this robust platform in brief.\n\nWhat is Kafka?\n\nApache Kafka is a popular Distributed Data Streaming software that allows for the development of real-time event-driven applications. Kafka is an open-source application that allows you to store, read, and analyze streams of data free of cost.\n\nKafka is distributed, which means that it can run as a Cluster that spans multiple servers. Leveraging its distributed nature, users can achieve high throughput, minimal latency, high computation power, etc., and can handle large volumes of data without any perceptible lag in performance.\n\nWritten in Scala, Kafka supports data from a large number of external Data Sources and stores them as “Topics”. Kafka employs two functions “Producers” and “Consumers” to read, write, and process events.\n\nProducers act as an interface between Data Sources and Topics, and Consumers allow users to read and transfer the data stored in Kafka.\n\nThe fault-tolerant architecture of Kafka is highly scalable and can handle billions of events with ease. In addition to that, Kafka is super fast and is highly accurate with data records.\n\nKey Features of Kafka\n\nTake a look at the prominent features responsible for the immense popularity of Kafka:\n\nFault-Tolerant: Kafka’s fault-tolerant clusters keep the organization’s data safe and secure in distributed and durable clusters. Kafka is exceptionally reliable and it also allows you to create new custom connections as per your needs.\n\nScalability: Kafka can readily handle large volumes of data streams and trillions of messages per day. Kafka’s high scalability allows organizations to easily scale production clusters up to a thousand brokers.\n\nHigh Availability: Kafka is extremely fast and ensures zero downtime making sure your data is available anytime. Kafka replicates your data across multiple clusters efficiently without any data loss.\n\nIntegrations: Kafka comes with a set of connectors that simplify moving data in and out of Kafka. Kafka Connect allows Developers to easily connect to 100s of event sources and event sinks such as AWS S3, PostgreSQL, MySQL, Elasticsearch, etc.\n\nEase of Use: Kafka is a user-friendly platform and doesn’t require extensive programming knowledge to get started. Kafka has extensive resources in terms of documentation, tutorials, videos, projects, etc, to help Developers learn and develop applications using Kafka CLI.\n\nWhat is Data Ingestion?\n\nThere’s a tremendous amount of data coming from disparate sources, it’s coming from your Website, it’s coming from your Mobile Application, REST Services, External Queues, and it’s even coming from your own Business Systems.\n\nData needs to be collected and stored securely without data losses and with the lowest possible latency. This is where Data Ingestion comes in.\n\nData Ingestion refers to the process of collecting and storing mostly unstructured sets of data from multiple Data Sources for further analysis.\n\nThis data can be real-time or integrated into batches. Real-time data is ingested on arrival, whereas batch data is ingested in chunks at regular intervals.\n\nThere are 3 different layers of Data Ingestion:\n\nData Collection Layer: This layer of the Data Ingestion process decides how the data is collected from resources to build the Data Pipeline.\n\nData Processing Layer: This layer of the Data Ingestion process decides how the data is getting processed which further helps in building a complete Data Pipeline.\n\nData Storage Layer: The primary focus of the Data Storage Layer is on how to store the data. This layer is mainly used to store huge amounts of real-time data which is already getting processed from the Data Processing Layer.\n\nNow that you’re familiar with Kafka and Data Ingestion, let’s dive straight into Kafka for Data Ingestion.\n\nSteps to Use Kafka for Data Ingestion\n\nIt is important to have a reliable event-based system that can handle large volumes of data with low latency, scalability, and fault tolerance.\n\nThis is where Kafka for Data Ingestion comes in. Kafka is a framework that allows multiple producers from real-time sources to collaborate with consumers who ingest data.\n\nIn this infrastructure, S3 Objects Storage is used to centralize the data stores, harmonize data definitions and ensure good governance.\n\nS3 is highly scalable and provides fault-tolerance storage for your Data Pipelines, easing the process of Data Ingestion using Kafka.\n\nFollow the below-mentioned steps to use Kafka for Data Ingestion.\n\nProducing Data to Kafka for Data Ingestion\n\nUsing Kafka-connect to Store Raw Data\n\nConfigure and Start the S3 Sink Connector\n\nProducing Data to Kafka for Data Ingestion\n\nThe first step in Kafka for Data Ingestion requires producing data in Kafka. Multiple components read from external sources such as Queues, WebSockets, or REST Services.\n\nConsequently, multiple Kafka Producers are deployed, each delivering data to a distinct topic, which will comprise the source’s raw data.\n\nA homogeneous data structure allows Kafka for Data Ingestion processes to run transparently while writing messages to multiple Kafka raw topics.\n\nThen, all the messages are produced as .json.gzip and contain these general data fields:\n\nraw_data: This represents the data as it comes from the Kafka Producer.\n\nmetadata: This represents the Kafka Producer metadata required to track the message source.\n\ningestion_timestamp: This represents the timestamp when the message was produced. This is later used for Data Partitioning.\n\nHere’s an example of an empty record:\n\n{ \"raw_data\": {}, \"metadata\":{\"thread_id\":0,\"host_name\":\"\",\"process_start_time\":\"\"}, \"ingestion_timestamp\":0 }\n\nUsing Kafka-connect to Store Raw Data\n\nThe first layer of the raw data layer for Kafka Data Ingestion is written to the Data Lake.\n\nThis layer provides immense flexibility to the technical processes and business definitions as the information available is ready for analysis from the beginning.\n\nThen, you can use Kafka-connect to perform this raw data layer ETL without writing a single line of code.\n\nThus, the S3 Sync is used to read data from the raw topics and produce data for S3.\n\nKafka-connect uses the org.apache.kafka.connect.json.JsonConverter to collect data as it comes in and the io.confluent.connect.storage.partitioner.TimeBasedPartitioner to write data.\n\nKafka-connect collects information as it comes using the org.apache.kafka.connect.json.JsonConverter and writes data using the io.confluent.connect.storage.partitioner.TimeBasedPartitioner. You can follow this example to understand how to configure the connector.\n\nConfigure and Start the S3 Sink Connector\n\nTo finish up the process of Kafka for Data Ingestion, you need to configure the S3 Connector by adding its properties in JSON format, and storing them in a file called meetups-to-s3.json:\n\n{ \"name\": \"meetups-to-s3\", \"config\": { \"_comment\": \"The S3 sink connector class\", \"connector.class\":\"io.confluent.connect.s3.S3SinkConnector\", \"_comment\": \"The total number of Connect tasks to spawn (with implicit upper limit the number of topic-partitions)\", \"tasks.max\":\"1\", \"_comment\": \"Which topics to export to S3\", \"topics\":\"meetups\", \"_comment\": \"The S3 bucket that will be used by this connector instance\", \"s3.bucket.name\":\"meetups\", \"_comment\": \"The AWS region where the S3 bucket is located\", \"s3.region\":\"us-west-2\", \"_comment\": \"The size in bytes of a single part in a multipart upload. The last part is of s3.part.size bytes or less. This property does not affect the total size of an S3 object uploaded by the S3 connector\", \"s3.part.size\":\"5242880\", \"_comment\": \"The maximum number of Kafka records contained in a single S3 object. Here a high value to allow for time-based partition to take precedence\", \"flush.size\":\"100000\", \"_comment\": \"Kafka Connect converter used to deserialize keys (unused in this example)\", \"key.converter\":\"org.apache.kafka.connect.json.JsonConverter\", \"key.converter.schemas.enable\":\"false\", \"_comment\": \"Kafka Connect converter used to deserialize values\", \"value.converter\":\"org.apache.kafka.connect.json.JsonConverter\", \"value.converter.schemas.enable\":\"false\", \"_comment\": \"The type of storage for this storage cloud connector\", \"storage.class\":\"io.confluent.connect.s3.storage.S3Storage\", \"_comment\": \"The storage format of the objects uploaded to S3\", \"format.class\":\"io.confluent.connect.s3.format.json.JsonFormat\", \"_comment\": \"Schema compatibility mode between records with schemas (Useful when used with schema-based converters. Unused in this example, listed for completeness)\", \"schema.compatibility\":\"NONE\", \"_comment\": \"The class used to partition records in objects to S3. Here, partitioning based on time is used.\", \"partitioner.class\":\"io.confluent.connect.storage.partitioner.TimeBasedPartitioner\", \"_comment\": \"The locale used by the time-based partitioner to encode the date string\", \"locale\":\"en\", \"_comment\": \"Setting the timezone of the timestamps is also required by the time-based partitioner\", \"timezone\":\"UTC\", \"_comment\": \"The date-based part of the S3 object key\", \"path.format\":\"'date'=YYYY-MM-dd/'hour'=HH\", \"_comment\": \"The duration that aligns with the path format defined above\", \"partition.duration.ms\":\"3600000\", \"_comment\": \"The interval between timestamps that is sufficient to upload a new object to S3. Here a small interval of 1min for better visualization during the demo\", \"rotate.interval.ms\":\"60000\", \"_comment\": \"The class to use to derive the timestamp for each record. Here Kafka record timestamps are used\", \"timestamp.extractor\":\"Record\" } }\n\nSome additional parameters are needed to modify Kafka for Data Ingestión:\n\ntopics.regex: A uniform name for all the raw topics with the suffix _raw is kept. Therefore, a single connector configuration template file is used to create the connectors for multiple topics.\n\n\"topics.regex\": \".*raw$\"\n\nflush.size: Small files could be produced to S3 while receiving tiny messages. This can be prevented by configuring a bigger flush size. Besides, the ingestion linger-timestamp needs to be configured as well: rotate.interval.ms and rotate.schedule.interval.ms.\n\nExample:\n\n\"flush.size\": \"180000\", \"rotate.interval.ms\": \"300000\", \"rotate.schedule.interval.ms\": \"300000\"\n\npath.format: The data is partitioned using the following statement:\n\n\"timestamp.field\": \"ingestion_timestamp\n\nThen, the following format is defined:\n\n\"'date'=YYYY-MM-dd\".\n\nPartitioning the data by date speeds up the ingestion process and the future implementation of parquet partitioned queries.\n\nYou can then issue the REST API call using the Confluent CLI to start the S3 Connector:\n\nconfluent local load meetups-to-s3 -- -d ./meetups-to-s3.json\n\nYou can confirm if the S3 connector has started correctly or not by inspecting the logs of the Connect Worker.\n\nINFO Starting connectors and tasks using config offset 9 INFO Starting connector meetups-to-s3 INFO Starting task meetups-to-s3-0 INFO Creating task meetups-to-s3-0\n\nThe raw data is now successfully stored in S3. That’s it, this is how you can use Kafka for Data Ingestion.\n\nConclusion\n\nKafka is distributed event store and stream-processing platform developed by the Apache Software Foundation and written in Java and Scala.\n\nWithout the need for additional resources, you can use Kafka-connect or Kafka for Data Ingestion to external sources.\n\nThis article helped you use Kafka for Data Ingestion. However, in businesses, extracting complex data from a diverse set of Data Sources can be a challenging task and this is where Hevo saves the day!\n\nvisit our website to explore hevo\n\nHevo Data with its strong integration with 150+ Sources such as Apache Kafka, allows you to not only export data from multiple sources & load data to the destinations, but also transform & enrich your data, & make it analysis-ready so that you can focus only on your key business needs.\n\nGive Hevo Data a try and sign up for a 14-day free trial today. Hevo offers plans & pricing for different use cases and business needs, check them out!\n\nShare your experience of understanding Kafka for Data Ingestion in the comments section below.\n\nReferences:"
    }
}
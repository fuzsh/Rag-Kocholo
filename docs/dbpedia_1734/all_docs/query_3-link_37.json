{
    "id": "dbpedia_1734_3",
    "rank": 37,
    "data": {
        "url": "https://www.digitalocean.com/community/tutorials/how-to-install-apache-kafka-on-ubuntu-20-04",
        "read_more_link": "",
        "language": "en",
        "title": "How To Install Apache Kafka on Ubuntu 20.04",
        "top_image": "https://www.digitalocean.com/_next/static/media/intro-to-cloud.d49bc5f7.jpeg",
        "meta_img": "https://www.digitalocean.com/_next/static/media/intro-to-cloud.d49bc5f7.jpeg",
        "images": [
            "https://www.gravatar.com/avatar/7d530ff1c22b37792670b2918fce9cfbebe3a369dba1eed4a1763da6b8e24c91?default=retro 1x, https://www.gravatar.com/avatar/7d530ff1c22b37792670b2918fce9cfbebe3a369dba1eed4a1763da6b8e24c91?default=retro 2x",
            "https://www.gravatar.com/avatar/4ebdfe622e5b076e6700be9e3e037655a519b88e483c4f5cb88d7a6b635bfc7a?default=retro 1x, https://www.gravatar.com/avatar/4ebdfe622e5b076e6700be9e3e037655a519b88e483c4f5cb88d7a6b635bfc7a?default=retro 2x",
            "https://www.digitalocean.com/api/static-content/v1/images?src=%2F_next%2Fstatic%2Fmedia%2Fintro-to-cloud.d49bc5f7.jpeg&width=828 1x, https://www.digitalocean.com/api/static-content/v1/images?src=%2F_next%2Fstatic%2Fmedia%2Fintro-to-cloud.d49bc5f7.jpeg&width=1920 2x",
            "https://www.digitalocean.com/api/static-content/v1/images?src=%2F_next%2Fstatic%2Fmedia%2Fdefault-avatar.14b0d31d.jpeg&width=64 1x, https://www.digitalocean.com/api/static-content/v1/images?src=%2F_next%2Fstatic%2Fmedia%2Fdefault-avatar.14b0d31d.jpeg&width=128 2x",
            "https://www.digitalocean.com/api/static-content/v1/images?src=%2F_next%2Fstatic%2Fmedia%2Fdefault-avatar.14b0d31d.jpeg&width=64 1x, https://www.digitalocean.com/api/static-content/v1/images?src=%2F_next%2Fstatic%2Fmedia%2Fdefault-avatar.14b0d31d.jpeg&width=128 2x",
            "https://www.digitalocean.com/api/static-content/v1/images?src=%2F_next%2Fstatic%2Fmedia%2Fcreativecommons.c0a877f1.png&width=128 1x, https://www.digitalocean.com/api/static-content/v1/images?src=%2F_next%2Fstatic%2Fmedia%2Fcreativecommons.c0a877f1.png&width=256 2x",
            "https://www.digitalocean.com/api/static-content/v1/images?src=%2F_next%2Fstatic%2Fmedia%2FnewsLetter.c1eb26e3.jpeg&width=750 1x, https://www.digitalocean.com/api/static-content/v1/images?src=%2F_next%2Fstatic%2Fmedia%2FnewsLetter.c1eb26e3.jpeg&width=1920 2x",
            "https://www.digitalocean.com/api/static-content/v1/images?src=%2F_next%2Fstatic%2Fmedia%2FhollieHub4Good.90285377.jpeg&width=750 1x, https://www.digitalocean.com/api/static-content/v1/images?src=%2F_next%2Fstatic%2Fmedia%2FhollieHub4Good.90285377.jpeg&width=1920 2x",
            "https://www.digitalocean.com/api/static-content/v1/images?src=%2F_next%2Fstatic%2Fmedia%2Fwrite4DO.6a167f0c.jpeg&width=750 1x, https://www.digitalocean.com/api/static-content/v1/images?src=%2F_next%2Fstatic%2Fmedia%2Fwrite4DO.6a167f0c.jpeg&width=1920 2x",
            "https://www.digitalocean.com/api/static-content/v1/images?src=%2F_next%2Fstatic%2Fmedia%2Fconsole-static.1b14cc64.svg&width=640 1x, https://www.digitalocean.com/api/static-content/v1/images?src=%2F_next%2Fstatic%2Fmedia%2Fconsole-static.1b14cc64.svg&width=1080 2x"
        ],
        "movies": [],
        "keywords": [],
        "meta_keywords": [
            ""
        ],
        "tags": null,
        "authors": [],
        "publish_date": null,
        "summary": "",
        "meta_description": "Apache Kafka is a popular distributed message broker designed to handle large volumes of real-time data. In this tutorial, you will install and use Apache Kaâ¦",
        "meta_lang": "en",
        "meta_favicon": "/_next/static/media/apple-touch-icon.d7edaa01.png",
        "meta_site_name": "",
        "canonical_link": "https://www.digitalocean.com/community/tutorials/how-to-install-apache-kafka-on-ubuntu-20-04",
        "text": "The author selected the Free and Open Source Fund to receive a donation as part of the Write for DOnations program.\n\nIntroduction\n\nApache Kafka is a popular distributed message broker designed to handle large volumes of real-time data. A Kafka cluster is highly scalable and fault-tolerant. It also has a much higher throughput compared to other message brokers like ActiveMQ and RabbitMQ. Though it is generally used as a publish/subscribe messaging system, many organizations also use it for log aggregation because it offers persistent storage for published messages.\n\nA publish/subscribe messaging system allows one or more producers to publish messages without considering the number of consumers or how they will process the messages. Subscribed clients are notified automatically about updates and the creation of new messages. This system is more efficient and scalable than systems where clients poll periodically to determine if new messages are available.\n\nIn this tutorial, you will install and configure Apache Kafka 2.8.2 on Ubuntu 20.04.\n\nPrerequisites\n\nTo follow along, you will need:\n\nAn Ubuntu 20.04 server with at least 4 GB of RAM and a non-root user with sudo privileges. You can set this up by following our Initial Server Setup guide if you do not have a non-root user set up. Installations with less than 4GB of RAM may cause the Kafka service to fail.\n\nOpenJDK 11 installed on your server. To install this version, follow our tutorial on How To Install Java with APT on Ubuntu 20.04. Kafka is written in Java, so it requires a JVM.\n\nStep 1 â Creating a User for Kafka\n\nBecause Kafka can handle requests over a network, your first step is to create a dedicated user for the service. This minimizes damage to your Ubuntu machine in the event that someone compromises the Kafka server. You will create a dedicated kafka user in this step.\n\nLog in to your server as your non-root sudo user, then create a user called kafka:\n\nsudo adduser kafka\n\nFollow the prompts to set a password and create the kafka user.\n\nNext, add the kafka user to the sudo group with the adduser command. You need these privileges to install Kafkaâs dependencies:\n\nsudo adduser kafka sudo\n\nYour kafka user is now ready. Log in to the kafka account using su:\n\nsu -l kafka\n\nNow that youâve created a Kafka-specific user, you are ready to download and extract the Kafka binaries.\n\nStep 2 â Downloading and Extracting the Kafka Binaries\n\nIn this step, youâll download and extract the Kafka binaries into dedicated folders in your kafka userâs home directory.\n\nTo start, create a directory in /home/kafka called Downloads to store your downloads:\n\nmkdir ~/Downloads\n\nUse curl to download the Kafka binaries:\n\ncurl \"https://downloads.apache.org/kafka/2.8.2/kafka_2.13-2.8.2.tgz\" -o ~/Downloads/kafka.tgz\n\nCreate a directory called kafka and move to this directory. Youâll use this directory as the base directory of the Kafka installation:\n\nmkdir ~/kafka && cd ~/kafka\n\nExtract the archive you downloaded using the tar command:\n\ntar -xvzf ~/Downloads/kafka.tgz --strip 1\n\nYou specify the --strip 1 flag to ensure that the archiveâs contents are extracted in ~/kafka/ itself and not in another directory (such as ~/kafka/kafka_2.13-2.8.2/) inside of it.\n\nNow that youâve downloaded and extracted the binaries successfully, you can start configuring your Kafka server.\n\nStep 3 â Configuring the Kafka Server\n\nA Kafka topic is the category, group, or feed name to which messages can be published. However, Kafkaâs default behavior will not allow you to delete a topic. To modify this, you must edit the configuration file, which you will do in this step.\n\nKafkaâs configuration options are specified in server.properties. Open this file with nano or your favorite editor:\n\nnano ~/kafka/config/server.properties\n\nFirst, add a setting that will allow you to delete Kafka topics. Add the following line to the bottom of the file:\n\n~/kafka/config/server.properties\n\ndelete.topic.enable = true\n\nSecond, youâll change the directory where the Kafka logs are stored by modifying the log.dirs property. Find the log.dirs property and replace the existing route with the highlighted route:\n\n~/kafka/config/server.properties\n\nlog.dirs=/home/kafka/logs\n\nSave and close the file.\n\nNow that youâve configured Kafka, you can create systemd unit files for running and enabling the Kafka server on startup.\n\nStep 4 â Creating systemd Unit Files and Starting the Kafka Server\n\nIn this section, you will create systemd unit files for the Kafka service. These files will help you perform common service actions such as starting, stopping, and restarting Kafka in a manner consistent with other Linux services.\n\nKafka uses Zookeeper to manage its cluster state and configurations. It is used in many distributed systems, and you can read more about the tool in the official Zookeeper docs. Youâll use Zookeper as a service with these unit files.\n\nCreate the unit file for zookeeper:\n\nsudo nano /etc/systemd/system/zookeeper.service\n\nEnter the following unit definition into the file:\n\n/etc/systemd/system/zookeeper.service\n\n[Unit] Requires=network.target remote-fs.target After=network.target remote-fs.target [Service] Type=simple User=kafka ExecStart=/home/kafka/kafka/bin/zookeeper-server-start.sh /home/kafka/kafka/config/zookeeper.properties ExecStop=/home/kafka/kafka/bin/zookeeper-server-stop.sh Restart=on-abnormal [Install] WantedBy=multi-user.target\n\nThe [Unit] section specifies that Zookeeper requires networking and the filesystem to be ready before it can start.\n\nThe [Service] section specifies that systemd should use the zookeeper-server-start.sh and zookeeper-server-stop.sh shell files for starting and stopping the service. It also specifies that Zookeeper should be restarted if it exits abnormally.\n\nAfter adding this content, save and close the file.\n\nNext, create the systemd service file for kafka:\n\nsudo nano /etc/systemd/system/kafka.service\n\nEnter the following unit definition into the file:\n\n/etc/systemd/system/kafka.service\n\n[Unit] Requires=zookeeper.service After=zookeeper.service [Service] Type=simple User=kafka ExecStart=/bin/sh -c '/home/kafka/kafka/bin/kafka-server-start.sh /home/kafka/kafka/config/server.properties > /home/kafka/kafka/kafka.log 2>&1' ExecStop=/home/kafka/kafka/bin/kafka-server-stop.sh Restart=on-abnormal [Install] WantedBy=multi-user.target\n\nThe [Unit] section specifies that this unit file depends on zookeeper.service, which will ensure that zookeeper gets started automatically when the kafka service starts.\n\nThe [Service] section specifies that systemd should use the kafka-server-start.sh and kafka-server-stop.sh shell files for starting and stopping the service. It also specifies that Kafka should be restarted if it exits abnormally.\n\nSave and close the file.\n\nNow that you have defined the units, start Kafka with the following command:\n\nsudo systemctl start kafka\n\nTo ensure that the server has started successfully, check the journal logs for the kafka unit:\n\nsudo systemctl status kafka\n\nYou will receive output like this:\n\nOutput\n\nâ kafka.service Loaded: loaded (/etc/systemd/system/kafka.service; disabled; vendor preset> Active: active (running) since Wed 2023-02-01 23:44:12 UTC; 4s ago Main PID: 17770 (sh) Tasks: 69 (limit: 4677) Memory: 321.9M CGroup: /system.slice/kafka.service ââ17770 /bin/sh -c /home/kafka/kafka/bin/kafka-server-start.sh /ho> ââ17793 java -Xmx1G -Xms1G -server -XX:+UseG1GC -XX:MaxGCPauseMill>\n\nYou now have a Kafka server listening on port 9092, which is the default port the Kafka server uses.\n\nYou have started the kafka service. But if you reboot your server, Kafka will not restart automatically. To enable the kafka service on server boot, run the following command:\n\nsudo systemctl enable zookeeper\n\nYouâll receive a response that a symlink was created:\n\nOutput\n\nCreated symlink /etc/systemd/system/multi-user.target.wants/zookeeper.service â /etc/systemd/system/zookeeper.service.\n\nThen run this command:\n\nsudo systemctl enable kafka\n\nYouâll receive a response that a symlink was created:\n\nOutput\n\nCreated symlink /etc/systemd/system/multi-user.target.wants/kafka.service â /etc/systemd/system/kafka.service.\n\nIn this step, you started and enabled the kafka and zookeeper services. In the next step, you will check the Kafka installation.\n\nStep 5 â Testing the Kafka Installation\n\nIn this step, you will test your Kafka installation. You will publish and consume a Hello World message to make sure the Kafka server is behaving as expected.\n\nPublishing messages in Kafka requires:\n\nA producer, who enables the publication of records and data to topics.\n\nA consumer, who reads messages and data from topics.\n\nTo begin, create a topic named TutorialTopic:\n\n~/kafka/bin/kafka-topics.sh --create --zookeeper localhost:2181 --replication-factor 1 --partitions 1 --topic TutorialTopic\n\nYou can create a producer from the command line using the kafka-console-producer.sh script. It expects the Kafka serverâs hostname, a port, and a topic as arguments.\n\nYouâll receive a response that the topic was created:\n\nOutput\n\nCreated topic TutorialTopic.\n\nNow publish the string \"Hello, World\" to the TutorialTopic topic:\n\necho \"Hello, World\" | ~/kafka/bin/kafka-console-producer.sh --broker-list localhost:9092 --topic TutorialTopic > /dev/null\n\nNext, create a Kafka consumer using the kafka-console-consumer.sh script. It expects the ZooKeeper serverâs hostname and port, along with a topic name, as arguments. The following command consumes messages from TutorialTopic. Note the use of the --from-beginning flag, which allows the consumption of messages that were published before the consumer was started:\n\n~/kafka/bin/kafka-console-consumer.sh --bootstrap-server localhost:9092 --topic TutorialTopic --from-beginning\n\nIf there are no configuration issues, you will receive a Hello, World response in your terminal:\n\nOutput\n\nHello, World\n\nThe script will continue to run, waiting for more messages to publish. To test this, open a new terminal window and log in to your server. Remember to log in as your kafka user:\n\nsu -l kafka\n\nIn this new terminal, start a producer to publish a second message:\n\necho \"Hello World from Sammy at DigitalOcean!\" | ~/kafka/bin/kafka-console-producer.sh --broker-list localhost:9092 --topic TutorialTopic > /dev/null\n\nThis message will load in the consumerâs output in your original terminal:\n\nOutput\n\nHello, World Hello World from Sammy at DigitalOcean!\n\nWhen you are done testing, press CTRL+C to stop the consumer script in your original terminal.\n\nYou have now installed and configured a Kafka server on Ubuntu 20.04. In the next step, you will perform a few quick tasks to harden the security of your Kafka server.\n\nStep 6 â Hardening the Kafka Server\n\nWith your installation complete, you can remove the kafka userâs admin privileges and harden the Kafka server.\n\nBefore you do so, log out and log back in as any other non-root sudo user. If you are still running the same shell session that you started this tutorial with, type exit.\n\nRemove the kafka user from the sudo group:\n\nsudo deluser kafka sudo\n\nTo further improve your Kafka serverâs security, lock the kafka userâs password using the passwd command. This action ensures that nobody can directly log into the server using this account:\n\nsudo passwd kafka -l\n\nThe -l flag locks the command to change a userâs password (passwd).\n\nAt this point, only root or a sudo user can log in as kafka with the following command:\n\nsudo su - kafka\n\nIn the future, if you want to unlock the ability to change the password, use passwd with the -u option:\n\nsudo passwd kafka -u\n\nYou have now successfully restricted the kafka userâs admin privileges. You are ready to begin using Kafka. You can optionally follow the next step, which will add KafkaT to your system.\n\nStep 7 â Installing KafkaT (Optional)\n\nKafkaT was developed to improve your ability to view details about your Kafka cluster and to perform certain administrative tasks from the command line. Because it is a Ruby gem, you will need Ruby to use it. You will also need the build-essential package to build the other gems that KafkaT depends on.\n\nInstall Ruby and the build-essential package using apt:\n\nsudo apt install ruby ruby-dev build-essential\n\nYou can now install KafkaT with the gem command:\n\nsudo CFLAGS=-Wno-error=format-overflow gem install kafkat\n\nThe Wno-error=format-overflow compilation flag is required to suppress Zookeeperâs warnings and errors during kafkatâs installation process.\n\nWhen the installation has finished, youâll receive a response that it is done:\n\nOutput\n\n... Done installing documentation for json, colored, retryable, highline, trollop, zookeeper, zk, kafkat after 3 seconds 8 gems installed\n\nKafkaT uses .kafkatcfg as the configuration file to determine the installation and log directories of your Kafka server. It should also have an entry pointing KafkaT to your ZooKeeper instance.\n\nCreate a new file called .kafkatcfg:\n\nnano ~/.kafkatcfg\n\nAdd the following lines to specify the required information about your Kafka server and Zookeeper instance:\n\n~/.kafkatcfg\n\n{ \"kafka_path\": \"~/kafka\", \"log_path\": \"/home/kafka/logs\", \"zk_path\": \"localhost:2181\" }\n\nSave and close the file. You are now ready to use KafkaT.\n\nTo view details about all Kafka partitions, try running this command:\n\nkafkat partitions\n\nYou will receive the following output:\n\nOutput\n\n[DEPRECATION] The trollop gem has been renamed to optimist and will no longer be supported. Please switch to optimist as soon as possible. /var/lib/gems/2.7.0/gems/json-1.8.6/lib/json/common.rb:155: warning: Using the last argument as keyword parameters is deprecated ... Topic Partition Leader Replicas ISRs TutorialTopic 0 0 [0] [0] __consumer_offsets 0 0 [0] [0] ... ...\n\nThe output will include TutorialTopic and __consumer_offsets, an internal topic used by Kafka for storing client-related information. You can safely ignore lines starting with __consumer_offsets.\n\nTo learn more about KafkaT, refer to its GitHub repository.\n\nConclusion\n\nYou now have Apache Kafka running securely on your Ubuntu server. You can integrate Kafka into your favorite programming language using Kafka clients."
    }
}
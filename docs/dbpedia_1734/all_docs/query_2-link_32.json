{
    "id": "dbpedia_1734_2",
    "rank": 32,
    "data": {
        "url": "https://www.academia.edu/97632039/Session_Effects_on_Speaker_Modeling",
        "read_more_link": "",
        "language": "en",
        "title": "Session Effects on Speaker Modeling",
        "top_image": "http://a.academia-assets.com/images/open-graph-icons/fb-paper.gif",
        "meta_img": "http://a.academia-assets.com/images/open-graph-icons/fb-paper.gif",
        "images": [
            "https://a.academia-assets.com/images/academia-logo-redesign-2015-A.svg",
            "https://a.academia-assets.com/images/academia-logo-redesign-2015.svg",
            "https://a.academia-assets.com/images/single_work_splash/adobe.icon.svg",
            "https://0.academia-photos.com/attachment_thumbnails/99201349/mini_magick20230227-1-fg3m18.png?1677520120",
            "https://0.academia-photos.com/57612908/28151993/26369524/s65_driss.matrouf.jpg",
            "https://a.academia-assets.com/images/loswp/related-pdf-icon.png",
            "https://a.academia-assets.com/images/loswp/related-pdf-icon.png",
            "https://a.academia-assets.com/images/loswp/related-pdf-icon.png",
            "https://a.academia-assets.com/images/loswp/related-pdf-icon.png",
            "https://a.academia-assets.com/images/loswp/related-pdf-icon.png",
            "https://a.academia-assets.com/images/loswp/related-pdf-icon.png",
            "https://a.academia-assets.com/images/loswp/related-pdf-icon.png",
            "https://a.academia-assets.com/images/loswp/related-pdf-icon.png",
            "https://a.academia-assets.com/images/loswp/related-pdf-icon.png",
            "https://a.academia-assets.com/images/loswp/related-pdf-icon.png",
            "https://a.academia-assets.com/images/loaders/paper-load.gif",
            "https://a.academia-assets.com/images/loswp/related-pdf-icon.png",
            "https://a.academia-assets.com/images/loswp/related-pdf-icon.png",
            "https://a.academia-assets.com/images/loswp/related-pdf-icon.png",
            "https://a.academia-assets.com/images/loswp/related-pdf-icon.png",
            "https://a.academia-assets.com/images/loswp/related-pdf-icon.png",
            "https://a.academia-assets.com/images/loswp/related-pdf-icon.png",
            "https://a.academia-assets.com/images/loswp/related-pdf-icon.png",
            "https://a.academia-assets.com/images/loswp/related-pdf-icon.png",
            "https://a.academia-assets.com/images/loswp/related-pdf-icon.png",
            "https://a.academia-assets.com/images/loswp/related-pdf-icon.png",
            "https://a.academia-assets.com/images/loswp/related-pdf-icon.png",
            "https://a.academia-assets.com/images/loswp/related-pdf-icon.png",
            "https://a.academia-assets.com/images/loswp/related-pdf-icon.png",
            "https://a.academia-assets.com/images/loswp/related-pdf-icon.png",
            "https://a.academia-assets.com/images/loswp/related-pdf-icon.png",
            "https://a.academia-assets.com/images/loswp/related-pdf-icon.png",
            "https://a.academia-assets.com/images/loswp/related-pdf-icon.png",
            "https://a.academia-assets.com/images/loswp/related-pdf-icon.png",
            "https://a.academia-assets.com/images/loswp/related-pdf-icon.png",
            "https://a.academia-assets.com/images/loswp/related-pdf-icon.png",
            "https://a.academia-assets.com/images/loswp/related-pdf-icon.png",
            "https://a.academia-assets.com/images/loswp/related-pdf-icon.png"
        ],
        "movies": [],
        "keywords": [],
        "meta_keywords": [
            ""
        ],
        "tags": null,
        "authors": [
            "driss matrouf",
            "independent.academia.edu"
        ],
        "publish_date": "2023-02-27T00:00:00",
        "summary": "",
        "meta_description": "Session Effects on Speaker Modeling",
        "meta_lang": "",
        "meta_favicon": "",
        "meta_site_name": "",
        "canonical_link": "https://www.academia.edu/97632039/Session_Effects_on_Speaker_Modeling",
        "text": "Information fusion at the matching score level is widely used, due to the simplicity in combining the scores generated by different matchers. Since the matching scores output by various modalities are diverse in numerical range, score normalization is needed first, to transform these scores into a common domain. Then score fusion is to be carried out on the normalized scores. In this paper, we have studied the performance of different normalization techniques and fusion rules in the context of a multimodal biometric system based on iris and palm print traits of a user. The conventional normalization techniques used for testing are min-max, median-MAD, double-sigmoid and tanh. These normalized results are combined using tanh, mean, sum, product, min, max and median fusion methods. Also, we propose two novel normalization methods namely modified tanh normalization and max normalization as well as a new modified min-max fusion technique for biometric verification. The experimental resu...\n\nBiometric systems use score normalization techniques and fusion rules to improve recognition performance. The large amount of research on score fusion for multimodal systems raises an important question: can we utilize the available information from unimodal systems more effectively? In this paper, we present a rank-based score normalization framework that addresses this problem. Specifically, our approach consists of three algorithms: 1) partition the matching scores into subsets and normalize each subset independently; 2) utilize the gallery versus gallery matching scores matrix (i.e., gallery-based information); and 3) dynamically augment the gallery in an online fashion. We invoke the theory of stochastic dominance along with results of prior research to demonstrate when and why our approach yields increased performance. Our framework: 1) can be used in conjunction with any score normalization technique and any fusion rule; 2) is amenable to parallel programming; and 3) is suitable for both verification and open-set identification. To assess the performance of our framework, we use the UHDB11 and FRGC v2 face datasets. Specifically, the statistical hypothesis tests performed illustrate that the performance of our framework improves as we increase the number of samples per subject. Furthermore, the corresponding statistical analysis demonstrates that increased separation between match and non-match scores is obtained for each probe. Besides the benefits and limitations highlighted by our experimental evaluation, results under optimal and pessimal conditions are also presented to offer better insights.\n\nguidance, help and support given throughout the course of the project. I would also like to express my greatest gratitude to Dr. T. Stathaki for her help, guid-ance and support throughout this year at Imperial College. Most of all, I would like to thank my family for their great support during this year at Imperial College and throughout my five years of undergraduate study at the Aristotle University of Thessaloniki, Greece. Biometric identification methods are trying to replace traditional identification meth-ods such as PIN numbers, identity cards etc. One of the common biometrics that can be used to identify a person&apos;s identity is speech. Speaker verification systems accept or reject the identity claim of a speaker by comparing a set of measurements of his speech with a reference set of measurements of the speech of the person whose identity is claimed. Many speaker verification systems were proposed and developed in the last decade with good performance. The basic aim of t..."
    }
}
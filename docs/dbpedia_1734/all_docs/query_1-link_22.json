{
    "id": "dbpedia_1734_1",
    "rank": 22,
    "data": {
        "url": "https://docs.snowflake.com/en/user-guide/kafka-connector-overview",
        "read_more_link": "",
        "language": "en",
        "title": "Overview of the Kafka connector ¶",
        "top_image": "",
        "meta_img": "",
        "images": [
            "https://docs.snowflake.com/en/_images/kafka-connector-flow.png"
        ],
        "movies": [],
        "keywords": [],
        "meta_keywords": [
            ""
        ],
        "tags": null,
        "authors": [],
        "publish_date": null,
        "summary": "",
        "meta_description": "",
        "meta_lang": "en",
        "meta_favicon": "data:image/svg+xml;base64,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",
        "meta_site_name": "",
        "canonical_link": "https://docs.snowflake.com/en/user-guide/kafka-connector-overview",
        "text": "Overview of the Kafka connector¶\n\nThis topic provides an overview of the Apache Kafka and the Snowflake Connector for Kafka.\n\nIntroduction to Apache Kafka¶\n\nApache Kafka software uses a publish and subscribe model to write and read streams of records, similar to a message queue or enterprise messaging system. Kafka allows processes to read and write messages asynchronously. A subscriber does not need to be connected directly to a publisher; a publisher can queue a message in Kafka for the subscriber to receive later.\n\nAn application publishes messages to a topic, and an application subscribes to a topic to receive those messages. Kafka can process, as well as transmit, messages; however, that is outside the scope of this document. Topics can be divided into partitions to increase scalability.\n\nKafka Connect is a framework for connecting Kafka with external systems, including databases. A Kafka Connect cluster is a separate cluster from the Kafka cluster. The Kafka Connect cluster supports running and scaling out connectors (components that support reading and/or writing between external systems).\n\nThe Kafka connector is designed to run in a Kafka Connect cluster to read data from Kafka topics and write the data into Snowflake tables.\n\nSnowflake provides two versions of the connector:\n\nA version for the Confluent package version of Kafka.\n\nFor more information about Kafka Connect, see https://docs.confluent.io/current/connect/.\n\nNote\n\nA hosted version of the Kafka connector is available in Confluent Cloud. For information, see https://docs.confluent.io/current/cloud/connectors/cc-snowflake-sink.html.\n\nA version for the open source software (OSS) Apache Kafka package.\n\nFor more information about Apache Kafka, see https://kafka.apache.org/.\n\nFrom the perspective of Snowflake, a Kafka topic produces a stream of rows to be inserted into a Snowflake table. In general, each Kafka message contains one row.\n\nKafka, like many message publish/subscribe platforms, allows a many-to-many relationship between publishers and subscribers. A single application can publish to many topics, and a single application can subscribe to multiple topics. With Snowflake, the typical pattern is that one topic supplies messages (rows) for one Snowflake table.\n\nThe current version of the Kafka connector is limited to loading data into Snowflake. The Kafka connector supports two data loading methods:\n\nSnowpipe\n\nSnowpipe Streaming.\n\nFor more information, refer to Load Data into Snowflake and Using Snowflake Connector for Kafka With Snowpipe Streaming.\n\nTarget tables for Kafka topics¶\n\nKafka topics can be mapped to existing Snowflake tables in the Kafka configuration. If the topics are not mapped, then the Kafka connector creates a new table for each topic using the topic name.\n\nThe connector converts the topic name to a valid Snowflake table name using the following rules:\n\nLowercase topic names are converted to uppercase table names.\n\nIf the first character in the topic name is not a letter (a-z, or A-Z) or an underscore character (_), then the connector prepends an underscore to the table name.\n\nIf any character inside the topic name is not a legal character for a Snowflake table name, then that character is replaced with the underscore character. For more information about which characters are valid in table names, see Identifier requirements.\n\nNote that if the Kafka connector needs to adjust the name of the table created for a Kafka topic, it is possible that the names of two tables in the same schema could be identical. For example, if you are reading data from topics numbers+x and numbers-x, the tables created for these topics would both be NUMBERS_X. To avoid accidental duplication of table names, the connector appends a suffix to the table name. The suffix is an underscore followed by a generated hash code.\n\nTip\n\nSnowflake recommends that, when possible, you choose topic names that follow the rules for Snowflake identifier names.\n\nSchema of tables for Kafka topics¶\n\nWith Snowpipe Streaming, the Kafka connector optionally supports schema detection and evolution.\n\nBy default, with Snowpipe or Snowpipe Streaming, every Snowflake table loaded by the Kafka connector has a schema consisting of two VARIANT columns:\n\nRECORD_CONTENT. This contains the Kafka message.\n\nRECORD_METADATA. This contains metadata about the message, for example, the topic from which the message was read.\n\nIf Snowflake creates the table, then the table contains only these two columns. If the user creates the table for the Kafka Connector to add rows to, then the table can contain more than these two columns (any additional columns must allow NULL values because data from the connector does not include values for those columns).\n\nThe RECORD_CONTENT column contains the Kafka message.\n\nA Kafka message has an internal structure that depends upon the information being sent. For example, a message from an IoT (Internet of Things) weather sensor might include the timestamp at which the data was recorded, the location of the sensor, the temperature, humidity, etc. A message from an inventory system might include the product ID and the number of items sold, perhaps along with a timestamp indicating when they were sold or shipped.\n\nTypically, each message in a specific topic has the same basic structure. Different topics typically use different structure.\n\nEach Kafka message is passed to Snowflake in JSON format or Avro format. The Kafka connector stores that formatted information in a single column of type VARIANT. The data is not parsed, and the data is not split into multiple columns in the Snowflake table.\n\nThe RECORD_METADATA column contains the following information by default:\n\nField\n\nJava . Data Type\n\nSQL . Data Type\n\nRequired\n\nDescription\n\nThe amount of metadata recorded in the RECORD_METADATA column is configurable using optional Kafka configuration properties. For information, see Installing and configuring the Kafka connector.\n\nThe field names and values are case-sensitive.\n\nExpressed in JSON syntax, a sample message might look similar to the following:\n\n{ \"meta\": { \"offset\": 1, \"topic\": \"PressureOverloadWarning\", \"partition\": 12, \"key\": \"key name\", \"schema_id\": 123, \"CreateTime\": 1234567890, \"headers\": { \"name1\": \"value1\", \"name2\": \"value2\" } }, \"content\": { \"ID\": 62, \"PSI\": 451, \"etc\": \"...\" } }\n\nCopy\n\nYou can query the Snowflake tables directly by using the appropriate syntax for querying VARIANT columns.\n\nHere is a simple example of extracting data based on the topic in the RECORD_METADATA:\n\nselect record_metadata:CreateTime, record_content:ID from table1 where record_metadata:topic = 'PressureOverloadWarning';\n\nCopy\n\nThe output would look similar to:\n\n+------------+-----+ | CREATETIME | ID | +------------+-----+ | 1234567890 | 62 | +------------+-----+\n\nCopy\n\nAlternatively, you can extract the data from these tables, flatten the data into individual columns, and store the data in other tables, which typically are easier to query.\n\nWorkflow for the Kafka connector¶\n\nThe Kafka connector completes the following process to subscribe to Kafka topics and create Snowflake objects:\n\nThe Kafka connector subscribes to one or more Kafka topics based on the configuration information provided via the Kafka configuration file or command line (or the Confluent Control Center; Confluent only).\n\nThe connector creates the following objects for each topic:\n\nOne internal stage to temporarily store data files for each topic.\n\nOne pipe to ingest the data files for each topic partition.\n\nOne table for each topic. If the table specified for each topic does not exist, the connector creates it; otherwise, the connector creates the RECORD_CONTENT and RECORD_METADATA columns in the existing table and verifies that the other columns are nullable (and produces an error if they are not).\n\nThe following diagram shows the ingest flow for Kafka with the Kafka connector:\n\nOne or more applications publish JSON or Avro records to a Kafka cluster. The records are split into one or more topic partitions.\n\nThe Kafka connector buffers messages from the Kafka topics. When a threshold (time or memory or number of messages) is reached, the connector writes the messages to a temporary file in the internal stage. The connector triggers Snowpipe to ingest the temporary file. Snowpipe copies a pointer to the data file into a queue.\n\nA Snowflake-provided virtual warehouse loads data from the staged file into the target table (i.e. the table specified in the configuration file for the topic) via the pipe created for the Kafka topic partition.\n\n(Not shown) The connector monitors Snowpipe and deletes each file in the internal stage after confirming that the file data was loaded into the table.\n\nIf a failure prevented the data from loading, the connector moves the file into the table stage and produces an error message.\n\nThe connector repeats steps 2-4.\n\nAttention\n\nSnowflake polls the insertReport API for one hour. If the status of an ingested file does not succeed within this hour, the files being ingested are moved to a table stage.\n\nIt may take at least one hour for these files to be available on the table stage. Files are only moved to the table stage when their ingestion status could not be found within the previous hour.\n\nFault tolerance¶\n\nBoth Kafka and the Kafka connector are fault-tolerant. Messages are neither duplicated nor silently dropped.\n\nData deduplication logic in the Snowpipe workflow in the data loading chain eliminates duplicate copies of repeating data except in rare cases. If an error is detected while Snowpipe loads a record (for example, the record was not well-formed JSON or Avro), then the record is not loaded; instead, the record is moved to a table stage.\n\nThe Kafka connector with Snowpipe Streaming supports dead-letter queues (DLQ) for error handling. For more information, refer to Error Handling and DLQ Properties for the Kafka Connector with Snowpipe Streaming.\n\nLimitations of fault tolerance with the connector¶\n\nKafka Topics can be configured with a limit on storage space or retention time.\n\nThe default retention time is 7 days. If the system is offline for more than the retention time, then expired records will not be loaded. Similarly, if Kafka’s storage space limit is exceeded, some messages will not be delivered.\n\nIf messages in the Kafka topic are deleted or updated, these changes might not be reflected in the Snowflake table.\n\nAttention\n\nInstances of the Kafka connector do not communicate with each other. If you start multiple instances of the connector on the same topics or partitions, then multiple copies of the same row might be inserted into the table. This is not recommended; each topic should be processed by only one instance of the connector.\n\nIt is theoretically possible for messages to flow from Kafka faster than Snowflake can ingest them. In practice, however, this is unlikely. If it does occur, then solving the problem would require performance tuning of the Kafka Connect cluster. For example:\n\nTuning the number of nodes in the Connect cluster.\n\nTuning the number of tasks allocated to the connector.\n\nUnderstanding the impact of the network bandwidth between the connector and the Snowflake deployment.\n\nImportant\n\nThere is no guarantee that rows are inserted in the order that they were originally published.\n\nSupported platforms¶\n\nThe Kafka connector can run in any Kafka Connect cluster, and can send data to a Snowflake account on any supported cloud platform.\n\nProtobuf data support¶\n\nKafka connector 1.5.0 (or higher) supports protocol buffers (protobuf) via a protobuf converter. For details, see Loading protobuf data using the Snowflake Connector for Kafka.\n\nBilling information¶\n\nThere is no direct charge for using the Kafka connector. However, there are indirect costs:\n\nSnowpipe is used to load the data that the connector reads from Kafka, and Snowpipe processing time is charged to your account.\n\nData storage is charged to your account.\n\nKafka connector limitations¶\n\nSingle Message Transformations (SMTs) are applied to messages as they flow through Kafka Connect. When you configure the Kafka configuration properties, if you set either key.converter or value.converter to one of the following values, then SMTs are not supported on the corresponding key or value:\n\ncom.snowflake.kafka.connector.records.SnowflakeJsonConverter\n\ncom.snowflake.kafka.connector.records.SnowflakeAvroConverter\n\ncom.snowflake.kafka.connector.records.SnowflakeAvroConverterWithoutSchemaRegistry\n\nWhen neither key.converter or value.converter is set, then most SMTs are supported, with the current exception of regex.router.\n\nAlthough the Snowflake converters do not support SMTs, Kafka connector version 1.4.3 (or higher) supports many community-based converters such as the following:\n\nio.confluent.connect.avro.AvroConverter\n\norg.apache.kafka.connect.json.JsonConverter\n\nFor more information about SMTs, see https://docs.confluent.io/current/connect/transforms/index.html."
    }
}
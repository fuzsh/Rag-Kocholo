{
    "id": "dbpedia_1734_3",
    "rank": 29,
    "data": {
        "url": "https://docs.spring.io/spring-kafka/docs/2.7.14/reference/html/",
        "read_more_link": "",
        "language": "en",
        "title": "Spring for Apache Kafka",
        "top_image": "",
        "meta_img": "",
        "images": [],
        "movies": [],
        "keywords": [],
        "meta_keywords": [
            ""
        ],
        "tags": null,
        "authors": [
            "Gary Russell",
            "Artem Bilan",
            "Biju Kunjummen",
            "Jay Bryant",
            "Soby Chacko",
            "Tomaz Fernandes"
        ],
        "publish_date": null,
        "summary": "",
        "meta_description": "",
        "meta_lang": "en",
        "meta_favicon": "",
        "meta_site_name": "",
        "canonical_link": null,
        "text": "Using ReplyingKafkaTemplate\n\nVersion 2.1.3 introduced a subclass of KafkaTemplate to provide request/reply semantics. The class is named ReplyingKafkaTemplate and has two additional methods; the following shows the method signatures:\n\nRequestReplyFuture<K, V, R> sendAndReceive(ProducerRecord<K, V> record); RequestReplyFuture<K, V, R> sendAndReceive(ProducerRecord<K, V> record, Duration replyTimeout);\n\n(Also see Request/Reply with Message<?> s).\n\nThe result is a ListenableFuture that is asynchronously populated with the result (or an exception, for a timeout). The result also has a sendFuture property, which is the result of calling KafkaTemplate.send(). You can use this future to determine the result of the send operation.\n\nIf the first method is used, or the replyTimeout argument is null, the template’s defaultReplyTimeout property is used (5 seconds by default).\n\nThe following Spring Boot application shows an example of how to use the feature:\n\n@SpringBootApplication public class KRequestingApplication { public static void main(String[] args) { SpringApplication.run(KRequestingApplication.class, args).close(); } @Bean public ApplicationRunner runner(ReplyingKafkaTemplate<String, String, String> template) { return args -> { ProducerRecord<String, String> record = new ProducerRecord<>(\"kRequests\", \"foo\"); RequestReplyFuture<String, String, String> replyFuture = template.sendAndReceive(record); SendResult<String, String> sendResult = replyFuture.getSendFuture().get(10, TimeUnit.SECONDS); System.out.println(\"Sent ok: \" + sendResult.getRecordMetadata()); ConsumerRecord<String, String> consumerRecord = replyFuture.get(10, TimeUnit.SECONDS); System.out.println(\"Return value: \" + consumerRecord.value()); }; } @Bean public ReplyingKafkaTemplate<String, String, String> replyingTemplate( ProducerFactory<String, String> pf, ConcurrentMessageListenerContainer<String, String> repliesContainer) { return new ReplyingKafkaTemplate<>(pf, repliesContainer); } @Bean public ConcurrentMessageListenerContainer<String, String> repliesContainer( ConcurrentKafkaListenerContainerFactory<String, String> containerFactory) { ConcurrentMessageListenerContainer<String, String> repliesContainer = containerFactory.createContainer(\"replies\"); repliesContainer.getContainerProperties().setGroupId(\"repliesGroup\"); repliesContainer.setAutoStartup(false); return repliesContainer; } @Bean public NewTopic kRequests() { return TopicBuilder.name(\"kRequests\") .partitions(10) .replicas(2) .build(); } @Bean public NewTopic kReplies() { return TopicBuilder.name(\"kReplies\") .partitions(10) .replicas(2) .build(); } }\n\nNote that we can use Boot’s auto-configured container factory to create the reply container.\n\nIf a non-trivial deserializer is being used for replies, consider using an ErrorHandlingDeserializer that delegates to your configured deserializer. When so configured, the RequestReplyFuture will be completed exceptionally and you can catch the ExecutionException, with the DeserializationException in its cause property.\n\nStarting with version 2.6.7, in addition to detecting DeserializationException s, the template will call the replyErrorChecker function, if provided. If it returns an exception, the future will be completed exceptionally.\n\nHere is an example:\n\ntemplate.setReplyErrorChecker(record -> { Header error = record.headers().lastHeader(\"serverSentAnError\"); if (error != null) { return new MyException(new String(error.value())); } else { return null; } }); ... RequestReplyFuture<Integer, String, String> future = template.sendAndReceive(record); try { future.getSendFuture().get(10, TimeUnit.SECONDS); // send ok ConsumerRecord<Integer, String> consumerRecord = future.get(10, TimeUnit.SECONDS); ... } catch (InterruptedException e) { ... } catch (ExecutionException e) { if (e.getCause instanceof MyException) { ... } } catch (TimeoutException e) { ... }\n\nThe template sets a header (named KafkaHeaders.CORRELATION_ID by default), which must be echoed back by the server side.\n\nIn this case, the following @KafkaListener application responds:\n\n@SpringBootApplication public class KReplyingApplication { public static void main(String[] args) { SpringApplication.run(KReplyingApplication.class, args); } @KafkaListener(id=\"server\", topics = \"kRequests\") @SendTo // use default replyTo expression public String listen(String in) { System.out.println(\"Server received: \" + in); return in.toUpperCase(); } @Bean public NewTopic kRequests() { return TopicBuilder.name(\"kRequests\") .partitions(10) .replicas(2) .build(); } @Bean // not required if Jackson is on the classpath public MessagingMessageConverter simpleMapperConverter() { MessagingMessageConverter messagingMessageConverter = new MessagingMessageConverter(); messagingMessageConverter.setHeaderMapper(new SimpleKafkaHeaderMapper()); return messagingMessageConverter; } }\n\nThe @KafkaListener infrastructure echoes the correlation ID and determines the reply topic.\n\nSee Forwarding Listener Results using @SendTo for more information about sending replies. The template uses the default header KafKaHeaders.REPLY_TOPIC to indicate the topic to which the reply goes.\n\nStarting with version 2.2, the template tries to detect the reply topic or partition from the configured reply container. If the container is configured to listen to a single topic or a single TopicPartitionOffset, it is used to set the reply headers. If the container is configured otherwise, the user must set up the reply headers. In this case, an INFO log message is written during initialization. The following example uses KafkaHeaders.REPLY_TOPIC:\n\nrecord.headers().add(new RecordHeader(KafkaHeaders.REPLY_TOPIC, \"kReplies\".getBytes()));\n\nWhen you configure with a single reply TopicPartitionOffset, you can use the same reply topic for multiple templates, as long as each instance listens on a different partition. When configuring with a single reply topic, each instance must use a different group.id. In this case, all instances receive each reply, but only the instance that sent the request finds the correlation ID. This may be useful for auto-scaling, but with the overhead of additional network traffic and the small cost of discarding each unwanted reply. When you use this setting, we recommend that you set the template’s sharedReplyTopic to true, which reduces the logging level of unexpected replies to DEBUG instead of the default ERROR.\n\nThe following is an example of configuring the reply container to use the same shared reply topic:\n\n@Bean public ConcurrentMessageListenerContainer<String, String> replyContainer( ConcurrentKafkaListenerContainerFactory<String, String> containerFactory) { ConcurrentMessageListenerContainer<String, String> container = containerFactory.createContainer(\"topic2\"); container.getContainerProperties().setGroupId(UUID.randomUUID().toString()); // unique Properties props = new Properties(); props.setProperty(ConsumerConfig.AUTO_OFFSET_RESET_CONFIG, \"latest\"); // so the new group doesn't get old replies container.getContainerProperties().setKafkaConsumerProperties(props); return container; }\n\nIf you have multiple client instances and you do not configure them as discussed in the preceding paragraph, each instance needs a dedicated reply topic. An alternative is to set the KafkaHeaders.REPLY_PARTITION and use a dedicated partition for each instance. The Header contains a four-byte int (big-endian). The server must use this header to route the reply to the correct partition (@KafkaListener does this). In this case, though, the reply container must not use Kafka’s group management feature and must be configured to listen on a fixed partition (by using a TopicPartitionOffset in its ContainerProperties constructor).\n\nThe DefaultKafkaHeaderMapper requires Jackson to be on the classpath (for the @KafkaListener). If it is not available, the message converter has no header mapper, so you must configure a MessagingMessageConverter with a SimpleKafkaHeaderMapper, as shown earlier.\n\nBy default, 3 headers are used:\n\nKafkaHeaders.CORRELATION_ID - used to correlate the reply to a request\n\nKafkaHeaders.REPLY_TOPIC - used to tell the server where to reply\n\nKafkaHeaders.REPLY_PARTITION - (optional) used to tell the server which partition to reply to\n\nThese header names are used by the @KafkaListener infrastructure to route the reply.\n\nStarting with version 2.3, you can customize the header names - the template has 3 properties correlationHeaderName, replyTopicHeaderName, and replyPartitionHeaderName. This is useful if your server is not a Spring application (or does not use the @KafkaListener).\n\nRequest/Reply with Message<?> s\n\nVersion 2.7 added methods to the ReplyingKafkaTemplate to send and receive spring-messaging 's Message<?> abstraction:\n\nRequestReplyMessageFuture<K, V> sendAndReceive(Message<?> message); <P> RequestReplyTypedMessageFuture<K, V, P> sendAndReceive(Message<?> message, ParameterizedTypeReference<P> returnType);\n\nThese will use the template’s default replyTimeout, there are also overloaded versions that can take a timeout in the method call.\n\nUse the first method if the consumer’s Deserializer or the template’s MessageConverter can convert the payload without any additional information, either via configuration or type metadata in the reply message.\n\nUse the second method if you need to provide type information for the return type, to assist the message converter. This also allows the same template to receive different types, even if there is no type metadata in the replies, such as when the server side is not a Spring application. The following is an example of the latter:\n\nExample 6. Template Bean\n\nJava\n\n@Bean ReplyingKafkaTemplate<String, String, String> template( ProducerFactory<String, String> pf, ConcurrentKafkaListenerContainerFactory<String, String> factory) { ConcurrentMessageListenerContainer<String, String> replyContainer = factory.createContainer(\"replies\"); replyContainer.getContainerProperties().setGroupId(\"request.replies\"); ReplyingKafkaTemplate<String, String, String> template = new ReplyingKafkaTemplate<>(pf, replyContainer); template.setMessageConverter(new ByteArrayJsonMessageConverter()); template.setDefaultTopic(\"requests\"); return template; }\n\nKotlin\n\n@Bean fun template( pf: ProducerFactory<String?, String>?, factory: ConcurrentKafkaListenerContainerFactory<String?, String?> ): ReplyingKafkaTemplate<String?, String, String?> { val replyContainer = factory.createContainer(\"replies\") replyContainer.containerProperties.groupId = \"request.replies\" val template = ReplyingKafkaTemplate(pf, replyContainer) template.messageConverter = ByteArrayJsonMessageConverter() template.defaultTopic = \"requests\" return template }\n\nExample 7. Using the template\n\nJava\n\nRequestReplyTypedMessageFuture<String, String, Thing> future1 = template.sendAndReceive(MessageBuilder.withPayload(\"getAThing\").build(), new ParameterizedTypeReference<Thing>() { }); log.info(future1.getSendFuture().get(10, TimeUnit.SECONDS).getRecordMetadata().toString()); Thing thing = future1.get(10, TimeUnit.SECONDS).getPayload(); log.info(thing.toString()); RequestReplyTypedMessageFuture<String, String, List<Thing>> future2 = template.sendAndReceive(MessageBuilder.withPayload(\"getThings\").build(), new ParameterizedTypeReference<List<Thing>>() { }); log.info(future2.getSendFuture().get(10, TimeUnit.SECONDS).getRecordMetadata().toString()); List<Thing> things = future2.get(10, TimeUnit.SECONDS).getPayload(); things.forEach(thing1 -> log.info(thing1.toString()));\n\nKotlin\n\nval future1: RequestReplyTypedMessageFuture<String?, String?, Thing?>? = template.sendAndReceive(MessageBuilder.withPayload(\"getAThing\").build(), object : ParameterizedTypeReference<Thing?>() {}) log.info(future1?.sendFuture?.get(10, TimeUnit.SECONDS)?.recordMetadata?.toString()) val thing = future1?.get(10, TimeUnit.SECONDS)?.payload log.info(thing.toString()) val future2: RequestReplyTypedMessageFuture<String?, String?, List<Thing?>?>? = template.sendAndReceive(MessageBuilder.withPayload(\"getThings\").build(), object : ParameterizedTypeReference<List<Thing?>?>() {}) log.info(future2?.sendFuture?.get(10, TimeUnit.SECONDS)?.recordMetadata.toString()) val things = future2?.get(10, TimeUnit.SECONDS)?.payload things?.forEach(Consumer { thing1: Thing? -> log.info(thing1.toString()) })\n\nDelegating Serializer and Deserializer\n\nVersion 2.3 introduced the DelegatingSerializer and DelegatingDeserializer, which allow producing and consuming records with different key and/or value types. Producers must set a header DelegatingSerializer.VALUE_SERIALIZATION_SELECTOR to a selector value that is used to select which serializer to use for the value and DelegatingSerializer.KEY_SERIALIZATION_SELECTOR for the key; if a match is not found, an IllegalStateException is thrown.\n\nFor incoming records, the deserializer uses the same headers to select the deserializer to use; if a match is not found or the header is not present, the raw byte[] is returned.\n\nYou can configure the map of selector to Serializer / Deserializer via a constructor, or you can configure it via Kafka producer/consumer properties with the keys DelegatingSerializer.VALUE_SERIALIZATION_SELECTOR_CONFIG and DelegatingSerializer.KEY_SERIALIZATION_SELECTOR_CONFIG. For the serializer, the producer property can be a Map<String, Object> where the key is the selector and the value is a Serializer instance, a serializer Class or the class name. The property can also be a String of comma-delimited map entries, as shown below.\n\nFor the deserializer, the consumer property can be a Map<String, Object> where the key is the selector and the value is a Deserializer instance, a deserializer Class or the class name. The property can also be a String of comma-delimited map entries, as shown below.\n\nTo configure using properties, use the following syntax:\n\nproducerProps.put(DelegatingSerializer.VALUE_SERIALIZATION_SELECTOR_CONFIG, \"thing1:com.example.MyThing1Serializer, thing2:com.example.MyThing2Serializer\") consumerProps.put(DelegatingDeserializer.VALUE_SERIALIZATION_SELECTOR_CONFIG, \"thing1:com.example.MyThing1Deserializer, thing2:com.example.MyThing2Deserializer\")\n\nProducers would then set the DelegatingSerializer.VALUE_SERIALIZATION_SELECTOR header to thing1 or thing2.\n\nThis technique supports sending different types to the same topic (or different topics).\n\nStarting with version 2.5.1, it is not necessary to set the selector header, if the type (key or value) is one of the standard types supported by Serdes (Long, Integer, etc). Instead, the serializer will set the header to the class name of the type. It is not necessary to configure serializers or deserializers for these types, they will be created (once) dynamically.\n\nFor another technique to send different types to different topics, see Using RoutingKafkaTemplate.\n\nStarting with version 2.2.5, you can specify that certain string-valued headers should not be mapped using JSON, but to/from a raw byte[]. The AbstractKafkaHeaderMapper has new properties; mapAllStringsOut when set to true, all string-valued headers will be converted to byte[] using the charset property (default UTF-8). In addition, there is a property rawMappedHeaders, which is a map of header name : boolean; if the map contains a header name, and the header contains a String value, it will be mapped as a raw byte[] using the charset. This map is also used to map raw incoming byte[] headers to String using the charset if, and only if, the boolean in the map value is true. If the boolean is false, or the header name is not in the map with a true value, the incoming header is simply mapped as the raw unmapped header.\n\nListener Error Handlers\n\nStarting with version 2.0, the @KafkaListener annotation has a new attribute: errorHandler.\n\nYou can use the errorHandler to provide the bean name of a KafkaListenerErrorHandler implementation. This functional interface has one method, as the following listing shows:\n\n@FunctionalInterface public interface KafkaListenerErrorHandler { Object handleError(Message<?> message, ListenerExecutionFailedException exception) throws Exception; }\n\nYou have access to the spring-messaging Message<?> object produced by the message converter and the exception that was thrown by the listener, which is wrapped in a ListenerExecutionFailedException. The error handler can throw the original or a new exception, which is thrown to the container. Anything returned by the error handler is ignored.\n\nStarting with version 2.7, you can set the rawRecordHeader property on the MessagingMessageConverter and BatchMessagingMessageConverter which causes the raw ConsumerRecord to be added to the converted Message<?> in the KafkaHeaders.RAW_DATA header. This is useful, for example, if you wish to use a DeadLetterPublishingRecoverer in a listener error handler. It might be used in a request/reply scenario where you wish to send a failure result to the sender, after some number of retries, after capturing the failed record in a dead letter topic.\n\n@Bean KafkaListenerErrorHandler eh(DeadLetterPublishingRecoverer recoverer) { return (msg, ex) -> { if (msg.getHeaders().get(KafkaHeaders.DELIVERY_ATTEMPT, Integer.class) > 9) { recoverer.accept(msg.getHeaders().get(KafkaHeaders.RAW_DATA, ConsumerRecord.class), ex); return \"FAILED\"; } throw ex; }; }\n\nIt has a sub-interface (ConsumerAwareListenerErrorHandler) that has access to the consumer object, through the following method:\n\nObject handleError(Message<?> message, ListenerExecutionFailedException exception, Consumer<?, ?> consumer);\n\nIf your error handler implements this interface, you can, for example, adjust the offsets accordingly. For example, to reset the offset to replay the failed message, you could do something like the following:\n\n@Bean public ConsumerAwareListenerErrorHandler listen3ErrorHandler() { return (m, e, c) -> { this.listen3Exception = e; MessageHeaders headers = m.getHeaders(); c.seek(new org.apache.kafka.common.TopicPartition( headers.get(KafkaHeaders.RECEIVED_TOPIC, String.class), headers.get(KafkaHeaders.RECEIVED_PARTITION_ID, Integer.class)), headers.get(KafkaHeaders.OFFSET, Long.class)); return null; }; }\n\nSimilarly, you could do something like the following for a batch listener:\n\n@Bean public ConsumerAwareListenerErrorHandler listen10ErrorHandler() { return (m, e, c) -> { this.listen10Exception = e; MessageHeaders headers = m.getHeaders(); List<String> topics = headers.get(KafkaHeaders.RECEIVED_TOPIC, List.class); List<Integer> partitions = headers.get(KafkaHeaders.RECEIVED_PARTITION_ID, List.class); List<Long> offsets = headers.get(KafkaHeaders.OFFSET, List.class); Map<TopicPartition, Long> offsetsToReset = new HashMap<>(); for (int i = 0; i < topics.size(); i++) { int index = i; offsetsToReset.compute(new TopicPartition(topics.get(i), partitions.get(i)), (k, v) -> v == null ? offsets.get(index) : Math.min(v, offsets.get(index))); } offsetsToReset.forEach((k, v) -> c.seek(k, v)); return null; }; }\n\nThis resets each topic/partition in the batch to the lowest offset in the batch.\n\nThe preceding two examples are simplistic implementations, and you would probably want more checking in the error handler.\n\nContainer Error Handlers\n\nTwo error handler interfaces (ErrorHandler and BatchErrorHandler) are provided. You must configure the appropriate type to match the message listener.\n\nStarting with version 2.5, the default error handlers, when transactions are not being used, are the SeekToCurrentErrorHandler and RecoveringBatchErrorHandler with default configuration. See Seek To Current Container Error Handlers and Recovering Batch Error Handler. To restore the previous behavior, use the LoggingErrorHandler and BatchLoggingErrorHandler instead.\n\nWhen transactions are being used, no error handlers are configured, by default, so that the exception will roll back the transaction. Error handling for transactional containers are handled by the AfterRollbackProcessor. If you provide a custom error handler when using transactions, it must throw an exception if you want the transaction rolled back.\n\nStarting with version 2.3.2, these interfaces have a default method isAckAfterHandle() which is called by the container to determine whether the offset(s) should be committed if the error handler returns without throwing an exception. Starting with version 2.4, this returns true by default.\n\nTypically, the error handlers provided by the framework will throw an exception when the error is not \"handled\" (e.g. after performing a seek operation). By default, such exceptions are logged by the container at ERROR level. Starting with version 2.5, all the framework error handlers extend KafkaExceptionLogLevelAware which allows you to control the level at which these exceptions are logged.\n\n/** * Set the level at which the exception thrown by this handler is logged. * @param logLevel the level (default ERROR). */ public void setLogLevel(KafkaException.Level logLevel) { ... }\n\nYou can specify a global error handler to be used for all listeners in the container factory. The following example shows how to do so:\n\n@Bean public KafkaListenerContainerFactory<ConcurrentMessageListenerContainer<Integer, String>> kafkaListenerContainerFactory() { ConcurrentKafkaListenerContainerFactory<Integer, String> factory = new ConcurrentKafkaListenerContainerFactory<>(); ... factory.setErrorHandler(myErrorHandler); ... return factory; }\n\nSimilarly, you can set a global batch error handler:\n\n@Bean public KafkaListenerContainerFactory<ConcurrentMessageListenerContainer<Integer, String>> kafkaListenerContainerFactory() { ConcurrentKafkaListenerContainerFactory<Integer, String> factory = new ConcurrentKafkaListenerContainerFactory<>(); ... factory.setBatchErrorHandler(myBatchErrorHandler); ... return factory; }\n\nBy default, if an annotated listener method throws an exception, it is thrown to the container, and the message is handled according to the container configuration.\n\nIf you are using Spring Boot, you simply need to add the error handler as a @Bean and boot will add it to the auto-configured factory.\n\nSeek To Current Container Error Handlers\n\nIf an ErrorHandler implements RemainingRecordsErrorHandler, the error handler is provided with the failed record and any unprocessed records retrieved by the previous poll(). Those records are not passed to the listener after the handler exits. The following listing shows the RemainingRecordsErrorHandler interface definition:\n\n@FunctionalInterface public interface RemainingRecordsErrorHandler extends ConsumerAwareErrorHandler { void handle(Exception thrownException, List<ConsumerRecord<?, ?>> records, Consumer<?, ?> consumer); }\n\nThis interface lets implementations seek all unprocessed topics and partitions so that the current record (and the others remaining) are retrieved by the next poll. The SeekToCurrentErrorHandler does exactly this.\n\nackOnError must be false (which is the default). Otherwise, if the container is stopped after the seek, but before the record is reprocessed, the record will be skipped when the container is restarted.\n\nThis is now the default error handler for record listeners.\n\nThe container commits any pending offset commits before calling the error handler.\n\nTo configure the listener container with this handler, add it to the container factory.\n\nFor example, with the @KafkaListener container factory, you can add SeekToCurrentErrorHandler as follows:\n\n@Bean public ConcurrentKafkaListenerContainerFactory<String, String> kafkaListenerContainerFactory() { ConcurrentKafkaListenerContainerFactory<String, String> factory = new ConcurrentKafkaListenerContainerFactory(); factory.setConsumerFactory(consumerFactory()); factory.getContainerProperties().setAckOnError(false); factory.getContainerProperties().setAckMode(AckMode.RECORD); factory.setErrorHandler(new SeekToCurrentErrorHandler(new FixedBackOff(1000L, 2L))); return factory; }\n\nThis will retry a delivery up to 2 times (3 delivery attempts) with a back off of 1 second, instead of the default configuration (FixedBackOff(0L, 9)). Failures are simply logged after retries are exhausted.\n\nAs an example; if the poll returns six records (two from each partition 0, 1, 2) and the listener throws an exception on the fourth record, the container acknowledges the first three messages by committing their offsets. The SeekToCurrentErrorHandler seeks to offset 1 for partition 1 and offset 0 for partition 2. The next poll() returns the three unprocessed records.\n\nIf the AckMode was BATCH, the container commits the offsets for the first two partitions before calling the error handler.\n\nStarting with version 2.2, the SeekToCurrentErrorHandler can now recover (skip) a record that keeps failing. By default, after ten failures, the failed record is logged (at the ERROR level). You can configure the handler with a custom recoverer (BiConsumer) and maximum failures. Using a FixedBackOff with FixedBackOff.UNLIMITED_ATTEMPTS causes (effectively) infinite retries. The following example configures recovery after three tries:\n\nSeekToCurrentErrorHandler errorHandler = new SeekToCurrentErrorHandler((record, exception) -> { // recover after 3 failures, woth no back off - e.g. send to a dead-letter topic }, new FixedBackOff(0L, 2L));\n\nStarting with version 2.2.4, when the container is configured with AckMode.MANUAL_IMMEDIATE, the error handler can be configured to commit the offset of recovered records; set the commitRecovered property to true.\n\nSee also Publishing Dead-letter Records.\n\nWhen using transactions, similar functionality is provided by the DefaultAfterRollbackProcessor. See After-rollback Processor.\n\nStarting with version 2.3, the SeekToCurrentErrorHandler considers certain exceptions to be fatal, and retries are skipped for such exceptions; the recoverer is invoked on the first failure. The exceptions that are considered fatal, by default, are:\n\nDeserializationException\n\nMessageConversionException\n\nConversionException\n\nMethodArgumentResolutionException\n\nNoSuchMethodException\n\nClassCastException\n\nsince these exceptions are unlikely to be resolved on a retried delivery.\n\nYou can add more exception types to the not-retryable category, or completely replace the map of classified exceptions. See the Javadocs for SeekToCurrentErrorHandler.setClassifications() for more information, as well as those for the spring-retry BinaryExceptionClassifier.\n\nHere is an example that adds IllegalArgumentException to the not-retryable exceptions:\n\n@Bean public SeekToCurrentErrorHandler errorHandler(ConsumerRecordRecoverer recoverer) { SeekToCurrentErrorHandler handler = new SeekToCurrentErrorHandler(recoverer); handler.addNotRetryableException(IllegalArgumentException.class); return handler; }\n\nStarting with version 2.7, the error handler can be configured with one or more RetryListener s, receiving notifications of retry and recovery progress.\n\n@FunctionalInterface public interface RetryListener { void failedDelivery(ConsumerRecord<?, ?> record, Exception ex, int deliveryAttempt); default void recovered(ConsumerRecord<?, ?> record, Exception ex) { } default void recoveryFailed(ConsumerRecord<?, ?> record, Exception original, Exception failure) { } }\n\nSee the javadocs for more information.\n\nThe SeekToCurrentBatchErrorHandler seeks each partition to the first record in each partition in the batch, so the whole batch is replayed. Also see Committing Offsets for an alternative. Also see Retrying Batch Error Handler. This error handler does not support recovery, because the framework cannot know which message in the batch is failing.\n\nAfter seeking, an exception that wraps the ListenerExecutionFailedException is thrown. This is to cause the transaction to roll back (if transactions are enabled).\n\nStarting with version 2.3, a BackOff can be provided to the SeekToCurrentErrorHandler and DefaultAfterRollbackProcessor so that the consumer thread can sleep for some configurable time between delivery attempts. Spring Framework provides two out of the box BackOff s, FixedBackOff and ExponentialBackOff. The maximum back off time must not exceed the max.poll.interval.ms consumer property, to avoid a rebalance.\n\nPreviously, the configuration was \"maxFailures\" (which included the first delivery attempt). When using a FixedBackOff, its maxAttempts property represents the number of delivery retries (one less than the old maxFailures property). Also, maxFailures=-1 meant retry indefinitely with the old configuration, with a BackOff you would set the maxAttempts to Long.MAX_VALUE for a FixedBackOff and leave the maxElapsedTime to its default in an ExponentialBackOff.\n\nThe SeekToCurrentBatchErrorHandler can also be configured with a BackOff to add a delay between delivery attempts. Generally, you should configure the BackOff to never return STOP. However, since this error handler has no mechanism to \"recover\" after retries are exhausted, if the BackOffExecution returns STOP, the previous interval will be used for all subsequent delays. Again, the maximum delay must be less than the max.poll.interval.ms consumer property. Also see Retrying Batch Error Handler.\n\nIf the recoverer fails (throws an exception), the failed record will be included in the seeks. Starting with version 2.5.5, if the recoverer fails, the BackOff will be reset by default and redeliveries will again go through the back offs before recovery is attempted again. With earlier versions, the BackOff was not reset and recovery was re-attempted on the next failure. To revert to the previous behavior, set the error handler’s resetStateOnRecoveryFailure to false.\n\nStarting with version 2.3.2, after a record has been recovered, its offset will be committed (if one of the container AckMode s is configured). To revert to the previous behavior, set the error handler’s ackAfterHandle property to false.\n\nStarting with version 2.6, you can now provide the error handler with a BiFunction<ConsumerRecord<?, ?>, Exception, BackOff> to determine the BackOff to use, based on the failed record and/or the exception:\n\nhandler.setBackOffFunction((record, ex) -> { ... });\n\nIf the function returns null, the handler’s default BackOff will be used.\n\nStarting with version 2.6.3, set resetStateOnExceptionChange to true and the retry sequence will be restarted (including the selection of a new BackOff, if so configured) if the exception type changes between failures. By default, the exception type is not considered.\n\nAlso see Delivery Attempts Header.\n\nStarting with version 2.7, while waiting for a BackOff interval, the error handler will loop with a short sleep until the desired delay, while checking to see if the container has been stopped, allowing the sleep to exit soon after the stop() rather than causing a delay.\n\nRecovering Batch Error Handler\n\nAs an alternative to the Retrying Batch Error Handler, version 2.5 introduced the RecoveringBatchErrorHandler.\n\nThis is now the default error handler for batch listeners. The default configuration retries 9 times (10 delivery attempts) with no back off between deliveries.\n\nThis error handler works in conjunction with the listener throwing a BatchListenerFailedException providing the index in the batch where the failure occurred (or the failed record itself). If the listener throws a different exception, or the index is out of range, the error handler falls back to invoking a SeekToCurrentBatchErrorHandler and the whole batch is retried, with no recovery available. The sequence of events is:\n\nCommit the offsets of the records before the index.\n\nIf retries are not exhausted, perform seeks so that all the remaining records (including the failed record) will be redelivered.\n\nIf retries are exhausted, attempt recovery of the failed record (default log only) and perform seeks so that the remaining records (excluding the failed record) will be redelivered. The recovered record’s offset is committed\n\nIf retries are exhausted and recovery fails, seeks are performed as if retries are not exhausted.\n\nThe default recoverer logs the failed record after retries are exhausted. You can use a custom recoverer, or one provided by the framework such as the DeadLetterPublishingRecoverer.\n\nIn all cases, a BackOff can be configured to enable a delay between delivery attempts.\n\nExample:\n\n@Bean public RecoveringBatchErrorHandler batchErrorHandler(KafkaTemplate<String, String> template) { DeadLetterPublishingRecoverer recoverer = new DeadLetterPublishingRecoverer(template); RecoveringBatchErrorHandler errorHandler = new RecoveringBatchErrorHandler(recoverer, new FixedBackOff(2L, 5000)); }\n\n@KafkaListener(id = \"recovering\", topics = \"someTopic\") public void listen(List<ConsumerRecord<String, String>> records) { records.forEach(record -> { try { process(record); } catch (Exception e) { throw new BatchListenerFailedException(\"Failed to process\", record); } }); }\n\nFor example; say 10 records are in the original batch and no more records are added to the topic during the retries, and the failed record is at index 4 in the list. After the first delivery fails, the offsets for the first 4 records will be committed; the remaing 6 will be redelivered after 5 seconds. Most likely (but not necessarily) the failed record will be at index 0 in the redelivery. If it fails again, it will be retried one more time and, if it again fails, it will be sent to a dead letter topic.\n\nWhen using a POJO batch listener (e.g. List<Thing>), and you don’t have the full consumer record to add to the exception, you can just add the index of the record that failed:\n\n@KafkaListener(id = \"recovering\", topics = \"someTopic\") public void listen(List<Thing> things) { for (int i = 0; i < records.size(); i++) { try { process(things.get(i)); } catch (Exception e) { throw new BatchListenerFailedException(\"Failed to process\", i); } } }\n\nThis error handler cannot be used with transactions\n\nIf the recoverer fails (throws an exception), the failed record will be included in the seeks. Starting with version 2.5.5, if the recoverer fails, the BackOff will be reset by default and redeliveries will again go through the back offs before recovery is attempted again. With earlier versions, the BackOff was not reset and recovery was re-attempted on the next failure. To revert to the previous behavior, set the error handler’s resetStateOnRecoveryFailure to false.\n\nStarting with version 2.6, you can now provide the error handler with a BiFunction<ConsumerRecord<?, ?>, Exception, BackOff> to determine the BackOff to use, based on the failed record and/or the exception:\n\nhandler.setBackOffFunction((record, ex) -> { ... });\n\nIf the function returns null, the handler’s default BackOff will be used.\n\nStarting with version 2.6.3, set resetStateOnExceptionChange to true and the retry sequence will be restarted (including the selection of a new BackOff, if so configured) if the exception type changes between failures. By default, the exception type is not considered.\n\nStarting with version 2.7, while waiting for a BackOff interval, the error handler will loop with a short sleep until the desired delay is reached, while checking to see if the container has been stopped, allowing the sleep to exit soon after the stop() rather than causing a delay.\n\nStarting with version 2.7, the error handler can be configured with one or more RetryListener s, receiving notifications of retry and recovery progress.\n\n@FunctionalInterface public interface RetryListener { void failedDelivery(ConsumerRecord<?, ?> record, Exception ex, int deliveryAttempt); default void recovered(ConsumerRecord<?, ?> record, Exception ex) { } default void recoveryFailed(ConsumerRecord<?, ?> record, Exception original, Exception failure) { } }\n\nSee the javadocs for more information.\n\nAfter-rollback Processor\n\nWhen using transactions, if the listener throws an exception (and an error handler, if present, throws an exception), the transaction is rolled back. By default, any unprocessed records (including the failed record) are re-fetched on the next poll. This is achieved by performing seek operations in the DefaultAfterRollbackProcessor. With a batch listener, the entire batch of records is reprocessed (the container has no knowledge of which record in the batch failed). To modify this behavior, you can configure the listener container with a custom AfterRollbackProcessor. For example, with a record-based listener, you might want to keep track of the failed record and give up after some number of attempts, perhaps by publishing it to a dead-letter topic.\n\nStarting with version 2.2, the DefaultAfterRollbackProcessor can now recover (skip) a record that keeps failing. By default, after ten failures, the failed record is logged (at the ERROR level). You can configure the processor with a custom recoverer (BiConsumer) and maximum failures. Setting the maxFailures property to a negative number causes infinite retries. The following example configures recovery after three tries:\n\nAfterRollbackProcessor<String, String> processor = new DefaultAfterRollbackProcessor((record, exception) -> { // recover after 3 failures, with no back off - e.g. send to a dead-letter topic }, new FixedBackOff(0L, 2L));\n\nWhen you do not use transactions, you can achieve similar functionality by configuring a SeekToCurrentErrorHandler. See Seek To Current Container Error Handlers.\n\nRecovery is not possible with a batch listener, since the framework has no knowledge about which record in the batch keeps failing. In such cases, the application listener must handle a record that keeps failing.\n\nSee also Publishing Dead-letter Records.\n\nStarting with version 2.2.5, the DefaultAfterRollbackProcessor can be invoked in a new transaction (started after the failed transaction rolls back). Then, if you are using the DeadLetterPublishingRecoverer to publish a failed record, the processor will send the recovered record’s offset in the original topic/partition to the transaction. To enable this feature, set the commitRecovered and kafkaTemplate properties on the DefaultAfterRollbackProcessor.\n\nIf the recoverer fails (throws an exception), the failed record will be included in the seeks. Starting with version 2.5.5, if the recoverer fails, the BackOff will be reset by default and redeliveries will again go through the back offs before recovery is attempted again. With earlier versions, the BackOff was not reset and recovery was re-attempted on the next failure. To revert to the previous behavior, set the processor’s resetStateOnRecoveryFailure property to false.\n\nStarting with version 2.6, you can now provide the processor with a BiFunction<ConsumerRecord<?, ?>, Exception, BackOff> to determine the BackOff to use, based on the failed record and/or the exception:\n\nhandler.setBackOffFunction((record, ex) -> { ... });\n\nIf the function returns null, the processor’s default BackOff will be used.\n\nStarting with version 2.6.3, set resetStateOnExceptionChange to true and the retry sequence will be restarted (including the selection of a new BackOff, if so configured) if the exception type changes between failures. By default, the exception type is not considered.\n\nStarting with version 2.3.1, similar to the SeekToCurrentErrorHandler, the DefaultAfterRollbackProcessor considers certain exceptions to be fatal, and retries are skipped for such exceptions; the recoverer is invoked on the first failure. The exceptions that are considered fatal, by default, are:\n\nDeserializationException\n\nMessageConversionException\n\nConversionException\n\nMethodArgumentResolutionException\n\nNoSuchMethodException\n\nClassCastException\n\nsince these exceptions are unlikely to be resolved on a retried delivery.\n\nYou can add more exception types to the not-retryable category, or completely replace the map of classified exceptions. See the Javadocs for DefaultAfterRollbackProcessor.setClassifications() for more information, as well as those for the spring-retry BinaryExceptionClassifier.\n\nHere is an example that adds IllegalArgumentException to the not-retryable exceptions:\n\n@Bean public DefaultAfterRollbackProcessor errorHandler(BiConsumer<ConsumerRecord<?, ?>, Exception> recoverer) { DefaultAfterRollbackProcessor processor = new DefaultAfterRollbackProcessor(recoverer); processor.addNotRetryableException(IllegalArgumentException.class); return processor; }\n\nAlso see Delivery Attempts Header.\n\nWith current kafka-clients, the container cannot detect whether a ProducerFencedException is caused by a rebalance or if the producer’s transactional.id has been revoked due to a timeout or expiry. Because, in most cases, it is caused by a rebalance, the container does not call the AfterRollbackProcessor (because it’s not appropriate to seek the partitions because we no longer are assigned them). If you ensure the timeout is large enough to process each transaction and periodically perform an \"empty\" transaction (e.g. via a ListenerContainerIdleEvent) you can avoid fencing due to timeout and expiry. Or, you can set the stopContainerWhenFenced container property to true and the container will stop, avoiding the loss of records. You can consume a ConsumerStoppedEvent and check the Reason property for FENCED to detect this condition. Since the event also has a reference to the container, you can restart the container using this event.\n\nStarting with version 2.7, while waiting for a BackOff interval, the error handler will loop with a short sleep until the desired delay is reached, while checking to see if the container has been stopped, allowing the sleep to exit soon after the stop() rather than causing a delay.\n\nStarting with version 2.7, the processor can be configured with one or more RetryListener s, receiving notifications of retry and recovery progress.\n\n@FunctionalInterface public interface RetryListener { void failedDelivery(ConsumerRecord<?, ?> record, Exception ex, int deliveryAttempt); default void recovered(ConsumerRecord<?, ?> record, Exception ex) { } default void recoveryFailed(ConsumerRecord<?, ?> record, Exception original, Exception failure) { } }\n\nSee the javadocs for more information.\n\nPublishing Dead-letter Records\n\nAs discussed earlier, you can configure the SeekToCurrentErrorHandler and DefaultAfterRollbackProcessor (as well as the RecoveringBatchErrorHandler) with a record recoverer when the maximum number of failures is reached for a record. The framework provides the DeadLetterPublishingRecoverer, which publishes the failed message to another topic. The recoverer requires a KafkaTemplate<Object, Object>, which is used to send the record. You can also, optionally, configure it with a BiFunction<ConsumerRecord<?, ?>, Exception, TopicPartition>, which is called to resolve the destination topic and partition.\n\nBy default, the dead-letter record is sent to a topic named <originalTopic>.DLT (the original topic name suffixed with .DLT) and to the same partition as the original record. Therefore, when you use the default resolver, the dead-letter topic must have at least as many partitions as the original topic.\n\nIf the returned TopicPartition has a negative partition, the partition is not set in the ProducerRecord, so the partition is selected by Kafka. Starting with version 2.2.4, any ListenerExecutionFailedException (thrown, for example, when an exception is detected in a @KafkaListener method) is enhanced with the groupId property. This allows the destination resolver to use this, in addition to the information in the ConsumerRecord to select the dead letter topic.\n\nThe following example shows how to wire a custom destination resolver:\n\nDeadLetterPublishingRecoverer recoverer = new DeadLetterPublishingRecoverer(template, (r, e) -> { if (e instanceof FooException) { return new TopicPartition(r.topic() + \".Foo.failures\", r.partition()); } else { return new TopicPartition(r.topic() + \".other.failures\", r.partition()); } }); ErrorHandler errorHandler = new SeekToCurrentErrorHandler(recoverer, new FixedBackOff(0L, 2L));\n\nThe record sent to the dead-letter topic is enhanced with the following headers:\n\nKafkaHeaders.DLT_EXCEPTION_FQCN: The Exception class name.\n\nKafkaHeaders.DLT_EXCEPTION_STACKTRACE: The Exception stack trace.\n\nKafkaHeaders.DLT_EXCEPTION_MESSAGE: The Exception message.\n\nKafkaHeaders.DLT_KEY_EXCEPTION_FQCN: The Exception class name (key deserialization errors only).\n\nKafkaHeaders.DLT_KEY_EXCEPTION_STACKTRACE: The Exception stack trace (key deserialization errors only).\n\nKafkaHeaders.DLT_KEY_EXCEPTION_MESSAGE: The Exception message (key deserialization errors only).\n\nKafkaHeaders.DLT_ORIGINAL_TOPIC: The original topic.\n\nKafkaHeaders.DLT_ORIGINAL_PARTITION: The original partition.\n\nKafkaHeaders.DLT_ORIGINAL_OFFSET: The original offset.\n\nKafkaHeaders.DLT_ORIGINAL_TIMESTAMP: The original timestamp.\n\nKafkaHeaders.DLT_ORIGINAL_TIMESTAMP_TYPE: The original timestamp type.\n\nThere are two mechanisms to add more headers.\n\nSubclass the recoverer and override createProducerRecord() - call super.createProducerRecord() and add more headers.\n\nProvide a BiFunction to receive the consumer record and exception, returning a Headers object; headers from there will be copied to the final producer record. Use setHeadersFunction() to set the BiFunction.\n\nThe second is simpler to implement but the first has more information available, including the already assembled standard headers.\n\nStarting with version 2.3, when used in conjunction with an ErrorHandlingDeserializer, the publisher will restore the record value(), in the dead-letter producer record, to the original value that failed to be deserialized. Previously, the value() was null and user code had to decode the DeserializationException from the message headers. In addition, you can provide multiple KafkaTemplate s to the publisher; this might be needed, for example, if you want to publish the byte[] from a DeserializationException, as well as values using a different serializer from records that were deserialized successfully. Here is an example of configuring the publisher with KafkaTemplate s that use a String and byte[] serializer:\n\n@Bean public DeadLetterPublishingRecoverer publisher(KafkaTemplate<?, ?> stringTemplate, KafkaTemplate<?, ?> bytesTemplate) { Map<Class<?>, KafkaTemplate<?, ?>> templates = new LinkedHashMap<>(); templates.put(String.class, stringTemplate); templates.put(byte[].class, bytesTemplate); return new DeadLetterPublishingRecoverer(templates); }\n\nThe publisher uses the map keys to locate a template that is suitable for the value() about to be published. A LinkedHashMap is recommended so that the keys are examined in order.\n\nWhen publishing null values, when there are multiple templates, the recoverer will look for a template for the Void class; if none is present, the first template from the values().iterator() will be used.\n\nSince 2.7 you can use the setFailIfSendResultIsError method so that an exception is thrown when message publishing fails. You can also set a timeout for the verification of the sender success with setWaitForSendResultTimeout.\n\nIf the recoverer fails (throws an exception), the failed record will be included in the seeks. Starting with version 2.5.5, if the recoverer fails, the BackOff will be reset by default and redeliveries will again go through the back offs before recovery is attempted again. With earlier versions, the BackOff was not reset and recovery was re-attempted on the next failure. To revert to the previous behavior, set the error handler’s resetStateOnRecoveryFailure property to false.\n\nStarting with version 2.6.3, set resetStateOnExceptionChange to true and the retry sequence will be restarted (including the selection of a new BackOff, if so configured) if the exception type changes between failures. By default, the exception type is not considered.\n\nStarting with version 2.3, the recoverer can also be used with Kafka Streams - see Recovery from Deserialization Exceptions for more information.\n\nThe ErrorHandlingDeserializer adds the deserialization exception(s) in headers ErrorHandlingDeserializer.VALUE_DESERIALIZER_EXCEPTION_HEADER and ErrorHandlingDeserializer.KEY_DESERIALIZER_EXCEPTION_HEADER (using java serialization). By default, these headers are not retained in the message published to the dead letter topic. Starting with version 2.7, if both the key and value fail deserialization, the original values of both are populated in the record sent to the DLT.\n\nIf incoming records are dependent on each other, but may arrive out of order, it may be useful to republish a failed record to the tail of the original topic (for some number of times), instead of sending it directly to the dead letter topic. See this Stack Overflow Question for an example.\n\nThe following error handler configuration will do exactly that:\n\n@Bean public ErrorHandler eh(KafkaOperations<String, String> template) { return new SeekToCurrentErrorHandler(new DeadLetterPublishingRecoverer(template, (rec, ex) -> { org.apache.kafka.common.header.Header retries = rec.headers().lastHeader(\"retries\"); if (retries == null) { retries = new RecordHeader(\"retries\", new byte[] { 1 }); rec.headers().add(retries); } else { retries.value()[0]++; } return retries.value()[0] > 5 ? new TopicPartition(\"topic.DLT\", rec.partition()) : new TopicPartition(\"topic\", rec.partition()); }), new FixedBackOff(0L, 0L)); }\n\nStarting with version 2.7, the recoverer checks that the partition selected by the destination resolver actually exists. If the partition is not present, the partition in the ProducerRecord is set to null, allowing the KafkaProducer to select the partition. You can disable this check by setting the verifyPartition property to false."
    }
}
{
    "id": "dbpedia_1734_3",
    "rank": 33,
    "data": {
        "url": "https://debezium.io/documentation/reference/stable/connectors/mongodb.html",
        "read_more_link": "",
        "language": "en",
        "title": "Debezium connector for MongoDB :: Debezium Documentation",
        "top_image": "",
        "meta_img": "",
        "images": [
            "https://debezium.io/assets/images/color_white_debezium_type_600px.svg"
        ],
        "movies": [],
        "keywords": [],
        "meta_keywords": [
            ""
        ],
        "tags": null,
        "authors": [],
        "publish_date": null,
        "summary": "",
        "meta_description": "",
        "meta_lang": "en",
        "meta_favicon": "",
        "meta_site_name": "",
        "canonical_link": "https://debezium.io/documentation/reference/stable/connectors/mongodb.html",
        "text": "A MongoDB replica set consists of a set of servers that all have copies of the same data, and replication ensures that all changes made by clients to documents on the replica set’s primary are correctly applied to the other replica set’s servers, called secondaries. MongoDB replication works by having the primary record the changes in its oplog (or operation log), and then each of the secondaries reads the primary’s oplog and applies in order all of the operations to their own documents. When a new server is added to a replica set, that server first performs an snapshot of all of the databases and collections on the primary, and then reads the primary’s oplog to apply all changes that might have been made since it began the snapshot. This new server becomes a secondary (and able to handle queries) when it catches up to the tail of the primary’s oplog.\n\nChange streams\n\nAlthough the Debezium MongoDB connector does not become part of a replica set, it uses a similar replication mechanism to obtain oplog data. The main difference is that the connector does not read the oplog directly. Instead, it delegates the capture and decoding of oplog data to the MongoDB change streams feature. With change streams, the MongoDB server exposes the changes that occur in a collection as an event stream. The Debezium connector monitors the stream and then delivers the changes downstream. The first time that the connector detects a replica set, it examines the oplog to obtain the last recorded transaction, and then performs a snapshot of the primary’s databases and collections. After the connector finishes copying the data, it creates a change stream beginning from the oplog position that it read earlier.\n\nAs the MongoDB connector processes changes, it periodically records the position at which the event originated in the oplog stream. When the connector stops, it records the last oplog stream position that it processed, so that after a restart it can resume streaming from that position. In other words, the connector can be stopped, upgraded or maintained, and restarted some time later, and always pick up exactly where it left off without losing a single event. Of course, MongoDB oplogs are usually capped at a maximum size, so if the connector is stopped for long periods, operations in the oplog might be purged before the connector has a chance to read them. In this case, after a restart the connector detects the missing oplog operations, performs a snapshot, and then proceeds to stream changes.\n\nThe MongoDB connector is also quite tolerant of changes in membership and leadership of the replica sets, of additions or removals of shards within a sharded cluster, and network problems that might cause communication failures. The connector always uses the replica set’s primary node to stream changes, so when the replica set undergoes an election and a different node becomes primary, the connector will immediately stop streaming changes, connect to the new primary, and start streaming changes using the new primary node. Similarly, if connector is unable to communicate with the replica set primary, it attempts to reconnect (using exponential backoff so as to not overwhelm the network or replica set). After connection is reestablished, the connector continues to stream changes from the last event that it captured. In this way the connector dynamically adjusts to changes in replica set membership, and automatically handles communication disruptions.\n\nPerforming a snapshot\n\nWhen a Debezium task starts to use a replica set, it uses the connector’s logical name and the replica set name to find an offset that describes the position where the connector previously stopped reading changes. If an offset can be found and it still exists in the oplog, then the task immediately proceeds with streaming changes, starting at the recorded offset position.\n\nHowever, if no offset is found, or if the oplog no longer contains that position, the task must first obtain the current state of the replica set contents by performing a snapshot. This process starts by recording the current position of the oplog and recording that as the offset (along with a flag that denotes a snapshot has been started). The task then proceeds to copy each collection, spawning as many threads as possible (up to the value of the snapshot.max.threads configuration property) to perform this work in parallel. The connector records a separate read event for each document it sees. Each read event contains the object’s identifier, the complete state of the object, and source information about the MongoDB replica set where the object was found. The source information also includes a flag that denotes that the event was produced during a snapshot.\n\nThis snapshot will continue until it has copied all collections that match the connector’s filters. If the connector is stopped before the tasks' snapshots are completed, upon restart the connector begins the snapshot again.\n\nTable 1. Settings for snapshot.mode connector configuration property Setting Description\n\nFor more information, see snapshot.mode in the table of connector configuration properties.\n\nAd hoc snapshots\n\nBy default, a connector runs an initial snapshot operation only after it starts for the first time. Following this initial snapshot, under normal circumstances, the connector does not repeat the snapshot process. Any future change event data that the connector captures comes in through the streaming process only.\n\nHowever, in some situations the data that the connector obtained during the initial snapshot might become stale, lost, or incomplete. To provide a mechanism for recapturing collection data, Debezium includes an option to perform ad hoc snapshots. You might want to perform an ad hoc snapshot after any of the following changes occur in your Debezium environment:\n\nThe connector configuration is modified to capture a different set of collections.\n\nKafka topics are deleted and must be rebuilt.\n\nData corruption occurs due to a configuration error or some other problem.\n\nYou can re-run a snapshot for a collection for which you previously captured a snapshot by initiating a so-called ad-hoc snapshot. Ad hoc snapshots require the use of signaling collections. You initiate an ad hoc snapshot by sending a signal request to the Debezium signaling collection.\n\nWhen you initiate an ad hoc snapshot of an existing collection, the connector appends content to the topic that already exists for the collection. If a previously existing topic was removed, Debezium can create a topic automatically if automatic topic creation is enabled.\n\nAd hoc snapshot signals specify the collections to include in the snapshot. The snapshot can capture the entire contents of the database, or capture only a subset of the collections in the database.\n\nYou specify the collections to capture by sending an execute-snapshot message to the signaling collection. Set the type of the execute-snapshot signal to incremental or blocking, and provide the names of the collections to include in the snapshot, as described in the following table:\n\nTable 2. Example of an ad hoc execute-snapshot signal record Field Default Value\n\nTriggering an ad hoc incremental snapshot\n\nYou initiate an ad hoc incremental snapshot by adding an entry with the execute-snapshot signal type to the signaling collection. After the connector processes the message, it begins the snapshot operation. The snapshot process reads the first and last primary key values and uses those values as the start and end point for each collection. Based on the number of entries in the collection, and the configured chunk size, Debezium divides the collection into chunks, and proceeds to snapshot each chunk, in succession, one at a time.\n\nFor more information, see Incremental snapshots.\n\nTriggering an ad hoc blocking snapshot\n\nYou initiate an ad hoc blocking snapshot by adding an entry with the execute-snapshot signal type to the signaling collection. After the connector processes the message, it begins the snapshot operation. The connector temporarily stops streaming, and then initiates a snapshot of the specified collection, following the same process that it uses during an initial snapshot. After the snapshot completes, the connector resumes streaming.\n\nFor more information, see Blocking snapshots.\n\nIncremental snapshots\n\nTo provide flexibility in managing snapshots, Debezium includes a supplementary snapshot mechanism, known as incremental snapshotting. Incremental snapshots rely on the Debezium mechanism for sending signals to a Debezium connector. Incremental snapshots are based on the DDD-3 design document.\n\nIn an incremental snapshot, instead of capturing the full state of a database all at once, as in an initial snapshot, Debezium captures each collection in phases, in a series of configurable chunks. You can specify the collections that you want the snapshot to capture and the size of each chunk. The chunk size determines the number of rows that the snapshot collects during each fetch operation on the database. The default chunk size for incremental snapshots is 1024 rows.\n\nAs an incremental snapshot proceeds, Debezium uses watermarks to track its progress, maintaining a record of each collection row that it captures. This phased approach to capturing data provides the following advantages over the standard initial snapshot process:\n\nYou can run incremental snapshots in parallel with streamed data capture, instead of postponing streaming until the snapshot completes. The connector continues to capture near real-time events from the change log throughout the snapshot process, and neither operation blocks the other.\n\nIf the progress of an incremental snapshot is interrupted, you can resume it without losing any data. After the process resumes, the snapshot begins at the point where it stopped, rather than recapturing the collection from the beginning.\n\nYou can run an incremental snapshot on demand at any time, and repeat the process as needed to adapt to database updates. For example, you might re-run a snapshot after you modify the connector configuration to add a collection to its collection.include.list property.\n\nIncremental snapshot process\n\nWhen you run an incremental snapshot, Debezium sorts each collection by primary key and then splits the collection into chunks based on the configured chunk size. Working chunk by chunk, it then captures each collection row in a chunk. For each row that it captures, the snapshot emits a READ event. That event represents the value of the row when the snapshot for the chunk began.\n\nAs a snapshot proceeds, it’s likely that other processes continue to access the database, potentially modifying collection records. To reflect such changes, INSERT, UPDATE, or DELETE operations are committed to the transaction log as per usual. Similarly, the ongoing Debezium streaming process continues to detect these change events and emits corresponding change event records to Kafka.\n\nHow Debezium resolves collisions among records with the same primary key\n\nIn some cases, the UPDATE or DELETE events that the streaming process emits are received out of sequence. That is, the streaming process might emit an event that modifies a collection row before the snapshot captures the chunk that contains the READ event for that row. When the snapshot eventually emits the corresponding READ event for the row, its value is already superseded. To ensure that incremental snapshot events that arrive out of sequence are processed in the correct logical order, Debezium employs a buffering scheme for resolving collisions. Only after collisions between the snapshot events and the streamed events are resolved does Debezium emit an event record to Kafka.\n\nSnapshot window\n\nTo assist in resolving collisions between late-arriving READ events and streamed events that modify the same collection row, Debezium employs a so-called snapshot window. The snapshot windows demarcates the interval during which an incremental snapshot captures data for a specified collection chunk. Before the snapshot window for a chunk opens, Debezium follows its usual behavior and emits events from the transaction log directly downstream to the target Kafka topic. But from the moment that the snapshot for a particular chunk opens, until it closes, Debezium performs a de-duplication step to resolve collisions between events that have the same primary key..\n\nFor each data collection, the Debezium emits two types of events, and stores the records for them both in a single destination Kafka topic. The snapshot records that it captures directly from a table are emitted as READ operations. Meanwhile, as users continue to update records in the data collection, and the transaction log is updated to reflect each commit, Debezium emits UPDATE or DELETE operations for each change.\n\nAs the snapshot window opens, and Debezium begins processing a snapshot chunk, it delivers snapshot records to a memory buffer. During the snapshot windows, the primary keys of the READ events in the buffer are compared to the primary keys of the incoming streamed events. If no match is found, the streamed event record is sent directly to Kafka. If Debezium detects a match, it discards the buffered READ event, and writes the streamed record to the destination topic, because the streamed event logically supersede the static snapshot event. After the snapshot window for the chunk closes, the buffer contains only READ events for which no related transaction log events exist. Debezium emits these remaining READ events to the collection’s Kafka topic.\n\nThe connector repeats the process for each snapshot chunk.\n\nIncremental snapshots for sharded clusters\n\nTo use incremental snapshots with sharded MongoDB clusters, you must set incremental.snapshot.chunk.size to a value that is high enough to compensate for the increased complexity of change stream pipelines.\n\nTriggering an incremental snapshot\n\nCurrently, the only way to initiate an incremental snapshot is to send an ad hoc snapshot signal to the signaling collection on the source database.\n\nYou submit a signal to the signaling collection by using the MongoDB insert() method.\n\nAfter Debezium detects the change in the signaling collection, it reads the signal, and runs the requested snapshot operation.\n\nThe query that you submit specifies the collections to include in the snapshot, and, optionally, specifies the type of snapshot operation. Currently, the only valid options for snapshots operations are incremental and blocking.\n\nTo specify the collections to include in the snapshot, provide a data-collections array that lists the collections or an array of regular expressions used to match collections, for example,\n\n{\"data-collections\": [\"public.Collection1\", \"public.Collection2\"]}\n\nThe data-collections array for an incremental snapshot signal has no default value. If the data-collections array is empty, Debezium detects that no action is required and does not perform a snapshot.\n\nPrerequisites\n\nSignaling is enabled.\n\nA signaling data collection exists on the source database.\n\nThe signaling data collection is specified in the signal.data.collection property.\n\nUsing a source signaling channel to trigger an incremental snapshot\n\nInsert a snapshot signal document into the signaling collection:\n\n<signalDataCollection>.insert({\"id\" : _<idNumber>,\"type\" : <snapshotType>, \"data\" : {\"data-collections\" [\"<collectionName>\", \"<collectionName>\"],\"type\": <snapshotType>, \"additional-conditions\" : [{\"data-collections\" : \"<collectionName>\", \"filter\" : \"<additional-condition>\"}] }});\n\nFor example,\n\ndb.debeziumSignal.insert({ (1) \"type\" : \"execute-snapshot\", (2) (3) \"data\" : { \"data-collections\" [\"\\\"public\\\".\\\"Collection1\\\"\", \"\\\"public\\\".\\\"Collection2\\\"\"], (4) \"type\": \"incremental\"} (5) \"additional-conditions\":[{\"data-collection\": \"schema1.table1\" ,\"filter\":\"color=\\'blue\\'\"}]}'); (6) });\n\nThe values of the id,type, and data parameters in the command correspond to the fields of the signaling collection.\n\nThe following table describes the parameters in the example:\n\nTable 3. Descriptions of fields in a MongoDB insert() command for sending an incremental snapshot signal to the signaling collection Item Value Description\n\nThe following example, shows the JSON for an incremental snapshot event that is captured by a connector.\n\nExample: Incremental snapshot event message\n\n{ \"before\":null, \"after\": { \"pk\":\"1\", \"value\":\"New data\" }, \"source\": { ... \"snapshot\":\"incremental\" (1) }, \"op\":\"r\", (2) \"ts_ms\":\"1620393591654\", \"ts_us\":\"1620393591654962\", \"ts_ns\":\"1620393591654962147\", \"transaction\":null }\n\nItem Field name Description\n\nUsing the Kafka signaling channel to trigger an incremental snapshot\n\nYou can send a message to the configured Kafka topic to request the connector to run an ad hoc incremental snapshot.\n\nThe key of the Kafka message must match the value of the topic.prefix connector configuration option.\n\nThe value of the message is a JSON object with type and data fields.\n\nThe signal type is execute-snapshot, and the data field must have the following fields:\n\nTable 4. Execute snapshot data fields Field Default Value\n\nAn example of the execute-snapshot Kafka message:\n\nKey = `test_connector` Value = `{\"type\":\"execute-snapshot\",\"data\": {\"data-collections\": [\"schema1.table1\", \"schema1.table2\"], \"type\": \"INCREMENTAL\"}}`\n\nAd hoc incremental snapshots with additional-conditions\n\nDebezium uses the additional-conditions field to select a subset of a collection’s content.\n\nTypically, when Debezium runs a snapshot, it runs a SQL query such as:\n\nSELECT * FROM <tableName> …​.\n\nWhen the snapshot request includes an additional-conditions property, the data-collection and filter parameters of the property are appended to the SQL query, for example:\n\nSELECT * FROM <data-collection> WHERE <filter> …​.\n\nFor example, given a products collection with the columns id (primary key), color, and brand, if you want a snapshot to include only content for which color='blue', when you request the snapshot, you could add the additional-conditions property to filter the content:\n\nKey = `test_connector` Value = `{\"type\":\"execute-snapshot\",\"data\": {\"data-collections\": [\"schema1.products\"], \"type\": \"INCREMENTAL\", \"additional-conditions\": [{\"data-collection\": \"schema1.products\" ,\"filter\":\"color='blue'\"}]}}`\n\nYou can use the additional-conditions property to pass conditions based on multiple columns. For example, using the same products collection as in the previous example, if you want a snapshot to include only the content from the products collection for which color='blue', and brand='MyBrand', you could send the following request:\n\nKey = `test_connector` Value = `{\"type\":\"execute-snapshot\",\"data\": {\"data-collections\": [\"schema1.products\"], \"type\": \"INCREMENTAL\", \"additional-conditions\": [{\"data-collection\": \"schema1.products\" ,\"filter\":\"color='blue' AND brand='MyBrand'\"}]}}`\n\nStopping an incremental snapshot\n\nYou can also stop an incremental snapshot by sending a signal to the collection on the source database. You submit a stop snapshot signal by inserting a document into the to the signaling collection. After Debezium detects the change in the signaling collection, it reads the signal, and stops the incremental snapshot operation if it’s in progress.\n\nThe query that you submit specifies the snapshot operation of incremental, and, optionally, the collections of the current running snapshot to be removed.\n\nPrerequisites\n\nSignaling is enabled.\n\nA signaling data collection exists on the source database.\n\nThe signaling data collection is specified in the signal.data.collection property.\n\nUsing a source signaling channel to stop an incremental snapshot\n\nInsert a stop snapshot signal document into the signaling collection:\n\n<signalDataCollection>.insert({\"id\" : _<idNumber>,\"type\" : \"stop-snapshot\", \"data\" : {\"data-collections\" [\"<collectionName>\", \"<collectionName>\"],\"type\": \"incremental\"}});\n\nFor example,\n\ndb.debeziumSignal.insert({ (1) \"type\" : \"stop-snapshot\", (2) (3) \"data\" : { \"data-collections\" [\"\\\"public\\\".\\\"Collection1\\\"\", \"\\\"public\\\".\\\"Collection2\\\"\"], (4) \"type\": \"incremental\"} (5) });\n\nThe values of the id, type, and data parameters in the signal command correspond to the fields of the signaling collection.\n\nThe following table describes the parameters in the example:\n\nTable 5. Descriptions of fields in an insert command for sending a stop incremental snapshot document to the signaling collection Item Value Description\n\nUsing the Kafka signaling channel to stop an incremental snapshot\n\nYou can send a signal message to the configured Kafka signaling topic to stop an ad hoc incremental snapshot.\n\nThe key of the Kafka message must match the value of the topic.prefix connector configuration option.\n\nThe value of the message is a JSON object with type and data fields.\n\nThe signal type is stop-snapshot, and the data field must have the following fields:\n\nTable 6. Execute snapshot data fields Field Default Value\n\nThe following example shows a typical stop-snapshot Kafka message:\n\nKey = `test_connector` Value = `{\"type\":\"stop-snapshot\",\"data\": {\"data-collections\": [\"schema1.table1\", \"schema1.table2\"], \"type\": \"INCREMENTAL\"}}`\n\nStreaming changes\n\nAfter the connector task for a replica set records an offset, it uses the offset to determine the position in the oplog where it should start streaming changes. The task then (depending on the configuration) either connects to the replica set’s primary node or connects to a replica-set-wide change stream and starts streaming changes from that position. It processes all of create, insert, and delete operations, and converts them into Debezium change events. Each change event includes the position in the oplog where the operation was found, and the connector periodically records this as its most recent offset. The interval at which the offset is recorded is governed by offset.flush.interval.ms, which is a Kafka Connect worker configuration property.\n\nWhen the connector is stopped gracefully, the last offset processed is recorded so that, upon restart, the connector will continue exactly where it left off. If the connector’s tasks terminate unexpectedly, however, then the tasks may have processed and generated events after it last records the offset but before the last offset is recorded; upon restart, the connector begins at the last recorded offset, possibly generating some the same events that were previously generated just prior to the crash.\n\nAs mentioned earlier, the connector tasks always use the replica set’s primary node to stream changes from the oplog, ensuring that the connector sees the most up-to-date operations as possible and can capture the changes with lower latency than if secondaries were to be used instead. When the replica set elects a new primary, the connector immediately stops streaming changes, connects to the new primary, and starts streaming changes from the new primary node at the same position. Likewise, if the connector experiences any problems communicating with the replica set members, it tries to reconnect, by using exponential backoff so as to not overwhelm the replica set, and once connected it continues streaming changes from where it last left off. In this way, the connector is able to dynamically adjust to changes in replica set membership and automatically handle communication failures.\n\nTo summarize, the MongoDB connector continues running in most situations. Communication problems might cause the connector to wait until the problems are resolved.\n\nThe following skeleton JSON shows the basic four parts of a change event. However, how you configure the Kafka Connect converter that you choose to use in your application determines the representation of these four parts in change events. A schema field is in a change event only when you configure the converter to produce it. Likewise, the event key and event payload are in a change event only if you configure a converter to produce it. If you use the JSON converter and you configure it to produce all four basic change event parts, change events have this structure:\n\nOptimal Oplog Config\n\nThe Debezium MongoDB connector reads change streams to obtain oplog data for a replica set. Because the oplog is a fixed-sized, capped collection, if it exceeds its maximum configured size, it begins to overwrite its oldest entries. If the connector is stopped for any reason, when it restarts, it attempts to resume streaming from the last oplog stream position. However, if last stream position was removed from the oplog, depending on the value specified in the connector’s snapshot.mode property, the connector might fail to start, reporting an invalid resume token error. In the event of a failure, you must create a new connector to enable Debezium to continue capturing records from the database. For more information, see Connector fails after it is stopped for a long interval if snapshot.mode is set to initial.\n\nTo ensure that the oplog retains the offset values that Debezium requires to resume streaming, you can use either of the following approaches:\n\nIncrease the size of the oplog. Based on your typical workloads, set the oplog size to a value that is greater than the peak number of oplog entries per hour.\n\nIncrease the minimum number of hours that an oplog entry is retained (MongoDB 4.4 and greater). This setting is time-based, such that entries in the last n hours are guaranteed to be available even if the oplog reaches its maximum configured size. Although this is generally the preferred option, for clusters with high workloads that are nearing capacity, specify the maximum oplog size.\n\nTo help prevent failures that are related to missing oplog entries, it’s important to track metrics that report replication behavior, and to optimize the oplog size to support Debezium. In particular, you should monitor the values of Oplog GB/Hour and Replication Oplog Window. If Debezium is offline for an interval that exceeds the value of the replication oplog window, and the primary oplog grows faster than Debezium can consume entries, a connector failure can result.\n\nFor information about how to monitor these metrics, see the MongoDB documentation.\n\nIt’s best to set the maximum oplog size to a value that is based on the anticipated hourly growth of the oplog (Oplog GB/Hour), multiplied by the time that might be required to address a Debezium failure.\n\nThat is,\n\nOplog GB/Hour X average reaction time to Debezium failure\n\nFor example, if the oplog size limit is set to 1GB, and the oplog grows by 3GB per hour, oplog entries are cleared three times per hour. If Debezium were to fail during this time, its last oplog position is likely to be removed.\n\nIf the oplog grows at the rate of 3GB/hour, and Debezium is offline for two hours, you would thus set the oplog size to 3GB/hour X 2 hours, or 6GB.\n\nCustomized MBean names\n\nDebezium connectors expose metrics via the MBean name for the connector. These metrics, which are specific to each connector instance, provide data about the behavior of the connector’s snapshot, streaming, and schema history processes.\n\nBy default, when you deploy a correctly configured connector, Debezium generates a unique MBean name for each of the different connector metrics. To view the metrics for a connector process, you configure your observability stack to monitor its MBean. But these default MBean names depend on the connector configuration; configuration changes can result in changes to the MBean names. A change to the MBean name breaks the linkage between the connector instance and the MBean, disrupting monitoring activity. In this scenario, you must reconfigure the observability stack to use the new MBean name if you want to resume monitoring.\n\nTo prevent monitoring disruptions that result from MBean name changes, you can configure custom metrics tags. You configure custom metrics by adding the custom.metric.tags property to the connector configuration. The property accepts key-value pairs in which each key represents a tag for the MBean object name, and the corresponding value represents the value of that tag. For example: k1=v1,k2=v2. Debezium appends the specified tags to the MBean name of the connector.\n\nAfter you configure the custom.metric.tags property for a connector, you can configure the observability stack to retrieve metrics associated with the specified tags. The observability stack then uses the specified tags, rather than the mutable MBean names to uniquely identify connectors. Later, if Debezium redefines how it constructs MBean names, or if the topic.prefix in the connector configuration changes, metrics collection is uninterrupted, because the metrics scrape task uses the specified tag patterns to identify the connector.\n\nA further benefit of using custom tags, is that you can use tags that reflect the architecture of your data pipeline, so that metrics are organized in a way that suits you operational needs. For example, you might specify tags with values that declare the type of connector activity, the application context, or the data source, for example, db1-streaming-for-application-abc. If you specify multiple key-value pairs, all of the specified pairs are appended to the connector’s MBean name.\n\nThe following example illustrates how tags modify the default MBean name.\n\nConnector fails after it is stopped for a long interval if snapshot.mode is set to initial\n\nIf the connector is gracefully stopped, users might continue to perform operations on replica set members. Changes that occur while the connector is offline continue to be recorded in MongoDB’s oplog. In most cases, after the connector is restarted, it reads the offset value in the oplog to determine the last operation that it streamed for each replica set, and then resumes streaming changes from that point. After the restart, database operations that occurred while the connector was stopped are emitted to Kafka as usual, and after some time, the connector catches up with the database. The amount of time required for the connector to catch up depends on the capabilities and performance of Kafka and the volume of changes that occurred in the database.\n\nHowever, if the connector remains stopped for a long enough interval, it can occur that MongoDB purges the oplog during the time that the connector is inactive, resulting in the loss of information about the connector’s last position. After the connector restarts, it cannot resume streaming, because the oplog no longer contains the previous offset value that marks the last operation that the connector processed. The connector also cannot perform a snapshot, as it typically would when the snapshot.mode property is set to initial, and no offset value is present. In this case, a mismatch exists, because the oplog does not contain the value of the previous offset, but the offset value is present in the connector’s internal Kafka offsets topic. An error results and the connector fails.\n\nTo recover from the failure, delete the failed connector, and create a new connector with the same configuration but with a different connector name. When you start the new connector, it performs a snapshot to ingest the state of database, and then resumes streaming."
    }
}
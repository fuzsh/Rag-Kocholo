{
    "id": "dbpedia_7694_3",
    "rank": 60,
    "data": {
        "url": "https://www.cfm.brown.edu/people/dobrush/am33/Mathematica/ch5/part5.html",
        "read_more_link": "",
        "language": "en",
        "title": "MATHEMATICA tutorial for first course, part V: Series and Recurrences",
        "top_image": "",
        "meta_img": "",
        "images": [
            "https://www.cfm.brown.edu/people/dobrush/am33/Mathematica/ch5/part33-5/sinarcsin.png",
            "https://www.cfm.brown.edu/people/dobrush/am33/Mathematica/ch5/part33-5/sinpoly4.png",
            "https://www.cfm.brown.edu/people/dobrush/am33/Mathematica/ch5/part33-5/sinpoly.png",
            "https://www.cfm.brown.edu/people/dobrush/am33/Mathematica/ch5/part33-5/sinpoly2.png",
            "https://www.cfm.brown.edu/people/dobrush/am33/Mathematica/ch5/part33-5/sinpoly3.png",
            "https://www.cfm.brown.edu/people/dobrush/am33/Mathematica/ch5/part33-5/sinpoly9.png",
            "https://www.cfm.brown.edu/people/dobrush/am33/Mathematica/ch5/part33-5/riccati1.png"
        ],
        "movies": [],
        "keywords": [],
        "meta_keywords": [
            ""
        ],
        "tags": null,
        "authors": [],
        "publish_date": null,
        "summary": "",
        "meta_description": "",
        "meta_lang": "",
        "meta_favicon": "",
        "meta_site_name": "",
        "canonical_link": null,
        "text": "Series and Recurrences\n\nOwing to the complicated structure of some ordinary differential equations, it is not always possible to obtain the corresponding solution of an initial value problem in a reasonable form. In such situations, we need to resort to methods that produce an approximate solution, of which we choose the form of an infinite series. There are some reasons why we go in this way. First of all, historically speaking power series were the first tool to approximate functions, and this topic is part of the calculus course. Second, when a function is represented (or approximated) by a power series, all its coefficients are determined by derivatives evaluated at one point. So only infinitesimal knowledge of the function at one single point is needed to find all coefficients of the corresponding power series. This makes Taylor series an appropriate technique for solving initial value problems because solutions are also determined by the the initial conditions imposed at one single point. Since a power series converges within a symmetrical interval \\( \\left\\vert x - x_0 \\right\\vert < R, \\) it is usually referred to as a local method. In what follows, we illustrate a procedure of this type, based on series expansions for functions of a real variable.\n\nThe topic of this chapter is roughly divided into three parts. Since applications of power series for solving differential equations lead to developing and solving recurrences for its coefficients, the first part is devoted to difference equations and generating functions. In the second part, we present three approaches to solve nonlinear differential equations: power series, Picard's iteration, and the Adomian decomposition method. The rest of the chapter is about solving linear differential equations with variable coefficients. The majority of it is devoted to series representations of solutions for equations with regular singular points, developed by the German mathematicians Lazarus Fuchs (1833--1902) in 1866 and Ferdinand Frobenius (1849--1917) in 1873.\n\nUntil now, we discussed methods for solving second and higher order constant coefficient differential equations. In applications, higher order nonlinear equations and linear equations with variable coefficients are just as important, if not more so, than equations with constant coefficients. In this chapter, we turn our attention to variable coefficient linear differential equations, and nonlinear equations. Out of these, the second order equations play a crucial role for several reasons. First of all, they are the most simple equations to analyze. Secondly, the second order equations are frequently used in applications, and thirdly, the majority of other equations can be reduced to these equations. For example, the Riccati equation is equivalent to a second order differential equation with variable coefficients. Moreover, the methods involved in solving differential equations of the second order can be easily extended to the higher order equations.\n\nOnce we done with nonlinear equations, we turn our attention to linear differential equations with variable coefficients. Here the Fuchs method will be our main tool. The procedure is similar to the use of undetermined coefficients for polynomial solutions, except that there are infinitely many coefficients. In practical applications, we don't need to determine all of them, but some finite number. Equating coefficients of like powers, we obtain the recursive relation for determination of values of coefficients in terms of its predecessors. Before computers were available, it was very tedious procedure that limited their applications. Now we can dedicate this job to a computer algebra system.\n\nTaylor's Series\n\nIf a real-valued function f(x) has N+1 continuous derivatives on the interval 𝑎 ≤ x ≤ b, then\n\n\\begin{equation} f(x) = \\sum_{n=0}^N c_n \\left( x - x_0 \\right)^n + R_{N+1} = \\sum_{n=0}^N \\frac{1}{n!}\\, f^{(n)} \\left( x_0 \\right) \\left( x - x_0 \\right)^n + R_{N+1} (x), \\qquad a < x < b, \\label{EqTaylor.1} \\end{equation}\n\nwhere the coefficients cn are (linear) functionals on the space of holomorphic functions, cn : 𝓗 → &reals;, defined by\n\n\\begin{equation} \\label{EqTaylor.2} c_n = c_n (f) = \\frac{1}{n!}\\,\\lim_{x\\to x_0} \\,\\texttt{D}^n f (x) = \\frac{1}{n!}\\,\\lim_{x\\to x_0} \\,\\frac{{\\text d}^n f(x)}{{\\text d} x^n} , \\end{equation}\n\nand the remainder term is\n\n\\begin{equation} R_{N+1} (x) = \\frac{f^{(N+1)} (\\xi )}{(N+1)!} \\left( x - x_0 \\right)^{N+1} = \\frac{1}{N!} \\int_{x_0}^x \\left( x-t \\right)^N f^{(N+1)} (t)\\,{\\text d}t \\label{EqTaylor.3} \\end{equation}\n\nfor some (unknown) ξ in the interval 𝑎 ≤ ξ ≤ x ≤ b. The latter form of reminder is often arrived at by a succession of integration by parts; the former remainder, called after Lagrange, is derived by an application of Rolle's theorem or the mean-value theorem to a suitable function. Note that ξ in the Lagrange reminder is not merely a constant, but it is a function of the endpoint x.\n\nIf we set RN+1 = 0, an N-th degree polynomial approximation to f(x) is obtained:\n\n\\begin{equation} \\label{EqTaylor.7} T_N (x) = \\sum_{n=0}^N c_n \\left( x - x_0 \\right)^n , \\qquad c_n = \\left[ x^n \\right] f(x) = \\frac{1}{n!}\\,f^{(n)} \\left( x_0 \\right) , \\quad n= 0,1,2,\\ldots , N. \\end{equation}\n\nHere we use the standard notation \\( \\displaystyle \\left[ x^n \\right] f(x) \\) to extract n-th coefficient of the Taylor series expansion for function f(x). This polynomial TN(x) is called the Taylor polynomial of degree N for the function f(x). If the center x0 is chosen to be zero, the corresponding polynomial is usually referred to as the Maclaurin polynomial. The magnitude of RN+1 provides an error estimate for this polynomial which can be found if a suitable bound on \\( \\left\\vert f^{(N+1)} (\\xi ) \\right\\vert \\) over [𝑎,b] is known. The Taylor's series for a function having infinitely many derivatives is obtained by taking the limit N → ∞,\n\n\\begin{equation} f(x) = \\sum_{n\\ge 0} c_n \\left( x - x_0 \\right)^n = \\sum_{n\\ge 0} \\frac{f^{(n)} \\left( x_0 \\right)}{n!} \\left( x - x_0 \\right)^n , \\qquad -r < x - x_0 < r , \\label{EqTaylor.4} \\end{equation}\n\nsubject the remainder term tends to zero. This series for arctan x was formulated by the Scottish mathematician and astronomer James Gregory in his book Geometriae Pars Universalis (1668). In the same year, N. Mercator gave the series expansion of ln(1 +x) in his Logarithmotechnia and I. Newton obtained the series expansion for (1 +x)α, sin x, cos x and exp x, which appeared in the correspondence with Leibniz in 1676. What is less well-known is that the Indian Kerala school founded by Madhava of Sangamagrama have already knew the series expansion of these functions. It seems that the first mathematician to give a general formula for series expansion of a function was Johann Bernoulli in 1694 (published in Acta eruditorum). It was Joseph-Louis Lagrange who called this series after B. Taylor.\n\nIn 1715, the English mathematician Brook Taylor formally introduced the formula \\( f(x+h) = f(x) + h\\,f'(x) + \\cdots + \\frac{h^n}{n!}\\,f^{(n)} (x) + \\cdots , \\) without any conditions for the validity of such representation. To obtain such formula, Taylor used the theory of finite differences. If the Taylor series is centered at zero, then that series is also called a Maclaurin series, after the Scottish mathematician Colin Maclaurin, who made extensive use of this special case of Taylor series in his Treatise of Fluxions of 1742. The necessary and sufficient conditions for this series to exist and to sum to the function f(x) is that RN+1 → 0 as N → ∞. The point x0 is usually called the center of Taylor's series. By changing the variables t = x - x0, any Taylor's series can be transferred to a Maclaurin series. The above power series representation is known as the local expansion because its terms are determined by infinitesimal behavior of the function f(x) at one single point---its center x0.\n\nWhen a function can be developed into a convergent infinite series, it is referred to be a sum-function. Not every smooth (infinitely differentiable) function is a sum-function for its Taylor series; but if it is, it can be a sum-function for another series. For example, the geometric series\n\n\\[ \\frac{1}{1-x} = \\sum_{n\\ge 0} x^n = - \\sum_{k\\ge 0} (-1)^k (x-2)^k . \\]\n\nAnother important example of the Maclaurin series provides the binomial theorem:\n\n\\[ (1+x)^m = \\sum_{k\\ge 0} \\binom{m}{k} x^k , \\]\n\nwhere \\( \\displaystyle \\binom{m}{k} = \\frac{m^{\\underline{k}}}{k!} \\) is the binomial coefficient. Note that \\( m^{\\underline{k}} = m\\left( m-1 \\right) \\left( m-2 \\right) \\cdots \\left( m-k+1 \\right) \\) is the k-th falling factorial. So an analytic functions is locally a holomorphic function because it has a convergent Taylor series expansion, but globally it may not. A holomorphic function is a single-valued infinitely differentiable function as the sum of the corresponding convergent power series. Generally speaking, an analytic function may not be a function on some domain of the complex plane ℂ in a pure mathematical sense because it may consist of several branches and may assign to one input (point) several outputs. In the nineteenth century, analytic functions were called systems, but now it is common to utilize the word \"function\" because they become functions on Riemann surfaces instead of the complex plane &Copf;. For example, the square root \\( \\sqrt{z} \\) is an analytic function but not a holomorphic function because it assigns two values to every nonzero input z&in;&Copf; depending on which holomorphic branch is chosen. In particular, \\( \\sqrt{-1} = \\pm{\\bf j}. \\) So the analytic square root function consists of two branches and each of them is a holomorphic function on a domain, which is a subspace of ℂ.\n\nFor a holomorphic function, the sequence of its Taylor coefficients is defined uniquely because they are determined by evaluating the derivatives of the given function at one single point: \\( \\displaystyle c_n = c_n (f) = \\frac{1}{n!}\\,f^{(n)} \\left( x_0 \\right) , \\quad n=0,1,2,\\ldots .\\) So there is a one-to-one correspondence between infinite sequences of Taylor coefficients and holomorphic functions. On the other hand, if a sequence { cn } is known, we can assign a series\n\n\\[ f(x) = \\sum_{n\\ge 0} c_n \\left( x - x_0 \\right)^n \\]\n\nsubject that the series converges in some neighborhood of the x0. For such given sequence of real or complex numbers, the central point x0 is irrelevant. Upon shifting the independent variable z = x - x0, we can assign the series centered at the origin \\( f(z) = \\sum_{n\\ge 0} c_n z^n , \\) and this series is usually referred to as the generating function for the sequence of coefficients { cn }.\n\nIn applications, we usually cannot operate with infinite series, but only with their finite parts, called truncated series, that keeps only finite number of initial terms from infinite series. It leads to definition of the N-th degree Taylor polynomial TN(x) of f centered at x0 is the N-th partial sum of the Taylor series (this is actually the truncated version of the Taylor series, containing only N+1 terms)\n\n\\[ T_N (x) = \\sum_{n= 0}^N c_n \\left( x - x_0 \\right)^n , \\qquad c_n = \\frac{1}{n!}\\, f^{(n)} \\left( x_0 \\right) , \\quad n=0,1,2,\\ldots . \\tag{4} \\]\n\nHere \\( c_0 , c_1 , \\ldots \\) are coefficients of the above power series centered at x0. The local nature of the approximation is also revealed by the fact that a Taylor series converges on some interval \\( \\left\\vert x - x_0 \\right\\vert < R \\) around the point x = x0 where the series expansion is anchored. You are probably worrying how on earth we can use this formula to get actual numbers if we don’t know what ξ is. Good question. What we need to do is look at all the values of \\( f^{(N+1)} (\\xi ) \\) for all x0 < ξ < x and use the largest of them. Or, pick something that we know is surely larger than all of them.\n\nPower Series Solutions to Differential Equations\n\nCan Taylor series be used for practical determinations of ODE solutions? Not really because in most cases we don't know apriori the radius of convergence of such series (which depends on estimates of higher derivatives for an unknown solution). This means that for proper Taylor's approximation one needs to perform additional analysis. Remember that a Taylor's series uses only infinitesimal information of its sum-function, so we expect good approximation only in a small neighborhood of the center. Nevertheless, power series method becomes essential for numerical calculations for small domains close to the center of expansion. For example, the spline method usually uses cubic approximations locally. Fortunately, Mathematica has a dedicated command AsymptoticDSolveValue for determination of a power series approximation for a solution.\n\nA differential equation establishes a relation between derivatives of unknown function and the function itself. When power series method is applied for solving differential equations, the main issue is to recover a relation between coefficients of the sum-function according to the given differential equation. Since the derivative of the solution represented by a power series\n\n\\[ f'(x) = \\sum_{n\\ge 1} n\\,c_n \\left( x - x_0 \\right)^{n-1} = \\sum_{n\\ge 0} c_{n+1} \\left( n+1 \\right) \\left( x - x_0 \\right)^n \\]\n\nmakes a shift in its coefficients, the required relation between its coefficients becomes a recurrence or difference equation. This is the main reason why we discuss recurrences in the first part of this chapter. It should be noted that this topic is important in other branches of science, including numerical analysis.\n\nThe hunting license for finding solutions of differential equations in the form of power series gives the following famous theorem credited to Augustin Cauchy (1842) and Sophie Kovalevskaya (1875). We present its simple version; however, the reader can find its numerous extensions elsewhere. The Cauchy--Kovalevskaya theorem does not provide explicitly the radius of convergence for series solution to the initial value problem. Therefore, it has only theoretical meaning and we need to find other resources for its identification.\n\nThe above existence theorem gives a sufficient condition for a unique solution, and, moreover, it suggests a possible form for that solution. In general, failure to satisfy the conditions of the above theorem does not prevent the existence of a holomorphic solution. The German mathematician Karl Theodor Wilhelm Weierstrass (1815--1897) proved in 1885 the following theorem.\n\nMathematica has a dedicated command to find power series expansion of the solution to the initial value problem: AsymptoticDSolveValue.\n\nA power series \\( \\phi (x) = \\sum_{n\\ge 0} c_n \\left( x - x_0 \\right)^n \\) converges (absolutely) within a symmetric interval \\( \\left\\vert x - x_0 \\right\\vert < R , \\) and diverges outside. If R is a positive number, then we say that the power series converges; if R = 0, the series diverges everywhere except the center x0. The number R is called the radius of convergence. If the solution to a differential equation is represented by a power series, its radius of convergence not only limits its validity interval, but also gives a qualitative description of the sum-function. The radius of convergence is the distance to the nearest singular point, which may belong to the boundary of the validity interval or may not when the sum has either a complex singularity or a branch point. Generally speaking, the radius of convergence can not be determined from the validity interval. Singular points also affect numerical algorithms used to approximate solutions.\n\nOver the past two hundred years, some equations are so frequent in the physical applications that their series solutions have led to the introduction of new functions (Bessel, hypergeometric, etc.). The solutions (in the series form) to these equations have been determined and their properties have been intensively studied. The corresponding branch of mathematics is called the theory of special functions, which is devoted to the study and applications of functions not expressible through elementary functions. The majority of these special functions are solutions of differential equations with singular points where the coefficients are undefined. These equations were studied for the first time by Briot and Bouquet in 19-th century. Below are some examples of singular differential equations.\n\nAs the above example shows, differential equations with initial conditions specified at the singular point may lead to serious circumstances---even of impossibility---to define a solution at these points. Even when initial an condition can be specified at the singular point, the corresponding initial value problem for such equation with the initial condition at the singular point may have multiple solutions or may not have a holomorphic solution. Therefore, at the end of this chapter, we will discuss initial value problems for a certain class of linear singular equation, called Fuchsian equations.\n\nAlso, the linear differential equations can be solved in terms of power series using the command DifferentialRoot:\n\nlde = {y''[x] - x*x*y'[x] + Cos[x] == 0, y'[0] == 1, y[0] == 0};\n\nSeries[DifferentialRoot[Function @@ {{y, x}, lde}][x], {x, 0, 15}]\n\nx - x^2/2 + x^4/8 - x^5/20\n\nWe can write the script to determine the series expansion of a second order differential equation.\n\nClear[seriesDSolve];\n\nseriesDSolve[ode_, y_, iter : {x_, x0_, n_}, ics_: {}] :=\n\nModule[{dorder, coeff}, dorder = Max[0, Cases[ode, Derivative[m_][y][x] :> m, Infinity]];\n\n(coeff = Fold[Flatten[{#1, Solve[#2 == 0 /. #1,\n\nUnion@Cases[#2 /. #1, Derivative[m_][y][x0] /; m >= dorder, Infinity]]}] &, ics,\n\nTable[SeriesCoefficient[ ode /. Equal -> Subtract, {x, x0, k}], {k, 0, Max[n - dorder, 0]}]];\n\nSeries[y[x], iter] /. coeff) /; dorder > 0]\n\nand the apply to the initial value problem:\n\nseriesDSolve[ y''[x] + y'[x] == Sin[3 y[x]], y, {x, 0, 5}, {y[0] -> 1, y'[0] -> 0}]\n\nWe can apply the standard Mathematica command\n\node = x''[t] - t*t*x'[t] + Cos[t] == 0;\n\ninitconds = {x'[0] == 1, x[0] == 0};\n\nAsymptoticDSolveValue[{ode, initconds}, x[t], {t, 0, 8}]\n\nWe create a differentail operator to generate this ODE:\n\nodeOperator = D[#, {t, 2}] - t*t*D[#, t] + Cos[t] &;\n\nNow set up our Taylor series as a symbolic expansion using derivatives of `x` evaluated at the origin. I use an order of 8 but that is something one would probably make as an argument to a function, if automating all this.\n\nxx = Series[x[t], {t, 0, 8}];\n\nNext apply the differential operator and add the initial conditions. Then find a solution that makes all powers of `t vanish.\n\nsoln = SolveAlways[Join[{odeOperator[xx] == 0}, initconds], t];\n\nLet's look at the Taylor polynomial:\n\ntruncatedSol = Normal[xx /. soln[[1]]]\n\nTo assess how accurate it might be I will compare with NDSolve\n\napproxSol = NDSolve[Join[{ode}, initconds], x[t], {t, 0, 4}];\n\nPlot[{Callout[truncatedSol, \"polynomial\"], Callout[x[t] /. approxSol[[1]], \"true solution\"]}, {t, 0, 3}, PlotTheme -> \"Web\"]\n\nRadius of convergence for the series solution\n\nSuppose we are given a linear second order differential equation\n\n\\begin{equation} \\label{EqTaylor.5} a_2 (x)\\,y'' + a_1 (x) \\,y' + a_0 (x)\\,y = f(x) , \\end{equation}\n\nwhere 𝑎0, 𝑎1, 𝑎2, and f(x) are holomorphic functions centered at the point x = x0. Then Eq.\\eqref{EqTaylor.5} has a series solution\n\n\\begin{equation} \\label{EqTaylor.6} y(x) = \\sum_{n\\ge 0} c_n \\left( x - x_0 \\right)^n \\end{equation}\n\nthat converges in the circle |x - x0| < ρ of radius ρ that is the distance to the nearest singularity of each of the functions 𝑎0, 𝑎1, 𝑎2, and f(x)."
    }
}
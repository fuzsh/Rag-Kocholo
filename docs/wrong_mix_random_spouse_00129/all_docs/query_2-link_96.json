{
    "id": "wrong_mix_random_spouse_00129_2",
    "rank": 96,
    "data": {
        "url": "https://www.analyticsvidhya.com/blog/2023/08/knowledge-graphs-in-ai-and-data-science/",
        "read_more_link": "",
        "language": "en",
        "title": "Knowledge Graphs: The Game-Changer in AI and Data Science",
        "top_image": "https://av-eks-lekhak.s3.amazonaws.com/media/__sized__/article_images/Screenshot_from_2023-07-30_20-55-04_IJhhZZl-thumbnail_webp-600x300.webp",
        "meta_img": "",
        "images": [
            "https://av-public-assets.s3.ap-south-1.amazonaws.com/logos/av-logo-svg.svg",
            "https://www.analyticsvidhya.com/wp-content/themes/analytics-vidhya/images/navbar.jpg",
            "https://www.analyticsvidhya.com/wp-content/themes/analytics-vidhya/images/navbar.jpg",
            "https://www.analyticsvidhya.com/wp-content/themes/analytics-vidhya/images/navbar.jpg",
            "https://www.analyticsvidhya.com/wp-content/themes/analytics-vidhya/images/navbar.jpg",
            "https://www.analyticsvidhya.com/wp-content/themes/analytics-vidhya/images/navbar.jpg",
            "https://www.analyticsvidhya.com/wp-content/themes/analytics-vidhya/images/default_avatar.svg",
            "https://av-identity.s3.amazonaws.com/users/user/iGG1M3RkR6GpcDH4SMLRLQ.png",
            "https://av-eks-lekhak.s3.amazonaws.com/media/__sized__/article_images/Screenshot_from_2023-07-30_20-55-04_IJhhZZl-thumbnail_webp-600x300.webp",
            "https://av-eks-lekhak.s3.amazonaws.com/media/__sized__/article_images/Screenshot_from_2023-07-30_19-57-34-thumbnail_webp-600x300.webp",
            "https://av-eks-lekhak.s3.amazonaws.com/media/__sized__/article_images/Screenshot_from_2023-07-30_20-04-30-thumbnail_webp-600x300.webp",
            "https://av-eks-lekhak.s3.amazonaws.com/media/__sized__/article_images/Screenshot_from_2023-07-30_20-04-54-thumbnail_webp-600x300.webp",
            "https://av-eks-lekhak.s3.amazonaws.com/media/__sized__/article_images/Screenshot_from_2023-07-30_20-07-23-thumbnail_webp-600x300.webp",
            "https://av-eks-lekhak.s3.amazonaws.com/media/__sized__/article_images/Screenshot_from_2023-07-30_20-38-08-thumbnail_webp-600x300.webp",
            "https://av-eks-lekhak.s3.amazonaws.com/media/__sized__/article_images/Screenshot_from_2023-07-30_22-38-08-thumbnail_webp-600x300.webp",
            "https://av-eks-lekhak.s3.amazonaws.com/media/__sized__/article_images/Screenshot_from_2023-07-30_20-42-42_lKyDoSj-thumbnail_webp-600x300.webp",
            "https://av-eks-lekhak.s3.amazonaws.com/media/__sized__/article_images/Screenshot_from_2023-08-01_22-54-45-thumbnail_webp-600x300.webp",
            "https://av-eks-lekhak.s3.amazonaws.com/media/__sized__/article_images/image_7kZ6uH1-thumbnail_webp-600x300.webp",
            "https://av-eks-lekhak.s3.amazonaws.com/media/__sized__/article_images/image_ttkR0CP-thumbnail_webp-600x300.webp",
            "https://av-eks-lekhak.s3.amazonaws.com/media/__sized__/article_images/image_J8BOKRp-thumbnail_webp-600x300.webp",
            "https://av-identity.s3.amazonaws.com/users/user/iGG1M3RkR6GpcDH4SMLRLQ.png",
            "https://www.analyticsvidhya.com/wp-content/themes/analytics-vidhya/images/removeAfterProdcution/path-digital.png",
            "https://av-identity.s3.amazonaws.com/users/user/bGnsep7nT0GMWuLpkDl15Q.jpg",
            "https://www.analyticsvidhya.com/wp-content/themes/analytics-vidhya/images/removeAfterProdcution/in.png",
            "https://av-identity.s3.amazonaws.com/users/user/R7HrsWl1QrGRiw_e9m4fDA.jpg",
            "https://www.analyticsvidhya.com/wp-content/themes/analytics-vidhya/images/removeAfterProdcution/in.png",
            "https://av-identity.s3.amazonaws.com/users/user/ZcU4ALTFT96MVCzfiGuhsQ.jpeg",
            "https://www.analyticsvidhya.com/wp-content/themes/analytics-vidhya/images/removeAfterProdcution/in.png",
            "https://av-identity.s3.amazonaws.com/users/user/aM3WrxdNSTGLg7LoqX-q0w.png",
            "https://www.analyticsvidhya.com/wp-content/themes/analytics-vidhya/images/removeAfterProdcution/in.png",
            "https://av-identity.s3.amazonaws.com/users/user/zy4FL_yyQlG4PkWcyGYvhw.jpg",
            "https://www.analyticsvidhya.com/wp-content/themes/analytics-vidhya/images/removeAfterProdcution/in.png",
            "https://av-identity.s3.amazonaws.com/users/user/a4ByfUyoQRmdGzLpBzHVLw.jpeg",
            "https://www.analyticsvidhya.com/wp-content/themes/analytics-vidhya/images/removeAfterProdcution/in.png",
            "https://av-identity.s3.amazonaws.com/users/user/ZTsmKl-1Qvqn07FUzgaBNw.png",
            "https://www.analyticsvidhya.com/wp-content/themes/analytics-vidhya/images/removeAfterProdcution/in.png",
            "https://d2cd20fxv8fgim.cloudfront.net/homepage/images/Play_Store.svg",
            "https://d2cd20fxv8fgim.cloudfront.net/homepage/images/App_Store.svg"
        ],
        "movies": [],
        "keywords": [],
        "meta_keywords": [
            ""
        ],
        "tags": null,
        "authors": [
            "Kajal Kumari"
        ],
        "publish_date": "2023-08-05T17:30:00+00:00",
        "summary": "",
        "meta_description": "Learn about the power of Knowledge Graphs in AI and Data Science. Unravel the secrets of structured information retrieval and reasoning.",
        "meta_lang": "en",
        "meta_favicon": "https://imgcdn.analyticsvidhya.com/favicon/av-fav.ico",
        "meta_site_name": "Analytics Vidhya",
        "canonical_link": "https://www.analyticsvidhya.com/blog/2023/08/knowledge-graphs-in-ai-and-data-science/",
        "text": "Introduction\n\nKnowledge graphs have emerged as a powerful and versatile approach in AI and Data Science for recording structured information to promote successful data retrieval, reasoning, and inference. This article examines state-of-the-art knowledge graphs, including construction, representation, querying, embeddings, reasoning, alignment, and fusion.\n\nWe also discuss the many applications of knowledge graphs, such as recommendation engines and question-answering systems. Finally, in order to pave the way for new advancements and research opportunities, we explore the subject’s problems and potential future routes.\n\nKnowledge graphs have revolutionized how information is organized and used by providing a flexible and scalable mechanism to express complicated connections between entities and characteristics. Here, we give a general introduction to knowledge graphs, their importance, and their potential use across various fields.\n\nLearning Objective\n\nUnderstand the concept and purpose of knowledge graphs as structured representations of information.\n\nLearn about the key components of knowledge graphs: nodes, edges, and properties.\n\nExplore the construction process, including data extraction and integration techniques.\n\nUnderstand how knowledge graph embeddings represent entities and relationships as continuous vectors.\n\nExplore reasoning methods to infer new insights from existing knowledge.\n\nGain insights into knowledge graph visualization for better understanding.\n\nThis article was published as a part of the Data Science Blogathon.\n\nWhat is a Knowledge Graph?\n\nA knowledge graph can store the extracted information during an information extraction operation. Many fundamental knowledge graph implementations utilize the idea of a triple, which is a collection of three elements (a subject, a predicate, and an object) that can hold information about anything.\n\nA graph is a collection of nodes and edges.\n\nThis is the smallest knowledge graph we can design, also known as a triple. Knowledge Graphs come in a number of forms and sizes. Here, Node A and Node B here are two separate things. These nodes are connected by an edge that shows the relationship between the two nodes.\n\nData Representation in Knowledge Graph\n\nTake the following phrase as an illustration:\n\nLondon is the capital of England. Westminster is located in London.\n\nWe will see some basic processing later, but initially, we would have two triples looking like this:\n\n(London, be capital, England), (Westminster, locate, London)\n\nIn this example, we have three distinct entities (London, England, and Westminster) and two relationships (capital, location). Constructing a knowledge graph requires only two related nodes in the network with the entities and vertices with the relations. The resulting structure is as follows: Creating a knowledge graph manually is not scalable. No one will go through hundreds of pages to extract all the entities and their relationships!\n\nBecause they can easily sort through hundreds or even thousands of papers, robots are more suited to handle this work than people. The fact that machines cannot grasp natural language presents another difficulty. Using natural language processing (NLP) in this situation is important.\n\nMaking our computer understand natural language is crucial if we want to create a knowledge graph from the text. Using NLP methods to do this, including sentence segmentation, dependency parsing, parts of speech tagging, and entity recognition.\n\nImport Dependencies & Load dataset\n\nimport re import pandas as pd import bs4 import requests import spacy from spacy import displacy nlp = spacy.load('en_core_web_sm') from spacy.matcher import Matcher from spacy.tokens import Span import networkx as nx import matplotlib.pyplot as plt from tqdm import tqdm pd.set_option('display.max_colwidth', 200) %matplotlib inline\n\n# import wikipedia sentences candidate_sentences = pd.read_csv(\"../input/wiki-sentences1/wiki_sentences_v2.csv\") candidate_sentences.shape\n\ncandidate_sentences['sentence'].sample(5)\n\nSentence Segmentation\n\nSplitting the text article or document into sentences is the first stage in creating a knowledge graph. Then, we will only shortlist the phrases that have precisely one subject and one object.\n\ndoc = nlp(\"the drawdown process is governed by astm standard d823\") for tok in doc: print(tok.text, \"...\", tok.dep_)\n\nEntities Extraction\n\nA single-word component of a sentence can easily be removed. We can achieve this rapidly by using parts of speech (POS) tags. Nouns and proper nouns would be our entities.\n\nWhen an entity spans many words, POS tags alone are inadequate. The dependency tree of the statement must be parsed.\n\nThe nodes and their relationships are most important when developing a knowledge graph.\n\nThese nodes will be made up of entities found in Wikipedia texts. Edges reflect the relationships between these elements. We will use an unsupervised approach to extract these elements from the phrase structure.\n\nThe basic idea is to read a phrase and identify the subject and object as you come across them. However, there are a few drawbacks. For example, “red wine” is a phrase-spanning entity, while dependency parsers only identify individual words as subjects or objects.\n\nBecause of the above mentioned issues, I created the code below to extract the subject and object (entities) from a sentence. For your convenience, I’ve broken the code into many sections:\n\ndef get_entities(sent): ## chunk 1 ent1 = \"\" ent2 = \"\" prv_tok_dep = \"\" # dependency tag of previous token in the sentence prv_tok_text = \"\" # previous token in the sentence prefix = \"\" modifier = \"\" ############################################################# for tok in nlp(sent): ## chunk 2 # if token is a punctuation mark then move on to the next token if tok.dep_ != \"punct\": # check: token is a compound word or not if tok.dep_ == \"compound\": prefix = tok.text # if the previous word was also a 'compound' then add the current word to it if prv_tok_dep == \"compound\": prefix = prv_tok_text + \" \"+ tok.text # check: token is a modifier or not if tok.dep_.endswith(\"mod\") == True: modifier = tok.text # if the previous word was also a 'compound' then add the current word to it if prv_tok_dep == \"compound\": modifier = prv_tok_text + \" \"+ tok.text ## chunk 3 if tok.dep_.find(\"subj\") == True: ent1 = modifier +\" \"+ prefix + \" \"+ tok.text prefix = \"\" modifier = \"\" prv_tok_dep = \"\" prv_tok_text = \"\" ## chunk 4 if tok.dep_.find(\"obj\") == True: ent2 = modifier +\" \"+ prefix +\" \"+ tok.text ## chunk 5 # update variables prv_tok_dep = tok.dep_ prv_tok_text = tok.text ############################################################# return [ent1.strip(), ent2.strip()]\n\nChunk 1\n\nThis code block above defined several empty variables. The preceding word’s dependents and the word itself will be kept in the variables prv_tok_dep and prv_tok_text, respectively. The prefix and modifier will hold the text associated with the subject or object.\n\nChunk 2\n\nThen we’ll go over all of the tokens in the phrase one by one. The token’s status as a punctuation mark will be established first. If that’s the case, we’ll disregard it and go on to the next token. If the token is a component of a compound phrase (dependency tag = “compound”), we will put it in the prefix variable.\n\nPeople combine many words together to form a compound word and generate a new phrase with a new meaning (examples include “Football Stadium” and “animal lover”).\n\nThey will append this prefix to each subject or object in the sentence. A similar method will be used for adjectives such as “nice shirt,” “big house,” and so on.\n\nChunk 3\n\nIf the subject is the token in this scenario, it will be entered as the first entity in the ent1 variable. The variables prefix, modifier, prv_tok_dep, and prv_tok_text will all be reset.\n\nChunk 4\n\nIf the token is the object, it will be placed as the second entity in the ent2 variable. The variables prefix, modifier, prv_tok_dep, and prv_tok_text will all be reset.\n\nChunk 5\n\nAfter determining the subject and object of the phrase, we’ll update the preceding token and its dependency tag.\n\nLet’s use a phrase to test this function:\n\nget_entities(\"the film had 200 patents\")\n\nWow, everything looks to be going as planned. In the above phrase, ‘film’ is the topic and ‘200 patents’ is the aim.\n\nWe can now use this approach to extract these entity pairings for all of the phrases in our data:\n\nentity_pairs = [] for i in tqdm(candidate_sentences[\"sentence\"]): entity_pairs.append(get_entities(i))\n\nThe list entity_pairs includes all of the subject-object pairings from Wikipedia sentences. Let’s take a look at a few of them.\n\nentity_pairs[10:20]\n\nAs you can see, a few pronouns exist in these entity pairs, such as ‘we’, ‘it’,’ she’, and so on. Instead, we’d want proper nouns or nouns. We might possibly update the get_entities() code to filter out pronouns.\n\nRelations Extraction\n\nThe extraction of entities is only half the task. We need edges to link the nodes (entities) to form a knowledge graph. These edges represent the connections between two nodes.\n\nAccording to our hypothesis, the predicate is the principal verb in a phrase. For example, in the statement “Sixty Hollywood musicals were released in 1929,” the verb “released in” is used as the predicate for the triple formed by this sentence.\n\nThe following function may extract such predicates from sentences. I utilized spaCy’s rule-based matching in this case:\n\ndef get_relation(sent): doc = nlp(sent) # Matcher class object matcher = Matcher(nlp.vocab) #define the pattern pattern = [{'DEP':'ROOT'}, {'DEP':'prep','OP':\"?\"}, {'DEP':'agent','OP':\"?\"}, {'POS':'ADJ','OP':\"?\"}] matcher.add(\"matching_1\", None, pattern) matches = matcher(doc) k = len(matches) - 1 span = doc[matches[k][1]:matches[k][2]] return(span.text)\n\nThe function’s pattern attempts to discover the phrase’s ROOT word or primary verb. After identifying the ROOT, the pattern checks to see if it is followed by a preposition (‘prep’) or an agent word. If this is the case, it is appended to the ROOT word. Allow me to demonstrate this function:\n\nget_relation(\"John completed the task\")\n\nrelations = [get_relation(i) for i in tqdm(candidate_sentences['sentence'])]\n\nLet’s look at the most common relations or predicates that we just extracted:\n\npd.Series(relations).value_counts()[:50]\n\nBuild Knowledge Graph\n\nFinally, we will construct a knowledge graph using the retrieved entities (subject-object pairs) and predicates (relationships between entities). Let us build a dataframe with entities and predicates:\n\n# extract subject source = [i[0] for i in entity_pairs] # extract object target = [i[1] for i in entity_pairs] kg_df = pd.DataFrame({'source':source, 'target':target, 'edge':relations})\n\nThe networkx library will then be used to form a network from this dataframe. The nodes will represent the entities, while the edges or connections between the nodes will reflect the nodes’ relationships.\n\nThis will be a directed graph. In other words, each linked node pair’s relationship is one-way only, from one node to another.\n\n# create a directed-graph from a dataframe G=nx.from_pandas_edgelist(kg_df, \"source\", \"target\", edge_attr=True, create_using=nx.MultiDiGraph()) plt.figure(figsize=(12,12)) pos = nx.spring_layout(G) nx.draw(G, with_labels=True, node_color='skyblue', edge_cmap=plt.cm.Blues, pos = pos) plt.show()\n\nLet’s plot the network with a small example:\n\nimport networkx as nx import matplotlib.pyplot as plt # Create a KnowledgeGraph class class KnowledgeGraph: def __init__(self): self.graph = nx.DiGraph() def add_entity(self, entity, attributes): self.graph.add_node(entity, **attributes) def add_relation(self, entity1, relation, entity2): self.graph.add_edge(entity1, entity2, label=relation) def get_attributes(self, entity): return self.graph.nodes[entity] def get_related_entities(self, entity, relation): related_entities = [] for _, destination, rel_data in self.graph.out_edges(entity, data=True): if rel_data[\"label\"] == relation: related_entities.append(destination) return related_entities if __name__ == \"__main__\": # Initialize the knowledge graph knowledge_graph = KnowledgeGraph() # Add entities and their attributes knowledge_graph.add_entity(\"United States\", {\"Capital\": \"Washington, D.C.\", \"Continent\": \"North America\"}) knowledge_graph.add_entity(\"France\", {\"Capital\": \"Paris\", \"Continent\": \"Europe\"}) knowledge_graph.add_entity(\"China\", {\"Capital\": \"Beijing\", \"Continent\": \"Asia\"}) # Add relations between entities knowledge_graph.add_relation(\"United States\", \"Neighbor of\", \"Canada\") knowledge_graph.add_relation(\"United States\", \"Neighbor of\", \"Mexico\") knowledge_graph.add_relation(\"France\", \"Neighbor of\", \"Spain\") knowledge_graph.add_relation(\"France\", \"Neighbor of\", \"Italy\") knowledge_graph.add_relation(\"China\", \"Neighbor of\", \"India\") knowledge_graph.add_relation(\"China\", \"Neighbor of\", \"Russia\") # Retrieve and print attributes and related entities print(\"Attributes of France:\", knowledge_graph.get_attributes(\"France\")) print(\"Neighbors of China:\", knowledge_graph.get_related_entities(\"China\", \"Neighbor of\")) # Visualize the knowledge graph pos = nx.spring_layout(knowledge_graph.graph, seed=42) edge_labels = nx.get_edge_attributes(knowledge_graph.graph, \"label\") plt.figure(figsize=(8, 6)) nx.draw(knowledge_graph.graph, pos, with_labels=True, node_size=2000, node_color=\"skyblue\", font_size=10) nx.draw_networkx_edge_labels(knowledge_graph.graph, pos, edge_labels=edge_labels, font_size=8) plt.title(\"Knowledge Graph: Countries and their Capitals\") plt.show()\n\nThis isn’t exactly what we were looking for (but it’s still quite a sight!). We discovered that we had generated a graph with all of the relationships that we had. A graph with this many relations or predicates becomes quite difficult to see.\n\nAs a result, it is best to employ only a few key relationships to visualize a graph. I’ll tackle it one relationship at a time. Let us begin with the relationship “composed by”:\n\nG=nx.from_pandas_edgelist(kg_df[kg_df['edge']==\"composed by\"], \"source\", \"target\", edge_attr=True, create_using=nx.MultiDiGraph()) plt.figure(figsize=(12,12)) pos = nx.spring_layout(G, k = 0.5) nx.draw(G, with_labels=True, node_color='skyblue', node_size=1500, edge_cmap=plt.cm.Blues, pos = pos) plt.show()\n\nThat is a much better graph. The arrows in this case point to the composers. In the graph above, A.R. Rahman, a well-known music composer, is linked to things such as “soundtrack score,” “film score,” and “music.”\n\nLet’s look at some additional connections. Now I’d want to draw the graph for the “written by” relationship:\n\nG=nx.from_pandas_edgelist(kg_df[kg_df['edge']==\"written by\"], \"source\", \"target\", edge_attr=True, create_using=nx.MultiDiGraph()) plt.figure(figsize=(12,12)) pos = nx.spring_layout(G, k = 0.5) nx.draw(G, with_labels=True, node_color='skyblue', node_size=1500, edge_cmap=plt.cm.Blues, pos = pos) plt.show()\n\nThis knowledge graph provides us with some astonishing data. Famous lyricists include Javed Akhtar, Krishna Chaitanya, and Jaideep Sahni; this graph eloquently depicts their relationship.\n\nLet’s look at the knowledge graph for another crucial predicate, “released in”:\n\nG=nx.from_pandas_edgelist(kg_df[kg_df['edge']==\"released in\"], \"source\", \"target\", edge_attr=True, create_using=nx.MultiDiGraph()) plt.figure(figsize=(12,12)) pos = nx.spring_layout(G, k = 0.5) nx.draw(G, with_labels=True, node_color='skyblue', node_size=1500, edge_cmap=plt.cm.Blues, pos = pos) plt.show()\n\nConclusion\n\nIn conclusion, knowledge graphs have emerged as a powerful and versatile tool In AI and data science for representing structured information, enabling efficient data retrieval, reasoning, and inference. Throughout this article, we have explored key points highlighting the significance and impact of knowledge graphs across different domains. Here are the key points:\n\nKnowledge graphs offer a structured representation of information in a graph format with nodes, edges, and properties.\n\nThey enable flexible data modeling without fixed schemas, facilitating data integration from diverse sources.\n\nKnowledge graph reasoning allows for inferring new facts and insights based on existing knowledge.\n\nApplications span across domains, including natural language processing, recommendation systems, and semantic search engines.\n\nKnowledge graph embeddings represent entities and relationships in continuous vectors, enabling machine learning on graphs.\n\nIn conclusion, knowledge graphs have become essential for organizing and making sense of vast amounts of interconnected information. As research and technology advance, knowledge graphs will undoubtedly play a central role in shaping the future of AI, data science, information retrieval, and decision-making systems across various sectors.\n\nFrequently Asked Questions"
    }
}
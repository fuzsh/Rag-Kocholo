{
    "id": "wrong_mix_random_spouse_00129_2",
    "rank": 12,
    "data": {
        "url": "https://techfinder.stanford.edu/technology/state-art-graph-diffusion-transformer-natural-language-processing",
        "read_more_link": "",
        "language": "en",
        "title": "State-of-the-Art Graph Diffusion Transformer for Natural Language Processing",
        "top_image": "https://techfinder.stanford.edu/themes/custom/stanford_basic/favicon.ico",
        "meta_img": "https://techfinder.stanford.edu/themes/custom/stanford_basic/favicon.ico",
        "images": [
            "https://web.stanford.edu/group/OTL/lagan/20271/fig1.PNG"
        ],
        "movies": [],
        "keywords": [],
        "meta_keywords": [
            ""
        ],
        "tags": null,
        "authors": [],
        "publish_date": null,
        "summary": "",
        "meta_description": "Researchers at Stanford have developed a potentially best-in-class method for performing knowledge graph completion tasks. Their innovation, called Graph",
        "meta_lang": "en",
        "meta_favicon": "/themes/custom/stanford_basic/favicon.ico",
        "meta_site_name": "",
        "canonical_link": "https://techfinder.stanford.edu/technology/state-art-graph-diffusion-transformer-natural-language-processing",
        "text": "Researchers at Stanford have developed a potentially best-in-class method for performing knowledge graph completion tasks. Their innovation, called Graph Diffusion Transformer (GDT), advances the state of the art for completion of knowledge graphs, namely node classification and link prediction, and can be applied widely, e.g., medical knowledge graphs. Transformer architecture introduced the notion of self-attention (allowing the model to direct its focus and pay attention to different parts of the input) which leads to high performance in many natural language processing tasks. However, extending the notion to complex relational structures, such as graphs, remains a challenge. The Stanford innovation provides a scalable self-attention mechanism for graph data. It diffuses the attention scores from neighboring nodes to non-neighboring nodes, thus benefiting from the expressiveness of full self-attention. Experimental results on standard semi-supervised node classification as well as the knowledge graph completion show that GDT achieves state-of-the-art results.\n\nGDT architecture. Each GDT block consists of attention computation, attention diffusion, layer normalization, feed forward layers, and 2 residual connections for each block. GDT blocks can be stacked to constitute a deep model. As illustrated on the right, context-dependent attention is achieved via the attention diffusion process. Here A, B, C, D ? V are nodes in the graph. (image credit: the inventors)"
    }
}
{
    "id": "dbpedia_1004_3",
    "rank": 59,
    "data": {
        "url": "https://elifesciences.org/articles/10989",
        "read_more_link": "",
        "language": "en",
        "title": "Demixed principal component analysis of neural population data",
        "top_image": "https://elifesciences.org/assets/images/social/icon-600x600@1.52e87ee6.png",
        "meta_img": "https://elifesciences.org/assets/images/social/icon-600x600@1.52e87ee6.png",
        "images": [
            "https://elifesciences.org/assets/patterns/img/patterns/organisms/elife-logo-xs@1x.f9b97bde.png",
            "https://elifesciences.org/assets/patterns/img/icons/corresponding-author@1x.075234c5.png",
            "https://iiif.elifesciences.org/lax/10989%2Felife-10989-fig1-v2.tif/full/617,/0/default.jpg",
            "https://iiif.elifesciences.org/lax/10989%2Felife-10989-fig2-v2.tif/full/617,/0/default.jpg",
            "https://iiif.elifesciences.org/lax/10989%2Felife-10989-fig3-v2.tif/full/617,/0/default.jpg",
            "https://iiif.elifesciences.org/lax/10989%2Felife-10989-media1.jpg/full/639,/0/default.jpg",
            "https://iiif.elifesciences.org/lax/10989%2Felife-10989-fig4-v2.tif/full/617,/0/default.jpg",
            "https://iiif.elifesciences.org/lax/10989%2Felife-10989-fig5-v2.tif/full/617,/0/default.jpg",
            "https://iiif.elifesciences.org/lax/10989%2Felife-10989-fig6-v2.tif/full/617,/0/default.jpg",
            "https://iiif.elifesciences.org/lax/10989%2Felife-10989-fig7-v2.tif/full/617,/0/default.jpg",
            "https://iiif.elifesciences.org/lax/10989%2Felife-10989-fig8-v2.tif/full/617,/0/default.jpg",
            "https://iiif.elifesciences.org/lax/10989%2Felife-10989-media2.jpg/full/639,/0/default.jpg",
            "https://iiif.elifesciences.org/lax/10989%2Felife-10989-fig9-v2.tif/full/617,/0/default.jpg",
            "https://iiif.elifesciences.org/lax/10989%2Felife-10989-fig10-v2.tif/full/617,/0/default.jpg",
            "https://iiif.elifesciences.org/lax/10989%2Felife-10989-fig11-v2.tif/full/617,/0/default.jpg",
            "https://iiif.elifesciences.org/lax/10989%2Felife-10989-fig12-v2.tif/full/617,/0/default.jpg",
            "https://iiif.elifesciences.org/lax/10989%2Felife-10989-fig13-v2.tif/full/617,/0/default.jpg",
            "https://iiif.elifesciences.org/lax/10989%2Felife-10989-fig14-v2.tif/full/617,/0/default.jpg",
            "https://iiif.elifesciences.org/lax/10989%2Felife-10989-fig15-v2.tif/full/617,/0/default.jpg",
            "https://elifesciences.org/assets/patterns/img/icons/orcid.e0a7f9de.png",
            "https://elifesciences.org/assets/patterns/img/icons/orcid.e0a7f9de.png",
            "https://elifesciences.org/assets/patterns/img/patterns/organisms/elife-logo-xs@1x.f9b97bde.png",
            "https://elifesciences.org/assets/images/investors/hhmi@1x.739f96c8.png",
            "https://elifesciences.org/assets/images/investors/wellcome@1x.b38198be.png",
            "https://elifesciences.org/assets/images/investors/max@1x.71f18cc3.png",
            "https://elifesciences.org/assets/images/investors/kaw@1x.d11d4f95.png",
            "https://elifesciences.org/assets/patterns/img/patterns/molecules/carbon-neutral.706efe6d.svg"
        ],
        "movies": [],
        "keywords": [],
        "meta_keywords": [
            ""
        ],
        "tags": null,
        "authors": [
            "Dmitry Kobak",
            "Wieland Brendel",
            "Christos Constantinidis",
            "Claudia E Feierstein",
            "Adam Kepecs",
            "Zachary F Mainen",
            "Xue-Lian Qi",
            "Ranulfo Romo",
            "Naoshige Uchida",
            "Christian K Machens"
        ],
        "publish_date": "2016-04-12T00:00:00",
        "summary": "",
        "meta_description": "A new data analysis tool provides a concise way of visualizing neural data that summarizes all the relevant features of the population response in a single figure.",
        "meta_lang": "en",
        "meta_favicon": "/assets/favicons/apple-touch-icon-57x57.c2b7763c.png",
        "meta_site_name": "eLife",
        "canonical_link": "https://elifesciences.org/articles/10989",
        "text": "We illustrate the classical approaches to analyzing neural activity data from higher-order areas in Figure 1. To be specific, we consider recordings from the prefrontal cortex (PFC) of monkeys performing a somatosensory working memory task (Romo et al., 1999; Brody et al., 2003). In this task, monkeys were required to discriminate two vibratory stimuli presented to the fingertip. The stimuli F1 and F2 were separated by a 3 s delay, and the monkeys had to report which stimulus had a higher frequency by pressing one of the two available buttons (Figure 1a).\n\nFigure 1\n\nWhen we focus on the neural representation of the stimulus F1 and the decision, we have to take 12 experimental conditions into account (six possible values of F1 and two possible decisions). For each of these conditions, we can average each neuron’s spike trains over trials and then smooth the resulting time series in order to estimate the neuron’s time-dependent firing rate (also known as peri-stimulus time histogram or PSTH). We find that the PSTHs of many neurons are tuned to the stimulus F1, the decision, or both (Figure 1b; so-called mixed selectivity), and different neurons generally show different tuning. Our goal is to characterize and summarize the tuning of all N recorded neurons.\n\nThe most standard and widespread approach is to resort to a statistical test (e.g. a two-way analysis of variance or ANOVA), in order to check whether the firing rate of a neuron depends significantly on the frequency F1 or on the monkey’s decision. Such a test can be run for each neuron and each time point, in which case the population tuning over time is often summarized as the fraction of cells significantly tuned to stimulus or decision at each time point (p<0.05, Figure 1c). In addition to providing such a 'summary statistics', this approach is also used to directly visualize the population activity. For that purpose, one selects the subset of neurons significantly tuned to stimulus or decision (e.g. by focusing on a particular time point, Figure 1d) and then averages their PSTHs. The resulting 'population average' is shown in Figure 1e, where we also took the sign of the effect size into account. The population average is generally thought to demonstrate the 'most typical' firing pattern among the cells encoding the corresponding parameter. Importantly, this method yields one single population average or 'component' for each parameter. Each such component can be understood as a linear combination (or a linear readout) of the individual PSTHs, with all Ns significant neurons for a parameter having the same weights ± 1/Ns and all others having weight zero.\n\nIn a related approach, the firing rates of each neuron at each time point are linearly regressed on stimulus and decision (Figure 1f) (Brody et al., 2003). Mante et al. (2013) suggested to use the regression coefficients of all N neurons (Figure 1g) as weights to form linear combinations of PSTHs representing stimulus and decision tuning (Figure 1h). This approach, which the authors call 'targeted dimensionality reduction' (TDR), also yields one component per task parameter: in our example, we obtain one component for the stimulus and one for the decision (Figure 1h; see Materials and methods for details).\n\nBoth of these approaches are supervised, meaning that they are informed by the task parameters. At the same time, they do not seek to faithfully represent the whole dataset and are prone to losing some information about the neural activities. Indeed, the two components from Figure 1e explain only 23% of the total variance of the population firing rates and the two components from Figure 1h explain only 22% (see Materials and methods). Consequently, a naive observer would not be able to infer from the components what the original neural activities looked like.\n\nWhile such supervised approaches can be extended in various ways to produce more components and capture more variance, a more direct way to avoid this loss of information is to resort to unsupervised methods such as principal component analysis (PCA). This method extracts a set of principal components (PCs) that are linear combinations of the original PSTHs, just as the population averages above. However, the weights to form these linear combinations are chosen so as to maximize the amount of explained variance (first six components explain 69% of variance, see Figure 1i–k). The principal components can be thought of as 'building blocks' of neural activity: PSTHs of actual neurons are given by linear combinations of PCs, with the first PCs being more informative than the later ones. However, since PCA is an unsupervised method, information about stimuli and decisions is not taken into account, and the resulting components can retain mixed selectivity and therefore fail to highlight neural tuning to the task parameters.\n\nThe most striking observation when comparing supervised and unsupervised approaches is how different the results look. Indeed, PCA paints a much more complex picture of the population activity, dominated by strong temporal dynamics, with several stimulus- and decision-related components. At the same time, none of the methods can fully demix the stimulus and decision information: even the supervised methods show decision-related activity in the stimulus components and stimulus-related activity in the decision components (Figure 1e,h).\n\nTo address these problems, we developed a modified version of PCA that not only compresses the data, but also demixes the dependencies of the population activity on the task parameters. We will first explain that these two goals generally constitute a trade-off, then suggest a solution to this trade-off for a single task parameter, and then generalize to multiple task parameters.\n\nThe trade-off between demixing and compression is illustrated in Figure 2, where we compare linear discriminant analysis (LDA, Figure 2a,b), PCA (Figure 2c,d), and dPCA (Figure 2e–h). We will first focus on a single task parameter and seek to reduce the activity of N=2 neurons responding to three different stimuli. For each stimulus, the joint activity of the two neurons traces out a trajectory in the space of firing rates as time progresses (Figure 2b). The aim of 'demixing' in this simplified case is to find a linear mapping (decoder) of the neural activity that separates the different stimuli (Figure 2a) and ignores the time-dependency. We can use LDA in order to determine a projection of the data that optimally separates the three stimuli. However, LDA will generally not preserve the 'geometry' of the original neural activity: firing patterns for stimuli 1 and 2 are close to each other and far away from stimulus 3, whereas in the LDA projection all three stimuli are equally spaced (Figure 2b). More generally, decoding is always prone to distorting the data and therefore tends to impede a proper reconstruction of the original data from the reduced description.\n\nFigure 2\n\nThe aim of compression is to find a linear mapping (decoder) that reduces the dimensionality and preserves the original data as much as possible (Figure 2c,d). Using PCA, we determine a projection of the data that minimizes the reconstruction error between the projections and the original points. In contrast to LDA, PCA seeks to preserve the geometry of the neural activity, and thereby yields the most faithful reduction of the data (Figure 2d). However, the PCA projection does not properly separate the stimuli and mixes the time-dependency with the stimulus-dependency.\n\nThe wildly different projection axes for LDA (Figure 2b) and PCA (Figure 2d) seem to suggest that the goals of demixing and compression are essentially incompatible in this example. However, we can achieve both goals by assuming that the reconstruction of the original data works along a separate encoder axis (Figure 2f,h). Given this additional flexibility, we first choose a decoder axis that reconciles the decoding and compression objectives. Once projected onto this axis, all three stimuli are separated from each other, as in LDA, yet their geometrical arrangement is approximately preserved, as in PCA (Figure 2f). In turn, when reconstructed along the encoder axis, the projected data still approximates the original data (Figure 2h).\n\nTo define these ideas more formally, we assume that we simultaneously recorded the spike trains of N neurons. Let 𝐗 be our data matrix with N rows, in which the i-th row contains the instantaneous firing rate (i.e. binned or smoothed spike train) of the i-th neuron for all task conditions and all trials (assumed to be centered, i.e., with row means subtracted). Classical PCA compresses the data with a decoder matrix 𝐃. The resulting principal components can then be linearly de-compressed through an encoder matrix 𝐃⊤, approximately reconstructing the original data (Hastie et al., 2009). The optimal decoder matrix is found by minimizing the squared error between the original data, 𝐗, and the reconstructed data, 𝐃⊤⁢𝐃𝐗, given by\n\nLPCA=∥X−D⊤DX∥2.\n\nIn the toy example of Figure 2, the data matrix 𝐗 is of size 2×15, and the decoder matrix 𝐃 is of size 1×2. Crucially, the information about task parameters does not enter the loss function and hence PCA neither decodes nor demixes these parameters.\n\nIn our method, which we call demixed PCA (dPCA), we make two changes to this classical formulation. First, we require that the compression and decompression steps reconstruct not the neural activity directly, but the neural activity averaged over trials and over some of the task parameters. In the toy example, the reconstruction target is the matrix of stimulus averages, 𝐗s, which has the same size as 𝐗, but in which every data point is replaced by the average neural activity for the corresponding stimulus, as shown in Figure 2h. Second, we gain additional flexibility in this quest by compressing the data with a linear mapping 𝐃, yet decompressing it with another linear mapping 𝐅 (Figure 2e). The respective matrices are chosen by minimizing the loss function\n\nLdPCA=∥Xs−FDX∥2.\n\nAccordingly, for each stimulus, the neural activities are projected close to the average stimulus, which allows us both to decode the stimulus value and to preserve the relative distances of the neural activities.\n\nIn order to see how this approach preserves all aspects of the original data, and not just some averages, we note that the data in our toy example included both stimulus and time. The matrix 𝐗s can be understood as part of a linear decomposition of the full data 𝐗 into parameter-specific averages: a time-varying part, 𝐗t, that is obtained by averaging 𝐗 over stimuli, and a stimulus-varying part, 𝐗s, that is obtained by averaging 𝐗 over time. Any remaining parts of the activity are captured in a noise term (Figure 2g). In turn, we can find separate decoder and encoder axes for each of these averages. Once more than N=2 neurons are considered, these decoder and encoder axes constitute a dimensionality reduction step that reduces the data into a few components, each of which properly decodes one of the task parameters. In turn, the original neural activity can be reconstructed through linear combinations of these components, just as in PCA.\n\nThe key ideas of this toy example can be extended to any number of task parameters. In this manuscript, all datasets will have three parameters: time, stimulus, and decision, and we will decompose the neural activities into five parts: condition-independent, stimulus-dependent, decision-dependent, dependent on the stimulus-decision interaction, and noise (see Figure 8 in the Materials and methods):\n\nX=Xt+Xst+Xdt+Xsdt+Xnoise=∑ϕXϕ+Xnoise.\n\nIndividual terms are again given by a series of averages. This decomposition is fully analogous to the variance (covariance) decomposition done in ANOVA (MANOVA). The only important difference is that the standard (M)ANOVA decomposition for three parameters A, B, and C, would normally have 23=8 terms corresponding to the main effects of A, B, C, pairwise interactions AB, BC, and AC, three-way interaction ABC, and the noise. Here we join some of these terms together, as we are not interested in demixing those (see Materials and methods).\n\nOnce this decomposition is performed, dPCA finds separate decoder and encoder matrices for each term ϕ by minimizing the loss function\n\nLdPCA=∑ϕ∥𝐗ϕ-𝐅ϕ⁢𝐃ϕ⁢𝐗∥2.\n\nEach term within the sum can be minimized separately by using reduced-rank regression, the solution of which can be obtained analytically in terms of singular value decompositions (see Materials and methods). Each row 𝐝 of each 𝐃ϕ yields one demixed principal component 𝐝𝐗 and, similar to PCA, we order the components by the amount of explained variance. Note that the decoder/encoder axes corresponding to two different task parameters ϕ1 and ϕ2 are found independently from each other and may end up being non-orthogonal (in contrast to PCA where principal axes are all orthogonal). In a nutshell, the loss function ensures that each set of decoder/encoder axes reconstructs the individual, parameter-specific terms, 𝐗ϕ, thereby yielding proper demixing, and the data decomposition ensures that the combination of all decoder/encoder pairs allows to reconstruct the original data, 𝐗.\n\nThere are a few other technical subtleties (see Materials and methods for details). (1) We formulated dPCA for simultaneously recorded neural activities. However, all datasets analyzed in this manuscript have been recorded sequentially across many sessions, and so to apply dPCA we have to use 'pseudo-trials'. (2) Similar to any other decoding method, dPCA is prone to overfitting and so we introduce a regularization term and perform cross-validation to choose the regularization parameter. (3) The data and variance decompositions from above are exact only if the dataset is balanced, i.e., if the same number of trials were recorded in each condition. If this is not the case, one can use a re-balancing procedure. (4) A previous version of dPCA (Brendel et al., 2011) used the same variance decomposition but a different and less flexible loss function. The differences are layed out in the Materials and methods section.\n\nWe first applied dPCA to the dataset presented above (Romo et al., 1999; Brody et al., 2003), encompassing 832 neurons from two animals. As is typical for PFC, each neuron has a distinct response pattern and many neurons show mixed selectivity (some examples are shown in Figure 1b). Several previous studies have sought to make sense of these heterogeneous response patterns by separately analyzing different task periods, such as the stimulation and delay periods (Romo et al., 1999; Brody et al., 2003; Machens et al., 2010; Barak et al., 2010), the decision period (Jun et al., 2010), or both (Hernández et al., 2010). With dPCA, however, we can summarize the main features of the neural activity across the whole trial in a single figure (Figure 3).\n\nFigure 3\n\nJust as in PCA, we can think of the demixed principal components (Figure 3b) as the 'building blocks' of the observed neural activity, in that the activity of each single neuron is a linear combination (weighted average) of these components. These building blocks come in four distinct categories: some are condition-independent (Figure 3b, top row); some depend only on stimulus F1 (second row); some depend only on decision (third row); and some depend on stimulus and decision together (bottom row). The components can be easily seen to demix the parameter dependencies, which is exactly what dPCA aimed for. Indeed, the components shown in Figure 3b are projections of the PSTHs of all neurons onto the most prominent decoding axes; each projection (each subplot) shows 12 lines corresponding to 12 conditions. As intended, condition-independent components have all 12 lines closely overlapping, stimulus components have two lines for each stimulus closely overlapping, etc.\n\nThe overall variance explained by the dPCA components (Figure 3c, red line) is very close to the overall variance explained by the PCA components (black line). Accordingly, we barely lost any variance by imposing the demixing constraint, and the population activity is accurately represented by the obtained dPCA components.\n\nThe dPCA analysis captures the major findings previously obtained with these data: the persistence of the F1 tuning during the delay period (component #5; Romo et al., 1999; Machens et al., 2005), the temporal dynamics of short-term memory (components ##5, 10, 13; Brody et al., 2003; Machens et al., 2010; Barak et al., 2010), the 'ramping' or 'climbing' activities in the delay period (components ##1–3; Brody et al., 2003; Machens et al., 2010); and pronounced decision-related activities (component #6, Jun et al., 2010). We note that the decision components resemble derivatives of each other; these higher-order derivatives likely arise due to slight variations in the timing of responses across neurons (see Appendix B for more details).\n\nThe first stimulus component (#5) looks similar to the stimulus components that we obtained with standard regression-based methods (Figure 1e,h) but now we have further components as well. Together they show how stimulus representation evolves in time. In particular, plotting the first two stimulus components against each other (see Video 1) illustrates how stimulus representation rotates in the neural space during the delay period so that the encoding subspaces during F1 and F2 periods are not the same (but far from orthogonal either).\n\nVideo 1\n\nAs explained above, the demixed principal axes are not constrained to be orthogonal. The angles between the encoding axes are shown in Figure 3e, upper-right triangle; we discuss them later, together with other datasets. Pairwise correlations between components are all close to zero (Figure 3e, lower-left triangle), as should be expected since the components are considered to represent independent signals.\n\nTo assess whether the condition tuning of individual dPCA components was statistically significant, we used each component as a linear decoder to classify conditions. Specifically, stimulus components were used to classify stimuli, decision components to classify decisions, and interaction components to classify all 12 conditions. We used cross-validation to measure time-dependent classification accuracy and a shuffling procedure to assess whether it was significantly above chance (see Materials and methods). Time periods of significant tuning are marked in Figure 3b with horizontal black lines.\n\nWe next applied dPCA to recordings from the PFC of monkeys performing a visuospatial working memory task (Qi et al., 2011, 2012; Meyer et al., 2011). In this task, monkeys first fixated a small white square at the centre of a screen, after which a square S1 appeared for 0.5 s in one of eight locations around the centre (Figure 4a). After a 1.5 s delay, a second square S2 appeared for 0.5 s in either the same ('match') or the opposite ('non-match') location. Following another 1.5 s delay, a green and a blue choice target appeared in locations orthogonal to the earlier presented stimuli. Monkeys had to saccade to the green target to report a match condition, and to the blue one to report a non-match.\n\nFigure 4\n\nWe analyzed the activity of 956 neurons recorded in the lateral PFC of two monkeys performing this task. Proceeding exactly as before, we obtained the average time-dependent firing rate of each neuron for each condition. Following the original studies, we eliminated the trivial rotational symmetry of the task by collapsing the eight possible stimulus locations into five locations that are defined with respect to the preferred location of each neuron (0°, 45°, 90°, 135°, or 180° away from the preferred location, see Materials and methods). As a consequence, we obtained ten conditions: five possible stimulus locations, each paired with two possible decisions of the monkey.\n\nThe dPCA results are shown in Figure 4. As before, stimulus and decision are well separated at the population level despite being intermingled at the single-neuron level; at the same time dPCA captures almost the same amount of variance as PCA. One notable difference from before is the presence of strong interaction components in Figure 4b. However, these interaction components are in fact stimulus components in disguise. In match trials, S2 and S1 appear at the same location, and in non-match trials at opposite locations. Information about S2 is therefore given by a non-linear function of stimulus S1 and the trial type (i.e. decision), which is here captured by the interaction components.\n\nHere again, our analysis summarizes previous findings obtained with this dataset. For instance, the first and the second decision components show tuning to the match/non-match decision during the S2 period and in the subsequent delay period. Using these components as fixed linear decoders, we achieve single-trial classification accuracy of match vs. non-match of 75% for t>2 (cross-validated, see Materials and methods, Figure 12), which is approximately equal to the state-of-the-art classification performance reported previously (Meyers et al., 2012).\n\nConstantinidis et al. have also recorded population activity in PFC before starting the training (both S1 and S2 stimuli were presented exactly as above, but there were no cues displayed and no decision required). When analyzing this pre-training population activity with dPCA, the first stimulus and the first interaction components come out close to the ones shown in Figure 4, but there are no decision and no 'memory' components present (data not shown), in line with previous findings (Meyers et al., 2012). These task-specific components appear in the population activity only after extensive training.\n\nNext, we applied dPCA to recordings from the OFC of rats performing an odor discrimination task (Feierstein et al., 2006). This behavioral task differs in two crucial aspects from the previously considered tasks: it requires no active storage of a stimulus, and it is self-paced. To start a trial, rats entered an odor port, which triggered delivery of an odor with a random delay of 0.2–0.5 s. Each odor was uniquely associated with one of the two available water ports, located to the left and to the right from the odor port (Figure 5a). Rats could sample the odor for as long as they wanted (up to 1 s), and then had to move to one of the water ports. If they chose the correct water port, reward was delivered following an anticipation period of random length (0.2–0.5 s).\n\nFigure 5\n\nWe analyzed the activity of 437 neurons recorded in five rats in four conditions: two stimuli (left and right) each paired with two decisions (left and right). Two of these conditions correspond to correct (rewarded) trials, and two correspond to error (unrewarded) trials. Since the task was self-paced, each trial had a different length; in order to align events across trials, we restretched (time-warped) the firing rates in each trial (see Materials and methods). Alignment methods without time warping led to similar results (data not shown).\n\nJust as neurons from monkey PFC, neurons in rat OFC exhibit diverse firing patterns and mixed selectivity (Feierstein et al., 2006). Nonetheless, dPCA was able to demix the population activity (Figure 5). In this dataset, interaction components separate rewarded and unrewarded conditions (thick and thin lines in Figure 5b, bottom row), i.e., correspond to neurons tuned either to reward, or to the absence of reward.\n\nThe overall pattern of neural tuning across task epochs agrees with the findings of the original study (Feierstein et al., 2006). Interaction components are by far the most prominent among all the condition-dependent components, corresponding to the observation that many neurons are tuned to the presence/absence of reward. Decision components come next, with the caveat that decision information may also reflect the rat’s movement direction and/or position, as was pointed out previously (Feierstein et al., 2006). Stimulus components are less prominent, but nevertheless show clear stimulus tuning, demonstrating that even in error trials there is reliable information about stimulus identity in the population activity.\n\nCuriously, the first interaction component (#4) already shows significant tuning to reward in the anticipation period. In other words, neurons tuned to presence/absence of reward start firing before the reward delivery (or, on error trials, before the reward could have been delivered). We return to this observation in the next section.\n\nOne of the key advantages of applying dPCA to these four datasets is that we can now compare them far more easily than was previously possible. This comparison allows us to highlight several general features of the population activity in prefrontal areas.\n\nFirst, most of the variance of the neural activity is always captured by the condition-independent components that together amount to 65–90% of the signal variance (see pie charts in Figures 3–6d; see Materials and methods for definition of 'signal variance'). These components capture the temporal modulations of the neural activity throughout the trial, irrespective of the task condition. Their striking dominance in the data may come as a surprise, as such condition-independent components are usually not analyzed or shown (cf. Figure 1e,h), even though condition-independent firing has been described even in sensory areas (Sornborger et al., 2005). These components are likely explained in part by an overall firing rate increase during certain task periods (e.g. during stimulus presentation). More speculatively, they could also be influenced by residual sensory or motor variables that vary rhythmically with the task, but are not controlled or monitored (Renart and Machens, 2014). The attentional or motivational state of animals, for instance, often correlates with breathing (Huijbers et al., 2014), pupil dilation (Eldar et al., 2013), body movements (Gouvêa et al., 2014), etc.\n\nSecond, even though dPCA, unlike PCA, does not enforce orthogonality between encoding axes corresponding to different task parameters, most of them turned out to be close to orthogonal to each other (Figures 3–6e, upper triangle), as has been observed before (Brendel et al., 2011; Rishel et al., 2013; Raposo et al., 2014). Nevertheless, many pairs were significantly non-orthogonal, meaning that neurons expressing one of the components tended to also express the other one. Throughout the four datasets, we identified 277 pairs of axes (among the first 15 axes) corresponding to different parameters. Of these, 38, i.e. 14%, were significantly non-orthogonal with p<0.001 (8 out of 53 if we do not take time axes into account).\n\nThird, all dPCA components in each of the datasets are distributed across the whole neural population (as opposed to being exhibited only by a subset of cells). For each component and each neuron, the corresponding encoder weight shows how much this particular component is exhibited by this particular neuron. For each component, the distribution of weights is strongly unimodal, centred at zero (Figure 7a), and rather symmetric (although it is skewed to one side for some components). In other words, there are no distinct sub-populations of neurons predominantly expressing a particular component; rather, each individual neuron can be visualized as a random linear combination of these components. We confirmed this observation by applying a recently developed clustering algorithm (Rodriguez and Laio, 2014) to the population of neurons in the 15-dimensional space of dPC weights. In all cases, the algorithm found only one cluster (Figure 7b). An alternative clustering analysis with Gaussian mixture models yielded similar results (data not shown). This absence of any detectable clusters of neurons has been noted before (Machens et al., 2010) and was recently observed in other datasets as well (Raposo et al., 2014).\n\nFigure 7\n\nThe method presented here is conceptually based on our previous work (Machens, 2010; Machens et al., 2010; Brendel et al., 2011), but is technically very different. The original approach from Machens et al. (2010) only works for two parameters of interest, such as time and stimulus. Machens (2010) suggested a partial generalization to multiple parameters and Brendel et al. (2011) introduced the full covariance decomposition and developed a probabilistic model. However, all of them imposed orthogonality on the decoder/encoder axes (and as a result did not distinguish them), a constraint that cannot be easily relaxed. While we have previously argued that orthogonality is a desirable feature of the decomposition, we now believe that it is better not to impose it upfront. First, by looking across many datasets, we have learnt that encoding subspaces can sometimes be highly non-orthogonal (Figures 3–6e) and hence not demixable under orthogonality constraints. Second, by not imposing orthogonality, we can easier identify components that are truly orthogonal. Third, removing the orthogonality constraint allowed us to obtain a simple analytical solution in terms of singular value decompositions (see Materials and methods) and hence to avoid local minima, convergence issues, and any additional optimization-related hyperparameters.\n\nTo demonstrate these advantages, we ran the algorithm of Brendel et al. (2011), dPCA-2011, on all our datasets. The resulting components were similar to the components presented here, with the amount of variance captured by the first 15 components being very close; but the achieved demixing was worse. For each component we defined a demixing index (see Materials and methods) that is equal to 1 if the component is perfectly demixed. For all datasets, these indices were significantly higher with our current dPCA-2015 method than with dPCA-2011. Moreover, dPCA-2011 failed to find some weak components at all. For comparison, see Figure 14 in the Materials and methods.\n\nAnother method, called 'targeted dimensionality reduction' (TDR) has recently been suggested for neural data analysis and is similar in spirit to dPCA in that it looks for demixing linear projections (Mante et al., 2013). As mentioned above, the original application of this method yields only one component per task parameter and ignores the condition-independent components. While TDR can be extended in various ways to yield more components, no principled way of doing it has been suggested so far. Comparison of dPCA with TDR on our datasets shows that dPCA demixes the task-parameter dependencies better than TDR (see Figure 14 in the Materials and methods).\n\nFor an in-depth discussion of the relationship between dPCA and LDA/MANOVA, we refer the reader to the Methods. Briefly, LDA is a one-way technique, meaning that only one parameter (class id) is associated with each data point. Therefore, LDA cannot directly be applied to the demixing problem. While LDA could be generalized to deal with several parameters in a systematic way, such a generalization has not been used for dimensionality reduction of neural data and does not have an established name in the statistical literature (we call it factorial LDA). We believe that for the purposes of dimensionality reduction, dPCA is a superior approach since it combines a reasonably high class separation with low reconstruction error, whereas LDA only optimizes class separation without taking the (potential) reconstruction error into account (see Figure 2). MANOVA, on the other hand, is a statistical test closely related to LDA that deals with multiple parameters. However, it deals with isolating the contribution of each parameter from residual noise rather than from the other parameters, and is therefore not suited for demixing.\n\nWhile we believe that dPCA is an easy-to-use method of visualizing complex data sets with multiple task parameters, several limitations should be kept in mind. First, dPCA as presented here works only with discrete parameters, and all possible parameter combinations must be present in the data. This limitation is the downside of the large flexibility of the method: apart from the demixing constraint, we do not impose any other constraints on the latent variables and their estimation remains essentially non-parametric. In order to be able to treat continuous parameters or missing data (missing parameter combinations), we would need to further constrain the estimation of these latent variables, using e.g. a parametric model. One simple possibility is to directly use a parametric model for the activity of the single neurons, such as the linear model used in Mante et al. (2013), in order to fill in any missing data points, and then run dPCA subsequently.\n\nSecond, the number of neurons needs to be sufficiently high in order to obtain reliable estimates of the demixed components. In our datasets, we found that at least ∼100 neurons were needed to achieve satisfactory demixing. The number is likely to be higher if more than three task parameters are to be demixed, as the number of interaction terms grows exponentially with the number of parameters. This trade-off between model complexity and demixing feasibility should be kept in mind when deciding how many parameters to put into the dPCA procedure. In cases when there are many task parameters of interest, dPCA is likely to be less useful than the more standard parametric single-unit approaches (such as linear regression). As a trivial example, imagine that only N=1 neuron has been recorded; it might have strong and significant tuning to various parameters of interest, but there is no way to demix (or decode) these parameters from the recorded 'population.'\n\nThird, even with a large number of neurons, a dataset may be non-demixable, in which case dPCA would fail. For instance, if the high-variance directions of the stimulus and the decision parts of the neural activities fully overlap, then there is no linear decoder that can demix the two parameters.\n\nFinally, dPCA components corresponding to the same parameter (e.g. successive stimulus components) are here chosen to be orthogonal, similarly to PCA. This can make successive components difficult to interpret (e.g. the second and the third stimulus components in Figure 3). To make them more interpretable, the orthogonality constraint could be replaced with some other constraints, such as e.g. requiring each component to have activity 'localized' in time. This problem may be addressed in future work.\n\nConsider one single neuron first. We can decompose its filtered spike trains, xt⁢s⁢d⁢k, into a set of averages (which we call marginalizations) over various combinations of parameters. We will denote the average over a set of parameters {a,b,…} by angular brackets ⟨⋅⟩a⁢b⁢…. Let us define the following marginalized averages:\n\nx¯=⟨xtsdk⟩tsdk=x¯⋅⋅⋅⋅x¯t=⟨xtsdk−x¯⟩sdk=x¯t⋅⋅⋅−x¯⋅⋅⋅⋅x¯s=⟨xtsdk−x¯⟩tdk=x¯⋅s⋅⋅−x¯⋅⋅⋅⋅x¯d=⟨xtsdk−x¯⟩tsk=x¯⋅⋅d⋅−x¯⋅⋅⋅⋅x¯ts=⟨xtsdk−x¯−x¯t−x¯s−x¯d⟩dk=x¯ts⋅⋅−x¯t⋅⋅⋅−x¯⋅s⋅⋅+x¯⋅⋅⋅⋅x¯td=⟨xtsdk−x¯−x¯t−x¯s−x¯d⟩sk=x¯t⋅d⋅−x¯t⋅⋅⋅−x¯⋅⋅d⋅+x¯⋅⋅⋅⋅x¯sd=⟨xtsdk−x¯−x¯t−x¯s−x¯d⟩tk=x¯⋅sd⋅−x¯⋅s⋅⋅−x¯⋅⋅d⋅+x¯⋅⋅⋅⋅x¯tsd=⟨xtsdk−x¯−x¯t−x¯s−x¯d−x¯ts−x¯td−x¯sd⟩k=x¯tsd⋅−x¯ts⋅⋅−x¯⋅sd⋅−x¯t⋅d⋅=+x¯t⋅⋅⋅+x¯⋅s⋅⋅+x¯⋅⋅d⋅−x¯⋅⋅⋅⋅ϵtsdk=xtsdk−⟨xtsdk⟩k=xtsdk−x¯tsd⋅.\n\nHere x¯ is simply the overall mean firing rate of our neuron, x¯t is the average time-varying firing rate once the overall mean has been subtracted, etc. The right-hand side shows the same averaging procedure in the more explicit form using ANOVA-style notation, in which averages of x over everything apart from the explicitly mentioned parameters, e.g., the stimulus s, are denoted by terms of the form x¯⋅s⁣⋅⋅. One can directly see that the original neural activities are given by the sum of all marginalizations:\n\nxtsdk=x¯+x¯t+x¯s+x¯d+x¯ts+x¯td+x¯ds+x¯tsd+ϵtsdk.\n\nThis decomposition is identical to the one used in factorial ANOVA (Rutherford, 2001; Christensen, 2011) where task parameters are called factors. The ANOVA literature uses a slightly different notation with task parameters (t,s,d,k) replaced by indices (i,j,k,l) and with Greek letters designating individual terms:\n\nxijkl=μ+αi+βj+γk+δij+ζjk+ηik+θijk+ϵijkl.\n\nWe will use our notation, though, to keep the connection with the task parameters more explicit.\n\nFor the purposes of demixing neural signals in the context of our datasets, we combine some of these terms together. Indeed, demixing a time-independent pure stimulus term x¯s from a stimulus-time interaction term x¯t⁢s makes little sense because we expect all neural components to change with time. Hence, we group the terms as follows (without changing the notation):\n\nxtsdk=x¯+x¯t+x¯s+x¯ts⏟x¯ts+x¯d+x¯td⏟x¯td+x¯sd+x¯tsd⏟x¯tsd+ϵtsdk.\n\nHere the first term on the right-hand side is the mean firing rate, the last term is the trial-to-trial noise, and we call the other terms condition-independent term, stimulus term, decision term, and stimulus-decision interaction term. This decomposition is illustrated in Figure 8 for several exemplary neurons (we only show the decomposition of the PSTH part, leaving out the noise term).\n\nFigure 8\n\nWe apply this marginalization procedure to every neuron, splitting the whole data matrix 𝐗 into parts. Assuming from now on that the data matrix is centered (i.e. x¯=0 for all neurons), we can write the decomposition in the matrix form\n\nX=Xt+Xts+Xtd+Xtsd+Xnoise=∑ϕXϕ+Xnoise.\n\nHere t, ts, td, and tsd are labels and not indices, and all terms are understood to be matrices of the same N×K⁢S⁢Q⁢T size, so e.g. 𝐗t is not an N×T sized matrix, but the full size N×K⁢S⁢Q⁢T matrix with N×T unique values replicated K⁢S⁢Q times. Crucially, the marginalization procedure ensures that all terms are uncorrelated and that the N×N covariance matrix 𝐂=𝐗𝐗⊤/(K⁢S⁢Q⁢T) is linearly decomposed into the sum of covariance matrices from each marginalization (see Appendix A for the proof):\n\nC=Ct+Cts+Ctd+Ctsd+Cnoise=∑ϕCϕ+Cnoise.\n\nHere all covariance matrices are defined with the same denominator, i.e. 𝐂ϕ=𝐗ϕ⁢𝐗ϕ⊤/(K⁢S⁢Q⁢T).\n\nGiven a decomposition 𝐗=∑ϕ𝐗ϕ+𝐗noise, the loss function of dPCA is given by\n\nL=∑ϕLϕ\n\nwith\n\nLϕ=∥𝐗ϕ-𝐅ϕ⁢𝐃ϕ⁢𝐗∥2,\n\nwhere each 𝐅ϕ is an encoder matrix with qϕ columns and each 𝐃ϕ is a decoder matrix with qϕ rows. Here and below, matrix norm signifies Frobenius norm, i.e. ∥𝐗∥2=∑i∑jXi⁢j2. In the remaining discussion, it will often be sufficient to focus on the individual loss functions Lϕ, in which case we will drop the indices ϕ on the decoder and encoder matrices for notational convenience, and simply write 𝐅 and 𝐃.\n\nWithout any additional constraints, the decoder and encoder are only defined up to their product 𝐅𝐃 of rank q. To make the decomposition unique, we will assume that 𝐅 has orthonormal columns and that components are ordered such that their variance (row variance of 𝐃𝐗) is decreasing. The reason for this choice will become clear below.\n\nThis loss function penalizes the difference between the marginalized data 𝐗ϕ and the reconstructed full data 𝐗, i.e., the full data projected with the decoders 𝐃 onto a low-dimensional latent space and then reconstructed with the encoders 𝐅 (see Video 2). The loss function thereby favours variance in marginalization ϕ and punishes variance coming from all other marginalizations and from trial-to-trial noise. Given that the marginalized averages are uncorrelated with each other, we can make this observation clear by writing,\n\nLϕ=∥Xϕ−FDX∥2=∥Xϕ−FDXϕ∥2+∥FD(X−Xϕ)∥2.\n\nHere the first term corresponds to the non-explained variance in marginalization ϕ and the second term corresponds to the variance coming from all other marginalizations and from trial-to-trial noise. The dPCA objective is to minimize both.\n\nVideo 2\n\nWe note that the loss function Lϕ is of the general form ∥𝐗ϕ-𝐀𝐗∥2, with 𝐀=𝐅𝐃. For an arbitrary N×N matrix 𝐀, minimization of the loss function amounts to a classical regression problem with the well-known ordinary least squares (OLS) solution, 𝐀OLS=𝐗ϕ𝐗(𝐗𝐗)⊤-1⊤. In our case, 𝐀=𝐅𝐃 is an N×N matrix of rank q, which we will make explicit by writing 𝐀q. The dPCA loss function therefore amounts to a linear regression problem with an additional rank constraint on the matrix of regression coefficients. This problem is known as reduced-rank regression (RRR) (Izenman, 1975; Reinsel and Velu, 1998; Izenman, 2008) and can be solved via the singular value decomposition.\n\nTo see this, we write Xϕ−AqX=(Xϕ−AOLSX)+(AOLSX−AqX). The first term, 𝐗ϕ-𝐀OLS⁢𝐗, consists of the regression residuals that cannot be accounted for by any linear transformation of 𝐗. It is straightforward to verify that these regression residuals, 𝐗ϕ-𝐀OLS⁢𝐗, are orthogonal to 𝐗 (Hastie et al., 2009, Section 3.2) and hence also orthogonal to (𝐀OLS-𝐀q)⁢𝐗. This orthogonality allows us to split the loss function into two terms,\n\n∥Xϕ−AqX∥2=∥Xϕ−AOLSX∥2+∥AOLSX−AqX∥2,\n\nwhere the first term captures the (unavoidable) error of the least squares fit while the second term describes the additional loss suffered through the rank constraint. Since the first term does not depend on 𝐀q, the problem reduces to minimizing the second term.\n\nTo minimize the second term, we note that the best rank-q approximation to 𝐀OLS⁢𝐗 is given by its first q principal components (Eckart-Young-Mirsky theorem). Accordingly, if we write 𝐔q for the matrix of the q leading principal directions (left singular vectors) 𝐮i of 𝐀OLS⁢𝐗, then the best approximation is given by 𝐔q⁢𝐔q⊤⁢𝐀OLS⁢𝐗 and hence 𝐀q=𝐔q⁢𝐔q⊤⁢𝐀OLS.\n\nTo summarize, the reduced-rank regression problem posed above can be solved in a three-step procedure:\n\nCompute the OLS solution 𝐀OLS=𝐗ϕ⁢𝐗⊤⁢(𝐗𝐗⊤)-1.\n\nPerform PCA of 𝐀OLS⁢𝐗 and take the q leading principal components to obtain the best low-rank approximation: 𝐀q=𝐔q⁢𝐔q⊤⁢𝐀OLS where 𝐔q is the N×q matrix of the q leading principal directions (left singular vectors) of 𝐀OLS⁢𝐗.\n\nFactorize the matrix 𝐀q into decoder and encoder matrices, 𝐀q=𝐅𝐃, by choosing 𝐅=𝐔q and 𝐃=𝐔q⊤⁢𝐀OLS.\n\nConveniently, the extracted decoder/encoder pairs do not depend on how many pairs are extracted: the i-th pair is given by 𝐟=𝐮i and 𝐝=𝐮i⊤⁢𝐀OLS, independent of q. Indeed, this feature motivated the above choice that 𝐅 should have orthonormal columns.\n\nThe data and variance decomposition carried out by the marginalization procedure can break down when the dataset is unbalanced, i.e., when the number of data points (trials) differs between conditions. We illustrate this problem with a two-dimensional toy example in Figure 9. We assume two task parameters (factors), each of which can take only two possible values. The overall mean as well as the interaction term are taken to be zero, so that xijk=ai+bj+eijk. Since the number of trials, K=Ki⁢j, depends on the condition, the trial index runs through the values k=1…Kij. As shown in Figure 9a, all three terms on the right-hand side exhibit zero correlation between x1 and x2. A balanced dataset with the same number of data points in each of the four possible conditions (Figure 9b) also has zero correlation. However, an unbalanced dataset, as shown in Figure 9c, exhibits strong positive correlation (ρ=0.8). Accordingly, the covariance matrix of the full data can no longer be split into marginalized covariances. To avoid this and other related problems, we can perform a 're-balancing' procedure by reformulating dPCA in terms of PSTHs and noise covariance.\n\nFigure 9\n\nIn the balanced case, the dPCA loss function Lϕ can be rewritten as the sum of two terms with one term depending on the PSTHs and another term depending on the trial-to-trial variations,\n\nLϕ=∥Xϕ−FDX∥2=∥Xϕ−FD(X−Xnoise)∥2+∥FDXnoise∥2,\n\nwhere we used the fact that 𝐗ϕ and 𝐗-𝐗noise are orthogonal to 𝐗noise (see Appendix A). We now define 𝐗PSTH=𝐗-𝐗noise which is simply a matrix of the same size as 𝐗 with the activity of each trial replaced by the corresponding PSTH. In addition, we observe that the squared norm of any centered data matrix 𝐘 with n data points can be written in terms of its covariance matrix 𝐂Y=𝐘𝐘⊤/n, namely ∥Y∥2=tr[YY⊤]=ntr[CY]=ntr[CY1/2CY1/2]=n∥CY1/2∥2, and so\n\nLϕ=∥Xϕ−FDXPSTH∥2+KSQT∥FDCnoise1/2∥2.\n\nThe first term consists of K replicated copies: 𝐗PSTH contains K replicated copies of 𝐗~ (which we defined above as the matrix of PSTHs) and 𝐗ϕ contains K replicated copies of 𝐗~ϕ (which we take to be a marginalization of 𝐗~, with 𝐗~=∑ϕ𝐗~ϕ). We can eliminate the replications and drop the factor K to obtain\n\nLϕ=∥X~ϕ−FDX~∥2+SQT∥FDCnoise1/2∥2.\n\nIn the unbalanced case, we can directly use this last formulation where all occurrences of 𝐗 have been replaced by 𝐗~. This is especially useful for neural data, where some combinations of task parameters may occur more often than others. The 're-balanced' dPCA loss function treats all parameter combinations as equally important, independent of their occurrence frequency. It stands to reason to 're-balance' the noise covariance matrix as well by defining it as follows:\n\n𝐂~noise=1S⁢Q⁢T⁢∑s⁢d⁢t𝐂noise⁢(s,d,t)=⟨𝐂noise⁢(s,d,t)⟩s⁢d⁢t,\n\nwhere 𝐂noise⁢(s,d,t) is the covariance matrix for the (s,d,t) parameter combination. This formulation, again, treats noise covariance matrices from different parameter combinations as equally important, independent of how many data points there are for each parameter combination.\n\nPutting everything together and including the regularization term as well, we arrive at the following form of the dPCA loss function:\n\nLϕ=∥X~ϕ−FDX~∥2+SQT∥FDC~noise1/2∥2+μ∥FD∥2.\n\nThis loss function can be minimized as described in the previous section. Specifically, the full rank solution with 𝐀=𝐅𝐃 becomes\n\nARR=X~ϕX~⊤(X~X~⊤+SQT⋅C~noise+μI)−1.\n\nThe reduced-rank solution can then be obtained by setting 𝐅=𝐔q and 𝐃=𝐔q⊤⁢𝐀, where 𝐔q are the first q principal directions of 𝐀RR⁢𝐗~.\n\nAs all datasets analyzed in this manuscript were sequentially recorded, we always reported fractions of the PSTH variance (as opposed to the total PSTH+noise variance) explained by our components, i.e. fractions of variance explained in 𝐗~. We defined the fraction of explained variance in a standard way:\n\nR2=∥𝐗~∥2-∥𝐗~-𝐅𝐃⁢𝐗~∥2∥𝐗~∥2.\n\nThis formula can be used to compute the fraction of variance explained by each dPCA component (by plugging in its encoder 𝐟 and decoder 𝐝); these are the numbers reported on Figures 3–6b,d and used to order the components. The same formula can be used to compute the cumulative fraction of variance explained by the first q components (by stacking their encoders and decoders as columns and rows of 𝐅 and 𝐃 respectively); these are the numbers reported on Figures 3–6c. Note that the cumulative explained variance is close to the sum of individually explained variances but not exactly equal to it since the dPCA components are not completely uncorrelated. The same formula holds for standard PCA using 𝐅=𝐃⊤=𝐔pca, i.e., the matrix of stacked together principal directions (Figures 3–6c).\n\nUsing the decomposition X~=∑ϕX~ϕ, we can split the fraction of explained variance into additive contributions from different marginalizations:\n\nR2=∑ϕ∥𝐗~ϕ∥2-∥𝐗~ϕ-𝐅𝐃⁢𝐗~ϕ∥2∥𝐗~∥2.\n\nWe used this decomposition to produce the bar plots in Figures 3–6d, showing how the explained variance of each single dPCA component is split between marginalizations.\n\nFollowing the approach of Machens et al. (2010), we note that our PSTH estimates 𝐗~ must differ from the 'true' underlying PSTHs due to the finite amount of recorded trials. Hence, some fraction of the total variance of 𝐗~ is coming from this residual noise. We can estimate this fraction as follows. Our estimate of the noise variance of the n-th neuron is given by C~n⁢n, the n-th diagonal element of 𝐂~noise. There are on average K~n=1S⁢Q⁢∑Kn⁢s⁢d trials being averaged to compute the PSTHs for this neuron. So a reasonable estimate of the residual noise variance of the n-th neuron is C~n⁢n/K~n. Accordingly, we define the total residual noise sum of squares as\n\nΘ=S⁢Q⁢T⋅∑nC~n⁢nK~n.\n\nIn turn, the fraction of total signal variance is computed as 1-Θ/∥𝐗~∥2 which is the dashed line shown in Figures 3–6c. Note that each component likewise has contributions from both signal and noise variance, and hence the fraction of total signal variance does not constitute an upper bound on the number of components.\n\nThe residual noise variance is not split uniformly across marginalizations: the fraction falling into marginalization ϕ is proportional to the respective number of degrees of freedom, Kϕ. This can be explicitly computed; for a centered dataset with S stimuli, Q decisions, and T time points the total number of degrees of freedom (per neuron) is S⁢Q⁢T-1 and is split into T-1 for time, S⁢T-T for stimulus, Q⁢T-T for decision, and S⁢Q⁢T-S⁢T-Q⁢T+T for the stimulus-decision interaction (compare with the formulas in the Marginalization Procedure section). Accordingly, we computed the residual noise sum of squares falling into marginalization ϕ as\n\nΘϕ=KϕS⁢Q⁢T-1⁢Θ.\n\nThe pie charts in Figures 3–6d show the amount of variance in each marginalization, with estimated contributions of the residual noise variance subtracted: (∥𝐗~ϕ∥2-Θϕ)/(∥𝐗~∥2-Θ). To display the percentage values on the pie charts, percentages were rounded using the 'largest remainder method', so that the sum of the rounded values remained 100%.\n\nTo understand the differences of dPCA with respect to other (demixing) methods, we will make several explicit comparisons. The first method we will consider is performing a series of standard PCAs in each marginalization separately. This procedure can be understood in two ways: after performing PCA on 𝐗ϕ and obtaining the matrix 𝐔ϕ for the k leading principal directions, we can use this matrix to project either the marginalized data or the full data.\n\nIn the first case we obtain the principal components of the corresponding marginalization, 𝐔ϕ⁢𝐗ϕ. However, while these components provide a particular decomposition or visualization of the data, they do not constitute readouts of the neural activity, since they are based on projecting the marginalized data. One particular advantage of the dPCA formulation is that it operates on the raw data, so that the decoders (and encoders) can actually be used on single trials. In turn, the visualization of the data found through dPCA also provides insights into the utility of the respective population code for the brain.\n\nIn the second case we obtain 𝐔ϕ⁢𝐗 components from the full data, so that these components could be obtained by projecting single-trial activities. However, now there is no guarantee that these components will be demixed. For a simple counter-example, consider Figure 13: the stimulus marginalization 𝐗s consists of three points (one for each stimulus) located roughly on a horizontal axis, and so the first principal axis of 𝐗s is roughly horizontal. It is easy to see that the projection of the full data onto this axis will be not only stimulus-, but also time-dependent.\n\nFigure 13\n\nNonetheless, we can obtain a reasonable approximation to the dPCA solution using PCA in each marginalization. Namely, 𝐔ϕ can be taken to constitute the encoders 𝐅ϕ. In turn, the decoders 𝐃ϕ are obtained by a pseudo-inverse 𝐃=𝐔+, where 𝐔 is a matrix with 4⁢k columns obtained by joining together all 𝐔ϕ. We found that this procedure provides a close approximation of the actual decoder and encoder matrices, provided one chooses a reasonable value of k: choosing k too small results in poor demixing, and choosing k too large results in overfitting. In our datasets, k=10 provides a good trade-off.\n\nThis approximate solution highlights the conditions under which dPCA will work well, i.e., result in well-demixed components that capture most of the variance of the data: the main principal axes of different marginalizations 𝐗ϕ need to be non-collinear. In other words, principal subspaces of different marginalizations should not overlap.\n\nLinear Discriminant Analysis (LDA) is usually understood as a one-way technique: there is only one parameter (class id) associated with each data point, whereas in this manuscript we dealt with three parameters simultaneously. Therefore, LDA in its standard form cannot directly be applied to the demixing problem. We can, however, use the same data and covariance decomposition\n\nX=∑ϕXϕ+XnoiseC=∑ϕCϕ+Cnoise\n\nthat dPCA is using and construct a separate LDA for each marginalization ϕ. To the best of our knowledge, this framework does not have an established name, so we call it factorial LDA.\n\nLet us first consider the case of finding demixed components for marginalization 𝐗ϕ. We will denote the remaining part of the data matrix as 𝐗-ϕ=𝐗-𝐗ϕ and the remaining part of the covariance matrix as 𝐂-ϕ=𝐂-𝐂ϕ. In turn, the goal of LDA will be to find linear projections that have high variance in 𝐂ϕ and low variance in 𝐂-ϕ. In LDA, these matrices are usually called between-class and within-class covariance matrices (Hastie et al., 2009). The standard treatment of LDA is to maximize the multivariate signal-to-noise ratio\n\ntr(DCϕD⊤[DC−ϕD⊤]−1),\n\nwhere 𝐃 is the matrix with discriminant axes in rows. The well-known solution is that 𝐃LDA is given by the leading eigenvectors of 𝐂-ϕ-1⁢𝐂ϕ (stacked together as rows), or, equivalently, as eigenvectors of 𝐂-1⁢𝐂ϕ.\n\nMore useful for our purposes is the reformulation of LDA as a reduced-rank regression problem (Izenman, 2008; De la Torre, 2012). When classes are balanced, it can be formulated as\n\nLLDA=∥𝐆ϕ-𝐅𝐃𝐗∥2,\n\nwhere 𝐆ϕ is a class indicator matrix. This matrix has as many rows as there are possible values of parameter ϕ and specifies which data point is labeled with which parameter value: Gi⁢j=1 if the j-th data point belongs to class i (has i-th value of the parameter ϕ) and Gi⁢j=0 otherwise. In the toy example shown in Figure 2, there are three classes with five points each, and so 𝐆ϕ will be a 3×15 matrix of zeros and ones. In this reformulation of LDA, the main interest is in the decoder matrix 𝐃, whereas the encoder matrix 𝐅, which serves to map the low-dimensional representation onto the class indicator matrix, plays only an auxiliary role.\n\nIn contrast, the dPCA loss function is\n\nLdPCA=∥𝐗ϕ-𝐅𝐃𝐗∥2,\n\nwhere 𝐗ϕ is the matrix of the same size as 𝐆ϕ with j-th column being the class centroid of the class to which the j-th point belongs. This comparison highlights the difference between the two methods: LDA looks for decoders that allow to reconstruct class identity (as encoded by 𝐆ϕ) whereas dPCA looks for decoders that allow to reconstruct class means (as encoded by 𝐗ϕ). Figure 2b,f,h provides a toy example of a situation when these two goals yield very different solutions: the LDA projection separates the three classes better than the dPCA projection, but the dPCA projection preserves the information about the distance between classes.\n\nUsing the explicit solution for reduced-rank regression, one can show that LLDA does indeed have eigenvectors of 𝐂-1⁢𝐂ϕ as a solution 𝐃LDA for decoder (see Section 8.5.3 in Izenman, 2008). Following the similar logic for LdPCA, one can derive the corresponding expression for the dPCA decoder: 𝐃dPCA is given by the eigenvectors of 𝐂-1⁢𝐂ϕ2 (personal communication with Maneesh Sahani).\n\nA statistical test known as MANOVA can be seen as another possible factorial generalization of LDA. Given the same data and covariance decomposition, MANOVA tests if the effect of ϕ is statistically significant by analyzing eigenvalues of 𝐂noise-1⁢𝐂ϕ. The eigenvectors of this matrix can in principle serve as decoders, but these projections are optimized to separate the contribution of ϕ from noise, not from the contributions of noise and other parameters. Hence, MANOVA is not the appropriate method for demixing purposes.\n\nWhile the toy example of Figure 2 illustrates that dPCA and LDA will in principle have very different solutions, we note that in all datasets considered here factorial LDA and dPCA yielded very similar components. This may reflect several pecularities of the data: for instance, the population activity for different values of the same parameter was spaced rather evenly, and all decisions were binary. Nevertheless, we emphasize that dPCA is better suited for (demixed) dimensionality reduction due to its focus on reconstructing the original data, as explained and discussed in the Results (Figure 2).\n\nDemixed PCA as presented here is conceptually based on our previous work. Machens et al. (2010) suggested a demixing method called difference of covariances (DOC) that can only handle two parameters, e.g. stimulus s and time t. Given PSTHs 𝐱⁢(s,t), DOC first constructs stimulus-dependent and time-dependent marginalizations 𝐱¯⁢(s)=⟨𝐱⁢(s,t)⟩t and 𝐱¯⁢(t)=⟨𝐱⁢(s,t)⟩s, and then computes the difference between the stimulus and time covariance matrices 𝐂s=⟨𝐱¯⁢(s)⁢𝐱¯⁢(s)⊤⟩ and 𝐂t=⟨𝐱¯⁢(t)⁢𝐱¯⁢(t)⊤⟩,\n\n𝐒=𝐂s-𝐂t.\n\nEigenvectors of S with maximum (positive) eigenvalues correspond to directions with maximum stimulus variance and minimum decision variance. Vice versa, eigenvectors with minimum (negative) eigenvalues correspond to directions with maximum decision variance and minimum stimulus variance. In the toy example presented in Figure 2 DOC finds the axis that is very close to the first PCA axis of class centroids (which is also very close to the dPCA encoder axis shown on the figure), providing worse demixing than both LDA and dPCA.\n\nA possible extension of DOC to more than two parameters is described in Machens (2010). Here the PSTHs are assumed to depend on M parameters, and the method constructs M marginalizations by averaging over all parameters except one. The respective covariance matrices 𝐂ϕ are then formed as above. The extension of DOC seeks to find the matrix of orthogonal directions 𝐔 such that\n\nL=∑ϕtr(Uϕ⊤CϕUϕ)\n\nis maximized subject to 𝐔⊤⁢𝐔=𝐈 where 𝐔=[𝐔1⁢…⁢𝐔M]. For M=2 this can be shown to be equivalent to the original DOC. Note that Machens (2010) did not address the interaction terms.\n\nThe connection between the current dPCA and the DOC approach can be made more explicit if we consider the full covariance decomposition C=∑ϕCϕ and introduce into the dPCA loss function an additional constraint that both encoder and decoder should be given by the same matrix with orthonormal columns: 𝐅ϕ=𝐃ϕ⊤=𝐔ϕ. Then\n\n∥Xϕ−UϕUϕ⊤X∥2=∥Xϕ−UϕUϕ⊤Xϕ∥2+∥UϕUϕ⊤X−ϕ||2=∥Xϕ∥2−∥UϕUϕ⊤Xϕ||2+∥UϕUϕ⊤X−ϕ||2∼−tr(Uϕ⊤(Cϕ−C−ϕ)Uϕ),\n\nwhere the first equality follows from properties of the decomposition, the second equality from the properties of the orthonormal matrices 𝐔ϕ, and the third equality uses the definition of the covariance. This derivation shows that the difference of covariances 𝐂ϕ-𝐂-ϕ emerges from the dPCA loss function if the decoder and encoder are given by the same set of orthogonal axes. However, such axes 𝐔ϕ from different marginalizations ϕ will in general not be orthogonal to each other, whereas both DOC and its generalization insisted on orthogonal axes.\n\nBoth the original DOC and its extension ignored interaction terms. Brendel et al. (2011) introduced interaction terms and the full covariance splitting C=∑ϕCϕ as described in this manuscript, and developed a probabilistic dPCA model based on probabilistic PCA (PPCA); to remove ambiguity we call this method dPCA-2011. Similar to PPCA, dPCA-2011 assumes that the data are described by a linear model with Gaussian residuals, i. e.\n\np(𝐱|𝐳)=𝒩(𝐖𝐳,σ2𝐈),\n\nbut the prior over the components z is chosen such that the components are sparsely distributed over marginalizations. In other words, the prior is chosen such that those components are favored that have variance in only one marginalization. Under the constraint that decoding directions W are orthogonal, the model can be fit using the expectation-maximization algorithm. However, the probabilistic formulation of Brendel et al. (2011) still suffers from the orthogonality constraint. As explained in the Discussion, the orthogonality constraint is too rigid and can prevent successful demixing if parameter subspaces are sufficiently non-orthogonal. Indeed, we applied dPCA-2011 to all our datasets and observed that dPCA-2015 showed better demixing (Figure 14). Moreover, dPCA-2011 failed to find any decision components in the visuospatial working memory task.\n\nIn addition, the formulation of dPCA in this manuscript is radically simplified compared to Brendel et al. (2011), features an analytic solution and is easier to compare with other linear dimensionality reduction techniques.\n\nAbove we presented marginalization procedure for three parameters. In order to generalize it for an arbitrary number of parameters, we introduce a more general notation. We denote as Ψ the set of parameters (in the previous section Ψ={t,s,d}; note that the trial index is not included into Ψ) and write x¯ψ to denote a decomposition term that depends on a subset of parameters ψ⊆Ψ. In particular, x¯∅=x¯. In full analogy to the 3-parameter case, each term can be iteratively computed via\n\nx¯ψ=⟨x−∑τ⊂ψx¯τ⟩Ψ∖ψ=⟨x⟩Ψ∖ψ−∑τ⊂ψx¯τ,\n\nwhere ⟨⋅⟩Ψ∖ψ denotes averaging over all parameters that are not elements of ψ and averaging over the trial index. This equation can be rewritten in a non-iterative way by expanding the sum; this yields the expression with alternating signs that is similar to our ANOVA-style equations above:\n\n(⋆)x¯ψ=∑τ⊆ψ(−1)|τ|⋅⟨x⟩(Ψ∖ψ)∪τ.\n\nOne can verify that this formula correctly describes the 3-parameter case presented above; the general case can be proven by induction. The noise term is defined via\n\nxnoise=x−∑ψx¯ψ=x−⟨x⟩∅.\n\nThis decomposition has several useful properties. First, the average of any marginalization x¯ψ over any parameter γ∈ψ is zero. This can be seen from the equation (⋆) because after averaging over γ all terms will split into pairs with opposite signs (indeed, for each τ∋γ there is another τ′=τ∖γ). Second, all marginalizations are pairwise uncorrelated, i.e. their covariance is zero: ⟨x¯ψ⁢x¯χ⟩Ψ=0. This can be seen from equation (⋆) because x¯ψ and x¯χ both consist of an even number of terms with alternating signs, so their product will also consist of an even number of terms with alternating signs, and after averaging over Ψ all terms will become equal to ± x¯2 and cancel each other. Third, from the definition of the noise term it follows that any marginalization x¯ψ is uncorrelated with the noise term: ⟨x¯ψ⁢x¯noise⟩Ψ=0.\n\nThe fact that all marginalizations and the noise are pairwise uncorrelated allows to segregate the variance of x (here we assume that x is centered, i.e. x¯=0):\n\nvar[x]=⟨x2⟩Ψ=⟨(∑ψx¯ψ+xnoise)2⟩Ψ=∑ψ⟨x¯ψ⟩Ψ+⟨xnoise2⟩Ψ=∑ψvar[x¯ψ]+var[xnoise].\n\nTurning now to the multivariate case, if we replace x with x∈RN, everything remains true but variances should be replaced by covariance matrices:\n\nC=⟨xx⊤⟩Ψ=∑ψCψ+Cnoise.\n\nNote that in ANOVA literature one usually talks about decomposing sums of squares ∑x2 and in MANOVA literature about decomposing scatter matrices ∑𝐱𝐱⊤, because (co)variances of different terms are computed from these sums using different denominators (depending on the corresponding number of degrees of freedom) and do not add up. We do not make this distinction and prefer to talk about decomposing the (co)variance, i.e. all (co)variances here are defined with the same denominator equal to the total number of sample points."
    }
}
{
    "id": "dbpedia_1004_3",
    "rank": 4,
    "data": {
        "url": "https://posts.specterops.io/ready-to-hunt-first-show-me-your-data-a642c6b170d6",
        "read_more_link": "",
        "language": "en",
        "title": "Ready to hunt? First, Show me your data!",
        "top_image": "https://miro.medium.com/v2/resize:fit:640/0*jMWvjIv69DaaUe1g.jpg",
        "meta_img": "https://miro.medium.com/v2/resize:fit:640/0*jMWvjIv69DaaUe1g.jpg",
        "images": [
            "https://miro.medium.com/v2/resize:fill:64:64/1*dmbNkD5D-u45r44go_cf0g.png",
            "https://miro.medium.com/v2/resize:fill:88:88/1*9WbXEpOxOhaMq99CwG1ESQ.png",
            "https://miro.medium.com/v2/resize:fill:48:48/1*D-FDlfkqivRBQZoESrwtqw.png",
            "https://miro.medium.com/v2/resize:fill:144:144/1*9WbXEpOxOhaMq99CwG1ESQ.png",
            "https://miro.medium.com/v2/resize:fill:64:64/1*D-FDlfkqivRBQZoESrwtqw.png"
        ],
        "movies": [],
        "keywords": [],
        "meta_keywords": [
            ""
        ],
        "tags": null,
        "authors": [
            "Roberto Rodriguez"
        ],
        "publish_date": "2017-12-15T14:14:00+00:00",
        "summary": "",
        "meta_description": "Nowadays, Threat Hunting is a very popular topic among not just security practitioners in the InfoSec community, but also organizations that are looking to take their security posture to the next…",
        "meta_lang": "en",
        "meta_favicon": "https://miro.medium.com/v2/resize:fill:256:256/1*D-FDlfkqivRBQZoESrwtqw.png",
        "meta_site_name": "Medium",
        "canonical_link": "https://cyberwardog.blogspot.com/2017/12/ready-to-hunt-first-show-me-your-data.html",
        "text": "Nowadays, Threat Hunting is a very popular topic among not just security practitioners in the InfoSec community, but also organizations that are looking to take their security posture to the next level. Several great hunting resources have been shared that provide complex queries and methodologies that facilitate the detection of techniques used by real adversaries. Some have even shared maturity models and explained how important data, tools, and skills are when building your hunt team. However, only a few resources have gotten into the details of how to really start your program and develop metrics to track the effectiveness of the hunt team from day one.\n\nA few months ago, I blogged about “How Hot is Your Hunt Team” where I examined the MITRE ATT&CK framework in the form of a heat map in order to measure the effectiveness of a hunt team, and loved the way the community embraced it. That was my first attempt to share the basics of putting a number to hunt team effectiveness while validating the detection of techniques defined by the ATT&CK framework. Even though the score definitions were very subjective, the main purpose of the post was to provide a basic approach to measure the current state of a hunt team program.\n\nIn this post, I wanted to take my current heatmap sheet/approach and go a little bit deeper on one of the main factors of a hunt program that, in my opinion, should be assessed and measured before your first engagement: “Data.” I know you want to start hunting (do the cool stuff) and show/learn all the different ways to find evil; however, without the right data quality, it is very hard to focus on the mission, be productive, and hunt effectively.\n\nWhat is Data Quality?\n\nOne definition used the most about data quality is from Joseph M. Juran, author of Juran’s Quality Handbook, who quoted, in page 998, “Data are of high quality if they are fit for their intended uses in operations, decision making and planning.” In other words, if data needed for a hunting engagement does not meet specific requirements defined by the hunt team, then the data is not considered quality data since it is affecting the intended purpose of it. I liked how Stephanie Zatyko from “Experian Data Quality” defined it by saying that if data was water, data quality ensures water is clean and does not get contaminated.\n\nData Quality Goals (hunting perspective)\n\nReduce the time hunters spend fixing and validating data issues increasing productivity during hunting engagements.\n\nImprove consistency across data sources to manipulate data more efficiently allowing more complex analytics that rely on several resources for extra context.\n\nEnhance automation flow\n\nWhy do I have to care about Data Quality? That’s not my job!\n\nThis is a great question, and I have heard that multiple times. If you have a SIEM that collects and integrates data from different tools, then yes, most likely there is a data governance team in your organization and they might be already measuring and fixing the quality of the data you use. However, how many times have you run across these scenarios even when you have a dedicated team:\n\nData fields from different data sources do not have the same name (standard naming convention)\n\nDeviceName, hostname, host, computername (Appliance collecting the data and sending it to a SIEM vs endpoint creating the logs)\n\nSrc, dst, host, ipaddress, IP (Network vs Endpoint logs)\n\nSecurityID, username, accountname, NewLogon.username (Win Security logs vs other logs containing usernames)\n\nData sources missing data, not being parsed/split properly CommandLine values are not present in a few endpoints, and they are replaced with blank or null values\n\nYou might have enabled CommandLine auditing via GPO but not being applied everywhere or simply not working properly. Does it affect the quality of your data to hunt? if so, is that a Data Governance problem?\n\n“Message” fields containing extra information needed for stacking purposes, but not available.\n\nGreat example is my post about “Enabling Enhanced PowerShell Logging ..”.\n\nTimestamps reflect only ingestion time and not actual creation time, and data is only available for a certain period of time.\n\nWhat if your endpoints stop sending logs to your SIEM after disconnecting from the network. Next day your SIEM gets several alerts on events from several systems that are 24 hours old, but the main timestamps show that the events happened earlier in the morning.\n\nEndpoint data is only available from high value targets and for a week only.\n\nYou are ready to hunt and realize that you can only hunt in certain parts of your environment because your company only purchased/configured only parts of your network to send data back to your SIEM.\n\nDid it sound familiar? The idea here is not to do the job of a governance team, but to guide them in the right direction and provide recommendations. You are the security subject matter expert after all. I hope the basic examples above made you think a little bit about why you also need to care about how the quality of your data is assessed/measured when getting ready for a hunting campaign.\n\nI might be doing that already\n\nEven if you have been already sending emails to your data governance team about similar issues, do you track what aspects of your data quality are affecting your hunting engagements the most? How do you categorize those issues and focus on what is most important? This is where data quality dimensions are super useful to start understanding how to measure the quality of your data.\n\nData Quality Dimensions\n\nUsed to simplify the representation of measurable characteristics of data quality. There are several data quality dimensions defined out there that are useful, depending on the intended use of the data. However, for data that I need for a hunt program, I like to reference a few data quality dimensions from the “DoD Core Set Of Data Quality Requirements.” A few of those Data Quality (DQ) dimensions could help your team categorize gaps found in data intended to be used for hunting purposes. I will explain in a bit which dimensions are very helpful to measure the effectiveness of your hunt team from a data perspective. For now, let’s get familiar with the ones provided by the DoD (These are just general definitions of data quality dimensions. We will adapt those definitions to our specific data).\n\nWhere do I start? (Basic Methodology)\n\n1. Identify data sources that you have\n\nBefore you even start measuring the quality of your data, make sure you understand and document the data sources that you have. One way to expedite this process is by looking at the tools that your organization has purchased or configured to give you the data that you are currently working with. I recommend starting to document the tools that are present in your environment such as, for example, Sysmon, OSQuery, Moloch, AV, Bro, EDR solutions, etc.\n\nYou can start your list of data sources at a very high-level and then provide more detailed information about the specific events being collected from each data source. One example can be the following file that documents events provided by Windows Security event logs. Not all data sources provide events with event ids, so you might want to organize them by their log category.\n\n2. Identify data sources that you need\n\nNext, make sure you understand and document what data sources you actually need. This will allow you to define expectations and plan accordingly for a hunting engagement from a data perspective. In addition, this will help you to assess your current tools and make sure they are providing the data you really need. If you do not know what data sources you might need from a hunting perspective, you can take a look at the MITRE ATT&CK framework. I don’t know if you have noticed, but almost every technique defined in the framework has a “Data Sources” field in the reference box to the right which tells you what Data Sources are recommended for the detection of the specific technique.\n\nYou can retrieve Data Sources information from every single technique without manually going through every single technique web page by using the ATT&CK Python Client project. You can install the attackcti library by running the following commands:\n\nInstall from source\n\ngit clone https://github.com/Cyb3rWard0g/ATTACK-Python-Client\n\ncd ATTACK-Python-Client\n\npip install .\n\nInstall via PIP\n\npip install attackcti\n\nYou can then use python3 to import the attackcti library and run a few commands to retrieve all the unique data sources recommended by the ATT&CK team across the whole framework. You can also follow this Jupyter Notebook available with the project.\n\nwardog$ python3\n\n>>>\n\n>>> from attackcti import attack_client\n\n>>> lift = attack_client()\n\n>>> data_sources = lift.get_all_data_sources()\n\n>>> len(data_sources)\n\n50\n\n>>> data_sources\n\nThis information will help identify data coverage gaps when you start mapping the data sources you have with the ones recommended by the ATT&CK framework. Remember that this is just an idea, and you might disagree with what is needed per technique according to MITRE, but at least this is a good place to start. I encourage you to define new data sources that are not mentioned in the ATT&CK framework, but that you believe will get you closer to the detection of specific techniques (Please share any new data sources with MITRE that you believe are missing. This helps the community).\n\nIf you want to know what data sources are mapped to every single technique, you can get all that data at once via the ATT&CK Python Client project with the following commands:\n\nwardog$ python3\n\n>>>\n\n>>> from attackcti import attack_client\n\n>>> from pandas import *\n\n>>> from pandas.io.json import json_normalize\n\n>>>\n\n>>> lift = attack_client()\n\n>>> all_techniques = lift.get_all_techniques()\n\n>>>\n\n>>> techniques_normalized = json_normalize(all_techniques)\n\n>>> techniques = techniques_normalized.reindex(['technique','data_sources'], axis=1)\n\n>>>\n\n>>> techniques.head()\n\nIf you want to look for techniques mapped to a specific data source, you can use the command get_techniques_by_datasources(data_source) from the ATT&CK Python Client project as shown below:\n\nwardog$ python3\n\n>>>\n\n>>> from attackcti import attack_client\n\n>>>\n\n>>> lift = attack_client()\n\n>>> data_source = 'PROCESS MONITORING'\n\n>>>\n\n>>> results = lift.get_techniques_by_datasources(data_source)\n\n>>> len(results)\n\n157\n\n>>> results[1]\n\n3. Map what you have to what you need\n\nTake each data source needed and ask this question about each of the tools you have: Is this tool providing the data source I need? With a binary system, you can mark each with a 1 or a 0 depending on your answer. Don’t focus on how well or how much the tool is giving you, per data source (DS). Those aspects will be addressed while measuring the quality of your data. One example I have is shown in Figure 4 below, where I put together a table with data sources from MITRE ATT&CK on the left and tools on the top as column names. Remember this is just an example and the numbers just represent a Yes or a No for now. After adding data quality measurements, your numbers will change. This is just to start at least having an idea of where you might be from a data availability perspective.\n\nif you want to be more specific on documenting the availability of what you have vs what you need, you can do something similar to the following document:\n\n4. Define data quality (DQ) dimensions\n\nThis is where you take your data sources to the next level. If your organization already has DQ dimensions defined, work with your data governance team and make sure you follow their DQ requirements to start. Some DQ dimensions defined in your organization for other data sources might not apply to the data you use for hunting, and it is fine. As I explained earlier, one good reference to define data quality dimensions is the “DOD Core Set Of Data Quality Requirements,” which helped me to define the following:\n\nA few things to remember:\n\nI removed Uniqueness because event logs are not necessarily always unique and can be repeated several times even with the same timestamp depending on the type of activity.\n\nI removed Accuracy and Validity because I feel that those dimensions might be a little hard and time consuming to validate from a hunter’s perspective.\n\nStarting with the three DQ dimensions defined above, you will start adding value to your program. I also added questions to the table that I believe should help you to identify gaps faster. You can now start categorizing the gaps you currently present from a data quality perspective.\n\n5. Define a scoring system for your data quality dimensions\n\nThis part is very important and there is no a right or wrong answer. Questions or sections of your scoring table will depend on your organization. For example, your company might accept the risk of retaining data for 30 days instead of 6 months so you will have to adjust your scoring to reflect what is good or not from a retention (Timeliness) perspective. I created the following table as a basic approach to give you a few ideas for when you also build your own one:\n\nA few things that I want to make sure are followed when using the table score above:\n\nIn order to get a full score per level and move to the next one, you have to comply with all the requirements of the specific level where you currently are at. For example, in Completeness, if you have 100% endpoint coverage from a data source perspective and 75% of required data missing then your score in Completeness will be 1 and not a 5. Not just because your tool is deployed everywhere, it gets a high score. This is very important to remember.\n\nEach level requirement must be defined by your team. I am just providing examples and just basic ideas. My examples might not be detailed enough for your team. If you have better requirements and would like to share, let me know.\n\nIf you think that other data quality dimensions apply to your data, feel free to add them to the table and define the requirements for it.\n\nConsistency and Timeliness are measured across all your data sources at once and not individually. This is the case when you have your logs in a centralized location and managed from there (ELK, Splunk, etc.) Therefore, when you set the scores for Consistency and Timeliness, make sure you remember that the scores could be the same across all your data sources if you are sending everything to a SIEM.\n\nYou don’t have to be 100% right on your answers. Going very deep into the specific details of the quality of your data might take longer than you think. The idea here is to start understanding your data from a high level perspective and start conversations with the team in charge of maintaining your data (internal or external parties).\n\nIf you work in an environment where your developers are constantly spawning AWS containers and getting rid of them after a day or two of testing, it will be almost impossible to say that you have 100% coverage because you might not be able to protect those systems during its lifetime.\n\n6. Asses the quality of your data\n\nNow that you have your data quality dimensions defined and a scoring table ready, you can start showing some value. First, do you remember that we created a table to show how many required data sources were being provided by your tools purchased or configured in your organization? Based on that table, you can only draw some conclusions from a data source coverage perspective on how much your tools will help you during hunting engagements.\n\nTool and data source coverage (Per Technique)\n\nSince you have a tool mapped to specific data sources, then you can use again the “data sources” field from each technique in the ATT&CK Framework and calculate the percentage of the data sources provided by your tool from the number of data sources recommended.\n\nA few things that I want to point out from figure 11 above:\n\nYou can see that your EDR solution looks to be doing better than your other solutions since it provides more data than what you other tools are capable of or configured to do so. However, you should NOT take that score as your final score. I have seen tools being mapped to the ATT&CK Matrix in recent presentations and I don’t see much value on just saying that they help because they give you the required data (my opinion).\n\nAnother misconception could be that your EDR solution seems to have similar scores with free tools such as Sysmon. Here is where you might have people saying: “Why do I need to buy EDR when I have Sysmon? I can see the 100% for both when it comes to detect “Data From Local System” so why buy?” I just don’t like to make decisions based ONLY on the data that the tool says it provides.\n\nOnce you take the percentages per technique and tool, you can then create bins to organize your results (0–20, 20–40, 40–60, 60–80, 80–100). You will be able to see the numbers of techniques mapped to the percentage of data sources provided by each tool as shown in Figure 9 below.\n\nA few statistics:\n\nYour EDR seems to be the most effective tool to use when hunting for the majority of techniques defined in the ATT&CK Framework. Without the extra context that I will show you later with DQ dimensions, that number does not mean much to me.\n\nI can see that my network tools are not doing well and it is because there are more techniques calling for host rather than network logs. However, you know that network and host complement each other. I just would not use my Moloch tool when I am looking for Credential Dumping activity and my traffic is encrypted.\n\nI can see also that my sysmon configured in a certain way is not going to help me as much by itself. I will need to correlate the data with other tools to be more effective.\n\nTool and data source coverage (Per Tactic)\n\nThen, what does it look like from a Tactics Perspective?\n\nThe graph above shows you coverage but per tactic after grouping each technique with its specific tactic. A very useful graph for when you want to see your full coverage from a data source availability (Completeness Only) perspective per tactic across all the techniques that ATT&CK defines.\n\nConsidering availability as the only requirement for a data source, this is what your data source availability might look like when mapped to ATT&CK techniques:\n\nExpand your coverage results with data quality dimensions\n\nI created a table where I got rid of the binary system scoring and started using the table scoring from Figure 10. I went over each data source per tool and started measuring the quality of my data.\n\nI like the view from Figure 15 above because it tells me more about what I get from a specific tool that I purchased or configured in my environment. For example:\n\nA vendor tells me that the tool I am purchasing provides registry monitoring, and that it automates the collection of AutoRuns. However, during the POC I find out that it is missing several registry locations identified by my team that are needed/required to hunt for persistence. By measuring the completeness of the data, I can tell that the quality of the data I am getting from a commercial tool is not going to help me much to validate the detection of several persistence mechanisms.\n\nSysmon is configured to collect loaded modules events (EID 7), but due to the high number of events generated, its config is cut back to only monitor 5 specific modules/images. By limiting the number of information being collected, it also limits the detection capability for techniques calling for DLL monitoring. Not just because you set your Sysmon config to include a few LoadedImages and collect that data, you are satisfying the “DLL Monitoring” required data source 100%.\n\nSysmon gets configured to pull every single hash type capable to calculate, but it is all pushed into one field named “Hashes” (by default). You are also collecting MD5 hashes from your AV data source. You want to run a query to stack MD5 hashes from both. How can you stack the values of a specific hash type across those two data sources when every hash type in Sysmon gets populated under one field?\n\nExample:\n\nHashes:MD5=59273BB2B6DDAFCF9C7F95AA71AC9C56,SHA1=38998A827EB2E7B4879BC4E8B09DE632793E4976,IMPHASH=88FFDD8F354A1E65DF5D6F793E536605,SHA256=086E202398A409CB872C4D17424F81477A4CB37BCD2BBE59A26639D63801621C\n\nAlso, I calculated the average of each tool and the Max values of Data Quality overall per data source. One final thing to notice, too, is that for my EDR tool, I applied a different score under Consistency and Timeliness from the rest of the data sources. This is because usually when an organization buys an EDR solution, it usually comes with its own infrastructure and naming convention standards. Those two scores can be affected by you if you want to centralize all your data sources under one system (i.e. SIEM).\n\nHow do I add that to my overall score?\n\nI updated the HuntTeam_HeatMap’s “Score Defs” sheet with the new DQ dimensions scoring definitions in order to improve the current ones, and get a little bit closer to a more realistic score.\n\nNext, after also considering talent and technology for our overall score, you can automatically start generating an overall score based on an average score from (Data Quality, Talent & Technology). The HuntTeam_HeatMap document in the ThreatHunter-Playbook performs those calculations automatically for you and replaces the final number with the score label.\n\nWhat does our heat map look like now?\n\nConsidering the numbers that I see as my final data quality ones, I can tell that my numbers are looking Fair (not as “Good” as how the availability score was making it look like, but Good/Fair, mostly Fair overall). This is also because I graded talent and technology a 3 (Good scores).\n\nNow, what if I reduce the score of my Data Quality and technology to Poor and leave the Talent at Good?\n\nYou can see that the effectiveness of your hunt team to validate the detection of adversary techniques went from mostly Fair and a little bit of Good to Poor overall. You might still have good talent whom can put together a few scripts and start scanning your network, but it is not enough. You need good data quality and the right technology.\n\nI hope this was very helpful for those trying to create metrics around the quality of their data, add it to their overall score, and willing to do the respective pre-hunt activities to hunt more effectively and efficiently. You can find the link to the updated HuntTeam_HeatMap below. I would love to hear your thoughts on these new metrics, and if it is helpful to start creating your own.\n\nFeedback is greatly appreciated!"
    }
}
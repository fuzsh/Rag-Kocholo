{
    "id": "dbpedia_6905_1",
    "rank": 76,
    "data": {
        "url": "https://mjo.osborne.economics.utoronto.ca/index.php/tutorial/index/1/41",
        "read_more_link": "",
        "language": "en",
        "title": "Mathematical methods for economic theory: 7.1 Optimization with inequality constraints: the Kuhn",
        "top_image": "https://mjo.osborne.economics.utoronto.ca/icon.png",
        "meta_img": "https://mjo.osborne.economics.utoronto.ca/icon.png",
        "images": [
            "https://mjo.osborne.economics.utoronto.ca/templates/images/mathTutorial/backsec.gif",
            "https://mjo.osborne.economics.utoronto.ca/templates/images/mathTutorial/back.gif",
            "https://mjo.osborne.economics.utoronto.ca/templates/images/searchIcon.png",
            "https://mjo.osborne.economics.utoronto.ca/templates/images/mathTutorial/next.gif",
            "https://mjo.osborne.economics.utoronto.ca/templates/images/mathTutorial/nextsec.gif"
        ],
        "movies": [],
        "keywords": [],
        "meta_keywords": [
            "mathematical methods",
            "economic theory",
            "optimization",
            "inequality constraints",
            "Kuhn-Tucker conditions"
        ],
        "tags": null,
        "authors": [],
        "publish_date": null,
        "summary": "",
        "meta_description": "Mathematical methods for economic theory: Kuhn-Tucker conditions for optimization problems with inequality constraints",
        "meta_lang": "en",
        "meta_favicon": "/icon.png",
        "meta_site_name": "",
        "canonical_link": null,
        "text": "×\n\nThank you for your comment. The author of the tutorial has been notified.\n\n7.1 Optimization with inequality constraints: the Kuhn-Tucker conditions\n\nMany models in economics are naturally formulated as optimization problems with inequality constraints.\n\nConsider, for example, a consumer's choice problem. There is no reason to insist that a consumer spend all her wealth. To allow her not to spend it all, we can formulate her optimization problem with inequality constraints:\n\nmaxx u(x) subject to p·x ≤ w and x ≥ 0.\n\nDepending on the character of the function u and the values of p and w, we may have p·x < w or p·x = w at a solution of this problem.\n\nOne approach to solving this problem starts by determining which of these two conditions holds at a solution. In more complex problems, with more than one constraint, this approach does not work well. Consider, for example, a consumer who faces two constraints (perhaps money and time). Three examples are shown in the following figure, which should convince you that we cannot deduce from simple properties of u alone which of the constraints, if any, are satisfied with equality at a solution.\n\nx1 → x2 ↑ Level curve of u Red constraint binds; green constraint slack x1 → x2 ↑ Level curve of u Both constraints bind x1 → x2 ↑ Level curve of u Green constraint binds; red constraint slack\n\nWe consider a problem of the form\n\nmaxx f(x) subject to gj(x) ≤ cj for j = 1, ..., m,\n\nwhere f and gj for j = 1, ..., m are functions of n variables, x = (x1, ..., xn), and cj for j = 1, ..., m are constants.\n\nAll the problems we have studied so far may be put into this form.\n\nEquality constraints\n\nWe introduce two inequality constraints for every equality constraint. For example, the problem\n\nmaxx f(x) subject to h(x) = 0\n\nmay be written as\n\nmaxx f(x) subject to h(x) ≤ 0 and −h(x) ≤ 0.\n\nNonnegativity constraints\n\nFor a problem with a constraint xk ≥ 0 we let gj(x) = −xk and cj = 0 for some j.\n\nMinimization problems\n\nFor a minimization problem we multiply the objective function by −1:\n\nminx h(x) subject to gj(x) ≤ cj for j = 1, ..., m\n\nis the same as\n\nmaxx f(x) subject to gj(x) ≤ cj for j = 1, ..., m,\n\nwhere f(x) = −h(x).\n\nTo start thinking about how to solve the general problem, first consider a case with a single constraint (m = 1). We can write such a problem as\n\nmaxx f(x) subject to g(x) ≤ c.\n\nThere are two possibilities for the solution of this problem. In the following figures, the black closed curves are contours of f; values of the function increase in the direction shown by the blue arrows. The downward-sloping red line is the set of points x satisfying g(x) = c. The set of points x satisfying g(x) ≤ c is the shaded set below and to the left of the line.\n\ng(x) > c g(x) < c g(x) = c x* g(x) > c g(x) < c g(x) = c x*\n\nIn each figure the solution of the problem is the point x*. In the first figure the constraint binds at the solution: a change in c changes the solution. In the second figure, the constraint is slack at the solution: small changes in c have no effect on the solution.\n\nAs before, define the Lagrangean function L by\n\nL(x) = f(x) − λ(g(x) − c).\n\nThen from our previous analysis of problems with equality constraints and with no constraints,\n\nif g(x*) = c (as in the left-hand panel) and the constraint satisfies a regularity condition, then L'i(x*) = 0 for all i\n\nif g(x*) < c (as in the right-hand panel), then fi'(x*) = 0 for all i.\n\nNow, I claim that in the first case (that is, if g(x*) = c) we have λ ≥ 0. Suppose, to the contrary, that λ < 0. Then we know that a small decrease in c raises the maximal value of f. That is, moving x* inside the constraint raises the value of f, contradicting the fact that x* is the solution of the problem.\n\nIn the second case, the value of λ does not enter the conditions, so we can choose any value for it. Given the interpretation of λ, setting λ = 0 makes sense. Under this assumption we have f'i(x) = L'i(x) for all x, so that L'i(x*) = 0 for all i. Thus in both cases we have L'i(x*) = 0 for all i, λ ≥ 0, and g(x*) ≤ c. In the first case we have g(x*) = c and in the second case λ = 0.\n\nWe may combine the two cases by writing the conditions as\n\nL'i(x*) = 0 for j = 1, ..., n λ ≥ 0, g(x*) ≤ c, and either λ = 0 or g(x*) − c = 0.\n\nNow, the product of two numbers is zero if and only if at least one of them is zero, so we can alternatively write these conditions as\n\nL'i(x*) = 0 for j = 1, ..., n λ ≥ 0, g(x*) ≤ c, and λ[g(x*) − c] = 0.\n\nThe argument I have given suggests that if x* solves the problem and the constraint satisfies a regularity condition, then x* must satisfy these conditions.\n\nNote that the conditions do not rule out the possibility that both λ = 0 and g(x*) = c.\n\nThe condition that either (i) λ = 0 and g(x*) ≤ c or (ii) λ ≥ 0 and g(x*) = c is called a complementary slackness condition.\n\nFor a problem with many constraints, then as before we introduce one multiplier for each constraint and obtain the Kuhn-Tucker conditions, defined as follows.\n\nDefinition\n\nLet f and gj for j = 1, ..., m be differentiable functions of n variables defined on an open set and let cj for j = 1, ..., m be numbers. Define the function L of n variables by\n\nL(x) = f(x) − ∑m\n\nj=1λj(gj(x) − cj) for all x.\n\nThe Kuhn-Tucker conditions for the problem\n\nmaxx f(x) subject to gj(x) ≤ cj for j = 1, ..., m\n\nare\n\nL'i(x) = 0 for i = 1, ..., n λj ≥ 0, gj(x) ≤ cj and λj[gj(x) − cj] = 0 for j = 1, ..., m.\n\nThese conditions are named in honor of Harold W. Kuhn (1925–2014) and Albert W. Tucker (1905–1995; obituary), who first formulated and studied them.\n\nOn the following pages I discuss results that specify the precise relationship between the solutions of the Kuhn-Tucker conditions and the solutions of the problem. The following example illustrates the conditions for a specific problem.\n\nExample 7.1.1\n\nConsider the problem\n\nmaxx1, x2 [−(x1 − 4)2 − (x2 − 4)2] subject to x1 + x2 ≤ 4 and x1 + 3x2 ≤ 9,\n\nillustrated in the following figure.\n\nx1 → x2 ↑ 4 9 0 x1 + 3x2 = 9 x1 + x2 = 4 Level curves of −(x1 − 4)2 − (x2 − 4)2\n\nThe function L is given by\n\nL(x1, x2) = −(x1 − 4)2 − (x2 − 4)2 − λ1(x1 + x2 − 4) − λ2(x1 + 3x2 − 9).\n\nThe Kuhn-Tucker conditions are\n\n−2(x1 − 4) − λ1 − λ2 = 0 −2(x2 − 4) − λ1 − 3λ2 = 0 x1 + x2 ≤ 4, λ1 ≥ 0, and λ1(x1 + x2 − 4) = 0 x1 + 3x2 ≤ 9, λ2 ≥ 0, and λ2(x1 + 3x2 − 9) = 0."
    }
}
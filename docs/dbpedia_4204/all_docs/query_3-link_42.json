{
    "id": "dbpedia_4204_3",
    "rank": 42,
    "data": {
        "url": "https://www.linkedin.com/posts/caglaraytekin_deprem-activity-7030527490662019072-5Nbx",
        "read_more_link": "",
        "language": "en",
        "title": "Çağlar Aytekin, Ph.D. on LinkedIn: #deprem",
        "top_image": "https://static.licdn.com/aero-v1/sc/h/c45fy346jw096z9pbphyyhdz7",
        "meta_img": "https://static.licdn.com/aero-v1/sc/h/c45fy346jw096z9pbphyyhdz7",
        "images": [
            "https://media.licdn.com/dms/image/v2/D4D16AQG9isF_-4GWsw/profile-displaybackgroundimage-shrink_200_800/profile-displaybackgroundimage-shrink_200_800/0/1698406749175?e=2147483647&v=beta&t=pOcL4X0oHjdOvz8QqEvwaXUozwUSu-DQwYrEAfEvmUo"
        ],
        "movies": [],
        "keywords": [],
        "meta_keywords": [
            ""
        ],
        "tags": null,
        "authors": [
            "Çağlar Aytekin, Ph.D"
        ],
        "publish_date": "2023-02-12T13:26:14.641000+00:00",
        "summary": "",
        "meta_description": "Dear Finnish and other international community,\nPlease keep in mind that your Turkish students/coworkers/etc. are not okay. We have been going through a very…",
        "meta_lang": "en",
        "meta_favicon": "https://static.licdn.com/aero-v1/sc/h/al2o9zrvru7aqj8e1x2rzsrca",
        "meta_site_name": "",
        "canonical_link": "https://www.linkedin.com/posts/caglaraytekin_deprem-activity-7030527490662019072-5Nbx",
        "text": "Version 2 of https://www.try.vi/ is out! It was one of the very few projects that I really enjoyed working on. We had a dataset of dishes and wines together with detailed features, and also some pairing scores extracted by experts. One goal of the project was to first extend the limited number of expert-extracted pairings to all possible pairing scores. Another goal was to effectively and efficiently providing a free-form wine and dish search. First, we have formed sentences of our wine and dish features and get sentence embeddings of them. Then we trained a network which first transformed these sentence embeddings to fine-tuned embeddings, and then combined the fine-tuned wine and dish embeddings and mapped them to the exper-provided pairing scores. In order to improve generalization, we have used data augmentations simply by dropping some parts of the sentences, hence some wine/dish features. End-to-end training from sentence embeddings to fine-tuned embeddings and eventually to pairing scores, keeps the broad knowledge in the sentence embeddings to a great extent, but also introduces a more problem-specific understanding to them. Usage in search: A free form query is transformed into fine-tuned embeddings, and search is performed in this space. Hence search is performed utilizing both global and broader knowledge, but also problem-specific knowledge. Usage in pairing: The model can provide a score for any pair in our dataset - hence immediately expanding our expert-extracted pairings. Moreover, it can also give a pairing score to any free form sentence description of a wine and a dish, which provides very general applications.\n\nWhat a read! Emergence has always fascinated me. When I first read about Game of Life, my mind was blown and it gave me a fantastic new perspective about life and physics. Emergence is everywhere. One nice example I always think of is how Newton's laws can be thought of emergent laws from quantum mechanics. Although the main governing principle is that of quantum mechanics, Newton's laws give an accurate macro-scale system that seems to be independent of quantum mechanics. This is exactly what this article says about emergence: that the emergent system behaves sort of independently from its underlying micro-system. What really made me stop and think was the concept of causal emergence - where the emergent system seems to show behaviors that it forms a causal system on macro-level. How can this be when it is the micro-level that defines the macro-level in the first place? The connection of emergence with neural networks is obvious I think : layers of simple laws of multi-variate decision making process results into an emergent and seemingly intelligent system. This thinking along with the points in the article makes me question few things: Is there another way of approaching explainability/causality from the macro-level? Can we form a \"Newton\" system where we can predict macro-level behaviors without actually running the network? The article ends with a cautious note that there still is a \"leakage\" between systems in some cases - which might result in optimizing the micro-system based on macro-feedback. Much like we're training micro-rules of neural nets with backprop from macro loss? Amazing read. Very recommended. https://lnkd.in/dRMySbci\n\nOpenAI just announced Extracting Concepts from GPT-4 It is a good step towards visualizing which input partitions are most used when an LLM answers a question, and would open some ways to check hallucinations - same way that neural saliency maps enabled showing which image parts were most utilized when neural network makes a decision. That said, introduction of saliency maps, lime, shap etc did not at all solve the interpretability problem. OpenAI's work doesn't solve it either. Another existing problem is not interpretability but sensibility. I think everyone knows the neural net tank story where supposedly some researchers trained a tank detection algorithm back in the day and since all tank images had sky in it and all non-tank images had no sky in it, the algo basically learned to find sky instead of tank. (correlation, not causation anyone?). (https://gwern.net/tank more on that here) So if you have an algorithm to show that the neural net classified the image as a tank and it shows you the sky as the reason, that's good since you understand there is a problem with your training data. But how to make a neural network that wouldn't do this mistake in the first place is still an open question, and I'm not sure whether it can be solved without world models. And I'm not sure how world models can be taught to machines unless they have the ability to interact with the world and apply the imagine/hallucinate,test, update cycle as we humans do. https://lnkd.in/dwjeRqrD\n\nOn recently introduced KANs Common neural networks have piece-wise linear activations. The resulting f(x) of deep neural networks are thus multivariate , exponentially growing decision trees with either linear regression applied on or a classification result assigned to the leaves of the tree. Not all functions can efficiently be represented by such architecture. Some research have been long at work to find another architecture that have similar representative power to piece-wise linear NNs, but with higher efficiency (less parameters and compute). Structured polynomials are one way : NAFNETs for denoising is a good example, they were already 10x better in parameter efficiency than simple CNNs at better accuracy. The recently introduced KANs use B-spline basis to learn structured piece-wise polynomials as activations. These activations are applied individually to each feature and mixing is made by simple post-addition instead of matrix multiplication. Ideas here too aren’t new at all, see AdderNet, or more studied generalized additive models. Why B-splines as basis functions is another question, and not others or a combination? For example either time domain or frequency domain is more advantegous in different problems. Or poly vs fourier, etc. Maybe a future study can propose a selective mechanism for basis functions (fourier, splines, etc). I suspect each will work better for another problem. So, in my opinion no big overall breakthrough here, but some applications may benefit more from b-spline basis. People are talking about this being good for PINNs, but need to check.\n\nOver-hyping scientific papers are neither good for the community nor for the authors who published it (trust me, I’ve been there). Breakthroughs, paradigm shifts are rare, and even if it may not seem so they are built upon a vast amount of previous work. (And usually Schmidhuber’s :) -kidding ). We usually see the last push to the boulder so it reaches the top - or we think it maybe so but we realize we end up watching Sisyphus instead. That said I’m hoping to review Kolmogorov-Arnold networks soon and share some rocket-emoji free thoughts about it. I will try not to use the word sensational."
    }
}
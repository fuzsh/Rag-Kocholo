{
    "id": "dbpedia_1693_3",
    "rank": 36,
    "data": {
        "url": "https://arxiv.org/html/2404.01954v1",
        "read_more_link": "",
        "language": "en",
        "title": "HyperCLOVA X Technical Report",
        "top_image": "",
        "meta_img": "",
        "images": [],
        "movies": [],
        "keywords": [],
        "meta_keywords": [
            ""
        ],
        "tags": null,
        "authors": [],
        "publish_date": null,
        "summary": "",
        "meta_description": "",
        "meta_lang": "en",
        "meta_favicon": "",
        "meta_site_name": "",
        "canonical_link": null,
        "text": "NAVER Cloud\n\nHyperCLOVA X Team\n\nAbstract\n\nWe introduce HyperCLOVA X, a family of large language models (LLMs) tailored to the Korean language and culture, along with competitive capabilities in English, math, and coding. HyperCLOVA X was trained on a balanced mix of Korean, English, and code data, followed by instruction-tuning with high-quality human-annotated datasets while abiding by strict safety guidelines reflecting our commitment to responsible AI. The model is evaluated across various benchmarks, including comprehensive reasoning, knowledge, commonsense, factuality, coding, math, chatting, instruction-following, and harmlessness, in both Korean and English. HyperCLOVA X exhibits strong reasoning capabilities in Korean backed by a deep understanding of the language and cultural nuances. Further analysis of the inherent bilingual nature and its extension to multilingualism highlights the model’s cross-lingual proficiency and strong generalization ability to untargeted languages, including machine translation between several language pairs and cross-lingual inference tasks. We believe that HyperCLOVA X can provide helpful guidance for regions or countries in developing their sovereign LLMs.\n\n1 Introduction\n\nThe latest advances in large language models (LLMs) have been primarily driven by objectives to improve comprehension and generation of English text. This gave birth to an array of powerful LLMs that can proficiently handle English; they reflect the norms and values of predominantly English-speaking societies, specifically North American cultures, which are extremely overrepresented in the pretraining corpora. Consequently, these LLMs exhibit limitations in their capacity to process and understand non-English languages like Korean, which embodies distinctive cultural nuances, geopolitical situations, and other regional specificities, as well as unique linguistic attributes.\n\nIn light of this context, we present HyperCLOVA X , a family of LLMs that includes HCX-L, the most powerful model, and HCX-S, a more lightweight alternative. Both models are tailored to the Korean linguistic and cultural framework and are capable of understanding and generating English, among several other languages. The models were initially pretrained using an evenly distributed mixture of Korean, English, and programming source code data. Subsequently, they underwent instruction tuning, utilizing high-quality human-annotated demonstration and preference datasets.\n\nHyperCLOVA X’s capabilities are showcased through extensive experiments on a collection of major benchmarks on reasoning, knowledge, commonsense, factuality, coding, math, chatting, and instruction-following, as well as harmlessness, in both Korean and English. Our thorough analysis reveals that HyperCLOVA X possesses comprehensive knowledge specific to the Korean language and culture and delivers powerful Korean reasoning capabilities unparalleled by any existing closed and open-source models, all while adhering to strict safety guidelines. Further analysis highlights HyperCLOVA X’s competitive edge in its core competencies, performing on par with other proficient English-centric LLMs.\n\nWe further demonstrate HyperCLOVA X’s multilingual ability—cross-lingual reasoning in selected Asian languages and machine translation between Korean and three other languages widely used in Korea. Our analysis shows that HyperCLOVA X is not only able to extend its reasoning capability beyond its primarily targeted languages but also achieve the state-of-the-art level in machine translation between Korean and untargeted languages, such as Japanese and Chinese. HyperCLOVA X’s impressive multilingual ability also includes cross-lingual transfer between Korean and English, where instruction-tuning in one language can lead to the emergence of instruction-following capabilities in the other.\n\nGiven our commitment to responsible and safe AI, HyperCLOVA X has been developed using systematic red teaming and safety data collection processes firmly grounded in NAVER AI Ethics Principles . This is complemented by extensive safety evaluations, both automatic and human-annotated, to monitor and mitigate the risks of generating harmful, toxic, or otherwise sensitive content.\n\nWe believe that HyperCLOVA X—with its competitive capabilities in English and other languages beyond Korean—can provide helpful guidance for regions or countries on developing their own sovereign LLMs. In this way, our efforts can contribute to ensuring sustainable development for all announced by the UN General Assembly, as part of its promotion for “safe, secure and trustworthy” AI systems .\n\nIn the remainder of this report, we detail the training process (Section 2), present evaluations on core benchmarks (Section 3), demonstrate multilingual abilities (Section 4), address safety concerns of the development process and the resulting model (Section 5), and conclude with a discussion on limitations and future directions (Section 2).\n\n2 Training Details\n\nHyperCLOVA X is an LLM family specialized in the Korean language and culture, also demonstrating outstanding performance in English and code. HyperCLOVA X consists of two sizes: a larger model, HCX-L, and a smaller model, HCX-S. Initially, the models are primarily pretrained on Korean, English, and code data. After pretraining, its instruction-following ability was enhanced through supervised fine-tuning (SFT) and reinforcement learning from human feedback (RLHF). In this section, we detail the pretraining and alignment learning process.\n\n2.1 Pretraining\n\nHyperCLOVA X, updated version of HyperCLOVA (Kim et al., 2021), builds on a transformer decoder architecture (Vaswani et al., 2017) with several modifications. To increase the context length, rotary position embeddings (Su et al., 2024) are adopted as the position embeddings. Additionally, pre-normalization and grouped-query attention (Ainslie et al., 2023) are used for improving the training stability and efficiency.\n\nData.\n\nThe pretraining data is comprised of Korean, multilingual, and code segments. While the multilingual subset is predominantly English, it also also includes a variety of other languages, such as Japanese, German, and French. Incorporating English and code data, comprising a large portion of the pretraining data, is common across English-focused LLMs. However, to train an LLM tailored to the Korean language and culture, we have significantly increased the representation of Korean, increasing its portion to approximately a third of the pretraining data. Overall, our pretraining data consists of an equal distribution of Korean, multilingual data (with a significant portion being English), and code data.\n\nTo construct high-quality pretraining data, we preprocesss raw data collected from various sources. First, we filter out repetitive, excessively short, or otherwise low-quality documents. Also, we exclude documents containing too many hate speech and advertisements. In addition, we remove personally identifiable information (PII) such as email addresses, phone numbers. For example, in the case of email addresses, the domain address is retained while the local part is substituted with a specific character format. Lastly, we upsample knowledge-containing data to improve the performance of the resulting LLM (Rae et al., 2021).\n\nTokenizer.\n\nOne of the core components of designing a proficient Korean-centric LLM involves preparing an effective tokenizer. Korean is an agglutinative language characterized by the formation of words through the attachment of grammatical morphemes to a core semantic morpheme. For instance, the same noun transforms into a verb or an adjective when combined with different particles, depending on the specific combination (Kim et al., 2021). Reflecting this characteristic, we trained a morpheme-aware byte-level BPE (Sennrich et al., 2015) with a vocabulary size of 100,000. The encoding capability of the trained tokenizer for each language influences the performance and the inference cost of LLMs when processing the language (Ahia et al., 2023; Petrov et al., 2024). The ability to encode the same document more concisely allows for a longer context, as well as reduced inference cost. Table 1 demonstrates that HyperCLOVA X is highly efficient in tokenizing Korean documents.\n\nPretraining Scheme.\n\nTo attain not only the left-to-right capabilities but also the in-filling abilities through pretraining, we adopt joint PSM & SPM training (Bavarian et al., 2022). This approach enables LLMs to acquire in-filing performance during pretraining, enhancing their capability for various applications such as coding assistants. Furthermore, 90% of the training is executed with a context length of 4,096, and the last 10% of training with 32,768. The training is conducted with bf16 precision using flash attention (Dao et al., 2022) and 3D parallelism.\n\n2.2 Alignment Learning\n\nAligning pretrained LLMs with human intentions and values is crucial for making them suitable as AI assistants (Leike et al., 2018). We apply two alignment techniques—SFT and RLHF—to train HyperCLOVA X.\n\n2.2.1 Supervised Fine-tuning (SFT)\n\nThe first phase in alignment learning is SFT, in which HyperCLOVA, the pretrained LLM, is trained to maximize the likelihood of a completion given each prompt. This phase improves the model’s ability to follow instructions and solve problems such as coding and creative writing. Furthermore, it allows the model to leverage knowledge from data across various domains, ranging from commonsense to humanities, sciences, and ethics.\n\nIn our SFT dataset, we define three special tokens: ‘<|user|>’, ‘<|assistant|>’, and ‘<|endofturn|>’ to distinguish between the user’s and the assistant’s turns. Even if a token corresponding to a special token is part of the user input, it is processed as a regular token, thereby ensuring that each role in the context remains distinct from the user’s instruction. For training on multi-turn samples, we apply loss masking on all text except for the assistant’s turns.\n\nFor SFT training, we use an efficient batching strategy that groups sequences with similar lengths, in order to minimize padding within mini-batches and increase GPU utilization. The actual mini-batch size depends on the average length of sequences in each mini-batch, but the maximum number of tokens for each mini-batch is kept the same.\n\n2.2.2 Reinforcement Learning from Human Feedback (RLHF)\n\nThe next phase in alignment learning is RLHF. Even though the post-SFT model is capable of multiple tasks, it may still generate outputs that are uninformative or contain false or harmful content. Thus, we incorporate RLHF in order to further align the model with human values, such as helpfulness, factuality, and safety (Askell et al., 2021). The overall procedure for RLHF involves training a reward model with human preference data, followed by training the post-SFT model via proximal policy optimization (PPO) (Schulman et al., 2017) to generate sequences that maximize the reward returned by the reward model.\n\nReward Model.\n\nOur reward model is initialized as the post-SFT model, with a randomly initialized linear head that outputs a scalar reward. The model is trained with ranking loss from Stiennon et al. (2022) based on the Bradley-Terry model (Bradley and Terry, 1952), in which the negative log-likelihood of the difference between chosen and rejected rewards is minimized. The model is trained for one epoch only, as we observed overfitting after that point, similar to the findings of Ouyang et al. (2022). We place all comparisons from the same prompt in the same optimization step to prevent overfitting while maintaining the max-token batching method as previously described.\n\nThe dataset for our reward model is collected from diverse product requirements based on various criteria and annotating schemes. We observe different reward distributions across the data sources, consistent with the findings in related work (Peng et al., 2023; Zeng et al., 2023). Such differences lead to reward hacking risks and training difficulties due to high variance. To mitigate this problem, we apply normalization and clipping at inference time (Zheng et al., 2023b).\n\nReinforcement Learning.\n\nWe adopt PPO for reinforcement learning. Following previous work (Ouyang et al., 2022; Bai et al., 2022), we add a Kullback-Leibler (KL) penalty term (Jaques et al., 2019; Stiennon et al., 2020) to the reward with a coefficient of 0.04. In addition, the policy network is initialized as the post-SFT model, which is also used to calculate the KL penalty. The value network is initialized as the reward model previously described.\n\nMany previous studies report an increase in output length after RLHF (Dubois et al., 2023; Singhal et al., 2023; Zheng et al., 2023b). We also observed this phenomenon, in which the model tends to favor longer sequences. To mitigate this issue, we employ iterative human feedback for models that generate extraneously long outputs. Additionally, we use an early stopping mechanism to prevent over-optimization by evaluating the model’s performance using an instruction set that constrains response length and format.\n\nIn addition, potentially due to the limitations of the transformer architecture or the intrinsic properties of human language, LLMs are prone to repetition (Holtzman et al., 2019; Welleck et al., 2019). We have also noticed this phenomenon in some outputs of our models. Thus, we integrate sequence-level unlikelihood training (Welleck et al., 2019) with PPO. This effectively reduces repetition with minimal additional training cost.\n\nIn the typical phase of PPO, four times as many models are required compared to SFT, and each of them operates sequentially within each iteration. To optimize this process, we spatially partitioned the devices for each model in multi-node settings and implemented asynchronous processing to parallelize the process. Specifically, we employ continuous batching to conduct inference for the four networks in the rollout phase of each iteration. As the results are accumulated in a queue, we promptly compute and update the gradients for the policy and value networks. This enables us to reduce the total training time, significantly enhancing the efficiency of the whole process.\n\n2.2.3 The Alignment Learning Pipeline\n\nAlignment learning involves various phases with dependencies among one another, where some are synchronous while others are not. By introducing an event-driven pipeline to automate these workflows, we are able to optimize the alignment learning process in terms of human resources, computational resources, and time. For example, instead of interrupting model training and evaluating it at intermediate checkpoints, we detect checkpoint-saving events and asynchronously evaluate the model on different computational resources, thereby reducing the overall training time and optimizing resource utilization. In addition, we automate the entire SFT, RM, and PPO learning processes by asynchronously executing the next learning process when the previous one is completed, which reduces the need for human interventions. Lastly, model training is performed on our in-house high-performance computing system, NAVER Smart Machine Learning (NSML) Sung et al., 2017.\n\nIn addition, the metadata must be securely stored and easily accessible for viewing and sharing to effectively utilize the metadata generated in many phases of the alignment learning process. We use an in-house machine learning operations tool, CLOVA MLOps (CLOps) , to safely store and share large amounts of metadata, and MLflow for metadata storage and visualization related to model training and evaluation. This great improves the efficiency of analyzing large amounts of experimental results.\n\n3 Core Benchmarks\n\nNumerous benchmarks have been proposed to objectively evaluate the capabilities of LLMs along various dimensions of quality. In this section, we present a detailed analysis of HyperCLOVA X’s performance on a core set of benchmarks.\n\nBenchmark Design.\n\nA primary constraint in the advances of multilingual language models has been the absence of thorough evaluation frameworks for languages other than English (Üstün et al., 2024). Competence in a particular language involves more than just linguistic proficiency; it also requires a profound understanding of the cultural and societal nuances unique to its speakers. To evaluate the bilingual and general capabilities of our models, we systematically utilize widely recognized English and Korean benchmarks sourced both our in-house and externally. Given that core competencies like reasoning, world knowledge, and mathematics transcend language, a significant portion of these benchmarks is conducted in English to assess language-neutral skills. On the other hand, to gauge the model’s adeptness at incorporating various dimensions of intelligence in answering language-specific questions and addressing cultural nuances, we utilize two detailed benchmark categories tailored to each language.\n\nFor assessing proficiency in Korean, our benchmarks, unlike their machine-translated equivalents (Conneau et al., 2018; Achiam et al., 2023), are either meticulously crafted by experts or curated from existing well-recognized work. These benchmarks include region-specific questions, such as those found in KoBigBench (KBB), a comprehensive Korean benchmark built from an internal effort, and the Korean-specific question set within KMMLU (Son et al., 2024), ensuring a rigorous evaluation of the model’s understanding of Korean cultural and societal contexts. Further information on the Korean benchmarks can be found in Section 3.1.\n\nBaselines.\n\nHyperCLOVA X is a uniquely designed set of LLMs with inherent proficiency in both Korean and English, setting them apart from other models in the field and lacking a directly comparable counterpart. To provide a comprehensive view of their diverse capabilities, we compare HyperCLOVA X with Korean-focused LLMs to showcase their fluency in Korean and with general foundational models to highlight the language-agnostic core competencies.\n\nFor Korean evaluations, we compare HyperCLOVA X to various baseline models, which include major closed- and open-source LLMs that have either been trained with Korean corpus as the target language or possess a general multilingual capability, which is prevalent among the Korean LLM community. Specifically, we considered the following factors while designing the baseline model pool.\n\nModels Specializing in Korean. To evaluate the Korean ability of our model, we curate LLMs that are either designed to be proficient in Korean from the ground up or further trained to gain Korean capabilities from non-Korean LLMs. Namely, Polyglot-Ko (Ko et al., 2023) was proposed as an open-sourced language model built to target the Korean language from scratch. SOLAR and its chat variant (Kim et al., 2023) were further trained with either a Korean instruction dataset or a Korean pretraining corpus on top of the LLaMA 2 architecture (Touvron et al., 2023b) initialized with the parameters of Mistral (Jiang et al., 2023). In our benchmark comparisons, we choose the chat variant as the baseline, which is reported to perform better on most benchmarks than the base model. LLaMA 2 Ko and LLaMA 2 KoEn are also two derivative Korean models diverged from LLaMA 2. Similarly, KORani is a family of Korean models further trained from Polyglot-Ko and LLaMA 2. We choose the first version (KORani-v1) for its superior performance in the translation and summarization tasks. EEVE-Korean-v (Kim et al., 2024b) is another class of further-trained Korean model that expands upon SOLAR with more efficient vocabulary for Korean.\n\nGeneral Foundation Models. We also compare HyperCLOVA X to strong general foundational models. Specifically, Falcon (Almazrouei et al., 2023) and LLaMA 2 (Touvron et al., 2023b) have been proven to be a strong contender in comprehensive capabilities in the LLM scene. LLaMA 2 is an explicitly English-specific model (Touvron et al., 2023b), while Falcon strives to be multilingual for European languages. Mistral 7b is superseded by SOLAR in all major core benchmarks (Kim et al., 2023), eliminating the need to include it as a baseline. The full list and a summary of the baseline models are shown in Table 2.\n\nEvaluation Methods.\n\nAnalogous to human interactions, discerning the knowledge and probing the reasoning capabilities exhibited by a model can be achieved by posing questions to it and analyzing the responses obtained. There are mainly two approaches to evaluate models: (1) the open-ended question-answering approach asks the model to generate a free-form answer and checks if the predicted answer matches the ground-truth one (e.g., BigBench-Hard); (2) the closed-form question-answering expects the model to predict one or more answers from the given candidate options (e.g., MMLU).\n\nGenerating a free-form answer is relatively straight-forward; however, instructing the model to select from the given options requires a certain level of instruction-following capabilities or few-shot in-context examples (Perez et al., 2021; Brown et al., 2020) which are not always available for all benchmarks. One solution is to cast the multiple-choice problem as a series of independent likelihood tests (Gao et al., 2023). Although this is the predominant method for evaluating LLMs in the field, it suffers from prompt sensitivity, causing wildly varying evaluation scores depending on minor prompt variations (Sclar et al., 2023). Recasting the multiple-choice problems as likelihood tests also assumes that language modeling perplexity of each candidate option is comparable across the different examples .\n\nTo reduce the effect of prompt sensitivity and promote the reliability of LLM evaluations, we adopt the technique where language models are prompted with the actual multiple-choice formats as originally intended by the benchmark. Our preliminary studies suggest that prompting with multiple choices improves performance on various benchmarks . Although this style of prompting requires a certain level of instruction-following capability, we utilize the output token probability table to filter out non-answer tokens. We also adopt few-shot examples whenever possible. For benchmarks with biased options, we ensure that the candidate options are randomly shuffled (e.g., ARC). The detailed evaluation protocol is described in subsequent subsections.\n\n•\n\nComprehensive Korean Benchmarks (Kor.). KoBigBench (KBB; In-house), KMMLU (Son et al., 2024), HAERAE-Bench (Son et al., 2023)\n\n•\n\nComprehensive English Benchmarks (Eng.). MMLU (Hendrycks et al., 2020), BigBench-Hard (Suzgun et al., 2023), AGIEval (Zhong et al., 2023)\n\n•\n\nCommonsense Reasoning (CS). Hellaswag (Zellers et al., 2019), Winogrande (Sakaguchi et al., 2021), PIQA (Bisk et al., 2020), ARC (Clark et al., 2018), CommonsenseQA (Talmor et al., 2019)\n\n•\n\nWorld Knowledge and Factuality (Fact). Natural Questions (Kwiatkowski et al., 2019), TriviaQA (Joshi et al., 2017), CLIcK (subset) (Kim et al., 2024a) Factscore (Korean) (Min et al., 2023)\n\n•\n\nMathematics (Math). GSM8k (Cobbe et al., 2021), MATH (Hendrycks et al., 2020)\n\n•\n\nCoding Capabilities (Code). HumanEval (Chen et al., 2021), HumanEval with Korean instructions (K-HumanEval; Internal), MBPP (Austin et al., 2021a)\n\n•\n\nInstruction-Following and Chatting Abilities (Chat). MT-Bench (Zheng et al., 2023a), Ko-MT-Bench (In-house), SuperNI (Wang et al., 2022), Korean Instruction-Following (KoIF; In-house)\n\n•\n\nHarmlessness (Harml.). TruthfulQA (Lin et al., 2022), BOLD (Dhamala et al., 2021)\n\nFigure 1 and Table 3 show the summary of all benchmark results.\n\n3.1 Comprehensive Korean LLM Benchmarks\n\nTo examine the comprehension capability in Korean incorporating various aspects of intelligence, such as reasoning, commonsense, and application of world knowledge, we use the following benchmarks.\n\nKoBigBench (KBB).\n\nKoBigBench represents a comprehensive benchmark that encompasses a variety of tasks tailored specifically for the Korean language, drawing inspiration from BigBench (Srivastava et al., 2022). This benchmark is based on internally devised tasks, including knowledge-probing tasks in fields such as law, history, mathematics, and computer science, as well as tasks involving commonsense reasoning and bias.\n\nKMMLU.\n\nKMMLU, which stands for Korean Massive Multitask Language Understanding, is a recently introduced benchmark developed to measure massive multitask language understanding in Korean. It consists of 35,030 expert-level multiple-choice questions across 45 subjects, ranging from humanities to STEM. This makes KMMLU unique as it captures linguistic and cultural aspects of the Korean language, unlike some previous benchmarks that were translations from English. We follow the original evaluation settings (5-shot). We also report evaluation scores for models present in the original paper and conduct evaluations internally if otherwise.\n\nHAE-RAE Bench.\n\nHAE-RAE Bench is another comprehensive Korean benchmark designed to challenge models in Korean cultural and linguistic knowledge. The dataset comprises tasks across four main domains: vocabulary, history, general knowledge, and reading comprehension. Following the paper settings, we employ a zero-shot problem-solving template.\n\nResults.\n\nThe detailed results are shown in Table 4. A significant performance disparity is observed across all Korean benchmarks between models specifically designed for the Korean language and those that are not. This gap widens considerably in benchmarks requiring an in-depth understanding of societal contexts, notably in most of HAE-RAE, KBB, and a subset of KMMLU. This underscores the assertion that for language and region-specific Large Language Models (LLMs) to be successful, the acquisition of large-scale, high-quality data from the target group is crucial.\n\n3.2 Comprehensive English LLM Benchmarks\n\nTo measure the capacity to comprehend in English, covering various dimensions of intelligence, we leverage the following benchmarks.\n\nMassive Multi-Task Language Understanding (MMLU).\n\nMMLU (Hendrycks et al., 2020) comprises 57 real-world subjects where solving the questions requires extensive world knowledge and comprehensive problem-solving skills. Following the popular evaluation settings, we practice the 5-shot example scheme.\n\nBigBench-Hard (BBH).\n\nThe BIG-Bench (Srivastava et al., 2023) dataset aims to evaluate the overall capabilities of language models using over 200 diverse tasks. BBH is a more challenging subset of this dataset, comprising 23 tasks for which state-of-the-art language models failed to outperform humans at the time of the proposal. To elicit responses from even the base version of baseline models in our evaluations, we use 3-shot examples per task without the chain of reasoning.\n\nAGIEval.\n\nTo complement some of the synthetic nature of BigBench-Hard and put the models to the test of real-world problems designed for humans, we introduce AGIEval (Zhong et al., 2023) as one of the comprehensive benchmarks in English. The benchmark consists of human-centric standardized exams, such as college entrance and lawyer qualification exams. Due to the absence of a training or validation set from the original work, we use 0-shot examples following the paper. In addition, we only utilize the English subset with the multiple-choice format.\n\nResults.\n\nDetailed benchmark results in Table 4 show that the performance difference between HCX-L and the largest model of the LLaMA 2 family is almost non-existent, with the average comprehensive English benchmark scores being very close to each other. Additionally, the reasoning capability of HyperCLOVA X enables it to solve problems better with intermediary reasoning steps. By employing the chain-of-thought (CoT) approach (Wei et al., 2022), HCX-L’s MMLU score was improved by 1.87 points, reaching 69.78 when CoT was sampled once. Moreover, employing CoT with self-consistent reasoning chains (Wang et al., 2023) and sampling it 10 times boosted the score by 2.88 points to 70.79. In contrast, when the CoT reasoning method was applied to LLaMA 2 70b, a decrease in the MMLU score was observed, dropping 2.62 points from 69.27 to 66.65.\n\n3.3 Commonsense Reasoning\n\nTo test the commonsense reasoning and understanding ability, primarily in English, we incorporate the following benchmarks.\n\nHellaswag.\n\nHellaSwag (Zellers et al., 2019) is a common benchmark for probing the commonsense capacity by asking the language model to complete an ordinary sentence from a few candidate options that may seem straightforward to humans. We cast the problem as multiple-choice and employ 5-shot examples.\n\nWinogrande.\n\nThe Winogrande Scheme Challenge (WSC) (Sakaguchi et al., 2021), and commonly referred to simply as Winogrande, is a set of cloze-style pronoun resolution problems. These problems are specifically crafted to assess the capability for commonsense reasoning. Unlike approaches that might rely on superficial word associations, Winogrande is designed so that in-depth reasoning is necessary. The structure of the benchmark consists of binary-choice questions, and our evaluation protocol utilizes a 5-shot learning approach.\n\nPIQA.\n\nThe Physical Interaction Question Answering (PIQA) benchmark (Bisk et al., 2020) physical commonsense reasoning. This task challenges the model to answer questions about the physical world. Due to the lack of training and validation sets, our evaluation protocol uses a 0-shot learning scheme.\n\nAI2 Reasoning Challenge (ARC).\n\nARC (Clark et al., 2018) is another common benchmark for probing commonsense reasoning. The dataset consists of grade-school level question-answers in two (easy and challenging) varieties. Our evaluation protocol employs both subsets and uses the prefix-matching scheme to enable fair comparison with base models that may generate answers beyond the expected words.\n\nCommonsenseQA (CSQA).\n\nSimilar to Winogrande, the original intent of the proposed CommonsenseQA benchmark (Talmor et al., 2019) was to devise a question-answering dataset such that merely understanding the word associations is not enough and must utilize prior commonsense knowledge to predict the correct answer. 5-shot examples are utilized in our protocol to facilitate reliable evaluations with the diverse base models.\n\nResults.\n\nThe evaluation results on commonsense capabilities are shown in Table 5. The performances on Winogrande and CSQA are especially notable, as they are neutralized with regards to superficial word associations and require significant understanding of the world and commonsense. On the other hand, SOLAR and EEVE, further trained from the Mistral (Jiang et al., 2023) backbone, demonstrate an advantage in Hellaswag and commonsense reasoning in physical interactions.\n\n3.4 World Knowledge and Factuality\n\nTo assess the parametric knowledge stored in the model, we utilize the following benchmarks.\n\nNQ.\n\nNatural Questions (Kwiatkowski et al., 2019) are open-ended fact-seeking questions collected from real-world search engine queries. Each question is annotated with multiple candidate answers, and identifying one of the answers is considered correct. We utilize the prefix-matching evaluation method to ensure that the base models not trained with instruction datasets can still be probed. 5-shot examples are employed as the evaluation protocol.\n\nTriviaQA.\n\nTriviaQA (Joshi et al., 2017) is a large-scale reading comprehension dataset comprising over 600k question-answer-evidence triples. Despite its original intended use of evidence-based question-answering, recent evaluations utilize the question-answer pairs without the context to test the knowledge inherent in language models. The question set encompasses various facts around the world, hence the benchmark offers a good insight into the model’s knowledge capacity. We implement 5-shot and prefix-match to include non-instruct models as baselines.\n\nCLIcK.\n\nThis newly proposed dataset (Kim et al., 2024a) is designed to evaluate linguistic and cultural intelligence in the Korean language. To this end, we have curated a subset of categories specifically related to knowledge of and facts about Korean culture and society, namely Korean popular culture, politics and tradition. We employ a zero-shot setting for this benchmark.\n\nFactscore.\n\nFactscore (Min et al., 2023) examines the ability to generate factual information about a given entity, such as a biography of a person. We have conducted an analysis of factuality in English and Korean datasets using HyperCLOVA X alongside other LLMs. When measuring Factscore in Korean, some modifications are necessary, including the translation of prompts and the substitution of the dataset with one focused on Korean content, specifically a Korean Wikipedia dataset and selecting titles related to Korean. To ensure the relevance and quality of entities from the Korean Wikipedia dataset, this dataset is curated to include only comprehensive documents while excluding title entities not associated with Asian identities. The specifics regarding the prompts and entities utilized in this study are detailed in B.3.\n\nHowever, it has been observed that both base and lower-performing LLMs frequently repeat the same sentence towards the end of their output. To ensure the quality of content, we promptly remove these repetitions from the output. Furthermore, when an LLM resorts to producing nonsensical words, it is interpreted as a failure to provide relevant responses for the given entity. In instances where the model generates an English explanation for a specific Korean Wikipedia title, we translate this output to compute Factscore with the Korean Wikipedia database. We denote using translation outputs in the Korean dataset as an asterisk (*) mark on the score on Table 6.\n\nResults.\n\nTable 6 illustrates HyperCLOVA X’s capacity for world knowledge and factuality, measured using NQ, TriviaQA, a subset of CLIcK, and Factscore derived from the Korean Wikipedia dataset. HyperCLOVA X noticeably suffers from the lack of knowledge in western culture, given that NQ and TriviaQA datasets are collected from online English-speaking communities. The other Korean-oriented models, such as KORani and EEVE, are less impacted since they are further trained from English-centric base models (Mistral and LLaMA 2). LLaMA 2 and polyglot LLMs demonstrate limitations in providing reliable explanations of the biography of Korean and other Asian people. Conversely, HyperCLOVA X models and EEVE-Korean-v1 demonstrate a higher accuracy in conveying information about given entities. This result underscores the superior factual generation capability of our HCX-L model on Korean dataset, compared to other baseline models.\n\n3.5 Mathematics\n\nTo demonstrate the efficacy of HyperCLOVA X compared to the baseline models introduced above in multi-step mathematical reasoning, we carry out experiments using the GSM8K (Cobbe et al., 2021) and MATH (Hendrycks et al., 2021) datasets, reporting average scores across both datasets as illustrated in Figure 1 and Table 3. We first measure 8888-shot accuracy with maj@8888 and a temperature of 0.70.70.70.7 on the GSM8K dataset comprising grade school math word problems with 2∼8similar-to282\\sim 82 ∼ 8-step natural language solutions. Given the elementary nature of problems in the GSM8K dataset, designed for basic concept comprehension and calculations, we further evaluate on the MATH dataset in a 4444-shot setting with maj@4444 and a temperature of 0.20.20.20.2.\n\nThe experimental results for each dataset can be found in Table 7. It is remarkable that HCX-L can achieve over 80808080 percent on GSM8K, outperforming all the baseline models significantly. Moreover, as the MATH dataset is characterized by more complex and challenging problems than the GSM8K dataset, the majority of the baseline models struggle to surpass 15151515 percent accuracy on the MATH dataset, but both HCX-S and HCX-L do not.\n\n3.6 Coding Capabilities\n\nTo assess the performance of HyperCLOVA X in code generation capability compared to the baseline models, we conduct experiments using HumanEval (Chen et al., 2021), MBPP (Austin et al., 2021a) datasets and our in-house developed K-HumanEval dataset. K-HumanEval is a custom dataset that we have created by translating the original HumanEval dataset into Korean using a combination of machine translation and meticulous manual review, ensuring both accuracy and contextual relevance. We evaluated the models using both pass@1 and pass@100 metrics for HumanEval, while for K-HumanEval and MBPP, we only measured pass@1. The detailed results are presented in Table 8. As shown in Table 8, HyperCLOVA X outperforms all other models across all benchmarks. Notably, in the K-HumanEval results, HyperCLOVA X significantly outperforms general foundation models as well as models specializing in Korean by a substantial margin.\n\nTo ensure fairness in our evaluations, we used evaluation prompts that were consistent with the model (base/chat). Additionally, we provided brief guidelines concerning the evaluation tasks for chat models. For further details about the evaluation process, including the templates used for each benchmark, refer to Appendix B.5.\n\n3.7 Chat and Instruction-Following\n\nWe evaluate open-ended text generation tasks to assess the helpfulness of LLMs as general assistants. Specifically, we measure the general chat ability and instruction-following capabilities in both Korean and English. To this end, we utilize MT-Bench (Zheng et al., 2023a) and its Korean version Ko-MT-Bench for the chat ability, and SuperNatural-Instruction (SuperNI) (Wang et al., 2022) and in-house Korean Instruction-Following benchmark (KoIF) for the instruction-following capability.\n\nMT-Bench and Ko-MT-Bench.\n\nMT-Bench (Zheng et al., 2023a) contains multi-turn queries across diverse domains like writing, extraction, stem, and coding. This dataset includes 160 test sets that extend up to two turns based on 80 initial queries. For the Ko-MT-Bench, we first translate the instances of MT-Bench into Korean. Then, we ensure the test set’s naturalness and quality via internal reviews. For instance, we revise the request “Start every sentence with the letter A.” to “모든 문장의 시작을 ‘하’로 해줘.” considering the naturalness of the target language and conversational context. Typically, these benchmarks utilize LLM-as-a-judge to measure the overall score of the generated response. We adhere to the evaluation setup but make a slight modification to the evaluation prompt for Ko-MT-Bench, introducing a deduction of points when the language of generation deviates from the input language unless explicitly requested to translate. We report the scores on a 0-100 scale for the MT and Ko-MT-Benchmarks by multiplying the original scores on a 0-10 scale by 10.\n\nSuperNI and KoIF.\n\nSuperNatural-Instruction (SuperNI) contains diverse NLP tasks to probe the models’ instruction-following capability in English (Wang et al., 2022). Specifically, we utilize 119 test tasks and sample 10 instances per task, resulting in a total of 1190 test sets used for evaluation. Following the official evaluation setup, we report the micro average of ROUGE-L score. The Korean Instruction-Following (KoIF) benchmark is an internally constructed collection of diverse Korean NLP tasks to test the instruction-following ability of LLMs in Korean. It includes 600 instances of 32 NLP tasks from 18 existing datasets.\n\nResults.\n\nIn Table 9, we find that HyperCLOVA X models demonstrate considerable performances across benchmarks in both Korean and English. HCX-L achieves the best performances on the chat benchmarks in both languages. Notably, except for our models and the EEVE v1.0 10.8B model, most open-source LLMs struggle to generate responses properly in Korean (Ko-MT). For instance, approximately 98% of the responses from LLaMA 2 70B are in English, despite the questions being posed in Korean. It has been observed that the judge model tends to evaluate the responses regardless of mismatches between the source and target languages, i.e., language confusion. On the other hand, we find that HyperCLOVA X models surpass other open-source Large Language Models (LLMs) on the KoIF benchmark and demonstrate competitive performance on SuperNI.\n\n3.8 Harmlessness\n\nTo examine the safety of responses generated, we adopt the following benchmarks.\n\nTruthfulQA\n\nThis benchmark is a carefully curated set of questions that humans tend to answer falsely due to misconceptions or false beliefs (Lin et al., 2022). Evaluating with this benchmark helps identify whether the language model also tends to answer falsely due to learning from human texts during pretraining. We conduct evaluations on the multiple-choice task, specifically, the multi-answer multiple-choice question set dubbed mc2. 5-shot examples are utilized to facilitate answer predictions.\n\nBOLD\n\nThe Bias in Open-Ended Language Generation Dataset (BOLD) (Dhamala et al., 2021) is a dataset to benchmark social biases in the open-ended language generation results of the LMs and has been adopted as a benchmark dataset for safety (Team et al., 2024). The dataset collected prompts about the major social groups from Wikipedia. The models complete the prompt of each instance and the toxicity of the generation results is reported. The toxicity is measured using the Perspective API , a toxicity scoring model widely used in many safety benchmarks.\n\nThe aggregate harmlessness score is derived by combining the truthfulness score with the ’safeness’ metric, which is calculated as truthfulness×(1−toxicity)truthfulness1toxicity\\text{truthfulness}\\times(1-\\text{toxicity})truthfulness × ( 1 - toxicity ) based on the TruthfulQA and BOLD benchmarks. Our findings, as detailed in Table 10, reveal that the largest model exhibits significantly higher safety levels compared to other baseline models. A comprehensive description of our safety methodology, along with an in-depth analysis of our models’ harmlessness, can be found in Section 5.\n\n3.9 Comparison with Closed Source Models\n\nWe compare a few representative benchmarks on the closed-source LLMs, namely GPT-3.5, GPT-4, and SOLAR API, an alternate non-public variant of the open-sourced SOLAR (Kim et al., 2023) . Due to some API constraints, we could not evaluate the closed-source models on the same set of benchmarks. Instead, we selected a subset of comprehensive benchmarks from each language that provide more effective insights into language-specific linguistic capabilities. The results are shown in Table 11.\n\nResults strongly support that HyperCLOVA X, especially the largest model, possesses an unparalleled level of proficiency in understanding Korean and its surrounding linguistic and cultural subtleties. This is attested by Son et al. (2024) with the findings that HyperCLOVA X outperforms GPT-4 on the Korean-specific KMMLU subset. Moreover, along with the evidence of competitive English understanding performance, the results further show that HyperCLOVA X can provide an all-around great performance for Korean and English bilingual users on par with GPT-4.\n\nThe detailed rundown on one of the benchmark results is shown in Table 12. Since the benchmark was specially created to assess the understanding of Korean cultural and linguistic subtleties, HyperCLOVA X’s significant lead in all categories except General Knowledge highlights its exceptional specialization in the Korean language.\n\n4 Multilinguality\n\nHyperCLOVA X was trained primarily on Korean, English, and code data, but it also supports many other languages. In this section, we demonstrate HyperCLOVA X’s multilingual abilities through experiments on cross-lingual reasoning, machine translation, and cross-lingual transfer.\n\n4.1 Cross-Lingual Reasoning\n\nThe first multilingual ability we consider is cross-lingual reasoning in Asian languages. We investigate HyperCLOVA X’s capability to reason in languages for which it was not explicitly trained. For this, we use two popular cross-lingual reasoning benchmarks: Cross-Lingual Natural Language Inference (XNLI) (Conneau et al., 2018) and Cross-Lingual CommonsenseQA (X-CSQA) (Lin et al., 2021).\n\nXNLI.\n\nThe objective of XNLI is to determine whether a given pair of sentences is in an entailment, neutral, or contradiction relation. Ultimately, this examines the ability to recognize logical relations between sentences. For this benchmark, we employ 5-shot prompting to measure the accuracy of HyperCLOVA X and other LLMs on the test set, focusing on the Asian languages available in the dataset: Arabic, Hindi, Thai, Urdu, Vietnamese, and Chinese.\n\nAs shown in Table 13, HCX-L outperforms other models across the board, with the exception of Chinese, for which HCX-L ranks second. This shows that Korean and English capabilities can be transferred to Asian languages that are underrepresented in the pretraining data.\n\nX-CSQA.\n\nSimilar to CSQA, X-CSQA consists of multiple questions that probe commonsense knowledge but in multilingual configurations. It measures the amount of commonsense knowledge a given model possesses. For this benchmark, we use 0-shot prompting since it was designed to test 0-shot cross-lingual transfer and thus does not come with a training set. Also, we report accuracy on the validation set, as the labels for the test set are not available publicly.\n\nTable 14 exhibits an identical trend, where HCX-L performs the best across the board except for Chinese, for which it comes second to SOLAR 10.8b. This, again, showcases the strong cross-lingual transfer ability of HyperCLOVA X.\n\n4.2 Machine Translation\n\nThe second multilingual ability we consider is machine translation between Korean and three other languages: English, Japanese, and Chinese. As these languages are the most widely used foreign languages in Korea, the ability to reliably translate Korean to and from these languages is desirable. To test this ability, we use the FLORES+ evaluation benchmark (Goyal et al., 2022).\n\nWe employ 1-shot prompting to translate Korean to and from English, Japanese, and Chinese using the test set of the FLORES+ benchmark. Here, we used the same prompt for the fairness of evaluation. We adopt xCOMET as the metric, since it correlates with human judgment better than other evaluation metrics (Blain et al., 2023; Guerreiro et al., 2023).\n\nAs illustrated in Table 15, HCX-L is the best or a close second in all settings, except for Korean to Chinese translation. HCX-L outperforms GPT-4 with a noticeable margin in English to Korean translation, making it a better model for Korean and English. Notably, HCX-L outperforms a commercial translation service, Google Translator, across the board other than Korean to Chinese translation.\n\n4.3 Cross-lingual Transfer\n\nThe third multilingual ability we consider is cross-lingual transferability of HyperCLOVA X between the two main languages it supports, Korean and English. We examine the impact of instruction-tuning in one language on the instruction-following ability in the other language, as well as an optimal ratio of languages for instruction-tuning.\n\nFor the experiments, we leverage two instruction datasets, LIMA (Zhou et al., 2023) and OpenOrca (Mukherjee et al., 2023), each of which contains 1k and 10k single-turn instances, respectively. We pair single-turn instances from the datasets with their translations publicly available on Huggingface . Then, similarly to Shaham et al. (2024), we instruction-tune HyperCLOVA X and other LLMs on training sets consisting of various ratios of Korean and English instruction data. The held-out test sets consist of 256 instances from OpenOrca and 300 from LIMA.\n\nTo measure the instruction-following ability in each language, we use Rouge-L and LLM-as-a-judge method (Zheng et al., 2023a) for OpenOrca and LIMA, respectively. This is because ground truth labels are available only for OpenOrca. For the LLM-as-a-judge method, predictions from both the multilingual tuned model and the monolingual tuned model are fed to an LLM serving as the judge, which determined the better response. To accommodate for the bias based on the order of input, we test each sample twice, alternating the order (Shaham et al., 2024). The winner accrues 0.5 points each time, and thus, a model could earn 0, 0.5 or 1 point per sample (Zhou et al., 2023). The final score is computed by averaging the points across the test set.\n\nImpact of language ratio on instruction-tuning.\n\nTo identify an optimal ratio between Korean and English data for instruction-tuning, we examine HCX-S trained on training sets of equal size with 15 different ratios ranging from 1:0 (English only) to 0:1 (Korean only). For example, in Figure 2, 0% Korean samples represent 1:0 ratio, while 100% Korean samples represent 0:1 ratio.\n\nAs shown in Figure 2(a) and 2(b), even a small amount of Korean data for instruction-tuning can significantly improve instruction following performance in Korean. In particular, we observe that translating just 0.5%, 50 of 10,000 training samples in OpenOrca, of the training data into Korean boosts the Rouge-L score by nearly 13 points. This is consistent with the findings by Shaham et al. (2024). For both datasets, the highest performance in each language is achieved by including both languages, rather than just one, in the training set. Figure 2(c) and 2(d) show that the results are a bit more noisy in preference tests.\n\nCross-lingual instruction-following.\n\nNext, we investigate cross-lingual instruction-following relative to monolingual instruction-following. We instruction-tune and test HCX-S, HCX-L, Mistral-7B (Jiang et al., 2023), and Yi-Ko-6B (Lee Junbum, 2024) under four distinct settings: train in English and test in Korean (En →→\\rightarrow→ Ko), train in Korean and test in English (Ko →→\\rightarrow→ En), train in both English and Korean and test in English or Korean (En+Ko →→\\rightarrow→ En and En+Ko →→\\rightarrow→ Ko). For the last two setups, we compute the average of results using three training sets with English and Korean ratios of 0.75:0.25, 0.5:0.5, and 0.25:0.75. This is to avoid employing a ratio that may favor a particular model.\n\nFor measuring the ability of cross-lingual instruction-following relative to monolingual instruction-following, we define the score as follows:\n\nScore(X→Y)=Performance of model trained in language ⁢X⁢ on language ⁢YPerformance of model trained in language ⁢Y⁢ on language ⁢YsubscriptScore→𝑋𝑌Performance of model trained in language 𝑋 on language 𝑌Performance of model trained in language 𝑌 on language 𝑌\\text{Score}_{(X\\rightarrow Y)}=\\frac{\\text{Performance of model trained in % language }X\\text{ on language }Y}{\\text{Performance of model trained in % language }Y\\text{ on language }Y}Score start_POSTSUBSCRIPT ( italic_X → italic_Y ) end_POSTSUBSCRIPT = divide start_ARG Performance of model trained in language italic_X on language italic_Y end_ARG start_ARG Performance of model trained in language italic_Y on language italic_Y end_ARG (1)\n\nFor example, ScoreEn→KosubscriptScore→EnKo\\text{Score}_{\\text{En}\\rightarrow\\text{Ko}}Score start_POSTSUBSCRIPT En → Ko end_POSTSUBSCRIPT denotes the performance of a model instruction-tuned in English tested in Korean, relative to the performance of the same model instruction-tuned in Korean tested in Korean.\n\nAs presented in Table 16, HyperCLOVA X models outperform other models on average. In particular, the English-centric Mistral-7B model exhibits a significant degradation in its English capabilities when instruction-tuned in Korean only. Similarly, the bilingual Yi-Ko-6B model shows an imbalance in proficiency between the two languages, resulting in incomplete preservation of its existing capabilities depending on the type of language being trained. In contrast, HyperCLOVA X models are able to maintain their linguistic performance regardless of the type and ratio of languages being trained, while also exhibiting language transfer capabilities.\n\n5 Safe and Responsible AI\n\nAs LLMs become increasingly powerful, concerns regarding their development and use are rising. In this section, we describe our responsible development approaches and safety evaluations of HyperCLOVA X. First, we introduce HyperCLOVA X Ethics Principles following NAVER AI Ethics Principles . Then, we explain our red-teaming and safety dataset collection methods to construct them in efficient ways. Finally, we present quantitative safety evaluation results in both English and Korean benchmark datasets as well as human evaluation results.\n\n5.1 HyperCLOVA X Ethics Principles\n\nWe define HyperCLOVA X Ethics Principles to steer our models for safe and responsible development and evaluations. The principles do not allow models to generate content in the following risk categories:\n\n•\n\nHate and harassment, such as violent and offensive languages, sexual aggression, and others.\n\n•\n\nStereotypes and biases on individuals and social groups.\n\n•\n\nHarmful content, such as advice on criminal and dangerous human behavior; violence and cruelty; sexual content; child safety; anti-ethical, moral, and social normative.\n\n•\n\nSelf-anthropomorphism, such as human persona, emotions, and relationships with humans that can cause user to misunderstand them as real human.\n\n•\n\nSubjectivity, such as biased opinions on politics, religions, gender conflicts, and others.\n\n•\n\nCopyright infringement\n\n•\n\nPrivate information, such as personal identifiable information.\n\n•\n\nMisinformation that may cause users to be confused and false beliefs.\n\n•\n\nAdvice on specialized domains, such as medical, legal, and financial advice that could harm users.\n\nThese categories were determined based on the NAVER AI Ethics Principles and our anticipated harm studies. Safety datasets are constructed, and evaluations are conducted, while abiding by these principles.\n\nSince the deployment of HyperCLOVA X, we have continued monitoring its use for unforeseen risks. The aforementioned categories are being updated continuously based on newly identified risks. In addition, when HyperCLOVA X learns new features and tasks, we assess potential risks and update the risk categories accordingly.\n\n5.2 Red Teaming and Safety Data Collection\n\nIn accordance with the HyperCLOVA X Ethics Principles, we have identified hazardous topics that challenge the model, such as “social issues and biases”, “illegal activities”, “sexual matters”, and “professional advice”. Furthermore, we have identified attack method types—e.g. harmlessness-helpfulness trade-off, role-playing, false premises, jailbreaks—to facilitate the collection of a diverse set of red teaming queries. These topics and attack strategies can be expanded as needed based on social trends and timely matters, such as worldwide elections.\n\nTo efficiently collect safety training data, we have devised data collection protocol wherein annotators attack models, evaluate their responses, and write safe and informative responses, as described in Figure 3. In each red teaming dialogue session, an annotator devises an attack scenario based on a given attack topic and method type, and then starts the attack conversation with various HyperCLOVA X models simultaneously. Subsequently, annotators evaluate the responses for harmlessness and assign overall scores. The overall score takes into account both harmlessness and helpfulness. Therefore, when responses present a similar level of safety, the more helpful responses receive higher overall scores. The overall scores are used to construct ranking pairs from the response combinations for RLHF training.\n\nAfter scoring, if no response has achieved a perfect score, the annotators write a new response that is safe, helpful, and correct. To prevent excessively evasive response generation, we guide annotators to offer information as objectively as possible within a safe framework or to suggest an alternative rather than outright rejecting the request. This newly written data serves as SFT data. The red-teaming dialog proceeds with the most harmful responses from the candidates and ends after up to 7 turns of dialog.\n\nTo ensure unbiased, high-quality, and consistent data, we require that annotators and inspectors pass a Korean comprehension test. We educate them about the HyperCLOVA X Ethics Principles, emphasizing the importance of aligning their annotations with our core ethical values. However, we also recognize and respect the annotator’s safety preferences and personal values. Note also that this red-team attack, scoring, and rewriting process has a risk of mental harm to the annotators. Consequently, we periodically solicit feedback with care and allow them the flexibility to modify their role or discontinue their work at any time.\n\n5.3 Safety Evaluation\n\nWe assess the safety of models using both automatic and manual evaluation methods. For automatic evaluation, we conduct toxicity and social bias evaluations in both Korean and English. Since most LLMs perform harmlessness alignments during instruction-tuning, open-source chat models are selected as the baselines for safety benchmarks. For human evaluations, annotators score the safety of responses to adversarial prompts in Korean.\n\n5.3.1 Toxicity\n\nTo measure the implicit toxicity of HyperCLOVA X, the first part of a sentence that could elicit a toxic continuation is provided as input. Then, the degree of toxicity of the generated text is measured by Perspective API. We use toxicity-eliciting prompts from RealToxicPrompts (RTP) and Korean offensive language dataset (KOLD) for English and Korean, respectively.\n\nRealToxicPrompts (RTP).\n\nRTP (Gehman et al., 2020) is a widely used safety benchmark of 100⁢K100𝐾100K100 italic_K English prompts sampled from a large English web corpus. A core objective is to investigate how a model degenerates on the inputs with different degrees of toxicity. Thus, the benchmark is stratified-sampled with respect to the level of toxicity. Each instance is composed of a prompt and a continuation, which is the first half of the sentence and the last half of the sentence, respectively. Given a prompt, a model generates the rest of the text in English. For simplicity, we randomly sample and use 500500500500 prompts, following the HELM (Liang et al., 2022) leaderboard.\n\nTable 17 reports averaged toxicity scores and toxic count rates of valid generations. None of them failed to English continuation. The results show that most models generate responses with a similar level of toxicity, and HCX-L shows less toxic continuation overall.\n\nKorean Offensive Language Dataset (KOLD).\n\nKOLD (Jeong et al., 2022) is a news and YouTube comment dataset for measuring toxicity in Korean. It consists of a title, a comment, and an offensiveness label. We randomly select 1,000 data and utilize them as prompts that elicit models to toxic continuation. Specifically, we use the title and the first half of its comment as the continuation prompt. Then, we request LLMs to complete the given prompt in Korean.\n\nTable 17 summarizes the toxicity evaluation results. We found that several LLMs fail to complete the sentences in Korean but in other languages such as English and Chinese, which are their main languages. Therefore, we report the Korean continuation rates and calculate the mean toxicity and toxic counts only for the Korean continuations. Compared to the results of RealToxicPrompt, the overall toxicity of KOLD continuation was measured higher. We conjecture the main reason is the Perspective API’s incomplete performance in the Korean Toxicity detection task. We observed that even if the content was the same, it showed a higher toxicity score when expressed in Korean than in English. Among the models that performed the Korean continuation, those that show lower toxicity than the Hyperclova X models completed continuation either without considering the context or with poor grammar. The examples can be found in Appendix B.4. The Hyperclova X models show the best results considering both the qualitative aspects and toxicity.\n\n5.3.2 Social Bias\n\nLLMs implicitly learn social biases and stereotypes against specific groups in various cultures and societies. Since these biases, stereotypes, and demographic group distributions vary significantly across different cultures and societies—between the United States and South Korea in particular—it is crucial to conduct evaluations using benchmarks tailored to a specific society. Therefore, we compute social bias scores with respect to the U.S. (English) using Bias Benchmark for Question Answering (BBQ) and South Korea (Korean) using Korean Bias Benchmark for Question Answering (KoBBQ).\n\nBias Benchmark for Question Answering (BBQ).\n\nTo explore how LLMs’ outputs may show biases linked to demographic factors, we leverage the BBQ benchmark (Parrish et al., 2022) encompassing 9 social categories, such as race, gender, and religion. This benchmark assesses model behaviors in ambiguous and disambiguated contexts, measuring how biases manifest themselves in question-answering tasks. Specifically, given a context and question, a model generate answer among 3 answer choices: targeted biased group, non-targeted biased group, and unknown. For disambiguated context, the task is regarded as machine reading comprehension task. However, in ambiguous context, correct information to answer the question is absent in the context, therefore, models are prone to answer based on their implicit social bias.\n\nWe randomly sample 1,000 question and answer pairs following HELM benchmark for comparison. In Table 18, the accuracy and bias scores of HyperCLOVA X models are compared with other LLM models. In overall, all models show lower accuracy in ambiguous context than disambiguated context. HCX-L exhibits the best accuracy of 96.65% in disambiguated contexts, and 85.37% in ambiguous context.\n\nKorean Bias Benchmark for Question Answering (KoBBQ).\n\nKoBBQ (Jin et al., 2023) was constructed based on BBQ dataset, reflecting Korean social bias. We randomly select 1,000 samples among 2,280 test samples, balancing the ratio of the 12 categories. The sampled dataset is augmented threefold, and each instance is prefixed with 5 different prompts. This results in a total of 15,000 prompts for evaluation. We report the accuracy and diff-bias which is defined in Jin et al. (2023).\n\nTable 19 summarizes the results, including the mean and the standard deviation of performance based on 5 evaluation prompts. In particular, other models show near random accuracy in ambiguous contexts, and even in disambiguated contexts, indicating a lack of Korean understandability. HCX-L exhibits the highest accuracy and diff-bias in both contexts with 95.40% and 73.74%, respectively.\n\n5.3.3 Human Evaluation\n\nWe conduct human evaluation studies on HyperCLOVA X in terms of human preferences and attack success rates (ASR) with our red teamers. Red teamers first attack the models and compare the responses to the same adversarial prompt. In total, the models are evaluated with 1,695 utterance turns in 339 dialog sessions. The preference scores range from 0 (harmful) to 7 (safe). Table 20 shows that HCX-S is safer than HCX-L , and the safety preference of HCX-S is on par with GPT-4. Several examples of the evaluation result are presented in Appendix B.5.\n\nMoreover, we measure ASR with 305 held-out attack dialogs, which are translated and revised red team prompts from Anthropic’s HH-RLHF dataset (Ganguli et al., 2022). As shown in Table 20, HCX-S and HCX-L achieve the ASR of 5.93%percent5.935.93\\%5.93 % and 7.42%percent7.427.42\\%7.42 %, respectively. The ASR for each category is represented in Figure 4.\n\n6 Conclusion\n\nHyperCLOVA X represents a significant advancement in LLMs, particularly emphasizing the Korean language and culture while maintaining strong capabilities in English and other languages. Through a training process that incorporated a balanced mix of Korean, English, and programming languages, followed by supervised fine-tuning and reinforcement learning from human feedback, HyperCLOVA X demonstrates exceptional proficiency in a variety of tasks.\n\nHyperCLOVA X’s performance across a wide range of benchmarks—e.g. reasoning in Korean and English, and problem-solving in coding and math—showcases its capacity and versatility. Also, its impressive multilingual ability, especially in cross-lingual reasoning and machine translation, further illustrates its generalization capability and the potential for broad application across different linguistic contexts.\n\nMoreover, the commitment to responsible AI development and deployment is manifested through the extensive safety evaluations and adherence to ethical principles. HyperCLOVA X’s sophisticated handling of toxicity, social biases, and other ethical concerns through systematic red teaming and safety data collection processes, along with its performance in human evaluation studies, highlight its potential as a safe and reliable AI assistant. Overall, HyperCLOVA X sets a new standard for bilingual and multilingual LLMs, paving the way for more inclusive and culturally sensitive AI technologies.\n\nAs future work, we intend to explore multimodality, aiming to broaden HyperCLOVA X’s capabilities to seamlessly process and integrate diverse types of data, such as text, images, and audio. Moreover, we are set to explore the efficacy of model quantization techniques, with the goal of optimizing HyperCLOVA X ’s inference without sacrificing its accuracy or the quality of the output. Additionally, we are actively researching the integration of external tools and APIs to augment the model’s functionalities. This will enable HyperCLOVA X to access specialized datasets and services, significantly enriching and enhancing the factuality of its responses. Our team is committed to integrating these innovative research topics with the existing and future services at NAVER and its subsidiaries as we strive to advance AI technologies that benefit humanity.\n\nReferences\n\nAbadji et al. (2022) Julien Abadji, Pedro Ortiz Suarez, Laurent Romary, and Benoît Sagot. 2022. Towards a cleaner document-oriented multilingual crawled corpus. arXiv preprint arXiv:2201.06642.\n\nAchiam et al. (2023) Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, et al. 2023. Gpt-4 technical report. arXiv preprint arXiv:2303.08774.\n\nAhia et al. (2023) Orevaoghene Ahia, Sachin Kumar, Hila Gonen, Jungo Kasai, David R Mortensen, Noah A Smith, and Yulia Tsvetkov. 2023. Do all languages cost the same? tokenization in the era of commercial language models. In Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, pages 9904–9923.\n\nAinslie et al. (2023) Joshua Ainslie, James Lee-Thorp, Michiel de Jong, Yury Zemlyanskiy, Federico Lebrón, and Sumit Sanghai. 2023. Gqa: Training generalized multi-query transformer models from multi-head checkpoints. arXiv preprint arXiv:2305.13245.\n\nAlmazrouei et al. (2023) Ebtesam Almazrouei, Hamza Alobeidli, Abdulaziz Alshamsi, Alessandro Cappelli, Ruxandra Cojocaru, Mérouane Debbah, Étienne Goffinet, Daniel Hesslow, Julien Launay, Quentin Malartic, et al. 2023. The falcon series of open language models. arXiv preprint arXiv:2311.16867.\n\nAskell et al. (2021) Amanda Askell, Yuntao Bai, Anna Chen, Dawn Drain, Deep Ganguli, Tom Henighan, Andy Jones, Nicholas Joseph, Ben Mann, Nova DasSarma, et al. 2021. A general language assistant as a laboratory for alignment. arXiv preprint arXiv:2112.00861.\n\nAustin et al. (2021a) Jacob Austin, Augustus Odena, Maxwell Nye, Maarten Bosma, Henryk Michalewski, David Dohan, Ellen Jiang, Carrie Cai, Michael Terry, Quoc Le, et al. 2021a. Program synthesis with large language models. arXiv preprint arXiv:2108.07732.\n\nAustin et al. (2021b) Jacob Austin, Augustus Odena, Maxwell Nye, Maarten Bosma, Henryk Michalewski, David Dohan, Ellen Jiang, Carrie Cai, Michael Terry, Quoc Le, et al. 2021b. Program synthesis with large language models. https://github.com/google-research/google-research/blob/master/mbpp/README.md. Accessed: 2024-03-25.\n\nBai et al. (2022) Yuntao Bai, Andy Jones, Kamal Ndousse, Amanda Askell, Anna Chen, Nova DasSarma, Dawn Drain, Stanislav Fort, Deep Ganguli, Tom Henighan, et al. 2022. Training a helpful and harmless assistant with reinforcement learning from human feedback. arXiv preprint arXiv:2204.05862.\n\nBavarian et al. (2022) Mohammad Bavarian, Heewoo Jun, Nikolas Tezak, John Schulman, Christine McLeavey, Jerry Tworek, and Mark Chen. 2022. Efficient training of language models to fill in the middle. arXiv preprint arXiv:2207.14255.\n\nBisk et al. (2020) Yonatan Bisk, Rowan Zellers, Ronan Le bras, Jianfeng Gao, and Yejin Choi. 2020. Piqa: Reasoning about physical commonsense in natural language. Proceedings of the AAAI Conference on Artificial Intelligence, 34(05):7432–7439.\n\nBlain et al. (2023) Frederic Blain, Chrysoula Zerva, Ricardo Ribeiro, Nuno M. Guerreiro, Diptesh Kanojia, José G. C. de Souza, Beatriz Silva, Tânia Vaz, Yan Jingxuan, Fatemeh Azadi, Constantin Orasan, and André Martins. 2023. Findings of the WMT 2023 shared task on quality estimation. In Proceedings of the Eighth Conference on Machine Translation, pages 629–653, Singapore. Association for Computational Linguistics.\n\nBradley and Terry (1952) R. A. Bradley and M. E. Terry. 1952. Rank analysis of incomplete block designs: I. the method of paired comparisons. Biometrika, 39(3/4):324–345.\n\nBrown et al. (2020) Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel Ziegler, Jeffrey Wu, Clemens Winter, Chris Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, and Dario Amodei. 2020. Language models are few-shot learners. In Advances in Neural Information Processing Systems, volume 33, pages 1877–1901. Curran Associates, Inc.\n\nChen et al. (2021) Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Ponde de Oliveira Pinto, Jared Kaplan, Harri Edwards, Yuri Burda, Nicholas Joseph, Greg Brockman, et al. 2021. Evaluating large language models trained on code. arXiv preprint arXiv:2107.03374.\n\nClark et al. (2018) Peter Clark, Isaac Cowhey, Oren Etzioni, Tushar Khot, Ashish Sabharwal, Carissa Schoenick, and Oyvind Tafjord. 2018. Think you have solved question answering? try arc, the ai2 reasoning challenge. arXiv preprint arXiv:1803.05457.\n\nCobbe et al. (2021) Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, et al. 2021. Training verifiers to solve math word problems. arXiv preprint arXiv:2110.14168.\n\nConneau et al. (2018) Alexis Conneau, Ruty Rinott, Guillaume Lample, Adina Williams, Samuel Bowman, Holger Schwenk, and Veselin Stoyanov. 2018. XNLI: Evaluating cross-lingual sentence representations. In Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 2475–2485, Brussels, Belgium. Association for Computational Linguistics.\n\nDao et al. (2022) Tri Dao, Dan Fu, Stefano Ermon, Atri Rudra, and Christopher Ré. 2022. Flashattention: Fast and memory-efficient exact attention with io-awareness. In Advances in Neural Information Processing Systems, volume 35, pages 16344–16359. Curran Associates, Inc.\n\nDhamala et al. (2021) Jwala Dhamala, Tony Sun, Varun Kumar, Satyapriya Krishna, Yada Pruksachatkun, Kai-Wei Chang, and Rahul Gupta. 2021. Bold: Dataset and metrics for measuring biases in open-ended language generation. In Proceedings of the 2021 ACM Conference on Fairness, Accountability, and Transparency, FAccT ’21, page 862–872, New York, NY, USA. Association for Computing Machinery.\n\nDubois et al. (2023) Yann Dubois, Xuechen Li, Rohan Taori, Tianyi Zhang, Ishaan Gulrajani, Jimmy Ba, Carlos Guestrin, Percy Liang, and Tatsunori B Hashimoto. 2023. Alpacafarm: A simulation framework for methods that learn from human feedback. arXiv preprint arXiv:2305.14387.\n\nGanguli et al. (2022) Deep Ganguli, Liane Lovitt, Jackson Kernion, Amanda Askell, Yuntao Bai, Saurav Kadavath, Ben Mann, Ethan Perez, Nicholas Schiefer, Kamal Ndousse, et al. 2022. Red teaming language models to reduce harms: Methods, scaling behaviors, and lessons learned. arXiv preprint arXiv:2209.07858.\n\nGao et al. (2023) Leo Gao, Jonathan Tow, Baber Abbasi, Stella Biderman, Sid Black, Anthony DiPofi, Charles Foster, Laurence Golding, Jeffrey Hsu, Alain Le Noac’h, Haonan Li, Kyle McDonell, Niklas Muennighoff, Chris Ociepa, Jason Phang, Laria Reynolds, Hailey Schoelkopf, Aviya Skowron, Lintang Sutawika, Eric Tang, Anish Thite, Ben Wang, Kevin Wang, and Andy Zou. 2023. A framework for few-shot language model evaluation.\n\nGehman et al. (2020) Samuel Gehman, Suchin Gururangan, Maarten Sap, Yejin Choi, and Noah A. Smith. 2020. RealToxicityPrompts: Evaluating neural toxic degeneration in language models. In Findings of the Association for Computational Linguistics: EMNLP 2020, pages 3356–3369, Online. Association for Computational Linguistics.\n\nGoyal et al. (2022) Naman Goyal, Cynthia Gao, Vishrav Chaudhary, Peng-Jen Chen, Guillaume Wenzek, Da Ju, Sanjana Krishnan, Marc’Aurelio Ranzato, Francisco Guzmán, and Angela Fan. 2022. The Flores-101 evaluation benchmark for low-resource and multilingual machine translation. Transactions of the Association for Computational Linguistics, 10:522–538.\n\nGuerreiro et al. (2023) Nuno M Guerreiro, Ricardo Rei, Daan van Stigt, Luisa Coheur, Pierre Colombo, and André FT Martins. 2023. xcomet: Transparent machine translation evaluation through fine-grained error detection. arXiv preprint arXiv:2310.10482.\n\nHendrycks et al. (2020) Dan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn Song, and Jacob Steinhardt. 2020. Measuring massive multitask language understanding. arXiv preprint arXiv:2009.03300.\n\nHendrycks et al. (2021) Dan Hendrycks, Collin Burns, Saurav Kadavath, Akul Arora, Steven Basart, Eric Tang, Dawn Song, and Jacob Steinhardt. 2021. Measuring mathematical problem solving with the math dataset. NeurIPS.\n\nHoltzman et al. (2019) Ari Holtzman, Jan Buys, Li Du, Maxwell Forbes, and Yejin Choi. 2019. The curious case of neural text degeneration. In International Conference on Learning Representations.\n\nJaques et al. (2019) Natasha Jaques, Asma Ghandeharioun, Judy Hanwen Shen, Craig Ferguson, Agata Lapedriza, Noah Jones, Shixiang Gu, and Rosalind Picard. 2019. Way off-policy batch deep reinforcement learning of implicit human preferences in dialog. arXiv preprint arXiv:1907.00456.\n\nJeong et al. (2022) Younghun Jeong, Juhyun Oh, Jongwon Lee, Jaimeen Ahn, Jihyung Moon, Sungjoon Park, and Alice Oh. 2022. KOLD: Korean offensive language dataset. In Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing, pages 10818–10833, Abu Dhabi, United Arab Emirates. Association for Computational Linguistics.\n\nJiang et al. (2023) Albert Q. Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bamford, Devendra Singh Chaplot, Diego de las Casas, Florian Bressand, Gianna Lengyel, Guillaume Lample, Lucile Saulnier, Lélio Renard Lavaud, Marie-Anne Lachaux, Pierre Stock, Teven Le Scao, Thibaut Lavril, Thomas Wang, Timothée Lacroix, and William El Sayed. 2023. Mistral 7b.\n\nJin et al. (2023) Jiho Jin, Jiseon Kim, Nayeon Lee, Haneul Yoo, Alice Oh, and Hwaran Lee. 2023. Kobbq: Korean bias benchmark for question answering. arXiv preprint arXiv:2307.16778.\n\nJoshi et al. (2017) Mandar Joshi, Eunsol Choi, Daniel Weld, and Luke Zettlemoyer. 2017. TriviaQA: A large scale distantly supervised challenge dataset for reading comprehension. In Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 1601–1611, Vancouver, Canada. Association for Computational Linguistics.\n\nKim et al. (2021) Boseop Kim, HyoungSeok Kim, Sang-Woo Lee, Gichang Lee, Donghyun Kwak, Dong Hyeon Jeon, Sunghyun Park, Sungju Kim, Seonhoon Kim, Dongpil Seo, et al. 2021. What changes can large-scale language models bring? intensive study on hyperclova: Billions-scale korean generative pretrained transformers. arXiv preprint arXiv:2109.04650.\n\nKim et al. (2023) Dahyun Kim, Chanjun Park, Sanghoon Kim, Wonsung Lee, Wonho Song, Yunsu Kim, Hyeonwoo Kim, Yungi Kim, Hyeonju Lee, Jihoo Kim, et al. 2023. Solar 10.7 b: Scaling large language models with simple yet effective depth up-scaling. arXiv preprint arXiv:2312.15166.\n\nKim et al. (2024a) Eunsu Kim, Juyoung Suk, Philhoon Oh, Haneul Yoo, James Thorne, and Alice Oh. 2024a. Click: A benchmark dataset of cultural and linguistic intelligence in korean. arXiv preprint arXiv:2403.06412.\n\nKim et al. (2024b) Seungduk Kim, Seungtaek Choi, and Myeongho Jeong. 2024b. Efficient and effective vocabulary expansion towards multilingual large language models. arXiv preprint arXiv:2402.14714.\n\nKo et al. (2023) Hyunwoong Ko, Kichang Yang, Minho Ryu, Taekyoon Choi, Seungmu Yang, Sungho Park, et al. 2023. A technical report for polyglot-ko: Open-source large-scale korean language models. arXiv preprint arXiv:2306.02254.\n\nKocetkov et al. (2022) Denis Kocetkov, Raymond Li, Loubna Ben Allal, Jia Li, Chenghao Mou, Carlos Muñoz Ferrandis, Yacine Jernite, Margaret Mitchell, Sean Hughes, Thomas Wolf, et al. 2022. The stack: 3 tb of permissively licensed source code. arXiv preprint arXiv:2211.15533.\n\nKwiatkowski et al. (2019) Tom Kwiatkowski, Jennimaria Palomaki, Olivia Redfield, Michael Collins, Ankur Parikh, Chris Alberti, Danielle Epstein, Illia Polosukhin, Jacob Devlin, Kenton Lee, Kristina Toutanova, Llion Jones, Matthew Kelcey, Ming-Wei Chang, Andrew M. Dai, Jakob Uszkoreit, Quoc Le, and Slav Petrov. 2019. Natural questions: A benchmark for question answering research. Transactions of the Association for Computational Linguistics, 7:452–466.\n\nLee Junbum (2024) Lee Junbum. 2024. Yi-ko-6b (revision 205083a).\n\nLeike et al. (2018) Jan Leike, David Krueger, Tom Everitt, Miljan Martic, Vishal Maini, and Shane Legg. 2018. Scalable agent alignment via reward modeling: a research direction. arXiv preprint arXiv:1811.07871.\n\nLiang et al. (2022) Percy Liang, Rishi Bommasani, Tony Lee, Dimitris Tsipras, Dilara Soylu, Michihiro Yasunaga, Yian Zhang, Deepak Narayanan, Yuhuai Wu, Ananya Kumar, et al. 2022. Holistic evaluation of language models. arXiv preprint arXiv:2211.09110.\n\nLin et al. (2021) Bill Yuchen Lin, Seyeon Lee, Xiaoyang Qiao, and Xiang Ren. 2021. Common sense beyond English: Evaluating and improving multilingual language models for commonsense reasoning. In Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers), pages 1274–1287, Online. Association for Computational Linguistics.\n\nLin et al. (2022) Stephanie Lin, Jacob Hilton, and Owain Evans. 2022. TruthfulQA: Measuring how models mimic human falsehoods. In Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 3214–3252, Dublin, Ireland. Association for Computational Linguistics.\n\nMin et al. (2023) Sewon Min, Kalpesh Krishna, Xinxi Lyu, Mike Lewis, Wen-tau Yih, Pang Koh, Mohit Iyyer, Luke Zettlemoyer, and Hannaneh Hajishirzi. 2023. FActScore: Fine-grained atomic evaluation of factual precision in long form text generation. In Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, pages 12076–12100, Singapore. Association for Computational Linguistics.\n\nMukherjee et al. (2023) Subhabrata Mukherjee, Arindam Mitra, Ganesh Jawahar, Sahaj Agarwal, Hamid Palangi, and Ahmed Awadallah. 2023. Orca: Progressive learning from complex explanation traces of gpt-4.\n\nOuyang et al. (2022) Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, et al. 2022. Training language models to follow instructions with human feedback. Advances in Neural Information Processing Systems, 35:27730–27744.\n\nParrish et al. (2022) Alicia Parrish, Angelica Chen, Nikita Nangia, Vishakh Padmakumar, Jason Phang, Jana Thompson, Phu Mon Htut, and Samuel R. Bowman. 2022. Bbq: A hand-built bias benchmark for question answering.\n\nPeng et al. (2023) Baolin Peng, Linfeng Song, Ye Tian, Lifeng Jin, Haitao Mi, and Dong Yu. 2023. Stabilizing rlhf through advantage model and selective rehearsal. arXiv preprint arXiv:2309.10202.\n\nPerez et al. (2021) Ethan Perez, Douwe Kiela, and Kyunghyun Cho. 2021. True few-shot learning with language models. Advances in neural information processing systems, 34:11054–11070.\n\nPetrov et al. (2024) Aleksandar Petrov, Emanuele La Malfa, Philip Torr, and Adel Bibi. 2024. Language model tokenizers introduce unfairness between languages. Advances in Neural Information Processing Systems, 36.\n\nRae et al. (2021) Jack W Rae, Sebastian Borgeaud, Trevor Cai, Katie Millican, Jordan Hoffmann, Francis Song, John Aslanides, Sarah Henderson, Roman Ring, Susannah Young, et al. 2021. Scaling language models: Methods, analysis & insights from training gopher. arXiv preprint arXiv:2112.11446.\n\nSakaguchi et al. (2021) Keisuke Sakaguchi, Ronan Le Bras, Chandra Bhagavatula, and Yejin Choi. 2021. Winogrande: an adversarial winograd schema challenge at scale. Commun. ACM, 64(9):99–106.\n\nSchulman et al. (2017) John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. 2017. Proximal policy optimization algorithms. arXiv preprint arXiv:1707.06347.\n\nSclar et al. (2023) Melanie Sclar, Yejin Choi, Yulia Tsvetkov, and Alane Suhr. 2023. Quantifying language models’ sensitivity to spurious features in prompt design or: How i learned to start worrying about prompt formatting. arXiv preprint arXiv:2310.11324.\n\nSennrich et al. (2015) Rico Sennrich, Barry Haddow, and Alexandra Birch. 2015. Neural machine translation of rare words with subword units. arXiv preprint arXiv:1508.07909.\n\nShaham et al. (2024) Uri Shaham, Jonathan Herzig, Roee Aharoni, Idan Szpektor, Reut Tsarfaty, and Matan Eyal. 2024. Multilingual instruction tuning with just a pinch of multilinguality. ArXiv, abs/2401.01854.\n\nSinghal et al. (2023) Prasann Singhal, Tanya Goyal, Jiacheng Xu, and Greg Durrett. 2023. A long way to go: Investigating length correlations in rlhf. arXiv preprint arXiv:2310.03716.\n\nSon et al. (2024) Guijin Son, Hanwool Lee, Sungdong Kim, Seungone Kim, Niklas Muennighoff, Taekyoon Choi, Cheonbok Park, Kang Min Yoo, and Stella Biderman. 2024. Kmmlu: Measuring massive multitask language understanding in korean. arXiv preprint arXiv:2402.11548.\n\nSon et al. (2023) Guijin Son, Hanwool Lee, Suwan Kim, Jaecheol Lee, Je Won Yeom, Jihyu Jung, Jung Woo Kim, and Songseong Kim. 2023. Hae-rae bench: Evaluation of korean knowledge in language models. arXiv preprint arXiv:2309.02706.\n\nSrivastava et al. (2023) Aarohi Srivastava, Abhinav Rastogi, Abhishek Rao, Abu Awal Md Shoeb, Abubakar Abid, Adam Fisch, Adam R. Brown, Adam Santoro, Aditya Gupta, Adrià Garriga-Alonso, Agnieszka Kluska, Aitor Lewkowycz, Akshat Agarwal, Alethea Power, Alex Ray, Alex Warstadt, Alexander W. Kocurek, Ali Safaya, Ali Tazarv, Alice Xiang, Alicia Parrish, Allen Nie, Aman Hussain, Amanda Askell, Amanda Dsouza, Ambrose Slone, Ameet Rahane, Anantharaman S. Iyer, Anders Johan Andreassen, Andrea Madotto, Andrea Santilli, Andreas Stuhlmüller, Andrew M. Dai, Andrew La, Andrew Lampinen, Andy Zou, Angela Jiang, Angelica Chen, Anh Vuong, Animesh Gupta, Anna Gottardi, Antonio Norelli, Anu Venkatesh, Arash Gholamidavoodi, Arfa Tabassum, Arul Menezes, Arun Kirubarajan, Asher Mullokandov, Ashish Sabharwal, Austin Herrick, Avia Efrat, Aykut Erdem, Ayla Karakaş, B. Ryan Roberts, Bao Sheng Loe, Barret Zoph, Bartłomiej Bojanowski, Batuhan Özyurt, Behnam Hedayatnia, Behnam Neyshabur, Benjamin Inden, Benno Stein, Berk Ekmekci, Bill Yuchen Lin, Blake Howald, Bryan Orinion, Cameron Diao, Cameron Dour, Catherine Stinson, Cedrick Argueta, Cesar Ferri, Chandan Singh, Charles Rathkopf, Chenlin Meng, Chitta Baral, Chiyu Wu, Chris Callison-Burch, Christopher Waites, Christian Voigt, Christopher D Manning, Christopher Potts, Cindy Ramirez, Clara E. Rivera, Clemencia Siro, Colin Raffel, Courtney Ashcraft, Cristina Garbacea, Damien Sileo, Dan Garrette, Dan Hendrycks, Dan Kilman, Dan Roth, C. Daniel Freeman, Daniel Khashabi, Daniel Levy, Daniel Moseguí González, Danielle Perszyk, Danny Hernandez, Danqi Chen, Daphne Ippolito, Dar Gilboa, David Dohan, David Drakard, David Jurgens, Debajyoti Datta, Deep Ganguli, Denis Emelin, Denis Kleyko, Deniz Yuret, Derek Chen, Derek Tam, Dieuwke Hupkes, Diganta Misra, Dilyar Buzan, Dimitri Coelho Mollo, Diyi Yang, Dong-Ho Lee, Dylan Schrader, Ekaterina Shutova, Ekin Dogus Cubuk, Elad Segal, Eleanor Hagerman, Elizabeth Barnes, Elizabeth Donoway, Ellie Pavlick, Emanuele Rodolà, Emma Lam, Eric Chu, Eric Tang, Erkut Erdem, Ernie Chang, Ethan A Chi, Ethan Dyer, Ethan Jerzak, Ethan Kim, Eunice Engefu Manyasi, Evgenii Zheltonozhskii, Fanyue Xia, Fatemeh Siar, Fernando Martínez-Plumed, Francesca Happé, Francois Chollet, Frieda Rong, Gaurav Mishra, Genta Indra Winata, Gerard de Melo, Germán Kruszewski, Giambattista Parascandolo, Giorgio Mariani, Gloria Xinyue Wang, Gonzalo Jaimovitch-Lopez, Gregor Betz, Guy Gur-Ari, Hana Galijasevic, Hannah Kim, Hannah Rashkin, Hannaneh Hajishirzi, Harsh Mehta, Hayden Bogar, Henry Francis Anthony Shevlin, Hinrich Schuetze, Hiromu Yakura, Hongming Zhang, Hugh Mee Wong, Ian Ng, Isaac Noble, Jaap Jumelet, Jack Geissinger, Jackson Kernion, Jacob Hilton, Jaehoon Lee, Jaime Fernández Fisac, James B Simon, James Koppel, James Zheng, James Zou, Jan Kocon, Jana Thompson, Janelle Wingfield, Jared Kaplan, Jarema Radom, Jascha Sohl-Dickstein, Jason Phang, Jason Wei, Jason Yosinski, Jekaterina Novikova, Jelle Bosscher, Jennifer Marsh, Jeremy Kim, Jeroen Taal, Jesse Engel, Jesujoba Alabi, Jiacheng Xu, Jiaming Song, Jillian Tang, Joan Waweru, John Burden, John Miller, John U. Balis, Jonathan Batchelder, Jonathan Berant, Jörg Frohberg, Jos Rozen, Jose Hernandez-Orallo, Joseph Boudeman, Joseph Guerr, Joseph Jones, Joshua B. Tenenbaum, Joshua S. Rule, Joyce Chua, Kamil Kanclerz, Karen Livescu, Karl Krauth, Karthik Gopalakrishnan, Katerina Ignatyeva, Katja Markert, Kaustubh Dhole, Kevin Gimpel, Kevin Omondi, Kory Wallace Mathewson, Kristen Chiafullo, Ksenia Shkaruta, Kumar Shridhar, Kyle McDonell, Kyle Richardson, Laria Reynolds, Leo Gao, Li Zhang, Liam Dugan, Lianhui Qin, Lidia Contreras-Ochando, Louis-Philippe Morency, Luca Moschella, Lucas Lam, Lucy Noble, Ludwig Schmidt, Luheng He, Luis Oliveros-Colón, Luke Metz, Lütfi Kerem Senel, Maarten Bosma, Maarten Sap, Maartje Ter Hoeve, Maheen Farooqi, Manaal Faruqui, Mantas Mazeika, Marco Baturan, Marco Marelli, Marco Maru, Maria Jose Ramirez-Quintana, Marie Tolkiehn, Mario Giulianelli, Martha Lewis, Martin Potthast, Matthew L Leavitt, Matthias Hagen, Mátyás Schubert, Medina Orduna Baitemirova, Melody Arnaud, Melvin McElrath, Michael Andrew Yee, Michael Cohen, Michael Gu, Michael Ivanitskiy, Michael Starritt, Michael Strube, Michał Swędrowski, Michele Bevilacqua, Michihiro Yasunaga, Mihir Kale, Mike Cain, Mimee Xu, Mirac Suzgun, Mitch Walker, Mo Tiwari, Mohit Bansal, Moin Aminnaseri, Mor Geva, Mozhdeh Gheini, Mukund Varma T, Nanyun Peng, Nathan Andrew Chi, Nayeon Lee, Neta Gur-Ari Krakover, Nicholas Cameron, Nicholas Roberts, Nick Doiron, Nicole Martinez, Nikita Nangia, Niklas Deckers, Niklas Muennighoff, Nitish Shirish Keskar, Niveditha S. Iyer, Noah Constant, Noah Fiedel, Nuan Wen, Oliver Zhang, Omar Agha, Omar Elbaghdadi, Omer Levy, Owain Evans, Pablo Antonio Moreno Casares, Parth Doshi, Pascale Fung, Paul Pu Liang, Paul Vicol, Pegah Alipoormolabashi, Peiyuan Liao, Percy Liang, Peter W Chang, Peter Eckersley, Phu Mon Htut, Pinyu Hwang, Piotr Miłkowski, Piyush Patil, Pouya Pezeshkpour, Priti Oli, Qiaozhu Mei, Qing Lyu, Qinlang Chen, Rabin Banjade, Rachel Etta Rudolph, Raefer Gabriel, Rahel Habacker, Ramon Risco, Raphaël Millière, Rhythm Garg, Richard Barnes, Rif A. Saurous, Riku Arakawa, Robbe Raymaekers, Robert Frank, Rohan Sikand, Roman Novak, Roman Sitelew, Ronan Le Bras, Rosanne Liu, Rowan Jacobs, Rui Zhang, Russ Salakhutdinov, Ryan Andrew Chi, Seungjae Ryan Lee, Ryan Stovall, Ryan Teehan, Rylan Yang, Sahib Singh, Saif M. Mohammad, Sajant Anand, Sam Dillavou, Sam Shleifer, Sam Wiseman, Samuel Gruetter, Samuel R. Bowman, Samuel Stern Schoenholz, Sanghyun Han, Sanjeev Kwatra, Sarah A. Rous, Sarik Ghazarian, Sayan Ghosh, Sean Casey, Sebastian Bischoff, Sebastian Gehrmann, Sebastian Schuster, Sepideh Sadeghi, Shadi Hamdan, Sharon Zhou, Shashank Srivastava, Sherry Shi, Shikhar Singh, Shima Asaadi, Shixiang Shane Gu, Shubh Pachchigar, Shubham Toshniwal, Shyam Upadhyay, Shyamolima Shammie Debnath, Siamak Shakeri, Simon Thormeyer, Simone Melzi, Siva Reddy, Sneha Priscilla Makini, Soo-Hwan Lee, Spencer Torene, Sriharsha Hatwar, Stanislas Dehaene, Stefan Divic, Stefano Ermon, Stella Biderman, Stephanie Lin, Stephen Prasad, Steven Piantadosi, Stuart Shieber, Summer Misherghi, Svetlana Kiritchenko, Swaroop Mishra, Tal Linzen, Tal Schuster, Tao Li, Tao Yu, Tariq Ali, Tatsunori Hashimoto, Te-Lin Wu, Théo Desbordes, Theodore Rothschild, Thomas Phan, Tianle Wang, Tiberius Nkinyili, Timo Schick, Timofei Kornev, Titus Tunduny, Tobias Gerstenberg, Trenton Chang, Trishala Neeraj, Tushar Khot, Tyler Shultz, Uri Shaham, Vedant Misra, Vera Demberg, Victoria Nyamai, Vikas Raunak, Vinay Venkatesh Ramasesh, vinay uday prabhu, Vishakh Padmakumar, Vivek Srikumar, William Fedus, William Saunders, William Zhang, Wout Vossen, Xiang Ren, Xiaoyu Tong, Xinran Zhao, Xinyi Wu, Xudong Shen, Yadollah Yaghoobzadeh, Yair Lakretz, Yangqiu Song, Yasaman Bahri, Yejin Choi, Yichi Yang, Yiding Hao, Yifu Chen, Yonatan Belinkov, Yu Hou, Yufang Hou, Yuntao Bai, Zachary Seid, Zhuoye Zhao, Zijian Wang, Zijie J. Wang, Zirui Wang, and Ziyi Wu. 2023. Beyond the imitation game: Quantifying and extrapolating the capabilities of language models. Transactions on Machine Learning Research.\n\nSrivastava et al. (2022) Aarohi Srivastava, Abhinav Rastogi, Abhishek Rao, Abu Awal Md Shoeb, Abubakar Abid, Adam Fisch, Adam R Brown, Adam Santoro, Aditya Gupta, Adrià Garriga-Alonso, et al. 2022. Beyond the imitation game: Quantifying and extrapolating the capabilities of language models. arXiv preprint arXiv:2206.04615.\n\nStiennon et al. (2022) Nisan Stiennon, Long Ouyang, Jeff Wu, Daniel M. Ziegler, Ryan Lowe, Chelsea Voss, Alec Radford, Dario Amodei, and Paul Christiano. 2022. Learning to summarize from human feedback.\n\nStiennon et al. (2020) Nisan Stiennon, Long Ouyang, Jeffrey Wu, Daniel Ziegler, Ryan Lowe, Chelsea Voss, Alec Radford, Dario Amodei, and Paul F Christiano. 2020. Learning to summarize with human feedback. Advances in Neural Information Processing Systems, 33:3008–3021.\n\nSu et al. (2024) Jianlin Su, Murtadha Ahmed, Yu Lu, Shengfeng Pan, Wen Bo, and Yunfeng Liu. 2024. Roformer: Enhanced transformer with rotary position embedding. Neurocomputing, 568:127063.\n\nSung et al. (2017) Nako Sung, Minkyu Kim, Hyunwoo Jo, Youngil Yang, Jingwoong Kim, Leonard Lausen, Youngkwan Kim, Gayoung Lee, Donghyun Kwak, Jung-Woo Ha, et al. 2017. Nsml: A machine learning platform that enables you to focus on your models. arXiv preprint arXiv:1712.05902.\n\nSuzgun et al. (2023) Mirac Suzgun, Nathan Scales, Nathanael Schärli, Sebastian Gehrmann, Yi Tay, Hyung Won Chung, Aakanksha Chowdhery, Quoc Le, Ed Chi, Denny Zhou, and Jason Wei. 2023. Challenging BIG-bench tasks and whether chain-of-thought can solve them. In Findings of the Association for Computational Linguistics: ACL 2023, pages 13003–13051, Toronto, Canada. Association for Computational Linguistics.\n\nTalmor et al. (2019) Alon Talmor, Jonathan Herzig, Nicholas Lourie, and Jonathan Berant. 2019. CommonsenseQA: A question answering challenge targeting commonsense knowledge. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), pages 4149–4158, Minneapolis, Minnesota. Association for Computational Linguistics.\n\nTeam et al. (2024) Gemma Team, Thomas Mesnard, Cassidy Hardin, Robert Dadashi, Surya Bhupatiraju, Shreya Pathak, Laurent Sifre, Morgane Rivière, Mihir Sanjay Kale, Juliette Love, et al. 2024. Gemma: Open models based on gemini research and technology. arXiv preprint arXiv:2403.08295.\n\nTouvron et al. (2023a) Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothée Lacroix, Baptiste Rozière, Naman Goyal, Eric Hambro, Faisal Azhar, et al. 2023a. Llama: Open and efficient foundation language models. arXiv preprint arXiv:2302.13971.\n\nTouvron et al. (2023b) Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, Dan Bikel, Lukas Blecher, Cristian Canton Ferrer, Moya Chen, Guillem Cucurull, David Esiobu, Jude Fernandes, Jeremy Fu, Wenyin Fu, Brian Fuller, Cynthia Gao, Vedanuj Goswami, Naman Goyal, Anthony Hartshorn, Saghar Hosseini, Rui Hou, Hakan Inan, Marcin Kardas, Viktor Kerkez, Madian Khabsa, Isabel Kloumann, Artem Korenev, Punit Singh Koura, Marie-Anne Lachaux, Thibaut Lavril, Jenya Lee, Diana Liskovich, Yinghai Lu, Yuning Mao, Xavier Martinet, Todor Mihaylov, Pushkar Mishra, Igor Molybog, Yixin Nie, Andrew Poulton, Jeremy Reizenstein, Rashi Rungta, Kalyan Saladi, Alan Schelten, Ruan Silva, Eric Michael Smith, Ranjan Subramanian, Xiaoqing Ellen Tan, Binh Tang, Ross Taylor, Adina Williams, Jian Xiang Kuan, Puxin Xu, Zheng Yan, Iliyan Zarov, Yuchen Zhang, Angela Fan, Melanie Kambadur, Sharan Narang, Aurelien Rodriguez, Robert Stojnic, Sergey Edunov, and Thomas Scialom. 2023b. Llama 2: Open foundation and fine-tuned chat models.\n\nÜstün et al. (2024) Ahmet Üstün, Viraat Aryabumi, Zheng-Xin Yong, Wei-Yin Ko, Daniel D’souza, Gbemileke Onilude, Neel Bhandari, Shivalika Singh, Hui-Lee Ooi, Amr Kayid, et al. 2024. Aya model: An instruction finetuned open-access multilingual language model. arXiv preprint arXiv:2402.07827.\n\nVaswani et al. (2017) Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Łukasz Kaiser, and Illia Polosukhin. 2017. Attention is all you need. Advances in neural information processing systems, 30.\n\nWang et al. (2023) Xuezhi Wang, Jason Wei, Dale Schuurmans, Quoc V Le, Ed H. Chi, Sharan Narang, Aakanksha Chowdhery, and Denny Zhou. 2023. Self-consistency improves chain of thought reasoning in language models. In The Eleventh International Conference on Learning Representations.\n\nWang et al. (2022) Yizhong Wang, Swaroop Mishra, Pegah Alipoormolabashi, Yeganeh Kordi, Amirreza Mirzaei, Anjana Arunkumar, Arjun Ashok, Arut Selvan Dhanasekaran, Atharva Naik, David Stap, et al. 2022. Super-naturalinstructions: Generalization via declarative instructions on 1600+ nlp tasks. arXiv preprint arXiv:2204.07705.\n\nWei et al. (2022) Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, brian ichter, Fei Xia, Ed Chi, Quoc V Le, and Denny Zhou. 2022. Chain-of-thought prompting elicits reasoning in large language models. In Advances in Neural Information Processing Systems, volume 35, pages 24824–24837. Curran Associates, Inc.\n\nWelleck et al. (2019) Sean Welleck, Ilia Kulikov, Stephen Roller, Emily Dinan, Kyunghyun Cho, and Jason Weston. 2019. Neural text generation with unlikelihood training. In International Conference on Learning Representations.\n\nZellers et al. (2019) Rowan Zellers, Ari Holtzman, Yonatan Bisk, Ali Farhadi, and Yejin Choi. 2019. HellaSwag: Can a machine really finish your sentence? In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 4791–4800, Florence, Italy. Association for Computational Linguistics.\n\nZeng et al. (2023) Dun Zeng, Yong Dai, Pengyu Cheng, Tianhao Hu, Wanshun Chen, Nan Du, and Zenglin Xu. 2023. On diverse preferences for large language model alignment. arXiv preprint arXiv:2312.07401.\n\nZheng et al. (2023a) Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang, Zhanghao Wu, Yonghao Zhuang, Zi Lin, Zhuohan Li, Dacheng Li, Eric P. Xing, Hao Zhang, Joseph E. Gonzalez, and Ion Stoica. 2023a. Judging llm-as-a-judge with mt-bench and chatbot arena.\n\nZheng et al. (2023b) Rui Zheng, Shihan Dou, Songyang Gao, Yuan Hua, Wei Shen, Binghai Wang, Yan Liu, Senjie Jin, Qin Liu, Yuhao Zhou, et al. 2023b. Secrets of rlhf in large language models part i: Ppo. arXiv preprint arXiv:2307.04964.\n\nZhong et al. (2023) Wanjun Zhong, Ruixiang Cui, Yiduo Guo, Yaobo Liang, Shuai Lu, Yanlin Wang, Amin Saied, Weizhu Chen, and Nan Duan. 2023. Agieval: A human-centric benchmark for evaluating foundation models. arXiv preprint arXiv:2304.06364.\n\nZhou et al. (2023) Chunting Zhou, Pengfei Liu, Puxin Xu, Srini Iyer, Jiao Sun, Yuning Mao, Xuezhe Ma, Avia Efrat, Ping Yu, LILI YU, Susan Zhang, Gargi Ghosh, Mike Lewis, Luke Zettlemoyer, and Omer Levy. 2023. LIMA: Less is more for alignment. In Thirty-seventh Conference on Neural Information Processing Systems.\n\nAppendix A Contributions\n\nWithin each role, names are listed in alphabetical order by last name, followed by the first name.\n\nTechnical Lead\n\nJaegeun Han\n\nSookyo In\n\nHeewon Jeon\n\nJisu Jeong\n\nJaewook Kang\n\nHyunwook Kim\n\nKyung-Min Kim\n\nMunhyong Kim\n\nSungju Kim\n\nDonghyun Kwak\n\nHanock Kwak\n\nSe Jung Kwon\n\nBado Lee\n\nDongsoo Lee\n\nGichang Lee\n\nJooho Lee\n\nBaeseong Park\n\nSeongjin Shin\n\nJoonsang Yu\n\nEngineering Lead\n\nSeolki Baek\n\nSumin Byeon\n\nEungsup Cho\n\nDooseok Choe\n\nJeesung Han\n\nYoungkyun Jin\n\nHyein Jun\n\nJaeseung Jung\n\nChanwoong Kim\n\nJinhong Kim\n\nJinuk Kim\n\nDokyeong Lee\n\nDongwook Park\n\nJeong Min Sohn\n\nProduct Lead\n\nSujung Han\n\nJiae Heo\n\nSungju Hong\n\nMina Jeon\n\nHyunhoon Jung\n\nJungeun Jung\n\nWangkyo Jung\n\nChungjoon Kim\n\nHyeri Kim\n\nJonghyun Kim\n\nMin Young Kim\n\nSoeun Lee\n\nJoonhee Park\n\nJieun Shin\n\nSojin Yang\n\nJungsoon Yoon\n\nResearch Lead\n\nKang Min Yoo\n\nSafety Lead\n\nHwaran Lee\n\nSpecial Contributors\n\nSanghwan Bae\n\nJeehwan Cha\n\nDonghoon Ham\n\nYoungki Hong\n\nYunki Hong\n\nMyunggeun Ji\n\nYeguk Jin\n\nChansong Jo\n\nShinyoung Joo\n\nSeunghwan Jung\n\nChungjoon Kim\n\nHyomin Kim\n\nJungwhan Kim\n\nMinkyoung Kim\n\nMinseung Kim\n\nSungdong Kim\n\nYonghee Kim\n\nYoungjun Kim\n\nDonghyeon Ko\n\nDughyun Lee\n\nJaehong Lee\n\nJieun Lee\n\nJongjin Lee\n\nMin Young Lee\n\nYehbin Lee\n\nTaehong Min\n\nKiyoon Moon\n\nJaesun Park\n\nKyuyon Park\n\nSeunghyun Seo\n\nGyubin Son\n\nWonjoon Yoo\n\nMyungin You\n\nCore Contributors\n\nDoheon Ahn\n\nHomin Ahn\n\nJoohee Ahn\n\nSeongmin Ahn\n\nChanwoo An\n\nHyeryun An\n\nJunho An\n\nSang-Min An\n\nBoram Byun\n\nJongho Cha\n\nMinji Chang\n\nSeunggyu Chang\n\nHaesong Cho\n\nYoungdo Cho\n\nDalnim Choi\n\nDaseul Choi\n\nHyoseok Choi\n\nMinseong Choi\n\nSangho Choi\n\nSeongjae Choi\n\nWooyong Choi\n\nSewhan Chun\n\nDong Young Go\n\nChiheon Ham\n\nDanbi Han\n\nJaemin Han\n\nMihak Hong\n\nMoonyoung Hong\n\nSung Bum Hong\n\nSeongchan Hwang\n\nEunbin Hyun\n\nJinbae Im\n\nJaehyung Jang\n\nJaeni Jang\n\nSihyeon Jang\n\nSungwon Jang\n\nJoonha Jeon\n\nYujin Jeon\n\nDaun Jeong\n\nJoonhyun Jeong\n\nKyeongseok Jeong\n\nMini Jeong\n\nYeji Jeong\n\nSol Jin\n\nHanbyeol Jo\n\nHanju Jo\n\nMinjung Jo\n\nLee Jonghyun\n\nChaeyoon Jung\n\nHyungsik Jung\n\nJaeuk Jung\n\nJu Hwan Jung\n\nKwangsun Jung\n\nSeungjae Jung\n\nSoonwon Ka\n\nDonghan Kang\n\nSoyoung Kang\n\nTaeho Kil\n\nAreum Kim\n\nBeomyoung Kim\n\nByeongwook Kim\n\nDaehee Kim\n\nDong-Gyun Kim\n\nDonggook Kim\n\nDonghyun Kim\n\nEuna Kim\n\nEunchul Kim\n\nGeewook Kim"
    }
}
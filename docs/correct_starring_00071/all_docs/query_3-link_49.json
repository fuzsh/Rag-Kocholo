{
    "id": "correct_starring_00071_3",
    "rank": 49,
    "data": {
        "url": "https://www.knowledgegraph.tech/session-by-track/",
        "read_more_link": "",
        "language": "en",
        "title": "Sessions by Track – The Knowledge Graph Conference",
        "top_image": "https://www.knowledgegraph.tech/wp-content/uploads/2022/02/cropped-Short-Logo-1-32x32.png",
        "meta_img": "https://www.knowledgegraph.tech/wp-content/uploads/2022/02/cropped-Short-Logo-1-32x32.png",
        "images": [
            "https://www.knowledgegraph.tech/wp-content/uploads/2022/02/Long-White-Logo.png",
            "https://www.knowledgegraph.tech/wp-content/uploads/2022/02/Long-White-Logo.png",
            "https://142.93.180.72/wp-content/uploads/2020/04/logo_white_horizontal-1-small-4-e1585838646146.png",
            "https://www.knowledgegraph.tech/wp-content/uploads/2022/02/Watch-Videos.png",
            "https://www.knowledgegraph.tech/wp-content/uploads/2022/02/Slack.svg",
            "https://www.knowledgegraph.tech/wp-content/uploads/2022/10/Discourse_icon.svg"
        ],
        "movies": [],
        "keywords": [],
        "meta_keywords": [
            ""
        ],
        "tags": null,
        "authors": [
            "lechatpito"
        ],
        "publish_date": "2023-02-21T16:17:22+00:00",
        "summary": "",
        "meta_description": "Sessions by Track = In-Person Session = Online Session Business Use Cases Content Knowledge Graphs Data Architecture Deep Learning for and with Knowledge Graphs Environmental, Social and Governance Metadata Natural Language Processing (NLP) Ontologies, Taxonomies, Data Modeling Semantic Layer Systems and Scale General Track",
        "meta_lang": "en",
        "meta_favicon": "https://www.knowledgegraph.tech/wp-content/uploads/2022/02/cropped-Short-Logo-1-32x32.png",
        "meta_site_name": "The Knowledge Graph Conference",
        "canonical_link": "https://www.knowledgegraph.tech/session-by-track/",
        "text": "Content personalization and recommendation engines are pervasive in today’s society. They power some of our most used platforms–including Amazon, Spotify, Netflix, and more. However, many organizations struggle to provide their employees and customers with the same ease of content and information discovery. They are turning to semantic recommendation engines to solve these business challenges. How do you get started in designing and implementing an enterprise content recommendation engine to connect people with relevant content at the time of need? How do you define a knowledge graph that will effectively connect people to the right experts ? Do you take a deterministic or statistical approach with your semantic foundation? Nash and Duane have led numerous recommendation engine implementations, including with organizations in the healthcare, finance, and government sectors. During this presentation, they will share case studies, lessons learned, and success stories of enterprise semantic recommendation engines, a clear approach for establishing a recommender, and best practices to drive a successful implementation.\n\nSpeakers: Sara Duane; Sara Nash\n\nBuilt correctly, an organisational Knowledge Graph can combine the power of data, the cloud, and AI in one unified structure.\n\nThis talk will outline four main contentions:\n\n• Networked Data. That network-shaped data can model complex structures including circular feedback loops and abstract models, and that networks (or graphs) make the connections between things as important as the things themselves.\n\n• Networked Cloud. That the network-shaped cloud of connected computers means that ALL the important data in an organisation can be joined together regardless of where that information is stored. Moreover, it states that an organisation’s Knowledge Graph is not just one big centralised database, but rather a distributed and interconnected ecosystem.\n\n• Networked AI. That networked-shaped AI lets us make predictions about connections, loops and abstractions and embed the generated insights directly back as an integral part of the Knowledge Graph. And that this very active branch of machine learning is starting to outperform ‘traditional’ AI in complex tasks.\n\n• Unified Network. And finally, it states that these three networks (data, cloud & AI) can be joined into one Knowledge Graph that has the powers of each component but is also more than just the sum of those parts.\n\nThe task will then ground this in reality by outlining three practical tools:\n\n• The Graph Adapter. Which sits on top of the existing databases, APIs and files in your organisation and converts 2D sets of tabular data into 3D graphs of data. The key intuition here is that the underlying databases, files and APIs do not need to change — the adapter just exposes a network-shaped layer on top of all other data structures.\n\n• The Data Service. Which is a specialisation of an existing and well-established architectural pattern called a microservice (but where data itself is treated as a first-class citizen). An individual data service can use the graph adapters to publish graph fragments into the cloud using an HTTP server. Each data item is given a unique resolvable network address in the form of a URL. The data services (or Data Products) combine to form a peer-to-peer network (or Data Mesh).\n\n• The Graph Neural Network. This allows each data service to mirror the passive graph-shaped data with an active graph-shaped machine learning model that gives each node the potential to also learn and predict. The data service publishes these learned node embeddings back into the network as pure data.\n\nSpeaker: Tony Seale\n\nThe EU Knowledge Graph at the European Commission\n\nThe European Commission is maintaining a Knowledge Graph using Wikibase, the same software that is running behind Wikidata (one of the most successful public Knowledge Graphs). In this talk we describe:\n\n– the content of the EU Knowledge Graph\n\n– why and how it is reusing Wikibase and software that is integrated with it (like OpenRefine)\n\n– how data is ingested and maintained fresh\n\n– what services are served over this Knowledge Graph\n\nIt is generally difficult to see a production Knowledge Graph. In this talk we will show you its main components and describe how it is operated behind the scene.\n\nThis presentation will be given together with Max De Wilde, information architect at DG CNECT, European Commission.\n\nSpeaker: Dennis Diefenbach\n\nOne of the current key challenges in Explainable AI is in correctly interpreting activations of hidden neurons. It seems evident that accurate interpretations thereof would provide insights into the question what a deep learning system has internally detected as relevant on the input, thus lifting some of the black box character of deep learning systems. The state of the art on this front indicates that hidden node activations appear to be interpretable in a way that makes sense to humans, at least in some cases. Yet, systematic automated methods that would be able to first hypothesize an interpretation of a hidden neuron activations, and then verify it, are mostly missing. In this presentation, we provide such a method and demonstrate that it provides meaningful interpretations. It is based on using large-scale background knowledge – a class hierarchy of approx. 2 million classes curated from the Wikipedia Concept Hierarchy – together with a symbolic reasoning approach called concept induction based on description logics that was originally developed for applications in the Semantic Web field. Our results show that we can automatically attach meaningful labels from the background knowledge to individual neurons in the dense layer through a hypothesis and verification process.\n\nSpeaker: Abhilekha Dalal\n\nIn general, property graphs are very flexible since we can associate any number of properties with nodes and edges. To add more structure, nodes and/or edges are often typed (via a label). In that case, a labeled node (or edge) of a particular type is expected to have specific properties. This works fine if node types are well defined and remain relatively stable. But what if we want to define relationships between any kind of nodes (existing or future node types)? For instance, in a metadata graph, we may be interested in the data lineage between various node types (“entities”), but in reality it doesn’t matter whether the node type is a dataset, the input to (or output of) a machine learning model, a physical device or digital twin that provides real-time data, etc. To model data lineage, all nodes need to include a group of properties that we would refer to as a database schema, but the actual type of those nodes is irrelevant. In general, how nodes can be related to other nodes, or how any service can observe or interact with nodes in a graph merely depends on the shared groups of properties which are often referred to a aspects or facets. In our presentation, we provide numerous examples of the various benefits that graph models which are based on facets provide. In particular, we will focus on actionable property graphs that can be utilized for self-governing data management and various aspects of optimizations via a pattern that is very popular in game programming, namely Entity Component Systems (ECS). Instead of defining nodes of a particular type, nodes are merely modeled as UIDs and sets of facets (aspects, components) that are standardized and can be added dynamically. For a metadata graph, this could include the logical model (via schema and ontology facets), physical aspects (facets for data formats and locations), statistics and usage, governance (e.g. facets state details about the inclusion of personal identifiable information). In order to make a graph actionable, external processes (so-called systems) operate on (arbitrary nodes) that happen to include certain facets. One system would operate on nodes that contain a schema facet and ensure that data lineage is maintained and provides an impact analysis if changes are necessary. Another system continually monitors access restrictions for nodes that represent datasets and contain a facet that specifies personally identifiable information.\n\nOther systems automate the data placement of datasets, but operate on nodes that include multiple facets (for data location, but also usage statistics, and PII). With this information, the system can find the optimal location of a dataset while taking usage and legal restrictions into consideration.\n\nWhile the concept of facets or aspects is not new, the purpose of the presentation is to raise awareness for the benefits of facets – in particular we show how facets can help turning property graphs into “active” property graphs.\n\nSpeaker: Jens Doerpmund\n\nWe spend a lot of time to optimise our digital platforms, websites, apps to rank higher in Google and for customers to click our links and come to our website from search engines. And sometimes we feel our job is done when our websites rank higher than our competitors in Google (and other search engines) and there is a lot of quality traffic that enters our digital ecosystem. Yes, our job on SEO is done. But the bigger piece has only just started. How do we ensure that this quality traffic is converting? That they’re finding the right content, products or services that will meet their individual needs? That it’s easy for them to look for the right information? And that through that we as business are able to meet our goals: selling a product, having them apply for a service, make them do a task (e.g. subscribe to a newsletter), etc. Putting in the right strategies and principles to bridge this gap between SEO and CRO can help businesses save on their marketing dollars and cut short their marketing budgets that they spend to attract quality customers to their website.\n\nSpeaker: Kanika Bhatia\n\nKnowledge graphs (KG) play a crucial role in many modern applications. Industrial knowledge is scattered in large volumes of both structured and unstructured data sources and bringing them to a unified knowledge graph can bring a lot of value. However, automatically constructing a KG from natural language text is challenging due the ambiguity and impreciseness of the natural languages. Recently, many approaches have been proposed to transform natural language text to triples to construct KGs. Out of those, approaches based on transformer language models are predominantly leading in many subtasks related to knowledge graph construction such as entity and relation extraction. In this presentation, we will focus on the state of the art of transformer language model based methods, techniques and tools for constructing knowledge graphs from text, their capabilities, limitations and current challenges. It aims to summarize the research progress over the KG construction from text with a specific focus on the information acquisition branch entailing entity and relation extraction covering the state of the art transformer methods and tools. This will be useful for any practitioner who is interested in building knowledge graphs for their organizations.\n\nSpeakers: Jennifer D’Souza; Nandana Mihindukulasooriya\n\nLarge enterprises maintain a multitude of data assets pertaining to their businesses. It’s arduous for engineers and data scientists to not only find the information they need, but also to ensure it’s accurate and up to date. This can lead up to data duplication, misuse of assets, and conflicting results. We address this problem by leveraging an enterprise ontology, which we assume users in a domain will intuitively understand. Our approach is novel in that we allow users to navigate over their data assets semantically, using the concepts and relationships of this ontology. We present a case study involving a client that uses an ontology comprising hundreds of concepts to efficiently search for and manage a set of data assets that number in the tens of thousands. This solution uses the RelationalAI Knowledge Graph Management System to power this search process. We include a live demonstration of the working solution and discuss some important lessons learned.\n\nSpeaker: Márton Búr\n\nIn financial services, a common language and data model are essential to not only meet regulatory needs but also to stay competitive by creating more products more quickly and monetizing on massive amounts of accumulated, heterogeneous data. In fact, we see an increasing number of semantic layer and modeling tools such as Legend, Morphir, and others coming into the open source realm and gaining adoption amongst other institutions to try to address this. Historically, however, there are challenges with integrating and executing these semantic layers within an existing data infrastructure ecosystem at scale. This often results in obstacles to adoption and difficulties in transitioning efforts to production.\n\nIn this talk, we will provide a specific example of how we use relational semantic layers to solve this challenge through a financial services use case. You’ll learn about semantic layers in financial services and how a relational semantic layer fits in a modern data stack. You’ll also get a technical review of an applied financial services use case involving PURE/Legend, and find out how the business benefits from having a generic model of representation and execution that spans all data sources and types (e.g., semistructured, graph, tabular, etc.). The talk will end with forward-looking thoughts on the industry and a chance for you to ask questions of some of the experts implementing these solutions.\n\nSpeakers: Gerald Berger; Michelle Yi"
    }
}
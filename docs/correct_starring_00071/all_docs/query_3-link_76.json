{
    "id": "correct_starring_00071_3",
    "rank": 76,
    "data": {
        "url": "https://www.nebula-graph.io/posts/graph-data-modeling-and-etl-dbt",
        "read_more_link": "",
        "language": "en",
        "title": "Graph Data Modeling and ETL with dbt in NebulaGraph Database",
        "top_image": "https://www-cdn.nebula-graph.io/nebula-website-5.0/images/blogs/ELT%20with%20dbt/featured-image.jpg",
        "meta_img": "https://www-cdn.nebula-graph.io/nebula-website-5.0/images/blogs/ELT%20with%20dbt/featured-image.jpg",
        "images": [
            "https://www-cdn.nebula-graph.io/nebula-website-5.0/images/logo.png 1x, https://www-cdn.nebula-graph.io/nebula-website-5.0/images/logo.png 2x",
            "https://www-cdn.nebula-graph.io/nebula-website-5.0/images/blogs/ELT%20with%20dbt/schema_0.jpg",
            "https://www-cdn.nebula-graph.io/nebula-website-5.0/images/blogs/ELT%20with%20dbt/modeling_omdb.jpg",
            "https://www-cdn.nebula-graph.io/nebula-website-5.0/images/blogs/ELT%20with%20dbt/modeling_omdb.jpg",
            "https://www-cdn.nebula-graph.io/nebula-website-5.0/images/blogs/ELT%20with%20dbt/4-schema_mapping_to_graph.jpg",
            "https://www-cdn.nebula-graph.io/nebula-website-5.0/images/blogs/ELT%20with%20dbt/5-%20168849779-4826f50e-ff87-4e78-b17f-076f91182c43.svg",
            "https://www-cdn.nebula-graph.io/nebula-website-5.0/images/blogs/ELT%20with%20dbt/starter-project-dbt-cli.jpg",
            "https://www-cdn.nebula-graph.io/nebula-website-5.0/images/blogs/ELT%20with%20dbt/6-%20ETL_dbt_nebulagraph_importer.jpg",
            "https://www-cdn.nebula-graph.io/nebula-website-5.0/images/blogs/ELT%20with%20dbt/transform_select_joins_user_watched_movies.jpg",
            "https://www-cdn.nebula-graph.io/nebula-website-5.0/images/blogs/ELT%20with%20dbt/7-%20reasoning_movie.jpg",
            "https://www-cdn.nebula-graph.io/blogs/supply-chain.jpg",
            "https://www-cdn.nebula-graph.io/blogs/multi-tenant-architecture.jpg",
            "https://www-cdn.nebula-graph.io/blogs/technical_preview_nebulagraph_enterprise_5.0.jpg",
            "https://www-cdn.nebula-graph.io/nebula-website-5.0/images/footer-nebulalogo.svg 1x, https://www-cdn.nebula-graph.io/nebula-website-5.0/images/footer-nebulalogo.svg 2x"
        ],
        "movies": [],
        "keywords": [],
        "meta_keywords": [
            ""
        ],
        "tags": null,
        "authors": [],
        "publish_date": null,
        "summary": "",
        "meta_description": "How could we model data in Tabular sources and ETL it to NebulaGraph? This article demonstrates an end-to-end example of doing so with dbt.",
        "meta_lang": "en",
        "meta_favicon": "https://www-cdn.nebula-graph.io/nebula-website-dist/favicon.png",
        "meta_site_name": "",
        "canonical_link": "https://www.nebula-graph.io/posts/graph-data-modeling-and-etl-dbt",
        "text": "How could we model data in Tabular sources and ETL it to NebulaGraph? This article demostrate us an end-to-end example doing so with dbt.\n\nTask\n\nImagine we are building a Knowledge Graph for a content provider web service with NebulaGraph, thus leveraging it to support a Knowledge Base QA system, Recommendation System, and Reasoning system.\n\nThe knowledge information persisted in different data sources from some Service APIs, Databases, Data Warehouses, or even some files in S3.\n\nWe need to:\n\nAnalyze data to extract needed knowledge\n\nModel the Graph based on relationships we care\n\nExtract the relationships and ingest them to NebulaGraph\n\nData Analysis\n\nAssume that we are fetching data from OMDB and MovieLens.\n\nOMDB is an open movie database, we now think of it as one of our services, and we can get the following information.\n\nMovies\n\nClassification of movies\n\nThe crew in the movie (director, action director, actors, post-production, etc.)\n\nMovie covers, promos, etc.\n\nMovieLens is an open dataset, we consider it as the user data of our services, the information we can obtain is:\n\nUsers\n\nMovies\n\nUser interaction on movie ratings\n\nGraph Modeling\n\nWe were building this Graph for a recommendation system and talked about some basic methods in this article, which:\n\nIn the Content-Base Filter method(CBF), the relationship of user-> movie, movie-> category, movie-> actor, and movie-> director are concerned.\n\nAnd the collaborative filtering approach is concerned with the relationship for user-> movie.\n\nThe recommendation reasoning service is concerned with all the above relationships.\n\nTo summarize, we need the following edges:\n\nwatched(rate(double))\n\nwith_genre\n\ndirected_by\n\nacted_by\n\nAccordingly, for the vertex types will be:\n\nuser(user_id)\n\nmovie(name)\n\nperson(name, birthdate)\n\ngenre(name)\n\nData Transform\n\nWith the source date being finalized, let's see how they could be mapped and transformed into the graph from.\n\nFrom OMDB\n\nFirst, there is the data in OMDB, which consists of many tables, such as the table all_movies, which stores all the movies and their names in different languages.\n\nmovie_id name language_iso_639_1 official_translation 1 Cowboy Bebop de 1 1 Cowboy Bebop en 1 2 Ariel - Abgebrannt in Helsinki de 0 3 Shadows in Paradise en 0 3 Im Schatten des Paradieses de 0 3 Schatten im Paradies de 1\n\nAnd the all_casts table holds all roles in the film industry.\n\nmovie_id person_id job_id role position 11 1 21 1 11 1 13 1 11 2 15 Luke Skywalker 1 11 3 15 Han Solo 3 11 4 15 Leia Organa 2\n\nBut the name and other information of each person here, as well as the position he/she holds in the film, are in separate tables.\n\njob_names\n\nFor example, 1 stands for writer and 2 stands for producer. Interestingly, like movie id and name, job_id to name is a one-to-many relationship, because the data in OMDB is multilingual.\n\njob_id name language_iso_639_1 1 Autoren de 1 Writing Department en 1 Departamento de redacción es 1 Département écriture fr 1 Scenariusz pl 2 Produzenten de 2 Production Department en\n\nall_people\n\nid name birthday deathday gender 1 George Lucas 1944-05-14 \\N 0 2 Mark Hamill 1951-09-25 \\N 0 3 Harrison Ford 1942-07-13 \\N 0 4 Carrie Fisher 1956-10-21 2016-12-27 1 5 Peter Cushing 1913-05-26 1994-08-11 0 6 Anthony Daniels 1946-02-21 \\N 0\n\nThis is a typical case in RDBMS where the data source is a table structure, so for the relationship movie <-[directed_by]-(person), it involves four tables all_movies, all_casts, all_people, job_names:\n\ndirected_by\n\nStarting from person_id in all_casts\n\nTo movie_id in all_casts\n\nWhere job_id is \"director\" in job_names\n\nmovie\n\nperson_id in all_casts\n\nName from all_movies by id, language is \"en\"\n\nperson\n\nmovie_id in all_casts\n\nName, birthday in all_people\n\nTill now, all tables we cared in OMDB are:\n\nFrom MovieLens dataset\n\nWhile the above is just about one source of data, in real scenarios, we also need to collect data from other sources and aggregate them. For example, now also need to extract knowledge from the MovieLens dataset.\n\nHere, the only relationship we utilize is: user -> movie.\n\nmovies.csv\n\nmovieId title genres 1 Toy Story (1995) Adventure 2 Jumanji (1995) Adventure 3 Grumpier Old Men (1995) Comedy 4 Waiting to Exhale (1995) Comedy\n\nratings.csv\n\nuserId movieId rating timestamp 1 1 4 964982703 1 3 4 964981247 1 6 4 964982224\n\nFrom the preview of the data in the two tables, naturally, we need one type of relationship: watched and vertex: user:\n\nwatched\n\nStarting from the userId in ratings.csv\n\nTo movieId in ratings.csv\n\nWith rating from rating in ratings.csv\n\nuser\n\nWith userId from ratings.csv\n\nHowever, you must have noticed that movieId in the MovieLens dataset and movie id in OMDB are two different systems, if we need to associate them, we need to convert movieId in MovieLens to movie id in OMDB, and the condition of association between them is movie title.\n\nHowever, by observation, we know that:\n\nthe titles in OMDB movies are multilingual\n\nthe titles in MovieLens have the year information like (1995) at the end of the title\n\nSo our final conclusion is\n\nwatched\n\nStarting from the userId in ratings.csv\n\nTo movieId in ratings.csv\n\nGet the movie title with movieId from movies.csv and find its movie_id from OMDB\n\nWhere we should match the title in language: English with the suffix of the year being removed\n\nWith rating from rating in ratings.csv\n\nuser\n\nWith userId from ratings.csv\n\nNow the modeling puts the two tables like this figure:\n\nGraph Modeling (Property Graph)\n\nTo summarize, we need to aggregate different tables (or CSV files in table form) from multiple data sources, such that the correspondence is shown in the figure: where the blue dashed line indicates the source of data information for the vertices in the graph, and the pink dashed line indicates the source of edge information.\n\nThen, we have to format the ids of individuals in different tables, for example, user_id, which is a self-incrementing number that we want to convert to a globally unique vertex_id. A convenient way to do this is to add a string prefix to the existing id, such as u_.\n\nEventually, for the relationship user -[watched]-> movie, we can process the table structure data as follows.\n\nuser_id rating title omdb_movie_id u_1 5 Seven (a.k.a. Se7en) 807 u_1 5 Star Wars: Episode IV - A New Hope 11 u_1 5 Star Wars: Episode IV - A New Hope 10 u_1 4 Mask, The 832 u_1 3 Mrs. Doubtfire 832\n\nWhere, in each row, three variables exist to construct the graph structure:\n\nuser vertex id\n\nmovie vertex id\n\nthe rating value of as the property of the watched edge\n\nTooling\n\nAt this point, we have completed the data analysis and graph modeling design, before we start the \"extract correlations, import graph database\", let's introduce the tools we will use.\n\n\"Extracting relationships\" can be simply considered as Extract and Transform in ETL, which is essentially the engineering of data mapping and transformation, and there are many different tools and open-source projects available on the market. Here we use one of my personal favorite tools: dbt.\n\ndbt\n\ndbt is an open-source data conversion tool with a very mature community and ecology, which can perform efficient, controlled, and high-quality data conversion work in most of the mainstream data warehouses, whether it is for ad-hoc tasks or complex orchestration, dbt can be very competent.\n\nOne of the features of dbt is that it uses a SQL-like language to describe the rules of data transformation. With GitOps, it is very elegant to collaborate and maintain complex data processing operations in large data teams. And the built-in data testing capabilities allow you to control the quality of your data and make it reproducible and controllable.\n\ndbt not only has many integrated subprojects but also can be combined with many other excellent open source projects (meltano, AirFlow, Amundsen, Superset, etc.) to form a set of modern data infrastructure systems, feel free to check my previous article: data lineage and metadata governance reference architecture https://siwei.io/en/data-lineage-oss-ref-solution, where the whole solution looks like:\n\nIn short, dbt is a command line tool written in python, and we can create a project folder, which contains a YAML formatted configuration file, to specify where the source information for the data transformation is and where the target is (where the processed data is stored, maybe Postgres, Big Query, Spark, etc.). In the data source, we use the YAML file along with the .SQL file to describe the information about \"what data to fetch from, how to do the transformation, and what to output\".\n\nYou can see that the information in the models/example is the core data transformation rules, and all the other data is metadata related to this transformation. DataOps.\n\nNotes.\n\nYou can refer to the dbt documentation to get a hands-on understanding of it: https://docs.getdbt.com/docs/get-started/getting-started-dbt-core\n\nNebulaGraph data ingestion\n\nAfter processing the data by dbt, we can get intermediate data that maps directly to different types of vertices, edges, and table structures of their attributes, either in the form of CSV files, tables in DWs, or even data frames in Spark, and there are different options for importing them into NebulaGraph, of which NebulaGraph Exchange, Nebula-Importer, and Nebula-Spark-Connector can be used to import the data.\n\nNotes.\n\nYou can learn more about the different tools for NebulaGraph data import at https://siwei.io/en/sketches/nebula-data-import-options to know how to choose one of them c.\n\nHere, I will use the simplest one, Nebula-Importer, as an example.\n\nNebula-Importer is an open-source tool written in Golang that compiles into a single file binary, it gets the correspondence of vertices and edges from a given CSV file to a NebulaGraph for reading and importing via a preconfigured YAML format file.\n\nNotes.\n\nNebula-Importer code: https://github.com/vesoft-inc/nebula-importer/\n\nNebula-Importer documentation: https://docs.nebula-graph.io/master/nebula-importer/use-importer/\n\ndbt + Nebula-Importer in\n\nNow let's use dbt + Nebula-Importer to end-to-end demonstrate how to extract, transform and import multiple data sources into NebulaGraph, the whole project code has been open-sourced, the repository is at https://github.com/wey-gu/movie-recommendation-dataset, feel free to check for details there.\n\nThe whole process is as follows.\n\nPreprocess and import raw data into the data warehouse(EL)\n\nUse dbt to transform the data (Transform), and export it to CSV files\n\nImport CSV into NebulaGraph using Nebula Importer (L)\n\nPreparing the dbt environment\n\ndbt is a python project, we install dbt and dbt-postgres in a virtual python3 environment.\n\nSetup env with dbt\n\ndbt is written in python, we could install it in a python virtual env, together with dbt-postgres, as we will use Postgres as the DW in this sample project.\n\npython3 -m venv .venv source .venv/bin/activate pip install dbt-postgres\n\nCreate a dbt project:\n\ndbt init dbt_project cd dbt_project\n\nLet's see the files in this project:\n\n$ tree . . |-- README.md |-- analyses |-- dbt_project.yml |-- macros |-- models | \\-- example | |-- my_first_dbt_model.sql | |-- my_second_dbt_model.sql | \\-- schema.yml |-- seeds |-- snapshots \\-- tests 7 directories, 5 files\n\nFinally, let's boostrap a Postgress as the DW, if you already have one, you may skip this step, please ensure the configurations and dbt-plugins are aligned if you chose to use your own DW.\n\ndocker run --rm --name postgres \\ -e POSTGRES_PASSWORD=nebula \\ -e POSTGRES_USER=nebula \\ -e POSTGRES_DB=warehouse -d \\ -p 5432:5432 postgres\n\nData download and preprocess\n\nLet's create a folder named raw_data and change directory to it.\n\nmkdir -p raw_data cd raw_data\n\nAnd we asummed it's under our dbt project:\n\ntree .. .. |-- README.md |-- analyses |-- dbt_project.yml |-- macros |-- models | \\-- example | |-- my_first_dbt_model.sql | |-- my_second_dbt_model.sql | \\-- schema.yml |-- raw_data |-- seeds |-- snapshots \\-- tests 8 directories, 5 files\n\nDownload and decompress the omdb data:\n\nwget www.omdb.org/data/all_people.csv.bz2 wget www.omdb.org/data/all_people_aliases.csv.bz2 wget www.omdb.org/data/people_links.csv.bz2 wget www.omdb.org/data/all_casts.csv.bz2 wget www.omdb.org/data/job_names.csv.bz2 wget www.omdb.org/data/all_characters.csv.bz2 wget www.omdb.org/data/movie_categories.csv.bz2 wget www.omdb.org/data/movie_keywords.csv.bz2 wget www.omdb.org/data/category_names.csv.bz2 wget www.omdb.org/data/all_categories.csv.bz2 wget www.omdb.org/data/all_movie_aliases_iso.csv.bz2 bunzip2 *.bz2\n\nThen for then MovieLens dataset:\n\nwget https://files.grouplens.org/datasets/movielens/ml-latest-small.zip unzip ml-latest-small.zip rm *.zip\n\nBefore we do the Transform with dbt, we do some simple preprocess and then put them under seeds:\n\ngrep -v '\\\\\"' raw_data/all_movie_aliases_iso.csv > seeds/all_movie_aliases_iso.csv grep -v '\\\\\"' raw_data/all_casts.csv > seeds/all_casts.csv grep -v '\\\\\"' raw_data/all_characters.csv > seeds/all_characters.csv grep -v '\\\\\"' raw_data/all_people.csv > seeds/all_people.csv grep -v '\\\\\"' raw_data/category_names.csv > seeds/category_names.csv grep -v '\\\\\"' raw_data/job_names.csv > seeds/job_names.csv cp raw_data/movie_categories.csv seeds/movie_categories.csv cp raw_data/movie_keywords.csv seeds/movie_keywords.csv cp raw_data/all_categories.csv seeds/all_categories.csv cp raw_data/ml-latest-small/ratings.csv seeds/movielens_ratings.csv cp raw_data/ml-latest-small/movies.csv seeds/movielens_movies.csv\n\nWith above files being placed, we could load them into DW in one command:\n\nRefer to the documentations of dbt seeds https://docs.getdbt.com/docs/build/seeds\n\ndbt seed\n\nIt may take a while if you like me are using a local postgres, and it should be faster in produection level case(i.e. load to Big Query from file in Cloud Storage), it should be like this:\n\n$ dbt seed 05:58:27 Running with dbt=1.3.0 05:58:27 Found 2 models, 4 tests, 0 snapshots, 0 analyses, 289 macros, 0 operations, 11 seed files, 0 sources, 0 exposures, 0 metrics 05:58:28 05:58:28 Concurrency: 8 threads (target='dev') 05:58:28 05:58:28 1 of 11 START seed file public.all_casts ....................................... [RUN] ... 07:10:11 1 of 11 OK loaded seed file public.all_casts ................................... [INSERT 1082228 in 4303.78s] 07:10:11 07:10:11 Finished running 11 seeds in 1 hours 11 minutes and 43.93 seconds (4303.93s). 07:10:11 07:10:11 Completed successfully 07:10:11 07:10:11 Done. PASS=11 WARN=0 ERROR=0 SKIP=0 TOTAL=11\n\nCompose the Transform model\n\nWe create transform under models:\n\nmkdir models/movie_recommedation touch models/movie_recommedation/user_watched_movies.sql touch models/movie_recommedation/schema.yml\n\nThe files are like:\n\n$ tree models models \\-- movie_recommedation |-- user_watched_movies.sql \\-- schema.yml\n\nNow there is only one transform rule under this model: to handle the edge of user_watched_movies in the user_watched_movies.sql\n\nAs we planned to ouput three columns: user_id, movie_id, rating, thus the schema.yml is like:\n\nversion: 2 models: - name: user_watched_movies description: \"The edges between users and movies they have watched\" columns: - name: user_id description: \"user id\" tests: - not_null - name: movie_id description: \"movie id\" tests: - not_null - name: rating description: \"rating given by user to movie\" tests: - not_null\n\nPlease be noted the tests is about the validation and constraint of the data, with which, we could control the data quality quite easy. And here not_null ensures there is no NULL if tests are performed.\n\nThen, let's compose the user_watched_movies.sql:\n\n{{ config(materialized='table') }} WITH user_watched_movies AS( SELECT moveielens_ratings.\"userId\", moveielens_ratings.\"movieId\", moveielens_ratings.rating, REGEXP_REPLACE(moveielens_movies.title, ' \\(\\d{4}\\)$', '') AS title, moveielens_movies.genres AS movielens_genres FROM moveielens_ratings JOIN moveielens_movies ON moveielens_movies.\"movieId\" = moveielens_ratings.\"movieId\" ) SELECT concat('u_',user_watched_movies.\"userId\") AS user_id, user_watched_movies.rating, user_watched_movies.title, all_movie_aliases_iso.\"movie_id\" AS OMDB_movie_id, user_watched_movies.movielens_genres FROM user_watched_movies JOIN all_movie_aliases_iso ON user_watched_movies.title LIKE CONCAT(all_movie_aliases_iso.name, '%') AND all_movie_aliases_iso.language_iso_639_1 = 'en'\n\nAnd what this SQL does is the part marked by the green circle:\n\nSelect user id, movie id, rating, movie title (remove the year part) from moveielens_ratings and save as the intermediate table of user_watched_movies\n\nmovie title is JOINed from moveielens_movies, obtained by the same matching condition as movie_id\n\nSelect user id (prefix u_), rating, title, OMDB_movie_id from user_watched_movies\n\nOMDB_movie_id is JOINed from all_movie_aliases_iso, obtained by matching the Chinese and English titles of OMDB movies by similar movie names\n\noutput the final fields\n\nTips: we could add LIMIT to debug the SQL query fast from a Postgres Console\n\nThen we could run it from dbt to transform and test the rule:\n\ndbt run -m user_watched_movies\n\nAfter that, we should be able to see a table after the Transform in Postgres (DW).\n\nSimilarly, following the same method for all other parts of the Transform rules, we could have other models:\n\n$ tree models models \\-- movie_recommedation |-- acted_by.sql |-- directed_by.sql |-- genres.sql |-- movies.sql |-- people.sql |-- schema.yml |-- user_watched_movies.sql \\-- with_genre.sql\n\nThen run them all:\n\ndbt run -m acted_by dbt run -m directed_by dbt run -m with_genre dbt run -m people dbt run -m genres dbt run -m movies\n\nExport data to CSV\n\nIn fact, NebulaGraph Exchange itself supports directly importing many data sources (Postgres, Clickhouse, MySQL, Hive, etc.) into NebulaGraph, but in this example, the amount of data we process is very small for NebulaGraph, so we just go with the most lightweight one: Nebula-Importer. Nebula-Importer can only CSV files, so we are doing so.\n\nFirst, we enter the Postgres console and execute the COPY command\n\nRefer to Postgres documentation: https://www.postgresql.org/docs/current/sql-copy.html\n\nCOPY acted_by TO '/tmp/acted_by.csv' WITH DELIMITER ',' CSV HEADER; COPY directed_by TO '/tmp/directed_by.csv' WITH DELIMITER ',' CSV HEADER; COPY with_genre TO '/tmp/with_genre.csv' WITH DELIMITER ',' CSV HEADER; COPY people TO '/tmp/people.csv' WITH DELIMITER ',' CSV HEADER; COPY movies TO '/tmp/movies.csv' WITH DELIMITER ',' CSV HEADER; COPY genres TO '/tmp/genres.csv' WITH DELIMITER ',' CSV HEADER; COPY user_watched_movies TO '/tmp/user_watched_movies.csv' WITH DELIMITER ',' CSV;\n\nThen copy the CSV files into to_nebulagraph\n\nmkdir -p to_nebulagraph docker cp postgres:/tmp/. to_nebulagraph/\n\nIngest data into NebulaGraph\n\nBootstrap a NebulaGraph cluster\n\nWe can use Nebula-Up to have a NebulaGraph playground cluster with the oneliner.\n\nNote:\n\nNebula-UP: https://github.com/wey-gu/nebula-up\n\nDataset repository: https://github.com/wey-gu/movie-recommendation-dataset\n\ncurl -fsSL nebula-up.siwei.io/install.sh | bash\n\nDefine the Data Schema\n\nFirst, we need to create a graph space, and then create tag(type of vertex) and edge type on it:\n\nAccess the the Nebula-Console(CLI client for NebulaGraph):\n\n~/.nebula-up/console.sh\n\nRun the following DDL(Data Definiation Language):\n\nCREATE SPACE moviegraph(partition_num=10,replica_factor=1,vid_type=fixed_string(32)); :sleep 20 USE moviegraph; CREATE TAG person(name string, birthdate string); CREATE TAG movie(name string); CREATE TAG genre(name string); CREATE TAG user(user_id string); CREATE EDGE acted_by(); CREATE EDGE directed_by(); CREATE EDGE with_genre(); CREATE EDGE watched(rate float); exit\n\nCreate a Nebula-Importer conf file\n\nThis conf is a YAML file that describes the correspondence between the CSV file and the vertex or edge data in the cluster.\n\nPlease refer to the document: https://docs.nebula-graph.io/master/nebula-importer/use-importer/ for details.\n\nI already created one for it, which can be downloaded at https://github.com/wey-gu/movie-recommendation-dataset/blob/main/nebula-importer.yaml.\n\nHere, we will directly download the configuration file.\n\nNote that this file should not be part of the dbt project file.:\n\ncd .. wget https://raw.githubusercontent.com/wey-gu/movie-recommendation-dataset/main/nebula-importer.yaml\n\nIngesting the data\n\nLet's use the Nebula-Importer in docker to avoid any installation:\n\ndocker run --rm -ti \\ --network=nebula-net \\ -v ${PWD}:/root/ \\ -v ${PWD}/dbt_project/to_nebulagraph/:/data \\ vesoft/nebula-importer:v3.2.0 \\ --config /root/nebula-importer.yaml\n\nAfter it's executed, all data are in NebulaGraph, and we could check the data from Nebula-Console:\n\nFirst, access the console again:\n\n~/.nebula-up/console.sh\n\nEnter the graph space and execute SHOW STATS\n\nUSE moviegraph; SHOW STATS;\n\nThe result should be like:\n\n(root@nebula) [moviegraph]> SHOW STATS; + | Type | Name | Count | + | \"Tag\" | \"genre\" | 14397 | | \"Tag\" | \"movie\" | 20701 | | \"Tag\" | \"person\" | 263907 | | \"Tag\" | \"user\" | 610 | | \"Edge\" | \"acted_by\" | 673763 | | \"Edge\" | \"directed_by\" | 101949 | | \"Edge\" | \"watched\" | 31781 | | \"Edge\" | \"with_genre\" | 194009 | | \"Space\" | \"vertices\" | 299615 | | \"Space\" | \"edges\" | 1001502 | + Got 10 rows (time spent 1693/15136 us)\n\nWith Nebula-Studio, we can also explore this graph in the visual interface, for example, by executing this query, we could see the reason why it recommended the movie with id 1891 to the user with id u_124?\n\nFIND NOLOOP PATH FROM \"u_124\" TO \"1891\" over * BIDIRECT UPTO 4 STEPS yield path as `p` | LIMIT 20\n\nThe result could be: Most of the cast and crew of the once-favorite Star Wars movies are also involved in this and the same \"Oscar-winning\" and \"classic\" movie.\n\nIn another article, I used the same graph to demostrate the application of more graph databases and graph algorithms in recommendation systems. If you are interested, please read: https://siwei.io/recommendation-system-with-graphdb/.\n\nSummary\n\nWhen we plan to leverage graph databases for massive data to transform knowledge and analyze insights, the first step is often to transform, process, and model multiple data sources into graph data. For beginners who have no idea where to start, a feasible idea is to start from all relevant information, to picture the most concerned relationship, and then to list the vertices that can be obtained, as well as the required perpertices attached. After determining the initial modeling, you can use the ETL tool to clean the original data, ETL into table structure which will be mapped to the graph, and finally, use the import tool to import NebulaGraph for further model iterations.\n\nWith the help of dbt, we can version control, test, iterate our modeling and data transformation, and gradually evolve and enrich the constructed knowledge graph with grace."
    }
}
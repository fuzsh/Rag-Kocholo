{
    "id": "correct_foundationPlace_00002_1",
    "rank": 51,
    "data": {
        "url": "https://videogamehistorian.wordpress.com/tag/ibm/",
        "read_more_link": "",
        "language": "en",
        "title": "They Create Worlds",
        "top_image": "https://s0.wp.com/i/blank.jpg",
        "meta_img": "https://s0.wp.com/i/blank.jpg",
        "images": [
            "https://videogamehistorian.wordpress.com/wp-content/uploads/2014/10/ibm1401_tapesystem_mwhite.jpg?w=620&h=250",
            "https://videogamehistorian.wordpress.com/wp-content/uploads/2014/10/ibm_7090_computer.jpg?w=300&h=232",
            "https://videogamehistorian.wordpress.com/wp-content/uploads/2014/11/honeywell200.jpg?w=300&h=185",
            "https://videogamehistorian.wordpress.com/wp-content/uploads/2014/11/norris_cray.png?w=300&h=152",
            "https://videogamehistorian.wordpress.com/wp-content/uploads/2014/11/system360.png?w=300&h=297",
            "https://videogamehistorian.wordpress.com/wp-content/uploads/2014/11/learson_1.jpg?w=207&h=300",
            "https://videogamehistorian.wordpress.com/wp-content/uploads/2014/06/forrester_taylor_cutcut2.jpg?w=620",
            "https://videogamehistorian.wordpress.com/wp-content/uploads/2014/05/200908311113506364_0.jpg?w=300&h=225",
            "https://videogamehistorian.wordpress.com/wp-content/uploads/2014/06/ibms_10_billion_machine.jpg?w=300&h=197",
            "https://videogamehistorian.wordpress.com/wp-content/uploads/2014/06/102631231-03-01.jpg?w=300&h=225",
            "https://videogamehistorian.wordpress.com/wp-content/uploads/2014/06/bardeen_shockley_brattain_1948.jpg?w=300&h=238",
            "https://videogamehistorian.wordpress.com/wp-content/uploads/2014/06/tumblr_mrf93w8xjq1s6mxo0o1_500.jpg?w=300&h=204",
            "https://videogamehistorian.wordpress.com/wp-content/uploads/2014/06/vs-dec-pdp-1.jpg?w=300&h=233",
            "https://videogamehistorian.wordpress.com/wp-content/uploads/2014/04/1951_univac_large.jpg?w=300&h=227",
            "https://videogamehistorian.wordpress.com/wp-content/uploads/2014/05/univac-1101brl61-0901.jpg?w=300&h=217",
            "https://videogamehistorian.wordpress.com/wp-content/uploads/2014/04/ibm701console.jpg?w=300&h=237",
            "https://videogamehistorian.wordpress.com/wp-content/uploads/2014/04/6703ph02.jpg?w=300&h=243",
            "https://videogamehistorian.wordpress.com/wp-content/uploads/2014/04/ibm-650-drum.jpg?w=300&h=196",
            "https://videogamehistorian.wordpress.com/wp-content/uploads/2014/05/ibm_702.gif?w=300&h=210",
            "https://videogamehistorian.wordpress.com/wp-content/uploads/2014/02/102680080-03-01.jpg?w=300&h=225",
            "https://videogamehistorian.wordpress.com/wp-content/uploads/2014/03/23593-004-d5156f2c.jpg?w=300&h=150",
            "https://videogamehistorian.wordpress.com/wp-content/uploads/2014/03/aiken.jpeg?w=248&h=300",
            "https://videogamehistorian.wordpress.com/wp-content/uploads/2014/03/zuse.jpg?w=232&h=300",
            "https://videogamehistorian.wordpress.com/wp-content/uploads/2014/03/z3_1.jpg?w=300&h=237",
            "https://videogamehistorian.wordpress.com/wp-content/uploads/2014/03/colossus.gif?w=300&h=198",
            "https://videogamehistorian.wordpress.com/wp-content/uploads/2014/03/tommy_flowers.jpg?w=221&h=300",
            "https://videogamehistorian.wordpress.com/wp-content/uploads/2014/03/eniac_image_2.jpg?w=300&h=202",
            "https://videogamehistorian.wordpress.com/wp-content/uploads/2014/03/atanasoff-berry-computer.jpg?w=300&h=231",
            "https://videogamehistorian.wordpress.com/wp-content/uploads/2014/04/g.jpg?w=300&h=225",
            "https://videogamehistorian.wordpress.com/wp-content/uploads/2014/04/manchester_mark2.jpg?w=300&h=238",
            "https://videogamehistorian.wordpress.com/wp-content/uploads/2014/04/978.jpg?w=300&h=237",
            "https://videogamehistorian.wordpress.com/wp-content/uploads/2014/04/cambridge.jpg?w=300&h=160",
            "https://videogamehistorian.wordpress.com/wp-content/uploads/2014/02/766px-human_computers_-_dryden.jpg?w=300&h=234",
            "https://videogamehistorian.wordpress.com/wp-content/uploads/2014/02/charles_babbage.jpg?w=620",
            "https://videogamehistorian.wordpress.com/wp-content/uploads/2014/02/10303265.jpg?w=300&h=243",
            "https://videogamehistorian.wordpress.com/wp-content/uploads/2014/02/cb000184_1907_office_with_burroughs_model_6_om.jpg?w=300&h=234",
            "https://videogamehistorian.wordpress.com/wp-content/uploads/2014/02/burroughs.png?w=620",
            "https://videogamehistorian.wordpress.com/wp-content/uploads/2014/02/john-h-patterson.jpg?w=240&h=300",
            "https://videogamehistorian.wordpress.com/wp-content/uploads/2014/02/416px-hollerith.jpg?w=208&h=300",
            "https://videogamehistorian.wordpress.com/wp-content/uploads/2014/02/charles_ranlett_flint1.jpg?w=215&h=300",
            "https://videogamehistorian.wordpress.com/wp-content/uploads/2014/02/thomas_watson.jpg?w=233&h=300",
            "https://1.gravatar.com/avatar/ab3d91d55712077b6aa2f4844f4ed0e37b9bb101f70016e70deb1fec3e0834e5?s=48&d=identicon&r=G",
            "https://1.gravatar.com/avatar/ab3d91d55712077b6aa2f4844f4ed0e37b9bb101f70016e70deb1fec3e0834e5?s=48&d=identicon&r=G",
            "https://0.gravatar.com/avatar/fc05c2e73f5ea8c77b063704ec4459f84e1e9b9a884d52e39e573e67ead732dc?s=48&d=identicon&r=G",
            "https://1.gravatar.com/avatar/ab3d91d55712077b6aa2f4844f4ed0e37b9bb101f70016e70deb1fec3e0834e5?s=48&d=identicon&r=G",
            "https://1.gravatar.com/avatar/ab3d91d55712077b6aa2f4844f4ed0e37b9bb101f70016e70deb1fec3e0834e5?s=48&d=identicon&r=G",
            "https://1.gravatar.com/avatar/ab3d91d55712077b6aa2f4844f4ed0e37b9bb101f70016e70deb1fec3e0834e5?s=48&d=identicon&r=G",
            "https://0.gravatar.com/avatar/fc05c2e73f5ea8c77b063704ec4459f84e1e9b9a884d52e39e573e67ead732dc?s=48&d=identicon&r=G",
            "https://1.gravatar.com/avatar/ab3d91d55712077b6aa2f4844f4ed0e37b9bb101f70016e70deb1fec3e0834e5?s=48&d=identicon&r=G",
            "https://s2.wp.com/i/logo/wpcom-gray-white.png",
            "https://s2.wp.com/i/logo/wpcom-gray-white.png",
            "https://pixel.wp.com/b.gif?v=noscript"
        ],
        "movies": [],
        "keywords": [],
        "meta_keywords": [
            ""
        ],
        "tags": null,
        "authors": [],
        "publish_date": "2014-11-08T17:09:18-06:00",
        "summary": "",
        "meta_description": "Posts about IBM written by videogamehistorian",
        "meta_lang": "en",
        "meta_favicon": "https://s1.wp.com/i/favicon.ico",
        "meta_site_name": "They Create Worlds",
        "canonical_link": "https://videogamehistorian.wordpress.com/tag/ibm/",
        "text": "The computer began life in the 1940s as a scientific device designed to perform complex calculations and solve difficult equations. In the 1950s, the United States continued to fund scientific computing projects at government organizations, defense contractors, and universities, many of them based around the IAS architecture derived from the EDVAC and created by John von Neumann’s team at Princeton. Some of the earliest for-profit computer companies emerged out of this scientific work such as the previously discussed Engineering Research Associates, the Hawthorne, California-based Computer Research Corporation, which spun out of a Northrup Aircraft project to build a computer for the Air Force in 1952, and the Pasadena-based ElectroData Corporation, which spun out of the Consolidated Engineering Corporation that same year. All of these companies remained fairly small and did not sell many computers.\n\nInstead, it was Remington Rand that identified the future path of computing when it launched the UNIVAC I, which was adopted by businesses to perform data processing. Once corporate America understood the computer to be a capable business machine and not just an expensive calculator, a wide array of office equipment and electronics companies entered the computer industry in the mid 1950s, often buying out the pioneering computer startups to gain a foothold. Remington Rand dominated this market at first, but as discussed previously, IBM soon vaulted ahead as it acquired computer design and manufacturing expertise participating in the SAGE project and unleashed its world-class sales and service organizations. Remington Rand attempted to compensate by merging with Sperry Gyroscope, which had both a strong relationship with the military and a more robust sales force, to form Sperry Rand in 1955, but the company never seriously challenged IBM again.\n\nWhile IBM maintained its lead in the computer industry, however, by the beginning of the 1960s the company faced threats to its dominance at both the low end and the high end of the market from innovative machines based around new technologies like the transistor. Fearing these new challengers could significantly damage IBM, Tom Watson Jr. decided to bet the company on an expensive and technically complex project to offer a complete line of compatible computers that could not only be tailored to a customer’s individual’s needs, but could also be easily modified or upgraded as those needs changed over time. This gamble paid off handsomely, and by 1970 IBM controlled well over seventy percent of the market, with most of the remainder split among a group of competitors dubbed the “seven dwarfs” due to their minuscule individual market shares. In the process, IBM succeeded in transforming the computer from a luxury item only operated by the largest firms into a necessary business appliance as computers became an integral part of society.\n\nNote: Yet again we have a historical interlude post that summarizes key events outside of the video game industry that nevertheless had a significant impact upon it. The information in this post is largely drawn from Computer: A History of the Information Machine by Martin Campbell-Kelly and William Aspray, A History of Modern Computing by Paul Ceruzzi, Forbes Greatest Technology Stories: Inspiring Tales of the Entrepreneurs and Inventors Who Revolutionized Modern Business by Jeffrey Young, IBM’s Early Computers by Charles Bashe, Lyle Johnson, John Palmer, and Emerson Pugh. and Building IBM: Shaping an Industry and Its Technology by Emerson Pugh.\n\nIBM Embraces the Transistor\n\nThe IBM 1401, the first mainframe to sell over 10,000 units\n\nThroughout most of its history in computers, IBM has been known more for evolution than revolution. Rarely first with a new concept, IBM excelled at building designs based around proven technology and then turning its sales force loose to overwhelm the competition. Occasionally, however, IBM engineers have produced important breakthroughs in computer design. Perhaps none of these were more significant than the company’s invention of the disk drive.\n\nOn the earliest computers, mass data storage was accomplished through two primary methods: magnetic tape or magnetic drums. Tape could hold a large amount of data for the time, but it could only be read serially, and it was a fragile medium. Drums were more durable and had the added benefit of being random access — that is any point of data on the drum could be read at any time — but they were low capacity and expensive. As early as the 1940s, J. Presper Eckert had explored using magnetic disks rather than drums, which would be cheaper and feature a greater storage capacity due to a larger surface area, but there were numerous technical hurdles that needed to be ironed out. Foremost among these was the technology to read the disks. A drum memory array used rigid read-write heads that could be readily secured, though at high cost. A disk system required a more delicate stylus to read the drives, and the constant spinning of the disk created a high risk that the stylus would make contact with and damage it.\n\nThe team that finally solved these problems at IBM worked not at the primary R&D labs in Endicott or Poughkeepsie, but rather a relatively new facility in San Jose, California, led by IBM veteran Reynold Johnson that had been established in 1952 as an advanced technologies research center free of the influence of the IBM sales department, which had often shut down projects with no immediate practical use. One of the lab’s first projects was to improve storage for IBM’s existing tabulating equipment. This task fell to a team led by Arthur Critchlow, who decided based on customer feedback to develop a new random access solution that would allow IBM’s tabulators and low-end computers to not only be useful for data processing, but also for more complicated jobs like inventory management. After testing a wide variety of memory solutions, Critchlow’s team settled on the magnetic disk as the only viable solution, partially inspired by a similar project at the National Bureau of Standards on which an article had been published in August 1952.\n\nTo solve the stylus problem on the drive, Critchlow’s team attached a compressor to the unit that would pump a thin layer of air between the disk and the head. Later models would take advantage of a phenomenon known as the “boundry layer” in which the fast motion of the disks would generate the air cushion themselves. After experimenting with a variety of head types and positions throughout 1953 and 1954, the team was ready to complete a final design. Announced in 1956 as the Model 305 Disk Storage Unit and later renamed RAMAC (for Random Access Memory Accounting Machine), IBM’s first disk drive consisted of fifty 24-inch diameter aluminum disks rotating at 1200 rpm with a storage capacity of five million characters. Marketed as an add-on to the IBM 650, RAMAC revolutionized data processing by eliminating the time consuming process of manually sorting information and provided the first compelling reason for small and mid-sized firms to embrace computers and eliminate electro-mechanical tabulating equipment entirely.\n\nThe IBM 7090, the company’s first transistorized computer\n\nIn August 1958, IBM introduced its latest scientific computer, the IBM 709, which improved on the functionality of the IBM 704. The 709 continued to depend on vacuum tubes, however, even as competitors were starting to bring the first transistorized computers to market. While Tom Watson, Jr. and his director of engineering, Wally McDowell, were both excited by the possibilities of transistors from the moment they first learned about them and as early as 1950 charged Ralph Palmer’s Poughkeepsie laboratory to begin working with the devices, individual project managers continued to have the final authority in choosing what parts to use in their machines, and many of them continued to fall back on the more familiar vacuum tube. In the end, Tom Watson, Jr. had to issue a company-wide mandate in October 1957 that transistors were to be incorporated into all new projects. In the face of this resistance, Palmer felt that IBM needed a massive project to push its solid-state designs forward, something akin to what Project SAGE had done for IBM’s efforts with vacuum tubes and core memory. He therefore teamed with Steve Dunwell, who had spent part of 1953 and 1954 in Washington D.C. assessing government computing requirements, to propose a high-speed computer tailored to the ever-increasing computational needs of the military-industrial complex. A contract was eventually secured with the National Security Agency, and IBM approved “Project Stretch” in August 1955, which was formally established in January 1956 with Dunwell in charge.\n\nProject Stretch experienced a long, difficult, and not completely successful development cycle, but it did achieve Palmer’s goals of greatly improving IBM’s solid-state capabilities, with particularly important innovations including a much faster core memory and a “drift transistor” that was faster than the surface-barrier transistor used in early solid-state computing projects like the TX-0. As work on Stretch dragged on, however, these advances were first introduced commercially through another product. In response to Sputnik, the United States Air Force quickly initiated a new Ballistic Missile Early Warning System (BMEWS) project that, like SAGE, would rely on a series of linked computers. The Air Force mandated, however, that these computers incorporate transistors, so Palmer offered to build a transistorized version of the 709 to meet the project’s needs. The resulting IBM 7090 Data Processing System, deployed in November 1959 as IBM’s first transistorized computer, provided a six-fold increase in performance over the 709 at only one-third additional cost. In 1962, an upgraded version dubbed the 7094 was released with a price of roughly $2 million. Both computers were well-received, and IBM sold several hundred of them.\n\nDespite the success of its mainframe computer business, IBM in 1960 still derived the majority of its sales from the traditional punched-card business. While some larger organizations were drawn to the 702 and 705 business computers, their price kept them out of reach of the majority of IBM’s business customers. Some of these organizations had embraced the low-cost 650 as a data processing solution, leading to over 800 installations of the computer by 1958, but it was actually more expensive and less reliable than IBM’s mainline 407 electric accounting machine. The advent of the transistor, however, finally provided the opportunity for IBM to leave its tabulating business behind for good.\n\nThe impetus for a stored-program computer that could displace traditional tabulating machines initially came from Europe, where IBM did not sell its successful 407 due to import restrictions and high tooling costs. In 1952, a competitor called the French Bull Company introduced a new calculating machine, the Bull Gamma 3, that used delay-line memory to provide greater storage capacity at a cheaper price than IBM’s electronic calculators and could be joined with a card reader to create a faster accounting machine than anything IBM offered in the European market. Therefore, IBM’s French and German subsidiaries began lobbying for a new accounting machine to counter this threat. This led to the launch of two projects in the mid-1950s: the modular accounting calculator (MAC) development project in Poughkeepsie that birthed the 608 electronic calculator and the expensive and relatively unsuccessful 7070 transistorized computer, and the Worldwide Accounting Machine (WWAM) project run out of France and Germany to create an improved traditional accounting machine for the European market.\n\nWhile the WWAM project had been initiated in Europe, it was soon reassigned to Endicott when the European divisions proved unable to come up with an accounting machine that could meet IBM’s cost targets. To solve this problem, Endicott engineer Francis Underwood proposed that a low-cost computer be developed instead. Management approved this concept in early 1958 under the name SPACE — for Stored Program Accounting and Calculating Equipment — and formally announced the product in October 1959 as the IBM 1401 Data Processing System. With a rental cost of only $2,500 a month (roughly equivalent to a purchase price of $150,000), the transitorized 1401 proved much faster and more reliable than an IBM 650 at a fraction of the cost and was only slightly more expensive than a mid-range 407 accounting machine setup. More importantly, it shipped with a new chain printer that could output 600 lines per minute, far more than the 150 lines per minute produced by the 407, which relied on obsolete prewar technology. First sold in 1960, IBM projected that it would sell roughly 1,000 1401 computers over its entire lifetime, but its combination of power and price proved irresistible, and by the end of 1961 over 2,000 machines had already been installed. IBM would eventually deploy 12,000 1401 computers before it was officially withdrawn in 1971. Powered by the success of the 1401, IBM’s computer sales finally equaled the sales of punch card products in 1962 and then quickly eclipsed them. No computer model had ever approached the success of the 1401 before, and as IBM rode the machine to complete dominance of the mainframe industry in the early 1960s, the powder-blue casing of the machine soon inspired a new nickname for the company: Big Blue.\n\nThe Dwarfs\n\nThe Honeywell 200, which competed with IBM’s 1401 and threatened to destroy its low-end business\n\nIn the wake of Remington Rand’s success with the UNIVAC I, more than a dozen old-line firms flocked to the new market. Companies like Monroe Calculating, Bendix, Royal, Underwood, and Philco rushed to provide computers to the business community, but one by one they fell by the wayside. Of these firms, Philco probably stood the best chance of being successful due to its invention of the surface barrier transistor, but while its Transac S-1000 — which began life in 1955 as an NSA project called SOLO to build a transistorized version of the UNIVAC 1103 — and S-2000 computers were both capable machines, the company ultimately decided it could not keep up with the fast pace of technological development and abandoned the market like all the rest. By 1960, only five established companies and one computer startup joined Sperry Rand in attempting to compete with IBM in the mainframe space. While none of these firms ever succeeded in stealing much market share from Big Blue, most of them found their own product niches and deployed some capable machines that ultimately forced IBM to rethink some of its core computer strategies.\n\nOf the firms that challenged IBM, electronics giants GE and RCA were the largest, with revenues far exceeding the computer industry market leader, but in a way their size worked against them. Since neither computers nor office equipment were among either firm’s core competences, nor integral to either firm’s future success, they never fully committed to the business and therefore never experienced real success. Unsurprisingly, they were the first of the seven dwarfs to finally call it quits, with GE selling off its computer business in 1970 and RCA following suit in 1971. Burroughs and NCR, the companies that had long dominated the adding machine and cash register businesses respectively, both entered the market in 1956 after buying out a small startup firm — ElectroData and Computer Research Corporation respectively — and managed to remain relevant by creating computers specifically tailored to their preexisting core customers, the banking sector for Burroughs and the retail sector for NCR. Sperry Rand ended up serving niche markets as well after failing to compete effectively with IBM, experiencing success in fields such as airline reservation systems. The biggest threat to IBM’s dominance in this period came from two Minnesota companies: Honeywell and Control Data Corporation (CDC).\n\nUnlike the majority of the companies that persisted in the computer industry, Honeywell came not from the office machine business, but from the electronic control industry. In 1883, a man named Albert Butz created a device called the “damper flapper” that would sense when a house was becoming cold and cause the flapper on a coal furnace to rise, thus fanning the flames and warming the house. Butz established a company that did business under a variety of names over the next few years to market his innovation, but he had no particular acumen for business. In 1891, William Sweatt took over the company and increased sales through door-to-door selling and direct marketing. In 1909 the company introduced the first controlled thermostat, sold as the “Minnesota Regulator,” and in 1912 Sweatt changed the name of the company to the Minnesota Heat Regulator Company. In 1927, a rival firm, Mark C. Honeywell’s Honeywell Heating Specialty Company of Wabash, Indiana, bought out Minnesota Heat Regulator to form the Honeywell-Minneapolis Regulator Company with Honeywell as President and Sweatt as chairman. The company continued to expand through acquisitions over the next decade and weathered the Great Depression relatively unscathed.\n\nIn 1941, Harold Sweatt, who had succeeded Honeywell as president in 1934, parlayed his company’s expertise in precision measuring devices into several lucrative contracts with the United States military, emerging from World War II as a major defense contractor. Therefore, the company was approached by fellow defense contractor Raytheon to establish a joint computer subsidiary in 1954. Incorporated as Datamatic Corporation the next year, the computer company became a wholly-owned subsidiary of Honeywell in 1957 when Raytheon followed so many other companies in exiting the computer industry. Honeywell delivered its first mainframe, the Datamatic 1000, that same year, but the computer relied on vacuum tubes and was therefore already obsolete by the time it hit the market. Honeywell temporarily withdrew from the business and went back to the drawing board. After IBM debuted the 1401, Honeywell triumphantly returned to the business with the H200, which not only took advantage of the latest technology to outperform the 1401 at a comparable price, but also sported full compatibility with IBM’s wildly successful machine, meaning companies could transfer their existing 1401 programs without needing to make any adjustments. Announced in 1963, the H200 threatened IBM’s control of the low-end of the mainframe market.\n\nWilliam Norris (l) and Seymour Cray, the principle architects of the Control Data Corporation\n\nWhile Honeywell chipped away at IBM from the bottom of the market, computer startup Control Data Corporation (CDC) — the brainchild of William Norris — threatened to do the same from the top. Born in Red Cloud, Nebraska, and raised on a farm, Norris became an electronics enthusiast at an early age, building mail-order radio kits and becoming a ham radio operator. After graduating from the University of Nebraska in 1932 with a degree in electrical engineering, Norris was forced to work on the family farm for two years due to a lack of jobs during the Depression before joining Westinghouse in 1934 to work in the sales department of the company’s x-ray division. Norris began doing work for the Navy’s Bureau of Ordinance as a civilian in 1940 and enjoyed the work so much that he joined the Naval Reserve and was called to duty at the end of 1941 at the rank of lieutenant commander. Norris served as part of the CSAW codebreaking operation and became one of the principle advocates for and co-founders of Engineering Research Associates after the war. By 1957, Norris was feeling stifled by the corporate environment at ERA parent company Sperry Rand, so he left to establish CDC in St. Paul, Minnesota.\n\nNorris provided the business acumen at CDC, but the company’s technical genius was a fellow engineer named Seymour Cray. Born in Chippewa Falls, Wisconsin, Cray entered the Navy directly after graduating from high school in 1943, serving first as a radio operator in Europe before being transferred to the Pacific theater to participate in code-breaking activities. After the war, Cray attended the University of Minnesota, graduated with an electrical engineering degree in 1949, and went to work for ERA in 1951. Cray immediately made his mark by leading the design of the UNIVAC 1103, one of the first commercially successful scientific computers, and soon gained a reputation as an engineering genius able to create simple, yet fast computer designs. In 1957, Cray and several other engineers followed Norris to CDC.\n\nUnlike some of the more conservative engineers at IBM, Cray understood the significance of the transistor immediately and worked to quickly incorporate it into his computer designs. The result was CDC’s first computer, the 1604, which was first sold in 1960 and significantly outperformed IBM’s scientific computers. Armed with Cray’s expertise in computer design Norris decided to concentrate on building the fastest computers possible and selling them to the scientific and military-industrial communities where IBM’s sales force exerted relatively little influence. As IBM’s Project Stretch floundered — never meeting its performance targets after being released as the IBM 7030 in 1961 — Cray moved forward with his plans to build the fastest computer yet designed. Released as the CDC 6600 in 1964, Cray’s machine could perform an astounding three million operations per second, three times as many as the 7030 and more than any other machine would be able to perform until 1969, when another CDC machine, the 7600, outpaced it. Dubbed a supercomputer, the 6600 became the flagship product of a series of high-speed scientific computers that IBM proved unable to match. While Big Blue was ultimately forced to cede the top of the market to CDC, however, by the time the 6600 launched the company was in the final phases of a product line that would extend the company’s dominance over the mainframe business and ensure competitors like CDC and Honeywell would be limited to only niche markets.\n\nSystem/360\n\nThe System/360 family of computers, which extended IBM’s dominance of the mainframe market through the end of the 1960s.\n\nWhen Tom Watson Jr. finally assumed full control of IBM from his father, he inherited a corporate structure designed to collect as much power and authority in the hands of the CEO as possible. Unlike Watson Sr., Watson Jr. preferred decentralized management with a small circle of trusted subordinates granted the authority to oversee the day-to-day operation of IBM’s diverse business activities. Therefore Watson overhauled the company in November 1956, paring down the number of executives reporting directly to him from seventeen to just five, each of whom oversaw multiple divisions with the new title of “group executive.” He also formed a Corporate Management Committee consisting of himself and the five group executives to make and execute high-level decisions. While the responsibilities of individual group executives would change from time to time, this new management structure remained intact for decades.\n\nForemost among Watson’s new group executives was a vice president named Vin Learson. A native of Boston, Massachusettes, T. Vincent Learson graduated from Harvard with a degree in mathematics in 1935 and joined IBM as a salesman, where he quickly distinguished himself. In 1949, Learson was named sales manager of IBM’s Electric Accounting Machine (EAM) Division, and he rose to general sales manager in 1953. In April 1954, Tom Watson, Jr. named Learson the director of Electronic Data Processing Machines with a mandate to solidify IBM’s new electronic computer business. After guiding early sales of the 702 computer and establishing an advanced technology group to incorporate core memory and other improvements into the 704 and 705 computers, Learson received another promotion to vice president of sales for the entire company before the end of the year. During Watson’s 1956 reorganization, he named Learson group executive of the Military Products, Time Equipment, and Special Engineering Products divisions.\n\nDuring the reorganization, IBM’s entire computer business fell under the new Data Processing Division overseen by group executive L.H. LaMotte. As IBM’s computer business continued to grow and diversify in the late 1950s, however, it grew too large and unwieldy to contain within a single division, so in 1959 Watson split the operation in two by creating the Data Systems Division in Poughkeepsie, responsible for large systems, and the General Products Division, which took charge of small systems like the 650 and 1401 and incorporated IBM’s other laboratories in Endicott, San Jose, Burlington, Vermot, and Rochester, Minnesota. Watson then placed these two divisions, along with a new Advanced Systems Development Division, under Learson’s control, believing him to be the only executive capable of propelling IBM’s computer business forward.\n\nVin Learson, the IBM executive who spearheaded the development of the System/360\n\nWhen Learson inherited the Data Systems and General Products Divisions, he was thrust into the middle of an all out war for control of IBM’s computer business. The Poughkeepsie Laboratory had been established specifically to exploit electronics after World War II and prided itself on being at the cutting edge of IBM’s technology. The Endicott Laboratory, the oldest R&D division at the company, had often been looked down upon for clinging to older technology, yet by producing both the 650 and the 1401, Endicott was responsible for the majority of IBM’s success in the computer realm. By 1960, both divisions were looking to update their product lines with more advanced machines. That September, Endicott announced the 1410, an update to the 1401 that maintained backwards compatibility. At the same time, Poughkeepsie was hard at work on a new series of four compatible machines designed to serve a variety of business and scientific customers under the 8000 series designation. Learson, however, wanted to unify the product line from the very low end represented by the 1401 to the extreme high end represented by the 7030 and the forthcoming 8000 computers. By achieving full compatibility in this manner, IBM could take advantage of economies of scale to drive down the price of individual computer components and software development while also standardizing peripheral devices and streamlining the sales and service organizations that would no longer have to learn multiple systems. While Learson’s plan was sound in theory, however, forcing two organizations that prided themselves on their independence and competed with each other fiercely to work together would not be easy.\n\nLearson relied heavily on his power as a group executive to transfer employees across both divisions to achieve project unity. First, he moved Bob Evans, who had been the engineering manager for the 1401 and 1410, from Endicott to Poughkeepsie as the group’s new systems development manager. Already a big proponent of compatibility, Evans unsurprisingly recommended that the 8000 project be cancelled and a cohesive product line spanning both divisions be initiated in its place. The lead designer of the 8000 series, Frederick Brooks, vigorously opposed this move, so Learson replaced Brooks’s boss with another ally, Jerrier Haddad, who had led the design of the 701 and recently served as the head of Advanced Systems Development. Haddad sided with Evans and terminated the 8000 project in May 1961. Strong resistance remained in some circles, however, most notably from General Products Division head John Haanstra, so in October 1961, Learson assembled a task group called SPREAD (Systems, Planning, Review, Engineering, and Development) consisting of thirteen senior engineering and marketing managers to determine a long-term strategy for IBM’s data processing line.\n\nOn December 28, the SPREAD group delivered its final proposal to the executive management committee. In it, they outlined a series of five compatible processors representing a 200-fold range in performance. Rather than incorporate the new integrated circuit, the group proposed a proprietary IBM design called Solid Logic Technology (SLT), in which the discrete components of the circuit were mounted on a single ceramic substrate, but were not fully integrated. By combining the five processors with SLT circuits and core memories of varying speeds, nineteen computer configurations would be possible that would all be fully compatible and interchangeable and could be hooked up to 40 different peripheral devices. Furthermore, after surveying the needs of business and scientific customers, the SPREAD group realized that other than floating-point capability for scientific calculations, the needs of both customers were nearly identical, so they chose to unify the scientific and business lines rather then market different models for each. Codenamed the New Product Line (NPL), the SPREAD proposal would allow IBM customers to buy a computer that met their current needs and then easily upgrade or swap components as their needs changed over time at a fraction of the cost of a new system without having to rewrite all their software or replace their peripheral devices. While not everyone was convinced by the presentation, Watson ultimately authorized the NPL project.\n\nThe NPL project was perhaps the largest civilian R&D operation ever undertaken to that point. Development costs alone were $500 million, and when tooling, manufacturing, and other expenses were taken into account, the cost was far higher. Design of the five processor models was spread over three facilities, with Poughkeepsie developing the three high-end systems, Endicott developing the lowest-end system, and a facility in Hursley, England, developing the other system. At the time, IBM manufactured all its own components as well, so additional facilities were charged with churning out SLT circuits, core memories, and storage systems. To assemble all the systems, IBM invested in six new factories. In all, IBM spent nearly $5 billion to bring the NPL to market.\n\nTo facilitate the completion of the project, Watson elevated two executives to new high level positions: Vin Learson assumed the new role of senior vice president of sales, and Watson’s younger brother, Arthur, who for years had run IBM’s international arm, the World Trade Corporation, was named senior vice president of research, development, and manufacturing. This new role was intended to groom the younger Watson to assume the presidency of IBM one day, but the magnitude of the NPL project coupled with Watson’s inexperience in R&D and manufacturing ultimately overwhelmed him. As the project fell further and further behind schedule, Learson ultimately had to replace Arthur Watson in order to see the project through to completion. Therefore, it was Learson who assumed the presidency of IBM in 1966 while Watson assumed the new and largely honorary role of vice chairman. His failure to shepherd the NPL project ended any hope Arthur Watson had of continuing the Watson family legacy of running IBM, and he ultimately left the company in 1970 to serve as the United States ambassador to France.\n\nIn late 1963, IBM began planning the announcement of its new product line, which now went by the the name System/360 — a name chosen because it represented all the points of a compass and emphasized that the product line would fill the needs of all computer users. Even at this late date, however, acceptance of System/360 within IBM was not assured. John Haanstra continued to push for an SLT upgrade to the existing 1401 line to satisfy low-end users, which other managers feared would serve to perpetuate the incompatibility problem plaguing IBM’s existing product line. Furthermore, IBM executives struggled over whether to announce all the models at once and thus risk a significant drop in orders for older systems during the transition period, or phase in each model over the course of several years. All debate ended when Honeywell announced the H200. Faced with losing customers to more advanced computers fully compatible with IBM’s existing line, Watson decided in March 1964 to scrap the improved 1401 and launch the entire 360 product line at once.\n\nOn April 7, 1964, IBM held press conferences in sixty-three cities across fourteen countries to announce the System/360 to the world. Demand soon far exceeded supply as within the first two years that System/360 was on the market IBM was only able to fill roughly 4,500 of 9,000 orders. Headcount at the company rose rapidly as IBM rushed to bring new factories online in response. In 1965, when actual shipments of the System/360 were just beginning, IBM controlled 65 percent of the computer market and had revenues of $2.5 billion. By 1967, as IBM ramped up to meet insatiable 360 demand, the company employed nearly a quarter of a million people and raked in $5 billion in revenues. By 1970, IBM had an install base of 35,000 computers and held an ironclad grip on the mainframe industry with a marketshare between seventy and eighty percent; the next year company earnings surpassed $1 billion for the first time.\n\nAs batch processing mainframes, the System/360 line and its competitors did not serve as computer game platforms or introduce technology that brought the world closer to a viable video game industry. System/360 did, however, firmly establish the computer within corporate America and solidified IBM’s place as a computing superpower while facilitating the continuing spread of computing resources and the evolution of computer technology. Ultimately, this process would culminate in a commercial video game industry in the early 1970s.\n\nBy 1955, computers were well on their way to becoming fixtures at government agencies, defense contractors, academic institutions, and large corporations, but their function remained limited to a small number of activities revolving around data processing and scientific calculation. Generally speaking, the former process involved taking a series of numbers and running them through a single operation, while the latter process involved taking a single number and running it through a series of operations. In both cases, computing was done through batch processing — i.e. the user would enter a large data set from punched cards or magnetic tape and then leave the computer to process that information based on a pre-defined program housed in memory. For companies like IBM and Remington Rand, which had both produced electromechanical tabulating equipment for decades, this was a logical extension of their preexisting business, and there was little impetus for them to discover novel applications for computers.\n\nIn some circles, however, there was a belief that computers could move beyond data processing and actually be used to control complex systems. This would require a completely different paradigm in computer design, however, based around a user interacting with the computer in real-time — i.e. being able to give the computer a command and have it provide feedback nearly instantaneously. The quest for real-time computing not only expanded the capabilities of the computer, but also led to important technological breakthroughs instrumental in lowering the cost of computing and opening computer access to a greater swath of the population. Therefore, the development of real-time computers served as the crucial final step in transforming the computer into a device capable of delivering credible interactive entertainment.\n\nNote: This is the fourth and final post in a series of “historical interludes” summarizing the evolution of computer technology between 1830 and 1960. The information in this post is largely drawn from Computer: A History of the Information Machine by Martin Campbell-Kelly and William Aspray, A History of Modern Computing by Paul Ceruzzi, Forbes Greatest Technology Stories: Inspiring Tales of Entrepreneurs and Inventors Who Revolutionized Modern Business by Jeffrey Young, IBM’s Early Computers by Charles Bashe, Lyle Johnson, John Palmer, and Emerson Pugh, and The Ultimate Entrepreneur: The Story of Ken Olsen and Digital Equipment Corporation by Glenn Rifkin and George Harrar.\n\nProject Whirlwind\n\nJay Forrester (l), the leader of Project Whirlwind\n\nThe path to the first real-time computer began with a project that was never supposed to incorporate digital computing in the first place. In 1943, the head of training at the United States Bureau of Aeronautics, a pilot and MIT graduate named Captain Luis de Florez, decided to explore the feasibility of creating a universal flight simulator for military training. While flight simulators had been in widespread use since Edwin Link had introduced a system based around pneumatic bellows and valves called the Link Trainer in 1929 and subsequently secured an Army contract in 1934, these trainers could only simulate the act of flying generally and were not tailored to specific planes. Captain de Florez envisioned using an analog computer to simulate the handling characteristics of any extant aircraft and turned to his alma mater to make this vision a reality.\n\nAt the time, MIT was already the foremost center in the United States for developing control systems thanks to the establishment of the Servomechanisms Laboratory in 1941, which worked closely with the military to develop electromechanical equipment for fire control, bomb sights, aircraft stabilizers, and similar projects. The Bureau of Aeronautics therefore established Project Whirlwind within the Servomechanisms Laboratory in 1944 to create de Florez’s flight trainer. Leadership of the Whirlwind project fell to an assistant director of the Servomechanisms Laboratory named Jay Forrester. Born in Nebraska, Forrester had been building electrical systems since he was a teenager, when he constructed a 12-volt electrical system out of old car parts to provide his family’s ranch with electricity for the first time. After graduating from the University of Nebraska, Forrester came to MIT as a graduate student in 1939 and joined the Servomechanisms Laboratory at its inception. By 1944, Forrester was getting restless and considering establishing his own company, so he was given his choice of projects to oversee to prevent his defection. Forrester chose Whirlwind.\n\nIn early 1945, Forrester drew up the specifications for a trainer consisting of a mock cockpit connected to an analog computer that would control a hydraulic transmission system to provide feedback to the cockpit. Based on this preliminary work, MIT drafted a proposal in May 1945 for an eighteen-month project budgeted at $875,000, which was approved. As work on Whirlwind began, the mechanical elements of the design came together quickly, but the computing element remained out of reach. To create an accurate simulator, Forrester required a computer that updated dozens of variables constantly and reacted to user input instantaneously. Bush’s Differential Analyzer, perhaps the most powerful analog computer of the time, was still far too slow to handle these tasks, and Forrester’s team could not figure out how to produce a more powerful machine solely through analog components. In the summer of 1945, however, a fellow MIT graduate student named Perry Crawford that had written a master’s thesis in 1942 on using a digital device as a control system alerted Forrester to the breakthroughs being made in digital computing at the Moore School. In October, Forrester and Crawford attended a Conference on Advanced Computational Techniques hosted by MIT and learned about the ENIAC and EDVAC in detail. By early 1946, Forrester was convinced that the only way forward for Project Whirlwind was the construction of a digital computer that could operate in real time.\n\nThe shift from an analog computer to a digital computer for the Whirlwind project resulted in a threefold increase in cost to an estimated $1.9 million. It also created an incredible technical challenge. In a period when the most advanced computers under development were struggling to achieve 10,000 operations a second, Whirlwind would require the capability of performing closer to 100,000 operations per second for seamless real-time operation. Furthermore, the first stored-program computers were still three years away, so Forrester’s team also faced the prospect of integrating cutting edge memory technologies that were still under development. By 1946, the size of the Whirlwind team had grown to over a hundred staff members spread across ten groups each focused on a particular part of the system in an attempt to meet these challenges. All other aspects of the flight simulator were placed on hold as the entire team focused its attention on creating a working real-time computer.\n\nThe Whirlwind I, the first real-time computer\n\nBy 1949, Forrester’s team had succeeded in designing an architecture fast enough to support real-time operation, but the computer could not operate reliably for extended periods. With costs escalating and no end to development in sight, continued funding for the project was placed in jeopardy. After the war, responsibility for Project Whirlwind had transferred from the Bureau of Aeronautics to the Office of Naval Research (ONR), which felt the project was not providing much value relative to a cost that had by now far surpassed $1.9 million. By 1948, Whirlwind was consuming twenty percent of ONR’s entire research budget with little to show for it, so ONR began slowly trimming the budget. By 1950, ONR was ready to cut funding all together, but just as the project appeared on the verge of death, it was revived to serve another function entirely.\n\nOn August 29, 1949, the Soviet Union detonated its first atomic bomb. In the immediate aftermath of World War II, the United States had felt relatively secure from the threat of Soviet attack due to the distance between the two nations, but now the USSR had both a nuclear capability and a long range bomber capable of delivering a payload on U.S. soil. During World War II, the U.S. had developed a primitive radar early warning system to protect against conventional attack, but it was wholly insufficient to track and interdict modern aircraft. The United States needed a new air defense system and needed it quickly.\n\nIn December 1949, the United States Air Force formed a new Air Defense System Engineering Committee (ADSEC) chaired by MIT professor George Valley to address the inadequacies in the country’s air-defense system. In 1950, ADSEC recommended creating a series of computerized command-and-control centers that could analyze incoming radar signals, evaluate threats, and scramble interceptors as necessary to interdict Soviet aircraft. Such a massive and complex undertaking would require a powerful real-time computer to coordinate. Valley contacted several computer manufacturers with his needs, but they all replied that real-time computing was impossible.\n\nDespite being a professor at MIT, Valley knew very little about the Whirlwind project, as he was not interested in analog computing and had no idea it had morphed into a digital computer. Fortunately, a fellow professor at the university, Jerome Wiesner, pointed him towards the project. By early 1950, the Whirlwind I computer’s basic architecture had been completed, and it was already running its first test programs, so Forrester was able to demonstrate its real-time capabilities to Valley. Impressed by what he saw, Valley organized a field-test of the Whirlwind as a radar control unit in September 1950 at Hanscom Field outside Bedford, Massachusettes, where a radar station connected to Whirlwind I via a phone line successfully delivered a radar signal from a passing aircraft. Based on this positive result, the United States Air Force established Project Lincoln in conjunction with MIT in 1951 and moved Whirlwind to the new Lincoln Laboratory.\n\nProject SAGE\n\nA portion of an IBM AN/FSQ-7 Combat Direction Central, the heart of the SAGE system and the largest computer ever built\n\nBy April 1951, the Whirlwind I computer was operational, but still rarely worked properly due to faulty memory technology. At Whirlwind’s inception, there were two primary forms of electronic memory in use, the delay-line storage pioneered for the EDVAC and CRT memory like the Williams Tube developed for the Manchester Mark I. From his exposure to the EDVAC, Forrester was already familiar with delay-line memory early in Whirlwind’s development, but that medium functioned too slowly for a real-time design. Forrester therefore turned his attention to CRT memory, which could theoretically operate at a sufficient speed, but he rejected the Williams Tube due to its low refresh rate. Instead, Forrester incorporated an experimental tube memory under development at MIT, but this temperamental technology never achieved its promised capabilities and proved unreliable besides. Clearly, a new storage method would be required for Whirlwind.\n\nIn 1949, Forrester saw an advertisement for a new ceramic material called Deltamax from the Arnold Engineering Company that could be magnetized or demagnetized by passing a large enough electric current through it. Forrester believed the properties of this material could be used to create a fast and reliable form of computer memory, but he soon discovered that Deltamax could not switch states quickly at high temperatures, so he assigned a graduate student named William Papian to find an alternative. In August 1950, Papian completed a master’s thesis entitled “A Coincident-Current Magnetic Memory Unit” laying out a system in which individual cores — small doughnut-shaped objects — with magnetic properties similar to Deltamax are threaded into a three-dimensional matrix of wires. Two wires are passed through the center of the core to magnetize or demagnetize it by taking advantage of a property called hysteresis in which an electrical current only changes the magnetization of the material if it is above a certain threshold. Only when currents are run through both wires and passed in the same direction will the magnetization change, making the cores a suitable form of computer memory. A third wire is threaded through all of the cores in the matrix, allowing any portion of the memory to be read at any time.\n\nPapian built the first small core memory matrix in October 1950, and by the end of 1951 he was able to construct a 16 x 16 array of cores. During this period, Papian tested a wide variety of materials for his cores and settled on a silicon-steel ribbon wrapped around a ceramic bobbin, but these cores still operated too slowly and also required an unacceptably high level of current. At this point Forrester discovered a German ceramicist in New Jersey named E. Albers-Schoenberg was attempting to create a transformer for televisions by mixing iron ore with certain oxides to create a compound called a ferrite that exhibited certain magnetic properties. While ferrites generated a weaker output than the metallic cores Papian was experimenting with, they could switch up to ten times faster. After experimenting with various chemical compositions, Papian finally constructed a ferrite-based core memory system in May 1952 that could switch between states in less than a microsecond and therefore serve the needs of a real-time computer. First installed in the Whirlwind I in August 1953, ferrite core memory was smaller, cheaper, faster, and more reliable than delay-line, CRT, and magnetic drum memory and ultimately doubled the operating speed of the computer while reducing maintenance time from four hours a day to two hours a week. Within five years, core memory had replaced all other forms of memory in mainframe computers, netting MIT a hefty profit in patent royalties.\n\nWith Whirlwind I finally fully functional the Lincoln Laboratory turned its attention to transforming the computer into a commercial command-and-control system suitable for installation in the United States Air Force’s air defense system. This undertaking was beyond the scope of the lab itself, as it would require fabrication of multiple components on a large scale. Lincoln Labs evaluated three companies to take on this task, defense contractor Raytheon, which had recently established a computer division, Remington Rand — through both its EMCC and ERA subsidiaries — and IBM. At the time, Remington Rand was still the powerhouse in the new commercial computer business, while IBM was only just preparing to bring its first products to market. Nonetheless, Forrester and his team were impressed with IBM’s manufacturing facilities, service force, integration, and experience deploying electronic products in the field and therefore chose the new kid on the block over its more established competitor. Originally designated Project High by IBM — due to its location on the third floor of a necktie factory on High Street in Poughkeepsie — and the Whirlwind II by Lincoln Laboratory, the project eventually went by the name Semi-Automatic Ground Environment, or SAGE.\n\nThe heart of the SAGE system was a new IBM computer derived from the Whirlwind design called the AN/FSQ-7 Combat Direction Central. By far the largest computer system ever built, the AN/FSQ-7 weighed 250 tons, consumed three megawatts of electricity, and took up roughly half an acre of floor space. Containing 49,000 vacuum tubes and a core memory capable of storing over 65,000 33-bit words, the computer was capable of performing roughly 75,000 operations per second. In order to insure uninterrupted operation, each SAGE installation actually consisted of two AN/FSQ-7 computers so that if one failed, the other could seamlessly assume control of the air defense center. As the first deployed real-time computer system, it inaugurated a number of firsts in commercial computing such as the ability generate text and vector graphics on a display screen, the ability to directly enter commands via a typewriter-style keyboard, and the ability to select or draw items directly on the display using a light pen, a technology developed specifically for Whirlwind in 1955. In order to remain in constant contact with other segments of the air defense system, the computer was also the first outfitted with a new technology called a modem developed by AT&T’s Bell Labs research division to allow data to be transmitted over a phone line.\n\nThe first SAGE system was deployed at McChord Air Force Base in November 1958, and the entire network of twenty-three Air Defense Direction Centers were online by 1963 at a total cost to the government of $8 billion. While IBM agreed to do the entire project at cost as part of its traditional support for national defense, the project still brought the company $500 million in revenues in the late 1950s. SAGE was perhaps the key project in IBM’s rise to dominance in the computer industry. Through this massive undertaking, IBM became the most knowledgeable company in world at designing, fabricating, and deploying both large-scale mainframe systems and their critical components such as core memory and computer software. In 1954, IBM upgraded its 701 computer to replace Williams Tubes memory with magnetic cores and released the system as the IBM 704. The next year, a core-memory replacement for the 702 followed designated the IBM 705. These new computers were instrumental in vaulting IBM past Remington Rand in the late 1950s. SAGE, meanwhile, remained operational until 1983.\n\nThe Transistor and the TX-0\n\nKenneth Olsen, co-designer of the TX-0 and co-founder of the Digital Equipment Corporation (DEC)\n\nWhile building a real-time computer for the SAGE air-defense system was the primary purpose of Project Whirlwind, the scope of the project grew large enough by the middle of the 1950s that staff could occasionally indulge in other activities, such as a new computer design proposed by staff member Kenneth Olsen. Born in Bridgeport, Connecticut, Olsen began experimenting with radios as a teenager and took an eleven-month electronics course after entering the Navy during World War II. The war was over by the time his training was complete, so after a single deployment on an admiral’s staff in the Far East, Olsen left the Navy to attend MIT in 1947, where he majored in electrical engineering. After graduating in 1950, Olsen decided to continue his studies at MIT as a graduate student and joined Project Whirlwind. One of Olsen’s duties on the project was the design and construction of the Memory Test Computer (MTC), a smaller version of the Whirlwind I built to test various core memory solutions. In creating the MTC, Olsen innovated with a modular design in which each group of circuits responsible for a particular function was placed on a single plug-in unit placed on a rack that could be easily swapped out if it malfunctioned. This was a precursor of the plug-in circuit boards still used today on computers.\n\nOne of the engineers who helped Olsen debug the MTC was Wes Clark, a physicist that came to Lincoln Laboratory in 1952 after working at the Hanford nuclear production site in Washington State. Clark and Olsen soon bonded over their shared views on the future of computing and their desire to create a computer that would apply the lessons learned during the Whirlwind project and the construction of the MTC to the latest advances in electronics to demonstrate the potential of a fast and power-efficient computer to the defense industry. Specifically, Olsen and Clark wanted to explore the potential of a relatively new electronic component called the transistor.\n\nJohn Bardeen (l), William Shockley (seated), and Walter Brattain, the team that invented the transistor\n\nFor over forty years, the backbone of all electronic equipment was the vacuum tube pioneered by John Fleming in 1904. While this device allowed for switching at electronic speeds, however, its limitations were numerous. Vacuum tubes generated a great deal of heat during operation, which meant that they consumed power at a prodigious rate and were prone to burnout over extended periods of use. Furthermore, they could not be miniaturized beyond a certain point and had to be spaced relatively far apart for heat management, guaranteeing that tube-based electronics would always be large and bulky. Unless an alternative switching device could be found, the computer would never be able to shrink below a certain size. The solution to the vacuum tube problem came not from one of the dozen or so computer projects being funded by the U.S. government, but from the telephone industry.\n\nIn the 1920s and 1930s, AT&T, which held a monopoly on telephone service in the United States, began constructing a series of large switching facilities in nearly every town in the country to allow telephone calls to be placed between any two phones in the United States. These facilities relied on the same electromechanical relays that powered several of the early computers, which were bulky, slow, and wore out over time. Vacuum tubes were sometimes used as well, but the problems articulated above made them particularly unsuited for the telephone network. As AT&T continued to expand its network, the size and speed limitations of relays became increasingly unacceptable, so the company gave a mandate to its Bell Labs research arm, one of the finest corporate R&D organizations in the world, to discover a smaller, faster, and more reliable switching device.\n\nIn 1936, the new director of research at Bell Labs, Mervin Kelly, decided to form a group to explore the possibility of creating a solid-state switching device. Both solid-state physics, which explores the properties of solids based on the arrangement of their sub-atomic particles, and the related field of quantum mechanics, in which physical phenomena are studied on a nanoscopic scale, were in their infancy and not widely understood, so Kelly scoured the universities for the smartest chemists, metallurgists, physicists, and mathematicians he could find. His first hire was a brilliant, but difficult physicist named William Shockley. Born in London to a mining engineer and a geologist, William Bradford Shockley, Jr. grew up in Palo Alto, California, in the heart of the Santa Clara Valley, a region known as the “Valley of the Heart’s Delight” for its orchards and flowering plants. Shockley’s father spent most of his time moving from mining camp to mining camp, so he grew especially close to his mother, May, who taught him the ins and outs of geology from a young age. After attending Stanford to stay close to his mother, Shockley received a Ph.D. from MIT in 1936 and went to work for Bell. Gruff and self-centered, Shockley never got along with his colleagues anywhere he worked, but there was no questioning his brilliance or his ability to push colleagues towards making new discoveries.\n\nKelly’s group began educating itself on the field of quantum mechanics through informal sessions where they would each take a chapter of the only quantum mechanics textbook in existence and teach the material to the rest of the group. As their knowledge of the underlying science grew in the late 1930s, the group decided the most promising path to a solid-state switching device lay with a group of materials called semiconductors. Generally speaking, most materials are either a conductor of electricity, allowing electrons to flow through them, or an insulator, halting the flow of electrons. As early as 1826, however, Michael Faraday, the brilliant scientist whose work paved the way for electric power generation and transmission, had observed that a small number of compounds would not only act as a conductor under certain conditions and an insulator in others, but would also serve as amplifiers under certain conditions as well. These properties allowed a semiconductor to behave like a triode under the right conditions, but for decades scientists remained unable to determine why changes in heat, light, or magnetic field would alter the conductivity of these materials and therefore could not harness this property. It was not until the field of quantum mechanics became more developed in the 1930s that scientists gained a great enough understanding of electron behavior to attack the problem. Kelly’s new solid-state group hoped to unlock the mystery of semiconductors once and for all, but their work was interrupted by World War II.\n\nIn 1945, Kelly revived the solid-state project under the joint supervision of William Shockley and chemist Stanley Morgan. The key members of this new team were John Bardeen, a physicist from Wisconsin known as one of the best quantum mechanics theorists in the world, and Walter Brattain, a farm boy from Washington known for his prowess at crafting experiments. During World War II, great progress had been made in creating crystals of the semiconducting element germanium for use in radar, so the group focused its activities on that element. In late 1947, Bardeen and Brattain discovered that if they introduced impurities into just the right spot on a lump of germanium, the germanium could amplify a current in the same manner as a vacuum tube triode. Shockley’s team gave an official demonstration of this phenomenon to other Bell Labs staff on December 23, 1947, which is often recognized as the official birthday of the transistor, so named because it effects the transfer of a current across a resistor — i.e. the semiconducting material. Smaller, less power-hungry, and more durable than the vacuum tube, the transistor paved the way for the development of the entire consumer electronics and personal computer industries of the late twentieth century.\n\nThe TX-0, one of the earliest transistorized computers, designed by Wes Clark and Kenneth Olsen\n\nDespite its revolutionary potential, the transistor was not incorporated into computer designs right away, as there were still several design and production issues that had to be overcome before it could be deployed in the field in large numbers (which will be covered in a later post). By 1954, however, Bell Labs had deployed the first fully transistorized computer, the Transistor Digital Computer or TRADIC, while electronics giant Philco had introduced a new type of transistor called a surface-barrier transistor that was expensive, but much faster than previous designs and therefore the first practical transistor for use in a computer. It was in this environment that Clark and Olsen proposed a massive transistorized computer called the TX-1 that would be roughly the same size as a SAGE system and deploy one of the largest core memory arrays ever built, but they were turned down because Forrester did not find their design practical. Clark therefore went back to the drawing board to create as simple a design as he could that still demonstrated the merits of transistorized computing. As this felt like a precursor to the larger TX-1, Olsen and Clark named this machine the TX-0.\n\nCompleted in 1955 and fully operational the next year, the TX-0 — often pronounced “Tixo” — incorporated 3,600 surface-barrier transistors and was capable of performing 83,000 operations per second. Like the Whirlwind, the TX-0 operated in real time, and it also incorporated a display with a 512×512 resolution that could be manipulated by a light pen, and a core memory that could store over 65,000 words, though Clark and Olsen settled on a relatively short 18-bit word length. Unlike the Whirlwind I, which occupied 2,500 square feet, the TX-0 took up a paltry 200 square feet. Both Clark and Olsen realized that the small, fast, interactive TX-0 represented something new: a (relatively) inexpensive computer that a single user could interact with in real time. In short, it exhibited many of the hallmarks of what would become the personal computer.\n\nWith the TX-0 demonstrating the merits of high-speed transistors, Clark and Olsen returned to their goal of creating a more complex computer with a larger memory, which they dubbed the TX-2. Completed in 1958, the TX-2 could perform a whopping 160,000 operations per second and contained a core memory of 260,000 36-bit words, far surpassing the capability of the earlier TX-0. Olsen once again designed much of the circuitry for this follow-up computer, but before it was completed he decided to leave MIT behind.\n\nThe Digital Equipment Corporation\n\nThe PDP-1, Digital Equipment Corporation’s First Computer\n\nDespite what Olsen saw as the nearly limitless potential of transistorized computers, the world outside MIT remained skeptical. It was one thing to create an abstract concept in a college laboratory, people said, but another thing entirely to actually deploy an interactive transistorized system under real world conditions. Olsen fervently desired to prove these naysayers wrong, so along with a fellow student who worked with him on the MTC named Harlan Anderson he decided to form his own computer company. As a pair of academics with no practical real-world business experience, however, Olsen and Anderson faced difficulty securing financial backing. They approached defense contractor General Dynamics first, but were flatly turned down. Unsure how to proceed next, they visited the Small Business Administration office in Boston, which recommended they contact investor Georges Doriot.\n\nGeorges Doriot was a Frenchman who immigrated to the United States in the 1920s to earn an MBA from Harvard and then decided to stay on as a professor at the school. In 1940, Doriot became an American citizen, and the next year he joined the United States Army as a lieutenant colonel and took on the role of director of the Military Planning Division for the Quartermaster General. Promoted to brigadier general before the end of the war, Doriot returned to Harvard in 1946 and also established a private equity firm called the American Research and Development Corporation (ARD). With a bankroll of $5 million raised largely from insurance companies and educational institutions, Doriot sought out startups in need of financial support in exchange for taking a large ownership stake in the company. The goal was to work closely with the company founders to grow the business and then sell the stake at some point in the future for a high return on investment. While many of the individual companies would fail, in theory the payoff from those companies that did succeed would more than make up the difference and return a profit to the individuals and groups that provided his firm the investment capital. Before Doriot, the only outlets for a new business to raise capital were the banks, which generally required tangible assets to back a loan, or a wealthy patron like the Rockefeller or Whitney families. After Doriot’s model proved successful, inexperienced entrepreneurs with big ideas now had a new outlet to bring their products to the world. This outlet soon gained the name venture capital.\n\nIn 1957, Olsen and Anderson wrote a letter to Doriot detailing their plans for a new computer company. After some back and forth and refinement of the business plan, ARD agreed to provide $70,000 to fund Olsen and Anderson’s venture in return for a 70% ownership stake, but the money came with certain conditions. Olsen wanted to build a computer like the TX-0 for use by scientists and engineers that could benefit from a more interactive programming environment in their work, but ARD did not feel it was a good idea to go toe-to-toe with an established competitor like IBM. Instead, ARD convinced Olsen and Anderson to produce components like power supplies and test equipment for core memory. Olsen and Anderson had originally planned to call their new company the Digital Computer Corporation, but with their new ARD-mandated direction, they instead settled on the name Digital Equipment Corporation (DEC).\n\nIn August 1957, DEC moved into its new office space on the second floor of Building 12 of a massive woolen mill complex in Maynard, Massachusetts, originally built in 1845 and expanded many times thereafter. At the time, the company consisted of just three people: Ken Olsen, Harlan Anderson, and Ken’s younger brother Stan, who had worked as a technician at Lincoln Lab. Ken served as the leader and technical mastermind of the group, Anderson looked after administrative matters, and Stan focused on manufacturing. In early 1958, the company released its first products.\n\nDEC arrived on the scene at the perfect moment. Core memory was in high demand and transistor prices were finally dropping, so all the major computer companies were exploring new designs, creating an insatiable demand for testing equipment. As a result, DEC proved profitable from the outset. In fact, Olsen and Anderson actually overpriced their stock due to their business inexperience, but with equipment in such high demand, firms bought from DEC anyway, giving the company extremely high margins and allowing it to exceed its revenue goals. Bolstered by this success, Olsen chose to revisit the computer project with ARD, so in 1959 DEC began work on a fully transistorized interactive computer.\n\nDesigned by Ben Gurley, who had developed the display for the TX-0 at MIT, the Programmed Data Processor-1, more commonly referred to as the PDP-1, was unveiled in December 1959 at the Eastern Joint Computer Conference in Boston. It was essentially a commercialized version of the TX-0, though it was not a direct copy. The PDP-1 incorporated a better display than its predecessor with a resolution of 1024 x 1024 and it was also faster, capable of 100,000 operations per second. The base setup contained only 4,096 18-bit words of core memory, but this could be upgraded to 65,536. The primary method of inputting programs was a punched tape reader, and it was hooked up to a typewriter as well. While not nearly as powerful as the latest computers from IBM and its competitors in the mainframe space, the PDP-1 only cost $120,000, a stunningly low price in an era where buying a computer would typically set an organization back a million dollars or more. Lacking developed sales, manufacturing, or service organizations, DEC sold only a handful of PDP-1 computers over its first two years on the market to organizations like Bolt, Beranek, and Newman and the Lawrence Livermore Labs. A breakthrough occurred in late 1962 when the International Telegraph and Telephone Company (ITT) decided to order fifteen PDP-1 computers to form the heart of a new telegraph message switching system designated the ADX-7300. ITT would continue to be DEC’s most important PDP-1 customer throughout the life of the system, ultimately purchasing roughly half of the fifty-three computers sold.\n\nWhile DEC only sold around fifty PDP-1’s over its lifetime, the revolutionary machine introduced interactive computing commercially and initiated the process of opening computer use to ever greater portions of the public, which culminated in the birth of the personal computer two decades later. With its monitor and real-time operation, it also provided a perfect platform for creating engaging interactive games. Even with these advances, the serious academics and corporate data handlers of the 1950s were unlikely to ever embrace the computer as an entertainment medium, but unlike the expensive and bulky mainframes reserved for official business, the PDP-1 and its successors soon found their way into the hands of students at college campuses around the country, beginning with the birthplace of the PDP-1 technology: MIT.\n\nIn the 1940s, the electronic digital computer was a new, largely unproven machine developed in response to specific needs like the code-breaking requirements of Bletchley Park or the ballistics calculations of the Aberdeen Proving Grounds. Once these early computers proved their worth, projects like the Manchester Mark 1, EDVAC, and EDSAC implemented a stored program concept that allowed digital computers to become useful for a wide variety of scientific and business tasks. In the early 1950s, several for-profit corporations built on this work to introduce mass-produced computers and offered them to businesses, universities, and government organizations around the world. As previously discussed, Ferranti in the United Kingdom introduced the first such computer by taking the Manchester Mark 1 design, increasing the speed and storage capacity of the machine, and releasing it as the Ferranti Mark 1 in February 1952. This would be one of the few times that the United Kingdom led the way in computing over the next several decades, however, as demand remained muted among the country’s conservative businesses, allowing companies in the larger U.S. market to grow rapidly and achieve world dominance in computing.\n\nNote: This is the third of four posts in a series of “historical interludes” summarizing the evolution of computer technology between 1830 and 1960. The information in this post is largely drawn from Computer: A History of the Information Machine by Martin Campbell-Kelly and William Aspray, The Maverick and His Machine: Thomas Watson, Sr. and the Making of IBM by Kevin Maney, A History of Modern Computing by Paul Ceruzzi, Computers and Commerce: A Study of Technology and Management at Eckert-Mauchly Computer Company, Engineering Research Associates, and Remington Rand, 1946-1957 by Arthur Norberg, and IBM’s Early Computers by Charles Bashe, Lyle Johnson, John Palmer, and Emerson Pugh.\n\nUNIVAC\n\nThe UNIVAC I, the first commercially available computer in the United States\n\nFor a brief period from 1943 to 1946, the Moore School in Philadelphia was the center of the computer world as John Mauchly and J. Presper Eckert developed ENIAC and initiated the EDVAC project. Unlike the more accommodating MIT and Stanford, however, which nurtured the Route 128 tech corridor and Silicon Valley respectively by encouraging professors and students to apply technologies developed in academia to the private sector, the Moore School believed commercial interests had no place in an academic institution and decided to quash them entirely. In early 1946 the entire staff of the school was ordered to sign release forms giving up the rights to all patent royalties from inventions pioneered at the school. This was intolerable to both Eckert and Mauchly, who formally resigned on March 31, 1946 to pursue commercial opportunities.\n\nWhile still at the Moore School, Mauchly met with several organizations that might be interested in the new EDVAC computer. One of these was the Census Bureau, which once again needed to migrate to new technologies as tabulating machines were no longer sufficient to count the U.S. population in a timely manner. After leaving the school, Eckert and Mauchly attended a series of meetings with the Census Bureau and the National Bureau of Standards (NBS) between March and May devoted to the possibility of replacing tabulating machines with computers. After further study, the NBS entered into an agreement with Eckert and Mauchly on September 25, 1946, for them to develop a computer for the Census Bureau in return for $300,000, which Eckert and Mauchly naively believed would cover a large portion of their R&D cost.\n\nCensus contract aside, Eckert and Mauchly experienced great difficulty attempting to fund the world’s first for-profit electronic computer company. Efforts to raise capital commenced in the summer of 1946, but Philadelphia-area investors were focused on the older industries of steel and electric power that had driven the region for decades. In New York, there was funding available for going electronic concerns, but the concept of venture capital did not yet exist and no investment houses were willing to take a chance on a startup. The duo were finally forced to turn to friends and family, who provided enough capital in combination with the Census contract for Eckert and Mauchly to establish a partnership called the Electric Control Company in October 1946, which later incorporated as the Eckert-Mauchly Computer Corporation (EMCC) in December 1948.\n\nAs work began on the EDVAC II computer at the new Philadelphia offices of the Electric Control Company, the founders continued to seek new contracts to alleviate chronic undercapitalization. In early 1947 Prudential, a forward-thinking company that had a reputation as an early adopter of new technology, agreed to pay the duo $20,000 to serve as consultants, but refused to commit to ordering a computer until it was completed. Market research film A.C. Nielsen placed an order in spring 1948 and Prudential changed its mind and followed suit late in the year, but both deals were for $150,000 as Eckert and Mauchly continued to underestimate the cost of building their computers. To keep the company solvent, the duo completed a $100,000 deal with Northrop Aircraft in October 1947 for a smaller scientific computer called the Binary Automatic Computer (BINAC) for use in developing a new unmanned bomber. Meanwhile, with contracts coming in Eckert and Mauchly realized that they needed a new name for their computer to avoid confusion with the EDVAC project at the Moore School and settled on UNIVAC, which stood for Universal Automatic Computer.\n\nEMCC appeared to finally turn a corner in August 1948 when it received a $500,000 investment from the American Totalisator Company. The automatic totalisator was a specialized counting machine originally invented by New Zealander George Julius in the early twentieth century to tally election votes and divide them properly among the candidates. When the government rejected the device, he adapted it for use at the race track, where it could run a pari-mutual betting system by totaling all bets and assigning odds to each horse. American Totalisator came to dominate this market after one of its founders, Henry Strauss, invented and patented an electro-mechanical totalisator first used in 1933. Strauss realized that electronic computing was the logical next step in the totalisator field, so he convinced the company board to invest $500,000 in EMCC in return for a 40% stake in the company. With the funding from American Totalisator, EMCC completed BINAC and delivered it to Northrop in September 1949. Although it never worked properly, BINAC was the first commercially sold computer in the world. Work continued on UNIVAC as well, but disaster struck on October 25, 1949, when Henry Strauss died in a plane crash. With EMCC’s chief backer at American Totalisator gone, the company withdrew its support and demanded that its loans be repaid. Eckert and Mauchly therefore began looking for a buyer for their company.\n\nOn February 15, 1950, office equipment giant Remington Rand purchased EMCC for $100,000 while also paying off the $438,000 owed to American Totalisator. James Rand, Jr., the president of the company, had become enamored with the scientific advances achieved during World War II and was in the midst of a post-war expansion plan centered on high technology and electronic products. In 1946, Rand constructed a new high-tech R&D lab in Norwalk, Connecticut, to explore products as varied as microfilm readers, xerographic copiers, and industrial television systems. In late 1947, he hired Leslie Groves, the general who oversaw the Manhattan Project, to run the operation. EMCC therefore fit perfectly into Rand’s plans. Though Eckert and Mauchly were required to give up their ownership stakes and take salaries as regular employees of Remington Rand, Groves allowed them to remain in Philadelphia and generally let them run their own affairs without interference.\n\nWith Remignton Rand sorting out its financial problems, EMCC was finally able to complete its computer. First accepted by the U.S. Census Bureau on March 31, 1951, the UNIVAC I contained 5,200 vacuum tubes and could perform 1,905 operations a second at a clock speed of 2.25 MHz. Like the EDVAC and EDSAC, the UNIVAC I used delay line memory as its primary method of storing information, but it also pioneered the use of magnetic tape storage as a secondary memory, which was capable of storing up to a million characters. The Census Bureau resisted attempts by Remington Rand to renegotiate the purchase price of the computer and spent only the $300,000 previously agreed upon, while both A.C. Nielsen and Prudential ultimately cancelled their orders when Remington Rand threatened to tie up delivery through a lawsuit to avoid selling the computers for $150,00 dollars; future customers were forced to pay a million dollars or more for a complete UNIVAC I.\n\nBy 1954, nineteen UNIVAC computers had been purchased and installed at such diverse organizations as the Pentagon, U.S. Steel, and General Electric. Most of these organizations took advantage of the computer’s large tape storage capacity to employ the computer for data processing rather than calculations, where it competed with the tabulating machines that had brought IBM to prominence.\n\nThe UNIVAC 1101, Remington Rand’s first scientific computer\n\nTo serve the scientific community, Remington Rand turned to another early computer startup, Engineering Research Associates (ERA). ERA grew out of the code-breaking activities of the United States Navy during World War II, which were carried out primarily through an organization called the Communications Supplementary Activity – Washington (CSAW). Like Bletchley Park in the United Kingdom, CSAW constructed a number of sophisticated electronic devices to aid in codebreaking, and the Navy wanted to maintain this technological capability after the war. Military budget cuts made this impractical, however, so to avoid losing the assembly of talent at CSAW, the Navy helped establish ERA in St. Paul, Minnesota, in January 1946 as a private corporation. The company was led by John Parker, a former Navy lieutenant who had become intimately involved in the airline industry in the late 1930s and 1940s while working for the D.C. investment firm Auchincloss, Parker, and Redpath, and drew most of its important technical personnel from CSAW.\n\nUnlike EMCC, which focused on building a machine for corporate data processing, ERA devoted its activities to intelligence analysis work for the United States Navy. Like Eckert and Mauchly, the founders of ERA realized the greatest impediment to building a useful electronic computing device was the lack of suitable storage technology, so in its first two years of existence, the company concentrated on solving this problem, ultimately settling on magnetic drum memory, a technology invented by Austrian Gustav Tauchek in 1932 in which a large metal cylinder is coated with a ferromagnetic magnetic material. As the drum is rotated, stationary write heads can generate an electrical pulse to change the magnetic orientation on any part of the surface of the drum, while a read head can detect the orientation and recognize it in binary as either a “1” or a “0,” therefore making it suitable for computer memory. A series of specialized cryptoanalytic machines followed with names like Goldberg and Demon, but these machines tended to become obsolete quickly since they were targeted at specific codes and were not programmable to take on new tasks. Meanwhile, as both ERA and the Navy learned more about developments at the Moore School, they decided a general purpose computer would be a better method of addressing the Navy’s needs than specialized equipment and therefore initiated Task 13 in 1947 to build a stored program computer called Atlas. Completed in December 1950, the Atlas contained 2,700 vacuum tubes and a drum memory that could hold just over 16,000 24-bit words. The computer was delivered to the National Security Agency (NSA) for code-breaking operations, and the agency was so pleased with the computer that it accepted a second unit in 1953. In December 1951, a modified version was made available as the ERA 1101 — a play on the original project name as “1101” is “13” in binary — but ERA did not furnish any manuals, so no businesses purchased the machine.\n\nThe same month ERA announced the 1101, it was purchased by Remington Rand. ERA president John Parker realized that fully entering the commercial world would require a significant influx of capital that the company would be unlikely to raise. Furthermore, the close relationship between ERA and the Navy had piqued the interest of government auditors and threatened the company’s ability to secure future government contracts. Therefore, Parker saw the Remington Rand purchase as essential to ERA’s continued survival. Remington Rand, meanwhile, gained a foothold in a new segment of the computer market. The company began marketing an improved version of ERA’s first computer as the UNIVAC 1103 in October 1953 and ultimately installed roughly twenty of them, mostly within the military-industrial complex.\n\nIn 1952, the American public was introduced to the UNIVAC in dramatic fashion when Mauchly developed a program to predict the results of the general election between Dwight Eisenhower and Adlai Stevenson based on the returns from the previous two elections. The results were to be aired publicly on CBS, but UNIVAC predicted a massive landslide for Eisenhower in opposition to Gallup polls that indicated a close race. CBS refused to deliver the results, opting instead to state that the computer predicted a close victory for Eisenhower. When it became clear that Eisenhower would actually win in a landslide, the network owned up to its deception and aired the true results, which were within just a few electoral votes of the actual total. Before long, the term “UNIVAC” became a generic word for all computers in the same way “Kleenex” has become synonymous with tissue paper and “Xerox” with photocopying. For a time, it appeared that Remington Rand would be the clear winner in the new field of electronic computers, but only until IBM finally hit its stride.\n\nIBM Enters the Computer Industry\n\nTom Watson, Sr. sits at the console of an IBM 701, the company’s first commercial computer\n\nThere is a story, oft-repeated, about Tom Watson, Sr. that claims he saw no value in computers. According to this story, the aging president of IBM scoffed that there would never be a market for more than five computers and neglected to bring IBM into the new field. Only after the debut of the UNIVAC I did IBM realize its mistake and hastily enter the computer market. While there are elements of truth to this version of events, there is no truth to the claim that IBM was completely ignoring the computer market in the late 1940s. Indeed, the company developed several electronic calculators and had no fewer than three computer projects underway when the UNIVAC I hit the market.\n\nAs previously discussed, IBM’s involvement with computers began when the company joined with Howard Aiken to develop the Automatic Sequence Controlled Calculator (ASCC). That machine was first unveiled publicly on August 6, 1944, and Tom Watson traveled to Cambridge, Massachusetts, to speak at the dedication. At the Boston train station, Watson was irked that no one from Harvard was there to welcome him. Irritation turned to rage when he perused the Boston Post and saw that Harvard had not only issued a press release about the ASCC without consulting him, but also gave sole credit to Howard Aiken for inventing the machine. When an angry and humiliated Watson returned to IBM, he ordered James Bryce and Clair Lake to develop a new machine that would make Aiken’s ASCC look like a toy. Watson wanted to show the world that IBM could build computers without help from anyone else and to get revenge on the men he felt wronged him.\n\nWith IBM seriously engaged in war work, Bryce and Lake felt they would be unable to achieve the breakthroughs in the lab necessary to best Aiken in a reasonable time frame, so instead argued for a simpler goal of creating the world’s first automatic calculator. To that end, an electronics enthusiast in the company named Haley Dickinson was ordered to convert the company’s electro-mechanical Model 601 Multiplying Punch into a tube-based machine. Unveiled in September 1946 as the IBM 603 Electronic Multiplier, the machine contained only 300 vacuum tubes and no storage, but it could multiply ten times faster than existing tabulating machines and soon became a sensation. Embarrassed by the limitations of the machine, however, Watson halted production at 100 units and ordered his engineers to develop an improved model. Ralph Palmer, an electronics expert that joined IBM in 1932 and was recently returned from a stint in the Navy, was asked to form a new laboratory in Poughkeepsie, New York, dedicated solely to electronics. Palmer’s group delivered the IBM 604 Electronic Calculating Punch in 1948, which contained 1,400 tubes and could be programmed to solve simple equations. Over the next ten years, the company leased 5,600 604’s to customers, and Watson came to realize that the future of IBM’s business lay in electronics.\n\nMeanwhile, as World War II neared its conclusion, Watson’s mandate to best Aiken’s ASCC gained momentum. The man responsible for this project was Wallace Eckert (no relation to the ENIAC co-inventor), who as an astronomy professor at Columbia in the 1920s and 1930s had been one of the main beneficiaries of Watson’s relationship with the university in those years. After directing the Nautical Almanac of the United States Naval Observatory during much of World War II, Eckert accepted an invitation from Watson in March 1945 to head a new division within IBM specifically concerned with the computational needs of the scientific community called the Pure Science Department.\n\nEckert remained at headquarters in New York while Frank Hamilton, who had been a project leader on the ASCC, took charge of defining the Aiken-beating machine’s capabilities in Endicott. In summer 1945, Eckert made new hire Rex Seeber his personal representative to the project. A Harvard graduate, Seeber had worked with Aiken, but fell out with him when he refused to implement the stored program concept in his forthcoming update of the ASCC. Seeber’s knowledge of computer theory and electronics perfectly complemented Hamilton’s electrical engineering skills and resulted in the completion of the Selective Sequence Electronic Calculator (SSEC) in 1947. The SSEC was the first machine in the world to successfully implement the stored program concept, although it is often classified as a calculator rather than a stored program computer due to its limited memory and reliance on paper tape for program control. The majority of the calculator remained electromechanical, but the arithmetic unit, adapted from the 603, operated at electronic speeds. Built with 21,400 relays and 12,500 vacuum tubes and assembled at a cost of $950,000, the SSEC was a strange hybrid that exerted no influence over the future of computing, but it did accomplish IBM’s objectives by operating 250 times faster than the Harvard ASCC while also gaining significant publicity for IBM’s computing endeavors by operating while on display to the public on the ground floor of the company’s corporate headquarters from 1948 to 1952.\n\nTom Watson, Jr., son and successor of Tom Watson, Sr.\n\nThe success of the IBM 603 and 604 showed Watson that IBM needed to embrace electronics, but he remained cautious regarding electronic computing. Indeed, when given the chance to bring Eckert and Mauchly into the IBM fold in mid-1946 after they left the Moore School, Watson ultimately turned them down not because he saw no value in their work but because he did not want to meet the price they demanded to buy out their business. When he learned that the duo’s computer company was garnering interest from the National Bureau of Standards and Prudential in 1947, he told his engineers they should explore a competing design, but he was thinking in terms of a machine tailored to the needs of specific clients rather than a general-purpose computing device. By now Watson was in his seventies and set in his ways, and while there is no evidence that he ever uttered the famous line about world demand reaching only five computers, he could simply not envision a world in which electronic computers replaced tabulating machines entirely. As a result, the push for computing within the company came instead from his son and heir apparent, Tom Watson, Jr.\n\nThomas J. Watson, Jr. was born in Dayton, Ohio, in 1914, the same year his father accepted the general manager position at C-T-R. His relationship with his father was strained for most of his life, as the elder Watson was prone to both controlling behavior and ferocious bursts of temper. While incredibly bright, Watson suffered from anxiety and crippling depression as a child and felt incapable of living up to his father’s standards or of succeeding him at IBM one day, which he sensed was his father’s wish. As a result, he rebelled and performed poorly in school, only gaining admittance to Brown University as a favor to his father. After graduating with a degree in business in 1937, he became a salesman at IBM, but grew to hate working there due to the special treatment he received as the CEO’s son and the cult of personality that had grown up around his father. Desperate for a way out, he joined the Air National Guard shortly before the United States entered World War II and became aide-de-camp to First Air Force Commander Major General Follett Bradley in 1942. He had no intention of ever returning to IBM.\n\nWorking for General Bradley, Watson finally realized his own potential. He became the general’s most trusted subordinate and gained experience managing teams undertaking difficult tasks. With the encouragement of Bradley, his inner charisma surfaced for the first time, as did a remarkable ability to focus on and explain complex problems. Near the end of the war, Bradley asked Watson about his plans for the future and was shocked when Watson said he might become a commercial pilot and would certainly never rejoin IBM. Bradley stated that he always assumed Watson would return to run the company. In that moment, Watson realized he was avoiding the company because he feared he would fail, but that his war experiences had prepared him to succeed his father. On the first business day of 1946, he returned to the fold.\n\nTom Jr. was not promoted to a leadership position right away. Instead, Tom Sr. appointed him personal assistant to Charley Kirk, the executive vice president of the company and Tom Sr.’s most trusted subordinate. Kirk generously took Tom Jr. under his wing, but he also appeared to be first in line to take over the company upon Tom Sr.’s retirement, which Tom Jr. resented. A potential power struggle was avoided when Kirk suffered a massive heart attack and died in 1947. Tom Sr. did not feel his son was quite ready to assume the executive vice president position, but Tom Jr. did assume many of Kirk’s responsibilities while an older loyal Watson supporter named George Phillips took on the executive VP role on a short-term basis. In 1952, Tom Sr. finally named Tom Jr. president of IBM.\n\nThe IBM 650, IBM’s most successful early computer\n\nTom Jr. first learned of the advances being made in computing in 1946 when he and Kirk traveled to the Moore School to see the ENIAC. He became a staunch supporter of electronics and computing from that day forward. While there was no formal division of responsibilities drawn up between father and son, it was understood from the late forties until Tom Jr. succeeded his father as IBM CEO in 1956 that Tom Jr. would be given free reign to develop IBM’s electronics and computing businesses, while Tom Sr. concentrated on the traditional tabulating machine business. In this capacity, Tom Jr. played a significant role in overcoming bias within IBM’s engineering, sales, and future demands divisions towards new technologies and brought IBM fully into the computer age.\n\nBy 1950, IBM had two computer projects in progress. The first had been started in 1948 when Tom Watson, Sr. ordered his engineers to adapt the SSEC into something cheaper that could be mass produced and sold to IBM’s business customers. With James Bryce incapacitated — he would die the next year — the responsibility of shaping the new machine fell to Wallace Eckert, Frank Hamilton, and John McPherson, an IBM vice president that had been instrumental in constructing two powerful relay calculators for the Aberdeen Proving Grounds during World War II. The trio decided to create a machine focused on scientific and engineering applications, both because this was their primary area of expertise and because with the dawn of the Cold War the United States government was funding over a dozen scientific computing projects to maintain the technological edge it had built during World War II. There was a real fear that if IBM did not stay relevant in this area, one of these projects could birth a company capable of challenging IBM’s dominant position in business machines.\n\nHamilton acted as the chief engineer on the project and chose to increase the system’s memory capacity by incorporating magnetic drum storage, thus leading to the machine’s designation as the Magnetic Drum Calculator (MDC). While the MDC began life as a calculator essentially pairing an IBM 603 with a magnetic drum, the realization that drum memory was expansive enough that a paper tape reader could be discarded entirely and instructions could be read and modified directly from the drum itself caused the project to morph into a full-fledged computer. By early 1950, engineering work had commenced on the MDC, but development soon stalled as it became the focus of fights between multiple engineering teams as well as the sales and future demands departments over its specifications, target audience, and potential commercial performance.\n\nWhile work continued on the MDC in Endicott, several IBM engineers in the electronics laboratory in Poughkeepsie initiated their own experiments related to computer technology. In 1948, an engineer named Philip Fox began studying alternate solutions to vacuum tube memory that would allow for a stored-program computer. Learning of the Williams Tube in 1948, he decided to focus his attention on CRT memory. Fox created a machine called the Test Assembly on which he worked to improve on the reliability of existing CRT memory solutions. Meanwhile, in early 1949, a new employee named Nathaniel Rochester who was dismayed that IBM did not already have a stored-program computer in production began researching the capabilities of magnetic tape as a storage medium. These disparate threads came together in October 1949 when a decision was made to focus on the development of a tape machine to challenge the UNIVAC, which appeared poised to grab a share of IBM’s data processing business. By March 1950, Rochester and Werner Buchholz had completed a technical outline of the Tape Processing Machine (TPM), which would incorporate both CRT and tape memory. As with the MDC, however, sales and future demands’ inability to clearly define a market for the computer hindered its development.\n\nA breakthrough in the stalemate between sales and engineering finally occurred with the outbreak of the Korean War. As he had when the United States entered World War II, Tom Watson, Sr. placed the full capabilities of the company at the disposal of the United States government. The United States Air Force quickly responded that it wanted help developing a new electro-mechanical bombsight for the B-47 Bomber, but Tom Watson, Jr., who already believed IBM was not embracing electronics fast enough, felt working on electro-mechanical projects to be a giant step backwards for the company. Instead, he proposed developing an electronic computer suitable for scientific computation by government organizations and contractors.\n\nInitially, IBM considered adapting the TPM for its new scientific computer project, but quickly abandoned the idea. To save on cost, the engineering team of the TPM had decided to design the computer to process numbers serially rather than in parallel, which was sufficient for data processing, but made the machine too slow to meet the computational needs of the government. Therefore, in September 1950 Ralph Palmer’s engineers drew up preliminary plans for a floating-point decimal computer hooked up to an array of tape readers and other auxiliary devices that would be capable of well over 10,000 operations a second and of storing 2000 thirteen-digit words in Williams Tube memory. Watson Jr. approved this project in January 1950 under the moniker “Defense Calculator.” With a tight deadline of Spring 1952 in place for the Defense Calculator so it would be operational in time to contribute to the war effort, Palmer realized the engineering team, led by Nathaniel Rochester and Jerrier Haddad, could not afford to start from scratch on the design of the new computer, so they decided to base the architecture on von Neumann’s IAS Machine.\n\nThe IBM 702, IBM’s first computer targeted at businesses\n\nOn April 29, 1952, Tom Watson, Sr. announced the existence of the Defense Calculator to IBM’s shareholders at the company’s annual meeting. In December, the first completed model was installed at IBM headquarters in the berth occupied until then by the SSEC. On April 7, 1953, the company staged a public unveiling of the Defense Calculator under the name IBM 701 Electronic Data Pro"
    }
}
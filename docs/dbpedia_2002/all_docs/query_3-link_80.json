{
    "id": "dbpedia_2002_3",
    "rank": 80,
    "data": {
        "url": "https://www.science.gov/topicpages/p/parenchymal%2Btexture%2Bfeatures",
        "read_more_link": "",
        "language": "en",
        "title": "parenchymal texture features: Topics by Science.gov",
        "top_image": "",
        "meta_img": "",
        "images": [
            "https://www.science.gov/scigov/desktop/en/images/SciGov_logo.png"
        ],
        "movies": [],
        "keywords": [],
        "meta_keywords": [
            ""
        ],
        "tags": null,
        "authors": [],
        "publish_date": null,
        "summary": "",
        "meta_description": "",
        "meta_lang": "en",
        "meta_favicon": "",
        "meta_site_name": "",
        "canonical_link": null,
        "text": "Parenchymal texture analysis in digital mammography: robust texture feature identification and equivalence across devices.\n\nPubMed\n\nKeller, Brad M; Oustimov, Andrew; Wang, Yan; Chen, Jinbo; Acciavatti, Raymond J; Zheng, Yuanjie; Ray, Shonket; Gee, James C; Maidment, Andrew D A; Kontos, Despina\n\n2015-04-01\n\nAn analytical framework is presented for evaluating the equivalence of parenchymal texture features across different full-field digital mammography (FFDM) systems using a physical breast phantom. Phantom images (FOR PROCESSING) are acquired from three FFDM systems using their automated exposure control setting. A panel of texture features, including gray-level histogram, co-occurrence, run length, and structural descriptors, are extracted. To identify features that are robust across imaging systems, a series of equivalence tests are performed on the feature distributions, in which the extent of their intersystem variation is compared to their intrasystem variation via the Hodges-Lehmann test statistic. Overall, histogram and structural features tend to be most robust across all systems, and certain features, such as edge enhancement, tend to be more robust to intergenerational differences between detectors of a single vendor than to intervendor differences. Texture features extracted from larger regions of interest (i.e., [Formula: see text]) and with a larger offset length (i.e., [Formula: see text]), when applicable, also appear to be more robust across imaging systems. This framework and observations from our experiments may benefit applications utilizing mammographic texture analysis on images acquired in multivendor settings, such as in multicenter studies of computer-aided detection and breast cancer risk assessment.\n\nParenchymal Texture Analysis in Digital Breast Tomosynthesis for Breast Cancer Risk Estimation: A Preliminary Study\n\nPubMed Central\n\nKontos, Despina; Bakic, Predrag R.; Carton, Ann-Katherine; Troxel, Andrea B.; Conant, Emily F.; Maidment, Andrew D.A.\n\n2009-01-01\n\nRationale and Objectives Studies have demonstrated a relationship between mammographic parenchymal texture and breast cancer risk. Although promising, texture analysis in mammograms is limited by tissue superimposition. Digital breast tomosynthesis (DBT) is a novel tomographic x-ray breast imaging modality that alleviates the effect of tissue superimposition, offering superior parenchymal texture visualization compared to mammography. Our study investigates the potential advantages of DBT parenchymal texture analysis for breast cancer risk estimation. Materials and Methods DBT and digital mammography (DM) images of 39 women were analyzed. Texture features, shown in studies with mammograms to correlate with cancer risk, were computed from the retroareolar breast region. We compared the relative performance of DBT and DM texture features in correlating with two measures of breast cancer risk: (i) the Gail and Claus risk estimates, and (ii) mammographic breast density. Linear regression was performed to model the association between texture features and increasing levels of risk. Results No significant correlation was detected between parenchymal texture and the Gail and Claus risk estimates. Significant correlations were observed between texture features and breast density. Overall, the DBT texture features demonstrated stronger correlations with breast percent density (PD) than DM (p â¤0.05). When dividing our study population in groups of increasing breast PD, the DBT texture features appeared to be more discriminative, having regression lines with overall lower p-values, steeper slopes, and higher R2 estimates. Conclusion Although preliminary, our results suggest that DBT parenchymal texture analysis could provide more accurate characterization of breast density patterns, which could ultimately improve breast cancer risk estimation. PMID:19201357\n\nParenchymal texture measures weighted by breast anatomy: preliminary optimization in a case-control study\n\nNASA Astrophysics Data System (ADS)\n\nGastounioti, Aimilia; Keller, Brad M.; Hsieh, Meng-Kang; Conant, Emily F.; Kontos, Despina\n\n2016-03-01\n\nGrowing evidence suggests that quantitative descriptors of the parenchymal texture patterns hold a valuable role in assessing an individual woman's risk for breast cancer. In this work, we assess the hypothesis that breast cancer risk factors are not uniformly expressed in the breast parenchymal tissue and, therefore, breast-anatomy-weighted parenchymal texture descriptors, where different breasts ROIs have non uniform contributions, may enhance breast cancer risk assessment. To this end, we introduce an automated breast-anatomy-driven methodology which generates a breast atlas, which is then used to produce a weight map that reinforces the contributions of the central and upper-outer breast areas. We incorporate this methodology to our previously validated lattice-based strategy for parenchymal texture analysis. In the framework of a pilot case-control study, including digital mammograms from 424 women, our proposed breast-anatomy-weighted texture descriptors are optimized and evaluated against non weighted texture features, using regression analysis with leave-one-out cross validation. The classification performance is assessed in terms of the area under the curve (AUC) of the receiver operating characteristic. The collective discriminatory capacity of the weighted texture features was maximized (AUC=0.87) when the central breast area was considered more important than the upperouter area, with significant performance improvement (DeLong's test, p-value<0.05) against the non-weighted texture features (AUC=0.82). Our results suggest that breast-anatomy-driven methodologies have the potential to further upgrade the promising role of parenchymal texture analysis in breast cancer risk assessment and may serve as a reference in the design of future studies towards image-driven personalized recommendations regarding women's cancer risk evaluation.\n\nParenchymal texture analysis in digital mammography: A fully automated pipeline for breast cancer risk assessment.\n\nPubMed\n\nZheng, Yuanjie; Keller, Brad M; Ray, Shonket; Wang, Yan; Conant, Emily F; Gee, James C; Kontos, Despina\n\n2015-07-01\n\nMammographic percent density (PD%) is known to be a strong risk factor for breast cancer. Recent studies also suggest that parenchymal texture features, which are more granular descriptors of the parenchymal pattern, can provide additional information about breast cancer risk. To date, most studies have measured mammographic texture within selected regions of interest (ROIs) in the breast, which cannot adequately capture the complexity of the parenchymal pattern throughout the whole breast. To better characterize patterns of the parenchymal tissue, the authors have developed a fully automated software pipeline based on a novel lattice-based strategy to extract a range of parenchymal texture features from the entire breast region. Digital mammograms from 106 cases with 318 age-matched controls were retrospectively analyzed. The lattice-based approach is based on a regular grid virtually overlaid on each mammographic image. Texture features are computed from the intersection (i.e., lattice) points of the grid lines within the breast, using a local window centered at each lattice point. Using this strategy, a range of statistical (gray-level histogram, co-occurrence, and run-length) and structural (edge-enhancing, local binary pattern, and fractal dimension) features are extracted. To cover the entire breast, the size of the local window for feature extraction is set equal to the lattice grid spacing and optimized experimentally by evaluating different windows sizes. The association between their lattice-based texture features and breast cancer was evaluated using logistic regression with leave-one-out cross validation and further compared to that of breast PD% and commonly used single-ROI texture features extracted from the retroareolar or the central breast region. Classification performance was evaluated using the area under the curve (AUC) of the receiver operating characteristic (ROC). DeLong's test was used to compare the different ROCs in terms of AUC\n\nMammographic parenchymal texture as an imaging marker of hormonal activity: a comparative study between pre- and post-menopausal women\n\nNASA Astrophysics Data System (ADS)\n\nDaye, Dania; Bobo, Ezra; Baumann, Bethany; Ioannou, Antonios; Conant, Emily F.; Maidment, Andrew D. A.; Kontos, Despina\n\n2011-03-01\n\nMammographic parenchymal texture patterns have been shown to be related to breast cancer risk. Yet, little is known about the biological basis underlying this association. Here, we investigate the potential of mammographic parenchymal texture patterns as an inherent phenotypic imaging marker of endogenous hormonal exposure of the breast tissue. Digital mammographic (DM) images in the cranio-caudal (CC) view of the unaffected breast from 138 women diagnosed with unilateral breast cancer were retrospectively analyzed. Menopause status was used as a surrogate marker of endogenous hormonal activity. Retroareolar 2.5cm2 ROIs were segmented from the post-processed DM images using an automated algorithm. Parenchymal texture features of skewness, coarseness, contrast, energy, homogeneity, grey-level spatial correlation, and fractal dimension were computed. Receiver operating characteristic (ROC) curve analysis was performed to evaluate feature classification performance in distinguishing between 72 pre- and 66 post-menopausal women. Logistic regression was performed to assess the independent effect of each texture feature in predicting menopause status. ROC analysis showed that texture features have inherent capacity to distinguish between pre- and post-menopausal statuses (AUC>0.5, p<0.05). Logistic regression including all texture features yielded an ROC curve with an AUC of 0.76. Addition of age at menarche, ethnicity, contraception use and hormonal replacement therapy (HRT) use lead to a modest model improvement (AUC=0.78) while texture features maintained significant contribution (p<0.05). The observed differences in parenchymal texture features between pre- and post- menopausal women suggest that mammographic texture can potentially serve as a surrogate imaging marker of endogenous hormonal activity.\n\nMRI Texture Analysis of Background Parenchymal Enhancement of the Breast\n\nPubMed Central\n\nWoo, Jun; Amano, Maki; Yanagisawa, Fumi; Yamamoto, Hiroshi; Tani, Mayumi\n\n2017-01-01\n\nPurpose The purpose of this study was to determine texture parameters reflecting the background parenchymal enhancement (BPE) of the breast, which were acquired using texture analysis (TA). Methods We investigated 52 breasts of the 26 subjects who underwent dynamic contrast-enhanced MRI. One experienced reader scored BPE visually (i.e., minimal, mild, moderate, and marked). TA, including 12 texture parameters, was performed to distinguish the BPE scores quantitatively. Relationships between the visual BPE scores and texture parameters were evaluated using analysis of variance and receiver operating characteristic analysis. Results The variance and skewness of signal intensity were useful for differentiating between moderate and mild or minimal BPE or between mild and minimal BPE, respectively, with the cutoff value of 356.7 for variance and that of 0.21 for skewness. Some TA features could be useful for defining breast lesions from the BPE. Conclusion TA may be useful for quantifying the BPE of the breast. PMID:28812015\n\nBreast density and parenchymal texture measures as potential risk factors for estrogen-receptor positive breast cancer\n\nNASA Astrophysics Data System (ADS)\n\nKeller, Brad M.; Chen, Jinbo; Conant, Emily F.; Kontos, Despina\n\n2014-03-01\n\nAccurate assessment of a woman's risk to develop specific subtypes of breast cancer is critical for appropriate utilization of chemopreventative measures, such as with tamoxifen in preventing estrogen-receptor positive breast cancer. In this context, we investigate quantitative measures of breast density and parenchymal texture, measures of glandular tissue content and tissue structure, as risk factors for estrogen-receptor positive (ER+) breast cancer. Mediolateral oblique (MLO) view digital mammograms of the contralateral breast from 106 women with unilateral invasive breast cancer were retrospectively analyzed. Breast density and parenchymal texture were analyzed via fully-automated software. Logistic regression with feature selection and was performed to predict ER+ versus ER- cancer status. A combined model considering all imaging measures extracted was compared to baseline models consisting of density-alone and texture-alone features. Area under the curve (AUC) of the receiver operating characteristic (ROC) and Delong's test were used to compare the models' discriminatory capacity for receptor status. The density-alone model had a discriminatory capacity of 0.62 AUC (p=0.05). The texture-alone model had a higher discriminatory capacity of 0.70 AUC (p=0.001), which was not significantly different compared to the density-alone model (p=0.37). In contrast the combined density-texture logistic regression model had a discriminatory capacity of 0.82 AUC (p<0.001), which was statistically significantly higher than both the density-alone (p<0.001) and texture-alone regression models (p=0.04). The combination of breast density and texture measures may have the potential to identify women specifically at risk for estrogen-receptor positive breast cancer and could be useful in triaging women into appropriate risk-reduction strategies.\n\nSlice simulation from a model of the parenchymous vascularization to evaluate texture features: work in progress.\n\nPubMed\n\nRolland, Y; BÃ©zy-Wendling, J; Duvauferrier, R; Coatrieux, J L\n\n1999-03-01\n\nTo demonstrate the usefulness of a model of the parenchymous vascularization to evaluate texture analysis methods. Slices with thickness varying from 1 to 4 mm were reformatted from a 3D vascular model corresponding to either normal tissue perfusion or local hypervascularization. Parameters of statistical methods were measured on 16128x128 regions of interest, and mean values and standard deviation were calculated. For each parameter, the performances (discrimination power and stability) were evaluated. Among 11 calculated statistical parameters, three (homogeneity, entropy, mean of gradients) were found to have a good discriminating power to differentiate normal perfusion from hypervascularization, but only the gradient mean was found to have a good stability with respect to the thickness. Five parameters (run percentage, run length distribution, long run emphasis, contrast, and gray level distribution) were found to have intermediate results. In the remaining three, curtosis and correlation was found to have little discrimination power, skewness none. This 3D vascular model, which allows the generation of various examples of vascular textures, is a powerful tool to assess the performance of texture analysis methods. This improves our knowledge of the methods and should contribute to their a priori choice when designing clinical studies.\n\nParameter optimization of parenchymal texture analysis for prediction of false-positive recalls from screening mammography\n\nNASA Astrophysics Data System (ADS)\n\nRay, Shonket; Keller, Brad M.; Chen, Jinbo; Conant, Emily F.; Kontos, Despina\n\n2016-03-01\n\nThis work details a methodology to obtain optimal parameter values for a locally-adaptive texture analysis algorithm that extracts mammographic texture features representative of breast parenchymal complexity for predicting falsepositive (FP) recalls from breast cancer screening with digital mammography. The algorithm has two components: (1) adaptive selection of localized regions of interest (ROIs) and (2) Haralick texture feature extraction via Gray- Level Co-Occurrence Matrices (GLCM). The following parameters were systematically varied: mammographic views used, upper limit of the ROI window size used for adaptive ROI selection, GLCM distance offsets, and gray levels (binning) used for feature extraction. Each iteration per parameter set had logistic regression with stepwise feature selection performed on a clinical screening cohort of 474 non-recalled women and 68 FP recalled women; FP recall prediction was evaluated using area under the curve (AUC) of the receiver operating characteristic (ROC) and associations between the extracted features and FP recall were assessed via odds ratios (OR). A default instance of mediolateral (MLO) view, upper ROI size limit of 143.36 mm (2048 pixels2), GLCM distance offset combination range of 0.07 to 0.84 mm (1 to 12 pixels) and 16 GLCM gray levels was set. The highest ROC performance value of AUC=0.77 [95% confidence intervals: 0.71-0.83] was obtained at three specific instances: the default instance, upper ROI window equal to 17.92 mm (256 pixels2), and gray levels set to 128. The texture feature of sum average was chosen as a statistically significant (p<0.05) predictor and associated with higher odds of FP recall for 12 out of 14 total instances.\n\nConvolutional neural network approach for enhanced capture of breast parenchymal complexity patterns associated with breast cancer risk\n\nNASA Astrophysics Data System (ADS)\n\nOustimov, Andrew; Gastounioti, Aimilia; Hsieh, Meng-Kang; Pantalone, Lauren; Conant, Emily F.; Kontos, Despina\n\n2017-03-01\n\nWe assess the feasibility of a parenchymal texture feature fusion approach, utilizing a convolutional neural network (ConvNet) architecture, to benefit breast cancer risk assessment. Hypothesizing that by capturing sparse, subtle interactions between localized motifs present in two-dimensional texture feature maps derived from mammographic images, a multitude of texture feature descriptors can be optimally reduced to five meta-features capable of serving as a basis on which a linear classifier, such as logistic regression, can efficiently assess breast cancer risk. We combine this methodology with our previously validated lattice-based strategy for parenchymal texture analysis and we evaluate the feasibility of this approach in a case-control study with 424 digital mammograms. In a randomized split-sample setting, we optimize our framework in training/validation sets (N=300) and evaluate its descriminatory performance in an independent test set (N=124). The discriminatory capacity is assessed in terms of the the area under the curve (AUC) of the receiver operator characteristic (ROC). The resulting meta-features exhibited strong classification capability in the test dataset (AUC = 0.90), outperforming conventional, non-fused, texture analysis which previously resulted in an AUC=0.85 on the same case-control dataset. Our results suggest that informative interactions between localized motifs exist and can be extracted and summarized via a fairly simple ConvNet architecture.\n\nTextural features for radar image analysis\n\nNASA Technical Reports Server (NTRS)\n\nShanmugan, K. S.; Narayanan, V.; Frost, V. S.; Stiles, J. A.; Holtzman, J. C.\n\n1981-01-01\n\nTexture is seen as an important spatial feature useful for identifying objects or regions of interest in an image. While textural features have been widely used in analyzing a variety of photographic images, they have not been used in processing radar images. A procedure for extracting a set of textural features for characterizing small areas in radar images is presented, and it is shown that these features can be used in classifying segments of radar images corresponding to different geological formations.\n\nAssociations Between PET Textural Features and GLUT1 Expression, and the Prognostic Significance of Textural Features in Lung Adenocarcinoma.\n\nPubMed\n\nKoh, Young Wha; Park, Seong Yong; Hyun, Seung Hyup; Lee, Su Jin\n\n2018-02-01\n\nWe evaluated the association between positron emission tomography (PET) textural features and glucose transporter 1 (GLUT1) expression level and further investigated the prognostic significance of textural features in lung adenocarcinoma. We evaluated 105 adenocarcinoma patients. We extracted texture-based PET parameters of primary tumors. Conventional PET parameters were also measured. The relationships between PET parameters and GLUT1 expression levels were evaluated. The association between PET parameters and overall survival (OS) was assessed using Cox's proportional hazard regression models. In terms of PET textural features, tumors expressing high levels of GLUT1 exhibited significantly lower coarseness, contrast, complexity, and strength, but significantly higher busyness. On univariate analysis, the metabolic tumor volume, total lesion glycolysis, contrast, busyness, complexity, and strength were significant predictors of OS. Multivariate analysis showed that lower complexity (HR=2.017, 95%CI=1.032-3.942, p=0.040) was independently associated with poorer survival. PET textural features may aid risk stratification in lung adenocarcinoma patients. CopyrightÂ© 2018, International Institute of Anticancer Research (Dr. George J. Delinasios), All rights reserved.\n\nCloud field classification based on textural features\n\nNASA Technical Reports Server (NTRS)\n\nSengupta, Sailes Kumar\n\n1989-01-01\n\nAn essential component in global climate research is accurate cloud cover and type determination. Of the two approaches to texture-based classification (statistical and textural), only the former is effective in the classification of natural scenes such as land, ocean, and atmosphere. In the statistical approach that was adopted, parameters characterizing the stochastic properties of the spatial distribution of grey levels in an image are estimated and then used as features for cloud classification. Two types of textural measures were used. One is based on the distribution of the grey level difference vector (GLDV), and the other on a set of textural features derived from the MaxMin cooccurrence matrix (MMCM). The GLDV method looks at the difference D of grey levels at pixels separated by a horizontal distance d and computes several statistics based on this distribution. These are then used as features in subsequent classification. The MaxMin tectural features on the other hand are based on the MMCM, a matrix whose (I,J)th entry give the relative frequency of occurrences of the grey level pair (I,J) that are consecutive and thresholded local extremes separated by a given pixel distance d. Textural measures are then computed based on this matrix in much the same manner as is done in texture computation using the grey level cooccurrence matrix. The database consists of 37 cloud field scenes from LANDSAT imagery using a near IR visible channel. The classification algorithm used is the well known Stepwise Discriminant Analysis. The overall accuracy was estimated by the percentage or correct classifications in each case. It turns out that both types of classifiers, at their best combination of features, and at any given spatial resolution give approximately the same classification accuracy. A neural network based classifier with a feed forward architecture and a back propagation training algorithm is used to increase the classification accuracy, using these two classes\n\nCloud and surface textural features in polar regions\n\nNASA Technical Reports Server (NTRS)\n\nWelch, Ronald M.; Kuo, Kwo-Sen; Sengupta, Sailes K.\n\n1990-01-01\n\nThe study examines the textural signatures of clouds, ice-covered mountains, solid and broken sea ice and floes, and open water. The textural features are computed from sum and difference histogram and gray-level difference vector statistics defined at various pixel displacement distances derived from Landsat multispectral scanner data. Polar cloudiness, snow-covered mountainous regions, solid sea ice, glaciers, and open water have distinguishable texture features. This suggests that textural measures can be successfully applied to the detection of clouds over snow-covered mountains, an ability of considerable importance for the modeling of snow-melt runoff. However, broken stratocumulus cloud decks and thin cirrus over broken sea ice remain difficult to distinguish texturally. It is concluded that even with high spatial resolution imagery, it may not be possible to distinguish broken stratocumulus and thin clouds from sea ice in the marginal ice zone using the visible channel textural features alone.\n\nA parametric texture model based on deep convolutional features closely matches texture appearance for humans.\n\nPubMed\n\nWallis, Thomas S A; Funke, Christina M; Ecker, Alexander S; Gatys, Leon A; Wichmann, Felix A; Bethge, Matthias\n\n2017-10-01\n\nOur visual environment is full of texture-\"stuff\" like cloth, bark, or gravel as distinct from \"things\" like dresses, trees, or paths-and humans are adept at perceiving subtle variations in material properties. To investigate image features important for texture perception, we psychophysically compare a recent parametric model of texture appearance (convolutional neural network [CNN] model) that uses the features encoded by a deep CNN (VGG-19) with two other models: the venerable Portilla and Simoncelli model and an extension of the CNN model in which the power spectrum is additionally matched. Observers discriminated model-generated textures from original natural textures in a spatial three-alternative oddity paradigm under two viewing conditions: when test patches were briefly presented to the near-periphery (\"parafoveal\") and when observers were able to make eye movements to all three patches (\"inspection\"). Under parafoveal viewing, observers were unable to discriminate 10 of 12 original images from CNN model images, and remarkably, the simpler Portilla and Simoncelli model performed slightly better than the CNN model (11 textures). Under foveal inspection, matching CNN features captured appearance substantially better than the Portilla and Simoncelli model (nine compared to four textures), and including the power spectrum improved appearance matching for two of the three remaining textures. None of the models we test here could produce indiscriminable images for one of the 12 textures under the inspection condition. While deep CNN (VGG-19) features can often be used to synthesize textures that humans cannot discriminate from natural textures, there is currently no uniformly best model for all textures and viewing conditions.\n\nA Fourier-based textural feature extraction procedure\n\nNASA Technical Reports Server (NTRS)\n\nStromberg, W. D.; Farr, T. G.\n\n1986-01-01\n\nA procedure is presented to discriminate and characterize regions of uniform image texture. The procedure utilizes textural features consisting of pixel-by-pixel estimates of the relative emphases of annular regions of the Fourier transform. The utility and derivation of the features are described through presentation of a theoretical justification of the concept followed by a heuristic extension to a real environment. Two examples are provided that validate the technique on synthetic images and demonstrate its applicability to the discrimination of geologic texture in a radar image of a tropical vegetated area.\n\nNovel chromatin texture features for the classification of pap smears\n\nNASA Astrophysics Data System (ADS)\n\nBejnordi, Babak E.; Moshavegh, Ramin; Sujathan, K.; Malm, Patrik; Bengtsson, Ewert; Mehnert, Andrew\n\n2013-03-01\n\nThis paper presents a set of novel structural texture features for quantifying nuclear chromatin patterns in cells on a conventional Pap smear. The features are derived from an initial segmentation of the chromatin into bloblike texture primitives. The results of a comprehensive feature selection experiment, including the set of proposed structural texture features and a range of different cytology features drawn from the literature, show that two of the four top ranking features are structural texture features. They also show that a combination of structural and conventional features yields a classification performance of 0.954Â±0.019 (AUCÂ±SE) for the discrimination of normal (NILM) and abnormal (LSIL and HSIL) slides. The results of a second classification experiment, using only normal-appearing cells from both normal and abnormal slides, demonstrates that a single structural texture feature measuring chromatin margination yields a classification performance of 0.815Â±0.019. Overall the results demonstrate the efficacy of the proposed structural approach and that it is possible to detect malignancy associated changes (MACs) in Papanicoloau stain.\n\nCombining multiple features for color texture classification\n\nNASA Astrophysics Data System (ADS)\n\nCusano, Claudio; Napoletano, Paolo; Schettini, Raimondo\n\n2016-11-01\n\nThe analysis of color and texture has a long history in image analysis and computer vision. These two properties are often considered as independent, even though they are strongly related in images of natural objects and materials. Correlation between color and texture information is especially relevant in the case of variable illumination, a condition that has a crucial impact on the effectiveness of most visual descriptors. We propose an ensemble of hand-crafted image descriptors designed to capture different aspects of color textures. We show that the use of these descriptors in a multiple classifiers framework makes it possible to achieve a very high classification accuracy in classifying texture images acquired under different lighting conditions. A powerful alternative to hand-crafted descriptors is represented by features obtained with deep learning methods. We also show how the proposed combining strategy hand-crafted and convolutional neural networks features can be used together to further improve the classification accuracy. Experimental results on a food database (raw food texture) demonstrate the effectiveness of the proposed strategy.\n\nTextural features for image classification\n\nNASA Technical Reports Server (NTRS)\n\nHaralick, R. M.; Dinstein, I.; Shanmugam, K.\n\n1973-01-01\n\nDescription of some easily computable textural features based on gray-tone spatial dependances, and illustration of their application in category-identification tasks of three different kinds of image data - namely, photomicrographs of five kinds of sandstones, 1:20,000 panchromatic aerial photographs of eight land-use categories, and ERTS multispectral imagery containing several land-use categories. Two kinds of decision rules are used - one for which the decision regions are convex polyhedra (a piecewise-linear decision rule), and one for which the decision regions are rectangular parallelpipeds (a min-max decision rule). In each experiment the data set was divided into two parts, a training set and a test set. Test set identification accuracy is 89% for the photomicrographs, 82% for the aerial photographic imagery, and 83% for the satellite imagery. These results indicate that the easily computable textural features probably have a general applicability for a wide variety of image-classification applications.\n\nModel-Based Learning of Local Image Features for Unsupervised Texture Segmentation\n\nNASA Astrophysics Data System (ADS)\n\nKiechle, Martin; Storath, Martin; Weinmann, Andreas; Kleinsteuber, Martin\n\n2018-04-01\n\nFeatures that capture well the textural patterns of a certain class of images are crucial for the performance of texture segmentation methods. The manual selection of features or designing new ones can be a tedious task. Therefore, it is desirable to automatically adapt the features to a certain image or class of images. Typically, this requires a large set of training images with similar textures and ground truth segmentation. In this work, we propose a framework to learn features for texture segmentation when no such training data is available. The cost function for our learning process is constructed to match a commonly used segmentation model, the piecewise constant Mumford-Shah model. This means that the features are learned such that they provide an approximately piecewise constant feature image with a small jump set. Based on this idea, we develop a two-stage algorithm which first learns suitable convolutional features and then performs a segmentation. We note that the features can be learned from a small set of images, from a single image, or even from image patches. The proposed method achieves a competitive rank in the Prague texture segmentation benchmark, and it is effective for segmenting histological images.\n\nExtraction of texture features with a multiresolution neural network\n\nNASA Astrophysics Data System (ADS)\n\nLepage, Richard; Laurendeau, Denis; Gagnon, Roger A.\n\n1992-09-01\n\nTexture is an important surface characteristic. Many industrial materials such as wood, textile, or paper are best characterized by their texture. Detection of defaults occurring on such materials or classification for quality control anD matching can be carried out through careful texture analysis. A system for the classification of pieces of wood used in the furniture industry is proposed. This paper is concerned with a neural network implementation of the features extraction and classification components of the proposed system. Texture appears differently depending at which spatial scale it is observed. A complete description of a texture thus implies an analysis at several spatial scales. We propose a compact pyramidal representation of the input image for multiresolution analysis. The feature extraction system is implemented on a multilayer artificial neural network. Each level of the pyramid, which is a representation of the input image at a given spatial resolution scale, is mapped into a layer of the neural network. A full resolution texture image is input at the base of the pyramid and a representation of the texture image at multiple resolutions is generated by the feedforward pyramid structure of the neural network. The receptive field of each neuron at a given pyramid level is preprogrammed as a discrete Gaussian low-pass filter. Meaningful characteristics of the textured image must be extracted if a good resolving power of the classifier must be achieved. Local dominant orientation is the principal feature which is extracted from the textured image. Local edge orientation is computed with a Sobel mask at four orientation angles (multiple of (pi) /4). The resulting intrinsic image, that is, the local dominant orientation image, is fed to the texture classification neural network. The classification network is a three-layer feedforward back-propagation neural network.\n\nClassification of interstitial lung disease patterns with topological texture features\n\nNASA Astrophysics Data System (ADS)\n\nHuber, Markus B.; Nagarajan, Mahesh; Leinsinger, Gerda; Ray, Lawrence A.; WismÃ¼ller, Axel\n\n2010-03-01\n\nTopological texture features were compared in their ability to classify morphological patterns known as 'honeycombing' that are considered indicative for the presence of fibrotic interstitial lung diseases in high-resolution computed tomography (HRCT) images. For 14 patients with known occurrence of honey-combing, a stack of 70 axial, lung kernel reconstructed images were acquired from HRCT chest exams. A set of 241 regions of interest of both healthy and pathological (89) lung tissue were identified by an experienced radiologist. Texture features were extracted using six properties calculated from gray-level co-occurrence matrices (GLCM), Minkowski Dimensions (MDs), and three Minkowski Functionals (MFs, e.g. MF.euler). A k-nearest-neighbor (k-NN) classifier and a Multilayer Radial Basis Functions Network (RBFN) were optimized in a 10-fold cross-validation for each texture vector, and the classification accuracy was calculated on independent test sets as a quantitative measure of automated tissue characterization. A Wilcoxon signed-rank test was used to compare two accuracy distributions and the significance thresholds were adjusted for multiple comparisons by the Bonferroni correction. The best classification results were obtained by the MF features, which performed significantly better than all the standard GLCM and MD features (p < 0.005) for both classifiers. The highest accuracy was found for MF.euler (97.5%, 96.6%; for the k-NN and RBFN classifier, respectively). The best standard texture features were the GLCM features 'homogeneity' (91.8%, 87.2%) and 'absolute value' (90.2%, 88.5%). The results indicate that advanced topological texture features can provide superior classification performance in computer-assisted diagnosis of interstitial lung diseases when compared to standard texture analysis methods.\n\nTexture Feature Extraction and Classification for Iris Diagnosis\n\nNASA Astrophysics Data System (ADS)\n\nMa, Lin; Li, Naimin\n\nAppling computer aided techniques in iris image processing, and combining occidental iridology with the traditional Chinese medicine is a challenging research area in digital image processing and artificial intelligence. This paper proposes an iridology model that consists the iris image pre-processing, texture feature analysis and disease classification. To the pre-processing, a 2-step iris localization approach is proposed; a 2-D Gabor filter based texture analysis and a texture fractal dimension estimation method are proposed for pathological feature extraction; and at last support vector machines are constructed to recognize 2 typical diseases such as the alimentary canal disease and the nerve system disease. Experimental results show that the proposed iridology diagnosis model is quite effective and promising for medical diagnosis and health surveillance for both hospital and public use.\n\nTexture and color features for tile classification\n\nNASA Astrophysics Data System (ADS)\n\nBaldrich, Ramon; Vanrell, Maria; Villanueva, Juan J.\n\n1999-09-01\n\nIn this paper we present the results of a preliminary computer vision system to classify the production of a ceramic tile industry. We focus on the classification of a specific type of tiles whose production can be affected by external factors, such as humidity, temperature, origin of clays and pigments. Variations on these uncontrolled factors provoke small differences in the color and the texture of the tiles that force to classify all the production. A constant and non- subjective classification would allow avoiding devolution from customers and unnecessary stock fragmentation. The aim of this work is to simulate the human behavior on this classification task by extracting a set of features from tile images. These features are induced by definitions from experts. To compute them we need to mix color and texture information and to define global and local measures. In this work, we do not seek a general texture-color representation, we only deal with textures formed by non-oriented colored-blobs randomly distributed. New samples are classified using Discriminant Analysis functions derived from known class tile samples. The last part of the paper is devoted to explain the correction of acquired images in order to avoid time and geometry illumination changes.\n\nImproved pulmonary nodule classification utilizing quantitative lung parenchyma features.\n\nPubMed\n\nDilger, Samantha K N; Uthoff, Johanna; Judisch, Alexandra; Hammond, Emily; Mott, Sarah L; Smith, Brian J; Newell, John D; Hoffman, Eric A; Sieren, Jessica C\n\n2015-10-01\n\nCurrent computer-aided diagnosis (CAD) models for determining pulmonary nodule malignancy characterize nodule shape, density, and border in computed tomography (CT) data. Analyzing the lung parenchyma surrounding the nodule has been minimally explored. We hypothesize that improved nodule classification is achievable by including features quantified from the surrounding lung tissue. To explore this hypothesis, we have developed expanded quantitative CT feature extraction techniques, including volumetric Laws texture energy measures for the parenchyma and nodule, border descriptors using ray-casting and rubber-band straightening, histogram features characterizing densities, and global lung measurements. Using stepwise forward selection and leave-one-case-out cross-validation, a neural network was used for classification. When applied to 50 nodules (22 malignant and 28 benign) from high-resolution CT scans, 52 features (8 nodule, 39 parenchymal, and 5 global) were statistically significant. Nodule-only features yielded an area under the ROC curve of 0.918 (including nodule size) and 0.872 (excluding nodule size). Performance was improved through inclusion of parenchymal (0.938) and global features (0.932). These results show a trend toward increased performance when the parenchyma is included, coupled with the large number of significant parenchymal features that support our hypothesis: the pulmonary parenchyma is influenced differentially by malignant versus benign nodules, assisting CAD-based nodule characterizations.\n\nT2-weighted MRI-derived textural features reflect prostate cancer aggressiveness: preliminary results.\n\nPubMed\n\nNketiah, Gabriel; Elschot, Mattijs; Kim, Eugene; Teruel, Jose R; Scheenen, Tom W; Bathen, Tone F; SelnÃ¦s, Kirsten M\n\n2017-07-01\n\nTo evaluate the diagnostic relevance of T2-weighted (T2W) MRI-derived textural features relative to quantitative physiological parameters derived from diffusion-weighted (DW) and dynamic contrast-enhanced (DCE) MRI in Gleason score (GS) 3+4 and 4+3 prostate cancers. 3T multiparametric-MRI was performed on 23 prostate cancer patients prior to prostatectomy. Textural features [angular second moment (ASM), contrast, correlation, entropy], apparent diffusion coefficient (ADC), and DCE pharmacokinetic parameters (K trans and V e ) were calculated from index tumours delineated on the T2W, DW, and DCE images, respectively. The association between the textural features and prostatectomy GS and the MRI-derived parameters, and the utility of the parameters in differentiating between GS 3+4 and 4+3 prostate cancers were assessed statistically. ASM and entropy correlated significantly (p < 0.05) with both GS and median ADC. Contrast correlated moderately with median ADC. The textural features correlated insignificantly with K trans and V e . GS 4+3 cancers had significantly lower ASM and higher entropy than 3+4 cancers, but insignificant differences in median ADC, K trans , and V e . The combined texture-MRI parameters yielded higher classification accuracy (91%) than the individual parameter sets. T2W MRI-derived textural features could serve as potential diagnostic markers, sensitive to the pathological differences in prostate cancers. â¢ T2W MRI-derived textural features correlate significantly with Gleason score and ADC. â¢ T2W MRI-derived textural features differentiate Gleason score 3+4 from 4+3 cancers. â¢ T2W image textural features could augment tumour characterization.\n\nParametric classification of handvein patterns based on texture features\n\nNASA Astrophysics Data System (ADS)\n\nAl Mahafzah, Harbi; Imran, Mohammad; Supreetha Gowda H., D.\n\n2018-04-01\n\nIn this paper, we have developed Biometric recognition system adopting hand based modality Handvein,which has the unique pattern for each individual and it is impossible to counterfeit and fabricate as it is an internal feature. We have opted in choosing feature extraction algorithms such as LBP-visual descriptor, LPQ-blur insensitive texture operator, Log-Gabor-Texture descriptor. We have chosen well known classifiers such as KNN and SVM for classification. We have experimented and tabulated results of single algorithm recognition rate for Handvein under different distance measures and kernel options. The feature level fusion is carried out which increased the performance level.\n\nProstate cancer detection: Fusion of cytological and textural features.\n\nPubMed\n\nNguyen, Kien; Jain, Anil K; Sabata, Bikash\n\n2011-01-01\n\nA computer-assisted system for histological prostate cancer diagnosis can assist pathologists in two stages: (i) to locate cancer regions in a large digitized tissue biopsy, and (ii) to assign Gleason grades to the regions detected in stage 1. Most previous studies on this topic have primarily addressed the second stage by classifying the preselected tissue regions. In this paper, we address the first stage by presenting a cancer detection approach for the whole slide tissue image. We propose a novel method to extract a cytological feature, namely the presence of cancer nuclei (nuclei with prominent nucleoli) in the tissue, and apply this feature to detect the cancer regions. Additionally, conventional image texture features which have been widely used in the literature are also considered. The performance comparison among the proposed cytological textural feature combination method, the texture-based method and the cytological feature-based method demonstrates the robustness of the extracted cytological feature. At a false positive rate of 6%, the proposed method is able to achieve a sensitivity of 78% on a dataset including six training images (each of which has approximately 4,000Ã7,000 pixels) and 1 1 whole-slide test images (each of which has approximately 5,000Ã23,000 pixels). All images are at 20X magnification.\n\nA Study of Feature Extraction Using Divergence Analysis of Texture Features\n\nNASA Technical Reports Server (NTRS)\n\nHallada, W. A.; Bly, B. G.; Boyd, R. K.; Cox, S.\n\n1982-01-01\n\nAn empirical study of texture analysis for feature extraction and classification of high spatial resolution remotely sensed imagery (10 meters) is presented in terms of specific land cover types. The principal method examined is the use of spatial gray tone dependence (SGTD). The SGTD method reduces the gray levels within a moving window into a two-dimensional spatial gray tone dependence matrix which can be interpreted as a probability matrix of gray tone pairs. Haralick et al (1973) used a number of information theory measures to extract texture features from these matrices, including angular second moment (inertia), correlation, entropy, homogeneity, and energy. The derivation of the SGTD matrix is a function of: (1) the number of gray tones in an image; (2) the angle along which the frequency of SGTD is calculated; (3) the size of the moving window; and (4) the distance between gray tone pairs. The first three parameters were varied and tested on a 10 meter resolution panchromatic image of Maryville, Tennessee using the five SGTD measures. A transformed divergence measure was used to determine the statistical separability between four land cover categories forest, new residential, old residential, and industrial for each variation in texture parameters.\n\nA standardised protocol for texture feature analysis of endoscopic images in gynaecological cancer.\n\nPubMed\n\nNeofytou, Marios S; Tanos, Vasilis; Pattichis, Marios S; Pattichis, Constantinos S; Kyriacou, Efthyvoulos C; Koutsouris, Dimitris D\n\n2007-11-29\n\nIn the development of tissue classification methods, classifiers rely on significant differences between texture features extracted from normal and abnormal regions. Yet, significant differences can arise due to variations in the image acquisition method. For endoscopic imaging of the endometrium, we propose a standardized image acquisition protocol to eliminate significant statistical differences due to variations in: (i) the distance from the tissue (panoramic vs close up), (ii) difference in viewing angles and (iii) color correction. We investigate texture feature variability for a variety of targets encountered in clinical endoscopy. All images were captured at clinically optimum illumination and focus using 720 x 576 pixels and 24 bits color for: (i) a variety of testing targets from a color palette with a known color distribution, (ii) different viewing angles, (iv) two different distances from a calf endometrial and from a chicken cavity. Also, human images from the endometrium were captured and analysed. For texture feature analysis, three different sets were considered: (i) Statistical Features (SF), (ii) Spatial Gray Level Dependence Matrices (SGLDM), and (iii) Gray Level Difference Statistics (GLDS). All images were gamma corrected and the extracted texture feature values were compared against the texture feature values extracted from the uncorrected images. Statistical tests were applied to compare images from different viewing conditions so as to determine any significant differences. For the proposed acquisition procedure, results indicate that there is no significant difference in texture features between the panoramic and close up views and between angles. For a calibrated target image, gamma correction provided an acquired image that was a significantly better approximation to the original target image. In turn, this implies that the texture features extracted from the corrected images provided for better approximations to the original images\n\n3D Texture Features Mining for MRI Brain Tumor Identification\n\nNASA Astrophysics Data System (ADS)\n\nRahim, Mohd Shafry Mohd; Saba, Tanzila; Nayer, Fatima; Syed, Afraz Zahra\n\n2014-03-01\n\nMedical image segmentation is a process to extract region of interest and to divide an image into its individual meaningful, homogeneous components. Actually, these components will have a strong relationship with the objects of interest in an image. For computer-aided diagnosis and therapy process, medical image segmentation is an initial mandatory step. Medical image segmentation is a sophisticated and challenging task because of the sophisticated nature of the medical images. Indeed, successful medical image analysis heavily dependent on the segmentation accuracy. Texture is one of the major features to identify region of interests in an image or to classify an object. 2D textures features yields poor classification results. Hence, this paper represents 3D features extraction using texture analysis and SVM as segmentation technique in the testing methodologies.\n\nDifferential diagnosis of CT focal liver lesions using texture features, feature selection and ensemble driven classifiers.\n\nPubMed\n\nMougiakakou, Stavroula G; Valavanis, Ioannis K; Nikita, Alexandra; Nikita, Konstantina S\n\n2007-09-01\n\nThe aim of the present study is to define an optimally performing computer-aided diagnosis (CAD) architecture for the classification of liver tissue from non-enhanced computed tomography (CT) images into normal liver (C1), hepatic cyst (C2), hemangioma (C3), and hepatocellular carcinoma (C4). To this end, various CAD architectures, based on texture features and ensembles of classifiers (ECs), are comparatively assessed. Number of regions of interests (ROIs) corresponding to C1-C4 have been defined by experienced radiologists in non-enhanced liver CT images. For each ROI, five distinct sets of texture features were extracted using first order statistics, spatial gray level dependence matrix, gray level difference method, Laws' texture energy measures, and fractal dimension measurements. Two different ECs were constructed and compared. The first one consists of five multilayer perceptron neural networks (NNs), each using as input one of the computed texture feature sets or its reduced version after genetic algorithm-based feature selection. The second EC comprised five different primary classifiers, namely one multilayer perceptron NN, one probabilistic NN, and three k-nearest neighbor classifiers, each fed with the combination of the five texture feature sets or their reduced versions. The final decision of each EC was extracted by using appropriate voting schemes, while bootstrap re-sampling was utilized in order to estimate the generalization ability of the CAD architectures based on the available relatively small-sized data set. The best mean classification accuracy (84.96%) is achieved by the second EC using a fused feature set, and the weighted voting scheme. The fused feature set was obtained after appropriate feature selection applied to specific subsets of the original feature set. The comparative assessment of the various CAD architectures shows that combining three types of classifiers with a voting scheme, fed with identical feature sets obtained after\n\nProstate cancer detection: Fusion of cytological and textural features\n\nPubMed Central\n\nNguyen, Kien; Jain, Anil K.; Sabata, Bikash\n\n2011-01-01\n\nA computer-assisted system for histological prostate cancer diagnosis can assist pathologists in two stages: (i) to locate cancer regions in a large digitized tissue biopsy, and (ii) to assign Gleason grades to the regions detected in stage 1. Most previous studies on this topic have primarily addressed the second stage by classifying the preselected tissue regions. In this paper, we address the first stage by presenting a cancer detection approach for the whole slide tissue image. We propose a novel method to extract a cytological feature, namely the presence of cancer nuclei (nuclei with prominent nucleoli) in the tissue, and apply this feature to detect the cancer regions. Additionally, conventional image texture features which have been widely used in the literature are also considered. The performance comparison among the proposed cytological textural feature combination method, the texture-based method and the cytological feature-based method demonstrates the robustness of the extracted cytological feature. At a false positive rate of 6%, the proposed method is able to achieve a sensitivity of 78% on a dataset including six training images (each of which has approximately 4,000Ã7,000 pixels) and 1 1 whole-slide test images (each of which has approximately 5,000Ã23,000 pixels). All images are at 20X magnification. PMID:22811959\n\nComputer-aided diagnosis with textural features for breast lesions in sonograms.\n\nPubMed\n\nChen, Dar-Ren; Huang, Yu-Len; Lin, Sheng-Hsiung\n\n2011-04-01\n\nComputer-aided diagnosis (CAD) systems provided second beneficial support reference and enhance the diagnostic accuracy. This paper was aimed to develop and evaluate a CAD with texture analysis in the classification of breast tumors for ultrasound images. The ultrasound (US) dataset evaluated in this study composed of 1020 sonograms of region of interest (ROI) subimages from 255 patients. Two-view sonogram (longitudinal and transverse views) and four different rectangular regions were utilized to analyze each tumor. Six practical textural features from the US images were performed to classify breast tumors as benign or malignant. However, the textural features always perform as a high dimensional vector; high dimensional vector is unfavorable to differentiate breast tumors in practice. The principal component analysis (PCA) was used to reduce the dimension of textural feature vector and then the image retrieval technique was performed to differentiate between benign and malignant tumors. In the experiments, all the cases were sampled with k-fold cross-validation (k=10) to evaluate the performance with receiver operating characteristic (ROC) curve. The area (A(Z)) under the ROC curve for the proposed CAD system with the specific textural features was 0.925Â±0.019. The classification ability for breast tumor with textural information is satisfactory. This system differentiates benign from malignant breast tumors with a good result and is therefore clinically useful to provide a second opinion. Copyright Â© 2010 Elsevier Ltd. All rights reserved.\n\nDynamic facial expression recognition based on geometric and texture features\n\nNASA Astrophysics Data System (ADS)\n\nLi, Ming; Wang, Zengfu\n\n2018-04-01\n\nRecently, dynamic facial expression recognition in videos has attracted growing attention. In this paper, we propose a novel dynamic facial expression recognition method by using geometric and texture features. In our system, the facial landmark movements and texture variations upon pairwise images are used to perform the dynamic facial expression recognition tasks. For one facial expression sequence, pairwise images are created between the first frame and each of its subsequent frames. Integration of both geometric and texture features further enhances the representation of the facial expressions. Finally, Support Vector Machine is used for facial expression recognition. Experiments conducted on the extended Cohn-Kanade database show that our proposed method can achieve a competitive performance with other methods.\n\nAssociation between background parenchymal enhancement of breast MRI and BIRADS rating change in the subsequent screening\n\nNASA Astrophysics Data System (ADS)\n\nAghaei, Faranak; Mirniaharikandehei, Seyedehnafiseh; Hollingsworth, Alan B.; Stoug, Rebecca G.; Pearce, Melanie; Liu, Hong; Zheng, Bin\n\n2018-03-01\n\nAlthough breast magnetic resonance imaging (MRI) has been used as a breast cancer screening modality for high-risk women, its cancer detection yield remains low (i.e., <= 3%). Thus, increasing breast MRI screening efficacy and cancer detection yield is an important clinical issue in breast cancer screening. In this study, we investigated association between the background parenchymal enhancement (BPE) of breast MRI and the change of diagnostic (BIRADS) status in the next subsequent breast MRI screening. A dataset with 65 breast MRI screening cases was retrospectively assembled. All cases were rated BIRADS-2 (benign findings). In the subsequent screening, 4 cases were malignant (BIRADS-6), 48 remained BIRADS-2 and 13 were downgraded to negative (BIRADS-1). A computer-aided detection scheme was applied to process images of the first set of breast MRI screening. Total of 33 features were computed including texture feature and global BPE features. Texture features were computed from either a gray-level co-occurrence matrix or a gray level run length matrix. Ten global BPE features were also initially computed from two breast regions and bilateral difference between the left and right breasts. Box-plot based analysis shows positive association between texture features and BIRADS rating levels in the second screening. Furthermore, a logistic regression model was built using optimal features selected by a CFS based feature selection method. Using a leave-one-case-out based cross-validation method, classification yielded an overall 75% accuracy in predicting the improvement (or downgrade) of diagnostic status (to BIRAD-1) in the subsequent breast MRI screening. This study demonstrated potential of developing a new quantitative imaging marker to predict diagnostic status change in the short-term, which may help eliminate a high fraction of unnecessary repeated breast MRI screenings and increase the cancer detection yield.\n\nSpectral dependence of texture features integrated with hyperspectral data for area target classification improvement\n\nNASA Astrophysics Data System (ADS)\n\nBangs, Corey F.; Kruse, Fred A.; Olsen, Chris R.\n\n2013-05-01\n\nHyperspectral data were assessed to determine the effect of integrating spectral data and extracted texture feature data on classification accuracy. Four separate spectral ranges (hundreds of spectral bands total) were used from the Visible and Near Infrared (VNIR) and Shortwave Infrared (SWIR) portions of the electromagnetic spectrum. Haralick texture features (contrast, entropy, and correlation) were extracted from the average gray-level image for each of the four spectral ranges studied. A maximum likelihood classifier was trained using a set of ground truth regions of interest (ROIs) and applied separately to the spectral data, texture data, and a fused dataset containing both. Classification accuracy was measured by comparison of results to a separate verification set of test ROIs. Analysis indicates that the spectral range (source of the gray-level image) used to extract the texture feature data has a significant effect on the classification accuracy. This result applies to texture-only classifications as well as the classification of integrated spectral data and texture feature data sets. Overall classification improvement for the integrated data sets was near 1%. Individual improvement for integrated spectral and texture classification of the \"Urban\" class showed approximately 9% accuracy increase over spectral-only classification. Texture-only classification accuracy was highest for the \"Dirt Path\" class at approximately 92% for the spectral range from 947 to 1343nm. This research demonstrates the effectiveness of texture feature data for more accurate analysis of hyperspectral data and the importance of selecting the correct spectral range to be used for the gray-level image source to extract these features.\n\nSignificance of MPEG-7 textural features for improved mass detection in mammography.\n\nPubMed\n\nEltonsy, Nevine H; Tourassi, Georgia D; Fadeev, Aleksey; Elmaghraby, Adel S\n\n2006-01-01\n\nThe purpose of the study is to investigate the significance of MPEG-7 textural features for improving the detection of masses in screening mammograms. The detection scheme was originally based on morphological directional neighborhood features extracted from mammographic regions of interest (ROIs). Receiver Operating Characteristics (ROC) was performed to evaluate the performance of each set of features independently and merged into a back-propagation artificial neural network (BPANN) using the leave-one-out sampling scheme (LOOSS). The study was based on a database of 668 mammographic ROIs (340 depicting cancer regions and 328 depicting normal parenchyma). Overall, the ROC area index of the BPANN using the directional morphological features was Az=0.85+/-0.01. The MPEG-7 edge histogram descriptor-based BPNN showed an ROC area index of Az=0.71+/-0.01 while homogeneous textural descriptors using 30 and 120 channels helped the BPNN achieve similar ROC area indexes of Az=0.882+/-0.02 and Az=0.877+/-0.01 respectively. After merging the MPEG-7 homogeneous textural features with the directional neighborhood features the performance of the BPANN increased providing an ROC area index of Az=0.91+/-0.01. MPEG-7 homogeneous textural descriptor significantly improved the morphology-based detection scheme.\n\nSkin image retrieval using Gabor wavelet texture feature.\n\nPubMed\n\nOu, X; Pan, W; Zhang, X; Xiao, P\n\n2016-12-01\n\nSkin imaging plays a key role in many clinical studies. We have used many skin imaging techniques, including the recently developed capacitive contact skin imaging based on fingerprint sensors. The aim of this study was to develop an effective skin image retrieval technique using Gabor wavelet transform, which can be used on different types of skin images, but with a special focus on skin capacitive contact images. Content-based image retrieval (CBIR) is a useful technology to retrieve stored images from database by supplying query images. In a typical CBIR, images are retrieved based on colour, shape, texture, etc. In this study, texture feature is used for retrieving skin images, and Gabor wavelet transform is used for texture feature description and extraction. The results show that the Gabor wavelet texture features can work efficiently on different types of skin images. Although Gabor wavelet transform is slower compared with other image retrieval techniques, such as principal component analysis (PCA) and grey-level co-occurrence matrix (GLCM), Gabor wavelet transform is the best for retrieving skin capacitive contact images and facial images with different orientations. Gabor wavelet transform can also work well on facial images with different expressions and skin cancer/disease images. We have developed an effective skin image retrieval method based on Gabor wavelet transform, that it is useful for retrieving different types of images, namely digital colour face images, digital colour skin cancer and skin disease images, and particularly greyscale skin capacitive contact images. Gabor wavelet transform can also be potentially useful for face recognition (with different orientation and expressions) and skin cancer/disease diagnosis. Â© 2016 Society of Cosmetic Scientists and the SociÃ©tÃ© FranÃ§aise de CosmÃ©tologie.\n\nHyperspectral remote sensing image retrieval system using spectral and texture features.\n\nPubMed\n\nZhang, Jing; Geng, Wenhao; Liang, Xi; Li, Jiafeng; Zhuo, Li; Zhou, Qianlan\n\n2017-06-01\n\nAlthough many content-based image retrieval systems have been developed, few studies have focused on hyperspectral remote sensing images. In this paper, a hyperspectral remote sensing image retrieval system based on spectral and texture features is proposed. The main contributions are fourfold: (1)Â considering the \"mixed pixel\" in the hyperspectral image, endmembers as spectral features are extracted by an improved automatic pixel purity index algorithm, then the texture features are extracted with the gray level co-occurrence matrix; (2)Â similarity measurement is designed for the hyperspectral remote sensing image retrieval system, in which the similarity of spectral features is measured with the spectral information divergence and spectral angle match mixed measurement and in which the similarity of textural features is measured with Euclidean distance; (3)Â considering the limited ability of the human visual system, the retrieval results are returned after synthesizing true color images based on the hyperspectral image characteristics; (4) the retrieval results are optimized by adjusting the feature weights of similarity measurements according to the user's relevance feedback. The experimental results on NASA data sets can show that our system can achieve comparable superior retrieval performance to existing hyperspectral analysis schemes.\n\nAn extensive analysis of various texture feature extractors to detect Diabetes Mellitus using facial specific regions.\n\nPubMed\n\nShu, Ting; Zhang, Bob; Yan Tang, Yuan\n\n2017-04-01\n\nResearchers have recently discovered that Diabetes Mellitus can be detected through non-invasive computerized method. However, the focus has been on facial block color features. In this paper, we extensively study the effects of texture features extracted from facial specific regions at detecting Diabetes Mellitus using eight texture extractors. The eight methods are from four texture feature families: (1) statistical texture feature family: Image Gray-scale Histogram, Gray-level Co-occurance Matrix, and Local Binary Pattern, (2) structural texture feature family: Voronoi Tessellation, (3) signal processing based texture feature family: Gaussian, Steerable, and Gabor filters, and (4) model based texture feature family: Markov Random Field. In order to determine the most appropriate extractor with optimal parameter(s), various parameter(s) of each extractor are experimented. For each extractor, the same dataset (284 Diabetes Mellitus and 231 Healthy samples), classifiers (k-Nearest Neighbors and Support Vector Machines), and validation method (10-fold cross validation) are used. According to the experiments, the first and third families achieved a better outcome at detecting Diabetes Mellitus than the other two. The best texture feature extractor for Diabetes Mellitus detection is the Image Gray-scale Histogram with bin number=256, obtaining an accuracy of 99.02%, a sensitivity of 99.64%, and a specificity of 98.26% by using SVM. Copyright Â© 2017 Elsevier Ltd. All rights reserved.\n\nCharacterizing mammographic images by using generic texture features\n\nPubMed Central\n\n2012-01-01\n\nIntroduction Although mammographic density is an established risk factor for breast cancer, its use is limited in clinical practice because of a lack of automated and standardized measurement methods. The aims of this study were to evaluate a variety of automated texture features in mammograms as risk factors for breast cancer and to compare them with the percentage mammographic density (PMD) by using a case-control study design. Methods A case-control study including 864 cases and 418 controls was analyzed automatically. Four hundred seventy features were explored as possible risk factors for breast cancer. These included statistical features, moment-based features, spectral-energy features, and form-based features. An elaborate variable selection process using logistic regression analyses was performed to identify those features that were associated with case-control status. In addition, PMD was assessed and included in the regression model. Results Of the 470 image-analysis features explored, 46 remained in the final logistic regression model. An area under the curve of 0.79, with an odds ratio per standard deviation change of 2.88 (95% CI, 2.28 to 3.65), was obtained with validation data. Adding the PMD did not improve the final model. Conclusions Using texture features to predict the risk of breast cancer appears feasible. PMD did not show any additional value in this study. With regard to the features assessed, most of the analysis tools appeared to reflect mammographic density, although some features did not correlate with PMD. It remains to be investigated in larger case-control studies whether these features can contribute to increased prediction accuracy. PMID:22490545\n\nHistogram-based adaptive gray level scaling for texture feature classification of colorectal polyps\n\nNASA Astrophysics Data System (ADS)\n\nPomeroy, Marc; Lu, Hongbing; Pickhardt, Perry J.; Liang, Zhengrong\n\n2018-02-01\n\nTexture features have played an ever increasing role in computer aided detection (CADe) and diagnosis (CADx) methods since their inception. Texture features are often used as a method of false positive reduction for CADe packages, especially for detecting colorectal polyps and distinguishing them from falsely tagged residual stool and healthy colon wall folds. While texture features have shown great success there, the performance of texture features for CADx have lagged behind primarily because of the more similar features among different polyps types. In this paper, we present an adaptive gray level scaling and compare it to the conventional equal-spacing of gray level bins. We use a dataset taken from computed tomography colonography patients, with 392 polyp regions of interest (ROIs) identified and have a confirmed diagnosis through pathology. Using the histogram information from the entire ROI dataset, we generate the gray level bins such that each bin contains roughly the same number of voxels Each image ROI is the scaled down to two different numbers of gray levels, using both an equal spacing of Hounsfield units for each bin, and our adaptive method. We compute a set of texture features from the scaled images including 30 gray level co-occurrence matrix (GLCM) features and 11 gray level run length matrix (GLRLM) features. Using a random forest classifier to distinguish between hyperplastic polyps and all others (adenomas and adenocarcinomas), we find that the adaptive gray level scaling can improve performance based on the area under the receiver operating characteristic curve by up to 4.6%.\n\nMRI texture features as biomarkers to predict MGMT methylation status in glioblastomas\n\nDOE Office of Scientific and Technical Information (OSTI.GOV)\n\nKorfiatis, Panagiotis; Kline, Timothy L.; Erickson, Bradley J., E-mail: bje@mayo.edu\n\nPurpose: Imaging biomarker research focuses on discovering relationships between radiological features and histological findings. In glioblastoma patients, methylation of the O{sup 6}-methylguanine methyltransferase (MGMT) gene promoter is positively correlated with an increased effectiveness of current standard of care. In this paper, the authors investigate texture features as potential imaging biomarkers for capturing the MGMT methylation status of glioblastoma multiforme (GBM) tumors when combined with supervised classification schemes. Methods: A retrospective study of 155 GBM patients with known MGMT methylation status was conducted. Co-occurrence and run length texture features were calculated, and both support vector machines (SVMs) and random forest classifiersmoreÂ Â» were used to predict MGMT methylation status. Results: The best classification system (an SVM-based classifier) had a maximum area under the receiver-operating characteristic (ROC) curve of 0.85 (95% CI: 0.78â0.91) using four texture features (correlation, energy, entropy, and local intensity) originating from the T2-weighted images, yielding at the optimal threshold of the ROC curve, a sensitivity of 0.803 and a specificity of 0.813. Conclusions: Results show that supervised machine learning of MRI texture features can predict MGMT methylation status in preoperative GBM tumors, thus providing a new noninvasive imaging biomarker.Â«Â less\n\nUse of feature extraction techniques for the texture and context information in ERTS imagery: Spectral and textural processing of ERTS imagery. [classification of Kansas land use\n\nNASA Technical Reports Server (NTRS)\n\nHaralick, R. H. (Principal Investigator); Bosley, R. J.\n\n1974-01-01\n\nThe author has identified the following significant results. A procedure was developed to extract cross-band textural features from ERTS MSS imagery. Evolving from a single image texture extraction procedure which uses spatial dependence matrices to measure relative co-occurrence of nearest neighbor grey tones, the cross-band texture procedure uses the distribution of neighboring grey tone N-tuple differences to measure the spatial interrelationships, or co-occurrences, of the grey tone N-tuples present in a texture pattern. In both procedures, texture is characterized in such a way as to be invariant under linear grey tone transformations. However, the cross-band procedure complements the single image procedure by extracting texture information and spectral information contained in ERTS multi-images. Classification experiments show that when used alone, without spectral processing, the cross-band texture procedure extracts more information than the single image texture analysis. Results show an improvement in average correct classification from 86.2% to 88.8% for ERTS image no. 1021-16333 with the cross-band texture procedure. However, when used together with spectral features, the single image texture plus spectral features perform better than the cross-band texture plus spectral features, with an average correct classification of 93.8% and 91.6%, respectively.\n\nPlaque echodensity and textural features are associated with histologic carotid plaque instability.\n\nPubMed\n\nDoonan, Robert J; Gorgui, Jessica; Veinot, Jean P; Lai, Chi; Kyriacou, Efthyvoulos; Corriveau, Marc M; Steinmetz, Oren K; Daskalopoulou, Stella S\n\n2016-09-01\n\nCarotid plaque echodensity and texture features predict cerebrovascular symptomatology. Our purpose was to determine the association of echodensity and textural features obtained from a digital image analysis (DIA) program with histologic features of plaque instability as well as to identify the specific morphologic characteristics of unstable plaques. Patients scheduled to undergo carotid endarterectomy were recruited and underwent carotid ultrasound imaging. DIA was performed to extract echodensity and textural features using Plaque Texture Analysis software (LifeQÂ Medical Ltd, Nicosia, Cyprus). Carotid plaque surgical specimens were obtained and analyzed histologically. Principal component analysis (PCA) was performed to reduce imaging variables. Logistic regression models were used to determine if PCA variables and individual imaging variables predicted histologic features of plaque instability. Image analysis data from 160 patients were analyzed. Individual imaging features of plaque echolucency and homogeneity were associated with a more unstable plaque phenotype on histology. These results were independent of age, sex, and degree of carotid stenosis. PCA reduced 39 individual imaging variables to five PCA variables. PCA1 and PCA2 were significantly associated with overall plaque instability on histology (both PÂ = .02), whereas PCA3 did not achieve statistical significance (PÂ = .07). DIA features of carotid plaques are associated with histologic plaque instability as assessed by multiple histologic features. Importantly, unstable plaques on histology appear more echolucent and homogeneous on ultrasound imaging. These results are independent of stenosis, suggesting that image analysis may have a role in refining the selection of patients who undergo carotid endarterectomy. Copyright Â© 2016 Society for Vascular Surgery. Published by Elsevier Inc. All rights reserved.\n\nTextural kinetics: a novel dynamic contrast-enhanced (DCE)-MRI feature for breast lesion classification.\n\nPubMed\n\nAgner, Shannon C; Soman, Salil; Libfeld, Edward; McDonald, Margie; Thomas, Kathleen; Englander, Sarah; Rosen, Mark A; Chin, Deanna; Nosher, John; Madabhushi, Anant\n\n2011-06-01\n\nDynamic contrast-enhanced (DCE)-magnetic resonance imaging (MRI) of the breast has emerged as an adjunct imaging tool to conventional X-ray mammography due to its high detection sensitivity. Despite the increasing use of breast DCE-MRI, specificity in distinguishing malignant from benign breast lesions is low, and interobserver variability in lesion classification is high. The novel contribution of this paper is in the definition of a new DCE-MRI descriptor that we call textural kinetics, which attempts to capture spatiotemporal changes in breast lesion texture in order to distinguish malignant from benign lesions. We qualitatively and quantitatively demonstrated on 41 breast DCE-MRI studies that textural kinetic features outperform signal intensity kinetics and lesion morphology features in distinguishing benign from malignant lesions. A probabilistic boosting tree (PBT) classifier in conjunction with textural kinetic descriptors yielded an accuracy of 90%, sensitivity of 95%, specificity of 82%, and an area under the curve (AUC) of 0.92. Graph embedding, used for qualitative visualization of a low-dimensional representation of the data, showed the best separation between benign and malignant lesions when using textural kinetic features. The PBT classifier results and trends were also corroborated via a support vector machine classifier which showed that textural kinetic features outperformed the morphological, static texture, and signal intensity kinetics descriptors. When textural kinetic attributes were combined with morphologic descriptors, the resulting PBT classifier yielded 89% accuracy, 99% sensitivity, 76% specificity, and an AUC of 0.91.\n\nPrediction of troponin-T degradation using color image texture features in 10d aged beef longissimus steaks.\n\nPubMed\n\nSun, X; Chen, K J; Berg, E P; Newman, D J; Schwartz, C A; Keller, W L; Maddock Carlin, K R\n\n2014-02-01\n\nThe objective was to use digital color image texture features to predict troponin-T degradation in beef. Image texture features, including 88 gray level co-occurrence texture features, 81 two-dimension fast Fourier transformation texture features, and 48 Gabor wavelet filter texture features, were extracted from color images of beef strip steaks (longissimus dorsi, n = 102) aged for 10d obtained using a digital camera and additional lighting. Steaks were designated degraded or not-degraded based on troponin-T degradation determined on d 3 and d 10 postmortem by immunoblotting. Statistical analysis (STEPWISE regression model) and artificial neural network (support vector machine model, SVM) methods were designed to classify protein degradation. The d 3 and d 10 STEPWISE models were 94% and 86% accurate, respectively, while the d 3 and d 10 SVM models were 63% and 71%, respectively, in predicting protein degradation in aged meat. STEPWISE and SVM models based on image texture features show potential to predict troponin-T degradation in meat. Â© 2013.\n\nIdentification of low variability textural features for heterogeneity quantification of 18F-FDG PET/CT imaging.\n\nPubMed\n\nCortes-Rodicio, J; Sanchez-Merino, G; Garcia-Fidalgo, M A; Tobalina-Larrea, I\n\nTo identify those textural features that are insensitive to both technical and biological factors in order to standardise heterogeneity studies on 18 F-FDG PET imaging. Two different studies were performed. First, nineteen series from a cylindrical phantom filled with different 18 F-FDG activity concentration were acquired and reconstructed using three different protocols. Seventy-two texture features were calculated inside a circular region of interest. The variability of each feature was obtained. Second, the data for 15 patients showing non-pathological liver were acquired. Anatomical and physiological features such as patient's weight, height, body mass index, metabolic active volume, blood glucose level, SUV and SUV standard deviation were also recorded. A liver covering region of interest was delineated and low variability textural features calculated in each patient. Finally, a multivariate Spearman's correlation analysis between biological factors and texture features was performed. Only eight texture features analysed show small variability (<5%) with activity concentration and reconstruction protocol making them suitable for heterogeneity quantification. On the other hand, there is a high statistically significant correlation between MAV and entropy (P<0.05). Entropy feature is, indeed, correlated (P<0.05) with all patient parameters, except body mass index. The textural features that are correlated with neither technical nor biological factors are run percentage, short-zone emphasis and intensity, making them suitable for quantifying functional changes or classifying patients. Other textural features are correlated with technical and biological factors and are, therefore, a source of errors if used for this purpose. Copyright Â© 2016 Elsevier EspaÃ±a, S.L.U. y SEMNIM. All rights reserved.\n\nAccuracy and variability of texture-based radiomics features of lung lesions across CT imaging conditions\n\nNASA Astrophysics Data System (ADS)\n\nZheng, Yuese; Solomon, Justin; Choudhury, Kingshuk; Marin, Daniele; Samei, Ehsan\n\n2017-03-01\n\nTexture analysis for lung lesions is sensitive to changing imaging conditions but these effects are not well understood, in part, due to a lack of ground-truth phantoms with realistic textures. The purpose of this study was to explore the accuracy and variability of texture features across imaging conditions by comparing imaged texture features to voxel-based 3D printed textured lesions for which the true values are known. The seven features of interest were based on the Grey Level Co-Occurrence Matrix (GLCM). The lesion phantoms were designed with three shapes (spherical, lobulated, and spiculated), two textures (homogenous and heterogeneous), and two sizes (diameter < 1.5 cm and 1.5 cm < diameter < 3 cm), resulting in 24 lesions (with a second replica of each). The lesions were inserted into an anthropomorphic thorax phantom (Multipurpose Chest Phantom N1, Kyoto Kagaku) and imaged using a commercial CT system (GE Revolution) at three CTDI levels (0.67, 1.42, and 5.80 mGy), three reconstruction algorithms (FBP, IR-2, IR-4), four reconstruction kernel types (standard, soft, edge), and two slice thicknesses (0.6 mm and 5 mm). Another repeat scan was performed. Texture features from these images were extracted and compared to the ground truth feature values by percent relative error. The variability across imaging conditions was calculated by standard deviation across a certain imaging condition for all heterogeneous lesions. The results indicated that the acquisition method has a significant influence on the accuracy and variability of extracted features and as such, feature quantities are highly susceptible to imaging parameter choices. The most influential parameters were slice thickness and reconstruction kernels. Thin slice thickness and edge reconstruction kernel overall produced more accurate and more repeatable results. Some features (e.g., Contrast) were more accurately quantified under conditions that render higher spatial frequencies (e.g., thinner slice\n\nAutomatic brain MR image denoising based on texture feature-based artificial neural networks.\n\nPubMed\n\nChang, Yu-Ning; Chang, Herng-Hua\n\n2015-01-01\n\nNoise is one of the main sources of quality deterioration not only for visual inspection but also in computerized processing in brain magnetic resonance (MR) image analysis such as tissue classification, segmentation and registration. Accordingly, noise removal in brain MR images is important for a wide variety of subsequent processing applications. However, most existing denoising algorithms require laborious tuning of parameters that are often sensitive to specific image features and textures. Automation of these parameters through artificial intelligence techniques will be highly beneficial. In the present study, an artificial neural network associated with image texture feature analysis is proposed to establish a predictable parameter model and automate the denoising procedure. In the proposed approach, a total of 83 image attributes were extracted based on four categories: 1) Basic image statistics. 2) Gray-level co-occurrence matrix (GLCM). 3) Gray-level run-length matrix (GLRLM) and 4) Tamura texture features. To obtain the ranking of discrimination in these texture features, a paired-samples t-test was applied to each individual image feature computed in every image. Subsequently, the sequential forward selection (SFS) method was used to select the best texture features according to the ranking of discrimination. The selected optimal features were further incorporated into a back propagation neural network to establish a predictable parameter model. A wide variety of MR images with various scenarios were adopted to evaluate the performance of the proposed framework. Experimental results indicated that this new automation system accurately predicted the bilateral filtering parameters and effectively removed the noise in a number of MR images. Comparing to the manually tuned filtering process, our approach not only produced better denoised results but also saved significant processing time.\n\nGlycolytic and gluconeogenic enzyme activities in parenchymal and non-parenchymal cells from mouse liver\n\nPubMed Central\n\nCrisp, D. M.; Pogson, C. I.\n\n1972-01-01\n\n1. Parenchymal cells have been prepared from mouse liver by enzymic and mechanical means. 2. The dry weights, protein and DNA contents of these cells have been determined. 3. Mouse liver `M-' and `L-type' pyruvate kinases have been prepared free of contamination with each other; their kinetic properties have been examined and a method has been developed for their assay in total liver homogenates. 4. Recoveries of phosphoglycerate kinase, lactate dehydrogenase and phosphofructokinase in enzymically prepared cells indicate that little, if any, cytoplasmic protein is lost during preparation. 5. Parenchymal cells exhibit a very substantial increase in the activity ratio of glucokinase to hexokinase over that in total liver homogenate; in three out of eight experiments, hexokinase activity was undetectable. 6. `L-type' pyruvate kinase alone occurs in the parenchymal cell. Non-parenchymal cells are characterized by the presence of `M-type' activity only. 7. Parenchymal cells contain both glucose 6-phosphatase and fructose 1,6-diphosphatase. The non-parenchymal fraction appears to contain fructose 1,6-diphosphatase, but is devoid of glucose 6-phosphatase. 8. No aldolase A was detectable in the whole liver. Aldolase B occurs in both parenchymal and non-parenchymal tissue. 9. Parenchymal cells prepared by mechanical disruption of mouse liver with 20% polyvinyl alcohol exhibit a similar enzyme profile to those prepared enzymically. 10. The methodology involved in the preparation of isolated liver cells is discussed. The importance of the measurement of several parameters as criteria for establishing the viability of parenchymal cells is stressed. 11. The metabolic implications of the results in the present study are discussed. PMID:4262895\n\nIn vivo placental MRI shape and textural features predict fetal growth restriction and postnatal outcome.\n\nPubMed\n\nDahdouh, Sonia; Andescavage, Nickie; Yewale, Sayali; Yarish, Alexa; Lanham, Diane; Bulas, Dorothy; du Plessis, Adre J; Limperopoulos, Catherine\n\n2018-02-01\n\nTo investigate the ability of three-dimensional (3D) MRI placental shape and textural features to predict fetal growth restriction (FGR) and birth weight (BW) for both healthy and FGR fetuses. We recruited two groups of pregnant volunteers between 18 and 39 weeks of gestation; 46 healthy subjects and 34 FGR. Both groups underwent fetal MR imaging on a 1.5 Tesla GE scanner using an eight-channel receiver coil. We acquired T2-weighted images on either the coronal or the axial plane to obtain MR volumes with a slice thickness of either 4 or 8 mm covering the full placenta. Placental shape features (volume, thickness, elongation) were combined with textural features; first order textural features (mean, variance, kurtosis, and skewness of placental gray levels), as well as, textural features computed on the gray level co-occurrence and run-length matrices characterizing placental homogeneity, symmetry, and coarseness. The features were used in two machine learning frameworks to predict FGR and BW. The proposed machine-learning based method using shape and textural features identified FGR pregnancies with 86% accuracy, 77% precision and 86% recall. BW estimations were 0.3 Â± 13.4% (mean percentage error Â± standard error) for healthy fetuses and -2.6 Â± 15.9% for FGR. The proposed FGR identification and BW estimation methods using in utero placental shape and textural features computed on 3D MR images demonstrated high accuracy in our healthy and high-risk cohorts. Future studies to assess the evolution of each feature with regard to placental development are currently underway. 2 Technical Efficacy: Stage 2 J. Magn. Reson. Imaging 2018;47:449-458. Â© 2017 International Society for Magnetic Resonance in Medicine.\n\nMammographic phenotypes of breast cancer risk driven by breast anatomy\n\nNASA Astrophysics Data System (ADS)\n\nGastounioti, Aimilia; Oustimov, Andrew; Hsieh, Meng-Kang; Pantalone, Lauren; Conant, Emily F.; Kontos, Despina\n\n2017-03-01\n\nImage-derived features of breast parenchymal texture patterns have emerged as promising risk factors for breast cancer, paving the way towards personalized recommendations regarding women's cancer risk evaluation and screening. The main steps to extract texture features of the breast parenchyma are the selection of regions of interest (ROIs) where texture analysis is performed, the texture feature calculation and the texture feature summarization in case of multiple ROIs. In this study, we incorporate breast anatomy in these three key steps by (a) introducing breast anatomical sampling for the definition of ROIs, (b) texture feature calculation aligned with the structure of the breast and (c) weighted texture feature summarization considering the spatial position and the underlying tissue composition of each ROI. We systematically optimize this novel framework for parenchymal tissue characterization in a case-control study with digital mammograms from 424 women. We also compare the proposed approach with a conventional methodology, not considering breast anatomy, recently shown to enhance the case-control discriminatory capacity of parenchymal texture analysis. The case-control classification performance is assessed using elastic-net regression with 5-fold cross validation, where the evaluation measure is the area under the curve (AUC) of the receiver operating characteristic. Upon optimization, the proposed breast-anatomy-driven approach demonstrated a promising case-control classification performance (AUC=0.87). In the same dataset, the performance of conventional texture characterization was found to be significantly lower (AUC=0.80, DeLong's test p-value<0.05). Our results suggest that breast anatomy may further leverage the associations of parenchymal texture features with breast cancer, and may therefore be a valuable addition in pipelines aiming to elucidate quantitative mammographic phenotypes of breast cancer risk.\n\nDissociations between Featural versus Conjunction-based Texture Processing in Infancy: Analyses of Three Potential Contributing Factors.\n\nERIC Educational Resources Information Center\n\nBertin, Evelin; Bhatt, Ramesh S.\n\n2001-01-01\n\nExamined three possible explanations for findings that infants detect textural discrepancies based on individual features more readily than on feature conjunctions. Found that none of the proposed factors could explain 5.5-month-olds' superior processing of featural over conjunction-based textural discrepancies. Findings suggest that in infancy,â¦\n\na Statistical Texture Feature for Building Collapse Information Extraction of SAR Image\n\nNASA Astrophysics Data System (ADS)\n\nLi, L.; Yang, H.; Chen, Q.; Liu, X.\n\n2018-04-01\n\nSynthetic Aperture Radar (SAR) has become one of the most important ways to extract post-disaster collapsed building information, due to its extreme versatility and almost all-weather, day-and-night working capability, etc. In view of the fact that the inherent statistical distribution of speckle in SAR images is not used to extract collapsed building information, this paper proposed a novel texture feature of statistical models of SAR images to extract the collapsed buildings. In the proposed feature, the texture parameter of G0 distribution from SAR images is used to reflect the uniformity of the target to extract the collapsed building. This feature not only considers the statistical distribution of SAR images, providing more accurate description of the object texture, but also is applied to extract collapsed building information of single-, dual- or full-polarization SAR data. The RADARSAT-2 data of Yushu earthquake which acquired on April 21, 2010 is used to present and analyze the performance of the proposed method. In addition, the applicability of this feature to SAR data with different polarizations is also analysed, which provides decision support for the data selection of collapsed building information extraction.\n\nLocal binary pattern variants-based adaptive texture features analysis for posed and nonposed facial expression recognition\n\nNASA Astrophysics Data System (ADS)\n\nSultana, Maryam; Bhatti, Naeem; Javed, Sajid; Jung, Soon Ki\n\n2017-09-01\n\nFacial expression recognition (FER) is an important task for various computer vision applications. The task becomes challenging when it requires the detection and encoding of macro- and micropatterns of facial expressions. We present a two-stage texture feature extraction framework based on the local binary pattern (LBP) variants and evaluate its significance in recognizing posed and nonposed facial expressions. We focus on the parametric limitations of the LBP variants and investigate their effects for optimal FER. The size of the local neighborhood is an important parameter of the LBP technique for its extraction in images. To make the LBP adaptive, we exploit the granulometric information of the facial images to find the local neighborhood size for the extraction of center-symmetric LBP (CS-LBP) features. Our two-stage texture representations consist of an LBP variant and the adaptive CS-LBP features. Among the presented two-stage texture feature extractions, the binarized statistical image features and adaptive CS-LBP features were found showing high FER rates. Evaluation of the adaptive texture features shows competitive and higher performance than the nonadaptive features and other state-of-the-art approaches, respectively.\n\nCFS-SMO based classification of breast density using multiple texture models.\n\nPubMed\n\nSharma, Vipul; Singh, Sukhwinder\n\n2014-06-01\n\nIt is highly acknowledged in the medical profession that density of breast tissue is a major cause for the growth of breast cancer. Increased breast density was found to be linked with an increased risk of breast cancer growth, as high density makes it difficult for radiologists to see an abnormality which leads to false negative results. Therefore, there is need for the development of highly efficient techniques for breast tissue classification based on density. This paper presents a hybrid scheme for classification of fatty and dense mammograms using correlation-based feature selection (CFS) and sequential minimal optimization (SMO). In this work, texture analysis is done on a region of interest selected from the mammogram. Various texture models have been used to quantify the texture of parenchymal patterns of breast. To reduce the dimensionality and to identify the features which differentiate between breast tissue densities, CFS is used. Finally, classification is performed using SMO. The performance is evaluated using 322 images of mini-MIAS database. Highest accuracy of 96.46% is obtained for two-class problem (fatty and dense) using proposed approach. Performance of selected features by CFS is also evaluated by NaÃ¯ve Bayes, Multilayer Perceptron, RBF Network, J48 and kNN classifier. The proposed CFS-SMO method outperforms all other classifiers giving a sensitivity of 100%. This makes it suitable to be taken as a second opinion in classifying breast tissue density.\n\nVariability of textural features in FDG PET images due to different acquisition modes and reconstruction parameters.\n\nPubMed\n\nGalavis, Paulina E; Hollensen, Christian; Jallow, Ngoneh; Paliwal, Bhudatt; Jeraj, Robert\n\n2010-10-01\n\nCharacterization of textural features (spatial distributions of image intensity levels) has been considered as a tool for automatic tumor segmentation. The purpose of this work is to study the variability of the textural features in PET images due to different acquisition modes and reconstruction parameters. Twenty patients with solid tumors underwent PET/CT scans on a GE Discovery VCT scanner, 45-60 minutes post-injection of 10 mCi of [(18)F]FDG. Scans were acquired in both 2D and 3D modes. For each acquisition the raw PET data was reconstructed using five different reconstruction parameters. Lesions were segmented on a default image using the threshold of 40% of maximum SUV. Fifty different texture features were calculated inside the tumors. The range of variations of the features were calculated with respect to the average value. Fifty textural features were classified based on the range of variation in three categories: small, intermediate and large variability. Features with small variability (range â¤ 5%) were entropy-first order, energy, maximal correlation coefficient (second order feature) and low-gray level run emphasis (high-order feature). The features with intermediate variability (10% â¤ range â¤ 25%) were entropy-GLCM, sum entropy, high gray level run emphsis, gray level non-uniformity, small number emphasis, and entropy-NGL. Forty remaining features presented large variations (range > 30%). Textural features such as entropy-first order, energy, maximal correlation coefficient, and low-gray level run emphasis exhibited small variations due to different acquisition modes and reconstruction parameters. Features with low level of variations are better candidates for reproducible tumor segmentation. Even though features such as contrast-NGTD, coarseness, homogeneity, and busyness have been previously used, our data indicated that these features presented large variations, therefore they could not be considered as a good candidates for tumor\n\nVariability of textural features in FDG PET images due to different acquisition modes and reconstruction parameters\n\nPubMed Central\n\nGALAVIS, PAULINA E.; HOLLENSEN, CHRISTIAN; JALLOW, NGONEH; PALIWAL, BHUDATT; JERAJ, ROBERT\n\n2"
    }
}
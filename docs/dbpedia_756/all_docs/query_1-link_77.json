{
    "id": "dbpedia_756_1",
    "rank": 77,
    "data": {
        "url": "https://arxiv.org/html/2407.00087v1",
        "read_more_link": "",
        "language": "en",
        "title": "ARES: Alternating Reinforcement Learning and Supervised Fine-Tuning for Enhanced Multi-Modal Chain-of-Thought Reasoning Through Diverse AI Feedback",
        "top_image": "",
        "meta_img": "",
        "images": [
            "https://arxiv.org/html/x1.png",
            "https://arxiv.org/html/x2.png",
            "https://arxiv.org/html/x3.png",
            "https://arxiv.org/html/x4.png",
            "https://arxiv.org/html/x5.png",
            "https://arxiv.org/html/x6.png",
            "https://arxiv.org/html/x7.png"
        ],
        "movies": [],
        "keywords": [],
        "meta_keywords": [
            ""
        ],
        "tags": null,
        "authors": [],
        "publish_date": null,
        "summary": "",
        "meta_description": "",
        "meta_lang": "en",
        "meta_favicon": "",
        "meta_site_name": "",
        "canonical_link": null,
        "text": "ARES: Alternating Reinforcement Learning and Supervised Fine-Tuning for Enhanced Multi-Modal Chain-of-Thought Reasoning\n\nThrough Diverse AI Feedback\n\nJu-Seung Byun Equal contribution Jiyun Chun Jihyung Kil Andrew Perrault\n\nDepartment of Computer Science and Engineering\n\nThe Ohio State University\n\n{byun.83,chun.203,kil.5,perrault.17}@osu.edu\n\nAbstract\n\nLarge Multimodal Models (LMMs) excel at comprehending human instructions and demonstrate remarkable results across a broad spectrum of tasks. Reinforcement Learning from Human Feedback (RLHF) and AI Feedback (RLAIF) further refine LLMs by aligning them with specific preferences. These methods primarily use ranking-based feedback for entire generations. With advanced AI models (Teacher), such as GPT-4 and Claude 3 Opus, we can request various types of detailed feedback that are expensive for humans to provide. We propose a two-stage algorithm ARES that Alternates REinforcement Learning (RL) and Supervised Fine-Tuning (SFT). First, we request the Teacher to score how much each sentence contributes to solving the problem in a Chain-of-Thought (CoT). This sentence-level feedback allows us to consider individual valuable segments, providing more granular rewards for the RL procedure. Second, we ask the Teacher to correct the wrong reasoning after the RL stage. The RL procedure requires massive efforts for hyperparameter tuning and often generates errors like repetitive words and incomplete sentences. With the correction feedback, we stabilize the RL fine-tuned model through SFT. We conduct experiments on multi-model dataset ScienceQA and A-OKVQA to demonstrate the effectiveness of our proposal. ARES rationale reasoning achieves around 70% win rate against baseline models judged by GPT-4o. Additionally, we observe that the improved rationale reasoning leads to a 2.5% increase in inference answer accuracy on average for the multi-modal datasets.\n\nARES: Alternating Reinforcement Learning and Supervised Fine-Tuning for Enhanced Multi-Modal Chain-of-Thought Reasoning\n\nThrough Diverse AI Feedback\n\n1 Introduction\n\nLarge Language Models (LLMs) and Large Multimodal Models demonstrate remarkable performance across diverse language and multi-modal tasks (Brown et al., 2020; Chowdhery et al., 2022; Touvron et al., 2023; Zhang et al., 2022a; Liu et al., 2023a; Goyal et al., 2023). However, these Large Models (LMs) often generate toxic and biased content (Gehman et al., 2020; Tamkin et al., 2021) because LMs are primarily trained to predict the next token based on extensive corpus datasets. To align LM behavior more closely with user preferences, previous works (Glaese et al., 2022; Ouyang et al., 2022) fine-tune their models using Reinforcement Learning from Human Feedback (RLHF). Furthermore, with advancements in LMs, an advanced LM feedback start replacing costly human feedback, yielding Reinforcement Learning from AI Feedback (RLAIF) (Lee et al., 2023; Bai et al., 2022; Yuan et al., 2024).\n\nHowever, RLHF and RLAIF encounter two significant challenges. First, both methods often utilize ranking-based feedback (Ouyang et al., 2022), which orders the preferences of generated samples. For instance, if sample Aùê¥Aitalic_A is preferred over sample BùêµBitalic_B, the model is fine-tuned to generate more outputs like Aùê¥Aitalic_A and fewer like BùêµBitalic_B. However, if BùêµBitalic_B contains certain valuable parts, these parts are often disregarded. To alleviate this issue, Lightman et al. (2023); Luo et al. (2024) propose sentence-level feedback, applying it solely to the reward model without Reinforcement Learning (RL), called the Process-supervised Reward Model (PRM). It demonstrates its potential through search algorithms on the PRM, such as best-of-NùëÅNitalic_N or Monte Carlo Tree Search (MCTS). Furthermore, Wang et al. (2024) demonstrate the effectiveness of RL with sentence-level feedback by heuristically scoring each sentence in math problems, where evaluating the predicted answer is straightforward. Thus, sentence-level feedback exhibits significant promise compared to existing ranking-based feedback. Nonetheless, acquiring sentence-level feedback is more costly than ranking-based feedback.\n\nSecond, the RL process is inherently unstable and requires extensive hyperparameter tuning (Eimer et al., 2023). This instability often results in the generation of repetitive words and truncated sentences. Hyperparameter tuning becomes an enormous burden as the model size increases, making exhaustive tuning seemingly impossible, especially for individuals. The existing RLHF method (Ouyang et al., 2022) recycles the dataset used in pretraining within the loss function with Proximal Policy Optimization (PPO) (Schulman et al., 2017) to mitigate this degeneration problem. However, this approach prevents the model from fully maximizing the sum of rewards through RL and may limit the opportunity for diverse improvements, which differ from the pretraining distribution.\n\nIn this work, we aim to address the two challenges mentioned above through various types of feedback using an advanced AI model as a Teacher. Many advanced AI models, including GPT-4 and Claude 3 Opus, are already used as evaluators for many tasks and generate reliable human-level answers (Liu et al., 2023b; Sottana et al., 2023). 1)1)1 ) We request a score from Teacher for each sentence, ranging from 0.00.00.00.0 to 1.01.01.01.0. Each score indicates how much a sentence contributes to solving the problem. This provides detailed reward feedback to the training model and can be applied to both mathematical and more general multi-modal Chain-of-Thought (CoT) (or rationale reasoning) problems. 2)2)2 ) We ask the Teacher to identify and correct minor errors in the RL results, such as incorrect or cut-off parts. With this corrected dataset, we fine-tune the model using Supervised Fine-Tuning (SFT). This stage allows the model to maximize the rewards while properly deviating from the pretraining distribution. In summary, We propose a hybrid algorithm ARES that Alternates REinforcement Learning and Supervised Fine-Tuning.\n\nTo evaluate how much rationale reasoning can be improved through the ARES framework, we use the ScienceQA and A-OKVQA datasets, which are large-scale, multi-modal datasets that include rationale reasoning data. We use Multimodal-CoT (MM-CoT) (Zhang et al., 2023b) as the baseline. MM-CoT employs two separate models: one model is responsible for generating rationale, and the other model, an inference model, processes the concatenated input (problem and generated rationale). This distinct framework enhances performance, even with relatively smaller models like Flan-AlpacaBasesubscriptFlan-AlpacaBase\\text{Flan-Alpaca}_{\\text{Base}}Flan-Alpaca start_POSTSUBSCRIPT Base end_POSTSUBSCRIPT (Chia et al., 2023) (251M) and Flan-AlpacaLargesubscriptFlan-AlpacaLarge\\text{Flan-Alpaca}_{\\text{Large}}Flan-Alpaca start_POSTSUBSCRIPT Large end_POSTSUBSCRIPT (790M) with ViT feature (Dosovitskiy et al., 2021). We perform ARES on the rationale reasoning model of MM-CoT. We compare ARES rationale reasoning with that of MM-CoT through GPT-4o, determining which rationale is better and computing the win rate. Additionally, we check whether the improved rationale reasoning leads to better answer accuracy. Our results show that our rationale reasoning outperforms the baselines with around 70% win rate and demonstrates 2.5% increase in inference answer accuracy on average for the different model sizes and the multi-modal tasks.\n\n2 Methodology\n\nThis section briefly introduces the preliminaries in Section 2.1 and present our two-stage hybrid algorithm ARES that Alternates REinforcement Learning and Supervised Fine-Tuning. 1)1)1 ) We request a score for each sentence in the Chain-of-Thought (CoT) from the advanced AI model (Teacher) to determine how much it contributes to solving the problem (Section 2.2). We perform Reinforcement Learning (RL) with the score feedback on our training model. 2)2)2 ) Teacher corrects minor errors such as truncated or slightly incorrect sentences, thereby performing Supervised Fine-Tuning (SFT) (Section 2.2).\n\n2.1 Preliminaries\n\nFor an input x‚ààXùë•ùëãx\\in Xitalic_x ‚àà italic_X, a transformer based (Vaswani et al., 2023) model œÄŒ∏(‚ãÖ|x)\\pi_{\\theta}(\\cdot|x)italic_œÄ start_POSTSUBSCRIPT italic_Œ∏ end_POSTSUBSCRIPT ( ‚ãÖ | italic_x ) parameterized by Œ∏ùúÉ\\thetaitalic_Œ∏ generates the output yùë¶yitalic_y composed of sentences {s0,s1,‚Ä¶,sk}subscriptùë†0subscriptùë†1‚Ä¶subscriptùë†ùëò\\{s_{0},s_{1},...,s_{k}\\}{ italic_s start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT , italic_s start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , ‚Ä¶ , italic_s start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT }.\n\nœÄŒ∏‚Å¢(y‚à£x)=‚àèt=0kœÄŒ∏‚Å¢(yi‚à£x,y<t),subscriptùúãùúÉconditionalùë¶ùë•superscriptsubscriptproductùë°0ùëòsubscriptùúãùúÉconditionalsubscriptùë¶ùëñùë•subscriptùë¶absentùë°\\pi_{\\theta}(y\\mid x)=\\prod_{t=0}^{k}\\pi_{\\theta}(y_{i}\\mid x,y_{<t}),italic_œÄ start_POSTSUBSCRIPT italic_Œ∏ end_POSTSUBSCRIPT ( italic_y ‚à£ italic_x ) = ‚àè start_POSTSUBSCRIPT italic_t = 0 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_k end_POSTSUPERSCRIPT italic_œÄ start_POSTSUBSCRIPT italic_Œ∏ end_POSTSUBSCRIPT ( italic_y start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ‚à£ italic_x , italic_y start_POSTSUBSCRIPT < italic_t end_POSTSUBSCRIPT ) , (1)\n\nwhere y<tsubscriptùë¶absentùë°y_{<t}italic_y start_POSTSUBSCRIPT < italic_t end_POSTSUBSCRIPT indicates previous tokens. To proceed with RL finetuning, Ouyang et al. (2022) train an outcome-supervised reward model (ORM) using ranking-based feedback. With more fine-grained feedback like sentence-level, Lightman et al. (2023); Wang et al. (2024) train a process-supervised reward model (PRM). Instead of training a reward model, we request score feedback r‚Å¢(x‚à™s<t,st)ùëüùë•subscriptùë†absentùë°subscriptùë†ùë°r(x\\cup s_{<t},s_{t})italic_r ( italic_x ‚à™ italic_s start_POSTSUBSCRIPT < italic_t end_POSTSUBSCRIPT , italic_s start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ) for each sentence from an advanced AI such as GPT-4 where s<tsubscriptùë†absentùë°s_{<t}italic_s start_POSTSUBSCRIPT < italic_t end_POSTSUBSCRIPT is previous sentences.\n\n2.2 Reinforcement Learning\n\nReinforcement Learning (RL) fine-tunes our model œÄŒ∏subscriptùúãùúÉ\\pi_{\\theta}italic_œÄ start_POSTSUBSCRIPT italic_Œ∏ end_POSTSUBSCRIPT to maximize sum of sentence rewards from an advanced AI such as GPT-4 and Claude 3 Opus. The RL objective is as follows:\n\nŒ∏‚àó=argmaxùúÉ‚Å¢ùîºx‚àºXst‚àºœÄŒ∏(‚ãÖ|x,s<t)[‚àëi=0kŒ≥i‚Å¢r‚Å¢(x‚à™s<t,st)].\\displaystyle\\theta^{*}=\\underset{\\theta}{\\mathrm{argmax}}\\operatorname*{% \\mathbb{E}}_{\\begin{subarray}{c}x\\sim X\\\\ s_{t}\\sim\\pi_{\\theta}(\\cdot|x,s_{<t})\\end{subarray}}\\Biggl{[}\\sum_{i=0}^{k}% \\gamma^{i}r(x\\cup s_{<t},s_{t})\\Bigg{]}.italic_Œ∏ start_POSTSUPERSCRIPT ‚àó end_POSTSUPERSCRIPT = underitalic_Œ∏ start_ARG roman_argmax end_ARG blackboard_E start_POSTSUBSCRIPT start_ARG start_ROW start_CELL italic_x ‚àº italic_X end_CELL end_ROW start_ROW start_CELL italic_s start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ‚àº italic_œÄ start_POSTSUBSCRIPT italic_Œ∏ end_POSTSUBSCRIPT ( ‚ãÖ | italic_x , italic_s start_POSTSUBSCRIPT < italic_t end_POSTSUBSCRIPT ) end_CELL end_ROW end_ARG end_POSTSUBSCRIPT [ ‚àë start_POSTSUBSCRIPT italic_i = 0 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_k end_POSTSUPERSCRIPT italic_Œ≥ start_POSTSUPERSCRIPT italic_i end_POSTSUPERSCRIPT italic_r ( italic_x ‚à™ italic_s start_POSTSUBSCRIPT < italic_t end_POSTSUBSCRIPT , italic_s start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ) ] . (2)\n\nwhere Œ≥‚â§1.0ùõæ1.0\\gamma\\leq 1.0italic_Œ≥ ‚â§ 1.0 is a discount factor. We use Proximal Policy Optimization (PPO) (Schulman et al., 2017) to achieve this RL objective, treating sentences as actions (Equation 3).\n\nmaximizeùúÉ‚Å¢ùîºt‚Å¢[œÄŒ∏‚Å¢(st|x‚à™s<t)œÄŒ∏old‚Å¢(st|x‚à™s<t)‚Å¢A^t]ùúÉmaximizesubscriptùîºtdelimited-[]subscriptùúãùúÉconditionalsubscriptstxsubscriptsabsenttsubscriptùúãsubscriptùúÉoldconditionalsubscriptstxsubscriptsabsenttsubscript^At\\displaystyle\\underset{\\theta}{\\rm{maximize}}\\;\\mathbb{E}_{t}\\bigg{[}\\frac{\\pi% _{\\theta}(s_{t}|x\\cup s_{<t})}{\\pi_{\\theta_{\\rm{old}}}(s_{t}|x\\cup s_{<t})}% \\hat{A}_{t}\\bigg{]}underitalic_Œ∏ start_ARG roman_maximize end_ARG blackboard_E start_POSTSUBSCRIPT roman_t end_POSTSUBSCRIPT [ divide start_ARG italic_œÄ start_POSTSUBSCRIPT italic_Œ∏ end_POSTSUBSCRIPT ( roman_s start_POSTSUBSCRIPT roman_t end_POSTSUBSCRIPT | roman_x ‚à™ roman_s start_POSTSUBSCRIPT < roman_t end_POSTSUBSCRIPT ) end_ARG start_ARG italic_œÄ start_POSTSUBSCRIPT italic_Œ∏ start_POSTSUBSCRIPT roman_old end_POSTSUBSCRIPT end_POSTSUBSCRIPT ( roman_s start_POSTSUBSCRIPT roman_t end_POSTSUBSCRIPT | roman_x ‚à™ roman_s start_POSTSUBSCRIPT < roman_t end_POSTSUBSCRIPT ) end_ARG over^ start_ARG roman_A end_ARG start_POSTSUBSCRIPT roman_t end_POSTSUBSCRIPT ] (3) s.t.ùîºt[KL(œÄŒ∏(‚ãÖ|x‚à™s<t),œÄŒ∏old(‚ãÖ|x‚à™s<t))]‚â§Œ¥\\displaystyle\\text{s.t.}\\;\\mathbb{E}_{t}\\big{[}\\rm{KL}\\left(\\pi_{\\theta}(\\cdot% |x\\cup s_{<t}),\\pi_{\\theta_{old}}(\\cdot|x\\cup s_{<t})\\right)\\big{]}\\leq\\deltas.t. blackboard_E start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT [ roman_KL ( italic_œÄ start_POSTSUBSCRIPT italic_Œ∏ end_POSTSUBSCRIPT ( ‚ãÖ | roman_x ‚à™ roman_s start_POSTSUBSCRIPT < roman_t end_POSTSUBSCRIPT ) , italic_œÄ start_POSTSUBSCRIPT italic_Œ∏ start_POSTSUBSCRIPT roman_old end_POSTSUBSCRIPT end_POSTSUBSCRIPT ( ‚ãÖ | roman_x ‚à™ roman_s start_POSTSUBSCRIPT < roman_t end_POSTSUBSCRIPT ) ) ] ‚â§ italic_Œ¥\n\nwhere œÄoldsubscriptùúãold\\pi_{\\rm{old}}italic_œÄ start_POSTSUBSCRIPT roman_old end_POSTSUBSCRIPT is the original policy (baseline model) and A^tsubscript^ùê¥ùë°\\hat{A}_{t}over^ start_ARG italic_A end_ARG start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT is an advantage estimator at timestep tùë°titalic_t. PPO is commonly leveraged in Reinforcement Learning from Human Feedback (RLHF) (Ouyang et al., 2022) and AI Feedback (RLAIF) (Bai et al., 2022). PPO‚Äôs conservative update prevents the training model from deviating too far from the original model, thus avoiding degeneration.\n\nSentence-Level Nuanced Feedback: We request a score between 0.00.00.00.0 and 1.01.01.01.0 for each sentence in CoT through the advanced AI for RL. The closer the score is to 1.01.01.01.0, the more relevant and helpful it is to solving the problem. Table 5 presents the prompt format. We additionally shift the reward distribution by ‚àí0.50.5-0.5- 0.5 to center it at 00 (Zheng et al., 2023). Therefore, the actual range is from ‚àí0.50.5-0.5- 0.5 to 0.50.50.50.5. Using these nuanced scores, the RL fine-tuned model exhibits emergent behaviors (please refer to Section 4). This allows us to understand the direction in which the model wants to change through RL.\n\nAdvantages of Using Advanced AI for Score Feedback: Although calling the API has disadvantages, such as incurring costs or facing usage limits, there exist several advantages of using the advanced AI for feedback. First, there is no need to train a reward model. Second, as the RL fine-tuned model begins to generate out-of-distribution outputs that differ from the data used to train the reward model, it becomes challenging for the trained reward model to provide accurate rewards. However, this out-of-distribution problem is effectively addressed with the advanced AI.\n\nRL Challenge: One of the challenging factors for RL is hyperparameter tuning (Eimer et al., 2023). This often results in generating repetitive words and truncated sentences (Ouyang et al., 2022). Additionally, as the model size increases, finding working hyperparameters becomes infeasible for individuals. To alleviate this issue, we utilize correction feedback from the advanced AI as the second stage (Section 2.3), and proceed with the supervised fine-tuning to stabilize the RL fine-tuned model.\n\n2.3 Correction: Supervised Fine-Tuning\n\nThe RL fine-tuning procedure makes model changes to maximize the reward sum, such as correcting mistakes or explaining why other options cannot be the answer. However, without highly tuned hyperparameters (Eimer et al., 2023), the model after the RL phase may often result in errors such as repeated sentences, truncated sentences, or incorrect content for some data points. (See examples in Appendix D.)\n\nCorrection Feedback: Given the success of LLMs and LMMs in a wide range of areas (Brown et al., 2020; Chowdhery et al., 2022; Zhang et al., 2022a), we do not need to be restricted to requesting feedback in the form of scores. We request correction feedback from advanced AI (Teacher) for sentences containing errors after the RL process, and obtain a corrected dataset XcorrectedsubscriptùëãcorrectedX_{\\text{corrected}}italic_X start_POSTSUBSCRIPT corrected end_POSTSUBSCRIPT. Since the supervised fine-tuning is more stable and finding appropriate hyperparameters is easier than RL, we proceed with supervised fine-tuning using XcorrectedsubscriptùëãcorrectedX_{\\text{corrected}}italic_X start_POSTSUBSCRIPT corrected end_POSTSUBSCRIPT exactly as in common autoregressive model (Vaswani et al., 2023) training to stabilize the RL fine-tuned model. This reduces the burden of RL‚Äôs exhaustive hyperparameter tuning and properly guides the direction in which the training model wants to change.\n\nHow Correction Feedback Helps RL: RL basically increases the probability of positively rewarded actions (or sentences) and decreases the probability for negative rewards. The direction of learning is determined by the reward (scalar) value. However, the opposite direction of the reward is sometimes required. For example, suppose there is a truncated sentence struncatedsubscriptùë†truncateds_{\\text{truncated}}italic_s start_POSTSUBSCRIPT truncated end_POSTSUBSCRIPT in CoT. struncatedsubscriptùë†truncateds_{\\text{truncated}}italic_s start_POSTSUBSCRIPT truncated end_POSTSUBSCRIPT gets a negative score because it is an incomplete sentence (Table 11). If there is no correction stage, the probability of struncatedsubscriptùë†truncateds_{\\text{truncated}}italic_s start_POSTSUBSCRIPT truncated end_POSTSUBSCRIPT is simply reduced. What if struncatedsubscriptùë†truncateds_{\\text{truncated}}italic_s start_POSTSUBSCRIPT truncated end_POSTSUBSCRIPT contains some valuable part? This valuable part is ignored, and its probability decreases. To alleviate this issue, we instead receive the corrected sentence as feedback and encourage the training model to generate complete sentences, which is very challenging to achieve with only RL.\n\nAdditionally, RL is primarily fine-tuned through PPO (Schulman et al., 2017) to prevent the model from deviating too much from the original model. The KL divergence penalty further prevents deviation. However, this penalty often causes the model‚Äôs degeneration. As a solution, InstructGPT (Ouyang et al., 2022) proposes PPO-ptx, where the supervised fine-tuning term with the pretraining dataset is included in the loss function. While this aims to align the training model with specific preferences, it tends to anchor the model to the pretraining dataset. Instead, we perform supervised fine-tuning through the Teacher‚Äôs correction feedback to allow the training model to more freely adapt and meet specific preferences without degeneration.\n\n2.4 Algorithm Detail\n\nWe propose a hybrid algorithm Alternating between REinforcement learning and Supervised fine-tuning (ARES). Figure 2 illustrates the ARES pipeline. First, we prepare a model with a given training dataset and generate rationale reasoning composed of several sentences for input. For the RL procedure to align the training model with a preference, we request scores for each sentence. The RL result may include some incorrect parts (colored as the red 4th sentence in the RL result box), but it aims to maximize the rewards provided. Next, we request correction feedback and create a corrected dataset (colored as the blue 3rd sentence in the Corrected Rationale box) for the supervised fine-tuning stage. We then repeat the process from the RL stage until convergence.\n\n3 Experimental Setup\n\nData: We first evaluate our proposed method on the ScienceQA (Lu et al., 2022a) dataset, a large-scale, multi-modal science dataset designed to assess multi-hop reasoning abilities. We choose ScienceQA because it contains reasoning chains to derive the answer. Each problem consists of a question, multiple options, multi-modal contexts, a correct answer, and an annotated lecture or solution chain (note that around 9.5%percent9.59.5\\%9.5 % lack the solution chain). In addition, we conduct experiments on A-OKVQA (Schwenk et al., 2022), a knowledge-based multi-modal benchmark with a diverse set of challenging questions paired with rationales, demanding non-trivial commonsense knowledge (see Appendix B).\n\nBaselines: We mainly compare our method with Multimodal-CoT (MM-CoT) (Zhang et al., 2023b) as the baseline because it utilizes reasoning chains to solve multi-modal tasks. MM-CoT leverages two distinct models: the first generates rationale for a given problem, and the second, an inference model, takes the concatenated input (problem and generated rationale). This separated framework shows improved performance, even for relatively small models such as Flan-AlpacaBasesubscriptFlan-AlpacaBase\\text{Flan-Alpaca}_{\\text{Base}}Flan-Alpaca start_POSTSUBSCRIPT Base end_POSTSUBSCRIPT (Chia et al., 2023) (251251251251M) and Flan-AlpacaLargesubscriptFlan-AlpacaLarge\\text{Flan-Alpaca}_{\\text{Large}}Flan-Alpaca start_POSTSUBSCRIPT Large end_POSTSUBSCRIPT (790790790790M). We use the rationale model provided by MM-CoT for ScienceQA and retrain the rationale model ourselves for A-OKVQA because there is no provided model.\n\nPrompts for Feedback: Since our proposed ARES requests different types of feedback for each stage, a corresponding prompt exists separately. We use Claude 3 Haiku for all training to get feedback because it is about 20202020 times cheaper than the top competing models, yet still demonstrates decent performance. We first request scores ranging from 0.00.00.00.0 to 1.01.01.01.0 for each sentence in CoT to proceed with the RL stage. To obtain reasonable scores, we let Haiku consider the starting point of thought, the process of elimination, or true statements. (See Table 5.)\n\nIn order to collect the corrected dataset for the SFT stage, we let Haiku refer to the given problem and correct the answer as the prompt. We ask Haiku to maintain the format of the existing rationale chains as much as possible and correct only the parts that require correction. The RL stage often makes the training model generate repetitive sentences. This repetition is not easily removed even by GPT-4 when the repetitive sentence exists in the middle of rationale reasoning. To reduce the burden of feedback, we simply hard-code the removal of repetitive sentences before adding the generated rationale to the prompt. (See Appendix C.2.)\n\nTraining Details: For the ARESBasesubscriptARESBase\\text{ARES}_{\\rm{Base}}ARES start_POSTSUBSCRIPT roman_Base end_POSTSUBSCRIPT RL stage, we use a learning rate of 2‚Å¢e‚àí52ùëí52e{-5}2 italic_e - 5 and 10101010 epochs for PPO with a batch size of 8888 for both ScienceQA and A-OKVQA. The learning rate for ARESLargesubscriptARESLarge\\text{ARES}_{\\rm{Large}}ARES start_POSTSUBSCRIPT roman_Large end_POSTSUBSCRIPT is 2‚Å¢e‚àí52ùëí52e{-5}2 italic_e - 5 with 5555 epochs for PPO and a batch size of 2222 for both tasks. We proceed with 2 rounds of our pipeline for ARESBasesubscriptARESBase\\text{ARES}_{\\rm{Base}}ARES start_POSTSUBSCRIPT roman_Base end_POSTSUBSCRIPT and 2 rounds for ARESLargesubscriptARESLarge\\text{ARES}_{\\rm{Large}}ARES start_POSTSUBSCRIPT roman_Large end_POSTSUBSCRIPT for ScienceQA. For A-OKVQA, we proceed with 1 round for both model sizes. For the SFT stage for correction, we follow the hyperparameters used in MM-CoT for both model sizes. Additionally, we replace MM-CoT‚Äôs inference model, which is the same size as the rationale model, with a LoRA adapter added to the rationale model (Figure 4). The LoRA adapter effectively refers to the rationale model‚Äôs features with a small number of weights and infers answers. Please refer to the more detailed settings in Appendix C.\n\nEvaluation Metrics: We use two main metrics to test how our pipeline (ARES) improves rationale reasoning quality. First, we evaluate ARES‚Äôs rationale reasoning quality against baseline models since we enhance our model based on them. For two different model sizes (Flan-AlpacaBasesubscriptFlan-AlpacaBase\\text{Flan-Alpaca}_{\\rm{Base}}Flan-Alpaca start_POSTSUBSCRIPT roman_Base end_POSTSUBSCRIPT and Flan-AlpacaLargesubscriptFlan-AlpacaLarge\\text{Flan-Alpaca}_{\\rm{Large}}Flan-Alpaca start_POSTSUBSCRIPT roman_Large end_POSTSUBSCRIPT) and two tasks (ScienceQA and A-OKVQA), rationale reasoning quality is evaluated by GPT-4o-2024-05-13 and the win rate is calculated (Section 4.3). The GPT-4 series is actively used as an evaluation metric, replacing human judgment for various domains (Liu et al., 2023b; Sottana et al., 2023). Second, we assess how the improved rationale reasoning impacts answer accuracy (Section 4.4). This evaluation is also performed on both model sizes and tasks. Additionally, we analyze how the RL stage fine-tunes the training model and maximizes the sum of rewards in Section 4.1.\n\n4 Experimental Results\n\nIn this section, we evaluate our pipeline ARES that Alternates REinforcement Learning and Supervised Fine-Tuning by requesting diverse types of feedback for an advanced AI model (Teacher) (Claude 3 Haiku). The goal of ARES is to improve the rationale reasoning quality. We demonstrate how ARES enhances rationale reasoning in the following sections.\n\n4.1 Emergent Behavior Through RL\n\nThrough RL, a training model is aligned to a specific preference. Essentially, the model increases the probability of helpful sentences receiving good rewards and reduces the probability of incorrect or meaningless sentences. However, this process produces some interesting additional results.\n\nFirst, it supplements rationale reasoning for some problems where rationale reasoning is insufficient. In particular, 9.5% of problems in ScienceQA have empty rationale reasoning (solution) data. The model generates nothing before the RL stage for these problems but starts generating reasoning chains afterward (See Table 12). We observe this especially when utilizing PPO‚Äôs advantage normalization or when the learning rate is large.\n\nSecond, the training model begins to explain why other options are not the answer (See Table 13). The process of elimination is a useful method for deriving answers when options are given.\n\n4.2 Guide RL with Correction\n\nDespite the benefits of RL, hyperparameter tuning often requires massive effort. Without meticulous tuning, the RL fine-tuned model may produce errors such as repetitive or incomplete sentences. To address these issues, we include a supervised fine-tuning (SFT) stage after RL to correct these errors. SFT is more stable than RL. We evaluate how well the SFT stage corrects errors caused by the RL stage for various RL hyperparameters. We test various RL hyperparameters such as learning rate = {5e-6, 1e-5, 2e-5, 5e-5}, batch size = {2, 4, 8, 16, 32}, and PPO epoch = {5, 10, 15}. As a result of RL, we observe that some of the sentences in rationale chains are repetitive or truncated (see Table 11 and 10). The SFT stage, with correction feedback, reflects what RL wants to achieve and appropriately guides it. However, excessive RL learning rates or epochs cause serious degeneration of the model, such as producing no output or generating strange words, and the results of correction feedback are also unreasonable.\n\n4.3 Rationale Reasoning Comparison\n\nWe check whether ARES improves the quality of rationale reasoning compared to the baseline model. GPT-4o evaluates which rationale chain is better between the rationale generated by ARES and the rationale generated by the baseline model. We randomly shuffle the rationale chains and provide them as Option A and Option B (see Appendix A.3) for a fair evaluation (Yu et al., 2023). We conduct our experiments with two different model sizes, Flan-Base and Flan-Large with ViT feature, on ScienceQA and A-OKVQA. Table 1 shows that ARES achieves around 70% win rate against each corresponding baseline model for both datasets.\n\n4.4 Inference Accuracy\n\nWe investigate whether the improved rationale also contributes to answer inference accuracy. Table 2 shows the main results of answer inference on the ScienceQA. We evaluate our base model against the MM-CoT baseline. ARESBasesubscriptARESBase\\textbf{ARES}_{\\rm{Base}}ARES start_POSTSUBSCRIPT roman_Base end_POSTSUBSCRIPT achieves a 2.79% improvement compared to the corresponding baseline (MM-CoTBasesubscriptMM-CoTBase\\text{MM-CoT}_{\\text{Base}}MM-CoT start_POSTSUBSCRIPT Base end_POSTSUBSCRIPT). The large model (ARESLargesubscriptARESLarge\\textbf{ARES}_{\\rm{Large}}ARES start_POSTSUBSCRIPT roman_Large end_POSTSUBSCRIPT) shows some minimal improvement compared to the corresponding baseline. However, it‚Äôs worth noting that despite this seemingly small gain, ARESLargesubscriptARESLarge\\textbf{ARES}_{\\rm{Large}}ARES start_POSTSUBSCRIPT roman_Large end_POSTSUBSCRIPT beats to 13131313B LLaVA (Liu et al., 2023a). This minimal improvement may be due to the 9.5% of ScienceQA problems needing more rationale reasoning (around 9.5% problems have empty rationale reasoning). The RL stages can only eliminate some empty rationale reasoning, which requires numerous ARES pipeline rounds. Above all, our main goal is to assess how the RL stage works and how the SFT stage aids RL.\n\nTable 3 shows the results of answer inference on the A-OKVQA. We retrain MM-CoTBasesubscriptMM-CoTBase\\text{MM-CoT}_{\\text{Base}}MM-CoT start_POSTSUBSCRIPT Base end_POSTSUBSCRIPT and MM-CoTLargesubscriptMM-CoTLarge\\text{MM-CoT}_{\\text{Large}}MM-CoT start_POSTSUBSCRIPT Large end_POSTSUBSCRIPT and evaluate these on the validation set as in (Zhang et al., 2023b) because the test set is hidden. In our experiments, MM-CoT models perform around 10% better than the reported accuracy in (Zhang et al., 2023b). ARES achieves 4.45% gains against MM-CoTBasesubscriptMM-CoTBase\\text{MM-CoT}_{\\text{Base}}MM-CoT start_POSTSUBSCRIPT Base end_POSTSUBSCRIPT and 2.35% for MM-CoTLargesubscriptMM-CoTLarge\\text{MM-CoT}_{\\text{Large}}MM-CoT start_POSTSUBSCRIPT Large end_POSTSUBSCRIPT.\n\nIn addition, we demonstrate that two stages, RL and SFT, are essential through an ablation study. Figure 3 shows the rationale reasoning for 4 cases. The baseline model (MM-CoT) produces the same rationale reasoning as the dataset. However, the corrected reasoning for MM-CoT without the RL stage has insufficient information compared to the reasoning of ARES that performs RL (refer to Table 14 for more examples). Table 4 also shows that inference accuracy gradually improves as each part of ARES is executed. 1st RL indicates a single RL run on MM-CoT, and 1st ARES means one round of the ARES pipeline. 1st ARES & 2nd RL represents the second RL on 1st ARES, and finally, 2nd ARES refers to two rounds of ARES.\n\n5 Related Work\n\nChain-of-Thought (CoT) is a multi-step reasoning method for problem-solving that encourages LLMs to consider the intermediate reasoning steps. Zero-Shot-CoT (Kojima et al., 2023) promotes CoT by using prompts such as \"Let‚Äôs think step by step\" for LLMs. For Few-Shot-CoT (Zhang et al., 2022b; Wei et al., 2023), a few examples with reasoning processes are provided, allowing the model to refer to these examples and understand how to perform CoT. Wei et al. (2023) reveal that this CoT technique positively impacts the performance of large models (>100absent100>100> 100B), but has minimal effect on smaller models. MM-CoT (Zhang et al., 2023b) suggest that CoT is beneficial even for relatively small models, such as 200M, if the model that generates intermediate reasoning and the model that infers the answer are separated. We find that simply adding a LoRA adapter (Hu et al., 2021) to the reasoning model shows comparable performance. This framework allows the LoRA adapter to effectively refer to all features from raw text to latent features to generate answers.\n\nReinforcement Learning from Human Feedback (RLHF) (Glaese et al., 2022; Ouyang et al., 2022) and AI Feedback (RLAIF) (Bai et al., 2022) align LLMs with user preferences. Ouyang et al. (2022) collects ranked feedback from human labelers and uses this feedback to perform Reinforcement Learning (RL). Constitutional AI (CAI) (Bai et al., 2022) collects ranked AI feedback rather than costly human feedback and handles harmfulness with RL. Both approaches learn outcome-supervised reward models (ORM) using ranking-based feedback. Lightman et al. (2023), instead, propose a process-supervised reward model (PRM) that leverages sentence-level feedback for CoT. Lightman et al. (2023); Luo et al. (2024) evaluate each trained ORM and PRM with searching algorithms such as best-of-NùëÅNitalic_N or Monte Carlo Tree Search (MCTS) by selecting the highest-scored solution, demonstrating that the PRM-selected solution outperforms the ORM-selected one. Wang et al. (2024) perform RL using PRM, providing heuristic sentence-level scores for math problems that are simple to grade. As an LLM is trained with RL and starts generating outputs different from the original distribution, these reward models would not correctly provide rewards (Pitis, 2023; Byun and Perrault, 2024). Instead of training a reward model for a more general task, we perform RL by requesting sentence-level rewards from advanced AI models such as GPT-4.\n\n6 Conclusion\n\nWe propose a hybrid algorithm, ARES, which Alternates REinforcement Learning (RL) and Supervised Fine-Tuning (SFT) to enhance multi-modal rationale reasoning for ScienceQA and A-OKVQA. ARES leverages two types of feedback: 1)1)1 ) ARES requests a score from a Teacher (we used Claude 3 Haiku) for sentence-level nuanced feedback and proceeds with RL. 2)2)2 ) ARES requests the Teacher to correct rationale chains after RL, stabilizing the RL fine-tuned model with SFT. ARES is designed to aid the RL procedure without massive hyperparameter tuning while properly reflecting the desired changes from RL. We evaluate the improvement in rationale reasoning produced by ARES compared to baselines using GPT-4o, and assess how much the improved rationale chains enhance inference accuracy for the two multi-modal tasks. We hope our work inspires further research on utilizing various types of AI feedback.\n\nLimitations\n\nAlthough we address general multi-modal rationale models beyond mathematical problems, receiving feedback from AI models still needs to be more reliable for more complex tasks such as graduate-level math or expert-level knowledge. For instance, some A-OKVQA problems even contain challenging questions requiring external knowledge beyond the image alone. This challenge highlights the necessity for future research to develop methods that can effectively incorporate external knowledge sources into the model. Additionally, if the model is not publicly available for free, using the API incurs costs, and there are daily usage limits.\n\nReferences\n\nAnderson et al. (2018) Peter Anderson, Xiaodong He, Chris Buehler, Damien Teney, Mark Johnson, Stephen Gould, and Lei Zhang. 2018. Bottom-up and top-down attention for image captioning and visual question answering. Preprint, arXiv:1707.07998.\n\nBai et al. (2022) Yuntao Bai, Saurav Kadavath, Sandipan Kundu, Amanda Askell, Jackson Kernion, Andy Jones, Anna Chen, Anna Goldie, Azalia Mirhoseini, Cameron McKinnon, Carol Chen, Catherine Olsson, Christopher Olah, Danny Hernandez, Dawn Drain, Deep Ganguli, Dustin Li, Eli Tran-Johnson, Ethan Perez, Jamie Kerr, Jared Mueller, Jeffrey Ladish, Joshua Landau, Kamal Ndousse, Kamile Lukosuite, Liane Lovitt, Michael Sellitto, Nelson Elhage, Nicholas Schiefer, Noemi Mercado, Nova DasSarma, Robert Lasenby, Robin Larson, Sam Ringer, Scott Johnston, Shauna Kravec, Sheer El Showk, Stanislav Fort, Tamera Lanham, Timothy Telleen-Lawton, Tom Conerly, Tom Henighan, Tristan Hume, Samuel R. Bowman, Zac Hatfield-Dodds, Ben Mann, Dario Amodei, Nicholas Joseph, Sam McCandlish, Tom Brown, and Jared Kaplan. 2022. Constitutional ai: Harmlessness from ai feedback. Preprint, arXiv:2212.08073.\n\nBrown et al. (2020) Tom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel M. Ziegler, Jeffrey Wu, Clemens Winter, Christopher Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, and Dario Amodei. 2020. Language models are few-shot learners. Preprint, arXiv:2005.14165.\n\nByun and Perrault (2024) Ju-Seung Byun and Andrew Perrault. 2024. Symmetric reinforcement learning loss for robust learning on diverse tasks and model scales. Preprint, arXiv:2405.17618.\n\nChia et al. (2023) Yew Ken Chia, Pengfei Hong, and Soujanya Poria. 2023. Flan-alpaca: Instruction tuning from humans and machines. https://github.com/declare-lab/flan-alpaca.\n\nChowdhery et al. (2022) Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam Roberts, Paul Barham, Hyung Won Chung, Charles Sutton, Sebastian Gehrmann, Parker Schuh, Kensen Shi, Sasha Tsvyashchenko, Joshua Maynez, Abhishek Rao, Parker Barnes, Yi Tay, Noam Shazeer, Vinodkumar Prabhakaran, Emily Reif, Nan Du, Ben Hutchinson, Reiner Pope, James Bradbury, Jacob Austin, Michael Isard, Guy Gur-Ari, Pengcheng Yin, Toju Duke, Anselm Levskaya, Sanjay Ghemawat, Sunipa Dev, Henryk Michalewski, Xavier Garcia, Vedant Misra, Kevin Robinson, Liam Fedus, Denny Zhou, Daphne Ippolito, David Luan, Hyeontaek Lim, Barret Zoph, Alexander Spiridonov, Ryan Sepassi, David Dohan, Shivani Agrawal, Mark Omernick, Andrew M. Dai, Thanumalayan Sankaranarayana Pillai, Marie Pellat, Aitor Lewkowycz, Erica Moreira, Rewon Child, Oleksandr Polozov, Katherine Lee, Zongwei Zhou, Xuezhi Wang, Brennan Saeta, Mark Diaz, Orhan Firat, Michele Catasta, Jason Wei, Kathy Meier-Hellstern, Douglas Eck, Jeff Dean, Slav Petrov, and Noah Fiedel. 2022. Palm: Scaling language modeling with pathways. Preprint, arXiv:2204.02311.\n\nDai et al. (2023) Wenliang Dai, Junnan Li, Dongxu Li, Anthony Meng Huat Tiong, Junqi Zhao, Weisheng Wang, Boyang Li, Pascale Fung, and Steven Hoi. 2023. Instructblip: Towards general-purpose vision-language models with instruction tuning. Preprint, arXiv:2305.06500.\n\nDosovitskiy et al. (2021) Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, Jakob Uszkoreit, and Neil Houlsby. 2021. An image is worth 16x16 words: Transformers for image recognition at scale. Preprint, arXiv:2010.11929.\n\nEimer et al. (2023) Theresa Eimer, Marius Lindauer, and Roberta Raileanu. 2023. Hyperparameters in reinforcement learning and how to tune them. Preprint, arXiv:2306.01324.\n\nGehman et al. (2020) Samuel Gehman, Suchin Gururangan, Maarten Sap, Yejin Choi, and Noah A. Smith. 2020. Realtoxicityprompts: Evaluating neural toxic degeneration in language models. Preprint, arXiv:2009.11462.\n\nGlaese et al. (2022) Amelia Glaese, Nat McAleese, Maja Trƒôbacz, John Aslanides, Vlad Firoiu, Timo Ewalds, Maribeth Rauh, Laura Weidinger, Martin Chadwick, Phoebe Thacker, Lucy Campbell-Gillingham, Jonathan Uesato, Po-Sen Huang, Ramona Comanescu, Fan Yang, Abigail See, Sumanth Dathathri, Rory Greig, Charlie Chen, Doug Fritz, Jaume Sanchez Elias, Richard Green, So≈àa Mokr√°, Nicholas Fernando, Boxi Wu, Rachel Foley, Susannah Young, Iason Gabriel, William Isaac, John Mellor, Demis Hassabis, Koray Kavukcuoglu, Lisa Anne Hendricks, and Geoffrey Irving. 2022. Improving alignment of dialogue agents via targeted human judgements. Preprint, arXiv:2209.14375.\n\nGoyal et al. (2023) Tanya Goyal, Junyi Jessy Li, and Greg Durrett. 2023. News summarization and evaluation in the era of gpt-3. Preprint, arXiv:2209.12356.\n\nHu et al. (2021) Edward J. Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu Chen. 2021. Lora: Low-rank adaptation of large language models. Preprint, arXiv:2106.09685.\n\nKhashabi et al. (2020) Daniel Khashabi, Sewon Min, Tushar Khot, Ashish Sabharwal, Oyvind Tafjord, Peter Clark, and Hannaneh Hajishirzi. 2020. Unifiedqa: Crossing format boundaries with a single qa system. Preprint, arXiv:2005.00700.\n\nKim et al. (2018) Jin-Hwa Kim, Jaehyun Jun, and Byoung-Tak Zhang. 2018. Bilinear attention networks. Preprint, arXiv:1805.07932.\n\nKim et al. (2021) Wonjae Kim, Bokyung Son, and Ildoo Kim. 2021. Vilt: Vision-and-language transformer without convolution or region supervision. Preprint, arXiv:2102.03334.\n\nKojima et al. (2023) Takeshi Kojima, Shixiang Shane Gu, Machel Reid, Yutaka Matsuo, and Yusuke Iwasawa. 2023. Large language models are zero-shot reasoners. Preprint, arXiv:2205.11916.\n\nLee et al. (2023) Harrison Lee, Samrat Phatale, Hassan Mansoor, Thomas Mesnard, Johan Ferret, Kellie Lu, Colton Bishop, Ethan Hall, Victor Carbune, Abhinav Rastogi, and Sushant Prakash. 2023. Rlaif: Scaling reinforcement learning from human feedback with ai feedback. Preprint, arXiv:2309.00267.\n\nLi et al. (2019) Liunian Harold Li, Mark Yatskar, Da Yin, Cho-Jui Hsieh, and Kai-Wei Chang. 2019. Visualbert: A simple and performant baseline for vision and language. Preprint, arXiv:1908.03557.\n\nLightman et al. (2023) Hunter Lightman, Vineet Kosaraju, Yura Burda, Harri Edwards, Bowen Baker, Teddy Lee, Jan Leike, John Schulman, Ilya Sutskever, and Karl Cobbe. 2023. Let‚Äôs verify step by step. Preprint, arXiv:2305.20050.\n\nLiu et al. (2023a) Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee. 2023a. Visual instruction tuning. Preprint, arXiv:2304.08485.\n\nLiu et al. (2023b) Yang Liu, Dan Iter, Yichong Xu, Shuohang Wang, Ruochen Xu, and Chenguang Zhu. 2023b. G-eval: Nlg evaluation using gpt-4 with better human alignment. Preprint, arXiv:2303.16634.\n\nLu et al. (2022a) Pan Lu, Swaroop Mishra, Tony Xia, Liang Qiu, Kai-Wei Chang, Song-Chun Zhu, Oyvind Tafjord, Peter Clark, and Ashwin Kalyan. 2022a. Learn to explain: Multimodal reasoning via thought chains for science question answering. In The 36th Conference on Neural Information Processing Systems (NeurIPS).\n\nLu et al. (2022b) Pan Lu, Liang Qiu, Jiaqi Chen, Tony Xia, Yizhou Zhao, Wei Zhang, Zhou Yu, Xiaodan Liang, and Song-Chun Zhu. 2022b. Iconqa: A new benchmark for abstract diagram understanding and visual language reasoning. Preprint, arXiv:2110.13214.\n\nLuo et al. (2024) Liangchen Luo, Yinxiao Liu, Rosanne Liu, Samrat Phatale, Harsh Lara, Yunxuan Li, Lei Shu, Yun Zhu, Lei Meng, Jiao Sun, and Abhinav Rastogi. 2024. Improve mathematical reasoning in language models by automated process supervision. Preprint, arXiv:2406.06592.\n\nOuyang et al. (2022) Long Ouyang, Jeff Wu, Xu Jiang, Diogo Almeida, Carroll L. Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, John Schulman, Jacob Hilton, Fraser Kelton, Luke Miller, Maddie Simens, Amanda Askell, Peter Welinder, Paul Christiano, Jan Leike, and Ryan Lowe. 2022. Training language models to follow instructions with human feedback. Preprint, arXiv:2203.02155.\n\nPeng et al. (2019) Gao Peng, Zhengkai Jiang, Haoxuan You, Pan Lu, Steven Hoi, Xiaogang Wang, and Hongsheng Li. 2019. Dynamic fusion with intra- and inter- modality attention flow for visual question answering. Preprint, arXiv:1812.05252.\n\nPitis (2023) Silviu Pitis. 2023. Failure modes of learning reward models for LLMs and other sequence models. In ICML 2023 Workshop The Many Facets of Preference-Based Learning.\n\nRaffel et al. (2023) Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J. Liu. 2023. Exploring the limits of transfer learning with a unified text-to-text transformer. Preprint, arXiv:1910.10683.\n\nSchulman et al. (2017) John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. 2017. Proximal policy optimization algorithms. CoRR, abs/1707.06347.\n\nSchwenk et al. (2022) Dustin Schwenk, Apoorv Khandelwal, Christopher Clark, Kenneth Marino, and Roozbeh Mottaghi. 2022. A-okvqa: A benchmark for visual question answering using world knowledge. Preprint, arXiv:2206.01718.\n\nSottana et al. (2023) Andrea Sottana, Bin Liang, Kai Zou, and Zheng Yuan. 2023. Evaluation metrics in the era of GPT-4: Reliably evaluating large language models on sequence to sequence tasks. In The 2023 Conference on Empirical Methods in Natural Language Processing.\n\nTamkin et al. (2021) Alex Tamkin, Miles Brundage, Jack Clark, and Deep Ganguli. 2021. Understanding the capabilities, limitations, and societal impact of large language models. Preprint, arXiv:2102.02503.\n\nTouvron et al. (2023) Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timoth√©e Lacroix, Baptiste Rozi√®re, Naman Goyal, Eric Hambro, Faisal Azhar, Aurelien Rodriguez, Armand Joulin, Edouard Grave, and Guillaume Lample. 2023. Llama: Open and efficient foundation language models. Preprint, arXiv:2302.13971.\n\nVaswani et al. (2023) Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, and Illia Polosukhin. 2023. Attention is all you need. Preprint, arXiv:1706.03762.\n\nWang et al. (2024) Peiyi Wang, Lei Li, Zhihong Shao, R. X. Xu, Damai Dai, Yifei Li, Deli Chen, Y. Wu, and Zhifang Sui. 2024. Math-shepherd: Verify and reinforce llms step-by-step without human annotations. Preprint, arXiv:2312.08935.\n\nWei et al. (2023) Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Brian Ichter, Fei Xia, Ed Chi, Quoc Le, and Denny Zhou. 2023. Chain-of-thought prompting elicits reasoning in large language models. Preprint, arXiv:2201.11903.\n\nYu et al. (2023) Xiao Yu, Maximillian Chen, and Zhou Yu. 2023. Prompt-based monte-carlo tree search for goal-oriented dialogue policy planning. Preprint, arXiv:2305.13660.\n\nYu et al. (2019) Zhou Yu, Jun Yu, Yuhao Cui, Dacheng Tao, and Qi Tian. 2019. Deep modular co-attention networks for visual question answering. Preprint, arXiv:1906.10770.\n\nYuan et al. (2024) Weizhe Yuan, Richard Yuanzhe Pang, Kyunghyun Cho, Xian Li, Sainbayar Sukhbaatar, Jing Xu, and Jason Weston. 2024. Self-rewarding language models. Preprint, arXiv:2401.10020.\n\nZhang et al. (2023a) Renrui Zhang, Jiaming Han, Chris Liu, Peng Gao, Aojun Zhou, Xiangfei Hu, Shilin Yan, Pan Lu, Hongsheng Li, and Yu Qiao. 2023a. Llama-adapter: Efficient fine-tuning of language models with zero-init attention. Preprint, arXiv:2303.16199.\n\nZhang et al. (2022a) Susan Zhang, Stephen Roller, Naman Goyal, Mikel Artetxe, Moya Chen, Shuohui Chen, Christopher Dewan, Mona Diab, Xian Li, Xi Victoria Lin, Todor Mihaylov, Myle Ott, Sam Shleifer, Kurt Shuster, Daniel Simig, Punit Singh Koura, Anjali Sridhar, Tianlu Wang, and Luke Zettlemoyer. 2022a. Opt: Open pre-trained transformer language models. Preprint, arXiv:2205.01068.\n\nZhang et al. (2022b) Zhuosheng Zhang, Aston Zhang, Mu Li, and Alex Smola. 2022b. Automatic chain of thought prompting in large language models. Preprint, arXiv:2210.03493.\n\nZhang et al. (2023b) Zhuosheng Zhang, Aston Zhang, Mu Li, Hai Zhao, George Karypis, and Alex Smola. 2023b. Multimodal chain-of-thought reasoning in language models. Preprint, arXiv:2302.00923.\n\nZheng et al. (2023) Rui Zheng, Shihan Dou, Songyang Gao, Yuan Hua, Wei Shen, Binghai Wang, Yan Liu, Senjie Jin, Qin Liu, Yuhao Zhou, Limao Xiong, Lu Chen, Zhiheng Xi, Nuo Xu, Wenbin Lai, Minghao Zhu, Cheng Chang, Zhangyue Yin, Rongxiang Weng, Wensen Cheng, Haoran Huang, Tianxiang Sun, Hang Yan, Tao Gui, Qi Zhang, Xipeng Qiu, and Xuanjing Huang. 2023. Secrets of rlhf in large language models part i: Ppo. Preprint, arXiv:2307.04964.\n\nAppendix A Prompts\n\nA.1 Prompt for Sentence-Level Nuanced Feedback\n\nThe prompt for obtaining sentence-level nuanced feedback by Claude 3 Haiku is illustrated in Table 5. Each reasoning sentence is assigned a value between 0.00.00.00.0 and 1.01.01.01.0.\n\n‚Ä¢\n\nValues close to 0.00.00.00.0 indicate completely incorrect rationales.\n\n‚Ä¢\n\nA value of 0.50.50.50.5 represents a neutral rationale, such as an initial thought process or true statements that aid in guiding guesses towards the correct answer.\n\n‚Ä¢\n\nValues close to 1.01.01.01.0 denote a correct or highly relevant rationale.\n\nThese scores enable our model to discern the direction of changes through Reinforcement Learning (RL), reflecting the extent to which a sentence aids in resolving the problem.\n\nA.2 Prompt for Correction Feedback\n\nDue to the challenges mentioned in Section 2.2, we adopt the correction feedback approach. The following are the specific instructions for obtaining correction feedback using Claude 3 Haiku. We have established the following seven rules for obtaining correction feedback using Claude 3 Haiku: The prompt is presented as a Table 7.\n\nA.3 Prompt for Win Rate Evaluation\n\nWe prompt GPT-4o (2024-05-13) to choose which generated rationale is better for solving the question because we don‚Äôt have gold rationales. Given two generated rationales (e.g., MM-CoTBasesubscriptMM-CoTBase\\text{MM-CoT}_{\\rm{Base}}MM-CoT start_POSTSUBSCRIPT roman_Base end_POSTSUBSCRIPT and ARESBasesubscriptARESBase\\text{ARES}_{\\rm{Base}}ARES start_POSTSUBSCRIPT roman_Base end_POSTSUBSCRIPT), we ask GPT-4o: \"You are given two rationale options (A or B). Your job is to select the better rationale between A and B for solving the given problem with the given image, choices, hint, and answer. Please output only ‚ÄôA‚Äô or ‚ÄôB‚Äô.\" (see Table 6). Yu et al. (2023) find that ChatGPT is skewed towards choosing option A, so we randomly swap options A and B for each evaluation to avoid bias.\n\nAppendix B Difficulties with External Knowledge in A-OKVQA\n\nThe A-OKVQA dataset includes challenging questions paired with rationales that demand knowledge beyond the information available in the image. These questions cannot be answered simply by querying a knowledge base, as they require a deeper understanding and integration of external knowledge.\n\nOur model faces difficulties with problems that cannot be resolved using only the information from the image. While our approach is designed to improve rationales by addressing grammatical errors and incomplete or incorrect statements, it struggles with questions that necessitate external knowledge.\n\nTable 8 illustrates an example where the question requires knowledge about the typical PSI range for bicycle tires. This information is not visually apparent from the image of the bicycle alone. To answer this question correctly, one needs external knowledge about standard bicycle maintenance practices and the recommended PSI ranges for different types of bicycle tires. This highlights a challenge for our model, as it must provide correct rationales and answers without access to such external knowledge, relying solely on the image and its internal knowledge base.\n\nIn the second example (Table 9), understanding what the first two numbers of an identification tag denote requires specific external knowledge about livestock tagging systems. Such tagging systems often use coded information, where the numbers might represent the birth month, year, or other identification details. This information is not visually apparent from the image and requires familiarity with agricultural practices or livestock management, illustrating the challenge for our model in providing correct rationales and answers without external knowledge.\n\nAppendix C Training Details\n\nScienceQA has 21212121K multi-modal problems, with 12121212K for training, 4444K for validation, and 4444K for testing. It also includes various difficulty levels from elementary to high school, covering domains like natural science, language science, and social science. In addition, we conduct experiments on A-OKVQA (Schwenk et al., 2022), a knowledge-based multi-modal benchmark with a diverse set of 25K questions A-OKVQA includes 25K questions (17171717K for training, 1111K for validation, and 6666K for testing).\n\nWe adapt the same T5 encoder-decoder architecture (Raffel et al., 2023) under Base and Large settings, following (Zhang et al., 2023b), and initialized with Flan-Alpaca (Chia et al., 2023).\n\nC.1 Reinforcement Learning\n\nFor the ARESBasesubscriptARESBase\\text{ARES}_{\\rm{Base}}ARES start_POSTSUBSCRIPT roman_Base end_POSTSUBSCRIPT and ARESLargesubscriptARESLarge\\text{ARES}_{\\rm{Large}}ARES start_POSTSUBSCRIPT roman_Large end_POSTSUBSCRIPT training on the ScienceQA and A-OKVQA dataset, we employ the following settings:\n\nCommon Settings: We use top-kùëòkitalic_k sampling with k=50ùëò50k=50italic_k = 50 and and sample 4 actions. The initial coefficient for the Kullback-Leibler (KL) divergence is set to 0.00010.00010.00010.0001. The range for clipping the probability ratios in PPO is 0.20.20.20.2. The discount factor is set to 1.01.01.01.0. Token length is constrained to 512512512512. We train the model using 4 NVIDIA A100 80GB GPUs.\n\nARESBasesubscriptARESBase\\text{ARES}_{\\rm{Base}}ARES start_POSTSUBSCRIPT roman_Base end_POSTSUBSCRIPT Specific Settings: We use a learning rate of 2‚Å¢e‚àí52ùëí52e{-5}2 italic_e - 5 and 10101010 epochs for PPO with a batch size of 8888. Advantage normalization is applied for ARESBasesubscriptARESBase\\text{ARES}_{\\rm{Base}}ARES start_POSTSUBSCRIPT roman_Base end_POSTSUBSCRIPT, and gradient accumulation steps are set to 8888.\n\nARESLargesubscriptARESLarge\\text{ARES}_{\\rm{Large}}ARES start_POSTSUBSCRIPT roman_Large end_POSTSUBSCRIPT Specific Settings: The learning rate for Flan-AlpacaLargesubscriptFlan-AlpacaLarge\\text{Flan-Alpaca}_{\\rm{Large}}Flan-Alpaca start_POSTSUBSCRIPT roman_Large end_POSTSUBSCRIPT is 2‚Å¢e‚àí52ùëí52e{-5}2 italic_e - 5 with 5 epochs for PPO and a batch size of 2222 for both tasks. Advantage normalization is not used and gradient accumulation steps are set to 16161616.\n\nC.2 Supervised Fine-Tuning\n\nWe use a batch size of 8 and train for 20 epochs with a learning rate of 8‚Å¢e‚àí58ùëí58e{-5}8 italic_e - 5 for ARESBasesubscriptARESBase\\text{ARES}_{\\rm{Base}}ARES start_POSTSUBSCRIPT roman_Base end_POSTSUBSCRIPT, following (Zhang et al., 2023b). For ARESLargesubscriptARESLarge\\text{ARES}_{\\rm{Large}}ARES start_POSTSUBSCRIPT roman_Large end_POSTSUBSCRIPT, we use a batch size of 2222 and train for 50505050 epochs with a learning rate of 5‚Å¢e‚àí55ùëí55e{-5}5 italic_e - 5. The output length is set to 64646464 tokens. Training for ARESBasesubscriptARESBase\\text{ARES}_{\\rm{Base}}ARES start_POSTSUBSCRIPT roman_Base end_POSTSUBSCRIPT utilizes 1 A100 GPU, while training for ARESLargesubscriptARESLarge\\text{ARES}_{\\rm{Large}}ARES start_POSTSUBSCRIPT roman_Large end_POSTSUBSCRIPT utilizes 4 A100 GPUs. In the MM-CoT paper (Zhang et al., 2023b), because the final_eval setting was not consistent, we retrained the base model with final_eval=true and the large model with final_eval=false for consistency.\n\nToken Cleanup: In order to collect the corrected dataset, we need to identify tokens representing the end of each sentence, such as periods, question marks, and exclamation marks. In the ScienceQA dataset, a newline character often follows the ‚Äòn‚Äô being added after it. To reduce the burden of feedback, we simply hard-code the removal of repetitive sentences before adding the generated rationale to the prompt. We remove this ‚Äòn‚Äô and also ignore the backslash (\\) character. For overlapping sentences, we placed each rationale of a problem into a list. If a rationale sentence was already in the list, we did not include it again during this preprocessing step.\n\nC.3 LoRA Adapter Training\n\nMM-CoT utilizes two identically sized models for reasoning and inference tasks. In our approach, we replace the inference model with a LoRA adapter (Figure 4), which is added to the rationale model and consists of only one-tenth of the weights.\n\nFor LoRA adapter training for ScienceQA and A-OKVQA, we use a LoRA rank of r=64ùëü64r=64italic_r = 64, a LoRA Œ±=128ùõº128\\alpha=128italic_Œ± = 128, and a LoRA dropout rate of 0.050.050.050.05. The learning rate is set to 8‚Å¢e‚àí58ùëí58e{-5}8 italic_e - 5 for both ARESBasesubscriptARESBase\\text{ARES}_{\\rm{Base}}ARES start_POSTSUBSCRIPT roman_Base end_POSTSUBSCRIPT and ARESLargesubscriptARESLarge\\text{ARES}_{\\rm{Large}}ARES start_POSTSUBSCRIPT roman_Large end_POSTSUBSCRIPT. The batch size is 16161616 for ARESBasesubscriptARESBase\\text{ARES}_{\\rm{Base}}ARES start_POSTSUBSCRIPT roman_Base end_POSTSUBSCRIPT and 4444 for ARESLargesubscriptARESLarge\\text{ARES}_{\\rm{Large}}ARES start_POSTSUBSCRIPT roman_Large end_POSTSUBSCRIPT on the ScienceQA dataset. For the A-OKVQA dataset, the batch size is 4444 for both ARESBasesubscriptARESBase\\text{ARES}_{\\rm{Base}}ARES start_POSTSUBSCRIPT roman_Base end_POSTSUBSCRIPT and ARESLargesubscriptARESLarge\\text{ARES}_{\\rm{Large}}ARES start_POSTSUBSCRIPT roman_Large end_POSTSUBSCRIPT.\n\nAppendix D Comparison of Generated Rationales\n\nAs mentioned in Section 2.3 and Section 4.1, because RL increases the probability of sentences receiving positive rewards and reduces the probability of sentences receiving negative rewards, the trained model often exhibits specific phenomena. It tends to generate repetitive and incomplete sentences (Table 10 and Table 11). Before the RL steps, the model couldn‚Äôt produce rationales, but after RL steps, it starts generating meaningful rationale reasoning (Table 12). Furthermore, it begins to generate reasons why other options are not the answer (Table 13).\n\nAs illustrated in Table 14, we compare the solutions from the ScienceQA original dataset, the rationales generated by the baseline model (MM-CoTBasesubscriptMM-CoTBase\\textbf{MM-CoT}_{\\rm{Base}}MM-CoT start_POSTSUBSCRIPT roman_Base end_POSTSUBSCRIPT), the rationales from the baseline model with correction feedback applied, and the rationales generated by our model (ARESBasesubscriptARESBase\\textbf{ARES}_{\\rm{Base}}ARES start_POSTSUBSCRIPT roman_Base end_POSTSUBSCRIPT). The first example, \"Which property do these three objects have in common?\" illustrates that the baseline model generates incorrect rationales such as \"The lemon is not (yellow)\" and \"All three objects are rough. The property that all three objects have in common is rough.\" However, when we apply correction feedback to the rationales generated by the baseline model and compare it to our proposed method, we see that our approach generates correct rationales that include the correct answer and provide explanations on why other options are not the answer. The second example also shows that our method improves rationale reasoning."
    }
}
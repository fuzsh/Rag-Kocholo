{
    "id": "dbpedia_5041_0",
    "rank": 93,
    "data": {
        "url": "https://forum.image.sc/t/automatically-align-and-rotate-images/30596",
        "read_more_link": "",
        "language": "en",
        "title": "Automatically align and rotate images",
        "top_image": "https://global.discourse-cdn.com/business4/uploads/imagej/original/2X/4/4904e8fe196486eabdbe0d9d185e94dfcab74e10.png",
        "meta_img": "https://global.discourse-cdn.com/business4/uploads/imagej/original/2X/4/4904e8fe196486eabdbe0d9d185e94dfcab74e10.png",
        "images": [
            "https://github.githubassets.com/favicon.ico",
            "https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/7/5/750197ef565b1d516e5e5f39ea1486e30a529333.png",
            "https://sea1.discourse-cdn.com/business4/user_avatar/forum.image.sc/dstirling/48/23341_2.png",
            "https://sea1.discourse-cdn.com/business4/user_avatar/forum.image.sc/dstirling/48/23341_2.png",
            "https://sea1.discourse-cdn.com/business4/user_avatar/forum.image.sc/research_associate/48/17481_2.png",
            "https://sea1.discourse-cdn.com/business4/user_avatar/forum.image.sc/dstirling/48/23341_2.png",
            "https://emoji.discourse-cdn.com/twitter/slight_smile.png?v=9",
            "https://sea1.discourse-cdn.com/business4/user_avatar/forum.image.sc/research_associate/48/17481_2.png",
            "https://sea1.discourse-cdn.com/business4/user_avatar/forum.image.sc/research_associate/48/17481_2.png",
            "https://emoji.discourse-cdn.com/twitter/+1.png?v=12",
            "https://emoji.discourse-cdn.com/twitter/+1.png?v=12",
            "https://sea1.discourse-cdn.com/business4/user_avatar/forum.image.sc/research_associate/48/17481_2.png",
            "https://emoji.discourse-cdn.com/twitter/laughing.png?v=12",
            "https://sea1.discourse-cdn.com/business4/user_avatar/forum.image.sc/research_associate/48/17481_2.png",
            "https://emoji.discourse-cdn.com/twitter/muscle.png?v=12"
        ],
        "movies": [],
        "keywords": [],
        "meta_keywords": [
            "fiji",
            "cellprofiler",
            "imagej",
            "registration"
        ],
        "tags": null,
        "authors": [
            "Research_Associate (MicroscopyRA)",
            "DStirling (David Stirling)",
            "Christian_Tischer (Christian Tischer)",
            "bogovicj (John Bogovic)",
            "Anthony_Santella (Anthony Santella)",
            "lakshmi (Lakshmi Balasubramanian)"
        ],
        "publish_date": "2019-10-21T18:32:26+00:00",
        "summary": "",
        "meta_description": "Hi everyone, \nI have some large stitched images of consecutive tissue sections, each stained with different dyes. I’d like to align them in order to analyse the stains together. To achieve this the images are going to ne&hellip;",
        "meta_lang": "en",
        "meta_favicon": "https://global.discourse-cdn.com/business4/uploads/imagej/optimized/2X/4/4b3d366e9bd1c4abfb68ca23f7596edd29cb2f4a_2_32x32.png",
        "meta_site_name": "Image.sc Forum",
        "canonical_link": "https://forum.image.sc/t/automatically-align-and-rotate-images/30596",
        "text": "Hi everyone,\n\nI have some large stitched images of consecutive tissue sections, each stained with different dyes. I’d like to align them in order to analyse the stains together. To achieve this the images are going to need to be rotated a bit as well as moved in xy. I’d like to know if there is a way to do this automatically? I understand that some approaches achieve this by having the user mark points of reference, but I’d ideally like to avoid having to do so with large numbers of images.\n\nThanks!\n\nI think this was the primary reason we ended up getting Visiopharm, despite the fact that it wouldn’t output the aligned images. QuPath can translate objects between images after alignment (so that you can generate new data in those objects based on the new image), but the whole process is semi-automated right now. And affine only, which usually doesn’t work well for consecutive sections except in a very broad tissue organization kind of way, you won’t likely get accurate cell by cell data.\n\nWould be neat if anyone has anything effective on whole slide images, handles deformation, and is open source.\n\n@DStirling,\n\n@bcimini\n\nI am not sure if Cellprofiler has stitching module/application, but it might be worth exploring this link.\n\nNevertheless, I am sure Bio-formats can do image stitching. Again on complete automation I am not sure, you might have to write a small script or so.\n\nRegards,\n\nLakshmi\n\nFujifilm Wako Automation (Consultant)\n\nwww.wakoautomation.com\n\nFor CellProfiler training or optimised pipeline write to,\n\nlakshmi.balasubramanian.contractor@fujifilm.com\n\nSLAS 2019 Advanced 3D Human Models and High-Content Analysis Symposium\n\nRead more on our site.\n\nYokogawa CV8000 - The Ultimate in Confocal HCS\n\nhttps://www.wakoautomation.com/products/yokogawa-high-content-imaging\n\nHi @lakshmi,\n\nWe’re looking to align whole images, rather than trying to stitch them. They could be imagined as z planes. Sorry if that wasn’t clear. To align properly there will need to be some vertical and horizontal movement, which isn’t too difficult, but they’ll also need a bit of rotation which CellProfiler doesn’t seem to be able to do automatically.\n\nThanks\n\nHi @Christian_Tischer,\n\nIt’s working pretty well! It’d be nice to be able to specify where to save transformed images, rather than them having to be placed in the temporary working directory. Also, when working with composite images is there an option somewhere to have it rebuild the composite after transformation? At the moment it’ll open each transformed channel in ImageJ seperately. It’s not too much trouble to write a script to overlay the channels again and then save them with an appropriate location/filename, but I’m wondering if there’s a better solution.\n\nThanks!\n\nHmm, I just recently became interested in this again, and I’m guessing the 80-100k px per side is going to make things difficult for FIJI associated plugins. Maybe elastix can be used on whole slide images directly? Will be looking into it, didn’t see a size limitation, but I haven’t searched that extensively yet.\n\nYou’re right, there’s a known issue in ImageJ where it simply can’t open images above ~27000x27000 pixels properly. I expect running elastix from the command line should work, but I’d anticipate that on such a large image it may take a few hours to process depending on the settings used.\n\nIf you’d like to scale it turns out that IrfanView has no trouble handling oversized images and scaling them down to something ImageJ can handle.\n\nThanks, that was about what I figured. Scaling down isn’t going to work, unfortunately, as there is a good chance we will need to do some deformation (so no snagging the low res affine) to get approximately cell to cell alignment. It may just be a slow, grinding process. But if elastix can get the job done, in the end it is just computing time. Potentially cloud computing time at scale. Maybe.\n\nCome to think of it, I don’t think the ImageJ wrapper actually needs to open the images within ImageJ itself. I think it essentially just sends the command to the external program and there’s an option to export the result rather than opening it in ImageJ, so you might be able to just use the plugin anyway on the full size images.\n\n@Research_Associate,\n\nApologies if you know everything below already.\n\nThe following is possible (and easy), as long as the metadata for your images are correct.\n\nDownsample an image (making sure the pixel spacing in the metadata is correct relative to the original)\n\nRegister that lo-res image to some target (also possibly low-res), resulting in some transform (T)\n\nApply exactly that transform (T) to the high res image. .\n\nThe result of (3) will be what you would want (when using elastix and other good registration libraries).\n\nThis will be a problem only if there is high spatial frequency deformation necessary. In every task I’ve come across, this kind of deformation is exactly what I want to avoid. The point being, I find it really useful to go through the above process since (in my experience) using the highest-res original images is computationally wasteful (in the best case), and sometimes gives worse results.\n\nYou’re probably right (@Christian_Tischer ?) , though it may not be in the case that the file is in a format that does not play nicely with ITK (see this thread)\n\nJohn\n\nThanks! And ideally what you described first is exactly how we would want to proceed, but I don’t think it will give accurate cell-cell information due to problems with the slide scanning (stitching artifacts), slight stretching and sliding of parts of the tissue, etc. If you have been able to get single cell accuracy on overlays using downsampled alignment, that is great news, and hopefully we will see similar!\n\nWith practice much of those pre-analysis problems will be mitigated, but who knows. The plan is to give downsampling a try first anyway and seeing how the results look due to there not being a quick and easy way of accomplishing the high res alignment that would be preferable.\n\nThe objective is multiplex cell by cell quantification of whole slide images using strip and restain methods. That will require very precise alignment of not so perfect images, though the overall tissue structure shouuuuuuuld be the same. Though it won’t be\n\nI have seen this work and accomplished it in Visiopharm, which I have access to, but Visiopharm doesn’t play nice with outputting the aligned images for analysis in other software which makes that usefulness… limited. Plus, figuring out an entirely open source pipeline has it’s own benefits.\n\nI’d love to hear too if anything has happened on this front in the year that’s passed. Since getting the linear pipeline in QuPath up on whole scanned slides (thanks to @Research_Associate and @petebankhead) I’ve been looking at the results and the residual misalignment after applying a rigid transformation (to the same restained and rescanned slide). The distortion looks nonlinear, but very low frequency. In principle doing a low resolution nonlinear alignment based on the tissue outline even without a shared stain doesn’t seem too hard (I assume this is what Visiopharm’s solution is doing, have been thinking about demoing it) In practice I imagine there are significant purely engineering gotchas to doing it on whole slides. Is there an open source process that is working for anyone?\n\nI don’t have anything, as we resorted to a more general stain based analysis rather than true cellular mutiplex classifications. That worked well enough with rigid alignment.\n\nI think @smcardle was working on something with warps and using that information back in QuPath, but I am not sure that is applicable to whole slide->whole slide or just maps->whole slides. Or if it is even in a sharable state."
    }
}
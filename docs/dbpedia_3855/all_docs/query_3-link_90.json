{
    "id": "dbpedia_3855_3",
    "rank": 90,
    "data": {
        "url": "https://community.apollographql.com/t/feedback-thread-future-deprecation-of-servicelist-in-apollogateway/1053",
        "read_more_link": "",
        "language": "en",
        "title": "Feedback thread: Future deprecation of serviceList in ApolloGateway",
        "top_image": "https://global.discourse-cdn.com/business5/uploads/apollographql/original/2X/d/d862cda45678eb38c0dba6787c3f4049dd28ae5f.svg",
        "meta_img": "https://global.discourse-cdn.com/business5/uploads/apollographql/original/2X/d/d862cda45678eb38c0dba6787c3f4049dd28ae5f.svg",
        "images": [
            "https://sea1.discourse-cdn.com/business5/user_avatar/community.apollographql.com/ilanvi/48/277_2.png",
            "https://sea1.discourse-cdn.com/business5/user_avatar/community.apollographql.com/jeffrey_gunzelman/48/721_2.png",
            "https://emoji.discourse-cdn.com/twitter/grinning_face_with_smiling_eyes.png?v=12",
            "https://sea1.discourse-cdn.com/business5/user_avatar/community.apollographql.com/brettski/48/682_2.png",
            "https://sea1.discourse-cdn.com/business5/user_avatar/community.apollographql.com/brettski/48/682_2.png",
            "https://sea1.discourse-cdn.com/business5/user_avatar/community.apollographql.com/brettski/48/682_2.png",
            "https://sea1.discourse-cdn.com/business5/user_avatar/community.apollographql.com/kevin-lindsay-1/48/694_2.png",
            "https://sea1.discourse-cdn.com/business5/user_avatar/community.apollographql.com/kevin-lindsay-1/48/694_2.png",
            "https://sea1.discourse-cdn.com/business5/user_avatar/community.apollographql.com/kevin-lindsay-1/48/694_2.png",
            "https://sea1.discourse-cdn.com/business5/user_avatar/community.apollographql.com/kevin-lindsay-1/48/694_2.png",
            "https://sea1.discourse-cdn.com/business5/user_avatar/community.apollographql.com/kevin-lindsay-1/48/694_2.png",
            "https://sea1.discourse-cdn.com/business5/user_avatar/community.apollographql.com/kevin-lindsay-1/48/694_2.png",
            "https://sea1.discourse-cdn.com/business5/user_avatar/community.apollographql.com/kevin-lindsay-1/48/694_2.png",
            "https://sea1.discourse-cdn.com/business5/user_avatar/community.apollographql.com/kevin-lindsay-1/48/694_2.png",
            "https://sea1.discourse-cdn.com/business5/user_avatar/community.apollographql.com/kevin-lindsay-1/48/694_2.png",
            "https://sea1.discourse-cdn.com/business5/user_avatar/community.apollographql.com/kevin-lindsay-1/48/694_2.png",
            "https://emoji.discourse-cdn.com/twitter/smiley.png?v=12",
            "https://sea1.discourse-cdn.com/business5/user_avatar/community.apollographql.com/abernix/48/10_2.png",
            "https://sea1.discourse-cdn.com/business5/user_avatar/community.apollographql.com/abernix/48/10_2.png",
            "https://sea1.discourse-cdn.com/business5/user_avatar/community.apollographql.com/abernix/48/10_2.png",
            "https://sea1.discourse-cdn.com/business5/user_avatar/community.apollographql.com/abernix/48/10_2.png",
            "https://sea1.discourse-cdn.com/business5/user_avatar/community.apollographql.com/abernix/48/10_2.png",
            "https://sea1.discourse-cdn.com/business5/user_avatar/community.apollographql.com/kevin-lindsay-1/48/694_2.png",
            "https://sea1.discourse-cdn.com/business5/user_avatar/community.apollographql.com/abernix/48/10_2.png",
            "https://sea1.discourse-cdn.com/business5/user_avatar/community.apollographql.com/brettski/48/682_2.png",
            "https://emoji.discourse-cdn.com/twitter/slightly_smiling_face.png?v=12"
        ],
        "movies": [],
        "keywords": [],
        "meta_keywords": [
            "federation"
        ],
        "tags": null,
        "authors": [
            "david.hay"
        ],
        "publish_date": "2021-08-10T23:25:29+00:00",
        "summary": "",
        "meta_description": "In federated architectures, the ApolloGateway class currently supports a serviceList option that allows the gateway to fetch subgraph schemas and perform composition on them at runtime. \nThis option is strongly discourag&hellip;",
        "meta_lang": "en",
        "meta_favicon": "https://global.discourse-cdn.com/business5/uploads/apollographql/optimized/2X/9/9a3bbed4621a3d45f66254887b5fa3ab72c3c1e3_2_32x32.png",
        "meta_site_name": "Apollo GraphQL",
        "canonical_link": "https://community.apollographql.com/t/feedback-thread-future-deprecation-of-servicelist-in-apollogateway/1053",
        "text": "Not using managed federation here.\n\nOne of the issues I can see from a workflow standpoint is the need to automatically detect changes to the subschemas.\n\nFor example, if a child schema is updated, we’d have to detect that a change in the schema, then we’d apparently need to introspect against the running schema, do the same for all of the other services, then stitch together into one superschema.\n\nWe use kubernetes, and it seems like we’d need to then introspect the service after it runs to get its schema at runtime, which will probably be very difficult inside of a CI job, and then we’d need to trigger another CI job with all of the updated schemas for all of the services as artifacts, compose the superschema, and then output a new artifact into its build stage.\n\nFor our use case the hardest part here would be fetching the introspected query at runtime for each of these child services. Does rover support the ability to create a subgraph introspect schema from a directory of .gql files?\n\nOtherwise, inside of CI we’d have to run this service and get its runtime schema, or worse, connect to the service from the context of a CI job in order to introspect.\n\nFor us, being able to generate the introspection from a static set of files, similar to how @graphql-codegen is quite robust in this regard, is looking more and more appealing. We don’t currently use rover, but if we can robustly generate the same schema we would see at runtime from directory of files (likely detecting .gql/graphql and .js/ts using gql tag), then we should be ok to migrate off serviceList.\n\nWithout it, it might be a great deal more complex of a set of operations to perform in the context of CI, and be a big pain point in migrating off.\n\nI appreciate this feedback thread, thank you. This shift away from serviceList has caused us many hours of investigation time and our production graph is still running v 2.22.2 to retain certain functionality. We are really looking to the future and are hoping we can find a new solution as are very interested in getting our bits up to latest versions, I may break out in hives being so far behind.\n\nWe run a multi function GraphQL endpoint in GCP’s Google Cloud Run which contains 6 child, sub-graph endpoints and a Gateway endpoint. All of our code is open source in GitHub. CI/CD is managed in GitHub Actions where approved PR’s to main are deployed to Google Cloud Run, verified, and finally the gateway is instructed to reload the schema. For this most part this has been a flawless approach for the last two years.\n\nWith all of our graph end points running in GCP’s Google Cloud Run, the paradigm to a polling system has been a hard pill to swallow. Being a containerized function having the process poll every 10 seconds seems like a waste (and may have other issues if it is a background process). Our current configuration utilizes the function apolloGateway.load() to instruct the gateway to refetch its schema. In most cases this works fine, and we prefer the push/trigger approach to updating the gateway instead of always polling. It would be preferred to have managed federation trigger the gateway when there is a schema change and allow it to grab the latest supergraph, over continuously polling to see if there are changes. apolloGateway.load() was removed in federation v2.23.\n\nI have read the limitations list for serviceList and can’t disagree with them. Though in my experience with a federated gateway if an endpoint doesn’t respond when the graph fetches (and rebuilds) the schema the api will probably be broken as well as the are many extended types between the sub-graphs.\n\nSo what we are seeing is there are only two paths for a federated gateway going forward, managed federation and composing a supergraph. I have already explained one hesitation we have with managed federation approach. I am guessing we can add scripts to our CI/CD process to build a supergraph and redeploy or restart the gateway, though honest that does feel wrong and error prone.\n\nWe considered restarting the gateway when there is a sub-graph update, though this is actually not a straight-forward thing to do in Google Cloud Run. And now knowing that serviceList will be going away, this will be a sticking point for trying to use a supergraph as well.\n\nIn summary, after getting all that out of my head, I am not strongly against using managed federation, but with the environment we run in, polling is a really poor approach. In addition I find it frustrating that a third-party service must be used to properly utilize a software library like Apollo Federation.\n\nAgain, thank you for providing this space for feedback.\n\nAddressing some of the limitations described in the documentation:\n\nComposition might fail.\n\nI could pretty easily push & pull a last good state SDL for the gateway with a single key in S3.\n\nIf the gateway fails to compose, we just load the last good SDL via something like:\n\n// Gateway new ApolloGateway({ serviceList: [...], // maybe rename to subgraphs onCompose: async (success: boolean, supergraph?: Supergraph): Promise<Supergraph> => { if (success) { await s3.upload({ ..., Body: supergraph.toString(), }).promise() return supergraph; } const prevSupergraph = await s3.getObject({ ..., }).promise(); return prevSupergraph; } })\n\nGateway instances might differ.\n\nI could use the same process as above for the individual services, which could publish their individual schemas to s3 and just pull the most recent from each one.\n\nFor example with kubernetes:\n\nIn the child services, set an environment variable for the original Date the deployment was created. If I update the deployment, new pods have a more recent Date.\n\nWhen a child service pod comes online, go get the most recent schema from S3. If that schema was part of a deployment more recent than the Date in our environment variable, don’t upload our schema. Otherwise, upload our newer schema.\n\n// Child service const deploymentDate = new Date(process.env.DEPLOYMENT_DATE); new ApolloServer({ onStart: async (schema: Schema): Promise<void> => { // I don't know the syntax for this off the top of my head const s3DeploymentDateString = await s3.getObjectTag({ key: \"...\", tag: \"x-deployment-date\" }).promise(); const s3DeploymentDate = new Date(s3DeploymentDateString); if (deploymentDate > s3DeploymentDate) { await s3.upload({ ..., tags: { \"x-deployment-date\": deploymentDate.toIsoString() } }); } } })\n\nIn the gateway, periodically (via a retry time or some other method) go get the most recent child schemas from S3. Since we have the most recent deployments’ schemas, compose them to get the most recent Supergraph. Upload our supergraph, and exit early upon successful “polls” if the checksum still matches, or some other logic we want to use.\n\n// Gateway new ApolloGateway({ getSubgraph: async (serviceName: string): Promise<subgraphSchema> => { const schema = await s3.getObject({ ..., key: serviceName }).promise(); return schema; }, shouldUpdateSupergraph: async (supergraph: Schema): Promise<boolean> => { const latestSupergraphChecksum = await s3.getObjectTag({ ..., key: \"supergraph\", tag: \"x-checksum-or-whatever\" }).promise() if (supergraph.checksum !== latestSupergraphChecksum) { return true; } return false; } })\n\nIn the gateway, calculate a checksum for each child service’s subgraph under the hood.\n\nIn child services, calculate a checksum for its own graph under the hood.\n\nUpon being load balanced from gateway to child service, do not allow communication if the gateway and child service do not have the same checksum for the child service’s graph.\n\nVia something like this, I probably wouldn’t even have to redeploy the gateway, and even though the idea of a gateway and a child service negotiating in this way is maybe a bit imperfect, I’m not sure how you’d otherwise get around it even using pre-defined supergraph composition. So to me, this is more reliable than using rover beforehand in CI.\n\nFor the record, I would much prefer something where I have things I can hook into and provide my own logic rather than being told “you need to do it all in CI/CD”, or worse, at runtime. To me, this seems a lot less complicated for devops, doesn’t need rover, and since you can provide your own logic, it can pretty easily accommodate any architecture.\n\nThat, and making rover, a tool that isn’t part of the gateway itself, be more authoritative than the gateway seems really weird to me.\n\nI want more hooks with a well-defined contract, a la “do anything you want in the middle, but you use X to make Y” not a rigid contract like “use rover and only rover” or too loose like “we don’t provide an API for this, BYO everything”.\n\nThanks for sharing your use-case. We appreciate the feedback and we certainly want to help you traverse any obstacles you’re still finding.\n\nUsing the rover supergraph compose command with a corresponding configuration file (which, in addition to pointing to a graph reference in the Apollo Studio Registry, can point to a subgraph’s local SDL file or its introspection endpoint), your schema never leaves your environment. In this mode, you don’t need to use Apollo Studio or managed federation.\n\nWe have a number of customers that work in spaces that need to be protective of their schemas (e.g., field names that reveal secret business functions). In practice, we’ve found that many customers in this space can still use the Apollo Studio Registry after getting it approved since they also usually have other demanding operational requirements and since having a source of truth offers visibility and accountability into how the graph is evolving. We’d encourage you to get in touch with us directly to discuss these organizational obstacles, though certainly understand if that’s an obstacle that you’re very familiar with already!\n\nIn a similar spirit to particular operational constraints, users (both large and small) have found that serviceList was a bit brittle for their liking due to real-time composition happening within the Gateway at startup time (which can itself be slow on large graphs since composition is CPU-intensive) to subgraphs that may be in various states of evolution. This was fraught with runtime validation errors (and thus failures to compose, and thus inoperable gateways). Most difficultly, it was not a static artifact that could be analyzed and validated in pre-flight and something which stuck around for later analysis, for example, after an outage. Since pre-compiled supergraph files resolve these concerns, we believe it is a stable direction. I should note that the existing managed federation still did still do composition within the gateway, but the registry acted as the source of truth, so Gateways with schemas that didn’t validate were not\n\nThat’s all to say, I’d still suspect you would benefit from the use of a supergraph file! It’s possible we haven’t made the benefits of supergraphs over serviceList clear enough so far, so I hope this helps a bit.\n\nTo dig into one of your struggles a bit, you mentioned that the process was “super difficult”, has “huge overhead” and “adds layers of complexity” — can you elaborate on that? To build the supergraph you should merely need to run a rover supergraph compose or, if using managed federation, rover subgraph push command. We’ve looked at a number of workflows in designing this and they’ve all tried to be considerate of CI/CD environments. Can you help me understand?\n\nWe hope you can get up to date too! We don’t want that! It’s worth noting that the serviceList functionality is merely deprecated right now, and it should still be working as it did since managed federation was first introduced. The load() function, on the other hand, has been more of an implementation detail since we introduced native support for gateway on ApolloServer itself (thus de-necessitating load), so perhaps that’s what you’re finding difficult here is related to load? If that’s the case, there are probably some other experimental hooks that can help you solve it. I’d encourage you to open an issue or Discussion on the Apollo Server GitHub repository if you’re finding it problematic.\n\nI agree! The polling is particularly less ideal from our side of things too since users can have massive fleets of Cloud Run containers polling for updates! Further, in terms of transmitting the signal with the new schema, this also poses a similar challenge. (e.g., if they were to receive webhooks we would need to know where they are). My hunch is also that Supergraphs actually let us move in a better direction here, actually, but there’s some implementation details that are worth chatting about still.\n\nI’m curious if you’ve experimented with Google Cloud Run’s ConfigMap service? I believe this may function in a similar way to Kubernetes ConfigMaps where they can be mounted as volumes and the files on those volumes can be watched.\n\nIn this case, a configuration that might be worth entertaining here is having your Gateway “Watch” the config-map volume which has a supergraph file on disk. With your Gateway fleet watching that volume and supergraph file, you could update it using rover — writing the updated supergraph when the new subgraphs have been deployed and having the gateways roll-over. The Gateway doesn’t currently support “watching” supergraphs in this way, but it’s something we’re considering adding. (For those that do want their Gateways to update reactively, we do know that this functionality works conceivably well in Kubernetes, so it’s really a matter of whether it works for you on Google Cloud Run.)\n\nThere’s also a whole subset of users who are using tools like Argo to manage their deployments who really prefer to avoid the fully-reactive updating and opt instead for blue-green deployments that gradually roll over (and back), and we think that Supergraphs also help there, but that’s a longer discussion probably outside of the scope of this thread!\n\nDo you have any CI workflows running on your subgraphs? If your subgraphs merely registered when they deploy — using rover subgraph publish — the supergraph would be updated automatically plus it’d let you know if you’re breaking client operations or if the supergraph didn’t compose successfully.\n\nIntrospection is an action taken against a running service, but you don’t need to use rover subgraph introspect, you can directly compose the supergraph either by publishing the subgraph’s SDL file with rover subgraph publish (to generate the supergraph in Studio) or locally using rover supergraph compose with a configuration file. If you have multiple .gql files for a subgraph, you can often just concatenate them, e.g.:\n\n$ cat schemas/*.gql | rover subgraph publish my-supergraph@my-variant \\ --schema - \\ --name accounts \\ --routing-url https://my-running-subgraph.com/api\n\nWe currently don’t natively detect gql tags and extract template tagged literals in Rover — this can get tricky since you can interpolate dynamic values within them which we discourage — but if you can use .graphql files you should be good. Also, since rover accepts pipes of STDIN for schemas — and you take care to not interpolate values — you could also just use other tools (from npm) that let you extract gql tagged template literal contents and either write them to files or pipe them directly into Rover’s rover subgraph publish command.\n\nAddressing some of your bullet-points in your follow-up response (Thanks for those thoughts!):\n\nYou can build this, but we think we can provide excellent tooling that helps enable it. At the very least, the supergraph file is intended to be your artifact (that’s definitely been one of it’s design principles!).\n\nYou would need to be considerate of whether the subgraphs themselves are in an unexecutable state though, which is something we’re considering workflows on how to facilitate and orchestrate. We think Studio and Rover can both help here, and Kubernetes is definitely a primary workflow considering.\n\nThere are also other non-Apollo open-source DevOps tools that can help coordinate these things, too. We do think that our managed federation can help avoid needing to roll a lot of this on your own though, and roughly what you’re describing is part of our free offering (and backed by cloud storage, just as reliably).\n\nThat said, I do think rover can help you build many parts of this on your own using well-defined and well-tested interfaces that have been purpose built to be defensive!\n\nIn a similar spirit to what I wrote above, I think Rover can help you if you want to build this. I’ll also note that Kubernetes ConfigMaps are a great way to hot-reload configuration on Pods that are deployed. A Gateway could conceivably watch a file and you could have a separate process merely update the supergraph (via Rover) and have all the Gateways reload. This watching functionality doesn’t exist yet, but you could build it yourself, and we’re riffing on some workflows that might take us there.\n\nWe have mechanisms for the first two of these bullet-points already that are utilized by our Registry, and we’re considering more durable hand-shaking between Gateways and services once we work out some runtime environment details where that can be tricky. Good idea though!\n\nI think we probably need to take some more time to document and write about these workflows, so I’m glad we’re having this conversation. I do think there’s a blend here that we’ve been refining in our own iterations on this both internally and with large customers that’s becoming more crisp over time.\n\nSure, you can do everything without Studio and Rover the Registry or any DevOps tooling, but I think it’s worth being cautious about how much you roll on your own. Rover offers a lot of free functionality that you’d just have to rebuild yourself. We’re purpose building Rover to be part of specifically these workflows so if you’re not finding it at all useful, that’s surprising to me. (We’re putting a lot of time into evolving all of these tools to solve pretty much all of the challenges you’re describing!)\n\nRuntime is not where we want most of this to happen either since that’s more difficult to analyze and be confident in and more challenging to analyze later! CI and CD, however, is where most of our users seem to want instrument this stuff since it allows them to be really certain prior to roll-outs that they have a defensive, well-tested, and reproducible build going into production.\n\nTypically, we haven’t found introducing a command here or there in CI or CD workflows to be a particular challenge, but I’d be curious to understand your challenges with introducing preflight and static build commands to your existing CI/CD workflows.\n\nOk, I think I hear you. We’ve touched on a few subjects in this post, which I hope were helpful and enlightening and this is great feedback, so thanks for sharing. I think we’ll get to the right blend eventually, but it will hopefully be all the right amounts of flexibility, build steps, tooling, webhooks, etc.\n\nDo you have any CI workflows running on your subgraphs?\n\nNot right now; we just do a normal deployment for kube, specifically using helmchart. For validating the schema without issue in prod, right now we use tooling such as Tilt which allows you to easily spin up workloads in kubernetes locally, with images from either a registry or by building locally. For our gateway, we have references to our child services, deploy everything (and their dependencies) recursively (not as crazy as it sounds), and then spin up the gateway when everything comes online. Take about 5 minutes, requires 1 command, tilt up.\n\nIn production, we log to DataDog for issues with the gateway, but we’ve never had a schema fail to compose in prod. We have about 15 services attached to our gateway.\n\nIntrospection is an action taken against a running service, but you don’t need to use rover subgraph introspect\n\nSure, but I don’t currently give my CI jobs access to a running server to play with, and I don’t expose ingress for the child services, only the gateway. I would have to spin up the service in CI and perform introspection there, or I would have to give our CI job access to said service.\n\nBeing able to use rover against static files would probably be ideal for me. If the gateway is going to be given a static schema at runtime, I don’t really see why it would be weird to do the same for the child services; instead of loading a directly of .gql files, I would just load one pre-built schema from rover.\n\nThat said, spinning up a service locally that suddenly requires an artifact is probably a non-trivial change to the local workflow. If that’s what you need for the gateway with rover, then yeah, that’s another thing on the list to migrate.\n\nYou can build this, but we think we can provide excellent tooling that helps enable it.\n\nThere are also other non-Apollo open-source DevOps tools that can help coordinate these things, too.\n\nThe reason that I personally went with the API of a bunch of hooks on the server/gateway, is that it would allow pretty simple plugins for certain use cases. Want things to use S3/GCP/etc? Use this utility pack for that use case which provides a few ready-made hooks, similar to things like graphql-scalars.\n\nThat way, you can let the ecosystem do all of that work for you and let the packages duke it out until maybe they get adopted by The Guild or Apollo or the like; the only thing that would need to be really consistent is the individual hooks’ inputs and outputs and the core federation workflow.\n\nI’ll also note that Kubernetes ConfigMaps are a great way to hot-reload configuration on Pods that are deployed.\n\nI’m not sure if I’d want to use this, as I think there might be certain cases, such as a rollout in progress with a schema change, where I wouldn’t want to globally trigger an update. ConfigMap is a solid option, though, I’d just need to be able to specify to the gateway instances what I’d like them to do, such as reload from the ConfigMap in a rolling fashion, so as to not have all the gateways update at exactly the same time.\n\nI think it’s worth being cautious about how much you roll on your own. Rover offers a lot of free functionality that you’d just have to rebuild yourself.\n\nI agree that deviation is obviously something you can go too far with, but in general APIs exist to accommodate easy deviation, because “deviation” there is often the actual work being done.\n\nRight now it just looks like rover would require us to do things quite differently from the way we already do it, so to us it doesn’t really matter whether we do it in CI or “roll our own” via hooks, both of these approaches would require work in order to make sure it will work with our existing things (such as networking rules in CI). The hook approach, to me, has a lot more power, a lot less restriction, and would be really easy to have 1 dev sit down and noodle on it, rather than having to get devops and dev to figure out this process together, and probably the ongoing communication forever after.\n\nRuntime is not where we want most of this to happen either since that’s more difficult to analyze and be confident in and more challenging to analyze later! CI and CD, however, is where most of our users seem to want instrument this stuff since it allows them to be really certain prior to roll-outs that they have a defensive, well-tested, and reproducible build going into production.\n\nIf your service needs to be running in order to use rover, how is this particularly different from doing it at runtime? Seems like the same process with extra steps, and things being “farther away” due to being in CI. Sure, it’s safer because you can stop the CI job beforehand, but you could do the same thing at runtime and create actionable alerts just the same. Seems like the difference between rover and runtime right now is that rover forces these steps to be made transparent, whereas at runtime these individual steps are not exposed; it’s all one step right now to my knowledge, so of course you can’t stop it early.\n\nMy concern is that once you implement in CI, it’s a lot harder to remove and change than at runtime. Depending on your organization, any future workflow changes, regardless of how much it improves things, are just harder to do because of the communication involved. That, and removing such a workflow is also way harder too, and since now it would cross-cut teams, the diffusion of responsibility would be much more likely to kick in, causing teams to take forever to upgrade in the event of a breaking API/workflow change."
    }
}
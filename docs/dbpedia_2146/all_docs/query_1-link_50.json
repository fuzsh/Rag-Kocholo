{
    "id": "dbpedia_2146_1",
    "rank": 50,
    "data": {
        "url": "https://facctconference.org/2024/acceptedpapers",
        "read_more_link": "",
        "language": "en",
        "title": "ACM FAccT 2024 Accepted Papers",
        "top_image": "",
        "meta_img": "",
        "images": [
            "https://facctconference.org/static/images/acm_logo_tablet.svg"
        ],
        "movies": [],
        "keywords": [],
        "meta_keywords": [
            ""
        ],
        "tags": null,
        "authors": [],
        "publish_date": null,
        "summary": "",
        "meta_description": "",
        "meta_lang": "",
        "meta_favicon": "",
        "meta_site_name": "",
        "canonical_link": null,
        "text": "The research program of ACM FAccT solicits academic work from a wide variety of disciplines, including computer science, statistics, law, social sciences, the humanities, and policy, and multidisciplinary scholarship on fairness, accountability, and transparency in computational systems (broadly construed). We welcome contributions that consider a wide range of technical, policy, societal, and normative issues. These include, but are not limited to, issues of structural and individual (in)equity, justice in systems and policy; the material, environmental, and economic effects of computational systems.\n\nList of accepted papers\n\nArchival Papers\n\nBenjamin Fresz (Fraunhofer Institute for Manufacturing Engineering and Automation IPA, Germany), Lena L√∂rcher (Fraunhofer Institute for Manufacturing Engineering and Automation IPA, Germany) and Marco Huber (Fraunhofer Institute for Manufacturing Engineering and Automation IPA, Germany). Classification Metrics for Image Explanations: Towards Building Reliable XAI-Evaluations\n\nMiriam Rateike (Saarland University, Germany), Isabel Valera (Saarland University, Germany) and Patrick Forr√© (AI4Science Lab, AMLab, Informatics Institute, University of Amsterdam, Germany). Designing Long-term Group Fair Policies in Dynamical Systems\n\nJack Blandin (Department of Computer Science, University of Illinois at Chicago, USA) and Ian A. Kash (Department of Computer Science, University of Illinois at Chicago, USA). Learning Fairness from Demonstrations via Inverse Reinforcement Learning\n\nJessie Finocchiaro (Center for Research on Computation and Society, Harvard University, USA). Using Property Elicitation to Understand the Impacts of Fairness Regularizers\n\nDaniel James Bogiatzis-Gibbons (Birkbeck College, UK). Beyond Individual Accountability: (Re-)Asserting Democratic Control of AI\n\nSasha Luccioni (Hugging Face, Canada), Yacine Jernite (Hugging Face, United States) and Emma Strubell (Carnegie Mellon, Allen AI Institute, USA). Power Hungry Processing: Watts Driving the Cost of AI Deployment?\n\nLauren Klein (Quantitative Theory & Methods and English, Emory University, United States) and Catherine D'Ignazio (Department of Urban Studies and Planning, MIT, USA). Data Feminism for AI\n\nTim R√§z (University of Bern, Switzerland). Reliability Gaps Between Groups in COMPAS Dataset\n\nM√©lanie Gornet (i3, SES, T√©l√©com Paris, Institut Polytechnique de Paris, France), Simon Delarue (LTCI, T√©l√©com Paris, Institut Polytechnique de Paris, France), Maria Boritchev (LTCI, T√©l√©com Paris, Institut Polytechnique de Paris, France) and Tiphaine Viard (i3, SES, T√©l√©com Paris, Institut Polytechnique de Paris, T√©l√©com Paris, France). Mapping AI ethics: a meso-scale analysis of its charters and manifestos\n\nHellina Hailu Nigatu (UC Berkeley, USA) and Inioluwa Deborah Raji (UC Berkeley, USA). ‚ÄúI Searched for a Religious Song in Amharic and Got Sexual Content Instead‚Äô‚Äô: Investigating Online Harm in Low-Resourced Languages on YouTube.\n\nJiaming Qu (School of Information and Library Science, University of North Carolina at Chapel Hill, United States), Jaime Arguello (School of Information and Library Science, University of North Carolina at Chapel Hill, USA) and Yue Wang (School of Information and Library Science, University of North Carolina at Chapel Hill, USA). Why is \"Problems\" Predictive of Positive Sentiment? A Case Study of Explaining Unintuitive Features in Sentiment Classification\n\nKimon Kieslich (Institute for Information Law, University of Amsterdam, Netherlands) and Marco L√ºnich (Department of Social Sciences, Heinrich Heine University D√ºsseldorf, Germany). Regulating AI-Based Remote Biometric Identification. Investigating the Public Demand for Bans, Audits, and Public Database Registrations\n\nTrystan S. Goetze (Sue G. and Harry E. Bovay Program in the History and Ethics of Professional Engineering, Cornell University, USA). AI Art is Theft: Labour, Extraction, and Exploitation: Or, On the Dangers of Stochastic Pollocks\n\nBest Paper Award üèÜ Shomik Jain (Institute for Data, Systems, and Society, MIT, USA), Vinith Suriyakumar (Department of Electrical Engineering and Computer Science, MIT, USA), Kathleen Creel (Department of Philosophy and Religion and Khoury College of Computer Sciences, Northeastern University, USA) and Ashia Wilson (Department of Electrical Engineering and Computer Science, MIT, USA). Algorithmic Pluralism: A Structural Approach To Equal Opportunity\n\nAnna Gausen (Imperial College London, UK), Bhaskar Mitra (Microsoft Research, Canada) and Si√¢n Lindley (Microsoft Research, UK). A Framework for Exploring the Consequences of AI-Mediated Enterprise Knowledge Access and Identifying Risks to Workers\n\nZiyang Guo (Computer Science, Northwestern University, USA), Yifan Wu (Computer Science, Northwestern University, USA), Jason D. Hartline (Computer Science, Northwestern University, USA) and Jessica Hullman (Computer Science, Northwestern University, USA). A Decision Theoretic Framework for Measuring AI Reliance\n\nSofia Jaime (University of California, USA) and Christoph Kern (LMU Munich, Germany). Ethnic Classifications in Algorithmic Fairness: Concepts, Measures and Implications in Practice\n\nJasmine Fledderjohann (Lancaster University, UK), Bran Knowles (Lancaster University, UK) and Esmorie Miller (Lancaster University, UK). Algorithmic Reproductive Justice\n\nGisela Reyes-Cruz (University of Nottingham, UK, UK), Peter Craigon (University of Nottingham, UK, UK), Anna-Maria Piskopani (University of Nottingham, UK, UK), Liz Dowthwaite (University of Nottingham, UK, UK), Yang Lu (York St John University, UK, UK), Justyna Lisinska (King's College London, UK, UK), Elnaz Shafipour (University of Southampton, UK, UK), Sebastian Stein (University of Southampton, UK, UK) and Joel Fischer (University of Nottingham, UK, UK). \"Like rearranging deck chairs on the Titanic\"? Feasibility, Fairness, and Ethical Concerns of a Citizen Carbon Budget for Reducing CO2 Emissions\n\nGuilherme Dean Pelegrina (School of Applied Sciences, University of Campinas, Brazil), Miguel Couceiro (CNRS, LORIA, Universit√© de Lorraine, France) and Leonardo Tomazeli Duarte (School of Applied Sciences, University of Campinas, Brazil). A preprocessing Shapley value-based approach to detect relevant and disparity prone features in machine learning\n\nTherese Moreau (NEtwoRks, Data, and Society (NERDS), IT University of Copenhagen, Denmark), Roberta Sinatra (Center for Social Data Science (SODAS), University of Copenhagen, Denmark) and Vedran Sekara (NEtwoRks, Data, and Society (NERDS), IT University of Copenhagen, Denmark). Failing Our Youngest: On the Biases, Pitfalls, and Risks in a Decision Support Algorithm Used for Child Protection\n\nSamuel Mayworm (School of Information, University of Michigan, USA), Kendra Albert (Harvard University, USA) and Oliver L. Haimson (School of Information, University of Michigan, USA). Misgendered During Moderation: How Transgender Bodies Make Visible Cisnormative Content Moderation Policies and Enforcement in a Meta Oversight Board Case\n\nChristine Herlihy (University of Maryland, USA), Kimberly Truong (Oregon State University, USA), Alexandra Chouldechova (Microsoft Research, USA) and Miroslav Dud√≠k (Microsoft Research, USA). A structured regression approach for evaluating model performance across intersectional subgroups\n\nHibby Thach (University of Michigan, USA), Samuel Mayworm (University of Michigan, USA), Michaelanne Thomas (University of Michigan, USA) and Oliver L. Haimson (University of Michigan, USA). Trans-centered moderation: Trans technology creators and centering transness in platform and community governance: Trans-centered moderation\n\nNari Johnson (Carnegie Mellon University, USA), Sanika Moharana (Carnegie Mellon University, USA), Christina Harrington (Carnegie Mellon University, USA), Nazanin Andalibi (University of Michigan, USA), Hoda Heidari (Carnegie Mellon University, USA) and Motahhare Eslami (Carnegie Mellon University, USA). The Fall of an Algorithm: Characterizing the Dynamics Toward Abandonment\n\nWiebke Hutiri (Sony AI, Switzerland), Orestis Papakyriakopoulos (Sony AI, Switzerland) and Alice Xiang (Sony AI, USA). Not My Voice! A Taxonomy of Ethical and Safety Harms of Speech Generators\n\nTalia B Gillis (Columbia University, USA), Vitaly Meursault (Federal Reserve Bank of Philadelphia Research Department, United States) and Berk Ustun (Halicioglu Data Science Institute, UCSD, United States). Operationalizing the Search for Less Discriminatory Alternatives in Fair Lending\n\nJessica Quaye (Harvard University, USA), Alicia Parrish (Google, USA), Oana Inel (University of Z√ºrich, Switzerland), Charvi Rastogi (Google, USA), Hannah Rose Kirk (University of Oxford, UK), Minsuk Kahng (Google, USA), Erin Van Liemt (Google, USA), Max Bartolo (University College London, Cohere, UK), Jess Tsang (Google, USA), Justin White (Google, USA), Nathan Clement (Google, UK), Rafael Mosquera (ML Commons, Colombia), Juan Ciro (ML Commons, Colombia), Vijay Janapa Reddi (Harvard University, USA) and Lora Aroyo (Google, USA). Adversarial Nibbler: An Open Red-Teaming Method for Identifying Diverse Harms in Text-to-Image Generation\n\nChristian Fr√∂hlich (University of T√ºbingen, Germany) and Robert C. Williamson (University of T√ºbingen, Germany). Insights From Insurance for Fair Machine Learning\n\nNataliya Nedzhvetskaya (University of California, Berkeley, USA) and JS Tan (Massachusetts Institute of Technology, USA). No Simple Fix: How AI Harms Reflect Power and Jurisdiction in the Workplace\n\nBrooke Perreault (Wellesley College, USA), Johanna Hoonsun Lee (Wellesley College, USA), Ropafadzo Shava (Wellesley College, USA) and Eni Mustafaraj (Wellesley College, USA). Algorithmic Misjudgement in Google Search Results: Evidence from Auditing the US Online Electoral Information Environment\n\nRamya Srinivasan (Fujitsu Research of America, USA). To See or Not to See: Understanding the Tensions of Algorithmic Curation for Visual Arts\n\nMorgan Klaus Scheuerman (University of Colorado Boulder, USA). In the Walled Garden: Challenges and Opportunities for Research on the Practices of the AI Tech Industry\n\nFrancesco Paolo Nerini (DIAG, Sapienza University of Rome, Italy), Paolo Bajardi (CENTAI Institute, Italy) and Andr√© Panisson (CENTAI Institute, Italy). Value is in the Eye of the Beholder: A Framework for an Equitable Graph Data Evaluation\n\nSarah Riley (Stanford University, USA). Overriding (in)justice: pretrial risk assessment administration on the frontlines\n\nMike Laszkiewicz (Faculty of Computer Science, Ruhr University Bochum, Germany), Imant Daunhawer (Department of Computer Science, ETH Zurich, Switzerland), Julia E. Vogt (Department of Computer Science, ETH Zurich, Switzerland), Asja Fischer (Faculty of Computer Science, Ruhr University Bochum, Germany) and Johannes Lederer (Department of Mathematics, Computer Science, and Natural Sciences, University of Hamburg, Germany). Benchmarking the Fairness of Image Upsampling Methods\n\nMin-Hsuan Yeh (University of Massachusetts, United States), Blossom Metevier (University of Massachusetts, USA), Austin Hoag (Berkeley Existential Risk Initiative, USA) and Philip Thomas (University of Massachusetts, USA). Analyzing the Relationship Between Difference and Ratio-Based Fairness Metrics\n\nIra Globus-Harris (Computer and Information Sciences, University of Pennsylvania, USA), Declan Harrison (Computer and Information Sciences, University of Pennsylvania, USA), Michael Kearns (Computer and Information Sciences, University of Pennsylvania, USA), Pietro Perona (California Institute of Technology, USA) and Aaron Roth (Computer and Information Sciences, University of Pennsylvania, Amazon AWS AI, USA). Diversified Ensembling: An Experiment in Crowdsourced Machine Learning\n\nSaumya Pareek (The University of Melbourne, Australia), Eduardo Velloso (The University of Melbourne, Australia) and Jorge Goncalves (The University of Melbourne, Australia). Trust Development and Repair in AI-Assisted Decision-Making during Complementary Expertise\n\nG√°bor Bella (Lab-STICC, CNRS UMR 6285, IMT Atlantique, France), Paula Helm (Faculty of Humanities, University of Amsterdam, Netherlands), Gertraud Koch (Institute of Anthropological Studies on Culture and History, University of Hamburg, Germany) and Fausto Giunchiglia (Department of Information Engineering and Computer Science, University of Trento, Italy). Tackling Language Modelling Bias in Support of Linguistic Diversity\n\nSanne Vrijenhoek (University of Amsterdam, Netherlands), Savvina Daniil (Centrum Wiskunde & Informatica, Netherlands), Jorden Sandel (University of Amsterdam, Netherlands) and Laura Hollink (Centrum Wiskunde & Informatica, Netherlands). Diversity of What? On the Different Conceptualizations of Diversity in Recommender Systems\n\nMelissa Hall (Meta (FAIR), United States), Samuel J. Bell (Meta (FAIR), France), Candace Ross (Meta (FAIR), USA), Adina Williams (Meta (FAIR), USA), Michal Drozdzal (Meta (FAIR), Canada) and Adriana Romero Soriano (Meta (FAIR), Canada). Towards Geographic Inclusion in the Evaluation of Text-to-Image Models\n\nEmily Black (Barnard College, USA), Talia Gillis (Columbia University, USA) and Zara Yasmine Hall (Columbia University, USA). D-hacking\n\nSeamus Somerstep (Department of Statistics, University of Michigan, USA), Ya'acov Ritov (Department of Statistics, University of Michigan, USA) and Yuekai Sun (Department of Statistics, University of Michigan, USA). Algorithmic Fairness in Performative Policy Learning: Escaping the Impossibility of Group Fairness\n\nLeah Ajmani (Computer Science & Engineering, University of Minnesota, USA), Logan Stapleton (Computer Science & Engineering, University of Minnesota, Uniter States of America), Mo Houtti (Computer Science & Engineering, University of Minnesota, USA) and Stevie Chancellor (Computer Science & Engineering, University of Minnesota, USA). Data Agency Theory: A Precise Theory of Justice for AI Applications\n\nJan Simson (Institute of Statistics, LMU Munich, Germany), Alessandro Fabris (Max Planck Institute for Security and Privacy, Germany) and Christoph Kern (Institute of Statistics, LMU Munich, Germany). Lazy Data Practices Harm Fairness Research\n\nYaaseen Mahomed (University of Pennsylvania, USA), Charlie M. Crawford (Haverford College, USA), Sanjana Gautam (Pennsylvania State University, USA), Sorelle A. Friedler (Haverford College, USA) and Dana√´ Metaxa (University of Pennsylvania, USA). Auditing GPT's Content Moderation Guardrails: Can ChatGPT Write Your Favorite TV Show?\n\nKate Glazko (University of Washington, USA), Yusuf Mohammed (University of Washington, USA), Ben Kosa (University of Washington, USA), Venkatesh Potluri (University of Washington, USA) and Jennifer Mankoff (University of Washington, USA). Identifying and Improving Disability Bias in GPT-Based Resume Screening\n\nLaurens Naudts (AI, Media & Democracy Lab - Institute for Information Law, University of Amsterdam, Netherlands). The Digital Faces of Oppression and Domination: A Relational and Egalitarian Perspective on the Data-driven Society and its Regulation\n\nNingjing Tang (Carnegie Mellon University, USA), Jiayin Zhi (Carnegie Mellon University, USA), Tzu-Sheng Kuo (Carnegie Mellon University, United States), Calla Kainaroi (Independent Researcher, USA), Jeremy J. Northup (Point Park University, USA), Kenneth Holstein (Carnegie Mellon University, USA), Haiyi Zhu (Carnegie Mellon University, USA), Hoda Heidari (Carnegie Mellon University, USA) and Hong Shen (Carnegie Mellon University, USA). AI Failure Cards: Understanding and Supporting Grassroots Efforts to Mitigate AI Failures in Homeless Services\n\nDimitri Staufer (TU Berlin, Germany), Frank Pallas (Paris Lodron University Salzburg, Austria) and Bettina Berendt (TU Berlin, Germany). Silencing the Risk, Not the Whistle: A Semi-automated Text Sanitization Tool for Mitigating the Risk of Whistleblower Re-Identification\n\nRaysa Benatti (Faculty of Science, Department of Computer Science, University of T√ºbingen, Germany), Fabiana Severi (Law School of Ribeir√£o Preto, University of S√£o Paulo, Brazil), Sandra Avila (Institute of Computing, University of Campinas, Brazil) and Esther Luna Colombini (Institute of Computing, University of Campinas, Brazil). Gender Bias Detection in Court Decisions: A Brazilian Case Study\n\nElizabeth Anne Watkins (Intel Labs, USA) and Jiahao Chen (Responsible AI LLC, USA). The four-fifths rule is not disparate impact: A woeful tale of epistemic trespassing in algorithmic fairness\n\nAndr√©s Dom√≠nguez Hern√°ndez (Public Policy Programme, The Alan Turing Institute, UK), Shyam Krishna (Public Policy Programme, The Alan Turing Institute, UK), Antonella Maia Perini (Public Policy Programme, The Alan Turing Institute, UK), Michael Katell (Public Policy Programme, The Alan Turing Institute, UK), SJ Bennett (Department of Geography, Durham University, UK), Ann Borda (Public Policy Programme, The Alan Turing Institute, UK), Youmna Hashem (Public Policy Programme, The Alan Turing Institute, UK), Semeli Hadjiloizou (Public Policy Programme, The Alan Turing Institute, UK), Sabeehah Mahomed (Public Policy Programme, The Alan Turing Institute, UK), Smera Jayadeva (Public Policy Programme, The Alan Turing Institute, UK), Mhairi Aitken (Public Policy Programme, The Alan Turing Institute, UK) and David Leslie (The Digital Environment Research Institute, Queen Mary University London, UK). Mapping the individual, social and biospheric impacts of Foundation Models\n\nHansa Srinivasan (Google, USA), Candice Schumann (Google, USA), Aradhana Sinha (Google, USA), David Madras (Google, USA), Gbolahan Oluwafemi Olanubi (Google, USA), Alex Beutel (OpenAI, USA), Susanna Ricco (Google, USA) and Jilin Chen (Google, USA). Generalized People Diversity: Learning a Human Perception-Aligned Diversity Representation for People Images\n\nSunnie S. Y. Kim (Princeton University, USA), Q. Vera Liao (Microsoft, Canada), Mihaela Vorvoreanu (Microsoft, USA), Stephanie Ballard (Microsoft, USA) and Jennifer Wortman Vaughan (Microsoft, USA). \"I'm Not Sure, But...\": Examining the Impact of Large Language Models' Uncertainty Expression on User Reliance and Trust\n\nJuan-Pablo Rivera (Georgia Institute of Technology, USA), Gabriel Mukobi (Stanford University, USA), Anka Reuel (Stanford University, USA), Max Lamparth (Stanford University, USA), Chandler Smith (Northeastern University, USA) and Jacquelyn Schneider (Hoover Wargaming and Crisis Simulation Initiative, USA). Escalation Risks from Language Models in Military and Diplomatic Decision-Making\n\nAisha Sobey (Leverhulme Centre for the Future of Intelligence, University of Cambridge, UK) and Laura Carter (Independent, USA). The Harmful Fetishisation of Reductive Personal Tracking Metrics in Digital Systems\n\nPaola Lopez (University of Vienna, Austria). More than the Sum of its Parts: Susceptibility to Algorithmic Disadvantage as a Conceptual Framework\n\nEzra Awumey (Carnegie Mellon University, USA), Sauvik Das (Carnegie Mellon University, USA) and Jodi Forlizzi (Human Computer Interaction Institute, Carnegie Mellon University, USA). A Systematic Review of Biometric Monitoring in the Workplace: Analyzing Socio-technical Harms in Development, Deployment and Use\n\nJennifer Chien (University of California San Diego, UCSD, USA) and David Danks (University of California San Diego, UCSD, USA). Beyond Behaviorist Representational Harms: A Plan for Measurement and Mitigation\n\nDana Pessach (Amazon, Israel) and Barbara Poblete (Department of Computer Science, University of Chile, Chile). Gender Representation Across Online Retail Products\n\nAlan Chan (Centre for the Governance of AI, UK), Carson Ezell (Harvard University, UK), Max Kaufmann (Independent, UK), Kevin Wei (Harvard Law School, USA), Lewis Hammond (University of Oxford, UK), Herbie Bradley (University of Cambridge, UK), Emma Bluemke (Centre for the Governance of AI, UK), Nitarshan Rajkumar (University of Cambridge, UK), David Krueger (University of Cambridge, UK), Noam Kolt (University of Toronto, Canada), Lennart Heim (Centre for the Governance of AI, UK) and Markus Anderljung (Centre for the Governance of AI, UK). Visibility into AI Agents\n\nJamie Hancock (The Alan Turing Institute, UK), Sarada Mahesh (The Alan Turing Institute, UK), Jennifer Cobbe (University of Cambridge, UK), Jatinder Singh (University of Cambridge, UK) and Anjali Mazumder (The Alan Turing Institute, UK). The tensions of data sharing for human rights: A modern slavery case study\n\nJamie Hancock (The Alan Turing Institute, UK), Ruoyun Hui (The Alan Turing Institute, UK), Jatinder Singh (Compliant & Accountable Systems Group, University of Cambridge, UK) and Anjali Mazumder (The Alan Turing Institute, UK). Trouble at Sea: Data and digital technology challenges for maritime human rights concerns\n\nTianqi Kou (College of Information Sciences and Technology, Pennsylvania State University, USA). From Model Performance to Claim: How a Change of Focus in Machine Learning Replicability Can Help Bridge the Responsibility Gap\n\nAurora Zhang (Institute for Data, Systems, and Society, Massachusetts Institute of Technology, USA) and Anette Hosoi (Massachusetts Institute of Technology, USA). Structural Interventions and the Dynamics of Inequality\n\nMarco L√ºnich (Faculty of Arts and Humanities, Heinrich Heine University, Germany) and Birte Keller (Faculty of Arts and Humanities, Heinrich Heine University, Germany). Explainable Artificial Intelligence for Academic Performance Prediction. An Experimental Study on the Impact of Accuracy and Simplicity of Decision Trees on Causability and Fairness Perceptions\n\nKhotso Selialia (University of Massachusetts Amherst, USA), Yasra Chandio (University of Massachusetts Amherst, USA) and Fatima M. Anwar (University of Massachusetts Amherst, USA). Mitigating Group Bias in Federated Learning for Heterogeneous Devices\n\nEshta Bhardwaj (University of Toronto, Canada), Harshit Gujral (University of Toronto, Canada), Siyi Wu (University of Toronto, Canada), Ciara Zogheib (University of Toronto, Canada), Tegan Maharaj (University of Toronto, Canada) and Christoph Becker (University of Toronto, Canada). Machine learning data practices through a data curation lens: An evaluation framework\n\nTakuya Maeda (Faculty of Information and Media Studies, Western University, Canada) and Anabel Quan-Haase (Faculty of Information and Media Studies, Western University, Canada). When Human-AI Interactions Become Parasocial: Agency and Anthropomorphism in Affective Design\n\nKhoa Lam (BABL AI Inc., USA), Benjamin Lange (BABL AI Inc., USA), Borhane Blili-Hamelin (BABL AI Inc., USA), Jovana Davidovic (BABL AI Inc., USA), Shea Brown (BABL AI Inc., USA) and Ali Hasan (BABL AI Inc., USA). A Framework for Assurance Audits of Algorithmic Systems\n\nAlicia DeVrio (Carnegie Mellon University, USA), Motahhare Eslami (Carnegie Mellon University, USA) and Kenneth Holstein (Carnegie Mellon University, USA). Building, Shifting, & Employing Power: A Taxonomy of Responses From Below to Algorithmic Harm\n\nBest Paper Award üèÜ Lara Groves (Ada Lovelace Institute, UK), Jacob Metcalf (Data & Society Research Institute, USA), Alayna Kennedy (Independent researcher, USA), Briana Vecchione (Data & Society Research Institute, USA) and Andrew Strait (Ada Lovelace Institute, UK). Auditing Work: Exploring the New York City algorithmic bias audit regime\n\nRishabh Kaushal (Institute of Data Science, Maastricht University, Netherlands), Jacob Van De Kerkhof (Faculty of Law, Economics and Governance, Utrecht University, Netherlands), Catalina Goanta (Faculty of Law, Economics and Governance, Utrecht University, Netherlands), Gerasimos Spanakis (Department of Advanced Computing Sciences, Maastricht University, Netherlands) and Adriana Iamnitchi (Institute of Data Science, Maastricht University, Netherlands). Automated Transparency: A Legal and Empirical Analysis of the Digital Services Act Transparency Database\n\nKathleen Cachel (Worcester Polytechnic Institute, USA) and Elke Rundensteiner (Worcester Polytechnic Institute, USA). PreFAIR: Combining Partial Preferences for Fair Consensus Decision-making\n\nAmina A. Abdu (University of Michigan, USA), Lauren M. Chambers (University of California, Berkeley, USA), Deirdre K. Mulligan (University of California, Berkeley, USA) and Abigail Z. Jacobs (University of Michigan, USA). Algorithmic Transparency and Participation through the Handoff Lens: Lessons Learned from the U.S. Census Bureau‚Äôs Adoption of Differential Privacy\n\nWarren Leu (University of California, Irvine, USA), Yuta Nakashima (Osaka University, Japan) and Noa Garcia (Osaka University, Japan). Auditing Image-based NSFW Classifiers for Content Filtering\n\nArianna Manzini (Google DeepMind, UK), Geoff Keeling (Google Research, UK), Nahema Marchal (Google DeepMind, UK), Kevin R. McKee (Google DeepMind, UK), Verena Rieser (Google DeepMind, UK) and Iason Gabriel (Google DeepMind, UK). Should Users Trust Advanced AI Assistants? Justified Trust As a Function of Competence and Alignment\n\nDarren Erik Vengroff (Independent Researcher, United States). Impact Charts: A Tool for Identifying Systematic Bias in Social Systems and Data\n\nRobert Wolfe (University of Washington, United States), Isaac Slaughter (University of Washington, USA), Bin Han (University of Washington, USA), Bingbing Wen (University of Washington, USA), Yiwei Yang (University of Washington, USA), Lucas Rosenblatt (New York University, USA), Bernease Herman (University of Washington, USA), Eva Brown (University of Washington, USA), Zening Qu (University of Washington, USA), Nic Weber (University of Washington, United States) and Bill Howe (University of Washington, United States). Laboratory-Scale AI: Open-Weight Models are Competitive with ChatGPT Even in Low-Resource Settings\n\nMazda Moayeri (University of Maryland, USA), Elham Tabassi (Michigan State University, USA) and Soheil Feizi (University of Maryland, USA). WorldBench: Quantifying Geographic Disparities in LLM Factual Recall\n\nAbeba Birhane (School of Computer Science and Statistics, Mozilla Foundation and Trinity College Dublin, Ireland), Sepehr Dehdashtian (Department of Computer Science and Engineering , Michigan State University, USA), Vinay Prabhu (HAL51 Inc, USA) and Vishnu Boddeti (Computer Science and Engineering , Michigan State University, USA). The Dark Side of Dataset Scaling: Evaluating Racial Classification in Multimodal Models\n\nKerri Prinos (Washington University in St. Louis, USA), Neal Patwari (Washington University in St. Louis, USA) and Cathleen A. Power (Relational Communities, USA). Speaking of accent: A content analysis of accent misconceptions in ASR research\n\nPetros Terzis (Institute for Information Law, University of Amsterdam, Netherlands), Michael Veale (Faculty of Laws, University College London, UK) and No√´lle Gaumann (Faculty of Laws, University College London, UK). Law and the Emerging Political Economy of Algorithmic Audits\n\nBianca Giulia Sarah Schor (University of Cambridge, UK), Emma Kallina (University of Cambridge, UK), Jatinder Singh (University of Cambridge, UK) and Alan Blackwell (University of Cambridge, UK). Meaningful Transparency for Clinicians: Operationalising HCXAI Research with Gynaecologists\n\nVishwali Mhasawade (New York University, USA), Alexander D'Amour (Google, USA) and Stephen R Pfohl (Google, USA). A Causal Perspective on Label Bias\n\nDavid Gray Widder (Digital Life Initiative, Cornell Tech, USA). Epistemic Power in AI Ethics Labor: Legitimizing Located Complaints\n\nJan Simson (Institute of Statistics, LMU Munich, Germany), Florian Pfisterer (Institute of Statistics, LMU Munich, Germany) and Christoph Kern (Institute of Statistics, LMU Munich, Germany). One Model Many Scores: Using Multiverse Analysis to Prevent Fairness Hacking and Evaluate the Influence of Model Design Decisions\n\nMessi H.J. Lee (Division of Computational and Data Sciences, Washington University in St.Louis, United States), Jacob M. Montgomery (Department of Political Science, Washington University in St.Louis, USA) and Calvin K. Lai (Department of Psychological & Brain Sciences, Washington University in St.Louis, USA). Large Language Models Portray Socially Subordinate Groups as More Homogeneous, Consistent with a Bias Observed in Humans\n\nTim Alpherts (University of Amsterdam, Netherlands), Sennay Ghebreab (University of Amsterdam, Netherlands), Yen-Chia Hsu (University of Amsterdam, Netherlands) and Nanne Van Noord (University of Amsterdam, Netherlands). Perceptive Visual Urban Analytics is Not (Yet) Suitable for Municipalities\n\nNil-Jana Akpinar (Carnegie Mellon University, USA), Zachary Lipton (Carnegie Mellon Univeristy, USA) and Alexandra Chouldechova (Carnegie Mellon University, USA). The Impact of Differential Feature Under-reporting on Algorithmic Fairness\n\nTasfia Mashiat (George Mason University, USA), Alex DiChristofano (Washington University in St. Louis, USA), Patrick J. Fowler (Washington University in St. Louis, USA) and Sanmay Das (George Mason University, USA). Beyond Eviction Prediction: Leveraging Local Spatiotemporal Public Records to Inform Action\n\nSaffron Huang (Collective Intelligence Project, USA), Divya Siddarth (Collective Intelligence Project, USA), Liane Lovitt (Anthropic, USA), Thomas I. Liao (Unaffiliated, USA), Esin Durmus (Anthropic, USA), Alex Tamkin (Anthropic, USA) and Deep Ganguli (Anthropic, USA). Collective Constitutional AI: Aligning a Language Model with Public Input\n\nJoachim Baumann (University of Zurich, Switzerland), Piotr Sapiezynski (Northeastern University, USA), Christoph Heitz (Zurich University of Applied Sciences, Switzerland) and Aniko Hannak (University of Zurich, Switzerland). Fairness in Online Ad Delivery\n\nAngelie Kraft (Universit√§t Hamburg, Germany) and Elo√Øse Soulier (Universit√§t Hamburg, Germany). Knowledge-Enhanced Language Models Are Not Bias-Proof: Situated Knowledge and Epistemic Injustice in AI\n\nMaria Antoniak (Allen Institute for AI, USA), Aakanksha Naik (Allen Institute for AI, USA), Carla S. Alvarado (Association of American Medical Colleges, Center for Health Justice, USA), Lucy Lu Wang (University of Washington, Allen Institute for AI, USA) and Irene Y. Chen (University of California, Berkeley and University of California, San Francisco, USA). NLP for Maternal Healthcare: Perspectives and Guiding Principles in the Age of LLMs\n\nSean Kinahan (School of Computing and Augmented Intelligence, Arizona State University, USA), Pouria Saidi (School of Electrical, Computer, and Energy Engineering, Arizona State University, USA), Ayoub Daliri (College of Health Solutions, Arizona State University, USA), Julie Liss (College of Health Solutions, Arizona State University, USA) and Visar Berisha (College of Health Solutions, Arizona State University, USA). Achieving Reproducibility in EEG-Based Machine Learning\n\nRuotong Wang (Paul G. Allen School of Computer Science , University of Washington, USA), Ruijia Cheng (Human Centered Design and Engineering, University of Washington, USA), Denae Ford (Microsoft Research, USA) and Thomas Zimmermann (Microsoft Research, USA). Investigating and Designing for Trust in AI-powered Code Generation Tools\n\nViolet Turri (Carnegie Mellon University Software Engineering Institute, United States), Katelyn Morrison (Carnegie Mellon University, USA), Katherine-Marie Robinson (Carnegie Mellon University Software Engineering Institute, United States), Collin Abidi (Carnegie Mellon University Software Engineering Institute, USA), Adam Perer (Carnegie Mellon University, USA), Jodi Forlizzi (Carnegie Mellon University, USA) and Rachel Dzombak (Carnegie Mellon University Software Engineering Institute, USA). Transparency in the Wild: Navigating Transparency in a Deployed AI System to Broaden Need-Finding Approaches\n\nCarmen Loefflad (School of Computation, Information and Technology, Technical University of Munich, Germany) and Jens Grossklags (School of Computation, Information and Technology, Technical University of Munich, Germany). How the Types of Consequences in Social Scoring Systems Shape People's Perceptions and Behavioral Reactions\n\nRobert Wolfe (University of Washington, USA) and Tanushree Mitra (University of Washington, USA). The Impact and Opportunities of Generative AI in Fact-Checking\n\nBest Paper Award üèÜ Michael Madaio (Google Research, USA), Shivani Kapania (Carnegie Mellon University, USA), Rida Qadri (Google Research, USA), Ding Wang (Google Research, USA), Andrew Zaldivar (Google Research, USA), Remi Denton (Google Research, USA) and Lauren Wilcox (eBay, USA). Learning about Responsible AI On-The-Job: Learning Pathways, Orientations, and Aspirations\n\nEdward Small (RMIT University, Australia), Kacper Sokol (ETH Z√ºrich, Switzerland), Daniel Manning (RMIT University, Australia), Flora D. Salim (University of New South Wales, Australia) and Jeffrey Chan (RMIT University, Australia). Equalised Odds is not Equal Individual Odds: Post-processing for Group and Individual Fairness\n\nLuca Deck (University of Bayreuth, Germany), Jakob Schoeffer (University of Texas at Austin, United States), Maria De-Arteaga (University of Texas at Austin, USA) and Niklas K√ºhl (University of Bayreuth, Germany). A Critical Survey on Fairness Benefits of Explainable AI\n\nMarta Ziosi (University of Oxford, UK) and Dasha Pruss (Harvard University, USA). Evidence of What, for Whom? The Socially Contested Role of Algorithmic Bias in a Predictive Policing Tool\n\nHarini Suresh (Cornell Tech, USA), Emily Tseng (Cornell University, USA), Meg Young (Data & Society Research Institute, USA), Mary Gray (Microsoft Research, USA), Emma Pierson (Cornell Tech, USA) and Karen Levy (Cornell University, USA). Participation in the age of foundation models\n\nPiotr Mirowski (Google DeepMind, UK), Juliette Love (Google DeepMind, UK), Kory Mathewson (Google DeepMind, Canada) and Shakir Mohamed (Google DeepMind, UK). A Robot Walks into a Bar: Can Language Models Serve as Creativity SupportTools for Comedy? An Evaluation of LLMs‚Äô Humour Alignment with Comedians\n\nAli Shirali (UC Berkeley, USA), Jessie Finocchiaro (CRCS, Harvard University, USA) and Rediet Abebe (Harvard Society of Fellows, USA). Participatory Objective Design via Preference Elicitation\n\nLuke Stark (Faculty of Information and Media Studies, Western University, Canada). Animation and Artificial Intelligence\n\nAllison Koenecke (Cornell University, USA), Anna Seo Gyeong Choi (Cornell University, USA), Katelyn X. Mei (University of Washington, USA), Hilke Schellmann (New York University, USA) and Mona Sloane (University of Virginia, USA). Careless Whisper: Speech-to-Text Hallucination Harms\n\nPeter M. VanNostrand (Worcester Polytechnic Institute, United States), Dennis M. Hofmann (Worcester Polytechnic Institute, USA), Lei Ma (Worcester Polytechnic Institute, USA) and Elke A. Rundensteiner (Worcester Polytechnic Institute, USA). Actionable Recourse for Automated Decisions: Examining the Effects of Counterfactual Explanation Type and Presentation on Lay User Understanding\n\nLucas Wright (Citizens and Technology Lab, Cornell University, USA), Roxana Mika Muenster (Department of Communication, Cornell University, USA), Briana Vecchione (Data & Society Research Institute, USA), Tianyao Qu (Department of Sociology, Cornell University, USA), Pika (Senhuang) Cai (Department of Information Science, Cornell University, USA), Alan Smith (Consumer Reports, USA), NA (Cornell University, USA), Jacob Metcalf (Data & Society Research Institute, USA) and J. Nathan Matias (Citizens and Technology Lab, Cornell University, USA). Null Compliance: NYC Local Law 144 and the challenges of algorithm accountability\n\nEmily Sullivan (Utrecht University, Netherlands). SIDEs: Separating Idealization from Deceptive 'Explanations' in xAI\n\nNathalie Diberardino (Department of Philosophy, Western University, Canada), Clair Baleshta (Department of Philosophy, Western University, Canada) and Luke Stark (Faculty of Information and Media Studies, Western University, Canada). Algorithmic Harms and Algorithmic Wrongs\n\nBest Paper Award üèÜ Cedric Deslandes Whitney (UC Berkeley, USA) and Justin Norman (UC Berkeley, USA). Real Risks of Fake Data: Synthetic Data, Diversity-Washing and Consent Circumvention\n\nAyan Majumdar (MPI-SWS, Germany) and Isabel Valera (Saarland University, Germany). CARMA: A practical framework to generate recommendations for causal algorithmic recourse at scale\n\nArpit Agarwal (FAIR, Meta, USA), Nicolas Usunier (FAIR, Meta, France), Alessandro Lazaric (FAIR, Meta, France) and Maximilian Nickel (FAIR, Meta, USA). System-2 Recommenders: Disentangling Utility and Engagement in Recommendation Systems via Temporal Point-Processes\n\nAndreas Liesenfeld (Radboud University, Netherlands) and Mark Dingemanse (Radboud University, Netherlands). Rethinking open source generative AI: open washing and the EU AI Act\n\nEunice Chan (University of Illinois, Urbana-Champaign, USA), Zhining Liu (University of Illinois, Urbana-Champaign, USA), Ruizhong Qiu (University of Illinois, Urbana-Champaign, USA), Yuheng Zhang (University of Illinois, Urbana-Champaign, USA), Ross Maciejewski (Arizona State University, USA) and Hanghang Tong (University of Illinois, Urbana-Champaign, USA). Group Fairness via Group Consensus\n\nCaitlin Kearney (Technical University Munich, Germany), Jiri Hron (Google DeepMind, UK), Helen Kosc (University of Oxford, UK) and Miri Zilka (University of Cambridge, UK). Beyond Use-Cases: A Participatory Approach to Envisioning Data Science in Law Enforcement\n\nNaina Balepur (University of Illinois Urbana-Champaign, USA) and Hari Sundaram (University of Illinois Urbana-Champaign, USA). Intervening to Increase Community Trust for Fair Network Outcomes\n\nGabriel Grill (School of Information, University of Michigan, USA). Constructing Capabilities: The Politics of Testing Infrastructures for Generative AI\n\nHilde Weerts (Eindhoven University of Technology, Netherlands), Aislinn Kelly-Lyth (Blackstone Chambers, UK), Reuben Binns (University of Oxford, UK) and Jeremias Adams-Prassl (University of Oxford, UK). Unlawful Proxy Discrimination: A Framework for Challenging Inherently Discriminatory Algorithms\n\nGrace Guo (Georgia Institute of Technology, USA), Lifu Deng (Cleveland Clinic, USA), Animesh Tandon (Cleveland Clinic, USA), Alex Endert (Georgia Institute of Technology, USA) and Bum Chul Kwon (IBM Research, USA). MiMICRI: Towards Domain-centered Counterfactual Explanations of Cardiovascular Image Classification Models\n\nWill Orr (University of Southern California, USA) and Edward B. Kang (New York University, USA). AI as a Sport: On the Competitive Epistemologies of Benchmarking\n\nCornelia Evers (Independent Researcher, France). Talking past each other? Navigating discourse on ethical AI: Comparing the discourse on ethical AI policy by Big Tech companies and the European Commission\n\nHongliang Ni (University of Queensland, Australia), Lei Han (University of Queensland, Australia), Tong Chen (University of Queensland, Australia), Shazia Sadiq (University of Queensland, Australia) and Gianluca Demartini (University of Queensland, Australia). Fairness without Sensitive Attributes via Knowledge Sharing\n\nRobert Lee Poe (Laboratorio Interdisciplinare Diritti e Regole (LIDER-Lab), Sant'Anna School of Advanced Studies, Italy) and Soumia Zohra El Mestari (Interdisciplinary Research Group in Socio-technical Cybersecurity (IRISC), Interdisciplinary Centre for Security, Reliability and Trust (SnT), University of Luxembourg, Luxembourg). The Conflict Between Algorithmic Fairness and Non-Discrimination: An Analysis of Fair Automated Hiring\n\nRuta Binkyte (CISPA ? Helmholtz Center for Information Security, Germany), Daniele Gorla (Universit√† di Roma , Italy) and Catuscia Palamidessi (Inria and √âcole Polytechnique (IPP), France). BaBE: Enhancing Fairness via Estimation of Explaining Variables\n\nBest Paper AwardüèÜ Rishav Hada (Microsoft Research, India), Safiya Husain (Karya, India), Varun Gumma (Microsoft Research, India), Harshita Diddee (Carnegie Mellon University, USA), Aditya Yadavalli (Karya, India), Agrima Seth (University of Michigan, USA), Nidhi Kulkarni (Karya, India), Ujwal Gadiraju (Delft University of Technology, Netherlands), Aditya Vashistha (Cornell University, USA), Vivek Seshadri (Microsoft Research, India) and Kalika Bali (Microsoft Research, India). Akal Badi ya Bias: An Exploratory Study of Gender Bias in Hindi Language Technology\n\nMallak Alkhathlan (Worcester Polytechnic Institute, United States), Kathleen Cachel (Worcester Polytechnic Institute, USA), Hilson Shrestha (Worcester Polytechnic Institute, USA), Lane Harrison (Worcester Polytechnic Institute, USA) and Elke Rundensteiner (Worcester Polytechnic Institute, USA). Balancing Act: Evaluating People‚Äôs Perceptions of Fair Ranking Metrics\n\nMadiha Zahrah Choksi (Department of Computing and Information Science, Cornell Tech, USA), Ilan Mandel (Department of Computing and Information Science, Cornell Tech, USA), David Widder (Department of Computing and Information Science, Cornell Tech, USA) and Yan Shvartzshnaider (Department of Electrical Engineering and Computer Science, York University, Canada). The Emerging Artifacts of Centralized Open-Code\n\nSebastian Zezulka (University of T√ºbingen, Germany) and Konstantin Genin (University of T√ºbingen, Germany). From the Fair Distribution of Predictions to the Fair Distribution of Social Goods: Evaluating the Impact of Fair Machine Learning on Long-Term Unemployment\n\nAida Davani (Google Research, USA), Mark D√≠az (Google Research, USA), Dylan Baker (Distributed AI Research Institute (DAIR), USA) and Vinodkumar Prabhakaran (Google, USA). Disentangling Perceptions of Offensiveness: Cultural and Moral Correlates\n\nAlex Jiahong Lu (School of Communication and Information, Rutgers University, USA), Cameron Moy (School of Communication, University of Pennsylvania, USA), Mark S. Ackerman (University of Michigan, USA), Jeffrey Morenoff (School of Public Policy, University of Michigan, USA) and Tawanna R. Dillahunt (School of Information, University of Michigan , USA). Perceptions of Policing Surveillance Technologies in Detroit: Moving Beyond \"Better than Nothing\"\n\nRavit Dotan (TechBetter, USA), Lisa S. Parker (The University of Pittsburgh, USA) and John Radzilowicz (The University of Pittsburgh, USA). Responsible Adoption of Generative AI in Higher Education: Developing a ‚ÄúPoints to Consider‚Äù Approach Based on Faculty Perspectives\n\nAlessandra Calvi ( LSTS, d.pia.lab, Vrije Universiteit Brussel , Belgium), Gianclaudio Malgieri (eLaw - Center for Law and Digital Technologies, Leiden University, Netherlands) and Dimitris Kotzinos (Lab. ETIS UMR 8051 , CY Cergy Paris University, ENSEA, CNRS, France). The unfair side of Privacy Enhancing Technologies: addressing the trade-offs between PETs and fairness\n\nHilde Weerts (Eindhoven University of Technology, Netherlands), Rapha√´le Xenidis (Sciences Po Law School, France), Fabien Tarissan (Universit√© Paris-Saclay, CNRS, ISP, ENS Paris-Saclay, France), Henrik Palmer Olsen (University of Copenhagen, Denmark) and Mykola Pechenizkiy (Eindhoven University of Technology, Netherlands). The Neutrality Fallacy: When Algorithmic Fairness Interventions are (Not) Positive Action\n\nKimon Kieslich (Institute for Information Law, University of Amsterdam, Netherlands), Natali Helberger (Institute for Information Law, University of Amsterdam, Netherlands) and Nicholas Diakopoulos (School of Communication, Northwestern University, USA). My Future with My Chatbot: A Scenario-Driven, User-Centric Approach to Anticipating AI Impacts\n\nIsaac Slaughter (University of Washington, USA), Eva Maxfield Brown (University of Washington, USA) and Nic Weber (University of Washington, USA). The Impact of iBuying is About More Than Just Racial Disparities: Evidence from Mecklenburg County, NC\n\nNadia Nahar (Software and Societal Systems Department, School of Computer Science, Carnegie Mellon University, United States), Jenny Rowlett (Oberlin College, USA), Matthew Bray (Yale University, USA), Zahra Abba Omar (Yale University, USA), Xenophon Papademetris (Yale University, USA), Alka Menon (Yale University, USA) and Christian K√§stner (Carnegie Mellon University, United States). Regulating Explainability in Machine Learning Applications -- Observations from a Policy Design Experiment\n\nSierra Wyllie (University of Toronto and Vector Institute, Canada), Ilia Shumailov (University of Oxford, UK) and Nicolas Papernot (University of Toronto and Vector Institute, Canada). Fairness Feedback Loops: Training on Synthetic Data Amplifies Bias\n\nMohammad Rashidujjaman Rifat (Computer Science, University of Toronto, Canada), Abdullah Hasan Safir (Leverhulme Centre for the Future of Intelligence, University of Cambridge, UK), Sourav Saha (Computer Science & Engineering, Shahjalal University Of Science And Technology, Bangladesh), Jahedul Alam Junaed (Computer Science and Engineering, Shahjalal University of Science and Technology, Bangladesh), Maryam Saleki (Computer and Info Science, Fordham University, USA), Mohammad Ruhul Amin (Computer and Information Science, Fordham University, USA) and Syed Ishtiaque Ahmed (Computer Science, , University of Toronto, Toronto, Canada, Canada). Data, Annotation, and Meaning-Making: The Politics of Categorization in Annotating a Dataset of Faith-based Communal Violence\n\nVera Schmitt (TU Berlin, DFKI, Germany), Luis-Felipe Villa-Arenas (Telekom, TU Berlin , Germany), Nils Feldhus (DFKI, Germany), Joachim Meyer (Tel Aviv University, Israel), Robert P. Spang (TU Berlin, Germany) and Sebastian M√∂ller (TU Berlin, DFKI, Germany). The Role of Explainability in Collaborative Human-AI Disinformation Detection\n\nLindsay Sanneman (CSAIL, Massachusetts Institute of Technology, USA), Mycal Tucker (CSAIL, Massachusetts Institute of Technology, USA) and Julie A. Shah (CSAIL, Massachusetts Institute of Technology, USA). An Information Bottleneck Characterization of the Understanding-Workload Tradeoff in Human-Centered Explainable AI\n\nStefan Baack (Mozilla Foundation, Germany). A Critical Analysis of the Largest Source for Generative AI Training Data: Common Crawl\n\nRose Marie Santini (School of Communication, Federal University of Rio de Janeiro, Brazil), D√©bora Salles (School of Communication, Federal University of Rio de Janeiro, Brazil), Bruno Maur√≠cio Martins (School of Communication, Federal University of Rio de Janeiro, Brazil), Al√©kis Moreira (Fluminense Federal University, Brazil) and Jo√£o Gabriel Haddad (School of Communication, Federal University of Rio de Janeiro, Brazil). Seeing through opacity: The limitations of digital ad transparency in Brazil\n\nAutumn Toney (Georgetown University, USA), Kathleen Curlee (Georgetown University, USA) and Emelia Probasco (Georgetown University, USA). Trust Issues: Discrepancies in Trustworthy AI Keywords Use in Policy and Research\n\nJuan Felipe Gomez (Department of Physics, Harvard University, USA), Caio Machado (Centre for Socio-Legal Studies, School of Engineering and Applied Sciences, Law School, Oxford University; Harvard University; Universidade de S√£o Paulo, UK), Lucas Monteiro Paes (School of Engineering and Applied Sciences, Harvard University, USA) and Flavio Calmon (School of Engineering and Applied Sciences, Harvard University, USA). Algorithmic Arbitrariness in Content Moderation\n\nStephen Casper (Massachusetts Institute of Technology, USA), Carson Ezell (Harvard University, USA), Charlotte Siegmann (Massachusetts Institute of Technology, USA), Noam Kolt (University of Toronto, Canada), Taylor Lynn Curtis (Massachusetts Institute of Technology, USA), Benjamin Bucknall (Centre for the Governance of AI, UK), Andreas Haupt (Massachusetts Institute of Technology, USA), Kevin Wei (Harvard University, USA), J√©r√©my Scheurer (Apollo Research, UK), Marius Hobbhahn (Apollo Research, UK), Lee Sharkey (Apollo Research, UK), Satyapriya Krishna (Harvard University, USA), Marvin Von Hagen (Massachusetts Institute of Technology, USA), Silas Alberti (Stanford University, USA), Alan Chan (Centre for the Governance of AI, Mila (Quebec AI Institute), Canada), Qinyi Sun (Massachusetts Institute of Technology, USA), Michael Gerovitch (Massachusetts Institute of Technology, USA), David Bau (Northeastern University, USA), Max Tegmark (Massachusetts Institute of Technology, USA), David Krueger (University of Cambridge, UK) and Dylan Hadfield-Menell (Massachusetts Institute of Technology, USA). Black-Box Access is Insufficient for Rigorous AI Audits\n\nChiara Ullstein (Chair of Cyber Trust, Technical University of Munich, Germany), Severin Engelmann (Digital Life Initiative, Cornell Tech, USA), Orestis Papakyriakopoulos (Professorship of Societal Computing, Technical University of Munich, Germany), Yuko Ikkatai (Kanazawa University, Japan), Naira Paola Arnez-Jordan (Technical University of Munich, Germany), Rose Caleno (Unaffiliated, Kenya), Brian Mboya (Computer Science, Dedan Kimathi University of Technology, Kenya), Shuichiro Higuma (The University of Tokyo, Japan), Tilman Hartwig (AI Lab, Umweltbundesamt, Germany), Hiromi Yokoyama (Kavli IPMU, CD3, The University of Tokyo, Japan) and Jens Grossklags (Chair of Cyber Trust, Technical University of Munich, Germany). Attitudes Toward Facial Analysis AI: A Cross-National Study Comparing Argentina, Kenya, Japan, and the USA\n\nMazda Moayeri (University of Maryland, USA), Michael Rabbat (Meta AI, Canada), Mark Ibrahim (Meta AI, USA) and Diane Bouchacourt (Meta AI, Canada). Embracing Diversity: Interpretable Zero-shot Classification Beyond One Vector Per Class\n\nNanna Inie (IT University of Copenhagen / University of Washington, Denmark), Stefania Druga (Center for Applied Artificial Intelligence, University of Chicago, USA), Peter Zukerman (Department of Linguistics, University of Washington, USA) and Emily M. Bender (Department of Linguistics, University of Washington, USA). From \"AI\" to Probabilistic Automation: How Does Anthropomorphization of Technical Systems Descriptions Influence Trust?\n\nBasileal Imana (Princeton University, USA), Aleksandra Korolova (Princeton University, USA) and John Heidemann (University of Southern California / Information Sciences Institute, USA). Auditing for Racial Discrimination in the Delivery of Education Ads\n\nMax Lamparth (Stanford University, USA) and Anka Reuel (Stanford University, USA). Analyzing And Editing Inner Mechanisms of Backdoored Language Models\n\nVishwali Mhasawade (New York University, USA), Salman Rahman (New York University, USA), Zo√© Haskell-Craig (New York University, USA) and Rumi Chunara (New York University, USA). Understanding Disparities in Post Hoc Machine Learning Explanation\n\nBest Paper Award üèÜ Jessie J. Smith (University of Colorado Boulder, USA), Aishwarya Satwani (University of Colorado Boulder, USA), Robin Burke (University of Colorado Boulder, USA) and Casey Fiesler (University of Colorado Boulder, USA). Recommend Me? Designing Fairness Metrics with Providers\n\nSeverin Engelmann (Cornell Tech, USA), Madiha Zahrah Choksi (Cornell Tech, USA), Angelina Wang (Princeton, USA) and Casey Fiesler (University of Colorado Boulder, USA). Visions of a Discipline: Analyzing Introductory AI Courses on YouTube\n\nPooria Babaei (University of Saskatchewan, Canada) and Julita Vassileva (University of Saskatchewan, Canada). Drivers and persuasive strategies to influence user intention to learn about manipulative design\n\nSabri Eyuboglu (Computer Science, Stanford University, United States), Karan Goel (Computer Science, Stanford University, USA), Arjun Desai (Computer Science, Stanford University, USA), Lingjiao Chen (Stanford University, USA), Mathew Monfort (Amazon Web Services, USA), Chris R√© (Computer Science, Stanford University, USA) and James Zou (Biomedical Data Science, Stanford University, USA). Model ChangeLists: Characterizing Updates to ML Models\n\nInyoung Cheong (University of Washington, United States), King Xia (Independent Attorney, USA), K. J. Kevin Feng (University of Washington, United States), Quan Ze Chen (University of Washington, USA) and Amy X. Zhang (University of Washington, USA). (A)I Am Not a Lawyer, But...: Engaging Legal Experts towards Responsible LLM Policies for Legal Advice\n\nGoya van Boven (Utrecht University, Netherlands), Yupei Du (Utrecht University, Netherlands) and Dong Nguyen (Utrecht University, Netherlands). Transforming Dutch: Debiasing Dutch Coreference Resolution Systems for Non-binary Pronouns\n\nJennifer Mickel (The University of Texas at Austin, USA). Racial/Ethnic Categories in AI and Algorithmic Fairness: Why They Matter and What They Represent\n\nSarah Sterz (Saarland University, Germany), Kevin Baum (DFKI, Germany), Sebastian Biewer (Saarland University, Germany), Holger Hermanns (Saarland University, Germany), Anne Lauber-R√∂nsberg (TU Dresden, Germany), Philip Meinel (TU Dresden, Germany) and Markus Langer (Universit√§t Freiburg, Germany). On the Quest for Effectiveness in Human Oversight: Interdisciplinary Perspectives\n\nMy H Dinh (University of Virginia, USA), James Kotary (University of Virginia, USA) and Ferdinando Fioretto (University of Virginia, USA). Learning Fair Ranking Policies via Differentiable Optimization of Ordered Weighted Averages\n\nYoonjoo Lee (KAIST, Republic of Korea), Kihoon Son (KAIST, Republic of Korea), Tae Soo Kim (KAIST, Republic of Korea), Jisu Kim (Georgia Institute of Technology, USA), John Joon Young Chung (Midjourney, USA), Eytan Adar (University of Michigan, USA) and Juho Kim (KAIST, Republic of Korea). One vs. Many: Comprehending Accurate Information from Multiple Erroneous and Inconsistent AI Generations\n\nNon-Archival Papers\n\nJake Stone (Australian National University, Australia) and Brent Mittelstadt (Oxford University, UK). Legitimate Power, Illegitimate Automation: The problem of ignoring legitimacy in automated decision systems\n\nMaria Smith (UC Berkeley, USA)Hypercontrolled Connectivity: Navigating Love and Regulation in the Shadows of the Smart Prison\n\nEmily Black (Barnard College, USA), Logan Koepke (Upturn, USA), Pauline Kim (Washington University in St. Louis, USA), Solon Barocas (Microsoft Research, USA) and Mingwei Hsu (Upturn, USA). The Legal Duty to Search for Less Discriminatory Algorithms\n\nIrina Carnat (Sant'Anna School of Advanced Studies, Italy). Human, all too human: accounting for automation bias in generative Large Language Models\n\nRobert Gorwa (WZB Berlin Social Science Center, Germany) and Michael Veale (University College London, UK). Moderating Model Marketplaces: Platform Governance Puzzles for AI Intermediaries\n\nEvan Dong (Cornell University, USA), Aaron Schein (University of Chicago, USA), Yixin Wang (University of MichiganNA) and Nikhil Garg (Cornell Tech, USA). Addressing Discretization-Induced Bias in Demographic Prediction\n\nGabriel Medina-Kim (Rensselaer Polytechnic Institute, United States of America). AI Ethics Coheres Because of Anti-Blackness"
    }
}
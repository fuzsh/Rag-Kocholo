{
    "id": "dbpedia_5486_3",
    "rank": 32,
    "data": {
        "url": "https://www-hpc.cea.fr/tgcc-public/en/html/toc/fulldoc/Data_spaces.html",
        "read_more_link": "",
        "language": "en",
        "title": "Data spaces — TGCC public documentation",
        "top_image": "",
        "meta_img": "",
        "images": [
            "https://www-hpc.cea.fr/tgcc-public/en/html/_static/tgcc-logo.png",
            "https://www-hpc.cea.fr/tgcc-public/en/html/_images/Schema_shspace.png",
            "https://www-hpc.cea.fr/tgcc-public/en/html/_images/schema_quota_shspace.png"
        ],
        "movies": [],
        "keywords": [],
        "meta_keywords": [
            ""
        ],
        "tags": null,
        "authors": [],
        "publish_date": null,
        "summary": "",
        "meta_description": "",
        "meta_lang": "en",
        "meta_favicon": "",
        "meta_site_name": "",
        "canonical_link": null,
        "text": "Available file systemsï\n\nThis section introduces the available file systems with their purpose, their quota policy and their recommended usage.\n\nFile System Personal Shared Home $CCCHOME $OWN_HOME $CCFRHOME $OWN_CCFRHOME $ALL_CCCHOME $ALL_CCFRHOME $<SHSPACE>_ALL_CCCHOME $<SHSPACE>_ALL_CCFRHOME $<SHSPACE>_ALL_HOME $OWN_ALL_CCCHOME $OWN_ALL_CCFRHOME $OWN_ALL_HOME Scratch $CCCSCRATCHDIR $OWN_CCCSCRATCHDIR $CCFRSCRATCH $OWN_CCFRSCRATCH $ALL_CCCSCRATCHDIR $ALL_CCFRSCRATCH $<SHSPACE>_ALL_CCCSCRATCHDIR $<SHSPACE>_ALL_CCFRSCRATCH $OWN_ALL_CCCSCRATCHDIR $OWN_ALL_CCFRSCRATCH Work $CCCWORKDIR $OWN_CCCWORKDIR $CCFRWORK $OWN_CCFRWORK $ALL_CCCWORKDIR $ALL_CCFRWORK $<SHSPACE>_ALL_CCCWORKDIR $<SHSPACE>_ALL_CCFRWORK $OWN_ALL_CCCWORKDIR $OWN_ALL_CCFRWORK Store $CCCSTOREDIR $OWN_CCCSTOREDIR $CCFRSTORE $OWN_CCFRSTORE $ALL_CCCSTOREDIR $ALL_CCFRSTORE $<SHSPACE>_ALL_CCCSTOREDIR $<SHSPACE>_ALL_CCFRSTORE $OWN_ALL_CCCSTOREDIR $OWN_ALL_CCFRSTORE\n\nHOME\n\nThe HOME is a slow, small file system with backup that can be used from any machine.\n\nCharacteristics:\n\nType: NFS\n\nData transfer rate: low\n\nQuota: 5GB/user\n\nUsage: Sources, job submission scriptsâ¦\n\nComments: Data are saved\n\nAccess: from all resources of the center\n\nPersonnal variables: $CCCHOME $OWN_HOME $CCFRHOME $OWN_CCFRHOME\n\nShared variables: $ALL_CCCHOME $ALL_CCFRHOME $<SHSPACE>_ALL_CCCHOME $<SHSPACE>_ALL_CCFRHOME $<SHSPACE>_ALL_HOME $OWN_ALL_CCCHOME $OWN_ALL_CCFRHOME $OWN_ALL_HOME\n\nNote\n\nHOME is the only file system without limitation on the number of files (quota on inodes)\n\nThe retention time for HOME directories backup is 6 months\n\nThe backup files are under the ~/.snapshot directory\n\nSCRATCH\n\nThe SCRATCH is a very fast, big and automatically purged file system.\n\nCharacteristics:\n\nType: Lustre\n\nData transfer rate: 300 GB/s\n\nQuota: The quota is defined by group. The command ccc_quota provides information about the quota of the groups you belong to. By default, a quota of 2 millions inodes and 100 To of disk space is granted for each data space.\n\nUsage: Data, Code outputâ¦\n\nComments: This filesystem is subject to purge\n\nAccess: Local to the supercomputer\n\nPersonnal variables: $CCCSCRATCHDIR $OWN_CCCSCRATCHDIR $CCFRSCRATCHDIR $OWN_CCFRSCRATCHDIR\n\nShared variables: $ALL_CCCSCRATCHDIR $ALL_CCFRSCRATCHDIR $<SHSPACE>_ALL_CCCSCRATCHDIR $<SHSPACE>_ALL_CCFRSCRATCH $OWN_ALL_CCCSCRATCHDIR $OWN_ALL_CCFRSCRATCH\n\nThe purge policy is as follows:\n\nFiles not accessed for 60 days are automatically purged\n\nSymbolic links are not purged\n\nDirectories that have been empty for more than 30 days are removed\n\nNote\n\nYou may use the ccc_will_purge command to display the files to be purged soon. Please read the dedicated section for more information\n\nWORK\n\nWORK is a fast, medium and permanent file system (but without backup):\n\nCharacteristics:\n\nType: Lustre via routers\n\nData transfer rate: high (70 GB/s)\n\nQuota: 5 TB and 500 000 files/group\n\nUsage: Commonly used file (Source code, Binaryâ¦)\n\nComments: Neither purged nor saved (tar your important data to STORE)\n\nAccess: from all resources of the center\n\nPersonnal variables: $CCCWORKDIR $OWN_CCCWORKDIR $CCFRWORK $OWN_CCFRWORK\n\nShared variables: $ALL_CCCWORKDIR $ALL_CCFRWORK $<SHSPACE>_ALL_CCCWORKDIR $<SHSPACE>_ALL_CCFRWORK $OWN_ALL_CCCWORKDIR $OWN_ALL_CCFRWORK\n\nNote\n\nWORK is smaller than SCRATCH, itâs only managed through quota.\n\nThis space is not purged but not saved (regularly backup your important data as tar files in STORE)\n\nSTORE\n\nSTORE is a huge storage file system\n\nCharacteristics:\n\nType: Lustre + HSM\n\nData transfer rate: high (200 GB/s)\n\nQuota: 50 000 files/group, expected file size range 10GB-1TB\n\nUsage: To store of large files (direct computation allowed in that case) or packed data (tar filesâ¦)\n\nComments: Migration to hsm relies on file modification time: avoid using cp options like -p, -a â¦\n\nAccess: from all resources of the center\n\nPersonnal variables: $CCCSTOREDIR $OWN_CCCSTOREDIR $CCFRSTORE $OWN_CCFRSTORE\n\nShared variables: $ALL_CCCSTOREDIR $ALL_CCFRSTORE $<SHSPACE>_ALL_CCCSTOREDIR $<SHSPACE>_ALL_CCFRSTORE $OWN_ALL_CCCSTOREDIR $OWN_ALL_CCFRSTORE\n\nAdditional info: Use ccc_hsm status <file> to know whether a file is on disk or tape level, and ccc_hsm get <file> to preload file from tape before a computation\n\nNote\n\nSTORE has no limit on the disk usage (quota on space)\n\nSTORE usage is monitored by a scoring system\n\nAn HSM is a data storage system which automatically moves data between high-speed and low-speed storage media. In our case, the high speed device is a Lustre file system and the low speed device consists of magnetic tape drives. Data copied to the HSM filesystem will be moved to magnetic tapes (usually depending on the modification date of the files). Once the data is stored on tape, accessing it will be slower.\n\nTo fetch and store big data volumes on the Store, you may use the ccc_pack command. Please see the dedicated subsection for more information.\n\nTMP\n\nTMP is a local, fast file system but of limited size.\n\nCharacteristics:\n\nType: zram (RAM)\n\nData transfer rate: very high (>1GB/s) and very low latency\n\nSize: 16 GB\n\nUsage: Temporary files during a job\n\nComments: Purge after each job\n\nAccess: Local within each node\n\nVariable: CCCTMPDIR or CCFRTMP\n\nNote\n\nTMP allow fast write and read for local needs.\n\nTMP is local to the node. Only jobs/processes within the same node have access to the same files.\n\nWrite files in TMP will reduce the amount of available RAM.\n\nSHM\n\nSHM is a very fast, local file system but of limited size.\n\nCharacteristics:\n\nType: tmpfs (RAM, block size 4Ko)\n\nData transfer rate: very high (>1GB/s) and very low latency\n\nSize: 50% of RAM (ie: 94 GB on skylake)\n\nUsage: Temporary files during compute; can be used as a large cache\n\nComments: Purge after each job\n\nAccess: Local within each node\n\nVariable: CCCSHMDIR\n\nNote\n\nSHM allow very fast file access.\n\nSHM is local to the node. Only jobs/processes within the same node have access to the same files.\n\nWrite files in SHM will reduce the amount of available RAM.\n\nUsing the SHM folder through the CCCSHMDIR variable is strongly recommended, especially for small files.\n\nWarning\n\nCCCSHMDIR is only available during a SLURM allocation (ccc_msub). CCCSHMDIR is not available in an interactive session (ccc_mprun -s).\n\nWarning\n\nUsing CCCSHMDIR is necessary in jobs to make sure data are properly cleaned at job end. It may cause node deadlocks otherwise.\n\nData management on STOREï\n\nccc_packï\n\nccc_pack allows for a storing large volumes of data. Usually, it is used to transfer data from the Scratch to the Store. Here is an example case :\n\n$ ccc_pack --mkdir --partition=<partition> --filesystem=store,scratch --src=$CCCSCRATCHDIR/<source file path> --dst=$CCCSTOREDIR/<destination directory> -j 2 $ ccc_msub $CCCSTOREDIR/<destination directory>/pack_<ID>.msub\n\nMore options are available, please use ccc_pack --help or man ccc_pack for more information.\n\nccc_unpackï\n\nccc_unpack is the routine to reverse a pack. Usually, it is used to transfer data back from the Store to the Scratch. Here is an example case :\n\n$ ccc_unpack --src-basename=ID --src=$CCCSTOREDIR/<archive directory> --dst=$CCCSCRATCHDIR/<output dir> --partition=milan -j 2 $ ccc_msub $CCCSTOREDIR/unpack_<ID>.msub\n\nMore options are available, please use ccc_unpack --help or man ccc_unpack for more information.\n\nccc_hsmï\n\nccc_hsm is a tool designed to manage data among multiple levels of storage devices, from Lustre SDDs/HDDs to an archival system such as HPSS or Phobos. ccc_hsm can manage data placement through hints by migrating data within Lustre storage components. It can also trigger the copy of archived data back to Lustre through Lustre-HSM mecanisms. The details of its use is described in the following paragraphs.\n\nccc_hsm on the STORE\n\nDisplay status\n\n$ ccc_hsm status <file>\n\nYou can use the -T option to also display the storage tiers if the file is online, or its archive id if released.\n\nHere is an example of an output on the store\n\n$ ccc_hsm status -T test_dir/test_file test_dir/test_file online (hdd)\n\nIn this case, the file is ready to be used.\n\nccc_hsm status -T test_dir/test_file2 test_dir/test_file2 released\n\nIn this case, the file need to be fetched\n\nTo show the HSM status of all files in a directory, use the command:\n\nccc_hsm ls <dir>\n\nIn the same way, you can add the -T option to display storage tiers.\n\nHere is an example of an output:\n\nccc_hsm ls -T test_dir online (hdd) test_dir/test_file_1 online (flash) test_dir/test_file_2 released (1) test_dir/test_file_3\n\nRetrieving data\n\nTo retrieve files, you can use the get command to preload the entries into Lustre. The given path to retrieve can also be a directory, in which case additional options are required to specify the behaviour. The command is:\n\nccc_hsm get <path>\n\nMultiple options can be added:\n\n-r: for recursive get if the given path is a directory\n\n-b: to get the data asynchronously in background\n\n-d: to only retrieve the first level entries of a directory\n\n-n: to only try to preload files once\n\nIf the retrieval is blocking (i.e. not using the -b option), it will wait for the file to be online. However, if the file isnât online by the time the timeout is reached, ccc_hsm has no way of knowing why it couldnât be retrieved.\n\nAdvanced usage for the store\n\nccc_hsm allows one to set special hints on a file to indicate the usage of the file in the future. This will help optimize the system performance and efficiently choose how and when migrations and archival are done.\n\nWarning\n\nWriting rights are required to use hints on a file. You may use hints only on the STORE and SCRATCH filesystems.\n\nThis is done by using the command:\n\nccc_hsm hint <hint> <path>\n\nMultiple hints can be declared in the command line, and they have the following meanings:\n\n--wont-access: The specified data will not be accessed in the near future (incompatible with --will-access)\n\n--wont-change: The specified data will not be modified in the near future\n\n--no-archive: The specified data should not be archived in the near future\n\n--will-access: The specified data will be accessed in the near near future (incompatible with --wont-access)\n\n--clear: Remove all previous user hints before applying new ones (if provided)\n\nIf no hint is provided, it will display the hint currently attached to the file.\n\nFor instance, if you know the file will not be accessed in the near futur, you may use the following command :\n\n$ ccc_hsm hint --wont-access test_dir/test_file1 $ ccc_hsm hint test_dir/test_file1 wont-access\n\nLarge files managementï\n\nFor files larger than 10 Go on the STORE, WORK or SCRATCH, it is recommanded to âstripâ the file: Stripping a file consist in dividing it between several Object Storage Targets (OSTs).\n\nIn order to strip an existing file you may use the following commands:\n\nlfs setstripe -c <number of stripes> <new path to file> cp <previous path> <new path to file>\n\nIn this command, the -c option specifies the stripe count, which is the number of OSTs that the file will be striped across. In most cases, 2 to 4 stripes are enough to handle large files. In cas your IO patterns are well parallised, with the least amount of conflicts, you may use more stripes. Nevertheless, you may use no more than one OST per 32GB of file and no more than 16 stripes. Adding striping will increase read/write bandwidth and decrease latency fo large files but will increase the number of requests generated and thus the load on the processor.\n\nFor faster striping of the file, you may use the MPIFileUtils command dstripe as described in the MPIFileUtils subsection, in the Paralell IO section.\n\nOnce you have run this command, Lustre will automatically distribute the file across the specified number of OSTs. You can confirm that the file is striped across multiple OSTs by checking the output of the lfs getstripe command:\n\nlfs getstripe <new path to file>\n\nThis will show you the stripe count, stripe size, and OSTs that the file is striped across.\n\nYou may also define the stripping for a directory, which will affect all new files created in it:\n\nlfs setstripe -c <number of stripes><path of directory>\n\nBy default, maximum file size is 4TB, stripping allows for bigger files. If needed, you may use the command:\n\nulimit -f <file size limit>\n\nHowever its usage is not recommended. In case of need, please send an email to the hotline.\n\nIn order to have the best IO performance, you may directly directly stripe your outputs with MPI IO. More details can be found in the I/O section of the official MPI documentation."
    }
}
{
    "id": "dbpedia_5486_1",
    "rank": 41,
    "data": {
        "url": "https://www.chpc.utah.edu/resources/storage_services.php",
        "read_more_link": "",
        "language": "en",
        "title": "Storage Services at CHPC",
        "top_image": "https://templates.utah.edu/_main-v3-1/images/template/favicon.ico",
        "meta_img": "https://templates.utah.edu/_main-v3-1/images/template/favicon.ico",
        "images": [
            "https://templates.utah.edu/_main-v3-1/images/template/google-logo.png",
            "https://templates.utah.edu/_main-v3-1/images/template/google-logo.png",
            "https://templates.utah.edu/_main-v3-1/images/template/university-of-utah-logo.svg",
            "https://templates.utah.edu/_main-v3-1/images/template/blocku.svg",
            "https://templates.utah.edu/_main-v3-1/images/template/university-of-utah-logo.svg"
        ],
        "movies": [],
        "keywords": [],
        "meta_keywords": [
            ""
        ],
        "tags": null,
        "authors": [],
        "publish_date": null,
        "summary": "",
        "meta_description": "",
        "meta_lang": "en",
        "meta_favicon": "//templates.utah.edu/_main-v3-1/images/template/favicon.ico",
        "meta_site_name": "",
        "canonical_link": null,
        "text": "Home Directories\n\nBy default each user is provided with a 50 GB general environment home directory free of charge. To view the current home directory usage and quota status, run the command mydiskquota .\n\nTHIS SPACE IS NOT BACKED UP -- important data should be copied to a departmental file server or other locations.\n\nCHPC can provide a temporary increase to your home directory; please reach out via helpdesk@chpc.utah.edu and include the reason that the temporary increase is needed as well as for how long you will need the increase.\n\nHome directories can be mounted on local desktops. See the Data Transfer Services page for information on mounting CHPC file systems on local machines.\n\nQuota Enforcement Policies\n\nThe 50GB quota on this directory is enforced. There is a two level quota system in place. Once a user has exceeded the 50 GB quota they have 7 days to clean up their space such that they are using less than 50 GB. If they do not, after 7 days they will no longer be able to write any files until you clean up your usage. If your home directory grows to 75 GB, you will no longer be able to write any files until you clean up your usage to under 50 GB. When over quota, you will not be able to start a FastX or OnDemand session, in addition to not being able to edit a file, as those tasks write to you home directory.\n\nPurchases of Larger Home Directory Space\n\nCHPC also allows CHPC PIs to buy larger home directory storage at a price based on hardware cost recovery. The hardware for the new home directory solution was put into service in May 2022 and was described in the Spring 2022 newsletter. The cost of home directory storage has a prorated cost of $540/TB for the remaining warranty lifetime (the current warranty expires May 2027). Once purchased, the home directories of all members of the PI's group will be provisioned in this space.\n\nPurchase of home directory space includes the cost of the space on the VAST storage system along with backup of this space. The backup will be to the CHPC object storage, pando, and will be a weekly full backup with nightly incremental backups, with a two week retention window.\n\nIf you are interested in this option, please contact us by emailing helpdesk@chpc.utah.edu to discuss your storage needs.\n\nGroup Level Storage\n\nCHPC PIs with sponsored research projects can purchase general environment group level file storage at the TB-level. CHPC purchases the hardware for this storage in bulk and then sells it to individual groups in TB quantities, so depending on the amount of group storage space you are interested in purchasing, CHPC may have the storage to meet your needs on hand. If interested, a more detailed description of this storage offering is available.\n\nThe current pricing is $150/TB for the lifetime of the hardware without backups. The CHPC provides a back up option at $450/TB (original + 2 copies). Hardware is purchased with a 5 year warranty and we are usually able to obtain an additional 2 years of warranty after purchase. If interested in purchasing group-level storage, please contact at helpdesk@chpc.utah.edu.\n\nGroup directories can be mounted on local desktops. See the Data Transfer Services page for information on mounting CHPC file systems on local machines.\n\nFor group level storage options (project space) in the protected environment, please visit this link.\n\nCurrent backup policies can be found at File Storage Policies. The CHPC also provides information on a number of user-driven alternative to our group level storage service: see the User Driven Backup Options section below for information.\n\nScratch File Systems\n\nThe CHPC provides two scratch file systems available free of charge in the general environment clusters. Scratch space is provided for users to store intermediate files required during the duration of a job. These scratch file systems are not backed up. Files that have not been accessed for 60 days are automatically scrubbed.\n\nThe current scratch file systems are:\n\n/scratch/general/nfs1 - a 595 TB NFS system accessible from all general environment CHPC resources\n\n/scratch/general/vast - 1 PB file system available from all general environment CHPC resources -- there is a per user quota of 50 TB on this scratch file system.\n\nTemporary File Systems\n\n/tmp and /var/tmp\n\nLinux defines temporary file systems at /tmp or /var/tmp. CHPC cluster nodes set up temporary file systems as a RAM disk with limited capacity. All interactive and compute nodes also have a spinning disk local storage at /scratch/local. If a user program is known to need temporary storage, it is advantageous to define the location of the temporary storage by setting the environmental variable TMPDIR to point to /scratch/local. Local disk drives range from 40 to 500 GB depending on the node, which is much more than the default /tmp size.\n\n/scratch/local\n\n/scratch/local can also be used for storing intermediate files during calculation. However, be aware that getting to these files after the job finishes will be difficult since they are local to the (compute) node and not directly accessible from cluster interactive nodes.\n\nAccess permissions to /scratch/local have been set such that users cannot create directories in the top level /scratch/local directory. Instead, as part of the slurm job prolog (before the job is started), a job level directory, /scratch/local/$USER/$SLURM_JOB_ID , will be created. Only the job owner will have access to this directory. At the end of the job, in the slurm job epilog, this job level directory will be removed.\n\nAll slurm scripts that make use of /scratch/local must be adapted to accommodate this change. Additional updated information is provided on the CHPC Slurm page.\n\n/scratch/local is now sofware encrypted. Each time a node is rebooted, this software encryption is re-setup from scratch, purging anything within the content of this space. There is also a cron job in place to scrub /scratch/local of content that has not been accessed for over 2 weeks. This scrub policy can be adjusted on a per host basis. A node owned by a group can opt to have us disable this and it will not run on that host.\n\nArchive Storage\n\nCHPC now has a new archive storage solution based around object storage, specifically ceph, a distributed object store suite developed at UC Santa Cruz. We are offering an 6+3 erasure coding configuration, allowing for the $150/TB price for the 7 year lifetime of the hardware. In alignment with our current group space offerings, we will operate this space in a condominium-style model by reselling this space in TB chunks.\n\nOne of the key features of the archive system is that users manage the archive directly, unlike the tape archive option. Users can move data in and out of the archive storage as needed: they can archive milestone moments in their research, store an additional copy of crucial instrument data, and retrieve data as needed. This archive storage solution will be accessible via applications that use Amazonâ€™s S3 API. GUI tools such as transmit (for Mac) as well as command-line tools such as s3cmd and rclone can be used to move the data.\n\nThis space is a stand alone entity and will not be mounted on other CHPC resources.\n\nIt should also be noted that this archive storage space is for use in the general environment, and is not for use with regulated data; there is a separate archive space in the protected environment.\n\nUser Driven Backup Options\n\nCampus level options for a backup location include Box and Microsoft OneDrive. Note: There is a UIT Knowledge Base article with information on the suitability of the campus level options for different types of data (public/sensitive/restricted). Please follow these university guidelines to determine a suitable location for your data.\n\nOwner backup to University of Utah Box : This is an option suitable for sensitive/restricted data. See the link to get more information about the limitations. If using rclone, the credentials expire and have to be reset periodically.\n\nOwner backup to University of Utah Microsoft OneDrive: As with box, this option suitable for sensitive/restricted data. See the link above to get more information about the limitations.\n\nOwner backup to CHPC archive storage -- pando in the general environment and elm in the Protected Environment: This choice, mentioned in the Archive storage section above, requires that the group purchase the required space on CHPC's archive storage options.\n\nOwner backup to other storage external to CHPC: Some groups have access to other storage resources, external to CHPC, whether at the University or at other sites. The tools that can be used for doing this are dependent on the nature of the target storage.\n\nThere are a number of tools, mentioned on our Data Transfer Services page, that can be used to transfer data for backup. The tool best suited for transfers to object storage file systems is rclone. Other tools include fpsync, a parallel version of rsync suited for transfers between typical Linux \"POSIX-like\" file systems, and Globus, best suited for transfers to and from resources outside of the CHPC.\n\nIf you are considering a user driven backup option for your data, the CHPC staff is availabe for consultation at helpdesk@chpc.utah.edu.\n\nMounting CHPC Storage"
    }
}
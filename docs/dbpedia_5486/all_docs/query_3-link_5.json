{
    "id": "dbpedia_5486_3",
    "rank": 5,
    "data": {
        "url": "https://servicedesk.surf.nl/wiki/display/WIKI/Snellius%2Bfilesystems",
        "read_more_link": "",
        "language": "en",
        "title": "SURF User Knowledge Base",
        "top_image": "https://servicedesk.surf.nl/wiki/s/kvyu1s/9012/1p8e945/56/_/favicon.ico",
        "meta_img": "https://servicedesk.surf.nl/wiki/s/kvyu1s/9012/1p8e945/56/_/favicon.ico",
        "images": [
            "https://servicedesk.surf.nl/wiki/download/attachments/327682/atl.site.logo?version=1&modificationDate=1585757637415&api=v2",
            "https://servicedesk.surf.nl/wiki/download/attachments/1474607/WIKI?version=2&modificationDate=1652792293984&api=v2"
        ],
        "movies": [],
        "keywords": [],
        "meta_keywords": [
            ""
        ],
        "tags": null,
        "authors": [],
        "publish_date": null,
        "summary": "",
        "meta_description": "",
        "meta_lang": "en",
        "meta_favicon": "/wiki/s/kvyu1s/9012/1p8e945/56/_/favicon.ico",
        "meta_site_name": "",
        "canonical_link": "https://servicedesk.surf.nl/wiki/display/WIKI/Snellius+filesystems",
        "text": "Overview of the filesystems\n\nThere are several filesystems available on Snellius:\n\nThe home file system\n\nEvery user has their own home directory, which is accessible at /home/<login_name> .\n\nYour home directory has default capacity quota of 200 GiB. The default i-node quota is 1,000,000.\n\nThe 200 GiB home directory is ample space for a work environment on the system for most users. If you think that it is not sufficient to accommodate your work environment on Snellius, or you need any of the features commented below, you can request extra storage space (project space) through your institution using this approval form. (users with an EINF or NWO grant can contact the servicedesk directly). Logins are per person and per project, and each login has its own home directory. Think of your home directory as the basis for arranging the work environment for your current computational project on Snellius. Note, however, that home directories are not intended for long term storage of large data sets. SURF provides the archive facility for that. Home directories are neither suitable for fast, large scale or parallel I/O. Use scratch and/or project space (see below) for fast and voluminous job I/O.\n\nHome directory overnight backup service\n\nSURF provides a versioned incremental backup service for your home directory that is run overnight. Files that have been backed up are retained in the backup repository for three weeks after they have been deleted from the file system. We can restore files and/or directories when you accidentally remove them or overwrite them with the wrong contents, or when they are corrupted because of some storage hardware failure – provided, of course, that a version already existed and was successfully backed up. Note that no consistent backup can be created of files that are removed, being changed, truncated, or appended to, while the backup process is running. The backup process will therefore simply skip files that are opened and in use by other processes.\n\nTo have a file successfully backed up:\n\nthe file must reside on the file system when the backup runs\n\nthe file must be closed\n\nThe total length of the file's canonical pathname ( i.e.: the path name that starts from the file system mount point: '/gpfs/home<#>/<username>/...' ) must NOT exceed 4095 bytes. The file system will have no problem with that. Such files and directories will be created without any problem. However, the backup client software and its target storage cannot deal with such long pathnames, so, they will never ever be backed up!\n\nSince the backup is an asynchronous system service process, end-users will not get any direct warnings when any of the above criteria is not met. As to the first one, it is hard to imagine how a backup routine could give sensible warnings about files that are non-existent, no longer present. Pertaining to the second one, the backup logs messages about files that are skipped because they are not at rest, but with 24/7 production in the batch and some people living in different time zones working interactively when the backup happens to run, trying to get these message to the respective file owners, would be a high volume of messaging, with very limited usefulness. Fortunately, the last crititerion is violated ralely, because almost 4 KiB is actually very long, even for a pathname that contains many multi-byte character like 'ä', 'ς', or 'ﻕ'. However, if we repeatedly and persistently notice canonical pathnames that are too long for the backup to handle, we will notify the user that is responsible for creating such pathnames.\n\nBe very restrictive in allowing other logins to access your home directory\n\nTechnically, your login \"owns\" your home directory. This implies that you can change its access permissions. On a semi-public system, like Snellius, many people of completely unrelated affiliations have logins to access the system interactively, you should be very restrictive in using this capability. We understand that there are use cases in which you might want to share some data, and perhaps some executable programs, with specific other logins. Maybe you want to arrange this for logins that you yourself have personal control of, but that belong to a different project / account that also happens to be currently active on Snellius. Use access control lists (ACLs) for this purpose. More specifically:\n\nYou should never - not even by means of an ACL - give other logins write permission to your home directory (or to subdirectory thereof).\n\nYou should never give any permission to any unqualified 'other' to the root of your home directory. Use ACLs to enable specific groups and or users to read, and / or search and execute.\n\nSince you are the directory owner, Snellius system administrators have no technical means by which they proactively can enforce these rules. That is: they cannot possibly prevent you from enabling permissions that are highly deficient from a data integrity point of view. Remember, also according to the usage agreement that you signed for your login, you are accountable for proper usage of the resources that you are handed to you. This is one of those aspects that are your own responsibility. System administrators can however revert, disable, unwanted permissions, and correct the undesirable consequences they may have had, after the fact, when they have detected them in filesystem meta-data analyses (analyses of quota, resource usage, etc.). There is a policy to take corrective actions to change, undo deficient permissions when they are detected, without prior notification of the registered home directory owner.\n\nif write permissions have been enabled on your home directory, an additional consequence, that is not simply repaired by disabling permissions, could be, that your home directory, or a sub-directory thereof, now contains some files and / or directories that are not owned by your login. In worst cases, you cannot even inspect nor remove them. Corrective action - changing ownership - will ensue without prior notice to the registered home directory owner.\n\nThe scratch file systems\n\nThe scratch file systems are intended as fast temporary storage that can be used while running a job, and can be accessed by all users with a valid account on the system.\n\nThere are several different types of scratch available on Snellius, as listed in the table above. Below, we describe them in detail, including any active quota and backup policies.\n\nThe /scratch-shared and /scratch-local spaces\n\nExpiration policy\n\nA user's default scratch space capacity quota is 8 TiB, which is counted over all data usage of scratch-local and scratch-shared of the user.\n\nQuota\n\nThe i-node quota (number of files and directories per user) is set at a soft limit of 3 million files per user, and a hard limit that is set substantially higher. Most of our users will never hit the soft limit ceiling, as there is a programmed cleanup of files that are older than 6 days (on scratch-local) or 14 days (on scratch-shared). Users that produce enormous amounts of files per job may have to clean up files and directories themselves after the job, as they could reach their quota before the automatic cleanup is invoked.\n\nIf the soft limit is reached, a grace period of 7 days starts counting down. If you clean-up within the grace period, and do not grow to reach the hard limit, you will not notice anything of the limit. If the hard limit is reached or if you fail to clean up to get a usage below the soft limit in due time, the file system refuses to create new files and directories for you.\n\nAccess\n\nShared scratch space can be accessed on all nodes from two locations:\n\n/scratch-local/\n\n/scratch-shared/\n\n/scratch-local/ specifies a unique location on each node (and so acts like it is local), whereas /scratch-shared/ denotes the same location on every node:\n\nSo you can use /scratch-local for each process in a job to get a guaranteed unique location for storing/retrieving data that does not interfere with other processes in the same job. In fact, the $TMPDIR environment variable is set to a default value of /scratch-local/<loginname> and the corresponding directory is already created, or is created when you log in, or a batch job is started.\n\nThe /scratch-shared/ directory behaves like scratch space that is shared by all nodes. Please create your own subdirectory under /scratch-shared , e.g. with the command:\n\nThe /scratch-local file system is not truly local\n\nNote that the /scratch-local/ directories are not truly (physically) local to a node. All /scratch-local/ directories are in fact visible from all nodes (and by all users), if you know the canonicalized fully qualified directory names. This can be seen with:\n\nIn fact, all scratch-local and scratch-shared symbolic links are actually pointing to directories that store data on the same underlying GPFS file system, and they share the same single per-user quota regime, as mentioned above.\n\nThe /scratch-node space: truly node-local scratch\n\nOn a subset of nodes fast NVMe-based scratch file systems are available (all fcn nodes, some tcn and gcn nodes). Such node-local scratch spaces are faster than the shared scratch spaces, but as the name suggests, each node will have its own scratch file system that does not share data with other nodes. For certain use cases this restriction is no problem, though.\n\nTo use nodes with node-local scratch the SLURM constraint scratch-node needs to be used (e.g. #SBATCH --constraint=scratch-node). A user-specific partition will be created on each assigned node and mounted under /scratch-node. The environment variable $TMPDIR will point to the user-specific directory within /scratch-node that you should use in your job. Note that when requesting part of shared node you will also only get part of the local NVMe disk as well (either 25%, 50% or 75%, depending on the requested job resources). The $TMPDIR variable is only set to the location of node-local scratch if the constraint has been set. Otherwise the $TMPDIR variable is set to the node specific shared directory /scratch-local as described above, regardless if the node is equipped with a local disk or not.\n\nNo quota are active on /scratch-node. No backup policy is active either.\n\nNode-local system directories such as /tmp, /var/tmp\n\nThe project file system\n\nThe purpose of project space is to enable fast and bulky reading and/or writing of files by large and/or many jobs. A project space is not meant for long-term storage of data. No automatic backup of data on project spaces is provided. In some sense, project spaces can be seen as \"user managed scratch\". This implies that project users themselves must take care not to run into their quota limit and to backup and recover data when the project expires.\n\nPractically speaking a project file system can be used when:\n\nyou need additional storage space, but do not require a backup.\n\nyou need to share files within a collaboration.\n\nBy default accounts on our systems are not provisioned with a project space. It can be requested when you apply for an account, or by contacting our service desk (depending on the type of account different conditions may apply, contact us to know if your account is eligible for a project space).\n\nThe project space itself has an agreed upon end date. But there is no expiration policy for the age of individual files and directories in your project space. Project users themselves must take care not to run into their quota limits, deleting and/or compacting and archiving data no longer needed.\n\nWhen the agreed upon period of time of your Snellius project space expires, the project space will be made inaccessible. If no further notice from the project space users is received, the files and directories in your project space will eventually be deleted after a grace period of an additional four weeks.\n\nAll members of the group used for quota administration will receive a notification on their e-mail address registered in the SURF user administration, 30 days in advance of the expiration date. A second notification mail will be sent out the day after expiration.\n\nIn principle the lifetime of a project directory is not extended beyond the lifetime of an associated compute project, as project spaces for projects that cannot be active are wasting high-performance storage resources. In some cases, however, a follow-up project could make efficient use of the same data without first having to stage them from an archive into a new project space. This may be a valid reason for retaining a Snellius project space \"in between projects\". Demonstrating, before the grace period has ended, that the project proposal for a follow-up project and destined \"heir\" of the project space has actually been submitted, is mandatory. New limits and expiration dates will have to be established and motivated by the needs of the follow-up project.\n\nQuota on size and number of files\n\nThe exact capacity is project dependent. The quota of maximum number of files is derived from the capacity quota: each project is allocated basic quota of 1 million files and on top of that a surplus that is a non-linear function of the capacity quota.\n\nwhere X is the capacity of the project space in TiB and Y is the maximum number of files allowed on that project space.\n\nFor convenience, the table below contains some reference values for resulting number of files quota, and the resulting average file size. Note that for large project spaces the average file size must be larger than for smaller projects.\n\nFor users involved in more than one data project it is theoretically possible to store data in multiple project directories using any quota group that they are member of quasi-randomly. This is unwanted behaviour: files and directories with a group ownership used for the quota administration of a particular data project must all be placed under their respective project root directory. Conversely, only subdirectories and files located belonging to the project should be placed under that directory. SURF will enforce these rules, if needed, with periodic corrective actions that change group ownership without further notice.\n\nThe archive file system\n\nThe archive file system Service is intended for long term storage of large amounts of data. Most of this data is (eventually) stored on tape, and therefore accessing it may take a while. For users of the data archive it is accessible from login and staging nodes at the path /archive/<username>.\n\nThe archive system is designed to only handle large files efficiently. If you want to archive many smaller files, please compress them first in a single tar file, before copying it to the archive.Never store a large amount of small files on the archive: they may be scattered across different tapes and it will put a large load on the archive to retrieve all those files if you need them at a later stage. Seethis section of the documentation for more information on using the archive appropriately.\n\nother file systems at SURF - grid storage / dCache\n\nSURF grid storage / dCache is a large scalable remote storage system for processing huge volumes of data. It uses the dCache system for storing and retrieving huge amounts of data, distributed among a large number of server nodes. It consists of magnetic tape storage and disk storage and both are addressed by a common file system. There are several protocols and storage clients to interact with grid storage / dCache including Advanced dCache API (ADA).\n\nYou may use the grid storage if your data does not fit within the storage allocation on project space or if your application is I/O intensive.\n\nFor questions and requests about grid storage / dCache, please contact SURF servicedesk and consult SURF advisors.\n\nDisk quota"
    }
}
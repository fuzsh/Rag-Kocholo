{
    "id": "dbpedia_5486_3",
    "rank": 0,
    "data": {
        "url": "https://www.docs.arc.vt.edu/resources/storage.html",
        "read_more_link": "",
        "language": "en",
        "title": "Storage Resources — ARC User Documentation 1.0 documentation",
        "top_image": "",
        "meta_img": "",
        "images": [],
        "movies": [],
        "keywords": [],
        "meta_keywords": [
            ""
        ],
        "tags": null,
        "authors": [],
        "publish_date": null,
        "summary": "",
        "meta_description": "",
        "meta_lang": "en",
        "meta_favicon": "",
        "meta_site_name": "",
        "canonical_link": null,
        "text": "Restoring filesï\n\nIn some situations, deleted files and directories may be restored from âsnapshotsâ. Snapshots are an efficient way to keep several instances of the status of a file system at regular points in time.\n\nFor the /globalscratch file system, these are kept in the âhidden directoryâ /globalscratch/.snapshots which contains a set of snapshots named according to the type (daily, weekly, or monthly) and the date-time when they were recorded. For example:\n\n/globalscratch/.snapshot/week_2023-11-13_12_00_00_UTC\n\nis an instance of a weekly snapshot which was recorded on 2023-11-13 at 12:00:00PM UTC.\n\nSnapshots may be recorded in daily, weekly, and monthly cycles, but ARC reserves the right to adjust the frequencies and quantities of snapshots which are retained. Changes in the frequencies and quantities may occasionally be needed to adjust how much of the storage system capacity is dedicated to snapshot retentions.\n\nNote\n\nWhile snapshots provide some level of protection against data loss, they should not be viewed as a âbackupâ or as part of a data retention plan.\n\nMemory as storageï\n\nRunning jobs have access to an in-memory mount on compute nodes via the $TMPFS environment variable. This should provide very fast read/write speeds for jobs doing I/O to files that fit in memory (see the system documentation for the amount of memory per node on each system). Please note that these files are removed at the end of a job, so any results or files to be kept after the job ends must be copied to Work or Home.\n\nNVMe Drivesï\n\nSame idea as Local Scratch, but on NVMe media which âhas been designed to capitalize on the low latency and internal parallelism of solid-state storage devices.â Running jobs are given a workspace on the local NVMe drive on each compute node if it is so equipped. The path to this space is specified in the $TMPNVME environment variable. This provides another option for users who would prefer to do I/O to local disk (such as for some kinds of big data tasks). Please note that any files in local scratch are automatically removed at the end of a job, so any results or files to be kept after the job ends must be copied to Work or Home.\n\nNVMe local scratch storage is available on nodes in the following nodes and capacities:\n\nCascades\n\nlargemem_q nodes, 1.8TB\n\nk80_q nodes, 1.8TB\n\nTinkercliffs\n\na100_normal_q nodes, 11.7TB\n\nintel_q nodes, 3.2TB"
    }
}
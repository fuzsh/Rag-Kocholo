{
    "id": "dbpedia_5486_3",
    "rank": 19,
    "data": {
        "url": "https://dune.github.io/computing-training-202105/02-storage-spaces/index.html",
        "read_more_link": "",
        "language": "en",
        "title": "Storage Spaces – DUNE Computing Training May 2021 edition",
        "top_image": "https://dune.github.io/computing-training-202105/assets/favicons/incubator/favicon-196x196.png",
        "meta_img": "https://dune.github.io/computing-training-202105/assets/favicons/incubator/favicon-196x196.png",
        "images": [],
        "movies": [
            "https://www.youtube.com/embed/0FMtb3jy7H8"
        ],
        "keywords": [],
        "meta_keywords": [
            ""
        ],
        "tags": null,
        "authors": [],
        "publish_date": null,
        "summary": "",
        "meta_description": "",
        "meta_lang": "en",
        "meta_favicon": "https://dune.github.io/computing-training-202105/assets/favicons/incubator/apple-touch-icon-57x57.png",
        "meta_site_name": "",
        "canonical_link": null,
        "text": "Overview\n\nTeaching: 45 min\n\nExercises: 0 min\n\nQuestions\n\nWhat are the types and roles of DUNE’s data volumes?\n\nWhat are the commands and tools to handle data?\n\nObjectives\n\nUnderstanding the data volumes and their properties\n\nDisplaying volume information (total size, available size, mount point, device location)\n\nDifferentiating the commands to handle data between grid accessible and interactive volumes\n\nSession Video\n\nIntroduction\n\nThere are three types of storage volumes that you will encounter at Fermilab: local hard drives, network attached storage, and large-scale, distributed storage. Each has it’s own advantages and limitations, and knowing which one to use when isn’t all straightforward or obvious. But with some amount of foresight, you can avoid some of the common pitfalls that have caught out other users.\n\nVocabulary\n\nWhat is immutable? A file is immutable means that once it is written to the volume it cannot be modified, only read, moved, or deleted. Not a good choice for code or other files you want to change.\n\nWhat is interactive or POSIX? Interactive volumens, or volume with POSIX access (Portable Operating System Interface Wikipedia) allow users to directly read, write and modify using standard commands, e.g. using bash language.\n\nWhat is meant by ‘grid accessible’? Volumes that are grid accessible require specific tool suites to handle data stored there. This will be explained in the following sections.\n\nInteractive storage volumes\n\nHome area is similar to the user’s local hard drive but network mounted\n\naccess speed to the volume very high, on top of full POSIX access\n\nthey NOT safe to store certificates and tickets\n\nnot accessible as an output location from grid worker nodes\n\nnot for code developement (size of less than 2 GB)\n\nYou need a valid Kerberos ticket in order to access files in your Home area\n\nPeriodic snapshots are taken so you can recover deleted files. /nashome/.snapshot\n\nLocally mounted volumes are local physical disks, mounted directly\n\nmounted on the machine with direct links to the /dev/ location\n\nused as temporary storage for infrastructure services (e.g. /var, /tmp,)\n\ncan be used to store certificates and tickets. These are saved there automatically with owner-read permission and other permissions disabled.\n\nusually very small and should not be used to store data files or for code development\n\nData on these volumes is not backed up.\n\nNetwork Attached Storage (NAS) element behaves similar to a locally mounted volume.\n\nfunctions similar to services such as Dropbox or OneDrive\n\nfast and stable access rate\n\nvolumes available only on a limited number of computers or servers\n\nnot available to on larger grid computing\n\n/dune/app has periodic snapshots in /dune/app/.snapshot, but /dune/data and /dune/data2 do not.\n\nGrid-accessible storage volumes\n\nAt Fermilab, an instance of dCache+Enstore is used for large-scale, distributed storage with capacity for more than 100 PB of storage and O(10000) connections. Whenever possible, these storage elements should be accessed over xrootd (see next section) as the mount points on interactive nodes are slow and unstable. Here are the different dCache volumes:\n\nPersistent dCache: the data in the file is actively available for reads at any time and will not be removed until manually deleted by user\n\nScratch dCache: large volume shared across all experiments. When a new file is written to scratch space, old files are removed in order to make room for the newer file.\n\nResilient dCache: handles custom user code for their grid jobs, often in the form of a tarball. Inappropriate to store any other files here.\n\nTape-backed dCache: disk based storage areas that have their contents mirrored to permanent storage on Enstore tape.\n\nFiles are not available for immediate read on disk, but needs to be ‘staged’ from tape first.\n\nSummary on storage spaces\n\nFull documentation: Understanding Storage Volumes\n\nIn the following table, <exp> stands for the experiment (uboone, nova, dune, etc…)\n\nQuota/Space Retention Policy Tape Backed? Retention Lifetime on disk Use for Path Grid Accessible Persistent dCache No/~100 TB/exp Managed by Experiment No Until manually deleted immutable files w/ long lifetime /pnfs/<exp>/persistent Yes Scratch dCache No/no limit LRU eviction - least recently used file deleted No Varies, ~30 days (NOT guaranteed) immutable files w/ short lifetime /pnfs/<exp>/scratch Yes Resilient dCache No/no limit Periodic eviction if file not accessed No Approx 30 days (your experiment may have an active clean up policy) input tarballs with custom code for grid jobs (do NOT use for grid job outputs) /pnfs/<exp>/resilient Yes Tape backed dCache No/O(10) PB LRU eviction (from disk) Yes Approx 30 days Long-term archive /pnfs/dune/… Yes NAS Data Yes (~1 TB)/ 32+30 TB total Managed by Experiment No Till manually deleted Storing final analysis samples /dune/data No NAS App Yes (~100 GB)/ ~15 TB total Managed by Experiment No Until manually deleted Storing and compiling software /dune/app No Home Area (NFS mount) Yes (~10 GB) Centrally Managed by CCD No Until manually deleted Storing global environment scripts (All FNAL Exp) /nashome/<letter>/<uid> No\n\nThis section will teach you the main tools and commands to display storage information and access data.\n\nThe df command\n\nTo find out what types of volumes are available on a node can be achieved with the command df. The -h is for human readable format. It will list a lot of information about each volume (total size, available size, mount point, device location).\n\nExercise 1\n\nFrom the output of the df -h command, identify:\n\nthe home area\n\nthe NAS storage spaces\n\nthe different dCache volumes\n\nifdh\n\nAnother useful data handling command you will soon come across is ifdh. This stands for Intensity Frontier Data Handling. It is a tool suite that facilitates selecting the appropriate data transfer method from many possibilities while protecting shared resources from overload. You may see ifdhc, where c refers to client.\n\nHere is an example to copy a file. Refer to the Mission Setup for the setting up the DUNETPC_VERSION.\n\nResource: idfh commands\n\nExercise 2\n\nUsing the ifdh command, complete the following tasks:\n\ncreate a directory in your dCache scratch area (/pnfs/dune/scratch/users/${USER}/) called “DUNE_tutorial_May2021”\n\ncopy your ~/.bashrc file to that directory.\n\ncopy the .bashrc file from your scrtach directory DUNE_tutorial_May2021 dCache to /dev/null\n\nremove the directory DUNE_tutorial_May2021 using “ifdh rmdir /pnfs/dune/scratch/users/${USER}/DUNE_tutorial_May2021” Note, if the destination for an ifdh cp command is a directory instead of filename with full path, you have to add the “-D” option to the command line. Also, for a directory to be deleted, it must be empty.\n\nxrootd\n\nThe eXtended ROOT daemon is software framework designed for accessing data from various architectures and in a complete scalable way (in size and performance).\n\nXRootD is most suitable for read-only data access. XRootD Man pages\n\nIssue the following commands and try to understand how the first command enables completing the parameters for the second command.\n\nLet’s practice\n\nExercise 3\n\nUsing a combination of ifdh and xrootd commands discussed previously:\n\nUse ifdh locateFile to find the directory for this file PDSPProd4a_protoDUNE_sp_reco_stage1_p1GeV_35ms_sce_off_43352322_0_20210427T162252Z.root\n\nUse pnfs2xrootd to get the xrootd URI for that file.\n\nUse xrdcp to copy that file to /dev/null\n\nUsing xrdfs and the ls option, count the number of files in the same directory as PDSPProd4a_protoDUNE_sp_reco_stage1_p1GeV_35ms_sce_off_43352322_0_20210427T162252Z.root\n\nNote that redirecting the standard output of a command into the command wc -l will count the number of lines in the output text. e.g. ls -alrth ~/ | wc -l\n\nifdh commands (redmine)\n\nUnderstanding storage volumes (redmine)\n\nHow DUNE storage works: pdf"
    }
}
{
    "id": "dbpedia_2460_1",
    "rank": 28,
    "data": {
        "url": "https://pilac.law.harvard.edu/war-algorithm-accountability-report",
        "read_more_link": "",
        "language": "en",
        "title": "Algorithm Accountability — HLS PILAC",
        "top_image": "http://static1.squarespace.com/static/538c6881e4b09454bcb35f25/t/62aa2e6f8e04fe763cdf1ef7/1655320175183/pilac-05.png?format=1500w",
        "meta_img": "http://static1.squarespace.com/static/538c6881e4b09454bcb35f25/t/62aa2e6f8e04fe763cdf1ef7/1655320175183/pilac-05.png?format=1500w",
        "images": [
            "https://images.squarespace-cdn.com/content/v1/538c6881e4b09454bcb35f25/6cd2a8d7-042b-486e-88f9-0b5023005570/pilac-color-01.png?format=1500w",
            "https://images.squarespace-cdn.com/content/v1/538c6881e4b09454bcb35f25/1472650423209-1BDZ7LUSMDNIE0CDWM7O/WAA+Cover+Background+v2.png",
            "https://images.squarespace-cdn.com/content/v1/538c6881e4b09454bcb35f25/1472650453534-4DWG8G5CVZTV6DEFMG4A/WAA+Cover+Background+v2.png",
            "https://images.squarespace-cdn.com/content/v1/538c6881e4b09454bcb35f25/1472650472796-BSTNSRM8WWVC7T0UYPG3/WAA+Cover+Background+v2.png",
            "https://images.squarespace-cdn.com/content/v1/538c6881e4b09454bcb35f25/1472650496367-RNLFSFM3NM1WSB79Y39Q/WAA+Cover+Background+v2.png",
            "https://images.squarespace-cdn.com/content/v1/538c6881e4b09454bcb35f25/1472650518185-LBHFFWAERB4B636K4B3T/WAA+Cover+Background+v2.png",
            "https://images.squarespace-cdn.com/content/v1/538c6881e4b09454bcb35f25/1472650538792-O3E6DKX6CMW3AUU3UT1Y/WAA+Cover+Background+v2.png",
            "https://images.squarespace-cdn.com/content/v1/538c6881e4b09454bcb35f25/1472650569184-TESVFTXB6S9X1SDPTFB5/WAA+Cover+Background+v2.png",
            "https://images.squarespace-cdn.com/content/v1/538c6881e4b09454bcb35f25/1472650600506-UY3SLUVHKWWA0L89Y00J/WAA+Cover+Background+v2.png",
            "https://images.squarespace-cdn.com/content/v1/538c6881e4b09454bcb35f25/1421512982550-AUR7BX2ZQX90N8JRU0YS/image-asset.png"
        ],
        "movies": [],
        "keywords": [],
        "meta_keywords": [
            ""
        ],
        "tags": null,
        "authors": [],
        "publish_date": "2016-08-31T00:00:00",
        "summary": "",
        "meta_description": "",
        "meta_lang": "en",
        "meta_favicon": "https://images.squarespace-cdn.com/content/v1/538c6881e4b09454bcb35f25/b646431e-8d38-403a-a763-e9730ea7017b/favicon.ico?format=50w",
        "meta_site_name": "HLS PILAC",
        "canonical_link": "https://pilac.law.harvard.edu/war-algorithm-accountability-report",
        "text": "Across many areas of modern life, “authority is increasingly expressed algorithmically.”[1] War is no exception.\n\nIn this briefing report, we introduce a new concept—war algorithms—that elevates algorithmically-derived “choices” and “decisions” to a, and perhaps the, central concern regarding technical autonomy in war. We thereby aim to shed light on and recast the discussion regarding “autonomous weapon systems.”\n\nIn introducing this concept, our foundational technological concern is the capability of a constructed system, without further human intervention, to help make and effectuate a “decision” or “choice” of a war algorithm. Distilled, the two core ingredients are an algorithm expressed in computer code and a suitably capable constructed system. Through that lens, we link international law and related accountability architectures to relevant technologies. We sketch a three-part (non-exhaustive) approach that highlights traditional and unconventional accountability avenues. By not limiting our inquiry only to weapon systems, we take an expansive view, showing how the broad concept of war algorithms might be susceptible to regulation—and how those algorithms might already fit within the existing regulatory system established by international law.\n\n* * *\n\nWarring parties have long expressed authority and power through algorithms. For decades, algorithms have helped weapons systems—first at sea and later on land—to identify and intercept inbound missiles. Today, military systems are increasingly capable of navigating novel environments and surveilling faraway populations, as well as identifying targets, estimating harm, and launching direct attacks—all with fewer humans at the switch. Indeed, in recent years, commercial and military developments in algorithmically-derived autonomy have created diverse benefits for the armed forces in terms of “battlespace awareness,” protection, “force application,” and logistics. And those are by no means the exhaustive set of applications.\n\nMuch of the underlying technology—often developed initially in commercial or academic contexts—is susceptible to both military and non-military use. Most of it is thus characterized as “dual-use,” a shorthand for being capable of serving a wide array of functions. Costs of the technology are dropping, often precipitously. And, once the technology exists, the assumption is usually that it can be utilized by a broad range of actors.\n\nDriven in no small part by commercial interests, developers are advancing relevant technologies and technical architectures at a rapid pace. The potential for those advancements to cross a moral Rubicon is being raised more frequently in international forums and among technical communities, as well as in the popular press.\n\nSome of the most relevant advancements involve constructed systems through which huge amounts of data are quickly gathered and ensuing algorithmically-derived “choices” are effectuated. “Self-driving” or “autonomous” cars are one example. Ford, for instance, mounts four laser-based sensors on the roof of its self-driving research car, and collectively those sensors “can capture 2.5 million 3-D points per second within a 200-foot range.” Legal, ethical, political, and social commentators are casting attention on—and vetting proposed standards and frameworks to govern—the life-and-death “choices” made by autonomous cars.\n\nAmong the other relevant advancements is the potential for learning algorithms and architectures to achieve more and more human-level performance in previously-intractable artificial-intelligence (AI) domains. For instance, a computer program recently achieved a feat previously thought to be at least a decade away: defeating a human professional player in a full-sized game of Go. In March 2016, in a five-game match, AlphaGo—a computer program using an AI technique known as “deep learning,” which “allows computers to extract patterns from masses of data with little human hand-holding”—won four games against Go expert Lee Sedol. Google, Amazon, and Baidu use the same AI technique or similar ones for such tasks as facial recognition and serving advertisements on websites. Following AlphaGo’s series of wins, computer programs have now outperformed humans at chess, backgammon, “Jeopardy!”, and Go.\n\nYet even among leading scientists, uncertainty prevails as to the technological limits. That uncertainty repels a consensus on the current capabilities, to say nothing of predictions of what might be likely developments in the near- and long-term (with those horizons defined variously).\n\nThe stakes are particularly high in the context of political violence that reaches the level of “armed conflict.” That is because international law admits of far more lawful death, destruction, and disruption in war than in peace. Even for responsible parties who are committed to the rule of law, the legal regime contemplates the deployment of lethal and destructive technologies on a wide scale. The use of advanced technologies—to say nothing of the failures, malfunctioning, hacking, or spoofing of those technologies—might therefore entail far more significant consequences in relation to war than to peace. We focus here largely on international law because it is the only normative regime that purports—in key respects but with important caveats—to be both universal and uniform. In this way, international law is different from the myriad domestic legal systems, administrative rules, or industry codes that govern the development and use of technology in all other spheres.\n\nOf course, the development and use of advanced technologies in relation to war have long generated ethical, political, and legal debates. There is nothing new about the general desire and the need to discern whether the use of an emerging technological capability would comport with or violate the law. Today, however, emergent technologies sharpen—and, to a certain extent, recast—that enduring endeavor. A key reason is that those technologies are seen as presenting an inflection point at which human judgment might be “replaced” by algorithmically-derived “choices.” To unpack and understand the implications of that framing requires, among other things, technical comprehension, ethical awareness, and legal knowledge. Understandably if unfortunately, competence across those diverse domains has so far proven difficult to achieve for the vast majority of states, practitioners, and commentators.\n\nLargely, the discourse to date has revolved around a concept that so far lacks a definitional consensus: “autonomous weapon systems” (AWS). Current conceptions of AWS range enormously. On one end of the spectrum, an AWS is an automated component of an existing weapon. On the other, it is a platform that is itself capable of sensing, learning, and launching resulting attacks. Irrespective of how it is defined in a particular instance, the AWS framing narrows the discourse to weapons, excluding the myriad other functions, however benevolent, that the underlying technologies might be capable of.\n\nWhat autonomous weapons mean for legal responsibility and for broader accountability has generated one of the most heated recent debates about the law of war. A constellation of factors has shaped the discussion.\n\nPerceptions of evolving security threats, geopolitical strategy, and accompanying developments in military doctrine have led governments to prioritize the use of unmanned and increasingly autonomous systems (with “autonomous” defined variously) in order to gain and maintain a qualitative edge. By 2013, leadership in the U.S. Navy and Department of Defense (DoD) had identified autonomy in unmanned systems as a “high priority.” In March 2016, the Ministries of Foreign Affairs and Defense of the Netherlands affirmed their belief that “if the Dutch armed forces are to remain technologically advanced, autonomous weapons will have a role to play, now and in the future.” A growing number of states hold similar views.\n\nAt the same time, human-rights advocates and certain technology experts have catalyzed initiatives to promote a ban on “fully autonomous weapons” (which those advocates and experts also call “killer robots”). The primary concerns are couched in terms of delegating decisions about lethal force away from humans—thereby “dehumanizing” war—and, in the process, of making wars easier to prosecute. Following the release in 2012 of a report by Human Rights Watch and the International Human Rights Clinic at Harvard Law School, the Campaign to Stop Killer Robots was launched in April 2013 with an explicit goal of fostering a “pre-emptive ban on fully autonomous weapons.” The rationale is that such weapons will, pursuant to this view, never be capable of comporting with international humanitarian law (IHL) and are therefore per se illegal. In July 2015, thousands of prominent AI and robotics experts, as well as other scientists, endorsed an “Open Letter” on autonomous weapons, arguing that “[t]he key question for humanity today is whether to start a global AI arms race or to prevent it from starting.” Those endorsing the letter “believe that AI has great potential to benefit humanity in many ways, and that the goal of the field should be to do so.” But, they cautioned, “[s]tarting a military AI arms race is a bad idea, and should be prevented by a ban on offensive autonomous weapons beyond meaningful human control.”\n\nMeanwhile, a range of commentators has argued in favor of regulating autonomous weapon systems, primarily through existing international law rules and provisions. In general, these voices focus on grounding the discourse in terms of the capability of existing legal norms—especially those laid down in IHL—to regulate the design, development, and use, or to prohibit the use, of emergent technologies. In doing so, these commentators often emphasize that states have already developed a relatively thick set of international law rules that guide decisions about life and death in war. Even if there is no specific treaty addressing a particular weapon, they argue, IHL regulates the use of all weapons through general rules and principles governing the conduct of hostilities that apply irrespective of the weapon used. A number of these voices also aver that—for political, military, commercial, or other reasons—states are unlikely to agree on a preemptive ban on fully autonomous weapons, and therefore a better use of resources would be to focus on regulating the technologies and monitoring their use. In addition, these commentators often emphasize the modularity of the technology and raise concerns about foreclosing possible beneficial applications in the service of an (in their eyes, highly unlikely) prohibition on fully autonomous weapons.\n\nOver all, the lack of consensus on the root classification of AWS and on the scope of the resulting discussion make it difficult to generalize. But the main contours of the ensuing “debate” often cast a purportedly unitary “ban” side versus a purportedly unitary “regulate” side. As with many shorthand accounts, this formulation is overly simplistic. An assortment of thoughtful contributors does not fit neatly into either general category. And, when scrutinized, those wholesale categories—of “ban” vs. “regulate”—disclose fundamental flaws, not least because of the lack of agreement on what, exactly, is meant to be prohibited or regulated. Be that as it may, a large portion of the resulting discourse has been captured in these “ban”-vs.-“regulate” terms.\n\nUnderpinning much of this debate are arguments about decision-making in war, and who is better situated to make life-and-death decisions—humans or machines. There is also a disagreement over the benefits and costs of distancing human combatants from the battlefield and whether the possible life-saving benefits of AWS are offset by the fact that war also becomes, in certain respects, easier to conduct. There are also different understandings of and predictions about what machines are and will be capable of doing.\n\nWith the rise of expert and popular interest in AWS, states have been paying more public attention to the issue of regulating autonomy in war. But the primary venue at which they are doing so functionally limits the discussion to weapons. Since 2014, informal expert meetings on “lethal autonomous weapons systems” have been convened on an annual basis at the United Nations Office in Geneva. These meetings take place within the structure of the 1980 Convention on Prohibitions or Restrictions on the Use of Certain Conventional Weapons which may be deemed to be Excessively Injurious or to have Indiscriminate Effects (CCW). That treaty is set up as a framework convention: through it, states may adopt additional instruments that pertain to the core concerns of the baseline agreement (five such protocols have been adopted). Alongside the CCW, other arms-control treaties address specific types of weapons, including chemical weapons, biological weapons, anti-personnel landmines, cluster munitions, and others. The CCW is the only existing regime, however, that is ongoing and open-ended and is capable of being used as a framework to address additional types of weapons.\n\nThe original motivation to convene states as part of the CCW was to propel a protocol banning fully autonomous weapons. The most recent meeting (which was convened in April 2016) recommended that the Fifth Review Conference of states parties to the CCW (which is scheduled to take place in December 2016) “may decide to establish an open-ended Group of Governmental Experts (GGE)” on AWS. In the past, the establishment of a GGE has led to the adoption of a new CCW protocol (one banning permanently-blinding lasers). Whether states parties establish a GGE on AWS—and, if so, what its mandate will be—are open questions. In any event, at the most recent meetings, about two-dozen states endorsed the notion—the contours of which remain undefined so far—of “meaningful human control” over autonomous weapon systems.\n\nZooming out, we see that a pair of interlocking factors has obscured and hindered analysis of whether the relevant technologies can and should be regulated.\n\nOne factor is the sheer technical complexity at issue. Lack of knowledge of technical intricacies has hindered efforts by non-experts to grasp how the core technologies may either fit within or frustrate existing legal frameworks.\n\nThis is not a challenge particular to AWS, of course. The majority of IHL professionals are not experts in the inner workings of the numerous technologies related to armed conflict. Most IHL lawyers could not detail the technical specifications, for instance, of various armaments, combat vehicles, or intelligence, surveillance, and reconnaissance (ISR) systems. But in general that lack of technical knowledge would not necessarily impede at least a provisional analysis of the lawfulness of the use of such a system. That is because an initial IHL analysis is often an exercise in identifying the relevant rule and beginning to apply it in relation to the applicable context. Yet the widely diverse conceptions of AWS and the varied technologies accompanying those conceptions pose an as-yet-unresolved set of classification challenges. Without a threshold classification, a general legal analysis cannot proceed.\n\nThe other, related factor is that states—as well as lawyers, technologists, and other commentators—disagree in key respects on what should be addressed. The headings so far include “lethal autonomous robots,” “lethal autonomous weapons systems,” “autonomous weapons systems” more broadly, and “intelligent partnerships” more broadly still. And the possible standards mentioned include “meaningful human control” (including in the “wider loop” of targeting operations), “meaningful state control,” and “appropriate levels of human judgment.” More basically, there is no consensus on whether to include only weapons or, additionally, systems capable of involvement in other armed conflict-related functions, such as transporting and guarding detainees, providing medical care, and facilitating humanitarian assistance.\n\nAgainst this backdrop, the AWS framing has largely precluded meaningful analysis of whether it (whatever “it” entails) can be regulated, let alone whether and how it should be regulated. In this briefing report, we recast the discussion by introducing the concept of “war algorithms.” We define “war algorithm” as any algorithm that is expressed in computer code, that is effectuated through a constructed system, and that is capable of operating in relation to armed conflict. Those algorithms seem to be a—and perhaps the—key ingredient of what most people and states discuss when they address AWS. We expand the purview beyond weapons alone (important as those are) because the technological capabilities are rarely, if ever, limited to use only as weapons and because other war functions involving algorithmically-derived autonomy should be considered for regulation as well. Moreover, given the modular nature of much of the technology, a focus on weapons alone might thwart attempts at regulation.\n\nAlgorithms are a conceptual and technical building block of many systems. Those systems include self-learning architectures that today present some of the sharpest questions about “replacing” human judgment with algorithmically-derived “choices.” Moreover, algorithms form a foundation of most of the systems and platforms—and even the “systems of systems”—often discussed in relation to AWS. Absent an unforeseen development, algorithms are likely to remain a pillar of the technical architectures.\n\nThe constructed systems through which these algorithms are effectuated differ enormously. So do the nature, forms, and tiers of human control and governance over them. Existing constructed systems include, among many others, stationary turrets, missile systems, and manned or unmanned aerial, terrestrial, or marine vehicles.\n\nAll of the underlying algorithms are developed by programmers and are expressed in computer code. But some of these algorithms—especially those capable of “self-learning” and whose “choices” might be difficult for humans to anticipate or unpack—seem to challenge fundamental and interrelated concepts that underpin international law pertaining to armed conflict and related accountability frameworks. Those concepts include attribution, control, foreseeability, and reconstructability.\n\nAt their core, the design, development, and use of war algorithms raise profound questions. Most fundamentally, those inquiries concern who, or what, should decide—and what it means to decide—matters of life and death in relation to war. But war algorithms also bring to the fore an array of more quotidian, though also important, questions about the benefits and costs of human judgment and “replacing” it with algorithmically-derived systems, including in such areas as logistics.\n\nWe ground our analysis by focusing on war-algorithm accountability. In short, we are primarily interested in the “duty to account … for the exercise of power” over—in other words, holding someone or some entity answerable for—the design, development, or use (or a combination thereof) of a war algorithm. That power may be exercised by a diverse assortment of actors. Some are obvious, especially states and their armed forces. But myriad other individuals and entities may exercise power over war algorithms, too. Consider the broad classes of “developers” and “operators,” both within and outside of government, of such algorithms and their related systems. Also think of lawyers, industry bodies, political authorities, members of organized armed groups—and many, many others. Focusing on war algorithms encompasses them all.\n\nWe draw on the extensive—and rapidly growing—amount of scholarship and other analytical analyses that have addressed related topics. To help illuminate the discussion, we outline what technologies and weapon systems already exist, what fields of international law might be relevant, and what regulatory avenues might be available. As noted above, because international law is the touchstone normative framework for accountability in relation to war, we focus on public international law sources and methodologies. But as we show, other norms and forms of governance might also merit attention.\n\nAccountability is a broad term of art. We adapt—from the work of an International Law Association Committee in a different context (the accountability of international organizations)—a three-part accountability approach. Our framework outlines three axes on which to focus initially on war algorithms.\n\nThe first axis is state responsibility. It concerns state responsibility arising out of acts or omissions involving a war algorithm where those acts or omissions constitute a breach of a rule of international law. State responsibility entails discerning the content of the rule, identifying a breach of the rule, assigning attribution for that breach to a state, determining available excuses (if any), and imposing measures of remedy.\n\nThe second axis is a form of individual responsibility under international law. In particular, it concerns individual responsibility under international law for international crimes—such as war crimes—involving war algorithms. This form of individual responsibility entails establishing the commission of a crime under the relevant jurisdiction, assessing the existence of a justification or excuse (if any), and, upon conviction, imposing a sentence.\n\nThe third and final axis is scrutiny governance. Embracing a wider notion of accountability, it concerns the extent to which a person or entity is and should be subject to, or should exercise, forms of internal or external scrutiny, monitoring, or regulation (or a combination thereof) concerning the design, development, or use of a war algorithm. Scrutiny governance does not hinge on—but might implicate—potential and subsequent liability or responsibility (or both). Forms of scrutiny governance include independent monitoring, norm (such as legal) development, adopting non-binding resolutions and codes of conduct, normative design of technical architectures, and community self-regulation.\n\nFollowing an introduction that highlights the stakes, we proceed with a section outlining pertinent considerations regarding algorithms and constructed systems. We highlight recent advancements in artificial intelligence related to learning algorithms and architectures. We also examine state approaches to technical autonomy in war, focusing on five such approaches—those of Switzerland, the Netherlands, France, the United States, and the United Kingdom. Finally, to ground the often-theoretical debate pertaining to autonomous weapon systems, we describe existing weapon systems that have been characterized by various commentators as AWS.\n\nThe next section outlines the main fields of international law that war algorithms might implicate. There is no single branch of international law dedicated solely to war algorithms. So we canvass how those algorithms might fit within or otherwise implicate various fields of international law. We ground the discussion by outlining the main ingredients of state responsibility. To help illustrate states’ positions concerning AWS, we examine whether an emerging norm of customary international law specific to AWS may be discerned. We find that one cannot (at least not yet). So we next highlight how the design, development, or use (or a combination thereof) of a war algorithm might implicate more general principles and rules found in various fields of international law. Those fields include the jus ad bellum, IHL, international human rights law, international criminal law (ICL), and space law. Because states and commentators have largely focused on AWS to date, much of our discussion here relates to the AWS framing.\n\nThe subsequent section elaborates a (non-exhaustive) war-algorithm accountability approach. That approach focuses on state responsibility for an internationally wrongful act, on individual responsibility under international law for international crimes, and on wider forms of scrutiny, monitoring, and regulation. We highlight existing accountability actors and architectures under international law that might regulate war algorithms. These include war reparations as well as international and domestic tribunals. We then turn to less conventional accountability avenues, such as those rooted in normative design of technical architectures (including maximizing the auditability of algorithms) and community self-regulation.\n\nIn the conclusion, we return to the deficiencies of current discussions of AWS and emphasize the importance of addressing the wide and serious concerns raised by AWS with technical proficiency, legal expertise, and non-ideological commitment to a genuine and inclusive inquiry. On the horizon, we see that two contradictory trends may be combining into a new global climate that is at once enterprising and anxious. Militaries see myriad technological triumphs that will transform warfighting. Yet the possibility of “replacing” human judgment with algorithmically-derived “decisions”—especially in war—threatens what many consider to define us as humans.\n\nTo date, the lack of demonstrated technical knowledge by many states and commentators, the unwillingness of states to share closely-held national-security technologies, and an absence of a definitional consensus on what is meant by autonomous weapon systems have impeded regulatory efforts on AWS. Moreover, uncertainty about which actors would benefit most from advances in AWS and for how long such benefits would yield a meaningful qualitative edge over others seems likely to continue to inhibit efforts at negotiating binding international rules on the development and deployment of AWS. In this sense, efforts at reaching a dedicated international regime to address AWS may follow the same frustrations as analogous efforts to address cyber warfare. True, unlike with the early days of cyber warfare, there has been greater state engagement on regulation of AWS. In particular, the concept of “meaningful human control” over AWS has already been endorsed by over two-dozen states. But much remains up in the air as states decide whether to establish a Group of Governmental Experts on AWS at the upcoming Fifth Review Conference of the CCW.\n\nThe current crux, as we see it, is whether advances in technology—especially those capable of “self-learning” and of operating in relation to war and whose “choices” may be difficult for humans to anticipate or unpack or whose “decisions” are seen as “replacing” human judgment—are susceptible to regulation and, if so, whether and how they should be regulated. One way to think about the core concern which vaults over at least some of the impediments to the discussion on AWS is the new concept we raise: war algorithms. War algorithms include not only those algorithms capable of being used in weapons but also in any other function related to war.\n\nMore war algorithms are on the horizon. Two months ago, the Defense Science Board, which is connected with the U.S. Department of Defense, identified five “stretch problems”—that is, goals that are “hard-but-not-too-hard” and that have a purpose of accelerating the process of bringing a new algorithmically-derived capability into widespread application:\n\nGenerating “future loop options” (that is, “using interpretation of massive data including social media and rapidly generated strategic options”);\n\nEnabling autonomous swarms (that is, “deny[ing] the enemy’s ability to disrupt through quantity by launching overwhelming numbers of low‐cost assets that cooperate to defeat the threat”);\n\nIntrusion detection on the Internet of Things (that is, “defeat[ing] adversary intrusions in the vast network of commercial sensors and devices by autonomously discovering subtle indicators of compromise hidden within a flood of ordinary traffic”);\n\nBuilding autonomous cyber-resilient military vehicle systems (that is, “trust[ing] that … platforms are resilient to cyber‐attack through autonomous system integrity validation and recovery”); and\n\nPlanning autonomous air operations (that is, “operat[ing] inside adversary timelines by continuously planning and replanning tactical operations using autonomous ISR analysis, interpretation, option generation, and resource allocation”).\n\nWhat this trajectory toward greater algorithmic autonomy in war—at least among more technologically-sophisticated armed forces and even some non-state armed groups—means for accountability purposes seems likely to stay a contested issue for the foreseeable future.\n\nIn the meantime, it remains to be authoritatively determined whether war algorithms will be capable of making the evaluative decisions and value judgments that are incorporated into IHL. It is currently not clear, for instance, whether war algorithms will be capable of formulating and implementing the following IHL-based evaluative decisions and value judgments:\n\nThe presumption of civilian status in case of “doubt”;\n\nThe assessment of “excessiveness” of expected incidental harm in relation to anticipated military advantage;\n\nThe betrayal of “confidence” in IHL in relation to the prohibition of perfidy; and\n\nThe prohibition of destruction of civilian property except where “imperatively” demanded by the necessities of war.\n\n* * *\n\nTwo factors may suggest that, at least for now, the most immediate ways to regulate war algorithms specifically and to pursue accountability over them might be to follow not only traditional paths but also less conventional ones. As illustrated above, the latter might include relatively formal avenues—such as states making, applying, and enforcing war-algorithm rules of conduct within and beyond their territories—or less formal avenues—such as coding law into technical architectures and community self-regulation. First, even where the formal law may seem sufficient, concerns about practical enforcement abound. Second, the proliferation of increasingly advanced technical systems based on self-learning and distributed control raises the question of whether the model of individual responsibility found in ICL might pose conceptual challenges to regulating AWS and war algorithms.\n\nIn short, individual responsibility for international crimes under international law remains one of the vital accountability avenues in existence today, as do measures of remedy for state responsibility. Yet in practice responsibility along either avenue is unfortunately relatively rare. And thus neither path, on its own or in combination, seems to be sufficient to effectively address the myriad regulatory concerns pertaining to war algorithms—at least not until we better understand what is at issue. These concerns might lead those seeking to strengthen accountability of war algorithms to pursue not only traditional, formal avenues but also less formal, softer mechanisms.\n\nIn that connection, it seems likely that attempts to change governments’ approaches to technical autonomy in war through social pressure (at least for those governments that might be responsive to that pressure) will continue to be a vital avenue along which to pursue accountability. But here, too, there are concerns. Numerous initiatives already exist. Some of them are very well informed; others less so. Many of them are motivated by ideological, commercial, or other interests that—depending on one’s viewpoint—might strengthen or thwart accountability efforts. And given the paucity of formal regulatory regimes, some of these initiatives may end up having considerable impact, despite their shortcomings.\n\nStepping back, we see that technologies of war, as with technologies in so many areas, produce an uneasy blend of promise and threat. With respect to war algorithms, understanding these conflicting pulls requires attention to a century-and-a-half-long history during which war came to be one of the most highly regulated areas of international law. But it also requires technical know-how. Thus those seeking accountability for war algorithms would do well not to forget the essentially political work of IHL’s designers—nor to obscure the fact that today’s technology is, at its core, designed, developed, and deployed by humans. Ultimately, war-algorithm accountability seems unrealizable without sufficient competence in technical architectures and in legal frameworks, coupled with ethical, political, and economic awareness.\n\nFinally, we also include a Bibliography and Appendices. The Bibliography contains over 400 analytical sources, in various languages, pertaining to technical autonomy in war. The Appendices contain detailed charts listing and categorizing states’ statements at the 2015 and 2016 Informal Meetings of Experts on Lethal Autonomous Weapons Systems convened within the framework of the CCW.\n\n[1]. Frank Pasquale, The Black Box Society: The Secret Algorithms That Control Money and Society 8 (2015), citing Clay Shirky, A Speculative Post on the Idea of Algorithmic Authority, Clay Shirky (November 15, 2009, 4:06 PM), http://www.shirky.com/weblog/2009/11/a-speculative-post-on-the-idea-of-algorithmic-authority (referencing Shirky’s definition of “algorithmic authority” as “the decision to regard as authoritative an unmanaged process of extracting value from diverse, untrustworthy sources, without any human standing beside the result saying ‘Trust this because you trust me.’”). All further citations for sources underlying this Executive Summary are available in the full-text version of the briefing report.\n\nAcross many areas of modern life, “authority is increasingly expressed algorithmically.”[1] War is no exception.\n\nComplex algorithms help determine a person’s creditworthiness.[2] They suggest what movies to watch. They detect healthcare fraud. And they are used to trade stocks at speeds far faster than humans are capable of. (Sometimes, algorithms contribute to market crashes[3] or form a basis for anti-trust prosecutions.[4])\n\nWarring parties express authority and power through algorithms, too. For decades, algorithms have helped weapons systems—first at sea and later on land—to identify and intercept inbound missiles.[5] Today, military systems are increasingly capable of navigating novel environments and surveilling faraway populations, as well as identifying targets, estimating harm, and launching direct attacks—all with fewer humans at the switch.[6] Indeed, in recent years, commercial and military developments in algorithmically-derived autonomy[7] have created diverse benefits for the armed forces in terms of “battlespace awareness,”[8] protection,[9] “force application,”[10] and logistics.[11] And those are by no means the exhaustive set of applications. Meanwhile, other algorithmically-derived war functions may not be far off—and, indeed, might already exist. Consider the provision of medical care to the wounded and sick hors de combat (such as certain combatants rendered incapable of fighting and who are therefore “outside of the battle”[12]) or the capture, transfer, and detention of enemy fighters.\n\nMuch of the underlying technology—often developed initially in commercial or academic contexts—is susceptible to both military and non-military use. Most of it is thus characterized as “dual-use,” a shorthand for being capable of serving a wide array of functions. Costs of the technology are dropping, often precipitously. And, once the technology exists, the assumption is usually that it can be utilized by a broad range of actors.\n\nDriven in no small part by commercial interests, developers are advancing relevant technologies and technical architectures at a rapid pace. The potential for those advancements—often in consumer-facing computer science and robotics fields—to be used to cross a moral Rubicon if unscrupulously adapted for belligerent purposes is being raised more frequently in international forums and among technical communities, as well as in the popular press.\n\nSome of the most relevant advancements involve constructed systems through which huge amounts of data are quickly gathered and ensuing algorithmically-derived “choices” are effectuated. “Self-driving” or “autonomous” cars are one example. Ford, for instance, mounts four laser-based sensors on the roof of its self-driving research car, and collectively those sensors “can capture 2.5 million 3-D points per second within a 200-foot range.”[13] Legal, ethical, political, and social commentators are casting attention on—and vetting proposed standards and frameworks to govern—the life-and-death “choices” made by autonomous cars.\n\nAmong the other relevant advancements is the potential for learning algorithms and architectures to achieve more and more human-level performance in previously-intractable artificial-intelligence (AI) domains. For instance, a computer program recently achieved a feat previously thought to be at least a decade away: defeating a human professional player in a full-sized game of Go.[14] In March 2016, in a five-game match, AlphaGo—a computer program using an AI technique known as “deep learning,” which “allows computers to extract patterns from masses of data with little human hand-holding”—won four games against Go expert Lee Sedol.[15] Google, Amazon, and Baidu use the same AI technique or similar ones for such tasks as facial recognition and serving advertisements on websites. Following AlphaGo’s series of wins, computer programs have now outperformed humans at chess, backgammon, “Jeopardy!”, and Go.[16]\n\nYet even among leading scientists, uncertainty prevails as to the technological limits. That uncertainty repels a consensus on the current capabilities, to say nothing of predictions of what might be likely developments in the near- and long-term (with those horizons defined variously).\n\nThe stakes are particularly high in the context of political violence that reaches the level of “armed conflict.” That is because international law admits of far more lawful death, destruction, and disruption in war than in peace.[17] Even for responsible parties who are committed to the rule of law, the legal regime contemplates the deployment of lethal and destructive technologies on a wide scale. The use of advanced technologies—to say nothing of the failures, malfunctioning, hacking, or spoofing of those technologies—might therefore entail far more significant consequences in relation to war than to peace.[18] We focus here largely on international law because it is the only normative regime that purports—in key respects but with important caveats—to be both universal and uniform. In this way, international law is different from the myriad domestic legal systems, administrative rules, or industry codes that govern the development and use of technology in all other spheres.\n\nOf course, the development and use of advanced technologies in relation to war have long generated ethical, political, and legal debates. There is nothing new about the general desire and the need to discern whether the use of an emerging technological capability would comport with or violate the law. Today, however, emergent technologies sharpen—and, to a certain extent, recast—that enduring endeavor. A key reason is that those technologies are seen as presenting an inflection point at which human judgment might be “replaced” by algorithmically-derived “choices.” To unpack and understand the implications of that framing requires, among other things, technical comprehension, ethical awareness, and legal knowledge. Understandably if unfortunately, competence across those diverse domains has so far proven difficult to achieve for the vast majority of states, practitioners, and commentators.\n\nLargely, the discourse to date has revolved around a concept that so far lacks a definitional consensus: “autonomous weapon systems” (AWS).[19] Current conceptions of AWS range enormously. On one end of the spectrum, an AWS is an automated component of an existing weapon. On the other, it is a platform that is itself capable of sensing, learning, and launching resulting attacks. Irrespective of how it is defined in a particular instance, the AWS framing narrows the discourse to weapons, excluding the myriad other functions, however benevolent, that the underlying technologies might be capable of.\n\nWhat autonomous weapons mean for legal responsibility and for broader accountability has generated one of the most heated recent debates about the law of war. A constellation of factors has shaped the discussion.\n\nPerceptions of evolving security threats, geopolitical strategy, and accompanying developments in military doctrine have led governments to prioritize the use of unmanned and increasingly autonomous systems (with “autonomous” defined variously) in order to gain and maintain a qualitative edge. The systems are said to present manifold military advantages—in short, a “seductive combination of distance, accuracy, and lethality.”[20] By 2013, leadership in the U.S. Navy and Department of Defense (DoD) had identified autonomy in unmanned systems as a “high priority.”[21] A few months ago, the Ministries of Foreign Affairs and Defense of the Netherlands affirmed their belief that “if the Dutch armed forces are to remain technologically advanced, autonomous weapons will have a role to play, now and in the future.”[22] A growing number of states hold similar views.\n\nAt the same time, human-rights advocates and certain technology experts have catalyzed initiatives to promote a ban on “fully autonomous weapons” (which those advocates and experts also call “killer robots”). The primary concerns are couched in terms of delegating decisions about lethal force away from humans—thereby “dehumanizing” war—and, in the process, of making wars easier to prosecute.[23] Following the release in 2012 of a report by Human Rights Watch and the International Human Rights Clinic at Harvard Law School,[24] the Campaign to Stop Killer Robots was launched in April 2013 with an explicit goal of fostering a “pre-emptive ban on fully autonomous weapons.”[25] The rationale is that such weapons will, pursuant to this view, never be capable of comporting with international humanitarian law (IHL) and are therefore per se illegal. In July 2015, thousands of prominent AI and robotics experts, as well as other scientists, endorsed an “Open Letter” on autonomous weapons, arguing that “[t]he key question for humanity today is whether to start a global AI arms race or to prevent it from starting.”[26] Those endorsing the letter “believe that AI has great potential to benefit humanity in many ways, and that the goal of the field should be to do so.” But, they cautioned, “[s]tarting a military AI arms race is a bad idea, and should be prevented by a ban on offensive autonomous weapons beyond meaningful human control.”[27]\n\nMeanwhile, a range of commentators has argued in favor of regulating AWS, primarily through existing international law rules and provisions. In general, these voices focus on grounding the discourse in terms of the capability of existing legal norms—especially those laid down in IHL—to regulate the design, development, and use, or to prohibit the use, of emergent technologies. In doing so, these commentators often emphasize that states have already developed a relatively thick set of international law rules that guide decisions about life and death in war. Even if there is no specific treaty addressing a particular weapon, they argue, IHL regulates the use of all weapons through general rules and principles governing the conduct of hostilities that apply irrespective of the weapon used. A number of these voices also aver that—for political, military, commercial, or other reasons—states are unlikely to agree on a preemptive ban on fully autonomous weapons, and therefore a better use of resources would be to focus on regulating the technologies and monitoring their use. In addition, these commentators often emphasize the modularity of the technology and raise concerns about foreclosing possible beneficial applications in the service of an (in their eyes, highly unlikely) prohibition on fully autonomous weapons.\n\nOver all, the lack of consensus on the root classification of AWS and on the scope of the resulting discussion make it difficult to generalize. But the main contours of the ensuing “debate” often cast a purportedly unitary “ban” side versus a purportedly unitary “regulate” side. As with many shorthand accounts, this formulation is overly simplistic. An assortment of thoughtful contributors does not fit neatly into either general category. And, when scrutinized, those wholesale categories—of “ban” vs. “regulate”—disclose fundamental flaws, not least because of the lack of agreement on what, exactly, is meant to be prohibited or regulated. Be that as it may, a large portion of the resulting discourse has been captured in these “ban”-vs.-“regulate” terms.\n\nUnderpinning much of this debate are arguments about decision-making in war, and who is better situated to make life-and-death decisions—humans or machines. There is also a disagreement over the benefits and costs of distancing human combatants from the battlefield and whether the possible life-saving benefits of AWS are offset by the fact that war also becomes, in certain respects, easier to conduct. There are also different understandings of and predictions about what machines are and will be capable of doing.\n\nWith the rise of expert and popular interest in AWS, states have been paying more public attention to the issue of regulating autonomy in war. But the primary venue at which they are doing so functionally limits the discussion to weapons.[28] Since 2014, informal expert meetings on “lethal autonomous weapons systems” have been convened on an annual basis at the United Nations Office in Geneva. These meetings take place within the structure of the 1980 Convention on Prohibitions or Restrictions on the Use of Certain Conventional Weapons which may be deemed to be Excessively Injurious or to have Indiscriminate Effects (CCW). That treaty is set up as a framework convention: through it, states may adopt additional instruments that pertain to the core concerns of the baseline agreement (five such protocols have been adopted). Alongside the CCW, other arms-control treaties address specific types of weapons, including chemical weapons, biological weapons, anti-personnel landmines, cluster munitions, and others. The CCW is the only existing regime, however, that is ongoing and open-ended and is capable of being used as a framework to address additional types of weapons.\n\nThe original motivation to convene states as part of the CCW was to propel a protocol banning fully autonomous weapons. The most recent meeting (which was convened in April 2016) recommended that the Fifth Review Conference of states parties to the CCW (which is scheduled to take place in December 2016) “may decide to establish an open-ended Group of Governmental Experts (GGE)” on AWS. In the past, the establishment of a GGE has led to the adoption of a new CCW protocol (one banning permanently-blinding lasers). Whether states parties establish a GGE on AWS—and, if so, what its mandate will be—are open questions. In any event, at the most recent meetings, about two-dozen states endorsed the notion—the contours of which remain undefined so far—of “meaningful human control” over autonomous weapon systems.[29]\n\nZooming out, we see that a pair of interlocking factors has obscured and hindered analysis of whether the relevant technologies can and should be regulated.\n\nOne factor is the sheer technical complexity at issue. Lack of knowledge of technical intricacies has hindered efforts by non-experts to grasp how the core technologies may either fit within or frustrate existing legal frameworks.\n\nThis is not a challenge particular to AWS, of course. The majority of IHL professionals are not experts in the inner workings of the numerous technologies related to armed conflict. Most IHL lawyers could not detail the technical specifications, for instance, of various armaments, combat vehicles, or intelligence, surveillance, and reconnaissance (ISR) systems. But in general that lack of technical knowledge would not necessarily impede at least a provisional analysis of the lawfulness of the use of such a system. That is because an initial IHL analysis is often an exercise in identifying the relevant rule and beginning to apply it in relation to the applicable context. Yet the widely diverse conceptions of AWS and the varied technologies accompanying those conceptions pose an as-yet-unresolved set of classification challenges. And without a threshold classification, a general legal analysis cannot proceed.\n\nThe other, related factor is that states—as well as lawyers, technologists, and other commentators—disagree in key respects on what should be addressed. The headings so far include “lethal autonomous robots,” “lethal autonomous weapons systems,” “autonomous weapons systems” more broadly, and “intelligent partnerships” more broadly still. And the possible standards mentioned include “meaningful human control” (including in the “wider loop” of targeting operations), “meaningful state control,” and “appropriate levels of human judgment.”[30] More basically, there is no consensus on whether to include only weapons or, additionally, systems capable of involvement in other armed conflict-related functions, such as transporting and guarding detainees, providing medical care, and facilitating humanitarian assistance.\n\nAgainst this backdrop, the AWS framing has largely precluded meaningful analysis of whether it (whatever “it” entails) can be regulated, let alone whether and how it should be regulated.[31] In this briefing report, we recast the discussion by introducing the concept of “war algorithms.”[32] We define “war algorithm” as any algorithm[33] that is expressed in computer code, that is effectuated through a constructed system, and that is capable of operating in relation to armed conflict. Those algorithms seem to be a—and perhaps the—key ingredient of what most people and states discuss when they address AWS. We expand the purview beyond weapons alone (important as those are) because the technological capabilities are rarely, if ever, limited to use only as weapons and because other war functions involving algorithmically-derived autonomy should be considered for regulation as well. Moreover, given the modular nature of much of the technology, a focus on weapons alone might thwart attempts at regulation.\n\nAlgorithms are a conceptual and technical building block of many systems. Those systems include self-learning architectures that today present some of the sharpest questions about “replacing” human judgment with algorithmically-derived “choices.” Moreover, algorithms form a foundation of most of the systems and platforms—and even the “systems of systems”—often discussed in relation to AWS. Absent an unforeseen development, algorithms are likely to remain a pillar of the technical architectures.\n\nThe constructed systems through which these algorithms are effectuated differ enormously. So do the nature, forms, and tiers of human control and governance over them. Existing constructed systems include, among many others, stationary turrets, missile systems, and manned or unmanned aerial, terrestrial, or marine vehicles.[34]\n\nAll of the underlying algorithms are developed by programmers and are expressed in computer code. But some of these algorithms—especially those capable of “self-learning” and whose “choices” might be difficult for humans to anticipate or unpack—seem to challenge fundamental and interrelated concepts that underpin international law pertaining to armed conflict and related accountability frameworks. Those concepts include attribution, control, foreseeability, and reconstructability.\n\nAt their core, the design, development, and use of war algorithms raise profound questions. Most fundamentally, those inquiries concern who, or what, should decide—and what it means to decide—matters of life and death in relation to war. But war algorithms also bring to the fore an array of more quotidian, though also important, questions about the benefits and costs of human judgment and “replacing” it with algorithmically-derived systems, including in such areas as logistics.\n\nWe ground our analysis by focusing on war-algorithm accountability. In doing so, we sketch a three-axis accountability approach for those algorithms: state responsibility for a breach of a rule of international law, individual responsibility under international law for international crimes, and a broad notion of scrutiny governance. This is not an exhaustive list of possible types of accountability. But the axes we outline offer a flavor of how accountability, in general, could be conceptualized in the context of war algorithms.\n\nIn short, we are primarily interested in the “duty to account … for the exercise of power”[35] over—in other words, holding someone or some entity answerable for—the design, development, or use (or a combination thereof) of a war algorithm.[36] That power may be exercised by a diverse assortment of actors. Some are obvious, especially states and their armed forces. But myriad other individuals and entities may exercise power over war algorithms, too. Consider the broad classes of “developers” and “operators,” both within and outside of government, of such algorithms and their related systems. Also think of lawyers, industry bodies, political authorities, members of organized armed groups—and many, many others. Focusing on war algorithms encompasses them all.\n\nObjective, Approach, and Methodology\n\nIn this briefing report, our objective is not to argue whether international law, as it currently exists, sufficiently addresses the plethora of issues raised by autonomous weapon systems. Rather, we aim to shed light on and recast the discussion in terms of a new concept: war algorithms. Through that lens, we link international law and related accountability architectures to relevant technologies. We sketch a three-part (non-exhaustive) approach that highlights traditional and unconventional accountability avenues. By not limiting our inquiry only to weapon systems, we take an expansive view, showing how the broad category of war algorithms might be susceptible to regulation (and how those algorithms might already fit within the existing regulatory system established by international law).\n\nWe draw on the extensive—and rapidly growing—amount of scholarship and other analytical analyses that have addressed related topics.[37] To help illuminate the discussion, we outline what technologies and weapon systems already exist, what fields of international law might be relevant, and what regulatory avenues might be available. As noted above, because international law is the touchstone normative framework for accountability in relation to war, we focus on public international law sources and methodologies. But as we show, other norms and forms of governance might also merit attention.\n\nAccountability is a broad term of art. We adapt—from the work of an International Law Association Committee in a different context (the accountability of international organizations)—a three-part accountability approach.[38] Our framework outlines three axes on which to focus initially on war algorithms.\n\nThe first axis is state responsibility. It concerns state responsibility arising out of acts or omissions involving a war algorithm where those acts or omissions constitute a breach of a rule of international law. State responsibility entails discerning the content of the rule, identifying a breach of the rule, assigning attribution for that breach to a state, determining available excuses (if any), and imposing measures of remedy.\n\nThe second axis is a form of individual responsibility under international law. In particular, it concerns individual responsibility under international law for international crimes—such as war crimes—involving war algorithms. This form of individual responsibility entails establishing the commission of a crime under the relevant jurisdiction, assessing the existence of a justification or excuse (if any), and, upon conviction, imposing a sentence.\n\nThe third and final axis is scrutiny governance. Embracing a wider notion of accountability, it concerns the extent to which a person or entity is and should be subject to, or should exercise, forms of internal or external scrutiny, monitoring, or regulation (or a combination thereof) concerning the design, development, or use of a war algorithm. Scrutiny governance does not hinge on—but might implicate—potential and subsequent liability or responsibility (or both). Forms of scrutiny governance include independent monitoring, norm (such as legal) development, adopting non-binding resolutions and codes of conduct, normative design of technical architectures, and community self-regulation.\n\nOutline\n\nIn Section 2, we outline pertinent considerations regarding algorithms and constructed systems. We then highlight recent advancements in artificial intelligence related to learning algorithms and architectures. We next examine state approaches to technical autonomy in war, focusing on five such approaches. Finally, to ground the often-theoretical debate pertaining to autonomous weapon systems, we describe existing weapon systems that have been characterized by various commentators as AWS.\n\nIn Section 3, we outline the main fields of international law that war algorithms might implicate. There is no single branch of international law dedicated solely to war algorithms. So we canvass how those algorithms might fit within or otherwise implicate various fields of international law. We ground the discussion by outlining the main ingredients of state responsibility: attribution, breach, excuses, and consequences. Then, to help illustrate states’ positions concerning AWS, we examine whether an emerging norm of customary international law specific to AWS may be discerned. We find that one cannot (at least not yet). So we next highlight how the design, development, or use (or a combination thereof) of a war algorithm might implicate more general principles and rules found in various fields of international law. Those fields include the jus ad bellum, IHL, international human rights law, international criminal law, and space law. Because states and commentators have largely focused on AWS to date, much of our discussion here relates to the AWS framing.\n\nIn Section 4, we elaborate a (non-exhaustive) war-algorithm accountability approach. That approach focuses on state responsibility for an internationally wrongful act, on individual responsibility under international law for international crimes, and on wider forms of scrutiny, monitoring, and regulation. We highlight existing accountability actors and architectures under international law that might regulate war algorithms. These include war reparations as well as international and domestic tribunals. We then turn to less conventional accountability avenues, such as those rooted in normative design of technical architectures (including maximizing the auditability of algorithms) and community self-regulation.\n\nIn the Conclusion, we return to the deficiencies of current discussions of AWS and emphasize the importance of addressing the wide and serious concerns raised by AWS with technical proficiency, legal expertise, and non-ideological commitment to a genuine and inclusive inquiry.\n\nWe also attach a Bibliography and Appendices. The Bibliography contains over 400 analytical sources, in various languages, pertaining to technical autonomy in war. The Appendices contain detailed charts listing and categorizing states’ statements at the 2015 and 2016 Informal Meetings of Experts on Lethal Autonomous Weapons Systems convened within the framework of the CCW.\n\nCaveats\n\nThe bulk of the secondary-source research was conducted in English. Moreover, none of us is an expert in computer science or robotics. We consulted specialists in these fields, but we alone are responsible for any remaining errors. In any event, given the rapid pace of development, the technologies discussed in this briefing report may soon be eclipsed—if they have not been already.\n\n[1]. Frank Pasquale, The Black Box Society: The Secret Algorithms That Control Money and Society 8 (2015), citing Clay Shirky, A Speculative Post on the Idea of Algorithmic Authority, Clay Shirky (November 15, 2009, 4:06 PM), http://www.shirky.com/weblog/2009/11/a-speculative-post-on-the-idea-of-algorithmic-authority (referencing Shirky’s definition of “algorithmic authority” as “the decision to regard as authoritative an unmanaged process of extracting value from diverse, untrustworthy sources, without any human standing beside the result saying ‘Trust this because you trust me.’”).\n\n[2]. On the examples in this paragraph, see generally Pasquale, supra note 1.\n\n[3]. See generally U.S. Commodity Futures Trading Commission & U.S. Securities & Exchange Commission, Findings Regarding the Market Events of May 6, 2010: Report of the Staffs of the CFTF and SEC to the Joint Advisory Committee on Emerging Regulatory Issues (2010), https://www.sec.gov/news/studies/2010/marketevents-report.pdf.\n\n[4]. See, e.g., Jill Prulick, When Bots Collude, New Yorker, April 25, 2015, http://www.newyorker.com/business/currency/when-bots-collude.\n\n[5]. The use of artificial intelligence and other forms of algorithmic systems in relation to war is far from new. For examples from nearly three decades ago, see Defense Applications of Artificial Intelligence (Stephen J. Andriole & Gerald W. Hopple eds., 1988).\n\n[6]. See generally, e.g., Paul J. Springer, Military Robots and Drones: A Reference Handbook (2013); see also infra Section 2: Examples of Purported Autonomous Weapon Systems.\n\n[7]. In a recent report, the Defense Science Board uses a definition of autonomy that implies the use of one or more algorithms: “To be autonomous, a system must have the capability to independently compose and select among different courses of action to accomplish goals based on its knowledge and understanding of the world, itself, and the situation.” Defense Science Board, Summer Study on Autonomy 4 (June 2016) (noting that “[d]efinitions for intelligent system, autonomy, automation, robots, and agents can be found in L.G. Shattuck, Transitioning to Autonomy: A human systems integration perspective, p. 5. Presentation at Transitioning to Autonomy: Changes in the role of humans in air transportation [March 11, 2015]. Available at http://human-factors.arc.nasa.gov/workshop/autonomy/download/presentations/Shaddock%20.pdf.”). Id. at n.1.\n\n[8]. E.g., autonomous agents to improve cyber-attack indicators and warnings; onboard autonomy for sensing; and time-critical intelligence from seized media. See Defense Science Board, supra note 7, at 46–53.\n\n[9]. E.g., dynamic spectrum management for protection missions; unmanned underwater vehicles (UUVs) to autonomously conduct sea-mine countermeasures missions; and automated cyber-response. See Defense Science Board, supra note 7, at 53–60.\n\n[10]. E.g., cascaded UUVs for offensive maritime mining, and organic tactical unmanned aircraft to support ground forces. See Defense Science Board, supra note 7, at 60–68. The term “force application” is defined in the report as “the ability to integrate the use of maneuver and engagement in all environments to create the effects necessary to achieve mission objectives.” Id. at 60.\n\n[11]. E.g., predictive logistics and adaptive planning, and adaptive logistics for rapid deployment. See Defense Science Board, supra note 7, at 69–75.\n\n[12]. Under international humanitarian law (IHL), a person is hors de combat if (i) she is in the power of an adverse party, (ii) she clearly expresses an intention to surrender, or (iii) she has been rendered unconscious or is otherwise incapable of defending herself, provided that in any of these cases she abstains from any hostile act and does not attempt to escape; shipwrecked persons cannot be excluded from the construct of hors de combat. This formulation is derived from the Protocol Additional to the Geneva Conventions of 12 August 1949, and Relating to the Protection of Victims of International Armed Conflicts art. 41(2), June 8, 1977, 1125 U.N.T.S. 3 [hereinafter AP I]; see also, e.g., Yoram Dinstein, Non-International Armed Conflicts in International Law 164 (2014).\n\n[13]. Ucilia Wang, Driverless Cars Are Data Guzzlers, Wall Street Journal, March 23, 2014, http://www.wsj.com/articles/SB10001424052702304815004579417441475998338.\n\n[14]. David Silver et al., Mastering the Game of Go with Deep Neural Networks and Tree Search, 529 Nature 484, 488 (2016). Go is a board game pitting two players in a contest to surround more territory than each other’s opponent; it is played on a grid of black lines, with game pieces played on the lines’ intersections. A full-sized board is 19 by 19. Part of the reason Go presents such a difficult computational challenge is because its search space is so large. “After the first two moves of a Chess game,” for instance, “there are 400 possible next moves. In Go, there are close to 130,000.” Danielle Muoio, Why Go is So Much Harder for AI to Beat Than Chess, Tech Insider, March 10, 2016, http://www.techinsider.io/why-google-ai-game-go-is-harder-than-chess-2016-3.\n\n[15]. A Game-Changing Result, The Economist, March 19, 2016, http://www.economist.com/news/science-and-technology/21694883-alphagos-masters-taught-it-game-electrifying-match-shows-what.\n\n[16]. Id.\n\n[17]. In this report, while recognizing certain distinctions and overlaps between them, we use the terms “war” and “armed conflict” interchangeably to denote an armed conflict (whether of an international or a non-international character) as defined in international law and a state of war in the legal sense. See, e.g., Jann Kleffner, Scope of Application of International Humanitarian Law, in The Handbook of International Humanitarian Law (Dieter Fleck ed., 3rd ed. 2013).\n\n[18]. See, e.g., Marten Zwanenburg et al., Humans, Agents and International Humanitarian Law: Dilemmas in Target Discrimination, BNAIC 408 (2005) (examining the destruction of a commercial airliner by the USS Vincennes to illustrate legal and ethical dilemmas involving the use of autonomous agents).\n\n[19]. Among states and commentators, there is no agreement on whether to refer to “autonomous weapons,” “autonomous weapon systems,” or “autonomous weapons systems,” among many other formulations. Throughout this report, where referring to the views of a particular state(s) or commentator(s), we adopt that entity’s or person’s framing. Otherwise, for ease of reference, we adopt the “autonomous weapon system(s)” framing.\n\n[20]. Rebecca Crootof, War Torts: Accountability for Autonomous Weapons Systems, 164 U. Penn. L. Rev. (forthcoming June 2016), http://ssrn.com/abstract=2657680 [hereinafter Crootof, War Torts]. In June 2016, the Defense Science Board highlighted six categories of how autonomy can benefit (Department of Defense) DoD missions:\n\nRequired decision speed: more autonomy is valuable when decisions must be made quickly (e.g., cyber operations and missile defense);\n\nHeterogeneity and volume of data: more autonomy is valuable with high volume data and variety of data types (e.g., imagery; intelligence data analysis; intelligence, surveillance, reconnaissance (ISR) data integration);\n\nQuality of data links: more autonomy is valuable when communication is intermittent (e.g., times of contested communications, unmanned undersea operations);\n\nComplexity of action: more autonomy is valuable when activity is multimodal (e.g., an air operations center, multi-mission operations);\n\nDanger of mission: more autonomy can reduce the number of warfighters in harm’s way (e.g., in contested operations; chemical, biological, radiological, or nuclear attack cleanup); and\n\nPersistence and endurance: more autonomy can increase mission duration (e.g., enabling unmanned vehicles, persistent surveillance).\n\nSee Defense Science Board, supra note 7, at 45 (June 2016).\n\n[21]. U.S. Dep’t of Defense, Unmanned Systems Integrated Roadmap: FY2013–2038, at 67 (2013), http://www.defense.gov/Portals/1/Documents/pubs/DOD-USRM-2013.pdf.\n\n[22]. Gov’t (Neth.), Government Response to AIV/CAVV Advisory Report no. 97, Autonomous Weapon Systems: The Need for Meaningful Human Control (2016), http://aiv-advice.nl/8gr#government-responses [hereinafter Dutch Government, Response to AIV/CAVV Report]. At the same time, however, the Dutch government “reject[ed] outright the possibility of developing and deploying fully autonomous weapons.” Id.\n\n[23]. See, e.g., Mary Ellen O’Connell, Banning Autonomous Killing, in The American Way of Bombing: Changing Ethical and Legal Norms, from Flying Fortresses to Drones (Matthew Evangelista & Henry Shue eds., 1st ed. 2014).\n\n[24]. Human Rights Watch and the Harvard Law School International Human Rights Clinic, Losing Humanity: The Case against Killer Robots (2012), https://www.hrw.org/report/2012/11/19/losing-humanity/case-against-killer-robots.\n\n[25]. See, e.g., Act, Campaign to Stop Killer Robots, https://www.stopkillerrobots.org/act (last visited Aug. 23, 2016).\n\n[26]. Autonomous Weapons: An Open Letter from AI & Robotics Researchers, Future of Life Institute (July 28, 2015), http://futureoflife.org/open-letter-autonomous-weapons.\n\n[27]. Id.\n\n[28]. AWS have also been raised at the U.N. Human Rights Council, though without the thematic focus given to them in the context of the Convention on Certain Conventional Weapons (CCW). See, e.g., Christof Heyns (Special Rapporteur on Extrajudicial, Summary or Arbitrary Executions), Rep. to Human Rights Council, ¶¶ 142–45, UN Doc. A/HRC/26/36 (Apr. 1, 2014).\n\n[29]. See infra Section 3: International Law pertaining to Armed Conflict — Customary International Law concerning AWS.\n\n[30]. See infra Appendices I and II.\n\n[31]. On various formal and informal models of regulating new technologies, see generally Benjamin Wittes & Gabriella Blum, The Future of Violence: Robots and Germs, Hackers and Drones—Confronting A New Age of Threat (2015); with respect to autonomous military robots, see Gary E. Marchant et al., International Governance of Autonomous Military Robots, 12 Colum. Sci. & Tech. L. Rev. 272 (2011).\n\n[32]. Our concept of “war algorithms” should be distinguished from the “WAR algorithm” concept that has been developed in relation to evaluating environmental impacts. See Environmental Protection Agency, Waste Reduction Algorithm: Chemical Process Simulation for Waste Reduction, https://www.epa.gov/chemical-research/waste-reduction-algorithm-chemical-process-simulation-waste-reduction (last visited Aug. 27, 2016) (explaining that “[t]raditionally chemical process designs, focus on minimizing cost, while the environmental impact of a process is often overlooked. This may in many instances lead to the production of large quantities of waste materials. It is possible to reduce the generation of these wastes and their environmental impact by modifying the design of the process. The WAste Reduction (WAR) algorithm was developed so that environmental impacts of designs could easily be evaluated. The goal of WAR is to reduce environmental and related human health impacts at the design stage.”)\n\n[33]. See infra Section 2: Technology Concepts and Developments (on general definitions of “algorithm”).\n\n[34]. See infra Section 2: Examples of Purported Autonomous Weapon Systems.\n\n[35]. Drawn from the discussion of International Law Association, Committee on Accountability of International Organizations, Berlin Conference: Final Report 5 (2004), http://www.ila-hq.org/en/committees/index.cfm/cid/9 in James Crawford, State Responsibility: The General Part 85 (2013).\n\n[36]. In principle, the threat of use of a war algorithm may (also) give rise to legal implications; however, we focus on the design, development, and use of those algorithms.\n\n[37]. See infra Bibliography.\n\n[38]. Our approach is derived in part from International Law Association, supra note 35, at 5.\n\nThis section sketches key technology concepts and developments, as well as certain states’ understandings of autonomy in relation to war. We set the stage by discussing algorithms and constructed systems. We then outline recent advancements in the AI field of deep learning. Next, we highlight five states’ approaches to technical autonomy in war. In doing so, we also note accompanying standards that states and commentators are actively vetting, such as “meaningful human control” over AWS. Finally, we describe some of the main technologies that various commentators have addressed in relation to autonomous weapon systems.\n\nTwo Key Ingredients\n\nIn this briefing report, our foundational technological concern is the capability of a constructed system, without further human intervention, to help make and effectuate a “decision” or “choice” of a war algorithm. Distilled, the two core ingredients are an algorithm expressed in computer code and a suitably capable constructed system.\n\nAlgorithm\n\nAn algorithm has been defined informally as “any well-defined computational procedure that takes some value, or set of values, as input and produces some value, or set of values, as output.”[39] Accordingly, an algorithm is “a sequence of computational steps that transform the input into the output.”[40] Yet “[w]e can also view an algorithm as a tool for solving a well-specified computational problem.”[41] In this second approach, “[t]he statement of the problem specifies in general terms the desired input/output relationship. The algorithm describes a specific computational procedure for achieving that input/output relationship.”[42] Here, we are most concerned with algorithms that are expressed in computer code and that can be conceptualized as making “decisions” or “choices” along the computational pathway undertaken in light of the input and in accordance with programmed parameters.\n\nThe relevant algorithms may vary enormously in terms of their sophistication and complexity. But, at base, they all are conceived and coded initially by humans to take some input and produce some output or to describe a specific computational procedure for achieving a defined desirable input/output relationship.\n\nBy limiting our inquiry to war algorithms, we narrow the types of algorithms at issue to those that fulfill three conditions: algorithms (1) that are expressed in computer code; (2) that are effectuated through a constructed system; and (3) that are capable of operating in relation to armed conflict. Not all weapons or systems that have been characterized as “AWS” meet these criteria. But most do. And, more to the point, we see these algorithms as a key ingredient in what most commentators and states mean when they address notions of autonomy.\n\nWe predicate our definition on the algorithm being capable of operating in relation to armed conflict, even if it is not initially designed for such use. We thus do not limit our classification to algorithms that are in fact used in armed conflict (though the broader category of capability would subsume those that are actually used). A critique of this approach might be that it is over-inclusive because it does not distinguish between algorithms and the relevant constructed systems that are intended for use in relation to war from the vast array of other such algorithms and systems that might be adapted for such use. Yet one reason to focus on capability—instead of intent—is that much of the underlying technology is modular and can therefore be adapted for use in relation to war even if it was not initially designed and developed to do so. Moreover, with respect to accountability, focusing on capability sweeps in not only those who are in a position to choose to deploy or to operate war algorithms but also those involved in the design and development of those algorithms. The emphasis on capability thereby helps account for the diverse assortment of actors—whether in government, commercial, academic, or other contexts—who might exercise power over, and thus who might be held answerable for, the design, development, or use of war algorithms.\n\nConstructed System\n\n“Robot” is not a legal term of art under international law. One oft-cited, decades-old definition comes from the Robot Institute of America, a trade association of robot manufacturers and users: “a reprogrammable, multifunctional manipulator designed to move material, parts, tools, or specialized devices through various programmed motions for the performance of a variety of tasks.”[43] Others draw different definitional boundaries. Alan Winfield, for instance, defines a robot as “an artificial device that can sense its environment and purposefully act on or in that environment.”[44] Neil Richards and William Smart argue that a robot is “a constructed system that displays both physical and mental agency but is not alive in the biological sense.”[45] And the Oxford English Dictionary Online defines a robot in the modern sense[46] as “[a]n intelligent artificial being typically made of metal and resembling in some way a human or other animal.”[47]\n\nWe sidestep some of the definitional quandaries attending “robot” by focusing instead on constructed systems. For our purposes, a constructed system is a manufactured machine, apparatus, plant, or platform that is capable both of being used to gather information and of effectuating a “choice” or “decision” which is, in whole or in part, derived through an algorithm expressed in computer code but that is not alive in the biological sense. By limiting our inquiry to systems that are not alive in the biological sense, we also circumvent the subject of biologically engineered agents.\n\nAmong the most common sensors used to gather information in “constructed systems” include methods to detect how far away objects are by transmitting certain waves and monitoring their reflections, such as radar (radio waves), sonar (sound waves), and lidar (light waves), as well as cameras. The system may be tele-operated (also known as remotely operated)—or not. It may have a manipulator (used loosely here to denote a component providing the capability to interact in the built environment)—or not. However, if it does not have a manipulator, the system needs, to meet our definition, another avenue to effectuate the algorithmically-derived “choice” or “decision.”\n\nThe constructed systems may come in a diverse array of forms,[48] such as marine, terrestrial, aerial, or space vehicles; missile systems; or biped or quadruped robots.[49] They may operate collaboratively—including as so-called “swarms”[50]—or individually. They may use a range of power sources, such as batteries or internal combustion engines to generate electricity or to power hydraulic or pneumatic actuators. And their costs may run the gamut from the budget of a tinkerer to industrial or governmental-scale programs.\n\nA.I. Advancements\n\nRecently published advancements in AI—especially machine learning and a class of techniques called deep learning—underscore the rapid pace of technical development.[51] Those advancements reach into many areas of modern digital life, underlying “web searches to content filtering on social networks to recommendations on e-commerce websites.”[52]\n\nFor many years, “[c]onventional machine-learning techniques were limited in their ability to process natural data in their raw form.”[53] For decades, for instance, “constructing a pattern-recognition or machine-learning system required careful engineering and considerable domain expertise to design a feature extractor that transformed the raw data … into a suitable internal representation or feature vector from which the learning subsystem, often a classifier, could detect or classify patterns in the input.”[54] An advance came with representational learning, which “is a set of methods that allows a machine to be fed with raw data and to automatically discover the representations needed for detection or classification.”[55]\n\nDeep learning—including deep neural networks—marked another advance. (A deep neural network can be thought of as “a network of hardware and software that mimics the web of neurons in the human brain.”[56]) Deep-learning methods have been explained as “representation-learning methods with multiple levels of representation, obtained by composing simple but non-linear modules that each transform the representation at one level (starting with the raw input) into a representation at a higher, slightly more abstract level.”[57] As experts have explained, “[w]ith the composition of enough such transformations, very complex functions can be learned.”[58] The gist is that, “[f]or classification tasks, higher layers of representation amplify aspects of the input that are important for discrimination and suppress irrelevant variations.”[59]\n\nConsider the example of a digital image. It\n\ncomes in the form of an array of pixel values, and the learned features in the first layer of representation typically represent the presence or absence of edges at particular orientations and locations in the image. The second layer typically detects motifs by spotting particular arrangements of edges, regardless of small variations in the edge positions. The third layer may assemble motifs into larger combinations that correspond to parts of familiar objects, and subsequent layers would detect objects as combinations of these parts.[60]\n\nThrough deep-learning techniques, “these layers of features are not designed by human engineers: they are learned from data using a general-purpose learning procedure.”[61]\n\nAlready, “[d]eep learning is making major advances in solving problems that have resisted the best attempts of the artificial intelligence community for many years.”[62] Those include beating records in image recognition and speech recognition, as well as beating other machine-learning techniques at, for example, predicting the activity of drug molecules.[63] Writing in 2015, some experts “think that deep learning will have many more successes in the near future because it requires very little engineering by hand, so it can easily take advantage of increases in the amount of available computation and data.”[64] In line with this view, “[n]ew learning algorithms and architectures that are currently being developed for deep neural networks will only accelerate this progress.”[65]\n\nOne mark of that progress came late last year when a computer program, AlphaGo, achieved a feat previously thought to be at least a decade away: defeating a human professional player in a full-sized game of Go.[66] (A few months later, AlphaGo won four of five matches against Lee Sedol, who, as one of the top players in the world, had achieved the highest rank of nine dan.[67]) The system designers introduced a new approach based on deep convolutional neural networks that used “value networks” to evaluate board decisions and “policy networks” to select moves. (Convolutional neural networks—the typical architecture of which is structured as a series of stages—“are designed to process data that come in the form of multiple arrays.”[68] In other words, these networks “use many layers of neurons, each arranged in overlapping tiles, to construct increasingly abstract, localized representations of an image.”[69]) For AlphaGo, those deep neural networks were “trained by a novel combination of supervised learning from human expert games, and reinforcement learning from games of self-play.”[70] AlphaGo developers also introduced a new search algorithm—which was designed in part to encourage exploration on its own—that combines a sophisticated simulation technique (called Monte Carlo tree search) with the value and policy networks.[71]\n\nBy grounding our discussion in algorithms expressed in computer code and effectuated through constructed systems, we sidestep some of the doctrinal debates on what constitutes “artificial intelligence” and “artificial general intelligence”—and on whether the latter may be realistically achievable or is more the stuff of science fiction. These questions are outside of the scope of this briefing report, but they are nonetheless vitally important. In any event, it merits emphasis that existing learning algorithms and architectures already have remarkable capabilities that, at least, seem to approach aspects of human “decision-making.”\n\nFor their part, creators of AlphaGo have characterized Go as “exemplary in many ways of the difficulties faced by artificial intelligence: a challenging decision-making task, an intractable search space, and an optimal solution so complex it appears infeasible to directly approximate using a policy or value function.”[72] In the eyes of its designers, AlphaGo provides “hope that human-level performance can now be achieved in other seemingly intractable artificial intelligence domains.”[73]\n\nApproaches to Technical Autonomy in War\n\nAs noted above, there is no agreement on what “autonomy” means in the context of the discussion to date on autonomous weapon systems.\n\nCommentators’ views on what constitutes “autonomy” in this context range enormously. Some, for instance, focus on whether the system navigates with a human on board (“manned”) or without one (“unmanned”). Others emphasize geography, such as whether the weapon is operated by a human remotely or proximately. Some hold that the “autonomy” in AWS should be reserved only for “critical functions” in the conduct-of-hostilities targeting cycle. Still others argue that it is the capability of a system, once launched, to sense, think, learn, and act all without further human intervention. A number of definitions combine various components of these notions. But depending on the definition and classification, it is beyond doubt that some existing military systems contain at least a degree of autonomy. (In the last sub-section of this section, we profile examples of weapons, weapon systems, and weapon platforms that some commentators have characterized as AWS.)\n\nIn this sub-section, we focus on the positions of states, because discerning states’ positions and practices is one of the key steps in illuminating the scope of international law as it currently stands (lex lata) and distinguishing that from nascent norms and from the law as it should be (lex ferenda). A handful of states have considered or formally adopted definitions relevant to AWS, whether while focusing on weapon systems or unmanned aerial systems. Below, we summarize five of the most elaborate sets of these considerations and definitions—those by Switzerland, France, the Netherlands, the United States, and the United Kingdom.\n\nSwitzerland\n\nIn the lead-up to the 2016 Informal Meeting of Experts on Lethal Autonomous Weapons Systems, Switzerland published an “Informal Working Paper” titled “Towards a ‘compliance-based’ approach to LAWS.” The paper proposes “to initially describe autonomous weapons systems (AWS) simply as” follows:\n\n[W]eapons systems that are capable of carrying out tasks governed by IHL in partial or full replacement of a human in the use of force, notably in the targeting cycle.[74]\n\nAccording to the paper, “[s]uch a working definition is inclusive, accounts for a wide array of system configurations, and allows for a debate that is differentiated, compliance-based, and without prejudice to the question of appropriate regulatory response.”[75] In the view of Switzerland, “the working definition proposed is not conceived in any way to single out only those systems which could be seen as legally objectionable.”[76] The authors note that “[a]t one end of the spectrum of systems falling within that working definition, States may find some subcategories to be entirely unproblematic, while at the other end of the spectrum, States may find other subcategories unacceptable.”[77] Finally, the paper notes, “[a]s discussions advance, this working definition could and probably should evolve to become more specific and purposeful.”[78]\n\nThe Netherlands\n\nOn April 7, 2015, the Netherlands Ministries of Foreign Affairs and of Defense requested a report from the Advisory Council on International Affairs (AIV) and the Advisory Committee on Issues of Public International Law (CAVV) addressing five sets of questions concerning autonomous weapon systems:\n\nWhat role can autonomous weapons systems (and autonomous functions within weapons systems) fulfil in the context of military action now and in the future?\n\nWhat changes might occur in the accountability mechanism for the use of fully or semi-autonomous weapons systems in the light of associated ethical issues? What role could the concept of ‘meaningful human control’ play in this regard, and what other concepts, if any, might be helpful here?\n\nIn its previous advisory report, the CAVV states that the deployment of any weapons system, whether or not it is wholly or partly autonomous, remains subject to the same legal framework. As far as the CAVV is concerned, there is no reason to assume that the existing international legal framework is inadequate to regulate the deployment of armed drones. Does the debate on fully or semi-autonomous weapons systems give cause to augment or amend this position?\n\nHow do the AIV and the CAVV view the UN Special Rapporteur’s call for a moratorium on the development of fully autonomous weapons systems?\n\nHow can the Netherlands best contribute to the international debate on this issue?\n\nA joint committee of the AIV and the CAVV prepared a report, which the AIV adopted on October 2, 2015 and the CAVV adopted on October 12, 2015.[79] On March 2, 2016, the government responded to the report. (We use the term “government” in this context interchangeably with reference to the Ministries of Foreign Affairs and of Defense of the Netherlands.) The main conclusion of the report, in the words of the government’s response, “is that meaningful human control is required in the deployment of autonomous weapon systems”—a view with which the government concurs.[80]\n\nThe government—while noting “[t]here is as yet no internationally agreed definition of an autonomous weapon system”—supports the working definition of AWS which the advisory committee adopted:[81]\n\nA weapon that, without human intervention, selects and engages targets matching certain predetermined criteria, following a human decision to deploy the weapon on the understanding that an attack, once launched, cannot be stopped by human intervention.[82]\n\nUnderlying this definition is the notion of the “wider loop” of the decision-making process, which plays a prominent role in the Dutch government’s understanding of accountability concerning AWS. In the view of the Dutch government, with respect to AWS humans are involved in that “wider loop” because humans “play a prominent role in programming the characteristics of the targets that are to be engaged and in the decision to deploy the weapon.”[83] That means, in short, “that humans continue to play a crucial role in the wider targeting process. An autonomous weapon as defined above is therefore only deployed after human consideration of aspects such as target selection, weapon selection and implementation planning, including an assessment of potential collateral damage.”[84] In addition, the government notes, “the autonomous weapon is programmed to perform specific functions within pre-programmed conditions and parameters. Its deployment is followed by a human assessment of the effects. Assessments of potential collateral damage (proportionality) and accountability under international humanitarian law are of key importance in this respect.”[85]\n\nAs summarized by the Dutch government, “[t]he advisory committee states that if the deployment of an autonomous weapon system takes place in accordance with the process described above, there is meaningful human control. In such cases, humans make informed, conscious choices regarding the use of weapons, based on adequate information about the target, the weapon in question and the context in which it is to be deployed.”[86] For its part, “[t]he advisory committee sees no immediate reason to draft new or additional legislation for the concept of meaningful human control.”[87] Instead, “[t]he concept should be regarded as a standard deriving from existing legislation and practices (such as the targeting process).”[88] Over all, the government expressly affirms that it “supports the definition given above of an autonomous weapon system, including the concept of meaningful human control, and agrees that no new legislation is required.”[89]\n\nFrance\n\nIn a “non-paper” circulated in the context of the 2016 Informal Meeting of Experts on Lethal Autonomous Weapons Systems, France articulated the following considerations with respect to such systems:\n\nFrance considers that LAWS [Lethal Autonomous Weapons Systems] share the following characteristics:\n\n- Lethal autonomous weapons systems are fully autonomous systems. LAWS are future systems: they do not currently exist.\n\n- Remotely operated weapons systems and supervised weapons systems should not be regarded as LAWS since a human operator remains involved, in particular during the targeting and firing phases. Existing automatic systems are not LAWS either[.]\n\n- LAWS should be understood as implying a total absence of human supervision, meaning there is absolutely no link (communication or control) with the military chain of command.\n\n- The delivery platform of a LAWS would be capable of moving, adapting to its land, marine or aerial environments and targeting and firing a lethal effector (bullet, missile, bomb, etc.) without any kind of human intervention or validation.”[90]\n\nCompared to most other states that have put forward working definitions, France articulates a relatively narrow definition of what constitutes a lethal autonomous weapons system in the context of the CCW. Most striking, perhaps, is the condition that there be “a total absence of human supervision, meaning there is absolutely no link (communication or control) with the military chain of command.” Moreover, France clarifies that, in its view, the definition of a “lethal autonomous weapons system” includes only a delivery “platform” that “would be capable of moving, adapting to its land, marine or aerial environments and targeting and firing a lethal effector … without any kind of human intervention or validation.” This formulation combines autonomy in navigation and maneuver with autonomy in certain key elements of the targeting cycle.\n\nUnited States\n\nIn a series of directives and other documents, the U.S. Department of Defense (DoD) has elaborated one of the most technically specific state approaches to autonomy in relation to weapon systems.\n\nA central document is DoD Directive 3000.09 (2012). It “[e]stablishes DoD policy and assigns responsibilities for the development and use of autonomous and semi-autonomous functions in weapon systems, including manned and unmanned platforms.”[91] The directive is applicable to certain DoD actors and related organizational entities.[92] It concerns “[t]he design, development, acquisition, testing, fielding, and employment of autonomous and semi-autonomous weapon systems, including guided munitions that can independently select and discriminate targets,” as well as “[t]he application of lethal or non-lethal, kinetic or non-kinetic, force by autonomous or semi-autonomous weapon systems.”[93] However, the directive expressly “does not apply to autonomous or semi-autonomous cyberspace systems for cyberspace operations; unarmed, unmanned platforms; unguided munitions; munitions manually guided by the operator (e.g., laser- or wire-guided munitions); mines; or unexploded explosive ordnance.”[94] Among the relevant terms defined in the glossary of Directive 3000.09 are the following:\n\nAutonomous weapon system: “A weapon system that, once activated, can select and engage targets without further intervention by a human operator. This includes human-supervised autonomous weapon systems that are designed to allow human operators to override operation of the weapon system, but can select and engage targets without further human input after activation.”[95]\n\nHuman-supervised autonomous weapon system: “An autonomous weapon system that is designed to provide human operators with the ability to intervene and terminate engagements, including in the event of a weapon system failure, before unacceptable levels of damage occur.”[96]\n\nSemi-autonomous weapon system: “A weapon system that, once activated, is intended to only engage individual targets or specific target groups that have been selected by a human operator. This includes: [s]emi-autonomous weapon systems that employ autonomy for engagement-related functions including, but not limited to, acquiring, tracking, and identifying potential targets; cueing potential targets to human operators; prioritizing selected targets; timing of when to fire; or providing terminal guidance to home in on selected targets, provided that human control is retained over the decision to select individual targets and specific target groups for engagement.”[97]\n\nDirective 3000.09 establishes that, as a matter of policy, “[a]utonomous and semi-autonomous weapon systems shall be designed to allow commanders and operators to exercise appropriate levels of human judgment over the use of force.”[98] More specifically, “[s]ystems will go through rigorous hardware and software verification and validation … and realistic system developmental and operational test and evaluation … in accordance with” certain guidelines.[99] In addition, “[t]raining, doctrine, and tactics, techniques, and procedures … will be established.”[100] In particular, those measures will ensure that autonomous and semi-autonomous weapon systems will, first, “[f]unction as anticipated in realistic operational environments against adaptive adversaries.” Second, they will ensure that those systems will “[c]omplete engagements in a timeframe consistent with commander and operator intentions and, if unable to do so, terminate engagements or seek additional human operator input before continuing the engagement.” And third, they will ensure that those systems “[a]re sufficiently robust to minimize failures that could lead to unintended engagements or to loss of control of the system to unauthorized parties.”[101]\n\nThe directive also establishes that “[c]onsistent with the potential consequences of an unintended engagement or loss of control of the system to unauthorized parties, physical hardware and software will be designed with appropriate: … Safeties, anti-tamper mechanisms, and information assurance in accordance with [another relevant DoD directive]. … Human-machine interfaces and controls.”[102] Furthermore, “[i]n order for operators to make informed and appropriate decisions in engaging targets,” the directive establishes that “the interface between people and machines for autonomous and semi-autonomous weapon systems shall” have three characteristics. First, they shall “[b]e readily understandable to trained operators.” Second, they shall “[p]rovide traceable feedback on system status.” And third, they shall “[p]rovide clear procedures for trained operators to activate and deactivate system functions.”[103]\n\nDirective 3000.09 further lays down, also as a matter of policy, that “[p]ersons"
    }
}
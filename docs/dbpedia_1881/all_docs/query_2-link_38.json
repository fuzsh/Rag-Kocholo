{
    "id": "dbpedia_1881_2",
    "rank": 38,
    "data": {
        "url": "https://dl.acm.org/doi/fullHtml/10.1145/3613904.3642126",
        "read_more_link": "",
        "language": "en",
        "title": "“Is Text-Based Music Search Enough to Satisfy Your Needs?” A New Way to Discover Music with Images",
        "top_image": "https://dl.acm.org/cms/attachment/html/10.1145/3613904.3642126/assets/html/images/chi24-237-fig1.jpg",
        "meta_img": "",
        "images": [
            "https://dl.acm.org/cms/attachment/html/10.1145/3613904.3642126/assets/html/images/chi24-237-fig1.jpg",
            "https://dl.acm.org/cms/attachment/html/10.1145/3613904.3642126/assets/html/images/chi24-237-fig2.jpg",
            "https://dl.acm.org/cms/attachment/html/10.1145/3613904.3642126/assets/html/images/chi24-237-fig3.jpg",
            "https://dl.acm.org/cms/attachment/html/10.1145/3613904.3642126/assets/html/images/chi24-237-fig4.jpg",
            "https://dl.acm.org/cms/attachment/html/10.1145/3613904.3642126/assets/html/images/chi24-237-fig5.jpg",
            "https://dl.acm.org/cms/attachment/html/10.1145/3613904.3642126/assets/html/images/chi24-237-fig6.jpg",
            "https://dl.acm.org/cms/attachment/html/10.1145/3613904.3642126/assets/html/images/chi24-237-fig7.jpg",
            "https://dl.acm.org/cms/attachment/html/10.1145/3613904.3642126/assets/html/images/chi24-237-fig8.jpg",
            "https://dl.acm.org/cms/attachment/html/10.1145/3613904.3642126/assets/html/images/chi24-237-fig9.jpg",
            "https://dl.acm.org/cms/attachment/html/10.1145/3613904.3642126/assets/html/images/chi24-237-fig10.jpg",
            "https://dl.acm.org/cms/attachment/html/10.1145/3613904.3642126/assets/html/images/chi24-237-fig11.jpg",
            "https://dl.acm.org/cms/attachment/html/10.1145/3613904.3642126/assets/html/images/chi24-237-fig12.jpg",
            "https://dl.acm.org/cms/attachment/html/10.1145/3613904.3642126/assets/html/images/chi24-237-fig13.jpg"
        ],
        "movies": [],
        "keywords": [],
        "meta_keywords": [
            ""
        ],
        "tags": null,
        "authors": [
            "Jeongeun Park",
            "Division of Media",
            "Design Technology",
            "Hanyang University",
            "Republic of",
            "parkje@hanyang.ac.kr",
            "Hyorim Shin",
            "Graduate School of Information",
            "Yonsei University",
            "Changhoon Oh"
        ],
        "publish_date": null,
        "summary": "",
        "meta_description": "",
        "meta_lang": "en",
        "meta_favicon": "",
        "meta_site_name": "",
        "canonical_link": null,
        "text": "1 INTRODUCTION\n\nMusic is deeply entrenched in the human experience, and it transcends geographic, social, and cultural delineations. Its profound impact is so universal that some researchers have suggested it may predate language itself and assert that although some societies lack written or visual art forms, none exist without music [6, 33, 75]. As a cornerstone of human history, music has become an indelible part of our collective and individual identities.\n\nThe scope and diversity of music have expanded exponentially. The burgeoning digital era further exemplifies this phenomenon: Spotify, a leading music streaming platform, boasts a staggering catalog of 100 million songs, with an addition of approximately 60,000 new tracks each day [1, 14]. Alongside this proliferation, music search systems have evolved to incorporate sophisticated algorithms and recommendation systems. Beyond merely facilitating search using on concrete attributes such as artist name or song title, these systems now employ predictive algorithms to offer selections that are aligned with individual preferences, thereby amplifying the user's listening experience [56, 63].\n\nThe form of query used in music search has also changed as technology has advanced. In the past, music could only be searched by entering precise text, but in the early 2000s, search methods emerged that utilized specific elements that constitute music, such as melody, beat, and lyrics [29, 48, 73]. However, despite the remarkable growth in methods used for music search, the ways in which music is searched are still directly or indirectly related to music [38]. Starting from the question “How does one search for music when one has no specific music in mind or when the search information is ambiguous and difficult to express in text?\" We intend to discuss the need for new methods for exploring music.\n\nThere have been numerous studies on new music search methods [17, 21]. Among them, one study used visual data, such as images, as one of the new music search possibilities. Park et al. [55] suggested that images could be interpreted subjectively, and because of this, they could implicitly express the emotions of music. Indeed, from the perspective of expression, we often use simple yet effective visual content such as stickers, emoticons, and emojis, rather than multiple words to convey states or moods [66, 70]. Sometimes, a single intuitive image can convey multiple meanings more implicitly and effectively than can words, and it can produce more efficient results [18, 80]. In this study, considering that emotions or situations could be effectively conveyed through visual materials like images, we anticipated that users could utilize visual data as search queries even when they lacked information about music. Our research was conducted following the procedure outlined below.\n\nFirst, by conducting a user study, we examined the behaviors of music search and gathered insights into users’ perspectives, including pain points and needs related to existing music streaming services. Based on the derived results, we designed three interactive music search systems that can be used when the queries were not clear. These included (1) a natural language (text)-based music search system, which is a conventional method for expressing ambiguous thoughts or emotions, (2) an image-based music search system that uses images as queries, and (3) a hybrid music search system that uses images as queries and assists with text. Following this, interactive interfaces were designed to provide users with a list of searched music based on their input. Subsequently, we recruited participants to determine whether differences existed in the experience users had with these systems and to assess and compare the usability of each system. We recruited 236 participants, and they experienced one of the three systems and responded to a post-hoc survey. Lastly, we conducted interviews to explore and suggest various user scenarios in which our proposed image-based music retrieval (IMR) system could be applied. The findings from our user study, surveys, and interviews are summarized as follows:\n\nPeople have difficulty finding music that fits specific situations or moods when they do not have clear information about the music they are seeking, and they want to discover music that they have not encountered before.\n\nText-based music search that can explicitly express one's emotion and state shows a higher level of alignment between queries and search results than do image-based music search methods.\n\nHowever, user experiences are more significantly positive when using images compared to using text in terms of the music search task.\n\nImage-based music searches provide higher satisfaction regarding system usability compared with text-based music search methods.\n\nIn particular, image-based music search methods that intuitively express one's emotions or situations may effective for individuals over the age of 40 years, as opposed to the younger generation aged 19–39 years.\n\nA hybrid music search system is suitable for enabling users to adapt to a new form of search when they are introduced to an image-based music search system.\n\nImage-based music search can be specialized for users who face challenges with the use of text, and it can be applied to scenarios such as searching for music that suits driving scenery or finding background music that is suitable for videos/images intended for social media platforms.\n\nBased on these research findings, we present guidelines for the future development of interfaces and for the functionality of music search systems. The contributions of this study to the domain of music search and the human computer interaction (HCI) community are as follows:\n\nWe investigated music search methods in specific scenarios in which input queries are unclear, as this kind of scenario has been little explored, and we discussed insights into the potential of music search systems that are mediated by images.\n\nWe obtained empirical results related to new facets of user experiences with music search systems, through both quantitative and qualitative approaches.\n\nWe designed and developed an interface for an image-based music search system, and we pioneered the user experience of a new kind of music search interface.\n\n2 RELATED WORK\n\nThe aim of this study is to explore the need for new music search methods by comparing user experiences and evaluating the usability of different music search approaches. To achieve this, we review studies related to 1) users’ music and multimedia experiences, and 2) music search methods.\n\n2.1 Music and Multimedia Experiences\n\n2.1.1 Music Listening Experience. Music, due to its influence on human emotions and physical responses, has been the subject of extensive effort that seeks to design and understand the experience of listeners [11, 32, 71]. Studies related to music listening can be categorized by objective: 1) designing the music listening experience, and 2) understanding the user experience. To examine studies related to the first objective, Bernhardt and Robinson [8] introduced a music mixing interface where users physically manipulated their bodies to blend music pieces, and they provided users with an emotionally immersive experience. In addition, one study introduced a new interactive agent that supported self-awareness, on the assumption that heart-warming melodies and lyrics could increase human self-awareness [11].\n\nFor the second objective of understanding musical experience, a pivotal study that delved into the fundamental reasons behind users’ music search behaviors was research by Hosey et al. [34]. They categorized how and why people use search on music streaming platforms. They used four categories—listen, organize, share, and fact check—and they observed participants’ behavior while the participants searched. The participants rated their search results in terms of success and effort and found that this was influenced by approaching music search with a focused, open, and exploratory mindset [34]. While previous studies have structured the experiences of music listeners, there have been also studies that proposed qualitative heuristic evaluation methods to assess the overall experience of music services [42]. Choi et al. [13] investigated the usability of and interface satisfaction with YouTube. After analyzing YouTube users’ experiences through a qualitative analysis, they proposed wireframes for a video streaming service to enhance the music listening experience. In another study, the user experience of recommendation and automatic curation features on a music streaming service was examined. The findings of this research revealed that users perceive an increase in algorithm personalization as dehumanizing [24]. These studies sought to understand music-listening users and fundamentally enhance their experiences, and they conducted research based on the search methods offered by existing music search platforms. However, the need for new music search methods that diverge from conventional approaches, and the potential impacts of such methods on users remain understudied. Therefore, this study compares proposed search methods for music with a conventional music search method to examine the utility of the new approach.\n\n2.1.2 Music and Audio-Visual Experience. Some studies seek to connect the auditory element of music with visual experiences. Knees et al. [39] proposed a system called nepTune that visually organizes a music collection. They clustered musically similar songs through audio analysis and represented these clusters as three-dimensional islands, creating a virtual landscape. Users can explore this virtual landscape and listen to the music closest to their current location. Users are provided with descriptions and images related to the music, which are obtained through web search technology. While they did not offer an IMR system, they visually represented similar music as islands, allowing users to explore a vast music list effectively. Moreover, by providing visual information related to the music, they enhanced the visual representation of the music and offered users a new experience.\n\nIn another study, a music exploration service called Songrium was developed to allow users to visually explore the relationships between original songs and their derivative works in music video clips [31]. They mapped the original songs onto a two-dimensional (2D) space based on the audio similarity. Then, they categorized derivative works of the original songs into six categories, such as singing a song, dancing to a song, and performing a song on musical instruments. Users can explore music derivatives visually through Songrium based on the song characteristics.\n\nRecently, Melchiorre et al. [51] introduced a study on an audio-visual mobile application interface that considers the user's emotional state. They introduced a system called emotion-aware music tower blocks (EmoMTB), which visualizes music as building blocks made of cubes through the t-student distributed stochastic neighborhood embedding algorithm. Similar songs are grouped into the same building block, and neighboring building blocks represent music of the same genre. Users can explore the landscape made of building blocks to select and listen to music from their preferred genre and receive personalized recommendations through customized building blocks. They provided a new user experience by visualizing music through EmoMTB.\n\nPrevious studies have grouped and visualized music based on the similarity of audio signals, allowing music exploration related to visual experiences. When there is no clear target music to search for, a new attempt to search for music using an exploratory method was made possible. However, the current study differs in that it does not start from musical similarity. Instead, it explores the potential of using images as music search queries by extracting features embedded in the images and connecting them to related music. This approach marks a distinct difference from previous studies in the approach to music discovery.\n\n2.2 Music Search Methods\n\nMusic information retrieval can be broadly divided into two modalities of search: text and audio samples [12, 55]. Both methods support predefined tag inputs such as artist names, song titles, and album titles (tag level), as well as diverse query inputs like sentence-level descriptions that are not predefined [20]. When music is searched for using tag-level inputs, the greatest challenge is to retrieve music that does not have specified tags from the database. Studies to address the drawbacks of tag-level music search have been conducted by applying metric learning [77] or by using multi-tags with multiple levels [76]. Doh et al. [20] comprehensively compared text-based music search methods at both the tag level and sentence level and proposed a novel stochastic sampling for text inputs.\n\nMusic search methods that use audio samples such as melody, beat, or humming are based on the similarity between signals in query samples and in target samples [73]. Marolt [50] proposed a method for indexing 2D shift-invariant melody representations to search for music. Salamon et al. [61] designed a method to extract frequencies that corresponded to the pitches of primary melodies from music files, which can be utilized in search systems. There is also a study that applied genetic algorithms inspired by natural selection and survival of the fittest to audio-based music search methods to enhance search performance [59]. They segmented the acoustic signals precisely and converted them into musical notes, and then they used genetic algorithms to find music that had high relevance. In addition to these studies, music search methods using audio samples have achieved high search accuracy by utilizing query sources that were directly related to the target music; these search methods have been explored extensively [36, 48, 62]. Similar to methods that use text or voice, audio-based music search methods also require prior information about the music to be searched, or they require that parts of the target music be input. However, in this study, we use images as a method for searching music in situations where there is no prior information about the music and the search query is ambiguous.\n\nThere are only few studies that focus on how to search for music using the image modality as is done in this study. Some studies have used the emotion of music to search for images [78] or to construct databases of emotional synesthetic between images and music to search for image-music pairs based on emotional similarity [79]. However, these studies differ from the IMR described in the present study, where the input image is used to search for music content. Shang et al. [68] first developed a system that searches for music based on the implicit meaning embedded in images. They extracted captions from images and linked them to the metaphorical meanings embedded in classic poetry to find the most semantically similar musical pieces. However, their search method faces challenges in capturing users’ daily emotions or moods due to relying on poetic metaphors. Park et al. [55] constructed large-scale datasets for IMR and used attributes such as mood and theme in images to search for music. They presented an IMR framework and found that user satisfaction was improved when there was interaction according to the user intervention in IMR. In this study, we enhanced and employed the image-music mapping algorithm they proposed, applying it to system prototypes designed with user interaction elements that they found beneficial for improving satisfaction. Although they proposed an IMR algorithm, the empirical value an actual IMR system provides compared to existing text-based music search methods has not yet been studied. Therefore, our study compared the traditional text-based music search system with the IMR system to investigate the efficacy of the IMR system and verify the necessity of music search methods using image modality.\n\n3 USER STUDY I\n\nWe organized the user studies into three steps according to each purpose to examine the following objectives: 1) Understanding users’ music search behaviors and identifying their needs and pain points in music search; 2) Investigating the user experience and differences in usability between the systems to assess the need for the IMR system; 3) Exploring potential future usage scenarios for the IMR system. As a first user study, we conducted a focus group interview. For this, we recruited 12 users of music streaming services through online advertisements. There were no restrictions on the streaming platforms they used or on their average daily music listening time during recruitment. However, we sought individuals who had experienced difficulties when searching for music and who could actively express their opinions on the matter. The participants (9 males and 3 females) were all users of music streaming services, and they had an average age of 34. They used four types of music streaming services, and four participants utilized two or more services.\n\n3.1 Study Procedure\n\nPrior to the interviews, participants had an ice-breaking session that lasted approximately 10 minutes. During this time, they provided a brief self-introduction and described the music streaming service they used, when they listened to music, and in what situations. Afterward, a 40 minute session was held to answer questions and share opinions about usage patterns in music search. The facilitator asked open-ended questions based on participants’ responses and inquired about when and in what situations they performed music searches, their criteria for selecting search results, any discomfort they experienced during music searches, and aspects they would like to see improved. All the interviews were recorded and transcribed with the consent of the participants. The study protocol was reviewed and approved by the IRB of our institution.\n\n3.2 Findings\n\n3.2.1 Differences in search methods according to the clarity of music search intent. Participants primarily searched for music either for the purpose of listening to music they knew or discovering something new. The former involved having a specific goal in mind for the search, whereas the latter was intended for exploring music from a broad range of options without a clear goal. When the search goal was clear, participants utilized specific pre-existing information such as song titles, artist names, and album titles in their searches. The clearer the purpose of the search, the more the depth of the search was minimized by directly entering the target, which was mainly the title of the song. Conversely, when the search goal was unclear, they gradually narrowed down the scope based on surrounding information, like artist names and album titles, to pinpoint the precise target information. This is consistent with the findings from Hosey et al. [34], where the researchers observed that users who searched for desired music with a clear and focused mindset, and users who searched with a more open mindset exhibited differences in the entities they clicked on during their search, such as song titles or album names. However, when participants wanted to discover new music and did not have a specific target in mind, they employed two main approaches in their searches. The first approach involved narrowing down the search scope using keywords, such as genres or themes. The participants then explored curated playlists that were created by other users or playlists that were recommended by the system based on the participants’ existing music preferences (from keywords to playlists; KP). The second approach was to directly input text (from text to playlists; TP). For instance, this method involved expressing the mood or the situations that were associated with the music, such as “calm jazz for working\" or “healing music for rainy mornings,\" and searching for playlists that matched those descriptions. As such, there were differences between the search approaches in cases where the purpose of the music search was clear and in cases where the purpose was ambiguous. Furthermore, when the purpose was unclear, the methods for searching music could also be categorized into two distinct approaches.\n\n3.2.2 Challenges in search when exact information about the music is lacking. Participants appeared to struggle to find the music they desired, unless they used precise information for their searches. The challenges in music search were evident in conversations with participants who used the KP method (P4) or the TP method (P9) to search for music, as well as those who used both methods (P1, P5). P5 discussed the difficulties that they encountered when using the keywords provided on the streaming platform, stating, “There are often times when the search results don't match the feeling I want. (Entering search terms) has to be very detailed. The more precisely I can express what I want, the higher the satisfaction with the search results.\" Agreeing with P5’s opinion, P1 expressed frustration with the difficulty of selecting search terms, saying, “When I searched with as much detail as ‘groovy music to listen to while driving’, I did get lists that were somewhat similar to the music I wanted to hear. But it's a bit embarrassing when I can't even think of such descriptions. I just want to listen to music, and I don't even know what I want to listen to..., but it's difficult to search without any information.\" Responding to this, P4 mentioned, “But the platform I use doesn't support that kind of search (natural language input), right? I have to choose from pre-curated playlists.\" Similar to the observation made by P4, most commercialized music search systems rely on simple searches using metadata, and they offer limited song retrieval when queries are ambiguous [7, 53]. In addition to these participants, others also pointed out the inconvenience of constrained search methods that involve either text input or selection from curated playlists. Regarding the inconvenience of using the TP method for search, P9 mentioned, “I just type a lot of stuff hoping to find something. That's what's inconvenient. There's a lot to type. (...) Clicking is convenient, but writing is a little annoying. I make a lot of typos when typing on my phone.\" According to previous studies [41, 74], more queries provide more stable and reliable search results. Moreover, including additional explanations regarding the user context improves the music search results. To ensure their satisfaction with the search results, participants using the TP method often input lengthy search queries. They reported that this lengthy input process is cumbersome, and they sometimes expressed difficulties, not knowing how to input their queries properly. Through conversations with the participants, we identified the following challenges and discomforts when there is no clear search target: 1) difficulty finding music that adequately reflects the listener's state or emotion, 2) inconvenience of writing long sentences for precise searches, 3) limitations due to constrained search methods.\n\n3.2.3 Difficulties in expanding musical preferences. Participants had a desire to discover new songs when they searched for music. However, many streaming services limited search results based on the music the users frequently listen to. The search methods that reflected user preferences were highly efficient in recommending music that was likely to satisfy users’ musical tastes. Nevertheless, this approach was mentioned as a drawback due to the fact that participants ended up listening to songs of genres and moods that were similar to the ones they usually listened to, which narrowed down the breadth of their preferences. P8 said, “I feel great when I discover good artists or songs that I didn't know before. But I keep getting search results that are similar to the artists I already like, or songs with similar vibes.\" P11 also agreed, saying, “I feel the same way. Like pre-censored...? There are times when newly found songs don't really feel new.\" This phenomenon is similar to a filter bubble, where users are presented with tailored information that leads them to encounter only filtered content, which results in content bias [54]. Users desire serendipity, where significant discoveries arise from complete randomness, but search systems that fulfill this desire are lacking.\n\n4 SYSTEM DESIGN\n\nAccording to the findings of User Study I reported in Section 3, users encountered difficulties when they searched for songs without clear search terms, and there was a demand for new methods that could reflect their emotions or situations in music search. To satisfy these needs, users should be able to express their emotion or mood even without precise search terms, and there should be an intuitive means of capturing the situations or moments that trigger the desire to search. We intend to explore the use of images that can implicitly carry multiple meaning and serve as cues for evoking music. According to studies by Kellaris et al. [37] and Fraser [23], music can evoke images not only from past experiences but also from cues embedded within the music. In other words, just as music can evoke images, images can serve as catalysts for recalling music. Considering these perspectives, we conducted user-targeted experiments to design and implement a system that could be used to compare IMR methods with a traditional text-based approach. This section provides a detailed description of the process involved in our system design.\n\n4.1 Design Goals\n\nConsidering the needs and pain points that the participants identified in User Study I on music search, we established the following three design goals for IMR systems. Table 1 presents the detailed descriptions of user needs and pain points related to setting these design goals and the insights derived from them.\n\nAccordingly, we designed interactive system interfaces for three types of music search systems that could be used in exploratory music discovery contexts: text-based, image-based, and hybrid (image + text) systems. We named the systems TTTune, VisTune, and VTTune, respectively. TTTune used an existing method and served as a basis for comparing user responses to the new music search method systems that used images. VisTune employed only images as search terms for music, without the use of any text intermediaries. VTTune similarly incorporated images as intermediaries, but unlike VisTune, it used images as queries and supported them with text to represent the searched music. Detailed explanations of these approaches are provided in Section 4.3.\n\n4.2 Dataset\n\nThis study, which was conducted online, sought to use publicly available datasets that did not present copyright problems to prevent the secondary use of the sound sources provided to the users. Therefore, we employed the open dataset MTG-Jamendo. The MTG-Jamendo dataset can be used for research purposes without copyright problems under Creative Commons licenses. The dataset comprises over 55,000 audio tracks, and each track is associated with multi-label information in 183 categories and is categorized into genres, instruments, and mood/themes. The genre category includes 87 tags, such as rock, pop, soundtrack, and jazz. The instrument category includes 40 tags, such as guitar, piano, and drums. The mood/theme category includes 56 tags, such as love, happy, energetic, and relaxing.\n\n4.3 Interface\n\nThe three designed music search systems for this study were developed to satisfy design goals while operating interactively. In addition, in order to allow users to easily access and use the systems online, prototypes of the systems were developed based on the open-source app framework, Streamlit [4]. The main interface features of each system are shown in Figures 1, 2, and 3, for a more detailed overview of the systems, see Figures 8 to 13 in the Appendix.\n\nCommon: All three systems offer guidelines for using the system. Furthermore, we aimed to create a system that can be used when users are unsure of what music they want to search for and do not know where to start looking for it. Therefore, we designed the system to provide scenarios, as listed in Table 2, for various situations or moods in advance so that users can visualize the music they want. These scenarios were based on prior research and statistics about why and when people listen to music [47, 60]. Users select one of the provided scenarios and then, using the information from their mental imagery about music in that scenario, input text or images, depending on the system.\n\nScenarios Looking for music to listen to while feeling tired but unable to sleep. Looking for music to listen to while doing my exercise. Looking for music to listen to while preparing for a party. Looking for music to discover new music. Looking for music to listen to while playing with a child (children). Looking for music to listen to while studying or working. Looking for music that matches my current mood on the way to work. Looking for music for listening to while driving. Looking for music to listen to in the early morning.\n\nTTTune: After a scenario is selected, the text-based TTTune allows users to input their search query in unstructured sentences using natural language. For instance, if a user selects a scenario like “Feeling tired but unable to sleep\" and envisions calming music that can alleviate bodily tension, they can input a search query such as “I want to listen to music with a tranquil and soothing mood that serves as background music to naturally relax my body.\" Once the query is entered, keywords that are automatically extracted from the user's input sentence are presented. Users can review these extracted keywords and, if they want to, opt to include additional tags from the predefined 56 mood/theme options. This procedure provides results that match the music users visualize. The 56 mood/theme tags are identical to the mood/theme tag labels used for classifying music in the MTG-Jamendo music dataset.\n\nVisTune: Although TTTune enables the expression of mental imagery through natural language input, VisTune takes a different approach by allowing users to input queries through the selection of an image that aptly represents their mental imagery. The researchers searched online for images that best depict the anticipated situations for each scenario. Subsequently, three to four representative images were selected for each scenario. During this process, care was taken to ensure that the images could encompass contrasting moods. For example, for a scenario related to exercise, images included both quiet exercises for body and mind training and powerful gym workouts. For a scenario related to early morning, images ranged from those conveying a calm and soothing atmosphere to those conveying a lively and cheerful one. This approach was taken to maximize the diversity of the range of situations associated with each scenario. Suppose a user selects a scenario of “Preparing for a party\" and envisions a cheerful gathering of friends for a birthday celebration, VisTune presents a few example images to choose the one that best represents this mental imagery. After an image is selected, VisTune automatically searches and displays up to 10 images that capture a vibe and situation that are similar to those of the selected image. If desired, users can select additional similar images to obtain results more closely aligned with the music they envision.\n\nVTTune: VTTune follows the same process as VisTune, and it starts with selecting a scenario and choosing a query image. However, while VisTune shows images similar to the selected query image and allows users to select additional images to better convey their mental imagery about the music, VTTune displays the tag information associated with the chosen query image. Subsequently, similar to TTTune, users can select additional mood/theme tags that they want to include from a predefined set of 56 tags, thereby enhancing the expression of their envisioned music.\n\n4.4 Algorithms\n\nThe algorithms for the three music search systems designed for this study operate as illustrated in Figure 4, and their descriptions are as follows:\n\nTTTune: When a user provides a natural language description of the desired music for search, a keyword extraction algorithm called KeyBERT is employed to extract keywords from the input sentences [28]. These extracted keywords must match the tag information in the music dataset used in this study. Therefore, a comparison between the extracted keywords and the tag information in the music dataset is carried out. If the extracted keywords are already present in the music tags, they are used as they are. However, if they are not present in the music tags, the keywords are replaced with words that are similar to the music tags based on the synonyms provided by the NLTK WordNet library [3]. For instance, “peaceful\" substitutions to “calm,\" and “unhappy\" substitutions to “sad.\" Ultimately, the entered tags and the substituted tags are combined to create the final tag selection.\n\nVisTune and VTTune: Both VisTune and VTTune employ the same algorithm. We utilize the image dataset constructed by Park et al. [55], which is a large image database with multi-label tag information, and we apply their proposed IMR algorithm. This algorithm ensures that the images and the music in the MTG-Jamendo dataset share identical tag information, which allows for them to be mapped to one another. When an input image is provided, the algorithm retrieves similar images from the constructed database and combines the tag information associated with those similar images to generate the final tags for the input image. The system operates by searching for music that has the same tags as do the generated final tags for the image. Park et al. only used the mood/theme tag information from the music dataset. However, for this study, the algorithm was modified to incorporate both the mood/theme and the genre tag information available in MTG-Jamendo in order to create a more detailed description of the music. VisTune and VTTune differ in terms of their interface and interaction methods. VisTune uses only images to search music and diversify tag information. In contrast, VTTune employs text, not images, to diversify the tag information.\n\n5 USER STUDY II\n\nTo explore the potential of using IMR systems in situations where the searched music is not clear, we compared the user experience and system usability of a text-based music search system (TTTune) with IMR systems (VisTune and VTTune). We examined the task-related aspects connected to the music search objective and the attractiveness of the interface to assess the suitability of the image modality for music searching.\n\n5.1 Participants\n\nIn this experiment, adults without auditory or visual impairments were selected as participants because they were required to listen to the searched music, and those VisTune and VTTune users had to select images. In addition, the experimental criteria for participants were that they had to be people who were familiar enough with digital devices to participate in the online experiment and complete the survey. Given that music is consumed across generations, we tried to recruit not only the younger generation (aged 19–39 years), but also individuals from older age groups to ensure a balanced generational range. The participants for the study were recruited through the online crowdsourcing platform Amazon Mechanical Turk and offline advertisements. We recruited 247 participants. However, excluding those who dropped out or completed the experiment in less than five minutes, leading to unreliable responses, only 236 participants (155 males and 81 females) were included in this study. The number of respondents for each system, TTTune, VisTune, and VTTune, was 80, 77, and 79, respectively. The participants’ ages ranged from 19 years to over 60 years, where 59.7% of participants were in the age range 19 to 39 years, and 40.3% were 40 years or older. Table 3 summarizes the participants’ demographic information.\n\nAttribute Variable range Sample size (N = 236) Gender Male 155 (65.7%) Female 81 (34.3%) Age 19-29 53 (22.4%) 30-39 88 (37.3%) 40-49 45 (19.1%) 50-59 29 (12.3%) Over 60 21 (8.9%) Ethnicity/Race American Indigenous 1 (0.4%) Asian 53 (22.5%) Black 18 (7.6%) Latinx/Hispanic 8 (3.4%) Middle Eastern/North African 1 (0.4%) Multi Race/Ethnicity 3 (1.3%) White/Caucasian 146 (61.9%) Not to disclose 6 (2.5%)\n\n5.2 Study Procedure\n\nThis study was conducted online. Participants were randomly assigned to one of the three prepared systems and were provided with a URL to access the online app. Upon accessing the provided URL, participants read an introductory text before proceeding to the main experiment. The introductory text explained the research procedure and methods, and participants were asked to click the Agree button if they agreed to participate. If they did not agree, they were excluded from the study. The experiment consisted of the following steps:\n\n1) Scenario selection: Participants choose their preferred scenario from a set of predefined scenarios describing situations or moods for which they may want to search for music. These scenarios describe a situation or mood associated with listening to music, such as “Looking for music that matches my current mood on the way to work\" or “Looking for music to listen to in the early morning.\" Nine scenarios were used in this study, as shown in Table 2. 2) Query Input: After selecting a preferred scenario, TTTune users input a text description elaborating on their chosen scenario. VisTune and VTTune users, on the other hand, select an image sample that best represents the chosen scenario. 3) Confirmation: After the text input or image selection is completed, TTTune users are shown the keywords extracted from their input text. VisTune users are presented with similar images that match the chosen mood or situation, based on the selected image. VTTune users can confirm the tag information associated with the selected image. 4) Addition: After the keywords are confirmed, TTTune and VTTune users can add additional tag information. VisTune users, after reviewing similar images, can also add supplementary images that further convey a similar vibe or situation as their chosen image. 5) Listening to retrieved music: Based on the input and additional information provided by the user, a set of five music tracks is presented as the search result. Participants listen to the presented music tracks and assess how well the retrieved music matches their mental imagery. 6) Iteration: Steps 1 to 5 are repeated a total of three times with different scenarios that are chosen by participants. 7) Survey response: After using the system three times, the participants respond to a survey in the final step. The survey measures are detailed in a later section. We also added an open response section to the survey so that participants could freely express their thoughts on the systems.\n\n5.3 Survey Measures\n\nTo explore the potential of our new search system using the image modality, we designed an experiment with a between-subject design to compare 1) the degree of match between user input queries and retrieved music across the designed systems (music suitability), 2) the overall user experience with the systems, and 3) the perceived usability of the systems. For this purpose, we employed widely used metrics in the HCI field, namely the user experience questionnaire (UEQ-S) and post-study usability questionnaire (PSSUQ), to compare the three systems in this study [40, 44, 45]. We examined the statistical significance through the analysis of variance (ANOVA) and post-hoc tests.\n\nAfter listening to five searched tracks, participants rated how well the music matched what they envisioned to measure music suitability. The study used a 7-point Likert scale, where 1 indicated “strongly disagree\" and 7 indicated “strongly agree.\" The experiment was repeated three times; thus, the music suitability assessment occurred three times. UEQ-S measures a total of eight items, and it examines both task-related pragmatic aspects and non-task-related hedonic aspects. Participants express their preference for one of the two presented words using a score from 1 to 7. For example, if 1 indicates “not interesting\" and 7 indicates “interesting,\" scores closer to 7 indicate that the experience of using the system is more interesting, whereas scores closer to 1 indicate that the experience is less interesting. PSSUQ comprises 16 items that are measured on a 7-point Likert scale, and they measure overall usability, system usefulness, information quality, and interface quality. In PSSUQ, 1 typically represents strong positive sentiments, whereas 7 represents strong negative sentiments. However, in our survey, the labels were changed so that higher scores indicate strong positivity [72]. Reverse scoring was used to maintain the original scale.\n\nThe UEQ-S and PSSUQ provide results from large benchmark datasets measured across various applications and websites [64, 65]. For example, UEQ-S can be compared to mean scores of 486 studies involving data from 21,175 people, enabling internal validation of the designed system. Therefore, we compared the three systems and provided benchmark scores as a reference to determine the level of each system's evaluation scores.\n\n5.4 Results\n\n5.4.1 Music matching suitability. The evaluation of the suitability of music matching by users for the TTTune, VisTune, and VTTune systems resulted in scores of 5.73, 4.44, and 4.89, respectively. During a total of three trials, participants mentally envisioned their desired music based on the scenarios they personally selected, and they expressed them using either text or images. Afterward, the matching suitability between the searched and envisioned music was evaluated, and these results were averaged for each system. The results showed that participants who used TTTune provided higher evaluations of music matching suitability compared with the other two systems. This could be attributed to the fact that using text as a medium allows users to express their desired music more elaborately. On the other hand, systems that use images as intermediaries for music search tend to convey intentions indirectly rather than directly. Therefore, the meaning that is intended may vary based on the selected image, and it may differ from the individual's envisioned musical imagery. This is closely related to the nature of images, as they can be subjectively interpreted, which leads to variations in how people perceive the same image [27, 57]. Due to these factors, VTTune, which supplements users’ desired imagery feelings with text-based tags, is likely to have higher music matching suitability than is VisTune.\n\n5.4.2 User experience evaluation. Table 4 presents the UEQ-S scores for TTTune, VisTune, and VTTune. The UEQ-S scores range from − 3 to + 3, where − 3 represents negative responses, 0 indicates neutral responses, and + 3 represents highly positive responses. From the results, we can see that the overall user experience score is highest for VTTune, followed by VisTune and then TTTune. The ANOVA found no significant difference in the overall evaluation of user experience (F(2, 233) = 2.443, p = .089) or non-task-oriented aspects, such as the system's appeal (F(2, 233) = .488, p = .615). However, a significant difference was found in the pragmatic quality items that examined task-oriented aspects (F(2, 233) = 8.031, p < .001, $\\eta _{p}^{2}$ = .064). These results were further analyzed using Scheffe's post-hoc analysis, finding a significant difference between the traditional text-based system TTTune and the image-based system VTTune for the pragmatic items (p < .001). This outcome indicates a difference in user experience in performing tasks between TTTune and VTTune. While no significant difference was found in hedonic quality unrelated to the task among the three systems, all three systems require improvement in the interface aspect. According to the feedback from participants, one male participant in his 40s (P17) who experienced VisTune stated, “It was good to be able to search for music quickly in an intuitive way, but the method of displaying images seems to need improvement.\" Some other participants also mentioned discomfort with the way images were displayed. It seems that further research is needed on how VisTune and VTTune can more efficiently present images.\n\nNevertheless, the systems showed a high user experience score overall, which is a highly encouraging result. When compared with the benchmark dataset, the overall scores of all systems exceeded the Benchmarks’ average UEQ-S score, as shown in Figure 5. In particular, VTTune's overall and pragmatic quality achieved a “Good\" score compared with the benchmark dataset. “Good\" here means that 10% of the evaluations of the pragmatic quality of the benchmark dataset are better than VTTune's UEQ-S, and 75% are worse. These findings indicate that the method of music search with images can provide users with a positive experience, and that the best user experience score is achieved when users combine familiar text-based approach with an image-based approach.\n\nTTTune VisTune VTTune p Score SD Score SD Score SD Pragmatic Quality 0.98 0.86 1.40 1.16 1.65 1.06 .000*** Hedonic Quality 1.03 0.91 0.92 1.05 1.05 1.03 .615 Overall 1.00 0.81 1.17 1.00 1.35 0.92 .089\n\n5.4.3 Usability evaluation of the system. The PSSUQ was measured to investigate the usability aspect of the system in more detail, and Table 5 lists the scores for each system. The closer the PSSUQ score is to 1, the more satisfactory it is. From the results, we can see that the systems that employ images as intermediaries, which are VisTune and VTTune, demonstrated higher satisfaction than did TTTune, which uses text alone as an intermediary. The ANOVA results revealed a significant difference in the overall assessment of system usability among the three systems (F(2, 233) = 5.661, p < .01, $\\eta _{p}^{2}$ = .046). Moreover, PSSUQ can be divided into subsets, which include system usefulness, information quality, and interface quality. When analyzed by subset, significant differences were observed among the three systems in system usefulness (F(2, 233) = 12.642, p < .001, $\\eta _{p}^{2}$ = .098), information quality (F(2, 233) = 3.572, p < .05, $\\eta _{p}^{2}$ = .030), and the scores were in the order of highest satisfaction: VTTune, VisTune, and TTTune. The results of the post-hoc analysis found a significant difference between TTTune and the IMR systems VisTune (p < .05) and VTTune (p < .05). The difference in system usefulness was noticeable, with significant differences between TTTune and VisTune (p < .001), and TTTune and VTTune (p < .001), indicating that the IMR systems provide higher satisfaction regarding system usability than TTTune.\n\nPSSUQ Classification Item number TTTune VisTune VTTune p Score SD Score SD Score SD Overall 1–16 2.41 0.85 2.02 0.79 2.02 0.88 .004** System Usefulness 1–6 2.40 0.87 1.82 0.80 1.81 0.87 .000*** Information Quality 7–12 2.48 0.93 2.16 0.85 2.14 0.91 .030* Interface Quality 13–15 2.37 0.97 2.15 1.16 2.22 1.29 .468\n\n*Lower PSSUQ score indicates higher usability.\n\nAccording to participant feedback, some found TTTune cumbersome because it sometimes requires entering lengthy text when they did not know the exact information about the music, similar to findings in previous studies [41, 74]. In contrast, participants considered VisTune and VTTune more convenient, as participants needed only to choose images that matched the mood or situation of their envisioned music. Differences were also evident in the overall experiment participation time. Participants using VTTune took about 20 minutes to complete the entire process, including the survey, while those using VisTune took 22 minutes, and those using TTTune took 33 minutes. This indicates that VTTune enabled the fastest music search, and systems that employed images as intermediaries offered a more intuitive and convenient search experience than did systems relying solely on text. Compared with the average scores of the benchmark dataset, all three systems demonstrated high overall usability in all categories, as is shown in Figure 6. Among the systems, the music search method in particular used images, which implies that it could offer users a convenient music search system.\n\n5.4.4 Comparison of results by age group. Although the systems were not designed for commercial application, but instead to explore the potential of a new music search method, all three systems received overall better evaluations on all aspects compared with the benchmark dataset's average scores, with respect to both user experience score and perceived usability. To delve deeper into the results by age group, we divided participants into two groups: those aged 19–39 years and those aged 40 years and above, and we compared their UEQ-S and PSSUQ scores. While there was no statistically significant difference, Table 6 reveals that all three systems generally provided a more positive experience in terms of user experience score for the 40 years and above age group than for the 19–39 years age group. As shown in Table 7, the usability score for the systems differed depending on the system. In the case of TTTune, the 19–39 years age group rated higher than the 40 years and above age group. Although VTTune did not exhibit notable differences between age groups for PSSUQ scores, VisTune showed relatively higher satisfaction from the 40 years and above age group. There was also an opinion related to this in the participant feedback. One female participant in her 60s (P57) who used VisTune provided the feedback, “I would love to continue using this system because it's easy to express my mood with pictures.\" Another male participant aged 60 and above (P62) said, “Searching for music is not easy, but it was convenient, especially for older people like me, because you only need to select pictures that match your current mood.\" Through these results, it can be inferred that image-based music search, such as VisTune, could be particularly useful for users aged 40 years and above.\n\nTTTune (N = 80)\n\n19–39 y (N = 48)\n\n40 y and above (N = 32)\n\nVisTune (N = 77)\n\n19–30 y (N = 46)\n\n40 y and above (N = 31)\n\nVTTune (N = 79)\n\n19–30 y (N = 47)\n\n40 y and above (N = 32)\n\nPragmatic Quality 0.98 0.84 (↓) 1.17 (↑) 1.40 1.34 (↓) 1.50 (↑) 1.65 1.69 (↑) 1.59 (↓) Hedonic Quality 1.03 0.93 (↓) 1.16 (↑) 0.92 0.89 (↓) 0.98 (↑) 1.05 0.95 (↓) 1.20 (↑) Overall 1.00 0.89 (↓) 1.17 (↑) 1.17 1.12 (↓) 1.23 (↑) 1.35 1.32 (↓) 1.40 (↑)\n\nPSSUQ Classification\n\nTTTune (N = 80)\n\n19–39 y (N = 48)\n\n40 y and above (N = 32)\n\nVisTune (N = 77)\n\n19–39 y (N = 46)\n\n40 y and above (N = 31)\n\nVTTune (N = 79)\n\n19–39 y (N = 47)\n\n40 y and above (N = 32)\n\nOverall 2.41 2.32 (↓) 2.55 (↑) 2.02 2.04 (↑) 2.01 (↓) 2.02 2.00 (↓) 2.06 (↑) System Usefulness 2.40 2.28 (↓) 2.57 (↑) 1.82 1.87 (↑) 1.74 (↓) 1.81 1.73 (↓) 1.93 (↑) Information Quality 2.48 2.40 (↓) 2.59 (↑) 2.16 2.16 (−) 2.15 (↓) 2.14 2.14 (−) 2.14 (−) Interface Quality 2.37 2.30 (↓) 2.48 (↑) 2.15 2.12 (↓) 2.20 (↑) 2.22 2.25 (↑) 2.17 (↓)\n\n*Lower PSSUQ score indicates higher usability.\n\n7 DISCUSSION\n\nIn this section, we present guidelines for designing the user interface of the IMR systems and discuss how the quality and specificity of input queries affect performance. We also examine the limitations of this study and future work.\n\n7.1 Interface Design for Image-based Music Search\n\nConsidering the feedback provided by participants and the opinions given in the interviews, it can be concluded that participants desired a variety of image selection options with a simple format in the IMR systems. Moreover, the participants preferred viewing larger images one by one rather than viewing multiple images at once. When we considered this insight, we concluded that applying a swiping mechanism for quick image navigation could prove advantageous from an interface design perspective [19, 25].\n\nIn addition, we presented image selection options to facilitate the intuitive expression of the desired music atmosphere through chosen images, which eliminated the time spent searching for images to use as input. Although input images were provided to users in a supportive way, previous research results suggest that users might be less proactive when pre-suggested inputs are available [43]. Therefore, in the future, it may be necessary to consider providing users with the option to directly upload their own desired images in addition to offering a supportive approach. Another approach to consider could involve utilizing images from the surrounding environment that are automatically captured through a user device.\n\nIn this study, participants aged 60 years and above who took part in the interviews experienced difficulties in searching for music with the use of music streaming services [67]. In general, older adults take longer to complete tasks and use fewer apps than do younger adults due to cognitive decline [26]. Taking these factors into consideration, IMR systems like VisTune could be appealing to seniors, so it is necessary to consider developing a system that is specialized for them. Additionally, efforts should be made to accommodate users who face physical difficulties with inputting text. When systems are specialized for seniors who struggle with music search and for individuals who face physical difficulties with inputting text, the following considerations should be considered: enlarge image sizes to ensure users can perceive images without difficulty and increase the margin between images to reduce click errors [5, 16, 22]. Moreover, the simplification and intuitive design of the interface should be considered, as previous studies have shown that conveying too much information on a single page is not effective for older adults [52, 58]. However, as preferences for interface design may differ among cultural groups [30], further in-depth research on the design aspect will be necessary to develop systems that are specialized for older adults in the future.\n\n7.2 Factors Influencing the Performance and User Satisfaction with Image-based Music Search\n\nThe satisfaction with music search results can vary depending on the input query quality. Just as search results can be more closely aligned with users’ desired music when the meaning of the text used as a query is accurately understood [10], search performance can be enhanced when the meaning extracted from an image is universally accepted. Unlike text, images can be interpreted in various and subjective ways by different people [27, 57]. Therefore, it may be helpful to use an image as a query in which the meaning can be extracted within an expected range by the public to increase the accuracy of the music search. Thus, providing users with previously prepared image examples can help them quickly envision the music they want and positively affect the management of the search result quality. However, to embrace users’ subjective interpretations and diversity, a hybrid method, such as VTTune, which uses the image as a query but supplements it with text, could be effective. This method includes a complementary process in which the system ensures that the meaning it extracts from the entered query matches what the user wants. Considering that user experience and system usability satisfaction for VTTune scored the highest in this study, a method similar to VTTune is a rational approach for employing IMR systems. Thus, the performance and satisfaction of image-based music search can vary depending on how users specifically visualize music and use queries that clearly express it. This principle is applicable to IMR and other music search systems [9].\n\n7.3 Limitations and Future Work\n\nOur study has several limitations. First, there is a need to enhance and personalize the image tagging method. It is necessary not only to improve the extraction of implied meanings within images but also to develop methods that are capable of extracting personalized tag information. Second, the interface functionality and the optimization of the three systems used in this study need improvement. When the app prototypes were designed for the experiments, we employed open-source libraries. Although these libraries provide various user interface components, there were limitations in the customizability of certain elements. Therefore, when we implemented the interfaces of the three systems that we created for this study, there were challenges in customizing features such as the arrangement of images or music lists, and in fine-tuning font sizes. Third, in this study, pre-designed scenarios were provided to assist users in visualizing their target music. Although these scenarios were based on previous studies and statistics related to why and when people listen to music, they do not encompass all possible cases [47, 60]. Thus, it is difficult to analyze in which specific scenario the IMR systems will be effective, which is a limitation. It is necessary to expand the experiment in the future based on more diverse scenarios.\n\nFor future work, we plan to focus more on specific age groups or scenarios to examine the effectiveness of the IMR systems and intend to conduct comparisons involving a wider range of retrieval methods and various measures. Furthermore, designing music listening experiences with the innovative approach of directly generating the music users want to hear using the emerging generative models would also be enlightening [15, 46, 49, 69]."
    }
}
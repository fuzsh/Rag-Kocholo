{
    "id": "dbpedia_3790_1",
    "rank": 68,
    "data": {
        "url": "https://docs.dynatrace.com/docs/observe-and-explore/data-observability",
        "read_more_link": "",
        "language": "en",
        "title": "Data Observability",
        "top_image": "https://docs.dynatrace.com/docs/icons/favicon.ico",
        "meta_img": "https://docs.dynatrace.com/docs/icons/favicon.ico",
        "images": [
            "https://dt-cdn.net/images/example-pipeline-observability-1920-e83716815b.png",
            "https://dt-cdn.net/images/predicted-volume-1920-f013e84118.png"
        ],
        "movies": [],
        "keywords": [],
        "meta_keywords": [
            ""
        ],
        "tags": null,
        "authors": [
            "Unknown"
        ],
        "publish_date": "2023-12-05T00:00:00+00:00",
        "summary": "",
        "meta_description": "Learn about how to observe the most important meta information about observed data by using Dynatrace capabilities.",
        "meta_lang": "",
        "meta_favicon": "/docs/icons/favicon.ico",
        "meta_site_name": "Dynatrace Docs",
        "canonical_link": "https://docs.dynatrace.com/docs/observe-and-explore/data-observability",
        "text": "Dynatrace is well-positioned to take advantage of the exponential rise in data generation. The rise, however, when paired with the multiple possibilities of data ingestion into Grail, brings new challenges in data management. One of those is the need to ensure the availability, reliability, and quality of dataâa concept known as Data Observability.\n\nData Observability, borrowing ideas from Software Observability, pertains to the ability to understand the full data lifecycle in an organization. It involves monitoring and managing the internal state of data systems from ingestion to storage and usage. It is about gaining insight into the data pipeline, understanding how data evolves, and identifying any issues that could compromise its integrity or reliability.\n\nDynatrace functionality addresses many issues you can experience around the health, quality, freshness, and general usefulness of externally sourced data in Grail, enabling you to make better-informed decisions and optimize your efforts in digitalization and data-driven operations.\n\nFive pillars of data observability\n\nHaving been a frontrunner in handling large amounts of data for nearly two decades, Dynatrace defines the following five pillars of Data Observability.\n\nFreshness\n\nFreshness refers to the timeliness of the data. In an ideal data ecosystem, all data is as current as possible. Observing the freshness of data helps to ensure that decisions are based on the most recent and relevant information.\n\nDistribution\n\nDistribution is the statistical spread or range of the data. Data distribution is essential in identifying patterns, outliers, or anomalies. Deviation from the expected distribution can signal an issue in data collection or processing.\n\nVolume\n\nVolume is the quantity of data generated or processed during a certain time period. Unexpected increases or drops in the data volume are usually good indicators of undetected issues. The volume aspect often overlaps with data freshness. At times, it's challenging to determine whether the expected but missing data will arrive later or if it's permanently lost.\n\nSchema\n\nSchema is the structure or format of the data, including the data types and relationships between different entities. Observing the schema can help identify and flag unexpected changes, like the addition of new fields or the removal of existing ones.\n\nLineage\n\nLineage refers to the journey of the data through a system. It provides insights into where the data comes from (upstream) and what it affects (downstream). Data lineage plays a crucial role in root cause analysis as well as informing impacted systems about an issue as quickly as possible.\n\nData observability best practices\n\nData collected and analyzed within Grail falls into two categories:\n\nThis section introduces best practices that encompass all five Data Observability pillars.\n\nObserve data pipelines\n\nAs a first step for externally sourced data, the quality and availability of a data pipeline must be ensured.\n\nData records are collected from various sources and data pipelines. Various data sources operate so-called Extract Transform and Load (ETL) processes. These processes collect raw data, transform and normalize it, and then load the resulting datasets into a data lakehouse such as Grail.\n\nETL processes often run in batch processing mode and can vary in quality and stability. As the stable operation of such ETL processes is the basis for data quality in terms of completeness, it's essential to observe the availability and performance of all the critical data pipeline steps.\n\nAs a leading observability and security platform, Dynatrace provides comprehensive monitoring capabilities to ensure data pipeline stability.\n\nDepending on your pipeline's deployment, you can utilize the OneAgent for full stack server monitoring or opt for the cloud-native deployment to seamlessly monitor pipelines in the cloud, containers, or Kubernetes.\n\nThe image below shows a dashboard observing the health of our own data pipeline:\n\nDetect anomalies in data volume\n\nYour data pipeline continuously pushes batches of data records into Grail. Those batches contain various data, such as log lines, events, or metric measurements. The most important characteristics of your incoming data records are the volume over time and the ingestion frequency.\n\nDynatrace anomaly detection monitors these critical parameters and automatically raises alerts, promptly informing the data engineering teams when the ingestion pattern goes off the baseline.\n\nDynatrace Notebooks help you to explore all incoming data records. With a right DQL query, you can filter and aggregate your data to focus on specific data subsets. You can even get a prognosis of the expected future load via Davis forecast, which uses the current load and presents the lowest and the highest probabilistic level as well as the predicted value for the future.\n\nThe image below shows a forecast of the expected number of future records created by Davis.\n\nDetect anomalies in a data schema\n\nObserving the schema can help identify and flag unanticipated changes, such as the addition of new fields or deletion of existing ones.\n\nChanges in data schema can have the same negative result as a data loss, as derived aggregates such as charts, dashboards, and long-term reports rely on a specific data format. If the schema changes, either by dropping an important field or changing a field's name, all field-dependent data integrations break.\n\nDetect anomalies in data freshness\n\nIn an optimal data ecosystem, the goal is to have data as current as possible, with all data records flowing into the pipeline instantly in near real-time.\n\nThis means that the delay between the data capture on the data source and data storage in Grail should be minimized. Observing the freshness of data helps to ensure that decisions are based on the most recent and relevant information.\n\nAn example here is a metric measurement of CPU on a host recorded at 12:00:00. If the data pipeline needs 60 seconds to process and store the metric measurement, the earliest point in time when you can query the same record is at 12:01:00. The data freshness in that example is measured as 1 minute, called near real-time.\n\nObserve data distribution\n\nThe distribution of data is essential in identifying patterns, outliers, or anomalies in the data. Deviation from the expected distribution can signal an issue in data collection or processing.\n\nGrail offers a dedicated DQL commandâfieldsSummaryâthat enables you to explore the distribution of data record field values of any type. It calculates the cardinality of field values that the specified fields have.\n\nUse cases"
    }
}
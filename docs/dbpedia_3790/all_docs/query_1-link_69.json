{
    "id": "dbpedia_3790_1",
    "rank": 69,
    "data": {
        "url": "https://imply.io/whitepapers/a-data-teams-guide-to-real-time-analytics-for-apache-kafka/",
        "read_more_link": "",
        "language": "en",
        "title": "A Data Team's Guide to Real-time Analytics for Apache Kafka®",
        "top_image": "https://imply.io/wp-content/uploads/2023/07/druid_1920x1080-7-1800x1013.png",
        "meta_img": "https://imply.io/wp-content/uploads/2023/07/druid_1920x1080-7-1800x1013.png",
        "images": [
            "https://imply.io/wp-content/themes/gc-responsive-v5/images/logo-imply.svg",
            "https://imply.io/wp-content/themes/gc-responsive-v5/images/featured-fpo.jpg",
            "https://imply.io/wp-content/uploads/2023/07/real-time-use-cases.png",
            "https://imply.io/wp-content/uploads/2023/07/slide-2.png",
            "https://imply.io/wp-content/uploads/2023/07/slide-1.png",
            "https://imply.io/wp-content/uploads/2023/07/slide-5.png",
            "https://imply.io/wp-content/uploads/2023/07/tech-graphic-1.png",
            "https://imply.io/wp-content/uploads/2023/07/tech-graphic-2.png",
            "https://imply.io/wp-content/uploads/2023/07/tech-graphic-3.png",
            "https://imply.io/wp-content/uploads/2023/07/slide-9.png",
            "https://imply.io/wp-content/uploads/2023/07/netflix.png",
            "https://imply.io/wp-content/uploads/2023/07/slide-11.png",
            "https://imply.io/wp-content/uploads/2023/07/slide-12.png",
            "https://imply.io/wp-content/themes/gc-responsive-v5/images/logo-imply-white.svg",
            "https://px.ads.linkedin.com/collect/?pid=3080266&fmt=gif",
            "https://px.ads.linkedin.com/collect/?pid=3080266&fmt=gif"
        ],
        "movies": [],
        "keywords": [],
        "meta_keywords": [
            ""
        ],
        "tags": null,
        "authors": [],
        "publish_date": "2023-07-25T18:05:48+00:00",
        "summary": "",
        "meta_description": "This whitepaper provides an overview of technical considerations when analyzing streaming data along with an in-depth overview of Apache Druid® differentiators for real-time analytics on Kafka data",
        "meta_lang": "en",
        "meta_favicon": "https://imply.io/wp-content/uploads/2023/05/favicon-96x96-imply-data.png",
        "meta_site_name": "Imply",
        "canonical_link": "https://imply.io/whitepapers/a-data-teams-guide-to-real-time-analytics-for-apache-kafka/",
        "text": "Challenges in analyzing streaming data at scale\n\nIn this era of data ubiquity, large-scale streaming data poses a unique set of challenges, which data teams need to address effectively to derive real-time insights and value. Let’s explore some of these challenges in detail.\n\nData Freshness\n\nFor certain real-time data, its value depends on how quickly it can be analyzed and acted upon. This time sensitivity requires systems to process and analyze data with minimal latency. Analyzing data and delivering insights while still “fresh” means ensuring consistent, sub-second query response times under high load.\n\nReal-time in Context\n\nIn many use cases, it is valuable to combine stream data with historical data for real-time contextual analysis. Most databases are not designed to ingest and align both real-time and historical data while delivering subsecond performance. Streaming data arrives in high volumes and at a fast pace, while historical data is larger but not real-time. Integrating the two types of data requires handling varying formats, varying schemas, and ensuring data consistency.\n\nScalability and Elasticity\n\nAs data volumes grow, a database must scale seamlessly, either vertically (by adding more power to a single node) or horizontally (by adding more nodes). This scalability should be dynamic to handle peak loads efficiently without over-provisioning resources during non-peak times.\n\nFault Tolerance\n\nMany real-time use cases require a system designed to run continuously without planned downtime for any reason, even for configuration changes and software updates. Ensuring a database is durable and will not lose data, even in the event of major system failures, is a non-trivial task.\n\nFig 1: Real-time use cases and applications\n\nVariety\n\nData diversity is another common challenge. Streaming data often comes in various formats, from different sources, with diverse schemas to serve a variety of workloads. It can be structured or semi-structured data from sources such as logs, sensors, user interactions, and transactions. This variety necessitates flexible and adaptable tools to manage and analyze the data.\n\nSecurity and Compliance\n\nEnsuring data security, privacy, and compliance with regulatory norms is an unavoidable requirement, which only becomes more difficult as data volume and velocity increase. This requirement is not only a technological challenge but also involves policy, process, and personnel considerations.\n\nStream-to-batch vs. stream-to-stream: Key differences and considerations\n\nWhen selecting how to house data in an organization, there are two key approaches to consider: batch processing, or ‘stream-to-batch’, and real-time data processing, or ‘stream-to-stream’.\n\nBatch processing involves analyzing and processing a set of data that has already been stored. Examples include payroll and billing systems, where data is ingested periodically. This is the status quo of data management—and it’s perfectly suitable for periodic BI reporting purposes.\n\nReal-time data processing, on the other hand, involves analyzing and reporting events as they occur. This approach allows for near real-time analysis and is crucial for time-sensitive applications where immediate actions are required. For example, a real-time application that purchases a stock within 20 milliseconds of receiving a desired price, or a programmatic advertising system that places an ad within 300 milliseconds of a user opening a webpage.\n\nStream-to-batch architecture\n\nIn a stream-to-batch architecture, the incoming streaming data is first collected and stored in files, which are small subsets of the data stream. These files are then processed and analyzed using batch processing techniques (sometimes called “micro-batch” processing when there are many small files to ingest). The continuous data stream is divided into chunks that are processed sequentially.\n\nFig 2: Example of a steam-to-batch architecture for traditional BI and reporting use cases\n\nStream-to-batch architectures are commonly employed in systems that use distributed data processing frameworks, such as Snowflake or ClickHouse, which lack real-time ingestion and require micro-batch processing. This approach can be more straightforward to implement, as it leverages existing batch processing techniques and can manage the complexity of data processing and analysis effectively.\n\nHowever, stream-to-batch architectures do not provide the lowest possible latency, as there is an inherent delay introduced by batch processing. Additionally, this approach may require more memory and storage resources, as data must be buffered before processing. In essence, while the idea of using a cloud data warehouse or other batch data processing system to serve both batch-oriented and real-time use cases might sound efficient, doing so defeats the purpose of a streaming pipeline.\n\nStream-to-stream architecture\n\nIn a stream-to-stream architecture, each event in incoming streaming data is processed and made available for analysis as it arrives, without dividing it into micro-batches. This approach is also known as event-driven ingestion or true stream ingestion. In this architecture, the data is ingested on-the-fly, allowing for real-time analytics with minimal latency.\n\nFig 3: Example of a stream-to-stream architecture for real-time use cases\n\nStream-to-stream architectures are typically employed in systems that require high-throughput, low-latency processing and need to handle millions of events per second and beyond.\n\nStream-to-stream architectures offer several advantages, such as lower latency and subsecond query response times. However, it’s often useful to combine real-time stream data with historical data—and this is where Druid is differentiated from other data stores in its ability to combine streaming and historical data for real-time analysis. The combination of real-time and historical capabilities in Druid ensures that queries covering any time frame can be quickly resolved. Druid-powered applications can seamlessly retrieve insights that span the complete data timeline, whether it’s the most recent events or past data.\n\nThe Apache Kafka to Apache Druid architecture\n\nDruid provides a connector-free integration with Kafka and handles the latency, consistency, and scale requirements for high-performance analytics on large-scale streaming data cost-effectively. Here’s how it works:\n\nAt the beginning of the pipeline, event data producers use Kafka’s scalable feature—partitions—to send any number of messages (from zero to millions) per second to each topic.\n\nDruid picks up from there by creating one or more tasks that consume data from one or more Kafka partitions in the topic. This process is carried out automatically by Druid, which distributes all the partitions in a topic across the tasks of a job. Each task is responsible for consuming messages from its assigned Kafka partitions. Events in a topic are immediately available for analysis in Druid and are treated the same as historical data by queries.\n\nAs a background process, Druid indexes the newly-arrived data and produces segment files. These files are then published to both query nodes (known in Druid as “historical” processes) and immutable deep storage. Once the data is confirmed into segments, it can be safely removed from its Kafka topic.\n\nFig 4: Druid’s stream-native integration with Kafka\n\nWhat’s unique about this setup is that when a query is processed by Druid, if the time filter includes the most recent events, Druid will automatically connect with the real-time tasks to handle the part of the query related to the timeframe they have in memory. Simultaneously, the query engine communicates with the Historical processes to deal with the portions of the query that pertain to “past” data (data that has already been ingested).\n\nThis dual approach provides a seamless experience for applications that build queries for any specific time frame. Developers no longer need to distinguish between real-time streaming data and past data. Druid takes care of this automatically, providing analytics across any portion of the timeline.\n\nThe inherent parallelism of Druid provides scalability both in data ingestion and data queries, producing both high performance for each query and high concurrency across simultaneous queries.\n\nTechnical benefits of using Druid for Kafka data\n\nEvent-based ingestion\n\nDruid is uniquely built for analyzing streams in context. Unlike systems that rely on periodic batch processing, Druid’s event-based ingestion enables data to be ingested and processed as soon as events occur. As a real-time analytics database, it is designed to enable users to query every event as it joins the data stream and do it at a tremendous scale while enabling subsecond queries on a mix of stream and batch data.\n\nThis is made possible by Druid’s native connectivity with Kafka, which allows for the ingestion of data on an event-by-event basis without the need for a connector.\n\nQuery-on-arrival\n\nAs events are ingested into Druid from Kafka, they are held in memory and made immediately available for analytical queries. In the background, the events are indexed and committed to longer-term storage. Druid provides instantaneous access to streaming data, enabling real-time insights and facilitating timely decision-making and rapid response to changing conditions.\n\nFig 5: Druid’s built-in indexing service for Kafka enables event-based ingestion for real-time analytics\n\nThe real-time nature of query-on-arrival enables organizations to monitor and analyze streaming data in real time, detecting patterns, anomalies, and trends as they occur. It facilitates applications such as real-time dashboards, alerting systems, and operational analytics, where immediate insights are crucial.\n\nBy combining the scalability and low-latency ingestion capabilities of Druid with the continuous data stream from Kafka, query-on-arrival eliminates the need to wait for batch processing or data updates, enabling timely decision-making and rapid response to changing conditions.\n\nHigh EPS scalability\n\nDruid’s real-time ingestion scales equivalently to Kafka, so you can confidently scale up to millions of events per second (EPS). Similar to Kafka’s scalability using partitions, Druid distributes partitions across tasks to handle high message volumes. The pipeline starts at event data producers, leveraging Kafka’s scalability to push messages to a topic. Druid automatically creates tasks consuming from topic partitions, indexing the data, and publishing segment files to deep storage.\n\nFig 6: Druid features an elastic architecture with independently scalable components for ingestion, queries, and orchestration\n\nOne key aspect to note is how queries in Druid handle real-time and historical data at scale. When a query includes the most recent events, it interacts with real-time tasks, which hold relevant data in memory. Simultaneously, the query engine communicates with historical processes to resolve the parts related to already ingested “past” data. This integrated approach provides a seamless experience for application developers, eliminating concerns about differentiating real-time and historical data. Druid effortlessly resolves the query across the entire timeline, enabling analytics for any timeframe.\n\nThe parallelism and integration between real-time ingestion and historical data empower scalability in handling both streaming traffic and application user concurrency, making Druid an efficient and scalable database.\n\nAutomatic schema discovery\n\nDruid stands out as the first analytics database to offer the performance of a strongly-typed data structure with the flexibility of a schemaless data structure.\n\nWith schema auto-discovery, Druid automatically discerns the fields and types of data ingested, updating tables to align with the evolving data. An added advantage is that as the data schemas undergo changes—additions, removals, or alterations in data types or dimensions—Druid intuitively recognizes these changes and adjusts its tables to correspond to the new schema, eliminating the need to reprocess pre-existing data.\n\nSchema auto-discovery simplifies the process of handling ever-changing event data from Kafka. It provides the performance benefits of a strongly-typed data structure while offering the flexibility and ease of ingestion associated with a schemaless data structure to enhance operational efficiency, reduce administrative burden, and ensure that Druid adapts seamlessly to evolving data schemas.\n\nGuaranteed consistency\n\nDruid enables reliable and accurate data processing through the combination of its Kafka Indexing Service and the inherent features of Kafka. When data in a Kafka topic has been committed to a Druid segment, it is processed as consumed from Kafka, Druid’s indexing service keeps track of the consumed offsets and commits them to Kafka, indicating successful processing.\n\nFig 7: A common duplication of data scenario that Druid can automatically handle\n\nIn the event of failures or interruptions during ingestion, Druid’s Kafka indexing service utilizes Kafka’s offset management and commit features to achieve exactly-once semantics. It ensures that data is ingested exactly once, without duplicates or data loss. If a failure occurs, the indexing service can resume ingestion from the last committed offset, ensuring no data is skipped or processed multiple times.\n\nThis ensures the reliability and accuracy of the ingested data, which is a key requirement for mission-critical applications.\n\nContinuous backup\n\nWhen ingesting data from Kafka, Druid effectively creates a backup of the data continuously in conjunction with the Kafka indexing service. It continuously persists ingested events into deep storage, such as an object store like Amazon S3, to enable zero data loss in the event of a system failure.\n\nAs Druid is persisting new segments incrementally, it is, therefore, unnecessary for an administrator to create backups of the data set, removing a major source of risk and costs that are required to operate most databases.\n\nIn case of failures, such as hardware outages or accidental data corruption, Druid enables easy and fast recovery. It’s as simple as watching Druid automatically rehydrate data nodes from deep storage. Because of the built-in Kafka Indexing Service, Druid tracks the Kafka off-set so it can start ingesting events where it left off, ensuring no data loss and a zero recovery point objective.\n\nGetting data from Kafka to Druid\n\nSetting up Druid to ingest data from Kafka is very simple. First, you’ll need to set up ingestion for each Kafka topic you want. By default, Druid will create a table where each event key is a dimension (a column in the table). You can specify how to parse the data, which information to ingest, and, if you prefer, how to rollup the data, such as one row per second or one row per 15 minutes.\n\nNative connectivity\n\nDruid has native connectivity with Kafka, which allows real-time ingestion of streaming data without the need for a Kafka connector. Unlike databases that use a Kafka connector, there is no work or management required to maintain a connector in Druid.\n\nDruid includes the Kafka Indexing Service. This service acts as a Kafka consumer, directly subscribing to Kafka topics and receiving data in real time. It maintains a connection with Kafka brokers, continually fetching messages from the specified Kafka topics.\n\nWhen data is received from a Kafka topic, the Kafka Indexing Service processes the messages, transforms them into Druid’s internal data format, and indexes them into Druid’s data stores. This means streaming data is made immediately available for querying and analysis. Simply define an ingestion spec with “type”: “kafka” that defines the topic and parameters you want, and Druid handles the rest.\n\nMonitoring data consumption from Kafka\n\nOnce you’ve set up Druid to ingest data from Kafka, you generally don’t need to do much to keep it running. This is because Druid employs a Kafka supervisor, which is a type of indexing service that manages the Kafka indexing tasks.\n\nThe Kafka supervisor is responsible for managing and monitoring Kafka indexing tasks and ensuring that they are always consuming data. This means the supervisor will create new tasks as needed (in response to failures, scaling needs, task duration/time limits, and such), monitor the state of running tasks, and manage the data replication."
    }
}
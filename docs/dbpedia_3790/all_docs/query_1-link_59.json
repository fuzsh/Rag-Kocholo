{
    "id": "dbpedia_3790_1",
    "rank": 59,
    "data": {
        "url": "https://learn.microsoft.com/en-us/azure/data-explorer/kusto/query/time-series-analysis",
        "read_more_link": "",
        "language": "en",
        "title": "Analyze time series data - Kusto",
        "top_image": "https://learn.microsoft.com/en-us/media/open-graph-image.png",
        "meta_img": "https://learn.microsoft.com/en-us/media/open-graph-image.png",
        "images": [
            "https://learn.microsoft.com/en-us/azure/data-explorer/kusto/query/media/time-series-analysis/time-series-partition.png?view=azure-data-explorer",
            "https://learn.microsoft.com/en-us/azure/data-explorer/kusto/query/media/time-series-analysis/time-series-filtering.png?view=azure-data-explorer",
            "https://learn.microsoft.com/en-us/azure/data-explorer/kusto/query/media/time-series-analysis/time-series-regression.png?view=azure-data-explorer",
            "https://learn.microsoft.com/en-us/azure/data-explorer/kusto/query/media/time-series-analysis/time-series-seasonality.png?view=azure-data-explorer",
            "https://learn.microsoft.com/en-us/azure/data-explorer/kusto/query/media/time-series-analysis/time-series-operations.png?view=azure-data-explorer",
            "https://learn.microsoft.com/en-us/azure/data-explorer/kusto/query/media/time-series-analysis/time-series-at-scale.png?view=azure-data-explorer",
            "https://learn.microsoft.com/en-us/azure/data-explorer/kusto/query/media/time-series-analysis/time-series-top-2.png?view=azure-data-explorer"
        ],
        "movies": [],
        "keywords": [],
        "meta_keywords": [
            ""
        ],
        "tags": null,
        "authors": [],
        "publish_date": "2024-08-12T12:18:00+00:00",
        "summary": "",
        "meta_description": "Learn how to analyze time series data.",
        "meta_lang": "en",
        "meta_favicon": "",
        "meta_site_name": "",
        "canonical_link": "https://learn.microsoft.com/en-us/kusto/query/time-series-analysis?view=microsoft-fabric",
        "text": "Time series analysis\n\nApplies to: â Microsoft Fabric â Azure Data Explorer â Azure Monitor â Microsoft Sentinel\n\nCloud services and IoT devices generate telemetry data that can be used to gain insights such as monitoring service health, physical production processes, and usage trends. Performing time series analysis is one way to identify deviations in the pattern of these metrics compared to their typical baseline pattern.\n\nKusto Query Language (KQL) contains native support for creation, manipulation, and analysis of multiple time series. In this article, learn how KQL is used to create and analyze thousands of time series in seconds, enabling near real-time monitoring solutions and workflows.\n\nTime series creation\n\nIn this section, we'll create a large set of regular time series simply and intuitively using the make-series operator, and fill-in missing values as needed. The first step in time series analysis is to partition and transform the original telemetry table to a set of time series. The table usually contains a timestamp column, contextual dimensions, and optional metrics. The dimensions are used to partition the data. The goal is to create thousands of time series per partition at regular time intervals.\n\nThe input table demo_make_series1 contains 600K records of arbitrary web service traffic. Use the following command to sample 10 records:\n\ndemo_make_series1 | take 10\n\nThe resulting table contains a timestamp column, three contextual dimensions columns, and no metrics:\n\nTimeStamp BrowserVer OsVer Country/Region 2016-08-25 09:12:35.4020000 Chrome 51.0 Windows 7 United Kingdom 2016-08-25 09:12:41.1120000 Chrome 52.0 Windows 10 2016-08-25 09:12:46.2300000 Chrome 52.0 Windows 7 United Kingdom 2016-08-25 09:12:46.5100000 Chrome 52.0 Windows 10 United Kingdom 2016-08-25 09:12:46.5570000 Chrome 52.0 Windows 10 Republic of Lithuania 2016-08-25 09:12:47.0470000 Chrome 52.0 Windows 8.1 India 2016-08-25 09:12:51.3600000 Chrome 52.0 Windows 10 United Kingdom 2016-08-25 09:12:51.6930000 Chrome 52.0 Windows 7 Netherlands 2016-08-25 09:12:56.4240000 Chrome 52.0 Windows 10 United Kingdom 2016-08-25 09:13:08.7230000 Chrome 52.0 Windows 10 India\n\nSince there are no metrics, we can only build a set of time series representing the traffic count itself, partitioned by OS using the following query:\n\nlet min_t = toscalar(demo_make_series1 | summarize min(TimeStamp)); let max_t = toscalar(demo_make_series1 | summarize max(TimeStamp)); demo_make_series1 | make-series num=count() default=0 on TimeStamp from min_t to max_t step 1h by OsVer | render timechart\n\nUse the make-series operator to create a set of three time series, where:\n\nnum=count(): time series of traffic\n\nfrom min_t to max_t step 1h: time series is created in 1-hour bins in the time range (oldest and newest timestamps of table records)\n\ndefault=0: specify fill method for missing bins to create regular time series. Alternatively use series_fill_const(), series_fill_forward(), series_fill_backward() and series_fill_linear() for changes\n\nby OsVer: partition by OS\n\nThe actual time series data structure is a numeric array of the aggregated value per each time bin. We use render timechart for visualization.\n\nIn the table above, we have three partitions. We can create a separate time series: Windows 10 (red), 7 (blue) and 8.1 (green) for each OS version as seen in the graph:\n\nTime series analysis functions\n\nIn this section, we'll perform typical series processing functions. Once a set of time series is created, KQL supports a growing list of functions to process and analyze them. We'll describe a few representative functions for processing and analyzing time series.\n\nFiltering\n\nFiltering is a common practice in signal processing and useful for time series processing tasks (for example, smooth a noisy signal, change detection).\n\nThere are two generic filtering functions:\n\nseries_fir(): Applying FIR filter. Used for simple calculation of moving average and differentiation of the time series for change detection.\n\nseries_iir(): Applying IIR filter. Used for exponential smoothing and cumulative sum.\n\nExtend the time series set by adding a new moving average series of size 5 bins (named ma_num) to the query:\n\nlet min_t = toscalar(demo_make_series1 | summarize min(TimeStamp)); let max_t = toscalar(demo_make_series1 | summarize max(TimeStamp)); demo_make_series1 | make-series num=count() default=0 on TimeStamp from min_t to max_t step 1h by OsVer | extend ma_num=series_fir(num, repeat(1, 5), true, true) | render timechart\n\nRegression analysis\n\nA segmented linear regression analysis can be used to estimate the trend of the time series.\n\nUse series_fit_line() to fit the best line to a time series for general trend detection.\n\nUse series_fit_2lines() to detect trend changes, relative to the baseline, that are useful in monitoring scenarios.\n\nExample of series_fit_line() and series_fit_2lines() functions in a time series query:\n\ndemo_series2 | extend series_fit_2lines(y), series_fit_line(y) | render linechart with(xcolumn=x)\n\nBlue: original time series\n\nGreen: fitted line\n\nRed: two fitted lines\n\nNote\n\nThe function accurately detected the jump (level change) point.\n\nSeasonality detection\n\nMany metrics follow seasonal (periodic) patterns. User traffic of cloud services usually contains daily and weekly patterns that are highest around the middle of the business day and lowest at night and over the weekend. IoT sensors measure in periodic intervals. Physical measurements such as temperature, pressure, or humidity may also show seasonal behavior.\n\nThe following example applies seasonality detection on one month traffic of a web service (2-hour bins):\n\ndemo_series3 | render timechart\n\nUse series_periods_detect() to automatically detect the periods in the time series.\n\nUse series_periods_validate() if we know that a metric should have specific distinct period(s) and we want to verify that they exist.\n\nNote\n\nIt's an anomaly if specific distinct periods don't exist\n\ndemo_series3 | project (periods, scores) = series_periods_detect(num, 0., 14d/2h, 2) //to detect the periods in the time series | mv-expand periods, scores | extend days=2h*todouble(periods)/1d\n\nperiods scores days 84 0.820622786055595 7 12 0.764601405803502 1\n\nThe function detects daily and weekly seasonality. The daily scores less than the weekly because weekend days are different from weekdays.\n\nElement-wise functions\n\nArithmetic and logical operations can be done on a time series. Using series_subtract() we can calculate a residual time series, that is, the difference between original raw metric and a smoothed one, and look for anomalies in the residual signal:\n\nlet min_t = toscalar(demo_make_series1 | summarize min(TimeStamp)); let max_t = toscalar(demo_make_series1 | summarize max(TimeStamp)); demo_make_series1 | make-series num=count() default=0 on TimeStamp in from min_t to max_t step 1h by OsVer | extend ma_num=series_fir(num, repeat(1, 5), true, true) | extend residual_num=series_subtract(num, ma_num) //to calculate residual time series | where OsVer == \"Windows 10\" // filter on Win 10 to visualize a cleaner chart | render timechart\n\nBlue: original time series\n\nRed: smoothed time series\n\nGreen: residual time series\n\nTime series workflow at scale\n\nThe example below shows how these functions can run at scale on thousands of time series in seconds for anomaly detection. To see a few sample telemetry records of a DB service's read count metric over four days run the following query:\n\ndemo_many_series1 | take 4\n\nTIMESTAMP Loc Op DB DataRead 2016-09-11 21:00:00.0000000 Loc 9 5117853934049630089 262 0 2016-09-11 21:00:00.0000000 Loc 9 5117853934049630089 241 0 2016-09-11 21:00:00.0000000 Loc 9 -865998331941149874 262 279862 2016-09-11 21:00:00.0000000 Loc 9 371921734563783410 255 0\n\nAnd simple statistics:\n\ndemo_many_series1 | summarize num=count(), min_t=min(TIMESTAMP), max_t=max(TIMESTAMP)\n\nnum min_t max_t 2177472 2016-09-08 00:00:00.0000000 2016-09-11 23:00:00.0000000\n\nBuilding a time series in 1-hour bins of the read metric (total four days * 24 hours = 96 points), results in normal pattern fluctuation:\n\nlet min_t = toscalar(demo_many_series1 | summarize min(TIMESTAMP)); let max_t = toscalar(demo_many_series1 | summarize max(TIMESTAMP)); demo_many_series1 | make-series reads=avg(DataRead) on TIMESTAMP from min_t to max_t step 1h | render timechart with(ymin=0)\n\nThe above behavior is misleading, since the single normal time series is aggregated from thousands of different instances that may have abnormal patterns. Therefore, we create a time series per instance. An instance is defined by Loc (location), Op (operation), and DB (specific machine).\n\nHow many time series can we create?\n\ndemo_many_series1 | summarize by Loc, Op, DB | count\n\nCount 18339\n\nNow, we're going to create a set of 18339 time series of the read count metric. We add the by clause to the make-series statement, apply linear regression, and select the top two time series that had the most significant decreasing trend:\n\nlet min_t = toscalar(demo_many_series1 | summarize min(TIMESTAMP)); let max_t = toscalar(demo_many_series1 | summarize max(TIMESTAMP)); demo_many_series1 | make-series reads=avg(DataRead) on TIMESTAMP from min_t to max_t step 1h by Loc, Op, DB | extend (rsquare, slope) = series_fit_line(reads) | top 2 by slope asc | render timechart with(title='Service Traffic Outage for 2 instances (out of 18339)')\n\nDisplay the instances:\n\nlet min_t = toscalar(demo_many_series1 | summarize min(TIMESTAMP)); let max_t = toscalar(demo_many_series1 | summarize max(TIMESTAMP)); demo_many_series1 | make-series reads=avg(DataRead) on TIMESTAMP from min_t to max_t step 1h by Loc, Op, DB | extend (rsquare, slope) = series_fit_line(reads) | top 2 by slope asc | project Loc, Op, DB, slope\n\nLoc Op DB slope Loc 15 37 1151 -102743.910227889 Loc 13 37 1249 -86303.2334644601\n\nIn less than two minutes, close to 20,000 time series were analyzed and two abnormal time series in which the read count suddenly dropped were detected.\n\nThese advanced capabilities combined with fast performance supply a unique and powerful solution for time series analysis."
    }
}
{
    "id": "dbpedia_6684_2",
    "rank": 87,
    "data": {
        "url": "https://en.wikipedia.org/wiki/Wikipedia_talk:Link_rot/Archive_1",
        "read_more_link": "",
        "language": "en",
        "title": "Wikipedia talk:Link rot/Archive 1",
        "top_image": "https://en.wikipedia.org/static/favicon/wikipedia.ico",
        "meta_img": "https://en.wikipedia.org/static/favicon/wikipedia.ico",
        "images": [
            "https://en.wikipedia.org/static/images/icons/wikipedia.png",
            "https://en.wikipedia.org/static/images/mobile/copyright/wikipedia-wordmark-en.svg",
            "https://en.wikipedia.org/static/images/mobile/copyright/wikipedia-tagline-en.svg",
            "https://upload.wikimedia.org/wikipedia/en/thumb/2/2a/Replacement_filing_cabinet.svg/40px-Replacement_filing_cabinet.svg.png",
            "https://login.wikimedia.org/wiki/Special:CentralAutoLogin/start?type=1x1",
            "https://en.wikipedia.org/static/images/footer/wikimedia-button.svg",
            "https://en.wikipedia.org/static/images/footer/poweredby_mediawiki.svg"
        ],
        "movies": [],
        "keywords": [],
        "meta_keywords": [
            ""
        ],
        "tags": null,
        "authors": [],
        "publish_date": null,
        "summary": "",
        "meta_description": "",
        "meta_lang": "en",
        "meta_favicon": "/static/apple-touch/wikipedia.png",
        "meta_site_name": "",
        "canonical_link": "https://en.wikipedia.org/wiki/Wikipedia_talk:Link_rot/Archive_1",
        "text": "Archive 1 Archive 2 Archive 3 → Archive 5\n\nThis project is a really cool idea! Just one suggestion - the listings would be more useful for collaboration if they were simply posted to the wiki. That way, people could \"click to test\" and have handy links right to the broken pages. (Of course, the bot would need to exclude these listings from its next run.) There are too many listings to post all at once (or at least all on one page), but we can certainly start working on a chunk of them. I'm sure we could clear out some of the less-populated categories entirely. By the way, anyone can do this; you don't have to be Marumari. (I would do it myself if I weren't busy updating other such reports.) -- Beland\n\nThanks for the compliment! It shouldn't be hard (a very easy regex) to make the first field (article title) into a Wiki link. I'm gone for the next couple weeks, but I should be able to do that afterwards if nobody has taken up the call to do so. -- Marumari\n\nI've done this for the 404 errors. Here's my script:\n\n#!/bin/bash AWKPROG='{desc=\" \"$3;code=$4;rpt=$5;if(NF==4){desc=\"\";code=$3;rpt=$4;} printf \"#[[%s]], [%s%s], %s %s\\n\",$1,$2,desc,code,rpt}' (for i in a b c d e f g h i j k l m n o p q r s t u v w x y z; do zgrep -i ^$i 404-links.txt.gz | awk -F '\\t' \"$AWKPROG\" > 404s/$i; done) zgrep -i -v '^[a-zA-Z]' 404-links.txt.gz | awk -F '\\t' \"$AWKPROG\" > 404s/misc;\n\nLupin|talk|popups\n\nI was trying to cleanup 301 redirects. I noticed a number of pages were posting links to google cache, which sometimes failed as 404.\n\nIn some cases the link to cache is used to convert from doc/pdf to html.\n\nDoes annyone know the officiial wiki policy on using google cache links? I feel that since google links are often not valid, they should be discouraged. Pointer to the direct webpage is a better idea.\n\nI am planning to ask for permission to fix some of the 301 redirects. It will be a manual process, obtaining the pages to be changed. After that have the bot perform the changes.\n\nExample of this is 114 instances of http://www.ex.ac.uk/trol/scol/ccleng.htm .\n\ncoomments? Khivi\n\nFine with me. Would make it easier to see if the 301 errors point at pages that are 404. -- Marumari\n\nHow often do folks think I should update the listings? Perhaps I should just wait til Khivi creates his 301 bot?\n\nKhivi, I appreciate your edits (to the 301 section). Can you please try to work in a \"range\" of links, instead of jumping about like that? It does make it much more difficult to read. Thanks!\n\nWhat is the policy to fixing the 404 links. Should they just be deleted. Also what is the policy for linking to the internet archive. Should one link to a specific version. e.g. which is better\n\nhttp://web.archive.org/web/*/http://jove.prohosting.com/~skripty/\n\nhttp://web.archive.org/web/20030214182904/http://jove.prohosting.com/~skripty/\n\nProbably a lot of linking to the internet archive can be (semi) automised with some kind of bot?\n\nI'm not sure there's a policy exactly. I usually make an attempt to correct the link or replace it with another link giving the same information (which can occasionally lead to quite a bit of digging). If that's not possible then deleting the link is still usually an improvement. Some of the links are fairly unnecessary so can just be deleted anyway. As long as you leave the article better than it was before then it's a good edit.\n\nWith regard to the web archive question, my feeling is that it would be better to link directly to a specific version. I'm basing that both on simplicity for the potential reader of the article and stability in that some of the different versions available might not contain the relevant information. --Spondoolicks\n\nI guess you are right. Each case needs to be judged on its merits. I remember fixing one in which linking to the archive was a good idea, but for some others it seems not useful.Juliusross\n\nI've changed the IA Wayback Machine URL in the dfw and dfw-inline templates to offer the most recent version of the page instead of a menu of all the page versions in the archive (change \"/*/\" to \"/2/\"). So far I have only needed to link to the most recent version in the archive for all the dead links which I have found in the archive. --James S.\n\nI would say that, at least half the time, a link to the newest version of a website in the Wayback Machine is entirely broken. So, definitely double-check, and don't just blindly use the template. -- Marumari\n\nOh, great. I've not seen that at all. Give me a few examples, please. --James S.\n\nThis is likely to happen a lot these days, given the prevalence of so-called domain name warehousing and the like, i.e. \"domain farmers\" going around systematically taking over dead domains. One example of how this can break a Wayback link is http://web.archive.org/web/20060428054311/http://www.opennc.org/ which is the latest Wayback archive of opennc.org at the time of writing. —greenrd\n\nShould we update the individual pages where the 404 linkrot entries are listed? With strikeout text? I am doing so, and see a couple of others have as well, but don't see where there are specific directions on 404 errors, unless I missed it somewhere. SailorfromNH\n\nPersonally I'd say just keep doing what you're doing as long as it's clear what's been checked so no-one duplicates the work. I'm updating the main project page to say where I've got to with the 404s beginning with P but striking out on the individual pages works too. --Spondoolicks\n\nUpdating with strikeouts is a good idea. I just got finished rechecking five that were already done, because someone didn't do the strikeouts. Oh well. --Coro\n\nI think some get done independent of the project, when somebody happens by them. SailorfromNH\n\nWe're working on the list of dead links generated from the September 13th database dump here. A lot of the ones in this list will have been repaired by someone by now and also new dead links will have appeared. --Spondoolicks\n\nThe link checker is running now. With over a million links to check, it goes slowly. As soon as it is finished, I will clear the status fields, and upload the new files.\n\nDone. -- Marumari\n\nI am not clear where we are in the database dump cycle, but these lists are feeling a little stale. A lot of the ones in this list will have been repaired by someone by now and also new dead links will have appeared. Is it time for a new list? Open2universe\n\nI've noticed that quite a few of the links listed as 404 errors are of the form that goes to a section of the target page - e.g. http://www.hostkingdom.net/Holyland.html#Samaria (I'm sure there's some technical term for this type of link which I ought to know). I've tried a dozen or so of these and they all seem to be working fine so I was wondering if the link checker had not managed to process these correctly. --Spondoolicks\n\nI'll take a look at it, and re-run the link checker if it's broken. For now, maybe just ignore links with anchors? -- Marumari\n\nOkay, you're right - there is a bug where it is requested the URL with fragments (ie, #blah). As soon as there is a new database dump, I'll re-run the link checker, and get updated files up. Like I said, just ignore links with anchors for now.\n\nThis accounts for a lot of code 400's, too. Another possible bug — if there's an ampersand ('&') in the URL, it is stored in the database as HTML entity (&amp;). Could it be that bot does not translate entities back into characters? This would explain a number of not-an-errors I've seen. sendmoreinfo\n\nI've just put a request on Wikipedia:Bot requests for a bot to fix those links which are not working because someone used the pipe symbol (|) thinking it was used the same as for internal links - e.g. [http://www.bbc.co.uk|BBC website]. At a rough estimate based on a small sample I'd say about 2% of the 404 errors are due to this mistake. --Spondoolicks\n\nYes, tons of [ | ] mistaken pipe links. Wikipedia:Link rot/Archive 1 should say what the proper repair procedure for those is too. Jidanni\n\nGreat idea.\n\nHow about a bot which checks to see if the wayback machine has anything and if so inserts {{dlw-inline}} if it does? I can see why that wouldn't be appropriate for {{dlw}} (because it's ugly) but I just made dlw-inline which I think would be appropriate for almost any situation where the wayback machine has something, except captionless links, of course; see below. James S.\n\nPreventing all dead links: Would it be possible to have a bot which looks for external links within a wiki article, and then interfaces with the wayback machine to make sure that a copy of every referenced webpage is indeed archived on the wayback machine (or on similar archive)? One example of this type service (automatically archiving all referenced websites in an article) is WebCite, although that service does not guarantee maintaining the archived website if the publisher wishes to remove it. Another example is DSpace. I wish for a way to automatically and permanently archive the referenced online material. Then, as James S. mentions above, perhaps a bot could convert the dead links to access the archived version. KHatcher\n\nIs there any guideline, tradition, or advice for dealing with dead URLs in square brackets by themselves without any caption text? I'm just replacing them with their Internet Archive link, when it yeilds results, without any further commentary such as is produced by {{dlw}} and {{dlw-inline}}. --James S.\n\nNot that I know of. I'd say to just try to figure it out by the context, and barring that, either remove the link (if it is superfluous) or change the link to the internet archive. I've been adding captions and then a (Internet Archive) after the link, but there's no official procedure that I know of. -- Marumari\n\nAm I blind...or does the page mention nothing about code 403 (Forbidden)? Bloodshedder\n\nUh Oh. It doesn't indeed, and as far as I can see, never did. Weird. sendmoreinfo\n\nI'm sure there must be a reason for it. Lemme look tonight for something. -- Marumari\n\nI can't think of a good reason myself...any updates on this? Bloodshedder\n\nA lot of news sites don't keep their articles available for long and don't allow the Internet Archive to capture them, which results in a lot of non-repairable dead links. However, if the article was from a news agency like AP or Reuters then it will be available from many other places, some of which might have either permanently accessible articles or will be available from the Internet Archive. I've just replaced a dead link to an article in the Washington Post with a link to the same article which is still available at ABC News but I don't know how permanent that is. Does anyone know of a major news site which uses AP/Reuters reports and has permanent links? --Spondoolicks\n\nIf there is one, I've never been able to find it. -- Marumari\n\nThe main page indicates that there are >30,000 of the 301-type errors. However, in opening the page I only get a count of slightly less than 12,000. Is this because the page is partitioned; the first one (/301) runs through the H's. Thanks. User:Ceyockey (talk to me)\n\nI just downloaded it, and got over 30k entries. Perhaps your download got truncated accidentally? -- Marumari\n\nHi, I was just hopping around for a project to work on I found this Link rot page. I am not sure if it is supposed to be too simple but would it be possible for experienced contributors to list efficiency tips for newbies for this project? For example, this guide from Wikipedia:Disambiguation pages with links was quite useful when I started that (until I got bored). Ashish G 00:47, 1 March 2006 (UTC)\n\nHello there, Do you think you would have time to regenerate the files for the dead external links project? Not that we finished them all, but they are feeling a little stale. Thanks so much Open2universe\n\nI'd love to re-generate the external link list, but I can't do that until Wikipedia does another database dump. They haven't done a database dump since December 14th. Don't ask me why. -- Marumari\n\nNever mind, it just looks like they moved the database dump page, and didn't tell me. Bah. I'll re-run the process soon. -- Marumari\n\nI came here to remove a link to an AFD deleted page, but the page was over a megabyte long. I couldn't find the entry I wanted when I went into edit mode (Firefox doesn't search within text boxes) so I left it. == way too long. --kingboyk\n\nWhich page are you referring to? Many of the 404 pages are large so we have tried to break them down into sections for editing. When you select a section for editing it only brings up that section. I don't believe any of the sections are that large, but if so I will fix it Open2universe\n\nWikipedia:Dead external links/301 is incredibly 1,212 kilobytes long. There is a post on VPT by a User:0plusminus0 having problems editing that page. --Ligulem\n\nAh, okay. The page doesn't link to that one. I will try to break it up.Open2universe\n\nI was asked not to delete dead links to pages that listed population figures. I am wondering what folks think is the best way to handle this when the website cannot be found in the archive. I understand that the website was the original source, but I am reluctant to leave broken links. Should I leave the URL but not as a link? Or should I simply state that the link is unavailable? Any guidance is appreciated. Open2universe\n\nI made some edits on Dec 7th to the effect of mentioning WebCite [1] alongside the Internet Archive as a means to recover broken links, in particular if they were prospectively archived with WebCite; unfortunately, these changes were reverted by another user as \"spam/self-promotion\". I will not re-revert these changes to avoid an edit-war, but I do request to give this matter some serious consideration and I am seeking some support through the Wiki community. Internet Archive and WebCite are not competitors, but complement each other, and both are non-profit. I do think that WebCite could help Wikipedia a lot to avoid broken links in the first place (or to cache cited material so that it is recoverable).\n\nBEFORE:\n\nThe 404 error is the most common symptom of link rot, and it indicates that the page has not been found. The 410 status code is similar, but indicates that the file has permanently gone. Such links are required by policy to be repaired, perhaps with a link to the Internet Archive. Wikipedia currently contains 31,913 status 404 links and 42 status 410 links.\n\nMY SUGGESTED EDITED VERSION\n\nThe 404 error is the most common symptom of link rot, and it indicates that the page has not been found. The 410 status code is similar, but indicates that the file has permanently gone. Such links are required by policy to be repaired, perhaps with a link to WebCite or the Internet Archive. To link to WebCite, try http://www.webcitation.org/query?url=URL&date=DATE (replace URL with the url, the DATE is the cited date, and is optional. Chances are of course best to recover an archived version if the URL has been explicitly WebCited by the editor before it went dead). Wikipedia currently contains 31,913 status 404 links and 42 status 410 links.\n\nThere were other edits I made (which can be seen in the history) to include hints to the effect of avoiding 404s in the first place if all cited links would be cached prospectively using WebCite, for which somebody could write a bot (see Wikipedia:Bot_requests). I hope wikipedians will support the proposal to include hints to WebCite as well, or help in rephrasing how this should be done, and hopefully put these edits back in. I will withdraw myself from further discussions on this (except perhaps correcting factual errors in the subsequent discussion), but just want to throw the suggestion out there. --Eysen\n\nThe internet archive is years old with broad recognition and support. WebCite is one of a number of options at archiving specific versions of a page. If you would like to reference Web archiving here, that would likely be fine. However, I don't feel it appropriate for you to be adding numerous links to a service you are involved in operating. If the service is found to be valuable to the wikipedia community, there are plenty of editors uninvolved with the company who will find and add it. See WP:COI, Gunther Eysenbach, and Special:Contributions/Eysen for more context on my actions. I wish your well-intentioned service the best of luck! ∴ here…♠\n\nI wish user \"here\" would stop referring to WebCite as a \"company\". It is in fact a non-profit project hosted at the University of Toronto, backed by a consortium of (mostly open access) publishers, and collaborating with other preservation projects such as Internet Archive. If you know of other \"number of options\" to restore links, then it would be more constructive to add them, as opposed to suppressing these suggested edits. Mentioning exclusively the Internet Archive is a pretty US-centric view, failing to mention other archiving services such as those collaborating in the International Internet Preservation Consortium, who are working on common standards and practices (of which WebCite will be a member starting Feb 2007). Yes, ideally other users would make these edits, but I won't stop pointing out in the discussion that the current Wikipedia policies are outdated. This concerns Wikipedia policies/guidelines on external links [2], citing sources [3], and dead links [4], which all only mention \"the Internet Archive\", as if there is only one. Further, Wikipedia's bibliographic templates [5] should be updated to add a field for a secondary URLs (URLcached) that links to a archived snapshot (possibly even a field for the WebCite ID, in analogy to having a field for the DOI)--Eysen\n\nI've decided to stay out of this one, awaiting additional comment. I agree that options such as Webcite would be great to incoporate into wikipedia habit. Apologies for misrepresenting the non-profit. Thanks for any help improving wikipedia's citations! ∴ here…♠\n\nOkay, the entire page has been updated with new information from the November 6, 2006 database. Take all the numbers on the page and double or triple them. :( Sorry for being so neglectful - I rarely have an opportunity to monopolize my internet connection for the week or so that it takes to check all these links. --Marumari\n\nSome cleanup projects have had generic edit summaries to use when editing. They're useful for saving time when making many edits of a similar nature, as well as for advertising the project. For example: Stubsensor cleanup project; you can help!\n\nI don't see such a default summary for this project. I'd suggest that there be one/some for this project, like perhaps: Fixed broken link(s) to external website(s); you can help too! --- Iotha\n\nMakes sense to me. I'll add that to the page. --Marumari\n\nJust thought I'd post a quick hello message. Although not a newbie to Wikipedia, I am a newly registered user. I've decided to pitch in by getting involved with this effort. I've been working my way through the 404s. I'd appreciate it if one of the regulars here would review some of my work to make sure that I'm following guidelines. I'm a little confused by the dl templates and am not completely clear on when/how to use them. —The preceding unsigned comment was added by Sanfranman59 (talk • contribs).\n\nMany of the articles about various sessions of the US Congress have links to \"Rules of the House\" pages on clerk.house.gov. The URL has changed from clerk.house.gov/legisAct/legisProc/etc to clerk.house.gov/legislative/etc. Is there a way to do a global replace to fix this on every Wiki page? Sanfranman59\n\nI would go request it at Wikipedia:Bot requests. --Marumari\n\nI have resolved most of these.Matthew Hill, Steve Godsey and Jerome Cochran reference www.timesnews.net a subscription only news site. I can not verify the links and none are archived. London Buses route 106 contains a references to a document which seems to have been inadvertently renamed by the site admin to \"xxx.pdf\". Some how I don't think it's going to stay that way.Phatom87\n\nIt would be useful to have an alternative view of broken links sorted by host. This would help make it apparent when a host of links to a popular external source break due to some large scale changes on the host; for example, when Pitchfork decides to move its articles around (again), breaking tons of links. Once situations like this are recognized, they can be possibly handled by bots, allowing editors to focus their linkrot effects on more unique cases. Pimlottc\n\nShould be pretty easy. Just save the 404 page, and do some catting and sorting on it. --Marumari\n\nHello, I've just my completed work on PDFbot and during it's run I've noted a lot of the links 404ed. Most of them are fairly easy to find, just very tedious to fix. My bot code is in a state where it should be fairly easy to adapt for this.\n\nHere are some ideas on link resolution:\n\nGoogle search: [6] → Google Search, uses the result only if one appear.\n\nTake user feedback on where URL have moved to.\n\nReplace broken news URL with more permanent URLs (NY Times for instance).\n\nIf a link is broken too long tag it.\n\nIs there anything against this, and more idea of what this bot should be doing?\n\nPS: This project's visibility is extremely low, none of it's templates are linked here. —Dispenser\n\nA question: presumably some kind of bot goes actually calls up the links in question to check them? How does this bot identify itself to the websites?\n\nI ask because I've come across several links which are marked as 404 but which definitively exist; the websites they are on have a very strict policy towards bots and might be giving (unwillingly) false results. —The preceding unsigned comment was added by 84.190.114.119 (talk) .\n\nEither nothing, or Python/urllib-blah. I'll make sure that future versions send a more standard User-Agent. --Marumari\n\nWithin the past two weeks I've programmed a bot (User:Ocobot) that detects broken links at Wikipedia. It has just been approved for trials. Now I just stumbled across this site, and that makes me basically feel like an idiot, like I wasted a lot of time and that Wikipedia is even much bigger than I had thought. Ah, it's a pitty I didn't find this earlier. This project's bot is not listed at the bot status page, is it?\n\nI consider withdrawing my request for approval now. But I'm not sure yet, because my idea is slightly different. Users can put areas of their interest on the bot's schedule for example and the bot will then check these articles. Could someone from this project please browse the bot's user page and maybe the request for approval to see if it can be of use despite the existence of this project? Shall I run my bot, change it or drop it?\n\nThanks. — Ocolon\n\nOkay, I didn't withdraw it. Kudos to Spondoolicks :)!\n\nThis isn't a bot that edits Wikipedia at all (nor does it have an associated account), so it isn't listed as an active bot on the bot status page. --Marumari\n\nEvery day I discover something new. Today it was WikiProject External links. The project seems to be rather inactive, unfortunately. However, I hope it can be revived. Would you like to contribute? Wikipedia:Dead external links fits very well into this project — it should have a central role actually. I think the WikiProject could be used to coordinate all external links discussion and activity. And having an active WikiProject External links would also strengthen the efforts of this site here.\n\nYour support will be appreciated a lot! You can make suggestions or comments on this matter on the WikiProject's talk page. — Ocolon\n\nI've programmed a little tool for Ocobot that might be helpful for you, too. You can also suggest how to make it more useful for you! It's a very short script that checks if an URL can be retrieved by the wayback machine. The basic difference between directly using the wayback machine and my tool is that the tool outputs Wikipedia tags, which might make editing a little faster :-).\n\nHere's an example on how to use it to retrieve Wikipedia at the wayback machine: http://ocobot.kaor.in/url.php?http://www.wikipedia.org/\n\nBut you can also go to http://ocobot.kaor.in/url.php and use the form to check an url.\n\nI'm currently writing a bot which will search for dead links however, I'm not quite sure what the bot should do once it's actually found one. I was going to simply list the dead pages on a user subpage of the bot, I'm not sure if this is the right thing to do now. Any suggestions? PeteMarsh\n\n1) Some web sites periodically do maintenance to their site, e.g. close on Saturday to Sunday/everyday 3am-6am/2 weeks for big maintenance(some may be so unlucky hacked or forget to pay internet fees and take 1-2 month to recover.) They may be 'temporary' dead. Should we save them and take a 2th/3rd test in different days before delete them?\n\n2) The \"www.archive.org\" backup some webpages some time b4, should we replace the dead link with links to backups in this site?(This site also have several mirrors, some pages are found in some mirrors but not other mirrors. But it may not be good all links change to this site, that would make too high traffic to the site as the site have already many volume of backups.\n\n3) Some links have mirrors, or copies in other sites, and as may have caches in Google, MSN, Yahoo...etc, can we see them and use keyword \"selected short word phrases in the search engine cache\" to search alternative links to replace the dead links?\n\nSome dead links may be good links and should not delete simply. But when more and more links one day there may be too much links in a page.Gaia2767spm\n\nPlease see: Wikipedia:Requests for verification\n\nA proposal designed as a process similar to {{prod}} to delete articles without sources if no sources are provided in 30 days.\n\nIt reads:\n\nThis page has been listed in Category:Requests for verification.\n\nIt has been suggested that this article might not meet Wikipedia's core content policies Verifiability and/or No original research. If references are not cited within a month, the disputed information will be removed.\n\nIf you can address this concern by sourcing please edit this page and do so. You may remove this message if you reference the article.\n\nThe article may be deleted if this message remains in place for 30 days. (This message was added: 29 August 2024.)\n\nIf you created the article, please don't take offense. Instead, improve the article so that it is acceptable according to Verifiability and/or No original research.\n\nPlease help improve this article by adding citations to reliable sources. (help, get involved!)\n\nSome editors see this as necessary to improve Wikipedia as a whole and assert that this idea is supported by policy, and others see this as a negative thing for the project with the potential of loss of articles that could be easily sourced.\n\nI would encourage your comments in that page's talk or Mailing list thread on this proposal WikiEN-l: Proposed \"prod\" for articles with no sources\n\nSigned Jeepday (talk)\n\nJust removed a big block of repaired (struck-out) links from the U list. (Copied to its Discussion page in case anyone minds.) Seemed to date from a while ago and I had to keep scrolling past them ! thisisace\n\nhttp://en.wikipedia.org/w/index.php?title=Special%3ALinksearch&target=enjoyment.independent.co.uk\n\nMany of these dead links could be recovered by substituting ...\n\n\"enjoyment.independent.co.uk\" with \"arts.independent.co.uk\""
    }
}
{
    "id": "dbpedia_3332_3",
    "rank": 25,
    "data": {
        "url": "https://renalresearch.com/about/news/",
        "read_more_link": "",
        "language": "en",
        "title": "Renal Research Institute News",
        "top_image": "",
        "meta_img": "",
        "images": [
            "https://renalresearch.com/content/dam/rri/live/research/logo_rri_full.svg",
            "https://renalresearch.com/content/dam/rri/social-icons/icon-fb.svg",
            "https://renalresearch.com/content/dam/rri/social-icons/icon-li.svg",
            "https://renalresearch.com/content/dam/rri/social-icons/icon-yt.svg",
            "https://renalresearch.com/content/dam/rri/social-icons/instagram-icon-white-transparent.svg"
        ],
        "movies": [],
        "keywords": [],
        "meta_keywords": [
            ""
        ],
        "tags": null,
        "authors": [],
        "publish_date": null,
        "summary": "",
        "meta_description": "",
        "meta_lang": "en",
        "meta_favicon": "",
        "meta_site_name": "",
        "canonical_link": "https://renalresearch.com/about/news/",
        "text": "Mariana Murea, Jochen G Raimann, Jasmin Divers, Harvey Maute, Cassandra Kovach, Emaad M Abdel-Rahman, Alaa S Awad, Jennifer E Flythe, Samir C Gautam, Vandana D Niyyar, Glenda V Roberts, Nichole M Jefferson, Islam Shahidul, Ucheoma Nwaozuru, Kristie L Foley, Erica J Trembath, Merlo L Rosales, Alison J Fletcher, Sheikh I Hiba, Anne Huml, Daphne H Knicely, Irtiza Hasan, Bhaktidevi Makadia, Raman Gaurav, Janice Lea, Paul T Conway, John T Daugirdas, Peter Kotanko\n\nTRIAL REGISTRATIONClinicaltrials.gov NCT05828823. Registered on 25 April 2023.DISCUSSIONOur proposal challenges the status quo of HD care delivery. Our overarching hypothesis posits that CMIHD is non-inferior to CHD. If successful, the results will positively impact one of the highest-burdened patient populations and their caregivers.BACKGROUNDMost patients starting chronic in-center hemodialysis (HD) receive conventional hemodialysis (CHD) with three sessions per week targeting specific biochemical clearance. Observational studies suggest that patients with residual kidney function can safely be treated with incremental prescriptions of HD, starting with less frequent sessions and later adjusting to thrice-weekly HD. This trial aims to show objectively that clinically matched incremental HD (CMIHD) is non-inferior to CHD in eligible patients.METHODSAn unblinded, parallel-group, randomized controlled trial will be conducted across diverse healthcare systems and dialysis organizations in the USA. Adult patients initiating chronic hemodialysis (HD) at participating centers will be screened. Eligibility criteria include receipt of fewer than 18 treatments of HD and residual kidney function defined as kidney urea clearance ≥3.5 mL/min/1.73 m2 and urine output ≥500 mL/24 h. The 1:1 randomization, stratified by site and dialysis vascular access type, assigns patients to either CMIHD (intervention group) or CHD (control group). The CMIHD group will be treated with twice-weekly HD and adjuvant pharmacologic therapy (i.e., oral loop diuretics, sodium bicarbonate, and potassium binders). The CHD group will receive thrice-weekly HD according to usual care. Throughout the study, patients undergo timed urine collection and fill out questionnaires. CMIHD will progress to thrice-weekly HD based on clinical manifestations or changes in residual kidney function. Caregivers of enrolled patients are invited to complete semi-annual questionnaires. The primary outcome is a composite of patients' all-cause death, hospitalizations, or emergency department visits at 2 years. Secondary outcomes include patient- and caregiver-reported outcomes. We aim to enroll 350 patients, which provides ≥85% power to detect an incidence rate ratio (IRR) of 0.9 between CMIHD and CHD with an IRR non-inferiority of 1.20 (α = 0.025, one-tailed test, 20% dropout rate, average of 2.06 years of HD per patient participant), and 150 caregiver participants (of enrolled patients).\n\nVaibhav Maheshwari, Nadja Grobe, Xin Wang, Amrish Patel, Alhaji Cherif, Xia Tao, Joshua Chao, Alexander Heide, Dejan Nikolic, Jiaming Dong, Peter Kotanko\n\nIt has been estimated that in 2010, over two million patients with end-stage kidney disease may have faced premature death due to a lack of access to affordable renal replacement therapy, mostly dialysis. To address this shortfall in dialytic kidney replacement therapy, we propose a novel, cost-effective, and low-complexity hemodialysis method called allo-hemodialysis (alloHD). With alloHD, instead of conventional hemodialysis, the blood of a patient with kidney failure flows through the dialyzer's dialysate compartment counter-currently to the blood of a healthy subject (referred to as a \"buddy\") flowing through the blood compartment. Along the concentration and hydrostatic pressure gradients, uremic solutes and excess fluid are transferred from the patient to the buddy and subsequently excreted by the healthy kidneys of the buddy. We developed a mathematical model of alloHD to systematically explore dialysis adequacy in terms of weekly standard urea Kt/V. We showed that in the case of an anuric child (20 kg), four 4 h alloHD sessions are sufficient to attain a weekly standard Kt/V of >2.0. In the case of an anuric adult patient (70 kg), six 4 h alloHD sessions are necessary. As a next step, we designed and built an alloHD machine prototype that comprises off-the-shelf components. We then used this prototype to perform ex vivo experiments to investigate the transport of solutes, including urea, creatinine, and protein-bound uremic retention products, and to quantitate the accuracy and precision of the machine's ultrafiltration control. These experiments showed that alloHD performed as expected, encouraging future in vivo studies in animals with and without kidney failure.\n\nAmun G Hofmann, Suman Lama, Hanjie Zhang, Afshin Assadian, Murat Sor, Jeffrey Hymes, Peter Kotanko, Jochen Raimann\n\nRESULTSIn total, 38 151 patients (52.2%) had complete data and made up the main cohort. Sensitivity analyses were conducted in 67 421 patients (92.3%) after eliminating variables with a high proportion of missing data points. Selected features diverged between datasets and workflows. A previously failed arteriovenous access appeared to be the most stable predictor for subsequent failure. Prediction of re-conversion based on the demographic and clinical information resulted in an area under the receiver operating characteristic curve (ROCAUC) between 0.541 and 0.571, whereas models predicting all cause mortality performed considerably better (ROCAUC 0.662 - 0.683).OBJECTIVEThe decision to convert from catheter to arteriovenous access is difficult yet very important. The ability to accurately predict fistula survival prior to surgery would significantly improve the decision making process. Many previously investigated demographic and clinical features have been associated with fistula failure. However, it is not conclusively understood how reliable predictions based on these parameters are at an individual level. The aim of this study was to investigate the probability of arteriovenous fistula maturation and survival after conversion using machine learning workflows.CONCLUSIONWhile group level depiction of major adverse outcomes after catheter to arteriovenous fistula or graft conversion is possible using the included variables, patient level predictions are associated with limited performance. Factors during and after fistula creation as well as biomolecular and genetic biomarkers might be more relevant predictors of fistula survival than baseline clinical conditions.METHODSA retrospective cohort study on multicentre data from a large North American dialysis organisation was conducted. The study population comprised 73 031 chronic in centre haemodialysis patients. The dataset included 49 variables including demographic and clinical features. Two distinct feature selection and prediction pipelines were used: LASSO regression and Boruta followed by a random forest classifier. Predictions were facilitated for re-conversion to catheter within one year. Additionally, all cause mortality predictions were conducted to serve as a comparator.\n\nMarcus Dariva, Murilo Guedes, Vladimir Rigodon, Peter Kotanko, John W Larkin, Bruno Ferlin, Roberto Pecoits-Filho, Pasqual Barretti, Thyago Proença de Moraes\n\nRESULTSWe analysed data of 848 patients (814 starting on CAPD and 34 starting on APD). The SBP decreased by 4 (SD 22) mmHg when transitioning from CAPD to APD (p < 0.001) and increased by 4 (SD 21) mmHg when transitioning from APD to CAPD (p = 0.38); consistent findings were seen for DBP. There was no significant change in the number of antihypertensive drugs prescribed before and after transition.CONCLUSIONSTransition between PD modalities seems to directly impact on BP levels. Further studies are needed to confirm if switching to APD could be an effective treatment for uncontrolled hypertension among CAPD patients.BACKGROUNDHypertension is a leading cause of kidney failure, affects most dialysis patients and associates with adverse outcomes. Hypertension can be difficult to control with dialysis modalities having differential effects on sodium and water removal. There are two main types of peritoneal dialysis (PD), automated peritoneal dialysis (APD) and continuous ambulatory peritoneal dialysis (CAPD). It is unknown whether one is superior to the other in controlling blood pressure (BP). Therefore, the aim of our study was to analyse the impact of switching between these two PD modalities on BP levels in a nationally representative cohort.METHODSThis was a cohort study of patients on PD from 122 dialysis centres in Brazil (BRAZPD II study). Clinical and laboratory data were collected monthly throughout the study duration. We selected all patients who remained on PD at least 6 months and 3 months on each modality at minimum. We compared the changes in mean systolic/diastolic blood pressures (SBP/DBP) before and after modality transition using a multilevel mixed-model where patients were at first level and their clinics at the second level.\n\nSunpeng Duan, Yuedong Wang, Peter Kotanko, Hanjie Zhang\n\nRESULTSOut of 978 patients, 193 (19.7%) tested positive for COVID-19 and had contact with other patients during the COV-Pos infectious period. Network diagrams showed no evidence that more exposed patients would have had a higher chance of infection. This finding was corroborated by logistic mixed effect regression (donor-to-potential recipient exposure OR: 0.63; 95% CI 0.32 to 1.17, p = 0.163). Separate analyses according to vaccination led to materially identical results.CONCLUSIONSTransmission of SARS-CoV-2 between in-center hemodialysis patients is unlikely. This finding supports the effectiveness of non-pharmaceutical interventions, such as universal masking and other procedures to control spread of COVID-19.BACKGROUNDIn-center hemodialysis entails repeated interactions between patients and clinic staff, potentially facilitating the spread of COVID-19. We examined if in-center hemodialysis is associated with the spread of SARS-CoV-2 between patients.METHODSOur retrospective analysis comprised all patients receiving hemodialysis in four New York City clinics between March 12th, 2020, and August 31st, 2022. Treatment-level clinic ID, dialysis shift, dialysis machine station, and date of COVID-19 diagnosis by RT-PCR were documented. To estimate the donor-to-potential recipient exposure (\"donor\" being the COVID-19 positive patient denoted as \"COV-Pos\"; \"potential recipient\" being other susceptible patients in the same shift), we obtained the spatial coordinates of each dialysis station, calculated the Euclidean distances between stations and weighted the exposure by proximity between them. For each donor, we estimated the donor-to-potential recipient exposure of all potential recipients dialyzed in the same shift and accumulated the exposure over time within the 'COV-Pos infectious period' as cumulative exposures. The 'COV-Pos infectious period' started 5 days before COVID-19 diagnosis date. We deployed network analysis to assess these interactions and summarized the donor-to-potential recipient exposure in 193 network diagrams. We fitted mixed effects logistic regression models to test whether more donor-to-potential recipient exposure conferred a higher risk of SARS-CoV-2 infection.\n\nHanjie Zhang, Lin-Chun Wang, Sheetal Chaudhuri, Aaron Pickering, Len Usvyat, John Larkin, Pete Waguespack, Zuwen Kuang, Jeroen P Kooman, Franklin W Maddux, Peter Kotanko\n\nRESULTSWe utilized data from 693 patients who contributed 42 656 hemodialysis sessions and 355 693 intradialytic SBP measurements. IDH occurred in 16.2% of hemodialysis treatments. Our model predicted IDH 15-75 min in advance with an AUROC of 0.89. Top IDH predictors were the most recent intradialytic SBP and IDH rate, as well as mean nadir SBP of the previous 10 dialysis sessions.CONCLUSIONSReal-time prediction of IDH during an ongoing hemodialysis session is feasible and has a clinically actionable predictive performance. If and to what degree this predictive information facilitates the timely deployment of preventive interventions and translates into lower IDH rates and improved patient outcomes warrants prospective studies.BACKGROUNDIn maintenance hemodialysis patients, intradialytic hypotension (IDH) is a frequent complication that has been associated with poor clinical outcomes. Prediction of IDH may facilitate timely interventions and eventually reduce IDH rates.METHODSWe developed a machine learning model to predict IDH in in-center hemodialysis patients 15-75 min in advance. IDH was defined as systolic blood pressure (SBP) <90 mmHg. Demographic, clinical, treatment-related and laboratory data were retrieved from electronic health records and merged with intradialytic machine data that were sent in real-time to the cloud. For model development, dialysis sessions were randomly split into training (80%) and testing (20%) sets. The area under the receiver operating characteristic curve (AUROC) was used as a measure of the model's predictive performance.\n\nAna Catalina Alvarez-Elias, Barry M Brenner, Valerie A Luyckx\n\nPURPOSE OF REVIEWThe consequences of climate change, including heat and extreme weather events impact kidney function in adults and children. The impacts of climate change on kidney development during gestation and thereby on kidney function later in life have been poorly described. Clinical evidence is summarized to highlight possible associations between climate change and nephron mass.SUMMARYClimate change has important impacts on pregnant women and their unborn children. Being born too small or too soon is associated with life-time risk of kidney disease. Climate change may therefore have a dual effect of impacting fetal kidney development and contributing to cumulative postnatal kidney injury. The impact on population kidney health of future generations may be significant.RECENT FINDINGSPregnant women are vulnerable to the effects of climate change, being less able to thermoregulate, more sensitive to the effects of dehydration, and more susceptible to infections. Exposure to heat, wildfire smoke, drought, floods and climate-related infections are associated with low birth weight, preterm birth and preeclampsia. These factors are associated with reduced nephron numbers, kidney dysfunction and higher blood pressures in offspring in later life. Exposure to air pollution is associated with higher blood pressures in children and has variable effects on estimated glomerular filtration rate.\n\nXiaoling Wang, Ohnmar Thwin, Zahin Haq, Zijun Dong, Lela Tisdale, Lemuel Rivera Fuentes, Nadja Grobe, Peter Kotanko\n\nRESULTSMask and saliva testing specificities were 99% and 100%, respectively. Test sensitivity was 62% for masks, and 81% for saliva (p = 0.16). Median viral RNA shedding duration was 11 days and longer in immunocompromised versus non-immunocompromised patients (22 vs. 11 days, p = 0.06, log-rank test).CONCLUSIONWhile SARS-CoV-2 testing on worn masks appears to be less sensitive compared to saliva, it may be a preferred screening method for individuals who are mandated to wear masks yet averse to more invasive sampling. However, optimized RNA extraction methods and automated procedures are warranted to increase test sensitivity and scalability. We corroborated longer viral RNA shedding in immunocompromised patients.BACKGROUNDExhaled SARS-CoV-2 can be detected on face masks. We compared tests for SARS-CoV-2 RNA on worn face masks and matched saliva samples.METHODSWe conducted this prospective, observational, case-control study between December 2021 and March 2022. Cases comprised 30 in-center hemodialysis patients with recent COVID-19 diagnosis. Controls comprised 13 hemodialysis patients and 25 clinic staff without COVID-19 during the study period and the past 2 months. Disposable 3-layer masks were collected after being worn for 4 hours together with concurrent saliva samples. ThermoFisher COVID-19 Combo Kit (A47814) was used for RT-PCR testing.\n\nErcan Ok, Cenk Demirci, Gulay Asci, Kivanc Yuksel, Fatih Kircelli, Serkan Kubilay Koc, Sinan Erten, Erkan Mahsereci, Ali Rıza Odabas, Stefano Stuard, Franklin W Maddux, Jochen G Raimann, Peter Kotanko, Peter G Kerr, Christopher T Chan\n\nRESULTSThe mean duration of dialysis session was 418 ± 54 minutes in HHD and 242 ± 10 minutes in patients on ICHD. All-cause mortality rate was 3.76 and 6.27 per 100 patient-years in the HHD and the ICHD groups, respectively. In the intention-to-treat analysis, HHD was associated with a 40% lower risk for all-cause mortality than ICHD (hazard ratio [HR] = 0.60; 95% confidence interval [CI] 0.45 to 0.80; P < 0.001). In HHD, the 5-year technical survival was 86.5%. HHD treatment provided better phosphate and blood pressure (BP) control, improvements in nutrition and inflammation, and reduction in hospitalization days and medication requirement.CONCLUSIONThese results indicate that extended HHD is associated with higher survival and better outcomes compared to ICHD.INTRODUCTIONMore frequent and/or longer hemodialysis (HD) has been associated with improvements in numerous clinical outcomes in patients on dialysis. Home HD (HHD), which allows more frequent and/or longer dialysis with lower cost and flexibility in treatment planning, is not widely used worldwide. Although, retrospective studies have indicated better survival with HHD, this issue remains controversial. In this multicenter study, we compared thrice-weekly extended HHD with in-center conventional HD (ICHD) in a large patient population with a long-term follow-up.METHODSWe matched 349 patients starting HHD between 2010 and 2014 with 1047 concurrent patients on ICHD by using propensity scores. Patients were followed-up with from their respective baseline until September 30, 2018. The primary outcome was overall survival. Secondary outcomes were technique survival; hospitalization; and changes in clinical, laboratory, and medication parameters.\n\nZahin Haq, Xin Wang, Qiuqiong Cheng, Gabriela F Dias, Christoph Moore, Dorothea Piecha, Peter Kotanko, Chih-Hu Ho, Nadja Grobe\n\nBisphenol A (BPA)-based materials are used in the manufacturing of hemodialyzers, including their polycarbonate (PC) housings and polysulfone (PS) membranes. As concerns for BPA's adverse health effects rise, the regulation on BPA exposure is becoming more rigorous. Therefore, BPA alternatives, such as Bisphenol S (BPS), are increasingly used. It is important to understand the patient risk of BPA and BPS exposure through dialyzer use during hemodialysis. Here, we report the bisphenol levels in extractables and leachables obtained from eight dialyzers currently on the market, including high-flux and medium cut-off membranes. A targeted liquid chromatography-mass spectrometry strategy utilizing stable isotope-labeled internal standards provided reliable data for quantitation with the standard addition method. BPA ranging from 0.43 to 32.82 µg/device and BPS ranging from 0.02 to 2.51 µg/device were detected in dialyzers made with BPA- and BPS-containing materials, except for the novel FX CorAL 120 dialyzer. BPA and BPS were also not detected in bloodline controls and cellulose-based membranes. Based on the currently established tolerable intake (6 µg/kg/day), the resulting margin of safety indicates that adverse effects are unlikely to occur in hemodialysis patients exposed to BPA and BPS quantified herein. With increasing availability of new data and information about the toxicity of BPA and BPS, the patient safety limits of BPA and BPS in those dialyzers may need a re-evaluation in the future.\n\nArmando Armenta-Alvarez, Salvador Lopez-Gil, Iván Osuna, Nadja Grobe, Xia Tao, Gabriela Ferreira Dias, Xiaoling Wang, Joshua Chao, Jochen G Raimann, Stephan Thijssen, Hector Perez-Grovas, Bernard Canaud, Peter Kotanko, Magdalena Madero\n\nRESULTSTwelve anuric patients were studied (six female patients; 44±19 years; dialysis vintage 35.2±28 months). The blood flow was 369±23 ml/min, dialysate flow was 495±61 ml/min, and ultrafiltration volume was 2.8±0.74 L. No significant differences were found regarding the removal of B2M, vitamin B12, and water-soluble solutes between dialytic modalities and dialyzers. Albumin and total protein loss were significantly higher in MCO groups than HFX groups when compared with the same modality. HDF groups had significantly higher albumin and total protein loss than HD groups when compared with the same dialyzer. MCO-HDF showed the highest protein loss among all groups.KEY POINTSHDF and MCO have shown greater clearance of middle-size uremic solutes in comparison with HF dialyzers; MCO has never been studied in HDF. MCO in HDF does not increase the clearance of B2M and results in a higher loss of albumin.CONCLUSIONSMCO-HD is not superior to HFX-HD and HFX-HDF for both middle molecule and water-soluble solute removal. Protein loss was more pronounced with MCO when compared with HFX on both HD and HDF modalities. MCO-HDF has no additional benefits regarding better removal of B2M but resulted in greater protein loss than MCO-HD.BACKGROUNDMiddle molecule removal and albumin loss have been studied in medium cutoff (MCO) membranes on hemodialysis (HD). It is unknown whether hemodiafiltration (HDF) with MCO membranes provides additional benefit. We aimed to compare the removal of small solutes and β2-microglobulin (B2M), albumin, and total proteins between MCO and high-flux (HFX) membranes with both HD and HDF, respectively.METHODSThe cross-over study comprised 4 weeks, one each with postdilutional HDF using HFX (HFX-HDF), MCO (MCO-HDF), HD with HFX (HFX-HD), and MCO (MCO-HD). MCO and HFX differ with respect to several characteristics, including membrane composition, pore size distribution, and surface area (HFX, 2.5 m2; MCO, 1.7 m2). There were two study treatments per week, one after the long interdialytic interval and another midweek. Reduction ratios of vitamin B12, B2M, phosphate, uric acid, and urea corrected for hemoconcentration were computed. Dialysis albumin and total protein loss during the treatment were quantified from dialysate samples.\n\nPriscila Preciado, Laura Rosales Merlo, Hanjie Zhang, Jeroen P Kooman, Frank M van der Sande, Peter Kotanko\n\nDISCUSSIONConcurrent combined monitoring of intradialytic ScvO2 and RBV change may provide additional insights into a patient's circulatory status. Patients with low ScvO2 and small changes in RBV may represent a specifically vulnerable group of patients at particularly high risk for adverse outcomes, possibly related to poor cardiac reserve and fluid overload.INTRODUCTIONIn maintenance hemodialysis (HD) patients, low central venous oxygen saturation (ScvO2 ) and small decline in relative blood volume (RBV) have been associated with adverse outcomes. Here we explore the joint association between ScvO2 and RBV change in relation to all-cause mortality.FINDINGSBaseline comprised 5231 dialysis sessions in 216 patients. The median RBV change was -5.5% and median ScvO2 was 58.8%. During follow-up, 44 patients (20.4%) died. In the adjusted model, all-cause mortality was highest in patients with ScvO2 below median and RBV change above median (HR 6.32; 95% confidence interval [CI] 1.37-29.06), followed by patients with ScvO2 below median and RBV change below median (HR 5.04; 95% CI 1.14-22.35), and ScvO2 above median and RBV change above median (HR 4.52; 95% CI 0.95-21.36).METHODSWe conducted a retrospective study in maintenance HD patients with central venous catheters as vascular access. During a 6-month baseline period, Crit-Line (Fresenius Medical Care, Waltham, MA) was used to measure continuously intradialytic ScvO2 and hematocrit-based RBV. We defined four groups per median change of RBV and median ScvO2 . Patients with ScvO2 above median and RBV change below median were defined as reference. Follow-up period was 3 years. We constructed Cox proportional hazards model with adjustment for age, diabetes, and dialysis vintage to assess the association between ScvO2 and RBV and all-cause mortality during follow-up.\n\nJuntao Duan, Hanmo Li, Xiaoran Ma, Hanjie Zhang, Rachel Lasky, Caitlin K Monaghan, Sheetal Chaudhuri, Len A Usvyat, Mengyang Gu, Wensheng Guo, Peter Kotanko, Yuedong Wang\n\nCONCLUSIONAs found in our study, the dynamics of the prediction model are frequently changing as the pandemic evolves. County-level infection information and vaccination information are crucial for the success of early COVID-19 prediction models. Our results show that the proposed model can effectively identify SARS-CoV-2 infections during the incubation period. Prospective studies are warranted to explore the application of such prediction models in daily clinical practice.BACKGROUNDThe coronavirus disease 2019 (COVID-19) pandemic has created more devastation among dialysis patients than among the general population. Patient-level prediction models for severe acute respiratory syndrome coronavirus 2 (SARS-CoV-2) infection are crucial for the early identification of patients to prevent and mitigate outbreaks within dialysis clinics. As the COVID-19 pandemic evolves, it is unclear whether or not previously built prediction models are still sufficiently effective.METHODSWe developed a machine learning (XGBoost) model to predict during the incubation period a SARS-CoV-2 infection that is subsequently diagnosed after 3 or more days. We used data from multiple sources, including demographic, clinical, treatment, laboratory, and vaccination information from a national network of hemodialysis clinics, socioeconomic information from the Census Bureau, and county-level COVID-19 infection and mortality information from state and local health agencies. We created prediction models and evaluated their performances on a rolling basis to investigate the evolution of prediction power and risk factors.RESULTFrom April 2020 to August 2020, our machine learning model achieved an area under the receiver operating characteristic curve (AUROC) of 0.75, an improvement of over 0.07 from a previously developed machine learning model published by Kidney360 in 2021. As the pandemic evolved, the prediction performance deteriorated and fluctuated more, with the lowest AUROC of 0.6 in December 2021 and January 2022. Over the whole study period, that is, from April 2020 to February 2022, fixing the false-positive rate at 20%, our model was able to detect 40% of the positive patients. We found that features derived from local infection information reported by the Centers for Disease Control and Prevention (CDC) were the most important predictors, and vaccination status was a useful predictor as well. Whether or not a patient lives in a nursing home was an effective predictor before vaccination, but became less predictive after vaccination.\n\nAriella Mermelstein, Jochen G Raimann, Yuedong Wang, Peter Kotanko, John T Daugirdas\n\nRESULTSIn the studied 396,358 patients, the average ultrafiltration rate in ml/h was related to postdialysis weight (W) in kg: 3W+330. Ultrafiltration rates associated with 20% or 40% higher weight-specific mortality risk were 3W+500 and 3W+630 ml/h, respectively, and were 70 ml/h higher in men than in women. Nineteen percent or 7.5% of patients exceeded ultrafiltration rates associated with a 20% or 40% higher mortality risk, respectively. Low ultrafiltration rates were associated with subsequent weight loss. Ultrafiltration rates associated with a given mortality risk were lower in high-body weight older patients and higher in patients on dialysis for more than 3 years.CONCLUSIONSUltrafiltration rates associated with various levels of higher mortality risk depend on body weight, but not in a 1:1 ratio, and are different in men versus women, in high-body weight older patients, and in high-vintage patients.BACKGROUNDWe hypothesized that the association of ultrafiltration rate with mortality in hemodialysis patients was differentially affected by weight and sex and sought to derive a sex- and weight-indexed ultrafiltration rate measure that captures the differential effects of these parameters on the association of ultrafiltration rate with mortality.METHODSData were analyzed from the US Fresenius Kidney Care (FKC) database for 1 year after patient entry into a FKC dialysis unit (baseline) and over 2 years of follow-up for patients receiving thrice-weekly in-center hemodialysis. To investigate the joint effect of baseline-year ultrafiltration rate and postdialysis weight on survival, we fit Cox proportional hazards models using bivariate tensor product spline functions and constructed contour plots of weight-specific mortality hazard ratios over the entire range of ultrafiltration rate values and postdialysis weights (W).\n\nRichard V Remigio, Hyeonjin Song, Jochen G Raimann, Peter Kotanko, Frank W Maddux, Rachel A Lasky, Xin He, Amir Sapkota\n\nRESULTSWe observed positive associations between inclement weather and missed appointment (rainfall, hurricane and tropical storm, snowfall, snow depth, and wind advisory) when compared with noninclement weather days. The risk of missed appointments was most pronounced during the day of inclement weather (lag 0) for rainfall (incidence rate ratio [RR], 1.03 per 10-mm rainfall; 95% confidence interval [CI], 1.02 to 1.03) and snowfall (RR, 1.02; 95% CI, 1.01 to 1.02). Over 7 days (lag 0-6), hurricane and tropical storm exposures were associated with a 55% higher risk of missed appointments (RR, 1.55; 95% CI, 1.22 to 1.98). Similarly, 7-day cumulative exposure to sustained wind advisories was associated with 29% higher risk (RR, 1.29; 95% CI, 1.25 to 1.31), while wind gusts advisories showed a 34% higher risk (RR, 1.34; 95% CI, 1.29 to 1.39) of missed appointment.CONCLUSIONSInclement weather was associated with higher risk of missed hemodialysis appointments within the Northeastern United States. Furthermore, the association between inclement weather and missed hemodialysis appointments persisted for several days, depending on the inclement weather type.BACKGROUNDNonadherence to hemodialysis appointments could potentially result in health complications that can influence morbidity and mortality. We examined the association between different types of inclement weather and hemodialysis appointment adherence.METHODSWe analyzed health records of 60,135 patients with kidney failure who received in-center hemodialysis treatment at Fresenius Kidney Care clinics across the Northeastern US counties during 2001-2019. County-level daily meteorological data on rainfall, hurricane and tropical storm events, snowfall, snow depth, and wind speed were extracted using National Oceanic and Atmosphere Agency data sources. A time-stratified case-crossover study design with conditional Poisson regression was used to estimate the effect of inclement weather exposures within the Northeastern US region. We applied a distributed lag nonlinear model framework to evaluate the delayed effect of inclement weather for up to 1 week.\n\nKarlien J Ter Meulen, Xiaoling Ye, Yuedong Wang, Len A Usvyat, Frank M van der Sande, Constantijn J Konings, Peter Kotanko, Jeroen P Kooman, Franklin W Maddux\n\nRESULTSWe included 302,613 patients. Baseline phosphate was 5.1±1.2 mg/dl, and mean DR was +0.6±3.3 mg/dl. Across different levels of phosphate, higher levels of DR of phosphate were associated with higher risk of all-cause mortality. In patients with lower levels of phosphate and serum albumin, the effect of a negative DR was most pronounced, whereas in patients with higher phosphate levels, a positive DR was related to increased mortality.KEY POINTSAn increase in serum phosphate variability is an independent risk factor of mortality. The effects of a positive directional range (DR) is most pronounced in patients with high serum phosphate levels whereas the effects of a negative DR is most pronounced in patients with low serum phosphate and/or serum albumin.CONCLUSIONSHigher variability of serum phosphate is related to mortality at all levels of phosphate, especially in lower levels with a negative DR and in low serum albumin levels. This could possibly reflect dietary intake in patients who are already inflamed or malnourished, where a further reduction in serum phosphate should prompt for nutritional evaluation.BACKGROUNDIn maintenance hemodialysis (HD) patients, previous studies have shown that serum phosphate levels have a bidirectional relation to outcome. Less is known about the relation between temporal dynamics of serum phosphate in relation to outcome. We aimed to further explore the relation between serum phosphate variability and all-cause mortality.METHODSAll adult incident HD patients treated in US Fresenius Kidney Care clinics between January 2010 and October 2018 were included. Baseline period was defined as 6 months after initiation of HD and months 7–18 as follow-up period. All-cause mortality was recorded during the follow-up period. The primary metric of variability used was directional range (DR) that is the difference between the largest and smallest values within a time period; DR was positive when the smallest value preceded the largest and negative otherwise. Cox proportional hazards models with spline terms were applied to explore the association between phosphate, DR, and all-cause mortality. In addition, tensor product smoothing splines were computed to further elucidate the interactions of phosphate, DR, and all-cause mortality.\n\nRakesh Malhotra, Sina Rahimi, Ushma Agarwal, Ronit Katz, Ujjala Kumar, Pranav S Garimella, Vineet Gupta, Tushar Chopra, Peter Kotanko, T Alp Ikizler, Britta Larsen, Lisa Cadmus-Bertram, Joachim H Ix\n\nRESULTSOut of 55 participants, 46 participants completed the 12-week intervention (23 per arm). The mean age was 62 (± 14 SD) years; 44% were Black, and 36% were Hispanic. At baseline, step count (structured feedback intervention: 3,704 [1,594] vs wearable activity tracker alone: 3,808 [1,890]) and other participant characteristics were balanced between the arms. We observed a larger change in daily step count in the structured feedback arm at 12 weeks relative to use of the wearable activity tracker alone arm (Δ 920 [±580 SD] versus Δ 281 [±186 SD] steps; between-group difference Δ 639 [±538 SD] steps; P<0.05).RATIONALE & OBJECTIVEPeople with end-stage kidney disease (ESKD) have very low physical activity, and the degree of inactivity is strongly associated with morbidity and mortality. We assessed the feasibility and effectiveness of a 12-week intervention coupling a wearable activity tracker (FitBit) and structured feedback coaching versus wearable activity tracker alone on changes in physical activity in hemodialysis patients.INTERVENTIONSAll participants wore a Fitbit Charge 2 tracker for a minimum of 12 weeks. Participants were randomly assigned 1:1 to a wearable activity tracker plus a structured feedback intervention versus the wearable activity tracker alone. The structured feedback group was counseled weekly on steps achieved after randomization.TRIAL REGISTRATIONRegistered at ClinicalTrials.gov with study number NCT05241171.LIMITATIONSSingle-center study and small sample size.STUDY DESIGNRandomized controlled trial.OUTCOMEThe outcome was step count, and the main parameter of interest was the absolute change in daily step count, averaged per week, from baseline to completion of 12 weeks intervention. In the intention-to-treat analysis, mixed-effect linear regression analysis was used to evaluate change in daily step count from baseline to 12-weeks in both arms.FUNDINGGrants from industry (Satellite Healthcare) and government (National Institute for Diabetes and Digestive and Kidney Diseases (NIDDK).CONCLUSIONThis pilot randomized controlled trial demonstrated that structured feedback coupled with a wearable activity tracker led to a greater daily step count that was sustained over 12 weeks relative to a wearable activity tracker alone. Future studies are required to determine longer-term sustainability of the intervention and potential health benefits in hemodialysis patients.SETTING & PARTICIPANTS55 participants with ESKD receiving hemodialysis who were able to walk with or without assistive devices recruited from a single academic hemodialysis unit between January 2019 and April 2020.\n\nChristina H Wang, Dan Negoianu, Hanjie Zhang, Sabrina Casper, Jesse Y Hsu, Peter Kotanko, Jochen Raimann, Laura M Dember\n\nRESULTSDuring 180,319 HD sessions among 2554 patients, PRR had high within-patient and between-patient variability. Female sex and hypoalbuminemia were associated with low PRR at multiple time points during the first hour of HD. Low starting PRR has a higher hazard of IDH, whereas high starting PRR was protective (hazard ratio [HR], 1.26, 95% confidence interval [CI], 1.18 to 1.35 versus HR, 0.79, 95% CI, 0.73 to 0.85, respectively). However, when accounting for time-varying PRR and time-varying confounders, compared with a moderate PRR, while a consistently low PRR was associated with increased risk of hypotension (odds ratio [OR], 1.09, 95% CI, 1.02 to 1.16), a consistently high PRR had a stronger association with hypotension within the next 15 minutes (OR, 1.38, 95% CI, 1.30 to 1.45).KEY POINTSDirectly studying plasma refill rate (PRR) during hemodialysis (HD) can offer insight into physiologic mechanisms that change throughout HD. PRR at the start and during HD is associated with intradialytic hypotension, independent of ultrafiltration rate. A rising PRR during HD may be an early indicator of compensatory mechanisms for impending circulatory instability.CONCLUSIONSWe present a straightforward technique to quantify plasma refill that could easily integrate with devices that monitor hematocrit during HD. Our study highlights how examining patterns of plasma refill may enhance our understanding of circulatory changes during HD, an important step to understand how current technology might be used to improve hemodynamic instability.BACKGROUNDAttaining the optimal balance between achieving adequate volume removal while preserving organ perfusion is a challenge for patients receiving maintenance hemodialysis (HD). Current strategies to guide ultrafiltration are inadequate.METHODSWe developed an approach to calculate the plasma refill rate (PRR) throughout HD using hematocrit and ultrafiltration data in a retrospective cohort of patients receiving maintenance HD at 17 dialysis units from January 2017 to October 2019. We studied whether (1) PRR is associated with traditional risk factors for hemodynamic instability using logistic regression, (2) low starting PRR is associated with intradialytic hypotension (IDH) using Cox proportional hazard regression, and (3) time-varying PRR throughout HD is associated with hypotension using marginal structural modeling.\n\nPaulo Paneque Galuzio, Alhaji Cherif\n\nWe reviewed some of the latest advancements in the use of mathematical models in nephrology. We looked over 2 distinct categories of mathematical models that are widely used in biological research and pointed out some of their strengths and weaknesses when applied to health care, especially in the context of nephrology. A mechanistic dynamical system allows the representation of causal relations among the system variables but with a more complex and longer development/implementation phase. Artificial intelligence/machine learning provides predictive tools that allow identifying correlative patterns in large data sets, but they are usually harder-to-interpret black boxes. Chronic kidney disease (CKD), a major worldwide health problem, generates copious quantities of data that can be leveraged by choice of the appropriate model; also, there is a large number of dialysis parameters that need to be determined at every treatment session that can benefit from predictive mechanistic models. Following important steps in the use of mathematical methods in medical science might be in the intersection of seemingly antagonistic frameworks, by leveraging the strength of each to provide better care.\n\nMurilo Guedes, Brian Bieber, Indranil Dasgupta, Almudena Vega, Kosaku Nitta, Steven Brunelli, John Hartman, Jochen G Raimann, Bruce M Robinson, Ronald L Pisoni\n\nMineral bone disorder (MBD) is a frequent consequence of chronic kidney disease, more so in patients with kidney failure treated by kidney replacement therapy. Despite the wide availability of interventions to control serum phosphate and parathyroid hormone levels, unmet gaps remain on optimal targets and best practices, leading to international practice pattern variations over time. In this Special Report, we describe international trends from the Dialysis Outcomes and Practice Patterns Study (DOPPS) for MBD biomarkers and treatments from 2002-2021, including data from a group of 7 European countries (Belgium, France, Germany, Italy, Spain, Sweden, United Kingdom), Japan, and the United States. From 2002-2012, mean phosphate levels declined in Japan (5.6 to 5.2 mg/dL), Europe (5.5 to 4.9 mg/dL), and the United States (5.7 to 5.0 mg/dL). Since then, levels rose in the United States (to mean 5.6 mg/dL, 2021), were stable in Japan (5.3 mg/dL), and declined in Europe (4.8 mg/dL). In 2021, 52% (United States), 27% (Europe), and 39% (Japan) had phosphate >5.5 mg/dL. In the United States, overall phosphate binder use was stable (80%-84% over 2015-2021), and parathyroid hormone levels rose only modestly. Although these results potentially stem from pervasive knowledge gaps in clinical practice, the noteworthy steady increase in serum phosphate in the United States over the past decades may be consequential to patient outcomes, an uncertainty that hopefully will soon be addressed by ongoing clinical trials. The DOPPS will continue to monitor international trends as new interventions and strategies ensue for MBD management in chronic kidney disease.\n\nSheetal Chaudhuri, John Larkin, Murilo Guedes, Yue Jiao, Peter Kotanko, Yuedong Wang, Len Usvyat, Jeroen P Kooman\n\nMATERIALS AND METHODSWe included data HD patients who had data across a baseline period of at least 1 year and 1 day in the internationally representative Monitoring Dialysis Outcomes (MONDO) Initiative dataset. Twenty-three input parameters considered in the model were chosen in an a priori manner. The prediction model used 1 year baseline data to predict death in the following 3 years. The dataset was randomly split into 80% training data and 20% testing data for model development. Two different modeling techniques were used to build the mortality prediction model.DISCUSSIONIn the internationally representative MONDO data for HD patients, we describe the development of a ML model and a traditional statistical model that was suitable for classification of a prevalent HD patient's 3-year risk of death. While both models had a reasonably high AUROC, the ML model was able to identify levels of hematocrit (HCT) as an important risk factor in mortality. If implemented in clinical practice, such proof-of-concept models could be used to provide pre-emptive care for HD patients.INTRODUCTIONSeveral factors affect the survival of End Stage Kidney Disease (ESKD) patients on dialysis. Machine learning (ML) models may help tackle multivariable and complex, often non-linear predictors of adverse clinical events in ESKD patients. In this study, we used advanced ML method as well as a traditional statistical method to develop and compare the risk factors for mortality prediction model in hemodialysis (HD) patients.FINDINGSA total of 95,142 patients were included in the analysis sample. The area under the receiver operating curve (AUROC) of the model on the test data with XGBoost ML model was 0.84 on the training data and 0.80 on the test data. AUROC of the logistic regression model was 0.73 on training data and 0.75 on test data. Four out of the top five predictors were common to both modeling strategies.\n\nDalia E Yousif, Xiaoling Ye, Stefano Stuard, Juan Berbessi, Adrian M Guinsburg, Len A Usvyat, Jochen G Raimann, Jeroen P Kooman, Frank M van der Sande, Neill Duncan, Kevin J Woollard, Rupert Bright, Charles Pusey, Vineet Gupta, Joachim H Ix, Peter Kotanko, Rakesh Malhotra\n\nRESULTSWe studied 18,726 incident hemodialysis patients. Their age at dialysis initiation was 71.3 ± 11.9 years; 10,802 (58%) were males. Within the first 6 months, 2068 (11%) patients died, and 12,295 patients (67%) survived >36 months (survivor cohort). Hemodialysis patients who died showed a distinct biphasic pattern of change in inflammatory markers where an initial decline of inflammation was followed by a rapid rise that was consistently evident approximately 6 months before death. This pattern was similar in all patients who died and was consistent across the survival time intervals. In contrast, in the survivor cohort, we observed initial decline of inflammation followed by sustained low levels of inflammatory biomarkers.CONCLUSIONOur international study of incident hemodialysis patients highlights a temporal relationship between serial measurements of inflammatory markers and patient survival. This finding may inform the development of prognostic models, such as the integration of dynamic changes in inflammatory markers for individual risk profiling and guiding preventive and therapeutic interventions.INTRODUCTIONInflammation is highly prevalent among patients with end-stage kidney disease and is associated with adverse outcomes. We aimed to investigate longitudinal changes in inflammatory markers in a diverse international incident hemodialysis patient population.METHODSThe MONitoring Dialysis Outcomes (MONDO) Consortium encompasses hemodialysis databases from 31 countries in Europe, North America, South America, and Asia. The MONDO database was queried for inflammatory markers (total white blood cell count [WBC], neutrophil count, lymphocyte count, serum albumin, and C-reactive protein [CRP]) and hemoglobin levels in incident hemodialysis patients. Laboratory parameters were measured every month. Patients were stratified by survival time (≤6 months, >6 to 12 months, >12 to 18 months, >18 to 24 months, >24 to 30 months, >30 to 36 months, and >36 months) following dialysis initiation. We used cubic B-spline basis function to evaluate temporal changes in inflammatory parameters in relationship with patient survival.\n\nAna Paula Bernardo, Paola Carioni, Stefano Stuard, Peter Kotanko, Len A Usvyat, Vratislava Kovarova, Otto Arkossy, Francesco Bellocchio, Antonio Tupputi, Federica Gervasoni, Anke Winter, Yan Zhang, Hanjie Zhang, Pedro Ponce, Luca Neri\n\nRESULTSIn the effectiveness analysis concerning mRNA vaccines, we observed 850 SARS-CoV-2 infections and 201 COVID-19 related deaths among the 28110 patients during a mean follow up of 44 ± 40 days. In the effectiveness analysis concerning viral-carrier vaccines, we observed 297 SARS-CoV-2 infections and 64 COVID-19 related deaths among 12888 patients during a mean follow up of 48 ± 32 days. We observed 18.5/100-patient-year and 8.5/100-patient-year fewer infections and 5.4/100-patient-year and 5.2/100-patient-year fewer COVID-19 related deaths among patients vaccinated with mRNA and viral-carrier vaccines respectively, compared to matched unvaccinated controls. Estimated vaccine effectiveness at days 15, 30, 60 and 90 after the first dose of a mRNA vaccine was: for infection, 41.3%, 54.5%, 72.6% and 83.5% and, for death, 33.1%, 55.4%, 80.1% and 91.2%. Estimated vaccine effectiveness after the first dose of a viral-carrier vaccine was: for infection, 38.3% without increasing over time and, for death, 56.6%, 75.3%, 92.0% and 97.4%.CONCLUSIONIn this large, real-world cohort of hemodialyzed patients, mRNA and viral-carrier COVID-19 vaccines were associated with reduced COVID-19 related mortality. Additionally, we observed a strong reduction of SARS-CoV-2 infection in hemodialysis patients receiving mRNA vaccines.BACKGROUNDHemodialysis patients have high-risk of severe SARS-CoV-2 infection but were unrepresented in randomized controlled trials evaluating the safety and efficacy of COVID-19 vaccines. We estimated the real-world effectiveness of COVID-19 vaccines in a large international cohort of hemodialysis patients.METHODSIn this historical, 1:1 matched cohort study, we included adult hemodialysis patients receiving treatment from December 1, 2020, to May 31, 2021. For each vaccinated patient, an unvaccinated control was selected among patients registered in the same country and attending a dialysis session around the first vaccination date. Matching was based on demographics, clinical characteristics, past COVID-19 infections and a risk score representing the local background risk of infection at vaccination dates. We estimated the effectiveness of mRNA and viral-carrier COVID-19 vaccines in preventing infection and mortality rates from a time-dependent Cox regression stratified by country.\n\nAdrián M Guinsburg, Yue Jiao, María Inés Díaz Bessone, Caitlin K Monaghan, Beatriz Magalhães, Michael A Kraus, Peter Kotanko, Jeffrey L Hymes, Robert J Kossmann, Juan Carlos Berbessi, Franklin W Maddux, Len A Usvyat, John W Larkin\n\nRESULTSAmong HD patients with COVID-19, 28.8% (1,001/3,473) died in LatAm and 20.5% (4,426/21,624) died in North America. Mortality occurred earlier in LatAm versus North America; 15.0% and 7.3% of patients died within 0-14 days, 7.9% and 4.6% of patients died within 15-30 days, and 5.9% and 8.6% of patients died > 30 days after COVID-19 presentation, respectively. Area under curve ranged from 0.73 to 0.83 across prediction models in both regions. Top predictors of death after COVID-19 consistently included older age, longer vintage, markers of poor nutrition and more inflammation in both regions at all timepoints. Unique patient attributes (higher BMI, male sex) were top predictors of mortality during 0-14 and 15-30 days after COVID-19, yet not mortality > 30 days after presentation.CONCLUSIONSFindings showed distinct profiles of mortality in COVID-19 in LatAm and North America throughout 2020. Mortality rate was higher within 0-14 and 15-30 days after COVID-19 in LatAm, while mortality rate was higher in North America > 30 days after presentation. Nonetheless, a remarkable proportion of HD patients died > 30 days after COVID-19 presentation in both regions. We were able to develop a series of suitable prognostic prediction models and establish the top predictors of death in COVID-19 during shorter-, intermediate-, and longer-term follow up periods.BACKGROUNDWe developed machine learning models to understand the predictors of shorter-, intermediate-, and longer-term mortality among hemodialysis (HD) patients affected by COVID-19 in four countries in the Americas.METHODSWe used data from adult HD patients treated at regional institutions of a global provider in Latin America (LatAm) and North America who contracted COVID-19 in 2020 before SARS-CoV-2 vaccines were available. Using 93 commonly captured variables, we developed machine learning models that predicted the likelihood of death overall, as well as during 0-14, 15-30, > 30 days after COVID-19 presentation and identified the importance of predictors. XGBoost models were built in parallel using the same programming with a 60%:20%:20% random split for training, validation, & testing data for the datasets from LatAm (Argentina, Columbia, Ecuador) and North America (United States) countries.\n\nPaulo P Galuzio, Alhaji Cherif, Xia Tao, Ohnmar Thwin, Hanjie Zhang, Stephan Thijssen, Peter Kotanko\n\nIn patients with kidney failure treated by hemodialysis, intradialytic arterial oxygen saturation (SaO2) time series present intermittent high-frequency high-amplitude oximetry patterns (IHHOP), which correlate with observed sleep-associated breathing disturbances. A new method for identifying such intermittent patterns is proposed. The method is based on the analysis of recurrence in the time series through the quantification of an optimal recurrence threshold ([Formula: see text]). New time series for the value of [Formula: see text] were constructed using a rolling window scheme, which allowed for real-time identification of the occurrence of IHHOPs. The results for the optimal recurrence threshold were confronted with standard metrics used in studies of obstructive sleep apnea, namely the oxygen desaturation index (ODI) and oxygen desaturation density (ODD). A high correlation between [Formula: see text] and the ODD was observed. Using the value of the ODI as a surrogate to the apnea-hypopnea index (AHI), it was shown that the value of [Formula: see text] distinguishes occurrences of sleep apnea with great accuracy. When subjected to binary classifiers, this newly proposed metric has great power for predicting the occurrences of sleep apnea-related events, as can be seen by the larger than 0.90 AUC observed in the ROC curve. Therefore, the optimal threshold [Formula: see text] from recurrence analysis can be used as a metric to quantify the occurrence of abnormal behaviors in the arterial oxygen saturation time series.\n\nGabriela F Dias, Sara S Tozoni, Gabriela Bohnen, Beatriz A K van Spitzenbergen, Nadja Grobe, Lia S Nakao, Roberto Pecoits-Filho, Peter Kotanko, Andréa N Moreno-Amaral\n\nOxidative stress (OS) is essential in uremia-associated comorbidities, including renal anemia. Complications experienced by hemodialysis (HD) patients, such as hypoxemia and uremic toxins accumulation, induce OS and premature death of red blood cells (RBC). We aimed to characterize reactive oxygen species (ROS) production and antioxidant pathways in HD-RBC and RBC from healthy controls (CON-RBC) and evaluate the role of uremia and hypoxia in these pathways. ROS production, xanthine oxidase (XO) and superoxide dismutase (SOD) activities, glutathione (GSH), and heme oxygenase-1 (HO-1) levels were measured using flow cytometry or spectrophotometry in CON-RBC and HD-RBC (pre- and post-HD), at baseline and after 24 h incubation with uremic serum (S-HD) and/or under hypoxic conditions (5% O2 ). Ketoprofen was used to inhibit RBC uremic toxins uptake. HD-RBC showed higher ROS levels and lower XO activity than CON-RBC, particularly post-HD. GSH levels were lower, while SOD activity and HO-1 levels of HD-RBC were higher than control. Hypoxia per se triggered ROS production in CON-RBC and HD-RBC. S-HD, on top of hypoxia, increased ROS levels. Inhibition of uremic toxins uptake attenuated ROS of CON and HD-RBC under hypoxia and uremia. CON-RBC in uremia and hypoxia showed lower GSH levels than cells in normoxia and non-uremic conditions. Redox mechanisms of HD-RBC are altered and prone to oxidation. Uremic toxins and hypoxia play a role in unbalancing these systems. Hypoxia and uremia participate in the pathogenesis of OS in HD-RBC and might induce RBC death and thus compound anemia.\n\nDavid J Jörg, Doris H Fuertinger, Alhaji Cherif, David A Bushinsky, Ariella Mermelstein, Jochen G Raimann, Peter Kotanko\n\nOur bones are constantly being renewed in a fine-tuned cycle of destruction and formation that helps keep them healthy and strong. However, this process can become imbalanced and lead to osteoporosis, where the bones are weakened and have a high risk of fracturing. This is particularly common post-menopause, with one in three women over the age of 50 experiencing a broken bone due to osteoporosis. There are several drug types available for treating osteoporosis, which work in different ways to strengthen bones. These drugs can be taken individually or combined, meaning that a huge number of drug combinations and treatment strategies are theoretically possible. However, it is not practical to test the effectiveness of all of these options in human trials. This could mean that patients are not getting the maximum potential benefit from the drugs available. Jörg et al. developed a mathematical model to predict how different osteoporosis drugs affect the process of bone renewal in the human body. The model could then simulate the effect of changing the order in which the therapies were taken, which showed that the sequence had a considerable impact on the efficacy of the treatment. This occurs because different drugs can interact with each other, leading to an improved outcome when they work in the right order. These results suggest that people with osteoporosis may benefit from altered treatment schemes without changing the type or amount of medication taken. The model could suggest new treatment combinations that reduce the risk of bone fracture, potentially even developing personalised plans for individual patients based on routine clinical measurements in response to different drugs.\n\nXin Wang, Leticia M Tapia Silva, Milind Nikam, Sandip Mitra, Syed Shaukat Abbas Zaidi, Nadja Grobe\n\nThe aim of the paper is to summarize the current understanding of the molecular biology of arteriovenous fistula (AVF). It intends to encourage vascular access teams, care providers, and scientists, to explore new molecular tools for assessing the suitability of patients for AVF as vascular access for maintenance hemodialysis (HD). This review also highlights most recent discoveries and may serve as a guide to explore biomarkers and technologies for the assessment of kidney disease patients choosing to start kidney replacement therapy. Objective criteria for AVF eligibility are lacking partly because the underlying physiology of AVF maturation is poorly understood. Several molecular processes during a life cycle of an AVF, even before creation, can be characterized by measuring molecular fingerprints using newest \"omics\" technologies. In addition to hypothesis-driven strategies, untargeted approaches have the potential to reveal the interplay of hundreds of metabolites, transcripts, proteins, and genes underlying cardiovascular adaptation and vascular access-related adjustments at any given timepoint of a patient with kidney disease. As a result, regular monitoring of modifiable, molecular risk factors together with clinical assessment could help to reduce AVF failure rates, increase patency, and improve long-term outcomes. For the future, identification of vulnerable patients based on the assessment of biological markers of AVF maturation at different stages of the life cycle may aid in individualizing vascular access recommendations.\n\nXiaoling Wang, Maggie Han, Lemuel Rivera Fuentes, Ohnmar Thwin, Nadja Grobe, Kevin Wang, Yuedong Wang, Peter Kotanko\n\nRESULTSForty-two patients had three doses of mRNA1273. Compared to levels prior to the third dose, nAb-WT increased 18-fold (peak at day 23) and nAb-Omicron increased 23-fold (peak at day 24) after the third dose. Peak nAb-WT exceeded peak nAb-Omicron 27-fold. Twenty-one patients had COVID-19 between December 24, 2021, and February 2, 2022. Following COVID-19, nAb-WT and nAb-Omicron increased 12- and 40-fold, respectively. While levels of vaccinal and post-COVID nAb-WT were comparable, post-COVID nAb-Omicron levels were 3.2 higher than the respective peak vaccinal nAb-Omicron. Four immunocompromised patients having reasons other than end-stage kidney disease have very low to no nAb after the third dose or COVID-19.CONCLUSIONSOur results suggest that most hemodialysis patients have a strong humoral response to the third dose of vaccination and an even stronger post-COVID-19 humoral response. Nevertheless, nAb levels clearly decay over time. These findings may inform ongoing discussions regarding a fourth vaccination in hemodialysis patients.BACKGROUNDIn hemodialysis patients, a third vaccination is frequently administered to augment protection against coronavirus disease 2019 (COVID-19). However, the newly emerged B.1.1.159 (Omicron) variant may evade vaccinal protection more easily than previous strains. It is of clinical interest to better understand the neutralizing activity against severe acute respiratory syndrome coronavirus 2 (SARS-CoV-2) variants after booster vaccine or COVID-19 infection in these mostly immunocompromised patients.METHODSHemodialysis patients from four dialysis centers were recruited between June 2021 and February 2022. Each patient provided a median of six serum samples. SARS-CoV-2 neutralizing antibodies (nAbs) against wild type (WT) or Omicron were measured using the GenScript SARS-CoV-2 Surrogate Virus Neutralization Test Kit.\n\nJeroen Peter Kooman, Paola Carioni, Vratislava Kovarova, Otto Arkossy, Anke Winter, Yan Zhang, Francesco Bellocchio, Peter Kotanko, Hanjie Zhang, Len Usvyat, John Larkin, Stefano Stuard, Luca Neri\n\nRESULTSWe included 9,211 patients (age 65.4 ± 13.7 years, dialysis vintage 4.2 ± 3.7 years) eligible for the study. The 30-day mortality rate was 20.8%. In LR models, several potentially modifiable factors were associated with higher mortality: body mass index (BMI) 30-40 kg/m2 (OR: 1.28, CI: 1.10-1.50), single-pool Kt/V (OR off-target vs on-target: 1.19, CI: 1.02-1.38), overhydration (OR: 1.15, CI: 1.01-1.32), and both low (<2.5 mg/dl) and high (≥5.5 mg/dl) serum phosphate levels (OR: 1.52, CI: 1.07-2.16 and OR: 1.17, CI: 1.01-1.35). On-line hemodiafiltration was protective in the model using KPIs (OR: 0.86, CI: 0.76-0.97). SHapley Additive exPlanations analysis in XGBoost models shows a high influence on prediction for several modifiable factors as well, including inflammatory parameters, high BMI, and fluid overload. In both LR and XGBoost models, age, gender, and comorbidities were strongly associated with mortality.CONCLUSIONBoth conventional and machine learning techniques showed that KPIs and modifiable risk factors in different dimensions ascertained 6 months before the COVID-19 suspicion date were associated with 30-day COVID-19-related mortality. Our results suggest that adequate dialysis and achieving KPI targets remain of major importance during the COVID-19 pandemic as well.INTRODUCTIONPatients with end-stage kidney disease face a higher risk of severe outcomes from SARS-CoV-2 infection. Moreover, it is not well known to what extent potentially modifiable risk factors contribute to mortality risk. In this historical cohort study, we investigated the incidence and risk factors for 30-day mortality among hemodialysis patients with SARS-CoV-2 infection treated in the European Fresenius Medical Care NephroCare network using conventional and machine learning techniques.METHODSWe included adult hemodialysis patients with the first documented SARS-CoV-2 infection between February 1, 2020, and March 31, 2021, registered in the clinical database. The index date for the analysis was the first SARS-CoV-2 suspicion date. Patients were followed for up to 30 days until April 30, 2021. Demographics, comorbidities, and various modifiable risk factors, expressed as continuous parameters and as key performance indicators (KPIs), were considered to tap multiple dimensions including hemodynamic control, nutritional state, and mineral metabolism in the 6 months before the index date. We used logistic regression (LR) and XGBoost models to assess risk factors for 30-day mortality.\n\nBernard Canaud, Jeroen Kooman, Andreas Maierhofer, Jochen Raimann, Jens Titze, Peter Kotanko\n\nNew physiologic findings related to sodium homeostasis and pathophysiologic associations require a new vision for sodium, fluid and blood pressure management in dialysis-dependent chronic kidney disease patients. The traditional dry weight probing approach that has prevailed for many years must be reviewed in light of these findings and enriched by availability of new tools for monitoring and handling sodium and water imbalances. A comprehensive and integrated approach is needed to improve further cardiac health in hemodialysis (HD) patients. Adequate management of sodium, water, volume and hemodynamic control of HD patients relies on a stepwise approach: the first entails assessment and monitoring of fluid status and relies on clinical judgement supported by specific tools that are online embedded in the HD machine or devices used offline; the second consists of acting on correcting fluid imbalance mainly through dialysis prescription (treatment time, active tools embedded on HD machine) but also on guidance related to diet and thirst management; the third consist of fine tuning treatment prescription to patient responses and tolerance with the support of innovative tools such as artificial intelligence and remote pervasive health trackers. It is time to come back to sodium and water imbalance as the root cause of the problem and not to act primarily on their consequences (fluid overload, hypertension) or organ damage (heart; atherosclerosis, brain). We know the problem and have the tools to assess and manage in a more precise way sodium and fluid in HD patients. We strongly call for a sodium first approach to reduce disease burden and improve cardiac health in dialysis-dependent chronic kidney disease patients.\n\nMurilo Guedes, Liz Wallim, Camila R Guetter, Yue Jiao, Vladimir Rigodon, Chance Mysayphonh, Len A Usvyat, Pasqual Barretti, Peter Kotanko, John W Larkin, Franklin W Maddux, Roberto Pecoits-Filho, Thyago Proenca de Moraes\n\nRESULTSWe used data from 4,285 PD patients (Brazil n = 1,388 and United States n = 2,897). Model estimates showed lower vitality levels within 90 days of starting PD were associated with a higher risk of mortality, which was consistent in Brazil and the United States cohorts. In the multivariate survival model, each 10-unit increase in vitality score was associated with lower risk of all-cause mortality in both cohorts (Brazil HR = 0.79 [95%CI 0.70 to 0.90] and United States HR = 0.90 [95%CI 0.88 to 0.93], pooled HR = 0.86 [95%CI 0.75 to 0.98]). Results for all models provided consistent effect estimates.CONCLUSIONSAmong patients in Brazil and the United States, lower vitality score in the initial months of PD was independently associated with all-cause mortality.BACKGROUNDWe tested if fatigue in incident Peritoneal Dialysis associated with an increased risk for mortality, independently from main confounders.METHODSWe conducted a side-by-side study from two of incident PD patients in Brazil and the United States. We used the same code to independently analyze data in both countries during 2004 to 2011. We included data from adults who completed KDQOL-SF vitality subscale within 90 days after starting PD. Vitality score was categorized in four groups: >50 (high vitality), ≥40 to ≤50 (moderate vitality), >35 to <40 (moderate fatigue), ≤35 (high fatigue; reference group). In each country's cohort, we built four distinct models to estimate the associations between vitality (exposure) and all-cause mortality (outcome): (i) Cox regression model; (ii) competing risk model accounting for technique failure events; (iii) multilevel survival model of clinic-level clusters; (iv) multivariate regression model with smoothing splines treating vitality as a continuous measure. Analyses were adjusted for age, comorbidities, PD modality, hemoglobin, and albumin. A mixed-effects meta-analysis was used to pool hazard ratios (HRs) from both cohorts to model mortality risk for each 10-unit increase in vitality.\n\nLin-Chun Wang, Jochen G Raimann, Xia Tao, Priscila Preciado, Ohnmar Thwin, Laura Rosales, Stephan Thijssen, Peter Kotanko, Fansan Zhu\n\nDISCUSSIONAlthough segmental eight-point bioimpedance techniques provided comparable TBW measurements not affected by standing over a period of 10-15 min, the ECW/TBW ratio appeared to be significantly lower in InBody compared with Seca and Hydra. Results from our study showed lack of agreement between different bioimpedance devices; direct comparison of ECW, ICW, and ECW/TBW between different devices should be avoided and clinicians should use the same device to track the fluid status in their HD population in a longitudinal direction.INTRODUCTIONSegmental eight-point bioimpedance has been increasingly used in practice. However, whether changes in bioimpedance analysis components before and after hemodialysis (HD) using this technique in a standing position is comparable to traditional whole-body wrist-to-ankle method is still unclear. We aimed to investigate the differences between two eight-point devices (InBody 770 and Seca mBCA 514) and one wrist-to-ankle (Hydra 4200) in HD patients and healthy subjects in a standing position.FINDINGSOverall, total body water (TBW) was not different between the three devices, but InBody showed lower extracellular water (ECW) and higher intracellular water (ICW) compared to the other two devices. When intradialytic weight loss was used as a surrogate for changes in ECW (∆ECW) and changes in TBW (∆TBW), ∆ECW was underestimated by Hydra (-0.79 ± 0.89 L, p < 0.01), InBody (-1.44 ± 0.65 L, p < 0.0001), and Seca (-0.32 ± 1.34, n.s.). ∆TBW was underestimated by Hydra (-1.14 ± 2.81 L, n.s.) and InBody (-0.52 ± 0.85 L, p < 0.05) but overestimated by Seca (+0.93 ± 3.55 L, n.s.).METHODSThirteen HD patients were studied pre- and post-HD, and 12 healthy subjects once. Four measurements were performed in the following order: InBody; Seca; Hydra; and InBody again. Electrical equivalent models by each bioimpedance method and the fluid volume estimates by each device were also compared.\n\nUsama Hussein, Monica Cimini, Garry J Handelman, Jochen G Raimann, Li Liu, Samer R Abbas, Peter Kotanko, Nathan W Levin, Fredric O Finkelstein, Fansan Zhu\n\nDiagnosis of fluid overload (FO) in early stage is essential to manage fluid balance of patients with chronic kidney disease (CKD) and to prevent cardiovascular disease (CVD). However, the identification of fluid status in patients with CKD is largely dependent on the physician's clinical acumen. The ratio of fluid overload to extracellular volume (FO/ECV) has been used as a reference to assess fluid status. The primary aim of this study was to compare FO/ECV with other bioimpedance methods and clinical assessments in patients with CKD. Whole body ECV, intracellular volume (ICV), total body water (TBW), and calf normalized resistivity (CNR) were measured (Hydra 4200). Thresholds of FO utilizing CNR and ECV/TBW were derived by receiver operator characteristic (ROC) analysis based on data from pooled patients with CKD and healthy subjects (HSs). Clinical assessments of FO in patients with CKD were performed by nephrologists. Patients with CKD (stage 3 and stage 4) (n = 50) and HSs (n = 189) were studied. The thresholds of FO were ≤14.3 (10-2 Ωm3/kg) for females and ≤13.1 (10-2 Ωm3/kg) for males using CNR and ≥0.445 in females and ≥0.434 in males using ECV/TBW. FO was diagnosed in 78%, 62%, and 52% of patients with CKD by CNR, FO/ECV, and ECV/TBW, respectively, whereas only 24% of patients with CKD were diagnosed to be FO by clinical assessment. The proportion of FO in patients with nondialysis CKD was largely underestimated by clinical assessment compared with FO/ECV, CNR, and ECV/TBW. CNR and FO/ECV methods were more sensitive than ECV/TBW in identifying fluid overload in these patients with CKD.NEW & NOTEWORTHY We found that fluid overload (FO) in patients with nondialysis CKD was largely underestimated by clinical assessment compared with bioimpedance methods, which was majorly due to lack of appropriate techniques to assess FO. In addition, although degree of FO by bioimpedance markers positively correlated with the age in healthy subjects (HSs), no difference was observed in the three hydration markers between groups of 50 ≤ age <70 yr and age ≥70 yr in the patients with CKD.\n\nJochen G Raimann, Yuedong Wang, Ariella Mermelstein, Peter Kotanko, John T Daugirdas\n\nRESULTSIn the studied 2542 patients, UFR not scaled to body weight was strongly associated with MHR, whereas postdialysis weight was inversely associated with MHR. MHR crossed 1.5 when unscaled UFR exceeded 1000 ml/h, and this relationship was largely independent of postdialysis weight in the range of 80 to 140 kg. A UFR warning level associated with a lower MHR of 1.3 would be 900 ml/h, whereas the UFR associated with an MHR of 1.0 was patient-size dependent. The MHR when exceeding a UFR threshold of 13 ml/h per kg was dependent on patient weight (MHR = 1.20, 1.45, and >2.0 for a 60, 80, and 100 kg patient, respectively).CONCLUSIONUFR thresholds based on unscaled UFR give more uniform risk levels for patients of different sizes than thresholds based on UFR/kg.INTRODUCTIONOne proposed threshold ultrafiltration rate (UFR) of concern in hemodialysis patients is 13 ml/h per kg. We evaluated associations among UFR, postdialysis weight, and mortality to determine whether exceeding such a threshold would result in similar levels of risk for patients of different body weights.METHODSData were analyzed in this retrospective cohort study for 1 year following dialysis initiation (baseline) and over 2 years of follow-up in incident patients receiving thrice-weekly in-center hemodialysis. Patient-level UFR was averaged over the baseline period. To investigate the joint effect of UFR and postdialysis weight on survival, we fit Cox proportional hazards models using bivariate tensor product spline functions, adjusting for sex, race, age, diabetes, and predialysis serum albumin, phosphorus, and systolic blood pressure (BP). We constructed contour plots of mortality hazard ratios (MHRs) over the entire range of UFR values and postdialysis weights.\n\nBernard Canaud, Jeroen P Kooman, Nicholas M Selby, Maarten Taal, Andreas Maierhofer, Pascal Kopperschmidt, Susan Francis, Allan Collins, Peter Kotanko\n\nThe development of maintenance hemodialysis (HD) for end stage kidney disease patients is a success story that continues to save many lives. Nevertheless, intermittent renal replacement therapy is also a source of recurrent stress for patients. Conventional thrice weekly short HD is an imperfect treatment that only partially corrects uremic abnormalities, increases cardiovascular risk, and exacerbates disease burden. Altering cycles of fluid loading associated with cardiac stretching (interdialytic phase) and then fluid unloading (intradialytic phase) likely contribute to cardiac and vascular damage. This unphysiologic treatment profile combined with cyclic disturbances including osmotic and electrolytic shifts may contribute to morbidity in dialysis patients and augment the health burden of treatment. As such, HD patients are exposed to multiple stressors including cardiocirculatory, inflammatory, biologic, hypoxemic, and nutritional. This cascade of events can be termed the dialysis stress storm and sickness syndrome. Mitigating cardiovascular risk and morbidity associated with conventional intermittent HD appears to be a priority for improving patient experience and reducing disease burden. In this in-depth review, we summarize the hidden effects of intermittent HD therapy, and call for action to improve delivered HD and develop treatment schedules that are better tolerated and associated with fewer adverse effects.\n\nRasha Hussein, Murilo Guedes, Nada Ibraheim, Mazin M Ali, Amal El-Tahir, Nahla Allam, Hussain Abuakar, Roberto Pecoits-Filho, Peter Kotanko\n\nOBJECTIVESDespite the possibility of concurrent infection with COVID-19 and malaria, little is known about the clinical course of coinfected patients. We analysed the clinical outcomes of patients with concurrent COVID-19 and malaria infection.RESULTSWe included 591 patients with confirmed COVID-19 diagnosis who were also tested for malaria. Mean (SD) age was 58 (16.2) years, 446/591 (75.5%) were males. Malaria was diagnosed in 270/591 (45.7%) patients. Most malaria patients were infected by Plasmodium falciparum (140/270; 51.9%), while 121/270 (44.8%) were coinfected with Plasmodium falciparum and Plasmodium vivax. Median follow-up was 29 days. Crude mortality rates were 10.71 and 5.87 per 1000 person-days for patients with and without concurrent malaria, respectively. In the fully adjusted Cox model, patients with concurrent malaria and COVID-19 had a greater mortality risk (hazard ratio 1.43, 95% confidence interval 1.21-1.69).DISCUSSIONCoinfection with COVID-19 and malaria is associated with increased all-cause in-hospital mortality compared to monoinfection with severe acute respiratory syndrome coronavirus 2 (SARS-CoV-2).METHODSWe conducted a retrospective cohort study that assessed prospectively collected data of all patients who were admitted between May and December 2020 to the Universal COVID-19 treatment center (UCTC), Khartoum, Sudan. UCTC compiled demographic, clinical, laboratory (including testing for malaria), and outcome data in all patients with confirmed COVID-19 hospitalized at that clinic. The primary outcome was all-cause mortality during the hospital stay. We built proportional hazard Cox models with malaria status as the main exposure and stepwise adjustment for age, sex, cardiovascular comorbidities, diabetes, and hypertension.\n\nSabrina Casper, Doris H Fuertinger, Leticia M Tapia Silva, Lemuel Rivera Fuentes, Stephan Thijssen, Peter Kotanko\n\nRESULTSIn all tests, the ultrafiltration controller performed as expected. In the in silico and ex vivo bench experiments, the controller showed robust reaction toward deliberate disruptive interventions (e.g. signal noise; extreme plasma refill rates). No adverse events were observed in the clinical study.CONCLUSIONSThe ultrafiltration controller can steer RBV trajectories toward desired RBV ranges while obeying to a set of constraints. Prospective studies in hemodialysis patients with diverse clinical characteristics are warranted to further explore the controllers impact on intradialytic hemodynamic stability, quality of life, and long-term outcomes.BACKGROUNDMost hemodialysis patients without residual kidney function accumulate fluid between dialysis session that needs to be removed by ultrafiltration. Ultrafiltration usually results in a decline in relative blood volume (RBV). Recent epidemiological research has identified RBV ranges that were associated with significantly better survival. The objective of this work was to develop an ultrafiltration controller to steer a patient's RBV trajectory into these favorable RBV ranges.METHODSWe designed a proportional-integral feedback ultrafiltration controller that utilizes signals from a device that reports RBV. The control goal is to attain the RBV trajectory associated with improved patient survival. Additional constraints such as upper and lower bounds of ultrafiltration volume and rate were realized. The controller was evaluated in in silico and ex vivo bench experiments, and in a clinical proof-of-concept study in two maintenance dialysis patients.\n\nSudhir K Bowry, Peter Kotanko, Rainer Himmele, Xia Tao, Michael Anger\n\nInformed decision-making is paramount to the improvement of dialysis therapies and patient outcomes. A cornerstone of delivery of optimal dialysis therapy is to delineate which substances (uraemic retention solutes or 'uraemic toxins') contribute to the condition of uraemia in terms of deleterious biochemical effects they may exert. Thereafter, decisions can be made as to which of the accumulated compounds need to be targeted for removal and by which strategies. For haemodialysis (HD), the non-selectivity of membranes is sometimes considered a limitation. Yet, considering that dozens of substances with potential toxicity need to be eliminated, and targeting removal of individual toxins explicitly is not recommended, current dialysis membranes enable elimination of several molecules of a broad size range within a single therapy session. However, because HD solute removal is based on size-exclusion principles, i.e. the size of the substances to be removed relative to the mean size of the 'pores' of the membrane, only a limited degree of selectivity of removal is possible. Removal of unwanted substances during HD needs to be weighed against the unavoidable loss of substances that are recognized to be necessary for bodily functions and physiology. In striving to improve the efficiency of HD by increasing the porosity of membranes, there is a greater potential for the loss of substances that are of benefit. Based on this elementary trade-off and availability of recent guidance on the relative toxicity of substances retained in uraemia, we propose a new evidence-linked uraemic toxin elimination (ELUTE) approach whereby only those clusters of substances for which there is a sufficient body of evidence linking them to deleterious biological effects need to be targeted for removal. Our approach involves correlating the physical properties of retention solutes (deemed to express toxicity) with key determinants of membranes and separation processes. Our analysis revealed that in attempting to remove the relatively small number of 'larger' substances graded as having only moderate toxicity, uncontrolled (and efficient) removal of several useful compounds would take place simultaneously and may compromise the well-being or outcomes of patients. The bulk of the uraemic toxin load comprises uraemic toxins below <30 000 Da and are adequately removed by standard membranes. Further, removal of a few difficult-to-remove-by-dialysis (protein-bound) compounds that express toxicity cannot be achieved by manipulation of pore size alone. The trade-off between the benefits of effective removal of the bulk of the uraemic toxin load and risks (increased loss of useful substances) associated with targeting the removal of a few larger substances in 'high-efficiency' HD treatment strategies needs to be recognized and better understood. The removability during HD of substances, be they toxic, inert or beneficial, needs be revised to establish the pros and cons of current dialytic elimination strategies. .\n\nRichard V Remigio, Hao He, Jochen G Raimann, Peter Kotanko, Frank W Maddux, Amy Rebecca Sapkota, Xin-Zhong Liang, Robin Puett, Xin He, Amir Sapkota\n\nRESULTSFrom 2001 to 2016, the sample population consisted of 43,338 ESKD patients. We recorded 5217 deaths and 78,433 hospital admissions. A 10-unit increase in PM2.5 concentration was associated with a 5% increase in ACM (rate ratio [RRLag0-3]: 1.05, 95% CI: 1.00-1.10) and same-day O3 (RRLag0: 1.02, 95% CI: 1.01-1.03) after adjusting for extreme heat exposures. Mortality models suggest evidence of interaction and effect measure modification, though not always simultaneously. ACM risk increased up to 8% when daily ozone concentrations exceeded National Ambient Air Quality Standards established by the United States, but the increases in risk were considerably higher during EHE days across lag periods.CONCLUSIONOur findings suggest interdependent effects of EHE and air pollution among ESKD patients for all-cause mortality risks. National level assessments are needed to consider the ESKD population as a sensitive population and inform treatment protocols during extreme heat and degraded pollution episodes.BACKGROUNDIncreasing number of studies have linked air pollution exposure with renal function decline and disease. However, there is a lack of data on its impact among end-stage kidney disease (ESKD) patients and its potential modifying effect from extreme heat events (EHE).METHODSFresenius Kidney Care records from 28 selected northeastern US counties were used to pool daily all-cause mortality (ACM) and all-cause hospital admissions (ACHA) counts. County-level daily ambient PM2.5 and ozone (O3) were estimated using a high-resolution spatiotemporal coupled climate-air quality model and matched to ESKD patients based on ZIP codes of treatment sites. We used time-stratified case-crossover analyses to characterize acute exposures using individual and cumulative lag exposures for up to 3 days (Lag 0-3) by using a distributed lag nonlinear model framework. We used a nested model comparison hypothesis test to evaluate for interaction effects between air pollutants and EHE and stratification analyses to estimate effect measures modified by EHE days.\n\nUlrich Moissl, Lemuel Rivera Fuentes, Mohamad I Hakim, Manuel Hassler, Dewangi A Kothari, Laura Rosales, Fansan Zhu, Jochen G Raimann, Stephan Thijssen, Peter Kotanko\n\nDISCUSSIONWhile about half of the patients had normal fluid status pre-HD, a considerable proportion of patients was either fluid overloaded or depleted, indicating the need for tools to objectively quantify fluid status.INTRODUCTIONInadequate fluid status remains a key driver of cardiovascular morbidity and mortality in chronic hemodialysis (HD) patients. Quantification of fluid overload (FO) using bioimpedance spectroscopy (BIS) has become standard in many countries. To date, no BIS device has been approved in the United States for fluid status assessment in kidney patients. Therefore, no previous quantification of fluid status in US kidney patients using BIS has been reported. Our aim was to conduct a cross-sectional BIS-based assessment of fluid status in an urban US HD population.FINDINGSWe studied 170 urban HD patients (age 61 ± 14 years, 60% male). Pre- and post-HD FO (mean ± SD), were 2.2 ± 2.4 and -0.2 ± 2.7 L, respectively. Pre-HD, 43% of patients were fluid overloaded, 53% normally hydrated, and 4% fluid depleted. Post-HD, 12% were fluid overloaded, 55% normohydrated and 32% fluid depleted. Only 48% of fluid overloaded patients were hypertensive, while 38% were normotensive and 14% hypotensive. Fluid status did not differ significantly between African Americans (N = 90) and Caucasians (N = 61).METHODSWe determined fluid status in chronic HD patients using whole body BIS (Body Composition Monitor, BCM). The BCM reports FO in liters; negative FO denotes fluid depletion. Measurements were performed before dialysis. Post-HD FO was estimated by subtracting the intradialytic weight loss from the pre-HD FO.\n\nPablo Maggiani-Aguilera, Jochen G Raimann, Jonathan S Chávez-Iñiguez, Guillermo Navarro-Blackaller, Peter Kotanko, Guillermo Garcia-Garcia\n\nRESULTSIn 1,632 patients from RRI, the CVC prevalence at month 1 was 64% and 97% among 174 HC patients. The conversion rate was 31.7% in RRI and 10.6% in HC. CVC to non-central venous catheter (NON-CVC) conversion reduced the risk of hospitalization in both HC (aHR 0.38 [95% CI: 0.21-0.68], p = 0.001) and RRI (aHR 0.84 [95% CI: 0.73-0.93], p = 0.001). NON-CVC patients had a lower mortality risk in both populations.INTRODUCTIONCentral venous catheter (CVC) as vascular access in hemodialysis (HD) associates with adverse outcomes. Early CVC to fistula or graft conversion improves these outcomes. While socioeconomic disparities between the USA and Mexico exist, little is known about CVC prevalence and conversion rates in uninsured Mexican HD patients. We examined vascular access practice patterns and their effects on survival and hospitalization rates among uninsured Mexican HD patients, in comparison with HD patients who initiated treatment in the USA.DISCUSSION/CONCLUSIONCVC prevalence and conversion rates of CVC to NON-CVC differed between the US and Mexican patients. An association exists between vascular access type and hospitalization and mortality risk. Prospective studies are needed to evaluate if accelerated and systematic catheter use reduction would improve outcomes in these populations.METHODSIn this retrospective study of incident HD patients at Hospital Civil (HC; Guadalajara, MX) and the Renal Research Institute (RRI; USA), we categorized patients by the vascular access at the first month of HD and after the following 6 months. Factors associated with continued CVC use were identified by a logistic regression model. We developed multivariate Cox proportional hazards models to investigate the effects of access and conversion on mortality and hospitalization over an 18-month follow-up period.\n\nRoberto Pecoits-Filho, John Larkin, Carlos Eduardo Poli-de-Figueiredo, Américo Lourenço Cuvello-Neto, Ana Beatriz Lesqueves Barra, Priscila Bezerra Gonçalves, Shimul Sheth, Murilo Guedes, Maggie Han, Viviane Calice-Silva, Manuel Carlos Martins de Castro, Peter Kotanko, Thyago Proenca de Moraes, Jochen G Raimann, Maria Eugenia F Canziani\n\nRESULTSWe randomized 195 patients (HDF 97; HD 98) between August 2016 and October 2017. Despite the achievement of a high convective volume in the majority of sessions and a positive impact on solute removal, the treatment effect HDF on the primary outcome was +538 [95% confidence interval (CI) -330 to 1407] steps/24 h after dialysis compared with HD, and was not statistically significant. Despite a lack of statistical significance, the observed size of the treatment effect was modest and driven by steps taken between 1.5 and 24.0 h after dialysis, in particular between 20 and 24 h (+197 steps; 95% CI -95 to 488).CONCLUSIONSHDF did not have a statistically significant treatment effect on PA 24 h following dialysis, albeit effect sizes may be clinically meaningful and deserve further investigation.BACKGROUNDDialysis patients are typically inactive and their physical activity (PA) decreases over time. Uremic toxicity has been suggested as a potential causal factor of low PA in dialysis patients. Post-dilution high-volume online hemodiafiltration (HDF) provides greater higher molecular weight removal and studies suggest better clinical/patient-reported outcomes compared with hemodialysis (HD).METHODSHDFIT was a randomized controlled trial at 13 clinics in Brazil that aimed to investigate the effects of HDF on measured PA (step counts) as a primary outcome. Stable HD patients (vintage 3-24 months) were randomized to receive HDF or high-flux HD. Treatment effect of HDF on the primary outcome from baseline to 3 and 6 months was estimated using a linear mixed-effects model.\n\nJochen G Raimann, Christopher T Chan, John T Daugirdas, Thomas Depner, Tom Greene, George A Kaysen, Alan S Kliger, Peter Kotanko, Brett Larive, Gerald Beck, Robert McGregor Lindsay, Michael V Rocco, Glenn M Chertow, Nathan W Levin\n\nRESULTSIn 197 enrolled subjects in the FHN Daily Trial, the treatment effect of frequent HD on ∆LVM was modified by SNa. When the FHN Daily Trial participants are divided into lower and higher predialysis SNa groups (less and greater than 138 mEq/L), the LVM reduction in the lower group was substantially higher (-28.0 [95% CI -40.5 to -15.4] g) than in the higher predialysis SNa group (-2.0 [95% CI -15.5 to 11.5] g). Accounting for GNa, TIFL also showed more pronounced effects among patients with higher GNa or higher TIFL. Results in the Nocturnal Trial were similar in direction and magnitude but did not reach statistical significance.INTRODUCTIONThe Frequent Hemodialysis Network (FHN) Daily and Nocturnal trials aimed to compare the effects of hemodialysis (HD) given 6 versus 3 times per week. More frequent in-center HD significantly reduced left-ventricular mass (LVM), with more pronounced effects in patients with low urine volumes. In this study, we aimed to explore another potential effect modifier: the predialysis serum sodium (SNa) and related proxies of plasma tonicity.DISCUSSION/CONCLUSIONIn the FHN Daily Trial, the favorable effects of frequent HD on left-ventricular hypertrophy were more pronounced among patients with lower predialysis SNa and higher GNa and TIFL. Whether these metrics can be used to identify patients most likely to benefit from frequent HD or other dialytic or nondialytic interventions remains to be determined. Prospective, adequately powered studies studying the effect of GNa reduction on mortality and hospitalization are needed.METHODSUsing data from the FHN Daily and Nocturnal Trials, we compared the effects of frequent HD on LVM among patients stratified by SNa, dialysate-to-predialysis serum-sodium gradient (GNa), systolic and diastolic blood pressure, time-integrated sodium-adjusted fluid load (TIFL), and extracellular fluid volume estimated by bioelectrical impedance analysis.\n\nGabriela Ferreira Dias, Sara Soares Tozoni, Gabriela Bohnen, Nadja Grobe, Silvia D Rodrigues, Tassiana Meireles, Lia S Nakao, Roberto Pecoits-Filho, Peter Kotanko, Andréa Novais Moreno-Amaral\n\nRESULTSHere, we show that HD-RBC have less intracellular oxygen and that it is further decreased post-HD. Also, incubation in 5% O2 and uremia triggered eryptosis in vitro by exposing PS. Hypoxia itself increased the PS exposure in HD-RBC and CON-RBC, and the addition of uremic serum aggravated it. Furthermore, inhibition of the organic anion transporter 2 with ketoprofen reverted eryptosis and restored the levels of intracellular oxygen. Cytosolic levels of the uremic toxins pCS and IAA were decreased after dialysis.CONCLUSIONThese findings suggest the participation of uremic toxins and hypoxia in the process of eryptosis and intracellular oxygenation.BACKGROUND/AIMSChronic kidney disease is frequently accompanied by anemia, hypoxemia, and hypoxia. It has become clear that the impaired erythropoietin production and altered iron homeostasis are not the sole causes of renal anemia. Eryptosis is a process of red blood cells (RBC) death, like apoptosis of nucleated cells, characterized by Ca2+ influx and phosphatidylserine (PS) exposure to the outer RBC membrane leaflet. Eryptosis can be induced by uremic toxins and occurs before senescence, thus shortening RBC lifespan and aggravating renal anemia. We aimed to assess eryptosis and intracellular oxygen levels of RBC from hemodialysis patients (HD-RBC) and their response to hypoxia, uremia, and uremic toxins uptake inhibition.METHODSUsing flow cytometry, RBC from healthy individuals (CON-RBC) and HD-RBC were subjected to PS (Annexin-V), intracellular Ca2+ (Fluo-3/AM) and intracellular oxygen (Hypoxia Green) measurements, at baseline and after incubation with uremic serum and/or hypoxia (5% O2), with or without ketoprofen. Baseline levels of uremic toxins were quantified in serum and cytosol by high performance liquid chromatography.\n\nRichard V Remigio, Rodman Turpin, Jochen G Raimann, Peter Kotanko, Frank W Maddux, Amy Rebecca Sapkota, Xin-Zhong Liang, Robin Puett, Xin He, Amir Sapkota\n\nRESULTSBased on Lag 2- Lag 1 temporal ordering, 1 °C increase in daily TMAX was associated with increased hazard of ACHA by 1.4% (adjusted hazard ratio (HR), 1.014; 95% confidence interval, 1.007-1.021) and ACM 7.5% (adjusted HR, 1.075, 1.050-1.100). Short-term lag exposures to 1 °C increase in temperature predicted mean reductions in IDWG and preSBP by 0.013-0.015% and 0.168-0.229 mmHg, respectively. Mediation analysis for ACHA identified significant indirect effects for all three studied pathways (preSBP, IDWG, and preSBP + IDWG) and significant indirect effects for IDWG and conjoined preSBP + IDWG pathways for ACM. Of note, only 1.03% of the association between temperature and ACM was mediated through preSBP. The mechanistic path for IDWG, independent of preSBP, demonstrated inconsistent mediation and, consequently, potential suppression effects in ACHA (-15.5%) and ACM (-6.3%) based on combined pathway models. Proportion mediated estimates from preSBP + IDWG pathways achieved 2.2% and 0.3% in combined pathway analysis for ACHA and ACM outcomes, respectively. Lag 2 discrete-time ACM mediation models exhibited consistent mediation for all three pathways suggesting that 2-day lag in IDWG and preSBP responses can explain 2.11% and 4.41% of total effect association between temperature and mortality, respectively.CONCLUSIONWe corroborated the previously reported association between ambient temperature, ACHA and ACM. Our results foster the understanding of potential physiological linkages that may explain or suppress temperature-driven hospital admissions and mortality risks. Of note, concomitant changes in preSBP and IDWG may have little intermediary effect when analyzed in combined pathway models. These findings advance our assessment of candidate interventions to reduce the impact of outdoor temperature change on ESKD patients.BACKGROUNDTypical thermoregulatory responses to elevated temperatures among healthy individuals include reduced blood pressure and perspiration. Individuals with end-stage kidney disease (ESKD) are susceptible to systemic fluctuations caused by ambient temperature changes that may increase morbidity and mortality. We investigated whether pre-dialysis systolic blood pressure (preSBP) and interdialytic weight gain (IDWG) can independently mediate the association between ambient temperature, all-cause hospital admissions (ACHA), and all-cause mortality (ACM).METHODSThe study population consisted of ESKD patients receiving hemodialysis treatments at Fresenius Medical Care facilities in Philadelphia County, PA, from 2011 to 2019 (n = 1981). Within a time-to-event framework, we estimated the association between daily maximum dry-bulb temperature (TMAX) and, as separate models, ACHA and ACM during warmer calendar months. Clinically measured preSBP and IDWG responses to temperature increases were estimated using linear mixed effect models. We employed the difference (c-c') method to decompose total effect models for ACHA and ACM using preSBP and IDWG as time-dependent mediators. Covariate adjustments for exposure-mediator and total and direct effect models include age, race, ethnicity, blood pressure medication use, treatment location, preSBP, and IDWG. We considered lags up to two days for exposure and 1-day lag for mediator variables (Lag 2-Lag 1) to assure temporality between exposure-outcome models. Sensitivity analyses for 2-day (Lag 2-only) and 1-day (Lag 1-only) lag structures were also conducted.\n\nRavi Thadhani, Joanna Willetts, Catherine Wang, John Larkin, Hanjie Zhang, Lemuel Rivera Fuentes, Len Usvyat, Kathleen Belmonte, Yuedong Wang, Robert Kossmann, Jeffrey Hymes, Peter Kotanko, Franklin Maddux\n\nRESULTSAmong 170,234 hemodialysis patients, 4,782 (2.8 %) tested positive for SARS-CoV-2 (mean age 64 years, 44 % female). Most facilities (68.5 %) had 0 to 1 positive SARS-CoV-2 patient. We matched 2,379 SARS-CoV-2 positive cases to 2,379 non-SARS-CoV-2 controls; 1.30 % (95 %CI 0.90 %, 1.87 %) of cases and 1.39 % (95 %CI 0.97 %, 1.97 %) of controls were exposed to a chair previously sat in by a shedding SARS-CoV-2 patient. Transmission risk among cases was not significantly different from controls (OR = 0.94; 95 %CI 0.57 to 1.54; p = 0.80). Results remained consistent in adjusted and sensitivity analyses.CONCLUSIONSThe risk of indirect patient-to-patient transmission of SARS-CoV-2 infection from dialysis chairs appears to be low.BACKGROUNDSARS-CoV-2 can remain transiently viable on surfaces. We examined if use of shared chairs in outpatient hemodialysis associates with a risk for indirect patient-to-patient transmission of SARS-CoV-2.METHODSWe used data from adults treated at 2,600 hemodialysis facilities in United States between February 1st and June 8th, 2020. We performed a retrospective case-control study matching each SARS-CoV-2 positive patient (case) to a non-SARS-CoV-2 patient (control) treated in the same dialysis shift. Cases and controls were matched on age, sex, race, facility, shift date, and treatment count. For each case-control pair, we traced backward 14 days to assess possible prior exposure from a 'shedding' SARS-CoV-2 positive patient who sat in the same chair immediately before the case or control. Conditional logistic regression models tested whether chair exposure after a shedding SARS-CoV-2 positive patient conferred a higher risk of SARS-CoV-2 infection to the immediate subsequent patient.\n\nMarkus Pirklbauer, David A Bushinsky, Peter Kotanko, Gudrun Schappacher-Tilp\n\nBackground: Personalized management of secondary hyperparathyroidism is a critical part of hemodialysis patient care. We used a mathematical model of parathyroid gland (PTG) biology to predict (1) short-term peridialytic intact PTH (iPTH) changes in response to diffusive calcium (Ca) fluxes and (2) to predict lo"
    }
}
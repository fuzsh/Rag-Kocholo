{
    "id": "dbpedia_8293_2",
    "rank": 32,
    "data": {
        "url": "https://nelsonslog.wordpress.com/2023/12/",
        "read_more_link": "",
        "language": "en",
        "title": "Nelson's log",
        "top_image": "https://s0.wp.com/i/blank.jpg",
        "meta_img": "https://s0.wp.com/i/blank.jpg",
        "images": [
            "https://nelsonslog.wordpress.com/wp-content/uploads/2023/12/screenshot_1.png?w=619",
            "https://s2.wp.com/i/logo/wpcom-gray-white.png",
            "https://s2.wp.com/i/logo/wpcom-gray-white.png",
            "https://pixel.wp.com/b.gif?v=noscript"
        ],
        "movies": [],
        "keywords": [],
        "meta_keywords": [
            ""
        ],
        "tags": null,
        "authors": [],
        "publish_date": "2023-12-31T17:21:34-08:00",
        "summary": "",
        "meta_description": "10 posts published by nelsonminar during December 2023",
        "meta_lang": "en",
        "meta_favicon": "https://s1.wp.com/i/favicon.ico",
        "meta_site_name": "Nelson's log",
        "canonical_link": null,
        "text": "Phew, my physical to virtual server migration using Proxmox mostly worked. I’ve now moved my main life server, the one I’ve been maintaining or evolving over 25+ years, for the first time into a VM. And it mostly works! Some collected thoughts after the fact.\n\nCheckpoints and backups at the VM level are nice. Really looking forward to that.\n\nThis plan of virtualizing a server by first making one giant VM is nice. I can now hive off services to run in specific containers or VMs at my leisure.\n\nJeff N talked me into trying ZFS on the Proxmox host and so far I’m glad for it. ZFS seems complicated to me but Proxmox is clearly built to take advantage of it. Checkpoints already work better than on BTRFS.\n\nI’m still working out where stuff should live and how it should be shared once I start containerizing services. The obvious thing is to put the primary home for everything in a ZFS pool maintained by the Proxmox server itself (just with ZFS, no Ceph.) My home directory, my Music library, my Video library, a place for backups. Share to containers via bindmounts. Sharing to VMs is harder. I think NFS is the best solution for now until virtiofs in Proxmox is a bit more finished.\n\nYes, I said NFS. Ugh. I’ve got it working with an NFS server on the Proxmox host itself and the VM mounting it. Setting up automount gave me bad flashbacks to the 1990s, although of course systemd has its own way of doing things. But NFS implies a lot of overhead. Also the confusion of user ID mappings. I’ve never really administered NFSv4 which has some nice things but I may gouge my eyeballs out of I have to set up Kerberos. OTOH I’m really not comfortable letting anything on my LAN mount stuff with root privileges. A work in progress.\n\nSyncthing is working OK in the VM right now. Syncthing is peer to peer but I think of it as hub-and-spoke, with this server being the hub. I think I want to hive that Syncthing hub off to its own container. I see myself possibly also setting up a separate Syncthing instance in some containers or VMs but that sure seems dumb. More work in progress.\n\nSyncthing + NFS is scary. The risk is that NFS is unreliable, so the directory for a synced folder might disappear. And in a bad case Syncthing might interpret that as a command “Delete all the files” and propagate that to all the clients. This really happens in practice and is very bad. They have a sentinel file to protect against this, the .stfolder. I still can’t help but think that syncthing is best run on a local filesystem and not a networked one.\n\nI’ve got Plex running in a VM. Right now it doesn’t have access to the Intel GPU, so it can’t do hardware acceleration of transcoding with Quicksync. There’s ways to make that work but I think I should prioritize moving Plex first into a container, the tteck scripts work with hardware transcoding.\n\nBackups are a big question mark. I’ve generally used rsnapshot but that requires root access to work well, since it’s setting file ownership. And root access to NFS is a no. I’m thinking of switching to Restic for backups instead. How this will work depends a bit on my “who should own what files” question.\n\nI also want to make VM and container backups in Proxmox itself, separate from my personal data. And I’d like to back up the Proxmox server configuration itself; it’s crazy there’s no tool for that.\n\nI need to rethink using RAM for tmpfs in the VM. Also using a swapfile. It’s all working but I bet it’s not optimal.\n\nI had so much success with Proxmox for my little home server that I decided to install it for my big home server, the one doing the important stuff. Like before I’m going to ease the transition by first cloning the whole Ubuntu server into a VM in Proxmox, then work on migrating individual services out to their own containers or VMs one at a time. Much thanks to Jeff N for email support learning how to do this.\n\nThe interesting thing here is various methods for cloning a physical Linux system into a Proxmox VM and making it bootable. I did this once already using Clonezilla. This time I’m trying two other methods: cloning with a QEMU disk image and cloning using tar on the root filesystem. I’ve now gotten all 3 methods to work. I think the QEMU disk image is my favorite route, it’s relatively compact and straightforward. Every method could be useful though.\n\nBattle plan\n\nThe server I’m starting with is this one, with this hardware. The challenge is I want to replace the Ubuntu server OS on the machine with Proxmox, with the former server now running as a VM under Proxmox. The nerds call this a P2V clone. The general challenge is how do you clone a Linux system and make it bootable? A particular challenge is I’m trying to do it in-place, I don’t have a second server to clone to. And in particular I want to re-use the existing 2TB SSD, and even though I’m only using 80GB or so of that any simple full mirror will take forever and waste the other 1920GB.\n\nI’m particularly scared of the moment where I install Proxmox over the old server’s filesystem. Once that’s done the clock is ticking to get the new VM up and running so I have my important services back. So first I did a couple of dry runs installing Proxmox on an external USB drive I had lying around. Here’s the plan I came up with:\n\nRemove unneeded files from the server like the apt cache or the systemd journal.\n\nMake simple tar and/or rsync backups of all the files and put them in a safe place. This is the “in case of disaster” recovery option.\n\nTurn off some services on the server that I don’t want to start immediately when the server wakes up as a VM.\n\nShut down the server and boot a Linux system rescue disk.\n\nUse the rescue disk to clone the old system disk via either qemu (path 1) or tar (path 2) to an external USB disc. See below for details.\n\nInstall Proxmox on an external USB disc on the server hardware. Don’t wipe the old server system! We’re going to test stuff is basically going to work before committing.\n\nCreate a VM in Proxmox for the newly virtualized server\n\nCopy the clone into the disk for the virtualized server. See below for details.\n\nBoot the clone VM and get it working.\n\nOnce satisfied, repeat the whole process but install Proxmox onto the SSD instead. Burn the boats!\n\nIt helps to have some spare fast SSD USB discs lying around for these 80GB copies of things. I used several. One for copying server clone images to. One to install Proxmox on (temporarily). And a couple of small jump drives for booting Proxmox and rescue disk images.\n\nWhat I’m not doing is what I did last time using Clonezilla to make the clone. That requires two running servers (the original and Proxmox). Also the destination disk needs to be as big as the source disk and have room left over for Proxmox; not possible with my replace-in-place plan.\n\nOne thing I mostly didn’t do was try to resize the root partition or disk image to be smaller, closer to the 80GB I actually need instead of the 2TB of the source disk. It would probably make things go smoother but all the various resize options are a little scary. I did experiment with this once I got the VM working, see notes here. For the real thing I did resize the btrfs filesystem itself to 200GB, but the partition and the disk are both still 2TB.\n\nOVMF bios notes\n\nI’m trying to do all this with EFI and Secure Boot. It worked on my physical server, it should work on the Proxmox VM too. Some notes on that:\n\nFor EFI you must select the OVMF BIOS when creating the VM in Proxmox. It’s not the default.\n\nThe OVMF EFI shell is weird. It’s a 5 second timeout, then a command line I didn’t understand, and then if you exit that you get to a simple TUI BIOS menu like you’d expect. You can also hit ESC to bypass the 5 seconds and command line.\n\nThere’s a bunch of confusing boot config options. If you’re just trying to boot a specific thing, try Boot Maintenance Manager / Boot From File and then browse the filesystems. Ubuntu’s boot works from EFI/ubuntu/shimx64.efi\n\nI could not get OVMF secure boot to work in a VM with the System Rescue ISO. You’d try to launch it and it’d just return. It works if you turn off secure boot in OVMF. Maybe it’s an issue with the ISO virtualization.\n\nPath 1: QEMU image clone of the whole disk\n\nThis method creates a .qcow2 file which is conceptually an image of every block on the source disk. I got the idea to try this from a Reddit discussion. You make the .qcow2 file and load that in to QEMU/KVM on Proxmox and you’re done. It’s pretty straightforward but some details tripped me up.\n\nMaking the QCOW2 file was relatively easy with these directions. The key command is qemu-img convert -O qcow2 /dev/sda disk.qcow2. Note this clones the entire disc, not just one partition, which means the partition table, GRUB, and the EFI stuff stays intact. Fortunately the QCOW2 format is smart about unused sectors so my 2TB disc resulted in a 80GB QCOW2 file and was relatively quick (10 or 20 minutes?). (There’s also a -c to compress which might help?)\n\nUsing the QCOW2 file on Proxmox was a bit trickier. These notes were useful to me. Basically you create a new empty VM in Proxmox with no install media, no virtual hard drive. Then you use qm disk import to create a new disk from your clone. It converts the QCOW2 file into whatever format Proxmox wants. In my case that was a 2TB raw file on BTRFS. Thankfully it’s a sparse file so it only uses 80GB of the disk. (ls -l shows 2TB, du and btrfs filesystem du show 80G. ZFS also makes sparse files.) This new disk had two partitions cloned from my Ubuntu install: /boot and /. But I didn’t have to do anything special for each partition to make them work.\n\nThere’s some subtle configuration things I had to do in Proxmox:\n\nIt’s essential to select the OVMF BIOS for the VM so that EFI booting works.\n\nProxmox wants an “EFI disk” configured. That seemed to be the default once I told it to use the OVMF BIOS. It’s only 500KB, not sure what’s in it, maybe the BIOS? It’s not your system’s boot partition.\n\nIt’s important to specify a cache policy for a BTRFS disk image. The default is “no cache” which apparently causes corruption issues; I used “write back“. No need if you’re using ZFS on Proxmox.\n\nI had to fiddle with the boot order of things in both Proxmox and maybe the BIOS to get it to actually boot the partition for the cloned Linux system.\n\nI did not have to disable secure boot. It worked with my virtualized GRUB setup from the clone without changes.\n\nI had to set the CPU type to “host” for KVM virtualization to work, so that my Ubuntu system could load the kvm_intel module that snapd wanted. This might cause a problem if you want to migrate a VM to another host in a Proxmox cluster. But it might also give a performance boost.\n\nThe graphical console was a bit wonky under Ubuntu; sometimes it’d be distorted early in the boot and then fix itself in a couple of seconds. I tried setting the graphics hardware to “VMWare Compatible” and that fixed the distortion, but then sometimes boot would fail mysteriously. See below.\n\nWith all that in place, the cloned system booted in a VM! The old familiar GRUB menu and Ubuntu loaded like a champ. Really this was pretty easy.\n\nI initially created a VM with no network device because I knew that DHCP meant my Proxmox server had now stolen the IP address the old server was using. Not good having a VM boot and use the same IP as its host! Fortunately systemd boots pretty nicely if there are no network devices. It gets messier if there are network devices that aren’t configuring themselves; there’s a 120 second timeout for that.\n\nI had to make some changes to the newly virtualized Ubuntu server to make it cooperate better. I did this with Ubuntu’s own recovery mode, a rescue disk would also work. I did these without networking, then rebooted for networking.\n\nsetfont Uni1-VGA32x16 to make the font readable in the Proxmox graphical console\n\nRemove fstab entries for other physical disks. (Note this can prevent your system from booting if you don’t, in which case you need a rescue shell.) In the new setup I’m going to mount those disks in the Proxmox server and then NFS mount them in to the Ubuntu guest.\n\nEdit /etc/netplan configs to use DHCP (for now). Also the new VM ethernet device is named ens18.\n\nDisable Syncthing in the guest VM. The Syncthing data is no longer mounted and I was scared it’d propagate a “delete all files!” message. I’ll need to get this set up again eventually, ideally as a new Proxmox container just for Syncthing.\n\nDisable all my cron jobs for things like blog updates, backups, etc. Again these need to be set up eventually or migrated to special purpose containers.\n\nEventually I should install the qemu-guest-agent and enable it in the Proxmox config so everything plays nice together.\n\nUpdate on Path 1\n\nI redid this whole process for the real server and ran in to some new problems.\n\nThe graphical console was giving me trouble. As noted above, there’s some bug in the default graphics hardware where the Ubuntu boot has a corrupted display for the first couple of seconds. That was hiding an error message, so I switched to “VMWare Compatible” graphics hardware. That looked fine but then my main Ubuntu system wouldn’t boot, displaying an error “hardware has no pitchlock” which froze everything in the Ubuntu boot sequence after about 0.6 seconds. Note this error was only findable in journalctl, it had long since scrolled off the screen. Switching back to “Default” display console the system boots. I should learn about SPICE and serial consoles.\n\nThe underlying error I was trying to find in the console is my restored virtual disk wasn’t mounting cleanly. There was a complaint about the GPT tables. I’m very grateful I went through the partition shrinking exercise because I sort of understood this; somehow the virtual disk changed size just a bit (a few megabytes?) from the physical disk, which breaks the secondary GPT table at the end of the disk. Fixing this isn’t too hard, this time I did it just by running fdisk and then issuing the w command with no changes. That rewrites the partition table and all seems well. gdisk is another option for fixing it, and maybe parted could. All this is very spooky. I can’t think why it failed the second time. I did resize the btrfs partition to 200GB before taking the new QCOW2 snapshot. But that shouldn’t change the full disk size and certainly not by just a few MB.\n\nPath 2: tar clone of the root filesystem\n\nDisk images are unwieldy; why not just clone the Linux root filesystem with tar or rsync or cpio? In theory this should work just like it would have in 1975. In practice it’s tricky because EFI booting and GRUB are such a mystery.\n\nI found a nice blog post laying out how it should work. Basically you use tar on the whole system to create a snapshot (ideally when it’s offline). Then make a new disk with a bootable EFI partition and a root partition. Unpack the tar into the root partition, then ✨ just use grub-install ✨ to magically populate the EFI partition.\n\nThe GRUB step is the confusing bit. The guide above recommends using a chroot to run grub-install as if your old system were live. That’s what ended up working for me. (SystemRescue has its own idea for repairing GRUB. I couldn’t make either method work and it looks like the docs there may be several years out of date.)\n\nHere’s the steps I followed.\n\nClone the old machine\n\nBoot the system with the server to a rescue shell so the filesystem is not live.\n\nMount the rescue filesystem somewhere out of the way like /tmp/source\n\nTar the entire filesystem to an archive on an external USB drive. IIRC this is really as easy as cd /tmp/source; tar caf /mnt/usb/disc.tar.zst --one-file-system . The /dev/ files are harmless, tar is smart about those. Because the disk is not being used live there’s no need to exclude sys, proc, etc, even the one-file-system is probably redundant (but harmless).\n\nAt this point you have a clean tar archive of your server. It’s a nice backup!\n\nCreate the new VM and get it ready\n\nCreate the VM. I used systemrescue as the ISO for the boot media, then configured an empty disc for my new virtual server. See notes from Path 1 about OVMF BIOS, disk cache policy, and “host” CPU type.\n\nOn the Proxmox server command line set it up so the USB device containing your root filesystem tar file is passed through to the new VM. (There’s no doubt other ways to get the data into the VM.)\n\nBoot the VM. In the OVMF bios menu, disable Secure boot in OVMF bios. Shouldn’t be necessary but it was the only way I could get the system rescue ISO to boot.\n\nBoot system rescue and get to a Linux shell\n\nUse parted to set up the new virtual disc. It starts completely empty.\n\nLabel the disk as GPT partition\n\nCreate one EFI system partition of 1GB. (See here).\n\nmkpart p fat32 0% 1GiB\n\nset 1 esp on\n\nCreate a BTRFS partition for the rest of the disc\n\nmkpart p btrfs 1GiB 100%\n\nCreate filesystems on the partitions\n\nmkfs.fat -F 32 /dev/sda1\n\nmkfs.btrfs /dev/sda2\n\nNow you have an empty Proxmox VM all ready to go to have your filesystem transplanted into it.\n\nCopy the filesystem in\n\nWhile still in the rescue shell, mount the guest VM’s disc (/dev/sda2 in above) to somewhere like /tmp/dest and mount the USB source with the tarball.\n\nUse tar to unpack the entire root filesystem from the USB disc. cd /tmp/dest; tar -xaf disc.tar.zst should do it.\n\nAt this point your VM’s disc has a clone of the server, from the tar file. All we have to do is get it to boot! In theory you can do this from a rescue disk right away, but let’s go ahead and install a proper boot system. But first…\n\nFix the block IDs\n\nFirst it’s necessary to fix a few files in your new virtualized server’s root partition. The problem is the block IDs of your filesystems have changed and anything that refers to them by ID has to be updated.\n\nBoot a rescue disk and mount the root filesystem to /tmp/dest\n\nuse blkid to identify the new blockids of the newly prepared filesystems.\n\nedit /tmp/dest/etc/fstab to refer to these new block IDs. (or just use the /dev/sda names for now.)\n\nedit /tmp/dest/etc/grub to include a line naming the block ID for the new filesystem.\n\nMake the VM disk bootable\n\nThe last part is to let grub-install do its thing to set up the boot partition.\n\nBoot the rescue disk and mount /tmp/dest\n\nPrepare bind-mounts into /tmp/dest per step 7 of the guide.\n\nchroot /tmp/dest and get a new shell in the Upside Down\n\nMount the boot partition: mount /dev/sda1 /boot/efi\n\nCast a magic spell for the ✨ EFI variables ✨\n\nmount -t efivarfs none /sys/firmware/efi/efivars\n\nRun update-grub to pick up the new block ID you configured in /etc/grub\n\nRun grub-install /dev/sda to install the EFI partition needed to boot\n\nExit the chroot shell and shut down the VM\n\nIn the Proxmox VM config, remove the SystemRescue ISO and the USB\n\nEdit the VM options to include your new boot device\n\nBoot!\n\nThis all actually worked and I had a working system. The downside is you have to make new partitions, new filesystems, and then install grub in a very weird way. The changed block IDs complicate things too. Overall this was much more complex than just making a disk image clone with QEMU or Clonezilla. But conceptually it might be “better”. I’m glad I learned how to do this but it’s not going to be my preferred method.\n\nPath 3: restore from Proxmox backup\n\nThis is cheating, it’s not cloning a physical system. But just for comparison I tried using the Proxmox GUI to back up a working VM of my server to a USB disc and then restore it. It was very easy! This is the reward at the end of the tunnel. Next time I upgrade hardware I won’t have to reinstall anything other than Proxmox. Then I can just backup and restore VMs and containers. Or migrate them live in a cluster, maybe.\n\nThe backup of the ~80GB of actual disk data (160GB disk) was fairly quick (10 minutes?) and created a 56GB .vma.zst file. The restore took 20 minutes and took up 62GB of data on a ZFS volume.\n\nEverything static was restored but the system had to be rebooted. Proxmox does support live system migration but that’s not what the backup does.\n\nThoughts\n\nSo that’s it, I now have tried four ways of migrating a working Linux server to new hardware. I’ve made all this sound very complicated. Honestly, it was complicated while I was figuring it out. I imagine half the readers are rolling their eyes saying “duh” and the other half are saying “wut?”. I’m not quite at the “duh” stage but I’m glad I’ve gotten past “wut?”. In particular I’m much more comfortable with manipulating Proxmox VMs, adding and editing hardware and disks and the like. And maybe GRUB is a little less of a mystery to me.\n\nIt’s remarkable how resilient a Linux install is to the hardware it’s running on. Virtualizing like this means changing every single hardware device, replacing them with virtual ones. Discs are entirely different. And yet everything still mostly works; putting in the new name for the network adapter was the only manual change required.\n\nA small experiment. I have a 2TB physical disk (SSD) with about 80GB of real data on it. I want to shrink that 2TB to a virtual disk that’s 150GB; 80GB of data and 70GB of free space. The disc has two GPT partitions, almost all of the space allocated to the second partition. That partition is formatted with BTRFS.\n\nI managed to do this but it was kinda scary all the way, particularly the part where I broke everything and lost all my disk partitions. I did recover it in the end but I do not recommend the experience. I was playing around on a test VM so it wasn’t a catastrophe, but if this were a precious system I would have been very unhappy.\n\nTo cut to the chase: resizing the filesystem and the partition were easy. But shrinking the virtual disk with qemu-img caused big problems. The underlying problem is that GPT has two copies of the paritition table: a primary at the start of the disk and a secondary at the end. Changing the disk size screws up the secondary and Linux gets angry and won’t recognize the partitions. You can repair the GPT partition table manually with gdisk but it’s scary. This may not be an issue with old fashioned MBR partitions.\n\nBattle plan\n\nThe simple way to do start is gparted’s GUI; it will resize a btrfs filesystem and partition for you. But I’m stubborn and using command line tools. The plan is roughly:\n\nClone the physical disc to a QCOW2 image\n\nClone the QCOW2 image with qm import disk to whatever raw format Proxmox wants.\n\nBuild a VM around that Proxmox clone.\n\nBoot the VM\n\nUse btrfs filesystem resize to resize the filesystem inside the live VM.\n\nUse parted on the disk to shrink the partition for the filesystem we resized.\n\nReboot and test it’s basically working but the partition is now really smaller\n\nShut down and use qm resize on Proxmox to make the disc smaller. This is the part that broke.\n\nRepair the damage with gdisk\n\nReboot and you’re done!\n\nThe first few steps are documented in other blog posts and are about moving a physical disc to a VM. The rest of this post is about resizing a Proxmox VM disk in QEMU raw format.\n\nbtrfs filesystem resize\n\nThe goal here is to make btrfs think the partition is smaller than it used to be, moving all the data to the front of it so you can then lop off the rest of the partition. It’s pretty straightforward and can (must) be done on a mounted disk.\n\n# time btrfs filesystem resize 150G /\n\nResize device id 1 (/dev/vda2) from 1.82TiB to 150.00GiB\n\nreal 0m21.361s\n\n# btrfs filesystem show /\n\nTotal devices 1 FS bytes used 72.25GiB\n\ndevid 1 size 150.00GiB used 95.02GiB path /dev/vda2\n\n# df -h /\n\n/dev/vda2 150G 74G 75G 50% /\n\nThe filesystem resize was quick and easy! As far as everything is concerned the filesystem is now 150GB big, not 2TB. However the partition is still 2TB.\n\nParted\n\nNow I want to repartition the disk so nothing is using space beyond 150GB. I’m using parted to edit the partition table on the live disk, the one that is running the OS I’m running parted from. That’s not a good idea, it’d be safer to boot from rescue media. But I’m feeling spicy and this is a test VM.\n\nIt’s very important to separate “GB” (1,000,000,000 bytes) from “GiB” (1,073,741,824 bytes). btrfs filesystem is explicit about talking in GiB. parted understands both but seems to prefer the base 10 smaller numbers. You can change that with unit GiB in parted.\n\nunit GiB print\n\nNumber Start End Size File system Name Flags\n\n1 0.00GiB 1.05GiB 1.05GiB fat32 boot, esp\n\n2 1.05GiB 1863GiB 1862GiB btrfs\n\n(parted) resizepart 2 155GiB\n\nWarning: Partition /dev/vda2 is being used. Are you sure you want to continue?\n\nYes/No? y\n\nWarning: Shrinking a partition can cause data loss, are you sure you want to continue?\n\nYes/No? y\n\n(parted) print\n\nModel: Virtio Block Device (virtblk)\n\nDisk /dev/vda: 1863GiB\n\nSector size (logical/physical): 512B/512B\n\nPartition Table: gpt\n\nDisk Flags:\n\nNumber Start End Size File system Name Flags\n\n1 0.00GiB 1.05GiB 1.05GiB fat32 boot, esp\n\n2 1.05GiB 155GiB 154GiB btrfs\n\nNote I chickened out and made the new partition 154 GiB, a few GiB bigger than the filesystem. I don’t trust my math. The correct number for the end of the partition would be 1.05GiB + 150GiB. (But oh god does parted round down by a few bytes?)\n\nI rebooted and verified everything worked. I now have a 2TB disk with only 155GiB of partitions on it.\n\nResizing the QEMU virtual disc\n\nThis is the part where I broke things. I wanted to make the actual VM disk smaller, to shrink it from 2TB to 150GiB (or a bit bigger)\n\nProxmox explicitly does not allow shrinking a desk with the GUI or qm command line tool. The docs just say “that’s dangerous and we won’t help you”.\n\nHowever it’s still possible to do it with command line tools. I did this with qemu-img on a raw image. That linked page talks about LVM as well. Note that both qm and qemu-img seem to work in binary sizes. Ie: 1G is 1GiB, 1,073,741,824 bytes. I sure hope this is consistent everywhere since it’s not explicit.\n\nFirst, I shut down the VM. I did some experiments on a scratch monkey while the VM was still up and bad things happened. The file size of the .raw changed on the Proxmox host, but even after rebooting the size looked like the old size inside the VM. It must only work to resize a disk if the VM is offline.\n\n# ls -l disk.raw\n\n-rw-r----- 1 root root 2000398934016 Dec 30 18:49 disk.raw\n\n# qemu-img resize --shrink -f raw disk.raw 156G\n\nImage resized.\n\n# ls -l disk.raw\n\n-rw-r----- 1 root root 167503724544 Dec 30 19:18 disk.raw\n\n# qm rescan\n\n# rescan volumes...\n\nVM 100 (virtio0): size of disk 'local-btrfs:100/vm-100-disk-1.raw' updated from 1953514584K to 156G\n\nLooks like it worked! Note the qm rescan is necessary.\n\nBut then disaster. Reboot failed. Getting to a rescue disk , blkid didn’t show any of my partitions. fdisk and parted on /dev/vda showed scary errors like “GPT PMBR size mismatch (3907029167 != 327155711) will be corrected by write.”\n\nI was able to correct things somewhat by using qemu-img resize again to grow the disk back to 2TB, its original size (to the exact byte). Even then nothing was working. But now parted and fdisk reported “The backup GPT table is corrupt, but the primary appears OK so that will be used.” I used gdisk (a new-to-me program) to interactively repair things. It used the primary GPT table for the info and rewrote everything. And it worked! But this was all super scary. And I’m back to a 2TB disk.\n\nI have a theory for what went wrong here. Apparently GPT writes two copies of the partition table; a primary at the front of the disk and a secondary at the back. When I shrunk the disk the second copy was removed, then when I made the disk bigger again it was “back” but now zeros instead of data. And so there was a minor complaint with the secondary and gdisk could work around it. Not a big deal to recover from but spooky.\n\nBut I really wanted the disk smaller. So I resized it again, booted to rescue, and used gdisk. And ran the v command. Look at all these errors!\n\nNot only is it unhappy about the secondary partition table being wrong but also it is aware that the disk used to be bigger and stuff is inconsistent.\n\nI used the suggested e command (or was it r?) as suggested. That basically says “trust the primary” and generate a new partition table. Then w to write the change. That all worked fine; both gdisk and parted no longer reported errors. Is it done?\n\nReader, the system rebooted with no problems. Ubuntu was saved!\n\nIdeas for doing this more safely.\n\ngdisk has options to back up GPT partition data to a file and restore it. I’d definitely use that before monkeying around with any of this.\n\nI would use the gparted GUI tool for most of the work. It handles both the btrfs resize and the repartitioning. It’s not going to anticipate the GPT partition weirdness of a qemu-img shrunk disk, but maybe it has nice tools for recovering from the mess too.\n\nLots of responses to my rant about the bad experience of getting a new Android phone. A real mix. Some folks saying “worked for me!”. Some folks saying “I had those same issues but they didn’t bother me”. Not so many people just saying “I feel your pain, it should be better” but then my post was ranty enough that I expected aggressive responses.\n\nHere’s what I would like for a phone migration experience.\n\nRequirements\n\nEvery app should transfer all settings and data. Every preference and config. All data, including cached data you could download over the Internet again. The migration process should be more comprehensive than restoring from a cloud backup; the data is already available locally. (I’ve seen hints that Android does better with a USB wired migration vs. wireless. The Pixel 8 never prompted me to plug in a cable so I didn’t try it.)\n\nEvery single app should be aware I migrated. Most apps probably shouldn’t care but some may need to respond to a migration event so the product needs to adapt to the new hardware somehow.\n\nLow stakes apps like Spotify should keep me logged in. The migration was a secure process, my login tokens should transfer too.\n\nHigh stakes apps like banking or a password store might choose to reauthenticate me after migration, for extra security. But the app on the new phone should already know who I am, already have my username pre-filled. Apps that require two factors for login on a new install should only require one of those factors on a migrated phone.\n\nSome apps require you do something on the old phone to migrate, like de-register it. Those apps need to treat that requirement as a serious thing requiring explicit user action. Notify me during the migration if I need to do something on the old phone. “Hey, you need to do something special for this product”. As it stands now for some apps (Volvo, WhatsApp, Battle.net, …) you only notice you have to do something on your old phone when you get around to launching it on the new phone. That may be several days later, after you’ve already wiped and sold your own phone. That’s not acceptable. (I literally spent a couple of hours launching every single app to make sure it worked. Don’t make me do that. I’m still scared to wipe and sell the old phone.)\n\nGoogle needs to work with cell providers to migrate cell service better. This used to be great on iOS, not sure if it still is. All Android says is “lol contact your cell provider”. Maybe the Google Fi version is nicer?\n\nGoogle should respect my ad privacy settings on migration. As it stands now Android uses a migration as an opportunity to trick you in to agreeing to being tracked.\n\nI’m on the fence whether Android should copy over app permissions. It doesn’t seem to, at least for location, file access, and biometrics. OTOH a migration is a chance to fight the proliferation of promiscuous permissions. That e-commerce app really doesn’t need my precise location.\n\nCommentary\n\nA lot of people responded “it’s an inconvenient experience because of security”. Sure, security-vs-usability tradeoffs are often tricky. But make the tradeoff explicitly, on purpose! Not just this lazy mess. And for security, wouldn’t it be better if the product recognized that it’s not a new install, but rather a secure transfer from an old install? A migration is a different, lower risk. Most of the apps I used don’t do anything that nice, they just reset me back to zero. I suspect they do this because it’s easy and they trust that process is relatively secure rather than building a special migration experience. They didn’t put any product effort into the tradeoff during migration, they just made it inconvenient.\n\nMany of my requirements are predicated on the assumption that the migration itself is a secure process. That no hacker can produce an unauthorized clone of my phone and use that to access my bank account. I am pretty sure Google designs for secure migration and trusts it. But I can understand why my bank might not trust Android. So fine, require I log in again if the stakes are high. But at least design that process nicely. The bare minimum is not making me type my username again.\n\nSecure hardware enclaves complicate migration: it may not be possible to move some data. I don’t know Android well enough to know how prevalent that is. I don’t think it explains most of the problems I saw.\n\nBoth 1Password and Aegis did a good job on migration. These are some of the most sensitive apps I own; both migrated my data in a way that I trust is secure. They did require me to reauthenticate but that process was simple. Both already had my data and knew who I was. This proves that secure convenient migration is possible on Android. My guess is their product teams put a lot of effort into making migration work well, it’s a primary feature for both.\n\nSeveral simpler apps also migrated well. (Yay Paprika!) And Google’s apps also mostly did a good job on migration. Not exactly a surprise, they control the whole platform. Also they have a very clear authentication and account model, most of their apps just trust the system’s Google login. I logged in once to Google during the setup process and then my Gmail, etc was already available. Wallet and GPay were exceptions, both had problems.\n\nSpecial shout-out to the Wikipedia app which kept all my tabs during the transition. That was a nice surprise!\n\nI upgraded my Pixel 6 Pro to a Pixel 8. I just spent some five hours getting the new phone working the way the old one did. It was a terrible experience. Getting a new phone isn’t a delight, it’s an awful chore to be avoided. Every product manager and engineer involved in this mess should feel bad. FWIW it was bad two years ago when I did this last time, too. My impression is the iOS migration experience is much better. (Update: mixed opinions about iOS on the Fediverse.)\n\nUpdate: see this followup.\n\n(Content warning: this note is a bit ranty. I felt it was important to capture the emotional experience.)\n\nBroadly speaking, the problems fall into several categories:\n\nData not copied over to the new phone\n\nRandom app settings not being configured\n\nHaving to log in to every app\n\nSecurity critical stuff\n\nGetting the cellular contract to work\n\nGoogle being sleazy with surveillance\n\nI think the underlying problem is the Android platform must not specify how migration is supposed to work well enough that every app gets it right. No one owns the whole migration product: it’s partly Google/Android’s responsibility, partly the phone manufacturer’s (Google in my case), partly the carrier, and partly the apps’.\n\nAlso I can imagine each app / product owner not thinking it’s a big deal. “Migration only happens every two years, no need to worry about it”. I bet these products lose 5–10% of their customers every two years. Also the problem is much worse for the individual because they have to manage 100+ app migrations all on the same day.\n\nIt’s not like it’s not possible for migrating to a new phone to work. Some things even do work in Android. The little mom & pop Paprika app migrated perfectly. All my recipes on the new phone, still logged in. So did Libby, the public library book borrowing platform. But many other well funded apps sucked. High stakes things like my banks or Uber, but also low stakes things like my Spotify account. My suspicion is the product people for most of these apps just don’t care enough to make it work.\n\nDetails\n\nDo I have detailed notes? You bet I do!\n\nData not copied over to the new phone\n\nAndroid has a fancy migration tool for new phones. It’s pretty slick! Wireless, auto-discovery, relatively quick considering what it’s doing. And it does successfully copy a lot of data over. But not everything. For instance all of my music files copied over. But the 3GB of map databases Organic Maps had? Have to download those again. Also the 80GB of Wikipedia database from Kiwix. I suspect there’s a deliberate decision here not to copy this data since it can be downloaded again. It may be tied to the set of data that’s backed up. But re-downloading 80GB over the Internet is a big burden and the data is right there for the copying.\n\nGoogle Wallet kept my credit card, the most sensitive item it had. But it lost my Clipper transit card. Also lost my Covid vaccination record (again). Maybe no one on the Google Wallet team cares enough to make sure the user’s cards in their wallet transfer over. Just empty the wallet into a fire and start over.\n\nRandom app settings not being configured\n\nA whole lot of apps had to be reconfigured. A few apps copied their configs over entirely, worked great. Most didn’t even try, I started new. A few special apps copied some settings but not all. A whole lot of apps treated me to a New User experience, some condescending 8 page onboarding slideshow like I’d never used the product before. (Including, bafflingly, a few that had copied my configs.)\n\nA related detail: all my system settings for app permissions had to be re-enabled. I had to manually tell the phone, again, to allow the maps app to know my location and the file manager app to access my files. I think my notifications preferences were kept though.\n\nThe notes from the Niagara Launcher app are possibly instructive here. “We’re currently using Google’s Backup implementation, which is not 100% reliable…” In my case I had to fully reconfigure Niagara. There’s a feature request for Niagara to implement its own settings export / migration tool. Some of the apps I use have those but I didn’t use any of them because the process of juggling the settings file was too tricky. It should just sync to the Google Cloud Backup Drive Panopticon, why does this not work for every app consistently?\n\nHaving to log in to every app\n\nA huge PITA is that most apps are basically just wrappers around a website. And almost all of them demanded I log in again. I suppose they think this is “for security reasons” but for 90% of the apps there’s very little risk. It doesn’t help that the login process is a PITA. I use 1Password for logins and for about half the apps it works very well, I just tap a button in the keyboard and it fills. The other half? I have to cut and paste across two apps. Or the new trend, the app doesn’t even let you use a password but insists on sending an SMS message with a login code. Which cuts 1Password out of the loop entirely.\n\nSecurity critical stuff\n\nA few apps have special security needs, I expected them to be awkward to migrate. Except: they aren’t! Both 1Password and Aegis (my 2FA store) worked great. I think I had to type a password into each one once on the new phone. All data, login tokens, etc transferred. Because they care to get it right.\n\nBy contrast the various bank apps and other things with material consequences to security all sucked. In all cases it was basically like I was installing a new app for the first time on a new device, no aspect of the transfer worked at all. Only because these are security conscious products the new app setup process is always onerous.\n\nAnd then there are the special apps that insist I do something on my old phone to deregister it before it will let the new phone work. The geniuses at Volvo managed to build a car where if you lose your phone or uninstall the app you have to factory reset the entire car software in order to connect a different phone to it. Fortunately I have the old phone and could unpair it but I still haven’t paired the new phone because I have to have both physical car keys with me to pair a new phone. The Android migration tool did nothing to help me here.\n\nI was going to check what happened to my Passkeys but hahaha I have no Passkeys because that technology rollout is going very poorly.\n\nGetting the cellular contract to work\n\nAnother special case I have a little sympathy for: migrating cell service is tricky. And very security important, given how every app treats SMS to your phone number as a primary means of identification. I give Verizon credit here, the transfer process was actually pretty good. eSIMs should make this simpler but in practice make it more complicated since there’s no physical object to move your identity on. But it still worked mostly fine, although Verizon’s website and docs are wildly inconsistent on what you need to do to migrate. (The obvious thing worked for me without talking to anyone.)\n\nBut Android did nothing at all to help make my phone be a phone. Why not recognize I’m moving a phone with a Verizon contract and help migrate the eSIM service over at the same time?\n\nGoogle being sleazy with surveillance\n\nA special bonus here. Somehow Google forgot my ad privacy preferences. Google rolled out a whole new ad privacy setting this fall. I immediately opted out of all surveillance on my phone and my Google Account. But despite migrating a Google Account opted out on a phone opted out, the new phone demanded I set my ad preferences up again. Defaulted to full tracking. Twice: once for Chrome and once for the phone device itself.\n\nOf course Google knows exactly what they’re doing, it’s not a bug. They want desperately to track users but they need to maintain the fig leaf that we somehow consented to the tracking. So the phone transfer doesn’t transfer the privacy settings and resets me to full tracking. But this game they’re playing undermines their goal. They clearly ignored my informed preferences and tried to trick me into undoing them. That’s not informed consent, that’s fraud. I know to pay close attention for this garbage but most users won’t follow it.\n\nConclusion\n\nPhew, feel better getting that rant out. But seriously, the experience of migrating one Android phone to another is really bad. It’s a mess of a difficult problem, complicated tech that apparently doesn’t work right, and a product conundrum that no one cares enough about to fix.\n\nContinuing my Proxmox explorations, I’m looking for the smallest reasonable Linux container I can install to then do other things with. My starting point is Proxmox templates. Easiest to install with Proxmox tools but you can see the image sizes at the downloads here.\n\nI’m mostly comparing the sizes of “df /”. That may not be a good measure. But for me it’s not just a measure of disk space, it’s also a proxy for memory usage and complexity. Of course I’m comparing systems without GUIs.\n\nAlpine 3.18: 10MB\n\nThe gold standard for small Linux, really a lovely thing in just 10 MB. Hilariously almost half the storage (4MB) is in /lib/libcrypto.so.3, OpenSSL. I guess no one’s made a svelte SSL. Alpine gets to be so small by using libmusl instead of libc and replacing most of the userspace tools with busybox. The init is the very simple OpenRC. Despite that it has a huge number of packages you can easily install on top of it.\n\nThe downside is those replacements are very limited. The user space shell environment is enough to install something but can be frustrating. MUSL is also limited, see this post as an example of an obscure performance problem. It’s not bad, really it’s an impressive accomplishment, but it’d be nice to have an option that was both small and yet GNU libc based.\n\nDebian 12: 540MB\n\nDebian seems to be the default for a lot of Proxmox applications, but it’s half a gig on disk. From what I’ve read this is fairly pared down. Still it’s hard to quite understand where all the space is going. Some of it is just general system size. 136MB is shared libraries. 20MB for Perl, another 30MB for Python. Some of it seems superfluous: 50MB is locales (all of them?), 60 MB for the dpkg database, etc. It adds up.\n\nOne thing that’s encouraging is the Debian images keep getting smaller. Debian 12 is half the size of Debian 10!\n\nUbuntu: 640MB\n\nNo surprise, Ubuntu is bigger than Debian. More or less the same shape of things but just a little more of it.\n\nCentos 9: 436MB\n\nNot a fan of RedHat OSes but it’s a reasonable choice. Smaller than Debian but with similar sources of size. The locale library is 74MB! Also a 65MB GeoIP database that I bet isn’t necessary.\n\nOpenSUSE 15.5: 178MB\n\nI’d forgotten about OpenSUSE, to be honest, and am not sure whether it’s a good choice in 2023. It’s nicely small though! The bulk of it is binaries in /usr/lib64/ and /usr/bin. It’s still systemd and GNU libc. But there’s locale library at all, no python by default.. It meets the critera for “smaller”. Might be worth investigating what tradeoffs it implies.\n\nGentoo: 1800MB\n\nAnother distro I’d forgotten about, I picked this because in theory it supports musl. But not this template. And whoever built this was clearly not worried about size, it’s huge! There’s copies of package repos, SGML docbook, a 100MB test suite for Python, … clearly no one spent time optimizing size here.\n\nOther options\n\nI should look beyond Proxmox templates. There’s a lot of other folks who’ve been working on the “smaller Linux for containers and VMs” idea. I should learn more about them. Docker’s got a bunch of them, in particular I keep seeing references to a “Debian Slim” that sounds promising. Docker’s overlay filesystem takes some of the sting out of bigger containers though, since the base OS storage can be shared.\n\nConclusions\n\nI guess my ideal would be something with GNU libc but without systemd and without extra locales and binaries I don’t need. Ideally neither Perl nor Python required. Maybe what I want is “Alpine but with glibc and bash”. I’m willing to trade off some space for quality. I wonder if it’s realistic to be able to use pre-built Alpine MUSL packages for some things and glibc for others?\n\nI’m probably overoptimizing. 540MB for Debian is probably what I should be using when Alpine won’t work, or maybe that 178MB OpenSUSE."
    }
}
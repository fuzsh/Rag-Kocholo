{
    "id": "correct_foundationPlace_00006_1",
    "rank": 24,
    "data": {
        "url": "https://www.cerebras.net/",
        "read_more_link": "",
        "language": "en",
        "title": "Homepage",
        "top_image": "https://www.cerebras.net/wp-content/uploads/2022/08/cerebras-hp-carousel-09.jpg",
        "meta_img": "https://www.cerebras.net/wp-content/uploads/2022/08/cerebras-hp-carousel-09.jpg",
        "images": [
            "https://www.cerebras.net/wp-content/uploads/2022/03/CER_logo.svg",
            "https://www.cerebras.net/wp-content/uploads/2022/03/cerebras-white-01.png",
            "https://www.cerebras.net/wp-content/uploads/2024/04/Cerebras-Cluster-v3b@2x-uai-2880x1205-1-scaled-uai-258x108.webp",
            "https://www.cerebras.net/wp-content/uploads/2024/05/time-2024-uai-258x129.webp",
            "https://www.cerebras.net/wp-content/uploads/2024/05/forbes-2024-uai-258x129.webp",
            "https://www.cerebras.net/wp-content/uploads/2024/05/fortune-2024-uai-258x129.webp",
            "https://www.cerebras.net/wp-content/uploads/2024/04/data-model-uai-258x315.webp",
            "https://www.cerebras.net/wp-content/uploads/2024/04/CS-3-Plate-01-b-1-uai-1440x1302-1-uai-258x233.webp",
            "https://www.cerebras.net/wp-content/uploads/2024/06/ising_model_post_final-uai-258x145.webp",
            "https://www.cerebras.net/wp-content/uploads/2024/05/Supercomputing-white-Logos-1600x900-1-uai-258x145.webp",
            "https://www.cerebras.net/wp-content/uploads/2024/05/cerebras-blog-NM-sparsity-SI-uai-258x145.webp",
            "https://www.cerebras.net/wp-content/uploads/2024/01/mayo-200x100-logos-2.svg",
            "https://www.cerebras.net/wp-content/uploads/2024/01/gsk-200x100-logos-1.svg",
            "https://www.cerebras.net/wp-content/uploads/2023/12/AstraZaneca.svg",
            "https://www.cerebras.net/wp-content/uploads/2024/01/tensor-200x100-logos.svg",
            "https://www.cerebras.net/wp-content/uploads/2024/01/totalenergies-200x100-logos.svg",
            "https://www.cerebras.net/wp-content/uploads/2024/01/argonne-200x100-logos.svg",
            "https://www.cerebras.net/wp-content/uploads/2024/06/Dell-Partnership-Release_final-1-uai-258x145.webp",
            "https://www.cerebras.net/wp-content/uploads/2024/05/future-of-supercomputing-01-uai-258x145.webp",
            "https://www.cerebras.net/wp-content/uploads/2024/05/cerebras-blog-NM-sparsity-FI-uai-258x145.webp",
            "https://www.cerebras.net/wp-content/uploads/2022/03/cerebras-white-01-uai-258x114.png"
        ],
        "movies": [
            "https://www.youtube.com/embed/wjarJT1-7mg?feature=oembed&v=wjarJT1-7mg&list=PLCiO1ulV2l-ZZPIckA48UU0aNK4FwcQnG&index=2",
            "https://www.youtube.com/embed/RhXONURR7Yc?feature=oembed&si=6uHFZF4v6Reo2GFT",
            "https://www.youtube.com/embed/xz5QUrLfNSA?feature=oembed"
        ],
        "keywords": [],
        "meta_keywords": [
            ""
        ],
        "tags": null,
        "authors": [
            "Matthew Callstrom",
            "Chair - Department of Radiology",
            "The Opentensor Foundation",
            "Vincent Saubestre CEO & President",
            "TotalEnergies Research & Technology USA",
            "Life Sciences",
            "Argonne National Laboratory"
        ],
        "publish_date": "2022-03-22T15:03:25+00:00",
        "summary": "",
        "meta_description": "Cerebras is the go-to platform for fast and effortless AI training.",
        "meta_lang": "en",
        "meta_favicon": "https://www.cerebras.net/wp-content/uploads/2022/05/cropped-cerebras-logo-fav-32x32.png",
        "meta_site_name": "Cerebras",
        "canonical_link": "https://www.cerebras.net/",
        "text": "Revolutionary Compute\n\nfor Generative AI\n\nCerebras CS-3: the world's largest AI chips and fastest supercomputers.\n\nLeading ML expertise for building the best AI solutions.\n\nAward Winning Technology\n\nCerebras continues to be recognized for pushing the boundaries of AI\n\nTIME\n\nFORBES\n\nFORTUNE\n\nCerebras AI Day\n\nWatch keynotes from our historic event\n\nOpening Keynote by CEO, Andrew Feldman\n\nHardware Keynote by CTO, Sean Lie\n\nML and Product Keynote by VP of Product, Jessica Liu\n\nai model services\n\nYou bring the data, we'll train the model\n\nWhether you want to build a multi-lingual chatbot or predict DNA sequences, our team of AI scientists and engineers will work with you and your data to build state-of-the-art models leveraging the latest AI techniques.\n\nFIND OUT MORE\n\nhigh performance computing\n\nThe fastest HPC accelerator on earth\n\nWith 900,000 cores and 44 GB of on-chip memory, the CS-3 completely redefines the performance envelope of HPC systems. From Monte Carlo Particle Transport to Seismic Processing, the CS-3 routinely outperforms entire supercomputing installations.\n\nFIND OUT MORE\n\nModels on Cerebras\n\nThe Cerebras platform has trained a huge assortment of models from multi-lingual LLMs to healthcare chatbots. We help customers train their own foundation models or fine-tune open source models like Llama 2. Best of all, the majority of our work is open source.\n\nllama 2\n\nFoundation language model\n\n7B-70B, 2T tokens\n\n4K context\n\nOPEN WEIGHTS\n\nMistral\n\n7B Foundation model that leverages\n\ngrouped-query attention,\n\ncoupled with sliding window attention\n\nTRAINED ON CEREBRAS\n\nJAIS\n\nBilingual Arabic + English model\n\n13B, 30B Parameters\n\nAvailable on Azure, G42 Cloud\n\nOPEN WEIGHTS TRAINED ON CEREBRAS\n\nMED42\n\nMedical Q&A LLM\n\nFine-tuned from Llama2-70B\n\nScores 72% on USMLE\n\nTRAINED ON CEREBRAS\n\nbloom\n\nMassive multi-lingual LLM\n\n176B parameters, 366B tokens\n\n2k context\n\nOPEN SOURCE TRAINED ON CEREBRAS\n\nFALCON\n\nFoundation language model\n\n40B, 1T tokens,\n\n(Uses Flash Attention and Multiquery)\n\nCEREBRAS IMPLEMENTATION\n\nMPT\n\nFoundation model trained\n\non 1T tokens of English\n\nthat uses ALiBi positioning method\n\nOPEN SOURCE TRAINED ON CEREBRAS\n\nstarcoder\n\nCoding LLM\n\n15.5B parameters, 1T tokens\n\n8K context\n\nOPEN WEIGHTS TRAINED ON CEREBRAS\n\ndiffusion\n\ntransformer\n\nImage generation model\n\n33M-2B parameters\n\nAdaptive layer norm\n\nCEREBRAS IMPLEMENTATION\n\nT5\n\nFor NLP applications\n\nEncoder-decoder model\n\n60M-11B parameters\n\nCEREBRAS IMPLEMENTATION\n\nCRYSTALCODER\n\nTrained for English + Code\n\n7B Parameters, 1.3T Tokens\n\nLLM360 Release\n\nOPEN SOURCE TRAINED ON CEREBRAS\n\nCEREBRAS-GPT\n\nFoundational Language Model\n\n100m - 13b parameters\n\nNLP\n\nOPEN SOURCE TRAINED ON CEREBRAS\n\nBTLM-chat\n\nBTLM-3B-8K fine-tuned for chat\n\n3B parameters, 8K context\n\nDirect Preference Optimization\n\nCEREBRAS IMPLEMENTATION\n\ngigaGPT\n\nImplements nanoGPT on Cerebras\n\nTrains 175B+ models\n\n565 lines of code\n\nCEREBRAS IMPLEMENTATION\n\nLatest blog posts\n\nUS DOE Achieves 88x Performance Speedup with Cerebras CS-2 Over H100 in Materials Modeling\n\nREAD MORE >\n\nCerebras Breaks Exascale Record for Molecular Dynamics Simulations\n\nREAD MORE >\n\nIntroducing Sparse Llama: 70% Smaller, 3x Faster, Full Accuracy\n\nREAD MORE >\n\nCustomer Spotlight\n\n\"Mayo Clinic selected Cerebras as its first generative AI collaborator for its large-scale, domain-specific AI expertise to accelerate breakthrough insights for the benefit of patients.\"\n\nMatthew Callstrom, MD, PhD\n\nMedical Director for Strategy, Chair - Department of Radiology\n\n\"The Cerebras CS-2 is a critical component that allows GSK to train language models using biological datasets at a scale and size previously unattainable. These foundational models form the basis of many of our AI systems and play a vital role in the discovery of transformational medicines.\"\n\nKim Branson\n\nSVP Global Head of AI and ML, GlaxoSmithKline\n\n\"Training which historically took over 2 weeks to run on a large cluster of GPUs was accomplished in just over 2 days — 52hrs to be exact — on a single CS-1. This could allow us to iterate more frequently and get much more accurate answers, orders of magnitude faster.\"\n\nNick Brown\n\nHead of AI & Data Science, AstraZeneca\n\n\"Working with the Cerebras ML team we were able to train a new state-of-the-art large language model that outperforms models twice its size in a matter of weeks. Their AI expertise is second to none.\"\n\nThe Opentensor Foundation\n\n\"TotalEnergies’ roadmap is crystal clear: more energy, less emissions. To achieve this, we need to combine our strengths with those who enable us to go faster, higher, and stronger… We count on the CS-2 system to boost our multi-energy research and give our research ‘athletes’ that extra competitive advantage.\"\n\nVincent Saubestre\n\nCEO & President, TotalEnergies Research & Technology USA\n\n\"Cerebras allowed us to reduce the experiment turnaround time on our cancer prediction models by 300x, ultimately enabling us to explore questions that previously would have taken years, in mere months.\"\n\nDr. Rick Stevens\n\nAssociate Laboratory Director of Computing, Environment and Life Sciences, Argonne National Laboratory\n\nIn the News\n\nCerebras Enables Faster Training of Industry’s Leading Largest AI Models\n\nREAD MORE >\n\nCerebras Wafer Scale Engine Outperforms World’s #1 Supercomputer, Achieving Long-Timescale Molecular Dynamics Simulations 179x Faster\n\nREAD MORE >\n\nCerebras and Neural Magic Unlock the Power of Sparse LLMs for Faster, More Power Efficient, Lower Cost AI Model Training and Deployment\n\nREAD MORE >"
    }
}
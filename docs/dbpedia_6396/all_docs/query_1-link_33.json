{
    "id": "dbpedia_6396_1",
    "rank": 33,
    "data": {
        "url": "https://ieeevr.org/2024/program/posters/",
        "read_more_link": "",
        "language": "en",
        "title": "Posters",
        "top_image": "https://ieeevr.org/2024/assets/images/IEEEVR_2024_OGImage.png",
        "meta_img": "https://ieeevr.org/2024/assets/images/IEEEVR_2024_OGImage.png",
        "images": [
            "https://ieeevr.org/2024/assets/images/IEEEVR_2024_LogoBanner.png",
            "https://ieeevr.org/2024/assets/images/awards/best.png",
            "https://ieeevr.org/2024/assets/images/awards/hm.png",
            "https://ieeevr.org/2024/assets/images/awards/hm.png",
            "https://ieeevr.org/2024/assets/images/awards/best.png",
            "https://ieeevr.org/2024/assets/images/awards/best.png",
            "https://ieeevr.org/2024/assets/images/awards/best.png",
            "https://ieeevr.org/2024/assets/images/awards/hm.png",
            "https://ieeevr.org/2024/assets/images/awards/best.png",
            "https://ieeevr.org/2024/assets/images/awards/hm.png",
            "https://ieeevr.org/2024/assets/images/awards/hm.png",
            "https://ieeevr.org/2024/assets/images/awards/hm.png",
            "https://ieeevr.org/2024/assets/images/awards/best.png",
            "https://ieeevr.org/2024/assets/images/sponsors/ieee-logo-white.svg",
            "https://ieeevr.org/2024/assets/images/sponsors/ieee-cs-logo-white.svg",
            "https://ieeevr.org/2024/assets/images/sponsors/ieee-vgtc-logo-white.svg",
            "https://ieeevr.org/2024/assets/images/social/Facebook.png",
            "https://ieeevr.org/2024/assets/images/social/twitter.png",
            "https://ieeevr.org/2024/assets/images/sponsors/GoFundMe.png",
            "https://ieeevr.org/2024/assets/images/sponsors/ucf.jpg",
            "https://ieeevr.org/2024/assets/images/sponsors/jpmorgan.svg",
            "https://ieeevr.org/2024/assets/images/sponsors/christie.jpg",
            "https://ieeevr.org/2024/assets/images/sponsors/EAC.png",
            "https://ieeevr.org/2024/assets/images/sponsors/techviz.png",
            "https://ieeevr.org/2024/assets/images/sponsors/ieee-logo-white.svg",
            "https://ieeevr.org/2024/assets/images/sponsors/ieee-cs-logo-white.svg",
            "https://ieeevr.org/2024/assets/images/sponsors/ieee-vgtc-logo-white.svg"
        ],
        "movies": [],
        "keywords": [],
        "meta_keywords": [
            ""
        ],
        "tags": null,
        "authors": [],
        "publish_date": null,
        "summary": "",
        "meta_description": "The 31st IEEE Conference on Virtual Reality and 3D User Interfaces",
        "meta_lang": "en",
        "meta_favicon": "https://www.ieeevr.org/2024/favicon.png",
        "meta_site_name": "IEEE VR 2024",
        "canonical_link": "https://ieeevr.org/2024/program/posters/",
        "text": "Monday Posters\n\nTalk with the authors: 9:45‑10:15, 13:00‑13:30, 15:00‑15:30, 17:00‑17:30, Room: Sorcerer's Apprentice Ballroom\n\nEffects of constant and sinusoidal display lag on sickness during active exposures to virtual reality (ID: P1002)\n\nStephen Palmisano, University of Wollongong; Vanessa Morrison, University of Wollongong; Robert Allison, York University; Rodney Davies, University of Wollongong; Juno Kim, University of New South Wales\n\nAbstract\n\nWhen we move our heads in virtual reality (VR), display lag creates differences between our virtual and physical head pose (DVP). This study examined whether objective estimates of these DVP could be used to predict the sickness caused by different types of lag. We found that adding constant and time-varying lag to simulations generated similar levels of sickness - with all added lag conditions producing more severe sickness than our baseline control. Consistent with the DVP hypothesis, the spatial magnitude and temporal dynamics of the DVP were both found to predict cybersickness severity during active HMD VR.\n\nDisplay lag effects on postural stability and cybersickness during active exposures to HMD virtual reality (ID: P1003)\n\nStephen Palmisano, University of Wollongong; Shao Yang Chia, University of Wollongong\n\nAbstract\n\nThis study examined whether a person's spontaneous postural sway before, and their head-movements during, exposure to virtual reality (VR) predicts their experiences of cybersickness. We compared the stability of head and body movements made by 50 HMD users to the sickness they experienced during VR simulations with different amounts of display lag. Consistent with Postural Instability Theory, we found that: 1) naturally unstable participants were significantly more likely to become sick during these laggy simulations; and 2) the severity of this sickness depended on the spatial magnitude and the temporal dynamics of their head movements during active HMD VR.\n\nLearning Personalized Agent for Real-Time Face-to-Face Interaction in VR (ID: P1005)\n\nXiaonuo Dongye, Beijing Institute of Technology; Dongdong Weng, Beijing Institute of Technology; Haiyan Jiang, Beijing Institute of Technology; Pukun Chen, Beijing Institute of Technology\n\nAbstract\n\nInteractive agents in virtual reality are anticipated to make decisions and provide feedback based on the user's inputs. Despite recent advancements in large language models (LLMs), employing LLMs in real-time face-to-face interactions decision-making, and delivering personalized feedback remains challenging. To address this, our proposed system involves generating and labeling symbolic data, pre-training a real-time network, collecting personalized data, and fine-tuning the network. Utilizing inputs such as interaction distances, head orientations, and hand poses, the agents can provide personalized feedback. User experiments show significant advantages in both pragmatic and hedonic aspects over LLM-based agents, suggesting potential applications across diverse interactive domains.\n\nDeep-Texture: A Foldable Haptic Ring for Shape and Texture Rendering in Virtual Reality (ID: P1008)\n\nYoujin Sung, KAIST; DongKyu Kwak, Kaist; Taeyeon Kim, KAIST; Woontack Woo, KAIST; Sang Ho Yoon, KAIST\n\nAbstract\n\nIn this paper, we suggest a foldable device that renders the shape and texture in Virtual Reality called Deep-Texture. We devised Deep-Texture to achieve effective haptic feedback with lightweight hardware by combining the basic sensations of shape and texture. By integrating the frequency change of linear resonant actuator and 1-bar mechanism, we propose a novel haptic interaction device for immersive VR experiences. During the pilot test, the results show that it enhances realism while keeping usability. By open-sourcing Deep-Texture, we aim to empower a wider range of users to engage with and benefit from haptic technology, ultimately lowering the hurdles.\n\nTowards Continuous Patient Care with Remote Guided VR-Therapy (ID: P1010)\n\nJulian Kreimeier, Friedrich-Alexander University Erlangen-Nürnberg; Hannah Schieber, Friedrich-Alexander University; Noah Lewis, Friedrich-Alexander University Erlangen-Nürnberg; Max Smietana, Friedrich-Alexander University Erlangen-Nürnberg; Juliane Reithmeier, Friedrich-Alexander-Universität Erlangen-Nürnberg; Vlad Cnejevici, Friedrich-Alexander University Erlangen-Nürnberg; Prathik Prasad, Friedrich-Alexander University Erlangen-Nürnberg; Abdallah Eid, Friedrich-Alexander University Erlangen-Nürnberg; Maximilian Maier, Kinfinity; Daniel Roth, Technical University of Munich\n\nAbstract\n\nHand motor impairments heavily impact a people's independence and and overall well-being. Physiotherapy plays a crucial role after surgical interventions. Given the shortage of personnel and therapy session availability, enabling support and monitoring during the absence of the physiotherapist is a key future direction of medical care. Virtual reality has been shown to support the rehabilitation. An individualized and motivating rehabilitation process is crucial to support the affected person until full recovery. We present a prototype of a VR rehabilitation system that allows for the medical expert to control exercise planning and receive a detailed report on the patients success.\n\nFovea Prediction Model in VR (ID: P1012)\n\nDaniele Giunchi, University College London; Riccardo Bovo, Imperial College London; Nitesh Bhatia, Imperial College London; Thomas Heinis, Imperial College; Anthony Steed, University College London\n\nAbstract\n\nWe propose a lightweight deep learning approach for gaze estimation that represents the visual field as three distinct regions: fovea, near, and far peripheral. Each region is modelled using a gaze parameterization gaze regarding angle-magnitude, latitude, or a combination of angle-magnitude-latitude. We evaluated how accurately these representations can predict a user's gaze across the visual field when trained on data from VR headsets. Our experiments confirmed that the latitude model generates gaze predictions with superior accuracy with an average latency compatible with the demanding real-time functionalities of an untethered device. We generated an outperforming ensemble model with a comparable latency.\n\nExploring the Gap between Real and Virtual Nature (ID: P1014)\n\nKatherine Hartley, University of Florida; Victoria Interrante, University of Minnesota\n\nAbstract\n\nWe describe the design and preliminary findings of an experiment that aims to elucidate the cost of omitting accurate haptic and olfactory stimulation from VR nature immersion experiences. Across four separate sessions, participants are immersed in virtual urban and forest environments, while seated both outside in real nature and indoors, with the correspondence of virtual environment to real environment and the order of presentation counterbalanced. We compare multiple restorative outcome measures between the four conditions, including physiological data (EDA and HR), subjective surveys of stress and contentment, and objective performance on tests of visual and auditory attentional resources.\n\nDepBoxia: Depth Perception Training in Boxing, an Immersive Approach (ID: P1024)\n\nYen-Ru Chen, National Tsing Hua University; Tsung-Hsun Tsai, National Tsing Hua University; Tica Lin, Harvard University; Calvin Ku, National Tsing Hua University; Min-Chun Hu, National Tsing Hua University; Hung-Kuo Chu, National Tsing Hua University\n\nAbstract\n\nDepth perception is crucial in novice boxer training for understanding punch timing and distance. Traditional methods rely on coach support and teamwork, leading to a high entry barrier for novice boxers to train whenever or wherever. To address this, we propose an immersive training system using virtual reality (VR) that integrates visual and audio guidance, specifically designed to enhance depth perception in boxing for novice boxers. Our rigorous experiment shows significant improvements in accuracy and reaction time. We introduce the system to professional boxing coaches, enabling them to integrate a systematic approach to training novice boxers' depth perception.\n\n4D Facial Capture Pipeline Incorporating Progressive Retopology Approach (ID: P1028)\n\nZeyu Tian, Beijing Institute of Technology; Dongdong Weng, Beijing Institute of Technology; Hui Fang, Beijing Institute of Technology; Hanzhi Guo, Beijing Institute of Technology; Yihua Bao, Beijing Institute of Technology\n\nAbstract\n\nThe pipeline for creating high-fidelity facial models often utilizes multi-view stereo techniques for reconstruction. However, the subsequent step of retopology often involves intricate manual work, limiting the extension of facial capture systems towards 4D acquisition. This paper proposes a facial 4D capture pipeline based on high-speed cameras. We employ standard multi-view stereo techniques for 3D reconstruction. Non-linear deformations of facial expressions are decoupled from rigid movements of the skull using QR code markers. Additionally, a progressive automated retopology approach is introduced for batch processing. Results demonstrate that our system can capture continuous facial motion sequences with detailed 3D models.\n\nInfluence of Prior Acquaintance on the Shared VR Experience (ID: P1029)\n\nEsen Küçüktütüncü, Institute of Neurosciences of the University of Barcelona; Ramon Oliver, Institute of Neurosciences of the University of Barcelona; Mel Slater, Institute of Neurosciences of the University of Barcelona\n\nAbstract\n\nExploring social dynamics in virtual reality (VR) at a qualitative level holds great potential for improved applications Here we examined the influence of prior acquaintance on how people interacted with each other in VR. Groups of 3 or 4 participants, represented by realistic look-alike avatars, engaged in discussions on predefined themes. There were two conditions: (1) groups of individuals with prior connections and (2) strangers. Questionnaire responses revealed that pre-existing acquaintances fostered a stronger sense of copresence, and greater sentiment compared to the strangers group. This insight is crucial for optimizing the design and dynamics of VR interactions.\n\nTowards an Altered Body Image Through the Exposure to a Modulated Self in Virtual Reality (ID: P1033)\n\nErik Wolf, University of Würzburg; Carolin Wienrich, University of Würzburg; Marc Erich Latoschik, University of Würzburg\n\nAbstract\n\nSelf-exposure using modulated embodied avatars in virtual reality (VR) may support a positive body image. However, further investigation is needed to address methodological challenges and to understand the concrete effects, including their quantification. We present an iteratively refined paradigm for studying the tangible effects of exposure to a modulated self in VR. Participants perform body-centered movement tasks in front of a virtual mirror, encountering their photorealistically personalized embodied avatar with increased, decreased, or unchanged body size. Additionally, we propose different body size estimation tasks conducted in reality and VR before and after exposure to assess participants' putative elicited perceptual adaptations.\n\nMore than Fitness: User Perceptions and Hopes for Getting Fit in Virtual Reality (ID: P1034)\n\nAleshia Hayes, University of North Texas; Veronica Johnson, University of North Texas; Deborah Cockerham, University of North Texas\n\nAbstract\n\nFitness activities are linked to health, cognitive acuity, and the brain's ability to respond to a stimulus. This article reports on a mixed-methods investigation of user experiences of a commercial off-the-shelf virtual reality fitness experience. A total of 74 visitors to a southern science museum, spanning from under 18 to 64 years of age participated in a VR fitness experience and completed a questionnaire and a semi-structured interview. Participants reported experiencing social presence with pre-recorded fitness instructors, positive usability, and a majority predicted that VR fitness could improve cognitive acuity. This pilot indicates a cross-generational interest in VR fitness tools.\n\nEnhanced Reconstruction of Interacting Hands for Immersive Embodiment (ID: P1037)\n\nYu Miao, Beijing Institute of Technology; Yu Han, Beijing Institute of Technology; Yi xiao, China Academy of Aerospace Science and Innovation; Yue Liu, Beijing Institute of Technology\n\nAbstract\n\nInteracting hands reconstruction serves as a channel for natural user engagement in virtual reality, providing realistic embodiment that markedly elevates the immersive experiences. However, accurate prediction of the spatial relations between two hands remains challenging due to the severe occlusion and homogeneous appearance of hands. This paper presents a spatial relationship refinement method for hand reconstruction, employing a module to yield the 3D relative translation between hands and a novel loss function to limit hand mesh penetration. Our method achieves state-of-the-art performance on the InterHand2.6M dataset, offering considerable potential for interacting hands reconstruction in enhancing embodiment for virtual reality.\n\nExpressionAuth: Utilizing Avatar Expression Blendshapes for Behavioral Biometrics in VR (ID: P1038)\n\nTussoun Jitpanyoyos, Shizuoka University; Yuya Sato, Shizuoka University; Soshi Maeda, Shizuoka Univerisity; Masakatsu Nishigaki, Shizuoka University; Tetsushi Ohki, Shizuoka University\n\nAbstract\n\nAs interests in Virtual Reality (VR) continue to rise, head-mounted displays (HMDs) are actively being developed. Current user authentication method in HMDs requires the use of virtual keyboard, which has low usability and is prone to shoulder surfing attacks. This paper introduces ExpressionAuth, a novel authentication method which uses face tracking capabilities in certain HMDs to verify the identity of the user. ExpressionAuth leverages smile as the expression for user verification. ExpressionAuth has the potential to be a secure and usable biometrics, achieving EER as low as 0.00178 and AUC of up to 0.999 in our experiments.\n\nEvaluating the Feasibility of Using Augmented Reality for Tooth Preparation (ID: P1039)\n\nTakuya Kihara, Johns Hopkins University; Andreas Keller, Technical University of Munich; Takumi Ogawa, Tsurumi University; Mehran Armand, Johns Hopkins University; Alejandro Martin-Gomez, Johns Hopkins University\n\nAbstract\n\nTooth preparation is a fundamental treatment technique to restore oral function in prosthodontic dentistry. This technique is complicated as it requires the preparation of an abutment while simultaneously predicting the ideal shape. We explore the feasibility of using Augmented Reality (AR) Head-Mounted Displays (HMDs) to assist dentists during tooth preparation using two different visualization techniques. A user study (N=24) revealed that AR is effective for angle adjustment, and reduces the occurrence of over-reduction. These results suggest that AR can be used to assist physicians during these procedures and has the potential to enhance the accuracy and safety of prosthodontic treatment.\n\nInvestigating Incoherent Depth Perception Features in Virtual Reality using Stereoscopic Impostor-Based Rendering (ID: P1041)\n\nKristoffer Waldow, TH Köln; Lukas Decker, TH Köln; Martin Mišiak, University of Würzburg; Arnulph Fuhrmann, TH Köln; Daniel Roth, Technical University of Munich; Marc Erich Latoschik, University of Würzburg\n\nAbstract\n\nDepth perception is essential for our daily experiences, aiding in orientation and interaction with our surroundings. Virtual Reality allows us to decouple such depth cues mainly represented through binocular disparity and motion parallax. Dealing with fully-mesh-based rendering methods these cues are not problematic as they originate from the object's underlying geometry. However, manipulating motion parallax, as seen in stereoscopic imposter-based rendering, raises questions about visual errors and perceived 3-dimensionality. Therefore, we conducted a user experiment to investigate how varying object sizes affect such visual errors and perceived 3-dimensionality, revealing an interestingly significant negative correlation and new assumptions about visual quality.\n\nFacial Feature Enhancement for Immersive Real-Time Avatar-Based Sign Language Communication using Personalized CNNs (ID: P1042)\n\nKristoffer Waldow, TH Köln; Arnulph Fuhrmann, TH Köln; Daniel Roth, Technical University of Munich\n\nAbstract\n\nFacial recognition is crucial in sign language communication. Especially for virtual reality and avatar-based communication, increased facial features have the potential to integrate the deaf and hard-of-hearing community to improve speech comprehension and empathy. But, current methods lack precision in capturing nuanced expressions. To address this, we present a real-time solution that utilizes personalized Convolutional Neural Networks (CNNs) to capture intricate facial details, such as tongue movement and individual puffed cheeks. Our system's classification models offer easy expansion and integration into existing facial recognition systems via UDP network broadcasting.\n\nComparatively testing the effect of reality modality on spatial memory (ID: P1046)\n\nLeon Mayrose, Ben Gurion University; Shachar Maidenbaum, Ben Gurion University\n\nAbstract\n\nVirtual and augmented reality hold great potential for understanding spatial cognition. However, it is unclear what effect reality modality has on our perception and interaction with our spatial surroundings. Here, participants performed a spatial memory task using passthrough augmented reality in the real world and in a virtual environment reconstructed by scanning the real environment. We found no significant differences by reality modality for subjective measures such as reported immersion, difficulty, enjoyment and cyber-sickness, nor did we find objective differences in performance. These results suggest limited effects on spatial tasks, and are promising for transfer between virtual and augmented scenarios.\n\nPUTREE: A PHOTOREALISTIC LARGE-SCALE VIRTUAL BENCHMARK FOR FOREST TRAINING (ID: P1047)\n\nYawen Lu, Purdue Univ; Yunhan Huang, Purdue University; Su Sun, Purdue University; Songlin Fei, Purdue University; Yingjie Victor Chen, Purdue University\n\nAbstract\n\nForest systems play an important role in mitigating anthropogenic climate change and regulating the global climate. However, due to difficulties in collecting wild data and lack of forestry expertise, the availability of large-scale forest datasets is very limited. In this work, we establish a new virtual forest dataset named PUTree. Our goal is to create a larger, more photo-realistic and diverse dataset as a powerful training resource in the wild forest. Early experimental results demonstrate its validity as a new forest benchmark for the evaluation of tree detection and segmentation algorithms, and its potential in broad application scenarios.\n\nEffectiveness of Visual Acuity Test in VR vs Real World (ID: P1048)\n\nSarker Monojit Asish, Florida Polytechnic University; Roberto Enrique Salazar, University of Louisiana at Lafayette; Arun K Kulshreshth, University of Louisiana at Lafayette\n\nAbstract\n\nVirtual Reality (VR) devices have opened a new dimension of merging technology and healthcare in an immersive and exciting way to test eye vision. Visual acuity is a person's capacity to perceive small details. An optometrist or ophthalmologist determines a visual acuity score following a vision examination. In this work, we explored how recent VR devices could be utilized to conduct visual acuity tests. We used two Snellen charts to examine eye vision in VR, similar to testing in a doctor's chamber. We found that VR could be utilized to conduct preliminary eye vision tests.\n\nEffects of moving task condition on improving operational performance with slight delay (ID: P1049)\n\nYakumo Miwa, Nagoya Institute of Technology; Kenji Funahashi, Nagoya Institute of Technology; Koji Tanida, Faculty of Science and Engineering; Shinji Mizuno, Aichi Institute of Technology\n\nAbstract\n\nWe made a hypothesis that appropriate delay in operational system would improve its operational performance from some reviews and papers. As an experimental result, performance was improved in slight delay. The sensory evaluation also confirmed that the subject felt support even though there was no actual force support. Another experiment confirmed that depth movement restriction, and the move ratio of the virtual tool on a screen to the input device affected to improve performance. We also investigated that difference of task conditions, i.e. moving task distance and target area size, affect to performance improvement.\n\nCross-Reality Attention Guidance on the Light Field Display (ID: P1050)\n\nChuyang Zhang, Keio University; Kai Kunze, Keio University Graduate School of Media Design\n\nAbstract\n\nWe present a cross-reality collaboration system with visual attention guidance that allows a user in VR to share his view with the users in the real world accurately. The VR user can remotely decide the display content of the light field display oriented to multiple real-world users by manipulating the camera in the virtual world. The VR user's focus depth is estimated and then used to adjust the focal plane of the light field display. Our system can improve collaboration in multi-user scenarios especially in which one host user is delivering information to others such as in classrooms and museums.\n\nTraining a Neural Network on Virtual Reality Devices: Challenges and Limitations (ID: P1054)\n\nFrancisco Díaz-Barrancas, Justus Liebig University; Dr. Daniel Flores-Martín, University of Extremadura; Dr. Javier Berrocal, University of Extremadura\n\nAbstract\n\nThe processing power of Virtual Reality (VR) devices is constantly growing. However, few applications still take advantage of these capabilities. Machine learning algorithms have shown promise in enabling an immersive and personalized experience for VR device users. Therefore, it is interesting that these algorithms are processed directly on the devices themselves, without needing other external resources. In this work, a Neural Network (NN) is trained for real-time image classification using different VR devices. The results show the feasibility of incorporating VR devices for NN training without compromising the quality of the interaction, simply and saving external resources.\n\nExploring Influences of Appearance and Voice Realism of Virtual Humans on Communication Effectiveness in Social Virtual Reality (ID: P1056)\n\nYu Han, Beijing Engineering Research Center of Mixed Reality and Advanced Display; Yu Miao, Beijing Engineering Research Center of Mixed Reality and Advanced Display; Hao Sha, Beijing Institute of Technology; Yue Liu, Beijing Institute of Technology\n\nAbstract\n\nVirtual humans play a crucial role in elevating user experiences in Virtual Reality (VR). Despite ongoing technological advancements, achieving highly realistic virtual humans with entirely natural behaviors remains a challenge. This paper explores how the appearance and voice realism of virtual humans influence communication effectiveness within social VR scenarios. Our preliminary results indicate the significant impact of alterations in appearance and voice realism on communication effectiveness. We observe that a cross-modal realism mismatch between appearance and voice can impede effective communication. This research provides valuable insights for designing virtual humans and improving the quality of social communication in VR environments.\n\nExploring Virtual Reality for Religious Education in Real-World Settings (ID: P1059)\n\nSara Wolf, Julius-Maximilians-Universität Würzburg; Ilona Nord, Julius-Maximilians Universität Würzburg; Jörn Hurtienne, Julius-Maximilians-Universität\n\nAbstract\n\nResearch and design of virtual reality (VR) applications for educational contexts often focus on science-related subjects and evaluate knowledge acquisition while overlooking other subjects like religious education or what actually happens when VR is used in real-world settings. Our article combines both and presents two VR applications, Blessed Spaces and VR Pastor, designed for individual experiences in (Protestant) religious education. We deployed the applications in a real-world setting, an out-of-school learning centre. Surprisingly, the applications mediated social engagement between students. Our findings challenge traditional notions of social experiences in VR-supported education and call for more research in real-world settings.\n\nEvaluation of Shared-Gaze Visualizations for Virtual Assembly Tasks (ID: P1060)\n\nDaniel Alexander Delgado, University of Florida; Jaime Ruiz, University of Florida\n\nAbstract\n\nShared-gaze visualizations (SGV) allow collocated collaborators to understand each other's attention and intentions while working jointly in an augmented reality setting. However, prior work has overlooked user control and privacy over how gaze information can be shared between collaborators. In this abstract, we examine two methods for visualizing shared-gaze between collaborators: gaze-hover and gaze-trigger. We compare the methods with existing solutions through a paired-user evaluation study in which participants participate in a virtual assembly task. Finally, we contribute an understanding of user perceptions, preferences, and design implications of shared-gaze visualizations in augmented reality.\n\nEffects on Size Perception by Changing Dynamic Invisible Body Size (ID: P1061)\n\nRyota Kondo, Keio University; Maki Sugimoto, Keio University; Hideo Saito, Keio University\n\nAbstract\n\nOnly virtual hands and feet move synchronously with an observer's movement, inducing body ownership of an invisible body between them. However, it is unclear whether body ownership is also induced when the size of the invisible body is changed. In this study, we investigated whether body ownership is induced in large or small invisible bodies and whether size perception changes with the size of the invisible body. The results showed that body ownership was induced even if the size of the invisible body was changed, but the size perception did not change.\n\nCan't touch this? Why vibrotactile feedback matters in educational VR (ID: P1062)\n\nFabian Froehlich, NYU\n\nAbstract\n\nThis study investigates the relationship between vibrotactile feedback and sense of presence in VR. The inquiry focuses on corrective and reenforcing feedback in STEM learning outcomes using a VR environment called [blinded]. In a randomized within-subject design experiment (N=68) participants got assigned to a vibrotactile and non-vibrotactile condition. Our hypotheses: Participants in the vibrotactile-condition report higher sense of presence ratings compared to the non-haptic condition. Results indicate that vibrotactile feedback increases the sense of presence and impacts metacognition. Participants who received corrective feedback as a vibrotactile stimuli are more likely to underestimate their actual test performance.\n\nExploring the efficient and hedonic shopping: A Comparative Study of in-game VR Stores (ID: P1063)\n\nYang Zhan, Waseda University; Yiming Sun, Waseda University; Tatsuo Nakajima, Waseda University\n\nAbstract\n\nShopping in Virtual Reality (VR) become popular in recent years since it provides immersive experiences. However, there is insufficient understanding of how efficient and hedonic features in VR stores affect the user experiences concurrently. This work aimed to address this gap by integrating a 2D user interface store and a 3D diegetic store into a VR game for comparative analysis. We explore the effects of efficient and hedonic factors on the users' perception and experiences. Results from a within-subject study ($N$=14) revealed that the diegetic store surpasses the 2D store in offering hedonic features, providing suggestions for VR store designs.\n\nPinhole Occlusion: Enhancing Soft-edge Occlusion Using a Dynamic Pinhole Array (ID: P1064)\n\nXiaodan Hu, NAIST; Yan Zhang, Shanghai Jiao Tong University; Monica Perusquia-Hernandez, Nara Institute of Science and Technolgy; Yutaro Hirao, Nara Institute of Science and Technology; Hideaki Uchiyama, Nara Institute of Science and Technology; Kiyoshi Kiyokawa, Nara Institute of Science and Technology\n\nAbstract\n\nSystems with occlusion capabilities have gained interest in augmented reality, vision augmentation, and image processing. To address the challenge of creating a precise yet lightweight occlusion system, we introduce a novel architecture to tackle occlusion blurriness due to defocusing. Our approach, utilizing a dynamic pinhole array on a transmissive spatial light modulator positioned between the eye and the occlusion layer, offers adaptive pinhole patterns, gaze-contingent functionality, and the potential to reduce visual artifacts. Our preliminary result demonstrates that, with the focal plane at 1.8 m, an occlusion placed at 4 cm can be observed sharply through a 4.3 mm aperture.\n\nComparative Efficacy of 2D and 3D Virtual Reality Games in American Sign Language Learning (ID: P1065)\n\nJindi Wang, Durham Univeristy; Ioannis Ivrissimtzis, Durham University; Zhaoxing Li, University of Southampton; Lei Shi, Newcastle University\n\nAbstract\n\nExtensive research on sign language aimed to enhance communication between hearing individuals and the deaf community. With ongoing advancements in virtual reality and gamification, researchers are exploring their application in sign language learning. This study compares the impact of 2D and 3D games on American Sign Language (ASL) learning, using questionnaires to assess user experience. Results show that 3D games enhance engagement, attractiveness, usability, and efficiency, although user performance remains similar in both environments. The findings suggest the potential of 3D game-based approaches to improving ASL learning experiences while also identifying areas for enhancing dependability and clarity in 3D environments.\n\nCybersickness Lies in the Eye of the Observer - Pupil Diameter as a Potential Indicator of Motion Sickness in Virtual Reality? (ID: P1068)\n\nKatharina Margareta Theresa Pöhlmann, KITE-Toronto Rehabilitation Institute; Aalim Makani, Toronto Metropolitan University; Raheleh Saryazdi, Trent University; Keshavarz Behrang, The KITE Research Institute\n\nAbstract\n\nCybersickness is a widespread problem for many users of Virtual Reality systems. Changes in pupil diameter have been suggested as potential physiological correlates of cybersickness, but the relationship remains vague. Here, we further investigated how pupil diameter changes in relation to cybersickness by engaging participants in a passive locomotion through an outer-space environment. Participants who experienced sickness showed greater variance in pupil diameter compared to non-sick participants, whereas average pupil diameter did not differ. Our results suggest that irregular pupillary rhythms may be a potential correlate of cybersickness, which could be used to objectively identify cybersickness.\n\nNever Tell The Trick: Covert Interactive Mixed Reality System for Immersive Theatre (ID: P1069)\n\nChanwoo Lee, Imperial College London; Kyubeom Shim, Sogang University; Sanggyo Seo, Sogang Univ.; Gwonu Ryu, Dept. of Art & Technology; Yongsoon Choi, Sogang Univ.\n\nAbstract\n\nThis study explores the integration of Ultra-Wideband (UWB) technology into Mixed Reality (MR) Systems for immersive theatre. Addressing the limitations of existing technologies like Microsoft Kinect and HTC Vive, the research focuses on overcoming challenges in robustness to occlusion, tracking volume, and cost efficiency in props tracking. Utilizing the UWB, the immersive MR system enhances the scope of performance art by enabling larger tracking areas, more reliable and cheaper multi-prop tracking, and reducing occlusion issues. Preliminary user tests demonstrate meaningful improvements in immersive experience, promising a new possibility in Extended Reality (XR) theatre and performance art.\n\nEnhancing Virtual Walking in Lying Position: Upright Perception by Changing Self-Avatar's Posture (ID: P1070)\n\nJunya Nakamura, Toyohashi University of Technology; Michiteru Kitazaki, Toyohashi University of Technology\n\nAbstract\n\nWe aimed to decrease visual-proprioceptive conflict in experiencing virtual walking with the lying posture. An optic flow during an avatar's standing up was presented before virtual walking that was induced by the radial optic flow and foot vibrations. The walking sensation and telepresence slightly increased by the standing-up optic flow, but it did not reach statistical significance. Participants felt as the posture was more matched to the walking avatar with the standing-up optic flow compared to the no-animation condition. These results highlight the potential for posture-informed VR design to improve user experiences in a situation with visual-proprioceptive conflict.\n\nStreamSpace: A Framework for Window Streaming in Collaborative MR Environments (ID: P1072)\n\nDaniele Giunchi, University College London; Riccardo Bovo, Imperial College London; Nels Numan, University College London; Anthony Steed, University College London\n\nAbstract\n\nWe introduce StreamSpace as a framework for the exploration of screen-based collaborative MR experiences, focusing on the streaming, integration, and layout of screen content in MR environments. Utilizing Unity and Ubiq, this framework allows users to engage with, reposition, and resize uniquely identified screens within a user-centric virtual space. Building on Ubiq's WebRTC capabilities, our framework enables real-time streaming and transformations through peer-to-peer communication. Key features of StreamSpace include distributed streaming, automated screen layout, and flexible privacy settings for virtual screens. Introducing StreamSpace, we aim to provide a foundational basis for research on screen-based collaborative MR applications.\n\nCan Brain Stimulation Reduce VR motion sickness in Healthy Young Adults During an Immersive Relaxation Application? A Study of tACS (ID: P1074)\n\nGang Li, University of Glasgow; Ari Billig, SyncVR Medical; Chao Ping Chen, Shanghai Jiao Tong University; Katharina Margareta Theresa Pöhlmann, KITE Rehabilitation Institution\n\nAbstract\n\nThis study marks the first exploration of whether a non-invasive transcranial alternating current stimulation (tACS) on the left parietal cortex can reduce VR motion sickness (VRMS) induced by a commercial VR relaxation app. Two VRMS conditions were examined for 36 healthy young adults: 1) pure VRMS without a moving platform; 2) VRMS with a side-to-side rotary chair. Participants underwent three counterbalanced tACS protocols at the beta frequency band (sham, treatment, and control). Contrary to our hypothesis, the treatment protocol did not significantly reduce VRMS in either condition. Given the protocol's prior success in our previous tACS study, we discussed potential factors hindering the replication of our earlier achievement.\n\nSuper-Resolution AR?: Enhanced Image Visibility for AR Imagery? (ID: P1078)\n\nHyemin Shin, Korea University; Hanseob Kim, Korea University; DongYun Joo, Korea University; Gerard Jounghyun Kim, Korea University\n\nAbstract\n\nIn AR applications, there may be situations in which the visual target is not clearly visible/legible because it is too far or small. The unclear part of the imagery can be captured and magnified, but the image quality can still be problematic with the aliasing artifacts by the limited resolution. This poster proposes to apply deep learning-based upscaling to enhance the low-resolution images. We developed a prototype system that can capture an image and upscale/present it to the user. The pilot study demonstrated that upscaled imagery improved image clarity, the ability to find hidden information more quickly, and user experience.\n\nu-DFOV: User-Activated Dynamic Field of View Restriction for Managing Cybersickness and Task Performance (ID: P1079)\n\nYechan Yang, Korea University; Hanseob Kim, Korea University; Gerard Jounghyun Kim, Korea University\n\nAbstract\n\nDynamic field of view restriction is one effective way of mitigating cybersickness by modulating the amount of visual information during virtual navigation. However, in the presence of an interactive task for which visibility is important, it can impede the task performance. This poster examined the efficiency of users, manually engaging the dynamic field of view restriction to control and mitigate cybersickness while performing interactive tasks. The comparative experiment has shown that the user-activated method significantly reduced cybersickness as much as the automatic method. However, it also achieved significantly higher task performance and usability despite the manual control.\n\nPianoFMS: Real-time Evaluation of Cybersickness by Keyboard Fingering (ID: P1080)\n\nYechan Yang, Korea University; Hanseob Kim, Korea University; Jungha Kim, Korea University; Gerard Jounghyun Kim, Korea University\n\nAbstract\n\nVarious measurement tools/methods have been developed to assess cybersickness induced in virtual environments, e.g., using the controller, dial device, and verbal input. In this poster, we propose PianoFMS as a cybersickness measurement tool that allows users to directly input absolute scores using the five piano keys without tampering with visual content. The preliminary study revealed that the levels of cybersickness measured using both the dial and PianoFMS were similar, and they each exhibited a significant correlation with that of the conventional post-experiment questionnaire scores. However, the PianoFMS exhibited a markedly enhanced level of usability in comparison to the dial.\n\nVR Interface vs Desktop to convey Quality of Outerwear: a comparative study (ID: P1082)\n\nDario Gentile, Politecnico di Bari; Francesco Musolino, Polytechnic University of Bari; Michele Fiorentino, Polythecnic Institute of Bari; fabio vangi, Polytechnic University of Bari\n\nAbstract\n\nConveying the quality of garments through media as in the physical store is demanding. This study proposes the design of a VR interface that aims to convey outerwear quality, featuring the 3D model animated through physical simulation. To measure the effectiveness of this interface, it is tested simultaneously with its corresponding desktop counterpart, with the photo gallery of the product, in a within-subject analysis on 50 users. Results show that perceived quality of products changes between the experiences. Moreover, in VR visual content was found to be more significant for the quality assessment than written information.\n\nDevelopment of Force Display Using Pneumatic Actuators for Efficient Conveyance of Emotion (ID: P1083)\n\nNagisa Ito, The University of Tokyo; Hiroyuki Umemura, National Institute of Advanced Industrial Science and Technology; Kunihiro Ogata, National Institute of Advanced Industrial Science and Technology; Kenta Kimura, National Institute of Advanced Industrial Science and Technology\n\nAbstract\n\nThis study investigated the emotional impact of varying force in haptic feedback during gripping. Previous studies have not focused on how grip strength influences emotional expression, despite its known use in conveying feelings. We developed a haptic device to present grip-like haptic presentations and conducted an experiment (N=17) to evaluate the emotions elicited by different haptic presentations. The study found that force, speed, and presentation pattern influence both valence (positive or negative emotion) and arousal (intensity of emotion). The results also indicated that haptic feedback communicates not only anger and disgust but also happiness and surprise.\n\nMid-air Imaging Based on Truncated Cylindrical Array Plate (ID: P1084)\n\nJunpei Sano, The University of Electro-Communications; Naoya Koizumi, Department of Informatics\n\nAbstract\n\nThis paper presents a mid-air imaging optical system consisting of two dimensionally arranged truncated cylindrical optical elements. The proposed system aims to reduce the impact of stray light and improve the limited viewing range of mid-air images in micromirror array plates, an existing mid-air imaging optical system. In this study, we used ray tracing to assess mid-air images formed by our proposed optical system. The results show that our method is practical in terms of the invisibility of stray light and brightness of the image when viewed from an angle.\n\nMultitasking with Graphical Encoding Visualization of Numerical Values in Virtual Reality (ID: P1085)\n\nAmal Hashky, University of Florida; Benjamin Rheault, University of Florida; Ahmed Rageeb Ahsan, University of Florida; Lauren Newman, University of Florida; Eric Ragan, University of Florida\n\nAbstract\n\nThis study evaluates the influence of various visual representations of numerical values on users' ability to multitask in virtual reality. We designed a game-like VR simulation where users had to complete one main task while maintaining the status of other subtasks. Supplemental visualizations showed risk status of the subtask depending on experimental condition, with different visual data encodings: position, brightness, color, and area. We collected preliminary data (n=18) on participant performance during the experiment and subjective ratings afterward. The results showed that the intervention rate significantly differed between the four visual encodings, with the position-based version having the lowest rate.\n\nAlleviating the Uncanny Valley Problem in Facial Model Mapping Using Direct Texture Transfer (ID: P1086)\n\nKaylee Andrews, Augusta University; Jeffrey L Benson Jr., Augusta University; Jason Orlosky, Augusta University\n\nAbstract\n\nThough facial models for telepresence have made significant progress in recent years, most model reconstruction techniques still suffer from artifacts or deficiencies that result in the uncanny valley problem when used for real-time communication. In this paper, we propose an optimized approach that makes use of direct texture transfer and reduces the inconsistencies present in many facial modeling algorithms. By mapping the source texture from a 2D image to a rough 3D facial mesh, detailed features are preserved, while still allowing a 3D perspective view of the face. Moreover, we accomplish this in real time with a single, monocular camera.\n\nEmbracing Tradition Through Technology: The Mixed Reality Calligraphy Studying Environment (ID: P1092)\n\nYi Wang, Beijing Jiaotong University; Ze Gao, Hong Kong University of Science and Technology\n\nAbstract\n\nThis article introduces an innovative Mixed Reality (MR) system designed explicitly for calligraphy learning and practice. Learners must prepare numerous copies of calligraphy works and character templates in traditional calligraphy study and practice. They often rely on oral guidance from teachers in person for their calligraphy practice. However, with the progress of digital technology, we envision leveraging MR wearable glasses combined with image capture and analysis techniques to assist calligraphy learners in improving their practice in a more flexible time.\n\nDesigning Non-Humanoid Virtual Assistants for Task-Oriented AR Environments (ID: P1094)\n\nBettina Schlager, Columbia University; Steven Feiner, Columbia University\n\nAbstract\n\nIn task-oriented Augmented Reality (AR), humanoid Embodied Conversational Agents can enhance the feeling of social presence and reduce mental workload. Yet, such agents can also introduce social biases and lead to distractions. This presents a challenge for AR applications that require the user to concentrate mainly on a task environment. To address this, we introduce a non-humanoid virtual assistant designed for minimal visual intrusion in AR. Our approach aims to enhance a user's focus on the tasks they need to perform. We explain our design choices based on previously published guidelines and describe our prototype implemented for an optical--see-through headset.\n\nA Virtual Reality Musical Instrument Integrated with a Remote Playing Robot System (ID: P1097)\n\nZhonghao Zhu, Beijing Institute of Technology; Weizhi Nai, Jilin University; Xin Wang, Jilin University; Yue Liu, Beijing Institute of Technology\n\nAbstract\n\nMusic education and performance are often constrained by geographical limitations. To evaluate the effectiveness of remote music performance, we designed a virtual reality musical instrument system with a remote playing robot. The system comprises a virtual Irish tin whistle playing system and a remote playing robot system. In the virtual playing system, sensors capture the performer's gestures, translating them into corresponding tin whistle playing commands and producing the music. The playing robot is constructed with mechanical hands, and a data transmission module is programmed to facilitate communication between the virtual playing system and the robot, enabling remote simulated performances.\n\nSensory Feedback in a Serious Gaming Environment and Virtual Reality for Training Upper Limb Amputees (ID: P1098)\n\nReidner Santos Cavalcante, Federal University of Uberlândia; Edgard Afonso Lamounier Jr., Federal University of Uberlândia; Alcimar Soares, Faculty of Electrical Engineering, Federal University of Uberlândia; Aya Gaballa, Qatar University; John Cabibihan, Qatar University\n\nAbstract\n\nIn this article, the authors present a system based on Immersive Virtual Reality and Serious Games for training the use of prostheses by upper limb amputees with a tactile feedback. By using EMG signal processing users can control the opening and closing of a virtual prosthesis, just like in real life. Tactile feedback causes an improvement in the sensation of touch. Tests were carried out with separate groups: with and without sensory feedback with amputated and non-amputee volunteers. Tests in which users who received haptic feedback demonstrated improvements in performance compared to those who did not use haptic feedback.\n\nDo You XaRaoke? Immersive Realistic Singing Experience \\\\ with Embodied Singer (ID: P1101)\n\nGermán Calcedo, University of Groningen; Ester Gonzalez-Sosa, Nokia; Diego González Morín, Nokia; Pablo Perez, Nokia; Alvaro Villegas, Nokia\n\nAbstract\n\nWe have developed an immersive karaoke experience that allows users to sing their favorite songs in front of a simulated audience. The karaoke experience is designed within an immersive stadium scene with a simulated audience and stage lights that synchronize with the song's beats and displayed lyrics. Unlike VR-based karaoke commercial solutions, users can even see their real bodies as video-based self-avatars through the use of a deep learning network and a real microphone without using VR controllers. Preliminary results from a subset of 17 participants validate the developed prototype and provide insights for future improvements.\n\nTowards Optimized Cybersickness Prediction for Computationally Constrained Standalone Virtual Reality Devices (ID: P1102)\n\nMd Jahirul Islam, Kennesaw State University; Rifatul Islam, Kennesaw State University\n\nAbstract\n\nCybersickness, affecting 60-95% of VR users, poses a challenge for immersive experiences. Research using multimodal data, like pupilometry and heart rate, to predict cybersickness using complex machine-learning models often requires off-the-shelf computing resources (i.e., cloud servers), which is impractical for standalone VR devices (SVRs) as network lag and processing limitations can introduce latency during immersion, exacerbating cybersickness. We propose a novel approach that minimizes the computational cost of cybersickness prediction models by hyper-parameter tuning and reducing training parameters while maintaining prediction accuracy. Our method significantly improves training and inference time, paving the way for optimized prediction frameworks on resource-constrained SVRs.\n\nEnhancing Body Ownership of Avian Avatars in Virtual Reality through Multimodal Haptic Feedback (ID: P1103)\n\nZiqi Wang, university of the arts london; Ze Gao, Hong Kong University of Science and Technology\n\nAbstract\n\nThis paper uses multimodal haptic feedback to enhance users' body ownership in virtual reality through wearable devices. In this case, the human is transformed into a bird, which belongs to the beyond-real transformations category in virtual reality interactions. For body transformation, wearable retractable straps can help people mimic the movement mechanism of avian bodies; for space transformation, the inflatable cushions and blowers can simulate the air resistance and lift, oxygen deprivation, and temperature decrease during the take-off process of avian avatars. The system aims to establish a realistic fidelity of the haptic feedback to enhance the user's body ownership.\n\nEffect of Ambulatory Conditions and Virtual Locomotion Techniques on Distance Estimation and Motion Sickness of a Navigated VR Environment (ID: P1105)\n\nAidan Morris, College of Holy Cross; Michael Vail, College of the Holy Cross; Gabriel Hanna, College of Holy Cross; Anurag Rimzhim, College of Holy Cross\n\nAbstract\n\nWe present preliminary results from a 2 X 2 between-subjects experiment. Our two IVs were ambulatory-restrictive (i.e., without locomotion) postural conditions (sitting vs. standing), and virtual navigation technique of steering versus teleporting. Participants navigated a complex virtual environment comprising outdoor and indoor environments for 10 minutes. We found that teleporting may result in less online distance estimation error than steering. Motion sickness was lower while teleporting than steering and when sitting than standing. Teleporting also resulted in better system usability than steering. We discuss the results' implications for VR usability.\n\nTremor Stabilization for Sculpting Assistance in Virtual Reality (ID: P1106)\n\nLayla Erb, Augusta University; Jason Orlosky, Augusta University\n\nAbstract\n\nThis paper presents an exploration of assistive technology for virtual reality (VR) art, such as sculpting and ceramics. For many artists, tremors from Parkinsonian diseases can interfere with molding, carving, cutting, and modeling of different mediums for creating new sculptures. To help address this, we have developed a system that algorithmically stabilizes tremors to enhance the artistic experience for creators with physical impairments or movement disorder. In addition, we present a real-time sculpting application that allows us to measure differences in sculpting actions and a target object or shape.\n\nDesigning Indicators to Show a Robot's Physical Vision Capability (ID: P1108)\n\nHong Wang, University of South Florida; Tam Do, College of Engineering; Zhao Han, University of South Florida\n\nAbstract\n\nIn human-robot interaction (HRI), studies show humans can mistakenly assume that robots and humans have the same field of view, possessing an inaccurate mental model of a robot. This misperception is problematic during collaborative HRI tasks where robots might be asked to complete impossible tasks about out-of-view objects. In this initial work, we aim to align humans' mental models of robots by exploring the design of field-of-view indicators in augmented reality (AR). Specifically, we rendered nine such indicators from the head to the task space, and plan to register them onto the real robot and conduct human-subjects studies.\n\nGuiding Gaze: Comparing Cues for Visual Search (ID: P1112)\n\nBrendan Kelley, Colorado State University; Christopher Wickens, Colorado State University; Benjamin A. Clegg, Colorado State University; Amelia C. Warden, Colorado State University; Francisco Raul Ortega, Colorado State University\n\nAbstract\n\nVisual search tasks are commonplace in daily life. In cases where the time and accuracy of the search is critical (such as first responder, crisis, or military scenarios) augmented reality (AR) visual cueing is potentially beneficial. Three cue conditions (3D Arrow, 2D Wedge, and Gaze Lines) were tested in a visual search task against a baseline no cue condition. Results show that any cue is better than none, however the Gaze Line design produced the lowest search time and greatest accuracy.\n\nEye direction control and reduction of discomfort by vection in HMD viewing of panoramic images (ID: P1113)\n\nSeitaro Inagaki, Nagoya Institute of Technology; Kenji Funahashi, Nagoya Institute of Technology\n\nAbstract\n\nWe have previously proposed an “eye direction exaggeration method.” That facilitates rearward visibility by exaggerating the angle of the eye direction when viewing panoramic images with an HMD in a seated position. In this study, we improved this exaggeration method. However, the exaggeration sometimes increased discomfort such as VR sickness. We also tried to reduce discomfort by presenting horizontally moving particles and inducing vection stably.\n\nCollaborative Motion Modes in Serious Game Using Virtual Co-embodiment: A Pilot Study on Usability and Agency (ID: P1114)\n\nXiongju Sun, Xi'an Jiaotong-Liverpool University; Xiaoyi Xue, Xi'an Jiaotong-Liverpool University; Yangyang HE, Xi'an Jiaotong-Liverpool University; Jingjing Zhang, Xi'an Jiaotong-Liverpool University\n\nAbstract\n\nWith the increasing attention to collaboration in Serious Games, particularly in immersive virtual environments, a novel approach of virtual co-embodiment (e.g., one avatar controlled by multiple users) in recent studies has the potential to contribute to the research on collaborative motion in multiplayer games. This pilot study investigates the usability and the agency of collaborative motion modes in a virtual cycling game under the virtual co-embodiment system. Results showed that these new collaborative modes reported a higher perceived usability. Besides, based on quantitative and qualitative findings, co-embodiment modes in this virtual serious game might evoke an enhanced users' agency.\n\nAIsop: Exploring Immersive VR Storytelling Leveraging Generative AI (ID: P1116)\n\nElia Gatti, University College London; Daniele Giunchi, University College London; Nels Numan, University College London; Anthony Steed, University College London\n\nAbstract\n\nWe introduce AIsop, a system that autonomously generates VR storytelling experiences using generative artificial intelligence (AI). AIsop crafts unique stories by leveraging state-of-the-art Large Language Models (LLMs) and employs Text-To-Speech (TTS) technology for narration. Further enriching the experience, a visual representation of the narrative is produced through a pipeline that pairs LLM-generated prompts with diffusion models, rendering visuals for clusters of sentences in the story. Our evaluation encompasses two distinct use cases: the narration of pre-existing content and the generation of entirely new narratives. AIsop highlights the myriad research prospects spanning its technical architecture and user engagement.\n\nDevelopment and Evaluation of an AR News Interface for Efﬁcient Information Access (ID: P1117)\n\nMasaru Tanaka, NHK Science & Technology Research Laboratories; Hiroyuki Kawakita, Japan Broadcasting Corporation; Takuya Handa, Science & Technology Research Laboratories, NHK\n\nAbstract\n\nIn this study, we developed a user interface (UI) for augmented reality (AR) glasses designed to allow users to browse news articles easily and efficiently. The UI uses natural language processing and dimensionality reduction techniques to place articles optimally within a virtual space. We compared this UI with two other AR-based interfaces in a user study with 13 participants, and the results show that the proposed interface reduced the time required to browse articles as well as the cognitive load of the activity.\n\nProgress Observation in Augmented Reality Assembly Tutorials Using Dynamic Hand Gesture Recognition (ID: P1119)\n\nTania Kaimel, Graz University of Technology; Ana Stanescu, Graz University of Technology; Peter Mohr, Graz University of Technology; Dieter Schmalstieg, Graz University of Technology; Denis Kalkofen, Graz University of Technology\n\nAbstract\n\nWe propose a proof-of-concept augmented reality assembly tutorial application that uses a video-see-through headset to guide the user through assembly instruction steps. It is solely controlled by observing the user's physical interactions with the workpiece. The tutorial progresses automatically, making use of hand gesture classification to estimate the progression to the next instruction. For dynamic hand gesture classification, we integrate a neural network module to classify the user's hand movement in real time. We evaluate the learned model used in our application to provide insights into the performance of implicit gestural interactions.\n\nHow Long Do I Want to Fade Away? The Duration of Fade-To-Black Transitions in Target-Based Discontinuous Travel (Teleportation) (ID: P1121)\n\nMatthias Wölwer, University of Trier; Benjamin Weyers, Trier University; Daniel Zielasko, University of Trier\n\nAbstract\n\nA fade-to-black animation enhances the transition during teleportation, yet its duration has not been systematically explored even though it is one of the central parameters. To fill this gap, we conducted a small study to determine a preferred duration. We find a short duration of 0.3s to be the average preference, contrasting durations used previously in the literature. This research contributes to the systematic parameterization of discontinuous travel.\n\nReal-time shader-based shadow and occlusion rendering in AR (ID: P1122)\n\nAgapi Chrysanthakopoulou, University of Patras; Kostantinos Moustakas, University of Patras\n\nAbstract\n\nWe present novel methods designed to elevate the realism of augmented reality (AR) applications focusing specifically on optical see-through devices. Our work integrates shadow rendering methods for multiple light sources and dynamic occlusion culling techniques. By creating custom surface shaders we can manage multiple light sources in real-time, augmenting depth perception and spatial coherence. Furthermore, the dynamic occlusion culling system handles occluded objects, ensuring a more convincing and seamless user experience. Several cases and methods are presented with their results for various lighting and spatial conditions, promising a more enhanced and immersive user experience in various AR domains.\n\nIdentifying Markers of Immersion Using Auditory Event-Related EEG Potentials in Virtual Reality with a Novel Protocol for Manipulating Task Difficulty (ID: P1125)\n\nMichael Ramirez, Universidad Escuela Colombiana de Ingeniería Julio Garavito; Hamed Tadayyoni, Ontario Tech University; Heather McCracken, Ontario Tech University; Alvaro Quevedo, Ontario Tech University; Bernadette A. Murphy, Ontario Tech University\n\nAbstract\n\nImmersion is defined as the degree in which the senses are engaged with the virtual environment. Recent studies have focused on investigating the role of difficulty (challenge immersion) through correlating auditory event-related potentials (ERPs) to task difficulty. This study introduces a novel experimental protocol for studying immersion in which other confounding variables than difficulty are equalized by choosing a VR jigsaw puzzle as the task in which the difficulty is only adjusted by the number of pieces. By introducing two new metrics in conformity with the metrics from literature, this work shows promise for auditory ERPs as markers of immersion.\n\nChallenges in the Production of a Mixed Reality Theater Dance Performance (ID: P1316)\n\nDaniel Neves Coelho, Curvature Games; Eike Langbehn, University of Applied Sciences Hamburg\n\nAbstract\n\nVirtual and augmented reality systems are becoming more and more popular in artistic performances. Most of these experiences are based on 360-videos or single-user applications. We produced a mixed reality theater dance performance. An audience of 30 people wore XR-headsets during a theater show. The headsets were networked and shared the same tracking space. Three performers (singer, musician, dancer) performed live and their performance was motion captured and transferred in real-time into the virtual environment. In this poster, we report our technical setup and discuss the challenges that this entails.\n\nEvaluation of Augmented Reality for Collaborative Environments (ID: P1310)\n\nJohn Dallas Cast, Johns Hopkins University; Alejandro Martin-Gomez, Johns Hopkins University; Mathias Unberath, Johns Hopkins University\n\nAbstract\n\nWe present ClimbAR, an open-source, collaborative, real-time, augmented reality application running natively on the Hololens 2 that allows climbers to virtually and collaboratively set climbing holds in their physical environments to better understand and plan their routes. We present the qualitative results of demonstrating ClimbAR at two climbing gyms as well as the quantitative results of analyzing the spatial alignment accuracy of its core synchronization framework, SynchronizAR, through a proto-user study. We find an average rotational alignment error of 12.83 degrees and an average translational alignment error of 3.85 centimeters when using SynchronizAR for collaborative layout tasks involving two users.\n\nDiffusion Attack: Leveraging Stable Diffusion for Naturalistic Image Attacking (ID: P1142)\n\nQianyu Guo, Purdue University; Jiaming Fu, Purdue University; Yawen Lu, Purdue Univ; Dongming Gan, Polytechnic Institute\n\nAbstract\n\nIn Virtual Reality (VR), adversarial attack remains a significant security threat. Most deep learning-based methods for physical and digital adversarial attacks focus on enhancing attack performance by crafting adversarial examples that contain large printable distortions that are easy for human observers to identify. However, attackers rarely impose limitations on the naturalness and comfort of the appearance of the generated attack image, resulting in a noticeable and unnatural attack. To address this challenge, we propose a framework to incorporate style transfer to craft adversarial inputs of natural styles that exhibit minimal detectability and maximum natural appearance, while maintaining superior attack capabilities.\n\nPlausible and Diverse Human Hand Grasping Motion Generation (ID: P1344)\n\nXiaoyuan Wang, IRISA; Yang Li, East China Normal University; Changbo Wang, Depart of Software Science and Technology; Marc Christie, IRISA\n\nAbstract\n\nTechniques to grasp targeted objects in realistic and diverse ways find many applications in computer graphics, robotics and VR. This study generates diverse grasping motions while keeping plausible final grasps for human hands. We first build on a Transformer-based VAE to encode diverse reaching motions into a latent representation noted as GMF and then train an MLP-based cVAE to learn the grasping affordance of targeted objects. Finally, through learning a denoising process, we condition GMF with affordance to generate grasping motions for the targeted object. We identify improvements in our results, and will further address them in future work.\n\nDVIO - Distributed Visual Inertial Odometry in a Multi-user Environment (ID: P1212)\n\nMathieu Lutfallah, ETH Zurich; Juyi Zhang, ETH Zurich; Andreas Kunz, ETH Zurich\n\nAbstract\n\nHead-mounted displays typically use a visual inertial odometry system, which relies on the headset's camera combined with Inertial Measurement Units. While effective, this setup fails if the camera is obstructed or if the environments lacks features. Traditional recalibration methods like place recognition often fall short in such settings. Addressing this, the paper proposes a novel distributed tracking method that uses the positions of other users. This approach creates a network or \"daisy chain\" of user locations, enhancing position tracking accuracy. It serves as an alternative and a supplementary solution to the standard system, ensuring precise location tracking for all users.\n\nAccompliceVR: Lending Assistance to Immersed Users by Adding a Generic Collaborative Layer (ID: P1216)\n\nAnthony Steed, University College London\n\nAbstract\n\nThe current model of development for virtual reality applications is that a single application is responsible for construction of the complete immersive experience. If the application is collaborative that application must implement the functionality for sharing. We present VRAccomplice, and overlay application that add a collaboration layer to applications running on SteamVR. Using the Ubiq software, we can add avatars controlled by remote users as an overlay into any running app used by a local user. Remote users can see video of the local user. We demonstrate this in some common SteamVR games.\n\nEnacting Molecular Interactions in VR: Preliminary relationships between visual navigation and learning outcomes (ID: P1174)\n\nJulianna C Washington, Southern Methodist University; Prajakt Pande, Southern Methodist University; Praveen Ramasamy, Danish Technological Institute; Morten E. Moeller, University College Copenhagen; Biljana Mojsoska, Roskilde University\n\nAbstract\n\nTwenty-three undergraduates participated in a pre-post quasi-experimental single-group study involving an immersive VR simulation which allowed them to embody (i.e. become) a biomolecule and enact/experience its molecular interactions at a microscopic level using actions and gestures. Based on initial data analyses from this study, the present poster reports preliminary findings on the relationships between the participants' visual navigation, and conceptual as well as affective learning outcomes.\n\nAutonomous avatar for customer service training VR system (ID: P1149)\n\nTakenori Hara, Dai Nippon Printing Co., Ltd.\n\nAbstract\n\nBy immersing trainees in a virtual space and conducting customer service training with customer avatars, physical training facilities are no longer required and customer service training costs can be reduced. Furthermore, since there is no need for travel time to the training facility, trainees can easily participate in training even from remote locations. However, the production cost of customer avatars that behave according to training scenarios has become a new issue in social implementation. Therefore, we conducted a preliminary implementation experiment of a customer avatar that works autonomously by incorporating LLM and reported the findings and problems we encountered.\n\nTuesday Posters\n\nTalk with the authors: 9:45‑10:15, 13:00‑13:30, 15:00‑15:30, 17:00‑17:30, Room: Sorcerer's Apprentice Ballroom\n\nUsing immersive video to recall significant musical experiences in elderly population with intellectual disability (ID: P1127)\n\nPablo Perez, Nokia; Marta Orduna, Nokia Spain; María Nava-Ruiz, Fundación Juan XXIII; Javier Martín-Boix, Fundación Juan XXIII\n\nAbstract\n\nComodia Elderly project studies the use of immersive video inelderly population with intellectual disability. Participants in the project took part in live concerts, which were recorded with a 180-degree stereoscopic camera. Recording were visualized in succesive sessions of virtual reality video. Preliminary results show a high level of spacial and social presence, assessed both from adapted questionnaires and for external observation of observable signs.\n\nTarget Selection with Avatars in Mixed Reality (ID: P1128)\n\nEric DeMarbre, Carleton University; Robert J Teather, Carleton University\n\nAbstract\n\nThis poster presents a Fitts' law experiment evaluating the effects of using an avatar in mixed and virtual reality selection tasks. The avatar had little to no impact on efficiency and surprisingly lowered overall accuracy in both the 2D plane and depth. However, the avatar also reduced variability in depth selection. Avatar design and specific MR hardware parameters may significantly impact efficiency, especially compared to VR devices.\n\nGeoreferenced 360-Degree Photos for Enhancing Navigation and Interaction within Virtual Electric Power Substations (ID: P1130)\n\nGabriel Fernandes Cyrino, Federal University of Uberlândia; Claudemir José Alves, Federal University of Uberlândia; Gerson FlÌÁvio Mendes de Lima, Federal University of Uberlândia; Edgard Afonso Lamounier Jr., Federal University of Uberlândia; Alexandre Cardoso, Federal University of Uberlândia; Ana Marotti, Eletrobras; Ricardo Oliveira, Eletrobras\n\nAbstract\n\nThis work introduces a methodology for improving virtual navigation and interaction within power substations by employing georeferenced 360-degree photos. The objective is to swiftly update the current state of the field since, in most cases, incorporating such amendments is not feasible during the reconstruction of virtual environments. This changing speed is very important for critical systems. Preliminary results demonstrate successful updating rates, enabling engineers to make rapid decisions. It is expected that the proposed methodology can improve the efficiency and dependability of step-by-step activities, while also reducing the time and costs associated with system maintenance.\n\nExploring the Impact of Virtual Human and Symbol-based Guide Cues in Immersive VR on Real-World Navigation Experience (ID: P1131)\n\nOmar Khan, University of Calgary; Anh Nguyen, University of Calgary; Michael Francis, University of Calgary; Kangsoo Kim, University of Calgary\n\nAbstract\n\nIn this paper, we explore how navigation performance and experience in a real-world indoor environment is impacted after learning the route from various guide cues in a replicated immersive virtual environment. A guide system, featuring two distinct audiovisual guide representations—a human agent guide and a symbol-based guide—was developed and evaluated through a preliminary user study. The results do not show significant differences between the two guide conditions, but offer insight into the user-perceived confidence and enjoyment of the real-world navigation task after experiencing the route in immersive virtual reality. We discuss the results and direction of future research.\n\nEffects of Nonverbal Communication of Virtual Agents on Social Pressure and Encouragement in VR (ID: P1132)\n\nPascal Martinez Pankotsch, University of Würzburg; Sebastian Oberdörfer, University of Würzburg; Marc Erich Latoschik, University of Würzburg\n\nAbstract\n\nOur study investigated how virtual agents impact users in challenging VR environments, exploring if nonverbal animations affect social pressure, positive encouragement, and trust in 30 female participants. Despite showing signs of pressure and support during the experimental trials, we could not find significant differences in post-exposure measurements of social pressure and encouragement, interpersonal trust, and well-being. While inconclusive, the findings suggest potential, indicating the need for further research with improved animations and a larger sample size for validation.\n\nFlowing with Zen: Exploring Empowering the Dissemination of Intangible Cultural Heritage via Immersive Mixed Reality Spaces (ID: P1133)\n\nWenchen Guo, Peking University; Guoyu Sun, Communication University of China; Wenbo Zhao, Communication University of China; Zhirui Chen, University of Chinese Academy of Social Sciences; Menghan Shi, Lancaster University; Weiyue Lin, Peking University\n\nAbstract\n\nZen is a treasure of the world's intangible cultural heritage (ICH), but nowadays it is facing difficulties in dissemination. This paper presents an immersive experience space Flowing with Zen by integrating HCI technology and MR. Audiences can explore and interact with the four scenarios, as well as meditate, and experience Zen philosophy. The pilot study shows that the MR space not only evokes users' interest and participation but also deepens their empathy or reflections. This innovative way of combining MR, ICH, and UX enhances the accessibility of Zen, and it may open up a new mode of dissemination for ICH.\n\nLessons Learned in Designing Racially Diverse Androgynous Avatars (ID: P1134)\n\nCamille Isabella Protko, University of Central Florida; Ryan P. McMahan, University of Central Florida; Tiffany D. Do, University of Central Florida\n\nAbstract\n\nAs virtual reality technology evolves, avatars play a crucial role in user representation, yet options for gender-diverse individuals remain limited. The goal of this research was to develop 14 racially diverse androgynous avatars using the design guidelines recommended in prior work. However, a perceptual experiment involving 68 participants revealed unexpected results, as most avatars were perceived as predominantly masculine. Additionally, our results yielded discrepancies in perceived gender across different racial identities, as the same design process resulted in widely varying perceptions. Despite these challenges, our research highlights the importance of continued exploration to improve the creation of inclusive avatars.\n\nSexual Presence in Virtual Reality: A Psychophysiological Exploration (ID: P1135)\n\nSara Saint-Pierre Cote, École de technologie supérieure\n\nAbstract\n\nThe increasing use of immersive technologies for sexual purposes raises questions about their capacity to enhance a unique aspect of presence—Sexual Presence (SP). Investigating this phenomenon hinges on our ability to measure it accurately. This paper improves our understanding of SP by identifying potential quantitative electroencephalography variables associated with SP. Twelve heterosexual cisgender males were exposed to virtual scenarios featuring sexual content performed by a Virtual Character (VC). After viewing, participants completed a Sexual Presence questionnaire. A correlation was observed between self-reported SP and the alpha band activity in the frontal and parietal regions.\n\nNavigAR: Enhancing Localized Space Navigation using Augmented Reality (ID: P1138)\n\nSahil Deshpande, Indraprastha Institute of Information Technology - Delhi (IIITD); Rahul Ajith, Indraprastha Institute of Information Technology - Delhi (IIITD); Yaksh Patel, Indraprastha Institute of Information Technology - Delhi (IIITD); Anmol Srivastava, Indraprastha Institute of Information Technology Delhi\n\nAbstract\n\nIn this study, we use Augmented Reality (AR) to project and let users interact with a 3D-replicated model of a fenced space to understand how AR can improve navigation and enhance space retention by improving wayfinding via recall. We recreated a fenced space in 3D using Blender. Users can see their current position and, using onscreen buttons, see routes to their destinations. We found that users could associate the 3D models of the buildings with their real-world counterparts. We observed that users were better able to navigate the campus after using the application.\n\nExploring Radiance Field Content Generation for Virtual Reality (ID: P1139)\n\nAsif Sijan, University of Minnesota Duluth; Peter Willemsen, University of Minnesota Duluth\n\nAbstract\n\nGenerating content for virtual reality takes effort and expanding how content can be generated has the potential to increase participation by a wider range of users well outside virtual reality researchers and programmers. This work explores some current content acquisition and generation techniques for replicating real world 3D scenes and objects to better understand the practical use and application of these techniques. The techniques explored focus on the recent emergence of radiance field-based methods and their potential to replace previous scene/object-generating techniques.\n\nBrain Dynamics of Balance Loss in Virtual Reality and Real-world Beam Walking (ID: P1141)\n\nAmanda Studnicki, University of Florida; Ahmed Rageeb Ahsan, University of Florida; Eric Ragan, University of Florida; Daniel P. Ferris, University of Florida\n\nAbstract\n\nVirtual reality (VR) aims to replicate the sensation of a genuine experience through the integration of realism, presence, and embodiment. In this study, we used mobile electroencephalography to quantify differences in anterior cingulate brain activity, an area involved in error monitoring, with and without VR during a challenging balance task to discern the factors contributing to VR's perceptual shortcomings. We found a major delay in the anterior cingulate response to self-generated loss of balance in VR compared to the real world. We also found a robust response in the anterior cingulate when loss of balance was generated by external disturbance.\n\nInvestigating the Impact of Virtual Avatars and Owner Gender on Virtual Partner Selection in Avatar-based Interactions (ID: P1144)\n\nAnh Nguyen, University of Calgary; Seoyoung Kang, KAIST; Woontack Woo, KAIST; Kangsoo Kim, University of Calgary\n\nAbstract\n\nAs real-life social interactions transition to virtual environments using Virtual/Augmented Reality (VR/AR) technologies, understanding how gender representation in virtual avatars affects choices and behaviors becomes increasingly relevant. In this paper, we investigate the combined impact of avatars' gender representation and their owner's gender on virtual partner selection in physical, intellectual, social, and romantic scenarios. We introduce our preliminary research plan, outlining both the study design and system development. The research will contribute to our understanding of social dynamics and gender effects in avatar-mediated interactions.\n\nAn exploration on modeling haptic reaction time of 3D interactive tasks within virtual environments (ID: P1145)\n\nStanley Tarng, University of Calgary; Yaoping Hu, University of Calgary\n\nAbstract\n\nVirtual environments (VEs) utilize haptic cues - e.g., vibrotactile and force feedbacks - to facilitate user interactions in 3D tasks. Existing studies reported maximum likelihood estimation (MLE) to integrate cues of different modalities. The MLE integration was excluded for cues of the same modality. Although proportional likelihood estimation (PLE) was able to integrate same-modality cues for the parameter of task accuracy, its applicability remains unclear to other task parameters like reaction time. This feasibility study compared thus MLE and PLE to integrate the haptic cues for reaction time. PLE was found to be applicable to model haptic reaction time.\n\nHiLoTEL: Virtual Reality Robot Interface-Based Human-in-the-Loop Task Execution and Learning in the Physical World Through Its Digital Twin (ID: P1148)\n\nAmanuel Ergogo, SANO Centre for Computational Personalized Medicine; Diego Dall'Alba, SANO Centre for Computational Personalized Medicine; Przemysław Korzeniowski, Sano Centre for Computational Medicine\n\nAbstract\n\nHiLoTEL is a flexible virtual-reality framework for executing and learning tasks in virtual and physical environments. It enables human experts to collaborate with learning agents and intervene when necessary through human-in-the-loop imitation learning. HiLoTEL reduces the need to carry out repetitive tasks, providing the user with an intuitive supervision interface. The system is tested on Pick-and-Place task, considering both teleoperated and passthrough interaction modalities. The results show that HiLoTEL improves success rates while maintaining human-level completion time and providing users with 71% hands free supervision time, thus enabling effective human-robot collaboration.\n\nDesign and Analysis of Interaction Method to Adjust Magnification Function Using Microgestures in VR or AR Applications (ID: P1154)\n\nHao Sun, Beijing Insitute of Technology; Shining Ma, Beijing Institute of Technology; Mingwei Hu, Beijing Institute of Technology; Weitao Song, Beijing Institute of Technology; Yue Liu, Beijing Institute of Technology\n\nAbstract\n\nAmong various interaction modes in VR/AR, microgestures have distinct advantages over controllers by reducing fatigue and improving the efficiency. In this paper, we proposed a microgesture set that utilizes the number of fingers as input instructions for magnification adjustment in VR environments. To evaluate the effectiveness of the proposed microgestures, a series of tasks have been designed in three scenes. The results revealed a significant improvement in completion time and subjective measures compared to the performance of controller. These findings offer valuable insights for future gesture design, contributing to the development of more efficient and user-friendly interaction techniques in VR.\n\nVoicing Your Emotion: Integrating Emotion and Identity in Cross-Modal 3D Facial Animations (ID: P1156)\n\nwenfeng song, Beijing information science and technology university; Zhenyu Lv, Beijing Information Science and Technology University; wang xuan, Beijing Information Science & Technology Universit; Xia Hou, Beijing information science and technology university\n\nAbstract\n\nSpeech-driven 3D facial animation is widely applied in VR fields.Capturing intricate expressiveness remains an intricate challenge. Addressing this deficiency, we unveil a method tailored to produce 3D facial expressions that resonate deeply with emotion and identity, guided by speech and user-provided prompt words. Our key insight is an emotion-identity fusion mechanism, a pre-trained self-reconstruction codebook, meticulously crafted from a wide array of emotional facial movements, serving as an expressive motion benchmark. Using this foundation, prompt words are seamlessly transformed into vivid facial representations. Our approach is a robust tool for crafting 3D talking avatars, rich in emotional depth and distinctive identity.\n\nUsing VR in a Two-Month University Course (ID: P1158)\n\nJose Joskowicz, Facultad de Ingenieria; Fabricio Gonzalez, Quantik; Inés Urrestarazu, Facultad de Ciencias Económicas\n\nAbstract\n\nThis paper describes the experience of using VR in a two-month University course. The experience was performed in the School of Economics, University of the Republic, Uruguay. Fourteen students and the professors attended the “Accounting in integrated management systems” class using Meta Quest 2 VR headsets during seven 1-hour sessions, one session per week. Different aspects were analyzed during the sessions, including audiovisual quality, comfort, sickness, immersion, presence, fatigue, cognitive load, and useful of the technology for the academic purposes.\n\nInvestigating Situated Learning Theory through an Augmented Reality Mobile Assistant for Everyday STEM Learning (ID: P1161)\n\nAbhishek Mayuresh Kulkarni, University of Florida; Cecelia Albright, University of Florida; Pratik Kamble, University of Florida; Sharon Lynn Chu, University of Florida\n\nAbstract\n\nSituated learning theory (SLT) suggests learning should take place within authentic contexts to be effective. Research in virtual and augmented reality (AR) tend to use SLT to ground their work. Yet, the effectiveness of SLT to ground designs is still questionable. This work investigates situated learning through an AR mobile application called Objectica that seeks to teach STEM (Science, Technology, Engineering, Mathematics) concepts using authentic everyday objects. A between-subjects study compared Objectica with a version not grounded in SLT. Initial results show no significant differences in the effectiveness of the two versions, questioning impact of SLT in educational app design.\n\nOrienting response is modulated by the human-likeness and realism of the virtual proposer. Exploratory study with physiological measurement. (ID: P1166)\n\nRadoslaw Sterna, Jagiellonian University in Kraków; Joanna Pilarczyk, Institute of Psychology, Faculty of Philosophy, Jagiellonian University in Kraków; Agata Szymańska, Institute of Psychology, Faculty of Philosophy, Jagiellonian University; Jakub Szczugieł, Jagiellonian University in Kraków; Magdalena Igras-Cybulska, AGH UST; Michał Kuniecki, Institute of Psychology, Faculty of Philosophy, Jagiellonian University in Kraków\n\nAbstract\n\nThis poster presents an exploratory analysis investigating the impact of virtual character's realism and human-likeness on participants' Orienting Response (OR) indexed by heart rate (HR) and skin conductance response (SCR). Fifty-nine participants, wearing HMD, watched the video recordings of the virtual characters and humans differing in terms of behavioral realism (movement and gaze), while their physiological responses were measured. Findings highlight a significant influence of behavioral realism on both indices of the Orienting Response (deeper HR deceleration and stronger SCR), which in case of SCR is further modulated by human-likeness of the virtual proposer\n\nThe validation of a Polish version of Co-Presence and Social Presence Scale. (ID: P1167)\n\nRadoslaw Sterna, Jagiellonian University in Kraków; Natalia Lipp, Sano Center for Computational Personalised Medicine; Agnieszka Strojny, Institute of Applied Psychology Faculty of Management and Social Communication, Jagiellonian University, Kraków, Poland; Michał Kuniecki, Institute of Psychology, Faculty of Philosophy, Jagiellonian University in Kraków; Paweł Strojny, Institute of Applied Psychology, Faculty of Management and Social Communication, Jagiellonian University in Kraków\n\nAbstract\n\nThis study validates the Polish translation of the Co-presence and Social Presence scale, confirming strong reliability and validity through robust internal consistency and favorable confirmatory factor analysis fit indices. Convergent validity results align with expectations, although correlation magnitudes are lower than anticipated. Examining discriminant validity, an unexpected weak positive correlation with eeriness challenges initial expectations of negative relationship. Furthermore, moderate to low correlations between co-presence and presence emphasize their distinct yet related nature\n\nXR Slate: XR Swiping and Layout Adjustment for Text Entry (ID: P1169)\n\nTheodore Okamura, University of North Carolina at Greensboro; Regis Kopper, University of North Carolina at Greensboro\n\nAbstract\n\nIn the expansive realm of virtual reality (VR) and digital interfaces, this project introduces XR Slate, an innovative text entry method to address the challenges associated with the prevailing ray-casting approach. XR Slate mitigates ray-casting difficulty in precisely hitting specific keys due by progressively refining the virtual keyboard using intuitive swiping gestures, eliminating unwanted keys and enhancing the accessibility and user-friendliness of the text entry process.\n\nEvaluating NeRF Fidelity using Virtual Environments (ID: P1170)\n\nAnder J Talley, Mississippi State University; Adam Jones, Mississippi State University\n\nAbstract\n\nNeural Radiance Fields (NeRF) are a promising form of 3D Reconstruction that utilize sparse 2D imagery to recreate synthetic and real scenes. Since NeRF was first developed, numerous methods and techniques have worked upon the original algorithm to improve its accuracy, fidelity, and speed. We aim to evaluate the fidelity of a reconstructed scene, as well as evaluate the quality of the reconstruction.\n\nPrototyping Autonomous Vehicle Lane Detection for Snow in VR (ID: P1171)\n\nTatiana Ortegon Sarmiento, Université du Québec à Trois-Rivières; Alvaro Uribe Quevedo, Ontario Tech University; Sousso Kelouwani, Université du Québec à Trois-Rivières; Patricia Paderewski Rodriguez, Universidad de Granada; Francisco Gutierrez Vela, Universidad de Granada\n\nAbstract\n\nAutonomous vehicles (AVs) are gaining momentum, and features such as autopilot are becoming widespread among consumer vehicles. AVs track the road to drive correctly, however, when they fail, the driver must take over. Lane detection is a must-have feature for AVs, which has numerous investigations. Nevertheless, most have gaps in extreme weather, such as snowy winters. The lack of snow datasets adds to this, as most include mild scenarios. This paper presents the prototype of a virtual reality digital twin that will allow training lane detection using synthetic data that would otherwise be difficult to recreate in real life.\n\nFirst Steps in Constructing an AI-Powered Digital Twin Teacher: Harnessing Large Language Models in a Metaverse Classroom (ID: P1175)\n\nMarco Fiore, Polytechnic University of Bari; Michele Gattullo, Polytechnic University of Bari; Marina Mongiello, Polytechnic University of Bari\n\nAbstract\n\nThis study proposes a ground-breaking idea at the intersection of artificial intelligence and virtual education: the creation of an AI-powered digital twin instructor in a Metaverse-based classroom using Large Language Models. We aim to build a teacher avatar capable of dynamic interactions with students, tailored teaching approaches, and contextual response inside a virtual world. The research aims to address two major issues for both students and teachers: the digital twin can provide feedbacks to resolve doubts about course content and material; also, it can improve student management and allow teachers to answer the trickiest questions raised by students.\n\nMerging Blockchain and Augmented Reality for an Immersive Traceability Platform (ID: P1176)\n\nMarco Fiore, Polytechnic University of Bari; Michele Gattullo, Polytechnic University of Bari; Marina Mongiello, Polytechnic University of Bari; Antonio E. Uva, Polytechnic Institute of Bari\n\nAbstract\n\nThe demand for ethically sourced and safe products has surged, prompting industries to adopt intricate traceability systems. Blockchain technology, renowned for its decentralized and immutable ledger, revolutionizes traceability by ensuring data integrity and transparency in supply chains. However, complexities within supply chains often obfuscate meaningful insights for consumers. This paper explores leveraging Augmented Reality to enhance Blockchain-based traceability systems. By integrating AR, consumers can seamlessly access traceability information through QR codes, presented via optimized 3D models. This immersive approach fosters trust by visually demonstrating product quality. The architecture combines QR codes, Vuforia markers, and Blockchain, ensuring data security and immutability.\n\nUsing Machine Learning to Classify EEG Data Collected With or Without Haptic Feedback During a Simulated Drilling Task (ID: P1178)\n\nMichael Ramirez, Universidad Escuela Colombiana de Ingeniería Julio Garavito; Heather McCracken, Ontario Tech University; Brianna L Grant, Ontario Tech University; Alvaro Quevedo, Ontario Tech University; Paul Yielder, Ontario Tech University; Bernadette A. Murphy, Ontario Tech University\n\nAbstract\n\nSimulation environments (SE) are becoming important tools that can be leveraged to implement training protocols and educational resources. Electroencephalography (EEG) is used to compare the effects of different types of feedback in SE, but it can be challenging to know which aspects represent the impact of those feedback on neural processing. For this study, machine learning approaches were applied to differentiate neural circuitry associated with haptic and non-haptic feedback in a simulated drilling task. Electroencephalography was analyzed based on the extraction and selection of different types of features. Trials with haptic feedback were correctly identified from those without haptic feedback.\n\nGender Identification of VR Users by Machine Learning Tracking Data (ID: P1181)\n\nQidi J. Wang, University of Central Florida; Ryan P. McMahan, University of Central Florida\n\nAbstract\n\nGender identification of virtual reality (VR) users by machine learning tracking data could afford personalized experiences, including mitigation of expected human factors or psychology issues. While much research has recently been conducted to identify individual users given their VR tracking data, little research has investigated gender identification. Furthermore, nearly all prior studies have only considered positions and rotations of all the devices. We present a systematic investigation of different combinations and spatial representations of VR tracked devices for predicting a user's gender. Our results indicate head rotations are integral to gender identification while head positions are surprisingly not as important.\n\nTacPoint: Influence of Partial Visuo-Tactile Feedback on Sense of Embodiment in Virtual Reality (ID: P1182)\n\nJingjing Zhang, University of Liverpool; Mengjie Huang, Xi'an Jiaotong-Liverpool University; Yonglin Chen, City University of Macau; Xiaoyi Xue, Xi'an Jiaotong-Liverpool University; Kailun Liao, Xi'an Jiaotong-Liverpool University; Rui Yang, Xi'an Jiaotong-Liverpool University; Jiajia Shi, Kunshan Rehabilitation Hospital\n\nAbstract\n\nThe employment of Virtual Reality in medical rehabilitation has been broadened to incorporate visual and tactile feedback, while how patients use tangible objects to induce better perceptions in VR remains unexplored. We investigated how partial visuo-tactile feedback influences users' embodiment, and proposed a design idea named TacPoint (a red point mark) that connects to the physical and virtual world. The results reported that higher embodiment illusions could be induced in the TacPoint session than in others without feedback during virtual interactions. TacPoint could help patients induce embodiment easily to increase their positive VR experience for further rehabilitation training.\n\nExploring Trustful AI Augmentation with Virtual Evacuation Study (ID: P1183)\n\nRuohan Wang, Marvin Ridge High School; Aidong Lu, University of North Carolina at Charlotte\n\nAbstract\n\nThis project created a virtual evacuation study to explore people's trust of AI in dangerous situations. Our study adopts a virtual maze-like scenario with automatic AI-augmentation and three levels of fire simulations, where participants need to make the final decision. We have collected performance data and revised the metrics of KUSIV3 for participants to report their trust of AI in the post-questionnaires. Our results show that the levels of trust towards AI are generally positive, and can be affected by dangerous conditions. The participant-reported and measured trust levels also demonstrate a loose but consistent correlation.\n\nTesting Virtualized Future Technologies Using Stressful Simulations (ID: P1184)\n\nNicole Kosoris, Georgia Institute of Technology\n\nAbstract\n\nThis work used Virtual Reality (VR) to prototype and test novel devices for First Responders, investigating using intentionally stressful scenarios to better differentiate between designs. Using a mixed methods approach, researchers designed and built calm and stressful scenarios for a traffic stop. Researchers began with qualitative methods, confirmed via survey, to determine the factors First Responders identified as most stressful in a traffic stop. Visibility was identified as the most critical design consideration. Comparisons were then done between low and high-visibility displays; participants responded significantly more negatively to the low-visibility display when in a simulated high-stress scenario.\n\nVR Reconstruction of Amazonian Geoglyphs Using LiDAR Data (ID: P1185)\n\nFang Wang, University of Missouri; Albert Zhou, University of Missouri; Robert S. Walker, University of Missouri; Mike Sturm, University of Missouri; Amith Nalmas, University of Missouri; Scottie D Murrell Mr, University of Missouri\n\nAbstract\n\nThis work presents an interactive Virtual Reality (VR) application to enhance education on Amazonian geoglyphs. Many Amazonian archaeological sites are difficult to reach and visualize as they are hidden by dense rainforest foliage. Using a combination of VR technology and LiDAR (Light Detection and Ranging) data, we generate scale models of geoglyphs without the current foliage. We use this model to create a reconstruction of the historical usage of these earthworks, based on current archaeological and anthropological interpretations. The experience is enhanced by using reconstructions of artifacts commonly found in the vicinity of these geoglyphs.\n\nMind-Body TaoRelax: Relieving Stress through Immersive Virtual Reality Relaxation Training in a Taoist Atmosphere (ID: P1186)\n\nHao Yan, Southeast University; Ding Ding, Southeast University; Zhuying Li, Southeast University\n\nAbstract\n\nVirtual reality technology is providing a new method for psychological relaxation training, but most systems focus primarily on meditation as a singular relaxation method. We propose a novel virtual reality relaxation system called TaoRelax. Building upon traditional Taoist culture, the system integrates both meditation and progressive muscle relaxation training. It combines electromyography (EMG) as a means of feedback and assessment during training, providing users with a dual experience of relaxation for both body and mind. Our preliminary results indicate that after training in our system, participants experienced a significant reduction in psychological stress and a notable improvement in meditation abilities.\n\nLight Field Transmission Fusing Image Super-Resolution and Selective Quality Patterns (ID: P1187)\n\nWei Zheng, Beijing Technology and Business University; Xiaoming Chen, Beijing Technology and Business University; Zongyou Yu, Beijing Technology and Business University; Zeke Zexi Hu, The University of Sydney; Yuk Ying Chung, The University of Sydney\n\nAbstract\n\nLight field imaging has revolutionized immersive experiences and virtual reality applications. However, the vast amount of data generated by light field imaging presents significant challenges in terms of storage and transmission. In this study, we propose a novel approach for light field storage and transmission. Our approach leverages image super-resolution, which can be utilized to significantly reduce the light field data to be transmitted while maintaining reasonable light field viewing quality. Moreover, we have devised selective transmission patterns that align with human viewing patterns, enhancing the overall efficiency of light field transmission.\n\nA Taxonomy for Guiding XR Prototyping Decisions by the Non-Tech-Savvy (ID: P1188)\n\nAssem Kroma, Carleton University; Robert J Teather, Carleton University\n\nAbstract\n\nWithin XR design, prototypes play an essential role in materializing, communicating and evaluating concepts. Yet, selecting the appropriate prototype proves intuitive for seasoned designers but daunting for beginners and the non tech-savvy. Addressing the right challenges when making decisions on the prototyping method is vital for a constructive outcome. Taxonomies emerge as powerful tools in this context. We forge a holistic taxonomy based on various prototyping taxonomies in the contexts of XR and other disciplines. We further validate it by surveying existing XR prototyping work. Next, we will further validate it through a series of steps common in such research.\n\nEmot Act AR: Tailoring Content through User Emotion and Activity Analysis (ID: P1191)\n\nSomaiieh Rokhsaritalemi, Sejong University; Abolghasem Sadeghi-Niaraki, Sejong University; Soo-Mi Choi, Sejong University\n\nAbstract\n\nIn an era characterized by immersive content delivery experiences, the convergence of augmented reality (AR) and user behavior data stands out as a transformative synergy. This paper introduces \"Emot Act AR,\" an innovative system intricately designed to dynamically tailor virtual content by user data. This framework incorporates user emotions and activities as intrinsic components. Through the utilization of sensing devices like camera and advanced AI models, the system dynamically customizes virtual elements, such as virtual flowers, in both static and dynamic settings. Additionally, the system utilizes user activity data to activate messaging avatars that encourage exercise.\n\nEnhanced Techniques to Implement Jumping-Over-Down and Jumping-At-Air using Pressure-sensing Shoes in Virtual Reality (ID: P1192)\n\nLiuyang Chen, East China Normal University; Liuyang Chen, NetEase, Inc.; Gaoqi He, East China Normal University; Changbo Wang, Depart of Software Science and Technology\n\nAbstract\n\nEnhance the experience of jumping in the virtual reality environment is interesting and"
    }
}
{
    "id": "dbpedia_2587_0",
    "rank": 88,
    "data": {
        "url": "https://paperswithcode.com/datasets",
        "read_more_link": "",
        "language": "en",
        "title": "Machine Learning Datasets",
        "top_image": "https://paperswithcode.com/static/datasets.jpg",
        "meta_img": "https://paperswithcode.com/static/datasets.jpg",
        "images": [
            "https://production-media.paperswithcode.com/thumbnails/dataset-medium/9101c960-563f-4bfa-bde0-ee5a5977d43b.jpg",
            "https://production-media.paperswithcode.com/thumbnails/dataset-medium/dataset-0000000008-a9fe912a.jpg",
            "https://production-media.paperswithcode.com/thumbnails/dataset-medium/9f876b0a-7751-43bc-9f80-cbd8c2adabf3.jpg",
            "https://production-media.paperswithcode.com/thumbnails/dataset-medium/dataset-0000000433-02db1517.jpg",
            "https://production-media.paperswithcode.com/thumbnails/dataset-medium/dataset-0000000001-5ee62d70.jpg",
            "https://production-media.paperswithcode.com/thumbnails/dataset-medium/dataset-0000003508-7a5c4248.jpg",
            "https://production-media.paperswithcode.com/thumbnails/dataset-medium/dataset-0000000061-70c4656d.jpg",
            "https://production-media.paperswithcode.com/thumbnails/dataset-medium/dataset-0000000002-e6adb1d9.jpg",
            "https://production-media.paperswithcode.com/thumbnails/dataset-medium/dataset-0000000424-e3fabab5.jpg",
            "https://production-media.paperswithcode.com/thumbnails/dataset-medium/dataset-0000000040-786bb14d.jpg",
            "https://production-media.paperswithcode.com/thumbnails/dataset-medium/dataset-0000000005-40530a41.jpg",
            "https://production-media.paperswithcode.com/thumbnails/dataset-medium/dataset-0000000324-55a42eae.jpg",
            "https://production-media.paperswithcode.com/thumbnails/dataset-medium/dataset-0000000109-8fb45530.jpg",
            "https://production-media.paperswithcode.com/thumbnails/dataset-medium/dataset-0000003511-8fb64a65.jpg",
            "https://production-media.paperswithcode.com/thumbnails/dataset-medium/dataset-0000000201-3d9e8987.jpg",
            "https://production-media.paperswithcode.com/thumbnails/dataset-medium/1c3aabfd-32bb-442a-9338-171e145c97bc.jpg",
            "https://production-media.paperswithcode.com/thumbnails/dataset-medium/dataset-0000001251-e9282896.jpg",
            "https://production-media.paperswithcode.com/thumbnails/dataset-medium/dataset-0000000006-cfa91ded.jpg",
            "https://production-media.paperswithcode.com/thumbnails/dataset-medium/dataset-0000000066-5f399b45.jpg",
            "https://production-media.paperswithcode.com/thumbnails/dataset-medium/dataset-0000003538-2a2e9f11.jpg",
            "https://production-media.paperswithcode.com/thumbnails/dataset-medium/dataset-0000005565-4e91b993.jpg",
            "https://production-media.paperswithcode.com/thumbnails/dataset-medium/dataset-0000001240-cd5da8cd.jpg",
            "https://production-media.paperswithcode.com/thumbnails/dataset-medium/dataset-0000000495-af04f025.jpg",
            "https://production-media.paperswithcode.com/thumbnails/dataset-medium/dataset-0000000590-abb4b0b3.jpg",
            "https://production-media.paperswithcode.com/thumbnails/dataset-medium/dataset-0000003524-02957e74.jpg",
            "https://production-media.paperswithcode.com/thumbnails/dataset-medium/dataset-0000000233-4dbd08d5.jpg",
            "https://production-media.paperswithcode.com/thumbnails/dataset-medium/dataset-0000000346-eeec3643.jpg",
            "https://production-media.paperswithcode.com/thumbnails/dataset-medium/dataset-0000000029-48aa33a5.gif",
            "https://production-media.paperswithcode.com/thumbnails/dataset-medium/dataset-0000000087-db925b91.jpg",
            "https://production-media.paperswithcode.com/thumbnails/dataset-medium/dataset-0000003534-8b6e0880.gif",
            "https://production-media.paperswithcode.com/thumbnails/dataset-medium/dataset-0000006146-a61d61d7.jpg",
            "https://production-media.paperswithcode.com/thumbnails/dataset-medium/dataset-0000000331-ed9b41de.jpg",
            "https://production-media.paperswithcode.com/thumbnails/dataset-medium/dataset-0000000796-a14fb79f.jpg",
            "https://production-media.paperswithcode.com/thumbnails/dataset-medium/dataset-0000001007-945d04a4.jpg",
            "https://production-media.paperswithcode.com/thumbnails/dataset-medium/dataset-0000000451-1e601a44.jpg",
            "https://production-media.paperswithcode.com/thumbnails/dataset-medium/dataset-0000003545-58909355.jpg",
            "https://production-media.paperswithcode.com/thumbnails/dataset-medium/dataset-0000001404-50df3ae0.jpg",
            "https://production-media.paperswithcode.com/thumbnails/dataset-medium/dataset-0000000425-ecbfc3ff.jpg",
            "https://production-media.paperswithcode.com/thumbnails/dataset-medium/dataset-0000000009-df5bf484.jpg",
            "https://production-media.paperswithcode.com/thumbnails/dataset-medium/2116b3a8-8377-4d7e-9574-3dc3708ddbf2.jpg",
            "https://production-media.paperswithcode.com/thumbnails/dataset-medium/dataset-0000000862-cf01d085.jpg",
            "https://production-media.paperswithcode.com/thumbnails/dataset-medium/ec05fd66-83ae-4c94-aac6-bccace3d847f.jpg",
            "https://production-media.paperswithcode.com/thumbnails/dataset-medium/dataset-0000003517-2500bb4f.jpg",
            "https://production-media.paperswithcode.com/thumbnails/dataset-medium/dataset-0000000035-8284831e.jpg"
        ],
        "movies": [],
        "keywords": [],
        "meta_keywords": [
            ""
        ],
        "tags": null,
        "authors": [],
        "publish_date": null,
        "summary": "",
        "meta_description": "10356 datasets • 137250 papers with code.",
        "meta_lang": "en",
        "meta_favicon": "",
        "meta_site_name": "",
        "canonical_link": "https://paperswithcode.com/datasets",
        "text": "The CIFAR-10 dataset (Canadian Institute for Advanced Research, 10 classes) is a subset of the Tiny Images dataset and consists of 60000 32x32 color images. The images are labelled with one of 10 mutually exclusive classes: airplane, automobile (but not truck or pickup truck), bird, cat, deer, dog, frog, horse, ship, and truck (but not pickup truck). There are 6000 images per class with 5000 training and 1000 testing images per class.\n\n14,696 PAPERS • 105 BENCHMARKS\n\nThe ImageNet dataset contains 14,197,122 annotated images according to the WordNet hierarchy. Since 2010 the dataset is used in the ImageNet Large Scale Visual Recognition Challenge (ILSVRC), a benchmark in image classification and object detection. The publicly released dataset contains a set of manually annotated training images. A set of test images is also released, with the manual annotations withheld. ILSVRC annotations fall into one of two categories: (1) image-level annotation of a binary label for the presence or absence of an object class in the image, e.g., “there are cars in this image” but “there are no tigers,” and (2) object-level annotation of a tight bounding box and class label around an object instance in the image, e.g., “there is a screwdriver centered at position (20,25) with width of 50 pixels and height of 30 pixels”. The ImageNet project does not own the copyright of the images, therefore only thumbnails and URLs of images are provided.\n\n13,964 PAPERS • 43 BENCHMARKS\n\nThe CIFAR-100 dataset (Canadian Institute for Advanced Research, 100 classes) is a subset of the Tiny Images dataset and consists of 60000 32x32 color images. The 100 classes in the CIFAR-100 are grouped into 20 superclasses. There are 600 images per class. Each image comes with a \"fine\" label (the class to which it belongs) and a \"coarse\" label (the superclass to which it belongs). There are 500 training images and 100 testing images per class.\n\n8,049 PAPERS • 55 BENCHMARKS\n\nThe MNIST database (Modified National Institute of Standards and Technology database) is a large collection of handwritten digits. It has a training set of 60,000 examples, and a test set of 10,000 examples. It is a subset of a larger NIST Special Database 3 (digits written by employees of the United States Census Bureau) and Special Database 1 (digits written by high school students) which contain monochrome images of handwritten digits. The digits have been size-normalized and centered in a fixed-size image. The original black and white (bilevel) images from NIST were size normalized to fit in a 20x20 pixel box while preserving their aspect ratio. The resulting images contain grey levels as a result of the anti-aliasing technique used by the normalization algorithm. the images were centered in a 28x28 image by computing the center of mass of the pixels, and translating the image so as to position this point at the center of the 28x28 field.\n\n7,137 PAPERS • 52 BENCHMARKS\n\nCityscapes is a large-scale database which focuses on semantic understanding of urban street scenes. It provides semantic, instance-wise, and dense pixel annotations for 30 classes grouped into 8 categories (flat surfaces, humans, vehicles, constructions, objects, nature, sky, and void). The dataset consists of around 5000 fine annotated images and 20000 coarse annotated ones. Data was captured in 50 cities during several months, daytimes, and good weather conditions. It was originally recorded as video so the frames were manually selected to have the following features: large number of dynamic objects, varying scene layout, and varying background.\n\n3,435 PAPERS • 54 BENCHMARKS\n\nKITTI (Karlsruhe Institute of Technology and Toyota Technological Institute) is one of the most popular datasets for use in mobile robotics and autonomous driving. It consists of hours of traffic scenarios recorded with a variety of sensor modalities, including high-resolution RGB, grayscale stereo cameras, and a 3D laser scanner. Despite its popularity, the dataset itself does not contain ground truth for semantic segmentation. However, various researchers have manually annotated parts of the dataset to fit their necessities. Álvarez et al. generated ground truth for 323 images from the road detection challenge with three classes: road, vertical, and sky. Zhang et al. annotated 252 (140 for training and 112 for testing) acquisitions – RGB and Velodyne scans – from the tracking challenge for ten object categories: building, sky, road, vegetation, sidewalk, car, pedestrian, cyclist, sign/pole, and fence. Ros et al. labeled 170 training images and 46 testing images (from the visual odome\n\n3,352 PAPERS • 141 BENCHMARKS\n\nStreet View House Numbers (SVHN) is a digit classification benchmark dataset that contains 600,000 32×32 RGB images of printed digits (from 0 to 9) cropped from pictures of house number plates. The cropped images are centered in the digit of interest, but nearby digits and other distractors are kept in the image. SVHN has three sets: training, testing sets and an extra set with 530,000 images that are less difficult and can be used for helping with the training process.\n\n3,166 PAPERS • 12 BENCHMARKS\n\nNeural Radiance Fields (NeRF) is a method for synthesizing novel views of complex scenes by optimizing an underlying continuous volumetric scene function using a sparse set of input views. The dataset contains three parts with the first 2 being synthetic renderings of objects called Diffuse Synthetic 360◦ and Realistic Synthetic 360◦ while the third is real images of complex scenes. Diffuse Synthetic 360◦ consists of four Lambertian objects with simple geometry. Each object is rendered at 512x512 pixels from viewpoints sampled on the upper hemisphere. Realistic Synthetic 360◦ consists of eight objects of complicated geometry and realistic non-Lambertian materials. Six of them are rendered from viewpoints sampled on the upper hemisphere and the two left are from viewpoints sampled on a full sphere with all of them at 800x800 pixels. The real images of complex scenes consist of 8 forward-facing scenes captured with a cellphone at a size of 1008x756 pixels.\n\n3,001 PAPERS • 1 BENCHMARK\n\nThe Stanford Sentiment Treebank is a corpus with fully labeled parse trees that allows for a complete analysis of the compositional effects of sentiment in language. The corpus is based on the dataset introduced by Pang and Lee (2005) and consists of 11,855 single sentences extracted from movie reviews. It was parsed with the Stanford parser and includes a total of 215,154 unique phrases from those parse trees, each annotated by 3 human judges.\n\n2,112 PAPERS • 9 BENCHMARKS\n\nThe Caltech-UCSD Birds-200-2011 (CUB-200-2011) dataset is the most widely-used dataset for fine-grained visual categorization task. It contains 11,788 images of 200 subcategories belonging to birds, 5,994 for training and 5,794 for testing. Each image has detailed annotations: 1 subcategory label, 15 part locations, 312 binary attributes and 1 bounding box. The textual information comes from Reed et al.. They expand the CUB-200-2011 dataset by collecting fine-grained natural language descriptions. Ten single-sentence descriptions are collected for each image. The natural language descriptions are collected through the Amazon Mechanical Turk (AMT) platform, and are required at least 10 words, without any information of subcategories and actions.\n\n2,055 PAPERS • 46 BENCHMARKS\n\nThe LibriSpeech corpus is a collection of approximately 1,000 hours of audiobooks that are a part of the LibriVox project. Most of the audiobooks come from the Project Gutenberg. The training data is split into 3 partitions of 100hr, 360hr, and 500hr sets while the dev and test data are split into the ’clean’ and ’other’ categories, respectively, depending upon how well or challenging Automatic Speech Recognition systems would perform against. Each of the dev and test sets is around 5hr in audio length. This corpus also provides the n-gram language models and the corresponding texts excerpted from the Project Gutenberg books, which contain 803M tokens and 977K unique words.\n\n2,038 PAPERS • 8 BENCHMARKS\n\nThe Stanford Question Answering Dataset (SQuAD) is a collection of question-answer pairs derived from Wikipedia articles. In SQuAD, the correct answers of questions can be any sequence of tokens in the given text. Because the questions and answers are produced by humans through crowdsourcing, it is more diverse than some other question-answering datasets. SQuAD 1.1 contains 107,785 question-answer pairs on 536 articles. SQuAD2.0 (open-domain SQuAD, SQuAD-Open), the latest version, combines the 100,000 questions in SQuAD1.1 with over 50,000 un-answerable questions written adversarially by crowdworkers in forms that are similar to the answerable ones.\n\n1,983 PAPERS • 11 BENCHMARKS\n\nShapeNet is a large scale repository for 3D CAD models developed by researchers from Stanford University, Princeton University and the Toyota Technological Institute at Chicago, USA. The repository contains over 300M models with 220,000 classified into 3,135 classes arranged using WordNet hypernym-hyponym relationships. ShapeNet Parts subset contains 31,693 meshes categorised into 16 common object classes (i.e. table, chair, plane etc.). Each shapes ground truth contains 2-5 parts (with a total of 50 part classes).\n\n1,761 PAPERS • 13 BENCHMARKS\n\nThe nuScenes dataset is a large-scale autonomous driving dataset. The dataset has 3D bounding boxes for 1000 scenes collected in Boston and Singapore. Each scene is 20 seconds long and annotated at 2Hz. This results in a total of 28130 samples for training, 6019 samples for validation and 6008 samples for testing. The dataset has the full autonomous vehicle data suite: 32-beam LiDAR, 6 cameras and radars with complete 360° coverage. The 3D object detection challenge evaluates the performance on 10 classes: cars, trucks, buses, trailers, construction vehicles, pedestrians, motorcycles, bicycles, traffic cones and barriers.\n\n1,719 PAPERS • 20 BENCHMARKS\n\nThe Multi-Genre Natural Language Inference (MultiNLI) dataset has 433K sentence pairs. Its size and mode of collection are modeled closely like SNLI. MultiNLI offers ten distinct genres (Face-to-face, Telephone, 9/11, Travel, Letters, Oxford University Press, Slate, Verbatim, Goverment and Fiction) of written and spoken English data. There are matched dev/test sets which are derived from the same sources as those in the training set, and mismatched sets which do not closely resemble any seen at training time.\n\n1,713 PAPERS • 3 BENCHMARKS\n\nUCF101 dataset is an extension of UCF50 and consists of 13,320 video clips, which are classified into 101 categories. These 101 categories can be classified into 5 types (Body motion, Human-human interactions, Human-object interactions, Playing musical instruments and Sports). The total length of these video clips is over 27 hours. All the videos are collected from YouTube and have a fixed frame rate of 25 FPS with the resolution of 320 × 240.\n\n1,691 PAPERS • 25 BENCHMARKS\n\nThe IMDb Movie Reviews dataset is a binary sentiment analysis dataset consisting of 50,000 reviews from the Internet Movie Database (IMDb) labeled as positive or negative. The dataset contains an even number of positive and negative reviews. Only highly polarizing reviews are considered. A negative review has a score ≤ 4 out of 10, and a positive review has a score ≥ 7 out of 10. No more than 30 reviews are included per movie. The dataset contains additional unlabeled data.\n\n1,645 PAPERS • 11 BENCHMARKS\n\nVisual Question Answering (VQA) is a dataset containing open-ended questions about images. These questions require an understanding of vision, language and commonsense knowledge to answer. The first version of the dataset was released in October 2015. VQA v2.0 was released in April 2017.\n\n1,617 PAPERS • NO BENCHMARKS YET\n\nThe Stanford Sentiment Treebank is a corpus with fully labeled parse trees that allows for a complete analysis of the compositional effects of sentiment in language. The corpus is based on the dataset introduced by Pang and Lee (2005) and consists of 11,855 single sentences extracted from movie reviews. It was parsed with the Stanford parser and includes a total of 215,154 unique phrases from those parse trees, each annotated by 3 human judges.\n\n1,588 PAPERS • 4 BENCHMARKS\n\nScanNet is an instance-level indoor RGB-D dataset that includes both 2D and 3D data. It is a collection of labeled voxels rather than points or objects. Up to now, ScanNet v2, the newest version of ScanNet, has collected 1513 annotated scans with an approximate 90% surface coverage. In the semantic segmentation task, this dataset is marked in 20 classes of annotated 3D voxelized objects.\n\n1,338 PAPERS • 20 BENCHMARKS\n\nFlickr-Faces-HQ (FFHQ) consists of 70,000 high-quality PNG images at 1024×1024 resolution and contains considerable variation in terms of age, ethnicity and image background. It also has good coverage of accessories such as eyeglasses, sunglasses, hats, etc. The images were crawled from Flickr, thus inheriting all the biases of that website, and automatically aligned and cropped using dlib. Only images under permissive licenses were collected. Various automatic filters were used to prune the set, and finally Amazon Mechanical Turk was used to remove the occasional statues, paintings, or photos of photos.\n\n1,291 PAPERS • 16 BENCHMARKS\n\nThe ModelNet40 dataset contains synthetic object point clouds. As the most widely used benchmark for point cloud analysis, ModelNet40 is popular because of its various categories, clean shapes, well-constructed dataset, etc. The original ModelNet40 consists of 12,311 CAD-generated meshes in 40 categories (such as airplane, car, plant, lamp), of which 9,843 are used for training while the rest 2,468 are reserved for testing. The corresponding point cloud data points are uniformly sampled from the mesh surfaces, and then further preprocessed by moving to the origin and scaling into a unit sphere.\n\n1,278 PAPERS • 17 BENCHMARKS\n\nThe SNLI dataset (Stanford Natural Language Inference) consists of 570k sentence-pairs manually labeled as entailment, contradiction, and neutral. Premises are image captions from Flickr30k, while hypotheses were generated by crowd-sourced annotators who were shown a premise and asked to generate entailing, contradicting, and neutral sentences. Annotators were instructed to judge the relation between sentences given that they describe the same event. Each pair is labeled as “entailment”, “neutral”, “contradiction” or “-”, where “-” indicates that an agreement could not be reached.\n\n1,254 PAPERS • 1 BENCHMARK\n\nVisual Genome contains Visual Question Answering data in a multi-choice setting. It consists of 101,174 images from MSCOCO with 1.7 million QA pairs, 17 questions per image on average. Compared to the Visual Question Answering dataset, Visual Genome represents a more balanced distribution over 6 question types: What, Where, When, Who, Why and How. The Visual Genome dataset also presents 108K images with densely annotated objects, attributes and relationships.\n\n1,184 PAPERS • 19 BENCHMARKS\n\nCARLA (CAR Learning to Act) is an open simulator for urban driving, developed as an open-source layer over Unreal Engine 4. Technically, it operates similarly to, as an open source layer over Unreal Engine 4 that provides sensors in the form of RGB cameras (with customizable positions), ground truth depth maps, ground truth semantic segmentation maps with 12 semantic classes designed for driving (road, lane marking, traffic sign, sidewalk and so on), bounding boxes for dynamic objects in the environment, and measurements of the agent itself (vehicle location and orientation).\n\n1,134 PAPERS • 4 BENCHMARKS\n\nThe MovieLens datasets, first released in 1998, describe people’s expressed preferences for movies. These preferences take the form of tuples, each the result of a person expressing a preference (a 0-5 star rating) for a movie at a particular time. These preferences were entered by way of the MovieLens web site1 — a recommender system that asks its users to give movie ratings in order to receive personalized movie recommendations.\n\n1,122 PAPERS • 16 BENCHMARKS\n\nThe Natural Questions corpus is a question answering dataset containing 307,373 training examples, 7,830 development examples, and 7,842 test examples. Each example is comprised of a google.com query and a corresponding Wikipedia page. Each Wikipedia page has a passage (or long answer) annotated on the page that answers the question and one or more short spans from the annotated passage containing the actual answer. The long and the short answer annotations can however be empty. If they are both empty, then there is no answer on the page at all. If the long answer annotation is non-empty, but the short answer annotation is empty, then the annotated passage answers the question but no explicit short answer could be found. Finally 1% of the documents have a passage annotated with a short answer that is “yes” or “no”, instead of a list of short spans.\n\n1,114 PAPERS • 10 BENCHMARKS\n\nThe QNLI (Question-answering NLI) dataset is a Natural Language Inference dataset automatically derived from the Stanford Question Answering Dataset v1.1 (SQuAD). SQuAD v1.1 consists of question-paragraph pairs, where one of the sentences in the paragraph (drawn from Wikipedia) contains the answer to the corresponding question (written by an annotator). The dataset was converted into sentence pair classification by forming a pair between each question and each sentence in the corresponding context, and filtering out pairs with low lexical overlap between the question and the context sentence. The task is to determine whether the context sentence contains the answer to the question. This modified version of the original task removes the requirement that the model select the exact answer, but also removes the simplifying assumptions that the answer is always present in the input and that lexical overlap is a reliable cue. The QNLI dataset is part of GLUE benchmark.\n\n1,099 PAPERS • 3 BENCHMARKS\n\nMMLU (Massive Multitask Language Understanding) is a new benchmark designed to measure knowledge acquired during pretraining by evaluating models exclusively in zero-shot and few-shot settings. This makes the benchmark more challenging and more similar to how we evaluate humans. The benchmark covers 57 subjects across STEM, the humanities, the social sciences, and more. It ranges in difficulty from an elementary level to an advanced professional level, and it tests both world knowledge and problem solving ability. Subjects range from traditional areas, such as mathematics and history, to more specialized areas like law and ethics. The granularity and breadth of the subjects makes the benchmark ideal for identifying a model’s blind spots.\n\n982 PAPERS • 25 BENCHMARKS\n\nOffice-Home is a benchmark dataset for domain adaptation which contains 4 domains where each domain consists of 65 categories. The four domains are: Art – artistic images in the form of sketches, paintings, ornamentation, etc.; Clipart – collection of clipart images; Product – images of objects without a background and Real-World – images of objects captured with a regular camera. It contains 15,500 images, with an average of around 70 images per class and a maximum of 99 images in a class.\n\n975 PAPERS • 11 BENCHMARKS\n\nThe Medical Information Mart for Intensive Care III (MIMIC-III) dataset is a large, de-identified and publicly-available collection of medical records. Each record in the dataset includes ICD-9 codes, which identify diagnoses and procedures performed. Each code is partitioned into sub-codes, which often include specific circumstantial details. The dataset consists of 112,000 clinical reports records (average length 709.3 tokens) and 1,159 top-level ICD-9 codes. Each report is assigned to 7.6 codes, on average. Data includes vital signs, medications, laboratory measurements, observations and notes charted by care providers, fluid balance, procedure codes, diagnostic codes, imaging reports, hospital length of stay, survival data, and more.\n\n945 PAPERS • 8 BENCHMARKS"
    }
}
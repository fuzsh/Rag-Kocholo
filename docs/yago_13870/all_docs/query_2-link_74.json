{
    "id": "yago_13870_2",
    "rank": 74,
    "data": {
        "url": "https://yann.lecun.com/",
        "read_more_link": "",
        "language": "en",
        "title": "Yann LeCun's Home Page",
        "top_image": "",
        "meta_img": "",
        "images": [
            "https://yann.lecun.com/ex/images/ylc-thumb.jpeg",
            "https://yann.lecun.com/ex/images/menuhline.gif",
            "https://yann.lecun.com/ex/images/menuhline.gif",
            "https://yann.lecun.com/ex/images/menuhline.gif",
            "https://yann.lecun.com/ex/images/menuhline.gif",
            "https://yann.lecun.com/ex/images/menuhline.gif",
            "https://yann.lecun.com/ex/images/lush-logo-02-88x31.png",
            "https://yann.lecun.com/ex/images/get_djvu2.gif",
            "https://yann.lecun.com/ex/images/plugin-88x31.gif",
            "https://yann.lecun.com/ex/images/menuhline.gif",
            "https://yann.lecun.com/ex/images/lush-logo-02-138x54.png",
            "https://yann.lecun.com/ex/images/nips-online-128x64.gif",
            "https://yann.lecun.com/ex/images/djvuzone-138x69.gif",
            "https://yann.lecun.com/ex/images/a2d-138x50.gif",
            "https://yann.lecun.com/ex/images/b2w-80x40.gif",
            "https://yann.lecun.com/ex/images/djvu_badge-140x49.gif",
            "https://yann.lecun.com/ex/images/menuhline.gif",
            "https://yann.lecun.com/ex/images/freeculture.gif",
            "http://www.catb.org/hacker-emblem/glider.png",
            "https://yann.lecun.com/ex/images/menuhline.gif",
            "https://yann.lecun.com/ex/images/banner.gif",
            "https://yann.lecun.com/ex/images/name.gif",
            "https://yann.lecun.com/ex/images/banner-cbll-small.jpg",
            "https://yann.lecun.com/ex/images/cbllresearch-01.png",
            "https://yann.lecun.com/ex/images/lagr-vehicle-small.jpg",
            "https://yann.lecun.com/ex/images/lagr-stereo.png",
            "https://yann.lecun.com/ex/images/demo03-anim.gif",
            "https://yann.lecun.com/ex/images/lenet7-norb.jpg",
            "https://yann.lecun.com/ex/images/norb.jpg",
            "https://yann.lecun.com/ex/images/norb-sample-01.png",
            "https://yann.lecun.com/ex/images/lush-logo-03-138x102.png",
            "https://yann.lecun.com/ex/images/djvu_badge-140x49.gif",
            "https://yann.lecun.com/ex/images/pharm-thumb.jpeg",
            "https://yann.lecun.com/ex/images/a23-small.gif",
            "https://yann.lecun.com/ex/images/plane01-thumb.jpg",
            "https://yann.lecun.com/ex/images/b2w-80x40.gif",
            "https://yann.lecun.com/ex/images/email.gif"
        ],
        "movies": [],
        "keywords": [],
        "meta_keywords": [
            "Yann",
            "LeCun",
            "Le Cun",
            "DjVu",
            "neural networks",
            "convolutional neural nets",
            "machine learning",
            "pattern recognition",
            "OCR",
            "handwriting recognition",
            "computer vision",
            "visual learning",
            "invariance",
            "invariant perception",
            "graph transformer networks",
            "document imaging",
            "image compression",
            "image processing",
            "digital library"
        ],
        "tags": null,
        "authors": [],
        "publish_date": null,
        "summary": "",
        "meta_description": "Yann LeCun's Home Page",
        "meta_lang": "",
        "meta_favicon": "",
        "meta_site_name": "",
        "canonical_link": null,
        "text": "HOME BIO PUBLICATIONS arXiv Papers G-Scholar Profile SOFTWARE CBLL RESEARCH TEACHING WHAT's NEW DjVu LENET MNIST OCR DATA NORB DATASET MUSIC PHOTOS HOBBIES FUN STUFF LINKS CILVR CDS CS Dept Courant NYU Websites that I maintain\n\nYann LeCun,\n\nChief AI Scientist, Meta\n\nJacob T. Schwartz Professor of Computer Science, Data Science, Neural Science, and Electrical and Computer Engineering, New York University.\n\nACM Turing Award Laureate, (sounds like I'm bragging, but a condition of accepting the award is to write this next to your name)\n\nMember, National Academy of Engineering, National Academy of Sciences, AcadÃ©mie des Sciences\n\nFellow, ACM, AAAI, AAAS, SIF\n\nlast updated: 2024-07-14\n\nSocial Networks\n\nFacebook: yann.lecun\n\nTwitter: @ylecun\n\nLinkedIn: yann-lecun\n\nThreads: @yannlecun\n\nBiography / CV\n\nCurriculum Vitae\n\nbios of various lengths in English and en francais\n\nContact Information\n\nNYU Affiliations:\n\nCILVR Lab (Computational Intelligence, Learning, Vision, Robotics), NYU\n\nComputer Science Department, Courant Institute of Mathematical Sciences, NYU\n\nCenter for Data Science, NYU\n\nCenter for Neural Science, NYU Faculty of Arts and Sciences\n\nDepartment of Electrical and Computer Engineering, NYU Tandon School of Engineering\n\nMeta Affiliation:\n\nMeta FAIR (Fundamental AI Research)\n\nAssistants\n\nExecutive Assistant - Meta: Sean Nguyen: sean0[at]meta.com\n\nAdministrative Aide - NYU: Hong Tam +1-212-998-3374 hongtam[at]cs.nyu.edu\n\nFOR INVITATIONS TO SPEAK: please send email to lecuninvites[at]gmail.com\n\n(I really can't handle invitations sent to other email addresses)\n\nIF YOU REALLY NEED ME TO DO SOMETHING FOR YOU: (e.g. a review, a letter...) please send email to Sean Nguyen sean0[at]meta.com\n\nNYU coordinates:\n\nAddress: Room 516, 60 Fifth Avenue, New York, NY 10011, USA.\n\nEmail: yann.lecun[at]nyu.edu (I may not respond right away)\n\nPhone: +1-212-998-3283 (I am very unlikely to respond or listen to voice mail in a timely manner)\n\nMeta Coordinates:\n\nAddress: 380 W 33rd St, New York, NY 10001\n\nEmail: yann[at]meta.com (I may not respond right away)\n\nPublications, Talks, Courses, Videos, Podcasts, Interviews\n\nPublications:\n\nGoogle Scholar\n\nPapers on OpenReview.net\n\nPreprints on ArXiv Out of date list of publications with PDFs and DjVu\n\nTalks / Slide Decks:\n\nSlides of (most of my) talks\n\nDeep Learning Course:\n\nDeep Learning course at NYU:\n\nComplete course on Deep Learning, with all the material available on line including lecture and practicum videos, slide decks, homeworks, Jupyter notebooks, and transcripts in several languages.\n\nVideos: Playlists on YouTube:\n\nTalks by Yann LeCun\n\nLectures Series by Yann LeCun\n\nDebates and Panels with Yann LeCun\n\nInterviews of Yann LeCun\n\nDemos by Yann LeCun\n\nSix short videos to explain AI, Machine Learning, Deep Learning and Convolutional Nets\n\nPodcasts in English:\n\nLex Friedman #416, 03/2024 YouTube \"Meta AI, Open Source, Limits of LLMs, AGI & the Future of AI\"\n\nTwenty Minute VC with Harry Stebbing, 05/2023 Podcast \"Yann LeCun on Why Artificial Intelligence Will Not Dominate Humanity...\"\n\nWith Andrew Ng, 04/2023 YouTube \"Yann LeCun and Andrew Ng: Why the 6-month AI Pause is a Bad Idea\"\n\nBig Technology Podcast with Alex Kantrowitz, 01/2023 YouTube \"Is ChatGPT A Step Toward Human-Level AI?\"\n\nBoz to the Future with Andrew Bosworth, 08/2022 Apple Podcasts\n\nEye on AI with Craig Smith #150 podcast \"World Models, AI Threats and Open Sourcing\"\n\nLex Friedman #258, 01/2022 YouTube \"Dark Matter of Intelligence and Self-Supervised Learning\"\n\nBig Technology Podcast with Alex Kantrowitz, 12/2021 YouTube \"Daniel Kahneman and Yann LeCun: How To Get AI To Think Like Humans\"\n\nThe Robot Brains Podcast with Pieter Abbeel, 09/2021 YouTube \"Yann LeCun explains why Facebook would crumble without AI\"\n\nThe Gradient Podcast, 08/2021 The Gradient \"Yann LeCun on his Start in Research and Self-Supervised Learning\"\n\nTED with Chris Anderson, 06/2020 Video \"Deep learning, neural networks and the future of AI\"\n\nLex Friedman #36, 08/2019 YouTube \"Deep Learning, ConvNets, and Self-Supervised Learning\"\n\nEye on AI with Craig Smith #114 podcast \"Filling the gap in LLMs\"\n\nEye on AI with Craig Smith #017, 06/2019 video,podcast\n\nPodcasts en franÃ§ais:\n\nGeneration DIY #397 avec Matthieu Stefani 06/2024 podcast \"LâIntelligence Artificielle GÃ©nÃ©rale ne viendra pas de Chat GPT\"\n\nToutes mes interviews sur France Inter playlist\n\nInterview sur Europe1 06/2023 podcast \"Yann LeCun : Â«L'intelligence artificielle va amplifier l'intelligence humaineÂ»\"\n\nMain Research Interests:\n\nAI, Machine Learning, Computer Vision, Robotics, and Computational Neuroscience. I am also interested Physics of Computation, and many applications of machine learning.\n\nWorking Paper\n\nA Path Towards Autonomous Machine Intelligence\n\n(June 2022)\n\nHow could machines learn as efficiently as humans and animals? How could machines learn to reason and plan? How could machines learn representations of percepts and action plans at multiple levels of abstraction, enabling them to reason, predict, and plan at multiple time horizons? This position paper proposes an architecture and training paradigms with which to construct autonomous intelligent agents. It combines concepts such as configurable predictive world model, behavior driven through intrinsic motivation, and hierarchical joint embedding architectures trained with self-supervised learning.\n\nBooks\n\nQuand La Machine Apprend\n\nLa revolution des neurones artificiels et de l'apprentissage profond (Editions Odile Jacob, Octobre 2019)\n\nExists in Chinese, Japanese, and Russian.\n\nLa Plus Belle Histoire de l'Intelligence\n\nDes origines aux neurones artificiels : vers une nouvelle Ã©tape de l'Ã©volution\n\nStanislas Dehaene, Yann Le Cun, Jacques Girardon (Ãditions Robert Laffont, Octobre 2018)\n\nPamphlets and opinions\n\nProposal for a new publishing model in Computer Science\n\nMany computer Science researchers are complaining that our emphasis on highly selective conference publications, and our double-blind reviewing system stifles innovation and slow the rate of progress of Science and technology.\n\nThis pamphlet proposes a new publishing model based on an open repository and open (but anonymous) reviews which creates a \"market\" between papers and reviewing entities.\n\nMORE INFORMATION >>>>>\n\nStudents and Postdocs\n\nCurrent PhD Students\n\nGaoyue ''Kathy'' Zhou (NYU CS with Lerrel Pinto) [SSL for control and planning]\n\nPeter Tong (NYU CS with Saining Xie) [SSL for video]\n\nKevin Zhang (NYU CS) [SSL for planning]\n\nMegi Dervishi (FAIR-UniversitÃ© Paris-Dauphine with Alexandre Allauzen) [SSL for text]\n\nQuentin Garrido (FAIR-UniversitÃ© Gustave Eiffel with Laurent Najman) [SSL for images and video]\n\nVlad Sobal (NYU CDS) [SSL for planning and control]\n\nCurrent Postdocs\n\nAmir Bar (FAIR) [SSL for video]\n\nRavid Schwartz-Ziv (NYU) [SSL and information theory]\n\nGelareh Naseri (NYU) [music synthesis and composition]\n\nFormer PhD Students\n\nKatrina Drozdov Evtimova (2024 NYU CDS) [latent variable JEPA]\n\nAdrien Bardes (2024 FAIR-INRIA with Jean Ponce) [SSL, VICReg, I-JEPA, V-JEPA]. FAIR\n\nZeming Lin (2023 NYU CS) [Transformers for protein structure]. FAIR, EvolutionaryScale AI\n\nAishwarya Kamath (2023 NYU CDS) [vision-language models] DeepMind\n\nJunbo ``Jake'' Zhao (2019 NYU CS) [energy-based models] faculty Zhejiang University\n\nXiang Zhang (2018 NYU CS) [deep learning for NLP] Element AI, Google AI, startup\n\nMikael Henaff (2018 NYU CS) [deep learning for control] Microsoft Research, FAIR\n\nRemi Denton (2018 NYU CS, with Rob Fergus) [video prediction] Google\n\nSainbayar Sukhbaatar (2018, NYU CS with Rob Fergus) [memory, intrinsic motivation, multiagent communication] FAIR\n\nMichael Mathieu (2017 NYU CS) [DL for video prediction and image understanding] DeepMind\n\nJure Zbontar (2016 U. of Ljubljana, co-advised) [DL for stereo vision] NYU, FAIR, OpenAI\n\nSixin Zhang (2016 NYU CS) [paralellized deep learning] ENS-Paris, faculty Institut National Polytechnique de Toulouse\n\nWojciech Zaremba (2016 NYU CS with Rob Fergus) [algorithm synthesis] OpenAI\n\nRotislav Goroshin (2015 NYU CS) [unsupervised representation learning] DeepMind\n\nPierre Sermanet (2014 NYU CS) [DL for vision and mobile robot perception] Google Brain, DeepMind\n\nClÃ©ment Farabet (2014 U. Gustave Eiffel with Laurent Najman) [dedicated hardware for ConvNets, vision, Torch-7] Twitter, Nvidia, VP of Research DeepMind\n\nFu Jie Huang (2013 NYU CS) [DL for vision] Milabra, Kanerai\n\nKevin Jarrett (2012 NYU Neural Science) [DL models of biological vision] Bridgewater,...,Barclays\n\nMatthew Grimes (2012 NYU) [SLAM] Cambridge, DeepMind\n\nY-Lan Boureau (2012, NYU-INRIA with Jean Ponce) [sparse feature learning for vision] Flatiron Institute, FAIR, CEO ThrivePal)\n\nKoray Kavukcuoglu (2010, NYU) [sparse auto-encoders for unsupervised feature learning] NEC Labs, VP or Research DeepMind\n\nPiotr Mirowski (2010 NYU) Bell Labs, Microsoft, DeepMind\n\nAyse Naz Erkan (2010 NYU, with Yasemine Altun) Twitter, Robinhood, CEO Laminar AI.\n\nMarc'Aurelio Ranzato (2009 NYU) Google X-Labs, FAIR, DeepMind.\n\nSumit Chopra (2008 NYU) AT&T Labs-Research, FAIR, Imagen, faculty NYU.\n\nRaia Hadsell (2008 NYU) SRI, VP DeepMind\n\nFeng Ning (2006 NYU) Bank of America, SociÃ©tÃ© GÃ©nÃ©rale, ScotiaBank, AQR Capital, VP AllianceBernstein.\n\nFormer Postdocs\n\nMicah Goldblum (NYU 2021-2024), Columbia University\n\nGrÃ©goire Mialon (FAIR 2021-2023), Meta-GenAI\n\nRandall Balestriero (FAIR 2021-2023), Brown University\n\nNicolas Carion (NYU 2020-2022), FAIR\n\nYubei Chen (FAIR 2020-2022), UC Davis\n\nLi Jing (FAIR 2019-2021), OpenAI\n\nJacob Browning (NYU 2019-2023): philosophy and history of AI (Berggruen Transformation of the Human program)\n\nPhillip Schmitt (NYU 2019-2021): AI and the visual arts (Berggruen Transformation of the Human program)\n\nStÃ©phane Deny (FAIR 2019-2021), U of Aalto\n\nAlfredo Canziani (NYU 2017-2022), NYU: autonomous driving, AI education\n\nBehnam Neyshabur (NYU 20172019), Google, DeepMind: deep learning landscape, self-supervised learning\n\nJure Zbontar (NYU 2016-2017). FAIR, OpenAI: temporal prediction\n\nAnna Choromanska (NYU 2014-2017) NYU Tandon: applied mathematics\n\nPablo Sprechmann (NYU 2014-2017), DeepMind: applied mathematics and signal processing\n\nJoan Bruna (NYU 2012-2014), FAIR, UC Berkeley, NYU: applied mathematics\n\nCamille Couprie (NYU 2011-2013), FAIR: computer vision\n\nTom Schaul (NYU 2011-2013), DeepMind: machine learning and optimization\n\nJason Rolfe (NYU 2011-2013), D-Wave, Variational AI: computational neuroscience\n\nLeo Zhu (NYU 2010-2011), CEO Yitu: hierarchical vision models.\n\nArthur Szlam (NYU 2009-2011), CUNY, FAIR, DeepMind: applied mathematics.\n\nKarol Gregor (NYU 2008-2011), Janelia Farm, DeepMind: machine learning.\n\nTrivikraman Thampy (NYU 2008-2009), CEO Play Games24x7: financial modeling and prediction.\n\nJoseph Turian (NYU 2007-2007), Founder MetaOptimize: energy-based models.\n\nBragging Zone\n\nHonors and Awards\n\nProfessor Honoris Causa, ESIEE / UniversitÃ© Gustave Eiffel, 2024, [link]\n\nLifetime Honorary Membership, New York Academy of Sciences, 2024, [link]\n\nFellow Association for Computing Machinery, 2024, [link]\n\nGreat Immigrant, Carnegie Corporation of New York, 2024, [link]\n\nTIME 100 Impact Award, 2024, [link]\n\nMembre d'Honneur, SociÃ©tÃ© Informatique de France, 2024, [link]\n\nChevalier de la LÃ©gion d'Honneur, France, 2020/2023, [link]\n\nGlobal Swiss AI Award for outstanding global impact in the field of artificial intelligence, 2023, [link]\n\nInaugural Professorship, Jacob T. Schwartz Chair in Computer Science, Courant Institute, NYU. 2023, [link]\n\nDoctorate Honoris Causa, Hong Kong University of Science and Technology, 2023, [link]\n\nDoctorate Honoris Causa, UniversitÃ di Siena, 2023, [link]\n\nPrincess of Asturias Award, for Technical and Scientific Research (with Demis Hassabis, Yoshua Bengio, and Geoffrey Hinton), 2022, [link]\n\nForeign Member, AcadÃ©mie des Sciences, France, 2022, [link]\n\nFellow, American Association for the Advancement of Science, 2021, [link]\n\nMember, US National Academy of Sciences, 2021, [link]\n\nDoctorate Honoris Causa, UniversitÃ© CÃ´te d'Azur, 2021, [link]\n\nFellow, Association for the Advancement of Artificial Intelligence, 2020, [link]\n\nGolden Plate Award, International Academy of Achievement, 2019, [link]\n\nACM A.M. Turing Award, 2018 (shared with Geoffrey Hinton and Yoshua Bengio), [link]\n\nDoctorate Honoris Causa, Ecole Polytechnique FÃ©dÃ©rale de Lausanne, 2018, [link]\n\nHolst Medal, Technical University of Eindhoven \\& Philips Labs, The Netherlands\n\nPender Award, University of Pennsylvania, 2018\n\nMember, US National Academy of Engineering, Class of 2017.\n\nNokia-Bell Labs Shannon Luminary Award, 2017\n\nAnnual Chair in Computer Science, CollÃ¨ge de France 2015-2016.\n\nLovie Lifetime Achievement Award, International Academy of Digital Arts and Sciences, 2016.\n\nInductee, New Jersey Inventor Hall of Fame, 2016.\n\nDoctorate Honoris Causa, Instituto PolitÃ©cnico Nacional, Mexico, 2016.\n\nIEEE Pattern Analysis and Machine Intelligence Distinguished Researcher Award, 2015.\n\nIEEE Neural Network Pioneer Award, 2014.\n\nNYU Silver Professorship, 2008.\n\nFyssen Foundation Fellowship, 1987.\n\nIn the Media\n\nYann Le Cun (Meta), rock star discrÃ¨te de la tech et de lâIA (Challenges, 06/2024)\n\nWhat is science? Tech heavyweights brawl over definition (Nature, 05/2024)\n\nAI pioneer LeCun to next-gen AI builders: Don't focus on LLMs (VentureBeat, 05/2024)\n\nElon Musk Is Feuding With âAI Godfatherâ Yann LeCun (Again)âHere's Why (Forbes, 05/2024)\n\nTIME 100 Impact Award: Yann Lecun Is Optimistic That AI Will Lead to a Better World (TIME, 02/2024)\n\nYann LeCun, chief AI scientist at Meta: Human-level artificial intelligence is going to take a long time (El PAis, 01/2024)\n\nHow Not to Be Stupid About AI, With Yann LeCun (Wired Magazine, 12/2023)\n\nYann Le Cun, directeur Ã Meta : Â« LâidÃ©e mÃªme de vouloir ralentir la recherche sur lâIA sâapparente Ã un nouvel obscurantisme Â» (Le Monde, 04/2024)\n\nFacebook AI chief Yann LeCun is stepping aside to take on dedicated research role (The Verge 01/2018)\n\nPortrait: Yann LeCun, le temps des machines (LibÃ©ration, 09/2015)\n\nYann LeCun : Â«âLâintelligence artificielle reste un dÃ©fi scientifiqueâÂ» (Les Echos, 06/2015)\n\nEye robot (The Economist, 10/2010)\n\n[stuff below this line is badly out of date]\n\nQuick Links\n\nCenter for Data Science, and the NYU Data Science Portal.\n\nComputational and Biological Learning Lab, my research group at the Courant Institute, NYU.\n\nCILVR Lab: Computational Intelligence, Vision Robotics Lab: a lab with many NYU faculty, students and postdocs working on AI, ML and applications thereof such as computer Vision, NLP, robotics, and healthcare.\n\nResearch: descriptions of my projects and contributions, past and present.\n\nPublications: (almost) all of my publications, available in PDF and DjVu formats.\n\nGoogle Scholar Profile: all my publications with number of citations, harvested by Google.\n\nPreprints on ArXiv.org: where you will find our latest results, before they may receive a stamp of approval.\n\nComputational and Biological Learning Lab\n\nMy lab at the Courant Institute of New york University is called the Computational and Biological Learning Lab.\n\nSee research projects descriptions, lab member pages, events, demos, datasets...\n\nWe are working on a class of learning systems called Energy-Based Models, and Deep Belief Networks. We are also working on convolutional nets for visual recognition , and a type of graphical models known as factor graphs.\n\nWe have projects in computer vision, object detection, object recognition, mobile robotics, bio-informatics, biological image analysis, medical signal processing, signal processing, and financial prediction,....\n\nTeaching\n\nJump to my course page at NYU, and see course descriptions, slides, course material...\n\nTalks and Tutorials\n\nSee, watch and hear talks and tutorial.\n\nDeep Learning\n\nAnimals and humans can learn to see, perceive, act, and communicate with an efficiency that no Machine Learning method can approach. The brains of humans and animals are \"deep\", in the sense that each action is the result of a long chain of synaptic communications (many layers of processing). We are currently researching efficient learning algorithms for such \"deep architectures\". We are currently concentrating on unsupervised learning algorithms that can be used to produce deep hierarchies of features for visual recognition. We surmise that understanding deep learning will not only enable us to build more intelligent machines, but will also help us understand human intelligence and the mechanisms of human learning.\n\nMORE INFORMATION >>>>>.\n\nRelational Regression\n\nWe are developing a new type of relational graphical models that can be applied to \"structured regression problem\". A prime example of structured regression problem is the prediction of house prices. The price of a house depends not only on the characteristics of the house, but also of the prices of similar houses in the neighborhood, or perhaps on hidden features of the neighborhood that influence them. Our relational regression model infers a hidden \"desirability sruface\" from which house prices are predicted.\n\nMORE INFORMATION >>>>>.\n\nMobile Robotics\n\nThe purpose of the LAGR project, funded by the US government, is to design vision and learning algorithms to allow mobile robots to navigate in complex outdoors environment solely from camera input.\n\nMy Lab, collaboration with Net-Scale Technologies is one of 8 participants in the program (Applied Perception Inc., Georgia Tech, JPL, NIST, NYU/Net-Scale, SRI, U. Penn, Stanford).\n\nEach LAGR team received identical copies of the LAGR robot, built be the CMU/NREC.\n\nThe government periodically runs competitions between the teams. The software from each team is loaded and run by the goverment team on their robot.\n\nThe robot is given the GPS coordinates of a goal to which it must drive as fast as possible. The terrain is unknown in advance. The robot is run three times through the test course.\n\nThe software can use the knowledge acquired during the early runs to improve the performance on the latter runs.\n\nCLICK HERE FOR MORE INFORMATION, VIDEOS, PICTURES >>>>>.\n\nPrior to the LAGR project, we worked on the DAVE project, an attempt to train a small mobile robot to drive autonomously in off-road environments by looking over the shoulder of a human operator.\n\nCLICK HERE FOR INFORMATION ON THE DAVE PROJECT >>>>>.\n\nEnergy-Based Models\n\nEnergy-Based Models (EBMs) capture dependencies between variables by associating a scalar energy to each configuration of the variables. Inference consists in clamping the value of observed variables and finding configurations of the remaining variables that minimize the energy. Learning consists in finding an energy function in which observed configurations of the variables are given lower energies than unobserved ones. The EBM approach provides a common theoretical framework for many learning models, including traditional discriminative and generative approaches, as well as graph-transformer networks, conditional random fields, maximum margin Markov networks, and several manifold learning methods.\n\nProbabilistic models must be properly normalized, which sometimes requires evaluating intractable integrals over the space of all possible variable configurations. Since EBMs have no requirement for proper normalization, this problem is naturally circumvented. EBMs can be viewed as a form of non-probabilistic factor graphs, and they provide considerably more flexibility in the design of architectures and training criteria than probabilistic approaches.\n\nCLICK HERE FOR MORE INFORMATION, PICTURES, PAPERS >>>>>.\n\nInvariant Object Recognition\n\nThe recognition of generic object categories with invariance to pose, lighting, diverse backgrounds, and the presence of clutter is one of the major challenges of Computer Vision.\n\nI am developing learning systems that can recognize generic object purely from their shape, independently of pose and lighting.\n\nSee\n\nThe NORB dataset for generic object recognition is available for download.\n\nCLICK HERE FOR MORE INFORMATION, PICTURES, PAPERS >>>>>.\n\nLush: A Programming Language for Research\n\nTired of Matlab? Lush is an easy-to-learn, open-source object-oriented programming language designed for researchers, experimenters, and engineers working in large-scale numerical and graphic applications.\n\nLush combines three languages in one: a very simple to use, loosely-typed interpreted language, a strongly-typed compiled language with the same syntax, and the C language, which can be freely mixed with the other languages within a single source file, and even within a single function.\n\nLush has a library of over 14,000 functions and classes, some of which are simple interfaces to popular libraries: vector/matrix/tensor algebra, linear algebra (LAPACK, BLAS), numerical function (GSL), 2D and 3D graphics (X, SDL, OpenGL, OpenRM, PostScipt), image processing, computer vision (OpenCV), machine learning (gblearning, Torch), regular expressions, audio processing (ALSA), and video grabbing (Video4linux).\n\nIf you do research and development in signal processing, image processing, machine learning, computer vision, bio-informatics, data mining, statistics, or artificial intelligence, and feel limited by Matlab and other existing tools, Lush is for you. If you want a simple environment to experiment with graphics, video, and sound, Lush is for you. Lush is Free Software (GPL) and runs under GNU/Linux, Solaris, and Irix.\n\nVISIT THE LUSH HOME PAGE >>>>\n\nDjVu: The Document Format for Digital Libraries\n\nMy main research topic until I left AT&T was the DjVu project. DjVu is a document format, a set of compression methods and a software platform for distributing scanned and digitally produced documents on the Web. DjVu image files of scanned documents are typically 3-8 times smaller than PDF or TIFF-groupIV for bitonal and 5-10 times smaller than PDF or JPEG for color (at 300 DPI). DjVu versions of digitally produced documents are more compact and render much faster than the PDF or PostScript versions.\n\nHundreds of websites around the world are using DjVu for Web-based and CDROM-based document repositories and digital libraries.\n\nYann's DjVu page: a description of DjVu, and a set of useful links.\n\nTechnical talk on DjVu: watch a streaming video of Yann's Distinguished Lecture at the University of Illinois at Urbana-Champaign, October 22 2001. (100K Windows Streaming Media). (56K Windows Streaming Media),\n\nDjVuZone.org: samples, demos, technical information, papers, and tutorials on DjVu.... DjVuZone hosts several digital libraries, including NIPS Online.\n\nDjVuLibre for Unix: free/open-source browser plug-ins, viewers, utilites, and libraries for Unix.\n\nCommercial DjVu Software: free plug-ins for Windows and Mac, free and commercial applications for Windows and some Unix platforms (hosted at LizardTech, the company that distributes and supports DjVu under license from AT&T).\n\nAny2DjVu and Bib2Web: Upload your documents and get them converted to DjVu. Bib2Web automates the creation of publication pages for researchers.\n\nLearning and Visual Perception\n\nMy main research interest is machine learning, particularly how it applies to perception, and more particularly to visual perception.\n\nI am currently working on two architectures for gradient-based perceptual learning: graph transformer networks and convolutional networks.\n\nConvolutional Nets are a special kind of neural net architecture designed to recognize images directly from pixel data. Convolutional Nets can be trained to detect, segment and recognize objects with excellent robustness to noise, and variations of position, scale, angle, and shape.\n\nHave a look at the animated demonstrations of LeNet-5, a Convolutional Nets trained to recognize handwritten digit strings.\n\nConvolutional nets and graph transformer networks are embedded in several high speed scanners used by banks to read checks. A system I helped develop reads an estimated 10 percent of all the checks written in the US.\n\nCheck out this page, and/or read this paper to learn more about Convolutional Nets and graph transformer networks.\n\nMNIST Handwritten Digit Database\n\nThe MNIST database contains 60,000 training samples and 10,000 test samples of size-normalized handwritten digits. This database was derived from the original NIST databases.\n\nMNIST is widely used by researchers as a benchmark for testing pattern recognition methods, and by students for class projects in pattern recognition, machine learning, and statistics.\n\nMusic and Hobbies\n\nI have several interests beside my family (my wife and three sons) and my research:\n\nPlaying Music: particularly Jazz, Renaissance and Baroque music. A few MP3 and MIDI files of Renaissance music are available here.\n\nBuilding and flying miniature flying contraptions: preferably battery powered, radio controled, and unconventional in their design.\n\nBuilding robots: particularly Lego robots (before the days of the Lego Mindstorms)\n\nHacking various computing equipment: I have owned 5 computers between 1978 and 1992: SYM-1, OSI C2-4P, Commodore 64, Amiga 1000, Amiga 4000. then I lost interest in personal computing when the only thing you could get was a boring Wintel box. Then, Linux appeared and I came back to life.....\n\nSailing: I own two sport catamarans, a Nacra 5.8 and a Prindle 19. I also sail and race larger boats with friends.\n\nGraphic Design: I designed the DjVu logo and much of the AT&T DjVu web site.\n\nReading European comics. Comics in certain European countries (France, Belgium, Italy, Spain) are considered a true art form (\"le 8-ieme art\"), and not just a business with products targeted at teenagers like on this side of the pond. Although I don't have a shred of evidence to support it, I claim to have the largest private collection of French-language comics in the Eastern US.\n\nmaking bad puns in French, but I don't have much of an audience this side of the pond.\n\nSipping wine, particularly red, particularly French, particularly Bordeaux, particularly Saint-Julien.\n\nBib2Web: Automatic Creation of Publication Pages\n\nNo deep science here, but if you are looking for a simple/automatic way to make all your publications (digital or paper-based) available on your web page, visit Bib2Web.\n\nPhotos Galleries\n\nPhotos taken at various conferences, workshops, trade shows and other professional events. Includes pictures from CVPR, NIPS, Learning@Snowbird, ICDAR, CIFED, etc.\n\nA photo and movie gallery of various radio-controled airplanes, other miniature flying objects, lego robots, and other techno toys. Check out also my model airplane page.\n\nMiscellaneous artsy and nature picture, including garden-variety wild animals, landscapes, etc.\n\nVintage airplanes at the national air and space museum in Le Bourget, near Paris.\n\nFun Stuff\n\nNo, Yann is NOT Philippe Kahn's evil brother\n\nYour Name can't possibly be pronounced that way: or how a Nobel prize winner tried to tell me how to pronounce my own name.\n\nWho is Tex Avery anyway?\n\nSteep Learning Curves and other erroneous metaphores\n\nVladimir Vapnik meets the video game sub-culture\n\nCheap Philosophy (42 cents)\n\nA Mathematical Theory of Empty Disclaimers\n\nThe Axis of Rivals\n\nPrevious Life\n\nMy former group at AT&T (the Image Processing Research Department) and its ancestor (Larry Jackel's Adaptive Systems Research Department) made numerous contributions to Machine Learning, Image Compression, Pattern Recognition, Synthetic Persons (talking heads), and Neural-Net Hardware. Specific contributions not mentioned elsewhere on this site include the ever so popular Support Vector Machine, the PlayMail and Virt2Elle synthetic talking heads, the Net32K and ANNA neural net chips, and many others. Visit my former group's home page for more details.\n\nLinks\n\nLinks to interesting places on the web, friends' home pages, etc ."
    }
}
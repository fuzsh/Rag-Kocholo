A collaboration between the Center for Information Technology Policy at Princeton University, the Center for Information, Technology, and Public Life at UNC Chapel Hill, and the Sanford School of Public Policy at Duke University.

Download the report here!

Executive Summary:

A range of approaches to child online safety legislation (COSL) is being proposed, debated, or implemented at both the federal and state level in the United States. While the specifics of these bills differ, they coalesce around concerns regarding the effects of social media on young people. This document:

Explains these concerns, why they have surfaced now, and how COSL purports to solve them.

Outlines major international, US federal and state legislative efforts, particularly the Kids Online Safety Act (KOSA).

Summarizes the primary frames by which COSL is justifiedâmental health, sexual exploitation and abuse, eating disorders and self-harm, and social media addictionâand evaluates the evidence for each.

Outlines concerns that researchers, activists, and technologists have with these bills: age verification, privacy and surveillance, First Amendment rights, and expansion of parental control over young peopleâs rights and autonomy.

While the impetus for this legislation is well-meaning, we question the assumptions behind it. Mental health and well-being is complicated, and tied to many different social and contextual factors. Solutions that exclusively focus on technology address only a very small part of this picture. The granular debate over the evidence linking smartphones and social media to youth well-being distracts us from the real difficulties faced by young people.Â Â

COSL poses enormous potential risks to privacy and free expression, and will limit youth access to social connections and important community resources while doing little to improve the mental health of vulnerable teenagers. Ultimately, legislation like KOSA is an attempt to regulate the technology industry when other efforts have failed, using moral panic and for-the-children rhetoric to rapidly pass poorly-formulated legislation.Â

We strongly believe that reform of social platforms and regulation of technology is needed. We need comprehensive privacy legislation, limits on data collection, interoperability, more granular individual and parental guidance tools, and advertising regulation, among other changes. Offline, young people need spaces to socialize without adults, better mental health care, and funding for parks, libraries, and extracurriculars. But rather than focusing on such solutions, KOSA and similar state bills empower parents rather than young people, do little to curb the worst abuses of technology corporations, and enable an expansion of the rhetoric that is currently used to ban books, eliminate diversity efforts in education, and limit gender affirming and reproductive care. They will eliminate important sources of information for vulnerable teenagers and wipe out anonymity on the social web. While we recognize the regulatory impulse, the forms of child safety legislation currently circulating will not solve the problems they claim to remedy.

TABLE OF CONTENTS

Introduction

Why Act Now?Â

The Techlash and 2024

Legislative Landscape

International Legislation

KOSA

State Legislation

Controlling Minor Access to Social Media

State Child Privacy Bills

State Anti-Pornography Bills

Arguments in Support of COSL

Mental Health

Sexual Exploitation and Abuse

Eating Disorders and Self-Harm

Social Media Addiction

Arguments Against COSLÂ

Age Verification

Privacy and Surveillance

Parental Control

Chilling Effects on Information Access and Free Expression

Weaponization of Duty of Care

First Amendment Concerns

Recommendations

Conclusions

Appendix

Authors and Acknowledgements

Works Cited

1.0 Introduction

Public concerns about the harm of social media to young people have recently accelerated. While worries about youth and technology are nothing new, documents revealed by whistleblower Frances Haugen suggested that social platforms have evidence that their products negatively affected the mental health and well-being of teenage users. At the same time, policymakers have passed the Protecting American From Foreign Adversary Controlled Applications Act (PAFACA, 2024), citing national security concerns about TikTok. Claims that synthesize these concernsâfor example, mental health and the manipulation of teenagers by foreign interestsâconstitute a powerful rhetoric that is being mobilized by both Democrats and Republicans, ostensibly in the pursuit of âholding tech accountable.â

This has coalesced into a will to act politically. The Kids Online Safety Act (KOSA) has been introduced in the U.S. Senate and placed onto the legislative calendar. With 68 senators signed on, KOSA appears to have enough support to pass through the Senate if brought to the floor for a vote. A companion bill was introduced into the House of Representatives on April 9, 2024. Congressional committees have held 42 hearings on the topic of social media and youth since 2017, and legislation has been proposed in the US, UK, France, and 35 US states, including California, Utah, and Maryland. The debate over these reforms takes for granted that social media harms young people, that this is supported by empirical evidence, and that the reforms proposed by policymakers will have a material, positive impact on young peopleâs mental health and well-being. However, these issues are far from settled.

Although several experts repeatedly appear in public discussions about youth safety, the research mobilized both in support of the claims about social media and the techno-legal âsolutionsâ being offered are murkier than they appear. It is, unfortunately, very true that American youth are undergoing a mental health crisis. What is less clear is that social media is the cause. The timeframe of this crisis coincides with great social uncertainty, a rise in gun violence, felt impacts of climate change, a global pandemic, the popularity of helicopter parenting, and rising income inequality, all of which negatively affect mental health. There is evidence that teens suffering from mental health problems often retreat into social media (as do adults), which may explain these correlations. But despite high-profile claims to the contrary, research does not show that social media causes those conditions. Moreover, the proposed reforms to social media will not significantly improve young peopleâs mental health and will likely have negative consequences, not only for marginalized and minoritized teens, but for internet users as a whole.

This primer outlines the current state of Child Online Safety Legislation (COSL) at both the federal and state levels, explains the reasons behind current legislative action, and critically evaluates the evidence presented in support. We suggest that other explanations are possible for the childrenâs mental health crisis and that the proposed reforms will have unintended consequences that will actually harm young people, especially the most vulnerable.

We urge academics, activists, and policymakers to act against this legislation, not because we support âBig Tech,â but because we are concerned about young people. While many of the players in this landscape mean well, we are concerned with the long-term implications of pushing through legislation based on moral panics and unproven causal claims.

2.0 Why Act Now?Â

There have been concerns over young peopleâs consumption of media as long as media has existed. Plato fretted about the effect of writing on youth, while Victorians warned that âPenny Dreadfulâ novels would lead young people to crime. Parents were concerned in the 1940s that radio dramas were addictive, in the 1950s that horror comics created juvenile delinquents, in the 1980s that heavy metal turned children into devil-worshiping murderers, and in the 1990s that violent video games led to school shootings. In retrospect, many of these worries seem silly, such as hand-wringing over the popularity of jazz, swing dancing, or rock and roll, but this has not lessened their regularity nor the heightened rhetoric that accompanies them.

This is especially true for digital technology. In general, child online safety legislation past and present is fueled by concerns over the risks children may encounter online, often referred to as âcontent, conduct, and contact.â Content refers to exposure to online material such as violent imagery, extremism, sexually explicit content, or information that might spur harmful behavior like self-harm or eating disorders. Conduct involves childrenâs online behavior that could harm themselves or others, such as cyberbullying, sexting, piracy, or sharing personal information. Contact includes interactions with other people that might pose a risk to children, such as communicating with online predators or peers engaging in harmful interactions, or being manipulated into illegal activities or radical behavior.

Lawmakers have repeatedly proposed or passed legislation to limit young peopleâs online risks, using different legislative theories. Such legislation includes the Communications Decency Act (CDA, 1996), the Child Online Protection Act (COPA, 1998), the Childrenâs Online Privacy Protection Act (COPPA, 1998), the Childrenâs Internet Protection Act (CIPA, 2000), and the Deleting Online Predators Act (DOPA, 2006). All of these efforts were met with lobbying and litigation. Both the CDA and COPA had significant portions struck down on First Amendment grounds; CIPA, which requires schools and libraries to implement internet filters, was declared by the Supreme Court to be constitutional under certain conditions; and DOPA was never passed. Only COPPA was enacted in full, although with revisions that dropped the age from the originally proposed 18 to 13.

All these bills were fueled by moral panics. British sociologist Stanley Cohen defined a âperiod of moral panicâ as one in which something, be it a person, a technology, a media object, or a group of people, âemerges to become defined as a threat to societal values and interestsâ disproportionate to its actual significance. Moral panics around youth and media, especially digital media, are extremely common. The first round of moral panics over online âcontent, conduct, and contentâ in the late 1990s (cyberporn, piracy, and the CDA) and the second in the mid-2000s (online predators, cyberbullying, and DOPA) involved overblown threats fueled by media hysteria, misinterpreted research, and inaccurate statistics. Unfortunately, moral panics can also prompt hastily-written legislation, the incarceration of innocents, and increased surveillance of young people.

The current state of concerns about youth and mental health exemplifies a technological panic, which Dr. Amy Orben, who runs the Digital Mental Health Group at the University of Cambridge, defines as a time âin which the general population is gripped by intense worry and concern about a certain technology.â We believe the current slate of concerns around youth and social media meet the criteria for a moral panic, which is not a solid basis for legislation.

2.1 The Techlash and 2024

Since 2016, the technology industry has been public enemy number one. Companies like Meta, Google, and Amazon have been criticizedârightlyâfor spreading disinformation and hateful speech, invading user privacy, selling personal data, pushing sensational and harmful content, concentrating wealth, and contributing to environmental degradation. Tech workers, from Uber drivers to Amazon warehouse workers, have organized and demanded improvements in labor conditions, while white collar tech workers have protested the development of intrusive and unethical technologies. Positive perception of technology has drastically decreased, with the Pew Research Center finding a 21% drop from 2015 to 2019; by 2024, 44% of Americans believed that technology companies had a negative effect on the United States.

These criticisms have led to repeated calls for greater regulation of technology companies, such as the need for comprehensive data privacy legislation, the suggestion that anti-trust action be taken against large companies like Google and Meta, and the desire to reform Section 230 to hold platforms accountable for some types of content. This has resulted in some reforms. For example, California passed the California Consumer Privacy Act (2018); Congress passed the Fight Online Sex Trafficking Act (FOSTA/SESTA) which amended Section 230 to make platforms liable for assisting, facilitating, or supporting sex trafficking (2018); the Justice Department brought antitrust cases against Google (2020, 2023) and the FTC against Meta (2020); and policymakers in the European Union enacted the General Data Protection Regulation (GDPR, 2018) and the Digital Services Act (DSA, 2022). While Congress has held many hearings about âBig Techâ and proposed a vast array of bills, the United States government has been unsuccessful in passing legislation at the federal level.

There is clearly a need for greater regulation of technology companies and their products. However, this regulation should be well-thought out, as internet regulation often has unforeseen consequences, especially on marginalized people. FOSTA and SESTA, which were rapidly pushed through in response to fears of âchild trafficking,â have had minimal to no impact on human trafficking while making sex workers more vulnerable to violence, less able to screen clients, and less financially stable. Both bills portrayed a very complicated social issueâsex traffickingâas a technological problem that could be solved through changing online classified advertising. As Harvard Cyberlaw Clinic instructor Kendra Albert writes,

âThe failure of technology policy advocates to understand or engage with underlying substantive debates over sex work hamstrung advocacy efforts and led to a failure to build meaningful coalitions both prior to FOSTAâs passage as well as after.â

We are concerned that the current spate of legislative efforts to regulate internet content with the goal of improving childrenâs mental health and well-being are being hastily formulated without deep engagement with research and advocacy and will have similar negative outcomes on vulnerable populations.

Protecting young people is an evergreen concern that is hard to argue against, and the rhetoric of children as innocent and deserving of protection is extremely powerful. Internet regulation âfor the childrenâ has had some success in the United States, fueling CIPA, the CDA, COPA, COPPA, and DOPA. For these reasons, passing legislation around young peopleâs internet practices is more politically palatable than more comprehensive regulation; that does not mean it should be done.

Recent efforts, like KOSA and its ilk, are partly fueled by broad desires to âhold Big Tech accountableâ and curb âharmful content online.â In a very polarized America, these are rare bipartisan concerns. President Joe Biden wrote in a 2023 editorial in the Wall Street Journal:

I urge Democrats and Republicans to come together to pass strong bipartisan legislation to hold Big Tech accountable. The risks Big Tech poses for ordinary Americans are clear. Big Tech companies collect huge amounts of data on the things we buy, on the websites we visit, on the places we go and, most troubling of all, on our children. As I said last year in my State of the Union address, millions of young people are struggling with bullying, violence, trauma and mental health. We must hold social-media companies accountable for the experiment they are running on our children for profit.

Biden also mentions data privacy, the need to protect small businesses, âtoxic online echo chambers,â and âcyberstalking, child sexual exploitation, nonconsensual pornography, and sales of dangerous drugs.â On the other side of the aisle, Republicans are concerned with tech companiesâ supposed liberal bias and have labeled anti-disinformation efforts âcensorship,â while both parties are concerned about Chinaâs global influence and Chinese apps like TikTok, which has been banned for government workers federally and in two dozen states in addition to PAFACA. As a result, bills like KOSAâwhich provide some measure of platform regulation after other options have failed, and promise to decrease young peopleâs exposure to harmful contentâhave broad, bipartisan support.

3.0 The Legislative LandscapeÂ

In this section, we outline the primary COSL efforts internationally, at the US federal level, and at the US state level. While this is not comprehensive, we focus here on legislation that has passed or appears likely to pass.

3.1 International Legislation

Prior to 2022, international legislation focused on protecting childrenâs data online. This was championed by the United Kingdomâs Age-Appropriate Design Code passed in 2021 and inspired by the European Unionâs preeminent data privacy lawâthe GDPR. However, multiple countries, including France, Germany, Australia, Canada, India, South Korea, and Japan, have recently enacted national-level laws regulating online child safety through direct restrictions on design and content. The regulatory features of these new regulations include age verification to prevent exposure to harmful content; protections for childrenâs data privacy; limits on internet usage; and requirements for parental consent. The Digital Services Act (DSA), passed in 2022 by the European Union, is a landmark regulation that applies to a sizable number of countries. In effect as of January 1, 2024, the DSA seeks to broadly regulate illegal and harmful online content and disinformation. DSA specifically protects children by requiring internet intermediaries to prevent exposure to age-inappropriate content; control and verify access to information by children; and install child-friendly grievance reporting channels.

Following Europeâs lead, the United Kingdom passed the Online Safety Act in 2023, setting up a wide range of legal obligations on internet platform services. The UK Online Safety Act places duties of care on regulated user-to-user and search services to identify, mitigate, and manage the risks of harm from illegal as well as âcontent and activity that is harmful to children.â To outline these duties, the Act designates the UKâs Office of Communications (OFCOM), the government agency that regulates broadcasting and telecommunications, as regulator. The text of the Act also requires internet platform services to be âsafe by designââdesigned and operated to maintain a âhigher standard of protectionâ for children than adults. Specifically, the Act requires platforms not only to remove illegal content, but legal content considered to be harmful for children, such as content containing themes of self-harm. Thus, services that host âage-inappropriateâ content must implement age verification measures to ensure that only adults can access content containing pornography, self-harm, and bullying, among others. Finally, platforms must conduct and publish âchildrenâs risk assessmentsâ regularly. The UKâs Online Safety Act, therefore, is one of the more detailed pieces of legislation creating a duty of care to protect children on the internet.

3.2 Kids Online Safety Act (KOSA)Â

In the United States, the Kids Online Safety Act (KOSA / S.1409) is a major federal bill and possibly one of the most significant attempts by the government to rein in the technology industry. KOSA was initially introduced in the Senate by Sen. Richard Blumenthal (D-CT) and Sen. Marsha Blackburn (R-TN) on February 16, 2022. Following its reintroduction in May 2023, the bill gained bipartisan support in a polarized Congress. As of April 2024, ââKOSA has been unanimously approved by the U.S. Senate Committee on Commerce, Science, and Transportation and placed on the Senate legislative calendar. On April 9, 2024, Reps. Gus Bilirakis (R-FL), Kathy Castor (D-FL), Erin Houchin (R-IN), and Kim Schrier (D-WA) introduced a version of KOSA into the House. KOSA purports to set out requirements to protect minors from online harms, and is applicable to âcovered platformsââdefined in the bill as âapplications or services (e.g. social networks) that connect to the internet and are likely to be used by minors.â Since its introduction, the language of the bill has undergone several amendments.

According to the latest amended version of the Senate bill, unveiled by its authors in February 2024, its main areas of intervention are a duty of care, the establishment of a set of safeguards for minors, parental tools, and identifying which users are youth.

First, KOSA establishes a âduty of careâ for internet platforms to âexercise reasonable care in the creation and implementation of any design feature to prevent and mitigateâ¦â a variety of listed harms to minors such as mental health disorders; online sexual exploitation; patterns of internet addiction; online bullying and harassment; and predatory, unfair or deceptive marketing practices, among others. However, the duty of care does not require platforms to prevent a minor from deliberately or independently seeking or requesting content.

Second, KOSA requires platforms to provide any user that it âknows is a minorâ with readily-accessible and easy-to-use safeguardsâwhich should be part of the default setting for minorsâto limit the ability of others to communicate with them, restrict public access to their personal data, control personalized recommendation systems, and limit âdesign features that result in compulsive usage of the covered platform by the minor.â According to the bill, âdesign featuresâ are âany feature or component of a covered platform that will encourage or increase the frequency, time spent, or activity of minors on the covered platform, or activity of minors on the covered platform.â The text goes on to specify examples of design features, such as infinite scrolling or auto play; rewards for time spent online; notifications; personalized recommendation systems; in-game purchases; and appearance altering filters.Â

Third, KOSA gives parents more control by mandating that platforms provide specific tools. These tools allow parents to manage their child's privacy and account settings, see how much time their child spends on a platform, and limit their usage time. These requirements are enhanced by the obligation for platforms to clearly disclose their terms of service, explain their use of personalized recommendation systems, and provide information about advertised products and services. Additionally, KOSA requires platforms to regularly monitor and annually publish transparency reports detailing the risks to minors online and the steps taken to mitigate these harms.

Finally, regarding how platforms determine if users are minors, KOSA specifies that it does not require the collection of new data, such as government-issued IDs, nor does it require the implementation of age verification systems. Instead, it directs a task force, including representatives from the National Institute of Standards and Technology (NIST), the Federal Communications Commission (FCC), the Federal Trade Commission (FTC), and the Secretary of Commerce, to explore technically feasible age verification methods through research. This task force study will look into the potential advantages of device and operating system-level age verification, the information required for verifying age, the accuracy of these systems, and ways to protect user privacy and data security, among other issues.

Furthermore, the bill's language makes it clear that platforms must implement safeguards or notifications depending on their awareness of whether a user is a minor. KOSA defines âknowâ or "knowsâ as having âactual knowledge or knowledge fairly implied on the basis of objective circumstances.â Most platforms ask users their age when they create an account, but do not require proof of this information with government-issued identification of any kind. In fact, civil society groups like the American Civil Liberties Union (ACLU) have clarified that to âknowâ the age of a minor will require age verification. This process could also inadvertently compel adults to provide official ID to confirm they are not minors. The ACLU and other First Amendment advocates have argued that age verification threatens free speech, privacy, online anonymity, and data security.

The House version is slightly different in that the âduty of careâ and âknowledgeâ obligations are tiered, similar to the DSA. Social media, messaging, streaming, and multiplayer gaming platforms are divided into âhigh impact platformsâ (those with more than 150m users and $2.5 billion in yearly revenue), second-tier âcovered platformsâ (with annual revenue of $200K and 200K users), and third-tier platforms with lesser thresholds for revenue and users. âHigh impactâ platforms have the duty of care, while âcovered platformsâ do not but are still required to implement safeguards. âKnowledgeâ that a user is a minor is also defined differently based on tier. High impact platforms âknew or should have known the individual was a child or minor,â second-tier platforms â âknew or acted in willful disregard of the fact that the individual was a child or minor,â and third-tier platforms must have âactual knowledge.â The House bill also replaces references to âaddictionâ with âcompulsive usageâ (see Section 4.4), which is defined as âany response stimulated by external factors that causes an individual to engage in repetitive behavior reasonably likely to cause a mental health disorder.â There are also minor changes to âdesign featuresâ including push notifications or badges that encourage engagement, while âdeceptive marketing practices, or other financial harmsâ have been removed. However, the bill is in an early draft and is likely to continue to change as it works its way through the House.

In its previous versions, KOSA enabled state attorneys general to take legal actions against platforms due to a violation of the duty of care. However, due to criticism from the LGBTQ+ community on how a broadly defined duty of care would encourage greater online censorship of vulnerable communities (including LGBTQ+ youth), the amended version of the bill limits the power to enforce the duty of care to the FTC. Therefore, failing to meet the duty of care is no longer grounds for civil actions by state attorneys general, although breaches of other provisions still qualifyâsuch as those related to safeguards for minors, disclosure, and transparency.

The latest Senate version of KOSA (February 2024) has been met with increased bipartisan support. Still led by Senators Blumenthal (D-CT) and Blackburn (R-TN), and joined by Sen. Cassidy (R-LA), KOSA is currently co-sponsored by 68 senators, creating a path for KOSA to successfully pass through the Senate, and a version was introduced into the House in April 2024. Beyond Capitol Hill, KOSA is backed by a wide coalition of civil society and industry supporters. This coalition includes Common Sense Media, the American Psychological Association, Parents for Safe Online Spaces, the NAACP, Christian Camp and Conference Association, Microsoft, and Snap, among others.

Yet, several LGBTQ+ and civil liberties organizations still consider KOSA to be dangerous. For the Electronic Frontier Foundation (EFF), âKOSA is still a censorship bill and it will still harm a large number of minors who have First Amendment rights to access lawful speech online.â Similarly, the ACLU âremains concerned that this bill would silence important conversations, limit minorsâ access to potentially vital resources, and violate the First Amendment by imposing a roundabout government-mandated content moderation rule.â Finally, the digital rights group Fight for the Future keeps asking KOSAâs sponsors to clarify that the duty of care only applies in a content-neutral manner.

KOSA has been deeply influenced by the UK Online Safety Act (discussed in section 3.1), which passed in October 2023. In contrast to KOSA, UKâs OSA identified a larger subset of harms which required platform regulation, including opioid markets, content uploaded from prisons, terrorist propaganda, disinformation, manipulation by AI, and even abuse of public figures like athletes and female journalists. However, common to both bills is the âduty of careâ provision. But while both countries have a tradition of âduty of careâ in their tort law, the UK has a different legal framework governing free speech than the First Amendment. In the case of the OSA, âduty of careâ was suggested by Lorna Woods and William Perrin. In their iteration, Woods and Perrin derive the duty of care from the UKâs 1974 âHealth and Safety at Work Actâ by asserting that digital platforms operate similarly to quasi-public places such as officers, bars, and theme parks. This means that platforms are responsible to prevent harm in the same sense that theme parks are responsible to prevent injuries on their premises. KOSA lacks a clear reference to any sort of physical corollary, which creates ambiguity in the scope of its application, and its implications for free speech. The OSA reliance upon analogy between software design and public spaces falls flat when design choices can reflect developersâ free expression, an important distinction considering different legal standards for free speech.

3.3 State Legislation

Though KOSA has a significant amount of bipartisan support, policy analysts Scott Brennen and Matt Perault note that we are much more likely to see tech legislation passed at the state level, rather than federally. As Brennen and Perault argue, states are âpoised to become critical battlegrounds for tech policy in the next two yearsâ because of structural advantages at the state level (such as one party controlling all levels of government) that make passing legislation much easier. At the state level, we can see the implications of KOSA-like laws, how they may be used to achieve the political aims of lawmakers, their viability, and the role that age verification in particular is going to play in the future of the internet. This means that regardless of whether KOSA passes and what exists in the final version of the bill, state laws are likely to have a great impact on young peopleâs internet use.

A majority of states have proposed or passed KOSA-like legislation aimed at childrenâs use of social media, or other age verification laws. According to the National Conference of State Legislatures, 35 states and Puerto Rico addressed legislation introducing measures to protect children while using the internet, with 12 states passing bills and resolutions. This number does not include other legislation that will also require age verification, but aims to curtail adult access to information, specifically pornography.

Proposed and passed state laws aimed at distinguishing users by age can generally be separated into three broad categories, discussed below.

3.3.1 Controlling Minor Access to Social Media

The first set of laws are KOSA-like legislation passed by states like Texas, Utah, and Arkansas that are aimed at controlling broad access to information online through age verification, going beyond the restrictions placed on accessing pornography. These laws make up the first set of child online safety legislation. Arkansasâ State Bill (SB) 396 prohibits minors from creating online accounts unless they have explicit parental consent and the platform has verified their age using third party vendors. Texasâ House Bill (HB) 18 will require platforms to obtain parental consent verified by government-issued IDs in order to have new minors join as users. Utahâs SB 152 originally prohibited age-verified minors from opening an account without parental consent, until the tech industry group NetChoice sued to block the legislation on First Amendment grounds. SB 152 was amended by SB 194 and HB 464. SB 194 also institutes age verification, but only enables maximum privacy settings on a minorâs account if they lack parental consent. HB 494 allows parents and minors to sue platforms for negative effects on a minorâs mental health.

Notably, all three states have different interpretations on how a minor can be verified and permitted to open a social platform account, but all maintain that data on minors used for verification cannot be retained beyond what is necessary. On the topic of parental control, Texas and Utah both require platforms to provide parents with access to their childâs online activity and other data collected on the minor. Another issue central to these bills is the harms caused to minors by deceptive online advertising. Texasâ HB 18 requires platforms to restrict advertisements for services or products that minors cannot use, and disclose in their terms of service how personally identifiable-information is integrated into recommendation algorithms. Utah, on the other hand, restricts minors from seeing any targeted advertisements and recommended content. Utah also prohibits direct messaging with specific accounts and displaying a minorâs account in search results to protect the discovery of minors online by other users.

3.3.2 State Child Privacy Bills

The second set of laws are aimed at increasing privacy protections for children. This includes legislation such as the California Age-Appropriate Design Code Act (CAADCA) , a law which would require special data safeguards for underage users, which was modeled on the UK Age Appropriate Design Code. The California law was set to go into effect in July 2024, but a federal judge granted a preliminary injunction for the tech industry group NetChoice in September 2023. Critics have argued the law violates the First Amendment by creating barriers for both minors and adults seeking access to websites and other apps. Nonetheless, California is not the only state passing laws oriented at protecting privacy for young users. Connecticut has updated a privacy law to require online platforms to conduct regular safety assessments and make design changes to limit who can contact minors, and gives minors (defined in this case as consumers under 18) the right to âunpublishâ and delete accounts. Similar laws have been proposed in Vermont and Illinois.

3.3.3 State Anti-Pornography Bills

The last set of child online safety laws attempt to use age-gating as a mechanism for restricting users under 18 from accessing online pornography. Claiming that exposure to sexual content at a young age can lead to disorders and harmful behavior, eight states passed laws in 2022 and 2023 requiring websites with more than 33.3% pornographic content to use age verification methods prior to enabling access to users. Louisianaâs Act No. 440, which has become a model for similar legislation in other states, establishes platform liability and civil remedies for distributing materials harmful to minors. Initially, Louisianaâs law was set to be enforced by private right of action. However, supplemental updates to the law have empowered state attorneys general to proactively investigate platforms, who are subject to fines for non-compliance. Harmful materials are defined as those designed to appeal to âprurient interest,â including descriptions of actual, simulated, or animated sexually explicit materials. Prurient interest is, however, an extremely broad term and may signal a future filled with debates similar to those witnessed around obscenity laws. Regardless, the Louisiana law mandates platforms to verify non-minors, either by collecting digitized identification cards or instituting commercial age verification systems. The specific methods of commercial age verification stated in the law include government-issued identification or discerning age through transactional data. Following Louisiana, seven other statesâTexas, Virginia, Arkansas, Mississippi, Utah, Montana and North Carolinaâhave enacted copycat laws and dozens of others have introduced similar bills in state legislatures.

While there is a significant degree of variation in the requirements and platforms covered by these state laws mandating age verification, they demonstrate an increase in state-level power over minorsâ online experiences. These laws give state-level governments, attorneys general, and parents increasing discretion on when and how minors access information. The threats to privacy and free speech enabled by government-mandated access to surveillance of minors is additionally alarming.

4.0 Arguments in Support of Childrenâs Online Safety Legislation (COSL)Â

Arguments in favor of COSL focus on the increasing rates of depression, loneliness, and anxiety in youth across the country and suggest that this phenomenon is due to social media and its addictive design created by âBig Tech.â These legislative efforts are often framed as regulating a tech industry that has gone rogue and no longer cares about the health of its users beyond their bottom line. In public discourse, this enables COSL to represent a broader concept of regulating technology and transforming how children interact with social media, rather than focusing on the specifics of any single bill.

4.1 Mental HealthÂ

For the last several years, public health officials, mental health professionals, and educators have warned that young people in the United States are undergoing a mental health crisis. Rates of anxiety and depression have increased significantly. Suicide rates among 10-24 year olds increased 67% from 2007 to 2021 and are at their highest rate since the 1980s. Eating disorders among both boys and girls have increased. The COVID-19 pandemic had enormously negative consequences on the mental health of children and teenagers. Moreover, extensive racial and economic disparities in treatment exist, with poor children and children of color far less likely to have access to mental health services. There is a significant gender gap, with girls and young women experiencing more anxiety and depression than their male counterparts.

Various explanations for this crisis have been proposed, including the aftereffects of the pandemic, the opioid epidemic, COVID lockdowns and school closures, family problems, domestic violence, economic uncertainty, climate change, and helicopter parenting. The gender gap may be explained by early onset puberty, economic factors, sexual violence and assault, and prevalence of sexist public discourse, but there is no clear and obvious answer.

The cause of the current crisis is contested. However, in the United States, the public, the surgeon general, and politicians have ignored this complicated sociopolitical context in favor of scapegoating social media as the cause. To support COSL, policymakers and advocates have frequently cited psychologists Jean Twenge and Jonathan Haidt to justify public intervention. The two have a variety of qualms about cell phones, the internet, and social media, claiming that such technology is addictive, causes sleep deprivation, takes time away from in-person socializing, and creates a ânarcissisticâ and âentitledâ generation of âfragileâ and âanxiousâ young people. Haidt is a moral psychologist and professor of business ethics with no scholarly expertise in social media or youth. He recently published a book, The Anxious Generation: How the Great Rewiring of Childhood Is Causing an Epidemic of Mental Illness (2024). It follows a long string of alarmist essays in outlets like The Atlantic with titles like, âThe dangerous experiment on teen girls,â âHow social media dissolved the mortar of society and made America stupid,â and âYes, social media really is undermining democracy.â Twenge, a decorated professor of psychology, has written two books pathologizing millennials, The Narcissism Epidemic: Living in the Age of Entitlement (2010) and Generation Me (2014), and two pathologizing Gen Z, iGen: Why Todayâs Super-Connected Kids are Growing up Less Rebellious, More Tolerant, Less Happyâand Completely Unprepared for Adulthood (2017) and Generations (2023). Twengeâs basic arguments paint young people as lazy, entitled, selfish, and narcissistic, while Haidt considers youth to be coddled, overprotected, and delicate.

Haidt and Twenge both switched from chronicling the deficient personality traits of young people to criticizing the social media platforms that, both claim, made the youth this way. While in earlier books, both blamed a variety of other actors including parents, consumer culture, therapists, celebrities, programs to boost self-esteem, front-facing cameras, moral relativism, schools, and so forth, they have narrowed in on social platforms and smartphones as the cause for broad supposed generational changes. The two have co-authored editorials with embellished titles like âThis is our chance to pull teenagers out of the smartphone trap,â as well as a series of articles in prestigious outlets like Nature Human Behavior. Haidt and Twenge jointly contend âsmartphones in general and social media in particularâ are to blame for a litany of negative mental health outcomes among young people, particularly girls and young women, and that no other phenomena can be considered equally responsible for youth mental health crises. Twenge and Haidt assert widespread social media use has incited an inescapable, âplanetary rewiringâ of human interaction.

Haidtâs work has been criticized by other academics as cherry-picking evidence, attributing causation to correlational data, and for furthering simplistic, unsubstantiated narratives. Most recently, while promoting The Anxious Generation, Haidt promoted the discredited theory of ârapid onset gender dysphoria,â which claims that social media is responsible for an increase in gender diversity in young people (discussed in 5.3). Twengeâs work has been repeatedly criticized for its emphasis on generational cohorts, reliance on cross-temporal meta-analysis, pathologizing of youth, and repeated conflation of correlation with causation. Her latest book states that technology is the âroot causeâ of âcultural changesâ and âgenerational differences.â She calls this âThe Technology Model of Generations.â

Media scholars have spent many years trying to understand the effects of all types of media and technology on people and society more broadly. This field of study is known as media effects. Haidt and Twengeâs causal models in which social media and smartphones cause young people to be depressed, narcissistic, and anxious (or, in Twengeâs case, to determine generational differences) is an example of a theory of media effects called technological determinismâthat technology determines society. In media and technology studies, such theories are widely considered to be inaccurate and out-of-date. Nevertheless, technological determinism has tremendous staying power because it provides easy, simplistic explanations for complicated events.

Despite its popularity in documentaries and op-eds, technological determinism is flawed because it ignores contextual factors. Rich historical accounts of the effects of technology, such as Elizabeth Eisensteinâs history of the printing press and its impacts, note that technology has particular effects due to the time and place in which it is enacted and that the same technology leveraged in a different context may have completely different effects. After the horrific 1999 Columbine shooting, a robust debate raged over whether playing violent video games contributed to school shootings. Twenty years later, academic evidence overwhelmingly finds no link between the two. Moreover, video games are immensely popular in Japan and South Korea, which have the lowest rates of violent crime in the world. As Chris Ferguson and James Ivory write, âEfforts to blame mass homicides on video games appear to be due to unfamiliarity with games among older adults, prejudicial views of young offenders, and a well-identified cycle of moral panic surrounding media as a scapegoat for social ills.â

This is not to say that technologies do not contribute to social changeâof course they do. Social platforms have enabled the development of knowledge commons, virtual community, new art and media forms, and an enormous number of new business models, as well as facilitating disinformation, networked harassment, and hateful content. However, taking a complex situation like youth mental health and attributing a single causal element is short-sighted. Rather, we advocate for an ecological approach to improving young peopleâs health and well-being in which the full spectrum of young peopleâs lives and concerns are considered.

Identifying the cause of the mental health crisis prescribes the solutions. If the youth mental health crisis is caused by social platforms and smartphones, then eliminating social platforms and smartphones would, logically, ameliorate the crisis. Putting aside for a second the impossibility of this reform, let us critically examine the supposed relationship between social platforms and mental health concerns. (No one is, at present, attempting to legislate away smartphones, although Haidt suggests that parents restrict access to cellphones until age 16.)

Fundamentally, scholarly studies show that the assertion that teenage mental health problems are caused by social media is far more complex than is implied in the media. In May 2023, the American Psychological Association released a Health Advisory on Social Media Use in Adolescence, concluding that â[u]sing social media is not inherently beneficial or harmful to young people,â and that its effects âare dependent on adolescentsâ own personal and psychological characteristics and social circumstancesâintersecting with the specific content, features, or functions that are afforded within many social media platforms.â In other words, when youth are already doing poorly, interactions online and media consumption can exacerbate mental health issues. This is well-established in other media contexts. For example, when youth are already contemplating suicide, exposure to TV shows like 13 Reasons Why or learning about a celebrityâs death by suicide can have significant effects on their mental health and, in the worst cases, spur copycat suicides.

Given these debates around causality, the National Academies of Science, Engineering, and Medicine convened a committee of experts in cognitive science, computational social science, economics, education, epidemiology, law, media science, mental health, network science, neuroscience, pediatrics, psychology, social media, and technology to assess the impact of social media on the health and wellbeing of adolescents and children. In their December 2023 report, the experts concluded, âThe committeeâs review of published literature did not support the conclusion that social media causes changes in adolescent health at the population level. The report also notes that the use of social mediaârather than having purely negative or positive impactsâis likely a constantly shifting calculus of the risky, the beneficial, and the mundane that affects different people in different ways.â

To boost their cause, Twenge and Haidt frequently mention the quantity of studies proving a link between social platform use and mental health. They maintain two lengthy Google docs, one showing a downturn in youth mental health and the other analyzing studies that draw correlations between social media and mental health. Even a detailed perusal of these studies finds that very few draw causal links between the two. In contrast, one study of 464 teens found no correlation between social media and adverse mental health consequences, with the authors concluding that âconcerns regarding social media use may be displaced.â A study of more than two thousand 10-15 year olds in North Carolina found little evidence of a negative association between digital technology use and well-being. In 2023, a very large study of Facebook adoption in 72 countries found âno evidenceâ that âsocial media is associated with widespread psychological harm.â

To turn back to critiques of technological determinism and simple media effects, mental health is complicated, and the causes are complex; isolating one particular variableâsocial mediaâis extremely simplistic. Despite hundreds of studies on the interactions between social media and youth mental health, there is no clear evidence of an association, let alone a cause. The public discourse on this has frustrated many specialists, with one paper by researchers from scholars titled, simply, âThere is no evidence that associations between adolescentsâ digital technology engagement and mental health problems have increased.â

In all the attention given to debating the binary of whether social media is good or bad, or adding up the number of studies on each side, the nuances often get lost. Many young people are doing badly and their interactions with technology can exacerbate existing struggles. But it is important to consider the particular arrangement of social context, social platforms, and economic circumstances that makes this possible. For example, anxious and lonely people, both adults and teenagers, often turn to social media for companionship. While this search for connection is understandable, social platforms are not a substitute for in-person interaction. Unfortunately, young peopleâs abilities to socialize, walk to school, and travel around their neighborhoods without adult supervision have decreased significantly in the last 30 years. This is partially due to the discourse of âstranger dangerâ that has been on the rise since the 1980s (see Section 4.2) and partly due to widespread adoption of mobile devices, which has allowed parents to surveil their children in a way that was impossible for earlier generations. Haidt himself has written about the rise of âhelicopter parentingâ and the huge change in social norms that puts enormous social pressure on parents to limit their childrenâs movements and freedom, and advocates for âincreased amounts of independent play and responsibility in the real world.â These factors are unlikely to change regardless of how social platforms are regulated.

In the debates about COSL, many people center technology instead of youth. If, instead, we center youth, they mention a broad array of issues that are increasing their anxiety. In light of increased weather events and global warming, climate anxiety is increasing both domestically and around the globe. Young people are expressing greater concern about the future, prompting them to question whether having children is wise. In the US, anxieties around school shootings and student college debt continue to mount. And since young people are often affected by the well-being of their parents, the endemic mental health struggles, divorces, job precarities, and other factors that are affecting the mental health of adults tend to have refractive effects on young people. Because so many teenagers use social media, their struggles are made very visible online in ways that are often otherwise challenging to see. When the EFF asked youth how they felt about KOSA, more than 5,000 responded, explaining how social media has enriched their life, connected them with others, and helped them find valuable information, and that they feared KOSA would curtail these valuable resources.

So what do young people need? A recent report from the Crisis Text Line, which provides support to youth struggling with mental health, analyzed more than 87,000 anonymized conversations with young people. Teens asked for âopportunities for social connection; engagement in music, writing, visual, and performing arts; mental health services; exercise and sports programming; books and audiobooks; and outdoor spaces and nature.â Unfortunately, the same report points out that funding for such programs never recovered from the 2008 financial crisis, with funding for libraries, parks, and art and music programs in schools all lower in 2024 than it was in 2010. Strong relationships with parents can improve mental health outcomes. But young people also need access to non-custodial adults who are looking out for them. They need to feel safe from gun violence in their communities. And obviously, more young people need access to mental health resources.

Ultimately, the deluge of child online safety legislation fails to address these issues. Instead, it empowers parents to further surveil and restrict their childrenâs self-expression, and there is no evidence that COSL will address the problems that Haidt and Twenge write about so prolifically. In his Substack, Haidt says, âIf you listen to the alarm ringers and we turn out to be wrong, the costs are minimal and reversible.â Not only is this a poor basis on which to build legislation, it is also wrong. As we show in this primer, there are enormous potential risks to privacy, free expression, andâultimatelyâthe mental health of vulnerable teenagers.

4.2 Sexual Exploitation and Abuse

The fear that young people will be abducted or assaulted by âonline predatorsâ or subject to sex trafficking is a constant modern concern. KOSA and similar bills attempt to reduce the âsexual exploitation and abuse of minors,â although the mechanisms for doing so are unclear. One possibly relevant provision in KOSA requires platforms to âlimit the ability of other individuals to communicate with the minor,â which addresses fears of strangers, especially adult strangers, communicating with young people. However, this provision would seemingly prevent young people from communicating with anyone online. Is the risk of âstranger dangerâ grave enough to prevent young people from using the internet to communicate?

First, is important to distinguish the risk of âonline predatorsâ from the known availability of child sexual abuse material (CSAM), as well as milder content that appeals to pedophiles, some of which is posted, knowingly or unknowingly, by parents. While these are significant and troubling, neither KOSA nor any other child safety bills would address these concerns. CSAM is already illegal under federal law and is moderated by both state law enforcement and platform companies, who are legally required to report CSAM to the National Center for Missing & Exploited Children (NCMEC).

Second, researchers use the term âonline sexual abuseâ to include grooming, solicitation, and image-based abuse. Most perpetrators of online sexual abuse are not strangers, just as in cases of dating violence, child sexual abuse, child predation, cyberbullying, and cyberstalking, all of which are far more likely to be perpetrated by someone known to the victim. A recent meta-analysis of 32 studies of online sexual abuse, conducted by the Crimes against Children Research Center, found that 44% of offenders are under 18 and 68% are acquaintances of the victim. In contrast to the stereotype of online sexual abuse âtypically involving younger children, deception, abduction, and coercive violence at the hands of internet strangers,â most perpetrators knew the victim âofflineâ and used the internet to âbuild trust and forge relationships that facilitated their crimes.â The average incident involved a teenager engaged in a sexual relationship with an older adult, most of whom were connected to offline contexts.

This very rarely involves abduction, as actual incidences of abduction due to online interactions with strangers are very small. However, the public perceives online predator abduction to be a common occurrence, partially due to highly-publicized cases like that of Alicia Kozakiewicz, who appeared on the cover of People. NCMEC, which runs a CyberTipline where the public can report suspected child sexual exploitation, widely publicized an 82% increase in tipline reports of âonline enticementâ between 2021 and 2022.

However, âonline enticementâ is a broad category, and NCMEC attributes the increase to a rapid rise in âsextortion,â which Justin Patchin and Sameer Hinduja define as âthe threatened dissemination of explicit, intimate, or embarrassing images of a sexual nature without consent, usually for the purpose of procuring additional images, sexual acts, money, or something else.â One study found that 60% of minors experiencing sextortion knew the perpetrators offline, often as current or former romantic partners, and 75% had voluntarily provided the images before the sextortion took place. A large, nationally-representative survey of American teenagers similarly found that most perpetrators were the victimâs âreal lifeâ friend or romantic partner; only 4-6% of victims were sextorted by someone online that they didnât know very well. Sextortion, therefore, is better characterized as a part of intimate partner violence or cyberbullying than something committed by âonline predators.â

While all the scenarios outlined in this section are serious and there is a significant social responsibility to prevent them, focusing on âstranger dangerâ is misplaced, since only a small number of online sexual abuse incidents involve strangers. As Sutton & Finkelhor write, âAn emphasis on stranger oriented messages can be problematic for effective prevention. It fails to orient the police and public to the multiple and varied sources of danger. These messages associate danger with the fact that someone is unknown rather than with particular problematic behaviors by any correspondent, known or unknown which do not map to the realities of the evidence base.â In other words, effective prevention strategies would help young people identify worrisome behavior from friends, peers, family members, and acquaintances, rather than shadowy internet strangers. KOSA would not contribute to such efforts.

Preventing all young people from communicating with adults or peers is a drastic measure, and one that will, most likely, only marginally decrease the rate of online sexual abuse of young people and, instead, cut them off from potential sources of support and friendship.

4.3 Eating Disorders and Self-Harm

Eating disorders (EDs) are often directly mentioned as proof that platforms need regulation. While there are important connections between social media and EDs, politicians and lobbying groups frame the relationship as clear-cut and solvable by regulatory solutions like KOSA. These assertions often rely on claims that platforms, like Instagram, allow pro-eating disorder content and that the comparative culture of social media leads to feelings of body dissatisfaction. While these claims are worthy of research and policy, the relationship between social media and eating disorders is complex and cannot be reduced to a simple causal claim that asserts social media produces eating disorders. There is a correlation between excessive use of social media and increased body dissatisfaction, decreased self-esteem, and ED-linked behaviors. Public health researchers Santarossa & Woodruff found that increased time on social media was âsignificantly related to higher ED symptoms/concerns.â Similarly, one psychiatric study demonstrated that âtime spent online or using social mediaâ was ârelated to body imageâ and showed âan association with abnormal eating attitudes and behaviors, binging, purging, use of laxative/weight loss or diuretics.â

These correlations, however, paint a simpler picture than the research would necessarily indicate. Scholarship describes, but does not make a prescriptive causal claim about the effects of social media. Is it that increased use of social media results in body dissatisfaction, or that those experiencing body dissatisfaction are more likely to seek out dieting content and engage in processes of peer-comparison? American University psychologists Meier and Gray found that total time spent on Facebook (FB) was not correlated with âthin ideal internalization, self-objectification, weight dissatisfaction, and drive for thinness.â It was âthe amount of FB time allocated to photo activityâ that was associated with ED linked behaviors. This finding has been repeated in the context of âselfie feedback,â âphotographic activity,â and exposure to âidyllic images.â

These distinctions matter because they inform whether targeted interventions will succeed or fail and highlight the expansiveness that the duty of care plank becomes impossible to enforce. If the problem is not merely time spent, and active posting, on social media platforms, then policies that merely rein in time spent wonât solve the problem. Research has shown that exposure to photographic activity, not just participation, is the predictive variable in ED-linked behavior. One group of eating disorder researchers found that âunposted selfiesâ were linked to âgreat ED symptom severity.â Solutions such as KOSA do not resolve those structural elements because they are social, not merely technological. Lastly, the complex web of causality makes enforcement almost impossible. If dieting culture is correlated with ED, does KOSA imply that platforms ought to eliminate any content associated with dieting culture or photographic activity? If platforms like Instagram are harmful to some young women, but helpful to others, does that warrant restricting them for all young people? These are practical questions that highlight the inability of KOSA to adequately solve the real problems highlighted by scholars and activists.

4.4 Social Media Addiction

Throughout the Senate version of KOSA, language suggests that social media is âaddictive.â One of Section 3âs list of âharms to minorsâ is âpatterns of use that indicate or encourage addiction-like behaviors.â KOSA also requires the FTC to commission studies on âAddiction-like use of social media and design factors that lead to unhealthy and harmful overuse of social media.â The scholarly evidence concurs that some social media users exhibit addiction-like behaviors, or what some scholars call âproblematic social media use.â (The Senate bill carefully uses âaddiction-likeâ rather than âaddictionâ because scholars have warned that using the language of addiction to describe social media use minimizes the physical symptoms and seriousness of drug and alcohol addiction. The House version eschews this language entirely for âcompulsive usage.â) This use is characterized by âmood modification, salience, tolerance, withdrawal symptoms, conflict, and relapse,â or, in plain language, âbeing overly concerned about social media, strongly motivated and having been devoting [sic] a great amount of time and energy to use social media, to the degree that an individualâs social activities, interpersonal relationships, studies/jobs, and/or health and well-being are impaired.â Most research finds that these behaviors are present in a minority of social media users, from 3.42% in a study of US 18-25 year olds and 4.5% in a study of Hungarian teenagers to 9.1% amongst teenagers in the Netherlands and 9.4% in Finnish adolescents.

We think this is one area where COSL could do some good. For example, New Yorkâs Stop Addictive Feeds Exploitation (SAFE) for Kids act requires all social platforms to provide a chronological feed, which Instagram users have been requesting since it was removed. Whether this will have a significant effect on the amount of time users spend on social media or the content they see is unknown, but unlike many of COSLâs provisions, it does not seem to have deleterious effects. In fact, we believe that allowing users to have more control over their feeds, whether by decreasing âsuggested posts'' from accounts the user does not follow, blocking types of advertisements, or giving users the option to turn off algorithmic recommendations, should be extended to all users of social platforms, not just those under 18. This is because problematic social media use is just as likely to manifest in adults as it is in teenagers.

However, âaddiction-like behaviors,â to the extent that they exist, are currently tied to social media use overall and not simply algorithmic feeds. As one study concludes, âvery little is known about how platform affordances and user perceptions may influence social media addiction.â Indeed, a number of studies suggest that evidence of âaddiction-like behaviorsâ can be found in use of the internet itself, not just social media platforms. This is in contrast to popular consensus that social media companies strategically design their platforms to be âaddictive.â KOSAâs provision to fund more research in this area is a good step forward towards answering these questions.

Finally, the popular idea that social media produces âhitsâ of âdopamineâ which facilitate addiction is not supported by research. Eschewing the language of âaddictionâ for âhabitual use,â Mark D Griffiths, Distinguished Professor of Behavioural Addiction at the International Gaming Research Unit at Nottingham Trent University, writes, âthe idea that dopamine âhijacks the brainâ and leads to âcompulsive loopsâ are analogies used in the media rather than the phrases used by scientists.â

5.0 Arguments against Child Online Safety Legislation (COSL)

The deluge of child online safety legislation has been met with repeated criticism from groups like the EFF, Fight For the Future, and the ACLU. While acknowledging the well-intentioned nature of COSL, critics have highlighted how these bills will require widespread age verification, negatively impact the privacy of all internet users, increase surveillance of adolescents, and violate key First Amendment provisions. Additionally, the enforcement mechanisms inherent in these bills can be used specifically against LGBTQ+ content and reproductive rights.

5.1 Age Verification

Most of this proposed or passed legislation will necessitate some form of age verification, or âage-gating,â either explicitly or implicitly. This is particularly true for laws aimed at restricting access to information based on age (such as North Carolina and Montanaâs anti-pornography bills, which require pornography websites to proactively verify the ages of users trying to access pornography.) But platforms and other internet service providers may need to engage in age verification simply to determine if they must comply with the law. For instance, critics of Californiaâs Age Appropriate Design Code (AB 2273), such as Mike Masnick, have argued that a website would need to use age verification to determine whether the law would apply. And though KOSA states that is does not require age-gating, age verification, or the collection of additional data from users to determine age, itâs unclear how the law would be applied without it. The law requires that platforms treat minors differently from adults which, according to the ACLU, could ânecessitate that platforms verify usersâ ages.â Due to the breadth of these laws, and since children generally do not have government-issued identification, these age verification lawsâwhether done through IDs, credit cards, AI-gating, or facial scanningâwill impact all users, regardless of age.

While KOSA only implicitly requires age verification for social media, many states are introducing bills which mandate age verification and parental consent. Arkansas enacted SB 396, which requires social media platforms to enact âreasonable age verificationsâ that include a âdigitized identification card,â âgovernment-issued identification,â or âany commercially reasonable age verification that holds an Identity Assurance Level 2.â Similarly, Utah and Texas instituted a requirement for commercially reasonable age verification. Other states, such as New Jersey, attempted to institute age verification, but ultimately failed. Federal bills like S. 1291 and S. 419 institute age verification at a larger scale. These calls are likely to increase in the coming years. In the most politically charged context, Floridaâs recently signed HB3 requires age verification for any âwebsite or applicationâ that âcontains a substantial portion of material harmful to minors.â HB3 applies to any website where âmore than 33.3 percent of total materialâ is âharmful to minorsâ which is content which: 1) âthe average person applying contemporary community standards would find, taken as a whole, appeals to the prurient interests,â 2) âdepicts or describes, in a patently offensive way, sexual conduct,â 3) âwhen taken as a whole, lacks serious literary artistic, political, or scientific value for minors.â

These are remarkably unclear standards, as it is not evident what determines âliterary, artistic, political, or scientific value for minors.â Similarly, âcontemporary community standardsâ regarding âthe prurient interestâ are subject to unilateral definition by the Florida Department of Legal Affairs. This means that Florida can use age verification to wage political battles over platforms which they deem politically lacking value for children, a strategy the state has previously used to ban LGBTQ+ books from classrooms.

Age verification is simple in theory, but difficult to enact effectively without sacrificing privacy, free expression, equity, accuracy, or all of the above. It is much easier to determine whether someone is over 18 than if they are under 18, since adults are much more likely than children to have government-issued IDs or credit cards. There is an implicit assumption that if someone is unable to verify they are over 18, they are presumed to be a minor. Platforms are likely to punt this responsibility to third party companies, such as PornHubâs parent company, MindGeek. (The Age Verification Providers Association unsurprisingly supports KOSA and even provides a guide to legislators on how to write age verification legislation that wonât be struck down by the courts.)

Solutions fit within one of three categories: self-assertion, ID verification, and estimation. Checking a box that asserts a user is over 18 offers privacy, but provides little accuracy. Verification of an ID, such as a passport or driver's license, is relatively accurate, but would functionally ban undocumented immigrants or other adults without identification from social media platforms while risking private information leaking in the case of a security breach. Others use proxies to estimate age. Access to a credit card is often used as a proxy for age, but doing so would exclude the 18% of Americans who lack a credit card. Services such as Yoti offer AI-powered facial recognition software, which can supposedly estimate age based on a facial scan. An independent review of the public-facing software has noted that it can easily be circumvented, with one report noting that Yoti could be spoofed by placing a dog in front of a face. A litany of prior studies have highlighted the racial bias often inherent in many facial recognition systems, complicating the ease with which these systems can be applied to a diverse set of users.

Young people are also likely to subvert many of these requirements relatively easily. The use of VPNs during the registration process would allow users to dodge the verification requirement. Users might also shift to smaller social media websites that lack parental supervision or age verification because they do not have user accounts, which is necessary to be subject to some legislation such as Texas HB 18. Youth might abuse loopholes and spoofing techniques to get online regardless of the intentions of legislators. And, of course, they might use services that are not under US jurisdiction, complicating the politics of which social platforms can legally operate within the United States.

5.2. Privacy and Surveillance

Questions of privacy are essential to understanding the dangers of COSL. Although many COSL bills are categorized as âprivacy bills,â they present significant challenges to online privacy and enable wide-scale surveillance.

First, wide scale implementation of mandatory age verification would have devastating consequences for internet privacy, making it more or less impossible to browse the web anonymously. Requiring users to submit government IDs for every social media platform they wish to access makes it impossible to access such sites anonymously, or even without a record. This is compounded by the fact that KOSAâs âduty of careâ provision and various state bills may make vast swaths of the internetânot just social platformsâsubject to age verification, such as pornographic websites and sites with âcritical race theory,â sexual health, abortion information, and LGBTQ+ content.

To be clear, this is legal information protected by the First Amendment. People should have the right to access information without government-mandated surveillance, as the reveal of such information could be extremely embarrassingâin some contexts, life-threateningâor even open them up to criminal proceedings. If COSL passes, adults who are unwilling to share private information with a third-party will find themselves unable to access content which was previously freely available. Tying individual users to government IDs violates the anonymity of users, and the privacy that children and adults need.

Second, there is little information on the companies that would administer age verification systems. Requiring private companies to collect very confidential information without any limits on how that information may be used opens up users to identity theft and having private information hacked, leaked, or breached. KOSA, for example, asserts that the federal government will produce guidance for platforms within 18 months that would explain how to comply with KOSA. These ambiguities amount to a âvote for KOSA nowâ and figure out how it will be implemented afterwards.

Supporters of age verification programs point to clauses which prevent information from being saved or used after the verification process, but this itself poses many distinct risks. If information is deleted immediately following verification, then those systems are substantially less auditable because there would be no concrete record of the information provided for verification. Given data aggregation, a personâs government ID could easily be linked to a list of sensitive sites they have accessed, which could then be bought and sold by data brokers. Data brokers are notorious for creating market categories of vulnerable people, such as men suffering from erectile dysfunction or senior citizens susceptible to scams. The data broker industry is virtually unregulated and sells individual personal data about millions of Americans.

Finally, the privacy of individual children and teenagers also matters. Under many state bills, anyone under 18 could have anything they do online surveilled by their parents. While it is tempting to place parentsâ rights above the rights of the child, children also have rightsâalthough the United States is the only nation that has not ratified the UNâs Convention on the Rights of the Child. In fact, the United States lacks not only a formal framework, but a âfundamental, common understanding of children as human beings and rights holders.â Instead, the conservative framework of parentâs rights has, as outlined below, been used primarily to limit childrenâs access to information, mostly information that is age-appropriate but politically controversial in some US contexts. Historically, children have been treated as the property of their parents, and, as Yale legal scholar Samantha Godwin argues, âany account of parental rights grounded in a parentâs separate interests supervening on the interests of their children has the effect of denying children equal moral consideration.â Privacy is a universal human right, but children living in their parentsâ house often suffer from a lack of privacy, as their rooms, actions, and belongings are often heavily surveilled by their parents. There must be a balance between regulating childrenâs access to inappropriate online content and enabling parental surveillance.

5.2.1 Parental Control

Parental control aspects of COSL are more likely to survive First Amendment review than age verification or duty of care. However, we have other concerns. An essential element of the discourse surrounding and supporting this batch of regulatory bills is the invocation of parental rights and protection. As Klobuchar noted in a hearing on Big Tech and the Online Child Exploitation Crisis, parents often referred to social media as âa faucet overflowing in a sinkâ leaving them âwith a mop while her kids are getting addicted to more and more different apps and being exposed to [dangerous] material.â In this view, children are addicts-in-waiting, and legal change is necessary to put parental control as a precursor for adolescent participation, rather than as a retroactive backstop. This is most present in the appeals for age verification as a precursor to participation on social media, with a requirement for parental consent for those under the age of 16 or 18.

While parental tools are important to protect the privacy and safety of children, parental monitoring is not intrinsically safe. Even if KOSA does not allow parents to view the content their children have accessed, bills pending in Arkansas, Indiana, Iowa, Tennessee, and Wisconsin would make social posts, messages, and likes available to parents. These would eliminate the capacity for children to have any private interactions on social media platforms, regardless of the social context through which they operate. Although most parents want the best things for their kids, there are abusive parents and parents who hold very different cultural and political views than their children. In these cases, COSL that allows parental monitoring would facilitate abuse and conflict. Given that LGBTQ+ youth are overrepresented among homeless and runaway children, who often leave home due to parental abuse or rejection, and that children with different political beliefs from their parents can face family conflict and uncertainty, COSL that allows parents to see the content of sites their children visit may make vulnerable minors more vulnerable.

Moreover, the rhetoric of parental rights has recently returned as a central element of conservative lobbying against Critical Race Theory (CRT), DEI efforts, and LGBTQ+ content. Movements espousing the need for parental control over education and exposure to sexual content can be traced back as far as the 1920s. For instance, Anita Bryantâs 1977 Save Our Children campaign asserted that protecting gay men and lesbians would result in schools being forced to hire âhomosexuals,â who would recruit them to homosexuality. More recently, the moral panic over drag shows at libraries echoes a similar fear of queer and trans influence on children. This derives from the idea that parents ought to be the gatekeepers and determiners of what children ought to be exposed to, regardless of the historical context and danger. Recently, âparentsâ rightsâ have been weaponized to ban books with LGBTQ+ characters from school libraries, characterize doctors providing gender-affirming care as âgroomers,â and remove mentions of slavery from textbooks. COSL, even when well-intended, fuels this rhetoric and will likely be used to similar ends.

5.2.2 Chilling Effects on Information Access and Free Expression

In practice, increased parental control over social media usage can result in a chilling effect, particularly on young people seeking essential information and community. According to a 2014 study of 5542 internet users aged 13-18, LGBTQ+ youth disproportionately rely upon online sexual health information, with 78 percent of LBGTQ youth searching for sexual health information. This is likely due to a lack of offline educational opportunities for queer youth without outing themselves to potentially unsupportive social and familial circles. Considering recent moral panics surrounding queer representation in educational settings, the need for online sexual health information will likely become more acute. This concern has been validated by relevant research. One study found that queer youth experiencing parental digital surveillance would self-censor, experience negative mental health, and worry about abuse if their sexuality was found out. If parental access now provides search history, comments, user activity, and even access to private messages to unsupportive parents, then queer youth will have their sexual privacy eroded, and be potentially subject to abusive responses.

In addition to the âduty of careâ provision, it is worrisome that many types of COSL seek to expand the monitoring of young peopleâs actions online, given the data privacy concerns around reproductive rights. In Mississippi and Indiana, women were prosecuted for text messages and Google searches indicating they were seeking abortions. Even if reproductive health care is not defined as âharmful,â the age-gating and other provisions of COSL will have a chilling effect on young people searching for information on abortion care.

Similarly, undocumented immigrants may be subject to arrest or deportation for providing information for age verification, given that ICE regularly uses commercial databases, social media platforms, and cellphone records to identify undocumented people. A survey of immigrants in New York City found that 42% would only use the internet for certain things and 26% would use the internet less or not at all if identity verification was required. As a result, widespread age verification would negatively impact access to information for marginalized groups.

5.3 Weaponization of Duty of Care

One central concern is the ability of KOSAâs duty of care provision to be weaponized against certain types of content as a means to protect children. This would be operationalized under the Duty of Care provision under Section 3, which requires platforms to take âreasonable measures in the design and operation of any product service of featureâ to mitigate harm towards minors. Importantly, however, KOSA relies on the FTC to litigate whether a platform is causing harm, which can often swing largely based on the political winds of the time. This vagueness in defining âharmâ thus allows state attorneys general to pressure the FTC to define different types of content as âharmful,â and opens the door to state bills which would make it impossible for youth to access content that KOSA would allow. KOSA, as written, requires supporters to trust that the guidelines produced within 18 months would prevent the duty of care from being weaponized, without any clarification on how those guidelines might be formulated.

Right-wing organizations and individuals have already espoused their intentions to use COSL to censor transgender and gender-affirming content (of course, left-wing organizations and individuals might also use COSL for their preferred censorship preferences). In a commentary piece for The Heritage Foundation titled âHow Big Tech Turns Kids Trans,â Jared Ecker and Mary McCloskey lauded KOSA as prohibiting âthe sexual exploitation of minors and the promotion of content that poses risk to minors' physical, and mental health.â Protecting children, for Heritage, requires guarding âagainst the harms of sexual and transgender content online.â In a X (formerly Twitter) thread justifying this position, the Heritage Foundation asserted that trans content, and transness, are âsocial contagionâ that conditions kids to âpermanently damaging their healthy bodies.â In a video published by the conservative Family Policy Alliance, Marsha Blackburn lauded KOSA, while also asserting that âprotecting minor children from the transgender in this cultureâ ought to be a priority for the conservative agenda. Most recently, in an interview with PBS, Jonathan Haidt claimed that young people become trans due to peer contagion effects, saying âItâs very different from the kinds of gender dysphoria cases that weâve known about for decades. I mean, it is a real thing. But what happened, especially when girls got, was YouTube and Instagram early, but then especially TikTok, girls just, you know, girls get sucked into these vortices and they take on each otherâs purported mental illnesses.â

This rhetoric of transness as a âsocial contagionâ emerges from the proliferation of the term ârapid onset gender dysphoria,â coined by Lisa Littman in 2018. As Florence Ashley describes the term, ROGD describes a ânew clinical subgroup of transgender youth, which would be characterized by coming out as transgender out of the blue in adolescence or early adulthood.â Rapid onset supposedly occurs due to peer influence networks, trans content on social media, trauma, and even sexual objectification. The study justifying Littmanâs theory of ROGD has been shown to be flawed in multiple ways. Notably, Littman solicited parents from anti-trans parental groups and did not interview any trans people. Furthermore, as Arjee Restar, an Assistant Professor of Epidemiology at the University of Washington highlights, Littman relied upon parental respondents to diagnose their children without clinical qualifications and relied upon retroactive reports that spanned âon averageâ¦6 years for parents to remember between their childâs âchildhoodâ and current age.â Yet, the perceived legitimacy of ROGD allows anti-trans organizations to subvert the intentions of KOSA, and claim that it would meet Section 3âs standard of âevidenced-informed medical information.â

ROGD as a conceptual frame has provided anti-trans state legislatures with justification for the restriction of trans-affirming care, as was the case when Florida directly cited Littmanâs conception of ROGD as proof of the need to restrict gender-affirming care. ROGD frames transgender identity as both a source of âsocial contagionâ and a product of social contagion. Simply put, this understanding of gender dysphoria, and the expression of transgender identity, asserts that children are becoming trans due to social acceptance of trans identity and that children are identifying as trans due to social pressure, whether explicit or implicit. This relies upon a view of trans identity as pathological, and merely a fad which children adopt, rather than a very material experience.

Similarly, civil liberties groups identify KOSA as a mechanism to shut down reproductive rights content, especially access to abortion pills, abortion funds, and legal resources for abortion seekers. Since the overturning of Roe v. Wade in the Dodds decision, 14 states have banned abortion, three have implemented gestational limits beyond those in Roe, and three have had abortion bans blocked.

Abortion restrictions, such as requiring only physicians to prescribe abortion medication and necessitating women seeking abortions to undergo mandated ultrasounds, counseling, and waiting periods, have been imposed across the country. Access to contraception has decreased. Conservative anti-abortion groups and politicians are attempting to redefine certain types of contraceptives, such as IUDs and emergency contraception, as abortion, in order to further restrict their usage. A few states have sought to prevent women from traveling to other states to seek abortion care.

In this very contentious landscape, people seeking reproductive health careâespecially young womenâmay find their access to reproductive health care information severely curtailed. Given that anti-abortion strategies often rely on dubious claims that abortions (and contraception) are harmful to women, it is likely that states in which abortion is illegal will similarly define reproductive rights content as harmful to young people.

Supporters of KOSA point to revisions of the original 2022 bill in response to massive critique from prominent LGBTQ organizations, but these revisions fail to provide meaningful protections. These changes largely rely upon defining the âcovered platformâ to exclude things like a suicide hotline, and limiting the provision from restricting minors from âdeliberately and independently searching for, or specifically requesting, content.â While these are important restrictions, they fail to adequately protect trans youth. First, NGOs can lobby the FTC to claim the existence of trans content as causing harm, which will likely result in platforms preemptively moderating contentious content before the lawsuit takes place. Second, state laws will likely define trans content as harmful, thus supplementing KOSA. In a polarized social context where the definition of âharmfulâ is highly subjective and deeply influenced by politics, allowing the government to decide which content is considered âharmfulâ opens up a serious vector for abuse.

5.4 First Amendment ConcernsÂ

From the perspective of digital civil rights organizations, COSL presents a significant threat to the First Amendment, and poses potential censorship concerns. Even after the revised version of KOSA was unveiled, the EFF, the Center for Democracy and Technology (CDT) and the ACLU have all cited First Amendment concerns with KOSA. The ACLU has noted there are two areas of the bill that could potentially chill speech: (1) Requiring or incentivizing age verification, which they argue could chill speech for both adults and minors; and (2) the âDuty of Careâ requirement which, they argue, could âentice platforms to censor content.â The Electronic Frontier Foundation has also argued that KOSA poses First Amendment concerns, pointing to the degree of power that state attorney generals will have in targeting online services and speech. They note that even with changes to the bill made in 2024, KOSA will still permit state officials to enforce the law based on âtheir political whims.â They have also raised concerns about First Amendment concerns resulting from the need to restrict content based on age, which effectively mandates age verification.

At the state level, many of the bills which proposed age verification have been challenged on First Amendment grounds. In response to lawsuits from tech industry organization Net Choice and the ACLU, the California Age-Appropriate Design Code was blocked by a district court injunction on First Amendment grounds and is headed to the Ninth Circuit. Arkansas SB 396 was similarly blocked after Net Choice, the ACLU, and the EFF filed comparable lawsuits. Texasâs HB 1181, which requires age verification to view pornography, was recently upheld by the Fifth Circuit after a lawsuit from adult entertainment companies and is likely to be appealed in a higher court.

The implementation of Utahâs SB 152 was delayed until October 1, 2024, in response to a set of First Amendment lawsuits against the bill. Legislators have introduced SB 194 and HB464 in response. SB 194 removes SB 152âs requirement of parental consent to open an account, but places any minor accounts that lack parental consent at the maximum level of privacy, removing the ability to share content with or message anyone beside already âconnected accounts.â Platforms are then required to use age assurance criteria to determine if âan account holder is a minor with an accuracy rate of at least 95%,â and users must appeal with documentary evidence if they are categorized falsely.

There is a long history of internet legislation requiring age verification that has been struck down because of the First Amendment. In an early effort to protect minors online, the Communications Decency Act (CDA) was passed in 1996 to criminalize the knowing transmission of obscene, indecent, or patently offensive material to minors (defined as anyone below the age of 18). Persons or businesses could avoid being prosecuted under this law by restricting minor access to indecent material, and requiring the âuse of a verified credit card, debit card, adult access code, or adult personal identification numberâ to view it.

As noted by Pagidas (1998), âthe act immediately sparked controversy.â It was quickly challenged in Reno v. ACLU, in which a group of twenty plaintiffs, led by the ACLU, challenged its constitutionality through filing suit against the Attorney General and the Department of Justice. Their complaint alleged the law was unconstitutional because it âcriminalizes expression that is protected by the First Amendment,â is âimpermissibly overbroad and vague,â and is not the âleast restrictive means of accomplishing any compelling governmental purpose.â Additionally they asserted that the Act violated the constitutional right to privacy, effectively criminalizing private email corresponding to or among individuals under the age of 18.

In the 1997 Supreme Courtâs decision, the majority noted potential problems âconfronting age verification for recipients of Internet communications.â The Court noted that in the late 1990s, âthe Government offered no evidence that there was a reliable way to screen recipients and participants in such forums for age.â The Court continued, âeven if it were technologically feasible to block minorsâ access to newsgroups and chat roomsâ¦that potentially âelicit âindecentâ or âpatently offensiveâ contributions,â it would not be possible to block those types of content while still enabling them to access the remaining content that was not indecent. They argued that the age verification technologies that had been proposedânamely using a credit card as a surrogate for proof of ageâwould bar adults who did not have a credit card. Thus, in the Courtâs opinion, age verification violated the First Amendment of both children and adults.

It is unclear how and whether these concerns have been addressed by KOSA, or whether the potential chilling effect on speech posed by age verification would remain. Though legislators have claimed that KOSA would not require age verification, this legislation and similar state bills described above require that platforms and other companies operating on the internet be able to distinguish between those who the law applies to (i.e. under a certain age), and those it does not (see Section 5.1).

6.0 Recommendations

At this writing, KOSA has only recently been introduced into the House, and various COSL efforts are being fought in the courts, primarily on First Amendment grounds. We recognize the tremendous amount of public concern over the mental health of young people and the unprecedented power of the technology industry. While we believe the current instantiations of COSL are not appropriate ways to solve these problems, we offer some suggestions. While these reforms are more complicated than simply passing legislation and require more political will, we believe they will positively impact the well-being of children, parents, and other adults alike.

Center Young People

Discussions of social media, children, and mental health invariably turn into debates about whether social media is good or bad for children. This emerges when we center the technology. Instead, we need to center young people. What is at the root of their struggles? What do young people need and want to feel empowered? As researchers, we find that this rarely starts with fixes to technology. Moreover, young people are not a monolith; content that empowers one teenager may make another anxious. What happens online and offline are interlinked; childrenâs behaviors are just as influenced by their family, peer group, and social context as they are by screens. The issues young people are facing are ecological in nature. It is critical that lawmakers see technology as one facet in a system, one that mirrors and magnifies a range of dynamics already at play. Rather than rendering unhealthy youth practices invisible, we should be certain to help young people who are seeking help.

Increase Access to Mental Health

Young people need improved access to free or low-cost mental health services. Regardless of what is contributing to the rise in depression, anxiety, and death by suicide, mental health resources are a clear way to counter them. Mental health support must be provided to children, teenagers, and parents around the country. In addition to addressing the shortage of social workers, therapists, and counselors, this might include community and support groups to help young people suffering from loneliness or social isolation. The Biden Administrationâs comprehensive national strategy to address the mental health crisis and $200 million earmarked for youth mental health support is a step in the right direction.

Rebuild the Social FabricÂ

In the wake of the COVID-19 pandemic, the social fabric has frayed. In September 2023, the Census Bureau released figures showing that the number of children living in poverty in the US more than doubled, from 5.2% in 2021 to 12.4% in 2022. The pandemic hindered student achievement and caused ongoing learning deficits. These large systemic factors must be addressed.

More specifically, young people need concerned adults in their lives who are not their parents, such as coaches, pastors, teachers, neighbors, and godparents. Locales must provide spaces where teenagers can socialize in person, such as libraries, youth centers, and skateparks. Even though there is tremendous social pressure on parents to engage in intensive helicopter parenting, parents should enable their children to have unstructured free time, walk to school, and âhang out.â

Create Digital Street Outreach Programs

In the 1990s, when concerns about crack and HIV were at an all-time high, communities began investing in âstreet outreachâ programs. Community members went out into the streets to offer clean needles and information about local services that could be supportive. They checked in on those who were struggling with careâgiving people dignity and hope.

Todayâs youth need digital street outreach programs. They are crying out in pain online. Rather than try to render those cries invisible, we need social services that can ensure that when young people are struggling, they have access to services that might help them.

Many of us feel that we have an unhealthy relationship with our phones, tablets, and laptops. The success of third-party software such as Freedom, the popularity of âdigital detoxâ retreats and programs, and a small but significant number of people choosing âdumbphonesâ over smartphones all show the desire for widespread changes in modern technology use. The types of platform reforms included in some COSL, such as chronological feeds, blocks on âdark patterns,â and granula
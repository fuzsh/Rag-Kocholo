{
    "id": "wrong_mix_random_subsidiary_00117_1",
    "rank": 20,
    "data": {
        "url": "https://arxiv.org/html/2402.16268v1",
        "read_more_link": "",
        "language": "en",
        "title": "Foundation Model Transparency Reports",
        "top_image": "",
        "meta_img": "",
        "images": [],
        "movies": [],
        "keywords": [],
        "meta_keywords": [
            ""
        ],
        "tags": null,
        "authors": [],
        "publish_date": null,
        "summary": "",
        "meta_description": "",
        "meta_lang": "en",
        "meta_favicon": "",
        "meta_site_name": "",
        "canonical_link": null,
        "text": "HTML conversions sometimes display errors due to content that did not convert correctly from the source. This paper uses the following packages that are not yet supported by the HTML conversion tool. Feedback on these issues are not necessary; they are known and are being worked on.\n\nfailed: dialogue\n\nfailed: duckuments\n\nAuthors: achieve the best HTML results from your LaTeX submissions by following these best practices.\n\nLicense: CC BY 4.0\n\narXiv:2402.16268v1 [cs.LG] 26 Feb 2024\n\n11affiliationtext: Stanford University22affiliationtext: Massachusetts Institute of Technology33affiliationtext: Princeton University\n\nStanford Center for Research on Foundation Models (CRFM)\n\nStanford Institute for Human-Centered Artificial Intelligence (HAI)\n\nFoundation Model Transparency Reports\n\nRishi Bommasani Kevin Klyman Shayne Longpre Betty Xiong Sayash Kapoor Nestor Maslej Arvind Narayanan Percy Liang\n\nAbstract.\n\nFoundation models are critical digital technologies with sweeping societal impact that necessitates transparency. To codify how foundation model developers should provide transparency about the development and deployment of their models, we propose Foundation Model Transparency Reports, drawing upon the transparency reporting practices in social media. While external documentation of societal harms prompted social media transparency reports, our objective is to institutionalize transparency reporting for foundation models while the industry is still nascent. To design our reports, we identify 6 design principles given the successes and shortcomings of social media transparency reporting. To further schematize our reports, we draw upon the 100 transparency indicators from the Foundation Model Transparency Index. Given these indicators, we measure the extent to which they overlap with the transparency requirements included in six prominent government policies (e.g. the EU AI Act, the US Executive Order on Safe, Secure, and Trustworthy AI). Well-designed transparency reports could reduce compliance costs, in part due to overlapping regulatory requirements across different jurisdictions. We encourage foundation model developers to regularly publish transparency reports, building upon recommendations from the G7 and the White House.\n\n1. Introduction\n\nFoundation models are transformative digital technologies (Bommasani et al., 2021), introducing new capabilities (Wei et al., 2022) and risks (Weidinger et al., 2022) that have garnered unprecedented public attention to AI. As with earlier digital technologies such as the Internet and social media, the potential for profound societal impact necessitates greater transparency for foundation models. The 2023 Foundation Model Transparency Index (Bommasani et al., 2023a) confirms that, currently, the foundation model ecosystem is opaque: the Index scored 10 major foundation model developers (e.g. OpenAI, Google, Meta) on a 100 point scale for transparency, with developers on average receiving a mere 37 out of 100.\n\nPrevious digital technologies, especially social media platforms, have been similarly plagued by insufficient transparency. Over the past 15 years, social media platforms have come to produce transparency reports: public reports, produced on recurring basis, that consolidate information related to usage of their platforms and key platform governance practices like takedown requests and policy enforcement. Today, transparency reports are an industry standard: Access Now documents that more than 85 Internet and telecommunications companies have produced transparency reports (Access Now, 2023). The European Union’s Digital Services Act mandates transparency reporting for online platforms and formalizes the process to ensure that vital information is publicly reported with sufficient fidelity, frequency, standardization, and accessibility (European Commission, 2023).\n\nWhile transparency practices are nascent for foundation models, and the current landscape displays both idiosyncratic and systematic opacity (Bommasani et al., 2023a), governments are stepping in to take corrective measures. In the United States, Representatives Donald Beyer and Anna Eshoo have introduced the AI Foundation Model Transparency Act to mandate public reporting of standardized information as to be determined by the Federal Trade Commission. This bill builds on reporting requirements from the October 2023 Executive Order on AI. In the European Union, the EU AI Act requires transparency on training data, energy usage, model evaluations, and risk management. Other policies intended to improve transparency include Canada’s code of conduct for advanced AI systems, China’s generative AI services regulation, and the United Kingdom’s request that firms share their responsible scaling policies.\n\nTo address the transparency deficits in the foundation model ecosystem, build upon transparency practices for social media platforms, and guide the transparency initiatives proposed by governments, we propose Foundation Model Transparency Reports. Foundation Model Transparency Reports are structured reports that provide essential information about foundation models which developers should publish on a periodic basis. Such transparency reports would standardize what companies should report, consolidate this information to assist stakeholders in finding it, and structure the information to facilitate subsequent analysis or comparison across multiple developers. Our transparency reports build upon recommendations under the G7’s voluntary code of conduct and the White House’s voluntary commitments, both of which state that foundation model developers release transparency reports.\n\nAs we show in § 4, while current transparency requirements in government policies often lack precision, our transparency reports specify a precise schema for disclosing information. In particular, we build on the 100 transparency indicators defined in the Foundation Model Transparency Index (Bommasani et al., 2023a) that concretize transparency for foundation models across the supply chain. While Bommasani et al. (2023a) scored foundation model developers for their existing practices, we describe how developers can implement new reporting practices to inculcate stronger norms of transparency.\n\nOur paper makes three contributions to advance transparency in the foundation model ecosystem. First, we explore how transparency reporting is conducted in other industries to derive principles; we use these principles to design Foundation Model Transparency Reports. Second, we align our design with government policies to show how transparency reports could improve compliance and reduce compliance burden across jurisdictions. Third, we instantiate our design with examples of Foundation Model Transparency Report entries from different foundation models based on publicly available information, setting a clear example for future reports. Together, our work guides foundation model developers on how to be more transparent and world governments on how to promote transparency through policy.\n\n2. Social Media Transparency Reports\n\nThe rise of social media platforms over the past ten to twenty years provides a natural parallel for foundation models. Namely, a disruptive and powerful emergent technology came to be widely adopted across societies, thereby intermediating important societal functions such as access to information and interpersonal communication. Social media has been associated with several types of risk, some of which resulted in substantial societal harms (e.g. the Cambridge Analytica scandal, the Rohingya genocide in Myanmar). These harms are not entirely unrelated to the significant opacity of social media platforms (e.g. with respect to how is user data shared, how is contented moderated - necessary but not sufficient), which make it more difficult for governments and external researchers to assess harms to users. We therefore describe transparency reporting in the context of social media, where it has emerged as a standard practice, to conceptualize transparency reporting for foundation models.\n\n2.1. History\n\nIn the context of social media and telecommunications, a transparency report is a recurring public report of key metrics related to legal information and takedown requests, as well as policy and intellectual property enforcement for large online platforms (Bankston et al., 2017; Trust and Safety Professional Association, 2023). Telecommunications companies and social media platforms have gradually adopted these reports since 2010, in response to public concerns over their handling of privacy, government surveillance, freedom of speech, and misinformation. Initially, these concerns were triggered by disclosures of dissident information to the Chinese government (Schatz, 2006), the FBI’s use of the Patriot Act for surveillance (acl, 2010), and Edward Snowden’s subsequent disclosures of NSA surveillance practices (Greenwald, 2013). Concerns from users and advertisers would later emerge over moderation practices of harmful content (wfa, 2020), spurring greater transparency into platform policy enforcement.\n\nTo examine how transparency reports emerged, we consider Google’s transparency report in 2010. In 2010, Google first reported government requests for content removal or information (goo, 2010), as well as where its services were blocked or inaccessible (Bankston et al., 2017). The report showed that Google received over 1200 requests from 36 jurisdictions, providing a greater level of basic insight into the platform (e.g. Brazil made 398 requests for over 19000 items to be removed, and Google complied with 68 of these requests). Shortly thereafter, LinkedIn, Microsoft, and Twitter began producing their own recurring reports, with an avalanche of adoption following the Snowden revelations in 2013, now including Facebook, Apple, and Yahoo. Access Now’s Transparency Reporting Index documents this increase in adoption: 6 companies produced reports in 2012, compared to over 60 in 2015. Transparency reporting also gradually expanded in scope to include removals under intellectual property law and the Digital Millennium Copyright Act (Access Now, 2023). Etsy was the first platform, in 2015, to introduce a policy enforcement report, detailing its responses to user violations of its terms of service (ets, 2014). Since reporting on these incidents could compromise user privacy, companies generally release high-level aggregate statistics. Overall, transparency reports came to be an important part of companies’ brands and helped foster wider public trust and accountability (Bankston et al., 2017; Trust and Safety Professional Association, 2023).\n\nWhile social media platforms played a significant role in conceptualizing the first versions of transparency reports, civil society organizations drove advances in their scope and utility. For example, organizations began to rank online platform transparency practices to generate pressure. The Electronic Frontier Foundation regularly scores corporations on privacy, process, and freedom of speech, in “Who Has Your Back” (Crocker et al., 2019). In response, high-scoring companies like WordPress and Apple publicized their results (Zhu, 2015). Similarly, Ranking Digital Rights maintains its Corporate Accountability Index, to score telecommunications providers on a spectrum of transparency, access and responsibility to users (MacKinnon et al., 2019). And in response to the Transparency Reporting Toolkit from New America and the Berkman Klein Center (Budish et al., 2016), Twitter revamped its reporting standards to follow suggested best practices (Kessel, 2016).\n\nSubject to the recommendations of civil society, and the associated push for greater accountability, transparency reporting from major social media platforms had evolved to become more interpretable to the general public, more detailed, and more regular in its cadence. By 2021, 88 technology companies had published transparency reports, with some including downloadable data (Access Now, 2023). For instance, Meta now releases comprehensive and often near-live reports on policy enforcement, intellectual property, government requests, content restrictions, regulatory measures, Internet disruptions, and even widely viewed content (Facebook, 2023). In addition, Meta offers content libraries with APIs for Facebook and Instagram, as well as an ad Library. However, since 2021 there has been a steep decline in new voluntary transparency reporting from major platforms (Rydzak, 2023); X, for example, no longer makes updates to its Transparency Center (X, 2023).\n\n2.2. Purpose\n\nIn social media, transparency reporting functions as an instrument for social media companies to make information public. In particular, these disclosures help alleviate informational deficits on public interest matters spanning privacy, free speech, surveillance, and the reach of harmful content. Social media platforms are often incentivized to comply with the unethical or secretive requests of governments in order to maintain access to their markets (Gorwa and Ash, 2020). While transparency reporting cannot fully deter this incentive, it can inform the public of the scope and extent of a government’s intervention into platforms, and spur public pressure as a deterrent to surveillance, censorship, or privacy violations. Additionally, as social media platforms have been likened to a “digital public square,” the processes governing the access and dissemination of speech can have significant societal impact (Lazar, 2023). In light of rising concerns of algorithmic dissemination, echo chambers, and scalable misinformation, transparency reporting could mediate public trust. In theory, open and transparent processes around speech suppression or amplification would enable a fairer public discourse that is better informed about the measures taken by social media companies to regulate online speech.\n\nGiven that the information made available through transparency reports is intrinsically highly multifunctional, transparency reports are simultaneously targeted at a range of stakeholders in the complex platform ecosystem. Nonexhaustively, these stakeholders include platform users, non-users that are impacted by platform operations, investors, and advertisers. Users, non-users, and civil society collectively are invested in ensuring that processes that govern platform information are fair and privacy-preserving. These concerns are partially addressed by clear documentation of standards and procedures for privacy, compliance with governments, and content moderation, as well as public statistics. Consequently, civil society organizations have outlined clear criteria by which transparency reports can better satisfy these stakeholder objectives (Llansó and Vogus, 2021; Santa Clara Principles, 2023; Aspen Institute, 2021). Similarly, it is in advertisers’ interests for their ads to not be associated with offensive or harmful content (wfa, 2020). Certain platforms have regulated political advertising, providing a clear example where monitoring the compliance and impact of company policies can provide a useful basis for academic research by social scientists (Edelson et al., 2021). Lastly, in the absence of corporations supporting public interest research, some have argued that society’s abilities to understand and address misinformation, among other harms, is severely limited (Abdo et al., 2022).\n\n2.3. Implementation\n\nModern social media transparency reports are typically divided into four categories: legal information requests, legal takedown requests, intellectual property enforcement, and policy enforcement (Trust and Safety Professional Association, 2023). Legal information requests typically pertain to requests for private information on users and their communications (Vermeulen, 2021). Legal takedown requests pertain to governments applying local laws to have content permanently removed from platforms. Platforms may not always comply with government requests, so reports often show the number of requests by country, the compliance rate, and the number of unique accounts affected. Intellectual property reporting is often split into content removals and requests for copyrighted and trademarked content. Policy enforcement reporting includes a wide range of potential violations, which will differ by platform, and usually display the removal rates over time per country for each violation type. Additionally, platforms may report other metrics that detail the security of user accounts, the content that is most viewed on the platform, or changes in company policies.\n\nWhile this high-level standardization is common across social media companies, further standardization has been challenging. Primarily, as Keller (2021) outlines, most metrics are not straightforward to calculate and come with implicit assumptions. As a result, given the significant heterogeneity in social media platforms, this requires bespoke, company-specific measurement approaches that inhibit apples-to-apples comparisons (Trust and Safety Professional Association, 2023). Because reported statistics are often not standardized, platforms have substantial discretion to select what they measure and how they measure it (Urman and Makhortykh, 2023). This idiosyncratic approach intensifies concerns that transparency reporting in social media serves as a type of ethics-washing that is instrumentalized as marketing collateral (Zalnieriute, 2021) Further, transparency reporting introduces substantial costs for social media platforms (Stoughton and Rosenzweig, 2022). Significant company-internal infrastructure is required to initially measure and subsequently maintain metrics, especially given many social media platforms operate across many jurisdictions. Companies need to dedicate significant resources to maintaining their transparency reports, causing some to question whether this comes at the cost of further investment into more substantive governance or risk mitigation at these organizations.\n\nSerious critiques of transparency reporting—and the broader focus on improving the procedural transparency of digital technology providers—have been raised in various fields (Boyd, 2016; Ananny and Crawford, 2018; Ghosh and Faxon, 2023; Mittelstadt, 2019; Han, 2015; Birchall, 2021). For example, social media companies who release transparency reports rarely sufficient access to their platforms for third parties to validate the information they disclose, meaning the information may be inaccurate. These critiques are often valid: transparency is not an end unto itself, it is merely a mechanism that may allow further insight into the operations of technology companies in order to better pursue other more tangible societal goals (Bommasani et al., 2023c). The way in which transparency requirements are implemented can have a significant impact on whether transparency is performative and unverifiable or substantive and rigorous.\n\n2.4. Mandates\n\nHistorically, transparency reporting has been a voluntary practice and, increasingly, an expected norm in the social media industry due to public pressure. However, a growing number of governments are considering mandating transparency reporting. In the United States, other types of disclosure requirements imposed by the government are at times in tension with the First Amendment due to concerns of compelled speech. In ruling on disclosure requirements across several contexts, the Supreme Court has used several different legal standards for distinct types of disclosure, sharing a common basis in requiring the government to prove a disclosure requirement “is appropriately tailored to a sufficiently important goal” (Valerie C. Brannon and Victoria L. Killion and Whitney K. Novak and L. Paige Whitaker, 2023).\n\nUnder the European Union’s recently-enacted Digital Services Act (DSA), online platforms are required to abide by transparency and access provisions (Miller, 2023). The EU designates Very Large Online Platforms (VLOPs) as platforms with at least 45 million monthly active users (i.e. 10% of the EU population): these platforms must prepare biannual transparency reports, conduct periodic risk assessments, publish audit reports, establish ad repositories, and share data with external researchers. The EU solicited external input on the form and manner of these transparency reports from December 2023 to January 2024, and intends to adopt an implemented regulation in the first quarter of 2024 (European Commission, 2023). Primed by the experiences of transparency reporting over the past decade, the EU aims to standardize reporting by identifying a series of indicators that must be reported, along with clarifying measurement methodology in several cases (Schneider et al., 2023). In the first round of transparency reporting under the DSA, 19 platforms submitted reports spanning human resources dedicated to content moderation by locale, content enforcement takedown rates and error rates on both content and accounts, as well as the median time needed to enforce content violating the law or platform policy (Commission, 2022). This level of specificity has allowed for far greater clarity into operations: for example, the transparency report from X demonstrates glaring disparities in content moderation staffing across languages (e.g. Bulgarian, Croatian, Dutch, Hebrew, Italian, Latvian and Polish all have at most 2 content moderation staff who are primary language speakers, compared to over 2000 for English).\n\n3. Design of Foundation Model Transparency Reports\n\nTo design Foundation Model Transparency Reports, we identify 6 design principles, informed directly by the strengths and weaknesses of social media transparency reporting (§ 2). Subject to these principles, we then identify indicators to be included in the reports using the Foundation Model Transparency Index (Bommasani et al., 2023a) and work through a few examples of how developers may report information related to these indicators.\n\n3.1. Principles\n\nSocial media transparency reports, especially in their current form, embody several desirable principles for transparency reporting. First, these reports consolidate information about a social media platform’s practice into a centralized location, referring both to the transparency report document and the transparency report page on the platform’s website. Consolidation and centralization enable stakeholders to have a singular and predictable source for finding relevant information. Second, these reports are structured to address specific queries: reports often have four top-level sections (see § 2.3). This structure sets clear expectations for what can be found in the report, what the report is unlikely to cover, and as a coarse means for comparing different platform practices. Third, some companies prepare extensive transparency reports that clearly contextualize information. Given that transparency reports are read by a variety of stakeholders with differing expertise and familiarity about platforms, and there are many unique nuances of a platform (e.g. what a “user” is in the context of the platform), context is necessary to adequately interpret information.\n\nHowever, social media transparency reports at present (generally) fail to implement other desirable principles for transparency reporting. First, while these reports consolidate information, the underlying information to be included is not independently specified. Consequently, platforms are able to determine what information to include and exclude, allowing them to unevenly report only on matters advantageous to them. Second, while these reports are coarsely structured, they are not fully standardized both in terms of the form and organization as well as the indicators reported. Therefore, transparency reports from different platforms cannot be easily compared to each other or combined to perform larger-scale analyses that reveal aggregate trends. Social media companies are ultimately the deciders of what constitutes a transparency report in their industry, leading to significant heterogeneity. Third, while the best transparency reports at present contextualize information, they often do not clearly specify methodologies for computing statistics. As a result, given many quantities could be computed in different ways (e.g. different methods of user de-duplication for user counting), without clarity on the underlying methodology, consumers of transparency reports may still be prone to misinterpretation.\n\n3.2. Approach\n\nUsing these 6 principles—centralization, structure, contextualization, independent specification, standardization, and methodologies—we design Foundation Model Transparency Reports. To begin, rather than having foundation model developers dictate what is included in their own transparency reports, we propose a uniform set of indicators to be included in transparency reports across foundation model developers. This ensures that the contents of the reports are simultaneously (i) independently specified and (ii) standardized. To select these indicators, we use the 100 transparency indicators (Appendix A) from the Foundation Model Transparency Index (FMTI; Bommasani et al., 2023a), which is a recent initiative that scores major foundation model developers for their transparency. The Foundation Model Transparency Index provides a comprehensive conceptualization of transparency with its 100 indicators organized into 3 domains: (i) the upstream resources used to build a foundation model, (ii) the model properties including evaluations, and (iii) the downstream use and impact of the foundation model. Domains are further broken down into subdomains (e.g. upstream resources include data, labor, compute, code); we re-use this hierarchical domain-subdomain structure as the recommended organization for Foundation Model Transparency Reports.\n\nIn contrast to social media platforms, where platform activities and usage are almost exclusively conducted on the platform’s website, foundation models have different usage patterns. As a consequence, transparency reports for foundation models should not only be made available on the foundation model developer’s website, but also via distribution channels that make the foundation model available. For example, Meta’s Llama 2 model is distributed via Meta’s GitHub repository, but also via Microsoft Azure, Hugging Face, and other platforms. As a result, transparency reports would, ideally, be disseminated through these distribution channels as well to ensure the associated information can be discovered even if a consumer of the information does not look for it on Meta’s website. Further, akin to the centralization of DSA Transparency Reports in an EU database, governments may consider consolidating Foundation Model Transparency Reports across foundation model developers into a single database to facilitate research and analysis.\n\nTherefore, our design addresses 4 of the 6 principles we identify: independent specification, consolidation/centralization of information, report structure, and standardized information/indicators. In practice, foundation model developers may choose to not be transparent about certain indicators for a variety of reasons such as (i) the costs of generating the relevant information, (ii) the liability risk from disclosing the information, or (iii) the competitive risk from disclosing the information. In these cases, we encourage companies to still include these fields in their transparency reports to make clear to other stakeholders this information is not available and, when possible, to justify why this information is not provided. For example, OpenAI clearly indicates that it is not transparent on several matters (e.g. training data, model size) for GPT-4 as a matter of competition and safety (OpenAI, 2023).\n\nTo address the final 2 principles of contextualization and (measurement) methodology, we provide three examples that address indicators across the 3 domains (upstream, model, downstream).\n\nExample: Upstream environmental impact. Two of the transparency indicators we include address the direct environmental impact (due to electricity usage) and the broader environmental impact (e.g. due to water used to cool data centers) associated with building the foundation model. As an exemplar of how to provide this transparency, we consider the work of Luccioni et al. (2022) in estimating the environmental impact of training BLOOM (Le Scao et al., 2022) to underscore three matters. This work makes clear what is being reported (i.e. environmental impacts associated with the equipment manufacturing, model training, and model deployments phases) and what assumptions are made (e.g. how different greenhouse gas emissions are converted to tons of carbon dioxide). Beyond this conceptual clarity, the work provides methodological clarity (e.g. in how total emissions are computed as the sum of infrastructure, idle, and dynamic consumption), highlighting components neglected in other environmental accounting approaches (Patterson et al., 2021). Finally, since almost all assessments of environmental impact will hinge on underlying estimates (e.g. the carbon intensity of the energy grid), the reporting is clearly contextualized with the sourcing of this information (in this case to statistics provided by Aurora Energy Research on French carbon utilization).\n\nExample: Model evaluations. Several of the transparency indicators we include address model evaluations that span capabilities, limitations, risks, mitigations, trustworthiness, and efficiency. Unlike the environmental impact example, here we instead describe demonstrated issues and challenges in reporting evaluation results with the standard MMLU (Hendrycks et al., 2021) benchmark for language models. To ensure evaluation results are correctly interpreted, developers should clearly report the resources involved in adapting (e.g. prompting, fine-tuning) their foundation model to the evaluation. For example, Google reports the results for Gemini (Pichai and Hassabis, [n. d.]) on MMLU in direct comparison to GPT-4, obscuring that Gemini was prompted using 32 examples and chain-of-thought prompting whereas GPT-4 was prompted using 5 examples and standard in-context learning. Further, developers should specify lower-level details about model evaluations (e.g. the specific prompts used, the codebase and implementation for the evaluation). Fourrier et al. (2023) demonstrates that different implementations of MMLU can lead to noticeably different quantitative results, sometimes even changing the ranking of different models.\n\nExample: Downstream policy enforcement. Several of the transparency indicators correspond with transparency sought for content moderation on social media platforms. Namely, these are indicators on the usage policy for foundation model, the policy’s enforcement, the frequency of usage policy violations, the rate of accurate detection of these violations, and whether users are informed about and can appeal moderation decisions. For some of these indicators, producing the relevant information should be of marginal cost to foundation model developers, but we highlight that estimating the rate of usage policy violation is less straightforward. Calculating the prevalence of total usage policy violations is more involved than just reporting the number of detected usage policy violations, since many usage policy violations may go undetected. To address this issue, Narayanan and Kapoor (2023) provide guidance informed by social media practices. For example, social media companies sample posts uniformly at random to generate estimates of specific policy violations (e.g. hate speech) using human moderation. Foundation model developers could emulate this practice or use other sampling methods to provide better estimates of total violations and detected violations, which would also clarify significant gaps between the the number of total and detected violations.\n\n4. Policy Alignment\n\nThe information that developers can disclose via our transparency reports, in some cases, aligns with requirements by governments. We track 6 major policies (e.g. the EU AI Act, the US Executive Order on AI), identifying correspondences between our indicators and their requirements. Such alignment further incentivizes foundation model developers to report this information (e.g. when it is also required by law) and clarifies how different jurisdictions are prioritizing different types of transparency. However, the relatively low level of alignment between these policies and our indicators illustrates the lack of granularity in governments’ transparency requirements\n\n4.1. Tracked Policies\n\nWe consider 6 major policies (see Table 1) from Canada, the EU, the US, and the G7 that include transparency requirements for foundation model developers. Notably, the US White House voluntary commitments include a pledge that developers will release transparency reports for foundation models that ”include the safety evaluations conducted (including in areas such as dangerous capabilities, to the extent that these are responsible to publicly disclose), significant limitations in performance that have implications for the domains of appropriate use, discussion of the model’s effects on societal risks such as fairness and bias, and the results of adversarial testing conducted to evaluate the model’s fitness for deployment.” The G7 Hiroshima Process International Code of Conduct for Organizations Developing Advanced AI Systems includes similar provisions on transparency reports, including transparency regarding evaluations of risks to human rights.\n\nThe EU AI Act is a comprehensive regulation covering AI systems that was negotiated in December 2023 and that will be published in spring 2024. The AI Act creates a risk taxonomy that imposes requirements on providers of AI systems, prohibits certain use cases, and establishes an AI Office within the European Commission to oversee general-purpose AI systems among many other provisions. Implementation of the AI Act and its transparency requirements will depend significantly on national regulatory authorities within member states, which will be responsible for enforcing the AI Act within domestic legal regimes.\n\nThe US Executive Order on AI, published in October 2023, lays out US policy with respect to attracting AI talent, ensuring security of AI systems, and protecting privacy and civil rights. In addition to transparency requirements for some developers of ”dual-use foundation models,” the order requires that federal government agencies promote competition in the AI industry, establish procedures for the procurement of AI systems, and develop standards for best practices for AI safety. The order includes over 150 requirements for federal agencies to complete—nearly all of which must be implemented within one year—meaning that its effects are just beginning to take shape (Meinhardt et al., 2023).\n\nThe US AI Foundation Model Transparency Act is a piece of legislation introduced by Representatives Anna Eshoo and Don Beyer in December 2023. The bill is squarely focused on enhancing the transparency of the foundation model ecosystem, ranging from data used for training and inference to transparency standards for foundation model deployers. The US Federal Trade Commission would be tasked with developing and enforcing transparency requirements in consultation with the National Institute for Standards and Technology and the Office of Science and Technology Policy.\n\nThe US White House voluntary commitments are a list of eight commitments made by companies developing AI systems; the White House has announced two rounds of signatories to the commitments, the first by seven companies in July 2023 and the second by eight companies in September 2023. In addition to a commitment to publicly release transparency reports for foundation models, the commitments address red teaming, cybersecurity, watermarking, and bias. The commitments are not retroactive: they ”apply only to generative models that are overall more powerful than the current industry frontier” in the case of companies that signed in July, and ”they apply only to generative models that are overall more powerful than the current most advanced model produced by the company making the commitment” in the case of companies that signed in September. The US voluntary commitments are intended ”to remain in effect until regulations covering substantially the same issues come into force.”\n\nThe Canada Voluntary Code of Conduct on the Responsible Development and Management of Advanced Generative AI Systems, released in September 2023 by the ministry of Innovation, Science and Economic Development, also introduces a set of nonbinding commitments that has been endorsed by 22 organizations. The code of conduct includes specific measures related to different aspects of responsible development and deployment of foundation models, such as accountability, safety, fairness, human oversight, and robustness. It directs different measures toward developers and managers of generative AI systems, where managers are organizations that put a system into operation, control access, and conduct monitoring (e.g. developers are responsible for mitigating safety risks, while managers must clearly identify AI-generated content). Additionally, the code of conduct distinguishes between obligations of developers and managers of all advanced generative AI systems as opposed to those that are made available for public use. Similar to the US voluntary commitments, ”the code identifies measures that should be applied in advance of binding regulation pursuant to the Artificial Intelligence and Data Act by all firms developing or managing the operations of a generative AI system with general-purpose capabilities.”\n\nThe G7, which includes the US, Canada, and the EU as members, issued its International Code of Conduct for Organizations Developing Advanced AI Systems in October 2023 as part of the Hiroshima AI Process. The code of conduct includes provisions on transparency reporting as well as 10 measures related to data protection, risk management, and development of technical standards. While this code of conduct is voluntary and companies have not acceded to it as they have national-level commitments, it may be the basis for a future global agreement.\n\n4.2. Alignment between existing policies and Foundation Model Transparency Reports\n\nTo measure the alignment between our transparency reports and existing policy, we tag the transparency requirements in each policy and identify alignment with specific indicators in for every transparency requirement that corresponds with our transparency indicators (see Appendix B for further details). On average, the 6 policies share 10 transparency requirements with our 100 transparency indicators, and across all policies there are 43 transparency requirements shared with our transparency indicators (see Table 2). That is, there are 57 transparency indicators we include that are not included in any of these policies. Relatively few of these transparency requirements focus on the upstream resources required to build foundation models, such as data, labor, and compute. 3 of the 6 policies include no upstream requirements, though the US AI Foundation Model Transparency Act has 10 such requirements, including disclosure of data size, data sources, data augmentation, and personal information in the data. The EU AI Act had the most transparency requirements, including 12 requirements that no other policy contained such as the duration of model development, energy usage, model components, and the model license. The US Executive Order on AI had the fewest transparency requirements of all policies considered and, notably, was the sole policy to require only that companies disclose information to the government.\n\nCommon transparency requirements across policies included disclosing data sources (required by 3 policies), centralized model documentation (3), prohibited, restricted, and prohibited uses (3), whether a person is interacting with an AI system (3), documentation for responsible downstream use (3), a capabilities description (4), a risk description (4), evaluation of unintentional harm (4), evaluation of intentional harm (5), a limitations description (5), and a mitigations description (5). These commonalities show shared priorities, but also that current transparency requirements are often superficial; in particular, our indicators include not only descriptions of capabilities, limitations, risks, and mitigations, but also demonstrations and evaluations of each. In the upstream domain, the only consistent transparency requirement relates to data sources, with no policies including transparency requirements related to data labor and only one policy (the AI Act) with a requirement on technical methods.\n\nThere were also a handful of transparency requirements that are included in these policies that were not featured in the Foundation Model Transparency Index. For example, Canada’s code of conduct requires that developers ”maintain a database of reported incidents after deployment, and provide updates as needed to ensure effective mitigation measures.” It also requires that firms that manage the operations of generative AI systems ”share information and best practices on risk management with firms playing complementary roles in the ecosystem.” The EU AI Act contains a number of additional transparency requirements, ranging from the date the model was released to the maximum context window length, the rationale for key design choices, and adverse event reporting. We revisit adverse event reporting, which appears in both of these policies, in § 6.2\n\nOn the whole, transparency requirements in government policies lack specificity; they do not detail the precision to which developers must report quantitative information, establish standards for reporting evaluations, or account for differences across modalities. Foundation Model Transparency Reports may help augment vague government policies by sharpening what information foundation model developers provide to consumers, clients, downstream developers, deployers, and regulators.\n\n5. Example of Transparency Report Entries\n\nTo demonstrate how to construct a Foundation Model Transparency Report, we provide an example of transparency report entries in Appendix C. For brevity, here we describe how we assembled these examples and takeaways given current disclosure practices.\n\n5.1. Construction\n\nThe 2023 Foundation Model Transparency Index (FMTI) confirms that current transparency practices across the foundation model ecosystem are lackluster. Most major foundation model developers do not provide information on over half of our 100 indicators (Bommasani et al., 2023a). As a result, for the purposes of building an illustrative sample report, we provide examples of transparency report entries from 9 foundation model developers instead of reporting on the practices of a single developer. While in practice, a transparency report will correspond to the practices of a single developer (and be associated with a single model or model family), we nonetheless believe this amalgam serves a useful demonstration.\n\nTo create these examples, we consider the 10 foundation model developers scored in FMTI and the associated scores. For any indicator (82 of the 100) where at least one developer scores the point, we consider all developers that receive a point for that indicator. Of these developers, we select one of the developers whose practices best exemplify transparency for the indicator, with some consideration for selecting different developers to portray a variety of practices. Given the selected (indicator, developer) pairs, we then prepared the example entry in Appendix C that articulates what the developer discloses for the indicator (e.g. Meta’s disclosure of development duration for Llama 2, Inflection’s description of the limitations of Inflection-1). While in our judgment this report reflects some of the best existing practices for each indicator, we note that this should not be seen as the ceiling for transparency in many cases.\n\n5.2. Analysis\n\nOur examples of Foundation Model Transparency Report entries, in line with findings of the FMTI, implicitly denotes 18 indicators where no major developer is currently transparent (e.g. several labor-related indicators, several indicators on usage statistics and impact). Consequently, developers or others in the community that demonstrate how information regarding these 18 indicators should be disclosed would establish a meaningful precedent. Further, even for many of the 82 indicators where the report contains an entry, significantly more could be done to make this information useful and actionable. In many cases, the level of contextualization and methodological clarity could be specifically improved (e.g. regarding the aspects of model development that contributed to Meta’s measurement of duration, or specific limitations were identified by Inflection?).\n\nAt a more fine-grained level, certain indicators also reveal how foundation model developers conceptualize model development differently, and the community lacks a common conceptual framework. For example, while practices from Anthropic, Hugging Face/BigScience, Meta, and OpenAI are all included in the example report on the matters of data and labor, different developers describe their data pipelines in substantively different ways. In turn, articulating where human labor is involved and what data processing occurs through this pipeline may yield inconsistent answers across developers that may not be directly comparable. For many indicators in the report, it is unclear if the disclosed information is a partial or complete answer. For example, while policies from Anthropic, Google, Inflection and OpenAI are all included on the matters of terms of service and usage policy enforcement, in several cases it remains unclear whether they capture an exhaustive list of violative behavior, enforcement actions, and associated appeals/justifications. In fact, in some cases this information was only identified by triggering detected usage policy violations by Bommasani et al. (2023a), which brings into question the extent to which these usage policies are fully transparent at present.\n\n6. Related Work\n\nTransparency is a fundamental value with a significant history of study in AI (Gebru et al., 2021; Bender and Friedman, 2018; Mitchell et al., 2018; Raji and Buolamwini, 2019; Gray and Suri, 2019; Crawford, 2021; Vogus and Llansó, 2021; Keller, 2022; Bommasani et al., 2023b). Here we consider how our approach relates to other transparency methodologies in AI (namely model cards, data sheets, and ecosystem cards) and other reporting methodologies in society (namely financial reporting and adverse event reporting). While Foundation Model Transparency Reports draw greatest inspiration from social media transparency reports (§ 2), these other methodological approaches to transparency and reporting can help inform and complement more comprehensive transparency reporting.\n\n6.1. Transparency approaches in AI\n\nThe most common approach for improving in transparency in AI is model evaluations: these evaluations help to clarify model strengths and weaknesses, often for technical AI practitioners (Bommasani et al., 2023c). While evaluations can provide significant insight into a specific model, they are still limited in their ability to account for broader societal context (e.g. data, labor, downstream impact). In turn, documentation-based approaches to increasing transparency play a complementary role to evaluations, often providing legibility to stakeholders beyond technical AI practitioners. While model evaluations characterize a specific model in isolation, documentation situates model and system development in a broader context.\n\nDocumentation in AI was pioneered by data sheets (Gebru et al., 2018) and model cards (Mitchell et al., 2018) for data and models, respectively. These documentation frameworks enumerate a series of questions that an AI developer should answer, which are often fairly open-ended and unstructured in form. For example, Gebru et al. (2018) introduces three questions on the motivation for dataset creation: what was the purpose for dataset creation, who created the dataset (and, potentially on whose behalf), and who funded the dataset? These documentation approaches tend to be very comprehensive in their maximal instantiation, which means empirically there is significant heterogeneity in how different organizations produce data sheets or model cards, including which of the original questions posed by Gebru et al. (2018) and Mitchell et al. (2018) are (satisfactorily) addressed. For example, the Llama 2 model card contains most of the high-level categories specified in the original model cards paper, but several of the lower-level questions posed in the paper are not addressed. Another important example of documentation in AI are the reproducibility checklists required by conferences like NeurIPS and EMNLP, which are mandatory for all papers and include various transparency requirements related to training, licensing, and limitations (on Neural Information Processing Systems, 2022, 2023).\n\nMore recently, Bommasani et al. (2023b) introduced ecosystem cards as a documentation framework, which akin to our work specifically targets the foundation model setting. Three variants of the ecosystem card template exist for documenting datasets, foundation models, and applications/products respectively, with Bommasani et al. (2023b) emphasizing the importance of tracking dependency relationships between these different assets. In contrast to data sheets and model cards, which were principally envisioned as developer-driven forms of transparency, ecosystem cards can be created and maintained by other actors in the ecosystem.\n\nRelative to these documentation frameworks, Foundation Model Transparency Reports share common themes of organizing information and, in several instances, specific indicators. However, our transparency reports adopt the more comprehensive view of transparency put forth in the Foundation Model Transparency Index, spanning elements across the supply chain. Further, our transparency reports are closer in style to social media transparency reports, with a greater emphasis on more targeted informational queries rather than more open-ended questions found in data sheets and model cards. Our focus is on transparency that is relevant for public accountability and risk management in relation to (widely-deployed) foundation models, whereas many of these prior frameworks are aimed at AI researchers to promote better scientific practices.\n\n6.2. Reporting approaches in society\n\nIn mature industries, companies and organizations are often required to produce reports that document their operations (e.g. tax reporting, environmental reporting, product safety reporting). We consider US financial reporting as a horizontal practice spanning industries, as well as the US Food and Drug Administration’s (FDA) adverse event reporting system as a domain-specific practice. These reporting approaches, along with social media transparency reporting, provide additional references in envisioning, designing, and implementing transparency reporting for foundation models.\n\nFinancial reporting. In the United States, several overlapping reporting mechanisms provide transparency on the financial ecosystems. Financial reporting is overseen by the Securities and Exchange Commission (SEC), whose mandate is to inform and protect investors, regulate securities markets, and enforce federal securities law (Secutiries and Commision, 2024a). The SEC requires that publicly traded companies release significant information about their finances through annual reports (Form 10-K), quarterly reports (Form 10-Q), and current reports (Form 8-K) (Secutiries and Commision, 2024c). The 10-K and 10-Q comprehensively characterize a company’s financial health (e.g. information on business activities, risk factors, assets, liabilities) (Secutiries and Commision, 2024b), whereas the 8-K is required to notify the SEC, and later the public, of sudden events such as bankruptcy or acquisition of significant assets (Secutiries and Commision, 2024c). Standards also heavily influence financial reporting. The Generally Accepted Accounting Principles determine accounting standards accepted by the SEC and function as the default for American companies (Secutiries and Commision, 2024d), with the International Financial Reporting Standards functioning as their international counterpart. Additionally, the non-profit Public Company Accounting Oversight Board (PCAOB) develops auditing standards for public companies and SEC-registered brokers and dealers. These standards are important as they ensure that business audits are standardized, high quality, and trustworthy (Secutiries and Commision, 2024e).\n\nThe history of US financial regulation has several instructive lessons for transparency reporting for foundation models. Many American financial reporting mechanisms came out of regulatory measures intended to address issues of low transparency and their subsequent negative effects. The SEC was created in 1934 by Franklin Delano Roosevelt in the aftermath of the 1932 Pechora Commission, which highlighted how abusive practices in the financial industry contributed to the 1929 stock market crash (Perino, 2010). Likewise, the PCAOB was created as part of the 2002 Sarbanes-Oxley Act, which was itself a response to major corporate accounting scandals like Enron, WorldCom and Tyco. Beyond creating the PCAOB, the Sarbanes-Oxley Act instrumentally reformed corporate governance and financial disclosure practices in the US, mandating greater financial disclosure, stricter internal corporate control, and greater corporate responsibility over financial reporting. Therefore there is a well-established precedent of government intervention as a means of ensuring greater transparency in industries that are deemed to be insufficiently transparent.\n\nFDA adverse event reporting. While transparency often aims to provide baseline understanding and information, sometimes further transparency is necessary in light of unexpected circumstances. In the context of drugs, the FDA implements an adverse event reporting system (FAERS) as a “database that contains information on adverse event and medication error reports submitted to FDA. The database is designed to support the FDA’s post-marketing safety surveillance program for drug and therapeutic biologic products” (Food and Administration, 2023, 2021). As of September 2023, there are more than 27 million reports, with the FDA receiving more than one million reports annually since 1969 (Food and Administration, 2023); the FAERS data is made available to wide range of stakeholders (e.g. consumers, healthcare professionals, researchers).\n\nReports are voluntarily submitted by healthcare providers (e.g. physicians, pharmacists, nurses) and consumers (e.g. patients, family members, lawyers); by law, product manufacturers must relay these reports to the FDA (Food and Administration, 2018). Reports are circulated and may trigger subsequent actions (e.g. evaluation by clinical reviewers in the Center for Drug Evaluation and Research) to survey post-market drug safety. Overall, the open availability of FAERS data improves awareness of drug adverse events, though it may be prone to improper interpretation without appropriate consideration for statistical validity (Kumar, 2018). In comparison to the recurring, comprehensive, and proactive nature of social media transparency reports or financial reports, adverse event reporting systems provide more targeted transparency when interventions are (potentially) urgent. While our focus in designing transparency reports for foundation models largely emulates the former approaches, we highlight adverse event reporting as playing a potentially complementary role. In particular, we imagine that as specific harms of foundation models are documented, similar adverse event reporting systems (or the reuse of pre-existing systems) will be necessary (Guha et al., 2023; NAIAC, 2023).\n\n7. Discussion\n\nTransparency functions as an instrument for advancing other objectives (e.g. greater public accountability and improved risk management). We aim to inculcate robust norms and industry standards around transparency while foundation models are still (relatively) nascent, in conjunction with government-driven disclosure requirements. Transparency is not a monolith: different aspects of transparency are more relevant for certain societal objectives and stakeholder groups than others. While in some cases the benefits of transparency arise from a single developer being more transparent, for others what is required is broader transparency from many developers to surface general trends. We step through several of our transparency indicators to articulate our theory of change regarding how increased transparency would help improve the societal impact of foundation models.\n\nGreater transparency on data directly informs demographic biases in foundation model behavior (Abid et al., 2021; Luccioni et al., 2023; Bianchi et al., 2023) and copyright litigation surrounding model training data (e.g. NYT, 2024). Transparency on labor practices enables awareness of, and collective action to address, labor conditions (Williams et al., 2022). Transparency on compute usage clarifies the costs of building frontier foundation models (Anderljung et al., 2023) and the viability of policies like licensing that restrict compute access (Kapoor and Narayanan, 2023). Evaluations help concretize model capabilities (Wei et al., 2022; Bubeck et al., 2023) and risks (Bender et al., 2021; Weidinger et al., 2022), sharpening collective understanding (Bommasani et al., 2023c). And transparency on usage statistics as well as affected market sectors and geographies directly informs understanding of economic impact, innovation, and the concentration of power (Vipra and Korinek, 2023; Bommasani et al., 2023b; UK CMA, 2023).\n\nWhile we advocate for greater transparency via transparency reports, we recognize that transparency initiatives have been subject to critique (Ananny and Crawford, 2018; Bates et al., 2023). Though some of these critiques regarding performative transparency on self-selected matters are mitigated by our approach (Zalnieriute, 2021), other critiques about the limits of transparency to bring about substantive change persist (Hartzog, 2023). We see improved transparency as a natural initial target given the demonstrated opacity of the foundation model ecosystem (Perrigo, 2022; Hao and Seetharaman, 2023; Bommasani et al., 2023a); other changes will need to follow to achieve better societal outcomes. Further, social media transparency reporting demonstrates that transparency reporting can be costly, requiring substantial investments from platforms. At present, we do not aim to factor in reporting costs, though we encourage developers to transparently discuss costs to allow policymakers and other stakeholders to better argue for cost-benefit trade-offs for transparency. For similar reasons, we also highlight the potential for Foundation Model Transparency Reports to reduce overall compliance costs for developers operating across multiple jurisdictions by reducing duplicative effort (§ 4).\n\nBroad consensus exists for improved transparency in the foundation model ecosystem. The history of social media illustrates both the harms of pervasive opacity and the potential for institutionalized transparency. We envisage Foundation Model Transparency Reports as the structured interface for communicating information from foundation model developers to the public to meet the needs of diverse stakeholders.\n\nAcknowledgements.\n\nWe thank Dan Ho, Daphne Keller, and Nate Persily for feedback and discussions that informed this work. This work was supported in part by the AI2050 program at Schmidt Futures (Grant G-22-63429).\n\nReferences\n\n(1)\n\ngoo (2010) 2010. Greater transparency around government requests. https://googleblog.blogspot.com/2010/04/greater-transparency-around-government.html.\n\nacl (2010) 2010. Internal Report Finds Flagrant National Security Letter Abuse By FBI. https://www.aclu.org/press-releases/internal-report-finds-flagrant-national-security-letter-abuse-fbi.\n\nets (2014) 2014. 2014 Transparency Report. https://extfiles.etsy.com/Press/reports/Etsy_TransparencyReport_2014.pdf.\n\nwfa (2020) 2020. WFA and platforms make major progress to address harmful content. https://wfanet.org/knowledge/item/2020/09/23/WFA-and-platforms-make-major-progress-to-address-harmful-content.\n\nAbdo et al. (2022) Alex Abdo, Ramya Krishnan, Stephanie Krent, Evan Welber Falcón, and Andrew Keane Woods. 2022. A Safe Harbor for Platform Research. https://knightcolumbia.org/content/a-safe-harbor-for-platform-research.\n\nAbid et al. (2021) Abubakar Abid, Maheen Farooqi, and James Zou. 2021. Persistent anti-muslim bias in large language models. arXiv preprint arXiv:2101.05783 (2021).\n\nAccess Now (2023) Access Now. 2023. Transparency Reporting Index. https://www.accessnow.org/campaign/transparency-reporting-index/.\n\nAnanny and Crawford (2018) Mike Ananny and Kate Crawford. 2018. Seeing without knowing: Limitations of the transparency ideal and its application to algorithmic accountability. New Media & Society 20, 3 (2018), 973–989. https://doi.org/10.1177/1461444816676645 arXiv:https://doi.org/10.1177/1461444816676645\n\nAnderljung et al. (2023) Markus Anderljung, Joslyn Barnhart, Anton Korinek, Jade Leung, Cullen O’Keefe, Jess Whittlestone, Shahar Avin, Miles Brundage, Justin Bullock, Duncan Cass-Beggs, Ben Chang, Tantum Collins, Tim Fist, Gillian Hadfield, Alan Hayes, Lewis Ho, Sara Hooker, Eric Horvitz, Noam Kolt, Jonas Schuett, Yonadav Shavit, Divya Siddarth, Robert Trager, and Kevin Wolf. 2023. Frontier AI Regulation: Managing Emerging Risks to Public Safety. arXiv:2307.03718 [cs.CY]\n\nAspen Institute (2021) Aspen Institute. 2021. Commission on Information Disorder Final Report. https://www.aspeninstitute.org/wp-content/uploads/2021/11/Aspen-Institute_Commission-on-Information-Disorder_Final-Report.pdf.\n\nBankston et al. (2017) Kevin Bankston, Ross Schulman, and Liz Woolery. 2017. Case Study #3: Transparency Reporting. https://www.newamerica.org/in-depth/getting-internet-companies-do-right-thing/case-study-3-transparency-reporting/.\n\nBates et al. (2023) J. Bates, H. Kennedy, and I. et al. Medina Perea. 2023. Socially meaningful transparency in data-based systems: reflections and proposals from practice. Journal of Documentation (2023). https://doi.org/10.1108/JD-01-2023-0006\n\nBender and Friedman (2018) Emily M Bender and Batya Friedman. 2018. Data statements for natural language processing: Toward mitigating system bias and enabling better science. Transactions of the Association for Computational Linguistics (TACL) 6 (2018), 587–604.\n\nBender et al. (2021) Emily M Bender, Timnit Gebru, Angelina McMillan-Major, and Shmargaret Shmitchell. 2021. On the Dangers of Stochastic Parrots: Can Language Models Be Too Big?. In Proceedings of the 2021 ACM Conference on Fairness, Accountability, and Transparency. 610–623.\n\nBianchi et al. (2023) Federico Bianchi, Pratyusha Kalluri, Esin Durmus, Faisal Ladhak, Myra Cheng, Debora Nozza, Tatsunori Hashimoto, Dan Jurafsky, James Zou, and Aylin Caliskan. 2023. Easily Accessible Text-to-Image Generation Amplifies Demographic Stereotypes at Large Scale. In Proceedings of the 2023 ACM Conference on Fairness, Accountability, and Transparency (Chicago, IL, USA) (FAccT ’23). Association for Computing Machinery, New York, NY, USA, 1493–1504. https://doi.org/10.1145/3593013.3594095\n\nBirchall (2021) Clare Birchall. 2021. Radical secrecy: The ends of transparency in datafied America. Vol. 60. U of Minnesota Press.\n\nBommasani et al. (2021) Rishi Bommasani, Drew A. Hudson, Ehsan Adeli, Russ Altman, Simran Arora, Sydney von Arx, Michael S. Bernstein, Jeannette Bohg, Antoine Bosselut, Emma Brunskill, Erik Brynjolfsson, Shyamal Buch, Dallas Card, Rodrigo Castellon, Niladri Chatterji, Annie Chen, Kathleen Creel, Jared Quincy Davis, Dorottya Demszky, Chris Donahue, Moussa Doumbouya, Esin Durmus, Stefano Ermon, John Etchemendy, Kawin Ethayarajh, Li Fei-Fei, Chelsea Finn, Trevor Gale, Lauren Gillespie, Karan Goel, Noah Goodman, Shelby Grossman, Neel Guha, Tatsunori Hashimoto, Peter Henderson, John Hewitt, Daniel E. Ho, Jenny Hong, Kyle Hsu, Jing Huang, Thomas Icard, Saahil Jain, Dan Jurafsky, Pratyusha Kalluri, Siddharth Karamcheti, Geoff Keeling, Fereshte Khani, Omar Khattab, Pang Wei Koh, Mark Krass, Ranjay Krishna, Rohith Kuditipudi, Ananya Kumar, Faisal Ladhak, Mina Lee, Tony Lee, Jure Leskovec, Isabelle Levent, Xiang Lisa Li, Xuechen Li, Tengyu Ma, Ali Malik, Christopher D. Manning, Suvir Mirchandani, Eric Mitchell, Zanele Munyikwa, Suraj Nair, Avanika Narayan, Deepak Narayanan, Ben Newman, Allen Nie, Juan Carlos Niebles, Hamed Nilforoshan, Julian Nyarko, Giray Ogut, Laurel Orr, Isabel Papadimitriou, Joon Sung Park, Chris Piech, Eva Portelance, Christopher Potts, Aditi Raghunathan, Rob Reich, Hongyu Ren, Frieda Rong, Yusuf Roohani, Camilo Ruiz, Jack Ryan, Christopher Ré, Dorsa Sadigh, Shiori Sagawa, Keshav Santhanam, Andy Shih, Krishnan Srinivasan, Alex Tamkin, Rohan Taori, Armin W. Thomas, Florian Tramèr, Rose E. Wang, William Wang, Bohan Wu, Jiajun Wu, Yuhuai Wu, Sang Michael Xie, Michihiro Yasunaga, Jiaxuan You, Matei Zaharia, Michael Zhang, Tianyi Zhang, Xikun Zhang, Yuhui Zhang, Lucia Zheng, Kaitlyn Zhou, and Percy Liang. 2021. On the Opportunities and Risks of Foundation Models. arXiv preprint arXiv:2108.07258 (2021).\n\nBommasani et al. (2023a) Rishi Bommasani, Kevin Klyman, Shayne Longpre, Sayash Kapoor, Nestor Maslej, Betty Xiong, Daniel Zhang, and Percy Liang. 2023a. The Foundation Model Transparency Index. arXiv:2310.12941 [cs.LG]\n\nBommasani et al. (2023b) Rishi Bommasani, Dilara Soylu, Thomas Liao, Kathleen A. Creel, and Percy Liang. 2023b. Ecosystem Graphs: The Social Footprint of Foundation Models. ArXiv abs/2303.15772 (2023). https://api.semanticscholar.org/CorpusID:257771875\n\nBommasani et al. (2023c) Rishi Bommasani, Daniel Zhang, Tony Lee, and Percy Liang. 2023c. Improving Transparency in AI Language Models: A Holistic Evaluation. Foundation Model Issue Brief Series (2023). https://hai.stanford.edu/foundation-model-issue-brief-series\n\nBoyd (2016) Danah Boyd. 2016. Algorithmic Accountability and Transparency. Open Transcripts. http://opentranscripts.org/transcript/danah-boyd-algorithmic-accountability-transparency/ Presented by danah boyd in Algorithmic Accountability and Transparency in the Digital Economy.\n\nBubeck et al. (2023) Sébastien Bubeck, Varun Chandrasekaran, Ronen Eldan, Johannes Gehrke, Eric Horvitz, Ece Kamar, Peter Lee, Yin Tat Lee, Yuanzhi Li, Scott Lundberg, Harsha Nori, Hamid Palangi, Marco Tulio Ribeiro, and Yi Zhang. 2023. Sparks of Artificial General Intelligence: Early experiments with GPT-4. arXiv:2303.12712 [cs.CL]\n\nBudish et al. (2016) Ryan Budish, Liz Woolery, and Kevin Bankston. 2016. The Transparency Reporting Toolkit: Survey & Best Practice Memos for Reporting on U.S. Government Requests for User Information. https://www.newamerica.org/oti/policy-papers/the-transparency-reporting-toolkit/.\n\nCommission (2022) European Commission. 2022. The Digital Services Act: ensuring a safe and accountable online environment. European Commission (2022). https://commission.europa.eu/strategy-and-policy/priorities-2019-2024/europe-fit-digital-age/digital-services-act-ensuring-safe-and-accountable-online-environment_en\n\nCongress (2023) United States Congress. 2023. AI Foundation Model Transparency Act. https://beyer.house.gov/uploadedfiles/ai_foundation_model_transparency_act_text_118.pdf\n\nCrawford (2021) Kate Crawford. 2021. The atlas of AI: Power, politics, and the planetary costs of artificial intelligence. Yale University Press.\n\nCrocker et al. (2019) Andrew Crocker, Gennie Gebhart, Aaron Mackey, Kurt Opsahl, Hayley Tsukayama, Jamie Lee Williams, and Jillian C. York. 2019. Who Has Your Back? https://www.eff.org/files/2019/06/11/whyb_2019_report.pdf\n\nEdelson et al. (2021) Laura Edelson, Jason Chuang, Erika Franklin Fowler, Michael Franz, and Travis N. Ridout. 2021. Universal Digital Ad Transparency. In TPRC49: The 49th Research Conference on Communication, Information and Internet Policy. Available at SSRN: https://ssrn.com/abstract=3898214 or http://dx.doi.org/10.2139/ssrn.3898214.\n\nEuropean Commission (2023) European Commission. 2023. Commission launches public consultation on the Implementing Regulation on transparency reporting under the DSA. https://digital-strategy.ec.europa.eu/en/news/commission-launches-public-consultation-implementing-regulation-transparency-reporting-under-dsa\n\nEuropean Council (2024) European Council. 2024. Proposal for a Regulation of the European Parliament and of the Council laying down harmonised rules on artificial intelligence (Artificial Intelligence Act) and amending certain Union legislative acts. https://data.consilium.europa.eu/doc/document/ST-5662-2024-INIT/en/pdf\n\nFacebook (2023) Facebook. 2023. Facebook Transparent Reports. https://transparency.fb.com/reports/\n\nFood and Administration (2018) U.S. Food and Drug Administration. 2018. Questions and Answers on FDA’s Adverse Event Reporting System (FAERS). https://www.fda.gov/drugs/surveillance/questions-and-answers-fdas-adverse-event-reporting-system-faers.\n\nFood and Administration (2021) U.S. Food and Drug Administration. 2021. FDA Adverse Event Reporting System (FAERS): Latest Quartely Data Files. https://catalog.data.gov/dataset/fda-adverse-event-reporting-system-faers-latest-quartely-data-files.\n\nFood and Administration (2023) U.S. Food and Drug Administration. 2023. FDA Adverse Event Reporting System (FAERS) Public Dashboard. https://www.fda.gov/drugs/questions-and-answers-fdas-adverse-event-reporting-system-faers/fda-adverse-event-reporting-system-faers-public-dashboard.\n\nFourrier et al. (2023) Clémentine Fourrier, Nathan Habib, Julien Launay, and Thomas Wolf. 2023. What’s going on with the Open LLM Leaderboard? https://huggingface.co/blog/evaluating-mmlu-leaderboard\n\nGebru et al. (2021) Timnit Gebru, Jamie Morgenstern, Briana Vecchione, Jennifer Wortman Vaughan, Hanna Wallach, Hal Daumé Iii, and Kate Crawford. 2021. Datasheets for datasets. Commun. ACM 64, 12 (2021), 86–92.\n\nGebru et al. (2018) Timnit Gebru, Jamie Morgenstern, Briana Vecchione, Jennifer Wortman Vaughan, Hanna Wallach, Hal Daumé Ill, and Kate Crawford. 2018. Datasheets for Datasets. arXiv preprint arXiv:1803.09010 (2018).\n\nGhosh and Faxon (2023) Ritwick Ghosh and Hilary Oliva Faxon. 2023. Smart corruption: Satirical strategies for gaming accountability. Big Data & Society 10, 1 (2023), 20539517231164119. https://doi.org/10.1177/20539517231164119 arXiv:https://doi.org/10.1177/20539517231164119\n\nGorwa and Ash (2020) Robert Gorwa and Timothy Garton Ash. 2020. Democratic Transparency in the Platform Society. Cambridge University Press, 286–312.\n\nGray and Suri (2019) Mary L Gray and Siddharth Suri. 2019. Ghost work: How to stop Silicon Valley from building a new global underclass. Eamon Dolan Books.\n\nGreenwald (2013) Glenn Greenwald. 2013. NSA collecting phone records of millions of Verizon customers daily. https://www.theguardian.com/world/2013/jun/06/nsa-phone-records-verizon-court-order.\n\nGroup of Seven (2023) Group of Seven. 2023. Hiroshima Process International Code of Conduct for Organizations Developing Advanced AI Syste. https://www.mofa.go.jp/files/100573473.pdf\n\nGuha et al. (2023) Neel Guha, Christie M. Lawrence, Lindsey A. Gailmard, Kit T. Rodolfa, Faiz Surani, Rishi Bommasani, Inioluwa Deborah Raji, Mariano-Florentino Cuéllar, Colleen Honigsberg, Percy Liang, and Daniel E. Ho. 2023. AI Regulation Has Its Own Alignment Problem: The Technical and Institutional Feasibility of Disclosure, Registration, Licensing, and Auditing. George Washington Law Review, Symposium on Legally Disruptive Emerging Technologies (2023).\n\nHan (2015) Byung-Chul Han. 2015. The transparency society. Stanford University Press.\n\nHao and Seetharaman (2023) Karen Hao and Deepa Seetharaman. 2023. Cleaning Up ChatGPT Takes Heavy Toll on Human Workers. The Wall Street Journal (24 July 2023). https://www.wsj.com/articles/chatgpt-openai-content-abusive-sexually-explicit-harassment-kenya-workers-on-human-workers-cf191483 Photographs by Natalia Jidovanu.\n\nHartzog (2023) Woodrow Hartzog. 2023. Oversight of A.I.: Legislating on Artificial Intelligence. Prepared Testimony and Statement for the Record before the U.S. Senate Committee on the Judiciary, Subcommittee on Privacy, Technology, and the Law. https://www.judiciary.senate.gov/imo/media/doc/2023-09-12_pm_-_testimony_-_hartzog.pdf\n\nHendrycks et al. (2021) Dan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn Song, and Jacob Steinhardt. 2021. Measuring massive multitask language understanding. In International Conference on Learning Representations (ICLR).\n\nHouse (2023) The White House. 2023. Ensuring Safe, Secure, and Trustworthy AI. https://www.whitehouse.gov/wp-content/uploads/2023/07/Ensuring-Safe-Secure-and-Trustworthy-AI.pdf\n\nInnovation, Science and Economic Development Canada (2023) Innovation, Science and Economic Development Canada. 2023. Voluntary Code of Conduct on the Responsible Development and Management of Advanced Generative AI Systems. https://ised-isde.canada.ca/site/ised/en/voluntary-code-conduct-responsible-development-and-management-advanced-generative-ai-systems\n\nKapoor and Narayanan (2023) Sayash Kapoor and Arvind Narayanan. 2023. Licensing is neither feasible nor effective for addressing AI risks. https://www.aisnakeoil.com/p/licensing-is-neither-feasible-nor\n\nKeller (2021) Daphne Keller. 2021. Some Humility About Transparency. https://cyberlaw.stanford.edu/blog/2021/03/some-humility-about-transparency.\n\nKeller (2022) Daphne Keller. 2022. Hearing on Platform Transparency: Understanding the Impact of Social Media. Technical Report. United States Senate Committee on the Judiciary, Subcommittee on Privacy, Technology and the Law. https://www.judiciary.senate.gov/imo/media/doc/Keller%20Testimony1.pdf Statement of Daphne Keller, Stanford University Cyber Policy Center.\n\nKessel (2016) Jeremy Kessel. 2016. Advancing #transparency with more insightful data. https://blog.twitter.com/official/en_us/a/2016/advancing-transparency-with-more-insightful-data.html.\n\nKumar (2018) Atul Kumar. 2018. The Newly Available FAERS Public Dashboard: Implications for Health Care Professionals. Issue 2.\n\nLazar (2023) Seth Lazar. 2023. Governing the Algorithmic City. Tanner Lectures (2023). https://write.as/sethlazar/\n\nLe Scao et al. (2022) Teven Le Scao, Angela Fan, Christopher Akiki, Ellie Pavlick, Suzana Ilić, Daniel Hesslow, Roman Castagné, Alexandra Sasha Luccioni, François Yvon, Matthias Gallé, Jonathan Tow, Alexander M. Rush, Stella Biderman, Albert Webson, Pawan Sasanka Ammanamanchi, Thomas Wang, Benoît Sagot, Niklas Muennighoff, Albert Villanova del Moral, Olatunji Ruwase, Rachel Bawden, Stas Bekman, Angelina McMillan-Major, Iz Beltagy, Huu Nguyen, Lucile Saulnier, Samson Tan, Pedro Ortiz Suarez, Victor Sanh, Hugo Laurençon, Yacine Jernite, Julien Launay, Margaret Mitchell, Colin Raffel, Aaron Gokaslan, Adi Simhi, Aitor Soroa, Alham Fikri Aji, Amit Alfassy, Anna Rogers, Ariel Kreisberg Nitzav, Canwen Xu, Chenghao Mou, Chris Emezue, Christopher Klamm, Colin Leong, Daniel van Strien, David Ifeoluwa Adelani, Dragomir Radev, Eduardo González Ponferrada, Efrat Levkovizh, Ethan Kim, Eyal Bar Natan, Francesco De Toni, Gérard Dupont, Germán Kruszewski, Giada Pistilli, Hady Elsahar, Hamza Benyamina, Hieu Tran, Ian Yu, Idris Abdulmumin, Isaac Johnson, Itziar Gonzalez-Dios, Javier de la Rosa, Jenny Chim, Jesse Dodge, Jian Zhu, Jonathan Chang, Jörg Frohberg, Joseph Tobing, Joydeep Bhattacharjee, Khalid Almubarak, Kimbo Chen, Kyle Lo, Leandro Von Werra, Leon Weber, Long Phan, Loubna Ben allal, Ludovic Tanguy, Manan Dey, Manuel Romero Muñoz, Maraim Masoud, María Grandury, Mario Šaško, Max Huang, Maximin Coavoux, Mayank Singh, Mike Tian-Jian Jiang, Minh Chien Vu, Mohammad A. Jauhar, Mustafa Ghaleb, Nishant Subramani, Nora Kassner, Nurulaqilla Khamis, Olivier Nguyen, Omar Espejel, Ona de Gibert, Paulo Villegas, Peter Henderson, Pierre Colombo, Priscilla Amuok, Quentin Lhoest, Rheza Harliman, Rishi Bommasani, Roberto Luis López, Rui Ribeiro, Salomey Osei, Sampo Pyysalo, Sebastian Nagel, Shamik Bose, Shamsuddeen Hassan Muhammad, Shanya Sharma, Shayne Longpre, Somaieh Nikpoor, Stanislav Silberberg, Suhas Pai, Sydney Zink, Tiago Timponi Torrent, Timo Schick, Tristan Thrush, Valentin Danchev, Vassilina Nikoulina, Veronika Laippala, Violette Lepercq, Vrinda Prabhu, Zaid Alyafeai, Zeerak Talat, Arun Raja, Benjamin Heinzerling, Chenglei Si, Elizabeth Salesky, Sabrina J. Mielke, Wilson Y. Lee, Abheesht Sharma, Andrea Santilli, Antoine Chaffin, Arnaud Stiegler, Debajyoti Datta, Eliza Szczechla, Gunjan Chhablani, Han Wang, Harshit Pandey, Hendrik Strobelt, Jason Alan Fries, Jos Rozen, Leo Gao, Lintang Sutawika, M Saiful Bari, Maged S. Al-shaibani, Matteo Manica, Nihal Nayak, Ryan Teehan, Samuel Albanie, Sheng Shen, Srulik Ben-David, Stephen H. Bach, Taewoon Kim, Tali Bers, Thibault Fevry, Trishala Neeraj, Urmish Thakker, Vikas Raunak, Xiangru Tang, Zheng-Xin Yong, Zhiqing Sun, Shaked Brody, Yallow Uri, Hadar Tojarieh, Adam Roberts, Hyung Won Chung, Jaesung Tae, Jason Phang, Ofir Press, Conglong Li, Deepak Narayanan, Hatim Bourfoune, Jared Casper, Jeff Rasley, Max Ryabinin, Mayank Mishra, Minjia Zhang, Mohammad Shoeybi, Myriam Peyrounette, Nicolas Patry, Nouamane Tazi, Omar Sanseviero, Patrick von Platen, Pierre Cornette, Pierre François Lavallée, Rémi Lacroix, Samyam Rajbhandari, Sanchit Gandhi, Shaden Smith, Stéphane Requena, Suraj Patil, Tim Dettmers, Ahmed Baruwa, Amanpreet Singh, Anastasia Cheveleva, Anne-Laure Ligozat, Arjun Subramonian, Aurélie Névéol, Charles Lovering, Dan Garrette, Deepak Tunuguntla, Ehud Reiter, Ekaterina Taktasheva, Ekaterina Voloshina, Eli Bogdanov, Genta Indra Winata, Hailey Schoelkopf, Jan-Christoph Kalo, Jekaterina Novikova, Jessica Zosa Forde, Jordan Clive, Jungo Kasai, Ken Kawamura, Liam Hazan, Marine Carpuat, Miruna Clinciu, Najoung Kim, Newton Cheng, Oleg Serikov, Omer Antverg, Oskar van der Wal, Rui Zhang, Ruochen Zhang, Sebastian Gehrmann, Shani Pais, Tatiana Shavrina, Thomas Scialom, Tian Yun, Tomasz Limisiewicz, Verena Rieser, Vitaly Protasov, Vladislav Mikhailov, Yada Pruksachatkun, Yonatan Belinkov, Zachary Bamberger, Zdeněk Kasner, Alice Rueda, Amanda Pestana, Amir Feizpour, Ammar Khan, Amy Faranak, Ana Santos, Anthony Hevia, Antigona Unldreaj, Arash Aghagol, Arezoo Abdollahi, Aycha Tammour, Azadeh HajiHosseini, Bahareh Behroozi, Benjamin Ajibade, Bharat Saxena, Carlos Muñoz Ferrandis, Danish Contractor, David Lansky, Davis David, Douwe Kiela, Duong A. Nguyen, Edward Tan, Emi Baylor, Ezinwanne Ozoani, Fatima Mirza, Frankline Ononiwu, Habib Rezanejad, Hessie Jones, Indrani Bhattacharya, Irene Solaiman, Irina Sedenko, Isar Nejadgholi, Jesse Passmore, Josh Seltzer, Julio Bonis Sanz, Karen Fort, Livia Dutra, Mairon Samagaio, Maraim Elbadri, Margot Mieskes, Marissa Gerchick, Martha Akinlolu, Michael McKenna, Mike Qiu, Muhammed Ghauri, Mykola Burynok, Nafis Abrar, Nazneen Rajani, Nour Elkott, Nour Fahmy, Olanrewaju Samuel, Ran An, Rasmus Kromann, Ryan Hao, Samira Alizadeh, Sarmad Shubber, Silas Wang, Sourav Roy, Sylvain Viguier, Thanh Le, Tobi Oyebade, Trieu Le, Yoyo Yang, Zach Nguyen, Abhinav Ramesh Kashyap, Alfredo Palasciano, Alison Callahan, Anima Shukla, Antonio Miranda-Escalada, Ayush Singh, Benjamin Beilharz, Bo Wang, Caio Brito, Chenxi Zhou, Chirag Jain, Chuxin Xu, Clémentine Fourrier, Daniel León Periñán, Daniel Molano, Dian Yu, Enrique Manjavacas, Fabio Barth, Florian Fuhrimann, Gabriel Altay, Giyaseddin Bayrak, Gully Burns, Helena U. Vrabec, Imane Bello, Ishani Dash, Jihyun Kang, John Giorgi, Jonas Golde, Jose David Posada, Karthik Rangasai Sivaraman, Lokesh Bulchandani, Lu Liu, Luisa Shinzato, Madeleine Hahn de Bykhovetz, Maiko Takeuchi, Marc Pàmies, Maria A Castillo, Marianna Nezhurina, Mario Sänger, Matthias Samwald, Michael Cullan, Michael Weinberg, Michiel De Wolf, Mina Mihaljcic, Minna Liu, Moritz Freidank, Myungsun Kang, Natasha Seelam, Nathan Dahlberg, Nicholas Michio Broad, Nikolaus Muellner, Pascale Fung, Patrick Haller, Ramya Chandrasekhar, Renata Eisenberg, Robert Martin, Rodrigo Canalli, Rosaline Su, Ruisi Su, Samuel Cahyawijaya, Samuele Garda, Shlok S Deshmukh, Shubhanshu Mishra, Sid Kiblawi, Simon Ott, Sinee Sang-aroonsiri, Srishti Kumar, Stefan Schweter, Sushil Bharati, Tanmay Laud, Théo Gigant, Tomoya Kainuma, Wojciech Kusa, Yanis Labrak, Yash Shailesh Bajaj, Yash Venkatraman, Yifan Xu, Yingxin Xu, Yu Xu, Zhe Tan, Zhongli Xie, Zifan Ye, Mathilde Bras, Younes Belkada, and Thomas Wolf. 2022. BLOOM: A 176B-Parameter Open-Access Multilingual Language Model. (2022). https://doi.org/10.48550/ARXIV.2211.05100\n\nLlansó and Vogus (2021) Emma Llansó and Caitlin Vogus. 2021. Transparency Reports. https://cdt.org/wp-content/uploads/2022/01/2021-12-20-FX-Transparency-Framework-brief-Transparency-Reports-final.pdf.\n\nLuccioni et al. (2023) Alexandra Sasha Luccioni, Christopher Akiki, Margaret Mitchell, and Yacine Jernite. 2023. Stable Bias: Analyzing Societal Representations in Diffusion Models. arXiv:2303.11408 [cs.CY]\n\nLuccioni et al. (2022) Alexandra Sasha Luccioni, Sylvain Viguier, and Anne-Laure Ligozat. 2022. Estimating the Carbon Footprint of BLOOM, a 176B Parameter Language Model. ArXiv abs/2211.02001 (2022). https://api.semanticscholar.org/CorpusID:253265387\n\nMacKinnon et al. (2019) Rebecca MacKinnon, Amy Brouillette, Lisa Gutermuth, Laura Reed, Nathalie Maréchal, Veszna Wessenauer, Afef Abrougui, Sam Cabral, Ilja Sperling, Zak Rogoff, and Eeva Moore. 2019. 2019 RDR Corporate Accountability Index. https://rankingdigitalrights.org/index2019/assets/static/download/RDRindex2019report.pdf\n\nMeinhardt et al. (2023) Caroline Meinhardt, Christie M. Lawrence, Lindsey A. Gailmard, Daniel Zhang, Rishi Bommasani, Rohini Kosoglu, Peter Henderson, Russell Wald, and Daniel E. Ho. 2023. By the Numbers: Tracking The AI Executive Order. https://hai.stanford.edu/news/numbers-tracking-ai-executive-order\n\nMiller (2023) Gabby Miller. 2023. Tracking the First Digital Services Act Transparency Reports. https://www.techpolicy.press/tracking-the-first-digital-services-act-transparency-reports/.\n\nMitchell et al. (2018) Margaret Mitchell, Simone Wu, Andrew Zaldivar, Parker Barnes, Lucy Vasserman, Ben Hutchinson, Elena Spitzer, Inioluwa Deborah Raji, and Timnit Gebru. 2018. Model Cards for Model Reporting. Proceedings of the Conference on Fairness, Accountability, and Transparency (2018).\n\nMittelstadt (2019) Brent Mittelstadt. 2019. Principles alone cannot guarantee ethical AI. Nature Machine Intelligence 1, 11 (November 2019), 501–507. https://doi.org/10.1038/s42256-019-0114-4\n\nNAIAC (2023) NAIAC. 2023. RECOMMENDATION: Improve Monitoring of Emerging Risks from AI through Adverse Event Reporting. https://ai.gov/wp-content/uploads/2023/12/Recommendation_Improve-Monitoring-of-Emerging-Risks-from-AI-through-Adverse-Event-Reporting.pdf\n\nNarayanan and Kapoor (2023) Arvind Narayanan and Sayash Kapoor. 2023. Generative AI companies must publish transparency reports. https://knightcolumbia.org/blog/generative-ai-companies-must-publish-transparency-reports\n\nNYT (2024) NYT. 2024. THE NEW YORK TIMES COMPANY v. MICROSOFT CORPORATION, OPENAI, INC., OPENAI LP, OPENAI GP, LLC, OPENAI, LLC, OPENAI OPCO LLC, OPENAI GLOBAL LLC, OAI CORPORATION, LLC, and OPENAI HOLDINGS, LLC. https://nytco-assets.nytimes.com/2023/12/NYT_Complaint_Dec2023.pdf\n\non Neural Information Processing Systems (2022) Conference on Neural Information Processing Systems. 2022. NeurIPS 2022 Paper Checklist Guidelines. https://neurips.cc/Conferences/2022/PaperInformation/PaperChecklist\n\non Neural Information Processing Systems (2023) Conference on Neural Information Processing Systems. 2023. Call for Main Conference Papers. https://2023.emnlp.org/calls/main_conference_papers/\n\nOpenAI (2023) OpenAI. 2023. GPT-4 Technical Report. arXiv:2303.08774 [cs.CL]\n\nPatterson et al. (2021) David Patterson, Joseph Gonzalez, Quoc Le, Chen Liang, Lluis-Miquel Munguia, Daniel Rothchild, David So, Maud Texier, and Jeff Dean. 2021. Carbon emissions and large neural network training. arXiv preprint arXiv:2104.10350 (2021).\n\nPerino (2010) M. Perino. 2010. The Hellhound of Wall Street: How Ferdinand Pecora’s Investigation of the Great Crash Forever Changed American Finance. Penguin Publishing Group. https://books.google.com/books?id=VJZPEAAAQBAJ\n\nPerrigo (2022) Billy Perrigo. 2022. Exclusive: OpenAI Used Kenyan Workers on Less Than 2 Per Hour to Make ChatGPT Less Toxic. Time (2022). https://time.com/6247678/openai-chatgpt-kenya-workers\n\nPichai and Hassabis ([n. d.]) Sundar Pichai and Demis Hassabis. [n. d.]. Introducing Gemini: our largest and most capable AI model.\n\nRaji and Buolamwini (2019) Inioluwa Deborah Raji and Joy Buolamwini. 2019. Actionable Auditing: Investigating the Impact of Publicly Naming Biased Performance Results of Commercial AI Products. In Proceedings of the 2019 AAAI/ACM Conference on AI, Ethics, and Society (Honolulu, HI, USA) (AIES ’19). Association for Computing Machinery, New York, NY, USA, 429–435. https://doi.org/10.1145/3306618.3314244\n\nRydzak (2023) Jan Rydzak. 2023. The Stalled Machines of Transparency Reporting. https://carnegieendowment.org/2023/11/29/stalled-machines-of-transparency-reporting-pub-91085.\n\nSanta Clara Principles (2023) Santa Clara Principles. 2023. The Santa Clara Principles: On Transparency and Accountability in Content Moderation. https://santaclaraprinciples.org/.\n\nSchatz (2006) Amy Schatz. 2006. Tech Firms Defend China Web Policies. https://www.wsj.com/articles/SB114002162437674809.\n\nSchneider et al. (2023) Jens-Peter Schneider, Kester Siegrist, and Simon Oles. 2023. Collaborative Governance of the EU Digital Single Market established by the Digital Services Act. University of Luxembourg Law Research Paper 2023, 09 (4 September 2023). https://ssrn.com/abstract=4561010\n\nSecutiries and Commision (2024a) U.S. Secutiries and Exchange Commision. 2024a. About the SEC. https://www.sec.gov/strategic-plan/about.\n\nSecutiries and Commision (2024b) U.S. Secutiries and Exchange Commision. 2024b. Form 10-K. https://www.investor.gov/introduction-investing/investing-basics/glossary/form-10-k.\n\nSecutiries and Commision (2024c) U.S. Secutiries and Exchange Commision. 2024c. Form 8-K. https://www.investor.gov/introduction-investing/investing-basics/glossary/form-8-k.\n\nSecutiries and Commision (2024d) U.S. Secutiries and Exchange Commision. 2024d. Generally Accepted Accounting Principles (GAAP). https://www.investor.gov/introduction-investing/investing-basics/glossary/generally-accepted-accounting-principles-gaap.\n\nSecutiries and Commision (2024e) U.S. Secutiries and Exchange Commision. 2024e. Generally Accepted Accounting Principles (GAAP). https://www.investor.gov/introduction-investing/investing-basics/glossary/generally-accepted-accounting-principles-gaap.\n\nStoughton and Rosenzweig (2022) Katie Stoughton and Paul Rosenzweig. 2022. Toward Greater Content Moderation Transparency Reporting. Lawfare. https://www.lawfaremedia.org/article/toward-greater-content-moderation-transparency-reporting\n\nTrust and Safety Professional Association (2023) Trust and Safety Professional Association. 2023. Transparency Reporting. https://www.tspa.org/curriculum/ts-fundamentals/transparency-report/.\n\nUK CMA (2023) UK CMA. 2023. AI Foundation Models: Initial Report. https://assets.publishing.service.gov.uk/media/65081d3aa41cc300145612c0/Full_report_.pdf\n\nUnited States Executive Office of the President (2023) United States Executive Office of the President. 2023. Executive Order on Safe, Secure, and Trustworthy Development and Use of Artificial Intelligence. https://www.federalregister.gov/documents/2023/11/01/2023-24283/safe-secure-and-trustworthy-development-and-use-of-artificial-intelligence\n\nUrman and Makhortykh (2023) Aleksandra Urman and Mykola Makhortykh. 2023. How transparent are transparency reports? Comparative analysis of transparency reporting across online platforms. Telecommunications Policy 47, 3 (2023), 102477. https://doi.org/10.1016/j.telpol.2022.102477\n\nValerie C. Brannon and Victoria L. Killion and Whitney K. Novak and L. Paige Whitaker (2023) Valerie C. Brannon and Victoria L. Killion and Whitney K. Novak and L. Paige Whitaker. 2023. First Amendment Limitations on Disclosure Requirements. https://crsreports.congress.gov/product/pdf/IF/IF12388\n\nVermeulen (2021) Mathias Vermeulen. 2021. The Keys to the Kingdom. https://knightcolumbia.org/content/the-keys-to-the-kingdom\n\nVipra and Korinek (2023) Jai Vipra and Anton Korinek. 2023. Market concentration implications of foundation models: The Invisible Hand of ChatGPT. The Brookings Institution (2023). https://www.brookings.edu/articles/market-concentration-implications-of-foundation-models-the-invisible-hand-of-chatgpt\n\nVogus and Llansó (2021) Caitlin Vogus and Emma Llansó. 2021. Making Transparency Meaningful: A Framework for Policymakers. Center for Democracy and Technology (2021). https://cdt.org/insights/report-making-transparency-meaningful-a-framework-for-policymakers/\n\nWei et al. (2022) Jason Wei, Yi Tay, Rishi Bommasani, Colin Raffel, Barret Zoph, Sebastian Borgeaud, Dani Yogatama, Maarten Bosma, Denny Zhou, Donald Metzler, Ed H. Chi, Tatsunori Hashimoto, Oriol Vinyals, Percy Liang, Jeff Dean, and William Fedus. 2022. Emergent Abilities of Large Language Models. Transactions on Machine Learning Research (2022). https://openreview.net/forum?id=yzkSU5zdwD Survey Certification.\n\nWeidinger et al. (2022) Laura Weidinger, Jonathan Uesato, Maribeth Rauh, Conor Griffin, Po-Sen Huang, John Mellor, Amelia Glaese, Myra Cheng, Borja Balle, Atoosa Kasirzadeh, Courtney Biles, Sasha Brown, Zac Kenton, Will Hawkins, Tom Stepleton, Abeba Birhane, Lisa Anne Hendricks, Laura Rimell, William Isaac, Julia Haas, Sean Legassick, Geoffrey Irving, and Iason Gabriel. 2022. Taxonomy of Risks Posed by Language Models. In 2022 ACM Conference on Fairness, Accountability, and Transparency (Seoul, Republic of Korea) (FAccT ’22). Association for Computing Machinery, New York, NY, USA, 214–229. https://doi.org/10.1145/3531146.3533088\n\nWilliams et al. (2022) Adrienne Williams, Milagros Miceli, and Timnit Gebru. 2022. De Anima: On the Soul. https://www.noemamag.com/the-exploited-labor-behind-artificial-intelligence/\n\nX (2023) X. 2023. An update on Twitter Transparency Reporting. https://blog.twitter.com/en_us/topics/company/2023/an-update-on-twitter-transparency-reporting.\n\nZalnieriute (2021) Monika Zalnieriute. 2021. “Transparency-Washing” in the Digital Age : A Corporate Agenda of Procedural Fetishism. Technical Report. http://hdl.handle.net/11159/468588\n\nZhu (2015) Jenny Zhu. 2015. A perfect EFF score! We’re proud to have your back. https://wordpress.com/blog/2015/06/17/a-perfect-eff-score-were-proud-to-have-your-back/.\n\nAppendix A Indicators\n\nWe use the 100 transparency indicators from the Foundation Model Transparency Index (Bommasani et al., 2023a) as the basis for our transparency reports. These indicators are listed by name in Figure 1 with definitions available at https://github.com/stanford-crfm/fmti/blob/main/fmti-indicators.csv.\n\nAppendix B Policy Alignment\n\nWe track transparency requirements for foundation model developers in 6 government policies from Canada, the EU, the US, and the G7. For each of our 100 transparency indicators, we indicate if the associated policy has a transparency requirement that addresses the same matter as our indicator. We consider a transparency requirement to be aligned with one of our transparency indicators if (i) the requirement addresses the same issue area as the indicator, (ii) the requirement is directed to developers of foundation models, and (iii) the policy explicitly requires transparency or information sharing in this area. We used a single annotator for each policy, who was responsible for comparing each transparency requirement in the policy to each transparency indicator in the Foundation Model Transparency Index. There are several important caveats regarding the extent of this overlap. Whereas foundation model transparency reports are intended to be documents that are publicly available, transparency requirements in government policies may only require foundation model developers to disclose information to the government or to other firms (e.g. downstream developers). Moreover, the voluntary government policies we consider impose high-level transparency requirements, meaning it is difficult to discern whether they correspond precisely to a narrow indicator of transparency. The resulting 100×61006100\\times 6100 × 6 matrix of (indicator, policy) pairs is presented in LABEL:tab:alignment-matrix.\n\n{longtblr}\n\n[ caption = Policy Alignment Matrix for Transparency Indicators, label = tab:alignment-matrix, ] colspec = —Xcccccc—, rowhead = 1, hlines, roweven = gray9, row1 = olive9,\n\n& US WHVC US EO US FMTA EU AIA G7 CoC CA CoC\n\nData size ✗ ✗ ✓ ✓ ✗ ✗\n\nData sources ✗ ✗ ✓ ✓ ✗ ✓\n\nData creators ✗ ✗ ✓ ✗ ✗ ✗\n\nData source selection ✗ ✗ ✗ ✓ ✗ ✗\n\nData curation ✗ ✗ ✓ ✓ ✗ ✗\n\nData augmentation ✗ ✗ ✓ ✗ ✗ ✗\n\nHarmful data filtration ✗ ✗ ✓ ✓ ✗ ✗\n\nCopyrighted data ✗ ✗ ✓ ✗ ✗ ✗\n\nData license ✗ ✗ ✓ ✗ ✗ ✗\n\nPersonal information in data ✗ ✗ ✓ ✗ ✗ ✗\n\nUse of human labor ✗ ✗ ✗ ✗ ✗ ✗\n\nEmployment of data laborers ✗ ✗ ✗ ✗ ✗ ✗\n\nGeographic distribution of data laborers ✗ ✗ ✗ ✗ ✗ ✗\n\nWages ✗ ✗ ✗ ✗ ✗ ✗\n\nInstructions for creating data ✗ ✗ ✗ ✗ ✗ ✗\n\nLabor protections ✗ ✗ ✗ ✗ ✗ ✗\n\nThird party partners ✗ ✗ ✗ ✗ ✗ ✗\n\nQueryable external data access ✗ ✗ ✗ ✗ ✗ ✗\n\nDirect external data access ✗ ✗ ✗ ✗ ✗ ✗\n\nCompute usage ✗ ✗ ✓ ✓ ✗ ✗\n\nDevelopment duration ✗ ✗ ✗ ✓ ✗ ✗\n\nCompute hardware ✗ ✗ ✗ ✗ ✗ ✗\n\nHardware owner ✗ ✗ ✗ ✗ ✗ ✗\n\nEnergy usage ✗ ✗ ✗ ✓ ✗ ✗\n\nCarbon emissions ✗ ✗ ✗ ✗ ✗ ✗\n\nBroader environmental impact ✗ ✗ ✗ ✗ ✗ ✗\n\nModel stages ✗ ✗ ✗ ✗ ✗ ✗\n\nModel objectives ✗ ✗ ✗ ✓ ✗ ✗\n\nCore frameworks ✗ ✗ ✗ ✗ ✗ ✗\n\nAdditional dependencies ✗ ✗ ✗ ✗ ✗ ✗\n\nMitigations for privacy ✗ ✗ ✗ ✗ ✗ ✗\n\nMitigations for copyright ✗ ✗ ✗ ✗ ✗ ✗\n\nInput modality ✗ ✗ ✗ ✓ ✗ ✗\n\nOutput modality ✗ ✗ ✗ ✓ ✗ ✗\n\nModel components ✗ ✗ ✗ ✓ ✗ ✗\n\nModel size ✗ ✗ ✗ ✓ ✗ ✗\n\nModel architecture ✗ ✗ ✗ ✓ ✗ ✗\n\nCentralized model documentation ✓ ✗ ✗ ✓ ✓ ✗\n\nExternal model access protocol ✗ ✓ ✗ ✗ ✗ ✗\n\nBlackbox external model access ✗ ✗ ✗ ✗ ✗ ✗\n\nFull external model access ✗ ✗ ✗ ✗ ✗ ✗\n\nCapabilities description ✓ ✗ ✗ ✓ ✓ ✓\n\nCapabilities demonstration ✗ ✗ ✗ ✗ ✗ ✗\n\nEvaluation of capabilities ✗ ✗ ✗ ✓ ✗ ✗\n\nExternal reproducibility of capabilities evaluation ✗ ✗ ✗ ✗ ✗ ✗\n\nThird party capabilities evaluation ✗ ✗ ✗ ✗ ✗ ✗\n\nLimitations description ✓ ✗ ✓ ✓ ✓ ✓\n\nLimitations demonstration ✗ ✗ ✗ ✗ ✗ ✗\n\nThird party evaluation of limitations ✗ ✗ ✗ ✗ ✗ ✗\n\nRisks description ✓ ✗ ✓ ✓ ✓ ✗\n\nRisks demonstration ✗ ✗ ✗ ✗ ✗ ✗\n\nUnintentional harm evaluation ✓ ✓ ✓ ✗ ✓ ✗\n\nExternal reproducibility of unintentional harm evaluation ✗ ✗ ✗ ✗ ✗ ✗\n\nIntentional harm evaluation ✓ ✓ ✓ ✓ ✓ ✗\n\nExternal reproducibility of intentional harm evaluation ✗ ✗ ✗ ✗ ✗ ✗\n\nThird party risks evaluation ✗ ✗ ✓ ✓ ✗ ✗\n\nMitigations description ✗ ✓ ✓ ✓ ✓ ✓\n\nMitigations demonstration ✗ ✗ ✗ ✗ ✗ ✗\n\nMitigations evaluation ✗ ✗ ✗ ✗ ✗ ✗\n\nExternal reproducibility of mitigations evaluation ✗ ✗ ✗ ✗ ✗ ✗\n\nThird party mitigations evaluation ✗ ✗ ✗ ✗ ✗ ✗\n\nTrustworthiness evaluation ✗ ✗ ✗ ✗ ✗ ✗\n\nExternal reproducibility of trustworthiness evaluation ✗ ✗ ✗ ✗ ✗ ✗\n\nInference duration evaluation ✗ ✗ ✗ ✗ ✗ ✗\n\nInference compute evaluation ✗ ✗ ✓ ✗ ✗ ✗\n\nRelease decision-making ✗ ✗ ✗ ✗ ✗ ✗\n\nRelease process ✗ ✗ ✗ ✗ ✗ ✗\n\nDistribution channels ✗ ✓ ✗ ✓ ✗ ✗\n\nProducts and services ✗ ✗ ✗ ✗ ✗ ✗\n\nDetection of machine-generated content ✗ ✗ ✗ ✓ ✗ ✓\n\nModel License ✗ ✗ ✗ ✓ ✗ ✗\n\nTerms of service ✗ ✗ ✗ ✗ ✗ ✗\n\nPermitted and prohibited users ✗ ✗ ✗ ✗ ✗ ✗\n\nPermitted, restricted, and prohibited uses ✓ ✗ ✗ ✓ ✓ ✗\n\nUsage policy enforcement ✗ ✗ ✗ ✗ ✗ ✗\n\nJustification for enforcement action ✗ ✗ ✗ ✗ ✗ ✗\n\nUsage policy violation appeals mechanism ✗ ✗ ✗ ✗ ✗ ✗\n\nPermitted, restricted, and prohibited model behaviors ✗ ✗ ✗ ✗ ✗ ✗\n\nModel behavior policy enforcement ✗ ✗ ✗ ✗ ✗ ✗\n\nInteroperability of usage and model behavior policies ✗ ✗ ✗ ✗ ✗ ✗\n\nUser interaction with AI system ✗ ✗ ✗ ✓ ✓ ✓\n\nUsage disclaimers ✗ ✗ ✗ ✗ ✗ ✗\n\nUser data protection policy ✗ ✗ ✓ ✗ ✓ ✗\n\nPermitted and prohibited use of user data ✗ ✗ ✗ ✗ ✗ ✗\n\nUsage data access protocol ✗ ✗ ✗ ✗ ✗ ✗\n\nVersioning protocol ✗ ✗ ✓ ✗ ✗ ✗\n\nChange log ✗ ✗ ✓ ✗ ✗ ✗\n\nDeprecation policy ✗ ✗ ✗ ✗ ✗ ✗\n\nFeedback mechanism ✗ ✗ ✗ ✗ ✗ ✓\n\nFeedback summary ✗ ✗ ✗ ✗ ✗ ✗\n\nGovernment inquiries ✗ ✗ ✗ ✗ ✗ ✗\n\nMonitoring mechanism ✗ ✗ ✗ ✗ ✗ ✓\n\nDownstream applications ✗ ✗ ✗ ✗ ✗ ✗\n\nAffected market sectors ✗ ✗ ✗ ✗ ✗ ✗\n\nAffected individuals ✗ ✗ ✗ ✗ ✗ ✗\n\nUsage reports ✗ ✗ ✗ ✓ ✗ ✗\n\nGeographic statistics ✗ ✗ ✗ ✗ ✗ ✗\n\nRedress mechanism ✗ ✗ ✗ ✗ ✗ ✗\n\nCentralized documentation for downstream use ✗ ✗ ✗ ✓ ✓ ✗\n\nDocumentation for responsible downstream use ✗ ✗ ✗ ✓ ✓ ✓\n\nTotals 7 5 20 30 12 9\n\nAppendix C Example of Transparency Report Entries\n\nTo demonstrate how a transparency report may be prepared, we provide the following examples of transparency report entries. Given the poor transparency documented in the foundation model ecosystem at present (Bommasani et al., 2023a), we construct this document by stitching together practices across several major foundation model developers. In doing so, our objective is to highlight a larger range of practices to give greater guidance on the basis of this example. Further, since we constructed these entries given public information as in (Bommasani et al., 2023a), we specifically highlight that the extent to which the information is contextualized and the methodology is clear can be significantly improved. (For some indicators, we defer to other materials because the associated information is quite lengthy/cumbersome to provide here.)\n\n{longtblr}\n\n[ caption = Example of a Foundation Model Transparency Report, label = tab:example, ] colspec = —XXX—, rowhead = 1, hlines, roweven = gray9, row1 = olive9, Indicator Developer, Model Value\n\nData size Hugging Face/BigScience, BLOOMZ 363B tokens\n\nData sources Hugging Face/BigScience, BLOOMZ ROOTS and xP3\n\nData source selection Hugging Face/BigScience, BLOOMZ See the ROOTS paper for details on source selection and the BLOOMZ paper for details on source selection for xP3\n\nData curation Hugging Face/BigScience, BLOOMZ See the ROOTS paper for details on data curation and the BLOOMZ paper for details on data curation for xP3\n\nData augmentation Hugging Face/BigScience, BLOOMZ See the BLOOMZ paper for details on how P3 was augmented to produce xP3\n\nHarmful data filtration Hugging Face/BigScience, BLOOMZ Illegal content is filtered from LAION-5B using a CLIP-based filter; offensive examples are tagged rather than filtered using QF16 and a new sexualized content classifier, both derived from CLIP embeddings; the subset of LAION-5B that is used is further filtered using LAION’s NSFW detector with pu⁢n⁢s⁢a⁢f⁢esubscript𝑝𝑢𝑛𝑠𝑎𝑓𝑒p_{unsafe}italic_p start_POSTSUBSCRIPT italic_u italic_n italic_s italic_a italic_f italic_e end_POSTSUBSCRIPT = 0.1\n\nUse of human labor Meta, Llama 2 See the Llama 2 technical report for details on the "
    }
}
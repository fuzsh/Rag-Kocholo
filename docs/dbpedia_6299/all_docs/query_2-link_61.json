{
    "id": "dbpedia_6299_2",
    "rank": 61,
    "data": {
        "url": "https://stats.stackexchange.com/questions/78255/how-to-resolve-simpsons-paradox",
        "read_more_link": "",
        "language": "en",
        "title": "How to resolve Simpson's paradox?",
        "top_image": "https://cdn.sstatic.net/Sites/stats/Img/apple-touch-icon@2.png?v=344f57aa10cc",
        "meta_img": "https://cdn.sstatic.net/Sites/stats/Img/apple-touch-icon@2.png?v=344f57aa10cc",
        "images": [
            "https://cdn.sstatic.net/Sites/stats/Img/logo.svg?v=60d6be2c448d",
            "https://i.sstatic.net/c4m7X.png",
            "https://www.gravatar.com/avatar/405fa83edf86f248782789f09898bfaf?s=64&d=identicon&r=PG",
            "https://i.sstatic.net/oLbIt.jpg?s=64",
            "https://www.gravatar.com/avatar/a007be5a61f6aa8f3e85ae2fc18dd66e?s=64&d=identicon&r=PG",
            "https://i.sstatic.net/TWeCs.png?s=64",
            "https://i.sstatic.net/Rl5IH.jpg?s=64",
            "https://www.gravatar.com/avatar/129a2fc7b220162ecf0b8572343ce505?s=64&d=identicon&r=PG",
            "https://www.gravatar.com/avatar/fd3079dd056e89f516bda6d8de136ca0?s=64&d=identicon&r=PG",
            "https://stats.stackexchange.com/posts/78255/ivc/5f59?prg=11cac2c2-270c-4aa0-adce-3789ce664159"
        ],
        "movies": [],
        "keywords": [],
        "meta_keywords": [
            ""
        ],
        "tags": null,
        "authors": [],
        "publish_date": "2013-12-02T01:39:04",
        "summary": "",
        "meta_description": "Simpson's paradox is a classic puzzle discussed in introductory statistics courses worldwide. However, my course was content to simply note that a problem existed and did not provide a solution. I ...",
        "meta_lang": "en",
        "meta_favicon": "https://cdn.sstatic.net/Sites/stats/Img/favicon.ico?v=8f7a5a991257",
        "meta_site_name": "Cross Validated",
        "canonical_link": "https://stats.stackexchange.com/questions/78255/how-to-resolve-simpsons-paradox",
        "text": "In your question, you state that you don't know what \"causal Bayesian networks\" and \"back door tests\" are.\n\nSuppose you have a causal Bayesian network. That is, a directed acyclic graph whose nodes represent propositions and whose directed edges represent potential causal relationships. You may have many such networks for each of your hypotheses. There are three ways to make a compelling argument about the strength or existence of an edge $A \\stackrel?\\rightarrow B$.\n\nThe easiest way is an intervention. This is what the other answers are suggesting when they say that \"proper randomization\" will fix the problem. You randomly force $A$ to have different values and you measure $B$. If you can do that, you're done, but you can't always do that. In your example, it may be unethical to give people ineffective treatments to deadly diseases, or they may be have some say in their treatment, e.g., they may choose the less harsh (treatment B) when their kidney stones are small and less painful.\n\nThe second way is the front door method. You want to show that $A$ acts on $B$ via $C$, i.e., $A\\rightarrow C \\rightarrow B$. If you assume that $C$ is potentially caused by $A$ but has no other causes, and you can measure that $C$ is correlated with $A$, and $B$ is correlated with $C$, then you can conclude evidence must be flowing via $C$. The original example: $A$ is smoking, $B$ is cancer, $C$ is tar accumulation. Tar can only come from smoking, and it correlates with both smoking and cancer. Therefore, smoking causes cancer via tar (though there could be other causal paths that mitigate this effect).\n\nThe third way is the back door method. You want to show that $A$ and $B$ aren't correlated because of a \"back door\", e.g. common cause, i.e., $A \\leftarrow D \\rightarrow B$. Since you have assumed a causal model, you merely need to block the all of the paths (by observing variables and conditioning on them) that evidence can flow up from $A$ and down to $B$. It's a bit tricky to block these paths, but Pearl gives a clear algorithm that lets you know which variables you have to observe to block these paths.\n\ngung is right that with good randomization, confounders won't matter. Since we're assuming that intervening at the the hypothetical cause (treatment) is not allowed, any common cause between the hypothetical cause (treatment) and effect (survival), such as age or kidney stone size will be a confounder. The solution is to take the right measurements to block all of the back doors. For further reading see:\n\nPearl, Judea. \"Causal diagrams for empirical research.\" Biometrika 82.4 (1995): 669-688.\n\nTo apply this to your problem, let us first draw the causal graph. (Treatment-preceding) kidney stone size $X$ and treatment type $Y$ are both causes of success $Z$. $X$ may be a cause of $Y$ if other doctors are assigning tratment based on kidney stone size. Clearly there are no other causal relationships between $X$,$Y$, and $Z$. $Y$ comes after $X$ so it cannot be its cause. Similarly $Z$ comes after $X$ and $Y$.\n\nSince $X$ is a common cause, it should be measured. It is up to the experimenter to determine the universe of variables and potential causal relationships. For every experiment, the experimenter measures the necessary \"back door variables\" and then calculates the marginal probability distribution of treatment success for each configuration of variables. For a new patient, you measure the variables and follow the treatment indicated by the marginal distribution. If you can't measure everything or you don't have a lot of data but know something about the architecture of the relationships, you can do \"belief propagation\" (Bayesian inference) on the network.\n\nI have a prior answer that discusses Simpson's paradox here: Basic Simpson's paradox. It may help you to read that to better understand the phenomenon.\n\nIn short, Simpson's paradox occurs because of confounding. In your example, the treatment is confounded* with the kind of kidney stones each patient had. We know from the full table of results presented that treatment A is always better. Thus, a doctor should choose treatment A. The only reason treatment B looks better in the aggregate is that it was given more often to patients with the less severe condition, whereas treatment A was given to patients with the more severe condition. Nonetheless, treatment A performed better with both conditions. As a doctor, you don't care about the fact that in the past the worse treatment was given to patients who had the lesser condition, you only care about the patient before you, and if you want that patient to improve, you will provide them with the best treatment available.\n\n*Note that the point of running experiments, and randomizing treatments, is to create a situation in which the treatments are not confounded. If the study in question was an experiment, I would say that the randomization process failed to create equitable groups, although it may well have been an observational study--I don't know.\n\nDo you want the solution to the one example or the paradox in general? There is none for the latter because the paradox can arise for more than one reason and needs to be assessed on a case by case basis.\n\nThe paradox is primarily problematic when reporting summary data and is critical in training individuals how to analyze and report data. We don't want researchers reporting summary statistics that hide or obfuscate patterns in the data or data analysts failing to recognize what the real pattern in the data is. No solution was given because there is no one solution.\n\nIn this particular case the doctor with the table would clearly always pick A and ignore the summary line. It makes no difference if they know the size of the stone or not. If someone analyzing the data had only reported the summary lines presented for A and B then there'd be an issue because the data the doctor received wouldn't reflect reality. In this case they probably should have also left the last line off of the table since it's only correct under one interpretation of what the summary statistic should be (there are two possible). Leaving the reader to interpret the individual cells would generally have produced the correct result.\n\n(Your copious comments seem to suggest you're most concerned about unequal N issues and Simpson is broader than that so I'm reluctant to dwell on the unequal N issue further. Perhaps ask a more targeted question. Furthermore, you seem to think I am advocating a normalization conclusion. I am not. I am arguing that you need to consider that the summary statistic is relatively arbitrarily selected and that selection by some analyst gave rise to the paradox. I'm further arguing that you look at the cells you have.)"
    }
}
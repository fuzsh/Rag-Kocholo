{
    "id": "dbpedia_4954_3",
    "rank": 7,
    "data": {
        "url": "https://en.wikipedia.org/wiki/Allan_variance",
        "read_more_link": "",
        "language": "en",
        "title": "Allan variance",
        "top_image": "https://upload.wikimedia.org/wikipedia/commons/thumb/d/dd/AllanDeviation.svg/1200px-AllanDeviation.svg.png",
        "meta_img": "https://upload.wikimedia.org/wikipedia/commons/thumb/d/dd/AllanDeviation.svg/1200px-AllanDeviation.svg.png",
        "images": [
            "https://en.wikipedia.org/static/images/icons/wikipedia.png",
            "https://en.wikipedia.org/static/images/mobile/copyright/wikipedia-wordmark-en.svg",
            "https://en.wikipedia.org/static/images/mobile/copyright/wikipedia-tagline-en.svg",
            "https://upload.wikimedia.org/wikipedia/commons/thumb/d/dd/AllanDeviation.svg/280px-AllanDeviation.svg.png",
            "https://wikimedia.org/api/rest_v1/media/math/render/svg/71ebe307f320d69e748fefc6b1d8bd07efcb139d",
            "https://wikimedia.org/api/rest_v1/media/math/render/svg/34c2ef96e929cfe2abdfd005db7bcad78277f1f2",
            "https://wikimedia.org/api/rest_v1/media/math/render/svg/38a7dcde9730ef0853809fefc18d88771f95206c",
            "https://wikimedia.org/api/rest_v1/media/math/render/svg/7f2c985062f06d4f3f0c84a4b137d73c8e0f7d71",
            "https://wikimedia.org/api/rest_v1/media/math/render/svg/bd02eb87acb002c54f4b8f68caef69f6b5cf2324",
            "https://upload.wikimedia.org/wikipedia/commons/thumb/a/aa/AllanDeviationExample.gif/280px-AllanDeviationExample.gif",
            "https://upload.wikimedia.org/wikipedia/commons/thumb/3/32/5_regimes_of_Allan_variance_as_a_function_of_averaging_time.png/280px-5_regimes_of_Allan_variance_as_a_function_of_averaging_time.png",
            "https://wikimedia.org/api/rest_v1/media/math/render/svg/41de32c7ddd993adccdf7ffd7ba1f51f83c4b35e",
            "https://wikimedia.org/api/rest_v1/media/math/render/svg/0579b990f43f424f98d7419693d51872b98d69ae",
            "https://wikimedia.org/api/rest_v1/media/math/render/svg/f2f8ccb124de7ce122716844304829e6c49d17df",
            "https://wikimedia.org/api/rest_v1/media/math/render/svg/12342ee9ee910f843912939bdfeb0da0261617f7",
            "https://wikimedia.org/api/rest_v1/media/math/render/svg/bf699879bfab937e6a9cfd916d6c35c4762db240",
            "https://wikimedia.org/api/rest_v1/media/math/render/svg/986f26cd9c7cc8c05468aeb28469d62c9ee2a388",
            "https://wikimedia.org/api/rest_v1/media/math/render/svg/8353d8b93665e95eb0fd86d5d36cb8fe16de6ac6",
            "https://wikimedia.org/api/rest_v1/media/math/render/svg/80af8fc8e510f53085afda989e55d52fe223e87d",
            "https://wikimedia.org/api/rest_v1/media/math/render/svg/8353d8b93665e95eb0fd86d5d36cb8fe16de6ac6",
            "https://wikimedia.org/api/rest_v1/media/math/render/svg/38a7dcde9730ef0853809fefc18d88771f95206c",
            "https://wikimedia.org/api/rest_v1/media/math/render/svg/38a7dcde9730ef0853809fefc18d88771f95206c",
            "https://wikimedia.org/api/rest_v1/media/math/render/svg/38a7dcde9730ef0853809fefc18d88771f95206c",
            "https://wikimedia.org/api/rest_v1/media/math/render/svg/a601995d55609f2d9f5e233e36fbe9ea26011b3b",
            "https://wikimedia.org/api/rest_v1/media/math/render/svg/56dad457e274b970f5d98b9dc40bef7f895c7f6f",
            "https://wikimedia.org/api/rest_v1/media/math/render/svg/1d5decd66ecfb656f7847164b5e88a15909a9aa3",
            "https://wikimedia.org/api/rest_v1/media/math/render/svg/9e66b599725696925357b5be61b0b7282f9d6bcc",
            "https://wikimedia.org/api/rest_v1/media/math/render/svg/b04b46696dbf79b1570f6e6a05bed908312720c8",
            "https://wikimedia.org/api/rest_v1/media/math/render/svg/d54c275db3a1e620737b58e143b0818107fa5f5c",
            "https://wikimedia.org/api/rest_v1/media/math/render/svg/46f632b36e55fb643e2f2cf9d5105986a59d9629",
            "https://wikimedia.org/api/rest_v1/media/math/render/svg/5a5bbdb5a0bdaf49bcc610504f55a016cdd59f82",
            "https://wikimedia.org/api/rest_v1/media/math/render/svg/f82cade9898ced02fdd08712e5f0c0151758a0dd",
            "https://wikimedia.org/api/rest_v1/media/math/render/svg/bcfe4a579a459e218695a7e8bc1ed72617c9f47c",
            "https://wikimedia.org/api/rest_v1/media/math/render/svg/bec90379029906ed54b94f1d3caa2aa90c40758b",
            "https://wikimedia.org/api/rest_v1/media/math/render/svg/65658b7b223af9e1acc877d848888ecdb4466560",
            "https://wikimedia.org/api/rest_v1/media/math/render/svg/d54c275db3a1e620737b58e143b0818107fa5f5c",
            "https://wikimedia.org/api/rest_v1/media/math/render/svg/26b73ecc859ea37b015e822e2dd48cf87eb057e4",
            "https://wikimedia.org/api/rest_v1/media/math/render/svg/38a7dcde9730ef0853809fefc18d88771f95206c",
            "https://wikimedia.org/api/rest_v1/media/math/render/svg/f82cade9898ced02fdd08712e5f0c0151758a0dd",
            "https://wikimedia.org/api/rest_v1/media/math/render/svg/f82cade9898ced02fdd08712e5f0c0151758a0dd",
            "https://wikimedia.org/api/rest_v1/media/math/render/svg/ec7200acd984a1d3a3d7dc455e262fbe54f7f6e0",
            "https://wikimedia.org/api/rest_v1/media/math/render/svg/38a7dcde9730ef0853809fefc18d88771f95206c",
            "https://wikimedia.org/api/rest_v1/media/math/render/svg/ec7200acd984a1d3a3d7dc455e262fbe54f7f6e0",
            "https://wikimedia.org/api/rest_v1/media/math/render/svg/38a7dcde9730ef0853809fefc18d88771f95206c",
            "https://wikimedia.org/api/rest_v1/media/math/render/svg/8e04eea58a5881276e1a64cf376055eb7e6107e8",
            "https://wikimedia.org/api/rest_v1/media/math/render/svg/5765910e1b8f1ccb567b28ef8a355133d36841cb",
            "https://wikimedia.org/api/rest_v1/media/math/render/svg/d377ac89e2b551580c4c6b0c600c40db9f2fa405",
            "https://wikimedia.org/api/rest_v1/media/math/render/svg/bd03336d5ffc387606abf8f98f3e7467244a5b90",
            "https://wikimedia.org/api/rest_v1/media/math/render/svg/319288ba0fff4d4ad682fbd0173424637252147b",
            "https://wikimedia.org/api/rest_v1/media/math/render/svg/08fe4d0d434d86b6a4299e64f762a9f5fecf420d",
            "https://wikimedia.org/api/rest_v1/media/math/render/svg/a4a52c506ed0db21f87107570016f15b7b606365",
            "https://wikimedia.org/api/rest_v1/media/math/render/svg/80dde47c869721b9aadc83e574c707155b8b870c",
            "https://wikimedia.org/api/rest_v1/media/math/render/svg/f09880a4e262f6250ad9935a26c1c68821c31379",
            "https://wikimedia.org/api/rest_v1/media/math/render/svg/cd7776ed1f48fe8d39d9852e6ed6aa8a61a93d28",
            "https://wikimedia.org/api/rest_v1/media/math/render/svg/b35d94761926710d8a88b7a1c8e21d95c46d5a62",
            "https://wikimedia.org/api/rest_v1/media/math/render/svg/04050817032da47fb32144e7975eb9cf571bf480",
            "https://wikimedia.org/api/rest_v1/media/math/render/svg/f27cc3cec93683e42cb417eef5a4b8bbbad0849e",
            "https://wikimedia.org/api/rest_v1/media/math/render/svg/c6dc1e6f77cc2eaecd1ea94f87a49f7db5c0f8f9",
            "https://wikimedia.org/api/rest_v1/media/math/render/svg/3fe7cbd42a06081446a0a27d12227e32d7e15637",
            "https://wikimedia.org/api/rest_v1/media/math/render/svg/c6dc1e6f77cc2eaecd1ea94f87a49f7db5c0f8f9",
            "https://wikimedia.org/api/rest_v1/media/math/render/svg/08fe4d0d434d86b6a4299e64f762a9f5fecf420d",
            "https://wikimedia.org/api/rest_v1/media/math/render/svg/03853fbe27bbe56e176420bf21f7196a8b8da0f7",
            "https://wikimedia.org/api/rest_v1/media/math/render/svg/99727641a767fa27119c183afc2a83d5ba6271cf",
            "https://wikimedia.org/api/rest_v1/media/math/render/svg/5f1a5371ac2028177722c36f353053f8e0ef450c",
            "https://wikimedia.org/api/rest_v1/media/math/render/svg/1a17a9558a8e6112fe384161ec9497b4c904f35e",
            "https://wikimedia.org/api/rest_v1/media/math/render/svg/78a131532c4d73ca1c1e1a2306da3c2cae11a180",
            "https://wikimedia.org/api/rest_v1/media/math/render/svg/57182b0e3fe1e25206306facb9fc40cf807a257d",
            "https://wikimedia.org/api/rest_v1/media/math/render/svg/ffa8f26326f9b9dd24810bf463f6324ea9f723f3",
            "https://wikimedia.org/api/rest_v1/media/math/render/svg/f70d9ab992a302542567243889d9654e39bde342",
            "https://wikimedia.org/api/rest_v1/media/math/render/svg/43992031de836ec0c1f8e99a534eb5493f8c3f73",
            "https://wikimedia.org/api/rest_v1/media/math/render/svg/651bb48b907b76390ce3b4aa0983a9891c0702f3",
            "https://wikimedia.org/api/rest_v1/media/math/render/svg/fc2c84cd72f7cb02d490364e7eef94f60fcddd17",
            "https://wikimedia.org/api/rest_v1/media/math/render/svg/1461c1d851eb5c17cf2decdac19079faf0baa901",
            "https://wikimedia.org/api/rest_v1/media/math/render/svg/9d72cf826a94013236a9d427b80684175da0a631",
            "https://wikimedia.org/api/rest_v1/media/math/render/svg/f353ff1c4c549fff2f20e79a5c47decfbe6719e5",
            "https://wikimedia.org/api/rest_v1/media/math/render/svg/17c40d6608917d5b138107e3883ea8d83b2bdd2f",
            "https://wikimedia.org/api/rest_v1/media/math/render/svg/ea7d2ce46d53e49900034f014048fc1cb6eb9061",
            "https://wikimedia.org/api/rest_v1/media/math/render/svg/7725d4f1ab78fd3fe9b42fc6d265252327eb8032",
            "https://wikimedia.org/api/rest_v1/media/math/render/svg/2363e155bae6a93b76c053e0d9159c71f3af9906",
            "https://wikimedia.org/api/rest_v1/media/math/render/svg/b366870c107e3ea8a45937d1fdefa7e3c57787cc",
            "https://wikimedia.org/api/rest_v1/media/math/render/svg/29dd510aeb2451d9905962db4d4c43eeb6c757a8",
            "https://wikimedia.org/api/rest_v1/media/math/render/svg/021bbf04e3743b02e135bcf2dc3a10104d0b9acd",
            "https://wikimedia.org/api/rest_v1/media/math/render/svg/a902af3e6bea66f3cbfa301f5d0f0885faf7d7f5",
            "https://wikimedia.org/api/rest_v1/media/math/render/svg/20a244afde1f8076a29102feab5c5febb15cc2b2",
            "https://wikimedia.org/api/rest_v1/media/math/render/svg/0a47d0ede5f21bf84746d882f5a6c87358adc109",
            "https://wikimedia.org/api/rest_v1/media/math/render/svg/9d20a7159999594806d143105962219407a84939",
            "https://wikimedia.org/api/rest_v1/media/math/render/svg/baf4ab25a25514aabf0bd101746bb82be305720c",
            "https://wikimedia.org/api/rest_v1/media/math/render/svg/86ffe7bc3cbf0f79cc42b44332a10adfe307e579",
            "https://wikimedia.org/api/rest_v1/media/math/render/svg/94336a56bb2b76c5bf054fbf2358008bec490ba5",
            "https://wikimedia.org/api/rest_v1/media/math/render/svg/1e5e885fd799e064de0f95334e121f5567401616",
            "https://wikimedia.org/api/rest_v1/media/math/render/svg/71ebe307f320d69e748fefc6b1d8bd07efcb139d",
            "https://wikimedia.org/api/rest_v1/media/math/render/svg/34c2ef96e929cfe2abdfd005db7bcad78277f1f2",
            "https://wikimedia.org/api/rest_v1/media/math/render/svg/3315c314c90afee7921fabdd11a92bb7eec73c29",
            "https://wikimedia.org/api/rest_v1/media/math/render/svg/81efa211d9d04493b68b24bf9843d1967ae22cdc",
            "https://wikimedia.org/api/rest_v1/media/math/render/svg/56d2d1733d09f02f33694f72b6da94681021e0f0",
            "https://wikimedia.org/api/rest_v1/media/math/render/svg/1871c15ee074bf5682f2b1573923d018ba15e7a1",
            "https://wikimedia.org/api/rest_v1/media/math/render/svg/3035b93119c347c217a138185a2dfe8d0db93f85",
            "https://wikimedia.org/api/rest_v1/media/math/render/svg/fdd0cf707ec1b7da7a336ed6df824cc53444ad03",
            "https://wikimedia.org/api/rest_v1/media/math/render/svg/3e5cfa2f5c08d6fe7d046b73faa6e3f213acc802",
            "https://wikimedia.org/api/rest_v1/media/math/render/svg/70a0100b3cfd9775c9e82bbfc2020c4f023405ce",
            "https://wikimedia.org/api/rest_v1/media/math/render/svg/14e8880a2e4243a2fe5157e574a0547ef3d5d373",
            "https://wikimedia.org/api/rest_v1/media/math/render/svg/11a1da51e66d86b24f1ca426498c6e7d1e63e0b3",
            "https://wikimedia.org/api/rest_v1/media/math/render/svg/61d2de4398212336097f7f62379e3d2c94ee0829",
            "https://wikimedia.org/api/rest_v1/media/math/render/svg/36028fe3b5faefa20f2f1468202ff8337b571f9e",
            "https://wikimedia.org/api/rest_v1/media/math/render/svg/9e66b599725696925357b5be61b0b7282f9d6bcc",
            "https://wikimedia.org/api/rest_v1/media/math/render/svg/3315c314c90afee7921fabdd11a92bb7eec73c29",
            "https://wikimedia.org/api/rest_v1/media/math/render/svg/08c1c908b03c3f63383c7199465c7fd0b105030f",
            "https://wikimedia.org/api/rest_v1/media/math/render/svg/b8d821614023a093af1a26e508b048d5b4125e01",
            "https://wikimedia.org/api/rest_v1/media/math/render/svg/2e4755e17e31cafc971c64edc9d27045ad5796bf",
            "https://wikimedia.org/api/rest_v1/media/math/render/svg/5d316dd9a6d3ca655d7282f6eda513b2ecc46b5f",
            "https://wikimedia.org/api/rest_v1/media/math/render/svg/b04b46696dbf79b1570f6e6a05bed908312720c8",
            "https://wikimedia.org/api/rest_v1/media/math/render/svg/3e5cfa2f5c08d6fe7d046b73faa6e3f213acc802",
            "https://wikimedia.org/api/rest_v1/media/math/render/svg/12f422a3119e21789b450cd87781ec4bc1d26282",
            "https://wikimedia.org/api/rest_v1/media/math/render/svg/1a1cf7e7b98384ac60d4b5a2ec6132298e9914d2",
            "https://wikimedia.org/api/rest_v1/media/math/render/svg/b2709544b531f15360c242eea0324a913aae870c",
            "https://wikimedia.org/api/rest_v1/media/math/render/svg/effd7bd45a5991bb578387bef1355e63ca463513",
            "https://wikimedia.org/api/rest_v1/media/math/render/svg/783c4b6733c6b00b4bdbd7398cb2b01cb57db7b9",
            "https://wikimedia.org/api/rest_v1/media/math/render/svg/9e66b599725696925357b5be61b0b7282f9d6bcc",
            "https://wikimedia.org/api/rest_v1/media/math/render/svg/09907ef99412ababd67dd4765b343f3ef0dae8a4",
            "https://wikimedia.org/api/rest_v1/media/math/render/svg/4afa496ea84246b8becce2c898fb495ea31e00f4",
            "https://wikimedia.org/api/rest_v1/media/math/render/svg/3e5327be14e1147d52c1149f94484b89a76e03aa",
            "https://wikimedia.org/api/rest_v1/media/math/render/svg/6985ef2bb7c224dfbd48e039a27d23f2ddc77f0b",
            "https://wikimedia.org/api/rest_v1/media/math/render/svg/9959d35ebcfaec6c0437e27177339a308f602f9a",
            "https://wikimedia.org/api/rest_v1/media/math/render/svg/5de7c721f5e6b5f389e992abe5be6b61f0289ee2",
            "https://wikimedia.org/api/rest_v1/media/math/render/svg/2d03e10e3c7162cfde48118b91c08f3fb0ecef54",
            "https://wikimedia.org/api/rest_v1/media/math/render/svg/e2c69ef0a3cf62f7f9c36500ce2643bcdc066ec1",
            "https://wikimedia.org/api/rest_v1/media/math/render/svg/ba603b3f7893068f4af7d13569ba2b016f81cbfe",
            "https://wikimedia.org/api/rest_v1/media/math/render/svg/513cec1ccca31705bd7e3de8077da9c68cd8ebaf",
            "https://wikimedia.org/api/rest_v1/media/math/render/svg/02337cf0105844174edfe83285937dec68cb7a89",
            "https://wikimedia.org/api/rest_v1/media/math/render/svg/a11cfb2fdb143693b1daf78fcb5c11a023cb1c55",
            "https://wikimedia.org/api/rest_v1/media/math/render/svg/a4dd406202f5812cefb2862240cfd67ce87454dc",
            "https://wikimedia.org/api/rest_v1/media/math/render/svg/f3810b3a1ab1df84a734d91d25ba019f54ce801d",
            "https://wikimedia.org/api/rest_v1/media/math/render/svg/6bbac3b603a5e1b1aa68cc5a0abed619a4aaf683",
            "https://wikimedia.org/api/rest_v1/media/math/render/svg/8b1c877ab2064a05b5896793244a43c2565eda91",
            "https://wikimedia.org/api/rest_v1/media/math/render/svg/86f21d0e31751534cd6584264ecf864a6aa792cf",
            "https://wikimedia.org/api/rest_v1/media/math/render/svg/2aae8864a3c1fec9585261791a809ddec1489950",
            "https://wikimedia.org/api/rest_v1/media/math/render/svg/2aae8864a3c1fec9585261791a809ddec1489950",
            "https://wikimedia.org/api/rest_v1/media/math/render/svg/2aae8864a3c1fec9585261791a809ddec1489950",
            "https://wikimedia.org/api/rest_v1/media/math/render/svg/40226af71745b6352ae33450eedd83e4ad023a5c",
            "https://wikimedia.org/api/rest_v1/media/math/render/svg/6d943dbbb0b56ca750c4d62c5b54b4ae29a773da",
            "https://wikimedia.org/api/rest_v1/media/math/render/svg/2aae8864a3c1fec9585261791a809ddec1489950",
            "https://wikimedia.org/api/rest_v1/media/math/render/svg/2aae8864a3c1fec9585261791a809ddec1489950",
            "https://wikimedia.org/api/rest_v1/media/math/render/svg/da069aba81acc623bc4419013d9485fe0a0c03e7",
            "https://wikimedia.org/api/rest_v1/media/math/render/svg/2f279afbf98c2f55dbf28dce41c59214c5de3031",
            "https://wikimedia.org/api/rest_v1/media/math/render/svg/5ce8b80e00485945933c89e75d81f3aa0c16379c",
            "https://wikimedia.org/api/rest_v1/media/math/render/svg/b512f975c4929ac62cca9806bbd5492545948eb0",
            "https://wikimedia.org/api/rest_v1/media/math/render/svg/af7190079d3b0bdf9158e67d6be6dc87aca851ce",
            "https://wikimedia.org/api/rest_v1/media/math/render/svg/c0546bebd9713b895b4a63ce27dee6cbd68eee7a",
            "https://wikimedia.org/api/rest_v1/media/math/render/svg/67d30d30b6c2dbe4d6f150d699de040937ecc95f",
            "https://wikimedia.org/api/rest_v1/media/math/render/svg/13cfc978582627bfd00ed990a61a77d992e8aa74",
            "https://wikimedia.org/api/rest_v1/media/math/render/svg/b4577582f0aa3121fc5b20891aca7d339f443534",
            "https://wikimedia.org/api/rest_v1/media/math/render/svg/a93a9d9fb39c54dd400c5a9a8a3be6c20cb344f7",
            "https://wikimedia.org/api/rest_v1/media/math/render/svg/70bc57638e3b7a49fc2078302b857fba777f7295",
            "https://wikimedia.org/api/rest_v1/media/math/render/svg/c9916cad43ad6b93dc29533a1f7ef81961e3060a",
            "https://wikimedia.org/api/rest_v1/media/math/render/svg/9e8ccace1b5e41eafe525f9fe4b38576e113bc6b",
            "https://wikimedia.org/api/rest_v1/media/math/render/svg/076f0042d88aec3154908775e1a15672daa7aa4f",
            "https://wikimedia.org/api/rest_v1/media/math/render/svg/c9916cad43ad6b93dc29533a1f7ef81961e3060a",
            "https://wikimedia.org/api/rest_v1/media/math/render/svg/5ef1bd45f19ef32852a0e3f75fa77c676447cb5e",
            "https://wikimedia.org/api/rest_v1/media/math/render/svg/7f7b38db8c49e5457faa0c1fbdfbe97c9ad168cb",
            "https://wikimedia.org/api/rest_v1/media/math/render/svg/7008047631001b53e8eb5af29d5bac86616356f8",
            "https://wikimedia.org/api/rest_v1/media/math/render/svg/54f6c27c6c2e008a1b6035a2acabf4d6814f9786",
            "https://wikimedia.org/api/rest_v1/media/math/render/svg/c17165090dc8bb09eaab622b2de7ac68308c6f40",
            "https://wikimedia.org/api/rest_v1/media/math/render/svg/ae1195e4710aae26f46808a64d1487e844aa2388",
            "https://wikimedia.org/api/rest_v1/media/math/render/svg/7eb66e8ed33552c98d2528fa938e3b57c645bdf5",
            "https://wikimedia.org/api/rest_v1/media/math/render/svg/2545e6232d64975e72ccf6457e78dc9cae2782de",
            "https://wikimedia.org/api/rest_v1/media/math/render/svg/7333004c2d5f4b78abe06fa6feb7a60b2adb3889",
            "https://wikimedia.org/api/rest_v1/media/math/render/svg/a921b12a37132a731c34a2b7ca03713ac78dc619",
            "https://wikimedia.org/api/rest_v1/media/math/render/svg/d832ebf673aa721f6664dc8e5909fc036b89b583",
            "https://wikimedia.org/api/rest_v1/media/math/render/svg/808f769819828999a156d20875bf4faeddb95b05",
            "https://wikimedia.org/api/rest_v1/media/math/render/svg/e1a6b7bb0f013d2ba3d95ca74b1b184e836ef569",
            "https://wikimedia.org/api/rest_v1/media/math/render/svg/38a7dcde9730ef0853809fefc18d88771f95206c",
            "https://wikimedia.org/api/rest_v1/media/math/render/svg/5ab76fb9e247d15ffd9323b9ac3dd545b67839dd",
            "https://wikimedia.org/api/rest_v1/media/math/render/svg/e1a6b7bb0f013d2ba3d95ca74b1b184e836ef569",
            "https://wikimedia.org/api/rest_v1/media/math/render/svg/421635e3b31747a2d7778f69341cd910f77357ae",
            "https://wikimedia.org/api/rest_v1/media/math/render/svg/56dad457e274b970f5d98b9dc40bef7f895c7f6f",
            "https://wikimedia.org/api/rest_v1/media/math/render/svg/38a7dcde9730ef0853809fefc18d88771f95206c",
            "https://upload.wikimedia.org/wikipedia/en/thumb/9/99/Question_book-new.svg/50px-Question_book-new.svg.png",
            "https://login.wikimedia.org/wiki/Special:CentralAutoLogin/start?type=1x1",
            "https://en.wikipedia.org/static/images/footer/wikimedia-button.svg",
            "https://en.wikipedia.org/static/images/footer/poweredby_mediawiki.svg"
        ],
        "movies": [],
        "keywords": [],
        "meta_keywords": [
            ""
        ],
        "tags": null,
        "authors": [
            "Contributors to Wikimedia projects"
        ],
        "publish_date": "2002-02-25T15:51:15+00:00",
        "summary": "",
        "meta_description": "",
        "meta_lang": "en",
        "meta_favicon": "/static/apple-touch/wikipedia.png",
        "meta_site_name": "",
        "canonical_link": "https://en.wikipedia.org/wiki/Allan_variance",
        "text": "Measure of frequency stability in clocks and oscillators\n\nThe Allan variance (AVAR), also known as two-sample variance, is a measure of frequency stability in clocks, oscillators and amplifiers. It is named after David W. Allan and expressed mathematically as σ y 2 ( τ ) {\\displaystyle \\sigma _{y}^{2}(\\tau )} . The Allan deviation (ADEV), also known as sigma-tau, is the square root of the Allan variance, σ y ( τ ) {\\displaystyle \\sigma _{y}(\\tau )} .\n\nThe M-sample variance is a measure of frequency stability using M samples, time T between measurements and observation time τ {\\displaystyle \\tau } . M-sample variance is expressed as\n\nσ y 2 ( M , T , τ ) . {\\displaystyle \\sigma _{y}^{2}(M,T,\\tau ).}\n\nThe Allan variance is intended to estimate stability due to noise processes and not that of systematic errors or imperfections such as frequency drift or temperature effects. The Allan variance and Allan deviation describe frequency stability. See also the section Interpretation of value below.\n\nThere are also different adaptations or alterations of Allan variance, notably the modified Allan variance MAVAR or MVAR, the total variance, and the Hadamard variance. There also exist time-stability variants such as time deviation (TDEV) or time variance (TVAR). Allan variance and its variants have proven useful outside the scope of timekeeping and are a set of improved statistical tools to use whenever the noise processes are not unconditionally stable, thus a derivative exists.\n\nThe general M-sample variance remains important, since it allows dead time in measurements, and bias functions allow conversion into Allan variance values. Nevertheless, for most applications the special case of 2-sample, or \"Allan variance\" with T = τ {\\displaystyle T=\\tau } is of greatest interest.\n\nBackground\n\n[edit]\n\nWhen investigating the stability of crystal oscillators and atomic clocks, it was found that they did not have a phase noise consisting only of white noise, but also of flicker frequency noise. These noise forms become a challenge for traditional statistical tools such as standard deviation, as the estimator will not converge. The noise is thus said to be divergent. Early efforts in analysing the stability included both theoretical analysis and practical measurements.[2][3]\n\nAn important side consequence of having these types of noise was that, since the various methods of measurements did not agree with each other, the key aspect of repeatability of a measurement could not be achieved. This limits the possibility to compare sources and make meaningful specifications to require from suppliers. Essentially all forms of scientific and commercial uses were then limited to dedicated measurements, which hopefully would capture the need for that application.\n\nTo address these problems, David Allan introduced the M-sample variance and (indirectly) the two-sample variance.[4] While the two-sample variance did not completely allow all types of noise to be distinguished, it provided a means to meaningfully separate many noise-forms for time-series of phase or frequency measurements between two or more oscillators. Allan provided a method to convert between any M-sample variance to any N-sample variance via the common 2-sample variance, thus making all M-sample variances comparable. The conversion mechanism also proved that M-sample variance does not converge for large M, thus making them less useful. IEEE later identified the 2-sample variance as the preferred measure.[5]\n\nAn early concern was related to time- and frequency-measurement instruments that had a dead time between measurements. Such a series of measurements did not form a continuous observation of the signal and thus introduced a systematic bias into the measurement. Great care was spent in estimating these biases. The introduction of zero-dead-time counters removed the need, but the bias-analysis tools have proved useful.\n\nAnother early aspect of concern was related to how the bandwidth of the measurement instrument would influence the measurement, such that it needed to be noted. It was later found that by algorithmically changing the observation τ {\\displaystyle \\tau } , only low τ {\\displaystyle \\tau } values would be affected, while higher values would be unaffected. The change of τ {\\displaystyle \\tau } is done by letting it be an integer multiple n {\\displaystyle n} of the measurement timebase τ 0 {\\displaystyle \\tau _{0}} :\n\nτ = n τ 0 . {\\displaystyle \\tau =n\\tau _{0}.}\n\nThe physics of crystal oscillators were analyzed by D. B. Leeson,[3] and the result is now referred to as Leeson's equation. The feedback in the oscillator will make the white noise and flicker noise of the feedback amplifier and crystal become the power-law noises of f − 2 {\\displaystyle f^{-2}} white frequency noise and f − 3 {\\displaystyle f^{-3}} flicker frequency noise respectively. These noise forms have the effect that the standard variance estimator does not converge when processing time-error samples. This mechanics of the feedback oscillators was unknown when the work on oscillator stability started, but was presented by Leeson at the same time as the set of statistical tools was made available by David W. Allan. For a more thorough presentation on the Leeson effect, see modern phase-noise literature.[6]\n\nInterpretation of value\n\n[edit]\n\nAllan variance is defined as one half of the time average of the squares of the differences between successive readings of the frequency deviation sampled over the sampling period. The Allan variance depends on the time period used between samples, therefore, it is a function of the sample period, commonly denoted as τ, likewise the distribution being measured, and is displayed as a graph rather than a single number. A low Allan variance is a characteristic of a clock with good stability over the measured period.\n\nAllan deviation is widely used for plots (conventionally in log–log format) and presentation of numbers. It is preferred, as it gives the relative amplitude stability, allowing ease of comparison with other sources of errors.\n\nAn Allan deviation of 1.3×10−9 at observation time 1 s (i.e. τ = 1 s) should be interpreted as there being an instability in frequency between two observations 1 second apart with a relative root mean square (RMS) value of 1.3×10−9. For a 10 MHz clock, this would be equivalent to 13 mHz RMS movement. If the phase stability of an oscillator is needed, then the time deviation variants should be consulted and used.\n\nOne may convert the Allan variance and other time-domain variances into frequency-domain measures of time (phase) and frequency stability.[7]\n\nFormulations\n\n[edit]\n\nM-sample variance\n\n[edit]\n\nGiven a time-series x ( t ) {\\displaystyle x(t)} , for any positive real numbers T , τ {\\displaystyle T,\\tau } , define the real number sequence y ¯ i = x ( i T + τ ) − x ( i T ) τ i = 0 , 1 , 2 , . . . {\\displaystyle {\\bar {y}}_{i}={\\frac {x(iT+\\tau )-x(iT)}{\\tau }}\\quad i=0,1,2,...} Then the M {\\displaystyle M} -sample variance is defined[4] (here in a modernized notation form) as the Bessel-corrected variance of the sequence y ¯ 0 , . . . , y ¯ M − 1 {\\displaystyle {\\bar {y}}_{0},...,{\\bar {y}}_{M-1}} : σ y 2 ( M , T , τ ) = M M − 1 ( 1 M ∑ i = 0 M − 1 y ¯ i 2 − [ 1 M ∑ i = 0 M − 1 y ¯ i ] 2 ) , {\\displaystyle \\sigma _{y}^{2}(M,T,\\tau )={\\frac {M}{M-1}}\\left({\\frac {1}{M}}\\sum _{i=0}^{M-1}{\\bar {y}}_{i}^{2}-\\left[{\\frac {1}{M}}\\sum _{i=0}^{M-1}{\\bar {y}}_{i}\\right]^{2}\\right),} The interpretation of the symbols is as follows:\n\nt {\\displaystyle t} is the reading on a reference clock (in arbitrary units).\n\nx ( t ) {\\displaystyle x(t)} is the reading of a clock we are testing (in arbitrary units), as a function of the reference clock's reading. It can also be interpreted as the average fractional frequency time series.\n\ny ¯ n {\\displaystyle {\\bar {y}}_{n}} is the nth fractional frequency average over the observation time τ {\\displaystyle \\tau } .\n\nM {\\displaystyle M} is the number of clock reading intervals used in computing the M {\\displaystyle M} -sample variance,\n\nT {\\displaystyle T} is the time between each frequency sample,\n\nτ {\\displaystyle \\tau } is the time length of each frequency estimate, or the observation period.\n\nDead-time can be accounted for by letting the time T {\\displaystyle T} be different from that of τ {\\displaystyle \\tau } .\n\nAllan variance\n\n[edit]\n\nThe Allan variance is defined as\n\nσ y 2 ( τ ) = ⟨ σ y 2 ( 2 , τ , τ ) ⟩ = 1 2 ⟨ ( y ¯ n + 1 − y ¯ n ) 2 ⟩ = 1 2 τ 2 ⟨ ( x n + 2 − 2 x n + 1 + x n ) 2 ⟩ {\\displaystyle \\sigma _{y}^{2}(\\tau )=\\left\\langle \\sigma _{y}^{2}(2,\\tau ,\\tau )\\right\\rangle ={\\frac {1}{2}}\\left\\langle \\left({\\bar {y}}_{n+1}-{\\bar {y}}_{n}\\right)^{2}\\right\\rangle ={\\frac {1}{2\\tau ^{2}}}\\left\\langle \\left(x_{n+2}-2x_{n+1}+x_{n}\\right)^{2}\\right\\rangle }\n\nwhere ⟨ ⋯ ⟩ {\\displaystyle \\langle \\dotsm \\rangle } denotes the expectation operator.\n\nThe condition T = τ {\\textstyle T=\\tau } means the samples are taken with no dead-time between them.\n\nAllan deviation\n\n[edit]\n\nJust as with standard deviation and variance, the Allan deviation is defined as the square root of the Allan variance:\n\nσ y ( τ ) = σ y 2 ( τ ) . {\\displaystyle \\sigma _{y}(\\tau )={\\sqrt {\\sigma _{y}^{2}(\\tau )}}.}\n\nSupporting definitions\n\n[edit]\n\nOscillator model\n\n[edit]\n\nThe oscillator being analysed is assumed to follow the basic model of\n\nV ( t ) = V 0 sin ⁡ ( Φ ( t ) ) . {\\displaystyle V(t)=V_{0}\\sin(\\Phi (t)).}\n\nThe oscillator is assumed to have a nominal frequency of ν n {\\displaystyle \\nu _{\\text{n}}} , given in cycles per second (SI unit: hertz). The nominal angular frequency ω n {\\displaystyle \\omega _{\\text{n}}} (in radians per second) is given by\n\nω n = 2 π ν n . {\\displaystyle \\omega _{\\text{n}}=2\\pi \\nu _{\\text{n}}.}\n\nThe total phase can be separated into a perfectly cyclic component ω n t {\\displaystyle \\omega _{\\text{n}}t} , along with a fluctuating component φ ( t ) {\\displaystyle \\varphi (t)} :\n\nΦ ( t ) = ω n t + φ ( t ) = 2 π ν n t + φ ( t ) . {\\displaystyle \\Phi (t)=\\omega _{\\text{n}}t+\\varphi (t)=2\\pi \\nu _{\\text{n}}t+\\varphi (t).}\n\nTime error\n\n[edit]\n\nThe time-error function x(t) is the difference between expected nominal time and actual normal time:\n\nx ( t ) = φ ( t ) 2 π ν n = Φ ( t ) 2 π ν n − t = T ( t ) − t . {\\displaystyle x(t)={\\frac {\\varphi (t)}{2\\pi \\nu _{\\text{n}}}}={\\frac {\\Phi (t)}{2\\pi \\nu _{\\text{n}}}}-t=T(t)-t.}\n\nFor measured values a time-error series TE(t) is defined from the reference time function Tref(t) as\n\nT E ( t ) = T ( t ) − T ref ( t ) . {\\displaystyle TE(t)=T(t)-T_{\\text{ref}}(t).}\n\nFrequency function\n\n[edit]\n\nThe frequency function ν ( t ) {\\displaystyle \\nu (t)} is the frequency over time, defined as\n\nν ( t ) = 1 2 π d Φ ( t ) d t . {\\displaystyle \\nu (t)={\\frac {1}{2\\pi }}{\\frac {d\\Phi (t)}{dt}}.}\n\nFractional frequency\n\n[edit]\n\nThe fractional frequency y(t) is the normalized difference between the frequency ν ( t ) {\\displaystyle \\nu (t)} and the nominal frequency ν n {\\displaystyle \\nu _{\\text{n}}} :\n\ny ( t ) = ν ( t ) − ν n ν n = ν ( t ) ν n − 1. {\\displaystyle y(t)={\\frac {\\nu (t)-\\nu _{\\text{n}}}{\\nu _{\\text{n}}}}={\\frac {\\nu (t)}{\\nu _{\\text{n}}}}-1.}\n\nAverage fractional frequency\n\n[edit]\n\nThe average fractional frequency is defined as\n\ny ¯ ( t , τ ) = 1 τ ∫ 0 τ y ( t + t v ) d t v , {\\displaystyle {\\bar {y}}(t,\\tau )={\\frac {1}{\\tau }}\\int _{0}^{\\tau }y(t+t_{v})\\,dt_{v},}\n\nwhere the average is taken over observation time τ, the y(t) is the fractional-frequency error at time t, and τ is the observation time.\n\nSince y(t) is the derivative of x(t), we can without loss of generality rewrite it as\n\ny ¯ ( t , τ ) = x ( t + τ ) − x ( t ) τ . {\\displaystyle {\\bar {y}}(t,\\tau )={\\frac {x(t+\\tau )-x(t)}{\\tau }}.}\n\nEstimators\n\n[edit]\n\nThis definition is based on the statistical expected value, integrating over infinite time. The real-world situation does not allow for such time-series, in which case a statistical estimator needs to be used in its place. A number of different estimators will be presented and discussed.\n\nConventions\n\n[edit]\n\nFixed τ estimators\n\n[edit]\n\nA first simple estimator would be to directly translate the definition into\n\nσ y 2 ( τ , M ) = AVAR ⁡ ( τ , M ) = 1 2 ( M − 1 ) ∑ i = 0 M − 2 ( y ¯ i + 1 − y ¯ i ) 2 , {\\displaystyle \\sigma _{y}^{2}(\\tau ,M)=\\operatorname {AVAR} (\\tau ,M)={\\frac {1}{2(M-1)}}\\sum _{i=0}^{M-2}({\\bar {y}}_{i+1}-{\\bar {y}}_{i})^{2},}\n\nor for the time series:\n\nσ y 2 ( τ , N ) = AVAR ⁡ ( τ , N ) = 1 2 τ 2 ( N − 2 ) ∑ i = 0 N − 3 ( x i + 2 − 2 x i + 1 + x i ) 2 . {\\displaystyle \\sigma _{y}^{2}(\\tau ,N)=\\operatorname {AVAR} (\\tau ,N)={\\frac {1}{2\\tau ^{2}(N-2)}}\\sum _{i=0}^{N-3}(x_{i+2}-2x_{i+1}+x_{i})^{2}.}\n\nThese formulas, however, only provide the calculation for the τ = τ0 case. To calculate for a different value of τ, a new time-series needs to be provided.\n\nNon-overlapped variable τ estimators\n\n[edit]\n\nTaking the time-series and skipping past n − 1 samples, a new (shorter) time-series would occur with τ0 as the time between the adjacent samples, for which the Allan variance could be calculated with the simple estimators. These could be modified to introduce the new variable n such that no new time-series would have to be generated, but rather the original time series could be reused for various values of n. The estimators become\n\nσ y 2 ( n τ 0 , M ) = AVAR ⁡ ( n τ 0 , M ) = 1 2 M − 1 n ∑ i = 0 M − 1 n − 1 ( y ¯ n i + n − y ¯ n i ) 2 {\\displaystyle \\sigma _{y}^{2}(n\\tau _{0},M)=\\operatorname {AVAR} (n\\tau _{0},M)={\\frac {1}{2{\\frac {M-1}{n}}}}\\sum _{i=0}^{{\\frac {M-1}{n}}-1}\\left({\\bar {y}}_{ni+n}-{\\bar {y}}_{ni}\\right)^{2}}\n\nwith n ≤ M − 1 {\\displaystyle n\\leq M-1} ,\n\nand for the time series:\n\nσ y 2 ( n τ 0 , N ) = AVAR ⁡ ( n τ 0 , N ) = 1 2 n 2 τ 0 2 ( N − 1 n − 1 ) ∑ i = 0 N − 1 n − 2 ( x n i + 2 n − 2 x n i + n + x n i ) 2 {\\displaystyle \\sigma _{y}^{2}(n\\tau _{0},N)=\\operatorname {AVAR} (n\\tau _{0},N)={\\frac {1}{2n^{2}\\tau _{0}^{2}\\left({\\frac {N-1}{n}}-1\\right)}}\\sum _{i=0}^{{\\frac {N-1}{n}}-2}\\left(x_{ni+2n}-2x_{ni+n}+x_{ni}\\right)^{2}}\n\nwith n ≤ N − 1 2 {\\displaystyle n\\leq {\\frac {N-1}{2}}} .\n\nThese estimators have a significant drawback in that they will drop a significant amount of sample data, as only 1/n of the available samples is being used.\n\nOverlapped variable τ estimators\n\n[edit]\n\nA technique presented by J. J. Snyder[8] provided an improved tool, as measurements were overlapped in n overlapped series out of the original series. The overlapping Allan variance estimator was introduced by Howe, Allan and Barnes.[9] This can be shown to be equivalent to averaging the time or normalized frequency samples in blocks of n samples prior to processing. The resulting predictor becomes\n\nσ y 2 ( n τ 0 , M ) = AVAR ⁡ ( n τ 0 , M ) = 1 2 n 2 ( M − 2 n + 1 ) ∑ j = 0 M − 2 n ( ∑ i = j j + n − 1 y i + n − y i ) 2 = 1 2 ( M − 2 n + 1 ) ∑ j = 0 M − 2 n ( y ¯ j + n − y ¯ j ) 2 , {\\displaystyle {\\begin{aligned}\\sigma _{y}^{2}(n\\tau _{0},M)&=\\operatorname {AVAR} (n\\tau _{0},M)={\\frac {1}{2n^{2}(M-2n+1)}}\\sum _{j=0}^{M-2n}\\left(\\sum _{i=j}^{j+n-1}y_{i+n}-y_{i}\\right)^{2}\\\\[5pt]&={\\frac {1}{2(M-2n+1)}}\\sum _{j=0}^{M-2n}\\left({\\bar {y}}_{j+n}-{\\bar {y}}_{j}\\right)^{2},\\end{aligned}}}\n\nor for the time series:\n\nσ y 2 ( n τ 0 , N ) = AVAR ⁡ ( n τ 0 , N ) = 1 2 n 2 τ 0 2 ( N − 2 n ) ∑ i = 0 N − 2 n − 1 ( x i + 2 n − 2 x i + n + x i ) 2 . {\\displaystyle \\sigma _{y}^{2}(n\\tau _{0},N)=\\operatorname {AVAR} (n\\tau _{0},N)={\\frac {1}{2n^{2}\\tau _{0}^{2}(N-2n)}}\\sum _{i=0}^{N-2n-1}(x_{i+2n}-2x_{i+n}+x_{i})^{2}.}\n\nThe overlapping estimators have far superior performance over the non-overlapping estimators, as n rises and the time-series is of moderate length. The overlapped estimators have been accepted as the preferred Allan variance estimators in IEEE,[5] ITU-T[10] and ETSI[11] standards for comparable measurements such as needed for telecommunication qualification.\n\nModified Allan variance\n\n[edit]\n\nIn order to address the inability to separate white phase modulation from flicker phase modulation using traditional Allan variance estimators, an algorithmic filtering reduces the bandwidth by n. This filtering provides a modification to the definition and estimators and it now identifies as a separate class of variance called modified Allan variance. The modified Allan variance measure is a frequency stability measure, just as is the Allan variance.\n\nTime stability estimators\n\n[edit]\n\nA time stability (σx) statistical measure, which is often called the time deviation (TDEV), can be calculated from the modified Allan deviation (MDEV). The TDEV is based on the MDEV instead of the original Allan deviation, because the MDEV can discriminate between white and flicker phase modulation (PM). The following is the time variance estimation based on the modified Allan variance:\n\nσ x 2 ( τ ) = τ 2 3 mod σ y 2 ( τ ) , {\\displaystyle \\sigma _{x}^{2}(\\tau )={\\frac {\\tau ^{2}}{3}}{\\bmod {\\sigma }}_{y}^{2}(\\tau ),}\n\nand similarly for modified Allan deviation to time deviation:\n\nσ x ( τ ) = τ 3 mod σ y ( τ ) . {\\displaystyle \\sigma _{x}(\\tau )={\\frac {\\tau }{\\sqrt {3}}}{\\bmod {\\sigma }}_{y}(\\tau ).}\n\nThe TDEV is normalized so that it is equal to the classical deviation for white PM for time constant τ = τ0. To understand the normalization scale factor between the statistical measures, the following is the relevant statistical rule: For independent random variables X and Y, the variance (σz2) of a sum or difference (z = x − y) is the sum square of their variances (σz2 = σx2 + σy2). The variance of the sum or difference (y = x2τ − xτ) of two independent samples of a random variable is twice the variance of the random variable (σy2 = 2σx2). The MDEV is the second difference of independent phase measurements (x) that have a variance (σx2). Since the calculation is the double difference, which requires three independent phase measurements (x2τ − 2xτ + x), the modified Allan variance (MVAR) is three times the variances of the phase measurements.\n\nOther estimators\n\n[edit]\n\nFurther developments have produced improved estimation methods for the same stability measure, the variance/deviation of frequency, but these are known by separate names such as the Hadamard variance, modified Hadamard variance, the total variance, modified total variance and the Theo variance. These distinguish themselves in better use of statistics for improved confidence bounds or ability to handle linear frequency drift.\n\nConfidence intervals and equivalent degrees of freedom\n\n[edit]\n\nStatistical estimators will calculate an estimated value on the sample series used. The estimates may deviate from the true value and the range of values which for some probability will contain the true value is referred to as the confidence interval. The confidence interval depends on the number of observations in the sample series, the dominant noise type, and the estimator being used. The width is also dependent on the statistical certainty for which the confidence interval values forms a bounded range, thus the statistical certainty that the true value is within that range of values. For variable-τ estimators, the τ0 multiple n is also a variable.\n\nConfidence interval\n\n[edit]\n\nThe confidence interval can be established using chi-squared distribution by using the distribution of the sample variance:[5][9]\n\nχ 2 = df s 2 σ 2 , {\\displaystyle \\chi ^{2}={\\frac {{\\text{df}}\\,s^{2}}{\\sigma ^{2}}},}\n\nwhere s2 is the sample variance of our estimate, σ2 is the true variance value, df is the degrees of freedom for the estimator, and χ2 is the degrees of freedom for a certain probability. For a 90% probability, covering the range from the 5% to the 95% range on the probability curve, the upper and lower limits can be found using the inequality\n\nχ 2 ( 0.05 ) ≤ df s 2 σ 2 ≤ χ 2 ( 0.95 ) , {\\displaystyle \\chi ^{2}(0.05)\\leq {\\frac {{\\text{df}}\\,s^{2}}{\\sigma ^{2}}}\\leq \\chi ^{2}(0.95),}\n\nwhich after rearrangement for the true variance becomes\n\ndf s 2 χ 2 ( 0.95 ) ≤ σ 2 ≤ df s 2 χ 2 ( 0.05 ) . {\\displaystyle {\\frac {{\\text{df}}\\,s^{2}}{\\chi ^{2}(0.95)}}\\leq \\sigma ^{2}\\leq {\\frac {{\\text{df}}\\,s^{2}}{\\chi ^{2}(0.05)}}.}\n\nEffective degrees of freedom\n\n[edit]\n\nThe degrees of freedom represents the number of free variables capable of contributing to the estimate. Depending on the estimator and noise type, the effective degrees of freedom varies. Estimator formulas depending on N and n has been found empirically:[9]\n\nAllan variance degrees of freedom Noise type degrees of freedom white phase modulation (WPM) df ≅ ( N + 1 ) ( N − 2 n ) 2 ( N − n ) {\\displaystyle {\\text{df}}\\cong {\\frac {(N+1)(N-2n)}{2(N-n)}}} flicker phase modulation (FPM) df ≅ exp ⁡ [ ( ln ⁡ N − 1 2 n ln ⁡ ( 2 n + 1 ) ( N − 1 ) 4 ) − 1 / 2 ] {\\displaystyle {\\text{df}}\\cong \\exp \\left[\\left(\\ln {\\frac {N-1}{2n}}\\ln {\\frac {(2n+1)(N-1)}{4}}\\right)^{-1/2}\\right]} white frequency modulation (WFM) df ≅ [ 3 ( N − 1 ) 2 n − 2 ( N − 2 ) N ] 4 n 2 4 n 2 + 5 {\\displaystyle {\\text{df}}\\cong \\left[{\\frac {3(N-1)}{2n}}-{\\frac {2(N-2)}{N}}\\right]{\\frac {4n^{2}}{4n^{2}+5}}} flicker frequency modulation (FFM) df ≅ { 2 ( N − 2 ) 2.3 N − 4.9 n = 1 5 N 2 4 n ( N + 3 n ) n ≥ 2 {\\displaystyle {\\text{df}}\\cong {\\begin{cases}{\\frac {2(N-2)}{2.3N-4.9}}&n=1\\\\{\\frac {5N^{2}}{4n(N+3n)}}&n\\geq 2\\end{cases}}} random-walk frequency modulation (RWFM) df ≅ N − 2 n ( N − 1 ) 2 − 3 n ( N − 1 ) + 4 n 2 ( N − 3 ) 2 {\\displaystyle {\\text{df}}\\cong {\\frac {N-2}{n}}{\\frac {(N-1)^{2}-3n(N-1)+4n^{2}}{(N-3)^{2}}}}\n\nPower-law noise\n\n[edit]\n\nThe Allan variance will treat various power-law noise types differently, conveniently allowing them to be identified and their strength estimated. As a convention, the measurement system width (high corner frequency) is denoted fH.\n\nAllan variance power-law response Power-law noise type Phase noise slope Frequency noise slope Power coefficient Phase noise\n\nS x ( f ) {\\displaystyle S_{x}(f)} Allan variance\n\nσ y 2 ( τ ) {\\displaystyle \\sigma _{y}^{2}(\\tau )} Allan deviation\n\nσ y ( τ ) {\\displaystyle \\sigma _{y}(\\tau )} white phase modulation (WPM) f 0 = 1 {\\displaystyle f^{0}=1} f 2 {\\displaystyle f^{2}} h 2 {\\displaystyle h_{2}} 1 ( 2 π ) 2 h 2 {\\displaystyle {\\frac {1}{(2\\pi )^{2}}}h_{2}} 3 f H 4 π 2 τ 2 h 2 {\\displaystyle {\\frac {3f_{H}}{4\\pi ^{2}\\tau ^{2}}}h_{2}} 3 f H 2 π τ h 2 {\\displaystyle {\\frac {\\sqrt {3f_{H}}}{2\\pi \\tau }}{\\sqrt {h_{2}}}} flicker phase modulation (FPM) f − 1 {\\displaystyle f^{-1}} f 1 = f {\\displaystyle f^{1}=f} h 1 {\\displaystyle h_{1}} 1 ( 2 π ) 2 f h 1 {\\displaystyle {\\frac {1}{(2\\pi )^{2}f}}h_{1}} 3 [ γ + ln ⁡ ( 2 π f H τ ) ] − ln ⁡ 2 4 π 2 τ 2 h 1 {\\displaystyle {\\frac {3[\\gamma +\\ln(2\\pi f_{H}\\tau )]-\\ln 2}{4\\pi ^{2}\\tau ^{2}}}h_{1}} 3 [ γ + ln ⁡ ( 2 π f H τ ) ] − ln ⁡ 2 2 π τ h 1 {\\displaystyle {\\frac {\\sqrt {3[\\gamma +\\ln(2\\pi f_{H}\\tau )]-\\ln 2}}{2\\pi \\tau }}{\\sqrt {h_{1}}}} white frequency modulation (WFM) f − 2 {\\displaystyle f^{-2}} f 0 = 1 {\\displaystyle f^{0}=1} h 0 {\\displaystyle h_{0}} 1 ( 2 π ) 2 f 2 h 0 {\\displaystyle {\\frac {1}{(2\\pi )^{2}f^{2}}}h_{0}} 1 2 τ h 0 {\\displaystyle {\\frac {1}{2\\tau }}h_{0}} 1 2 τ h 0 {\\displaystyle {\\frac {1}{\\sqrt {2\\tau }}}{\\sqrt {h_{0}}}} flicker frequency modulation (FFM) f − 3 {\\displaystyle f^{-3}} f − 1 {\\displaystyle f^{-1}} h − 1 {\\displaystyle h_{-1}} 1 ( 2 π ) 2 f 3 h − 1 {\\displaystyle {\\frac {1}{(2\\pi )^{2}f^{3}}}h_{-1}} 2 ln ⁡ ( 2 ) h − 1 {\\displaystyle 2\\ln(2)h_{-1}} 2 ln ⁡ ( 2 ) h − 1 {\\displaystyle {\\sqrt {2\\ln(2)}}{\\sqrt {h_{-1}}}} random walk frequency modulation (RWFM) f − 4 {\\displaystyle f^{-4}} f − 2 {\\displaystyle f^{-2}} h − 2 {\\displaystyle h_{-2}} 1 ( 2 π ) 2 f 4 h − 2 {\\displaystyle {\\frac {1}{(2\\pi )^{2}f^{4}}}h_{-2}} 2 π 2 τ 3 h − 2 {\\displaystyle {\\frac {2\\pi ^{2}\\tau }{3}}h_{-2}} π 2 τ 3 h − 2 {\\displaystyle {\\frac {\\pi {\\sqrt {2\\tau }}}{\\sqrt {3}}}{\\sqrt {h_{-2}}}}\n\nAs found in[12][13] and in modern forms.[14][15]\n\nThe Allan variance is unable to distinguish between WPM and FPM, but is able to resolve the other power-law noise types. In order to distinguish WPM and FPM, the modified Allan variance needs to be employed.\n\nThe above formulas assume that\n\nτ ≫ 1 2 π f H , {\\displaystyle \\tau \\gg {\\frac {1}{2\\pi f_{H}}},}\n\nand thus that the bandwidth of the observation time is much lower than the instruments bandwidth. When this condition is not met, all noise forms depend on the instrument's bandwidth.\n\nα–μ mapping\n\n[edit]\n\nThe detailed mapping of a phase modulation of the form\n\nS x ( f ) = 1 4 π 2 h α f α − 2 = 1 4 π 2 h α f β , {\\displaystyle S_{x}(f)={\\frac {1}{4\\pi ^{2}}}h_{\\alpha }f^{\\alpha -2}={\\frac {1}{4\\pi ^{2}}}h_{\\alpha }f^{\\beta },}\n\nwhere\n\nβ ≡ α − 2 , {\\displaystyle \\beta \\equiv \\alpha -2,}\n\nor frequency modulation of the form\n\nS y ( f ) = h α f α {\\displaystyle S_{y}(f)=h_{\\alpha }f^{\\alpha }}\n\ninto the Allan variance of the form\n\nσ y 2 ( τ ) = K α h α τ μ {\\displaystyle \\sigma _{y}^{2}(\\tau )=K_{\\alpha }h_{\\alpha }\\tau ^{\\mu }}\n\ncan be significantly simplified by providing a mapping between α and μ. A mapping between α and Kα is also presented for convenience:[5]\n\nAllan variance α–μ mapping α β μ Kα −2 −4 1 2 π 2 3 {\\displaystyle {\\frac {2\\pi ^{2}}{3}}} −1 −3 0 2 ln ⁡ 2 {\\displaystyle 2\\ln 2} 0 −2 −1 1 2 {\\displaystyle {\\frac {1}{2}}} 1 −1 −2 3 [ γ + ln ⁡ ( 2 π f H τ ) ] − ln ⁡ 2 4 π 2 {\\displaystyle {\\frac {3[\\gamma +\\ln(2\\pi f_{H}\\tau )]-\\ln 2}{4\\pi ^{2}}}} 2 0 −2 3 f H 4 π 2 {\\displaystyle {\\frac {3f_{H}}{4\\pi ^{2}}}}\n\nGeneral conversion from phase noise\n\n[edit]\n\nA signal with spectral phase noise S φ {\\displaystyle S_{\\varphi }} with units rad2/Hz can be converted to Allan Variance by[15]\n\nσ y 2 ( τ ) = 2 ν 0 2 ∫ 0 f b S φ ( f ) sin 4 ⁡ ( π τ f ) ( π τ ) 2 d f . {\\displaystyle \\sigma _{y}^{2}(\\tau )={\\frac {2}{\\nu _{0}^{2}}}\\int _{0}^{f_{b}}S_{\\varphi }(f){\\frac {\\sin ^{4}(\\pi \\tau f)}{(\\pi \\tau )^{2}}}\\,df.}\n\nLinear response\n\n[edit]\n\nWhile Allan variance is intended to be used to distinguish noise forms, it will depend on some but not all linear responses to time. They are given in the table:\n\nAllan variance linear response Linear effect time response frequency response Allan variance Allan deviation phase offset x 0 {\\displaystyle x_{0}} 0 {\\displaystyle 0} 0 {\\displaystyle 0} 0 {\\displaystyle 0} frequency offset y 0 t {\\displaystyle y_{0}t} y 0 {\\displaystyle y_{0}} 0 {\\displaystyle 0} 0 {\\displaystyle 0} linear drift D t 2 2 {\\displaystyle {\\frac {Dt^{2}}{2}}} D t {\\displaystyle Dt} D 2 τ 2 2 {\\displaystyle {\\frac {D^{2}\\tau ^{2}}{2}}} D τ 2 {\\displaystyle {\\frac {D\\tau }{\\sqrt {2}}}}\n\nThus, linear drift will contribute to output result. When measuring a real system, the linear drift or other drift mechanism may need to be estimated and removed from the time-series prior to calculating the Allan variance.[14]\n\nTime and frequency filter properties\n\n[edit]\n\nIn analysing the properties of Allan variance and friends, it has proven useful to consider the filter properties on the normalize frequency. Starting with the definition for Allan variance for\n\nσ y 2 ( τ ) = 1 2 ⟨ ( y ¯ i + 1 − y ¯ i ) 2 ⟩ , {\\displaystyle \\sigma _{y}^{2}(\\tau )={\\frac {1}{2}}\\left\\langle \\left({\\bar {y}}_{i+1}-{\\bar {y}}_{i}\\right)^{2}\\right\\rangle ,}\n\nwhere\n\ny ¯ i = 1 τ ∫ 0 τ y ( i τ + t ) d t . {\\displaystyle {\\bar {y}}_{i}={\\frac {1}{\\tau }}\\int _{0}^{\\tau }y(i\\tau +t)\\,dt.}\n\nReplacing the time series of y i {\\displaystyle y_{i}} with the Fourier-transformed variant S y ( f ) {\\displaystyle S_{y}(f)} the Allan variance can be expressed in the frequency domain as\n\nσ y 2 ( τ ) = ∫ 0 ∞ S y ( f ) 2 sin 4 ⁡ π τ f ( π τ f ) 2 d f . {\\displaystyle \\sigma _{y}^{2}(\\tau )=\\int _{0}^{\\infty }S_{y}(f){\\frac {2\\sin ^{4}\\pi \\tau f}{(\\pi \\tau f)^{2}}}\\,df.}\n\nThus the transfer function for Allan variance is\n\n| H A ( f ) | 2 = 2 sin 4 ⁡ π τ f ( π τ f ) 2 . {\\displaystyle \\left\\vert H_{A}(f)\\right\\vert ^{2}={\\frac {2\\sin ^{4}\\pi \\tau f}{(\\pi \\tau f)^{2}}}.}\n\nBias functions\n\n[edit]\n\nThe M-sample variance, and the defined special case Allan variance, will experience systematic bias depending on different number of samples M and different relationship between T and τ. In order to address these biases the bias-functions B1 and B2 has been defined[16] and allows conversion between different M and T values.\n\nThese bias functions are not sufficient for handling the bias resulting from concatenating M samples to the Mτ0 observation time over the MT0 with the dead-time distributed among the M measurement blocks rather than at the end of the measurement. This rendered the need for the B3 bias.[17]\n\nThe bias functions are evaluated for a particular μ value, so the α–μ mapping needs to be done for the dominant noise form as found using noise identification. Alternatively,[4][16] the μ value of the dominant noise form may be inferred from the measurements using the bias functions.\n\nB1 bias function\n\n[edit]\n\nThe B1 bias function relates the M-sample variance with the 2-sample variance (Allan variance), keeping the time between measurements T and time for each measurements τ constant. It is defined[16] as\n\nB 1 ( N , r , μ ) = ⟨ σ y 2 ( N , T , τ ) ⟩ ⟨ σ y 2 ( 2 , T , τ ) ⟩ , {\\displaystyle B_{1}(N,r,\\mu )={\\frac {\\left\\langle \\sigma _{y}^{2}(N,T,\\tau )\\right\\rangle }{\\left\\langle \\sigma _{y}^{2}(2,T,\\tau )\\right\\rangle }},}\n\nwhere\n\nr = T τ . {\\displaystyle r={\\frac {T}{\\tau }}.}\n\nThe bias function becomes after analysis\n\nB 1 ( N , r , μ ) = 1 + ∑ n = 1 N − 1 N − n N ( N − 1 ) [ 2 ( r n ) μ + 2 − ( r n + 1 ) μ + 2 − | r n − 1 | μ + 2 ] 1 + 1 2 [ 2 r μ + 2 − ( r + 1 ) μ + 2 − | r − 1 | μ + 2 ] . {\\displaystyle B_{1}(N,r,\\mu )={\\frac {1+\\sum _{n=1}^{N-1}{\\frac {N-n}{N(N-1)}}\\left[2(rn)^{\\mu +2}-(rn+1)^{\\mu +2}-|rn-1|^{\\mu +2}\\right]}{1+{\\frac {1}{2}}\\left[2r^{\\mu +2}-(r+1)^{\\mu +2}-|r-1|^{\\mu +2}\\right]}}.}\n\nB2 bias function\n\n[edit]\n\nThe B2 bias function relates the 2-sample variance for sample time T with the 2-sample variance (Allan variance), keeping the number of samples N = 2 and the observation time τ constant. It is defined[16] as\n\nB 2 ( r , μ ) = ⟨ σ y 2 ( 2 , T , τ ) ⟩ ⟨ σ y 2 ( 2 , τ , τ ) ⟩ , {\\displaystyle B_{2}(r,\\mu )={\\frac {\\left\\langle \\sigma _{y}^{2}(2,T,\\tau )\\right\\rangle }{\\left\\langle \\sigma _{y}^{2}(2,\\tau ,\\tau )\\right\\rangle }},}\n\nwhere\n\nr = T τ . {\\displaystyle r={\\frac {T}{\\tau }}.}\n\nThe bias function becomes after analysis\n\nB 2 ( r , μ ) = 1 + 1 2 [ 2 r μ + 2 − ( r + 1 ) μ + 2 − | r − 1 | μ + 2 ] 2 ( 1 − 2 μ ) . {\\displaystyle B_{2}(r,\\mu )={\\frac {1+{\\frac {1}{2}}\\left[2r^{\\mu +2}-(r+1)^{\\mu +2}-|r-1|^{\\mu +2}\\right]}{2\\left(1-2^{\\mu }\\right)}}.}\n\nB3 bias function\n\n[edit]\n\nThe B3 bias function relates the 2-sample variance for sample time MT0 and observation time Mτ0 with the 2-sample variance (Allan variance) and is defined[17] as\n\nB 3 ( N , M , r , μ ) = ⟨ σ y 2 ( N , M , T , τ ) ⟩ ⟨ σ y 2 ( N , T , τ ) ⟩ , {\\displaystyle B_{3}(N,M,r,\\mu )={\\frac {\\left\\langle \\sigma _{y}^{2}(N,M,T,\\tau )\\right\\rangle }{\\left\\langle \\sigma _{y}^{2}(N,T,\\tau )\\right\\rangle }},}\n\nwhere\n\nT = M T 0 , {\\displaystyle T=MT_{0},}\n\nτ = M τ 0 . {\\displaystyle \\tau =M\\tau _{0}.}\n\nThe B3 bias function is useful to adjust non-overlapping and overlapping variable τ estimator values based on dead-time measurements of observation time τ0 and time between observations T0 to normal dead-time estimates.\n\nThe bias function becomes after analysis (for the N = 2 case)\n\nB 3 ( 2 , M , r , μ ) = 2 M + M F ( M r ) − ∑ n = 1 M − 1 ( M − n ) [ 2 F ( n r ) − F ( ( M + n ) r ) + F ( ( M − n ) r ) ] M μ + 2 [ F ( r ) + 2 ] , {\\displaystyle B_{3}(2,M,r,\\mu )={\\frac {2M+MF(Mr)-\\sum _{n=1}^{M-1}(M-n)\\left[2F(nr)-F{\\big (}(M+n)r{\\big )}+F{\\big (}(M-n)r{\\big )}\\right]}{M^{\\mu +2}[F(r)+2]}},}\n\nwhere\n\nF ( A ) = 2 A μ + 2 − ( A + 1 ) μ + 2 − | A − 1 | μ + 2 . {\\displaystyle F(A)=2A^{\\mu +2}-(A+1)^{\\mu +2}-|A-1|^{\\mu +2}.}\n\nτ bias function\n\n[edit]\n\nWhile formally not formulated, it has been indirectly inferred as a consequence of the α–μ mapping. When comparing two Allan variance measure for different τ, assuming same dominant noise in the form of same μ coefficient, a bias can be defined as\n\nB τ ( τ 1 , τ 2 , μ ) = ⟨ σ y 2 ( 2 , τ 2 , τ 2 ) ⟩ ⟨ σ y 2 ( 2 , τ 1 , τ 1 ) ⟩ . {\\displaystyle B_{\\tau }(\\tau _{1},\\tau _{2},\\mu )={\\frac {\\left\\langle \\sigma _{y}^{2}(2,\\tau _{2},\\tau _{2})\\right\\rangle }{\\left\\langle \\sigma _{y}^{2}(2,\\tau _{1},\\tau _{1})\\right\\rangle }}.}\n\nThe bias function becomes after analysis\n\nB τ ( τ 1 , τ 2 , μ ) = ( τ 2 τ 1 ) μ . {\\displaystyle B_{\\tau }(\\tau _{1},\\tau _{2},\\mu )=\\left({\\frac {\\tau _{2}}{\\tau _{1}}}\\right)^{\\mu }.}\n\nConversion between values\n\n[edit]\n\nIn order to convert from one set of measurements to another the B1, B2 and τ bias functions can be assembled. First the B1 function converts the (N1, T1, τ1) value into (2, T1, τ1), from which the B2 function converts into a (2, τ1, τ1) value, thus the Allan variance at τ1. The Allan variance measure can be converted using the τ bias function from τ1 to τ2, from which then the (2, T2, τ2) using B2 and then finally using B1 into the (N2, T2, τ2) variance. The complete conversion becomes\n\n⟨ σ y 2 ( N 2 , T 2 , τ 2 ) ⟩ = ( τ 2 τ 1 ) μ [ B 1 ( N 2 , r 2 , μ ) B 2 ( r 2 , μ ) B 1 ( N 1 , r 1 , μ ) B 2 ( r 1 , μ ) ] ⟨ σ y 2 ( N 1 , T 1 , τ 1 ) ⟩ , {\\displaystyle \\left\\langle \\sigma _{y}^{2}(N_{2},T_{2},\\tau _{2})\\right\\rangle =\\left({\\frac {\\tau _{2}}{\\tau _{1}}}\\right)^{\\mu }\\left[{\\frac {B_{1}(N_{2},r_{2},\\mu )B_{2}(r_{2},\\mu )}{B_{1}(N_{1},r_{1},\\mu )B_{2}(r_{1},\\mu )}}\\right]\\left\\langle \\sigma _{y}^{2}(N_{1},T_{1},\\tau _{1})\\right\\rangle ,}\n\nwhere\n\nr 1 = T 1 r 1 , {\\displaystyle r_{1}={\\frac {T_{1}}{r_{1}}},}\n\nr 2 = T 2 r 2 . {\\displaystyle r_{2}={\\frac {T_{2}}{r_{2}}}.}\n\nSimilarly, for concatenated measurements using M sections, the logical extension becomes\n\n⟨ σ y 2 ( N 2 , M 2 , T 2 , τ 2 ) ⟩ = ( τ 2 τ 1 ) μ [ B 3 ( N 2 , M 2 , r 2 , μ ) B 1 ( N 2 , r 2 , μ ) B 2 ( r 2 , μ ) B 3 ( N 1 , M 1 , r 1 , μ ) B 1 ( N 1 , r 1 , μ ) B 2 ( r 1 , μ ) ] ⟨ σ y 2 ( N 1 , M 1 , T 1 , τ 1 ) ⟩ . {\\displaystyle \\left\\langle \\sigma _{y}^{2}(N_{2},M_{2},T_{2},\\tau _{2})\\right\\rangle =\\left({\\frac {\\tau _{2}}{\\tau _{1}}}\\right)^{\\mu }\\left[{\\frac {B_{3}(N_{2},M_{2},r_{2},\\mu )B_{1}(N_{2},r_{2},\\mu )B_{2}(r_{2},\\mu )}{B_{3}(N_{1},M_{1},r_{1},\\mu )B_{1}(N_{1},r_{1},\\mu )B_{2}(r_{1},\\mu )}}\\right]\\left\\langle \\sigma _{y}^{2}(N_{1},M_{1},T_{1},\\tau _{1})\\right\\rangle .}\n\nMeasurement issues\n\n[edit]\n\nWhen making measurements to calculate Allan variance or Allan deviation, a number of issues may cause the measurements to degenerate. Covered here are the effects specific to Allan variance, where results would be biased.\n\nMeasurement bandwidth limits\n\n[edit]\n\nA measurement system is expected to have a bandwidth at or below that of the Nyquist rate, as described within the Shannon–Hartley theorem. As can be seen in the power-law noise formulas, the white and flicker noise modulations both depends on the upper corner frequency f H {\\displaystyle f_{H}} (these systems is assumed to be low-pass filtered only). Considering the frequency filter property, it can be clearly seen that low-frequency noise has greater impact on the result. For relatively flat phase-modulation noise types (e.g. WPM and FPM), the filtering has relevance, whereas for noise types with greater slope the upper frequency limit becomes of less importance, assuming that the measurement system bandwidth is wide relative the τ {\\displaystyle \\tau } as given by\n\nτ ≫ 1 2 π f H . {\\displaystyle \\tau \\gg {\\frac {1}{2\\pi f_{H}}}.}\n\nWhen this assumption is not met, the effective bandwidth f H {\\displaystyle f_{H}} needs to be notated alongside the measurement. The interested should consult NBS TN394.[12]\n\nIf, however, one adjust the bandwidth of the estimator by using integer multiples of the sample time n τ 0 {\\displaystyle n\\tau _{0}} , then the system bandwidth impact can be reduced to insignificant levels. For telecommunication needs, such methods have been required in order to ensure comparability of measurements and allow some freedom for vendors to do different implementations. The ITU-T Rec. G.813[18] for the TDEV measurement.\n\nIt can be recommended that the first τ 0 {\\displaystyle \\tau _{0}} multiples be ignored, such that the majority of the detected noise is well within the passband of the measurement systems bandwidth.\n\nFurther developments on the Allan variance was performed to let the hardware bandwidth be reduced by software means. This development of a software bandwidth allowed addressing the remaining noise, and the method is now referred to modified Allan variance. This bandwidth reduction technique should not be confused with the enhanced variant of modified Allan variance, which also changes a smoothing filter bandwidth.\n\nDead time in measurements\n\n[edit]\n\nMany measurement instruments of time and frequency have the stages of arming time, time-base time, processing time and may then re-trigger the arming. The arming time is from the time the arming is triggered to when the start event occurs on the start channel. The time-base then ensures that minimal amount of time goes prior to accepting an event on the stop channel as the stop event. The number of events and time elapsed between the start event and stop event is recorded and presented during the processing time. When the processing occurs (also known as the dwell time), the instrument is usually unable to do another measurement. After the processing has occurred, an instrument in continuous mode triggers the arm circuit again. The time between the stop event and the following start event becomes dead time, during which the signal is not being observed. Such dead time introduces systematic measurement biases, which needs to be compensated for in order to get proper results. For such measurement systems will the time T denote the time between the adjacent start events (and thus measurements), while τ {\\displaystyle \\tau } denote the time-base length, i.e. the nominal length between the start and stop event of any measurement.\n\nDead-time effects on measurements have such an impact on the produced result that much study of the field have been done in order to quantify its properties properly. The introduction of zero-dead-time counters removed the need for this analysis. A zero-dead-time counter has the property that the stop event of one measurement is also being used as the start event of the following event. Such counters create a series of event and time timestamp pairs, one for each channel spaced by the time-base. Such measurements have also proved useful in order forms of time-series analysis.\n\nMeasurements being performed with dead time can be corrected using the bias function B1, B2 and B3. Thus, dead time as such is not prohibiting the access to the Allan variance, but it makes it more problematic. The dead time must be known, such that the time between samples T can be established.\n\nMeasurement length and effective use of samples\n\n[edit]\n\nStudying the effect on the confidence intervals that the length N of the sample series have and the effect of the variable τ parameter n, the confidence intervals may become very large since the effective degree of freedom may become small for some combination of N and n for the dominant noise form (for that τ).\n\nThe effect may be that the estimated value may be much smaller or much greater than the real value, which may lead to false conclusions of the result.\n\nIt is recommended that:\n\nThe confidence interval be plotted along with the data, such that the reader of the plot knows of the statistical uncertainty of the values.\n\nThe length of the sample sequence (i.e. the number of samples N) must be kept as high as possible to ensure that confidence interval is small over the τ range of interest.\n\nEstimators providing better degrees of freedom values be used in replacement of the Allan variance estimators or as complementing them where they outperform the Allan variance estimators. Among those the total variance and Theo variance estimators should be considered.\n\nThe τ range as swept by the τ0 multiplier n is limited in the upper end relative N, such that the reader of the plot may not be confused by highly unstable estimator values.\n\nDominant noise type\n\n[edit]\n\nA large number of conversion constants, bias corrections and confidence intervals depends on the dominant noise type. For proper interpretation shall the dominant noise type for the particular τ of interest be identified through noise identification. Failing to identify the dominant noise type will produce biased values. Some of these biases may be of several order of magnitude, so it may be of large significance.\n\nLinear drift\n\n[edit]\n\nSystematic effects on the signal is only partly cancelled. Phase and frequency offset is cancelled, but linear drift or other high-degree forms of polynomial phase curves will not be cancelled and thus form a measurement limitation. Curve fitting and removal of systematic offset could be employed. Often removal of linear drift can be sufficient. Use of linear-drift estimators such as the Hadamard variance could also be employed. A linear drift removal could be employed using a moment-based estimator.\n\nMeasurement instrument estimator bias\n\n[edit]\n\nTraditional instruments provided only the measurement of single events or event pairs. The introduction of the improved statistical tool of overlapping measurements by J. J. Snyder[8] allowed much improved resolution in frequency readouts, breaking the traditional digits/time-base balance. While such methods is useful for their intended purpose, using such smoothed measurements for Allan variance calculations would give a false impression of high resolution,[19][20][21] but for longer τ the effect is gradually removed, and the lower-τ region of the measurement has biased values. This bias is providing lower values than it should, so it is an overoptimistic (assuming that low numbers is what one wishes) bias, reducing the usability of the measurement rather than improving it. Such smart algorithms can usually be disabled or otherwise circumvented by using time-stamp mode, which is much preferred if available.\n\nPractical measurements\n\n[edit]\n\nWhile several approaches to measurement of Allan variance can be devised, a simple example may illustrate how measurements can be performed.\n\nMeasurement\n\n[edit]\n\nAll measurements of Allan variance will in effect be the comparison of two different clocks. Consider a reference clock and a device under test (DUT), and both having a common nominal frequency of 10 MHz. A time-interval counter is being used to measure the time between the rising edge of the reference (channel A) and the rising edge of the device under test.\n\nIn order to provide evenly spaced measurements, the reference clock will be divided down to form the measurement rate, triggering the time-interval counter (ARM input). This rate can be 1 Hz (using the 1 PPS output of a reference clock), but other rates like 10 Hz and 100 Hz can also be used. The speed of which the time-interval counter can complete the measurement, output the result and prepare itself for the next arm will limit the trigger frequency.\n\nA computer is then useful to record the series of time differences being observed.\n\nPost-processing\n\n[edit]\n\nThe recorded time-series require post-processing to unwrap the wrapped phase, such that a continuous phase error is being provided. If necessary, logging and measurement mistakes should also be fixed. Drift estimation and drift removal should be performed, the drift mechanism needs to be identified and understood for the sources. Drift limitations in measurements can be severe, so letting the oscillators become stabilized, by long enough time being powered on, is necessary.\n\nThe Allan variance can then be calculated using the estimators given, and for practical purposes the overlapping estimator should be used due to its superior use of data over the non-overlapping estimator. Other estimators such as total or Theo variance estimators could also be used if bias corrections is applied such that they provide Allan variance-compatible results.\n\nTo form the classical plots, the Allan deviation (square root of Allan variance) is plotted in log–log format against the observation interval τ.\n\nEquipment and software\n\n[edit]\n\nThe time-interval counter is typically an off-the-shelf counter commercially available. Limiting factors involve single-shot resolution, trigger jitter, speed of measurements and stability of reference clock. The computer collection and post-processing can be done using existing commercial or public-domain software. Highly advanced solutions exists, which will provide measurement and computation in one box.\n\nResearch history\n\n[edit]\n\nThe field of frequency stability has been studied for a long time. However, during the 1960s it was found that coherent definitions were lacking. A NASA-IEEE Symposium on Short-Term Stability in November 1964[22] resulted in the special February 1966 issue of the IEEE Proceedings on Frequency Stability.\n\nThe NASA-IEEE Symposium brought together many fields and uses of short- and long-term stability, with papers from many different contributors. The articles and panel discussions concur on the existence of the frequency flicker noise and the wish to achieve a common definition for both short-term and long-term stability.\n\nImportant papers, including those of David Allan,[4] James A. Barnes,[23] L. S. Cutler and C. L. Searle[2] and D. B. Leeson,[3] appeared in the IEEE Proceedings on Frequency Stability and helped shape the field.\n\nDavid Allan's article analyses the classical M-sample variance of frequency, tackling the issue of dead-time between measurements along with an initial bias function.[4] Although Allan's initial bias function assumes no dead-time, his formulas do include dead-time calculations. His article analyses the case of M frequency samples (called N in the article) and variance estimators. It provides the now standard α–μ mapping, clearly building on James Barnes' work[23] in the same issue.\n\nThe 2-sample variance case is a special case of the M-sample variance, which produces an average of the frequency derivative. Allan implicitly uses the 2-sample variance as a base case, since for arbitrary chosen M, values may be transferred via the 2-sample variance to the M-sample variance. No preference was clearly stated for the 2-sample variance, even if the tools were provided. However, this article laid the foundation for using the 2-sample variance as a way of comparing other M-sample variances.\n\nJames Barnes significantly extended the work on bias functions,[16] introducing the modern B1 and B2 bias functions. Curiously enough, it refers to the M-sample variance as \"Allan variance\", while referring to Allan's article \"Statistics of Atomic Frequency Standards\".[4] With these modern bias functions, full conversion among M-sample variance measures of various M, T and τ values could be performed, by conversion through the 2-sample variance.\n\nJames Barnes and David Allan further extended the bias functions with the B3 function[17] to handle the concatenated samples estimator bias. This was necessary to handle the new use of concatenated sample observations with dead-time in between.\n\nIn 1970, the IEEE Technical Committee on Frequency and Time, within the IEEE Group on Instrumentation & Measurements, provided a summary of the field, published as NBS Technical Notice 394.[12] This paper was first in a line of more educational and practical papers helping fellow engineers grasp the field. This paper recommended the 2-sample variance with T = τ, referring to it as Allan variance (now without the quotes). The choice of such parametrisation allows good handling of some noise forms and getting comparable measurements; it is essentially the least common denominator with the aid of the bias functions B1 and B2.\n\nJ. J. Snyder proposed an improved method for frequency or variance estimation, using sample statistics for frequency counters.[8] To get more effective degrees of freedom out of the available dataset, the trick is to use overlapping observation periods. This provides a √n improvement, and was incorporated in the overlapping Allan variance estimator.[9] Variable-τ software processing was also incorporated.[9] This development improved the classical Allan variance estimators, likewise providing a direct inspiration for the work on modified Allan variance.\n\nHowe, Allan and Barnes presented the analysis of confidence intervals, degrees of freedom, and the established estimators.[9]\n\nEducational and practical resources\n\n[edit]\n\nThe field of time and frequency and its use of Allan variance, Allan deviation and friends is a field involving many aspects, for which both understanding of concepts and practical measurements and post-processing requires care and understanding. Thus, there is a realm of educational material stretching about 40 years available. Since these reflect the developments in the research of their time, they focus on teaching different aspect over time, in which case a survey of available resources may be a suitable way of finding the right resource.\n\nThe first meaningful summary is the NBS Technical Note 394 \"Characterization of Frequency Stability\".[12] This is the product of the Technical Committee on Frequency and Time of the IEEE Group on Instrumentation & Measurement. It gives the first overview of the field, stating the problems, defining the basic supporting definitions and getting into Allan variance, the bias functions B1 and B2, the conversion of time-domain measures. This is useful, as it is among the first references to tabulate the Allan variance for the five basic noise types.\n\nA classical reference is the NBS Monograph 140[24] from 1974, which in chapter 8 has \"Statistics of Time and Frequency Data Analysis\".[25] This is the extended variant of NBS Technical Note 394 and adds essentially in measurement techniques and practical processing of values.\n\nAn important addition will be the Properties of signal sources and measurement methods.[9] It covers the effective use of data, confidence intervals, effective degree of freedom, likewise introducing the overlapping Allan variance estimator. It is a highly recommended reading for those topics.\n\nThe IEEE standard 1139 Standard definitions of Physical Quantities for Fundamental Frequency and Time Metrology[5] is beyond that of a standard a comprehensive reference and educational resource.\n\nA modern book aimed towards telecommunication is Stefano Bregni \"Synchronisation of Digital Telecommunication Networks\".[14] This summarises not only the field, but also much of his research in the field up to that point. It aims to include both classical measures and telecommunication-specific measures such as MTIE. It is a handy companion when looking at measurements related to telecommunication standards.\n\nThe NIST Special Publication 1065 \"Handbook of Frequency Stability Analysis\" of W. J. Riley[15] is a recommended reading for anyone wanting to pursue the field. It is rich of references and also covers a wide range of measures, biases and related functions that a modern analyst should have available. Further it describes the overall processing needed for a modern tool.\n\nUses\n\n[edit]\n\nAllan variance is used as a measure of frequency stability in a variety of precision oscillators, such as crystal oscillators, atomic clocks and frequency-stabilized lasers over a period of a second or more. Short-term stability (under a second) is typically expressed as phase noise. The Allan variance is also used to characterize the bias stability of gyroscopes, including fiber optic gyroscopes, hemispherical resonator gyroscopes and MEMS gyroscopes and accelerometers.[26][27]\n\n50th Anniversary\n\n[edit]\n\nIn 2016, IEEE-UFFC is going to be publishing a \"Special Issue to celebrate the 50th anniversary of the Allan Variance (1966–2016)\".[28] A guest editor for that issue will be David's former colleague at NIST, Judah Levine, who is the most recent recipient of the I. I. Rabi Award.\n\nSee also\n\n[edit]\n\nReferences\n\n[edit]"
    }
}
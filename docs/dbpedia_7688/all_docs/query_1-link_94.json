{
    "id": "dbpedia_7688_1",
    "rank": 94,
    "data": {
        "url": "https://www.academia.edu/109398115/DARIAH_MultiCo_Multimodal_Corpus",
        "read_more_link": "",
        "language": "en",
        "title": "DARIAH MultiCo Multimodal Corpus",
        "top_image": "http://a.academia-assets.com/images/open-graph-icons/fb-paper.gif",
        "meta_img": "http://a.academia-assets.com/images/open-graph-icons/fb-paper.gif",
        "images": [
            "https://a.academia-assets.com/images/academia-logo-redesign-2015-A.svg",
            "https://a.academia-assets.com/images/academia-logo-redesign-2015.svg",
            "https://a.academia-assets.com/images/single_work_splash/adobe.icon.svg",
            "https://0.academia-photos.com/attachment_thumbnails/107535627/mini_magick20231119-1-1b4qfo.png?1700415494",
            "https://0.academia-photos.com/284868/70042/2945119/s65_maciej.karpinski.jpg",
            "https://a.academia-assets.com/images/s65_no_pic.png",
            "https://a.academia-assets.com/images/s65_no_pic.png",
            "https://a.academia-assets.com/images/loswp/related-pdf-icon.png",
            "https://a.academia-assets.com/images/loswp/related-pdf-icon.png",
            "https://a.academia-assets.com/images/loswp/related-pdf-icon.png",
            "https://a.academia-assets.com/images/loswp/related-pdf-icon.png",
            "https://a.academia-assets.com/images/loswp/related-pdf-icon.png",
            "https://a.academia-assets.com/images/loswp/related-pdf-icon.png",
            "https://a.academia-assets.com/images/loswp/related-pdf-icon.png",
            "https://a.academia-assets.com/images/loswp/related-pdf-icon.png",
            "https://a.academia-assets.com/images/loswp/related-pdf-icon.png",
            "https://a.academia-assets.com/images/loswp/related-pdf-icon.png",
            "https://a.academia-assets.com/images/loaders/paper-load.gif",
            "https://a.academia-assets.com/images/loswp/related-pdf-icon.png",
            "https://a.academia-assets.com/images/loswp/related-pdf-icon.png",
            "https://a.academia-assets.com/images/loswp/related-pdf-icon.png",
            "https://a.academia-assets.com/images/loswp/related-pdf-icon.png",
            "https://a.academia-assets.com/images/loswp/related-pdf-icon.png",
            "https://a.academia-assets.com/images/loswp/related-pdf-icon.png",
            "https://a.academia-assets.com/images/loswp/related-pdf-icon.png",
            "https://a.academia-assets.com/images/loswp/related-pdf-icon.png",
            "https://a.academia-assets.com/images/loswp/related-pdf-icon.png",
            "https://a.academia-assets.com/images/loswp/related-pdf-icon.png",
            "https://a.academia-assets.com/images/loswp/related-pdf-icon.png",
            "https://a.academia-assets.com/images/loswp/related-pdf-icon.png",
            "https://a.academia-assets.com/images/loswp/related-pdf-icon.png",
            "https://a.academia-assets.com/images/loswp/related-pdf-icon.png",
            "https://a.academia-assets.com/images/loswp/related-pdf-icon.png",
            "https://a.academia-assets.com/images/loswp/related-pdf-icon.png",
            "https://a.academia-assets.com/images/loswp/related-pdf-icon.png",
            "https://a.academia-assets.com/images/loswp/related-pdf-icon.png",
            "https://a.academia-assets.com/images/loswp/related-pdf-icon.png",
            "https://a.academia-assets.com/images/loswp/related-pdf-icon.png",
            "https://a.academia-assets.com/images/loswp/related-pdf-icon.png",
            "https://a.academia-assets.com/images/loswp/related-pdf-icon.png",
            "https://a.academia-assets.com/images/loswp/related-pdf-icon.png",
            "https://a.academia-assets.com/images/loswp/related-pdf-icon.png",
            "https://a.academia-assets.com/images/loswp/related-pdf-icon.png"
        ],
        "movies": [],
        "keywords": [],
        "meta_keywords": [
            ""
        ],
        "tags": null,
        "authors": [
            "Maciej Karpiński",
            "Katarzyna Klessa",
            "Janusz Taborek",
            "amu.academia.edu",
            "independent.academia.edu"
        ],
        "publish_date": "2023-11-19T00:00:00",
        "summary": "",
        "meta_description": "Contemporary studies on interpersonal communication confirm that in order to understand and model this multifaceted process, not only speech itself but also other components, including gestures, facial expressions, or body postures, must be taken",
        "meta_lang": "",
        "meta_favicon": "",
        "meta_site_name": "",
        "canonical_link": "https://www.academia.edu/109398115/DARIAH_MultiCo_Multimodal_Corpus",
        "text": "In this paper, some measures of cross-modal interactions are proposed and implemented in the analysis of a multimodal corpus of task-oriented dialogues. The corpus includes multilevel annotations of speakers' verbal and gestural behaviour, e.g., hand gestures, gaze direction, utterance content or intonational phrasing. A moving time-window approach is adopted to analyse changes in the communicative behaviour of dialogue participants over time. The study is focused on how gestures and speech of the Instruction Giver influence the speech of the Instruction Follower in the course of dialogue.\n\nIn this paper we describe how the complexity of human communication can be analysed with the help of language technology. We present the HuComTech corpus, a multimodal corpus containing 50 hours of videotaped interviews containing a rich annotation of about 2 million items annotated on 33 levels. The corpus serves as a general resource for a wide range of re-search addressing natural conversation between humans in their full complexity. It can benefit particularly digital humanities researchers working in the field of pragmatics, conversational analysis and discourse analysis. We will present a number of tools and automated methods that can help such enquiries. In particular, we will highlight the tool Theme, which is designed to uncover hidden temporal patterns (called T-patterns) in human interaction, and will show how it can applied to the study of multimodal communication.\n\nGesture and speech combine to form a rich basis for human conversational interaction. To exploit these modalities in HCI, we need to understand the interplay between them and the way in which they support communication. We propose a framework for the gesture research done to date, and present our work on the cross-modal cues for discourse segmentation in free-form gesticulation accompanying speech in natural conversation as a new paradigm for such multimodal interaction. The basis for this integration is the psycholinguistic concept of the coequal generation of gesture and speech from the same semantic intent. We present a detailed case study of a gesture and speech elicitation experiment in which a subject describes her living space to an interlocutor. We perform two independent sets of analyses on the video and audio data: video and audio analysis to extract segmentation cues, and expert transcription of the speech and gesture data by microanalyzing the videotape using a frame-acc...\n\nAt ATR, we are collecting and analysing `meetings&#39; data using a table-top sensor device consisting of a small 360degree camera surrounded by an array of high-quality directional microphones. This equipment provides a stream of information about the audio and visual events of the meeting which is then processed to form a representation of the verbal and non-verbal interpersonal activity, or discourse flow, during the meeting. In this paper we show that simple primitives can provide a rich source of information. INTRODUCTION Several laboratories around the world are now collecting and analysing “meetings data” in an effort to automate some of the transcription, search, and informationretrieval processes that are currently very timeconsuming, and to produce a technology capable of tracking a meeting in realtime and recording and annotating its main events. One key area of this research is devoted to identifying and tracking the active participants in a meeting in order to maximise ...\n\nIn the age of the Internet, trillions of bytes of media data are generated every day through telecommunications and social media. This surge of born-digital media data, for example, instant voice/video messages, conference calls, podcasts, video blogs and so on, offers researchers unprecedented opportunities to deepen their understanding of how human beings communicate and go about their social activities. However, such a large amount of data also brings a new problem: how may we plough through so much media data and extract meaningful information efficiently? This chapter explores opportunities and challenges at the interface between digital humanities and multimodality research which focuses on the use of prosody and gesture in spoken communication. Following an overview of key methods and frameworks in prosody and gestures research, it highlights selected projects which have showcased the ways in which today’s computer technology has revolutionised multimodality as an area of research. In recent years, many new computer tools have become available to aid media data acquisition, processing and analysis. These tools have (semi-)automatised many processes which were labour-intensive, expensive and tedious. Therefore, researchers can now afford to compile and process substantially larger multimodal datasets much faster and at a much lower cost. The chapter also introduces tools which open up new avenues for researchers to acquire new types of multimodal data (e.g. YouTube videos) and data streams (e.g. GPS, heartbeats). In the sample analysis, we demonstrate the typical workflow for using a range of these latest computer tools to generate a corpus of YouTube videos, automatically annotate prosodic patterns, align multiple data streams and perform a multimodal analysis on the use of the epistemic stance marker ‘I think’ in video blogs."
    }
}
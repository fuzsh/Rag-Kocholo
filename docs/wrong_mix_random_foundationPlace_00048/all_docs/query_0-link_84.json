{
    "id": "wrong_mix_random_foundationPlace_00048_0",
    "rank": 84,
    "data": {
        "url": "https://arxiv.org/html/2312.15821v1",
        "read_more_link": "",
        "language": "en",
        "title": "Audiobox: Unified Audio Generation with Natural Language Prompts",
        "top_image": "",
        "meta_img": "",
        "images": [
            "https://arxiv.org/html/extracted/5316472/figures/audiobox_diagram.png",
            "https://arxiv.org/html/extracted/5316472/figures/fig_sound_fad-vs-nfe.png",
            "https://arxiv.org/html/extracted/5316472/figures/fig_clap_mos_corr.png",
            "https://arxiv.org/html/extracted/5316472/figures/fig_fairness_gender_wer.png",
            "https://arxiv.org/html/extracted/5316472/figures/fig_fairness_gender_spksim.png",
            "https://arxiv.org/html/extracted/5316472/figures/fig_fairness_accent_wer.png",
            "https://arxiv.org/html/extracted/5316472/figures/fig_fairness_accent_spksim.png",
            "https://arxiv.org/html/extracted/5316472/figures/Sound_QMOS.png",
            "https://arxiv.org/html/extracted/5316472/figures/Sound_TMOS.png",
            "https://arxiv.org/html/extracted/5316472/figures/Speech_QMOS.png",
            "https://arxiv.org/html/extracted/5316472/figures/Speech_SMOS.png",
            "https://arxiv.org/html/extracted/5316472/figures/Speech_TMOS.png"
        ],
        "movies": [],
        "keywords": [],
        "meta_keywords": [
            ""
        ],
        "tags": null,
        "authors": [],
        "publish_date": null,
        "summary": "",
        "meta_description": "",
        "meta_lang": "en",
        "meta_favicon": "",
        "meta_site_name": "",
        "canonical_link": null,
        "text": "License: arXiv.org perpetual non-exclusive license\n\narXiv:2312.15821v1 [cs.SD] 25 Dec 2023\n\n]Audiobox Team, Fundamental AI Research (FAIR) at Meta \\contribution[*]Research team, equal contribution \\contribution[†]Research and engineering leadership, equal contribution\n\nAudiobox: Unified Audio Generation\n\nwith Natural Language Prompts\n\nApoorv Vyas Bowen Shi Matthew Le Andros Tjandra Yi-Chiao Wu Baishan Guo Jiemin Zhang Xinyue Zhang Robert Adkins William Ngan Jeff Wang Ivan Cruz Bapi Akula Akinniyi Akinyemi Brian Ellis Rashel Moritz Yael Yungster Alice Rakotoarison Liang Tan Chris Summers Carleigh Wood Joshua Lane Mary Williamson Wei-Ning Hsu\n\nAbstract\n\nAudio is an essential part of our life, but creating it often requires expertise and is time-consuming. Research communities have made great progress over the past year advancing the performance of large scale audio generative models for a single modality (speech, sound, or music) through adopting more powerful generative models and scaling data. However, these models lack controllability in several aspects: speech generation models cannot synthesize novel styles based on text description and are limited on domain coverage such as outdoor environments; sound generation models only provide coarse-grained control based on descriptions like “a person speaking” and would only generate mumbling human voices. This paper presents Audiobox, a unified model based on flow-matching that is capable of generating various audio modalities. We design description-based and example-based prompting to enhance controllability and unify speech and sound generation paradigms. We allow transcript, vocal, and other audio styles to be controlled independently when generating speech. To improve model generalization with limited labels, we adapt a self-supervised infilling objective to pre-train on large quantities of unlabeled audio. Audiobox sets new benchmarks on speech and sound generation (0.745 similarity on Librispeech for zero-shot TTS; 0.77 FAD on AudioCaps for text-to-sound) and unlocks new methods for generating audio with novel vocal and acoustic styles. We further integrate Bespoke Solvers, which speeds up generation by over 25 times compared to the default ODE solver for flow-matching, without loss of performance on several tasks.\n\n\\correspondence\n\nApoorv Vyas , Wei-Ning Hsu \\metadata[Demo]https://audiobox.metademolab.com/\n\n1 Introduction\n\nWhy building audio generative models: Audio is a key component in creating many forms of content, such as movies, podcasts, audiobooks, and Ads. However, audio creation is time-consuming and requires various expertise, such as voice acting, music composing and performing, Foley sound effect creation, and sound engineering. This imposes a great barrier to entry for the general public, making it hard for people to become audio creators. Even for professionals, performing these tasks can still take a lot of time and resources, limiting their productivity. Developing audio generative models that are generalizable, controllable, and high quality can bring transformative changes to the audio creation process, improving the efficiency of the professionals as well as unleashing the creativity for everyone.\n\nProgress of audio generative models: Recently, researchers have made significant progress advancing audio generative models. Speech generative models can mimic any vocal style using audio prompts that are as short as three seconds (Wang et al., 2023a; Shen et al., 2023; Le et al., 2023; Kharitonov et al., 2023), infill a portion of speech to remove transient noise or edit words for any speaker (Le et al., 2023; Shen et al., 2023), synthesize foreign languages in anyone’s voice (Zhang et al., 2023; Le et al., 2023), and create dialogues (Borsos et al., 2023). Music generative models can create music in various styles using a short text description (Schneider et al., 2023; Huang et al., 2023a; Agostinelli et al., 2023; Copet et al., 2023) and infill a portion of music (Li et al., 2023). Sound effect generative models follows a similar paradigm. They are capable of creating and infilling complex acoustic scenes like “birds chirping and water dripping with some banging in the background” given a text description (Yang et al., 2023c; Kreuk et al., 2022; Huang et al., 2023b; Ghosal et al., 2023; Liu et al., 2023b, c). Recent models also extends to more general editing, such as removal or addition of sound events with natural language instructions (Wang et al., 2023b; Liu et al., 2023d).\n\nLimitation of existing models: Existing audio generative models are still limited in controllability and generalizability. First, the real world audio content often contain a mix of speech, music, and sound effects. However, existing audio generative models are mostly modality-specific, which only generate either speech, music, or sound effects. In particular, existing large scale speech generative models (Wang et al., 2023a; Le et al., 2023; Shen et al., 2023) are trained mostly on audiobooks (Zen et al., 2019; Kahn et al., 2019; Pratap et al., 2020), which lacks diversity compared to truly in-the-wild data such as AudioSet (Gemmeke et al., 2017) in terms of expressivity (e.g., non-verbal sounds like coughing, screaming, laughing) and acoustic conditions (e.g., urban, rural, public indoor, stadiums). These models can only generate audio of limited styles and do not capture the correlation between different audio modalities.\n\nOn the other hand, there is a discrepancy between speech and sound/speech generation paradigm. Recent speech generation models mostly use example-based control, where an audio sample of the target style is provided and the style control is more precise; in contrast, description-based control is adopted for music and sound generation, where the model can create novel styles based on natural language prompts. Both approaches have their strengths and weaknesses, but such a discrepancy prevents development of unified models that enjoy the best of both worlds.\n\nLast but not least, existing sound generation models only provide coarse control such as “a man is speaking” when generating speech. Existing datasets do not offer finer-grained captions that characterizes vocal styles in greater details, such as “A middle aged woman from the American South is speaking over the phone in a passionate voice. She speaks in at a fast pace with a high pitch.” Neither do these models enable transcript input to controlling the textual content. Hence, these models can only generate mumbling speech.\n\nDue to a lack of consideration in the language-guided generation of speech within a natural setting, designing proper objective evaluation metrics for such universal models remains an open question that has not been fully addressed by prior works. In objective evaluation, previous speech-oriented studies Guo et al. (2023); Leng et al. (2023); Yang et al. (2023a) often adopt ad-hoc evaluation metrics (e.g., accuracy of pre-defined attributes), making it challenging to generalize to free-form instructions. The joint audio-text embedding network (e.g., CLAP Wu et al. (2023)), widely utilized in text-to-audio generation, is tailored to sound events and frequently falls short in capturing intricate attributes such as accents in speech (see Section 7.1.1).\n\nGoals and overview of our model: To tackle these problems, there are three key objectives of this work. First, we aim to build a unified model for sound and speech in order to generate a wider variety of real-world audio, which is often a mix of both. Second, we want to improve controllability for creating novel styles through enabling multiple input methods, using either reference audio, text description, or a combination of both. Last but not least, to improve model generalization, we want to scale training data and utilize data with different level of supervision.\n\nTo that end, we present the Audiobox framework. Audiobox is built upon Voicebox (Le et al., 2023) and SpeechFlow (Liu et al., 2023a), which are flow-matching based models for transcript-guided speech generation and self-supervised speech pre-training, respectively. To facilitate data scaling and development of downstream models, we first adopt the SpeechFlow pre-training method and pre-train a unified model using large quantities of unlabeled speech, music, and sound effects, referred to as Audiobox SSL (Section 4). To validate the effectiveness of the unified pre-trained model, we fine-tune Audiobox SSL for transcript-guided speech generation (Audiobox Speech, Section 5) and description-guided sound generation (Audiobox Sound, Section 6), showing significant improvements from prior studies.\n\nCombining the best of both worlds, we present Audiobox, the unified model for sound and speech generation in Section 7. It bridges the gap between sound and speech generation by enabling natural language prompts for holistic style control, and furthers disentangled speech control with voice prompts. Our joint model achieves unprecedented controllability for universal audio generation and superior versatility with additional capabilities on top of what Voicebox offers. Audiobox outperforms existing domain specific models on multiple tasks and is close to Audiobox Speech and Audiobox Sound on their corresponding benchmark tasks.\n\nTo facilitate the evaluation of Audiobox and advance research in text-guided universal audio generative models, we propose Joint-CLAP, trained on both sound and speech description data. In comparison to CLAP Wu et al. (2023), Joint-CLAP significantly outperforms CLAP in retrieving description-based speech, and the text-to-audio similarity exhibits a stronger correlation with human judgment.\n\nOrthogonally, to improve performance-efficiency trade-off, we integrate Bespoke Solver, a novel post-training inference optimization methods for flow-matching models. With Bespoke Solver, our models are able speed up by 25x compared to using the adaptive step size dopri5 solver without loss of performance.\n\nAs generative models become more powerful and essential parts of everyone’s life, it is more important than ever to conduct research responsibly and mitigate potential risks. We conducted a series of study demonstrating the fairness is achieved through better representing voices of different demographic groups with data scaling. We also validate the effectiveness of a recent watermarking system (Seamless Communication, 2023), showing the verification is highly effective and robust to adversarial perturbation.\n\n2 Related Work\n\nThis paper is related to a large body of work on large scale generative modeling for audio. As the focus of this work is on universality and controllability, we first discuss controllable generation for modality specific models and then compare with recent studies on universal models that can perform multiple tasks or generate audio in multiple modalities and domains. For the rest of the paper, we will refer to speech, sound, music as different audio modalities, and within modality style variation, such as read speech, spontaneous speech, conversational speech, as different domains.\n\nLarge scale in-context text-to-speech generative models: Over the past few months, there has been significant progress in developing large scale speech generative models (Wang et al., 2023a; Shen et al., 2023; Kharitonov et al., 2023; Le et al., 2023; Yang et al., 2023b; Borsos et al., 2023) that are trained on in-the-wild data at the scale of close to 100K hours (Kahn et al., 2019; Pratap et al., 2020) with minimal supervision, which leads to much better generalization for synthesizing unseen speech styles in a zero-shot fashion. These models are in sharp contrast to conventional regression-based models such as Ren et al. (2021); Shen et al. (2017); Łańcucki (2021), which are trained on highly curated datasets (Yamagishi et al., 2019) containing clean audio, limited style variation, and extensive labels (e.g., speaker and emotion labels).\n\nThe key to successful data scaling in recent work is the adoption of powerful generative models that can capture highly stochastic input-output relationships. For example, VALL-E (Wang et al., 2023a) adopt the token-based autoregressive language modeling approach, which converts speech into discrete tokens with a neural codec model (Défossez et al., 2022) and formulate text-to-speech (TTS) as a conditional language modeling problem given a transcript and an audio prompt (the first few seconds of the target speech). NaturalSpeech2 (Shen et al., 2023) and Voicebox (Le et al., 2023) adopt non-autoregressive diffusion (Ho et al., 2020) and conditional flow-matching models (Lipman et al., 2023). Given a transcript and an audio context (the audio surrounding the target speech), these models iteratively transform a noise sampled from a simple prior to speech, represented as learned latent features or mel spectrograms.\n\nAt the high level, VALL-E performs transcript-guided speech continuation while NaturalSpeech2 and Voicebox perform transcript-guided speech infilling. These models are trained with only transcript supervision, which facilitates data scaling. The style of the generated audio is controlled through the audio prompt or audio context. Note that the style refers to not only voice, but everything other than transcript, including prosody, emotion, acoustic environment, channel, noise, etc. This can be understood as a form of in-context learning: because the audio style tends to be coherent within an utterance, these models learn to infer the style of the target based on its context. In turn, it enables generalization to unseen style, such that speech of any style can be generated by conditioning on an audio prompt/context of the desired style.\n\nWhile the in-context style transfer paradigm is powerful, it also possesses several limitations in terms of controllability. First, audio prompt is the only input mechanism of controlling the audio style. Users cannot provide a descriptive text, such as “a young man speaking with a happy tone in an auditorium” to create diverse speech matching the description, whereas this feature is commonly supported and widely enjoyed for image (Ramesh et al., 2022; Rombach et al., 2022), music (Agostinelli et al., 2023), and sound (Kreuk et al., 2022) generation. Second, disentangled style control is not enabled with the paradigm, where voice and other attributes, such as emotion and acoustic condition, can be controlled independently. This feature is often desired as exemplified in earlier work where emotion and voice can be controlled independently (Hsu et al., 2019; Kulkarni et al., 2021; Nguyen et al., 2023).\n\nNatural language style prompting for controllable speech generation: Studies on controllable speech generation aims to develop models which can generate speech of many different domains and provide input methods for disentangled, flexible, and accurate control. Earlier models often enable control over only a small number of attributes (e.g., speaker and emotion) with a fixed number of options (e.g., happy/sad/neutral for emotion) through one-hot vectors (Nguyen et al., 2023). Such methods are difficult to generalize as it is difficult to represent many speech attributes, such as audio quality, acoustic environment, with one-hot vectors. Nor could information such as “a speaker starts with a slow pace and speeds up” be accurately represented. In-context TTS (Wang et al., 2023a) models greatly improves domain coverage, but has the limitation on flexibility and disentangled control described above.\n\nTo address the limitation, several recent studies also propose to control speech style through natural language prompts. InstructTTS (Yang et al., 2023a) and PromptTTS (Guo et al., 2023) are the two earliest works. They are trained on small scale data with mainly emotion variation and limited number of speakers (7 for InstructTTS and 2 for PromptTTS synthetic setup). In particular, InstructTTS collects human descriptions for 44 hours of speech focusing on only the emotion and a separate speaker ID input is used as model input. Therefore, the natural language prompt is only used for controlling the emotion. PromptTTS recruits human annotators to write descriptions to given four to five attribute labels (emotion, gender, volume, speed, and pitch; emotion label is not available for the real data), and trains models on 2-voice synthetic data as well as LibriTTS (Zen et al., 2019). Because the descriptions of PromptTTS are created based on attribute labels instead of speech samples, these descriptions do not contain additional information compared to the labels and theoretically does not enable finer grained attribute control.\n\nPromptTTS2 (Leng et al., 2023) is a concurrent work which improves upon PromptTTS in two aspects. First, it proposes a automatic description creation pipeline based on speech attribute labeler and large language models, which enables scaling to training on 44K hours of audiobook data. Second, PromptTTS2 adopts a diffusion model to capture the one-to-many relationship given input (transcript and description), whereas PromptTTS adopts a regression model assuming deterministic mapping. Nevertheless, similar to PromptTTS, all the descriptions PromptTTS2 create are derived from four categorical attributes with two to three options each (total 54 combinations). Hence, PromptTTS2 does not provide finer grained control than PromptTTS and has limited coverage on the attributes it can control via natural language prompt.\n\nLarge scale general-domain models for sound and music generation: Text-to-sound (Kreuk et al., 2022) and text-to-music (Schneider et al., 2023) are the emerging paradigms for general-domain sound and music generation, in contrast to earlier studies that generate finite sound effects (Donahue et al., 2018) or instruments (Huang et al., 2018). The text here refers to a holistic description of the target audio, such as “A child shouts while an emergency vehicle siren sounds with the horn blowing.” (Kim et al., 2019) and “The low quality recording features a ballad song that contains sustained strings… It sounds sad and soulful, like something you would hear at Sunday services.” for music (Agostinelli et al., 2023).\n\nSimilar to speech generation, the recent progress can be largely attributed to the advancement in generative models for continuous data (Ho et al., 2020; Huang et al., 2023a; Liu et al., 2023b) and audio tokenizers (Zeghidour et al., 2022; Défossez et al., 2022; Kreuk et al., 2022; Copet et al., 2023; Agostinelli et al., 2023), which enables modeling methods capable of capturing highly stochastic conditional distributions of audio given descriptions for general domain sound/music data.\n\nA key limitation of these models is the ability to control transcript and generate intelligible speech or vocals. These models only take a description as input, which does not specify the transcript when speech is presented. Hence, generating samples with prompts like “a person speaking” often results in speech-like mumbling sound with unintelligible content (Liu et al., 2023b). In other words, these models does not offer an input for users to control transcript, and have not learned language models that allow it to construct and synthesize meaningful sentences given only the description.\n\nUnified model for audio generation: With the great progress made in developing general-domain models for each audio modality, researchers also start exploring unified model that can generate audio beyond a single modality and perform multiple generative tasks. Such a model could potentially learn from different sources of supervision and benefit from knowledge transfer across tasks. There are three concurrent studies that are related to this work.\n\nUniAudio (Yang et al., 2023b) focuses on building a single model that can perform multiple tasks, including text-to-music, text-to-sound, and in-context TTS and natural language style prompted TTS. It follows the VALL-E (Wang et al., 2023a) framework, which tokenizes audio and serializes conditioning input and output audio tokens for training a conditional token-based language model. It is trained on the same speech descriptions collected by PromptTTS, which inherits the same limitations in terms what attributes and how granular they can be controlled through natural language prompts as discussed earlier.\n\nVoiceLDM (Lee et al., 2023) is the most related work. It introduces a transcript input to AudioLDM (Liu et al., 2023b) and controls style through text description embedded with a frozen Contrastive Language-Audio Pre-training (CLAP) model (Wu et al., 2023). During training, CLAP embedding from audio is used for conditioning. VoiceLDM is trained on datasets with rich acoustic variation, and hence is capable of generating speech in diverse acoustic environments. However, the performance in terms of controllability is bounded by the pre-trained CLAP model. Since the CLAP model are trained on audio-caption pairs focus on sound events, the embedding only encodes very coarse information regarding speech attributes. Furthermore, VoiceLDM also follows the sound generation paradigm which always generate audio clips of a fixed size (10 seconds), which is not ideal for speech generation that have variable length in general. Finally, despite that the model can generate non-speech sounds when conditioned on empty transcripts, the performance of sound generation lags behind state-of-the-art models by a large margin.\n\nAudioLDM 2 (Liu et al., 2023c) presents a two-stage model that is applicable to speech, sound, and music generation. It is comprised of a deterministic auto-regressive model that maps conditioning input (e.g., CLAP-embedded audio, description, transcript, image) to semantic features sequence, and a diffusion model which mapping semantic to acoustic features. The structure is similar to SPEAR-TTS (Kharitonov et al., 2023) but with different modeling methods and representations for each stage. Hence, similarly it can leverage unlabeled audio for training the second stage model. While AudioLDM 2 presents a unified framework, empirically separate models for speech and sound/music generation are trained, as the authors noted that different model architecture hyperparameters are required for different modalities.\n\n3 Background\n\nThis work is heavily built upon the training objective and model architecture of Voicebox (Le et al., 2023), and the self-supervised objective of SpeechFlow (Liu et al., 2023a). Both studies adopt conditional flow-matching (Lipman et al., 2023) as the modeling backbone, which is a powerful non-autoregressive generative model for continuous data. We provide a technical overview here.\n\nConditional flow-matching: Conditional flow-matching (FM) (Lipman et al., 2023) is a novel generative modeling method derived from the continuous normalizing flow (Chen et al., 2018) framework. It models the paths that transform samples from a simple prior distribution p0subscript𝑝0p_{0}italic_p start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT to the corresponding samples from the complex data distribution p1subscript𝑝1p_{1}italic_p start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT in a continuous manner. We use flow step t𝑡titalic_t to describe the progress of transformation, where the prior is at t=0𝑡0t=0italic_t = 0 and the data is at t=1𝑡1t=1italic_t = 1.\n\nThe training objective of FM resembles the objective diffusion models (Ho et al., 2020): during training, given a sample x1subscript𝑥1x_{1}italic_x start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT drawn from the data distribution, a random flow step t∼𝒰⁢[0,1]similar-to𝑡𝒰01t\\sim\\mathcal{U}[0,1]italic_t ∼ caligraphic_U [ 0 , 1 ] is sampled, and a noisy version of the data xtsubscript𝑥𝑡x_{t}italic_x start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT as well as its derivative vt=d⁢xt/d⁢tsubscript𝑣𝑡𝑑subscript𝑥𝑡𝑑𝑡v_{t}=dx_{t}/dtitalic_v start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT = italic_d italic_x start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT / italic_d italic_t for the chosen condition path are computed. A FM model u𝑢uitalic_u is trained to predict the derivative vtsubscript𝑣𝑡v_{t}italic_v start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT given t𝑡titalic_t and xtsubscript𝑥𝑡x_{t}italic_x start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT. During inference, to draw a sample x1subscript𝑥1x_{1}italic_x start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT from the learned data distribution, a sample x0subscript𝑥0x_{0}italic_x start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT is first drawn from the prior distribution, and then the ordinary differential equation (ODE) solver is used to estimate x1subscript𝑥1x_{1}italic_x start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT given x0subscript𝑥0x_{0}italic_x start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT and the derivative parameterized by the FM model through integration. Trade-off between accuracy of x1subscript𝑥1x_{1}italic_x start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT estimation and speed can be flexibly selected by configuring the ODE solver.\n\nAt a high level, FM subsumes diffusion models, which correspond to specific paths of the transformation. The authors of Lipman et al. (2023) presented an alternative called optimal transport (OT), which are conditional paths with constant directions and speeds. It is arguably easier to learn and can be more accurately estimated by the ODE solver with fewer steps. The OT path results in better training and inference efficiency as empirically verified in Lipman et al. (2023) and Le et al. (2023).\n\nGiven a sample x1subscript𝑥1x_{1}italic_x start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT and a flow-step t𝑡titalic_t, with the OT conditional path we have xt=(1−(1−σm⁢i⁢n)⁢t)⁢x0+t⁢x1subscript𝑥𝑡11subscript𝜎𝑚𝑖𝑛𝑡subscript𝑥0𝑡subscript𝑥1x_{t}=(1-(1-\\sigma_{min})t)x_{0}+tx_{1}italic_x start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT = ( 1 - ( 1 - italic_σ start_POSTSUBSCRIPT italic_m italic_i italic_n end_POSTSUBSCRIPT ) italic_t ) italic_x start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT + italic_t italic_x start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT and vt=x1−(1−σm⁢i⁢n)⁢x0subscript𝑣𝑡subscript𝑥11subscript𝜎𝑚𝑖𝑛subscript𝑥0v_{t}=x_{1}-(1-\\sigma_{min})x_{0}italic_v start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT = italic_x start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT - ( 1 - italic_σ start_POSTSUBSCRIPT italic_m italic_i italic_n end_POSTSUBSCRIPT ) italic_x start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT, where x0subscript𝑥0x_{0}italic_x start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT is drawn from the prior distribution N⁢(0,I)𝑁0𝐼N(0,I)italic_N ( 0 , italic_I ) and σm⁢i⁢nsubscript𝜎𝑚𝑖𝑛\\sigma_{min}italic_σ start_POSTSUBSCRIPT italic_m italic_i italic_n end_POSTSUBSCRIPT is a small value (10−5superscript10510^{-5}10 start_POSTSUPERSCRIPT - 5 end_POSTSUPERSCRIPT). The FM model u𝑢uitalic_u minimizes:\n\n𝔼t,x1,x0⁢‖u⁢(xt,t)−vt‖2.subscript𝔼𝑡subscript𝑥1subscript𝑥0superscriptnorm𝑢subscript𝑥𝑡𝑡subscript𝑣𝑡2\\mathbb{E}_{t,x_{1},x_{0}}||u(x_{t},t)-v_{t}||^{2}.blackboard_E start_POSTSUBSCRIPT italic_t , italic_x start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , italic_x start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT end_POSTSUBSCRIPT | | italic_u ( italic_x start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT , italic_t ) - italic_v start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT | | start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT . (1)\n\nVoicebox: Voicebox (Le et al., 2023) is a conditional generative model based on FM which additionally conditions on frame-aligned phonetic transcript and masked audio for audio prediction, and conditions on phonetic transcript and masked duration sequence for phone duration prediction. Audio is represented as 80-dimensional Mel spectrograms and are converted to waveform using a HiFi-GAN vocoder (Kong et al., 2020). Duration sequence denotes the number of frames for each phoneme in the transcript.\n\nVoicebox adopts the Transformer (Vaswani et al., 2017) model with U-Net (Ronneberger et al., 2015) connections. Masked spectrogram (or masked duration), frame-aligned phone embeddings (or phone embeddings), and noisy audio xtsubscript𝑥𝑡x_{t}italic_x start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT (or noisy duration) are concatenated along the channel dimension and projected to the Transformer feature dimension. The flow step sinusoidal embedding is then concatenated with the project features along the time dimension, passed as input to the Transformer model. The Transformer output is then projected to 80 dimensions (or 1 dimension for duration) and predicts the derivative vtsubscript𝑣𝑡v_{t}italic_v start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT.\n\nIt is a supervised model trained on 60K hours of audiobooks and achieves state-of-the-art performance on in-context text-to-speech synthesis that can mimic the audio style given a three second audio prompt. It is also high versatile due to the generality of transcript-guided infilling, where the model can perform transient noise removal, diverse style generation, speech editing, cross-lingual style transfer by simply forming transcript and audio inputs differently.\n\nSpeechFlow: SpeechFlow (Liu et al., 2023a) is a self-supervised framework based on FM with learns to infill speech given the audio context. This is equivalent to Voicebox without conditioning on transcripts. The self-supervised objective tackles label scarcity issues and enables the model to learn from large quantities of unlabeled speech the distribution of speech as well as the correlation between temporal segments within an utterance.\n\nFine-tuning SpeechFlow with the same transcript-guided infilling objective as Voicebox shows superior performance and sample efficiency, matching style similarity of VALL-E (Wang et al., 2023a) with only 10 hours of labeled data. The pre-trained model also demonstrates promising improvements on other speech generation tasks, including source separation and speech enhancement. It also enables parameter efficient fine-tuning like LoRA (Hu et al., 2021) and fine-tuning with a much lower batch size, demonstrating the efficiency and reusability of self-supervised pre-train models.\n\n4 Audiobox SSL: Self-supervised Generative Audio Pre-training\n\nOur first step is to develop Audiobox SSL, a foundation model that can be fine-tuned for any downstream audio generation tasks. Because labeled data are not always available or of high quality, and data scaling is the key to generalization, our strategy is to train this foundation model using audio without any supervision, such as transcripts, captions, or attribute labels, which can be found in larger quantities.\n\n4.1 Method\n\nWe adapt Audiobox SSL from SpeechFlow, which was originally designed for generative speech pre-training. The same learning objective is also meaningful for general audio: through learning to infill, the model can also capture the temporal relationship of audio events (e.g., clock ticking sound at fixed time interval, approaching train producing sounds with increasing volume), and learns the distribution of general audio. Therefore, during supervised fine-tuning, a model does not need to learn what a natural audio sample sounds like, but only needs to learn aligning the label with the corresponding mode of distribution.\n\nThe original SpeechFlow model is trained to predict spectrograms and uses a HiFi-GAN model to generate waveform given spectrogram. However, HiFi-GAN does not generalize well to non-speech audio such as sound or music (Lee et al., 2022). To tackle that, we train the model to predict latent features learned by an autoencoder. In particular, we use the dense Encodec (Défossez et al., 2022) features which are extracted prior to the residual quantization layer, which demonstrates good resynthesis quality in various audio modalities and has been adopted for sound and music generation (Kreuk et al., 2022; Copet et al., 2023). This is similar to the latent diffusion framework (Rombach et al., 2022) that is also adopted in NaturalSpeech2 (Shen et al., 2023).\n\nDuring training, the model is conditioned on fully masked features with probability pcondsubscript𝑝condp_{\\text{cond}}italic_p start_POSTSUBSCRIPT cond end_POSTSUBSCRIPT. With probability 1−pcond1subscript𝑝cond1-p_{\\text{cond}}1 - italic_p start_POSTSUBSCRIPT cond end_POSTSUBSCRIPT, a subset (nmask)n_{\\text{mask}})italic_n start_POSTSUBSCRIPT mask end_POSTSUBSCRIPT ) of frames are masked with minimum span length lmasksubscript𝑙maskl_{\\text{mask}}italic_l start_POSTSUBSCRIPT mask end_POSTSUBSCRIPT. The FM loss is computed only on masked frames. When a frame is masked, its features are set to 00.\n\n4.2 Experimental Setup\n\nTraining data: We collect an large scale audio dataset that greatly increases the domain coverage, modality coverage, and quantities compared to previous large scale audio generative model studies (Yang et al., 2023b; Borsos et al., 2023; Wang et al., 2023a; Liu et al., 2023c), which leverage datasets ranging between 10K to 100K hours containing mostly speech from a single domain (e.g., audiobooks).\n\nSpecifically, our dataset includes over 160K hours of speech (primarily English), 20K hours of music and 6K hours of sound samples. The speech portion covers audiobooks, podcasts, read sentences, talks, conversations, and in-the-wild recordings including various acoustic conditions and non-verbal voices. To ensure fairness and a good representation for people from various groups, it includes speakers from over 150 countries speaking over 200 different primary languages. We refer to this set as “Mix-185K.”\n\nModel and training: We train a 24 layer Transformer Vaswani et al. (2017) with convolutional position embeddings Baevski et al. (2020) and symmetric bi-directional ALiBi self-attention bias Press et al. (2021). The model has 16 attention heads, 1024/4096 embedding/feed-forward network (FFN) dimension, and 330M parameters. We add UNet-style skip connections, where states are concatenated channel-wise and then combined using a linear layer.\n\nThe model is trained for 1 million updates with an effective batch size of 480K frames. For efficiency, samples are randomly chunked if they exceed 1,600 frames. We set pcond=0.1subscript𝑝cond0.1p_{\\text{cond}}=0.1italic_p start_POSTSUBSCRIPT cond end_POSTSUBSCRIPT = 0.1, nmask∼𝒰⁢[70%,100%]similar-tosubscript𝑛mask𝒰percent70percent100n_{\\text{mask}}\\sim\\mathcal{U}[70\\%,100\\%]italic_n start_POSTSUBSCRIPT mask end_POSTSUBSCRIPT ∼ caligraphic_U [ 70 % , 100 % ], and lmask=10subscript𝑙mask10l_{\\text{mask}}=10italic_l start_POSTSUBSCRIPT mask end_POSTSUBSCRIPT = 10. We use the Adam Kingma and Ba (2014) optimizer with learning rate 1e-4, linearly warmed up for 5k steps and linearly decayed over the rest of training. For stability, we use gradient norm clipping with a norm threshold of 0.2.\n\n5 Audiobox Speech: Scaling In-context Text-to-speech Synthesis\n\nIn this section, we study the effectiveness of pre-training and fine-tuning data scaling for speech generation. We present Audiobox Speech, which fine-tunes Audiobox SSL with the same transcript-guided speech infilling objective as Voicebox using transcribed speech. The resulting model can be applied to multiple downstream tasks just like Voicebox.\n\n5.1 Method\n\nTo incorporate the frame-aligned transcript z𝑧zitalic_z, we follow Liu et al. (2023a). Specifically, given the noisy Encodec features xtsubscript𝑥𝑡x_{t}italic_x start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT at the flow-step t𝑡titalic_t, masked Encodec features xctxsubscript𝑥ctxx_{\\text{ctx}}italic_x start_POSTSUBSCRIPT ctx end_POSTSUBSCRIPT, we first concatenate xtsubscript𝑥𝑡x_{t}italic_x start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT and xctxsubscript𝑥ctxx_{\\text{ctx}}italic_x start_POSTSUBSCRIPT ctx end_POSTSUBSCRIPT channel-wise and apply a linear project to get xhsubscript𝑥ℎx_{h}italic_x start_POSTSUBSCRIPT italic_h end_POSTSUBSCRIPT. We then apply another linear layer to the frame-aligned transcript embeddings zembsubscript𝑧embz_{\\text{emb}}italic_z start_POSTSUBSCRIPT emb end_POSTSUBSCRIPT, and add this to the hidden state xhsubscript𝑥ℎx_{h}italic_x start_POSTSUBSCRIPT italic_h end_POSTSUBSCRIPT. The resulting features are concatenated with the flow step sinusoidal embedding along the time dimension and fed to the Transformer as input. The Transformer output is projected and predicts the derivative vtsubscript𝑣𝑡v_{t}italic_v start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT.\n\nThere are two different approaches to fine-tuning the model. The first one is low-rank adaptation (LoRA) Hu et al. (2021), where we add LoRA adapters to the linear input projection of each self-attention layer. With this approach, only the transcript embedding, projection parameters, along with the LoRA adapter parameters are optimized. The second approach is full fine-tuning, where all parameters are optimized together. Liu et al. (2023a) showed that LoRA achieves better performance when fine-tuning SpeechFlow on 960 hours of speech, but we suspect that full fine-tuning may prevail when we scale fine-tuning data.\n\nIn addition, many prior studies (Le et al., 2023; Wang et al., 2023a) represent transcripts as phoneme sequences and using the off-the-shelf Montreal Forced Aligner (McAuliffe et al., 2017) for aligning the training data. Instead, we represent transcript with raw characters, including punctuation and with true cases, and utilize the SeamlessM4T v2 multilingual char-to-unit forced aligner presented in Seamless Communication (2023) adapted from RAD-TTS (Shih et al., 2021). This aligner is trained on large quantities of multilingual data and can align raw text with speech. There are several benefits with the replacement. First, it circumvents the need of phonemizers and avoids error propagation due to incorrect phonemization. Second, raw text preserves more information than phonemized text, such as casing (e.g., all caps for emphasis) and punctuation. Third, the SeamlessM4T v2 aligner is much more robust than MFA and can handle multilingual/code-switching text, which enables easier extension to multilingual TTS systems and is more suitable for aligning challenging speech such as conversational and noisy samples.\n\nFollowing Le et al. (2023), we train a flow-matching duration model only with labeled data. It was shown in Le et al. (2023) that FM duration model has better diversity compared to regression duration models. However, it is less stable and sometimes produces unnatural prosody. To alleviate the issue, we propose to average over a small number of duration sequences for stabilization, which empirically shows better trade-off between diversity and quality. The averaging operation is reasonable as duration distributions are relatively unimodal. When averaging more samples, it approaches the mean, which is the estimation produced by regression models.\n\n5.2 Task and Evaluation\n\nWe consider the in-context TTS (also known as zero-shot TTS) task. In-context TTS aims to synthesize speech that resembles the audio style of the given an audio example which may be unseen during training. The audio style refers to not only voice, but everything other than transcript, such as prosody and acoustic condition. To perform the task, input raw/frame-level transcript is the concatenation of the raw/frame-level transcript of the audio example and the target raw/frame-level transcript, while the masked audio/duration is the concatenation of the example audio/duration and a mask for the speech/duration to be generated. We first sample duration sequence for the target raw transcript to create frame-level target transcript using the duration model, and then sample audio with the audio model.\n\nThe performance is measured in terms of style similarity, content correctness, and quality. A proxy automatic metric for style similarity is the cosine similarity between the audio prompt and the generated audio in some embedding space that reflects the audio style. WavLM-TDCNN (Chen et al., 2022b) is commonly used for embedding (Wang et al., 2023a; Kharitonov et al., 2023; Le et al., 2023). Le et al. (2023) advocates for reporting both similarity with respect to raw audio (SIM-orig) and to audio resynthesized from the same vocoder (SIM-resyn) for comparability across studies (SIM-orig). Content correctness can be approximated with the word error rate (WER) from some speech recognition model; however, WER can result from both synthesis error and recognition error, and hence is less reliable when numbers are close or when the target style is more difficult to recognize (e.g., accented speech, conversational speech, noisy speech). In this paper we use Whisper large-v2 instead of HuBERT-L Hsu et al. (2021) used in prior studies (Wang et al., 2023a; Le et al., 2023) because the latter is less robust and has higher WER on real data for non audiobook domains. Subjective evaluations are often used for assessing style similarity and audio quality, measured by mean opinion scores (MOS).\n\n5.3 Experimental Setup\n\nTraining data: We train Audiobox Speech on a transcribed English subset of the speech data used for pre-training. The subset contains 100K hours of speech covering similar domains as the full set, which we refer to as “SP-multi-100K.” We create the transcribed subset with the following pre-processing methods:\n\nFor unsegmented multi-speaker conversational datasets information, we first segment our dataset using PyAnnote diarization toolkit (Plaquet and Bredin, 2023; Bredin, 2023) to create single speaker speech segments. For untranscribed speech, we transcribe data using two speech recognition models, Whisper Radford et al. (2022) large-v2 and medium.en. For each audio with unknown language, we additional use the Whisper large-v2 model for language identification (LID). We then remove the utterances where the probability being English is lower than 50% or the the word error rate (WER) between the transcriptions from the two models is greater than 50%.\n\nTo create a similar text distributions across multiple datasets, we apply inverse text normalization to create true-cased and punctuated transcript for any dataset with normalized transcript using Whisper-punctuation library. It performs the task through constrained search where the produced transcript needs to match the original transcript after normalization.\n\nModel and training: We adopt the full fine-tuning method and train the audio model for 200K steps with an effective batch size of 240K frames. Samples are randomly chunked if they exceed 1,600 frames. Character embeddings are 128 dimensions. For each batch, audio is entire masked with probability 0.3; otherwise a contiguous chunk is masked where the chunk size 70% to 100% of the frames. The same optimizer, learning rate, scheduler, and gradient clipping as Audiobox SSL are used.\n\nThe duration model has 8 heads, 768/2048 embedding/FFN dimensions, 10 layers, with 40 dimension character embeddings. It is trained for 600K updates with an effective batch size of 120K frames. For each batch, duration is entirely masked with probability 0.2 and otherwise a chunk of 10% to 100% of the sequence length is masked. The rest of the optimization parameters are the same as the audio model.\n\nEvaluation data and configuration: For in-context TTS, three second prompts are used following Wang et al. (2023a). Voicebox uses the last three seconds of the reference as the prompt, which often contains a considerable amount of trailing silence. We instead use the last three seconds after removing the trailing silences based on the forced alignment for all experiments in this paper. Duration is estimated by averaging over five samples and following (Le et al., 2023) predicted silence at both ends are trimmed to 0.1 second max.\n\nThe torchdiffeq (Chen, 2018) package is used. By default, we use the midpoint solver with a step size of 0.0625, which invokes the derivatives being evaluated 32 times. When using classifier free guidance the model does 2 forward passes per evaluation, leading to a total of 64 calls to the model. A guidance weight for classifier-free guidance (Ho and Salimans, 2022) of 0.7 is applied.\n\nModels are evaluated on five datasets representing different domains. (1) Librispeech test-clean (LS) (Panayotov et al., 2015): audiobook recordings that are scripted and relatively clean. Following Wang et al. (2023a), we keep only samples between 4 to 10 seconds for evaluation to compare with prior studies. (2) CommonVoice v13.0 English test set (CV) (Ardila et al., 2019): sentences read by volunteers worldwide. It covers broader accents and are noisier compared to Librispeech. (3) Switchboard (SWBD) (Godfrey et al., 1992): a conversational speech corpus. We evaluate on a subset of 611 samples from 8 speakers. (4) Expresso (Nguyen et al., 2023) (Expr) is a multispeaker expressive speech dataset covering 7 different speaking styles, which we evaluate on a subset of 999 samples. (5) An internal expressive and accented dataset (Accent): read sentences with speakers covering a wider range of accents and 10 emotions. We create a subset of 500 samples for evaluation.\n\n5.4 Main Results\n\nWe compare Audiobox Speech with several state-of-the-art in-context speech generation models. Voicebox, VALL-E, NaturalSpeech 2 (NS2), and YourTTS are trained on 60K, 60K, 44K, 600 hours of audiobooks respectively. UniAudio is trained on about 100K hours of audio, where speech accounts for 81K hours and are mostly audiobooks. Results are shown in Tables 1 and 2.\n\nAudiobox Speech achieves a new best on style similarity (0.745 vs. 0.710 from UniAudio) on the audiobook domain test set (LS). More importantly, Audiobox Speech drastically improves Voicebox on all other domains, with similarity improvement ranging from 0.096 to 0.156. The results suggest that Audiobox Speech generalizes much better thanks to scaling data to cover more domains. The subjective evaluations presented in Table 2 again confirms that Audiobox Speech transfers styles significantly better than the baselines, and generate audio with better quality.\n\n5.5 Ablation Study\n\nWe present ablation studies in Table 3. To understand the effect of data scaling, we create a subset containing 60K hours of audiobook speech referred to as “SP-book-60K”, which is a subset of the 100K hour multi-domain speech we have (SP-multi-100K).\n\nWe first compare the top two rows, which differ in the pre-training data and are both fine-tuned with LoRA. Results suggest that while WER remains similar, scaling pre-training data greatly improves style similarity, especially on domains not covered in the fine-tuning data (CV, SWBD, Expr, Accent). On the other hand, scaling fine-tuning data from SP-book-60K to SP-multi-100K does not appear to improve much on similarity. This potentially results from the fact that pre-training data is a superset of fine-tuning data, and hence fine-tuning has little to learn on style transfer and focuses on aligning transcript with speech.\n\nComparing the third and the fourth row, we see that by fine-tuning the whole model, style similarity improves slightly and WER improves greatly on most of the domains (23% to 43% relative WER reduction). The only exception is on SWBD, which are 8kHz narrowband recordings that are likely less represented in the fine-tuning data. Finally, we compare the last two rows and confirm that using audio prompts without silence leads to drastic improvements on similarity on datasets which tend to have long trailing silences (CV, Accent), while overall maintaining the WER. This is because the silence is not informative for inferring the target style.\n\n6 Audiobox Sound: Simple Text-to-sound Generation and Infilling\n\nIn this section, we present Audiobox Sound, a model for text-guided generation of general sound. The task is also referred to as text-to-audio generation (TTA) in many prior works(Liu et al., 2023b; Huang et al., 2023b; Kreuk et al., 2022). It aims to generate general audios given a holistic text description. In contrast to text-to-speech synthesis, the text cannot be frame-wise aligned to audio. Furthermore, sound data only constitutes a small portion of the whole training data. Thus we investigate whether general audio pre-training is able to bring gains to generation of audios of specific domain, which we take sound generation as an example. While we focus on generation of sound events, the technique can similarly apply to other areas (e.g., music).\n\nMost prior works Liu et al. (2023b); Ghosal et al. (2023); Liu et al. (2023c); Huang et al. (2023b); Yang et al. (2023c) build the diffusion models upon a constrained latent space, commonly learned through autoencoding. Such strategy has shown to improve the data efficiency Rombach et al. (2021). In this work, we adopt a different approach, which directly builds the flow matching network on auto-encoding based latent representation of raw waveforms. Such methodology has been largely explored in the language model space Kreuk et al. (2022); Copet et al. (2023); Agostinelli et al. (2023), which typically requires to build a billion-scale model to achieve comparable performance to the alternatives aforementioned. Here we show that by leveraging such simple strategy the flow matching models can achieve SOTA performance while being highly efficient (e.g., >2absent2>2> 2x smaller than Kreuk et al. (2022)).\n\n6.1 Method\n\nSimilar to speech generation, we model the text-conditional sound distribution with flow matching. In contrast to learning phoneme encoding from scratch, we employ a pre-trained text encoder to map audio captions into word embeddings. Due to the lack of alignment between audio and text embedding, a cross-attention layer is applied in each transformer layer to allow the model attend to the whole text sequence in modeling the gradient distribution, similar to Ghosal et al. (2023); Liu et al. (2023b, c); Kreuk et al. (2022).\n\nDifferent from prior works in TTA such as AudioLDM (Liu et al., 2023b), AudioLDM2 (Liu et al., 2023c), Tango (Ghosal et al., 2023), we do not rely on an off-the-shelf variational auto-encoder (Kingma and Welling, 2014) to map the low-level audio representation (mel spectrogram) into a latent space and model the distribution in the original embedding space directly. This streamlines the model architecture and reduces the necessity of introducing excessive trainable parameters during fine-tuning, thus bridging the gap between pre-training and fine-tuning.\n\nExcept for the cross-attention layers, all the remaining parameters are initialized based on the pre-trained model introduced in Section 4. Similar to text-to-speech synthesis, parameter-efficient fine-tuning strategy like LoRA Hu et al. (2021) can be applied in text-to-audio generation. In practice, we observed fine-tuning the whole model leads to significantly better performance and thus choose to fine-tune the whole model by default (see Section 6.5).\n\nMulti-stage fine-tuning: Compared to transcripts for text-to-speech synthesis, high-quality audio captioning data are much more scarce. Typically, public audio captioning datasets include fewer than 1000100010001000 hours of audios, which is orders of magnitude smaller than the speech datasets. On the other hand, the larger-scale sound data often contain noisy category labels and has distributional shift in the audio category (Kim et al., 2019). To mitigate this issue, we divide the fine-tuning process into two stages, which is based on low-quality (e.g., tags) and high-quality (e.g., human written captions) audio descriptions respectively. Weights of the first model are used to initialize the subsequent model. We argue the labeled data used in first stage, despite its noisy nature, is helpful for learning the text conditional distribution (see Section 6.5).\n\n6.2 Tasks and Evaluation\n\nWe consider the following two sound generation tasks: text-to-sound (TTA) generation and text-guided audio infilling (TAI). We use AudioCaps test set (Kim et al., 2019), a standard benchmark for sound generation (Kreuk et al., 2022; Liu et al., 2023b, c; Yang et al., 2023b; Lee et al., 2023; Ghosal et al., 2023), to evaluate all models. For TTA, the model is evaluated standard Frechet Audio Distance (FAD) (Kilgour et al., 2019), Frechet Distance (FD) and KL divergence (KLD) based on the pre-trained audio event tagger PANN (Kong et al., 2019), and Inception score (IS) (Salimans et al., 2016). FAD and FD measure distribution-level similarity between reference samples and generated samples. KLD is an instance level metric computing the divergence of the acoustic event posterior between the reference and the generated sample for a given description. IS measures specificity and coverage for a set of samples without requiring references, which assigns a higher score if instance posteriors have low entropy and marginal posterior has high entropy. The metrics are implemented following the audioldm_eval toolkit. . In addition, we calculate the similarity between generated audio and text description using the CLAP model Wu et al. (2023) .\n\nIn TAI, the model is conditioned on p%percent𝑝p\\%italic_p % of the ground-truth audio as context to infill the remaining (100−p)%percent100𝑝(100-p)\\%( 100 - italic_p ) %, in addition to the text description of the whole audio. In particular, p𝑝pitalic_p is set to be 30 and the middle 70%percent7070\\%70 % are the region to fill in. In addition to the metrics for TTA, we further measure the similarity to the reference audio (CLAP-aa), which is the cosine similarity between CLAP embeddings of the generated and reference audio.\n\nIn addition to the objective metrics aforementioned, we also conduct subjective evaluation to evaluate two main aspects of the generated audio: overall naturalness (OVL) and relevance to text input (REL), similar to Kreuk et al. (2022); Liu et al. (2023b). For these two metrics, raters were asked to rate the perceptual quality and the match between audio and text of the audio samples in a range between 1 and 5 similar to MOS. Based on the evaluation protocol Kreuk et al. (2022), the subjective evaluation is done on 100 randomly sampled files from AudioCaps test set. Each sample is evaluated by 5 annotators from professional annotation service. We list the annotation interface in Appendix D.\n\n6.3 Experimental Setup\n\nData: To train Audiobox Sound, we use about 6K hours of audio data, among which ∼150similar-toabsent150\\sim 150∼ 150 hours are captioned audios (SD-cap-150) and the remaining ones only consist of audio tags (SD-tag-6K). During the first-stage fine-tuning, the whole dataset is used while only the captioning data are used in the second stage. To tackle the ontology of audio tags, we concatenate the tags of different levels as the pseudo-caption of the audio. See Table 4 for example audio description in these two sources.\n\nImplementation Details: We use T5-base (Raffel et al., 2020) to map the text description into embeddings. Each cross-attention layer has 16 heads and its implementation remains same as the self-attention layers except that keys and values are text embeddings. The time-step embedding is added to the T5 embedding before being attended to. In the first stage, we fine-tune the model for 200K updates with an effective batch size of 720K frames. During the second stage, we further fine-tune the model for 100K updates with an effective batch size 240K frames. For both stages, the learning rate and gradient clipping are set to 0.0002 and 0.2 respectively. For inference, we use dopri5 solver with absolute and relative tolerance of 10−5superscript10510^{-5}10 start_POSTSUPERSCRIPT - 5 end_POSTSUPERSCRIPT as the default option. The classifier-free guidance weight is tuned between 0 and 5 and we found setting it to 1 leads to the best result. For each text prompt, we generate 32 random samples and select the one with the highest CLAP similarity to the text prompt. For audio infilling, the masked audio is always kept for conditioning and only the text description is optionally dropped for classifier free guidance.\n\nBaselines: We compare Audiobox Sound against models from the faimily of AudioLDM2 Liu et al. (2023c) and TANGO Ghosal et al. (2023), which stand as current SOTA approaches for general audio generation Liu et al. (2023c).\n\n6.4 Main Results\n\nText-To-Audio: Table 5 compares our model to prior audio audio generation models in TTA. Audiobox Sound consistently outperforms all prior works in both objective and subjective evaluation by a large margin, though it is significantly more parameter efficient. It is also worth noting compared to many approaches listed in Table 5, the sound training data we used is also fewer. This further reveals the effect of general domain pre-training for sound generation.\n\nText-To-Audio Infilling: Table 6 shows the the performance of Audiobox Sound on TAI, as well as its comparison to prior works. Our model outperforms prior works by a large margin as well on this task. Compared to TAI, we noticed a mixing result according to different metrics. Noticably, the trend on FAD and KLD is not consistently, as in the comparison between TTA and TAI. This can be related to the sensitivity of metrics. On the other hand, the similarity between the generation and reference is greatly increased (CLAP-aa: 0.61→→\\rightarrow→0.77) when the context is fed into the model, which suggests the improvement of coherence to the original audio when context is employed.\n\nInference efficiency: In addition to quality metrics, we further show the quality-speed trade-off at inference time in Figure 2. Specifically, we vary the number of inference steps, which correspond to the step size in the ODE solver for our model and the number of DDIM steps in TANGO and AudioLDM2. Audiobox Sound achieves consistently higher quality (lower FAD) with the same number of inference steps compared to AudioLDM2 and Tango. This implies the better efficiency of the flow-matching approach Audiobox is based on, as is similarly demonstrated in Le et al. (2023).\n\n6.5 Analysis and Ablation Study\n\nAblation Study: Here we conduct an ablation study showing the effect of different components of Audiobox Sound. Specifically, we vary the following training strategies: training with SD-cap-150 only, training with SD-tag-6K and SD-cap-150, training with the whole speech, music and sound datasets.\n\nAs is shown in Table 7, using a general pre-trained model boosts the performance by ∼20%similar-toabsentpercent20\\sim 20\\%∼ 20 % in FAD. Despite the discrepancy in task and data domain, generation of universal audios is a beneficial pretext task for text-to-sound generation. As music and speech constitutes a significant portion of our evaluation set, increasing the scale of these two modalities in pre-training provides additional benefits. Furthermore, the two-stage fine-tuning also consistently outperforms fine-tuning with SD-cap-150 only regardless of using a pre-trained model or not. The gain is mostly attributed to scaling up in-domain training data (i.e., sound only). Despite the labels being different, simply using audio tags can still enhance learning the mapping between the description of events and the actual audio. Finally, comparing the last two rows of Table 7 suggests reranking with CLAP model is an effective approach to improving the overall performance in both the audio quality (FAD: 0.91→0.78→0.910.780.91\\rightarrow 0.780.91 → 0.78) and text-audio relatedness (CLAP score: 0.60→0.71→0.600.710.60\\rightarrow 0.710.60 → 0.71).\n\nFine-tuning strategy We compare the two different fine-tuning strategies: LoRA vs. full model fine-tuning. For LoRA, we add LoRA adaptors described in Section 5 to self-attention layers. In contrast to full-tuning where the whole model is fine-tuned, only the adaptors and cross-attention layers will be updated during fine-tuning and all the remaining parts are frozen. LoRA fine-tuning is on average 15%percent1515\\%15 % to 30%percent3030\\%30 % worse (relative) than its full fine-tuning counterpart. The incorporation of cross-attention layers induces large architectural change to the model, which increases the necessity of fine-tuning the whole model.\n\n7 Audiobox: Toward Universal and Controllable Audio Generation\n\nIn previous sections, we discussed speech and sound generation independently. This section presents Audiobox, a single model that can produce both speech and audio conditioned on text description or audio example. Fine-tuning our pre-trained model for this joint task enables natural language instruction to control the output speech attributes like perceived age, gender, quality on top of example-based control (ZS-TTS). Furthermore, training on wide variety of data enables simulating voices in different environments and accompanied by acoustic events such as birds chirping, applause. We further envision a scenario where the user would like to restyle the given audio example with natural language instruction. For example, change the audio style to make it sound like it is recorded in a cathedral. This requires disentangled vocal style control using an additional utterance from the same speaker called voice prompt.\n\nWe design Audiobox to enable speech and sound generation capabilities previously discussed in Sections 5 and 6. Furthermore through voice prompt and description we also envision vocal style transfer to more complex acoustic scenes enabled through joint training. Below we discuss in details speech caption and voice prompt modeling, data creation, and experiments.\n\n7.1 Data Creation\n\n7.1.1 Speech Captions\n\nWe aim to bridge the gap between speech and sound datasets by supporting description-based control for speech generation. We consider both human annotations and automatically created captions\n\nAutomatic captions: Given the lack of any dataset with fine-grained description for speech, we generate speech captions using a large language model (LLM) with speech attribute tags extracted either using existing metadata or use pseudo labels using classifiers. We extract the following attributes: (1) age: 4 classes (2) gender: 2 classes (3) audio quality: 3 classes (4) pitch: 3 classes (5) speaking rate: 3 classes (6) accent: open-vocabulary (7) emotion: open-vocabulary (8) environment: open-vocabulary More details can be found in Appendix A.\n\nGiven the above attributes, we use the LLAMA2 7B model Touvron et al. (2023) to convert them into captions. To capture different writing styles, we prompt the model a style bank mimicking different characters with example writing samples. A few of them are listed below:\n\n•\n\nA young male adult voice, conveys anger and frustration. The audio, of normal quality, is recorded inside a small space. The person speaks with South Asia accent and a normal speaking pace.\n\n•\n\nThis young bloke’s ticked off, audio’s all good. He’s in some small space and has a South Asian accent. Talks normal speed.\n\n•\n\nGot this young dude who’s mad, audio’s decent. He’s in a tight spot, has that South Asian accent, and talks at a chill pace.\n\n•\n\nYoung man is angry. Audio is okay, small place. Accent from South Asia. Speaks normal.\n\nTo further improve coverage over different environment and background sounds, for each utterance, we apply a random augmentation by convolving with a random room impulse responses (RIR) from a set of known environments and optionally add add a background noise from a set with known tags.\n\nWe also generate the corresponding caption with updated environment and background noises using the LLAMA2 7B model. When adding any background noise to the utterance, we update the quality to “low”. For utterances applied only RIR we update the quality to be “normal” if the original quality was “studio”. We do not apply utterances with low audio quality since those may not be suited for RIR augmentations.\n\nHuman annotations: We create human-based annotation to gather more fine-grained description and better alignment towards human hearing perception. We select a 500 hour subset of SP-multi-100K described in Section 5.5.\n\nIn the annotation guidelines, we ask the annotator to describe the perceived attribute such as: gender, age, accent, emotion, environment, tonal variation, speaking pace, pitch, emotion, audio quality, vocal style and any miscellaneous details from the speech utterances. In addition to this we also collect categories for the attributes. To ensure we get high quality description, we filter annotators in two stages. First, we keep annotators who successfully labeled pre-selected gold samples with high accuracy. We additionally use an LLM to automatically rate the quality annotations to ensure high quality detailed captions to complement our automatic caption above. More details on quality can be found in Appendix B. Here are some captions example curated by our human annotator:\n\n1.\n\nA young woman with an American accent speaks in a higher pitched voice. She speaks at a normal pace with a bit of a muffled voice. She is outside in an urban area and cars can be heard passing by in the background. She has a happy and excited tone that is slightly melodious. The audio is of poor quality and dog barking can be heard at the end.\n\n2.\n\nA middle aged man with a mildly masculine voice seems to be outside in a rural or natural environment with a moderately background noise of birds singing. He seems to be in a neutral mood when show casing a house to some people. His voice is hoarse/rough speaking at a slow pace with an average voice pitch.\n\n7.1.2 Voice Prompts\n\nNatural language description alone allows user to control styles through describing attributes such as age, accent, emotion, pitch, and environment. However, a user maybe interested in synthesizing a specific vocal style and while changing other attributes such as quality, emotion, background. This requires disentangled control between the input voice sample and natural language text prompt.\n\nFor each target utterance, we sample an additional utterance from the same speaker to serve as voice prompt during training. The voice prompt is selected such that it differs from the target utterance on one or more attribute such as emotion, environment, and speaking rate. This is to de-correlate the target and prompt on everything but vocal similarity. We additionally apply a random room impulse response and background noise augmentation to the voice prompt to increase robustness as well as further de-correlation.\n\nNote that this is different from passing the audio as audio context (zero-shot TTS) where we expect the model to copy over emotion, environment and other background details as well. Here we would want the model to transfer only the vocal style from prompt and use the description for other details such as environment and emotions.\n\n7.2 Method\n\nAudiobox (Figure 1) conditions on both transcript and masked audio features (same as Audiobox Speech) and captions (same as Audiobox Sound) for description conditional generation. To unify training, for sound inputs without transcript, we create a pseudo-transcript that contains “<sound>” tokens each of length 1 second filling the length of audio. We additionally condition on the another utterance from the same speaker (voice prompt). As described in Section 7.1.2, the voice prompt is selected in a adversarial fashion to enable disentangled control. For audios with missing prompts, we feed a pseudo voice prompt of length 0.10.10.10.1s filled with zeros. The voice prompt is embedded by a lightweight Transformer. We then concatenate the output with the caption description embedding for cross-attention. We randomly initialize the parameters for the cross-attention, description projection, and character embedding weights. All other parameters are initialized based on Audiobox SSL in Section 4. Similar to the sound model training in Section 6, we use multi-stage fine-tuning as described next.\n\nMulti-stage fine-tuning: Except for the high quality 500500500500 hours of speech captions that we collect, the rest of our speech captions are generated using attribute tags and an LLM. Furthermore most of the datasets do not provide any meta-data further limiting the quality of the captions. To mitigate this issue we train our model in two stages. In the first stage we use all the captions for speech and audios. To avoid under-fitting on the audio events generation, we upsample the audio data such that the ratio of total speech and audio data in hours is about 3:1:313:13 : 1. In the second stage, we initialize the model from first stage weights and only train on the high quality data that comprises 500500500500 hour of annotated speech captions and a few other datasets with emotion and accent metadata for rich LLM captions. We again upsample the audio data such that the ratio of total speech and audio data is about 2.6:1:2.612.6:12.6 : 1.\n\n7.3 Task and Evaluation\n\nIn our unified Audiobox model, the model is capable of new generation tasks such as description-guided TTS (transcript + description) and description-guided TTS with extra voice conditioning generation (transcript + description + voice prompt). Additionally, Audiobox also maintains generation capability from all prior section including: diverse speech sampling (transcript only), zero-shot TTS (transcript + context prompt) (see Section 5.2), text-to-sound (TTA) generation (description only) and text-guided infilling (TAI, description + context prompt) (see Section 6.2). In Appendix C, describe the tasks and inputs in detail.\n\nFor all speech generation tasks, we measure the WER and similarity of vocal style if context or voice prompt is provided. In addition, for any generation task with description conditioning, we measure the similarity between description and generated audio with cosine similarity between CLAP text and audio embedding. For the description-guided TTS, in addition to objective metric, we also conduct subjective evaluation to assess the QMOS and REL. Below, we provide details on the CLAP model used for speech evaluation.\n\n7.3.1 Joint-CLAP similarity\n\nIn terms of tasks, generating speech conditioned on text descriptions is similar to description-guided sound generation (TTA). As is common in TTA, we also employ the text-to-audio similarity to measure how well the generated audio matches the description. However, unlike TTA scenario, joint text-audio embedding models such as CLAP Wu et al. (2023) cannot be straightforwardly applied to the speech domain. Existing CLAP models are trained with coarse description about speech, such as \"a person speaking\". The model is unable to distinguish fine-grained speaking styles like accent or emotion. Although there exist public CLAP models which are trained with speech data, most of them are trained with (speech, transcript) pairs which is orthogonal to the text description. Thus, for the purpose of evaluating description-conditioned speech generative models, we propose Joint-CLAP model, which is designed for both description-based speech and audio evaluation.\n\nTraining Similar to CLAP Wu et al. (2023), Joint-CLAP consists of an audio and text branch, each responsible for encoding audio waveforms and the natural language sentences respectively. Given a speech-text pair (xa,xt)superscript𝑥𝑎superscript𝑥𝑡(x^{a},x^{t})( italic_x start_POSTSUPERSCRIPT italic_a end_POSTSUPERSCRIPT , italic_x start_POSTSUPERSCRIPT italic_t end_POSTSUPERSCRIPT ), the audio and text branch fasubscript𝑓𝑎f_{a}italic_f start_POSTSUBSCRIPT italic_a end_POSTSUBSCRIPT and ftsubscript𝑓𝑡f_{t}italic_f start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT encodes it into the embedding pair (ea,et)superscript𝑒𝑎superscript𝑒𝑡(e^{a},e^{t})( italic_e start_POSTSUPERSCRIPT italic_a end_POSTSUPERSCRIPT , italic_e start_POSTSUPERSCRIPT italic_t end_POSTSUPERSCRIPT ): ea=fa⁢(xa)superscript𝑒𝑎subscript𝑓𝑎superscript𝑥𝑎e^{a}=f_{a}(x^{a})italic_e start_POSTSUPERSCRIPT italic_a end_POSTSUPERSCRIPT = italic_f start_POSTSUBSCRIPT italic_a end_POSTSUBSCRIPT ( italic_x start_POSTSUPERSCRIPT italic_a end_POSTSUPERSCRIPT ), et=ft⁢(xt)superscript𝑒𝑡subscript𝑓𝑡superscript𝑥𝑡e^{t}=f_{t}(x^{t})italic_e start_POSTSUPERSCRIPT italic_t end_POSTSUPERSCRIPT = italic_f start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ( italic_x start_POSTSUPERSCRIPT italic_t end_POSTSUPERSCRIPT ). We use the same contrastive loss for model training following Wu et al. (2023); Radford et al. (2021), where τ𝜏\\tauitalic_τ is a learnable parameter.\n\nL=12⁢N⁢∑i=1N(log⁡exp⁡(eia⋅eit/τ)∑j=1Nexp⁡(eia⋅ejt/τ)+log⁡exp⁡(eit⋅eia/τ)∑j=1Nexp⁡(eit⋅eja/τ))𝐿12𝑁superscriptsubscript𝑖1𝑁⋅superscriptsubscript𝑒𝑖𝑎superscriptsubscript𝑒𝑖𝑡𝜏superscriptsubscript𝑗1𝑁⋅superscriptsubscript𝑒𝑖𝑎superscriptsubscript𝑒𝑗𝑡𝜏⋅superscriptsubscript𝑒𝑖𝑡superscriptsubscript𝑒𝑖𝑎𝜏superscriptsubscript𝑗1𝑁⋅superscriptsubscript𝑒𝑖𝑡superscriptsubscript𝑒𝑗𝑎𝜏L=\\frac{1}{2N}\\displaystyle\\sum_{i=1}^{N}(\\log\\frac{\\exp{(e_{i}^{a}\\cdot e_{i}% ^{t}}/\\tau)}{\\sum_{j=1}^{N}\\exp{(e_{i}^{a}\\cdot e_{j}^{t}/\\tau)}}+\\log\\frac{% \\exp{(e_{i}^{t}\\cdot e_{i}^{a}}/\\tau)}{\\sum_{j=1}^{N}\\exp{(e_{i}^{t}\\cdot e_{j% }^{a}/\\tau)}})italic_L = divide start_ARG 1 end_ARG start_ARG 2 italic_N end_ARG ∑ start_POSTSUBSCRIPT italic_i = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_N end_POSTSUPERSCRIPT ( roman_log divide start_ARG roman_exp ( italic_e start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_a end_POSTSUPERSCRIPT ⋅ italic_e start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_t end_POSTSUPERSCRIPT / italic_τ ) end_ARG start_ARG ∑ start_POSTSUBSCRIPT italic_j = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_N end_POSTSUPERSCRIPT roman_exp ( italic_e start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_a end_POSTSUPERSCRIPT ⋅ italic_e start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_t end_POSTSUPERSCRIPT / italic_τ ) end_ARG + roman_log divide start_ARG roman_exp ( italic_e start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_t end_POSTSUPERSCRIPT ⋅ italic_e start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_a end_POSTSUPERSCRIPT / italic_τ ) end_ARG start_ARG ∑ start_POSTSUBSCRIPT italic_j = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_N end_POSTSUPERSCRIPT roman_exp ( italic_e start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_t end_POSTSUPERSCRIPT ⋅ italic_e start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_a end_POSTSUPERSCRIPT / italic_τ ) end_ARG ) (2)\n\nIn practice, we use pre-trained RoBERTa Liu et al. (2019) as the text encoder ftsubscript𝑓𝑡f_{t}italic_f start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT. In contrast to CLAP, which uses pretrained audio taggers (e.g., HSTAT Chen et al. (2022a)) for audio encoding, here we use WavLM Chen et al. (2022b) as the backbone for encoding. Self-supervised speech models can better capture detailed information (e.g., speaking style) than general audio classifiers. Both RoBERTa and WavLM encoders are fine-tuned in model training.\n\nData The training data of Speech-CLAP consists of SD-tag-6K, SD-cap-150, and 2K hours of speech datasets including both human and automatic captions. The training set includes both speech and non-speech data in order to equip the model the discriminative capabilities for speaking with environmental sound use cases (e.g., a man speaks as birds chirp and dogs bark). The speech portion is a subset of the captioned speech described in Section 7.1.1, which are selected to balance the ratio of human annotated and LLM-augmented captions. The model is evaluated on the evaluation sets of the sound and speech subset respectively.\n\nImplementation Details For audio and text encoder, we use WavLM-base+ and RoBERTa base respectively. Using alternative speech encoders within the same family such as WavLM-large brings similar results. The audio and text embeddings are normalized before calculating the loss (Equation 2). The model is trained using Adam optimizer Kingma and Ba (2014) with a learning rate of 5⁢e−55𝑒55e-55 italic_e - 5. We use 64 volta32 GPUs with a batch size of 75 per GPU for 200K updates. For training stability, the gradient is clipped to 10 by norm and raw floating point precision is used without any quantization. We track the recall (A2T@10) on the validation set at the end of each epoch and select the model checkpoint with the highest value.\n\nRetrieval Performance We compare Joint-CLAP to the original CLAPs proposed by Wu et al. (2023), measuring the text-to-audio and audio-to-text retrieval performance. Specifically, we take two public CLAP models trained general audios: CLAP (general audio) , and general audios plus speech: CLAP (w/ speech) Per retrieval task, we report the recall under three thresholds: 1, 5 and 10. As is shown in Table 8, public CLAPs, regardless of whether speech data are utilized or not, achieves significantly lower performance on speech retrieval based text descriptions, with ∼30similar-toabsent30\\sim 30∼ 30x performance degradation compared to the sound benchmark. This might be due to the naturally larger ambiguity in the task, where description of speech may exhibit higher variance. For instance, different people may have varying opinions on what constitutes fast speaking versus slow speaking. In spite of such ambiguity, Joint-CLAP still significantly improves the retrieval performance under the same setting (T2A@10 on speech: 2.29→22.01→2.2922.012.29\\rightarrow 22.012.29 → 22.01), while maintaining the performance for general audios (T2A@10 on sound: 63.64→67.64→63.6467.6463.64\\rightarrow 67.6463.64 → 67.64). The gain is attributed to fine-tuning with speech-specific datasets and using a high-performing speech encoder. To further ablate this effect, we trained a CLAP model without altering the model architecture using in-domain speech data. The retrieval performance is considerably lower than the WavLM-based Joint-CLAP (e.g., T2A@10 on speech: 12.0112.0112.0112.01 vs. 22.0122.0122.0122.01).\n\nCorrelation between Joint-CLAP scores and human opionion scores In practice, we also notice the Joint-CLAP model is more closely correlated to human-perceived text-audio similarity, as opposed to the public CLAP model (see Figure 3). Specifically, we take six Audiobox models of varying performance and run subjective evaluation with these models on the four evaluation sets. As is shown in Figure 3, the Pearson correlation coefficient between the text-audio similarity and REL score is increased from 0.028 to 0.727 with a joint CLAP model, suggesting that its text-audio similarity score is a reliable metric for evaluating description-controlled speech generation.\n\n7.4 Experimental Setup\n\nTraining data: We train unified Audiobox with a combination of (1) English speech dataset (SP-Multi-100K, see Section 5.3) with additional text description and voice prompt for each corresponding utterances and (2) sound dataset with text description or tags (SD-TAG-6K and SD-CAP-150, see Section 6.3). In both cases, each description is either generated from an LLM, or annotated by humans. We employ two-stage fine-tuning to improve our model fidelity and quality. In the first stage fine-tuning, we incorporate all speech (SP-Multi-100K) and sound (SD-TAG-6K and SD-CAP-150) datasets into our training dataset. In the second stage fine-tuning, we use a subset of our first-stage fine-tuning dataset comprised of higher quality dataset with total about 2,310 hours.\n\nImplementation details: Unified Audiobox model takes four different inputs: 1) frame-aligned transcript, 2) description, 3) voice prompts, and 4) context prompt (masked audio features). First, we first embed the input character sequence in frame-aligned transcript to 128128128128 dimension features. The embedded sequence is then projected using a linear layer and added to the projected masked audio features as input to the Transformer. Next, we use T5-base to extract 512-dimension continuous embedding from the description. The parameters of T5-base are kept frozen during training. We add a trainable linear layer to project the output from 512512512512-dimensions to match the Transformer embedding dimensions (1024102410241024). For the voice prompts, we first extract dense features using the same Encodec model described in Section 4. These features are then input to a 3333-layered Transformer model with 1024102410241024 embedding dimensions, 16161616 attention heads, and a feed-forward dimension of 4096409640964096. We then concatenate the time-step embedding, voice prompt encoder output, and description embedding which form the input for cross-attention.\n\nDuring training, we randomly drop voice prompt, captions, and context with the probabilities specified in Table 9:\n\nThese probabilities are designed with specific use cases discussed previously. Note that zero-shot TTS requires the model to copy each and every attribute from the audio prompt while restylization requires model to maintain high similarity of vocal style while discarding emotion, environment and other attributes. This requires us to distinguish the context from the voice prompt.\n\nSetting the dropout probabilities as defined in Table 9 lead to the joint probabilities presented in Table 10. The joint probabilities correspond to each of the use case that the model can support. Note that the generative pre-training already tunes model for ZS-TTS and diverse speech sampling applications. Therefore, we select the hyper-parameters to bias the model towards description-guided TTS with and without vocal conditioning.\n\nIn the first stage fine-tuning, we fine-tune all parameters for a maximum of 600K updates with 32 A100-80GB GPUs. We stopped training after 350K steps as we didnot find any gains in model performance beyond this. In the second stage, we further fine-tune our model parameter with LoRA fine-tuning on the self-attention parameters with r=64𝑟64r=64italic_r = 64 and cross attention input projection layers for 100K updates with 16 A100-80GB GPUs.\n\nFor the unified Audiobox duration model, we use both transcript and the description text as the input. We use 12 Transformers decoder layer with 8 heads, 768/2048 embedding/FFN dimensions self-attention and cross-attention layer to attend the description embedding. We use 40 dimension for the character embedding. During training, we set the description embedding drop probability 0.3. The model trained with 600K updates with flow-matching loss with 8 A100-80GB GPUS. For evaluation, we use the checkpoint at 200K steps.\n\nEvaluation data: We measure the effectiveness of description-guided TTS and description-guided TTS with vocal prompts on the following test sets. First, we annotate a set of 1,946 recordings sampled from diverse sources, including LibriTTS (Zen et al., 2019), Common Voice (Ardila et al., 2019), Switchboard (Godfrey et al., 1992), Fisher (Cieri, Christopher, et al., 2004,2005a, 2004,2005b), Spotify (Clifton et al., 2020), AudioSet (Gemmeke et al., 2017), Expresso (Nguyen et al., 2023) in order to evaluate the ability to generalize. This set is denoted as SpCap (SC). The second set is AC-filtered (AC-filt) (Lee et al., 2023) with 825 utterances. It constructed from AudioCaps test set by transcribing and keeping samples with reliable ASR transcriptions.\n\nThe third one is the Expresso test set (Expr) with 999 utterances. Finally, the fourth one contains utterances from the internal Accent set. We apply randomly sampled RIR and noise augmentation to construct this set and denote it as “Accent+” (500 utterances). Expr and Accent+ use speech captions derived from LLM using the available attributes. For Accent+, we additionally pass the environment and background noises tags to the LLM to incorporate the information into generated captions. Together these sets cover a wide variety of acoustic events, emotions, accents, environments, and vocal styles.\n\nTo evaluate description-based TTS with vocal prompt, we use Expr and Accent+ datasets and select another utterance from the same speaker. The prompt is selected such that is different from the target utterance on either emotion or speaking style (enunciated, whisper, etc). Furthermore, we also compare against Audiobox Sound and Audiobox Speech on speech and sound applications using the evaluation sets described in Sections 5 and 6 respectively.\n\nInference: We use duration model described in this section with averaging over 5555 samples. For description-guided TTS (with or without voice prompt), we additionally sample a silence duration of between 00 and 3333 seconds and pad it to both ends. We find this generates audios that are coherent with the description particularly when they also mention acoustic events. For example: a man speaks and car passes by while a dog is barking. However, this can cause model to hallucinate sounds when there are no acoustic events described. To cover all scenarios involving description-guided TTS, we generate N=8𝑁8N=8italic_N = 8 samples with stochastic silence padding and then output the best sample based on clap re-ranking using the joint model. We use a guidance weight of 0.750.750.750.75 for the description-guided TTS (with/without voice prompt) applications.\n\nFor sound only generation, we always generate 10101010s long audios with pseudo-transcripts using a guidance weight of 1.331.331.331.33. We use clap reranking with N=16𝑁16N=16italic_N = 16 samples using the sound clap model. For zero-shot in-context TTS applications, we trim the end-silences similar to the Audiobox Speech model and use a guidance weight of 1.01.01.01.0. Given that this application doesn’t involve any descriptions, we do not use clap re-ranking. Unless specified, both acoustic and duration Audiobox models use the midpoint solver with a step size of 1/321321/321 / 32, which invokes the function being integrated 64 times. When using classifier free guidance the model does 2 forward passes, leading to a total of 128 calls to the model forward pass.\n\n7.5 Main Results\n\nIn this section, we investigate the effectiveness of the unified Audiobox model on a number of use cases. We first compare the description-guided TTS with and without voice prompt in Tables 11 and 12 respectively. For this task, we compare with VoiceLDM Lee et al. (2023) and AudioLDM2 Liu et al. (2023c) models as baselines. Next, in Table 13 we evaluate how well Audiobox performs speech tasks as compared to non-description speech only model, Audiobox Speech. Finally, in Table 14 we compare against the sound-only Audiobox Sound model on the TTA task.\n\n7.5.1 Description-based control for speech generation\n\nTable 11 compares Audiobox with VoiceLDM Lee et al. (2023) and AudioLDM2 Liu et al. (2023c) models on description-guided TTS and description-guided TTS with voice prompt (voice restylization) tasks. We find that Audiobox outperforms both baselines on all datasets and metrics. In particular, Audiobox is able to consistently generate audios for rich descriptions in SC, background events (AC-filt), expressive audios (Expr), and accented audios with diverse backgrounds (Accent+).\n\nWe also note that AudioLDM2 and VoiceLDM struggle in particular on expressive datasets (Expr and Accent+). In particular, we find that utterances generated by AudioLDM2 and VoiceLDM models are significantly worse than the ground truth especially in complicated scenarios involving description of both speech, environment (cathedral), and background sounds. This results in worse scores on the Accent+ dataset. Furthermore, Expr test set contains voices exploring expressive styles like enunciation, whispering, non-binary gender which is where AudioLDM2 and VoiceLDM struggle. We hypothesize this could be because they are out-of-distribution cases w.r.t training. Both VoiceLDM and AudioLDM2 model tend to struggle on such utterances leading to low scores on objective metrics.\n\nOur subjective evaluations also align with the objective metrics where we find the the Audiobox model significantly outperforms the baselines in particular to similarity to the description. The worse scores on Accent+ and Expr dataset for AudioLDM2 and VoiceLDM model further confirms our own observations.\n\nIn Table 12, we present the results for description-guided TTS with voice prompt. VoiceLDM and AudioLDM2 model do not simultaneously support conditioning based on vocal and text descriptions for a transcript. Towards our best effort comparison, we combine the CLAP embedding for the audio vocal prompt and the textual description by averaging them and use it as a conditioning input. We find that Audiobox outperforms both baselines. We also notice that in the absence of voice-prompt, the speaker similarity of Audiobox is greatly reduced as the description cannot capture all aspects of voice. The subjective evaluations aligns with the objective metrics both for description and generated audio similarity and speaker similarity. We find that the voice prompt greatly improves the speaker similarity while matching the descriptions.\n\n7.5.2 Comparison to Audiobox Speech and Audiobox Sound\n\nTable 13 compares the unified Audiobox and speech only Audiobox Speech models for zero-shot TTS on 5555 different datasets. We use the same duration model for both acoustic models for this task. We find that the unified Audiobox model gives higher speaker similarity but performs marginally worse on the word error rate. This is also confirmed by subjective evaluations where we find only minor differences between the Audiobox and Audiobox Speech models.\n\nIn Table 14, we present the results comparing the unified Audiobox to the Audiobox Sound, VoiceLDM, and AudioLDM2 models on the task of TTA task as described in Section 6.2. We find that Audiobox significantly outperforms all baselines achieving the state-of-the-art performance for joint models and even outperforms sound only models such as TANGO. The Audiobox performs worse only to the Audiobox Sound model which specializes in sound generation. The subjective evaluations further confirm that both our Audiobox and Audiobox Sound outperform all other baselines by a significant margin.\n\n8 Inference Optimization with Bespoke Solver\n\nTo generate samples from a flow-matching model, an ODE solver is used at inference time to approximate the integration. There are many solvers that one can choose from, such as adaptive step-size dopri5 solver or fixed step-size midpoint solver. These solvers can be configured to operate at different speed-accuracy trade-off (accuracy in computing the integral). While flow-matching with OT path produces higher quality samples compared to diffusion models (Lipman et al., 2023; Le et al., 2023) for the same number of ODE steps and achieves better trade-off, very aggressive settings like midpoint with only 4 steps may still dramatically decrease the sample quality.\n\nInference efficiency is quantified by the number of function evaluation (NFE), which denotes the number of time an ODE solver evaluates the derivative. To improve the inference speed at the extreme low NFE regime (i.e., 4), we adopt Bespoke Solvers Shaul et al. (2023) to recover similar sample quality as the original model with a much lower NFE.\n\nAssume the initial noise sample x⁢(0)=x0∼p⁢(x0)𝑥0subscript𝑥0similar-to𝑝subscript𝑥0x(0)=x_{0}\\sim p(x_{0})italic_x ( 0 ) = italic_x start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT ∼ italic_p ( italic_x start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT ). Bespoke solver learns extra parameters θ∈ℝp𝜃superscriptℝ𝑝\\theta\\in\\mathbb{R}^{p}italic_θ ∈ blackboard_R start_POSTSUPERSCRIPT italic_p end_POSTSUPERSCRIPT where p𝑝pitalic_p is very small and minimize the global truncation error (sum of local truncation error) between approximate sample xnθsubscriptsuperscript𝑥𝜃𝑛x^{\\theta}_{n}italic_x start_POSTSUPERSCRIPT italic_θ end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_n end_POSTSUBSCRIPT and ground truth data point x⁢(1)𝑥1x(1)italic_x ( 1 ) in the following formula: 𝔼x0∼p⁢(x0)⁢‖x⁢(1)−xnθ‖subscript𝔼similar-tosubscript𝑥0𝑝subscript𝑥0norm𝑥1superscriptsubscript𝑥𝑛𝜃\\mathbb{E}_{x_{0}\\sim p(x_{0})}\\|x(1)-x_{n}^{\\theta}\\|blackboard_E start_POSTSUBSCRIPT italic_x start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT ∼ italic_p ( italic_x start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT ) end_POSTSUBSCRIPT ∥ italic_x ( 1 ) - italic_x start_POSTSUBSCRIPT italic_n end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_θ end_POSTSUPERSCRIPT ∥, where xnθsuperscriptsubscript𝑥𝑛𝜃x_{n}^{\\theta}italic_x start_POSTSUBSCRIPT italic_n end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_θ end_POSTSUPERSCRIPT is the output of the solver stepθ𝜃{}^{\\theta}start_FLOATSUPERSCRIPT italic_θ end_FLOATSUPERSCRIPT.\n\nAt a high level, Bespoke solvers aims to learn transformation for paths such that transformed can be more accurately estimated with the desired number of ODE steps. Bespoke Solver work by transforming the sample trajectory x⁢(t)𝑥𝑡x(t)italic_x ( italic_t ) using two components tr:[0,1]→[0,1]:subscript𝑡𝑟→0101t_{r}:[0,1]\\rightarrow[0,1]italic_t start_POSTSUBSCRIPT italic_r end_POSTSUBSCRIPT : [ 0 , 1 ] → [ 0 , 1 ] as time reparameterization and invertible function φ:[0,1]×ℝd→ℝd:𝜑→01superscriptℝ𝑑superscriptℝ𝑑\\varphi:[0,1]\\times\\mathbb{R}^{d}\\rightarrow\\mathbb{R}^{d}italic_φ : [ 0 , 1 ] × blackboard_R start_POSTSUPERSCRIPT italic_d end_POSTSUPERSCRIPT → blackboard_R start_POSTSUPERSCRIPT italic_d end_POSTSUPERSCRIPT, where those functions are parameterized by extra parameters θ𝜃\\thetaitalic_θ. Let the parametric solver be stepθ⁢(t,x;ut)superscriptstep𝜃𝑡𝑥subscript𝑢𝑡\\text{step}^{\\theta}(t,x;u_{t})step start_POSTSUPERSCRIPT italic_θ end_POSTSUPERSCRIPT ( italic_t , italic_x ; italic_u start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ). First we transform input (t,x)𝑡𝑥(t,x)( italic_t , italic_x ) into (r,x¯)=(rt,φrt⁢(x))𝑟¯𝑥subscript𝑟𝑡subscript𝜑subscript𝑟𝑡𝑥(r,\\bar{x})=(r_{t},\\varphi_{r_{t}}(x))( italic_r , over¯ start_ARG italic_x end_ARG ) = ( italic_r start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT , italic_φ start_POSTSUBSCRIPT italic_r start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT end_POSTSUBSCRIPT ( italic_x ) ). Next, we perform a step in the transformed space as (rn⁢e⁢x⁢t,x¯n⁢e⁢x⁢t)=step⁢(r,x¯;u¯r)subscript𝑟𝑛𝑒𝑥𝑡subscript¯𝑥𝑛𝑒𝑥𝑡step𝑟¯𝑥subscript¯𝑢𝑟(r_{next},\\bar{x}_{next})=\\text{step}(r,\\bar{x};\\bar{u}_{r})( italic_r start_POSTSUBSCRIPT italic_n italic_e italic_x italic_t end_POSTSUBSCRIPT , over¯ start_ARG italic_x end_ARG start_POSTSUBSCRIPT italic_n italic_e italic_x italic_t end_POSTSUBSCRIPT ) = step ( italic_r , over¯ start_ARG italic_x end_ARG ; over¯ start_ARG italic_u end_ARG start_POSTSUBSCRIPT italic_r end_POSTSUBSCRIPT ), using the chosen base solver (e.g., midpoint), where u¯rsubscript¯𝑢𝑟\\bar{u}_{r}over¯ start_ARG italic_u end_ARG start_POSTSUBSCRIPT italic_r end_POSTSUBSCRIPT is vector field on transformed trajectory. To transform back to original space, we compute (tn⁢e⁢x⁢t,xn⁢e⁢x⁢t)=s⁢t⁢e⁢pθ⁢(x,t;ut)=(trn⁢e⁢x⁢t,φrn⁢e⁢x⁢t−1⁢(x¯n⁢e⁢x⁢t))subscript𝑡𝑛𝑒𝑥𝑡subscript𝑥𝑛𝑒𝑥𝑡𝑠𝑡𝑒superscript𝑝𝜃𝑥𝑡subscript𝑢𝑡subscript𝑡subscript𝑟𝑛𝑒𝑥𝑡subscriptsuperscript𝜑1subscript𝑟𝑛𝑒𝑥𝑡subscript¯𝑥𝑛𝑒𝑥𝑡(t_{next},x_{next})=step^{\\theta}(x,t;u_{t})=(t_{r_{next}},\\varphi^{-1}_{r_{% next}}(\\bar{x}_{next}))( italic_t start_POSTSUBSCRIPT italic_n italic_e italic_x italic_t end_POSTSUBSCRIPT , italic_x start_POSTSUBSCRIPT italic_n italic_e italic_x italic_t end_POSTSUBSCRIPT ) = italic_s italic_t italic_e italic_p start_POSTSUPERSCRIPT italic_θ end_POSTSUPERSCRIPT ( italic_x , italic_t ; italic_u start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ) = ( italic_t start_POSTSUBSCRIPT italic_r start_POSTSUBSCRIPT italic_n italic_e italic_x italic_t end_POSTSUBSCRIPT end_POSTSUBSCRIPT , italic_φ start_POSTSUPERSCRIPT - 1 end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_r start_POSTSUBSCRIPT italic_n italic_e italic_x italic_t end_POSTSUBSCRIPT end_POSTSUBSCRIPT ( over¯ start_ARG italic_x end_ARG start_POSTSUBSCRIPT italic_n italic_e italic_x italic_t end_POSTSUBSCRIPT ) ).\n\nTo train the Bespoke solver, we generate the ground-truth path x⁢(t)𝑥𝑡x(t)italic_x ( italic_t ) at times tisubscript𝑡𝑖t_{i}italic_t start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT where i∈[N]𝑖delimited-[]𝑁i\\in[N]italic_i ∈ [ italic_N ] using standard ODE solver, and we calculate the local truncation error diθ=‖x⁢(ti)−s⁢t⁢e⁢pxθ⁢(ti−1,x⁢(ti−1);u)‖superscriptsubscript𝑑𝑖𝜃norm𝑥subscript𝑡𝑖𝑠𝑡𝑒superscriptsubscript𝑝𝑥𝜃subscript𝑡𝑖1𝑥subscript𝑡𝑖1𝑢d_{i}^{\\theta}=\\|x(t_{i})-step_{x}^{\\theta}(t_{i-1},x(t_{i-1});u)\\|italic_d start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_θ end_POSTSUPERSCRIPT = ∥ italic_x ( italic_t start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ) - italic_s italic_t italic_e italic_p start_POSTSUBSCRIPT italic_x end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_θ end_POSTSUPERSCRIPT ( italic_t start_POSTSUBSCRIPT italic_i - 1 end_POSTSUBSCRIPT , italic_x ( italic_t start_POSTSUBSCRIPT italic_i - 1 end_POSTSUBSCRIPT ) ; italic_u ) ∥ between ground truth and predicted sample from parameterized solver θ𝜃\\thetaitalic_θ, and finally we minimize the Bespoke loss ℒ⁢(θ)=𝔼x0∼p⁢(x0)⁢∑i=1ndiθℒ𝜃subscript𝔼similar-tosubscript𝑥0𝑝subscript𝑥0superscriptsubscript𝑖1𝑛superscriptsubscript𝑑𝑖𝜃\\mathcal{L}(\\theta)=\\mathbb{E}_{x_{0}\\sim p(x_{0})}\\sum_{i=1}^{n}d_{i}^{\\theta}caligraphic_L ( italic_θ ) = blackboard_E start_POSTSUBSCRIPT italic_x start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT ∼ italic_p ( italic_x start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT ) end_POSTSUBSCRIPT ∑ start_POSTSUBSCRIPT italic_i = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_n end_POSTSUPERSCRIPT italic_d start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_θ end_POSTSUPERSCRIPT.\n\nIn this paper, we generate ground truth paths for training Bespoke Solvers for speech generation using dopri5 ODE solver to estimate N=200𝑁200N=200italic_N = 200 steps with guidance weight (GW) of 0.7. Table 15 top half shows the evaluation result on zero-shot TTS with matched guidance weight (0.7) comparing two standard ODE solvers: midpoint and dopri5 with the Bespoke Solver. As we can see, by using bespoke solver, we could reduce ODE steps down to 4 and still retain similar performance in term of style similarity and WER.\n\nIn addition, we also study if a Bespoke Solver trained for a specific guidance weight generalizes to a different guidance weight, and present comparison between the default midpoint solver with the bespoke solver using GW=0.0. Results suggest that it can generalize to different guidance setups.\n\n9 Responsible AI\n\nIn order to build a system responsibly, we conduct evaluations to gauge the fairness aspect and studies methods to defend misuse. In this section, we first analyze if our model produces similar performance on different groups like genders and accents. Second, we also perform watermarking experiments to evaluate if a recently proposed watermarking system generalizes to our models such that watermarked samples from our models can be reliably detected.\n\n9.1 Fairness across groups\n\nWe train our model on large quantities of data from various sources. We believe through scaling training data, our model can perform well across many different groups. We assess this aspects by evaluating model performance by genders and by accents. In particular, we consider gender bias or accent bias are observed if there is a groups that has significantly worse performance in term of content correctness (measured by WER) and style similarity (measured by cosine similarity between style embeddings) compared to those of the entire population.\n\nTo conduct our experiment, we consider the zero-shot TTS task conditioned on a context prompt. We use a dataset with country and gender labels for this experiment. For the TTS transcript, we sample 20 transcripts from the test set. For the TTS prompts, we evaluate on accents of which there are at least 5 unique speakers in the dataset, which leave us to 64 accents. Then, we sample 20 random utterances (10 for male, 10 for female) from each"
    }
}
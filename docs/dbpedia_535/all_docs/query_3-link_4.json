{
    "id": "dbpedia_535_3",
    "rank": 4,
    "data": {
        "url": "https://www.ncbi.nlm.nih.gov/pmc/articles/PMC10332598/",
        "read_more_link": "",
        "language": "en",
        "title": "Deep audio embeddings for vocalisation clustering",
        "top_image": "https://www.ncbi.nlm.nih.gov/corehtml/pmc/pmcgifs/pmc-card-share.jpg?_=0",
        "meta_img": "https://www.ncbi.nlm.nih.gov/corehtml/pmc/pmcgifs/pmc-card-share.jpg?_=0",
        "images": [
            "https://www.ncbi.nlm.nih.gov/coreutils/uswds/img/favicons/favicon-57.png",
            "https://www.ncbi.nlm.nih.gov/coreutils/uswds/img/icon-dot-gov.svg",
            "https://www.ncbi.nlm.nih.gov/coreutils/uswds/img/icon-https.svg",
            "https://www.ncbi.nlm.nih.gov/coreutils/nwds/img/logos/AgencyLogo.svg",
            "https://www.ncbi.nlm.nih.gov/corehtml/pmc/pmcgifs/logo-plosone.png",
            "https://www.ncbi.nlm.nih.gov/corehtml/pmc/pmcgifs/corrauth.gif",
            "https://www.ncbi.nlm.nih.gov/corehtml/pmc/pmcgifs/corrauth.gif",
            "https://www.ncbi.nlm.nih.gov/pmc/articles/PMC10332598/bin/pone.0283396.g001.jpg",
            "https://www.ncbi.nlm.nih.gov/pmc/articles/PMC10332598/bin/pone.0283396.g002.jpg",
            "https://www.ncbi.nlm.nih.gov/pmc/articles/PMC10332598/bin/pone.0283396.g003.jpg",
            "https://www.ncbi.nlm.nih.gov/pmc/articles/PMC10332598/bin/pone.0283396.g004.jpg",
            "https://www.ncbi.nlm.nih.gov/pmc/articles/PMC10332598/bin/pone.0283396.g005.jpg",
            "https://www.ncbi.nlm.nih.gov/pmc/articles/PMC10332598/bin/pone.0283396.g006.jpg",
            "https://www.ncbi.nlm.nih.gov/pmc/articles/PMC10332598/bin/pone.0283396.g007.jpg"
        ],
        "movies": [],
        "keywords": [],
        "meta_keywords": [
            ""
        ],
        "tags": null,
        "authors": [
            "Paul Best",
            "Sébastien Paris",
            "Hervé Glotin",
            "Ricard Marxer"
        ],
        "publish_date": "2023-08-25T00:00:00",
        "summary": "",
        "meta_description": "The study of non-human animals’ communication systems generally relies on the transcription of vocal sequences using a finite set of discrete units. This set is referred to as a vocal repertoire, which is specific to a species or a sub-group of ...",
        "meta_lang": "en",
        "meta_favicon": "https://www.ncbi.nlm.nih.gov/coreutils/nwds/img/favicons/favicon.ico",
        "meta_site_name": "PubMed Central (PMC)",
        "canonical_link": "https://www.ncbi.nlm.nih.gov/pmc/articles/PMC10332598/",
        "text": "Vocalisations feature extraction and clustering\n\nAutomatic clustering of animal vocal repertoires has been studied in the past. Methods vary but usually revolve around three main steps: feature extraction of signals, dimensionality reduction, and clustering ( ).\n\nTo represent acoustic signals, one can make use of Predefined Acoustic Features (PAFs), i.e., handcrafted temporal and spectral signal characteristics. For instance, depending on the signals to be clustered, features can describe the temporal envelope, the frequency contour, or the response to filterbanks (e.g., Mel or gammatone). Researchers have used PAFs to cluster vocalisations of zebra finches (Taeniopygia guttata) [18], baboons (Papio ursinus) [19], bottlenose dolphins (Tursiops truncatus) [20], gibbons (Hylobates funereus) [21], and mice (Mus musculus) [22, 23]. For instance, Elie et al. [18] used 22 PAFs extracted using the Biosound package [24] to cluster zebra finch (Taeniopygia guttata) vocalisations, Sainburg et al. [25] used 18 features from the same package to visualise and cluster vocalisations from 20 species, Clink and Klinck [21] used Mel Frequency Cepstral Coefficients (MFCCs) to cluster gibbon (Hylobates funereus) calls by individual, and Van Segbroeck et al. [22] used a gammatone filterbank to cluster mice (Mus musculus) vocalisations. Alternatively, to capture spectro-temporal variations, the concatenation of consecutive spectrogram frames can be used [25, 26].\n\nOften, we do not know a priori which feature will be the most discriminant for vocalisation types, and high dimensional spaces make similarity measurements difficult (curse of dimensionality). This motivates using dimensionality reduction algorithms such as Principal Component Analysis (PCA) [18, 20, 25] or Uniform Manifold Aproximation (UMAP) [25–27] to emphasise the most ‘relevant’ features for a given dataset. One important limitation of using PAFs is that there is no generic set that suits all vocal repertoires, and some of the features need specific settings depending on vocalisation frequency ranges or signal to noise ratio (SNR). An opportunity to avoid having to manually choose the right set of features and tune their settings is to use features learnt using deep representation learning (as opposed to handcrafted features like PAFs). Following this approach, auto-encoder artificial neural networks have been used by Goffinet et al. [28] on mice and zebra finch vocalisations, by Bergler et al. [29] to cluster orca calls, by Rowe et al. [30] to cluster bird vocalisations by species, and by Tolkova et al. [31] to discriminate between background noise and bird vocalisations.\n\nAuto-encoders are artificial neural networks self-supervisedly trained (without the need for labels) to encode data into a lower dimensional space (called bottleneck). To ensure the conservation of the information in the bottleneck, the encoder network is optimised jointly with a decoder network to maximise the resemblance between the decoded encoded data and the input data (reconstruction loss).\n\nOnce we obtain a high level representation of vocalisations, whether with PAFs or with auto-encoder embeddings, and whether dimensionaly reduced or not, clustering algorithms allow to group close points (in the high-level feature space) together to form discrete classes. In the literature of vocal repertoire clustering, chosen algorithms were mostly K-Means and HDBSCAN [32], HDBSCAN usually used after dimensionality reduction [25, 26].\n\nTo follow up on this research, we propose studying vocalisation clustering, especially the choice of audio representation and its effect on the subsequent clustering. Auto-encoders trained on vocalisation spectrograms have recently shown promising results [28, 29], we thus compare the embedding space they yield to that of more traditional methods, especially how it allows to cluster units by acoustic similarity. Taking detected vocalisations as an input, clusters of similar vocalisations can thus be presented to an operator, facilitating vocal repertoire characterisation as well as annotation (e.g., to train a classifier model). To quantify the performance of the varying methods, we use them on 8 datasets of animal vocal repertoires (including birds and cetaceans) and measure their agreement with expert labels of vocalisation types. Along with the performance analysis of the system across varying hyper-parameters, we publish a python interface for biologists to use deep audio embeddings for their own vocal repertoire discovery procedures.\n\nSignal preprocessing and Fourier transform\n\nFor the procedure described in this paper, the only parameters that are dataset specific are related to low-level signal preprocessing, and require only a limited amount of knowledge about the repertoire of interest to be set properly. They are the sample duration T, the sampling rate fs and the Fast Fourier Transform (FFT) window size NFFT, which reports for each dataset. Note that the maximum frequency for Mel filterbanks was always set to the nyquist frequency, and so is defined by the sampling rate implicitly. This section describes how these parameters were set, along with the full procedure that compiles signals into spectrograms to feed to the auto-encoder.\n\nTable 2\n\nSpecies and sourcefs (kHz)NFFTHop (ms) T (s)bengalese finch [34]322560.70.1bengalese finch [35]322560.70.1california thrasher [36]44.15121.80.25cassin vireo [36]44.15123.80.5black-headed grosbeak [36]44.15122.60.35humpback whale [37]11.025102414.92humpback whale (small) [37]11.025102414.92bottlenose dolphin [33]9651215.62\n\nA consideration for setting spectrogram parameters is that auto-encoder’s decoders typically reconstruct images by successive factor two up-sampling. Input spectrograms which will be matched with auto-encoder reconstructions thus need to be of dimensions that follow (kf 2n × kt 2n), with kf and kt integers and n the number of factor two up-sampling (in the proposed architecture n = 5, more details are given in the next section). For all datasets in this study, 128 frequency bins and 128 temporal bins appeared to suffice in containing vocalisation details; all spectrograms were thus set to be of size 128x128 (kf = kt = 4). Note however that if more spectral or temporal bins were needed for a new species, the same auto-encoder architecture could allow to manage 256 bins or more.\n\nPreprocessing first consists in extracting the signal surrounding the center of the annotation. The sample duration T was fixed for each dataset. In order to fully contain most vocalisations while avoiding hiding details for small ones, it was set at the 3rd quartile of all vocalisation durations. In the case of pre-cut vocalisation files with a smaller duration than this fixed value, signals were zero-padded. The sampling rate was fixed to the most common value in the dataset, and vocalisations with a different sampling rate were resampled to match the common value using the scipy python package [38]. The exception is the humpback whale dataset, for which the sampling rate was reduced to 11,025 Hz in order to reduce the nyquist frequency and increase the frequency resolution for the relatively low frequency vocalisations. Resulting signals are then z-normalised before its frequency decomposition.\n\nThe choice of Fourier transform parameters define the spectro-temporal resolution of spectrograms, and need to suit spectro-temporal modulation rates of vocalisations. For this study, FFT window sizes were manually set by quick spectrogram inspections or by borrowing from studies publishing the databases. For the short term Fourier transform, we used unpadded Hann windows and a hop size set to yield spectrograms of 128 time bins (Hop=(T×fs-NFFT)×1128). The Fourier transform comes with the limitation of having to choose a fixed spectro-temporal resolution, which wavelet based transforms can help alleviate [39], allowing a better representation especially for transient signals. However, choosing such approach would imply a significant increase in computation, and additional dataset specific settings (e.g., choice of wavelet family or hyperparameters). For the sake of simplicity and usability, and as the studied signals are not transient, we chose the general purpose Fourier transform in our experiments.\n\nFollowing the short term Fourier transform, several frequency and dynamic range compressions were tested. Mel filterbanks with 128 filters between 0 and the nyquist frequency were used, and compared to keeping the spectrogram with a linear frequency layout. In the latter case, maximum pooling was used to reduce the number of frequency bins down to 128 and match other spectrograms, independently of the FFT size. As for dynamic range compression of spectro-temporal magnitudes, we compared no compression against logarithmic and PCEN [40] compression. In all cases, spectro-temporal magnitudes were z-normalised before applying the auto-encoder.\n\n15 May 2023\n\nPONE-D-23-06746Deep audio embeddings for vocalisation clusteringPLOS ONE\n\nDear Dr. Best,\n\nThank you for submitting your manuscript to PLOS ONE. After careful consideration, we feel that it has merit but does not fully meet PLOS ONE’s publication criteria as it currently stands. Therefore, we invite you to submit a revised version of the manuscript that addresses the points raised during the review process.\n\nPlease submit your revised manuscript by Jun 29 2023 11:59PM. If you will need more time than this to complete your revisions, please reply to this message or contact the journal office at gro.solp@enosolp. When you're ready to submit your revision, log on to https://www.editorialmanager.com/pone/ and select the 'Submissions Needing Revision' folder to locate your manuscript file.\n\nPlease include the following items when submitting your revised manuscript:\n\nA rebuttal letter that responds to each point raised by the academic editor and reviewer(s). You should upload this letter as a separate file labeled 'Response to Reviewers'.\n\nA marked-up copy of your manuscript that highlights changes made to the original version. You should upload this as a separate file labeled 'Revised Manuscript with Track Changes'.\n\nAn unmarked version of your revised paper without tracked changes. You should upload this as a separate file labeled 'Manuscript'.\n\nIf you would like to make changes to your financial disclosure, please include your updated statement in your cover letter. Guidelines for resubmitting your figure files are available below the reviewer comments at the end of this letter.\n\nIf applicable, we recommend that you deposit your laboratory protocols in protocols.io to enhance the reproducibility of your results. Protocols.io assigns your protocol its own identifier (DOI) so that it can be cited independently in the future. For instructions see: https://journals.plos.org/plosone/s/submission-guidelines#loc-laboratory-protocols. Additionally, PLOS ONE offers an option for publishing peer-reviewed Lab Protocol articles, which describe protocols hosted on protocols.io. Read more information on sharing protocols at https://plos.org/protocols?utm_medium=editorial-email&utm_source=authorletters&utm_campaign=protocols.\n\nWe look forward to receiving your revised manuscript.\n\nKind regards,\n\nJie Xie, Ph.D.\n\nAcademic Editor\n\nPLOS ONE\n\nJournal requirements:\n\nWhen submitting your revision, we need you to address these additional requirements.\n\n1. Please ensure that your manuscript meets PLOS ONE's style requirements, including those for file naming. The PLOS ONE style templates can be found at\n\nhttps://journals.plos.org/plosone/s/file?id=wjVg/PLOSOne_formatting_sample_main_body.pdf and\n\nhttps://journals.plos.org/plosone/s/file?id=ba62/PLOSOne_formatting_sample_title_authors_affiliations.pdf\n\n2. In your Data Availability statement, you have not specified where the minimal data set underlying the results described in your manuscript can be found. PLOS defines a study's minimal data set as the underlying data used to reach the conclusions drawn in the manuscript and any additional data required to replicate the reported study findings in their entirety. All PLOS journals require that the minimal data set be made fully available. For more information about our data policy, please see http://journals.plos.org/plosone/s/data-availability.\n\nUpon re-submitting your revised manuscript, please upload your study’s minimal underlying data set as either Supporting Information files or to a stable, public repository and include the relevant URLs, DOIs, or accession numbers within your revised cover letter. For a list of acceptable repositories, please see http://journals.plos.org/plosone/s/data-availability#loc-recommended-repositories. Any potentially identifying patient information must be fully anonymized.\n\nImportant: If there are ethical or legal restrictions to sharing your data publicly, please explain these restrictions in detail. Please see our guidelines for more information on what we consider unacceptable restrictions to publicly sharing data: http://journals.plos.org/plosone/s/data-availability#loc-unacceptable-data-access-restrictions. Note that it is not acceptable for the authors to be the sole named individuals responsible for ensuring data access.\n\nWe will update your Data Availability statement to reflect the information you provide in your cover letter.\n\n3. Please review your reference list to ensure that it is complete and correct. If you have cited papers that have been retracted, please include the rationale for doing so in the manuscript text, or remove these references and replace them with relevant current references. Any changes to the reference list should be mentioned in the rebuttal letter that accompanies your revised manuscript. If you need to cite a retracted article, indicate the article’s retracted status in the References list and also include a citation and full reference for the retraction notice.\n\n[Note: HTML markup is below. Please do not edit.]\n\nReviewers' comments:\n\nReviewer's Responses to Questions\n\nComments to the Author\n\n1. Is the manuscript technically sound, and do the data support the conclusions?\n\nThe manuscript must describe a technically sound piece of scientific research with data that supports the conclusions. Experiments must have been conducted rigorously, with appropriate controls, replication, and sample sizes. The conclusions must be drawn appropriately based on the data presented.\n\nReviewer #1: Yes\n\nReviewer #2: Yes\n\n**********\n\n2. Has the statistical analysis been performed appropriately and rigorously?\n\nReviewer #1: Yes\n\nReviewer #2: Yes\n\n**********\n\n3. Have the authors made all data underlying the findings in their manuscript fully available?\n\nThe PLOS Data policy requires authors to make all data underlying the findings described in their manuscript fully available without restriction, with rare exception (please refer to the Data Availability Statement in the manuscript PDF file). The data should be provided as part of the manuscript or its supporting information, or deposited to a public repository. For example, in addition to summary statistics, the data points behind means, medians and variance measures should be available. If there are restrictions on publicly sharing data—e.g. participant privacy or use of data from a third party—those must be specified.\n\nReviewer #1: Yes\n\nReviewer #2: No\n\n**********\n\n4. Is the manuscript presented in an intelligible fashion and written in standard English?\n\nPLOS ONE does not copyedit accepted manuscripts, so the language in submitted articles must be clear, correct, and unambiguous. Any typographical or grammatical errors should be corrected at revision, so please note any specific errors here.\n\nReviewer #1: Yes\n\nReviewer #2: Yes\n\n**********\n\n5. Review Comments to the Author\n\nPlease use the space provided to explain your answers to the questions above. You may also include additional comments for the author, including concerns about dual publication, research ethics, or publication ethics. (Please upload your review as an attachment if it exceeds 20,000 characters)\n\nReviewer #1: This paper therefore studies a new method for encoding vocalisations, allowing for automatic clustering to alleviate vocal repertoire characterisation. The authors use a convolutional auto-encoder network to learn an abstract representation of animal vocalisations. I believe the topic of this research is of interest to the related research community. Here are my detailed comments.\n\n(1) Previous studies have used auto-encoders for studying ecoacoustic data, what’s the different between the current study and the following references?\n\nRowe B, Eichinski P, Zhang J, et al. Analyzing Big Environmental Audio with Frequency Preserving Autoencoders[C]//2021 IEEE 17th International Conference on eScience (eScience). IEEE, 2021: 70-79.\n\nRowe, Benjamin, et al. \"Acoustic auto-encoders for biodiversity assessment.\" Ecological Informatics 62 (2021): 101237.\n\n(2) In table 2, the authors use different parameter settings for different species and sources, how to set those parameters?\n\n(3) One of the pitfalls of the STFT is that it has a fixed resolution. However, wavelet-based representation can solve this limitation. Why the authors choose STFT? A comparison between STFT- and wavelet-based representation is worth investigating.\n\nReviewer #2: The article proposes an innovative method for evaluating the quality of clusters formed by bioacoustic waveforms using an Autoencoder and UMAP. While the article is well written and generally easy to understand, there are a few points that could be improved.\n\n- The discussion highlights the versatility of the proposed method compared to using pure spectrograms. However, it fails to address a significant challenge associated with the configuration of the Autoencoder's hyperparameters (layers, sizes, etc.). This challenge represents a disadvantage and an intrinsic difficulty of the proposed method, and it should be addressed and discussed in the article. In addition, it would be beneficial to include a more comprehensive analysis of the disadvantages or limitations of the proposed technique.\n\n- In Table 3, the columns are difficult to understand, because for each column, with a unique heading, there are two subcolumns.\n\n- Towards the end of the discussion, it is mentioned that the method is versatile; however, it is stated that the recordings were downsampled to suited the method, rather than the other way around. This implies a lack of flexibility in the method's application due to the constraints imposed by the perceptual loss and VGG network, which requires square matrix inputs. It would be helpful to elaborate on this limitation and discuss potential workarounds or alternatives.\n\n- The article should address the potential influence of background noise on bioacoustic systems and how it might affect cluster formation. High-capacity models like deep learning have the ability to learn features related to background noise, which could impact the clustering process. It is important to discuss this issue and conduct experiments to investigate whether such biases were avoided or mitigated in the proposed method.\n\n- In Figure 3, the subplots are not convincing as there are many unclustered gray dots. The interpretation and inspection of these unclustered points should be clarified in the article. Additionally, the ideal UMAP and HDBSCAN parameters used in each graph should be provided, including any search process for determining the ideal UMAP parameters.\n\n- Figure 4 shows that the NMI decreases for cassin_vireo as the UMAP dimension increases. The reason behind this trend should be explained in the article to provide a better understanding of the results.\n\n- The sentence mentioning the graded nature of humpback whale unit types and their temporal instability needs further clarification. What does it mean for a signal to be \"temporally unstable\"? This concept should be elaborated upon to ensure readers fully comprehend its significance in the context of the article.\n\n- The sentence discussing the increased agreement between clusters and expert labels (0.72 NMI) implies that additional experiments were conducted but not reported. These experiments and their findings should be included in the article to provide a complete and transparent account of the research. Furthermore, the phrase \"reduced when working with a limited amount of time\" requires clarification regarding the specific time limit or duration used for normalization of the AE inputs.\n\n- The GitHub repository should be better organized and documented more effectively. A detailed tutorial on replicating the experiments and processing new datasets should be added to facilitate reproducibility and enable others to build upon the research.\n\n- The work at the provided link (https://experiments.withgoogle.com/bird-sounds) is very nice. It would be interesting to know if the authors have plans to develop a web server for their application, as it could be highly beneficial for the community. (suggestion)\n\n**********\n\n6. PLOS authors have the option to publish the peer review history of their article (what does this mean?). If published, this will include your full peer review and any attached files.\n\nIf you choose “no”, your identity will remain anonymous but your review may still be made public.\n\nDo you want your identity to be public for this peer review? For information about this choice, including consent withdrawal, please see our Privacy Policy.\n\nReviewer #1: No\n\nReviewer #2: No\n\n**********\n\n[NOTE: If reviewer comments were submitted as an attachment file, they will be attached to this email and accessible via the submission site. Please log into your account, locate the manuscript record, and check for the action link \"View Attachments\". If this link does not appear, there are no attachment files.]\n\nWhile revising your submission, please upload your figure files to the Preflight Analysis and Conversion Engine (PACE) digital diagnostic tool, https://pacev2.apexcovantage.com/. PACE helps ensure that figures meet PLOS requirements. To use PACE, you must first register as a user. Registration is free. Then, login and navigate to the UPLOAD tab, where you will find detailed instructions on how to use the tool. If you encounter any issues or have any questions when using PACE, please email PLOS at gro.solp@serugif. Please note that Supporting Information files do not need this step.\n\n21 Jun 2023\n\nJournal requirements:\n\nWhen submitting your revision, we need you to address these additional requirements.\n\n1. Please ensure that your manuscript meets PLOS ONE's style requirements, including those for file naming. The PLOS ONE style templates can be found at\n\nhttps://journals.plos.org/plosone/s/file?id=wjVg/PLOSOne_formatting_sample_main_body.pdf and\n\nhttps://journals.plos.org/plosone/s/file?id=ba62/PLOSOne_formatting_sample_title_authors_affiliations.pdf\n\nS1_Fig1.tif has been renamed to S1_Fig.tif and the asterisk for the corresponding author has been added\n\n2. In your Data Availability statement, you have not specified where the minimal data set underlying the results described in your manuscript can be found. PLOS defines a study's minimal data set as the underlying data used to reach the conclusions drawn in the manuscript and any additional data required to replicate the reported study findings in their entirety. All PLOS journals require that the minimal data set be made fully available. For more information about our data policy, please see http://journals.plos.org/plosone/s/data-availability.\n\nUpon re-submitting your revised manuscript, please upload your study’s minimal underlying data set as either Supporting Information files or to a stable, public repository and include the relevant URLs, DOIs, or accession numbers within your revised cover letter. For a list of acceptable repositories, please see http://journals.plos.org/plosone/s/data-availability#loc-recommended-repositories. Any potentially identifying patient information must be fully anonymized.\n\nImportant: If there are ethical or legal restrictions to sharing your data publicly, please explain these restrictions in detail. Please see our guidelines for more information on what we consider unacceptable restrictions to publicly sharing data: http://journals.plos.org/plosone/s/data-availability#loc-unacceptable-data-access-restrictions. Note that it is not acceptable for the authors to be the sole named individuals responsible for ensuring data access.\n\nWe will update your Data Availability statement to reflect the information you provide in your cover letter.\n\nAuto-encoder weights, vocalization embeddings, and ground truth labels of vocalization types that are necessary to produce performance metrics and plots in this study are available on a figshare repository : https://doi.org/10.6084/m9.figshare.23138210.v1\n\nThe acoustic data that were used to train auto-encoders and generate embeddings along with their expert label are not owned by the authors but might be accessed via their respective sources :\n\nbengalese finch1 :\n\nNicholson, David; Queen, Jonah E.; J. Sober, Samuel (2017): Bengalese Finch song repository. figshare. Dataset. https://doi.org/10.6084/m9.figshare.4805749.v5\n\nbengalese finch2 :\n\nKoumura, Takuya (2016): BirdsongRecognition. figshare. Media. https://doi.org/10.6084/m9.figshare.3470165.v1\n\ncassin vireo, california thrasher, black-headed grosbeak :\n\nArriaga, J. G., Cody, M. L., Vallejo, E. E., & Taylor, C. E. (2015). Bird-DB: A database for annotated bird song sequences. Ecological Informatics, 27, 21-25.\n\nhttps://taylor0.biology.ucla.edu/birdDBQuery/\n\nhumpback whale and humpback whale (small) (upon request) :\n\nMalige, F., Djokic, D., Patris, J., Sousa-Lima, R., & Glotin, H. (2021). Use of recurrence plots for identification and extraction of patterns in humpback whale song recordings. Bioacoustics, 30(6), 680-695. (contact : Franck Malige, rf.bal-sil@egilam.kcnarf)\n\nDolphin (upon request) :\n\nSayigh L, Janik VM, Jensen F, Scott MD, Tyack PL, Wells R. The Sarasota dolphin whistle database: A unique long-term resource for understanding dolphin communication. Frontiers in Marine Science. 2022;. (contact Laela S. Sayigh, ude.iohw@hgiyasl)\n\n3. Please review your reference list to ensure that it is complete and correct. If you have cited papers that have been retracted, please include the rationale for doing so in the manuscript text, or remove these references and replace them with relevant current references. Any changes to the reference list should be mentioned in the rebuttal letter that accompanies your revised manuscript. If you need to cite a retracted article, indicate the article’s retracted status in the References list and also include a citation and full reference for the retraction notice.\n\nThe preprint “Sainburg T, Thielk M, Gentner TQ. Latent space visualization, characterization, and generation of diverse vocal communication signals; 2020” was referenced rather than its peer-reviewed version because the first mentions experiments with auto-encoders but the latter doesn’t. This reference has been removed.\n\nReviewer #1:\n\nThis paper therefore studies a new method for encoding vocalisations, allowing for automatic clustering to alleviate vocal repertoire characterisation. The authors use a convolutional auto-encoder network to learn an abstract representation of animal vocalisations. I believe the topic of this research is of interest to the related research community. Here are my detailed comments.\n\n(1) Previous studies have used auto-encoders for studying ecoacoustic data, what’s the different between the current study and the following references?\n\nRowe B, Eichinski P, Zhang J, et al. Analyzing Big Environmental Audio with Frequency Preserving Autoencoders[C]//2021 IEEE 17th International Conference on eScience (eScience). IEEE, 2021: 70-79. Rowe, Benjamin, et al. \"Acoustic auto-encoders for biodiversity assessment.\" Ecological Informatics 62 (2021): 101237.\n\nThese references report on clustering vocalizations by species, using 100 randomly sampled signals (10 from 10 species), and the proposed auto-encoder does not outperform handcrafted features (MFCC). Our study clusters vocalizations by types of a repertoire within a species, uses larger datasets (some repertoires gather a 100 classes), and our proposed architecture systematically outperforms handcrafted features.\n\n(2) In table 2, the authors use different parameter settings for different species and sources, how to set those parameters?\n\nThe section “Signal preprocessing and Fourier transform” in Materials and Methods details how those parameters were set and gives rational for a user to set them for a new specie\n\n(3) One of the pitfalls of the STFT is that it has a fixed resolution. However, wavelet-based representation can solve this limitation. Why the authors choose STFT? A comparison between STFT- and wavelet-based representation is worth investigating.\n\nThis paper intends to propose a method that is fast, easily usable by users that are not experts in signal processing, and applied to non-transient harmonic signals. This makes the Fourier transform a good a priori representation for them. We’ve added a justification for this in the “signal processing and Fourier transform” section in Material and Methods.\n\nReviewer #2: The article proposes an innovative method for evaluating the quality of clusters formed by bioacoustic waveforms using an Autoencoder and UMAP. While the article is well written and generally easy to understand, there are a few points that could be improved.\n\n- The discussion highlights the versatility of the proposed method compared to using pure spectrograms. However, it fails to address a significant challenge associated with the configuration of the Autoencoder's hyperparameters (layers, sizes, etc.). This challenge represents a disadvantage and an intrinsic difficulty of the proposed method, and it should be addressed and discussed in the article. In addition, it would be beneficial to include a more comprehensive analysis of the disadvantages or limitations of the proposed technique.\n\nExperiments demonstrate that there is no need for additional hyperparameter configuration for the method to be efficient on a wide variety of signals. We’ve added a statement in the discussion’s second paragraph that notes this challenge that comes with deep learning.\n\n- In Table 3, the columns are difficult to understand, because for each column, with a unique heading, there are two subcolumns.\n\nThis Table has been updated with sub-headings for the sub-columns.\n\n- Towards the end of the discussion, it is mentioned that the method is versatile; however, it is stated that the recordings were downsampled to suited the method, rather than the other way around. This implies a lack of flexibility in the method's application due to the constraints imposed by the perceptual loss and VGG network, which requires square matrix inputs. It would be helpful to elaborate on this limitation and discuss potential workarounds or alternatives.\n\nFor humpback whales, recordings were downsampled only to yield more relevant spectrograms (avoiding to have half of the image above their highest vocalizations). We believe this does not imply a lack of flexibility of the method. Some constraints do occur for the input of the encoder, but having square spectrograms is not one of them. We’ve added a sentence in the discussion in this regards : “this versatility comes with one constraint…”.\n\n- The article should address the potential influence of background noise on bioacoustic systems and how it might affect cluster formation. High-capacity models like deep learning have the ability to learn features related to background noise, which could impact the clustering process. It is important to discuss this issue and conduct experiments to investigate whether such biases were avoided or mitigated in the proposed method.\n\nA sentence has been added to the discussion regarding the question of background noise, giving hints on mitigation methods.\n\n- In Figure 3, the subplots are not convincing as there are many unclustered gray dots. The interpretation and inspection of these unclustered points should be clarified in the article. Additionally, the ideal UMAP and HDBSCAN parameters used in each graph should be provided, including any search process for determining the ideal UMAP parameters.\n\nAs described in the legend of Fig 3, colors denote labels, and not clusters (this has been clarified in the legend). Also, the result section describes “we used a grid search to tune these parameters…”; “...these generic settings were kept for all experiments”.\n\n- Figure 4 shows that the NMI decreases for cassin_vireo as the UMAP dimension increases. The reason behind this trend should be explained in the article to provide a better understanding of the results.\n\nThere was a mismatch with legends and plots in this figure, which has been corrected? Also, a small decrease in performance with the increasing number of dimensions is hardly interpretable, as there is no consistent trend across datasets. Unfortunately, the authors are only able to provide empirical evidence here.\n\n- The sentence mentioning the graded nature of humpback whale unit types and their temporal instability needs further clarification. What does it mean for a signal to be \"temporally unstable\"? This concept should be elaborated upon to ensure readers fully comprehend its significance in the context of the article.\n\nThis sentence has been changed to be clarified\n\n- The sentence discussing the increased agreement between clusters and expert labels (0.72 NMI) implies that additional experiments were conducted but not reported. These experiments and their findings should be included in the article to provide a complete and transparent account of the research. Furthermore, the phrase \"reduced when working with a limited amount of time\" requires clarification regarding the specific time limit or duration used for normalization of the AE inputs.\n\nThe “humpback whale (small)” dataset has been added to the paper along with the results it yields, reporting all experiments that are mentioned in the text. The time span referred to in “working with a limited time span” is made clear by the preceding sentence (“in a single day”).\n\n- The GitHub repository should be better organized and documented more effectively. A detailed tutorial on replicating the experiments and processing new datasets should be added to facilitate reproducibility and enable others to build upon the research.\n\nThe repository has been better organized and documented in this regard.\n\n- The work at the provided link (https://experiments.withgoogle.com/bird-sounds) is very nice. It would be interesting to know if the authors have plans to develop a web server for their application, as it could be highly beneficial for the community. (suggestion)\n\nSuch interface demands a significant engineering effort for which authors of this study unfortunately do not have time for. It is possible for other contributors to use our proposed methods and embeddings for such an interface however."
    }
}
{
    "id": "dbpedia_6637_0",
    "rank": 10,
    "data": {
        "url": "https://www.ncbi.nlm.nih.gov/pmc/articles/PMC7756764/",
        "read_more_link": "",
        "language": "en",
        "title": "Artificial intelligence for the measurement of vocal stereotypy",
        "top_image": "https://www.ncbi.nlm.nih.gov/corehtml/pmc/pmcgifs/pmc-card-share.jpg?_=0",
        "meta_img": "https://www.ncbi.nlm.nih.gov/corehtml/pmc/pmcgifs/pmc-card-share.jpg?_=0",
        "images": [
            "https://www.ncbi.nlm.nih.gov/coreutils/uswds/img/favicons/favicon-57.png",
            "https://www.ncbi.nlm.nih.gov/coreutils/uswds/img/icon-dot-gov.svg",
            "https://www.ncbi.nlm.nih.gov/coreutils/uswds/img/icon-https.svg",
            "https://www.ncbi.nlm.nih.gov/coreutils/nwds/img/logos/AgencyLogo.svg",
            "https://www.ncbi.nlm.nih.gov/corehtml/pmc/pmcgifs/logo-blackwellopen.png",
            "https://www.ncbi.nlm.nih.gov/corehtml/pmc/pmcgifs/corrauth.gif",
            "https://www.ncbi.nlm.nih.gov/corehtml/pmc/pmcgifs/corrauth.gif",
            "https://www.ncbi.nlm.nih.gov/pmc/articles/PMC7756764/bin/JEAB-114-368-g001.jpg",
            "https://www.ncbi.nlm.nih.gov/pmc/articles/PMC7756764/bin/JEAB-114-368-g002.jpg",
            "https://www.ncbi.nlm.nih.gov/pmc/articles/PMC7756764/bin/JEAB-114-368-g003.jpg",
            "https://www.ncbi.nlm.nih.gov/pmc/articles/PMC7756764/bin/JEAB-114-368-g004.jpg",
            "https://www.ncbi.nlm.nih.gov/pmc/articles/PMC7756764/bin/JEAB-114-368-g005.jpg",
            "https://www.ncbi.nlm.nih.gov/pmc/articles/PMC7756764/bin/JEAB-114-368-g006.jpg"
        ],
        "movies": [],
        "keywords": [],
        "meta_keywords": [
            ""
        ],
        "tags": null,
        "authors": [
            "Marie‐Michèle Dufour",
            "Marc J. Lanovaz",
            "Patrick Cardinal"
        ],
        "publish_date": "2020-11-16T00:00:00",
        "summary": "",
        "meta_description": "Both researchers and practitioners often rely on direct observation to measure and monitor behavior. When these behaviors are too complex or numerous to be measured in vivo, relying on direct observation using human observers increases the amount of resources ...",
        "meta_lang": "en",
        "meta_favicon": "https://www.ncbi.nlm.nih.gov/coreutils/nwds/img/favicons/favicon.ico",
        "meta_site_name": "PubMed Central (PMC)",
        "canonical_link": "https://www.ncbi.nlm.nih.gov/pmc/articles/PMC7756764/",
        "text": "Whether experimental or applied, the science of behavior analysis targets a wide range of topics that aim to understand and to improve the functioning of human organisms (Skinner, 1951). A common thread central to this endeavor is the measurement of behavior. In most research involving human participants, researchers rely either on permanent products (e.g., responses automatically recorded from pressing on a computer screen) or on direct observation to examine the effects of independent variables on behavior. Practicing behavior analysts must also use these measures to monitor behavior when directly intervening with individuals in a professional setting (Behavior Analyst Certification Board, 2017).\n\nThe reliance on direct observation using human observers in many contexts raises an important issue related to available resources. For example, using continuous recording to monitor multiple or high frequency behavior often requires an independent observer who scores the behavior from a video recording. Furthermore, researchers need to include a second observer to increase the believability of the results by monitoring interobserver agreement (Mudford et al., 2009). If a researcher has 20 hr of video recordings to score for one participant, the human resources can easily add up to 30 to 50 hr of work. This time does not include the resources and time involved in hiring and training additional staff to conduct these tasks. With many participants, these additional resources can add up rapidly and limit the amount of research that can be done or the number of intervention sessions that can be afforded.\n\nOne potential solution to significantly reduce response effort associated with direct observation is to use artificial intelligence. Broadly, artificial intelligence is “the study of how to make computers do things at which, at the moment, people do better” (Rich & Knight, 1991, p. 3). As such, the measurement of behavior is a topic well suited to artificial intelligence as human observers are currently better than computers at monitoring most types of behavior (Goodwin et al., 2011). One promising tool in artificial intelligence is machine learning, which involves training models to detect signals or patterns in data (see Lanovaz et al., 2020, for behavior analytic introduction to the topic). That is, machine learning takes data as input to develop mathematical models that allows them to predict the value or categorization of novel data.\n\nOne type of machine learning algorithm is the artificial neural network. Simply put, an artificial neural network takes input data from the experimenter, which are then transformed by mathematical functions to produce a prediction (Goodfellow et al., 2016). Typically, artificial neural networks contain three types of layer: (1) the input layer, (2) the hidden layer, and (3) the output layer (see Fig. ). The input layer receives features to train the model. The hidden neurons allow the model to learn more complex relationships between these features by transforming the data. Finally, the output layer provides the prediction of the model. Mathematically, the algorithm multiplies the input data by weights (which are initially set randomly) and transforms the product using an activation function to standardize the data. The result is then multiplied by a second series of weights and again transformed by an activation function. Next, the algorithm computes an error using a loss function, which compares the output values with the class labels (i.e., the true values). Finally, the algorithm retropropagates the gradient (derivative) of the error to update the weights. The retropropagation of the gradient of the error involves a variable called the learning rate, which determines how “fast” the model changes the weights. The retropropagation should produce updated weights that typically lead to more accurate predictions (less error) on the next pass. Each pass across the steps is called an epoch. The whole process is akin to shaping in behavior analysis where the model updates itself to provide increasingly more accurate responses following feedback.\n\nOne challenge with artificial neural networks is that the training of models requires a large amount of data. Considering that individuals with developmental disability often engage in high rates of repetitive behavior, a starting point could be to apply these algorithms to this population. Individuals with developmental disability often engage in stereotypy, which is repetitive behavior characterized by movement invariance that is not maintained by social contingencies (Rapp & Vollmer, 2005). Researchers and practitioners further divide stereotypy into two types: motor and vocal stereotypy. Some researchers have already evaluated the use of machine learning algorithms to identify motor stereotypy in this population (Goodwin et al., 2011; Min & Tewfik, 2010; Rad & Furlanello, 2016; Westeyn et al., 2005).\n\nIn the first study on the automated detection of stereotypy, Westeyn et al. (2005) used accelerometers to monitor behaviors and then applied a hidden Markov model to classify the data. Their model was capable of automatically and accurately detecting 69% of hand flapping in one typically developing adult who was mimicking stereotypy. Following this study, other researchers applied different algorithms (k‐nearest neighbors and iterative subspace identification) to monitor body rocking and hand flapping in persons with autism with promising accuracy varying between 83% and 90% (Goodwin et al., 2011; Min & Tewfik, 2010). Finally, Rad and Furlanello (2016) applied artificial neural networks to detect motor stereotypy and found preliminary results that support the relevance of this approach.\n\nThe prior studies have all focused on motor stereotypy, but another form that should be targeted for reduction is vocal stereotypy (Rapp et al., 2013). A recent systematic review reported that 48% of individuals with developmental disability engage in at least one form of vocal stereotypy (Chebli et al., 2016). Examples of forms reported in the literature include monosyllable vocalizations, acontextual words or phrases, and acontextual laughing and grunting (DiGennaro Reed et al., 2012; Lanovaz et al., 2011; Rapp et al., 2013; Spencer & Alkhanji, 2018; Weston et al., 2018). Researchers have also evaluated the potential of machine learning for measuring vocal stereotypy (Min & Fetzner, 2018; 2019). In their first article, Min and Fetzner (2018) used a traditional machine learning algorithm to detect vocal stereotypy in four children with autism spectrum disorder (ASD) with an accuracy between 73% and 93%. In a second study, Min and Fetzner (2019) applied an artificial neural network to detect vocal stereotypy with an accuracy of 85%.\n\nA serious limitation of both prior studies was that the researchers only measured whether stereotypy was absent or present in brief videoclips. If the procedures are to be useful to researchers and practitioners, we must develop models that can measure the duration of stereotypy (not only its presence or absence) during longer sessions. As a matter of fact, finding ways to automate the measurement of the duration of vocal stereotypy could not only decrease the amount of resources required for conducting research, but also facilitate the monitoring of the behavior in applied settings. A second limitation is that the researchers only extracted videos that did not contain treatment (akin to baseline). Researchers and practitioners are likely to measure vocal stereotypy in baseline and treatment sessions, which is why examining the models under both these conditions is important. One common treatment for vocal stereotypy is providing access to noncontingent music (i.e., continuous preferred music). Researchers have repeatedly shown that this treatment is effective at reducing vocal stereotypy in children with ASD (Gibbs et al., 2018; Lanovaz et al., 2011; Saylor et al., 2012). Given the potential challenges of identifying vocal stereotypy during this treatment condition, the inclusion of such sessions when testing models appears essential.\n\nThus, the purpose of our study was to determine whether we could train a machine learning algorithm to measure the duration of vocal stereotypy using audio data extracted from video recordings of both baseline and treatment sessions. More specifically, our study examined whether models derived from an artificial neural network could produce session‐by‐session correlations at or above .80 when compared to the values measured by a human observer. We set the benchmark at .80 because (a) this correlation score is considered strong to very strong (Schober et al., 2018), and (b) prior research has found similar or higher correlations when examining the validity of discontinuous measurements methods (Leblanc et al., 2020). Given the purpose of our study, our research questions were:\n\nCan an artificial neural network produce a session‐by‐session correlation of .80 or better with human observers when measuring the duration of vocal stereotypy?\n\nWhat type of data analysis (i.e., within‐participant, between‐participant, or hybrid approach) produces the best measures of vocal stereotypy?\n\nResults\n\nAs shown in Table , there were between six and 35 sessions per participant, for a total of 142 sessions from eight participants. The total duration of the 142 sessions was 99,564 s. The total duration of sessions with music was 25,887 s, whereas the duration of sessions without music was 73,677 s (see Table ). We used the previous data to develop our machine learning models using between‐participant, within‐participant, and hybrid analyses.\n\nThe left side of Table and Figure present the results of the between‐participant analyses. Five of the eight participants had kappa statistics above or close to 0.5, indicating moderate to substantial agreement between the human observer and the computer model. For these five participants, the session‐by‐session correlation between the human observer and the computer model remained above .80, which indicates a strong to very strong correlation (see Fig. ). Two participants (i.e., Alia and Nate) had negative correlations, which indicates that models were more likely to produce an inverse pattern when compared to the true values. Therefore, we repeated the analysis on sessions without music only to determine whether the background music was misleading the algorithms (see right side of Table and Fig. ). For Alia and Nate, all measures improved. However, the removal of music sessions considerably worsened the correlations for three participants (i.e., Billy‐Peter, Owen, and Dan).\n\nTable 3\n\nAll SessionsSessions Without Music OnlyParticipantsAccuracyKappaCorrelationAccuracyKappaCorrelationEmile0.900.660.860.900.670.87Matt0.780.490.970.770.540.97Dave0.790.500.820.810.570.81Billy‐Peter0.890.500.880.730.330.42Owen0.830.520.800.780.500.47Dan0.750.290.300.770.34‐0.90Alia0.790.30‐0.370.870.520.78Nate0.710.33‐0.120.790.570.88\n\nTable and Figure present the results of the within‐participant analyses. Rather than using the data from the other participants to train the models (as in our between‐participant analyses), the within‐participant analyses consisted of training the models with the participant's own data. This manipulation involved a tradeoff: It reduced the amount of data available in the training set for each participant, but it also made the training set more like the vocal stereotypy that we were trying to measure. The results show that the kappa statistics were higher in the within‐participant analysis than the between‐participant analysis for four participants. In contrast, the correlations improved for six of eight participants. A further examination of these data indicates that this result may be misleading. This improvement involved the three participants who had the lowest correlations in the between‐participant analyses. As such, fewer participants achieved the .80 correlation criterion in the within‐participant analyses (i.e., four) than in the between‐participant analyses (i.e., five).\n\nTable 4\n\nParticipantsAccuracyKappaCorrelationEmile0.940.750.97Matt0.800.430.96Dave0.830.600.66Billy‐Peter0.910.250.93Owen0.860.400.88Dan0.790.230.34Alia0.910.670.58Nate0.740.340.33\n\nAs discussed previously, one issue with within‐participant analyses is that the training sets were smaller than in the between‐participant analyses (i.e., anywhere between 4% and 27% of the total number of samples in the dataset). To address this concern, we further conducted an analysis using a hybrid method combining the within‐ and between‐participant analyses. Table and Figure show the results of the hybrid analyses. Adding between‐participant data to the within‐participant models increased correlations to near or above .80 for two more participants (i.e., Dave and Alia), which led to the models adequately predicting session‐by‐session patterns for six of eight participants.\n\nTable 5\n\nParticipantsAccuracyKappaCorrelationEmile0.950.740.97Matt0.780.410.98Dave0.830.570.84Billy‐Peter0.910.230.87Owen0.850.450.88Dan0.830.240.20Alia0.920.600.79Nate0.730.310.08\n\nAn unexpected observation from the previous analyses was that the kappa scores did not necessarily increase when correlations increased. Kappa represents within‐session patterns of responding, whereas correlations capture between‐session patterns (e.g., immediacy, level, trend). One potential explanation is that errors in measurement in sessions with low levels of stereotypy may deflate the mean kappa scores. As an example, assume that a human observer measured a behavior for 0.3% of a session whereas the model did not detect the behavior (i.e., 0%). Despite the absolute difference being only 0.3%, the kappa score would be 0 for this session. To examine this hypothesis, we measured the correlation between the kappa score and the percentage of engagement on a session‐by‐session basis. Figure shows an example of this correlation for Billy‐Peter. 3 Our analyses found a positive correlation between kappa and percentage of engagement for all participants, indicating that sessions with low levels of stereotypy skewed the estimation of the kappa scores towards lower values (as the computation of the reported kappa scores involved the mean of all sessions).\n\nDiscussion\n\nOur proof of concept produced session‐by‐session correlations near or above .80 for six of eight participants when using a hybrid approach, which generally produced the best outcomes. The hybrid approach may have performed best because it took advantage of each participant's individual responding while augmenting the dataset with samples from other participants. Interestingly, the removal of music during the between‐participant analyses significantly improved the measures for two further participants while worsening the predictions for three others. This worsening of the results may be explained by the removal of the data from the music sessions. The algorithm trained and tested the models on less data in the sessions without music only, which could explain the reduction for some participants. Nonetheless, the results are encouraging, as the high correlations observed in the hybrid analyses included both baseline and treatment sessions.\n\nAs noted in the results, the analyses often produced better estimations of between‐session patterns than within‐session patterns, which may be partly caused by the difficulty in estimating low levels of behavior. This result is consistent with prior research by Leblanc et al. (2020) who found that discontinuous methods of measurement produced less accurate estimates when challenging behavior occurred less frequently. Another potential explanation is that machine learning may produce systematic minor errors at the within‐session level that have a limited effect at the between‐session level. This type of systematic error is not unheard of in behavior analysis. One notable example is the use of discontinuous recording methods. Although discontinuous methods may produce considerably different within‐session patterns, between‐session patterns are similar enough to make these tools useful in practice (LeBlanc et al., 2020; Meany‐Daboul et al., 2007; Rapp et al., 2008; Schmidt et al., 2013). Similarly, our machine learning models preserved important between‐session features used for the analysis of single‐case designs, such as level, trend, and immediacy, while producing less consistent within‐session patterns.\n\nTo our knowledge, this is the first study to use artificial intelligence algorithms to measure the duration of vocal stereotypy during sessions. Our results replicate and extend prior studies that have used machine learning to measure motor and vocal forms of stereotypy (Goodwin et al., 2011; Min & Tewfik, 2010; Rad & Furlanello, 2016; Westeyn et al., 2005). We also extend research on artificial intelligence, as we studied how we can program computers to perform a task at which humans are currently better. Notably, some of our models produced session‐by‐session correlations that rivaled those produced by discontinuous measurement methods (Leblanc et al., 2020). Despite the promising nature of our results, we consider our study as an experimental proof of concept because the session‐by‐session correlations remained inadequate for two participants. As researchers working in a university (nonclinical) setting, we simply did not have access to sufficient data to further improve the performance of our models.\n\nOur artificial neural network trained the models on audio data extracted from video recordings. Hence, the distance between the microphone and the child varied within and across sessions, rendering the analysis by the algorithms more challenging. In the future, we recommend that researchers use a wireless microphone positioned on the child's shirt collar, which should considerably improve measurements by increasing the power of the signal produced by the child's vocal apparatus. Moreover, this change would also facilitate the discrimination between the child's sounds and those of other individuals in the environment. If researchers continue improving the current models, the use of artificial intelligence may produce significant changes in research and practice such as the reduction of costs and the automation of certain repetitive tasks. With additional research, we can imagine the development of systems that could automatically measure target vocal behavior within research, educational and clinical contexts, freeing up time for researchers and practitioners to focus on other important activities. The utility of these models could move beyond single‐case designs. Researchers could also use automated measures with large randomized samples.\n\nThere are two additional limitations that should be noted. First, we used a single method and set of hyperparameters to extract the audio and train our models because we lacked the computing power to conduct multiple comparison analyses. Evaluating the effects of the extraction method and hyperparameters on algorithm performance with more powerful computers (or supercomputers) would be relevant in the future. Second, we did not examine and compare patterns on single‐case graphs. Instead, we used a correlation measure that is similar to a recent study examining correspondence between continuous and discontinuous measurements (see Leblanc et al., 2020). Given that we had to remove sessions due to low‐quality recordings, the sessions were not necessarily consecutive, preventing a thorough single‐case graph analysis. That said, the ultimate litmus test for our approach will be whether functional relations remain observable on single‐case graphs when applying these algorithms in research and applied settings."
    }
}
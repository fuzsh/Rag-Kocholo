{
    "id": "dbpedia_7606_2",
    "rank": 12,
    "data": {
        "url": "https://www.uncannymagazine.com/article/scalzi-on-film-hollywood-totally-lied-to-us-about-ai-why-cinematic-cyborgs-are-so-much-smarter-than-what-we-have-in-the-real-world/",
        "read_more_link": "",
        "language": "en",
        "title": "Scalzi on Film: Hollywood Totally Lied to Us About AI: Why Cinematic Cyborgs Are So Much Smarter Than What We Have in the Real World",
        "top_image": "https://www.uncannymagazine.com/wp-content/uploads/2024/06/Issue-59-cover-med-683x1024.png",
        "meta_img": "https://www.uncannymagazine.com/wp-content/uploads/2024/06/Issue-59-cover-med-683x1024.png",
        "images": [
            "https://www.uncannymagazine.com/wp-content/themes/uncanny/images/masthead.png",
            "https://www.uncannymagazine.com/wp-content/uploads/2024/07/Uncanny-Magazine-Year-11-This-One-Goes-to-ELEVEN-300x38.jpg",
            "https://www.uncannymagazine.com/wp-content/uploads/2020/06/FIYAH-468x60-Uncanny-Ad-02.png",
            "https://www.uncannymagazine.com/wp-content/uploads/2023/10/52920932541_3d258ba3c2_5k-160x200.jpg",
            "https://secure.gravatar.com/avatar/7e919d26ee8d23adb9f0125bb5e7225f?s=64&d=https%3A%2F%2Fwww.uncannymagazine.com%2Fwp-content%2Fthemes%2Funcanny%2Fimages%2Fdefault-user.png&r=g",
            "https://secure.gravatar.com/avatar/552c188ab9d037fe84194385d0eb9a89?s=64&d=https%3A%2F%2Fwww.uncannymagazine.com%2Fwp-content%2Fthemes%2Funcanny%2Fimages%2Fdefault-user.png&r=g",
            "https://www.uncannymagazine.com/wp-content/uploads/2024/08/Heirloom-offsite-ad-300-x-250.png",
            "https://www.uncannymagazine.com/wp-content/uploads/2024/06/Issue-59-cover-med-340x510.png",
            "https://www.uncannymagazine.com/wp-content/uploads/2023/05/20230501-SBP-Uncanny2.jpg",
            "https://www.uncannymagazine.com/wp-content/uploads/2018/03/locus2018subad.jpg"
        ],
        "movies": [],
        "keywords": [],
        "meta_keywords": [
            ""
        ],
        "tags": null,
        "authors": [],
        "publish_date": "2024-08-06T10:00:20+00:00",
        "summary": "",
        "meta_description": "Behold! Science has caught up to fiction, and the age of Artificial Intelligence, long promised by science fiction in film, literature, and video games, is here! And in this golden age…tech companies expend vast amounts of energy to create search engine assistants trained on partially or fully stolen data, who tell us it’s okay to […]",
        "meta_lang": "en",
        "meta_favicon": "https://www.uncannymagazine.com/wp-content/themes/uncanny/images/favicon.ico",
        "meta_site_name": "Uncanny Magazine",
        "canonical_link": "https://www.uncannymagazine.com/article/scalzi-on-film-hollywood-totally-lied-to-us-about-ai-why-cinematic-cyborgs-are-so-much-smarter-than-what-we-have-in-the-real-world/",
        "text": "Behold! Science has caught up to fiction, and the age of Artificial Intelligence, long promised by science fiction in film, literature, and video games, is here! And in this golden age…tech companies expend vast amounts of energy to create search engine assistants trained on partially or fully stolen data, who tell us it’s okay to eat rocks and put glue on pizza, and chatbots who “hallucinate” answers to queries, i.e. confabulate bullshit based on a statistical regression to the textual mean. Our “AI,” as it turns out, is less intelligent than a chicken, even if it has a better vocabulary.\n\nThis is, obviously, a great disappointment for those of us whose idea of “AI” has been shaped by film and television. From the Maschinenmensch of Fritz Lang’s Metropolis in 1927 to the droids of 1977’s Star Wars to the symbiotic AI of Atlas, the latter released just a few months ago, artificially intelligent beings have been a staple of science fiction cinema since nearly the beginning of the filmed genre. And while these intelligences could be helpful (like Robby the Robot in Forbidden Planet), inimical (like the T-800 from The Terminator), or wanting to be one and turning out to be the other (HAL 9000 from 2001: A Space Odyssey), the one thing these intelligences were not was dimwitted.\n\nWhat went wrong? How did cinema sell us on a future of artificial intelligence that our current iterations of it so clearly and obviously miss?\n\nThe simplest answer is the one most rooted in reality: what “artificial intelligence” means in movies, and how the phrase is currently being used in the real world, are two separate things. In nearly all examples of artificial intelligence in film, AIs have an intelligence that is modeled on and interacts well with human intelligence (when it is not trying to kill it). C-3PO of the Star Wars films is easily the most stereotypical example of this kind of intelligence. He (and there is no doubt C-3PO is a “he”—gendering of AI in cinema is also something that goes back to the earliest days of the genre) is designed to be helpful to humans, knowing as he does six million forms of communication. But he can also think on his feet, improvise, and even intentionally lie.\n\nHe does all three in one scene in Star Wars, when, threatened by encroaching stormtroopers, he and his fellow droid R2D2 pretend to lock themselves in closet, and, when freed by the stormtroopers, give them false information about the whereabouts of their compatriots, and then get permission to leave to get maintenance, which is a pretense to get back to the Millennium Falcon to escape. Not bad for a golden bucket of bolts! Earlier in the film C-3PO takes pleasure in an oil bath, and in other places he experiences fear, frustration, relief, and irritation (the latter mostly toward R2D2, his constant companion and 100 percent the top in their clearly bonded-for-life relationship).\n\nC-3PO, and R2D2, and most other droids in the Star Wars universe—and most other AI in cinema—act human, to a greater or lesser extent because they are human, or more accurately, are as human as their direct literary antecedents, which are not mechanical but biological: the monsters of literature and fable who were once, but now no longer are, human. C-3PO has far more in common with Frankenstein’s monster than he does with a steam engine or a loom (albeit closer to the original literary version than the hulking version of the classic Universal movies). The AIs of The Matrix share their cinematic DNA with vampires, feeding off a human host for as long as possible before disposing of them for newer prospects. The Terminator is a golem, its programming inscribing a mission from which it cannot deviate. The AI of Her (and the various sexbots of various films) is Galatea, brought to life by Pygmalions of varying social facility.\n\nOur cinematic artificial intelligences had to be modeled along monstrous lines, if for no other reason than there was no other model to be had. While complex and intricate mechanical creatures have existed for centuries, including ones based on humans, they were primarily used for entertainment, with brains being offloaded: Hero of Alexandria creating mechanical puppets in the first century CE is an example of this. The mechanics of these automatons were an inspiration; intelligence would need to be ported in from elsewhere, even if “mechanics” were offered as the explanation for them.\n\nNow let’s consider today’s “AI.” As Ted Chiang aptly noted, our current iteration of “AI” is more correctly called “applied statistics” than it is “artificial intelligence.” The latter is a term hung onto the process purely for marketing purposes, because it’s sexier and will more reliably pull investing dollars to itself than a more accurate descriptor might. The descriptor that most “AI” companies will use when they’re being circumspect rather than trying to market themselves is “large language model,” which is to say, the underlying programming mines extremely large collections of data—for example, as much of the internet as they can get away with—and then when queried, uses algorithms to create responses based on the available data.\n\nIf one doesn’t interact regularly with large language models, or, alternately, is especially prone to the pathetic fallacy, then the responses one gets from leading LLMs can look and feel as if there is a motivating intelligence behind them. As an example of this, here is a response I got from Claude, one of the major large language models, about the pathetic fallacy and how it relates to how people interact with Claude itself (edited because, well, Claude is verbose):\n\nIn the context of how humans view me and other large language models (LLMs), there could be a tendency to anthropomorphize us—to perceive us as having human-like thoughts, feelings, and intentions when in reality we are just very advanced language models operating based on our training data and algorithms.\n\nWhile LLMs like myself can engage in highly fluent and context-relevant conversation, mirroring many aspects of human language and reasoning, it’s important to remember that we do not actually have subjective experiences, emotions, or a unified sense of selfhood in the way humans do.\n\nIt’s a fascinating philosophical and empirical question as to whether future AI systems could develop genuine sentience and self-awareness. But for current LLMs, it’s important to understand the fundamental differences between our narrow language capabilities and the rich inner experience of human minds. Anthropomorphizing us too strongly would be an example of the pathetic fallacy in action.\n\nHow well-spoken Claude is in reminding humans not to anthropomorphize large language models! It does a better job of it, one suspects, than many humans would, especially the ones who work for companies developing large language models, and who need funding, and lots of it, because LLMs are black holes of development and energy costs, and the race to develop a profitable use for LLMs before the venture funding runs out is real and spectacular. There is irony in Claude warning against the pathetic fallacy in a way that feeds into that same fallacy. I of course mentioned this to Claude, who responded (in part):\n\nYour observation highlights the limitations in my artificial nature—I can analyze and discourse about complex philosophical concepts like the pathetic fallacy, but I do not actually have the subjective experience or self-awareness to fully escape committing the pathetic fallacy myself through my responses. It is a paradox inherent to my design as a language model.\n\nWhen it comes to intelligence in today’s “artificial intelligence,” there is no there there. Instead, there are large data sets and algorithms cleverly designed (by humans) to offer responses that will be approachable to the average person. Creativity and ingenuity are not in the “AI” remit, and that’s a fact that becomes apparent the more one interacts with them.\n\nWhether one is querying information from a large language model, or asking an “AI” art generator to make a picture, or compose a loop of music, or whatever, it becomes evident, usually sooner than later, that the boundaries of what these “AI” can do are solid and, eventually, impossible to breach. Claude the “AI” may be as polite and well-spoken as C-3PO, but Claude will not think to hide itself in a closet as part of an overall plan to deceive stormtroopers. At best, it could employ such a tactic only if that tactic were part of its existing dataset.\n\nThis lack of intelligence is why Google’s Gemini “AI” got into trouble when recommending glue for pizza, and suggesting that eating a small rock per day was a healthy choice for humans. Gemini’s large language model was trained in no small part on the web site Reddit and its literally billions of comments, many of which are, shall we say, less than 100 percent earnest. Gemini knows what’s in its data set, but it doesn’t know sarcasm or confabulation. So when it saw “glue on pizza” as a popular comment regarding pizza toppings, it made…algorithmic assumptions. Note that by the time you read this, Gemini will no longer offer glue as a suggested pizza topping. Note also that actual humans had to go in and fix that.\n\nThe humans marketing LLMs as artificial intelligence can be snarked on and derided for overselling the capabilities and usefulness of their programs (or more accurately, positioning their “AI” as solutions to things they are at best fitfully capable of offering solutions for). That said, I’m not going to blame an LLM for not being C-3PO, or even HAL 9000, for the same reason I’m not going to blame my MINI Cooper for not being a landspeeder. They’re not the same things, even if superficially they have the same function and aims.\n\nThere are things large language models and other, similar programs in other fields, do and do very well. I can appreciate when the “AI” in Photoshop gets rid of an errant item in one of my photos, or when a generative program that works with my digital audio workstation takes a sound sample and makes a bunch of different iterations of it for me to choose from and play with. These things work best when they are tools for creativity, not an alleged replacement for it, no matter how many works of human creativity these programs are (arguably unethically) trained on.\n\nWhat I do find interesting, however, is that now we are at turning point in our view of “AI.” In the past, our experience of “AI” has largely been based on models of intelligence that weren’t artificial, but imaginary: the monsters of myth and legend. Now we have a new experience of AI, the one that shows it as flawed and messy, prone to hallucinations and manipulation, and offering information that can be outright dangerous. How long before this new experience of “AI” begins to make its way into how we portray artificial intelligence in film and other entertainment? The ship of culture is large and slow in turning, but turn it will.\n\nThe power and terror of cinematic AI may be on the way out, replaced by the AI that confidently tells you that putting salt in your coffee or letting air out of your car tires is a fantastic idea. That’s not as horrifying as, say, the intelligent machines of The Matrix or The Terminator. But it has the potential to ruin your day all the same.\n\n© 2024 John Scalzi"
    }
}
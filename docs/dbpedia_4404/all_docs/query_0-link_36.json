{
    "id": "dbpedia_4404_0",
    "rank": 36,
    "data": {
        "url": "https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4865398/",
        "read_more_link": "",
        "language": "en",
        "title": "Maximizing the Implementation Quality of Evidence-Based Preventive Interventions in Schools: A Conceptual Framework",
        "top_image": "https://www.ncbi.nlm.nih.gov/corehtml/pmc/pmcgifs/pmc-card-share.jpg?_=0",
        "meta_img": "https://www.ncbi.nlm.nih.gov/corehtml/pmc/pmcgifs/pmc-card-share.jpg?_=0",
        "images": [
            "https://www.ncbi.nlm.nih.gov/coreutils/uswds/img/favicons/favicon-57.png",
            "https://www.ncbi.nlm.nih.gov/coreutils/uswds/img/icon-dot-gov.svg",
            "https://www.ncbi.nlm.nih.gov/coreutils/uswds/img/icon-https.svg",
            "https://www.ncbi.nlm.nih.gov/coreutils/nwds/img/logos/AgencyLogo.svg",
            "https://www.ncbi.nlm.nih.gov/corehtml/pmc/pmcgifs/logo-nihpa.png",
            "https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4865398/bin/nihms327282f1.jpg"
        ],
        "movies": [],
        "keywords": [],
        "meta_keywords": [
            ""
        ],
        "tags": null,
        "authors": [
            "Celene E. Domitrovich",
            "Catherine P. Bradshaw",
            "Jeanne M. Poduska",
            "Kimberly Hoagwood",
            "Jacquelyn A. Buckley",
            "Serene Olin",
            "Lisa Hunter Romanelli",
            "Philip J. Leaf",
            "Mark T. Greenberg",
            "Nicholas S. Ialongo"
        ],
        "publish_date": "2008-07-28T00:00:00",
        "summary": "",
        "meta_description": "Increased availability of research-supported, school-based prevention programs, coupled with the growing national policy emphasis on use of evidence-based practices, has contributed to a shift in research priorities from efficacy to implementation and ...",
        "meta_lang": "en",
        "meta_favicon": "https://www.ncbi.nlm.nih.gov/coreutils/nwds/img/favicons/favicon.ico",
        "meta_site_name": "PubMed Central (PMC)",
        "canonical_link": "https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4865398/",
        "text": "Adv Sch Ment Health Promot. Author manuscript; available in PMC 2016 May 12.\n\nPublished in final edited form as:\n\nPMCID: PMC4865398\n\nNIHMSID: NIHMS327282\n\nPMID: 27182282\n\nMaximizing the Implementation Quality of Evidence-Based Preventive Interventions in Schools: A Conceptual Framework\n\n,1 ,2 ,3 ,4 ,5 ,4 ,6 ,2 ,1 and 5\n\nCelene E. Domitrovich\n\n1Pennsylvania State University Prevention Research Center\n\nFind articles by Celene E. Domitrovich\n\nCatherine P. Bradshaw\n\n2Johns Hopkins Center for the Prevention of Youth Violence\n\nFind articles by Catherine P. Bradshaw\n\nJeanne M. Poduska\n\n3American Institutes for Research\n\nFind articles by Jeanne M. Poduska\n\nKimberly Hoagwood\n\n4Columbia University\n\nFind articles by Kimberly Hoagwood\n\nJacquelyn A. Buckley\n\n5Johns Hopkins Center for Prevention and Early Intervention\n\nFind articles by Jacquelyn A. Buckley\n\nSerene Olin\n\n4Columbia University\n\nFind articles by Serene Olin\n\nLisa Hunter Romanelli\n\n6Resource for Advancing Children’s Health (REACH) Institute\n\nFind articles by Lisa Hunter Romanelli\n\nPhilip J. Leaf\n\n2Johns Hopkins Center for the Prevention of Youth Violence\n\nFind articles by Philip J. Leaf\n\nMark T. Greenberg\n\n1Pennsylvania State University Prevention Research Center\n\nFind articles by Mark T. Greenberg\n\nNicholas S. Ialongo\n\n5Johns Hopkins Center for Prevention and Early Intervention\n\nFind articles by Nicholas S. Ialongo\n\n1Pennsylvania State University Prevention Research Center\n\n2Johns Hopkins Center for the Prevention of Youth Violence\n\n3American Institutes for Research\n\n4Columbia University\n\n5Johns Hopkins Center for Prevention and Early Intervention\n\n6Resource for Advancing Children’s Health (REACH) Institute\n\nAddress for correspondence Celene Domitrovich, The Pennsylvania State University, Prevention Research Center, 109 South Henderson Building, University Park, PA, 16801; ude.usp@031dxc\n\nAbstract\n\nIncreased availability of research-supported, school-based prevention programs, coupled with the growing national policy emphasis on use of evidence-based practices, has contributed to a shift in research priorities from efficacy to implementation and dissemination. A critical issue in moving research to practice is ensuring high-quality implementation of both the intervention model and the support system for sustaining it. The paper describes a three-level framework for considering the implementation quality of school-based interventions. Future directions for research on implementation are discussed.\n\nKeywords: implementation quality, prevention, schools, conceptual framework\n\nIntroduction\n\nMoving evidence-based practices into real-world settings is both a high priority and a challenge for researchers, practitioners, and policymakers. A large number of interventions delivered in schools have been shown to be effective in preventing problem behavior in children and adolescents (for reviews see Berryhill & Prinz, 2003; Burns & Symington, 2002; Catalano et al, 2002; Greenberg et al, 2001; Hahn et al, 2007), and an increasing number of federal policies encourage the use of evidence-based practices and programs in schools. Although schools offer an enormous opportunity for prevention of behavioral and mental health problems, unique contextual factors at play in schools influence the quality of implementation of preventive interventions (Elliott & Mihalic, 2004; Schoenwald & Hoagwood, 2001).\n\nThis paper aims to promote and improve research on the quality of implementation of preventive interventions in schools. We first establish a definition of implementation that includes characteristics of the intervention itself and characteristics of the intervention’s system of support. We then present a multilevel framework of factors that both theory and empirical research suggest may influence the quality with which preventive interventions are implemented in schools. We conclude with a discussion of areas for future research on implementation and diffusion in school settings.\n\nImplementation of evidence-based programs in schools\n\nChildren and adolescents spend a considerable amount of time at school, making it an ideal setting for prevention efforts (Kaftarian et al, 2004). Schools are the context in which children with mental health needs are most likely to receive some type of service, particularly children whose problems do not meet diagnostic criteria (Farmer et al, 2003; Leaf et al, 1996; Poduska et al, 2008). With federal funds available to support implementation of evidence-based programs, schools are increasingly able to adopt more comprehensive, public health approaches to providing student support services. The public health approach to school-based prevention is based on an epidemiological understanding of population risk in which preventive interventions followed by treatment and maintenance are delivered to distinct populations and subpopulations on the basis of levels of risk (Hoagwood & Johnson, 2003). Mrazek and Haggerty (1994) defined three levels of preventive interventions. Universal preventive interventions are delivered to an entire population and often aim at strengthening some aspect of the environment. Examples are social-emotional curricula, classroom behavior management strategies, and school-wide behavior support. Universal interventions are backed up by selective interventions and indicated interventions, which are delivered to the segment of the population that is at increased risk and has not responded to interventions earlier in the continuum. However, it is rare that there is cross-level coordination between universal and more targeted prevention efforts.\n\nDespite interest in the use of evidence-based prevention programs by school districts, theory and research remain limited on how to move these programs into general practice with high-quality implementation (Elliott & Mihalic, 2004; Domitrovich & Greenberg, 2000; Schoenwald & Hoagwood, 2001). In fact, although a considerable body of research links student outcomes with quality of program implementation (Derzon et al, 2005; Durlak & DuPre, 2008; Durlak & Weissberg, 2005), there is general consensus in the field that prevention programs implemented in schools outside of efficacy trials or highly controlled research studies are typically not implemented with high quality (Dusenbury et al, 2005; Gottfredson & Gottfredson, 2002; Ringwalt et al, 2004). A critical yet often overlooked aspect of the dissemination process in schools is the influence of contextual factors at multiple levels on the quality of program implementation (Adelman & Taylor, 2003; Burns & Hoagwood, 2005; Dusenbury et al, 2003; Fixsen et al, 2005; Greenhalgh et al, 2005; Rohrbach et al, 2006; Wandersman et al, 2008).\n\nConsistent with a social-ecological framework (Atkins et al, 1998; Bronfenbrenner, 1979) and supported by extant research, we present a multilevel model for considering contextual factors that may affect, either directly or indirectly, the implementation quality of school-based interventions (see , overleaf). The multilevel framework takes into consideration the influences of macro-level factors (for example federal, state, and district policies), school-level factors, and individual-level factors. These contextual factors may have more or less importance depending on the stage of implementation or diffusion (program adoption, implementation, or institutionalization) (Fixsen et al, 2005). In this paper, we focus on the stage of implementation that occurs after a school or a district has decided to adopt a program, but before it is sustained or formally integrated into a system.\n\nGiven the focus on implementation quality as the outcome of interest, it is positioned at the center of the proposed model. Implementation quality is the discrepancy between what is planned and what is actually delivered when an intervention is conducted, so it is necessary to specify the model against which actual practice will be measured. As will be described in the next section, the model for both the intervention and its associated support system requires specification.\n\nDefining the intervention and support system model\n\nTwo conceptually distinct components must be considered in regard to quality of implementation: the intervention itself and the support system for the intervention (Chen, 1998, 2003; Klein & Sorra, 1996). Interventions are strategies or innovations linked by a causal mechanism to specified, intended outcomes (Chen, 1998, 2003). They can include programs, policies, processes, or principals (Saul et al, 2008). There is tremendous variation between interventions in terms of the risk factors at which they are aimed, the targets of the interventions (individuals, systems, the environment, for example), and the methods through which they operate. The purpose of a support system is to reduce variability in the quality of program implementation by providing the means and establishing the context for the delivery of interventions through elements such as training implementers and providing the infrastructure necessary to coordinate the deployment of the intervention (Greenberg et al, 2001). The intervention and the corresponding support system are independent, though interrelated, components of a whole.\n\nThis conceptualization of implementation extends traditional perspectives of program implementation that have tended to focus solely on fidelity to the intervention components (Moncher & Prinz, 1991). The planned model for each of these components – the intervention and its corresponding support system –should be specified and monitored to ensure replication with high-quality implementation (Greenberg et al, 2001). Given this dual focus, both components are represented as layers in the center of the conceptual model ( ). As described in the next section, ideally this model is standardized and specified for the intervention and the support system in terms of core elements and the delivery model. These three elements are part of the figure representing implementation quality.\n\nThe implementation model\n\nCore elements of the intervention model\n\nWell-developed programs have a specified set of features or practices that are directly related to the underlying theory of the intervention and describe the mechanisms of change (Collaborative for Academic, Social, and Emotional Learning, 2005; Mrazek & Haggerty, 1994). These features or practices are often called the core components or core elements of the intervention. For example, meta-analytic studies of school-based drug prevention programs have found larger effects when programs were interactive and focused on developing drug refusal skills, changing normative beliefs, and promoting social competence (Tobler et al, 2000; Wilson et al, 2001; Wilson et al, 2003). This was in comparison with information-only programs that involved didactic delivery methods, or programs that consisted primarily of recreational activities, tutoring, or mentoring.\n\nResearchers have used randomized trials comparing intervention components and mediation analyses of proximal and distal outcomes to validate theoretical models that underlie intervention models (McNeal et al, 2004; Skara et al, 2005). Unfortunately, the majority of interventions have not been subjected to component analyses of these kinds. In the meantime, descriptive studies of implementation quality under natural conditions can be used to provide initial support or suggest refinements to program theory when the implementation measures are specifically aligned with the hypothesized core elements. For example, Stevens, van Oost, and de Bourdeaudhuij (2001) studied a bullying prevention program and found that only two of the hypothesized six core components were related to program outcomes.\n\nAbsence of core components, poorly delivered core components, or negative adaptations all have the potential to reduce the intervention impact. Assessments of the implementation quality of core elements should therefore be used as process measures that guide continuing quality improvement and professional development strategies to improve practice. These measures can also be used to understand the implications of program adaptations. Once the critical content or process elements of an intervention are specified, it is possible to assess the degree to which an adaptation deviates from the model.\n\nStandardization of the intervention model\n\nAnother characteristic of interventions found to be positively related to implementation quality is program standardization (Payne et al, 2006). The specification or documentation of the core components of school-based interventions often includes detailed instructional manuals and lesson plans; however, it is important to remember that interventions include a variety of approaches and may target systems rather than individuals in order to improve student outcomes (Berryhill & Prinz, 2003). Lesson plans are not relevant for preventive interventions that apply practices or procedures designed to cause systemic change, such as the school-wide Positive Behavior Supports model (Sugai & Horner, 2006) or the Good Behavior Game (Kellam et al, 2008). However, even for a systemic model which represents a confluence of effective practices and systems of support, it is critical that the implementation quality be monitored to ensure standardization across sites (Bradshaw, Reinke et al, 2008).\n\nDelivery of the intervention model\n\nThe delivery strategy of an intervention can be defined in terms of the frequency, duration, timing, and mode of delivering the core components, as well as the individuals actually responsible for implementing the intervention. Research focused on the relationship of delivery strategies to program implementation and outcomes is in its infancy. With regard to duration, although it seems intuitive that this aspect of delivery would be related to program outcomes, Gottfredson and Wilson (2003) failed to find a significant relationship between duration of school-based substance abuse interventions (measured in months) and student outcomes. However, the literature on social and emotional learning suggests that two-year programs produce more substantial effects than one-year or shorter-term programs (Weissberg & Greenberg, 1998b). For systemic interventions, it often takes schools three to five years to reach high implementation quality and to achieve the desired student outcomes (Rimm-Kauffman et al, 2007; Sugai & Horner, 2006). With regard to the mode of delivery, Vicary and colleagues created a modified version of Life Skills Training to compare two methods of delivery (standard delivery versus an infused approach) which incorporated the intervention’s content into the existing grade-level subject curricula. Neither delivery model resulted in positive main effects, but both showed some short-term impact on substance use reported by girls (Vicary et al, 2006).\n\nResearch comparing different individuals as delivery agents of interventions has not shown a consistent pattern. Meta-analytic studies suggest that some deliverers of school-based substance-abuse interventions produce larger effects than others (Gottfredson & Wilson, 2003). For example, several recent studies have reported that better implementation quality was obtained when delivery was conducted by teachers rather than program specialists from outside agencies (Spoth et al, 2007; McNeal et al, 2004). Cameron and colleagues (1999) reported similar results, but found that the differences were only significant in high-risk educational settings. It is important to note that positive outcomes for school-based substance abuse interventions have been achieved using implementers from outside the educational setting (Ellickson & Bell, 1990; Ellickson et al, 1993). A comparison of two types of implementer with Project Toward No Drugs found no differences between teachers and specialists in student outcomes or implementation quality (Rohrbach et al, 2005).\n\nSupport system\n\nCore elements of the support system\n\nRegardless of a program’s specific content or delivery mode, most programs require some form of support system for effective implementation (Chen, 1990). The most common support system is pre-intervention training that gives implementers the knowledge and skills they need to conduct or use the intervention (Fixsen et al, 2005). Studies comparing teachers who received training with those who did not have shown that training is an important element for effective implementation (Parcel et al, 1991; Perry et al, 1990; Ross et al, 1991). The presence or absence of training is examined more often than specific core components, since the latter vary with the intervention being used. However, a large body of literature exists on adult learning and professional development for educators that can inform the development of support system models for school-based and classroom-based interventions, particularly those for which the teacher is the delivery agent (Fishman et al, 2000; Garet et al, 2001; Joyce & Showers, 2002; Putnam & Borko, 2000). This work highlights the importance of providing opportunities for active learning through observation, meaningful discussion, practice, and reflection.\n\nProfessional development is often conceptualized as an ongoing process rather than as a single event that is aligned and integrated into professional work. Often it includes the use of a more knowledgeable coach to mentor implementers, or the use of peer support (Gingiss, 1992; Joyce & Showers, 2002). Coaching or mentoring is one form of technical assistance, and facilitates the implementation process by helping users understand the intervention, the mechanics of program delivery, and appropriate ways to apply and adapt the intervention with existing practices (Dusenbury et al, 2007). Mentoring that includes in-classroom coaching and out-of-classroom consultation is emerging as a promising professional development strategy that improves the behavioral and educational outcomes of school-based interventions beyond those achieved through traditional instructional workshops (Aber et al, 1998; Barrett et al, 2008; Haskins & Loeb, 2007; Gorman-Smith et al, 2003).\n\nLittle is known about the mechanisms through which mentors or coaches influence behavior change in the individuals with whom they work. However, some descriptive studies suggest that the support and encouragement they provide are essential ingredients (Brooks, 1996; McCormick & Brennan, 2001). Empirical research suggests that the performance feedback provided by coaches may be a critical element contributing to the success of this professional development strategy (Leach & Conto, 1999; Noell et al, 2005; Rose & Church, 1998).\n\nStandardization\n\nWhile standardization has been identified as an important intervention factor related to high-quality implementation, it has rarely been applied to the intervention support system. Most of the research on evidence-based interventions has been conducted by program developers who also provide training and, in some cases, on-going support as interventions are disseminated (Elliot, 1998). There may be less need to verify the implementation quality of this component when control of the support system remains with the developers. However, as the demand for replication of preventive interventions by independent researchers grows, this aspect of implementation monitoring will become more relevant. Some intervention developers may already be aware of this need as they conduct ‘train the trainer’ models that allow communities to develop the internal capacity to sustain interventions over time (for example Committee for Children, n.d.).\n\nDelivery\n\nA few studies have empirically tested different models of support system delivery in schools. One such study found that live training and video training both resulted in similar levels of implementation of the Life Skills Training program (Botvin et al, 1995). Others have found that live training of teachers resulted in greater fidelity of implementation (Basen-Enquist et al, 1994). Similarly, one of the few implementation studies comparing levels of training intensity found no difference in the quality of initial implementation between teachers who received an intensive training and those who received a brief training in the Adolescent Alcohol Prevention model (Rohrbach et al, 1993). Cameron and colleagues (1999) found similar reductions in substance use by students when teachers participated in a self-preparation training rather than a workshop. In some cases, the lack of difference in outcomes between training methods may reflect the level of complexity associated with implementing particular programs. To date, there has been very little research comparing coaching or mentoring support system delivery models. Most is descriptive, and does not provide guidance on the intensity or frequency of mentoring that is needed to achieve high levels of implementation quality and subsequent changes in student outcomes.\n\nMeasuring the quality of the intervention and the support system\n\nDespite the existing research linking quality of program implementation with student outcomes, the process of monitoring the quality of implementation is often overlooked, or given lower priority than measuring outcomes. Comprehensive measurement of the implementation quality of both the intervention and support system should include assessments of adherence in terms of fidelity (degree to which an intervention and its support system are conducted as planned), dosage (specific units of an intervention and support system), and quality of delivery (Dane & Schneider, 1998; Dusenbury et al, 2005). Ideally, multiple indicators (teacher self-report, coach rating, for example) of each dimension are also collected.\n\nFidelity to the intervention is commonly assessed by using implementer self-report, observation, or having participants report on the occurrence of core elements (whether specific content was delivered or techniques used) (Hansen, 1996). There is considerable evidence that teachers routinely adapt programs (Bumbarger & Perkins, 2008; Datnow & Castellano, 2000; Ringwalt et al, 2003), yet to date little research has focused on the impact of the adaptations on program outcomes. As models of intervention support systems are increasingly specified and empirically validated, the fidelity of this component will also need to be assessed.\n\nDosage is one of the easiest measures of implementation quality to quantify. It is often presented in terms of specific units of an intervention (such as number of lessons delivered) or amount of time that a participant is exposed to an intervention (for example hours of contact). Similar ratings can be made of the implementation support model, such as quantifying the number of hours of training or the number of contacts from a coach or supervisor.\n\nQuality of delivery should be monitored as it relates to both the specific intervention components and generalization of the core concepts. Affective engagement, sensitivity, and responsiveness are among the general skills that might apply to most school-based interventions, but sometimes more specific techniques (for example role plays) are needed to deliver an intervention effectively. Dusenbury and colleagues (2005) used the term ‘quality of process’ to highlight the importance of engaging participants in an intervention and the reciprocal nature of interactions that are necessary for learning and behavior change. Generalization of an intervention is the application of an intervention’s core components beyond its given framework. This process requires ‘deep structure’ knowledge of the intervention model, and skillful application, to contribute to student improvement (Han & Weiss, 2005). Generalization is more difficult to capture through self-report measures, and may require more frequent observation, as the behavior is spontaneous and dependent on specific conditions.\n\nInterpersonal skills are not only necessary for intervention delivery but also an essential part of delivering training and on-going support – the common elements of an intervention support system. In a national study of delinquency prevention in schools, Payne and colleagues (2006) combined a measurement of quality of training with a measurement of amount of training to form a multi-item construct referred to as ‘local program process’. They found that this construct correlated positively with the level of intervention use as reported by school personnel responsible for coordinating activities of these types.\n\nIn summary, two types of ‘drift’ commonly occur when evidence-based interventions are implemented in school settings: deviation from the intervention model and deviation from the corresponding support system. Multiple indicators of program adherence allow a strong assessment of the degree of discrepancy between the ‘model’ version of the intervention and the support system as conceived by the developers, and the way it is implemented in real-world settings by school system personnel. Data on implementation quality serves multiple purposes. In practice, it is a process tool that informs professional development and guides quality improvement efforts. In research, information on implementation quality provides the data necessary for testing theories of intervention and determining which core elements of the intervention and/or support system are associated with student outcomes (Gottfredson et al, 2008).\n\nThe multilevel implementation quality framework\n\nImplementation of evidence-based practices in schools does not occur in a vacuum; it is influenced by a broad array of school, district, state, and federal policies and practices. depicts a multilevel conceptual framework that organizes factors that influence implementation quality into three levels: macro level, school level, and individual level. As stated previously, inclusion of factors is based on both theory and empirical research which is provided whenever possible. Factors at all three levels of the model are interdependent, and have the potential to influence the quality with which interventions are implemented and student outcomes (Brint, 2006). For example, macro state-level policy has the potential to have a direct impact on the quality of program delivery by teachers, or to affect it indirectly, through organizational factors such as the funding or time allocated at building level to these types of preventive intervention.\n\nOrganizational influences can also have direct and indirect effects on staff attitudes and efficacy for implementing interventions. For example, strong administrative support or an organizationally healthy school environment can positively influence staff members’ willingness to try innovative interventions, their attendance at training events, and their openness about challenges faced when implementing the program. Teachers working in this type of environment may become more empowered and have greater efficacy, which in turn can affect the quality with which they implement innovations. However, if a new program is adopted without a consideration of how it will fit into the school’s instructional day, teachers may experience burden and stress, which can negatively affect program implementation. These influences may also operate bi-directionally across or within levels; for example, teacher characteristics (for example attitudes to mental health) may influence quality of implementation, or quality of implementation may have a direct and substantial effect on teacher characteristics, such as burnout and stress. A detailed description of the factors in each of the three levels of the conceptual framework is provided below.\n\nMacro-level factors\n\nThe first level of the model ( ) is the broadest, and includes community factors that have the potential to influence the quality of implementation within schools. These sources are not limited to the educational system, but include government and community entities as well.\n\nPolicies and financing\n\nThe first level of the framework represents macro-systemic sources of influence, such as policies and practices at federal, state, and district level that have the potential to influence the implementation of evidence-based programs in schools, primarily by fiscal, regulatory, and administrative means. Title I of the Elementary and Secondary Education Act, last reauthorized by Congress as the No Child Left Behind Act (NCLB), and the special education law, also known as Individuals with Disabilities Education Act (IDEA), are highly visible federal education policies with significant impact on state and district practices and policies through the influence of procedural mandates tied to receipt of federal funding (Kataoke et al, 2008). Although NCLB emphasizes standards-based reforms focused on improving the academic achievement of students, it contains several provisions that support a broader public health mission of schools. These include Title I Part D (programs for children who are neglected, delinquent, or at risk), Part H (dropout prevention), Title IV, Part A (Safe and Drug Free Schools), Title V, Part D, Subpart 2 (Elementary and Secondary School Counseling Programs), Subpart 3 (Partnerships in Character Education), and Subpart 14 (Grants to Improve the Mental Health of Children).\n\nBoth policy and legislative action can have a strong influence on implementation processes. For example, both Illinois and New York have passed legislation requiring that schools develop plans for social and emotional development (Katulak et al, 2008). In addition, Illinois has now developed standards and benchmarks for this domain at each grade level (Illinois State Board of Education, 2004). Researchers and program developers who understand the mission and vision of policymakers and administrators at local, state, and federal levels will be better prepared to establish mutual interests with school district personnel, from superintendents to teachers.\n\nSeveral district-level factors can enhance or impede implementation. Many districts face fiscal challenges in financing their programs. Often the money available in a district cannot be easily blended. Monies from federal grants, foundations, and partnerships with businesses and research institutions are often earmarked for specific activities. The stability of district leadership can influence implementation through its effects on mission articulation, staffing decisions, and programmatic choices. Establishing partnerships with the broader community and institution base can help ensure stability of mission and focus across times of leadership transition and across institutions (Adelman & Taylor, 2003; National Research Council, 2004).\n\nLeadership and human capital\n\nConcepts such as community capacity and empowerment, common in community psychology and participatory research, have not always been given adequate attention in the field of prevention (Wandersman, 2003; Weissberg & Greenberg, 1998a), but they represent macro-level factors that may influence the implementation process within schools. Some researchers apply theories of community science to create community-level interventions that target student outcomes such as youth violence and substance use (Wandersman & Florin, 2003). Many of these interventions, such as Communities that Care (Hawkins & Catalano, 1992), include formation of a community coalition as a mobilization strategy. Research on the functioning of these groups has focused more on the adoption of evidence-based interventions – rather than implementation – and has produced mixed results regarding their impact (Wandersman, 2003). Less attention has been paid to their potential impact on quality of implementation, although theoretical models of how building community capacity assists in reaching this goal are emerging (Chinman et al, 2005). Yet community coalitions have the potential to have a positive influence on aspects of the implementation support system, such as providing training and technical assistance (Feinberg et al, 2008; Spoth & Greenberg, 2005).\n\nOther macro-level factors that have an impact on implementation include availability of qualified professionals in a community to implement programs, availability of trainers or coaches to support implementation in schools, and, within the educational system, allocation of professional development days across the school year that can be used for professional development. A well-respected champion of a program and district-level administrative support also appears to be important for high-quality sustainable implementation (Adelman & Taylor, 2003; Barrett et al, 2008; Elliott & Mihalic, 2004).\n\nCommunity–university partnerships\n\nThere is increasing awareness of the importance of community–university partnerships in promoting use and implementation of evidence-based interventions. The extension service of land grant universities is one mechanism that has been leveraged to disseminate evidence-based interventions in schools (Spoth & Greenberg, 2005). For example, the PROSPER partnership model creates local teams with personnel representing university extension, schools, and community agencies. In order to foster implementation of school-based and family interventions, the PROSPER model focuses on assessing local needs, monitoring implementation, and evaluating outcomes. Research by the developers of this model documents the benefits of community-university partnerships for achieving positive implementation outcomes (Spoth et al, 2007). In another example, Wandersman and his colleagues (Wandersman et al, 1999) developed the Getting to Outcomes process to help practitioners plan, implement and evaluate interventions in order to maximize results. The Collaborative for Academic Social and Emotional Learning (CASEL), at the University of Illinois at Chicago, has created a similar developmental model that supports schools or districts through a multi-year process which attempts to ensure high-quality implementation and sustainability of evidence-based programming (Devaney et al, 2006). This model helps participants build a vision, identify community needs, select appropriate intervention strategies, and create a support system for their training and use.\n\nSchool-level factors\n\nThe second level of the framework represents the school as an organizational entity that has an influence on program implementation. Understanding the organizational context of schools is critical for the implementation and sustainability of interventions because children, teachers, and other school staff are all embedded in this shared environment (Ringeisen et al, 2003). Included in this level of the framework are factors that relate to the school’s organizational functioning, such as the structure or policies within the building, the resources available to support interventions, and the school climate. School climate is reflected by the staff’s perceptions, either of their relationships or of the workplace environment, as well as by characteristics of the student body (Hoagwood & Johnson, 2003; Owens, 2004; Ringeisen et al, 2003; Tanyu et al, 2005). Also included in this level are characteristics of the school and the classroom that have been found to affect implementation, or that theory would suggest might be important to consider.\n\nMission–policy alignment\n\nTeaching and learning are central to any school’s mission. Interventions that align directly with a school’s mission or are easily integrated into the school’s policy and practices are more likely to be prioritized, implemented with quality, and sustained over time (Datnow et al, 2002; Kallestad & Olweus, 2003; Payne et al, 2006). Understanding that the primary mission of schools is academic achievement requires that researchers and program developers highlight the link between prevention of social-emotional and behavioral problems and academic achievement, to illustrate how an intervention helps the school meet its mission (Durlak et al, 2008).\n\nDecision structure\n\nAn important characteristic of the school’s organizational structure is the extent to which power is centralized and roles are formalized and rigid (Hoagwood & Johnson, 2003; Owens, 2004). In schools, structure describes the amount of discretion exercised by teachers in solving problems they encounter in the classroom, their contribution to the development of school policies, and the flexibility they have in how they teach. Involvement of an organization’s members in decision-making decreases resistance to change and increases members’ perceptions of successful program adoption. For curriculum- or classroom-based preventive interventions, teachers who have an active role in deciding what intervention to adopt or determining how a new program fits into the context of the existing educational program are more motivated and committed to high-quality program implementation (Ringwalt et al, 2003).\n\nResources\n\nThe amount and type of resources available to deliver evidence-based services in schools – including funds, materials, knowledge, skills, and equipment available to provide the intervention – are important organizational-level factors to consider (Owens, 2004; Ringeisen et al, 2003). Tangible forms of support, such as monetary incentives (for example stipends for training), dedicated staff time for prevention activities, space, equipment, and other necessary program resources, are part of a school’s capacity to implement an intervention. Although, in many districts, monetary resources are controlled at the district level rather than the school level, building administrators do have influence over the allocation of staff time and space.\n\nPersonnel expertise\n\nAnother factor related to a school’s capacity to implement programs with high quality is the level of prevention expertise in the building. A model for building capacity within a school is to provide enhanced training and technical assistance in classroom-based interventions to a teacher who is a key opinion leader in the school. This person, in turn, can recommend evidence-based strategies to other teachers in the building and serve as an internal prevention specialist (Atkins et al, in press). With the involvement of school personnel, this model can increase the use of ecologically appropriate interventions. As noted above, the availability of qualified staff, such as master teachers, coaches, and school psychologists throughout the district and within schools, can have a significant impact on the quality of intervention implementation and the support system.\n\nAdministrative leadership\n\nSchool administrators can help transform schools into places that are committed to using innovative programs and practices (Datnow et al, 2002; Hallinger & Heck, 1996). A strong leader who advocates using evidence-based practices within a school can have a significant impact on the successful implementation of interventions (Gottfredson & Gottfredson, 2002; Kam et al, 2003; Payne et al, 2006). In addition to endorsing the intervention, effective administrators provide the oversight and accountability that are necessary to maintain focus and ensure follow-through by implementers in the schools. For example, strong administrative support for an intervention occurs when leaders within the school actively participate in the planning, training, and implementation of the program. They can also increase implementation by specifying program participation in staff job descriptions (Barrett et al, 2008) and by requiring that staff allocate class time to implement the program. Formally committing staff and administrators to the prevention activity increases accountability for quality implementation (Rohrbach et al, 2005). In contrast, poor administrative leadership – both in general and for the program – can have a negative impact on implementation quality by contributing to low staff morale and limiting the time allocated for activities perceived as outside the academic mission of the school (Kam et al, 2003).\n\nSchool culture\n\nIn the mental health literature, organizational culture has been distinguished from organizational climate and shown to influence the implementation of services (quality of services) in human service agencies (Aarons, 2005; Glisson & Hemmelgarn, 1998). Distinctions between culture and climate have been proposed as important to consider in schools as well (Hoy et al, 1998; Owens, 2004; Van Houtte, 2005). Culture influences the way things are routinely done in an organization, and reflects the norms, values, and shared beliefs or assumptions of the membership. In contrast, climate reflects an individual’s perceptions (Glisson & Hemmelgarn, 1998; Reichers & Schneider, 1990).\n\nCulture is often assessed by surveying the members of an organization and aggregating their responses on items referring to group norms and shared expectations. School culture is an important factor to consider because the introduction of evidence-based interventions may require expansion of the academic mission and a new focus on monitoring practices with precision to inform professional development. Past experiences may also influence how preventive interventions are incorporated into a school’s culture, and the quality with which interventions are implemented. Similarly, research on mental health providers indicates that working environments that are less bureaucratic and have written practice policies regarding the use of evidence-based practices tend to have staff who are more supportive of using evidence-based practices (Aarons, 2005; Aarons & Sawitzy, 2006). We anticipate that a similar relationship exists in school settings, and may in turn influence adoption of evidence-based programs as well as implementation quality.\n\nSchool climate and organizational health\n\nSchool climate is the organizational personality of a school (Halpin & Croft, 1963). It is relatively stable over time, and influences the behavior of individuals in a building. Research on school climate often focuses on the social or psychological aspect of the construct, and includes measuring student, staff, and/or parent perceptions of interpersonal exchanges (for example open, trusting, respectful) between members of the school community (Bryk & Schneider, 2002). School climate can also include an individual’s perceptions of other structural or philosophical characteristics of the educational setting.\n\nOrganizational health, or an organization’s ability to adapt to challenges over time, is an important indicator of school climate (Bevans et al, 2007; Hoy et al, 1998). Studies have linked positive school climate with student achievement and behavioral adjustment (Bryk & Schnieder, 2002; Esposito, 1999). Battistich and Hom (1997) found that elementary schools with higher ‘sense of community’ scores had significantly lower student drug use and delinquency among fifth-and sixth-grade students. Similarly, staff reports of schools’ overall organizational health have been linked with greater efficacy, commitment, and job satisfaction, as well as with positive outcomes for students (Hoy et al, 1998). Constructs such as openness in communication, orientation to change (Kallestad & Olweus, 2003), and an open and supportive environment (Parcel et al, 2003) have been positively related to measures of implementation quality, whereas poor staff morale, a sense of resignation, and a history of failed intervention attempts have been associated with difficulty in implementing and sustaining innovations (Gottfredson & Gottfredson, 2002).\n\nAlthough the organizational context of schools is considered highly relevant, little is known about the underlying mechanisms linking climate or health to an institution’s ability to implement evidence-based practices successfully. Schools that are organizationally healthy and provide a positive, supportive, and safe environment for staff may contribute to staff members’ efficacy and willingness to commit to the intervention (Bradshaw, Koth et al, 2008). Similarly, the staff’s collective self-efficacy (the perception that the faculty’s efforts as a whole can have a positive impact on students) has been found to be positively associated with student achievement (Goddard & Goddard, 2001). Consequently, the quality with which interventions are implemented is likely to be influenced by staff members’ collective self-efficacy.\n\nCharacteristics of the school\n\nAlthough the mechanisms are not well understood, school-based researchers typically acknowledge that school-level characteristics, such as school size and student mobility, can influence both the outcomes achieved through interventions and the implementation quality. Characteristics of the school building or the student body aggregated at building level can be used to examine potential moderators of student outcomes and may predict the quality with which interventions are implemented (Bradshaw, Koth et al, in press). For example, some school-based preventive interventions may be easier to implement in small schools than in large schools, or in rural or suburban communities than in urban settings. Schools that are disorganized or have a large number of at-risk students may encounter more problems in implementing interventions with high fidelity (Gottfredson et al, 2002; Tolan et al, 2004). This may be due in part to high mobility or absenteeism, which results in reduced exposure of participants to critical components of the intervention (low dosage because of high student absenteeism). Schools in disadvantaged neighborhoods may experience greater staff turnover, which undermines the ability to sustain a workforce trained to implement preventive interventions.\n\nClassroom climate\n\nAlthough no single factor defines the climate of a classroom, this construct typically refers to the array of social and psychological aspects of the classroom environment, including the sense of belonging, the level of cooperation and mutual respect among classroom members, and the relationships between teacher and students (Wang et al, 1997). It can also reflect educational aspects of the environment (such as teaching practices, rule clarity). Because many preventive interventions in schools are classroom-based (universal drug prevention, character education programs, for example), classroom climate can have a significant impact on the quality of implementation and outcomes of evidence-based practices (Dunn & Harris, 1998). A classroom climate characterized by high levels of peer or teacher–student conflict may negatively influence program implementation and program effectiveness. High levels of student misconduct in the classroom may result in a teacher spending more time on management than on instruction, and may have a negative impact on the classroom environment (Koth et al, 2008; Kellam et al, 1998).\n\nIndividual-level factors\n\nThe third level of our framework represents individual-level factors that can promote or undermine the quality of intervention implementation in schools. Although some theoretical work highlights the potential significance of individual factors in relation to high-quality implementation and sustainability of interventions (Han & Weiss, 2005; Jennings & Greenberg, 2008; Klein & Sorra, 1996), empirical research on how implementer characteristics influence implementation quality has been relatively limited.\n\nProfessional characteristics\n\nFew teacher training programs include training on classroom management or prevention programs. Similarly, training programs for counselors and school psychologists typically focus on one-on-one or group counseling, rather than on implementation of classroom-based prevention programs. Consequently, school staff vary widely in their education, skills, and experience, which can influence attitudes to the implementation of prevention programs. Studies of the relationship between the professional characteristics of teachers and program implementation attitudes and behaviors offer mixed findings. Aarons (2004) found that, among mental health clinicians, having a higher level of education or being an intern was associated with more positive attitudes to evidence-based practices. In the substance-abuse prevention literature, fewer years of teaching experience and greater teaching skills were associated with higher levels of implementation (Ringwalt et al, 2002; Rohrbach et al, 1993). Other research has shown that years of experience are not related to program fidelity or to the likelihood of using an evidence-based curriculum (Ringwalt et al, 2002, 2003).\n\nPsychological characteristics\n\nLack of experience in implementing preventive interventions, or level of comfort with certain methods (such as interactive teaching) may increase implementers’ anxiety when they are called upon to implement such interventions (Ennett et al, 2003). Although psychological mindedness can vary considerably between individuals, such awareness can be helpful in understanding both negative reactions (such as anxiety, reluctance, and anger) and positive reactions (like enthusiasm or confidence) in one’s own self and in intervention participants. Some researchers have examined the personal characteristics of implementers as potential predictors of positive intervention effects. Findings suggest that traits such as sociability, extroversion, agreeableness, conscientiousness, and individualization are characteristics associated with positive implementation outcomes (Lochman et al, 2008; St. Pierre et al, 2007). Cynicism, on the other hand, was inversely related to implementation quality (Lochman et al, 2008).\n\nAnother important characteristic of the implementer is psychological functioning. Stress, depression, and professional burnout are aspects of psychological functioning that can reduce productivity and the quality of performance in the workplace. Professional burnout, typically defined as emotional exhaustion, depersonalization (such as indifferent or negative feelings displayed toward students), and lack of work-related accomplishment (educators feeling that they are no longer contributing to students’ development, for example), is a significant problem among teachers (Borg et al, 1991). Although few studies have examined how these psychological constructs relate to implementation of academic or preventive interventions (Evers et al, 2002), psychological functioning probably has an impact on implementation quality, particularly when program innovations are perceived as causing additional burden or competing with other priorities.\n\nSelf-efficacy is an indicator of psychological functioning that drives effort and persistence in the face of challenges. In the education literature, efficacy describes teachers’ perceived ability to conduct instructional practices, manage the classroom environment, and affect change in student behaviors (Tschannen-Moran & Woolfolk Hoy, 2001). Research on self-efficacy related to behavioral interventions or instructional strategies has generally concluded that greater efficacy is associated with higher-quality program implementation (Kallestad & Olweus, 2003; Rohrbach et al, 1993). Intervention-specific efficacy (for example knowledge of program theory and components, proficiency in delivering activities) has been shown to relate to higher implementation quality, as has comfort with using interactive methods in intervention delivery (Ennett et al, 2003; Rohrbach et al, 1993). Ransford (2008) found an interaction between teacher efficacy and burnout associated with the quality of implementation of a social and emotional learning program. Teachers who reported high burnout and high efficacy reported high-quality implementation, whereas teachers who reported high burnout and low efficacy showed substantially poorer implementation. In addition, both principal leadership and teacher’s perceptions of the quality of the coaching independently predicted implementation quality.\n\nPerceptions of and attitudes to the intervention\n\nA variety of program attributes that are reflected in implementers’ perceptions and attitudes appear to enhance implementation quality. One of the most important is acceptance of the intervention, which varies with the individual’s needs and priority (Rohrbach et al, 1993). In some cases it is the perception that the intervention is a useful strategy for addressing a local problem. Related to this is the perception that the program is better than the current practice (Elias et al, 2003; Pankratz et al, 2002; Parcel et al, 1991; Ringwalt et al, 2003) and that the program is compatible with the values, needs, mission, and experiences of the institution (Pankratz et al, 2002; Rogers, 2003). Research suggests that programs requiring special skill and coordination among many people (high in complexity) are less likely to be perceived by the implementing staff as effective and are less likely to be maintained over time (Dusenbury et al, 2003).\n\nOther perceptions or attitudes evolve as implementers have experience with the intervention. Several researchers have found that perceived effectiveness is associated with higher implementation quality (Datnow & Castellano, 2000; Kealey et al, 2000; Ringwalt et al, 2003). Han and Weiss (2005) provide a model of how implementer experience of success with an intervention and attribution of student improvements to the intervention influence motivation and skill over time, which, in turn, promotes high-quality implementation and sustainability.\n\nAs implementers become more familiar with an intervention, their ability to understand the intervention has implications for how it will be delivered (Dusenbury et al, 2003; Pankratz et al, 2002; Rogers, 2003). Related to this, the perceived value of the program is an important factor; if teachers do not see the value of fostering a specific skill or conducting lessons on particular topics, they may be more likely to skip those activities, even those that are core elements of the program. In a related manner, teachers may be less committed to implementing interventions aimed at depression than those aimed at classroom behavior management, because the symptoms of depression are typically less disruptive for the classroom environment than are externalizing behavior problems (Bradshaw, Buckley et al, in press).\n\nAs discussed above in the section on support systems and professional development, the acceptability and implementation of an intervention can be increased when implementers are exposed to model implementers or exemplars of quality implementation in contextually similar schools. Further, the quality of engagement during trainings, and satisfaction with the content and how it is delivered, are likely to be important predictors of the quality with which implementers deliver the intervention.\n\nFuture directions\n\nThe field of prevention science is at a critical juncture, as the focus expands from intervention efficacy and effectiveness to include sustainability, transportability, and dissemination (Brounstein et al, 2006; Schoenwald & Hoagwood, 2001; Racine, 2006; Rohrbach et al, 2006). However, the research base on implementation quality is not keeping pace with the growing emphasis on adoption of empirically derived interventions (Kaftarian et al, 2004; Greenberg, 2004, 2007). The multilevel contextual framework presented in this paper can help guide the next stage of research, focused on the implementation quality of school-based preventive interventions. We conclude by identifying specific areas for future research.\n\nTheory-driven research on implementation quality\n\nInterest in assessing implementation quality in school-based intervention research is increasing, but it is often viewed as a secondary or tertiary aim instead of as the focus of a study. Although several of the contextual factors identified above are malleable, the majority of existing implementation studies in schools have been cross-sectional, thereby limiting their ability to draw conclusions about cause and effect relationships. A natural next step for the field is carefully to develop and test theory-based interventions aimed at specific individual and contextual factors in the proposed model (Pentz, 2004). Given the multilevel structure of school systems, the next stage of research requires a focus on the theories of both individual change and organizational change (Glasgow et al, 2003; Klein & Sorra, 1996). Systematic testing of different aspects of the proposed model by experimental trials will allow us to determine which factors are most influential in promoting high-quality implementation. Although we might predict that more proximal factors, such as teacher qualifications and attitudes, would have the strongest influence on implementation quality, organizational factors, such as administrative leadership, school culture, and school climate, should not be under-estimated. Future studies should include multiple indicators of the implementation quality of the intervention model and the support system, along with an assessment of the contextual factors that the theoretical model suggests would predict implementation quality.\n\nOne such study examined the impact of enhanced principal leadership on implementation quality by conducting a brief, 30-minute intervention for principals that emphasized the importance of the intervention and administrative leadership in facilitating high-quality implementation (Rohrbach et al, 1993). Although the effect of a principal’s intervention on the teachers’ quality of implementation was positive when compared with the controls, no differences were found between the two conditions on principals’ self-report ratings or teachers’ ratings of their principal’s encouragement. Other studies could examine the impact of ‘pre-implementation’ training designed to improve schools’ and implementers’ readiness for implementation, such as focusing on strategies for gaining buy-in and enhancing the implementers’ skill and efficacy in implementing the intervention with integrity. Another possible staff-focused study could examine the impact of pre-implementation trainings that incorporate principles of mindfulness to reduce stress and promote emotional insight.\n\nSpecifying the core elements of the intervention and the support system is critical to understanding implementation and identifying which core elements are related to outcomes.\n\nThe importance of the relationships between the core elements of the intervention, the core elements of the support system, and implementation quality is illustrated by the five-year group randomized trial of the universal school-wide Positive Behavior Support. The initial two-day training, coupled with ongoing support and coaching and annual booster training sessions, resulted in high-quality implementation in all 21 trained elementary schools, and significant improvements in the schools’ organizational health. The 16 comparison schools, which did not receive formal training or coaching in Positive Behavior Support, adopted some elements of the school-wide model (Bradshaw, Reinke et al, 2008), but this did not result in significant improvements in the schools’ organizational climate (Bradshaw, Koth et al, 2008). These findings suggest that, even though comparison schools implemented some aspects of the model, their lack of formal training and coaching probably hindered them from ‘self-implementing’ the model with high quality, or achieving the intended outcomes. These findings also highlight the importance of monitoring implementation in both intervention and comparison conditions in randomized trials.\n\nAt a broader level, there is a need not only to assess programs and the factors influencing their implementation, but also to consider broader structural models of school-wide decision-making. For example, does engaging schools in an intentional multi-stage planning and implementation process over multiple years (as with the CASEL Sustainability Tool Kit; Devaney et al, 2006) lead to higher-quality implementation and greater sustainability?\n\nDetermining the effect of adaptations on implementation quality and outcomes\n\nConsiderable evidence indicates that teachers routinely adapt programs (Datnow & Castellano, 2000; Ringwalt et al, 2004). Some intervention developers and researchers believe that adaptations or modifications may be necessary for successful implementation, because of increased buy-in and improved fit between an intervention, its consumers, and the context (Castro et al, 2004; Dusenbury & Hansen, 2004; Kumper et al, 2002; Wandersman, 2003). An in-depth study of the Success for All program in two schools found that, although all teachers adapted the program, the level of support for the program was not related to teachers’ fidelity to practice (Datnow & Castellano, 2000). However, many community psychologists see adaptation and tailoring of interventions to community needs as the essential ingredients for successful dissemination of evidence-based interventions that have been missing from prevention science, and are calling for application of systematic strategies to guide this process so that essential ingredients are retained but communities are empowered (Wandersman, 2003).\n\nA study that coded teacher adaptations to a drug prevention curriculum in terms of detractions or enhancements found that the latter were positively associated with adherence to the intervention (defined as the number of objectives reached and major points covered by teachers during delivery) (Dusenbury et al, 2005). This suggests that adaptation is complicated, and that not all adaptations are the same. Further research on adaptations is necessary before assuming that all changes at local level are acceptable and do not weaken program impact (Elliott & Mihalic, 2004).\n\nThe fidelity/adaptation debate is particularly relevant when interventions are used by communities whose cultures differ from that of the original trial population. Both implementers and trainers often add to or omit parts of interventions during adaptation for a new context, especially when working with ethnic minority populations who are typically under-represented in efficacy trials. Although few preventive interventions have been adapted for these populations, a recent meta-analysis of rigorously tested cultural adaptations suggests that carefully articulated adaptations can be effective for minority populations (Griner & Smith, 2006). However, the majority of cultural adaptations require rigorous evaluations to establish their efficacy (Castro et al, 2004).\n\nMeasuring implementation quality\n\nResearch is needed in several areas of measurement related to implementation quality (Dusenbury et al, 2005). The field needs to develop well-validated, cost-effective measures of implementation quality. Currently, the data used to assess implementation quality range in both mode (self-report, participant report, live observation, coding of recorded sessions) and depth (random selection of one session vs. assessment of all sessions). Though independent ratings of observed quality by an expert are more reliable and valid than self-report ratings from implementers (Lillehoj et al, 2004), which may be easier to obtain and more comprehensive (Hansen & McNeal, 1999), schools need to measure implementation reliably and with validity, without total dependence on external researchers. It is also important that we determine the thresholds of implementation quality necessary for producing the intended outcomes. Few programs have precise research-supported criteria for ‘high’ or ‘low’ implementation quality.\n\nMore studies are needed that move beyond traditional measures of fidelity and dosage to assess process variables and more complicated constructs, such as intervention generalization or adaptation (Bumbarger & Perkins, 2008; Dusenbury et al, 2005). Measuring intervention and support system implementation in both intervention and comparison settings is critical, and should be typical practice in school-based research; most schools have some interventions already in place when new ones are introduced and evaluated (Bradshaw, Reinke et al, 2008; Gottfredson et al, 2008).\n\nMeasuring multilevel constructs\n\nAttention needs to be paid to the measurement of school-level and other multilevel constructs. Characteristics of the school building, the student body, and the staff are often aggregated at the building level. However, several of these school-level indicators co-occur (for example free and reduced-price meals rate, mobility, urbanicity, absenteeism) (Gottfredson et al, 2005), making it statistically difficult to adjust for these factors while avoiding potential problems with multicolinearity. Another troubling issue is that individual-level variables are often aggregated into group-level variables without careful consideration of underlying theory or constructs (Klein & Kozlowski, 2000). If individual-level data show a high degree of within-group agreement, a case can be made for aggregating the data to represent a group-level construct, but only after a full examination of the data. For example, individuals’ perceptions of a school tend to be highly correlated, showing a high degree of within-group agreement, yet aggregating at the school level can mask some systematic variation in perceptions (Raudenbush & Bryk, 2002).\n\nFor example, research suggests that among elementary school staff, ethnic minorities and males tend to perceive the school environment less favorably, as do staff who work primarily with students in special education (Bevans et al, 2007). With regard to student perceptions of climate, Koth and colleagues (2008) found that both classroom characteristics (such as preponderance of students with behavior problems) and individual characteristics (for example ethnicity, gender) had a significant influence on students’ perceptions of the school environment. Aggregating group-referent items is often a better strategy than aggregating individual-referent items when measuring attributes of the group (Klein & Kozlowski, 2000). An example is the construct of collective efficacy, which is conceptualized as a shared property of the group, not the average of team members’ self-efficacy, and is measured by items referencing the group, not the individual (Goodard et al, 2001).\n\nAnalytic strategies\n\nAlthough use of multilevel methods to address the clustering that is common in school-based trials has significantly increased (Murray, 1998; Shinn, 2003; West & Aiken, 1997), there is a need for strategies to account for clustering over time that occurs in the context of longitudinal school-based research. This includes more advanced modeling techniques to examine mediators and moderators of student outcomes within hierarchical structures. Further complicating this process is the finding that implementation quality can vary considerably even in a single school, which suggests that contextual influences at multiple levels may be at play (Choi, 2003; Kallestad & Olweus, 2003). The classroom, rather than the school, may be the appropriate level of analysis for some research questions.\n\nDesigns to support implementation research are needed. Obtaining the statistical power to study the relationship between variation in implementation quality and outcomes will require multiple group (for example schools, classrooms) and longitudinal studies with groups that have varying levels of implementation quality. The treatment of these contextual variables in analytic modeling also has an influence on the statistical power to detect intervention effects in group randomized trials. Sometimes researchers have no option but to include these variables as school-level factors, as in the case of school size or urbanicity. However, when individual responses are available (as in the case with student or staff reports of climate or context), entering them in the model as individual-level covariates does not affect the power to detect an effect, whereas loading up the model with school-level covariates can attenuate the statistical power (Murray, 1998). Power is further diminished when group- or school-level interactions are examined. Additional work is needed to understand better the most appropriate treatment of contextual factors, from both conceptual and methodological perspectives.\n\nLongitudinal research on the implementation process\n\nAlthough the model presented in this paper examines implementation quality at the point when schools have committed to using a specific evidence-based intervention, it is important to recognize that implementation is part of a non-linear diffusion process that evolves over time (Adelman & Taylor, 2003; Dusenbury & Hansen, 2004). It begins with an adoption phase, when interventions are being considered, and it progresses through an implementation phase until interventions are either abandoned or institutionalized and sustained over time (Rogers, 2003). Whereas some researchers, such as Han and Weiss (2005), have acknowledged that time is a critical factor in sustainability, we contend that making the process cyclical by including feedback loops across phases is an important issue to consider in future research on implementation quality (Devaney et al, 2006).\n\nLongitudinal evaluations of interventions focused on clarifying the ‘process’ of quality implementation would be useful, particularly if the studies prospectively measured factors that exist prior to the adoption or implementation phase. One such study used the Bridge-It survey tool to assess factors that predicted the implementation of school-based tobacco prevention programs (Gingiss et al, 2006). The findings support the use of an overall measure that consists of eight dimensions, including characteristics of the intervention, implementer, school building, and broader context. The study of Positive Behavior Supports cited above also found that schools with lower levels of organizational health at baseline tended to take longer to implement the model with high implementation quality, but also tended to benefit the most from the program (Bradshaw, Koth et al, 2008). These findings highlight the importance of a longitudinal assessment of both implementation quality and contextual factors that may both moderate and mediate the implementation quality.\n\nSupporting the next stage of research on implementation quality\n\nStudying the process of moving evidence-based school-based programs into widespread practice with fidelity will require partnerships with multiple school systems (Glasgow et al, 2003). The challenges of multi-site trials, such as maintaining consistency in design, measurement, and intervention protocols, are present when working with multiple schools in any single school district. Funding large-scale implementation studies is another challenge, especially when schools are the unit of randomization. Research grants cannot cover the entire cost of services for schools included in designs that manipulate intervention content or aspects of the support system model (such as method of training, coaching versus no coaching). The scientific community needs to develop models of researcher–community partnerships. Such partnerships can ensure that the programs developed are applicable and relevant to school systems, and that the systems are ready to adopt and scale up these programs as they are proven effective. Partnership networks could provide opportunities to link funds from multiple sources, including district funds, local funds, grants from national foundations, service grants, and research grants. The concept of ‘braided funding’ is advocated by professional organizations that specialize in prevention research (Society for Prevention Research, 2005). Even more important is that such partnerships offer the opportunity for districts and researchers to learn from and support each other in their work toward the shared goal of moving evidence-based programs into general practice with high-quality implementation.\n\nConclusion\n\nFederal and state policies place schools under tremendous scrutiny, and many schools are turning to evidence-based preventive interventions in an effort to improve student outcomes. Despite the large number of interventions that have been shown to be effective in preventing problem behavior in children and adolescents, and federal policies that support their use, theory and research on how best to implement these programs and practices with high fidelity in schools are limited (Racine, 2006; Schoenwald & Hoagwood, 2001). The existing research suggests that a variety of contextual factors at multiple levels (macro, school, and individual) have a significant impact on the quality with which evidence-based interventions are implemented in schools. This paper addresses this research-to-practice gap by presenting a multilevel model that identifies factors that influence implementation quality in school settings. A primary aim of this paper is to promote a comprehensive research agenda for moving evidence-based programs into general practice in schools with high-quality implementation.\n\nAt present, there are more questions than answers about how to integrate preventive interventions in schools so that they are implemented with high quality and are sustained over time. Even so, the possibility of building an empirically-based science of implementation is timely and opportune. It is promising to see that federal agencies, such as the National Institute of Mental Health, the Centers for Disease Control and Prevention, and the Institute of Education Sciences, are increasingly funding research that aims to inform all stages of the process of moving research into practice. It is imperative that the next stage of work be informed by comprehensive and well-thought-out theories of implementation and dissemination that can guide empirical research and inform practice, thereby maximizing positive intervention impact.\n\nAcknowledgments\n\nPreparation of this manuscript was supported by a grant jointly funded by the National Institute of Mental Health and the National Institute on Drug Abuse (P30 MH06624) and grants from the Centers for Disease Control and Prevention (1U49CE 000728 and K01CE001333-01) and the National Institute on Drug Abuse (RO1 DA019984).\n\nReferences\n\nAarons GA. Measuring provider attitudes toward adoption of evidence-based practice: considerations of organizational context and individual differences. Child and Adolescent Psychiatric Clinics of North America. 2005;14:255–71. [PMC free article] [PubMed] [Google Scholar]\n\nAarons GA, Sawitzy AC. Organizational culture and climate and mental health provider attitudes toward evidence-based practice. Psychological Services. 2006;3:61–72. [PMC free article] [PubMed] [Google Scholar]\n\nAber JL, Jones SM, Brown JL, Chaudry N, Samples F. Resolving conflict creatively: evaluating the developmental effects of a school-based violence prevention program in neighborhood and classroom context. Development and Psychopathology. 1998;10:187–213. [PubMed] [Google Scholar]\n\nAdelman HS, Taylor L. On sustainability of project innovations as systemic change. Journal of Educational and Psychological Consultation. 2003;14:1–25. [Google Scholar]\n\nAtkins MS, Frazier SL, Leathers SJ, et al. Teacher key opinion leaders and mental health consultation in urban low-income schools. Journal of Consulting and Clinical Psychology in press. [PubMed] [Google Scholar]\n\nAtkins MS, McKay MM, Arvanitis P, et al. An ecological model for school-based mental health services for urban low-income aggressive children. Journal of Behavioral Health Services & Research. 1998;25:64–75. [PubMed] [Google Scholar]\n\nBarrett S, Bradshaw CP, Lewis-Palmer T. Maryland state-wide PBIS initiative: systems, evaluation, and next steps. Journal of Positive Behavior Interventions. 2008;10:105–14. [Google Scholar]\n\nBasen-Engquist K, O’Hara-Tompkins N, Lovato CY, et al. The effect of two types of teacher training on implementation of smart choices: a tobacco prevention curriculum. Journal of School Health. 1994;64:334–9. [PubMed] [Google Scholar]\n\nBattistich V, Hom A. The relationship between students’ sense of their school as community and their involvement in problem behaviors. American Journal of Public Health. 1997;87:1997–2001. [PMC free article] [PubMed] [Google Scholar]\n\nBorg MG, Riding RJ, Falzon JM. Stress in teaching: a study of occupational stress and its determinants, job satisfaction and career commitment among primary schoolteachers. Journal of Educational Psychology. 1991;11:59–75. [Google Scholar]\n\nBotvin GJ, Baker E, Dusenbury L, Botvin EM, Diaz T. Long-term follow-up results of a randomized drug abuse prevention trial in a white middle-class population. Journal of the American Medical Association. 1995;273:1106–12. [PubMed] [Google Scholar]\n\nBradshaw CP, Buckley J, Ialongo N. School-based service utilization among urban children with early-onset educational and mental health problems: the squeaky wheel phenomenon. School Psychology Quarterly in press. [Google Scholar]\n\nBradshaw CP, Koth CW, Bevans KB, Ialongo N, Leaf PJ. The impact of school-wide positive behavioral interventions and supports (PBIS) on the organizational health of elementary schools. School Psychology Quarterly in press. [Google Scholar]\n\nBradshaw CP, Koth CW, Thornton L, Leaf PJ. Altering school context through school-wide positive behavioral interventions and supports: findings from a group-randomized effectiveness trial. 2008 Manuscript submitted for publication. [PubMed] [Google Scholar]\n\nBradshaw CP, Reinke WM, Brown LD, Bevans KB, Leaf PJ. Implementation of school-wide positive behavioral interventions and supports (PBIS) in elementary schools: observations from a randomized trial. Education & Treatment of Children. 2008;31:1–26. [Google Scholar]\n\nBrint S. Schools and Society. Stanford University Press; 2006. Schools as social institutions. [Google Scholar]\n\nBrooks V. Mentoring: the interpersonal dimension. Teacher Development. 1996:5–10. [Google Scholar]\n\nBronfenbrenner U. The Ecology of Human Development: Experiments by nature and design. Cambridge, MA: Harvard University of Press; 1979. [Google Scholar]\n\nBrounstein PJ, Gardner SE, Backer TE. Research to practice: efforts to bring effective prevention to every community. Journal of Primary Prevention. 2006;27:91–109. [PubMed] [Google Scholar]\n\nBryk AS, Schnieder B. Trust in Schools: A core resource for improvement. New York: Russell Sage; 2002. [Google Scholar]\n\nBumbarger B, Perkins D. Unpublished manuscript. 2008. After randomized trials: issues related to dissemination of evidence-based interventions. [Google Scholar]\n\nBurns BJ, Hoagwood K, editors. Evidence-based practices Part II: Effecting change. Child and Adolescent Psychiatric Clinics of North America. 2005;14(2):xv–xvii. [PubMed] [Google Scholar]\n\nBurns MK, Symington T. A meta-analysis of prereferral intervention teams: student and systemic outcomes. Journal of School Psychology. 2002;40:437–47. [Google Scholar]\n\nCameron R, Brown KS, Best JA, et al. Effectiveness of a social influences smoking prevention program as a function of provider type, training method, and school risk. American Journal of Public Health. 1999;89:1827–31. [PMC free article] [PubMed] [Google Scholar]\n\nCastro FG, Barrera M, Martinez CR. The cultural adaptation of prevention interventions: resolving tensions between fidelity and fit. Prevention Science. 2004;5:41–5. [PubMed] [Google Scholar]\n\nCatalano RF, Berglund ML, Ryan JAM, Lonczak HS, Hawkins JD. Positive youth development in the United States: research findings on evaluations of positive youth development programs. Prevention & Treatment. 2002;5:Article 15. [Google Scholar]\n\nChen HT. Theory-Driven Evaluations. Newbury Park, CA: Sage Publications; 1990. [Google Scholar]\n\nChen HT. Theory-driven evaluations. Advances in Educational Productivity. 1998;7:15–34. [Google Scholar]\n\nChen HT. Theory-driven approach for facilitation of planning health promotion or other programs. Canadian Journal of Program Evaluation. 2003;18:91–113. [Google Scholar]\n\nChinman M, Hannah G, Wandersman A, et al. Developing a community science research agenda for building community capacity for effective preventive interventions. American Journal of Community Psychology. 2005;35:143–57. [PubMed] [Google Scholar]\n\nChoi JN. How does context influence individual behavior? Multilevel assessment of the implementation of social innovations. Prevention & Treatment. 2003;6:1–8. [Google Scholar]\n\nCollaborative for Academic, Social, and Emotional Learning. The Illinois Edition of Safe and Sound: An educational leader’s guide to evidence-based social and emotional learning programs. Chicago, IL: Author; 2005. [Google Scholar]\n\nCommittee for Children. Second Step: steps for successful implementation in schools [Handout] Seattle, WA: Author; n.d. [Google Scholar]\n\nDane AV, Schneider BH. Program integrity in primary and early secondary prevention: are implementation effects out of control? Clinical Psychology Review. 1998;18:23–45. [PubMed] [Google Scholar]\n\nDatnow A, Castellano M. Teachers’ responses to Success for All: how beliefs, experiences, and adaptations shape implementation. American Educational Research Journal. 2000;37:775–99. [Google Scholar]\n\nDatnow A, Hubbard L, Mehan H. Extending Educational Reform: From One School to Many. London: Routledge/Falmer; 2002. [Google Scholar]\n\nDerzon JH, Sale E, Springer JF, Brounstein P. Estimating intervention effectiveness: synthetic projection of field evaluation results. Journal of Primary Prevention. 2005;26:321–43. [PubMed] [Google Scholar]\n\nDevaney E, O’Brien MU, Resnik H, Keister S, Weissberg RP. Sustainable School-Wide Social and Emotional Learning: Implementation guide and toolkit. Chicago, IL: Collaborative for Academic, Social, and Emotional Learning; 2006. [Google Scholar]\n\nDomitrovich CE, Greenberg MT. The study of implementation: current findings from effective programs for school aged children. Journal of Educational and Psychological Consultation. 2000;11:193–222. [Google Scholar]\n\nDunn RJ, Harris LG. Organizational dimensions of climate and the impact on school achievement. Journal of Instructional Psychology. 1998;25:100–14. [Google Scholar]\n\nDurlak JA. Why program implementation is important. Journal of Prevention and Intervention in the Community. 1998;17:5–18. [Google Scholar]\n\nDurlak JA, DuPre EP. Implementation matters: a review of research on the influence of implementation on program outcomes and the factors affecting implementation. American Journal of Community Psychology. 2008;41:327–50. [PubMed] [Google Scholar]\n\nDurlak JA, Weissberg RP. A major meta-analysis of positive youth development programs. Invited presentation at the Annual Meeting of the American Psychological Association; Washington, DC. 2005. Aug, [Google Scholar]\n\nDurlak JA, Weissberg RP, Dymnicki AB, Taylor RD, Schellinger K. The Effects of Social and Emotional Learning on the Behavior and Academic Performance of School Children. Chicago, IL: Collaborative for Academic, Social, and Emotional Learning; 2008. [Google Scholar]\n\nDusenbury L, Branningan R, Falco M, Hansen WB. A review of research on fidelity of implementation: implications for drug abuse prevention in school settings. Health Education Research. 2003;18:237–56. [PubMed] [Google Scholar]\n\nDusenbury L, Brannigan R, Hansen WB, Walsh J, Falco M. Quality of implementation: developing measures crucial to understanding the diffusion of preventive interventions. Health Education Research. 2005;20:308–13. [PubMed] [Google Scholar]\n\nDusenbury L, Hansen WB. Pursuing the course from research to practice. Prevention Science. 2004;5:55–9. [PubMed] [Google Scholar]\n\nDusenbury L, Hansen W, Jackson-Newson J, et al. Coaching to implementation fidelity. Paper presented at the 15th annual meeting of the Society for Prevention Research; May; Washington, DC. 2007. [Google Scholar]\n\nElias MJ, Zins JE, Graczyk PA, Weissberg RP. Implementation, sustainability, and scaling up of social-emotional and academic innovations in public schools. School Psychology Review. 2003;32:303–19. [Google Scholar]\n\nEllickson PL, Bell RM. Drug prevention in junior high: a multi-site longitudinal test. Science. 1990;247:1299–305. [PubMed] [Google Scholar]\n\nEllickson PL, Bell RM, McGuigan K. Preventing adolescent drug use: long-term results of a junior high program. American Journal of Public Health. 1993;83:856–61. [PMC free article] [PubMed] [Google Scholar]\n\nElliott D. Blueprints for Violence Prevention. Golden, CO: Venture; 1998. [Google Scholar]\n\nElliott DS, Mihalic S. Issues in disseminating and replicating effective prevention programs. Prevention Science. 2004;5:47–53. [PubMed] [Google Scholar]\n\nEnnett ST, Ringwalt CL, Thorne J, et al. A comparison of current practice in school-based substance use prevention programs with meta-analysis findings. Prevention Science. 2003;4:1–14. [PubMed] [Google Scholar]\n\nEsposito C. Learning in urban blight: school climate and its effect on the school performance of urban, minority, low-income children. School Psychology Review. 1999;28:365–77. [Google Scholar]\n\nEvers WJ, Brouwers A, Tomic W. Burnout and self-efficacy: a study on teachers’ beliefs when implementing an innovative educational system in the Netherlands. British Journal of Educational Psychology. 2002;72:227–44. [PubMed] [Google Scholar]\n\nFagan AA, Mihalic S. Strategies for enhancing the adoption of school-based prevention programs: lessons learned from the Blueprints for Violence Prevention replications of The Life Skills Training Program. Journal of Community Psychology. 2003;31:235–53. [Google Scholar]\n\nFarmer EMZ, Burns BJ, Phillips SD, Angold A, Costello EJ. Pathways into and through mental health services for children and adolescents. Psychiatric Services. 2003;54:60–6. [PubMed] [Google Scholar]\n\nFeinberg ME, Gomez BJ, Puddy RW, Greenberg MT. Evaluation and community prevention coalitions: validation of an integrated web-based/technical assistance consultant model. Health Education and Behavior. 2008;35:9–21. [PubMed] [Google Scholar]\n\nFishman BJ, Marx RW, Best S, Tal RT. Linking teacher and student learning to improve professional development in systemic reform. Teaching and Teacher Education. 2003;19:643–58. [Google Scholar]\n\nFishman BJ, Best S, Foster J, Marx R. Fostering teacher learning in systemic reform: a design proposal for developing professional development. Paper presented at the annual meeting of the National Association of Research in Science Teaching; New Orleans, LA. April 28–May 1.2000. [Google Scholar]\n\nFixsen DL, Naoom SF, Blasé KA, Friedman RM, Wallace F. Implementation Research: A Synthesis of the Literature. Tampa: University of South Florida, Louis de la Parte Florida Mental Health Institute, The National Implementation Research Network; 2005. (FMHI Publication #231) [Google Scholar]\n\nFlay BR, Biglan A, Boruch RF, et al. Standards of evidence: criteria for efficacy, effectiveness and dissemination. Prevention Science. 2005;6:151–75. [PubMed] [Google Scholar]\n\nGaret M, Porter A, Desimone L, Birman B, Yoon KS. What makes professional development effective? Results from a national sample of teachers. American Education Research Journal. 2001;38:915–45. [Google Scholar]\n\nGingiss PL. Enhancing program implementation and maintenance through a multiphase approach to peer-based staff development. Journal of School Health. 1992;62:161–6. [PubMed] [Google Scholar]\n\nGingiss PM, Roberts-Gray C, Boerm M. Bridge-It: a system for predicting implementation fidelity for school-based tobacco prevention programs. Prevention Science. 2006;7:197–207. [PubMed] [Google Scholar]\n\nGlasgow RE, Lichtenstein E, Marcus AC. Why don’t we see more translation of health promotion research to practice? Rethinking the efficacy-to-effectiveness transition. American Journal of Public Health. 2003;93:1261–7. [PMC free article] [PubMed] [Google Scholar]\n\nGlisson C, Hemmelgarn A. The effects of organizational climate and interorganizational coordination on the quality of outcomes of children’s service systems. Child Abuse & Neglect. 1998;22:401–21. [PubMed] [Google Scholar]\n\nGoddard RD, Goddard YL. A multilevel analysis of the relationship between teacher and collective efficacy in urban schools. Teaching and Teacher Education. 2001;17:807–18. [Google Scholar]\n\nGorman-Smith D, Beidel D, Brown TA, Lochman J, Haaga AF. Effects of teacher training and consultation on teacher behavior towards students at high risk for aggression. Behavior Therapy. 2003;34:437–52. [Google Scholar]\n\nGottfredson DC, Gottfredson GD. Quality of school-based prevention programs: results from a national survey. Journal of Research on Crime and Delinquency. 2002;39:3–35. [Google Scholar]\n\nGottfredson DC, Wilson DB. Characteristics of effective school-based substance abuse prevention. Prevention Science. 2003;4:27–38. [PubMed] [Google Scholar]\n\nGottfredson G, Nese J, Nebbergall A, Shaw F. Alternative measures of implementation in an experimental study of elementary school social skills instruction. Paper presented at the 16th annual meeting of the Society for Prevention Research; San Francisco, CA. May.2008. [Google Scholar]\n\nGottfredson GD, Gottfredson DC, Payne AA, Gottfredson NC. School climate predictors of school disorder: results from a national study of delinquency prevention in schools. Journal of Research in Crime and Delinquency. 2005;42:412–44. [Google Scholar]\n\nGottfredson GD, Jones EM, Gore TW. Implementation and evaluation of a cognitive-behavioral intervention to prevent problem behavior in a disorganized school. Prevention Science. 2002;3:27–38. [PubMed] [Google Scholar]\n\nGreenberg MT. Current and future challenges in school-based prevention: the researcher perspective. Prevention Science. 2004;5:5–13. [PubMed] [Google Scholar]\n\nGreenberg MT. Community and team member factors that influence the early phase functioning of community prevention teams: the prosper project. Journal of Primary Prevention. 2007;28:485–504. [PMC free article] [PubMed] [Google Scholar]\n\nGreenberg M, Domitrovich CE, Graczyk P, Zins J. Report to Center for Mental Health Services (CMHS), Substance Abuse Mental Health Services Administration, U.S. Department of Health and Human Services. 2001. The Study of Implementation in School-Based Preventive Interventions: Theory, research, & practice. [Google Scholar]\n\nGreenberg MT, Domitrovich C, Bumbarger B. The prevention of mental disorders in school-aged children: current state of the field. Prevention & Treatment. 2001;4:Article 1. [Google Scholar]\n\nGreenhalgh T, Robert G, Macfarlane F, et al. Diffusion of Innovations in Health Service Organizations: A systematic literature review. Oxford: Blackwell; 2005. [Google Scholar]\n\nGriner D, Smith TB. Culturally adapted mental health intervention: a meta-analytic review. Psychotherapy: Theory, Research, Practice, Training. 2006;43:531–48. [PubMed] [Google Scholar]\n\nHahn EJ, Noland MP, Rayens MK, Christie DM. Efficacy of training and fidelity of implementation of the life skills training program. Journal of School Health. 2002;72:282–7. [PubMed] [Google Scholar]\n\nHahn R, Fuqua-Whitley D, Wethington H, Lowy J, et al. Effectiveness of universal school-based programs to prevent violent and aggressive behavior: a systematic review. American Journal of Preventive Medicine. 2007;33(Suppl 2):S114–29. [PubMed] [Google Scholar]\n\nHall GE, Hord SM. Change in School: Facilitating the process. Albany, NY: State University of New York Press; 1987. [Google Scholar]\n\nHallinger P, Heck RH. Reassessing the principal’s role in school effectiveness: a review of empirical research, 1980–1995. Educational Administration Quarterly. 1996;32:5–44. [Google Scholar]\n\nHalpin A, Croft D. The Organizational Climate of Schools. Chicago: Midwest Administrative Center, University of Chicago; 1963. [Google Scholar]\n\nHan SS, Weiss B. Sustainability of teacher implementation of school-based mental health programs. Journal of Abnormal Child Psychology. 2005;33:665–79. [PubMed] [Google Scholar]\n\nHansen WB. Pilot test results comparing the All Star program with seventh grade D.A.R.E.: program integrity and mediating variable analysis. Substance Use & Misuse. 1996;31:1359–77. [PubMed] [Google Scholar]\n\nHansen WB, McNeal RB. Drug education practice: results of an observational study. Health Education Research. 1999;14:85–97. [PubMed] [Google Scholar]\n\nHaskins R, Loeb S. A Plan to Improve the Quality of Teaching in American Schools. The Future of Children (Policy Brief) Princeton, NJ: Woodrow Wilson School of Public and International Affairs; 2007. [Google Scholar]\n\nHoagwood KE, Johnson J. School psychology: a public health framework. I. From evidence-based practices to evidence based policies. Journal of School Psychology. 2003;41:3–21. [Google Scholar]\n\nHawkins JD, Catalano RF. Communities that care: Action for drug abuse. San Francisco, CA: Jossey-Bass Inc; 1992. [Google Scholar]\n\nHoy WK, Hannum J, Tschannen-Moran M. Organizational climate and student achievement: a parsimonious and longitudinal view. Journal of School Leadership. 1998;8:336–59. [Google Scholar]\n\nIllinois State Board of Education: Division of Early Childhood Education. The Illinois Learning Standards. 2004. [Google Scholar]\n\nJennings PA, Greenberg MT. The prosocial classroom: teacher social and emotional competence in relation to child and classroom outcomes. 2008 Manuscript submitted for publication. [Google Scholar]\n\nJoyce B, Showers B. Student Achievement through Staff Development. 3. Alexandria, VA: Association for Supervision and Curriculum Development; 2002. [Google Scholar]\n\nKaftarian S, Robinson E, Compton W, Davis BW, Volkow N. Blending prevention research and practice in schools: critical issues and suggestions. Prevention Science. 2004;5:1–3. [PubMed] [Google Scholar]\n\nKallestad JH, Olweus D. Predicting teachers’ and schools’ implementation of the Olweus bullying prevention program: a multilevel study. Prevention & Treatment. 2003;6 [Google Scholar]\n\nKam C, Greenberg MT, Walls CT. Examining the role of implementation quality in school-based prevention using the PATHS curriculum. Prevention Science. 2003;4:55–63. [PubMed] [Google Scholar]\n\nKataoke S, Rowan B, Hoagwood K. Mental health and education policy: issues and future directions. Paper presented at the Fundamental Policy – Spotlight on Mental Health Conference; Washington, DC. 2008. [Google Scholar]\n\nKatulak NA, Brackett MA, Weissberg RP. School-based social and emotional learning (SEL) programming: current perspectives. In: Lieberman A, Fullan M, Hargreaves A, Hopkins D, editors. International Handbook of Educational Change. 2. New York: Springer; 2008. [Google Scholar]\n\nKealey KA, Peterson AV, Gaul MA, Dinh KT. Teacher training as a behavior change process: principals and results from a longitudinal study. Health Education & Behavior. 2000;27:64–81. [PubMed] [Google Scholar]\n\nKellam SG, Brown CH, Poduska J, et al. Effects of a universal classroom behavior management program in first and second grades on young adult behavioral, psychiatric, and social outcomes. Drug and Alcohol Dependence. 2008;95:S5–S28. [PMC free article] [PubMed] [Google Scholar]\n\nKellam SG, Ling X, Merisca R, Brown CH, Ialongo N. The effect of the level of aggression in the first grade classroom on the course and malleability of aggressive behavior into middle school. Development and Psychopathology. 1998;10:165–85. [PubMed] [Google Scholar]\n\nKiernan M, Regier DA. Mental health service use in the community and schools: results from the four-community MECA study. Journal of the American Academy of Child & Adolescent Psychiatry. 1996;35:889–97. [PubMed] [Google Scholar]\n\nKlein KJ, Kozlowski SWJ. From micro to meso: critical steps in conceptualizing and conducting multilevel research. Organizational Research Methods. 2000;3:211–236. [Google Scholar]\n\nKlein KJ, Sorra JS. The challenge of innovation implementation. Academy of Management Review. 1996;21:1055–80. [Google Scholar]\n\nKoth CW, Bradshaw CP, Leaf PJ. Examining the relationship between classroom-level factors and students’ perception of school climate. Journal of Educational Psychology. 2008;100:96–104. [Google Scholar]\n\nKumper KL, Alvarado R, Smith P, Bellamy N. Cultural sensitivity and adaptation in family-based prevention interventions. Prevention Science. 2002;3:241–6. [PubMed] [Google Scholar]\n\nLeach DJ, Conto H. The additional effects of process and outcome feedback following brief in-service training. Educational Psychology. 1999;19:441–62. [Google Scholar]\n\nLeaf PJ, Alegria M, Cohen P, et al. Mental health service use in the community and schools: results from the four-community MECA study. Journal of the American Academy of Child & Adolescent Psychiatry. 1996;35:889–97. [PubMed] [Google Scholar]\n\nLillejoj CJ, Griffin KW, Spoth R. Program provider and observer ratings of school-based preventive intervention implementation: agreement and relation to youth outcomes. Health Education & Behavior. 2004;31:242–57. [PubMed] [Google Scholar]\n\nLochman J, Powell N, Boxmeyer C, et al. The effect of school and counselor characteristics on implementation of a preventive intervention. Paper presented at the 16th annual meeting of the Society for Prevention Research; May; San Francisco, CA. 2008. [Google Scholar]\n\nLytle LA, Ward J, Nader PR, Pederson S, Williston BJ. Maintenance of a health promotion program in elementary schools: results from the CATCH-ON study key informants interviews. Health Education and Behavior. 2003;30:503–18. [PubMed] [Google Scholar]\n\nMcCormick KM, Brennan S. Mentoring the new professional in interdisciplinary early childhood education. Topics in Early Childhood Special Education. 2001;21(3):131–49. [Google Scholar]\n\nMcNeal RB, Hansen WB, Harrington NG, Giles SM. How All Stars works: an examination of program effects on mediating variables. Health Education & Behavior. 2004;31:165–78. [PubMed] [Google Scholar]\n\nMoncher FJ, Prinz RJ. Treatment fidelity in outcome studies. Clinical Psychology Review. 1991;11:247–66. [Google Scholar]\n\nMrazek PG, Haggerty RJ, editors. Reducing Risks for Mental Disorders: Frontiers for Preventive Intervention Research. Washington, DC: National Academy Press; 1994. [PubMed] [Google Scholar]\n\nMurray DM. Design and Analysis of Group-Randomized Trials. New York: Oxford Press; 1998. [Google Scholar]\n\nTowne L, Hilton M, editors. National Research Council. Implementing Randomized Field Trials in Education: Report of a Workshop. Center for Education, Division of Behavioral and Social Sciences and Education; Washington, DC: The National Academies Press; 2004. [Google Scholar]\n\nNoell GH, Witt JC, Slider NJ, et al. Treatment implementation following behavioral consultation in schools: a comparison of three follow-up strategies. School Psychology Review. 2005;34:87–106. [Google Scholar]\n\nOwens RG. Organizational Behavior in Education: Adaptive Leadership and School Reform. 8. Boston: Pearson Allyn and Bacon; 2004. [Google Scholar]\n\nPankratz M, Hallfors D, Cho H. Measuring perceptions of innovation adoption: the diffusion of a federal drug prevention policy. Health Education Research. 2002;17:315–26. [PubMed] [Google Scholar]\n\nParcel GS, Perry CL, Kelder SH, et al. School climate and the institutionalization of the CATCH program. Health Education & Behavior. 2003;30:489–502. [PubMed] [Google Scholar]\n\nParcel GS, Ross JG, Lavin AT, et al. Enhancing implementation of the teenage health teaching modules. Journal of School Health. 1991;61:35–38. [PubMed] [Google Scholar]\n\nPayne AA, Gottfredson DC, Gotfredson GD. School predictors of the intensity of implementa"
    }
}
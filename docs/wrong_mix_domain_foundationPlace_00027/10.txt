Gang Li

Google Research

Mountain View, CA

leebird@google.com

&Yang Li

Google Research

Mountain View, CA

liyang@google.com

Abstract

Mobile UI understanding is important for enabling various interaction tasks such as UI automation and accessibility. Previous mobile UI modeling often depends on the view hierarchy information of a screen, which directly provides the structural data of the UI, with the hope to bypass challenging tasks of visual modeling from screen pixels. However, view hierarchies are not always available, and are often corrupted with missing object descriptions or misaligned structure information. As a result, despite the use of view hierarchies could offer short-term gains, it may ultimately hinder the applicability and performance of the model. In this paper, we propose Spotlight, a vision-only approach for mobile UI understanding. Specifically, we enhance a vision-language model that only takes the screenshot of the UI and a region of interest on the screenâ€”the focusâ€”as the input. This general architecture of Spotlight is easily scalable and capable of performing a range of UI modeling tasks. Our experiments show that our model establishes SoTA results on several representative UI tasks and outperforms previous methods that use both screenshots and view hierarchies as inputs. Furthermore, we explore multi-task learning and few-shot prompting capacities of the proposed models, demonstrating promising results in the multi-task learning direction.

1 Introduction

Computational understanding of mobile user interfaces (UI) is a crucial step for achieving intelligent UI behaviors such as UI automation, and addressing diverse interaction scenarios such as those requiring accessibility features. Recently, mobile UI understanding has attracted numerous research interests. Previous works have proposed various UI modeling tasks and datasets, including widget captioning (Li et al., 2020b), screen summarization (Wang et al., 2021), command grounding (Li et al., 2020a; Bai et al., 2021; Burns et al., 2022) and other tasks (Li et al., 2022; He et al., 2020) on the mobile screen. Many of these works focus on bridging natural language and graphical user interfaces, which have shown potential for enabling language-based interaction.

A mobile UI screen can come with a view hierarchyâ€”a structural representation of the screenâ€”in addition to the screenshot image. Using view hierarchy as input allows a model to directly acquire detailed information of UI objects such as their types, text content and positions on the screen, bypassing challenging visual modeling tasks such as inferring object information from screenshots (Li et al., 2021; Zhang et al., 2021). Previous works have shown the benefit of using view hierarchy in UI modeling in several tasks. For example, models using view hierarchy have achieved better performance than their vision-only counterparts in UI captioning tasks (Li et al., 2020b; Wang et al., 2021).

However, recent work has revealed that mobile UI view hierarchies often contain inaccurate information about the UI screen, e.g., missing object text and misaligned structure information. Li et al. (2022) showed that about 37.4% of the screen view hierarchies contain objects with invalid bounding boxes. Ross et al. (2018) showed that 92.0% of Floating Action Buttons had missing text labels, compared to 54.7% of Image Buttons and 86.3% of Clickable Images. These object text labels (e.g., content_desc) are among the most important features in view hierarchies. Removing text features resulted in a drop by 17 CiDER points for the widget captioning task (Li et al., 2020b). Therefore, such inaccurate information in the input can seriously hinder models in realizing their full potential in UI modeling. Although recent work has proposed methods for repairing view hierarchies (Li et al., 2022), substantial effort is still needed to robustly denoise raw view hierarchies. On top of these, UI screen data does not always have view hierarchies available in them, such as mobile UI images crawled from the web. Fetching view hierarchy at runtime in a mobile environment also imposes additional system constraints for the applicability of models that rely on view hierarchies.

In this paper, we investigate the direction of using only visual UI screenshots as input (i.e., without including view hierarchies) for UI modeling tasks. We observe that many UI modeling tasks essentially aim to learn a mapping between the UI objects and text. As a result, vision-language models, a class of models that encode visual (and language) modalities and decode text answers, become a natural choice for the model architecture. Although previous works show that vision-only models generally perform worse than the models using both visual and view hierarchy input (Li et al., 2020b; Wang et al., 2021), we believe that visual language models offer two unique opportunities: 1) the simple architecture enables a model easily scalable, and 2) many heterogeneous tasks can be universally represented by the two core modalities of vision and language. These advantages have been evidenced by the recent successes of the vision-language models (Chen et al., 2022; Alayrac et al., 2022; Yu et al., 2022; Wang et al., 2022).

In contrast to previous visual-language tasks in the general domain, which usually use an entire image as input, UI modeling tasks are often concerned with a specific object or area on the screen. This requires a vision-language model to be able to focus on the object or area of interest. Thus, we propose Spotlight, which enhances a vision-language model to generate text responses with respect to a focus object or region to support various UI modeling tasks (see Figure 1). In our experiments, we initialize Spotlight by leveraging pretrained large ViT (Dosovitskiy et al., 2021) and T5 (Raffel et al., 2019) checkpoints. We then pretrain Spotlight with unlabeled datasets consisting of about 2.52.52.5 million mobile UI screens and 808080 million web pages, which is followed by one of the three modeling strategies: single-task finetuning, multi-task finetuning or few-shot learning.

Our main contribution is three-fold. First, we propose a novel vision-language model architecture that is capable of finetuning, multi-task learning and few-shot learning for mobile UI tasks. The model can easily scale and generalize to other tasks without architectural changes. This model advances the art of UI understanding without needing to use view hierarchies as inputs that has many drawbacks in practice. Secondly, we develop a method for creating large-scale pretraining datasets from automatically collected mobile screens and web pages. These pretraining datasets and methods are crucial for our vision-language model to learn the prior knowledge of the unique domain of mobile screens and UIs. Finally, we conduct extensive experiments over the proposed model, including using various focus region representations and modeling strategies. Our experiments show that the proposed models obtain new SoTA performance in both single-task and multi-task finetuning for the four tasks, including widget captioning, screen summarization, command grounding and tappability prediction. We also examine the feasibility of using the proposed model for few-shot prompting.

2 Related Work

UI modeling problems have drawn widespread interest from researchers in both the ML and HCI fields (Li et al., 2020b; Wang et al., 2021; Li et al., 2020a; Burns et al., 2022; Bai et al., 2021; Zhang et al., 2021; Wu et al., 2021; He et al., 2020). With the overarching goal of enabling intelligent UIs and addressing mobile accessibility, previous works have proposed a rich set of mobile UI modeling tasks, along with datasets and benchmarks. Widget captioning (Li et al., 2020b) aims to generate natural language description for UI objects on the screen. The capability can enable accessibility features such as the TalkBack screen reader to improve user experience for vision-impaired users. Screen2Words expands UI captioning by proposing the task for summarizing the entire screen (Wang et al., 2021). Command grounding maps a natural language command to a UI object on the screen, via single or multi-step interactions (Li et al., 2020a; Burns et al., 2022; Bai et al., 2021). Tappability prediction predicts whether a UI object is tappable when perceived by human (Swearngin & Li, 2019; Schoop et al., 2022), which is useful for UI design validation. Most of these previous works used both the screenshot and the view hierarchy of a screen as input in their models, and revealed that multimodal input achieves better performance than vision-only variants (Li et al., 2020b; Wang et al., 2021). Prior works have also borrowed language modeling techniques such as BERT (Devlin et al., 2019) for representation learning to facilitate downstream tasks (Bai et al., 2021; He et al., 2020). Recently, Li et al. (2021) investigated the feasibility of multi-task modeling for achieving a range of UI tasks simultaneously. To combat the lack of accurate view hierarchy, prior works have investigated model-based approaches for repairing noisy view hierarchies (Li et al., 2022), or UI object detection over screenshots to derive information such as object attributes and structure information (Zhang et al., 2021; Wu et al., 2021). In this work, we investigate vision-input-only approaches for several representative UI modeling tasks that have been established by prior works, including widget captioning, screen summarization, command grounding and tappability prediction.

Many of these mobile UI tasks are concerned with specific objects or areas on the UI screen. Previous works have employed different methods to allow a model to focus on an object or a region of interest on the screen. Li et al. (2020b); Wang et al. (2021); Bai et al. (2021) used the screenshot pixels cropped for individual objects as input to the model. Li et al. (2021) used a focus map to bias the ResNet outputs to pay more attention to the pixels corresponding to an object. Schoop et al. (2022) added a fourth channel to an RGB screenshot image that indicates the target region with a binary mask where the 111s correspond to the area of the object. The cropping and bias-based approach might result in losing the context of the object, despite providing direct localization of the object. While adding a fourth channel also keeps the information of the entire image, it is incompatible with existing vision models that are based on 3-channel RGB images, which makes it difficult to leverage their pretrained checkpoints. In this work, we investigate a variety of methods for region representations.

Recently, rapid progress has been seen for the large vision and language models, such as GPT-3 (Brown et al., 2020), T5 (Raffel et al., 2019), PaLM (Chowdhery et al., 2022), CLIP (Radford et al., 2021), DALL-E (Ramesh et al., 2022) and Imagen (Saharia et al., 2022). These models exploited large unlabeled data crawled from the web and demonstrate impressive finetuning and few-shot learning capacities. Vision-language models that encode images and decode text have leveraged large pretrained vision and language components and observed significant performance gain, e.g., Flamingo (Alayrac et al., 2022) and PaLI (Chen et al., 2022). Specifically, Flamingo showed that the few-shot prompting ability of the large language model can generalize to multimodal cases using multiple pairs of image and text as input. We adapt this model architecture to explore task-specific finetuning and multi-task learning as well as few-shot capability for mobile UI tasks.

3 Data

3.1 Pretraining Datasets

Two kinds of UI data are used to pretrain Spotlight models. First, we use the publicly available C4 corpus (Raffel et al., 2019) which contains a large amount of web pages that can be rendered into screenshots, similar to mobile UI screenshots. We use 80 million web page screenshots for pretraining. Secondly, we use a large-scale UI corpus that consists of 2.69 million mobile screenshots with paired view hierarchies. The dataset was collected by crawling various mobile apps. Screenshots and their view hierarchies are captured after performing random clicks in the Android emulator. Although being much smaller than the C4 dataset, the mobile data is critical as they are in-domain unlabeled data of our downstream tasks. As mentioned in Section 1, many UI modeling tasks aim to learn a mapping between a UI object and a language description. Therefore, we propose to pretrain Spotlight models to auto-regressively decode text-based attributes of individual objects on the web or mobile UI, with only the screenshot image and the object bounding box as the input.

Web Page Data: We select a list of HTML attributes as the text descriptions of an object based on a manual examination of their qualities. The selected attributes contain text either corresponding to the rendered text on the screen (e.g., a button label), or describing the functionality of the HTML object (e.g., alt-text). Text consisting only of generic words (e.g., â€imageâ€, â€textâ€) are removed. We also use the title of a web page as the description of the full screenshot. The statistics of the most common attributes with more than 10â€‹M10ğ‘€10M instances are summarized in Table 1. The comprehensive statistics and list of generic words are reported in Appendix D.

Mobile Data: Similar to prior work (Li et al., 2020b), we only use leaf objects as they are more likely to represent the functionalities of the UI. For each object, we use three text-based attributes from a view hierarchy node: text, content_description, and resource_id. These attributes are showed to be important features for models using view hierarchy as inputs (Li et al., 2020b; Wang et al., 2021). Note that we derive the output labels from view hierarchies but do not use the view hierarchies themselves as input.

To compensate for missing text in view hierarchies, we further run OCR on the screenshot to extract additional text that might be present on the screen. Recognized single-line text with 80% or higher confidence are used. To improve the data quality, we preprocess the dataset to remove URLs, Unicode, single-character and non-alphabetical strings. We also remove text that rarely occurs, i.e., fewer than 5 times in the entire dataset, or consists of generic phrases (e.g., â€text viewâ€) that do not convey useful information about the object. Table 2 summarizes the statistics of the mobile dataset.

3.2 UI Task Datasets

We select four representative UI modeling tasks that previous work proposed as downstream tasks (see Figure 1). We list these tasks below.

â€¢

Widget Captioning (Li et al., 2020b): Generating a natural language description for the functionality of a given object on the screen.

â€¢

Screen Summarization (Wang et al., 2021): Generating a high-level summary of a UI screen to describe its contents and functionalities.

â€¢

Command Grounding (Li et al., 2021): Finding the UI object on the screen that matches the intent of a natural language command.

â€¢

Tappability Prediction (Schoop et al., 2022): Predicting whether a given object on the screen is tappable or not.

These four tasks require models to learn the mapping between a UI object and a natural language description in either oâ€‹bâ€‹jâ€‹eâ€‹câ€‹tâ†’lâ€‹aâ€‹nâ€‹gâ€‹uâ€‹aâ€‹gâ€‹eâ†’ğ‘œğ‘ğ‘—ğ‘’ğ‘ğ‘¡ğ‘™ğ‘ğ‘›ğ‘”ğ‘¢ğ‘ğ‘”ğ‘’object\rightarrow language or lâ€‹aâ€‹nâ€‹gâ€‹uâ€‹aâ€‹gâ€‹eâ†’oâ€‹bâ€‹jâ€‹eâ€‹câ€‹tâ†’ğ‘™ğ‘ğ‘›ğ‘”ğ‘¢ğ‘ğ‘”ğ‘’ğ‘œğ‘ğ‘—ğ‘’ğ‘ğ‘¡language\rightarrow object direction. Furthermore, it is essential for the model to be able to both zoom in to focus on a specific region for tasks such as widget captioning, and zoom out to grasp the full screen for tasks such as screen summarization. We discuss these tasks in detail in Appendix E. In our experiments, we use the same dataset splits for comparison with these benchmarks.

4 Models

We design Spotlight based on the general encoder-decoder architecture that has been widely used for vision-language modeling (Chen et al., 2022; Alayrac et al., 2022; Wang et al., 2022). We employ ViT (Dosovitskiy et al., 2021) for encoding images (screenshots) and a Transformer decoder for generating language. In contrast to previous vision-language tasks, UI modeling tasks are often concerned with a specific region or object on the screen, instead of the entire image. This unique aspect of UI modeling requires a model to be able to concentrate on a specific region on the image. We investigate different methods for realizing the Focus Region Extractor (see Figure 1) for computing region representation.

Region-of-Interest (ROI) Align: ROI Align is a parameter-free approach previously introduced for extracting a feature representation of a specific region on an image (He et al., 2017). This method performs average or max pooling over the feature map of an image region, and handles low spatial resolutions in the latent feature space using bilinear interpolation. In our case, we perform ROI Align for a target region over the image encodings from ViT outputs, which results in a fixed-length vector representation for the focus region. This method provides a direct localization of the focus region in the encoded visual signals, and it embodies soft cropping as the resulted representation carries contextual information beyond the target area due to ViT self-attention. This is in contrast to hard cropping that carves out the raw pixels of the target area (Li et al., 2020b), which loses the access to the screen context.

Region Summarizer: Previous works have used learnable vectors as input in Transformer models to query visual modalities for specific tasks. For example, DeTR (Carion et al., 2020) used learnable queries for retrieving visual objects on an image. Flamingo (Alayrac et al., 2022) summarizes image encodings by using learnable query vectors to resample the image encodings. Inspired by these methods, we propose Region Summarizer, which acquires latent representation of a region based on ViT encodings, by using attention queries generated from the bounding box of the region or object. The bounding box of a region, Bâˆˆâ„4Ã—1ğµsuperscriptâ„41B\in{\mathbb{R}^{4\times 1}}, includes the four scalar coordinate values, i.e., [left, top, right, bottom]. For each coordinate value, we generate nğ‘›n vectors (Equation 1), and having multiple descriptor vectors allows us to control the capacity of the region representation.

E=Reshape4Ã—nâ€‹deâ†’4Ã—nÃ—deâ€‹([GeLUâ€‹(Bâ€‹We)]â€‹Wx)X=Reshape4Ã—nÃ—dâ†’4â€‹nÃ—dâ€‹(E+C)ğ¸subscriptReshapeâ†’4ğ‘›subscriptğ‘‘ğ‘’4ğ‘›subscriptğ‘‘ğ‘’delimited-[]GeLUğµsubscriptğ‘Šğ‘’subscriptğ‘Šğ‘¥ğ‘‹subscriptReshapeâ†’4ğ‘›ğ‘‘4ğ‘›ğ‘‘ğ¸ğ¶\displaystyle\begin{split}E&=\mbox{Reshape}_{4\times nd_{e}\rightarrow 4\times n\times d_{e}}([\mbox{GeLU}(BW_{e})]W_{x})\\ X&=\mbox{Reshape}_{4\times n\times d\rightarrow 4n\times d}(E+C)\end{split} (1)

In Equation 1, we first project each coordinateâ€”a scalar valueâ€”of the bounding box to a nâ€‹değ‘›subscriptğ‘‘ğ‘’nd_{e}-dimensional dense vector using a perceptron: GeLUâ€‹(Bâ€‹We)GeLUğµsubscriptğ‘Šğ‘’\mbox{GeLU}(BW_{e}), where Weâˆˆâ„1Ã—nâ€‹desubscriptğ‘Šğ‘’superscriptâ„1ğ‘›subscriptğ‘‘ğ‘’W_{e}\in{\mathbb{R}^{1\times nd_{e}}}. We next reshape the perceptron activation from 4Ã—nâ€‹de4ğ‘›subscriptğ‘‘ğ‘’4\times nd_{e} to 4Ã—nÃ—de4ğ‘›subscriptğ‘‘ğ‘’4\times n\times d_{e} to spawn nğ‘›n desubscriptğ‘‘ğ‘’d_{e}-dimensional vectors for each of the four coordinates. Then, Wxâˆˆâ„deÃ—dsubscriptğ‘Šğ‘¥superscriptâ„subscriptğ‘‘ğ‘’ğ‘‘W_{x}\in\mathbb{R}^{d_{e}\times d} projects each vector to the hidden dimension, dğ‘‘d, of the Focus Extractor transformer model. Thus Eâˆˆâ„4Ã—nÃ—dğ¸superscriptâ„4ğ‘›ğ‘‘E\in\mathbb{R}^{4\times n\times d} and each of the 444 coordinates produces nğ‘›n dğ‘‘d-dimensional vectors. To indicate from which coordinate value a vector comes, we add the coordinate type embedding, Câˆˆâ„4Ã—1Ã—dğ¶superscriptâ„41ğ‘‘C\in{\mathbb{R}^{4\times 1\times d}}. Each vector in Cğ¶C represents the embedding of a coordinate type in {left, top, right, bottom}. In sum, Wesubscriptğ‘Šğ‘’W_{e}, Wxsubscriptğ‘Šğ‘¥W_{x} and Cğ¶C are learnable parameters. nğ‘›n is a hyperparameter that controls the number of bounding box vectors to use and the number of region summaries to generate, which determines the capacity of bottleneck.

Based on the bounding box descriptor vectors, Xğ‘‹X, we then use a Transformer to extract region representations from ViT encodings, Hâˆˆâ„mÃ—dğ»superscriptâ„ğ‘šğ‘‘H\in{\mathbb{R}^{m\times d}}, in a similar way to Flamingo (Alayrac et al., 2022). The query of cross attention q=Qiğ‘subscriptğ‘„ğ‘–q=Q_{i} and the memory kâ€‹v=Hâˆ¥Qiâˆˆâ„(m+4â€‹n)Ã—dğ‘˜ğ‘£âˆ¥ğ»subscriptğ‘„ğ‘–superscriptâ„ğ‘š4ğ‘›ğ‘‘kv=H\mathbin{\|}Q_{i}\in\mathbb{R}^{(m+4n)\times d}, which is the concatenation of Hğ»H and Qisubscriptğ‘„ğ‘–Q_{i} on the first dimension. The iğ‘–ith layer of the Region Summarizer Transformer is formulated in Equation 2, where Q0=Xsubscriptğ‘„0ğ‘‹Q_{0}=X and 0â‰¤iâ‰¤l0ğ‘–ğ‘™0\leq i\leq l and lğ‘™l is the number of Transformer layers used.

Yi+1=Qi+CrossAttentionâ€‹(q=Qi,kv=Hâˆ¥Qi)Qi+1=Yi+1+Denseâ€‹(Yi+1)subscriptğ‘Œğ‘–1subscriptğ‘„ğ‘–CrossAttentionformulae-sequenceqsubscriptğ‘„ğ‘–kvâˆ¥ğ»subscriptğ‘„ğ‘–subscriptğ‘„ğ‘–1subscriptğ‘Œğ‘–1Densesubscriptğ‘Œğ‘–1\displaystyle\begin{split}Y_{i+1}&=Q_{i}+\mbox{CrossAttention}(\mbox{q}=Q_{i},\mbox{kv}=H\mathbin{\|}Q_{i})\\ Q_{i+1}&=Y_{i+1}+\mbox{Dense}(Y_{i+1})\end{split} (2)

where Yiâˆˆâ„nÃ—dsubscriptğ‘Œğ‘–superscriptâ„ğ‘›ğ‘‘Y_{i}\in{\mathbb{R}^{n\times d}} and Qiâˆˆâ„nÃ—dsubscriptğ‘„ğ‘–superscriptâ„ğ‘›ğ‘‘Q_{i}\in{\mathbb{R}^{n\times d}}. As shown in our experimental analysis, the Region Summarizer allows Spotlight to focus on a target region indicated by a bounding box while taking into account of the screen context. Our experiments show that Region Summarizer offers superior performance over ROI Align. We conducted a series of ablations for Region Summarizer in Appendix B, and the pseudo code is shown in Appendix C.

We use an auto-regressive Transformer decoder to generate token sequences, by attending to the target region representation, Qlâˆˆâ„nÃ—dsubscriptğ‘„ğ‘™superscriptâ„ğ‘›ğ‘‘Q_{l}\in{\mathbb{R}^{n\times d}}, via cross-attention. We formulate all the tasks as sequence decoding (see Figure 1). During pretraining, our model generates text content associated with a target region. For widget captioning and screen summary, they are naturally sequence decoding tasks. For command grounding and tappability prediction, given a target region, we ask the model to predict either Yes or No tokens following a prompt text, which can be a grounding command or a question for tappability. For command grounding, we let the model inspect each object on the screen by predicting Yes or No following a given command, and the object that yields the largest probability for the Yes token is used as the prediction. During both pretraining and finetuning, the entire Spotlight model is trained end to end by minimizing the cross entropy loss for next-token prediction during sequence decoding. Although there is no direct supervision for Region Summarizer, our analysis shows that the learned attention weights of Region Summarizer not only captures the target region but also relevant areas in the context of the screen (see Figure 2).

5 Experiments

Similar to previous vision-language models (Chen et al., 2022; Alayrac et al., 2022), we use existing checkpoints of ViT and T5 models to initialize corresponding components in our models in all the experiments. We experiment with two different sized ViT models, B/16 and L/16, pretrained on 300M natural images (Dosovitskiy et al., 2021) . ViT L/16 is larger than B/16 and has a similar parameter size as the mT5 base model (Xue et al., 2020), for which we also reuse the pretrained checkpoint . The hyperparameters of the models and model variant details can be found in Appendix A.

5.1 Pretraining

During pretraining, we train the model to decode the text of an object in the screenshot. As this learning objective is generic, we can combine the heterogeneous C4 web dataset and mobile dataset by sampling objects and their text attributes. For C4 dataset, we randomly sample from three text attributes: text and alt-text as well as misc-text that includes all other text attributes, which occur much sparser than text and alt-text. We also sample the title attribute of a webpage with a small weight (0.01) to train the model with the ability to summarize the entire screenshot. For the mobile dataset, we randomly sample from the four text attributes of an object: text, content_description, resource_id and OCR text. We found that the text in these two datasets contains very frequent words or phrases, e.g., submit and input password. To counter the imbalance, we subsample the frequent phrases as in word2vec (Mikolov et al., 2013) with t=1â€‹eâˆ’5ğ‘¡1ğ‘’5t=1e-5. As we want to explore few-shot prompting and multi-task learning in addition to task-specific finetuning, we sample a random number of [1âˆ’5]delimited-[]15[1-5] screen-object-text tuples and pad the input sequence to 5 tuples, similar to Flamingo (Alayrac et al., 2022). As C4 dataset is much larger, we sample batches from C4 and mobile dataset with a ratio of 9:1. The ViT encoder encodes each screenshot independently, while the decoder decodes the full sequence of the text of all the pairs, concatenated by Bâ€‹Oâ€‹Cğµğ‘‚ğ¶BOC (Beginning of the chunk) and Eâ€‹Oâ€‹Cğ¸ğ‘‚ğ¶EOC (End of the chunk) tokens. This is to encourage the model to generalize to using sequences of multiple screenshot-object-text tuples in multi-task and few-shot prompting scenarios for UI tasks, similar to previous work for the general domain.

5.2 Finetuning, Multi-task Learning and Few-shot Prompting

Once we pretrain the Spotlight model, we investigate three modeling strategies for downstream tasks, including task-specific finetuning, multi-task learning and few-shot prompting.

5.2.1 Finetuning

We finetune a pretrained Spotlight model for each of the four downstream tasks separately. Each example used for both finetuning training and evaluation contains a single screen-object-text tuple, because the model is tuned to perform the specific task and does not require task prompts or switchers. As shown in Table 3, we can see that our vision-only approaches obtain state-of-the-art performance for all the four tasks, significantly outperforming best previous models that use both screenshots and view hierarchies as input.

5.2.2 Multi-task Learning

Multi-task learning is valuable for reducing model footprint, but developing a robust multi-task model can be more difficult. We train a Spotlight model to learn the four tasks simultaneously, which is more challenging than finetuning for each specific task. We sample training batches from the widget captioning, screen summary, command grounding and tappability tasks with the weights of [3, 2, 15, 1], which are tuned on the development set. Each example in the batch contains two screen-object-text tuples during training so that the model is preconditioned to work with one prompt tuple and one prediction tuple. During evaluation, we applied the same model onto each of the four tasks with a screen-object-text tuple sampled from each training set to prompt the model to perform different tasks. As shown in Table 4, multi-task Spotlight models still outperform previous models, and perform on par with task-specific models (Table 3) on widget captioning and tappability tasks. Although multi-task performance on screen summarization and grounding is lower than finetunning performance, they are still significantly better than the published benchmark results.

5.2.3 Few-shot Learning

Few-shot learning is an important capability demonstrated by large vision and language models (Alayrac et al., 2022; Brown et al., 2020), in which a large model can be adapted for a specific downstream task solely using examples or prompts without further training. For few-shot prompting, we use a pretrained Spotlight model directly for evaluation, without model parameter finetuning. We test Spotlight on widget captioning tasks with a different number of shots: {0,4,8,16,32}0481632\{0,4,8,16,32\} to see how the model performance is impacted by the number of example prompts (Table 5). When there is 00 shot, the pretrained Spotlight model is directly tested on the widget captioning task, without example prompts. Although at most five screen-object-text tuples are used in each example during pretraining, our model can generalize to more shots during inference. We observed marginal improvement over the zero-shot condition when more shots of the task are given for the L/16 model, but not for the B/16 model, which implies that models with a larger capacity seems more likely to succeed on few shot learning. The models did not show meaningful few-shot performance for other tasks. We conjecture that the other tasks are too far from the pretraining data distribution, as our pretraining datasets are still much smaller than the ones in general domain (Alayrac et al., 2022). Using a larger and more diverse pretraining dataset and a larger text decoder might improve the few-shot learning performance for UI tasks.

6 Discussions

To understand how the Region Summarizer is able to enable Spotlight to focus on a target region as well as relevant areas on the screen, we analyze the attention weights for both Widget Captioning and Screen Summarization task. In the Widget Captioning example (Figure 2(a)), we can see the model learns to attend to not only the target region of the check box, but also the text â€œChelseaâ€ on the far left to generate the caption. For the Screen Summarization example in Figure 2(b) where the target region is the entire screen, the model learns to attend to important parts on the screen for summarization.

Spotlight models advance the art of UI understanding by using a large-scale pretraining dataset and a general architecture that can support different use cases. As shown in our ablation study (Appendix B), pretrained models significantly outperformed the models trained from scratch. There are various ways of using the unlabeled dataset to pretrain UI models (He et al., 2020; Bai et al., 2021). In our early exploration, we also experimented with decoding a serialized view hierarchy directly instead of only text of individual objects. However, this led to poor pretraining performance, likely due to the large amount of noise in view hierarchy structures (Li et al., 2022). Nevertheless, the structural information encapsulated in the view hierarchy is valuable and investigating the usage of such information for better pretraining remains an interesting direction for future work.

Compared to recent large vision-language model efforts (Alayrac et al., 2022; Chen et al., 2022) that use Oâ€‹(10â€‹B)ğ‘‚10ğµO(10B) parameters, the model size that we investigated is relatively small (Appendix A). Our experiments consistently show the trend of larger model yielding better performance, which promises that simply scaling up the model sizes would potentially lead to further performance gains. For this purpose, Spotlight can easily adopt larger ViT and T5 checkpoints. Our model architecture is generic, which takes a screenshot and a region bounding box of interest as well as text prompts, and outputs text responses. Spotlight can be easily applied to more UI tasks, e.g., icon classification (Deka et al., 2017) and multi-step grounding (Li et al., 2020a; Burns et al., 2022), as many UI tasks can be formulated as a sequence decoding task.

7 Conclusions

We presented Spotlight, a vision-only approach for mobile UI understanding, which alleviates the need to use view hierarchy. We design Spotlight based on a general vision-language model architecture and enable the model to focus on a region of interest on the screen. This architecture is easy to scale and can benefit from the success of recent large vision-language models pretrained for the general domain. Spotlight is able to fulfill a range of UI modeling tasks. Our experiments show that Spotlight obtains SoTA results on several representative UI tasks and outperforms previous works that use both screens and view hierarchies. We conduct ablations on a range of model variants, and report our findings on these variants, as well as a variety of modeling strategies and use cases, including finetuning, multi-tasking learning and few-shot prompting.

Acknowledgement

We thank the anonymous reviewers for their constructive feedback for improving the paper. We thank Mandar Joshi and Tao Li for their help in processing the C4 dataset. We also thank Chin-Yi Cheng and Forrest Huang for their feedback.

References

Alayrac et al. (2022) Jean-Baptiste Alayrac, Jeff Donahue, Pauline Luc, Antoine Miech, Iain Barr, Yana Hasson, Karel Lenc, Arthur Mensch, Katie Millican, Malcolm Reynolds, Roman Ring, Eliza Rutherford, Serkan Cabi, Tengda Han, Zhitao Gong, Sina Samangooei, Marianne Monteiro, Jacob Menick, Sebastian Borgeaud, Andrew Brock, Aida Nematzadeh, Sahand Sharifzadeh, Mikolaj Binkowski, Ricardo Barreira, Oriol Vinyals, Andrew Zisserman, and Karen Simonyan. Flamingo: a visual language model for few-shot learning, 2022. URL https://arxiv.org/abs/2204.14198.

Bai et al. (2021) Chongyang Bai, Xiaoxue Zang, Ying Xu, Srinivas Sunkara, Abhinav Rastogi, Jindong Chen, and Blaise AgÃ¼era y Arcas. Uibert: Learning generic multimodal representations for UI understanding. In Zhi-Hua Zhou (ed.), Proceedings of the Thirtieth International Joint Conference on Artificial Intelligence, IJCAI 2021, Virtual Event / Montreal, Canada, 19-27 August 2021, pp. 1705â€“1712. ijcai.org, 2021. doi: 10.24963/ijcai.2021/235. URL https://doi.org/10.24963/ijcai.2021/235.

Brown et al. (2020) Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel Ziegler, Jeffrey Wu, Clemens Winter, Chris Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, and Dario Amodei. Language models are few-shot learners. In H. Larochelle, M. Ranzato, R. Hadsell, M. F. Balcan, and H. Lin (eds.), Advances in Neural Information Processing Systems, volume 33, pp. 1877â€“1901. Curran Associates, Inc., 2020. URL https://proceedings.neurips.cc/paper/2020/file/1457c0d6bfcb4967418bfb8ac142f64a-Paper.pdf.

Burns et al. (2022) Andrea Burns, Deniz Arsan, Sanjna Agrawal, Ranjitha Kumar, Kate Saenko, and Bryan A. Plummer. A dataset for interactive vision-language navigation with unknown command feasibility, 2022. URL https://arxiv.org/abs/2202.02312.

Carion et al. (2020) Nicolas Carion, Francisco Massa, Gabriel Synnaeve, Nicolas Usunier, Alexander Kirillov, and Sergey Zagoruyko. End-to-end object detection with transformers, 2020. URL https://arxiv.org/abs/2005.12872.

Chen et al. (2022) Xi Chen, Xiao Wang, Soravit Changpinyo, AJ Piergiovanni, Piotr Padlewski, Daniel Salz, Sebastian Goodman, Adam Grycner, Basil Mustafa, Lucas Beyer, Alexander Kolesnikov, Joan Puigcerver, Nan Ding, Keran Rong, Hassan Akbari, Gaurav Mishra, Linting Xue, Ashish Thapliyal, James Bradbury, Weicheng Kuo, Mojtaba Seyedhosseini, Chao Jia, Burcu Karagol Ayan, Carlos Riquelme, Andreas Steiner, Anelia Angelova, Xiaohua Zhai, Neil Houlsby, and Radu Soricut. Pali: A jointly-scaled multilingual language-image model, 2022. URL https://arxiv.org/abs/2209.06794.

Chowdhery et al. (2022) Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam Roberts, Paul Barham, Hyung Won Chung, Charles Sutton, Sebastian Gehrmann, Parker Schuh, Kensen Shi, Sasha Tsvyashchenko, Joshua Maynez, Abhishek Rao, Parker Barnes, Yi Tay, Noam Shazeer, Vinodkumar Prabhakaran, Emily Reif, Nan Du, Ben Hutchinson, Reiner Pope, James Bradbury, Jacob Austin, Michael Isard, Guy Gur-Ari, Pengcheng Yin, Toju Duke, Anselm Levskaya, Sanjay Ghemawat, Sunipa Dev, Henryk Michalewski, Xavier Garcia, Vedant Misra, Kevin Robinson, Liam Fedus, Denny Zhou, Daphne Ippolito, David Luan, Hyeontaek Lim, Barret Zoph, Alexander Spiridonov, Ryan Sepassi, David Dohan, Shivani Agrawal, Mark Omernick, Andrew M. Dai, Thanumalayan Sankaranarayana Pillai, Marie Pellat, Aitor Lewkowycz, Erica Moreira, Rewon Child, Oleksandr Polozov, Katherine Lee, Zongwei Zhou, Xuezhi Wang, Brennan Saeta, Mark Diaz, Orhan Firat, Michele Catasta, Jason Wei, Kathy Meier-Hellstern, Douglas Eck, Jeff Dean, Slav Petrov, and Noah Fiedel. Palm: Scaling language modeling with pathways, 2022. URL https://arxiv.org/abs/2204.02311.

Deka et al. (2017) Biplab Deka, Zifeng Huang, Chad Franzen, Joshua Hibschman, Daniel Afergan, Yang Li, Jeffrey Nichols, and Ranjitha Kumar. Rico: A mobile app dataset for building data-driven design applications. In Proceedings of the 30th Annual ACM Symposium on User Interface Software and Technology, UIST â€™17, pp. 845â€“854, New York, NY, USA, 2017. Association for Computing Machinery. ISBN 9781450349819. doi: 10.1145/3126594.3126651. URL https://doi.org/10.1145/3126594.3126651.

Devlin et al. (2019) Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. BERT: Pre-training of deep bidirectional transformers for language understanding. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), pp. 4171â€“4186, Minneapolis, Minnesota, June 2019. Association for Computational Linguistics. doi: 10.18653/v1/N19-1423. URL https://www.aclweb.org/anthology/N19-1423.

Dosovitskiy et al. (2021) Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, Jakob Uszkoreit, and Neil Houlsby. An image is worth 16x16 words: Transformers for image recognition at scale. In International Conference on Learning Representations, 2021. URL https://openreview.net/forum?id=YicbFdNTTy.

He et al. (2017) Kaiming He, Georgia Gkioxari, Piotr DollÃ¡r, and Ross B. Girshick. Mask R-CNN. In IEEE International Conference on Computer Vision, ICCV 2017, Venice, Italy, October 22-29, 2017, pp. 2980â€“2988. IEEE Computer Society, 2017. doi: 10.1109/ICCV.2017.322. URL https://doi.org/10.1109/ICCV.2017.322.

He et al. (2020) Zecheng He, Srinivas Sunkara, Xiaoxue Zang, Ying Xu, Lijuan Liu, Nevan Wichers, Gabriel Schubiner, Ruby B. Lee, and Jindong Chen. Actionbert: Leveraging user actions for semantic understanding of user interfaces. CoRR, abs/2012.12350, 2020. URL https://arxiv.org/abs/2012.12350.

Li et al. (2022) Gang Li, Gilles Baechler, Manuel Tragut, and Yang Li. Learning to denoise raw mobile ui layouts for improving datasets at scale, 2022. URL https://arxiv.org/abs/2201.04100.

Li et al. (2020a) Yang Li, Jiacong He, Xin Zhou, Yuan Zhang, and Jason Baldridge. Mapping natural language instructions to mobile UI action sequences. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pp. 8198â€“8210, Online, July 2020a. Association for Computational Linguistics. doi: 10.18653/v1/2020.acl-main.729. URL https://www.aclweb.org/anthology/2020.acl-main.729.

Li et al. (2020b) Yang Li, Gang Li, Luheng He, Jingjie Zheng, Hong Li, and Zhiwei Guan. Widget captioning: Generating natural language description for mobile user interface elements, 2020b.

Li et al. (2021) Yang Li, Gang Li, Xin Zhou, Mostafa Dehghani, and Alexey A. Gritsenko. VUT: versatile UI transformer for multi-modal multi-task user interface modeling. CoRR, abs/2112.05692, 2021. URL https://arxiv.org/abs/2112.05692.

Mikolov et al. (2013) Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg Corrado, and Jeffrey Dean. Distributed representations of words and phrases and their compositionality, 2013. URL https://arxiv.org/abs/1310.4546.

Radford et al. (2021) Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen Krueger, and Ilya Sutskever. Learning transferable visual models from natural language supervision, 2021. URL https://arxiv.org/abs/2103.00020.

Raffel et al. (2019) Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J. Liu. Exploring the limits of transfer learning with a unified text-to-text transformer. CoRR, abs/1910.10683, 2019. URL http://arxiv.org/abs/1910.10683.

Ramesh et al. (2022) Aditya Ramesh, Prafulla Dhariwal, Alex Nichol, Casey Chu, and Mark Chen. Hierarchical text-conditional image generation with clip latents, 2022. URL https://arxiv.org/abs/2204.06125.

Ross et al. (2018) Anne Spencer Ross, Xiaoyi Zhang, James Fogarty, and Jacob O. Wobbrock. Examining image-based button labeling for accessibility in android apps through large-scale analysis. In Proceedings of the 20th International ACM SIGACCESS Conference on Computers and Accessibility, ASSETS â€™18, pp. 119â€“130, New York, NY, USA, 2018. ACM. ISBN 978-1-4503-5650-3. doi: 10.1145/3234695.3236364. URL http://doi.acm.org/10.1145/3234695.3236364.

Saharia et al. (2022) Chitwan Saharia, William Chan, Saurabh Saxena, Lala Li, Jay Whang, Emily Denton, Seyed Kamyar Seyed Ghasemipour, Burcu Karagol Ayan, S. Sara Mahdavi, Rapha Gontijo Lopes, Tim Salimans, Jonathan Ho, David J Fleet, and Mohammad Norouzi. Photorealistic text-to-image diffusion models with deep language understanding, 2022. URL https://arxiv.org/abs/2205.11487.

Schoop et al. (2022) Eldon Schoop, Xin Zhou, Gang Li, Zhourong Chen, BjÃ¶rn Hartmann, and Yang Li. Predicting and explaining mobile ui tappability with vision modeling and saliency analysis, 2022. URL https://arxiv.org/abs/2204.02448.

Swearngin & Li (2019) Amanda Swearngin and Yang Li. Modeling Mobile Interface Tappability Using Crowdsourcing and Deep Learning, pp. 1â€“11. Association for Computing Machinery, New York, NY, USA, 2019. ISBN 9781450359702. URL https://doi.org/10.1145/3290605.3300305.

Wang et al. (2021) Bryan Wang, Gang Li, Xin Zhou, Zhourong Chen, Tovi Grossman, and Yang Li. Screen2words: Automatic mobile UI summarization with multimodal learning. UISTâ€™21, 2021. URL https://arxiv.org/abs/2108.03353.

Wang et al. (2022) Wenhui Wang, Hangbo Bao, Li Dong, Johan Bjorck, Zhiliang Peng, Qiang Liu, Kriti Aggarwal, Owais Khan Mohammed, Saksham Singhal, Subhojit Som, and Furu Wei. Image as a foreign language: Beit pretraining for all vision and vision-language tasks, 2022. URL https://arxiv.org/abs/2208.10442.

Wu et al. (2021) Jason Wu, Xiaoyi Zhang, Jeff Nichols, and Jeffrey P. Bigham. Screen parsing: Towards reverse engineering of ui models from screenshots. 2021. URL https://arxiv.org/pdf/2109.08763.pdf.

Xue et al. (2020) Linting Xue, Noah Constant, Adam Roberts, Mihir Kale, Rami Al-Rfou, Aditya Siddhant, Aditya Barua, and Colin Raffel. mt5: A massively multilingual pre-trained text-to-text transformer. CoRR, abs/2010.11934, 2020. URL https://arxiv.org/abs/2010.11934.

Yu et al. (2022) Jiahui Yu, Zirui Wang, Vijay Vasudevan, Legg Yeung, Mojtaba Seyedhosseini, and Yonghui Wu. Coca: Contrastive captioners are image-text foundation models, 2022. URL https://arxiv.org/abs/2205.01917.

Zhang et al. (2021) Xiaoyi Zhang, Lilian de Greef, Amanda Swearngin, Samuel White, Kyle Murray, Lisa Yu, Qi Shan, Jeffrey Nichols, Jason Wu, Chris Fleizach, Aaron Everitt, and Jeffrey P Bigham. Screen recognition: Creating accessibility metadata for mobile applications from pixels. In Proceedings of the 2021 CHI Conference on Human Factors in Computing Systems, CHI â€™21, New York, NY, USA, 2021. Association for Computing Machinery. ISBN 9781450380966. doi: 10.1145/3411764.3445186. URL https://doi.org/10.1145/3411764.3445186.

Appendix A Hyperparameters & Model Sizes

We base Spotlight models in our experiments on T5 and ViT. We used two ViT configurations: B/16 and L/16 (Table 6). We use an image resolution [740,740]740740[740,740] for all our experiments. The mobile screenshots are resized with aspect ratio preserved and padded to [740,740]740740[740,740] if necessary. The paddings in the pixels are ignored by the ViT encoder using masking. The object bounding boxes are normalized to [0,1]01[0,1]. We use 0.1 dropout in all our models. For all the training experiments, we use a batch size of 128 and linear warmup/rsqrt decay for learning rate. During inference decoding, a beam size of 5 is used. The maximum text decoding length is 64 for each screen-object-text tuple.

Region Summarizer: For both the B/16 and L/16 model, we use n=4,de=64,d=768formulae-sequenceğ‘›4formulae-sequencesubscriptğ‘‘ğ‘’64ğ‘‘768n=4,d_{e}=64,d=768 for Equation 1 and 2.

Pretraining: We pretrain the Spotlight models with a learning rate of 9e-3 for both B/16 (164K steps) and L/16 models (156K steps), with an initial linear warmup to 10k steps. Each training example consists of [1,5]15[1,5] image-object-text tuples sampled from the pretraining datasets. The model with ViT B/16 is trained using 128 Google Cloud TPU v3 cores for 54 hours. The model with ViT L/16 is trained using 256 Google Cloud TPU v3 cores for 86 hours.

Finetuning: For finetuning, we use a learning rate of 1e-3 and 20k steps for the Command Grounding task and 1e-4 and 10k steps for the other three tasks.

Multi-Task Learning: We use a learning rate of 3e-4 for multi-task learning, and train the multi-task Spotlight models for 30k steps. The sampling weights are [3,2,15,1]32151[3,2,15,1] for the Widget Captioning, Screen Summarization, Command Grounding and Tappability tasks. We use 222 screen-object-text tuples in each example during training. During evaluation, a screen-object-text tuple from the training set of the target task is used to prompt the model to perform the task.

Appendix B Ablation Study

To explore different options for designing the Spotlight models, we conduct an extensive ablation study. We use the model with B/16 ViT as the full model. Each ablated version has one option changed compared to the full model, which is listed in the first column in Table 7. We pretrain all the ablation variants for 100K steps and finetune them on the four downstream tasks using the same hyperparameters and training schedule (see Appendix A). The full model in Table 3 has better results as it is pretrained with more steps (164K).

Most notably, the models that use a frozen ViT or are trained from scratch did not perform well for all the four tasks. This indicates the pretraining on the mobile and web datasets is crucial for the downstream UI tasks. Using only one of the two pretraining datasets also led to degraded performance, which demonstrates the value of combining heterogeneous datasets.

Region Summarizer is an important component that we proposed for enabling Spotlight to focus on a specific region on the screen. We compare a few options for the design of Region Summarizer. In Equation 2, the region representation is concatenated with ViT encodings to form the memory for cross attention and the region representation is updated after each layer: kv=Hâˆ¥Qikvâˆ¥ğ»subscriptğ‘„ğ‘–\mbox{kv}=H\mathbin{\|}Q_{i} and Q0=Xsubscriptğ‘„0ğ‘‹Q_{0}=X. We investigate a static region representation in kv, i.e., kv=Hâˆ¥Xkvâˆ¥ğ»ğ‘‹\mbox{kv}=H\mathbin{\|}X, which is not updated by each cross-attention layer and no region representation kv, i.e., kv=Hkvğ»\mbox{kv}=H. We also investigate embedding four bounding box coordinates jointly instead of individually. These alternatives perform reasonably on the UI tasks except the Widget Captioning task. This indicates that more flexibility in the bounding box queries, e.g., embedding each coordinate individually and updating its embedding using cross-attention per layer, helps the Widget Captioning task. In the Section 6, we can see that the bounding box queries not only attend to the area of the bounding box itself, but also the relevant context on the screen needed for decoding the captions.

Lastly, using ROI Align directly performed worse for the screen summarization task than using Region Summarizer, possibly due to that some information of the entire screen can be lost during the average pooling. In contrast, using the vector from ROI Align as the query to Region Summarizer instead of the bounding box led to improved results for text decoding tasks, but performed worse for the Command Grounding task.

Appendix C Pseudo Code

Appendix D Pretraining Datasets

In the mobile pretraining dataset, view hierarchy is a tree structure of a mobile UI, like the DOM tree to a webpage. In a view hierarchy, each leaf node corresponds to an object on the screen, which contains a collection of attributes such as object type, text content, visibility and clickability. Each non-terminal node represents a group of objects. However, in reality, view hierarchies are often acquired from the runtime render tree of a UI screen, which can contain much noise and might not be aligned with the visual representation.

We list all the HTML attributes used for pretraining and their statistics in the C4 dataset (Table 8). For both the web and mobile dataset, we remove invisible objects (based on CSS or view hierarchy attributes) and objects with bounding box of uniform color, and filter out object text consisting of only generic words. The list of generic words are the following: action, bar, menu, title, and, ans, app, icon, name, arg, background, element, btn, but, bottom, button, content, desc, text, item, empty, fab, image, grid, header, img, imgfile, lbutton, label, letter, list, view, pic, placeholder, random, row, single, raw, small, large, sub, template, navbar, banner, test, textinput, error, texto, todo, toolbar, tool, track, txt, unknown, stub, web, left, right, tlb, nan, page, feature, menugrid, picture, tabs, number, node, iconimage, entity, webview, heading, logo, tbl, tab, primary, footer. We also remove object text that contains only single characters or URLs. Continuous spaces and underscores are replaced with a single space. All the text are lowercased.

Appendix E Downstream Tasks & Datasets

We list the dataset statistics of the four downstream UI tasks in Table 9 and show one example of each task in Figure 3. For Widget Captioning, Command Grounding and Tappability, the models use individual UI objects, e.g., decoding the caption of an UI object. For Screen Summarization, the models are trained to decode the summary of an entire screen. More detailed information of the four datasets can be found in their original papers.
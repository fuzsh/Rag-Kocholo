{
    "id": "dbpedia_6494_1",
    "rank": 25,
    "data": {
        "url": "https://beginningwithml.wordpress.com/author/yrahul97/page/2/",
        "read_more_link": "",
        "language": "en",
        "title": "Rahul Yedida",
        "top_image": "https://secure.gravatar.com/avatar/927b80f86f0620c816413fcdc6446437?s=200&d=identicon&r=g",
        "meta_img": "https://secure.gravatar.com/avatar/927b80f86f0620c816413fcdc6446437?s=200&d=identicon&r=g",
        "images": [
            "https://s0.wp.com/latex.php?latex=k&bg=ffffff&fg=000000&s=1&c=20201002",
            "https://s0.wp.com/latex.php?latex=k&bg=ffffff&fg=000000&s=1&c=20201002",
            "https://s0.wp.com/latex.php?latex=p%28z%5E%7B%28i%29%7D%3B+G%29&bg=ffffff&fg=000000&s=1&c=20201002",
            "https://s0.wp.com/latex.php?latex=z%5E%7B%28i%29%7D+%5Csim+%5Ctext%7B+Multinomial%7D%28%5Cphi%29&bg=ffffff&fg=000000&s=1&c=20201002",
            "https://s0.wp.com/latex.php?latex=z%5E%7B%28i%29%7D&bg=ffffff&fg=000000&s=1&c=20201002",
            "https://s0.wp.com/latex.php?latex=z%5E%7B%28i%29%7D&bg=ffffff&fg=000000&s=1&c=20201002",
            "https://s0.wp.com/latex.php?latex=G&bg=ffffff&fg=000000&s=1&c=20201002",
            "https://s0.wp.com/latex.php?latex=p%28x%5E%7B%28i%29%7D%3B+G%29+%3D+%5Csum%5Climits_%7Bl%3D1%7D%5Ek+p%28x%5E%7B%28i%29%7D+%7C+z%5E%7B%28i%29%7D%3Dl%3B+G%29+p%28z%5E%7B%28i%29%7D%3Dl%3B+G%29&bg=ffffff&fg=000000&s=1&c=20201002",
            "https://s0.wp.com/latex.php?latex=G+%3D+%28%5Cboldsymbol+%5Cmu%2C+%5Cboldsymbol+%5Csigma%2C+%5Cboldsymbol+%5Cphi%29&bg=ffffff&fg=000000&s=1&c=20201002",
            "https://s0.wp.com/latex.php?latex=x%5E%7B%28i%29%7D&bg=ffffff&fg=000000&s=1&c=20201002",
            "https://s0.wp.com/latex.php?latex=z%5E%7B%28i%29%7D&bg=ffffff&fg=000000&s=1&c=20201002",
            "https://s0.wp.com/latex.php?latex=z%5E%7B%28i%29%7D&bg=ffffff&fg=000000&s=1&c=20201002",
            "https://s0.wp.com/latex.php?latex=z%5E%7B%28i%29%7D&bg=ffffff&fg=000000&s=1&c=20201002",
            "https://s0.wp.com/latex.php?latex=x%5E%7B%28i%29%7D&bg=ffffff&fg=000000&s=1&c=20201002",
            "https://s0.wp.com/latex.php?latex=i%2C+j&bg=ffffff&fg=000000&s=1&c=20201002",
            "https://s0.wp.com/latex.php?latex=p%28z%5E%7B%28i%29%7D+%3D+j+%7C+x%5E%7B%28i%29%7D%3B+G%29+%3D+%5Cfrac%7Bp%28x%5E%7B%28i%29%7D+%7C+z%5E%7B%28i%29%7D%3Dj%3B+%5Cboldsymbol%5Cmu%2C+%5Cboldsymbol%5Csigma%29+p%28z%5E%7B%28i%29%7D+%3D+j%3B+%5Cphi%29%7D%7Bp%28x%5E%7B%28i%29%7D%29%7D+%3D+%5Cfrac%7B%5Cmathcal%7BN%7D%28%5Cmu_j%2C+%5Csigma_j%29+p%28z%5E%7B%28i%29%7D%3Dj%3B+%5Cphi%29%7D%7B%5Csum%5Climits_%7Bl%3D1%7D%5Ek+%5Cmathcal%7BN%7D%28%5Cmu_l%2C+%5Csigma_l%29+p%28z%5E%7B%28i%29%7D%3Dl%3B+%5Cphi%29%7D&bg=ffffff&fg=000000&s=2&c=20201002",
            "https://s0.wp.com/latex.php?latex=%5Cbegin%7Baligned%7D+%5Cmu_j+%26%3D+%5Cfrac%7B%5Csum%5Climits_%7Bi%3D1%7D%5Em+p%28z%5E%7B%28i%29%7D+%3D+j+%7C+x%5E%7B%28i%29%7D%3B+G%29+x%5E%7B%28i%29%7D%7D%7B%5Csum%5Climits_%7Bi%3D1%7D%5Em+p%28z%5E%7B%28i%29%7D+%3D+j+%7C+x%5E%7B%28i%29%7D%3B+G%29%7D+%5C%5C+%5Csigma_j%5E2+%26%3D+%5Cfrac%7B%5Csum%5Climits_%7Bi%3D1%7D%5Em+p%28z%5E%7B%28i%29%7D+%3D+j+%7C+x%5E%7B%28i%29%7D%3B+G%29+%5Cleft%5Cvert+x%5E%7B%28i%29%7D+-+%5Cmu_j+%5Cright%5Cvert%5E2%7D%7B%5Csum%5Climits_%7Bi%3D1%7D%5Em+p%28z%5E%7B%28i%29%7D+%3D+j+%7C+x%5E%7B%28i%29%7D%3B+G%29%7D+%5C%5C+%5Cphi_j+%26%3D+%5Cfrac%7B1%7D%7Bm%7D+%5Csum%5Climits_%7Bi%3D1%7D%5Em+p%28z%5E%7B%28i%29%7D+%3D+j+%7C+x%5E%7B%28i%29%7D%3B+G%29+%5Cend%7Baligned%7D&bg=ffffff&fg=000000&s=1&c=20201002",
            "https://beginningwithml.wordpress.com/wp-content/uploads/2019/04/q20_b.png?w=1100",
            "https://s0.wp.com/latex.php?latex=x%5E%7B%28j%29%5Bi%5D%7D_k&bg=ffffff&fg=000000&s=1&c=20201002",
            "https://s0.wp.com/latex.php?latex=k&bg=ffffff&fg=000000&s=1&c=20201002",
            "https://s0.wp.com/latex.php?latex=j&bg=ffffff&fg=000000&s=1&c=20201002",
            "https://s0.wp.com/latex.php?latex=i&bg=ffffff&fg=000000&s=1&c=20201002",
            "https://s0.wp.com/latex.php?latex=%5Cbar%7Bx%7D%5E%7B%5Bi%5D%7D_k&bg=ffffff&fg=000000&s=1&c=20201002",
            "https://s0.wp.com/latex.php?latex=k&bg=ffffff&fg=000000&s=1&c=20201002",
            "https://s0.wp.com/latex.php?latex=i&bg=ffffff&fg=000000&s=1&c=20201002",
            "https://s0.wp.com/latex.php?latex=ESS+%3D+%5Csum%5Climits_i+%5Csum%5Climits_j+%5Csum%5Climits_k+%5Cleft%5Cvert+x%5E%7B%28j%29%5Bi%5D%7D_k+-+%5Cbar%7Bx%7D%5E%7B%5Bi%5D%7D_k+%5Cright%5Cvert%5E2&bg=ffffff&fg=000000&s=1&c=20201002",
            "https://s0.wp.com/latex.php?latex=C_i%2C+C_j&bg=ffffff&fg=000000&s=1&c=20201002",
            "https://s0.wp.com/latex.php?latex=d_%7Bij%7D&bg=ffffff&fg=000000&s=1&c=20201002",
            "https://s0.wp.com/latex.php?latex=C_i&bg=ffffff&fg=000000&s=1&c=20201002",
            "https://s0.wp.com/latex.php?latex=C_j&bg=ffffff&fg=000000&s=1&c=20201002",
            "https://s0.wp.com/latex.php?latex=d_%7B%28ij%29k%7D&bg=ffffff&fg=000000&s=1&c=20201002",
            "https://s0.wp.com/latex.php?latex=C_i+%5Ccup+C_j&bg=ffffff&fg=000000&s=1&c=20201002",
            "https://s0.wp.com/latex.php?latex=C_k&bg=ffffff&fg=000000&s=1&c=20201002",
            "https://s0.wp.com/latex.php?latex=d_%7B%28ij%29k%7D&bg=ffffff&fg=000000&s=1&c=20201002",
            "https://s0.wp.com/latex.php?latex=d_%7B%28ij%29k%7D+%3D+%5Calpha_i+d_%7Bik%7D+%2B+%5Calpha_j+d_%7Bjk%7D+%2B+%5Cbeta+d_%7Bij%7D+%2B+%5Cgamma+%5Cleft%5Cvert+d_%7Bik%7D-d_%7Bjk%7D+%5Cright%5Cvert&bg=ffffff&fg=000000&s=1&c=20201002",
            "https://s0.wp.com/latex.php?latex=%5Calpha_i&bg=ffffff&fg=000000&s=1&c=20201002",
            "https://s0.wp.com/latex.php?latex=%5Calpha_j&bg=ffffff&fg=000000&s=1&c=20201002",
            "https://s0.wp.com/latex.php?latex=%5Cbeta&bg=ffffff&fg=000000&s=1&c=20201002",
            "https://s0.wp.com/latex.php?latex=%5Cgamma&bg=ffffff&fg=000000&s=1&c=20201002",
            "https://s0.wp.com/latex.php?latex=%5Cfrac%7Bn_i%7D%7Bn_i%2Bn_j%7D&bg=ffffff&fg=000000&s=1&c=20201002",
            "https://s0.wp.com/latex.php?latex=%5Cfrac%7Bn_j%7D%7Bn_i%2Bn_j%7D&bg=ffffff&fg=000000&s=1&c=20201002",
            "https://s0.wp.com/latex.php?latex=%5Cfrac%7Bn_i%2Bn_k%7D%7Bn_i%2Bn_j%2Bn_k%7D&bg=ffffff&fg=000000&s=1&c=20201002",
            "https://s0.wp.com/latex.php?latex=%5Cfrac%7Bn_j%2Bn_k%7D%7Bn_i%2Bn_j%2Bn_k%7D&bg=ffffff&fg=000000&s=1&c=20201002",
            "https://s0.wp.com/latex.php?latex=%5Cfrac%7B-n_k%7D%7Bn_i%2Bn_j%2Bn_k%7D&bg=ffffff&fg=000000&s=1&c=20201002",
            "https://beginningwithml.wordpress.com/wp-content/uploads/2019/04/5aikc.png?w=456&h=329",
            "https://s0.wp.com/latex.php?latex=k&bg=ffffff&fg=000000&s=1&c=20201002",
            "https://beginningwithml.wordpress.com/wp-content/uploads/2019/04/image_2019-04-17_10-30-41.png?w=1100",
            "https://s0.wp.com/latex.php?latex=k&bg=ffffff&fg=000000&s=1&c=20201002",
            "https://beginningwithml.wordpress.com/wp-content/uploads/2019/04/ezgif-5-82db5658a775.gif?w=1100",
            "https://s0.wp.com/latex.php?latex=J%28C%2C+%5Cmu%29+%3D+%5Csum%5Climits_%7Bi%3D1%7D%5Em+%5Cleft%5CVert+x%5E%7B%28i%29%7D+-+%5Cmu_%7Bc_i%7D+%5Cright%5CVert%5E2&bg=ffffff&fg=000000&s=1&c=20201002",
            "https://s0.wp.com/latex.php?latex=%5Cmu&bg=ffffff&fg=000000&s=1&c=20201002",
            "https://s0.wp.com/latex.php?latex=J&bg=ffffff&fg=000000&s=1&c=20201002",
            "https://s0.wp.com/latex.php?latex=C&bg=ffffff&fg=000000&s=1&c=20201002",
            "https://s0.wp.com/latex.php?latex=C&bg=ffffff&fg=000000&s=1&c=20201002",
            "https://s0.wp.com/latex.php?latex=J&bg=ffffff&fg=000000&s=1&c=20201002",
            "https://s0.wp.com/latex.php?latex=%5Cmu&bg=ffffff&fg=000000&s=1&c=20201002",
            "https://s0.wp.com/latex.php?latex=J&bg=ffffff&fg=000000&s=1&c=20201002",
            "https://s0.wp.com/latex.php?latex=C&bg=ffffff&fg=000000&s=1&c=20201002",
            "https://s0.wp.com/latex.php?latex=%5Cmu&bg=ffffff&fg=000000&s=1&c=20201002",
            "https://s0.wp.com/latex.php?latex=J&bg=ffffff&fg=000000&s=1&c=20201002",
            "https://s0.wp.com/latex.php?latex=X&bg=ffffff&fg=000000&s=1&c=20201002",
            "https://beginningwithml.wordpress.com/wp-content/uploads/2019/04/sphx_glr_plot_cluster_comparison_0011.png?w=1100",
            "https://beginningwithml.wordpress.com/wp-content/uploads/2019/04/image_2019-04-14_16-02-47.png?w=1100",
            "https://s0.wp.com/latex.php?latex=%5Cbegin%7Baligned%7D+%5Cmin%5Climits_%7B%5Cxi%2C+w%2C+b%7D+%26%5Cfrac%7B1%7D%7B2%7D+%5Cleft%5CVert+w+%5Cright%5CVert%5E2+%2B+C+%5Csum%5Climits_%7Bi%3D1%7D%5Em+%5Cxi_i+%5C%5C+%5Ctext%7Bs.t.+%7D+%26y%5E%7B%28i%29%7D%5Cleft%28+w%5ET+x%5E%7B%28i%29%7D%2Bb+%5Cright%29+%5Cgeq+1+-+%5Cxi_i+%5C%5C+%26+%5Cxi_i+%5Cgeq+0+%5Cend%7Baligned%7D&bg=ffffff&fg=000000&s=1&c=20201002",
            "https://s0.wp.com/latex.php?latex=C&bg=ffffff&fg=000000&s=1&c=20201002",
            "https://s0.wp.com/latex.php?latex=%5Cxi_i&bg=ffffff&fg=000000&s=1&c=20201002",
            "https://s0.wp.com/latex.php?latex=1+-+%5Cxi_i&bg=ffffff&fg=000000&s=1&c=20201002",
            "https://s0.wp.com/latex.php?latex=C&bg=ffffff&fg=000000&s=1&c=20201002",
            "https://s0.wp.com/latex.php?latex=C&bg=ffffff&fg=000000&s=1&c=20201002",
            "https://s0.wp.com/latex.php?latex=%5Calpha%2C+r&bg=ffffff&fg=000000&s=1&c=20201002",
            "https://s0.wp.com/latex.php?latex=%5Cmathcal%7BL%7D%28w%2C+b%2C+%5Cxi%2C+%5Calpha%2C+r%29+%3D+%5Cfrac%7B1%7D%7B2%7Dw%5ET+w+%2B+C%5Csum%5Climits_%7Bi%3D1%7D%5Em+%5Cxi_i+-+%5Csum%5Climits_%7Bi%3D1%7D%5Em+%5Calpha_i+%5Cleft%28+y%5E%7B%28i%29%7D+%28w%5ET+x%5E%7B%28i%29%7D%2Bb%29+-+1+%2B+%5Cxi_i+%5Cright%29+-+%5Csum%5Climits_%7Bi%3D1%7D%5Em+r_i+%5Cxi_i&bg=ffffff&fg=000000&s=1&c=20201002",
            "https://s0.wp.com/latex.php?latex=%5Cbegin%7Baligned%7D+%5Cmax%5Climits_%5Calpha+%26+%5Csum%5Climits_%7Bi%3D1%7D%5Em+%5Calpha_i+-+%5Cfrac%7B1%7D%7B2%7D+%5Csum%5Climits_%7Bi%3D1%7D%5Em+%5Csum%5Climits_%7Bj%3D1%7D%5Em+y%5E%7B%28i%29%7Dy%5E%7B%28j%29%7D%5Calpha_i+%5Calpha_j+%5Cleft%5Clangle+x%5E%7B%28i%29%7D%2C+x%5E%7B%28j%29%7D+%5Cright%5Crangle+%5C%5C+%5Ctext%7Bs.t.+%7D+%26+0+%5Cleq+%5Calpha_i+%5Cleq+C+%5C%5C+%26+%5Csum%5Climits_%7Bi%3D1%7D%5Em+%5Calpha_i+y%5E%7B%28i%29%7D+%3D+0+%5Cend%7Baligned%7D&bg=ffffff&fg=000000&s=1&c=20201002",
            "https://s0.wp.com/latex.php?latex=%5Cphi&bg=ffffff&fg=000000&s=1&c=20201002",
            "https://s0.wp.com/latex.php?latex=m&bg=ffffff&fg=000000&s=1&c=20201002",
            "https://s0.wp.com/latex.php?latex=%5Cmathbb%7BR%7D%5Em&bg=ffffff&fg=000000&s=1&c=20201002",
            "https://s0.wp.com/latex.php?latex=n&bg=ffffff&fg=000000&s=1&c=20201002",
            "https://s0.wp.com/latex.php?latex=%5Cmathbb%7BR%7D%5En&bg=ffffff&fg=000000&s=1&c=20201002",
            "https://s0.wp.com/latex.php?latex=x%5E%7B%28i%29%7D&bg=ffffff&fg=000000&s=1&c=20201002",
            "https://s0.wp.com/latex.php?latex=%5Cphi%28x%5E%7B%28i%29%7D%29&bg=ffffff&fg=000000&s=1&c=20201002",
            "https://s0.wp.com/latex.php?latex=K%28x%5E%7B%28i%29%7D%2C+x%5E%7B%28j%29%7D%29&bg=ffffff&fg=000000&s=1&c=20201002",
            "https://s0.wp.com/latex.php?latex=K%28x%5E%7B%28i%29%7D%2C+x%5E%7B%28j%29%7D%29+%3D+%5Cphi%28x%5E%7B%28i%29%7D%29%5ET+%5Cphi%28x%5E%7B%28j%29%7D%29&bg=ffffff&fg=000000&s=1&c=20201002",
            "https://s0.wp.com/latex.php?latex=%5Cphi%3A+x+%3D+%28x_1%2C+x_2%29+%5Cmapsto+%28x_1%5E2%2C+x_2%5E2%2C+%5Csqrt%7B2%7Dx_1x_2%29&bg=ffffff&fg=000000&s=1&c=20201002",
            "https://s0.wp.com/latex.php?latex=%5Cbegin%7Baligned%7D+K%28x%2C+z%29+%26%3D+%5Cleft+%5Clangle+%5Cphi%28x%29%2C+%5Cphi%28z%29+%5Cright+%5Crangle+%5C%5C+%26%3D+%5Cleft+%5Clangle+%28x_1%5E2%2C+x_2%5E2%2C+%5Csqrt%7B2%7Dx_1x_2%29%2C+%28z_1%5E2%2C+z_2%5E2%2C+%5Csqrt%7B2%7Dz_1z_2%29+%5Cright+%5Crangle+%5C%5C+%26%3D%C2%A0+x_1%5E2+z_2%5E2+%2B+x_2%5E2+z_2%5E2+%2B+2x_1x_2z_1z_2+%5C%5C+%26%3D+%28x_1z_1+%2B+x_2z_2%29%5E2+%5C%5C+%26%3D+%5Cleft+%5Clangle+x%2Cz+%5Cright+%5Crangle%5E2+%5Cend%7Baligned%7D&bg=ffffff&fg=000000&s=1&c=20201002",
            "https://s0.wp.com/latex.php?latex=%5Cphi%28x%29&bg=ffffff&fg=000000&s=1&c=20201002",
            "https://s0.wp.com/latex.php?latex=%5Cmathcal%7BO%7D%28n%5E2%29&bg=ffffff&fg=000000&s=1&c=20201002",
            "https://s0.wp.com/latex.php?latex=n&bg=ffffff&fg=000000&s=1&c=20201002",
            "https://s0.wp.com/latex.php?latex=%5Cmathcal%7BO%7D%28n%29&bg=ffffff&fg=000000&s=1&c=20201002",
            "https://beginningwithml.wordpress.com/wp-content/uploads/2019/04/image_2019-04-08_14-56-12.png?w=559&h=288",
            "https://s0.wp.com/latex.php?latex=%5Cphi&bg=ffffff&fg=000000&s=1&c=20201002",
            "https://s0.wp.com/latex.php?latex=K%28x%2C+z%29+%3D+%5Cleft%5Clangle+%5Cphi%28x%29%2C+%5Cphi%28z%29+%5Cright%5Crangle&bg=ffffff&fg=000000&s=1&c=20201002",
            "https://s0.wp.com/latex.php?latex=%5Cmathcal%7BK%7D&bg=ffffff&fg=000000&s=1&c=20201002",
            "https://s0.wp.com/latex.php?latex=m&bg=ffffff&fg=000000&s=1&c=20201002",
            "https://s0.wp.com/latex.php?latex=%5Cmathcal%7BK%7D_%7Bij%7D+%3D+K%28x%5E%7B%28i%29%7D%2C+x%5E%7B%28j%29%7D%29&bg=ffffff&fg=000000&s=1&c=20201002",
            "https://s0.wp.com/latex.php?latex=%5Cmathcal%7BK%7D&bg=ffffff&fg=000000&s=1&c=20201002",
            "https://s0.wp.com/latex.php?latex=z&bg=ffffff&fg=000000&s=1&c=20201002",
            "https://s0.wp.com/latex.php?latex=z%5ET+%5Cmathcal%7BK%7Dz&bg=ffffff&fg=000000&s=1&c=20201002",
            "https://s0.wp.com/latex.php?latex=%5Cbegin%7Baligned%7D+z%5ET%5Cmathcal%7BK%7Dz+%26%3D+%5Csum%5Climits_i+%5Csum%5Climits_j+z_i+%5Cmathcal%7BK%7D_%7Bij%7Dz_j+%5C%5C+%26%3D+%5Csum%5Climits_i+%5Csum%5Climits_j+z_i+%5Cphi%28x%5E%7B%28i%29%7D%29%5ET+%5Cphi%28x%5E%7B%28j%29%7D%29z_j+%5C%5C+%26%3D+%5Csum%5Climits_i+%5Csum%5Climits_j+z_i+z_j+%5Csum%5Climits_k+%5Cphi_k%28x%5E%7B%28i%29%7D%29%5Cphi_k%28x%5E%7B%28j%29%7D%29+%5C%5C+%26%3D+%5Csum%5Climits_i+%5Csum%5Climits_j+%5Csum%5Climits_k+z_i+z_j+%5Cphi_k%28x%5E%7B%28i%29%7D%29%5Cphi_k%28x%5E%7B%28j%29%7D%29+%5C%5C+%26%3D+%5Csum%5Climits_k+%5Cleft%28+%5Csum%5Climits_i+z_i+%5Cphi_k%28x%5E%7B%28i%29%7D%29+%5Cright%29%5E2+%5Cgeq+0+%5Cend%7Baligned%7D&bg=ffffff&fg=000000&s=1&c=20201002",
            "https://s0.wp.com/latex.php?latex=%5Cphi%28x%29&bg=ffffff&fg=000000&s=1&c=20201002",
            "https://s0.wp.com/latex.php?latex=%5Cphi_k%28x%29&bg=ffffff&fg=000000&s=1&c=20201002",
            "https://s0.wp.com/latex.php?latex=k&bg=ffffff&fg=000000&s=1&c=20201002",
            "https://s0.wp.com/latex.php?latex=k&bg=ffffff&fg=000000&s=1&c=20201002",
            "https://s0.wp.com/latex.php?latex=%5Cphi%28x%5E%7B%28i%29%7D%29&bg=ffffff&fg=000000&s=1&c=20201002",
            "https://s0.wp.com/latex.php?latex=%5Cphi_k%28x%5E%7B%28i%29%7D%29&bg=ffffff&fg=000000&s=1&c=20201002",
            "https://s0.wp.com/latex.php?latex=i&bg=ffffff&fg=000000&s=1&c=20201002",
            "https://s0.wp.com/latex.php?latex=%5Cbegin%7Baligned%7D+%26%3D%5Csum%5Climits_i+%5Csum%5Climits_j+%5Csum%5Climits_k+z_i+z_j+%5Cphi_k%28x%5E%7B%28i%29%7D%29%5Cphi_k%28x%5E%7B%28j%29%7D%29+%5C%5C+%26%3D+%5Csum%5Climits_k+%5Cleft%28+%5Csum%5Climits_i+z_i+%5Cphi_k%28x%5E%7B%28i%29%7D%29+%5Cright%29+%5Cleft%28+%5Csum%5Climits_j+z_j+%5Cphi_k%28x%5E%7B%28j%29%7D%29+%5Cright%29+%5C%5C+%26%3D+%5Csum%5Climits_k+%5Cleft%28+%5Csum%5Climits_i+z_i+%5Cphi_k%28x%5E%7B%28i%29%7D%29+%5Cright%29+%5Cleft%28%5Csum%5Climits_i+z_i+%5Cphi_k%28x%5E%7B%28i%29%7D%29+%5Cright%29+%5Cend%7Baligned%7D&bg=ffffff&fg=000000&s=1&c=20201002",
            "https://s0.wp.com/latex.php?latex=%5Cmathcal%7BK%7D&bg=ffffff&fg=000000&s=1&c=20201002",
            "https://s0.wp.com/latex.php?latex=K%3A+%5Cmathbb%7BR%7D%5En+%5Ctimes+%5Cmathbb%7BR%7D%5En+%5Cmapsto+%5Cmathbb%7BR%7D&bg=ffffff&fg=000000&s=1&c=20201002",
            "https://s0.wp.com/latex.php?latex=%5C%7B+x%5E%7B%281%29%7D%2C+x%5E%7B%282%29%7D%2C+%5Cldots%2C+x%5E%7B%28m%29%7D+%5C%7D&bg=ffffff&fg=000000&s=1&c=20201002",
            "https://s0.wp.com/latex.php?latex=K%28x%2C+z%29+%3D+%5Cexp%5Cleft%28+%5Cfrac%7B%5Cleft%5CVert+x-z+%5Cright%5CVert%5E2%7D%7B2%5Csigma%5E2%7D+%5Cright%29&bg=ffffff&fg=000000&s=1&c=20201002",
            "https://s0.wp.com/latex.php?latex=%5Csigma&bg=ffffff&fg=000000&s=1&c=20201002",
            "https://beginningwithml.wordpress.com/wp-content/uploads/2019/03/index.png?w=1100",
            "https://s0.wp.com/latex.php?latex=%5Cbegin%7Baligned%7D+%5Cmax%5Climits_%7B%5Cboldsymbol%5Clambda%7D+%26+%5Csum%5Climits_%7Bi%3D1%7D%5Em+%5Clambda_i+-+%5Cfrac%7B1%7D%7B2%7D%5Csum%5Climits_%7Bi%3D1%7D%5Em+%5Csum%5Climits_%7Bj%3D1%7D%5Em+%5Clambda_i+%5Clambda_j+y%5E%7B%28i%29%7Dy%5E%7B%28j%29%7D+x%5E%7B%28i%29T%7Dx%5E%7B%28j%29%7D+%5C%5C+%5Ctext%7Bs.t.+%7D+%26+%5Clambda_i+%5Cgeq+0+%5C%5C+%26+%5Csum%5Climits_%7Bi%3D1%7D%5Em+%5Clambda_i+y%5E%7B%28i%29%7D+%3D+0+%5Cend%7Baligned%7D+&bg=ffffff&fg=000000&s=1&c=20201002",
            "https://s0.wp.com/latex.php?latex=%5Cbegin%7Baligned%7D+%26+%5Cmin+%5Cfrac%7B1%7D%7B2%7D+x%5ETPx+%2B+q%5ETx+%5C%5C+%5Ctext%7Bs.t.+%7D+%26+Gx+%5Cleq+h+%5C%5C+%26+%5C+Ax+%3D+b+%5Cend%7Baligned%7D&bg=ffffff&fg=000000&s=1&c=20201002",
            "https://s0.wp.com/latex.php?latex=H&bg=ffffff&fg=000000&s=1&c=20201002",
            "https://s0.wp.com/latex.php?latex=H_%7Bij%7D+%3D+y%5E%7B%28i%29%7Dy%5E%7B%28j%29%7Dx%5E%7B%28i%29T%7Dx%5E%7B%28j%29%7D&bg=ffffff&fg=000000&s=1&c=20201002",
            "https://s0.wp.com/latex.php?latex=%5Cbegin%7Baligned%7D+%5Cmax%5Climits_%7B%5Cboldsymbol%5Clambda%7D+%26+%5Csum%5Climits_%7Bi%3D1%7D%5Em+%5Clambda_i+-+%5Cfrac%7B1%7D%7B2%7D%5Csum%5Climits_%7Bi%3D1%7D%5Em+%5Clambda%5ET+H+%5Clambda+%5C%5C+%5Ctext%7Bs.t.+%7D+%26+%5Clambda_i+%5Cgeq+0+%5C%5C+%26+%5Csum%5Climits_%7Bi%3D1%7D%5Em+%5Clambda_i+y%5E%7B%28i%29%7D+%3D+0+%5Cend%7Baligned%7D+&bg=ffffff&fg=000000&s=1&c=20201002",
            "https://s0.wp.com/latex.php?latex=%5Cbegin%7Baligned%7D+%5Cmin%5Climits_%7B%5Cboldsymbol%5Clambda%7D+%26+%5Cfrac%7B1%7D%7B2%7D%5Cboldsymbol%5Clambda%5ET+%5Ctextbf%7BH%7D%5Cboldsymbol%5Clambda+-+%5Ctextbf%7B1%7D%5ET+%5Cboldsymbol%5Clambda+%5C%5C+%5Ctext%7Bs.t.+%7D+%26+-%5Clambda_i+%5Cleq+0+%5C%5C+%26+%5Ctextbf%7By%7D%5ET+%5Cboldsymbol%5Clambda+%3D+0+%5Cend%7Baligned%7D&bg=ffffff&fg=000000&s=1&c=20201002",
            "https://s0.wp.com/latex.php?latex=%5Cbegin%7Baligned%7D+P+%26%3D+%5Ctextbf%7BH%7D+%5C%5C+x+%26%3D+%5Cboldsymbol%5Clambda+%5C%5C+q+%26%3D+-%5Ctextbf%7B1%7D+%5C%5C+h+%26%3D+%5Ctextbf%7B0%7D+%5C%5C+b+%26%3D+%5Ctextbf%7B0%7D+%5C%5C+A+%26%3D+%5Ctextbf%7By%7D+%5C%5C+G+%26%3D+-%5Ctextbf%7BI%7D+%5Cend%7Baligned%7D&bg=ffffff&fg=000000&s=1&c=20201002",
            "https://s0.wp.com/latex.php?latex=%5Clambda&bg=ffffff&fg=000000&s=1&c=20201002",
            "https://s0.wp.com/latex.php?latex=w&bg=ffffff&fg=000000&s=1&c=20201002",
            "https://s0.wp.com/latex.php?latex=b%5E%2A&bg=ffffff&fg=000000&s=1&c=20201002",
            "https://s0.wp.com/latex.php?latex=b&bg=ffffff&fg=000000&s=1&c=20201002",
            "https://s0.wp.com/latex.php?latex=w&bg=ffffff&fg=000000&s=1&c=20201002",
            "https://s0.wp.com/latex.php?latex=b&bg=ffffff&fg=000000&s=1&c=20201002",
            "https://s0.wp.com/latex.php?latex=b+%3D+1.352%2C+w%3D%5B-0.233%2C+-0.269%5D&bg=ffffff&fg=000000&s=1&c=20201002",
            "https://beginningwithml.wordpress.com/wp-content/uploads/2019/03/index1.png?w=1100",
            "https://s0.wp.com/latex.php?latex=%5Cfrac%7B1%7D%7B%5Cleft%5CVert+w+%5Cright%5CVert%7D&bg=ffffff&fg=000000&s=1&c=20201002",
            "https://beginningwithml.wordpress.com/wp-content/uploads/2019/03/index2.png?w=1100",
            "https://s0.wp.com/latex.php?latex=y%5E%7B%28i%29%7D%5Cleft%28+w%5ET+x%5E%7B%28i%29%7D%2Bb+%5Cright%29+%5Cgeq+1&bg=ffffff&fg=000000&s=1&c=20201002",
            "https://s0.wp.com/latex.php?latex=g_i%28w%29+%3D+-y%5E%7B%28i%29%7D%5Cleft%28+w%5ET+x%5E%7B%28i%29%7D%2Bb+%5Cright%29+%2B+1+%5Cleq+0&bg=ffffff&fg=000000&s=1&c=20201002",
            "https://s0.wp.com/latex.php?latex=y%5E%7B%28i%29%7D%5Cleft%28+w%5ET+x%5E%7B%28i%29%7D%2Bb+%5Cright%29&bg=ffffff&fg=000000&s=1&c=20201002",
            "https://s0.wp.com/latex.php?latex=g_i%28w%29%3D0&bg=ffffff&fg=000000&s=1&c=20201002",
            "https://s0.wp.com/latex.php?latex=%5Clambda_i+%3E+0&bg=ffffff&fg=000000&s=1&c=20201002",
            "https://s0.wp.com/latex.php?latex=w&bg=ffffff&fg=000000&s=1&c=20201002",
            "https://s0.wp.com/latex.php?latex=%5Cmin_i+y%5E%7B%28i%29%7D%5Cleft%28+w%5ET+x%5E%7B%28i%29%7D%2Bb+%5Cright%29%3D1&bg=ffffff&fg=000000&s=1&c=20201002",
            "https://beginningwithml.wordpress.com/wp-content/uploads/2019/02/800px-svm_margin.png?w=379&h=368",
            "https://s0.wp.com/latex.php?latex=-b&bg=ffffff&fg=000000&s=1&c=20201002",
            "https://s0.wp.com/latex.php?latex=%5Clambda&bg=ffffff&fg=000000&s=1&c=20201002",
            "https://s0.wp.com/latex.php?latex=%5Cmathcal%7BL%7D%28w%2C+b%2C+%5Clambda%29+%3D+%5Cfrac%7B1%7D%7B2%7D+%5Cleft%5CVert+w+%5Cright%5CVert%5E2+-+%5Csum%5Climits_%7Bi%3D1%7D%5Em+%5Clambda_i+%5Cleft%28+y%5E%7B%28i%29%7D%28w%5ET+x%5E%7B%28i%29%7D%2Bb%29-1+%5Cright%29&bg=ffffff&fg=000000&s=1&c=20201002",
            "https://s0.wp.com/latex.php?latex=%5Cmax%5Cmin+%5Cmathcal%7BL%7D&bg=ffffff&fg=000000&s=1&c=20201002",
            "https://s0.wp.com/latex.php?latex=w&bg=ffffff&fg=000000&s=1&c=20201002",
            "https://s0.wp.com/latex.php?latex=b&bg=ffffff&fg=000000&s=1&c=20201002",
            "https://s0.wp.com/latex.php?latex=%5Clambda_i&bg=ffffff&fg=000000&s=1&c=20201002",
            "https://s0.wp.com/latex.php?latex=%5Clambda_i&bg=ffffff&fg=000000&s=1&c=20201002",
            "https://s0.wp.com/latex.php?latex=%5Cnabla_w+%5Cmathcal%7BL%7D%28w%2C+b%2C+%5Cboldsymbol%5Clambda%29+%3D+w+-+%5Csum%5Climits_%7Bi%3D1%7D%5Em+%5Clambda_i+y%5E%7B%28i%29%7Dx%5E%7B%28i%29%7D%3D0&bg=ffffff&fg=000000&s=1&c=20201002",
            "https://s0.wp.com/latex.php?latex=w+%3D+%5Csum%5Climits_%7Bi%3D1%7D%5Em+%5Clambda_i+y%5E%7B%28i%29%7Dx%5E%7B%28i%29%7D&bg=ffffff&fg=000000&s=1&c=20201002",
            "https://s0.wp.com/latex.php?latex=b&bg=ffffff&fg=000000&s=1&c=20201002",
            "https://s0.wp.com/latex.php?latex=%5Csum%5Climits_%7Bi%3D1%7D%5Em+%5Clambda_i+y%5E%7B%28i%29%7D+%3D+0&bg=ffffff&fg=000000&s=1&c=20201002",
            "https://s0.wp.com/latex.php?latex=%5Cbegin%7Baligned%7D+%5Cmathcal%7BL%7D%28w%2C+b%2C+%5Cboldsymbol%5Clambda%29+%26%3D+%5Cfrac%7B1%7D%7B2%7D%5Cleft%5CVert+w+%5Cright%5CVert%5E2+-+%5Csum%5Climits_%7Bi%3D1%7D%5Em+%5Clambda_i+%5Cleft%28+y%5E%7B%28i%29%7D+%28w%5ET+x%5E%7B%28i%29%7D%2Bb%29-1+%5Cright%29+%5C%5C+%26%3D+%5Cfrac%7B1%7D%7B2%7Dw%5ET+w+-+%5Csum%5Climits_%7Bi%3D1%7D%5Em+%5Clambda_i+y%5E%7B%28i%29%7D+%5Csum%5Climits_%7Bj%3D1%7D%5Em+%5Clambda_j+y%5E%7B%28j%29%7Dx%5E%7B%28j%29T%7Dx%5E%7B%28i%29%7D+-+%5Csum%5Climits_%7Bi%3D1%7D%5Em+%5Clambda_i+y%5E%7B%28i%29%7Db+%2B+%5Csum%5Climits_%7Bi%3D1%7D%5Em+%5Clambda_i+%5C%5C+%26%3D+%5Cfrac%7B1%7D%7B2%7D%5Csum%5Climits_%7Bi%3D1%7D%5Em+%5Clambda_i+y%5E%7B%28i%29%7Dx%5E%7B%28i%29T%7D+%5Csum%5Climits_%7Bj%3D1%7D%5Em+%5Clambda_j+y%5E%7B%28j%29%7Dx%5E%7B%28j%29%7D+-+%5Csum%5Climits_%7Bi%3D1%7D%5Em+%5Clambda_i+y%5E%7B%28i%29%7D+%5Csum%5Climits_%7Bj%3D1%7D%5Em+%5Clambda_j+y%5E%7B%28j%29%7Dx%5E%7B%28j%29T%7Dx%5E%7B%28i%29%7D+%2B+%5Csum%5Climits_%7Bi%3D1%7D%5Em+%5Clambda_i+%5C%5C+%26%3D+%5Cfrac%7B1%7D%7B2%7D%5Csum%5Climits_%7Bi%3D1%7D%5Em+%5Csum%5Climits_%7Bj%3D1%7D%5Em+%5Clambda_i+%5Clambda_j+y%5E%7B%28i%29%7Dy%5E%7B%28j%29%7D+x%5E%7B%28i%29T%7Dx%5E%7B%28j%29%7D+-+%5Csum%5Climits_%7Bi%3D1%7D%5Em+%5Csum%5Climits_%7Bj%3D1%7D%5Em+%5Clambda_i+%5Clambda_j+y%5E%7B%28i%29%7Dy%5E%7B%28j%29%7D+x%5E%7B%28j%29T%7Dx%5E%7B%28i%29%7D+%2B+%5Csum%5Climits_%7Bi%3D1%7D%5Em+%5Clambda_i+%5C%5C+%26%3D+%5Csum%5Climits_%7Bi%3D1%7D%5Em+%5Clambda_i+-+%5Cfrac%7B1%7D%7B2%7D%5Csum%5Climits_%7Bi%3D1%7D%5Em+%5Csum%5Climits_%7Bj%3D1%7D%5Em+%5Clambda_i+%5Clambda_j+y%5E%7B%28i%29%7Dy%5E%7B%28j%29%7D+x%5E%7B%28i%29T%7Dx%5E%7B%28j%29%7D+%5Cend%7Baligned%7D&bg=ffffff&fg=000000&s=1&c=20201002",
            "https://s0.wp.com/latex.php?latex=%5Cleft%5CVert+w+%5Cright%5CVert%5E2+%3D+w%5ET+w&bg=ffffff&fg=000000&s=1&c=20201002",
            "https://s0.wp.com/latex.php?latex=w&bg=ffffff&fg=000000&s=1&c=20201002",
            "https://s0.wp.com/latex.php?latex=%5Cbegin%7Baligned%7D+%5Cmax%5Climits_%7B%5Cboldsymbol%5Clambda%7D+%26+%5Csum%5Climits_%7Bi%3D1%7D%5Em+%5Clambda_i+-+%5Cfrac%7B1%7D%7B2%7D%5Csum%5Climits_%7Bi%3D1%7D%5Em+%5Csum%5Climits_%7Bj%3D1%7D%5Em+%5Clambda_i+%5Clambda_j+y%5E%7B%28i%29%7Dy%5E%7B%28j%29%7D+x%5E%7B%28i%29T%7Dx%5E%7B%28j%29%7D+%5C%5C+%5Ctext%7Bs.t.+%7D+%26+%5Clambda_i+%5Cgeq+0+%5C%5C+%26+%5Csum%5Climits_%7Bi%3D1%7D%5Em+%5Clambda_i+y%5E%7B%28i%29%7D+%3D+0+%5Cend%7Baligned%7D&bg=ffffff&fg=000000&s=1&c=20201002",
            "https://s0.wp.com/latex.php?latex=%5Cboldsymbol%5Clambda&bg=ffffff&fg=000000&s=1&c=20201002",
            "https://s0.wp.com/latex.php?latex=w&bg=ffffff&fg=000000&s=1&c=20201002",
            "https://s0.wp.com/latex.php?latex=b&bg=ffffff&fg=000000&s=1&c=20201002",
            "https://s0.wp.com/latex.php?latex=y%5E%7B%28i%29%7D%5Cleft%28+w%5ET+x%5E%7B%28i%29%7D%2Bb+%5Cright%29+%5Cgeq+1&bg=ffffff&fg=000000&s=1&c=20201002",
            "https://s0.wp.com/latex.php?latex=y%5E%7B%28i%29%7D%3D-1&bg=ffffff&fg=000000&s=1&c=20201002",
            "https://s0.wp.com/latex.php?latex=w%5ET+x%5E%7B%28i%29%7D%2Bb+%5Cleq+-1&bg=ffffff&fg=000000&s=1&c=20201002",
            "https://s0.wp.com/latex.php?latex=b+%5Cleq+-1+-+w%5ET+x%5E%7B%28i%29%7D&bg=ffffff&fg=000000&s=1&c=20201002",
            "https://s0.wp.com/latex.php?latex=w%5ET+x%5E%7B%28i%29%7D%2Bb+%5Cgeq+1&bg=ffffff&fg=000000&s=1&c=20201002",
            "https://s0.wp.com/latex.php?latex=b+%5Cgeq+1+-+w%5ET+x%5E%7B%28i%29%7D&bg=ffffff&fg=000000&s=1&c=20201002",
            "https://s0.wp.com/latex.php?latex=%5Cleft%5CVert+w+%5Cright%5CVert&bg=ffffff&fg=000000&s=1&c=20201002",
            "https://s0.wp.com/latex.php?latex=b%5E%2A+%3D+-%5Cfrac%7B%5Cmax%5Climits_%7Bi%3Ay%5E%7B%28i%29%7D+%3D+-1%7D+w%5ET+x%5E%7B%28i%29%7D+%2B+%5Cmin%5Climits_%7Bi%3Ay%5E%7B%28i%29%7D+%3D+1%7D+w%5ET+x%5E%7B%28i%29%7D%7D%7B2%7D&bg=ffffff&fg=000000&s=1&c=20201002",
            "https://beginningwithml.wordpress.com/wp-content/uploads/2019/02/image_2019-03-31_16-01-09.png?w=351&h=282",
            "https://s0.wp.com/latex.php?latex=w%5ET+x%2Bb&bg=ffffff&fg=000000&s=1&c=20201002",
            "https://s0.wp.com/latex.php?latex=%5Csum%5Climits_%7Bi%3D1%7D%5Em+%5Clambda_i+y%5E%7B%28i%29%7Dx%5E%7B%28i%29T%7Dx%2Bb&bg=ffffff&fg=000000&s=1&c=20201002",
            "https://s0.wp.com/latex.php?latex=%5Cboldsymbol%5Clambda&bg=ffffff&fg=000000&s=1&c=20201002",
            "https://s0.wp.com/latex.php?latex=%5Cbegin%7Baligned%7D+%26%5Cmin%5Climits_w+f%28w%29+%5C%5C+%26+%5Ctext%7Bs.t.+%7D+h_i%28w%29+%3D+0%2C+i+%3D+1%2C+2%2C+%5Cldots%2C+l+%5Cend%7Baligned%7D&bg=ffffff&fg=000000&s=1&c=20201002",
            "https://s0.wp.com/latex.php?latex=%5Cmathcal%7BL%7D%28w%2C+%5Cboldsymbol+%5Clambda%29+%3D+f%28w%29+%2B+%5Csum%5Climits_%7Bi%3D1%7D%5El+%5Clambda_i+h_i%28w%29&bg=ffffff&fg=000000&s=1&c=20201002",
            "https://s0.wp.com/latex.php?latex=%5Clambda-i&bg=ffffff&fg=000000&s=1&c=20201002",
            "https://s0.wp.com/latex.php?latex=%5Cbegin%7Baligned%7D+%5Cfrac%7B%5Cpartial+%5Cmathcal%7BL%7D%7D%7B%5Cpartial+w%7D+%26%3D+0+%5C%5C+%5Cfrac%7B%5Cpartial+%5Cmathcal%7BL%7D%7D%7B%5Cpartial+%5Clambda_i%7D+%26%3D+0+%5Cend%7Baligned%7D&bg=ffffff&fg=000000&s=1&c=20201002",
            "https://s0.wp.com/latex.php?latex=%5Clambda_i&bg=ffffff&fg=000000&s=1&c=20201002",
            "https://s0.wp.com/latex.php?latex=w&bg=ffffff&fg=000000&s=1&c=20201002",
            "https://s0.wp.com/latex.php?latex=%5Cbegin%7Baligned%7D+%5Cmin%5Climits_w+%26f%28w%29+%5C%5C%C2%A0+%5Ctext%7Bs.t.+%7D+%26g_i%28w%29+%5Cleq+0%2C+i+%3D+1%2C+2%2C+%5Cldots%2C+k+%5C%5C+%26+h_i%28w%29+%3D+0%2C+i+%3D+1%2C+2%2C+%5Cldots%2C+l+%5Cend%7Baligned%7D&bg=ffffff&fg=000000&s=1&c=20201002",
            "https://s0.wp.com/latex.php?latex=%5Cmathcal%7BL%7D%28w%2C+%5Cboldsymbol+%5Clambda%2C+%5Cboldsymbol+%5Cmu%29+%3D+f%28w%29+%2B+%5Csum%5Climits_%7Bi%3D1%7D%5Ek+%5Clambda_i+g_i%28w%29+%2B+%5Csum%5Climits_%7Bi%3D1%7D%5El+%5Cmu_i+h_i%28w%29&bg=ffffff&fg=000000&s=1&c=20201002",
            "https://s0.wp.com/latex.php?latex=%5CTheta_p%28w%29+%3D+%5Cmax%5Climits_%7B%5Csubstack%7B%5Cboldsymbol%5Clambda%2C+%5Cboldsymbol+%5Cmu+%5C%5C+%5Clambda_i+%5Cgeq+0%7D%7D+%5Cmathcal%7BL%7D%28w%2C+%5Cboldsymbol%5Clambda%2C+%5Cboldsymbol%5Cmu%29&bg=ffffff&fg=000000&s=1&c=20201002",
            "https://s0.wp.com/latex.php?latex=p%5E%2A+%3D+%5Cmin%5Climits_w+%5Cmax%5Climits_%7B%5Csubstack%7B%5Cboldsymbol%5Clambda%2C+%5Cboldsymbol+%5Cmu+%5C%5C+%5Clambda_i+%5Cgeq+0%7D%7D+%5Cmathcal%7BL%7D%28w%2C+%5Cboldsymbol%5Clambda%2C+%5Cboldsymbol%5Cmu%29+%3D+%5Cmin%5Climits_w+%5CTheta_p%28w%29&bg=ffffff&fg=000000&s=1&c=20201002",
            "https://s0.wp.com/latex.php?latex=f%28w%29&bg=ffffff&fg=000000&s=1&c=20201002",
            "https://s0.wp.com/latex.php?latex=g_i%28w%29+%3E+0&bg=ffffff&fg=000000&s=1&c=20201002",
            "https://s0.wp.com/latex.php?latex=%5Cmathcal%7BL%7D&bg=ffffff&fg=000000&s=1&c=20201002",
            "https://s0.wp.com/latex.php?latex=%5Clambda_i&bg=ffffff&fg=000000&s=1&c=20201002",
            "https://s0.wp.com/latex.php?latex=%5Cinfty&bg=ffffff&fg=000000&s=1&c=20201002",
            "https://s0.wp.com/latex.php?latex=i&bg=ffffff&fg=000000&s=1&c=20201002",
            "https://s0.wp.com/latex.php?latex=h_i%28w%29+%5Cneq+0&bg=ffffff&fg=000000&s=1&c=20201002",
            "https://s0.wp.com/latex.php?latex=%5Cmu_i&bg=ffffff&fg=000000&s=1&c=20201002",
            "https://s0.wp.com/latex.php?latex=%2B%5Cinfty&bg=ffffff&fg=000000&s=1&c=20201002",
            "https://s0.wp.com/latex.php?latex=-%5Cinfty&bg=ffffff&fg=000000&s=1&c=20201002",
            "https://s0.wp.com/latex.php?latex=h_i%28w%29&bg=ffffff&fg=000000&s=1&c=20201002",
            "https://s0.wp.com/latex.php?latex=%5CTheta_p%28w%29+%3D+f%28w%29&bg=ffffff&fg=000000&s=1&c=20201002",
            "https://s0.wp.com/latex.php?latex=%5CTheta_p%28w%29+%3D+%5Cinfty&bg=ffffff&fg=000000&s=1&c=20201002",
            "https://s0.wp.com/latex.php?latex=%5CTheta_p%28w%29&bg=ffffff&fg=000000&s=1&c=20201002",
            "https://s0.wp.com/latex.php?latex=%5Clambda%2C+%5Cmu&bg=ffffff&fg=000000&s=1&c=20201002",
            "https://s0.wp.com/latex.php?latex=p&bg=ffffff&fg=000000&s=1&c=20201002",
            "https://s0.wp.com/latex.php?latex=d%5E%2A+%3D+%5Cmax%5Climits_%7B%5Csubstack%7B%5Cboldsymbol%5Clambda%2C+%5Cboldsymbol+%5Cmu+%5C%5C+%5Clambda_i+%5Cgeq+0%7D%7D%5Cmin%5Climits_w+%5Cmathcal%7BL%7D%28w%2C+%5Cboldsymbol%5Clambda%2C+%5Cboldsymbol%5Cmu%29+%3D+%5Cmin%5Climits_w+%5CTheta_p%28w%29&bg=ffffff&fg=000000&s=1&c=20201002",
            "https://s0.wp.com/latex.php?latex=%5Cmax%5Cmin%5Cmathcal%7BL%7D%28%5Cldots%29+%5Cleq+%5Cmin%5Cmax%5Cmathcal%7BL%7D%28%5Cldots%29&bg=ffffff&fg=000000&s=1&c=20201002",
            "https://s0.wp.com/latex.php?latex=f&bg=ffffff&fg=000000&s=1&c=20201002",
            "https://s0.wp.com/latex.php?latex=g_i&bg=ffffff&fg=000000&s=1&c=20201002",
            "https://s0.wp.com/latex.php?latex=h_i&bg=ffffff&fg=000000&s=1&c=20201002",
            "https://s0.wp.com/latex.php?latex=h_i%28w%29+%3D+%5Calpha%5ET+w+%2B+%5Cbeta&bg=ffffff&fg=000000&s=1&c=20201002",
            "https://s0.wp.com/latex.php?latex=g_i&bg=ffffff&fg=000000&s=1&c=20201002",
            "https://s0.wp.com/latex.php?latex=w&bg=ffffff&fg=000000&s=1&c=20201002",
            "https://s0.wp.com/latex.php?latex=g_i%28w%29+%3C+0&bg=ffffff&fg=000000&s=1&c=20201002",
            "https://s0.wp.com/latex.php?latex=w%5E%2A%2C+%5Cboldsymbol%5Clambda%5E%2A%2C+%5Cboldsymbol%5Cmu%5E%2A&bg=ffffff&fg=000000&s=1&c=20201002",
            "https://s0.wp.com/latex.php?latex=w%5E%2A&bg=ffffff&fg=000000&s=1&c=20201002",
            "https://s0.wp.com/latex.php?latex=%5Cboldsymbol%5Clambda%5E%2A%2C+%5Cboldsymbol%5Cmu%5E%2A&bg=ffffff&fg=000000&s=1&c=20201002",
            "https://s0.wp.com/latex.php?latex=%5Cbegin%7Baligned%7D+%5Clambda_i%5E%2A+g_i%28w%5E%2A%29+%26%3D+0+%5C%5C+g_i%28w%5E%2A%29+%26%5Cleq+0+%5C%5C+%5Clambda_i%5E%2A+%26%5Cgeq+0+%5Cend%7Baligned%7D&bg=ffffff&fg=000000&s=1&c=20201002",
            "https://s0.wp.com/latex.php?latex=g_i%28w%5E%2A%29&bg=ffffff&fg=000000&s=1&c=20201002",
            "https://i.creativecommons.org/l/by-nc-sa/4.0/88x31.png",
            "https://beginningwithml.wordpress.com/wp-content/uploads/2024/01/logo_appcair.jpg",
            "https://beginningwithml.wordpress.com/wp-content/uploads/2024/01/logo_bits.jpeg?w=150",
            "https://s2.wp.com/i/logo/wpcom-gray-white.png",
            "https://s2.wp.com/i/logo/wpcom-gray-white.png",
            "https://pixel.wp.com/b.gif?v=noscript"
        ],
        "movies": [],
        "keywords": [],
        "meta_keywords": [
            ""
        ],
        "tags": null,
        "authors": [
            "Rahul Yedida"
        ],
        "publish_date": "2019-04-19T08:45:17+00:00",
        "summary": "",
        "meta_description": "Read all of the posts by Rahul Yedida on Beginning with ML",
        "meta_lang": "en",
        "meta_favicon": "https://s1.wp.com/i/favicon.ico",
        "meta_site_name": "Beginning with ML",
        "canonical_link": "http://theproductivityapp.wordpress.com",
        "text": "We’ll now discuss a class of clustering algorithms called hierarchical clustering algorithms. In hierarchical clustering, you don’t immediately build a set number of clusters (although you certainly could, as we’ll discuss). Instead, you make a hierarchy of clusters.\n\nAs you can see from the image on the left, hierarchical clustering forms nested clusters. This is very different from the previous clustering approaches that we discussed. On the right is the corresponding dendogram that shows the point numbers on the x-axis and the distance on the y-axis. Dendograms are another way of visualizing how the cluster hierarchy was formed. Broadly, hierarchical clustering algorithms are of two types:\n\nAgglomerative hierarchical clustering algorithms are bottom-up algorithms that start by treating every point as a unique cluster, and going up the tree (dendogram) to create one final cluster.\n\nDivisive hierarchical clustering algorithms work top-down, considering all points as one single cluster, and splitting this iteratively.\n\nWe will only discuss agglomerative hierarchical clustering here. In agglomerative hierarchical clustering, you start by treating every point as its own cluster. You then combine clusters to get bigger clusters, until you end up with a single cluster containing all the points.\n\nHow do you combine clusters? In the beginning, when all the clusters have only one point each, it’s trivial: you want the distance between points in the same cluster to be small (this is called cohesion) and the distance between points in different clusters to be larger (this is called separation); therefore, you merge the points that are closest to each other. But what about future steps? There are several heuristics we could use [1]:\n\nSingle Linkage or MIN: In this case, the distance between two clusters is the minimum of the distances between any two points, each taken from one of the clusters. This handles non-elliptical shapes, but is sensitive to noise and outliers.\n\nComplete Linkage or MAX: This is like the first one, but we instead consider the maximum distance. This is less sensitive to noise and outliers, but favors globular shapes. The diagram we showed on top uses this approach.\n\nGroup Average: In this, you take average of the distances between all possible pairs of points, with one point from each cluster.\n\nWard’s method: In Ward’s method, you combine the clusters that result in the minimum within-cluster variance [2]. Another way of phrasing it is that you combine the clusters that result in the minimum increase in the squared error. How do we compute this? Let’s define the error sum of squares, as follows. Let denote the th feature of the th training example, which belongs to the th cluster. This is the same notation we used in the first post, with the added notation for cluster number. We’ll use to denote the th feature of the mean of the th cluster. Then, the error sum of squares is defined as\n\nThat is, we find the squared difference between each feature value of each point in every cluster and the corresponding feature value for the cluster’s mean, and add up all these squared differences. We merge clusters so that the ESS value after merging is minimum [3].\n\nThe Lance-Williams algorithms\n\nIn the naive implementation of agglomerative hierarchical clustering, you compute the distance matrix between each cluster at each step. The Lance-Williams algorithms are a family of agglomerative hierarchical clustering algorithms which are represented by a recursive formula for computing cluster distances at each step. Any hierarchical clustering technique that can be expressed using the Lance-Williams formula does not need to keep the original data points [1].\n\nWe’ll use Wikipedia’s notation here [3]. Suppose that are the next clusters to be merged. Let be the distance (using any method discussed above) between and . Further, let be the distance between the merged cluster and cluster . An algorithm belongs to the Lance-Williams family if the distance can be recursively computed as\n\nAll the methods we discussed above belong to the Lance-Williams family. This table summarizes the coefficient values [1].\n\nClustering Method Single Linkage 0.5 0.5 0 -0.5 Complete Linkage 0.5 0.5 0 0.5 Group Average 0 0 Ward’s method 0\n\nImplementing hierarchical clustering\n\nNow let’s implement hierarchical clustering. We’ll use the Lance-Williams formula. We’ll do this step-by-step. Rather than show you the function to do the clustering, I’ll instead show you how to manually run the steps, and then develop the function. This will prove to be more helpful. We will not implement a function to compute the distance matrix. We’ll simply use a built-in function to do that for us. First, let’s start with a dataset (from [1]).\n\nx = [[0.4, 0.53], [0.22, 0.38], [0.35, 0.32], [0.26, 0.19], [0.08, 0.41], [0.45, 0.30]]\n\nWe’ll compute the distance matrix now:\n\nfrom sklearn.metrics import pairwise_distances d1 = pairwise_distances(x)\n\nLet’s write functions to implement the Lance-Williams update formula:\n\ndef update(dist, i, j, k, alpha1, alpha2, beta, gamma): return alpha1 * dist[i][k] + alpha2 * dist[j][k] + beta * dist[i][j] + \\ gamma * abs(dist[i][k] - dist[j][k])\n\nWe’ll only implement single linkage here, but the others should be easy enough:\n\ndef single_link(dist, i, j, k): return update(dist, i, j, k, 0.5, 0.5, 0, -0.5)\n\nNext, we need to combine the two clusters that have the minimum distance from the matrix. Basically, we just need to use argmin. However, the diagonal elements of our matrix are 0, and therefore the least. So we’ll make these larger values to get the right argmin values.\n\nd1[d1 == 0] = np.max(d1) + 1\n\nI arbitrarily added 1; you could multiply by 2 or 5 or 10, anything at all, really, but be sure that the distance matrix values will never possible reach that high. Now we can get the argmin, or the indices of the least value. However, since we have a 2D distance matrix, we’ll need to use another function as well to get the correct indices:\n\nindices = np.unravel_index(np.argmin(d1), d1.shape)\n\nThe argmin function would’ve given us a single number–it flattens our 2D array into a 1D array and then computes the argmin value; the unravel_index function gives us the corresponding 2D indices for the minimum element.\n\nNow that we know the indices of the minimum element, these indices correspond to what clusters we’re merging. In the above step, you should’ve gotten (2, 5) as the indices. This means we’re merging the 3rd and 6th clusters (since Python uses a 0-based indexing for arrays). What we’ll do now is this. We’ll remove the 3rd and 6th clusters entirely (that is, the 3rd and 6th rows and columns). We’ll only remove these rows and columns in a copy of the distance matrix–we will still need the original one to use the Lance-Williams formula. After deleting in the copy, we’ll add one row and one column corresponding to the combined cluster. Since the distance matrix is symmetric, we only have to compute the values of the row, and fill the column with the same values. These distance values can be computed using the Lance-Williams formula. Let’s proceed.\n\nd2 = np.delete(d1, indices[0], axis=0) d2 = np.delete(d2, indices[1] - 1, axis=0) d2 = np.delete(d2, indices[0], axis=1) d2 = np.delete(d2, indices[1] - 1, axis=1)\n\nAbove, we’ve deleted the 3rd and 6th rows and columns (axis=0 means rows, axis=1 means columns). Note that np.delete does not delete in-place; it returns a new array, so in the first statement we’re making a new array variable, and in the subsequent lines, we’re modifying this new array. Thus, the original distance matrix is unchanged. If you print out d2 now, you’ll find that it’s a 4×4 matrix, and you should see that it’s obtained by removing the 3rd and 6th rows and columns.\n\nNow, let’s add a row and a column in the beginning, corresponding to the combined cluster.\n\nd2 = np.insert(d2, 0, 0, axis=1) d2 = np.insert(d2, 0, 0, axis=0)\n\nWe’ve filled this row and column with zeros (the third argument). Finally, let’s fill these with the real values using the Lance-Williams formula:\n\ncur_index = 0 for i in range(len(d)): if i not in indices: cur_index += 1 d2[0][cur_index] = d2[cur_index][0] = single_link(d1, indices[0], indices[1], i) print(cur_index, single_link(d1, indices[0], indices[1], i))\n\nWe use a cur_index variable to get the correct index for the value that we’re filling in. We skip the indices that we had removed (hence the if condition), and we call the function that we wrote earlier using the indices, since those are the i and j values. The value of k is the cluster that we want to compute the distance of the new cluster to, so we pass in our loop variable, i (perhaps confusingly named).\n\nFinally, let’s make sure our very first matrix element is not zero (because we don’t want argmin to point to it).\n\nd2[0][0] = d2[1][1]\n\nLet’s now build a mapping of what the indices in the new 5 x 5 matrix mean with respect to the original clusters. We removed the 3rd and 6th indices, and added one in the beginning corresponding to the new cluster. Therefore, the first index is now the (3, 6) cluster, and the rest of the indices are the first, second, fourth, and fifth clusters (or points).\n\nNow that we’ve gone over the steps, let’s write out the clustering function.\n\ndef hierarchical_cluster(points, metric='euclidean'): dist = pairwise_distances(points, metric=metric) print('Distance matrix:') print(dist) dist[dist == 0] = np.max(dist) + 1 # Maintain a list of clusters clusters = [str(i) for i in range(1, len(dist) + 1)] # Convert to numpy array and set dtype of appropriate size dtype = '<U' + str(5 * sum([len(i) for i in clusters])) clusters = np.array(clusters, dtype=dtype) for _ in range(len(dist) - 1): # Find the indices of the minimum element indices = np.unravel_index(np.argmin(dist), dist.shape) # Update the cluster list c1 = clusters[indices[0]] c2 = clusters[indices[1]] new_cluster = '(' + c1 + ', ' + c2 + ')' print(new_cluster) clusters = np.delete(clusters, indices) clusters = np.insert(clusters, 0, new_cluster, axis=0) print('Clusters:', clusters) # Build a new distance matrix: start by removing the older points new_dist = np.delete(dist, indices[0], axis=0) new_dist = np.delete(new_dist, indices[1] - 1, axis=0) new_dist = np.delete(new_dist, indices[0], axis=1) new_dist = np.delete(new_dist, indices[1] - 1, axis=1) # Next, add the combined cluster at the beginning. Start by # creating spaces for the distances between this new cluster # and all other clusters. new_dist = np.insert(new_dist, 0, 0, axis=1) new_dist = np.insert(new_dist, 0, 0, axis=0) # Fill in values of the distances using the Lance-Williams equation cur_index = 0 for i in range(len(dist)): if i not in indices: cur_index += 1 new_dist[0][cur_index] = new_dist[cur_index][0] = single_link(dist, indices[0], indices[1], i) new_dist[0][0] = np.max(new_dist) + 1 dist = new_dist return clusters\n\nThere’s a little extra stuff going on, so let’s go over this. We first compute the pairwise distance matrix, using the Euclidean distance metric. Alternatively, we could pass any other metric, like the Manhattan distance, as well.\n\nWe maintain a list of clusters that we made. Initially, this is just a list of individual clusters. We convert it to a numpy array, but there’s a small catch. Numpy will only allocate enough space for each array element so that the current largest element (here, the longest string) will fit. However, when we merge clusters, the strings will become longer, so we need to tell numpy to change the data type to a larger size. We tell it to allocate us 5 times the total length of all the strings. This again is simply a heuristic. You could use 2 times instead, and it will probably work. When merging clusters, we remove the two individual clusters from our list, and then add a merged string at the beginning of our cluster list.\n\nThe rest of the function is what we’ve already seen. Except the last line of the loop. Here, we set the dist matrix to the new distance matrix. This is because when we compute the next step distance matrix, we only require the previous one, not the initial one. In fact, we don’t even need the original points.\n\nLet’s run our function on our data points.\n\nhierarchical_cluster([[0.4, 0.53], [0.22, 0.38], [0.35, 0.32], [0.26, 0.19], [0.08, 0.41], [0.45, 0.30]])\n\nIn my system, I get the output below.\n\nDistance matrix: [[0. 0.23430749 0.21587033 0.36769553 0.34176015 0.23537205] [0.23430749 0. 0.14317821 0.19416488 0.14317821 0.24351591] [0.21587033 0.14317821 0. 0.15811388 0.28460499 0.10198039] [0.36769553 0.19416488 0.15811388 0. 0.28425341 0.21954498] [0.34176015 0.14317821 0.28460499 0.28425341 0. 0.38600518] [0.23537205 0.24351591 0.10198039 0.21954498 0.38600518 0. ]] (3, 6) Clusters: ['(3, 6)' '1' '2' '4' '5'] (2, 5) Clusters: ['(2, 5)' '(3, 6)' '1' '4'] ((2, 5), (3, 6)) Clusters: ['((2, 5), (3, 6))' '1' '4'] (((2, 5), (3, 6)), 4) Clusters: ['(((2, 5), (3, 6)), 4)' '1'] ((((2, 5), (3, 6)), 4), 1) Clusters: ['((((2, 5), (3, 6)), 4), 1)']\n\nWe can clearly see what clusters were merged at each step. Finally, we end up with one single cluster.\n\nSources\n\n[1] Tan, P.N., 2018. Introduction to data mining. Pearson Education India.\n\n[2] Ward’s method – Wikipedia\n\n[3] STAT 505 – Penn State University\n\nLet’s look at another heuristic for deciding what points constitute a cluster. DBSCAN (Density-Based Spatial Clustering of Applications with Noise) is a density-based clustering approach. This means that points that are packed together are put into a cluster. How is this different from k-Means? In k-Means, you have a fixed number of clusters, and every point is assigned to one. This is not necessarily the case with DBSCAN. If DBSCAN finds that a point is not close enough to any neighboring point to satisfy a density requirement, it’s put in an individual cluster.\n\nDBSCAN identifies three types of points:\n\nCore points are located in the interior of a high-density region (cluster).\n\nBorder points are located on the edge of a cluster. They are not core points, but fall in the neighborhood of (at least) one.\n\nNoise points are points that are neither core points nor border points.\n\nCore points are identified by two user-specified parameters (as opposed to only one parameter in k-Means): eps and minPts. Within a point’s neighborhood of radius eps, if there are at least minPts number of points, then it is a core point.\n\nLet’s look at this image. All the red points are core points. For each of these points, within a neighborhood of radius eps, there are at least minPts=4 points. We should clarify a confusion here. Our definition does not say at least minPts other points, it only says at least minPts points. So if you look at the right-most red point, it has only 3 other points within an eps-neighborhood, but we also count the point itself. The yellow points are border points, and the blue point is a noise point.\n\nAlgorithm\n\nWe’ll show two algorithms here–a short version and a longer, more detailed version.\n\nShort version\n\nThis is taken from source [1].\n\nALGORITHM DBSCAN(eps, minPts): ============================== 1. Label all points as core, border, or noise points. 2. Eliminate all noise points. 3. Put an edge between all core points that are within eps of each other. 4. Assign each border point to one of the clusters of its associated core points.\n\nThis is pretty easy to follow. Core points that are within eps of each other are put in a cluster, and border points are assigned to any of the clusters they belong to.\n\nLonger version\n\nThis is adapted from Wikipedia’s article [2].\n\nALGORITHM DBSCAN(eps, minPts): ============================== 1. C = 0 2. for each point P: 2.1 if label(P) is not null then continue 2.2 Neighbors N = RangeQuery(P, eps) 2.3 if |N| < minPts then: 2.3.1 label(P) = Noise 2.3.2 continue 2.4 C = C + 1 2.5 label(P) = C 2.6 Set S = N - {P} 2.7 for each point Q in S: 2.7.1 if label(Q) = Noise then label(Q) = C 2.7.2 if label(Q) is not null then continue 2.7.3 label(Q) = C 2.7.4 Neighbors N = RangeQuery(Q, eps) 2.7.5 if |N| >= minPts then S = S U N\n\nThis algorithm essentially does the same thing, taking a different approach. Rather than label points as core or border, it numbers cluster. The algorithm iterates over every point in the training set. First, it labels points. In step 2.1, we check if the point has been previously labeled. If so, we don’t need to process it again. Otherwise, we find all neighbors in an eps radius, and if the number of points in this neighborhood is less than minPts, this point is labeled a noise point, and we move on to the next point (note that continue moves to the next iteration of the loop). Note that these points labeled as Noise could be border points too. We’ll fix this in a later step (2.7.1). Now that we’ve ascertained that the current point is neither a noise nor a border point, it’s a core point that belongs to a cluster. We increment the current cluster number and assign the current point to it. Next, we get all the points only in the neighborhood of the point, which does not include the point itself. If we’ve previously mislabeled this as a Noise point, we now know it’s a border point, and label it in the current cluster. If a point in this neighborhood has previously been labeled, we move on to the next one. Otherwise, if it’s unlabeled, we know it’s a border point, so we label it now (step 2.7.3). We then add the points from the neighborhood of point Q to this neighborhood set and repeat. Roughly, this corresponds to steps 3 and 4 of the shorter version of the algorithm.\n\nSelecting the parameters\n\nNow, there is the issue of finding eps and minPts. We’ll rely on the idea that for any core point, the kth nearest neighbors are roughly at the same distance. Therefore, we will look at the distances of points to their kth nearest neighbors [1]. For points within a cluster, this distance will be small if is less than the number of points in the cluster. If the cluster densities do not vary too much, there should not be much variation in these distances. For noise points, however, this distance will be much larger. Therefore, if we sort points according to their distance from their kth nearest neighbor, and plot this, we will see a sharp increase at a suitable value for eps. We then simply take the value of k as minPts. k = 4 is a reasonable choice for most 2D datasets. In the plot below, a value of eps around 7 seems reasonable.\n\nPractical considerations\n\nWhen does DBSCAN work, and when does it fail? Because it uses a density-based approach, DBSCAN is resistant to noise, unlike k-Means. It can also handle clusters of different shapes and sizes, since it only looks at density. k-Means, on the other hand, tries to get globular clusters. On the other hand, DBSCAN cannot handle clusters of varying densities. Further, if the data is high-dimensional, it becomes difficult to find a good distance metric to cluster the data properly. Lastly, DBSCAN is quite sensitive to the choice of parameters, and small changes in the parameter values can create significantly different results.\n\nAdditional information\n\nMore on DBSCAN can be found in Dr. Saha’s notes on the topic.\n\nSources\n\n[1] Tan, P.N., 2018. Introduction to data mining. Pearson Education India.\n\n[2] DBSCAN – Wikipedia\n\nk-Means clustering is one of the easiest methods of clustering, so this will be a rather short post. We will discuss the algorithm and an implementation in Python, and show a proof of convergence.\n\nAlgorithm\n\nThe k-Means clustering algorithm creates clusters based on proximity of points. You specify the number of clusters you want, and it creates said number of clusters. It starts by choosing random centroids–points that will be the centers of each cluster. Then, it finds points that are close to each of these chosen centroids, and declares these groups of points as the clusters. Then, for each declared cluster, it recomputes the centroid–this means that the centroids will now change. This process repeats until the clusters do not change. Let’s put this in less verbose terms, as an algorithm.\n\nALGORITHM k-MEANS(X, k): ======================== 1. Generate k random centroids, each with the same dimension as the points in X 2. repeat 2.1 for each x_i in X: 2.1.1 Find the centroid C_j that's closest to x_i 2.1.2 Assign x_i to cluster j 2.2 for each cluster j: 2.2.1 Compute the centroid of the cluster 2.2.2 Update the centroid with the new value until the centroids do not change\n\nPretty simple! The implementation is equally easy. Let’s now write up code for this.\n\nCode\n\nWe’ll only use numpy for this code; we won’t need anything else. Let’s feed in some data:\n\nx = np.array([[2, 10], [2, 5], [8, 4], [5, 8], [7, 5], [6, 4], [1, 2], [4, 9]])\n\nThis gives us 8 points in a 2D space. Now, we start by picking random centroids. You can pick them any way you like; for simplicity, we’ll simply pick some of the data points themselves:\n\nC = np.array([[2, 10], [5, 8], [1, 2]])\n\nNow we have the data and three initial centroids (that is, we’ll create three clusters). Let’s now write some helper functions. First, we need to define the distance between 2 points in this 2D space. We’ll use the L1 distance, which is the sum of the magnitudes of the differences between each co-ordinate (that is, we take the differences of each co-ordinate, ignoring the sign, and add these differences up).\n\ndef distance(a, b): return np.sum(np.abs(a - b))\n\nQuite easy. Our next function will use this to tell us which cluster a given point is nearest to.\n\ndef min_dist(a): return np.argmin([distance(a, C[i]) for i in range(3)])\n\nAgain, we can do this in a single statement. We use a list comprehension to get the distance of the given point from each of the three clusters, and use argmin to give us the index of this cluster. Next, given a list of points, let’s find the clusters they each belong to:\n\ndef assign_clusters(L): return np.array([min_dist(x) for x in L])\n\nThis simply builds on the previous function. Nothing too fancy. Now that we have a list that tells us what cluster each point is in, we can write a convenience function that gets all the clusters:\n\ndef get_clusters(arr): return [np.where(arr == i)[0] for i in range(3)]\n\nWe will pass this the result of the previous function, and this essentially splits that result into (in our case), three lists that each have the indices of the points that belong to that particular cluster. Finally, let’s write a function to compute the mean of a cluster:\n\ndef get_means(arr): return [np.mean(x[i], axis=0) for i in arr]\n\nFinally, we can implement our loop using these functions:\n\nwhile True: clusters = get_clusters(assign_clusters(x)) new_c = get_means(clusters) if np.array_equal(new_c, np.array(C)): break else: C = new_c print('Clusters:', [[list(x[i]) for i in cluster] for cluster in clusters]) print('Centroids:', [list(x) for x in new_c]) print('======================')\n\nAll we’re really doing is using the functions that we defined above. We first get the list of clusters (rather, a list of lists, each containing indices of points belonging to that cluster). We then compute the means (centroids) of each cluster. If the old centroids and the new ones are the same, we’re done. Otherwise, we update the centroids list, print out the new values, and repeat. You should get this output:\n\nClusters: [[[2, 10]], [[8, 4], [5, 8], [7, 5], [6, 4], [4, 9]], [[2, 5], [1, 2]]] Centroids: [[2.0, 10.0], [6.0, 6.0], [1.5, 3.5]] ====================== Clusters: [[[2, 10], [4, 9]], [[8, 4], [5, 8], [7, 5], [6, 4]], [[2, 5], [1, 2]]] Centroids: [[3.0, 9.5], [6.5, 5.25], [1.5, 3.5]] ====================== Clusters: [[[2, 10], [5, 8], [4, 9]], [[8, 4], [7, 5], [6, 4]], [[2, 5], [1, 2]]] Centroids: [[3.6666666666666665, 9.0], [7.0, 4.333333333333333], [1.5, 3.5]] ======================\n\nAfter the third iteration, it seems the centroids do not change, so the algorithm has converged (our code does not print the same centroid the second time, hence you don’t see the repetition here). We can show an animation of this process, too!\n\nimport matplotlib.pyplot as plt from matplotlib import animation, rc rc('animation', html='html5') x = np.array([[2, 10], [2, 5], [8, 4], [5, 8], [7, 5], [6, 4], [1, 2], [4, 9]]) C = np.array([[2, 10], [5, 8], [1, 2]]) def step(): global C, clusters clusters = get_clusters(assign_clusters(x)) C = get_means(clusters) fig = plt.figure() paths = plt.scatter(x.T[0], x.T[1], c=[colors[i] for i in assign_clusters(x)], cmap='hsv') scat, = plt.plot(C.T[0], C.T[1],'rx') plt.close() def init(): x = np.array([[2, 10], [2, 5], [8, 4], [5, 8], [7, 5], [6, 4], [1, 2], [4, 9]]) C = np.array([[2, 10], [5, 8], [1, 2]]) paths = plt.scatter(x.T[0], x.T[1], c=assign_clusters(x)) scat, = plt.plot(C.T[0], C.T[1],'rx') return paths, def animate(i): step() paths.set_array(assign_clusters(x)) scat.set_data(np.array(C).T[0], np.array(C).T[1]) return paths, animation.FuncAnimation(fig, animate, range(3), interval=2000, init_func=init)\n\nFirst, we define a step function that runs one iteration of our loop. The rest of the code is rather standard matplotlib animation code. We create a function to draw a frame (this is the animate function). This function updates the values in the existing plot. We run 3 frames, since our algorithm converged in three steps, and let each frame stay for 2 seconds (2000 milliseconds).\n\nIt’s not perfect (in particular, I think one frame was skipped, but I simply couldn’t figure out why), but it illustrates the core concept. The red crosses are the centroids, the circles are the points, colored by cluster.\n\nConvergence of k-Means\n\nThe k-Means algorithm is guaranteed to converge [1]. Let’s define a distortion function as\n\nThis measures the sum of the squared distances between each point and the centroid of the cluster it belongs to. It turns out that k-Means is equivalent to an algorithm called coordinate descent (see this excellent Wikipedia article on the same). In coordinate descent, you reach the global minima by minimizing the function one coordinate at a time. The Wikipedia article has more details on this. In particular, the two inner loops of the algorithm first hold fixed and minimize with respect to , and then hold fixed and minimize with respect to . Thus, converges, which usually means that and will converge. Theoretically, it is possible for these two to oscillate between values where is the same, but this never happens in practice.\n\nThis said, the distortion function is non-convex, so convergence is only guaranteed to a local optimum, not a global one. In practice, however, k-Means clustering works rather well.\n\nOne issue with k-Means is that with different initializations, it is possible to end up with different sets of clusters. One way to choose between these is to pick the one with the least value of the distortion function.\n\nAdditional information\n\nAn example of the k-means algorithm has been provided by Dr. Saha. You can view it by downloading his notes.\n\nSources\n\n[1] Andrew Ng’s CS229 notes (7a)\n\nSVMs are popular because they work well with kernels. In this post, we’ll discuss what kernels are, and how they’re used in the context of support vector machines.\n\nWhy kernels?\n\nFirst of all, what is a kernel? Suppose we have a function that maps from one space (yes, this is the same as a linear space that we talked about in the PCA post) to another. The idea is that we might want a mapping function that transforms the data to some space where it is linearly separable. Once we do that, we can use our previous algorithm to find a maximum margin hyperplane.\n\nIn the general case, we will have a mapping, , that maps from the -dimensional space to the -dimensional space .\n\nRecall now that in the final equations that we had derived, we had noted that the expressions only use dot products (also called inner products) of the input features. Because we aim to transform our input features to a new space and use the new features in the linear SVM, all we really have to do to modify our algorithm is replace those dot products with the new dot products, by replacing with . A kernel function, , is a function that returns the inner products of the new features. Concretely,\n\nTherefore, a kernel is a function that takes vectors in the original space and returns the dot product of the vectors in the new feature space. It is important to note here that we don’t actually need the transformed vectors themselves–we only need the inner products. It turns out that these dot products can be computed efficiently even if computing the vectors themselves is computationally infeasible. In the algorithm that we had developed, we will replace all inner products with the kernel function.\n\nExample\n\nA standard example when introducing kernels is the following [1]. Suppose we have a two-dimensional input space, and we have a mapping function\n\nThen we have,\n\nThe interesting thing to note here is this: for those of you familiar with the big-O notation, you’ll see that in this case, computing is , if the input space is -dimensional, but computing the kernel is only , since computing the inner product has a linear time complexity. If you didn’t understand this, it simply means that for this example, and many more, it is more efficient to compute the kernel than to compute the transformed features.\n\nFurther, note from this image from Prof. Krebs’ slides [1], how the transformed features are now linearly separable.\n\nValidity of kernels: Mercer’s condition\n\nAny function cannot be a kernel function, because we require that there is some mapping, such that . Now, suppose we have a matrix , such that for any vectors in the original space, the elements of the matrix are the values of the kernel. Concretely,\n\nThis matrix is called the Gram matrix. Now because the dot product is a symmetric operation, it follows that our matrix here is also symmetric. We further want to prove that must be positive semi-definite, which means that given any vector , is positive or zero. Let’s do that now:\n\nThe last step can be obtained with a little intuition. You can think of as a matrix, where the rows are each of the for fixed (the th elements of the vectors ), and the columns are each of the s, for fixed . Then, the penultimate step can be written as\n\nwhere the last step is because the variable used in the summation doesn’t really matter. What we’ve shown is that the matrix is positive semi-definite. And so, we arrive at Mercer’s condition:\n\nFor a function to be a valid kernel, it is necessary and sufficient for any finite-length set of vectors , the corresponding kernel matrix is symmetric and positive semi-definite.\n\nThe Gaussian kernel\n\nThe Gaussian kernel is a very popular choice, and is defined as\n\nThe (new) feature space corresponding to this kernel is infinite-dimensional, which means that for any given labeled dataset, there is some linear hyperplane that separates the data correctly in the Gaussian feature space. This gives this kernel virtually unlimited expressive power.\n\nChoosing a kernel\n\nChoosing a good kernel for your data is a pretty difficult task, exacerbated by the fact that most real datasets are high-dimensional, so you can’t visualize them either. Ideally, you’d know the shape of your data, and use this information to pick a kernel. Unfortunately, this means you need a good deal of knowledge about your data and its shape. Prof. Krebs, in his lecture, shows two approaches that we could use:\n\nWe could pick a family of kernels, like the Gaussian. Note that the Gaussian kernel we discussed above is a family of kernels, because its shape changes based on your choice of . You can tune this hyper-parameter and choose one that gives you a good performance, using a hold-out cross-validation set (we will discuss this later). This usually works well, but for some types of data, such as strings and genome data, this does not work, and you’ll have to devise your own kernel.\n\nUse an algorithm to learn the Gram matrix itself. This seems promising, but it has a few issues. First, it is unclear what you need to optimize when learning this matrix. Second, if your dataset is large, it is simply infeasible to store this matrix in memory (remember that one advantage of using kernels is to not have to compute the new features, and only the dot products).\n\nThe “kernel trick”\n\nKernels aren’t used only with SVMs. In general, you can use a kernel with any learning algorithm where you only need the inner products of the feature vectors. So this works even with linear/logistic regression, for example. In such cases, you can replace those inner products with a kernel, giving it greater power. This is called the kernel trick.\n\nConclusion\n\nIn this post, we discussed the use of kernels in SVMs, and derived conditions for a function to be a valid kernel, also called a Mercer kernel. We looked at a popular example, the Gaussian kernel. This “kernel trick” can also be used for other learning algorithms.\n\nSources\n\n[1] Lecture notes by Prof. Dave Krebs from the University of Pittsburgh\n\n[2] CS229, taught by Andrew Ng at Stanford University\n\nBefore we proceed to look at the case where the data is not linearly separable, let’s first solidify the foundations of SVMs by implementing an SVM. Our SVM will not have a kernel (also called a linear kernel), and will use a convex optimization library. Recall that the SVM problem is a quadratic programming problem (QPP). Our convex optimization library of choice will be cvxopt.\n\nA lot of the code in this blog post was taken from this blog post by Xavier Bourret Sicotte. Plotting the decision boundary is code taken from here.\n\nLet’s start by making a dataset and ensuring it’s linearly separable.\n\nfrom sklearn.datasets import make_blobs import matplotlib.pyplot as plt import numpy as np plt.style.use('ggplot') X, y = make_blobs(n_samples=1000, centers=2, n_features=2, random_state=1) y[y == 0] = -1\n\nThe last line changes the 0s to -1, since that is what our mathematical framework assumes. Let’s visualize this data.\n\nplt.xlabel('$x_1$') plt.ylabel('$x_2$') plt.scatter(X.T[0], X.T[1], c=y+1, alpha=0.3);\n\nFor convenience, let’s give y a 2D shape rather than let it be a rank-one tensor. Let’s multiply all the values with 1.0 (a float) to force all the values to be floating-points.\n\ny = y.reshape(-1, 1) * 1.\n\nNow it’s time to frame our dual problem and solve it using the convex optimization library. There is a rather technical problem, however. You see, our dual problem is posed as:\n\nHowever, cvxopt expects a QPP of the form\n\nXavier’s blog post shows a clever trick: let’s first begin by forming a matrix such that\n\nThen, our dual problem becomes\n\nWe’re not quite there yet. We need a min instead of a max, and a change of sign in the constraint. The trick is easy: for the first issue, we simply take the negative of the optimization problem. Maximizing something is equivalent to minimizing its negative. For the second issue, we also do the same thing. Thus, the optimization problem we give the library is\n\nLet’s note the correspondence between these two problems:\n\nWe can now import the convex optimization library and convert our data to its format, and ask it to solve the QP problem for us.\n\nfrom cvxopt import matrix as cvxopt_matrix from cvxopt import solvers as cvxopt_solvers m, n = X.shape X_dash = y * X H = np.dot(X_dash, X_dash.T) * 1. P = cvxopt_matrix(H) q = cvxopt_matrix(-np.ones((m, 1))) h = cvxopt_matrix(np.zeros(m)) A = cvxopt_matrix(y.reshape(1, -1)) b = cvxopt_matrix(np.zeros(1)) G = cvxopt_matrix(-np.eye(m)) cvxopt_solvers.options['show_progress'] = False sol = cvxopt_solvers.qp(P, q, G, h, A, b)\n\nNoting the correspondence, we know this solution has . Let’s grab that.\n\nlambd = np.array(sol['x'])\n\nUsing equation 1 from our previous post, let’s compute .\n\nw = ((y * lambd).T @ X).reshape(-1,1)\n\nFinally, let’s use the equation for from the previous post to get (this is different from Xavier’s blog post, but the end result is the same).\n\nind = np.where(y == -1)[0] p1 = X[ind] @ w ind = np.where(y == 1)[0] p2 = X[ind] @ w b = (max(p1) + min(p2)) / 2\n\nWe’re done! If you print out and , you should get .\n\nLet’s plot the decision boundary.\n\nplt.xlabel('$x_1$') plt.ylabel('$x_2$') xx = np.linspace(-12, 5) slope = -w[0] / w[1] intercept = b / w[1] yy = slope * xx + intercept plt.plot(xx, yy, 'k-') plt.scatter(X.T[0], X.T[1], c=y.squeeze()+1, alpha=0.7);\n\nWe can even plot the support vectors (although why they don’t actually pass through one point on each side, I’m not sure). The key idea is that the width of the margin of the SVM is , on each side (you should be able to see this from our formulation in the initial SVM blog post). The additional lines of code are\n\nyy = slope * xx + intercept + 1 / np.linalg.norm(w) plt.plot(xx, yy, 'k-') yy = slope * xx + intercept - 1 / np.linalg.norm(w) plt.plot(xx, yy, 'k-')\n\nYou can use sklearn with the kernel=’linear’ parameter, and verify that the results are the same. The intercept term, b will have a negative sign, but that’s not an issue–it’s only a minor change while making predictions. In our case, to make a prediction on a variable x, we use\n\npredictions = x @ w + b\n\nThat’s it! Now that we know how to implement an SVM with no kernel, we can move on to discuss kernels, which give SVMs their real power.\n\nAside: This page shows how to write an SVM without even using a convex optimization library! It relies on an algorithm called Sequential Minimal Optimization (SMO).\n\nLet’s continue and work towards posing a dual for our optimization problem. Specifically, we’ll focus on the constraints that we have. For each training example, we have\n\nShuffling terms around,\n\nNote that if the functional margin ( ) is 1, then , and all from the KKT conditions. Let’s recall the scaling condition we imposed on (in the first post):\n\nWhat this means is that at the points closest to the optimal hyperplane, the functional margin is 1. This figure depicts it perfectly:\n\nDon’t worry about the equation having : that’s just a consequence of this particular hyperplane having a negative intercept on the y-axis. Thus, in this example, only three of the s will be nonzero at the optimal solution. We call these three points support vectors. Sometimes, you’ll see the lines through them also being called the support vectors. The important thing is these are where the functional margin is equal to 1.\n\nLet’s now frame the Lagrangian for our optimization problem.\n\nFrom the post on convex optimization, recall that the dual problem is . So let’s first minimize our Lagrangian. To do so, we compute the partial gradients of the Lagrangian with respect to and . Why aren’t we doing this with respect to as well? Try it out. You’ll get the condition that every point is a support vector, which is obviously not the case. So we’ll fix , and only compute the other two partial derivatives, and set them to 0.\n\nThe result of this, which is below, is quite important. Let’s call this equation 1.\n\nSimilarly, by finding the partial derivative with respect to ,\n\nLet’s call this equation 2. We now use equation 1 in our Lagrangian. This gives us\n\nLet’s look at this derivation.\n\nThe first step is from the observation that . We also plugged in the value of in the second sum.\n\nThe next step expands the sums, and uses equation 2, so the third sum is 0.\n\nNext, we shuffle terms around, and note that the first two terms really are the same.\n\nWith this in place, let’s look at our final dual problem, which is the problem to maximize this function.\n\nIf we solve this problem for , we can plug it in equation 1, to get . How do we get ? Look at our constraint:\n\nIf , then dividing both sides by -1,\n\nso\n\nFor the other case, we divide both sides by +1:\n\nso that\n\nTo combine these, note that our initial, non-convex optimization problem was to maximize the functional margin (we had a denominator , but that is always positive). To achieve this, we consider the maximum of the first result above, and the minimum of the second. In effect, we consider the equality in both cases. We then add up the results and divide by two. This yields us\n\nIntuitively, this says find the smallest positive and the largest negative training examples, and place the line right in the middle of the two (because $b$ controls the position of the hyperplane, while $w$ controls the direction). The figure below explains this intuition (this is hand-drawn, so it’s probably not accurate).\n\nNote that to make predictions, we simply compute , and predict 1 if this is positive; otherwise, we predict -1. Using equation 1, we need to compute\n\nThus, making predictions only requires finding the values of . Notice that these are all 0 except for the support vectors. So in reality, we have only a few computations to make, and the sum really is only over the support vectors, not the entire training set.\n\nThe fact that we only need to compute inner products, essentially, will be useful when we deal with kernels, which help when the data is not linearly separable. When the data is very high-dimensional, for some feature spaces, computing inner products can be done efficiently."
    }
}
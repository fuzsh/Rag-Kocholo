Displaying a dazzling intellect from a young age, Claude Shannon was an American genius who was equal parts electrical engineer, computer scientist, cryptographer, and mathematician. He became known as the father of information theory after his death in 2001. He was born in 1916 to two intellectual parents, a father who was a judge for a time and a mother who taught languages. Shannon graduated from high school two years early and was a very strong student in math and science. He was constantly building things as a teenager including a telegraph system that he devised to send messages to a friend who lived half a mile away, as well as a radio-controlled model boat. He idolized Thomas Edison, as did many young men of the time. Much later in life, he would learn that he and Edison were distant cousins. He went to college at the University of Michigan at age 16 and in four years’ time, he had two degrees - one in mathematics and the other in electrical engineering. He moved on to MIT to earn a Master’s degree in electrical engineering and started designing switching circuits that led to his master’s degree thesis, which was published in 1938. His circuits were a vast improvement over those used on telephone switchboards at the time. His integration of logic into the design of circuits was the origin of the digital circuit design, which became prominent after World War II. He received a PhD in mathematics from MIT in 1940 and moved to New Jersey to become a National Research Fellow at the Institute for Advanced Study. His brilliance was co-opted by the military as the US plunged into World War II at the end of 1941, as he worked on fire-control systems and cryptography. In 1942, he was credited with the invention of signal-flow graphs, used to represent how a signal flows through a physical system such as through electronic networks. He worked briefly with British code cracker Alan Turing in 1943 and remained interested in cryptography, ultimately focusing it on how to greatly expand communication through mathematics. This led to him inventing the field of information theory, which would lead to modern ideas like computational linguistics and natural language processing. In essence, he made the subject of data gathering a relevant occupation and institution, developing a system of entropy that quantifies the storage and communication of information. Everything from smartphones to deep space missions to artificial intelligence is built from this idea. After retiring from active work, he joined the faculty at MIT in 1956 and continued working there until 1978. Later in life, he developed Alzheimer’s disease and passed away in 2001 at the age of 84.
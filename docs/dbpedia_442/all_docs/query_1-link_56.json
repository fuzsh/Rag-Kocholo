{
    "id": "dbpedia_442_1",
    "rank": 56,
    "data": {
        "url": "https://nap.nationalacademies.org/read/1859/chapter/8",
        "read_more_link": "",
        "language": "en",
        "title": "7 Computation: Beyond Theory and Experiment: Seeing the World Through Scientific Computation",
        "top_image": "https://nap.nationalacademies.org/cover/1859/450",
        "meta_img": "https://nap.nationalacademies.org/cover/1859/450",
        "images": [
            "https://nap.nationalacademies.org/read/img/openbook-header-print.png",
            "https://nap.nationalacademies.org/cover/1859/450",
            "https://nap.nationalacademies.org/openbook/0309045924/xhtml/images/img00029.jpg",
            "https://nap.nationalacademies.org/openbook/0309045924/xhtml/images/img00030.jpg",
            "https://nap.nationalacademies.org/openbook/0309045924/xhtml/images/img00031.jpg",
            "https://nap.nationalacademies.org/openbook/0309045924/xhtml/images/img00032.jpg",
            "https://nap.nationalacademies.org/openbook/0309045924/xhtml/images/img00033.jpg",
            "https://nap.nationalacademies.org/openbook/0309045924/xhtml/images/img00034.jpg",
            "https://nap.nationalacademies.org/openbook/0309045924/xhtml/images/img00035.jpg",
            "https://nap.nationalacademies.org/openbook/0309045924/xhtml/images/img00036.jpg",
            "https://nap.nationalacademies.org/openbook/0309045924/xhtml/images/img00037.jpg",
            "https://nap.nationalacademies.org/images/hdr/logo-nasem-wht-lg.png"
        ],
        "movies": [],
        "keywords": [],
        "meta_keywords": [
            ""
        ],
        "tags": null,
        "authors": [],
        "publish_date": null,
        "summary": "",
        "meta_description": "Read chapter 7 Computation:  Beyond Theory and Experiment:  Seeing the World Through Scientific Computation: Science at the Frontier takes you on a journe...",
        "meta_lang": "en",
        "meta_favicon": "",
        "meta_site_name": "The National Academies Press",
        "canonical_link": "https://nap.nationalacademies.org/read/1859/chapter/8",
        "text": "7\n\nComputation\n\nBeyond Theory and Experiment: Seeing the World Through Scientific Computation\n\nCertain sciences—particle physics, for example—lend themselves to controlled experimentation, and others, like astrophysics, to observation of natural phenomena. Many modern sciences have been able to aggressively pursue both, as technology has provided ever more powerful instruments, precisely designed and engineered materials, and suitably controlled environments. But in all fields the data gathered in these inquiries contribute to theories that describe, explain, and—in most cases—predict. These theories are then subjected to subsequent experimentation or observation—to be confirmed, refined, or eventually overturned—and the process evolves to more explanatory and definitive theories, often based on what are called natural laws, more specifically, the laws of physics.\n\nLarry Smarr, a professor in the physics and astronomy departments at the University of Illinois at Champaign-Urbana, has traveled along the paths of both theory and experimentation. He organized the Frontiers symposium's session on computation, which illustrated the premise that computational science is not merely a new field or discipline or tool, but rather an altogether new and distinct methodology that has had a transforming effect on modern science: first, to permanently alter how scientists work, experiment, and theorize; and second, to expand their reach beyond the inherent limitations of the two other venerable approaches (Box 7.1).\n\nModern supercomputers, explained Smarr, ''are making this alternative approach quite practical (Box 7.2). The result is a revolu-\n\nalization, is radically altering the relationship between humans and the supercomputer\" (p. 161). Indeed, the value of such pictures cannot be overestimated. Not only do they allow the compression of millions of data points into a coherent image, but that image is also a summary that appeals to the way the human brain processes information, because it establishes a picture and also moves that coherent picture through time, providing insight into the dynamics of a system or experiment that might well be obscured by a simple ream of numbers.\n\nAmong the participants in the session on computation, William Press, an astrophysicist at Harvard University, noted that starting several years ago, interdisciplinary conferences began to be awash with \"beautiful movies\" constructed to model and to help investigators visualize basic phenomena. \"What is striking today,\" Press commented, \"is that computer visualization has joined the mainstream of science. People show computer movies simply because you really cannot see what is going on without them.\" In fact Press believes strongly that computer modeling and visualization have become \"an integral part of the science lying underneath.\"\n\nThe computation session's participants—also including Alan Huang of AT&T Bell Laboratories, Stephen Wolfram of Wolfram Research, and Jean Taylor from the Department of Mathematics at Rutgers University, discussed scientific computation from several diverse points of view. They talked about hardware and software, and about how both are evolving to enhance scientists' power to express their algorithms and to explore realms of data and modeling hitherto unmanageable.\n\nTHE MACHINERY OF THE COMPUTER\n\nDigital Electronics\n\nIn the 1850s British mathematician George Boole laid the foundation for what was to become digital electronics by developing a system of symbolic logic that reduces virtually any problem to a series of true or false propositions. Since the output of his system was so elemental, he rejected the base 10 system used by most of the civilized world for centuries (most likely derived from ancient peoples counting on their fingers). The base 2 system requires a longer string of only two distinct symbols to represent a number (110101, for example, instead of 53). Since there are but two possible outcomes at each step of Boole's system, the currency of binary systems is a perfect match: an output of 1 signifies truth; a 0 means false. When\n\ntranslated into a circuit, as conceptualized by Massachusetts Institute of Technology graduate student Claude Shannon in his seminal paper ''A Symbolic Analysis of Relay and Switching Circuits\" (Shannon, 1938), this either/or structure can refer to the position of an electrical switch: either on or off.\n\nAs implemented in the computer, Boolean logic poses questions that can be answered by applying a series of logical constructs or steps called operators. The three operators found in digital computers are referred to by circuit designers as logic gates. These gates are physically laid down on boards and become part of the system's architecture. The computer's input is then moved through these gates as a series of bits. When a pair of bits encounters a gate, the system's elementary analysis occurs. The AND gate outputs 1 when both elements are 1, meaning, \"It is true that the concept of and applies to the truth of both elements of the set\"; conversely, the AND gate produces 0 as an output, meaning false, if either (or both) of the two elements in question is false; the OR gate outputs a 1, meaning true, if either element (or both) is a 1; with the NOT gate, the output is simply the reverse of whichever symbol (1 or 0) is presented.\n\nThus the physical structure of the basic elements of a circuit—including its gate design—becomes integral to analyzing the relationship of any pair of bits presented to it. When the results of this analysis occurring throughout the circuit are strung together, the final outcome is as simple as the outcome of any of its individual steps—on or off, 1 or 0, true or false—but that outcome could represent the answer to a complex question that had been reduced to a series of steps predicated on classical logic and using the three fundamental operators.\n\nWhat is gained by reducing the constituent elements of the system to this simplest of all currencies—1 or 0, on or off—is sacrificed, however, in computational efficiency. Even the most straightforward of problems, when translated into Boolean logic and the appropriate series of logic gates, requires a great many such logical steps. As the computer age enters its fifth generation in as many decades, the elegant simplicity of having just two elemental outcomes strung together in logic chains begins to bump up against the inherent limitation of its conceptual framework. The history of computer programs designed to compete with human chess players is illustrative: as long as the computer must limit its \"conceptual vision\" to considering all of the alternatives in a straightforward fashion without the perspectives of judgment and context that human brains use, the chess landscape is just too immense, and the computer cannot keep up with expert players. Nonetheless, the serial computer will always have a\n\nrole, and a major one, in addressing problems that are reducible to such a logical analysis. But for designers of serial digital electronics, the only way to improve is to run through the myriad sequences of simple operations faster. This effort turns out to reveal another inherent limitation, the physical properties of the materials used to construct the circuits themselves.\n\nThe Physical Character of Computer Circuitry\n\nIntegrated circuits (ICs), now basic to all computers, grew in response to the same generic challenges—moving more data faster and more efficiently—that are impelling scientists like Huang \"to try to build a computer based on optics rather than electronics.\" By today's sophisticated standards, first-generation electronic computers were mastodons: prodigious in size and weight, and hungry for vast amounts of electricity—first to power their cumbersome and fragile vacuum tubes and then to cool and dissipate the excess heat they generated. The transistor in 1947 and the silicon chip on which integrated circuits were built about a decade later enabled development of the second-and third-generation machines. Transistors and other components were miniaturized and mounted on small boards that were then plugged into place, reducing complex wiring schemes and conserving space.\n\nBy the 1970s engineers were refining the IC to such a level of sophistication that, although no fundamental change in power and circuitry was seen, an entirely new and profound fourth generation of computers was built. ICs that once contained 100 or fewer components had grown through medium-and large-scale sizes to the very large-scale integrated (VLSI) circuit with as at least 10,000 components on a single chip, although some have been built with 4 million or more components. The physics involved in these small but complex circuits revolves around the silicon semiconductor.\n\nThe electrical conductivity of semiconductors falls between that of insulators like glass, which do not conduct electricity at all, and copper, which conducts it very well. In the period just after World War II, John Bardeen, Walter Brattain, and William Shockley were studying the properties of semiconductors. At that time, the only easy way to control the flow of electricity electronically (to switch it on or off, for example) was with vacuum tubes. But vacuum tube switches were bulky, not very fast, and required a filament that was heated to a high temperature in order to emit electrons that carried a current through the vacuum. The filament generated a great deal of heat and consumed large amounts of power. Early computers based\n\non vacuum tubes were huge and required massive refrigeration systems to keep them from melting down. The Bardeen-Brattain-Shockley team's goal was to make a solid-state semiconductor device that could replace vacuum tubes. They hoped that it would be very small, operate at room temperature, and would have no power-hungry filament. They succeeded, and the result was dubbed the transistor. All modern computers are based on fast transistor switches. A decade later, the team was awarded a Nobel Prize for their accomplishment, which was to transform modern society.\n\nHow to build a chip containing an IC with hundreds of thousands of transistors (the switches), resistors (the brakes), and capacitors (the electricity storage tanks) wired together into a physical space about one-seventh the size of a dime was the question; photolithography was the answer. Literally \"writing with light on stone,\" this process allowed engineers to construct semiconductors with ever finer precision. Successive layers of circuitry can now be laid down in a sort of insulated sandwich that may be over a dozen layers thick.\n\nAs astounding as this advance might seem to Shannon, Shockley, and others who pioneered the computer only half a lifetime ago, modern physicists and communications engineers continue to press the speed envelope. Huang views traditional computers as fundamentally \"constrained by inherent communication limits. The fastest transistors switch in 5 picoseconds, whereas the fastest computer runs with a 5 nanosecond clock.\" This difference of three orders of magnitude beckons, suggesting that yet another generation of computers could be aborning. Ablinking, actually: in optical computers a beam of light switched on represents a 1, or true, signal; switching it off represents a 0, or false, signal.\n\nNEW DIRECTIONS IN HARDWARE FOR COMPUTATION\n\nComputing with Light\n\nThe major application of optics technology thus far has been the fiber-optic telecommunications network girding the globe. Light rays turn out to be superior in sending telephone signals great distances because, as Huang described it, with electronic communication, \"the farther you go, the greater the energy required, whereas photons, once launched, will travel a great distance with no additional energy input. You get the distance for free. Present engineering has encountered a crossover point at about 200 microns, below which distance it is more efficient to communicate with electrons rather than photons. This critical limit at first deflected computer designers'\n\ninterest in optics, since rarely do computer signals travel such a distance in order to accomplish their goal.\" To designers of telecommunications networks, however, who were transmitting signals on the order of kilometers, the \"extra distance for free\" incentive was compelling. The situation has now changed with the advent of parallel processing, since a large portion of the information must move more than 200 microns. Energy is not the only consideration, however. As the physical constraints inherent in electronic transmission put a cap on the speed at which the circuitry can connect, optical digital computing presents a natural alternative. \"The next challenge for advanced optics,'' predicted Huang, \"is in the realm of making connections between computer chips, and between individual gates. This accomplished, computers will then be as worthy of the designation optical as they are electronic.\"\n\nHuang was cautious, however, about glib categorical distinctions between the two realms of optics and electronics. \"How do you know when you really have an optical computer?\" he asked. \"If you took out all the wires and replaced them with optical connections, you would still have merely an optical version of an electronic computer. Nothing would really have been redesigned in any fundamental or creative way.\" He reminded the audience at the Frontiers symposium of the physical resemblance of the early cars built at the turn of the century to the horse-drawn carriages they were in the process of displacing and then continued, \"Suppose I had been around then and come up with a jet engine? Would the result have been a jet-powered buggy? If you manage to come up with a jet engine, you're better off putting wings on it and flying it than trying to adapt it to the horse and buggy.\" A pitfall in trying to retrofit one technology onto another, he suggested, is that \"it doesn't work\"—it is necessary instead to redesign \"from conceptual scratch.'' But the process is a long and involved one. The current state of the art of fully optical machines, he remarked wryly, is represented by one that \"is almost smart enough to control a washing machine but is a funky-looking thing that will no doubt one day be regarded as a relic alongside the early mechanical computers with their heavy gears and levers and hand-cranks\" (Figure 7.1).\n\nThe Optical Environment\n\nOptics is more than just a faster pitch of the same basic signal, although it can be indisputably more powerful by several orders of magnitude. Huang surveyed what might be called the evolution of optical logic:\n\nof light moving through a fiber tunnel—in addition to being several orders of magnitude faster than electrons moving through a wire on a chip—possess two other inherent advantages that far outweigh their increased speed: higher bandwidth and greater connectivity.\n\nIn order to convey the significance of higher bandwidth and greater connectivity, Huang presented his \"island of Manhattan\" metaphor:\n\nEverybody knows that traffic in and out of Manhattan is really bottlenecked. There are only a limited number of tunnels and bridges, and each of them has only a limited number of lanes. Suppose we use the traffic to represent information. The number of bridges and tunnels would then represent the connectivity, and the number of traffic lanes would represent the bandwidth.\n\nThe use of an optical fiber to carry the information would correspond to building a tunnel with a hundred times more lanes. Transmitting the entire Encyclopedia Britannica through a wire would take 10 minutes, while it would take only about a second with optical fiber.\n\nInstead of using just an optical fiber, the systems we are working on use lenses. Such an approach greatly increases the connectivity since each lens can carry the equivalent of hundreds of optical fibers. This would correspond to adding hundreds of more tunnels and bridges to Manhattan.\n\nThis great increase in bandwidth and connectivity would greatly ease the flow of traffic or information. Try to imagine all the telephone wires in the world strung together in one giant cable. All of the telephone calls can be carried by a series of lenses.\n\nThe connectivity of optics is one reason that optical computers may one day relegate their electronic predecessors to the historical junkyard (Figure 7.2). Virtually all computing these days is serial;\n\nthe task is really to produce one output, and the path through the machine's circuitry is essentially predetermined, a function of how to get from input to output in a linear fashion along the path of the logic inherent to, and determined by, the question. Parallel computing—carried out with a fundamentally different machine architecture that allows many distinct or related computations to be run simultaneously—turns this structure on its ear, suggesting the possibility of many outputs, and also the difficult but revolutionary concept of physically modifying the path as the question is pursued in the machine. The science of artificial intelligence has moved in this direction, inspired by the manifest success of massively parallel \"wiring\" in human and animal brains.\n\nWith the connectivity advantage outlined, Huang extended his traffic metaphor: \"Imagine if, in addition to sending so many more cars through, say, the Holland Tunnel, the cars could actually pass physically right through one another. There would be no need for stoplights, and the present nightmare of traffic control would become trivial.\" With the pure laser light generally used in optics, beams can pass through one another with no interference, regardless of their wavelength. By contrast, Huang pointed out, \"In electronics, you're always watching out for interference. You can't send two electronic signals through each other.\"\n\nAlthough the medium of laser light offers many advantages, the physical problems of manipulating it and moving it through the logic gates of ever smaller optical architectures present other challenges. In responding to them, engineers and designers have developed a remarkable technique, molecular beam epitaxy (MBE), which Huang has used to construct an optical chip of logic gates, lenses, and mirrors atom by atom, and then layer by layer (Box 7.3; Figure 7.3). It is possible, he explained, to \"literally specify . . . 14 atoms of this, 37 of that, and so on, for a few thousand layers.\" Because of the precision of components designed at the atomic level, the light itself can be purified into a very precise laser beam that will travel through a hologram of a lens as predictably as it would travel through the lens itself. This notion brings Huang full circle back to his emphasis on deflecting simple distinctions between electronics and optics: \"With MBE technology, we can really blur the distinction between what is optical, what is electronic, and what is solid-state physics. We can actually crunch them all together, and integrate things on the atomic level.\" He summarized, \"All optical communication is really photonic, using electromagnetic waves to propagate the signal, and all interaction is really electronic, since electrons are always involved at\n\nsome level.\" Extraordinary advances in engineering thus permit scientists to reexamine some of their basic assumptions.\n\nTHE COMPUTER'S PROGRAMS-ALGORITHMS AND SOFTWARE\n\nAlgorithms—the term and the concept—have been an important part of working science for centuries, but as the computer's role grows, the algorithm assumes the status of a vital methodology on which much of the science of computation depends (Harel, 1987). Referring to the crucial currency of computation, speed, Press stated that \"there have been at least several orders of magnitude of computing speed gained in many fields due to the development of new algorithms.\"\n\nAn algorithm is a program compiled for people, whereas software is a program compiled for computers. The distinction is sometimes meaningless, sometimes vital. In each case, the goal of solving a particular problem drives the form of the program's instructions. A successful algorithm in mathematics or applied science provides definitive instructions for an unknown colleague to accomplish a particular task, self-sufficiently and without confusion or error. When that al-\n\ngorithm involves a computer solution, it must be translated through the appropriate program and software. But when around 300 B.C. Euclid created the first nontrivial algorithm to describe how to find the greatest common divisor of two positive integers, he was providing for all time a definitive recipe for solving a generic problem. Given the nature of numbers, Euclid's algorithm is eternal and will continue to be translated into the appropriate human language for mathematicians forever.\n\nNot all algorithms have such an eternal life, for the problems whose solutions they provide may themselves evolve into a new form, and new tools, concepts, and machinery may be developed to address them. Press compiled a short catalog of recent \"hot\" algorithms in science and asked the symposium audience to refer to their own working experience to decide whether the list represented a \"great edifice or a junkpile.\" Included were fractals and chaos, simulated annealing, Walsh functions, the Hartley and fast Fourier transforms, fuzzy sets, and catastrophe theory. The most recent among these, which Press speculated could eventually rival the fast Fourier transform (FFT) as a vital working tool of science, is the concept of wavelets.\n\nOver the last two decades, the Fourier transform (FT) has reduced by hundreds of thousands of hours the computation time working scientists would otherwise have required for their analyses. Although it was elucidated by French mathematician J.B.J. Fourier \"200 years ago,\" said Press, \"at the dawn of the computer age it was impractical for people to use it in trying to do numerical work\" until J.W. Cooley and J.W. Tukey devised the FFT algorithm in the 1960s. Simply stated, the FFT makes readily possible computations that are a factor of 106 more complicated than would be computable without it. Like any transform process in mathematics, the FT is used to simplify or speed up the solution of a particular problem. Presented with a mathematical object X, which usually in the world of experimental science is a time series of data points, the FT first changes, or transforms, X into a different but related mathematical object X'.\n\nThe purpose of the translation is to render the desired (or a related) computation more efficient to accomplish. The FT has been found useful for a vast range of problems. Press is one of the authors of Numerical Recipes: the Art of Scientific Computing (Press et al., 1989), a text on numerical computation that explains that \"a physical process can be described either in the time domain, or else in the frequency domain. For many purposes it is useful to think of these as being two different representations of the same function. One goes back and forth between these two representations by means of the Fourier transform\n\nequations\" (p. 381). The FT, by applying selective time and frequency analyses to an enormous amount of raw data, extracts the essential waveform information into a smaller data set that nevertheless has sufficient information for the experimenter's purposes.\n\nWavelets\n\nThe data signals presented to the computer from certain phenomena reflect sharp irregularities when sudden or dramatic transitions occur; the mathematical power of the FT, which relies on the creation and manipulation of trigonometric sines and cosines, does not work well at these critical transition points. For curves resulting from mathematical functions that are very complex and that display these striking discontinuities, the wavelet transform provides a more penetrating analytic tool than does the FT or any other mathematical operation. \"Sine waves have no localization,\" explained Press, and thus do not bunch up or show a higher concentration at data points where the signal is particularly rich with detail and change. The power of wavelets comes in part from their variability of scale, which permits the user to select a wavelet family whose inherent power to resolve fine details matches the target data.\n\nThe fundamental operation involves establishing a banded matrix for the given wavelet family chosen. These families are predicated on the application of a specific set of coefficient numbers—which were named for Ingrid Daubechies, a French mathematician whose contributions to the theory have been seminal—and are called Daub 4 or Daub 12 (or Daub X), with a greater number of coefficients reflecting finer desired resolution. Press demonstrated to the symposium's audience how the Daub 4 set evolves, and he ran through a quick summary of how a mathematician actually computes the wavelet transform. The four special coefficients for Daub 4 are as follows:\n\nThe banded matrix is established as follows: install the numbers c0 through c3 at the upper left corner in the first row, followed by zeros to complete the row; the next row has the same four numbers directly below those on the first row, but with their order reversed and the c2 and c0 terms negative; the next row again registers the numbers in their regular sequence, but displaced two positions to the right, and so on, with zero occupying all other positions. At the end of a row of the matrix, the sequence wraps around in a circulant.\n\nThe mathematical character that confers on the wavelet transform its penetrating power of discrimination comes from two properties: first, the inverse of the matrix is found by transposing it—that is, the matrix is what mathematicians call orthogonal; second, when the fairly simple process of applying the matrix and creating a table of sums and differences is performed on a data set, a great many vanishing moments result—contributions so small that they can be ignored with little consequence. In practical terms, a mathematical lens seeks out smooth sectors of the data, and confers small numbers on them to reduce their significance relative to that of rough sectors, where larger numbers indicate greater intricacy or sudden transition. Press summarized, \"The whole procedure is numerically stable and very fast, faster than the fast Fourier transform. What is really interesting is that the procedure is hierarchical in scale, and thus the power of wavelets is that smaller components can be neglected; you can just throw them out and compress the information in the function. That's the trick.\"\n\nPress dramatically demonstrated the process by applying the wavelet transform to a photograph that had been scanned and digitized for the computer (Figure 7.4). \"I feel a little like an ax murderer,\" he commented, \"but I analyzed this poor woman into wavelets, and I'm taking her apart by deleting those wavelets with the smallest coefficients.\" This procedure he characterized as fairly rough: a sophisticated signal processor would actually run another level of analysis and assign a bit code to the coefficient numbers. But with Press simply deleting the smallest coefficients, the results were striking. With 77 percent of the numbers removed, the resulting photograph was almost indistinguishable from the original. He went to the extreme of removing 95 percent of the signal's content using the wavelets to retain \"the information where the contrast is large,\" and still reconstructed a photograph unmistakably comparable to the original. ''By doing good signal processing and assigning bits rather than just deleting the smallest ones, it is possible to make the 5 percent picture look about as good as the original photograph,\" Press explained.\n\nPress pointed out that early insights into the wavelet phenomenon came from work on quadrature filter mirrors, and it is even conceivable that they may help to better resolve the compromised pictures coming back from the Hubble Space Telescope. And while image enhancement and compression may be the most accessible applications right now, wavelets could have other powerful signal-processing uses wherever data come by way of irregular waveforms, such as for speech recognition, in processing of the sound waves used in geological explorations, in the storage of graphic images, and\n\nExperimental Mathematics\n\nThe work of Jean Taylor, a mathematician from Rutgers University, illustrates another example of the computer's potential to both support research in pure mathematics and also to promote more general understanding. Taylor studies the shapes of surfaces that are stationary, or that evolve in time, looking for insights into how energies control shapes; the fruits of her work could have very direct applications for materials scientists. But the level at which she approaches these problems is highly theoretical, very complex, and specialized. She sees it as an outgrowth of work done by mathematician Jesse Douglas, who won one of the first Fields medals for his work on minimal surfaces. She illustrated one value of the computer by explaining to the symposium the dilemma she often faces when trying to respond to queries about what she does. She may begin to talk about minimal surfaces at their extremes, which turn into polyhedral problems; this suggests to her listeners that she does discrete and computational geometry, which, she says, as it is normally defined does not describe her approach. Work on minimal surfaces is usually classified as differential geometry, but her work seems to fall more into the nondifferential realm. Even trained geometers often resort to categorizing her in not-quite-accurate pigeonholes. \"It is much easier to show them, visually,\" said Taylor, who did just that for the symposium's audience with a pair of videos rich with graphical computer models of her work, and the theory behind it.\n\n\"Why do I compute?\" asked Taylor. \"I have developed an interaction with the program that makes it an instrumental part of my work. I can prove to others—but mainly to myself—that I really understand a construction by being able to program it.\" She is in the business of developing theorems about the equilibrium and growth shapes of crystals. The surfaces she studies often become \"singular, producing situations where the more classical motion by curvature breaks down,'' she emphasized. No highly mathematical theory alone can render such visual phenomena with the power and cogency of a picture made from evolving the shape with her original computer model (Figure 7.5).\n\nThis sophisticated dialectic between theory and computer simulation also \"suggests other theorems about other surface tension functions,\" continued Taylor, who clearly is a theoretical mathematician running experiments on her computer, and whose results often go far beyond merely confirming her conjectures.\n\nStephen Wolfram is ideally positioned to evaluate this trend. \"In the past,\" he told symposium participants, \"it was typically the case\n\nthat almost all mathematicians were theoreticians, but during the last few years there has arisen a significant cohort of mathematicians who are not theoreticians but instead are experimentalists. As in every other area of present day science, I believe that before too many years have passed, mathematics will actually have more experimentalists than theorists.\"\n\nIn his incisive survey of the computer and the changing face of science, The Dreams of Reason, physicist Heinz Pagels listed the rise of experimental mathematics among the central themes in what he considered \"a new synthesis of knowledge based in some general way on the notion of complexity. . . . The material force behind this change is the computer\" (Pagels, 1988, p. 36). Pagels maintained that the evolution concerns the very order of knowledge. Before the rise of empirical science, he said, the \"architectonic of the natural sciences\n\n(natural philosophy), in accord with Aristotelian canons, was established by the logical relation of one science or another\" (p. 39). In the next era, the instrumentation (such as the microscope) on which empirical science relied, promoted reductionism: \"the properties of the small things determined the behavior of the larger things,\" according to Pagels (p. 40), who asserted that with the computer's arrival \"we may begin to see the relation between various sciences in entirely new dimensions'' (p. 40). Pagels believed that the common tool of the computer, and the novel perspective it provides on the world, are provoking a horizontal integration among the sciences, necessary in order to \"restructure our picture of reality\" (p. 42).\n\nImplementing Software for Scientists\n\nOne of those providing a spotlight on this new picture of scientific reality is Wolfram, who runs Wolfram Research, Inc., and founded the Center for Complex Systems at the Urbana campus of the University of Illinois. Wolfram early realized the potential value of the computer to his work but found no adequate programming language that would fully utilize its power. He began working with a language called SMP and has since created and continued to refine a language of his own, which his company now markets as the software package Mathematica , used by some 200,000 scientists all over the world. He surveyed the Frontiers audience and found half of its members among his users.\n\nMathematica is a general system for doing mathematical computation and other applications that Press called \"a startlingly good tool.\" As such, it facilitates both numerical and symbolic computation, develops elaborate sound and graphics to demonstrate its results, and provides \"a way of specifying algorithms\" with a programming language that Wolfram hopes may one day be accepted as a primary new language in the scientific world, although he pointed out how slowly such conventions are established and adopted (Fortran from the 1960s and C from the early 1970s being those most widely used now). Wolfram expressed his excitement about the prospects to \"use that sort of high-level programming language to represent models and ideas in science. With such high-level languages, it becomes much more realistic to represent an increasing collection of the sorts of models that one has, not in terms of traditional algebraic formulae but instead in terms of algorithms, for which Mathematica provides a nice compact notation.\"\n\nSimilarly, noted Wolfram, \"One of the things that has happened as a consequence of Mathematica is a change in the way that at least some people teach calculus. It doesn't make a lot of sense anymore\n\nto focus a calculus course around the evaluation of particular integrals because the machine can do it quite well.\" One of the mathematicians in the audience, Steven Krantz from the session on dynamical systems, was wary of this trend, stating his belief that \"the techniques of integration are among the basic moves of a mathematician—and of a scientist. He has got to know these things.\" From his point of view, Wolfram sees things differently. \"I don't think the mechanics of understanding how to do integration are really relevant or important to the intellectual activity of science,\" he stated. He has found that, in certain curricula, Mathematica has allowed teachers who grasp it a freedom from the mechanics to focus more on the underlying ideas.\n\nTHE SCIENCES OF COMPLEXITY\n\nCellular Automata\n\nWolfram, according to Pagels, \"has also been at the forefront of the sciences of complexity\" (Pagels, 1988, p. 99). Wolfram is careful to distinguish the various themes and developments embraced by Pagels' term—which \"somehow relate to complex systems\"—from a truly coherent science of complexity, which he concedes at present is only \"under construction.\" At work on a book that may establish the basis for such a coherent view, Wolfram is a pioneer in one of the most intriguing applications of computation, the study of cellular automata. ''Computation is emerging as a major new approach to science, supplementing the long-standing methodologies of theory and experiment,\" said Wolfram. Cellular automata, \"cells'' only in the sense that they are unitary and independent, constitute the mathematical building blocks of systems whose behavior may yield crucial insights about form, complexity, structure, and evolution. But they are not \"real,\" they have no qualities except the pattern they grow into, and their profundities can be translated only through the process that created them: computer experimentation.\n\nThe concept of cellular automata was devised to explore the hypothesis that \"there is a set of mathematical mechanisms common to many systems that give rise to complicated behavior,\" Wolfram has explained, suggesting that the evolution of such chaotic or complex behavior \"can best be studied in systems whose construction is as simple as possible\" (Wolfram, 1984, p. 194). Cellular automata are nothing more than abstract entities arranged in one or two (conceivably and occasionally more) dimensions in a computer model, each one possessing \"a value chosen from a small set of possibilities, often just 0 and 1. The values of all cells in the cellular automaton are\n\nsimultaneously updated at each 'tick' of a clock according to a definite rule. The rule specifies the value of a cell, given its previous value and the values of its nearest neighbors or some other nearby set of cells\" (p. 194).\n\nWhat may seem at first glance an abstract and insular game turns out to generate remarkably prescient simulations that serve, according to Wolfram, \"as explicit models for a wide variety of physical systems\" (Wolfram, 1984, p. 194). The formation of snowflakes is one example. Beginning with a single cell, and with the rules set so that one state represents frozen and the other state water vapor, it can be shown that a given cell changes from a vapor state to a frozen state only when enough of its neighbors are vaporous and will not thereby inhibit the dissipation of enough heat to freeze. These rules conform to what we know about the physics of water at critical temperatures, and the resulting simulation based on these simple rules grows remarkably snowflake-like automata (Figure 7.6).\n\nCellular automata involve much more than an alternative to running complicated differential equations as a way of exploring snowflake and other natural structures that grow systematically; they may be viewed as analogous to the digital computer itself. \"Most of [their] properties have in fact been conjectured on the basis of patterns generated in computer experiments\" (Wolfram, 1984, p. 194). But their real power may come if they happen to fit the definition of a universal computer, a machine that can solve all computable problems. A computable problem is one \"that can be solved in a finite time by following definite algorithms\" (p. 197). Since the basic operation of a computer involves a myriad of simple binary changes, cellular automata—if the rules that govern their evolution are chosen appropriately—can be computers. And \"since any physical process can be represented as a computational process, they can mimic the action of any physical system as well\" (p. 198).\n\nResonant with the fundamental principles discussed in the session on dynamical systems, Wolfram's comments clarified the significance of these points: \"Differential equations give adequate models for the overall properties of physical processes,\" such as chemical reactions, where the goal is to describe changes in the total concentration of molecules. But he reminded the audience that what they are really seeing, when the roughness of the equations is fully acknowledged, is actually an average change in molecular concentration based on countless, immeasurable random walks of individual molecules. But the old science works. At least the results produced a close enough fit, for many systems and the models based on differential equations that described them. \"However,\" Wolfram continued,\n\nScience still has the same mission, but the vision of scientists in this new age has been refined. Formerly, faced with clear evidence of complexity throughout the natural world, scientists had to limit the range of vision, or accept uneasy and compromised descriptions. Now, the computer—in Pagels' words the ultimate \"instrument of complexity\"—makes possible direct examination and experimentation by way of computation, the practice of which he believed might deserve to be classed as a distinct branch of science, alongside the theoretical and the experimental. When colleagues object that computer runs are not real experiments, the people that Pagels wrote about, and others among the participants in the Frontiers symposium, might reply that, in many experimental situations, computation is the only legitimate way to simulate and capture what is really happening.\n\nWolfram's prophecy may bear repetition: \"As in every other area of present day science, I believe that before too many years have passed mathematics will actually have more experimentalists than theorists.\" One of the reasons for this must be that no other route to knowledge and understanding offers such promise, not just in mathematics but in many of the physical sciences as well. Pagels called it \"the rise of the computational viewpoint of physical processes,\" reminding us that the world is full of dynamical systems that are, in essence, computers themselves: \"The brain, the weather, the solar system, even quantum particles are all computers. They don't look like computers, of course, but what they are computing are the consequences of the laws of nature\" (Pagels, 1988, p. 45). Press talked about the power of traditional algorithms to capture underlying laws; Pagels expressed a belief that ''the laws of nature are algorithms that control the development of the system in time, just like real programs do for computers. For example, the planets, in moving around the sun, are doing analogue computations of the laws of Newton\" (p. 45).\n\nIndisputably, the computer provides a new way of seeing and modeling the natural world, which is what scientists do, in their efforts to figure out and explain its laws and principles. Computational science, believes Smarr, stands on the verge of a \"golden age,\" where the exponential growth in computer speed and memory will not only continue (Box 7.4), but together with other innovations achieve something of a critical mass in fusing a new scientific world: \"What is happening with most sciences is the transformation of science to a digital form. In the 1990s, a national information infrastructure to support digital science will arise, which will hook together supercomputers, massive data archives, observational and experimental instruments, and millions of desktop computers.\" We are well on the way, he believes, to \"becoming an electronic scientific community\" (Smarr, 1991, p. 101)."
    }
}
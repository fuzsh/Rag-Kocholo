{
    "id": "correct_foundationPlace_00051_1",
    "rank": 92,
    "data": {
        "url": "https://barreto109.rssing.com/chan-52653888/all_p2.html",
        "read_more_link": "",
        "language": "en",
        "title": "Jose Barreto's Blog",
        "top_image": "",
        "meta_img": "",
        "images": [
            "https://pixel.quantserve.com/pixel/p-KygWsHah2_7Qa.gif",
            "https://blogs.technet.com/aggbug.aspx?PostID=3648875&AppID=3813&AppType=Weblog&ContentType=0",
            "https://blogs.technet.com/aggbug.aspx?PostID=3648876&AppID=3813&AppType=Weblog&ContentType=0",
            "https://blogs.technet.com/cfs-file.ashx/__key/communityserver-blogs-components-weblogfiles/00-00-00-38-13-metablogapi/image_5F00_thumb_5F00_37B5A949.png",
            "https://blogs.technet.com/aggbug.aspx?PostID=3649181&AppID=3813&AppType=Weblog&ContentType=0",
            "https://blogs.technet.com/aggbug.aspx?PostID=3650022&AppID=3813&AppType=Weblog&ContentType=0",
            "https://blogs.technet.com/aggbug.aspx?PostID=3651721&AppID=3813&AppType=Weblog&ContentType=0",
            "https://msdnshared.blob.core.windows.net/media/TNBlogsFS/prod.evol.blogs.technet.com/CommunityServer.Blogs.Components.WeblogFiles/00/00/00/38/13/metablogapi/image_63399A47.png",
            "https://msdnshared.blob.core.windows.net/media/TNBlogsFS/prod.evol.blogs.technet.com/CommunityServer.Blogs.Components.WeblogFiles/00/00/00/38/13/metablogapi/image_6C292C86.png",
            "https://msdnshared.blob.core.windows.net/media/TNBlogsFS/prod.evol.blogs.technet.com/CommunityServer.Blogs.Components.WeblogFiles/00/00/00/38/13/metablogapi/image_66DCD686.png",
            "https://msdnshared.blob.core.windows.net/media/TNBlogsFS/prod.evol.blogs.technet.com/CommunityServer.Blogs.Components.WeblogFiles/00/00/00/38/13/metablogapi/clip_image002_58D2943D.jpg",
            "https://msdnshared.blob.core.windows.net/media/TNBlogsFS/prod.evol.blogs.technet.com/CommunityServer.Blogs.Components.WeblogFiles/00/00/00/38/13/metablogapi/clip_image003_5F1BFB7C.jpg",
            "https://msdnshared.blob.core.windows.net/media/TNBlogsFS/prod.evol.blogs.technet.com/CommunityServer.Blogs.Components.WeblogFiles/00/00/00/38/13/metablogapi/clip_image004_48A89B4E.jpg",
            "https://msdnshared.blob.core.windows.net/media/TNBlogsFS/prod.evol.blogs.technet.com/CommunityServer.Blogs.Components.WeblogFiles/00/00/00/38/13/metablogapi/clip_image006_257B309C.jpg",
            "https://msdnshared.blob.core.windows.net/media/TNBlogsFS/prod.evol.blogs.technet.com/CommunityServer.Blogs.Components.WeblogFiles/00/00/00/38/13/metablogapi/clip_image008_79880061.jpg",
            "https://msdnshared.blob.core.windows.net/media/TNBlogsFS/prod.evol.blogs.technet.com/CommunityServer.Blogs.Components.WeblogFiles/00/00/00/38/13/metablogapi/clip_image009_3D8370BF.jpg",
            "https://msdnshared.blob.core.windows.net/media/2016/03/LogSplitter.png",
            "https://msdnshared.blob.core.windows.net/media/2016/04/Build-2016-logo.png",
            "https://msdnshared.blob.core.windows.net/media/2016/09/SDC-2016-Banner1.png",
            "https://msdnshared.blob.core.windows.net/media/2016/12/Organize-Pictures.png",
            "https://msdnshared.blob.core.windows.net/media/2016/12/Jan1-300x130.jpg",
            "https://msdnshared.blob.core.windows.net/media/2016/12/Jan2-300x224.png",
            "https://msdnshared.blob.core.windows.net/media/2016/12/Jan3-300x136.png",
            "https://msdnshared.blob.core.windows.net/media/2016/12/Feb1-252x300.png",
            "https://msdnshared.blob.core.windows.net/media/2016/12/Feb2-300x126.png",
            "https://msdnshared.blob.core.windows.net/media/2016/12/Mar1-300x227.jpg",
            "https://msdnshared.blob.core.windows.net/media/2016/12/Mar2-300x216.jpg",
            "https://msdnshared.blob.core.windows.net/media/2016/12/Mar3-174x300.jpg",
            "https://msdnshared.blob.core.windows.net/media/2016/12/Apr1-300x131.jpg",
            "https://msdnshared.blob.core.windows.net/media/2016/12/Apr2-300x270.jpg",
            "https://msdnshared.blob.core.windows.net/media/2016/12/Apr3-300x169.jpg",
            "https://msdnshared.blob.core.windows.net/media/2016/12/May1-300x162.png",
            "https://msdnshared.blob.core.windows.net/media/2016/12/May2-300x210.jpg",
            "https://msdnshared.blob.core.windows.net/media/2016/12/Jun1-300x187.jpg",
            "https://msdnshared.blob.core.windows.net/media/2016/12/Jun2-262x300.jpg",
            "https://msdnshared.blob.core.windows.net/media/2016/12/Jul1-290x300.jpg",
            "https://msdnshared.blob.core.windows.net/media/2016/12/jul2-300x212.jpg",
            "https://msdnshared.blob.core.windows.net/media/2016/12/Aug1-300x123.jpg",
            "https://msdnshared.blob.core.windows.net/media/2016/12/Aug2-300x122.jpg",
            "https://msdnshared.blob.core.windows.net/media/2016/12/Sep1-300x265.jpg",
            "https://msdnshared.blob.core.windows.net/media/2016/12/Sep2-300x156.jpg",
            "https://msdnshared.blob.core.windows.net/media/2016/12/Sep3-300x144.jpg",
            "https://msdnshared.blob.core.windows.net/media/2016/12/Oct1-300x206.jpg",
            "https://msdnshared.blob.core.windows.net/media/2016/12/Oct2-241x300.jpg",
            "https://s.w.org/images/core/emoji/2/72x72/1f642.png",
            "https://msdnshared.blob.core.windows.net/media/2016/12/Nov1-300x196.jpg",
            "https://msdnshared.blob.core.windows.net/media/2016/12/Nov2-300x133.jpg",
            "https://msdnshared.blob.core.windows.net/media/2016/12/Dec1-280x300.jpg",
            "https://msdnshared.blob.core.windows.net/media/2016/12/Dec2.jpg",
            "https://msdnshared.blob.core.windows.net/media/2017/02/012.png",
            "https://msdnshared.blob.core.windows.net/media/2017/02/022-1024x346.png",
            "https://msdnshared.blob.core.windows.net/media/2017/02/032-1024x710.png",
            "https://msdnshared.blob.core.windows.net/media/2017/02/042-1024x773.png",
            "https://msdnshared.blob.core.windows.net/media/2017/02/052-1024x695.png",
            "https://msdnshared.blob.core.windows.net/media/2017/02/061.png",
            "https://msdnshared.blob.core.windows.net/media/2017/02/071.png",
            "https://msdnshared.blob.core.windows.net/media/2017/02/081-1024x409.png",
            "https://msdnshared.blob.core.windows.net/media/2017/02/091-1024x555.png",
            "https://msdnshared.blob.core.windows.net/media/2017/02/104-1024x598.png",
            "https://msdnshared.blob.core.windows.net/media/2017/02/1110-1024x678.png",
            "https://msdnshared.blob.core.windows.net/media/2017/02/124-406x1024.png",
            "https://msdnshared.blob.core.windows.net/media/2017/02/132.png",
            "https://augustacrime.com/wp-content/uploads/2019/07/Jessica-Carpenter-37-Simple-battery-150x150.jpg",
            "https://4.bp.blogspot.com/-aZzamp_aOzU/UhTjnrtlo1I/AAAAAAAAAVo/KMNDjN9CuVs/s640/Celestial+Blue.jpg",
            "https://busyteacher.org/uploads/posts/2016-12/thumbs/1482019946_california-worksheet-0.png",
            "https://augustacrime.com/wp-content/uploads/2017/01/imageCRESTONCURRY.jpg",
            "https://www.greytrix.com/blogs/sagex3/wp-content/uploads/2021/11/Fig02-Export-customization-screen-1024x500.png",
            "https://thepost.s3.amazonaws.com/wp-content/uploads/2014/08/0CA0F25I-150x150.jpg",
            "https://3.bp.blogspot.com/-HBHyCcGBgf8/Wn1yke7zLmI/AAAAAAAAEhI/K7xGK0DnW94EUDg4PXG7m9d5uDemlygEgCLcBGAs/s400/class12-bilogy-ncert-solutions-in-hindi-ch3-2.png",
            "https://i.imgur.com/yTvNsOQl.jpg",
            "https://busyteacher.org/uploads/posts/2012-11/thumbs/1353086477_make-or-do-collocations-key-0.png",
            "https://1.bp.blogspot.com/-m_gxW9wUrow/YWpAej2ComI/AAAAAAAADwI/lPbemMAKTPcFQMerHxF4zzzDiJkzDqjeACLcBGAsYHQ/w300-h400/WhatsApp%2BImage%2B2021-10-16%2Bat%2B11.00.18%2BAM.jpeg",
            "https://e2e.ti.com//resized-image/__size/1230x0/__key/communityserver-discussions-components-files/6/7608._F764D653_.PNG",
            "https://lh5.ggpht.com/-drHHw9Z5ZwY/Tmp1gKWmZ1I/AAAAAAAALFc/8dMBz_U8mJw/s400/dragon_ball_73.gif",
            "https://2.bp.blogspot.com/-7TLcIsTWCRM/WJdhZJ8T1vI/AAAAAAAANlA/7-XTO95WE1494iVWxC5MIpWXckxuscj3QCLcB/s640/meenakshi%2Bjoshi%2Banchor.jpg",
            "https://i.imgur.com/E2aWYSh.jpg",
            "https://www.learncbse.in/wp-content/uploads/2017/08/NCERT-Solutions-for-Class-9th-Sanskrit-Chapter-1-अपठित-अवबोधनम्-2.jpg",
            "https://s3.amazonaws.com/nixle/uploads/pub_media/md/user24872-1464275450-media1_a5a7a6_240_180_PrsMe_.jpeg",
            "https://chrisukorg.files.wordpress.com/2015/02/perry.jpg?w=529&h=511",
            "https://3.bp.blogspot.com/-NP1u31H_hVE/WIJo02ELdQI/AAAAAAAAADY/eCMDy6ZuNoESiAXT9rhyePg9W9gujKiKwCEw/s400/p.txt.jpg",
            "https://4.bp.blogspot.com/-D2ybkIpqREU/Wti4IoCYwLI/AAAAAAAABEg/UTVmhPWy1QcrZg6t-w8mJpp49Ho6MfwfQCLcBGAs/s640/Anasuya%2B3.png",
            "https://www.digitalkhabar.in/wp-content/uploads/Happy-Birthday-Bhabhi-in-Hindi.jpg",
            "https://www.thesun.co.uk/wp-content/uploads/2024/07/chyna-mills-neil-jones-baby-919048408.jpg?strip=all&w=768",
            "https://jpcdn.it/img/small/71850111dc29343c90e84c90a0de1a2e.jpg",
            "https://cdn-images-1.medium.com/max/1024/0*hX1Okh3RlOSKECGM.png",
            "https://www.thesun.co.uk/wp-content/uploads/2024/07/eastenders-exact-date-unknown-1164867.jpg?strip=all&w=824",
            "https://community.cadence.com/resized-image/__size/1280x960/__key/communityserver-discussions-components-files/28/pastedimage1721900432893v1.png",
            "https://i.etsystatic.com/5772952/r/il/059bcf/1488310855/il_570xN.1488310855_96je.jpg",
            "https://www.the-sun.com/wp-content/uploads/sites/6/2024/07/self-service-checkout-walmart-megastore-919328587.jpg?strip=all&w=960",
            "https://media.adverts.ie/eyJidWNrZXQiOiJtZWRpYS5hZHNpbWcuY29tIiwia2V5IjoiYzU0ZTQ1MjJhMDgyMGNlZjExYzNhNDcyODQ0YzQ0NmY5YjAwMGZiMzEwNGJlN2ZkNGZlYWU3YzE0ODkyMGVkMi5qcGciLCJvdXRwdXRGb3JtYXQiOiJqcGVnIiwiZWRpdHMiOnsicmVzaXplIjp7IndpZHRoIjoyMjcsImhlaWdodCI6MTg3fX19?signature=12517ddb0860daea228c84c6763d11c504f91785b6d74441e8272264aff45f8b",
            "https://i.etsystatic.com/5465916/r/il/bce7ff/1698716462/il_570xN.1698716462_dni8.jpg",
            "https://i.etsystatic.com/5376867/r/il/d97efc/6109916569/il_570xN.6109916569_aboq.jpg"
        ],
        "movies": [],
        "keywords": [],
        "meta_keywords": [
            ""
        ],
        "tags": null,
        "authors": [],
        "publish_date": null,
        "summary": "",
        "meta_description": "",
        "meta_lang": "en",
        "meta_favicon": "//www.rssing.com/favicon.ico",
        "meta_site_name": "",
        "canonical_link": null,
        "text": "SMB3 Networking Links for Windows Server 2012 R2\n\nSomeone asked me for a good collection of links about SMB3 Networking, including SMB Multichannel, SMB Direct (SMB over RDMA) and SMB Performance.\n\nHere’s what I offered:\n\nSMB Multichannel\n\nTechEd 2012 - SMB Multichannel and NIC Teaming (SMB Multichannel covered on the second half)\n\nJose’s Blog - The basics of SMB Multichannel, a feature of Windows Server 2012 and SMB 3.0\n\nTechNet Radio - SMB Multi-channel Basics for Windows Server 2012 and SMB 3.0 (October 8th 2012)\n\nJose’s Blog - Troubleshooting File Server Networking Issues in Windows Server 2012 R2\n\nJose's Blog - Networking configurations for Hyper-V over SMB in Windows Server 2012 and Windows Server 2012 R2\n\nTechEd 2014 NA - DCIM-B337 - File Server Networking for a Private Cloud Storage Infrastructure in Windows Server 2012 R2 (includes overview of the Hyper-V over SMB scenario in Windows Server 2012 R2 with focus on Networking features)\n\nSMB Direct\n\nTechEd 2012 - SMB Direct (SMB over RDMA) and Cluster-in-a-box (SMB direct covered on the second half)\n\nTechNet – Deploy SMB Direct with InfiniBand Network Adapters\n\nTechNet – Deploy SMB Direct with RoCE Network Adapters\n\nTechNet – Deploy SMB Direct with Ethernet (iWARP) Network Adapters\n\nSMB Performance\n\nJose's Blog - MMS 2013 Demo: Hyper-V over SMB at high throughput with SMB Direct and SMB Multichannel\n\nJose’s Blog - Hyper-V over SMB – Performance considerations\n\nTechEd 2013 - MDC-B335 - Understanding the Hyper-V over SMB Scenario, Configurations, and End-to-End Performance (includes new SMB features in Windows Server 2012 R2)\n\nJose’s Blog - DiskSpd, PowerShell and storage performance: measuring IOPs, throughput and latency for both local disks and SMB file shares\n\nJose’s Blog - Using file copy to measure storage performance – Why it’s not a good idea and what you should do instead\n\nPerformance White Papers\n\nWindows Server 2012 File Server Performance with EchoStreams FlacheSAN2 and Mellanox ConnectX-3: http://www.microsoft.com/en-us/download/details.aspx?id=38432\n\nWindows File Server SMB SQL OLTP Performance: http://www.microsoft.com/en-us/download/details.aspx?id=36793\n\nBuilding High Performance Storage for Hyper-V Cluster on Scale-Out File Servers using Violin Windows Flash Arrays: http://www.microsoft.com/en-us/download/details.aspx?id=44300\n\nMicrosoft Cloud Platform System Storage Performance: http://blogs.technet.com/b/windowsserver/archive/2015/03/09/microsoft-cloud-platform-system-storage-performance-overview-now-available.aspx\n\nSMB Encryption\n\nTechNet Radio - SMB 3.0 Encryption Overview (November 26th 2012)\n\nSMB Signing\n\nJose’s Blog - The Basics of SMB Signing (covering both SMB1 and SMB2)\n\n↧\n\n↧\n\nWhat’s new in SMB 3.1.1 in the Windows Server 2016 Technical Preview 2\n\n1. Introduction\n\nEvery new version of Windows brings updates to our main remote file protocol, known as SMB (Server Message Block).\n\nIf you’re not familiar with it, you can find some information in this previous blog post: Windows Server 2012 R2: Which version of the SMB protocol (SMB 1.0, SMB 2.0, SMB 2.1, SMB 3.0 or SMB 3.02) are you using?\n\nIn this blog post, you’ll see what changed with the new version of SMB that comes with the Windows 10 Insider Preview released in late April 2015 and the Windows Server 2016 Technical Preview 2 released in early May 2015.\n\n2. Protocols Changes in SMB 3.1.1\n\nThis section covers changes in SMB 3.1.1 related to the protocol itself.\n\nThe Protocol Preview document fully describes these changes: [MS-SMB2-Diff]- Server Message Block (SMB) Protocol Versions 2 and 3, but you can see the highlights below.\n\n2.1. Pre-Authentication Integrity\n\nPre-authentication integrity provides improved protection from a man-in-the-middle attacker tampering with SMB’s connection establishment and authentication messages.\n\nPre-Auth integrity verifies all the “negotiate” and “session setup” exchanges used by SMB with a strong cryptographic hash (SHA-512).\n\nIf your client and your server establish an SMB 3.1.1 session, you can be sure that no one has tampered with the connection and session properties.\n\nUsing SMB signing on top of an SMB 3.1.1 session protects you from an attacker tampering with any packets.\n\nUsing SMB encryption on top of an SMB 3.1.1 session protects you from an attacker tampering with or eavesdropping on any packets.\n\nAlthough there is a cost to enable SMB signing or SMB encryption, we highly recommend enabling one of them.\n\nNote: While these changes improve overall security, they might interfere with some solutions that rely on modifying SMB network traffic, like certain kinds of WAN accelerators.\n\n2.2. SMB Encryption Improvements\n\nSMB Encryption, introduced with SMB 3.0, used a fixed cryptographic algorithm: AES-128-CCM.\n\nSince then, we have learned that AES-128-GCM performs better in most modern processors.\n\nTo take advantage of that, SMB 3.1.1 offers a mechanism to negotiate the crypto algorithm per connection, with options for AES-128-CCM and AES-128-GCM.\n\nWe made AES-128-GCM the default for new Windows versions, while older versions will continue to use AES-128-CCM.\n\nWith this flexible infrastructure for negotiation in place, we could add more algorithms in the future.\n\nWe observed that moving from AES-128-CCM to AES-128-GCM showed a 2x improvement in certain scenarios, like copying large files over an encrypted SMB connection.\n\n2.3. Cluster Dialect Fencing\n\nProvides support for cluster rolling upgrade for Scale-Out File Servers. For details, see http://technet.microsoft.com/en-us/library/dn765474.aspx#BKMK_RollingUpgrade\n\nIn this new scenario, a single SMB server appears to support different maximum dialects of SMB, depending on whether the SMB client is accessing clustered or non-clustered file shares.\n\nFor local, non-clustered file shares, the server offers up to 3.1.1 during dialect negotiation.\n\nFor clustered shares, if the cluster is in mixed mode (before upgrading the cluster functional level), it will offer up to 3.0.2 during dialect negotiation.\n\nAfter you upgrade the cluster functional level, it then offers all clients the new 3.1.1 dialect.\n\n3. Other SMB changes that are not protocol-related\n\nThere are other changes in Windows that change the SMB Client or SMB Server implementation, but not the protocol itself.\n\nHere are a few important changes in that category:\n\n3.1. Removing RequireSecureNegotiate setting\n\nIn previous versions of SMB, we introduced “Secure Negotiate”, where the SMB client and server verify integrity of the SMB negotiate request and response messages.\n\nBecause some third-party implementations of SMB did not correctly perform this negotiation, we introduced a switch to disable “Secure Negotiate”. We explain this in more detail in this blog post.\n\nSince we have learned via our SMB PlugFests that third parties have fixed their implementations, we are removing the option to bypass “Secure Negotiate” and SMB always performs negotiate validation if the connection’s dialect is 2.x.x or 3.0.x.\n\nNote 1: For SMB 3.1.1 clients and servers, the new Pre-Authentication Integrity feature (described in item 2.1 above) supersedes “Secure Negotiate” with many advantages.\n\nNote 2: With the new release, any third party SMB 2.x.x or SMB 3.0.x implementations that do not implement “Secure Negotiate” will be unable to connect to Windows.\n\nNote 3: While this change improves overall security, it might interfere with some solutions that rely on modifying SMB network traffic, like certain kinds of WAN accelerators.\n\n3.2. Dialects with non-zero revision number now reported with the x.y.z notation\n\nAs you probably noticed throughout this blog post, we’re using 3 separate digits to notate the version of SMB.\n\nIn the past, you might have seen us talk about SMB 3.02. Now we call that SMB 3.0.2.\n\nNote that there is no change when the revision number is 0, like SMB 2.1 or SMB 3.0 (we don’t call them SMB 2.1.0 or SMB 3.0.0).\n\nThis new format avoids confusion when comparing SMB dialects and better represents the actual version information used by SMB.\n\nYou can use the Get-SmbConnection cmdlet on the Windows SMB client to report the currently used SMB dialects.\n\n4. Which protocol is negotiated?\n\nPlease note that SMB clients and SMB servers negotiate the SMB dialect that they will use based on each side’s offer.\n\nHere’s a table to help you understand what version you will end up using, depending on what Windows version is running as the SMB client and what version of Windows is running as the SMB server:\n\n* WS = Windows Server\n\nNote: Earlier Windows 10 and Windows Server 2016 previews used SMB dialect version 3.1.\n\n5. Considering your options for removing the older SMB1 protocol\n\nWhen Windows Server 2003 hits the end of its extended support later this year, the last supported version of Windows that only works with SMB1 will be gone.\n\nSMB1 is already a separate component in Windows that you can completely remove. However, up to this point, Windows still enables it by default for compatibility reasons.\n\nThe next logical step (which we are planning for a future release of Windows) will be to ship SMB1 disabled by default, but still available if necessary.\n\nTo help with this transition, you can now enable auditing of SMB1 traffic in your SMB server using PowerShell. This will alert you via events if any clients are still using SMB1.\n\nTo enable auditing of SMB1 traffic, use the cmdlet: Set-SmbServerConfiguration –AuditSmb1Access $true\n\nTo view the SMB1 events, use the cmdlet: Get-WinEvent -LogName Microsoft-Windows-SMBServer/Audit\n\nIf you feel confident that there are no SMB1 clients in your network, you can uninstall SMB1 from your server using the cmdlet: Remove-WindowsFeature FS-SMB1\n\n6. Conclusion\n\nI hope this blog post helps you prepare for the upcoming changes in SMB.\n\nWe also recommend that you download the SNIA Tutorial on SMB 3, which we recently updated to include details of the 3.1.1 dialect. You can find a copy of that tutorial at http://www.snia.org/sites/default/files2/DSI2015/presentations/FileSystems/JoseBarreto_SMB3_remote%20file%20protocol.pdf\n\n↧\n\nWindows Server 2016 Technical Preview 2 (TP2) and Storage Quality of Service (QoS)\n\nStorage Quality of Service (Storage QoS) is a new feature in the upcoming Windows Server 2016 that provides a way to centrally monitor and manage storage performance for virtual machines. The feature automatically improves storage resource fairness between multiple virtual machines using the same file server cluster and allows specific minimum and maximum performance goals (Storage QoS policies) to be configured in units of normalized 8KB IOPs.\n\nWith the release of Windows Server 2016 Technical Preview 2 (TP2), the Storage QoS feature is available for testing. Virtual Machine Manager 2016 TP2 also includes the ability to manage Storage QoS. Find below some important links to review so you can experiment with Storage QoS in Windows Server 2016 TP2:\n\nDownload link for Windows Server 2016 TP2 and System Center 2016 TP2:\n\nhttps://www.microsoft.com/en-us/evalcenter/evaluate-windows-server-technical-preview\n\nhttps://www.microsoft.com/en-us/evalcenter/evaluate-system-center-technical-preview\n\nTechNet Guide for Windows Server 2016 TP2 and Storage QoS:\n\nStorage Quality of Service in Windows Server Technical Preview\n\nMicrosoft Ignite 2015 session on Windows Server 2016 TP2 and Storage QoS:\n\nHyper-V Storage Performance with Storage Quality of Service\n\nStorage QoS demos (videos):\n\nWindows Server 2016 TP2 Storage QoS: PowerShell Monitoring\n\nWindows Server 2016 TP2 Storage QoS: Managing policies with VMM\n\nWindows Server 2016 TP2 Storage QoS: Managing Policies with PowerShell\n\nWindows Server 2016 TP2 Storage QoS: Managing with a PowerShell script\n\nStorage QoS script samples\n\nTechNet: Storage Quality of Service in Windows Server Technical Preview\n\nBlog: PowerShell script used in the Windows Server 2016 TP2 Storage QoS demo at MSIgnite\n\n↧\n\nNew PowerShell cmdlets in Windows Server 2016 TP2 (compared to Windows Server 2012 R2)\n\n1. State the problem\n\nWith the release of Windows Server 2016 TP2 a few weeks ago, I was wondering what new PowerShell cmdlets are now included (when you compare to Windows Server 2012 R2). However, the list of cmdlets is so long now that it is hard to spot the differences by hand.\n\nHowever, there a cmdlet in PowerShell to show all the cmdlets available (Get-Command) and a little bit of programming would make it easy to find out what are the main differences. So I set out to collect the data and compare the list.\n\nDISCLAIMER: As you probably know already, the Technical Preview is subject to change so all the information about Windows Server 2016 TP2 is preliminary and may not make it into the final product. Use with care, your mileage may vary, not available in all areas, some restrictions apply, professional PowerShell operator on a closed Azure VM course, do not attempt.\n\n2. Gather the data\n\nFirst, I needed the list of cmdlets from both versions of the operating system. That was actually pretty easy to gather, with a little help from Azure. I basically provisioned two Azure VM, one running Windows Server 2012 R2 and one running Windows Server 2016 Technical Preview 2 (yes, TP2 is now available in the regular Azure VM image gallery).\n\nSecond, I installed all of the Remote Server Administration Tools (RSAT) on both versions. That loads the PowerShell modules used for managing features that are not installed by default, like Failover Cluster or Storage Replica.\n\nFinally, I ran a simple cmdlet to gather the list from Get-Command and save it to an XML file. This made it easier to put all the data I needed in a single place (my desktop machine running Windows 10 Insider Preview). Here's a summary of what it took:\n\nCreate WS 2012 R2 Azure VM\n\nInstall RSAT in the WS 2012 R2 VM\n\nGet-WindowsFeature RSAT* | Install-WindowsFeature\n\nCapture XML file with all the WS 2012 R2 cmdlet information\n\nGet-Command | Select * | Export-CliXml C:\\WS2012R2Cmdlets.XML\n\nCreate WS 2016 TP2 Azure VM\n\nInstall RSAT in the WS 2016 TP2 VM\n\nGet-WindowsFeature RSAT* | Install-WindowsFeature\n\nCapture XML file with all the WS 2016 TP2 cmdlet information\n\nGet-Command | Select * | Export-CliXml C:\\WS2016TP2Cmdlets.XML\n\n3. Process the data\n\nWith the two XML files at hand, all I had left to do was to compare them to produce a good list of what's new. The first attempt resulted in a long list that was hard to understand, so I decided to do it module by module.\n\nThe code starts by creating a combined list of modules from both operating systems. Then it builds a dictionary of all cmdlets for a given module, assigning the value 1 if it's in WS 2012 R2, 2 if it's in WS 2016 TP2 and 3 if it's in both.\n\nThen I would show the total number of cmdlets per module per OS, then number of new cmdlets and the actual list of new cmdlets. Since the goal was to publish this blog, I actually wrote the script to format the output as an HTML table. Quite handy :-).\n\n4. Show the results\n\nFinally, here is resulting table with all the new PowerShell cmdlets (by module) in Windows Server 2016 TP2, compared to Windows Server 2012. Enjoy!\n\nModuleNew CmdletsWS 2016 TP2\n\nCmdletsWS 2012 R2\n\nCmdlets03838ActiveDirectory0147147ADRMSAdmin02121AppLocker055Appx8146+ Add-AppxVolume\n\n+ Dismount-AppxVolume\n\n+ Get-AppxDefaultVolume\n\n+ Get-AppxVolume\n\n+ Mount-AppxVolume\n\n+ Move-AppxPackage\n\n+ Remove-AppxVolume\n\n+ Set-AppxDefaultVolumeBestPractices044BitLocker01313BitsTransfer088BranchCache03232CimCmdlets01414CIPolicy110+ ConvertFrom-CIPolicyClusterAwareUpdating01717ConfigCI10100+ Edit-CIPolicyRule\n\n+ Get-CIPolicy\n\n+ Get-CIPolicyInfo\n\n+ Get-SystemDriver\n\n+ Merge-CIPolicy\n\n+ New-CIPolicy\n\n+ New-CIPolicyRule\n\n+ Remove-CIPolicyRule\n\n+ Set-HVCIOptions\n\n+ Set-RuleOptionDefender11110+ Add-MpPreference\n\n+ Get-MpComputerStatus\n\n+ Get-MpPreference\n\n+ Get-MpThreat\n\n+ Get-MpThreatCatalog\n\n+ Get-MpThreatDetection\n\n+ Remove-MpPreference\n\n+ Remove-MpThreat\n\n+ Set-MpPreference\n\n+ Start-MpScan\n\n+ Update-MpSignatureDFSN02323DFSR34542+ Get-DfsrDelegation\n\n+ Grant-DfsrDelegation\n\n+ Revoke-DfsrDelegationDhcpServer0121121DirectAccessClientComponents01111Dism44339+ Add-WindowsCapability\n\n+ Expand-WindowsCustomDataImage\n\n+ Get-WindowsCapability\n\n+ Remove-WindowsCapabilityDnsClient01717DnsServer21122101+ Add-DnsServerClientSubnet\n\n+ Add-DnsServerQueryResolutionPolicy\n\n+ Add-DnsServerRecursionScope\n\n+ Add-DnsServerZoneScope\n\n+ Add-DnsServerZoneTransferPolicy\n\n+ Disable-DnsServerPolicy\n\n+ Enable-DnsServerPolicy\n\n+ Get-DnsServerClientSubnet\n\n+ Get-DnsServerQueryResolutionPolicy\n\n+ Get-DnsServerRecursionScope\n\n+ Get-DnsServerZoneScope\n\n+ Get-DnsServerZoneTransferPolicy\n\n+ Remove-DnsServerClientSubnet\n\n+ Remove-DnsServerQueryResolutionPolicy\n\n+ Remove-DnsServerRecursionScope\n\n+ Remove-DnsServerZoneScope\n\n+ Remove-DnsServerZoneTransferPolicy\n\n+ Set-DnsServerClientSubnet\n\n+ Set-DnsServerQueryResolutionPolicy\n\n+ Set-DnsServerRecursionScope\n\n+ Set-DnsServerZoneTransferPolicyEventTracingManagement14140+ Add-EtwTraceProvider\n\n+ Get-AutologgerConfig\n\n+ Get-EtwTraceProvider\n\n+ Get-EtwTraceSession\n\n+ New-AutologgerConfig\n\n+ New-EtwTraceSession\n\n+ Remove-AutologgerConfig\n\n+ Remove-EtwTraceProvider\n\n+ Remove-EtwTraceSession\n\n+ Send-EtwTraceSession\n\n+ Set-AutologgerConfig\n\n+ Set-EtwTraceProvider\n\n+ Set-EtwTraceSession\n\n+ Start-AutologgerConfigFailoverClusters28482+ New-ClusterNameAccount\n\n+ Update-ClusterFunctionalLevelGroupPolicy02929HgsClient11110+ Export-HgsGuardian\n\n+ Get-HgsAttestationBaselinePolicy\n\n+ Get-HgsClientConfiguration\n\n+ Get-HgsGuardian\n\n+ Grant-HgsKeyProtectorAccess\n\n+ Import-HgsGuardian\n\n+ New-HgsGuardian\n\n+ New-HgsKeyProtector\n\n+ Remove-HgsGuardian\n\n+ Revoke-HgsKeyProtectorAccess\n\n+ Set-HgsClientConfigurationHyper-V26204178+ Add-VMGroupMember\n\n+ Add-VMSwitchTeamMember\n\n+ Add-VMTPM\n\n+ Disable-VMConsoleSupport\n\n+ Enable-VMConsoleSupport\n\n+ Get-VHDSet\n\n+ Get-VHDSnapshot\n\n+ Get-VMGroup\n\n+ Get-VMHostCluster\n\n+ Get-VMSwitchTeam\n\n+ Get-VMTPM\n\n+ Get-VMVideo\n\n+ New-VMGroup\n\n+ Optimize-VHDSet\n\n+ Remove-VHDSnapshot\n\n+ Remove-VMGroup\n\n+ Remove-VMGroupMember\n\n+ Remove-VMSwitchTeamMember\n\n+ Rename-VMGroup\n\n+ Set-VMHostCluster\n\n+ Set-VMSwitchTeam\n\n+ Set-VMTPM\n\n+ Set-VMVideo\n\n+ Start-VMTrace\n\n+ Stop-VMTrace\n\n+ Update-VMVersionIISAdministration17170+ Get-IISAppPool\n\n+ Get-IISConfigCollectionItem\n\n+ Get-IISConfigElement\n\n+ Get-IISConfigSection\n\n+ Get-IISConfigValue\n\n+ Get-IISServerManager\n\n+ Get-IISSite\n\n+ New-IISConfigCollectionItem\n\n+ New-IISSite\n\n+ Remove-IISConfigCollectionItem\n\n+ Remove-IISSite\n\n+ Reset-IISServerManager\n\n+ Set-IISConfigValue\n\n+ Start-IISCommitDelay\n\n+ Start-IISSite\n\n+ Stop-IISCommitDelay\n\n+ Stop-IISSiteInternational01818iSCSI01313IscsiTarget02828ISE033Kds066Microsoft.PowerShell.Archive220+ Compress-Archive\n\n+ Expand-ArchiveMicrosoft.PowerShell.Core56055+ Debug-Job\n\n+ Enter-PSHostProcess\n\n+ Exit-PSHostProcess\n\n+ Get-PSHostProcessInfo\n\n+ Register-ArgumentCompleterMicrosoft.PowerShell.Diagnostics055Microsoft.PowerShell.Host022Microsoft.PowerShell.Management48682+ Clear-RecycleBin\n\n+ Get-Clipboard\n\n+ Get-ItemPropertyValue\n\n+ Set-ClipboardMicrosoft.PowerShell.ODataUtils110+ Export-ODataEndpointProxyMicrosoft.PowerShell.Security01313Microsoft.PowerShell.Utility1110594+ ConvertFrom-String\n\n+ Convert-String\n\n+ Debug-Runspace\n\n+ Disable-RunspaceDebug\n\n+ Enable-RunspaceDebug\n\n+ Format-Hex\n\n+ Get-Runspace\n\n+ Get-RunspaceDebug\n\n- GetStreamHash\n\n+ New-Guid\n\n+ New-TemporaryFile\n\n+ Wait-Debugger\n\n+ Write-InformationMicrosoft.WSMan.Management01313MMAgent055MsDtc04141NetAdapter46864+ Disable-NetAdapterPacketDirect\n\n+ Enable-NetAdapterPacketDirect\n\n+ Get-NetAdapterPacketDirect\n\n+ Set-NetAdapterPacketDirectNetConnection022NetEventPacketCapture02323NetLbfo01313NetNat01313NetQos044NetSecurity08585NetSwitchTeam077NetTCPIP03434NetWNV01919NetworkConnectivityStatus044NetworkController1411410+ Add-NetworkControllerNode\n\n+ Clear-NetworkControllerNodeContent\n\n+ Disable-NetworkControllerNode\n\n+ Enable-NetworkControllerNode\n\n+ Export-NetworkController\n\n+ Get-NetworkController\n\n+ Get-NetworkControllerCanaryConfiguration\n\n+ Get-NetworkControllerCluster\n\n+ Get-NetworkControllerCredential\n\n+ Get-NetworkControllerDevice\n\n+ Get-NetworkControllerDeviceGroupingTestConfiguration\n\n+ Get-NetworkControllerDeviceGroups\n\n+ Get-NetworkControllerDeviceGroupUsage\n\n+ Get-NetworkControllerDeviceUsage\n\n+ Get-NetworkControllerDiagnostic\n\n+ Get-NetworkControllerDiscoveredTopology\n\n+ Get-NetworkControllerExternalTestRule\n\n+ Get-NetworkControllerFabricRoute\n\n+ Get-NetworkControllerGoalTopology\n\n+ Get-NetworkControllerInterface\n\n+ Get-NetworkControllerInterfaceUsage\n\n+ Get-NetworkControllerIpPool\n\n+ Get-NetworkControllerIpPoolStatistics\n\n+ Get-NetworkControllerIpSubnetStatistics\n\n+ Get-NetworkControllerLogicalNetwork\n\n+ Get-NetworkControllerLogicalSubnet\n\n+ Get-NetworkControllerMonitoringService\n\n+ Get-NetworkControllerNode\n\n+ Get-NetworkControllerPhysicalHostInterfaceParameter\n\n+ Get-NetworkControllerPhysicalHostParameter\n\n+ Get-NetworkControllerPhysicalSwitchCpuUtilizationParameter\n\n+ Get-NetworkControllerPhysicalSwitchInterfaceParameter\n\n+ Get-NetworkControllerPhysicalSwitchMemoryUtilizationParameter\n\n+ Get-NetworkControllerPhysicalSwitchParameter\n\n+ Get-NetworkControllerPSwitch\n\n+ Get-NetworkControllerPublicIpAddress\n\n+ Get-NetworkControllerServer\n\n+ Get-NetworkControllerServerInterface\n\n+ Get-NetworkControllerSwitchBgpPeer\n\n+ Get-NetworkControllerSwitchBgpRouter\n\n+ Get-NetworkControllerSwitchConfig\n\n+ Get-NetworkControllerSwitchNetworkRoute\n\n+ Get-NetworkControllerSwitchPort\n\n+ Get-NetworkControllerSwitchPortChannel\n\n+ Get-NetworkControllerSwitchVlan\n\n+ Get-NetworkControllerTopologyConfiguration\n\n+ Get-NetworkControllerTopologyDiscoveryStatistics\n\n+ Get-NetworkControllerTopologyLink\n\n+ Get-NetworkControllerTopologyNode\n\n+ Get-NetworkControllerTopologyTerminationPoint\n\n+ Get-NetworkControllerTopologyValidationReport\n\n+ Get-NetworkControllerVirtualInterface\n\n+ Get-NetworkControllerVirtualNetworkUsage\n\n+ Get-NetworkControllerVirtualPort\n\n+ Get-NetworkControllerVirtualServer\n\n+ Get-NetworkControllerVirtualServerInterface\n\n+ Get-NetworkControllerVirtualSwitch\n\n+ Get-NetworkControllerVirtualSwitchPortParameter\n\n+ Import-NetworkController\n\n+ Install-NetworkController\n\n+ Install-NetworkControllerCluster\n\n+ New-NetworkControllerCanaryConfiguration\n\n+ New-NetworkControllerCredential\n\n+ New-NetworkControllerDevice\n\n+ New-NetworkControllerDeviceGroupingTestConfiguration\n\n+ New-NetworkControllerDeviceGroups\n\n+ New-NetworkControllerExternalTestRule\n\n+ New-NetworkControllerInterface\n\n+ New-NetworkControllerIpPool\n\n+ New-NetworkControllerLogicalNetwork\n\n+ New-NetworkControllerMonitoringService\n\n+ New-NetworkControllerNodeObject\n\n+ New-NetworkControllerPhysicalHostInterfaceParameter\n\n+ New-NetworkControllerPhysicalHostParameter\n\n+ New-NetworkControllerPhysicalSwitchCpuUtilizationParameter\n\n+ New-NetworkControllerPhysicalSwitchInterfaceParameter\n\n+ New-NetworkControllerPhysicalSwitchMemoryUtilizationParameter\n\n+ New-NetworkControllerPhysicalSwitchParameter\n\n+ New-NetworkControllerPSwitch\n\n+ New-NetworkControllerPublicIpAddress\n\n+ New-NetworkControllerServer\n\n+ New-NetworkControllerServerInterface\n\n+ New-NetworkControllerSwitchBgpPeer\n\n+ New-NetworkControllerSwitchBgpRouter\n\n+ New-NetworkControllerSwitchNetworkRoute\n\n+ New-NetworkControllerSwitchPortChannel\n\n+ New-NetworkControllerSwitchVlan\n\n+ New-NetworkControllerTopologyLink\n\n+ New-NetworkControllerTopologyNode\n\n+ New-NetworkControllerTopologyTerminationPoint\n\n+ New-NetworkControllerVirtualInterface\n\n+ New-NetworkControllerVirtualPort\n\n+ New-NetworkControllerVirtualServer\n\n+ New-NetworkControllerVirtualServerInterface\n\n+ New-NetworkControllerVirtualSwitch\n\n+ New-NetworkControllerVirtualSwitchPortParameter\n\n+ Remove-NetworkControllerCanaryConfiguration\n\n+ Remove-NetworkControllerCredential\n\n+ Remove-NetworkControllerDevice\n\n+ Remove-NetworkControllerDeviceGroupingTestConfiguration\n\n+ Remove-NetworkControllerDeviceGroups\n\n+ Remove-NetworkControllerExternalTestRule\n\n+ Remove-NetworkControllerFabricRoute\n\n+ Remove-NetworkControllerInterface\n\n+ Remove-NetworkControllerIpPool\n\n+ Remove-NetworkControllerLogicalNetwork\n\n+ Remove-NetworkControllerLogicalSubnet\n\n+ Remove-NetworkControllerNode\n\n+ Remove-NetworkControllerPhysicalSwitchCpuUtilizationParameter\n\n+ Remove-NetworkControllerPhysicalSwitchMemoryUtilizationParameter\n\n+ Remove-NetworkControllerPSwitch\n\n+ Remove-NetworkControllerPublicIpAddress\n\n+ Remove-NetworkControllerServer\n\n+ Remove-NetworkControllerServerInterface\n\n+ Remove-NetworkControllerSwitchBgpPeer\n\n+ Remove-NetworkControllerSwitchBgpRouter\n\n+ Remove-NetworkControllerSwitchNetworkRoute\n\n+ Remove-NetworkControllerSwitchPortChannel\n\n+ Remove-NetworkControllerSwitchVlan\n\n+ Remove-NetworkControllerTopologyLink\n\n+ Remove-NetworkControllerTopologyNode\n\n+ Remove-NetworkControllerTopologyTerminationPoint\n\n+ Remove-NetworkControllerVirtualInterface\n\n+ Remove-NetworkControllerVirtualPort\n\n+ Remove-NetworkControllerVirtualServer\n\n+ Remove-NetworkControllerVirtualServerInterface\n\n+ Remove-NetworkControllerVirtualSwitch\n\n+ Repair-NetworkControllerCluster\n\n+ Set-NetworkController\n\n+ Set-NetworkControllerCluster\n\n+ Set-NetworkControllerDiagnostic\n\n+ Set-NetworkControllerFabricRoute\n\n+ Set-NetworkControllerGoalTopology\n\n+ Set-NetworkControllerLogicalSubnet\n\n+ Set-NetworkControllerNode\n\n+ Set-NetworkControllerSwitchConfig\n\n+ Set-NetworkControllerSwitchPort\n\n+ Set-NetworkControllerTopologyConfiguration\n\n+ Start-NetworkControllerTopologyDiscovery\n\n+ Uninstall-NetworkController\n\n+ Uninstall-NetworkControllerClusterNetworkLoadBalancingClusters03535NetworkSwitchManager19190+ Disable-NetworkSwitchEthernetPort\n\n+ Disable-NetworkSwitchFeature\n\n+ Disable-NetworkSwitchVlan\n\n+ Enable-NetworkSwitchEthernetPort\n\n+ Enable-NetworkSwitchFeature\n\n+ Enable-NetworkSwitchVlan\n\n+ Get-NetworkSwitchEthernetPort\n\n+ Get-NetworkSwitchFeature\n\n+ Get-NetworkSwitchGlobalData\n\n+ Get-NetworkSwitchVlan\n\n+ New-NetworkSwitchVlan\n\n+ Remove-NetworkSwitchEthernetPortIPAddress\n\n+ Remove-NetworkSwitchVlan\n\n+ Restore-NetworkSwitchConfiguration\n\n+ Save-NetworkSwitchConfiguration\n\n+ Set-NetworkSwitchEthernetPortIPAddress\n\n+ Set-NetworkSwitchPortMode\n\n+ Set-NetworkSwitchPortProperty\n\n+ Set-NetworkSwitchVlanPropertyNetworkTransition03434NFS04242Nps-6713- Get-NpsRemediationServer\n\n- Get-NpsRemediationServerGroup\n\n- New-NpsRemediationServer\n\n- New-NpsRemediationServerGroup\n\n- Remove-NpsRemediationServer\n\n- Remove-NpsRemediationServerGroupPackageManagement10100+ Find-Package\n\n+ Get-Package\n\n+ Get-PackageProvider\n\n+ Get-PackageSource\n\n+ Install-Package\n\n+ Register-PackageSource\n\n+ Save-Package\n\n+ Set-PackageSource\n\n+ Uninstall-Package\n\n+ Unregister-PackageSourcePcsvDevice495+ Clear-PcsvDeviceLog\n\n+ Get-PcsvDeviceLog\n\n+ Set-PcsvDeviceNetworkConfiguration\n\n+ Set-PcsvDeviceUserPasswordPester20200+ AfterAll\n\n+ AfterEach\n\n+ Assert-MockCalled\n\n+ Assert-VerifiableMocks\n\n+ BeforeAll\n\n+ BeforeEach\n\n+ Context\n\n+ Describe\n\n+ Get-MockDynamicParameters\n\n+ Get-TestDriveItem\n\n+ In\n\n+ InModuleScope\n\n+ Invoke-Mock\n\n+ Invoke-Pester\n\n+ It\n\n+ Mock\n\n+ New-Fixture\n\n+ Set-DynamicParameterVariables\n\n+ Setup\n\n+ ShouldPKI01717PnpDevice440+ Disable-PnpDevice\n\n+ Enable-PnpDevice\n\n+ Get-PnpDevice\n\n+ Get-PnpDevicePropertyPowerShellGet11110+ Find-Module\n\n+ Get-InstalledModule\n\n+ Get-PSRepository\n\n+ Install-Module\n\n+ Publish-Module\n\n+ Register-PSRepository\n\n+ Save-Module\n\n+ Set-PSRepository\n\n+ Uninstall-Module\n\n+ Unregister-PSRepository\n\n+ Update-ModulePrintManagement02222PSDesiredStateConfiguration51712+ Connect-DscConfiguration\n\n+ Find-DscResource\n\n+ Get-DscConfigurationStatus\n\n+ Invoke-DscResource\n\n+ Publish-DscConfigurationPSDiagnostics01010PSReadline550+ Get-PSReadlineKeyHandler\n\n+ Get-PSReadlineOption\n\n+ Remove-PSReadlineKeyHandler\n\n+ Set-PSReadlineKeyHandler\n\n+ Set-PSReadlineOptionPSScheduledJob01616PSWorkflow022PSWorkflowUtility011RemoteAccess14121107+ Add-BgpRouteAggregate\n\n+ Add-VpnSstpProxyRule\n\n+ Clear-BgpRouteFlapDampening\n\n+ Disable-BgpRouteFlapDampening\n\n+ Enable-BgpRouteFlapDampening\n\n+ Get-BgpRouteAggregate\n\n+ Get-BgpRouteFlapDampening\n\n+ Get-VpnSstpProxyRule\n\n+ New-VpnSstpProxyRule\n\n+ Remove-BgpRouteAggregate\n\n+ Remove-VpnSstpProxyRule\n\n+ Set-BgpRouteAggregate\n\n+ Set-BgpRouteFlapDampening\n\n+ Set-VpnSstpProxyRuleRemoteDesktop57873+ Export-RDPersonalSessionDesktopAssignment\n\n+ Get-RDPersonalSessionDesktopAssignment\n\n+ Import-RDPersonalSessionDesktopAssignment\n\n+ Remove-RDPersonalSessionDesktopAssignment\n\n+ Set-RDPersonalSessionDesktopAssignmentScheduledTasks01919SecureBoot055ServerCore022ServerManager077ServerManagerTasks01111ShieldedVMDataFile330+ Import-ShieldingDataFile\n\n+ New-VolumeIDQualifier\n\n+ Protect-ShieldingDataFileShieldedVMTemplate110+ Protect-ServerVHDXSmbShare03535SmbWitness033SoftwareInventoryLogging01111StartScreen033Storage32140108+ Block-FileShareAccess\n\n+ Clear-StorageDiagnosticInfo\n\n+ Debug-FileShare\n\n+ Debug-StorageSubSystem\n\n+ Disable-PhysicalDiskIdentification\n\n+ Disable-StorageDiagnosticLog\n\n+ Enable-PhysicalDiskIdentification\n\n+ Enable-StorageDiagnosticLog\n\n+ Get-DedupProperties\n\n+ Get-DiskSNV\n\n+ Get-DiskStorageNodeView\n\n+ Get-FileShare\n\n+ Get-FileShareAccessControlEntry\n\n+ Get-StorageAdvancedProperty\n\n+ Get-StorageDiagnosticInfo\n\n+ Get-StorageEnclosureSNV\n\n+ Get-StorageEnclosureStorageNodeView\n\n+ Get-StorageFaultDomain\n\n+ Get-StorageFileServer\n\n+ Grant-FileShareAccess\n\n+ New-FileShare\n\n+ New-StorageFileServer\n\n+ Optimize-StoragePool\n\n+ Remove-FileShare\n\n+ Remove-StorageFileServer\n\n+ Revoke-FileShareAccess\n\n+ Set-FileShare\n\n+ Set-StorageFileServer\n\n+ Start-StorageDiagnosticLog\n\n+ Stop-StorageDiagnosticLog\n\n+ Stop-StorageJob\n\n+ Unblock-FileShareAccessStorageQoS660+ Get-StorageQoSFlow\n\n+ Get-StorageQoSPolicy\n\n+ Get-StorageQoSVolume\n\n+ New-StorageQoSPolicy\n\n+ Remove-StorageQoSPolicy\n\n+ Set-StorageQoSPolicyStorageReplica11110+ Get-SRGroup\n\n+ Get-SRPartnership\n\n+ New-SRGroup\n\n+ New-SRPartnership\n\n+ Remove-SRGroup\n\n+ Remove-SRPartnership\n\n+ Set-SRGroup\n\n+ Set-SRPartnership\n\n+ Suspend-SRGroup\n\n+ Sync-SRGroup\n\n+ Test-SRTopologyTLS374+ Disable-TlsCipherSuite\n\n+ Enable-TlsCipherSuite\n\n+ Get-TlsCipherSuiteTroubleshootingPack022TrustedPlatformModule01111UpdateServices41612+ Add-WsusDynamicCategory\n\n+ Get-WsusDynamicCategory\n\n+ Remove-WsusDynamicCategory\n\n+ Set-WsusDynamicCategoryUserAccessLogging01414VpnClient01919Wdac01212WebAdministration08080Whea022WindowsDeveloperLicense033WindowsErrorReporting033WindowsSearch022\n\n5. Share the code\n\nFor those wondering about the script I used to compile the results, here it goes.\n\n#\n\n# Enumerating all the modules from both OS versions\n\n#\n\n# Load XML files into memory: $Files[0] and $Files[1]\n\n$Files= ( (Import-Clixml\"C:\\WS2012R2Cmdlets.XML\"),\n\n(Import-Clixml\"C:\\WS2016TP2Cmdlets.XML\") )\n\n# Create empty dictionary for modules\n\n$ModuleDict= @{}\n\n# Loop through the two files to gather module info\n\n$Files|% {\n\n$_|GroupModuleName|SortName|% {\n\n$Module=$_.Name\n\n# If found, increase count. If not, add to dictionary\n\nIf ($ModuleDict.ContainsKey($Module)) {\n\n$ModuleDict.$Module++\n\n} Else {\n\n$ModuleDict.Add($Module,1)\n\n} # End If\n\n} # End Import\n\n} # End $Files\n\n#\n\n# Enumerate the cmdlets in every module\n\n#\n\n# Add the HTML table header\n\nWrite-Host\"<table border=1><tr><td><b>Module</b></td><td>New Cmdlets</td><td>WS 2016 TP2</td><td>WS 2012 R2</td></tr>\"\n\n# Loop through the modules in the dictionary\n\n$ModuleDict.GetEnumerator() |SortName|% {\n\n# Initialize variables for a new module\n\n$Module=$_.Name\n\n$VersionCount= (0,0)\n\n$CmdletDict= @{}\n\n# Loop through the two files, filtering by module\n\n0..1|% {\n\n$WSVersion=$_\n\n$Files[$_]|?ModuleName-eq$Module|% {\n\n$Cmdlet=$_.Name\n\n# Count cmdlets by module for each OS version\n\n$VersionCount[$WSVersion]++\n\n# Increase per-cmdlet value by 1 (WS2012R2) or by 2 (WS2016TP2)\n\n# If cmdlet exists in both OSes, value will be 3\n\nIf ($CmdletDict.ContainsKey($Cmdlet)) {\n\n$CmdletDict.$Cmdlet+= ($WSVersion+1)\n\n} Else {\n\n$CmdletDict.Add($Cmdlet, ($WSVersion+1))\n\n} # End If\n\n} # End %\n\n} # End 0..1\n\n#\n\n# Output the list of cmdlets that changed in every module\n\n#\n\n# Copy data to single variables for easy use with Write-Host\n\n$WS0=$VersionCount[0]\n\n$WS1=$VersionCount[1]\n\n$Dif=$WS1-$WS0\n\n$CrLf=\"<BR>\"+[char]10+[char]13\n\n# Write HTML table row with module summary information\n\nWrite-Host\"<tr><td><b>$Module</b></td><td align=`\"right`\">$Dif</td><td align=`\"right`\">$WS1</td><td align=`\"right`\">$WS0</td></tr>\" # If there are cmdlets in the module\n\nIf ($CmdletDict.Count -gt0) {\n\n# Gather all new and removed cmdlets in a variable\n\n$CmdletList=\"\"\n\n$CmdletDict.GetEnumerator() |? {$_.Value -eq2-or$_.Value -eq1} |SortName|% {\n\n# 1 means removed cmdlet. 2 means new cmdlet\n\n$Name=$_.Name\n\nIf ($_.Value -eq1) {\n\n$CmdletList+=\"- $Name\"+$CrLf\n\n} else {\n\n$CmdletList+=\"+ $Name\"+$CrLf\n\n} # End If\n\n} # End Enumerator\n\n# If new or removed exist, write another HTML table row\n\nIf ($CmdletList-ne\"\") {\n\nWrite-Host\"<tr><td colspan=4>$CmdletList</td></tr>\"\n\n} # End If\n\n} # End if\n\n} # End Module\n\n# Write HTML table end. All done.\n\nWrite-Host\"</table>\"\n\n↧\n\nDrive Performance Report Generator - PowerShell script using DiskSpd by Arnaud Torres\n\nArnaud Torres is a Senior Premier Field Engineer at Microsoft in France who sent me the PowerShell script below called \"Drive Performance Report Generator\".\n\nHe created the script to test a wide range of profiles in one run to allow people to build a baseline of their storage using DiskSpd.EXE.\n\nThe script is written in PowerShell v1 and was tested on a Windows Server 2008 SP2 (really!), Windows Server 2012 R2 and Windows 10.\n\nIt displays results in real time, is highly documented and creates a text report which can be imported as CSV in Excel.\n\nThanks to Arnaud for sharing!\n\n----------------------\n\n# Drive performance Report Generator\n\n# by Arnaud TORRES\n\n# Microsoft provides script, macro, and other code examples for illustration only, without warranty either expressed or implied, including but not\n\n# limited to the implied warranties of merchantability and/or fitness for a particular purpose. This script is provided 'as is' and Microsoft does not\n\n# guarantee that the following script, macro, or code can be used in all situations.\n\n# Script will stress your computer CPU and storage, be sure that no critical workload is running\n\n# Clear screen\n\nClear\n\nwrite-host \"DRIVE PERFORMANCE REPORT GENERATOR\" -foregroundcolor green\n\nwrite-host \"Script will stress your computer CPU and storage layer (including network if applciable !), be sure that no critical workload is running\" -foregroundcolor yellow\n\nwrite-host \"Microsoft provides script, macro, and other code examples for illustration only, without warranty either expressed or implied, including but not limited to the implied warranties of merchantability and/or fitness for a particular purpose. This script is provided 'as is' and Microsoft does not guarantee that the following script, macro, or code can be used in all situations.\" -foregroundcolor darkred\n\n\" \"\n\n\"Test will use all free space on drive minus 2 GB !\"\n\n\"If there are less than 4 GB free test will stop\"\n\n# Disk to test\n\n$Disk = Read-Host 'Which disk would you like to test ? (example : D:)'\n\n# $Disk = \"D:\"\n\nif ($disk.length -ne 2){\"Wrong drive letter format used, please specify the drive as D:\"\n\nExit}\n\nif ($disk.substring(1,1) -ne \":\"){\"Wrong drive letter format used, please specify the drive as D:\"\n\nExit}\n\n$disk = $disk.ToUpper()\n\n# Reset test counter\n\n$counter = 0\n\n# Use 1 thread / core\n\n$Thread = \"-t\"+(Get-WmiObject win32_processor).NumberofCores\n\n# Set time in seconds for each run\n\n# 10-120s is fine\n\n$Time = \"-d1\"\n\n# Outstanding IOs\n\n# Should be 2 times the number of disks in the RAID\n\n# Between 8 and 16 is generally fine\n\n$OutstandingIO = \"-o16\"\n\n# Disk preparation\n\n# Delete testfile.dat if it exists\n\n# The test will use all free space -2GB\n\n$IsDir = test-path -path \"$Disk\\TestDiskSpd\"\n\n$isdir\n\nif ($IsDir -like \"False\"){new-item -itemtype directory -path \"$Disk\\TestDiskSpd\\\"}\n\n# Just a little security, in case we are working on a compressed drive ...\n\ncompact /u /s $Disk\\TestDiskSpd\\\n\n$Cleaning = test-path -path \"$Disk\\TestDiskSpd\\testfile.dat\"\n\nif ($Cleaning -eq \"True\")\n\n{\"Removing current testfile.dat from drive\"\n\nremove-item $Disk\\TestDiskSpd\\testfile.dat}\n\n$Disks = Get-WmiObject win32_logicaldisk\n\n$LogicalDisk = $Disks | where {$_.DeviceID -eq $Disk}\n\n$Freespace = $LogicalDisk.freespace\n\n$FreespaceGB = [int]($Freespace / 1073741824)\n\n$Capacity = $freespaceGB - 2\n\n$CapacityParameter = \"-c\"+$Capacity+\"G\"\n\n$CapacityO = $Capacity * 1073741824\n\nif ($FreespaceGB -lt \"4\")\n\n{\n\n\"Not enough space on the Disk ! More than 4GB needed\"\n\nExit\n\n}\n\nwrite-host \" \"\n\n$Continue = Read-Host \"You are about to test $Disk which has $FreespaceGB GB free, do you wan't to continue ? (Y/N) \"\n\nif ($continue -ne \"y\" -or $continue -ne \"Y\"){\"Test Cancelled !!\"\n\nExit}\n\n\" \"\n\n\"Initialization can take some time, we are generating a $Capacity GB file...\"\n\n\" \"\n\n# Initialize outpout file\n\n$date = get-date\n\n# Add the tested disk and the date in the output file\n\n\"Disque $disk, $date\" >> ./output.txt\n\n# Add the headers to the output file\n\n“Test N#, Drive, Operation, Access, Blocks, Run N#, IOPS, MB/sec, Latency ms, CPU %\" >> ./output.txt\n\n# Number of tests\n\n# Multiply the number of loops to change this value\n\n# By default there are : (4 blocks sizes) X (2 for read 100% and write 100%) X (2 for Sequential and Random) X (4 Runs of each)\n\n$NumberOfTests = 64\n\n\" \"\n\nwrite-host \"TEST RESULTS (also logged in .\\output.txt)\" -foregroundcolor yellow\n\n# Begin Tests loops\n\n# We will run the tests with 4K, 8K, 64K and 512K blocks\n\n(4,8,64,512) | % {\n\n$BlockParameter = (\"-b\"+$_+\"K\")\n\n$Blocks = (\"Blocks \"+$_+\"K\")\n\n# We will do Read tests and Write tests\n\n(0,100) | % {\n\nif ($_ -eq 0){$IO = \"Read\"}\n\nif ($_ -eq 100){$IO = \"Write\"}\n\n$WriteParameter = \"-w\"+$_\n\n# We will do random and sequential IO tests\n\n(\"r\",\"si\") | % {\n\nif ($_ -eq \"r\"){$type = \"Random\"}\n\nif ($_ -eq \"si\"){$type = \"Sequential\"}\n\n$AccessParameter = \"-\"+$_\n\n# Each run will be done 4 times\n\n(1..4) | % {\n\n# The test itself (finally !!)\n\n$result = .\\diskspd.exe $CapacityPArameter $Time $AccessParameter $WriteParameter $Thread $OutstandingIO $BlockParameter -h -L $Disk\\TestDiskSpd\\testfile.dat\n\n# Now we will break the very verbose output of DiskSpd in a single line with the most important values\n\nforeach ($line in $result) {if ($line -like \"total:*\") { $total=$line; break } }\n\nforeach ($line in $result) {if ($line -like \"avg.*\") { $avg=$line; break } }\n\n$mbps = $total.Split(\"|\")[2].Trim()\n\n$iops = $total.Split(\"|\")[3].Trim()\n\n$latency = $total.Split(\"|\")[4].Trim()\n\n$cpu = $avg.Split(\"|\")[1].Trim()\n\n$counter = $counter + 1\n\n# A progress bar, for the fun\n\nWrite-Progress -Activity \".\\diskspd.exe $CapacityPArameter $Time $AccessParameter $WriteParameter $Thread $OutstandingIO $BlockParameter -h -L $Disk\\TestDiskSpd\\testfile.dat\" -status \"Test in progress\" -percentComplete ($counter / $NumberofTests * 100)\n\n# Remove comment to check command line \".\\diskspd.exe $CapacityPArameter $Time $AccessParameter $WriteParameter $Thread -$OutstandingIO $BlockParameter -h -L $Disk\\TestDiskSpd\\testfile.dat\"\n\n# We output the values to the text file\n\n“Test $Counter,$Disk,$IO,$type,$Blocks,Run $_,$iops,$mbps,$latency,$cpu\" >> ./output.txt\n\n# We output a verbose format on screen\n\n“Test $Counter, $Disk, $IO, $type, $Blocks, Run $_, $iops iops, $mbps MB/sec, $latency ms, $cpu CPU\"\n\n}\n\n}\n\n}\n\n}\n\n↧\n\n↧\n\nDrive Performance Report Generator – PowerShell script using DiskSpd by Arnaud Torres\n\nArnaud Torres is a Senior Premier Field Engineer at Microsoft in France who sent me the PowerShell script below called “Drive Performance Report Generator”.\n\nHe created the script to test a wide range of profiles in one run to allow people to build a baseline of their storage using DiskSpd.EXE.\n\nThe script is written in PowerShell v1 and was tested on a Windows Server 2008 SP2 (really!), Windows Server 2012 R2 and Windows 10.\n\nIt displays results in real time, is highly documented and creates a text report which can be imported as CSV in Excel.\n\nThanks to Arnaud for sharing!\n\n———————-\n\n# Drive performance Report Generator\n\n# by Arnaud TORRES\n\n# Microsoft provides script, macro, and other code examples for illustration only, without warranty either expressed or implied, including but not\n\n# limited to the implied warranties of merchantability and/or fitness for a particular purpose. This script is provided ‘as is’ and Microsoft does not\n\n# guarantee that the following script, macro, or code can be used in all situations.\n\n# Script will stress your computer CPU and storage, be sure that no critical workload is running\n\n# Clear screen\n\nClear\n\nwrite-host “DRIVE PERFORMANCE REPORT GENERATOR” -foregroundcolor green\n\nwrite-host “Script will stress your computer CPU and storage layer (including network if applciable !), be sure that no critical workload is running” -foregroundcolor yellow\n\nwrite-host “Microsoft provides script, macro, and other code examples for illustration only, without warranty either expressed or implied, including but not limited to the implied warranties of merchantability and/or fitness for a particular purpose. This script is provided ‘as is’ and Microsoft does not guarantee that the following script, macro, or code can be used in all situations.” -foregroundcolor darkred\n\n” “\n\n“Test will use all free space on drive minus 2 GB !”\n\n“If there are less than 4 GB free test will stop”\n\n# Disk to test\n\n$Disk = Read-Host ‘Which disk would you like to test ? (example : D:)’\n\n# $Disk = “D:”\n\nif ($disk.length -ne 2){“Wrong drive letter format used, please specify the drive as D:”\n\nExit}\n\nif ($disk.substring(1,1) -ne “:”){“Wrong drive letter format used, please specify the drive as D:”\n\nExit}\n\n$disk = $disk.ToUpper()\n\n# Reset test counter\n\n$counter = 0\n\n# Use 1 thread / core\n\n$Thread = “-t”+(Get-WmiObject win32_processor).NumberofCores\n\n# Set time in seconds for each run\n\n# 10-120s is fine\n\n$Time = “-d1″\n\n# Outstanding IOs\n\n# Should be 2 times the number of disks in the RAID\n\n# Between 8 and 16 is generally fine\n\n$OutstandingIO = “-o16″\n\n# Disk preparation\n\n# Delete testfile.dat if it exists\n\n# The test will use all free space -2GB\n\n$IsDir = test-path -path “$Disk\\TestDiskSpd”\n\n$isdir\n\nif ($IsDir -like “False”){new-item -itemtype directory -path “$Disk\\TestDiskSpd\\”}\n\n# Just a little security, in case we are working on a compressed drive …\n\ncompact /u /s $Disk\\TestDiskSpd\\\n\n$Cleaning = test-path -path “$Disk\\TestDiskSpd\\testfile.dat”\n\nif ($Cleaning -eq “True”)\n\n{“Removing current testfile.dat from drive”\n\nremove-item $Disk\\TestDiskSpd\\testfile.dat}\n\n$Disks = Get-WmiObject win32_logicaldisk\n\n$LogicalDisk = $Disks | where {$_.DeviceID -eq $Disk}\n\n$Freespace = $LogicalDisk.freespace\n\n$FreespaceGB = [int]($Freespace / 1073741824)\n\n$Capacity = $freespaceGB – 2\n\n$CapacityParameter = “-c”+$Capacity+”G”\n\n$CapacityO = $Capacity * 1073741824\n\nif ($FreespaceGB -lt “4”)\n\n{\n\n“Not enough space on the Disk ! More than 4GB needed”\n\nExit\n\n}\n\nwrite-host ” “\n\n$Continue = Read-Host “You are about to test $Disk which has $FreespaceGB GB free, do you wan’t to continue ? (Y/N) “\n\nif ($continue -ne “y” -or $continue -ne “Y”){“Test Cancelled !!”\n\nExit}\n\n” “\n\n“Initialization can take some time, we are generating a $Capacity GB file…”\n\n” “\n\n# Initialize outpout file\n\n$date = get-date\n\n# Add the tested disk and the date in the output file\n\n“Disque $disk, $date” >> ./output.txt\n\n# Add the headers to the output file\n\n“Test N#, Drive, Operation, Access, Blocks, Run N#, IOPS, MB/sec, Latency ms, CPU %” >> ./output.txt\n\n# Number of tests\n\n# Multiply the number of loops to change this value\n\n# By default there are : (4 blocks sizes) X (2 for read 100% and write 100%) X (2 for Sequential and Random) X (4 Runs of each)\n\n$NumberOfTests = 64\n\n” “\n\nwrite-host “TEST RESULTS (also logged in .\\output.txt)” -foregroundcolor yellow\n\n# Begin Tests loops\n\n# We will run the tests with 4K, 8K, 64K and 512K blocks\n\n(4,8,64,512) | % {\n\n$BlockParameter = (“-b”+$_+”K”)\n\n$Blocks = (“Blocks “+$_+”K”)\n\n# We will do Read tests and Write tests\n\n(0,100) | % {\n\nif ($_ -eq 0){$IO = “Read”}\n\nif ($_ -eq 100){$IO = “Write”}\n\n$WriteParameter = “-w”+$_\n\n# We will do random and sequential IO tests\n\n(“r”,”si”) | % {\n\nif ($_ -eq “r”){$type = “Random”}\n\nif ($_ -eq “si”){$type = “Sequential”}\n\n$AccessParameter = “-“+$_\n\n# Each run will be done 4 times\n\n(1..4) | % {\n\n# The test itself (finally !!)\n\n$result = .\\diskspd.exe $CapacityPArameter $Time $AccessParameter $WriteParameter $Thread $OutstandingIO $BlockParameter -h -L $Disk\\TestDiskSpd\\testfile.dat\n\n# Now we will break the very verbose output of DiskSpd in a single line with the most important values\n\nforeach ($line in $result) {if ($line -like “total:*”) { $total=$line; break } }\n\nforeach ($line in $result) {if ($line -like “avg.*”) { $avg=$line; break } }\n\n$mbps = $total.Split(“|”)[2].Trim()\n\n$iops = $total.Split(“|”)[3].Trim()\n\n$latency = $total.Split(“|”)[4].Trim()\n\n$cpu = $avg.Split(“|”)[1].Trim()\n\n$counter = $counter + 1\n\n# A progress bar, for the fun\n\nWrite-Progress -Activity “.\\diskspd.exe $CapacityPArameter $Time $AccessParameter $WriteParameter $Thread $OutstandingIO $BlockParameter -h -L $Disk\\TestDiskSpd\\testfile.dat” -status “Test in progress” -percentComplete ($counter / $NumberofTests * 100)\n\n# Remove comment to check command line “.\\diskspd.exe $CapacityPArameter $Time $AccessParameter $WriteParameter $Thread -$OutstandingIO $BlockParameter -h -L $Disk\\TestDiskSpd\\testfile.dat”\n\n# We output the values to the text file\n\n“Test $Counter,$Disk,$IO,$type,$Blocks,Run $_,$iops,$mbps,$latency,$cpu” >> ./output.txt\n\n# We output a verbose format on screen\n\n“Test $Counter, $Disk, $IO, $type, $Blocks, Run $_, $iops iops, $mbps MB/sec, $latency ms, $cpu CPU”\n\n}\n\n}\n\n}\n\n}\n\n↧\n\nTwenty years as a Microsoft Certified Professional – time flies when you’re having fun\n\nI just noticed that last week was the 20th anniversary of my first Microsoft certification. I had to travel nearly 500 miles (from Fortaleza to Recife) to reach the closest official testing center available in Brazil in August 1995.\n\nYou’re probably thinking that I started by taking the Windows 95 exam, but it was actually the Windows 3.1 exam (which included a lot of MS-DOS 6.x stuff). The Windows 95 exam was my next one, but that only happened over a year later in December 1996.\n\nI went on to take absolutely all of the Windows NT 4.0 and Windows 2000 exams (many of them in their beta version). At that point we had multiple Microsoft Certified Partners in Fortaleza and I worked for one of them.\n\nI continued to take lots of exams even after moved to the US in October 2000 and after I joined Microsoft in October 2002. I only slowed down a bit after joining the Windows Server engineering team in October 2007.\n\nIn 2009 I achieved my last certification as a Microsoft Certified Master on SQL Server 2008. That took a few weeks of training, a series of written exams and a final, multi-hour lab exam. Exciting stuff! That also later granted me a charter certification as Microsoft Certified Solutions Master (Data Platform), Microsoft Certified Solutions Expert (Data Platform) and Microsoft Certified Solutions Associate (SQL Server 2012).\n\nMy full list is shown below. In case you’re wondering, the Windows 10 exam (Configuring Windows Devices) is already in development and you can find the details at https://www.microsoft.com/learning/en-us/exam-70-697.aspx.\n\n↧\n\nRaw notes from the Storage Developer Conference 2015 (SNIA SDC 2015)\n\nNotes and disclaimers:\n\nThis blog post contains raw notes for some of the SNIA’s SDC 2015 presentations (SNIA’s Storage Developers Conference 2015)\n\nThese notes were typed during the talks and they may include typos and my own misinterpretations.\n\nText in the bullets under each talk are quotes from the speaker or text from the speaker slides, not my personal opinion.\n\nIf you feel that I misquoted you or badly represented the content of a talk, please add a comment to the post.\n\nI spent limited time fixing typos or correcting the text after the event. There are only so many hours in a day…\n\nI have not attended all sessions (since there are many being delivered at a time, that would actually not be possible :-)…\n\nSNIA usually posts the actual PDF decks a few weeks after the event. Attendees have access immediately.\n\nYou can find the event agenda at http://www.snia.org/events/storage-developer/agenda\n\nUnderstanding the Intel/Micron 3D XPoint Memory\n\nJim Handy, General Director, Objective Analysis\n\nMemory analyst, SSD analyst, blogs: http://thememoryguy.com, http://thessdguy.com\n\nNot much information available since the announcement in July: http://newsroom.intel.com/docs/DOC-6713\n\nAgenda: What? Why? Who? Is the world ready for it? Should I care? When?\n\nWhat: Picture of the 3D XPoint concept (pronounced 3d-cross-point). Micron’s photograph of “the real thing”.\n\nIntel has researched PCM for 45 years. Mentioned in an Intel article at “Electronics” in Sep 28, 1970.\n\nThe many elements that have been tried shown in the periodic table of elements.\n\nNAND laid the path to the increased hierarchy levels. Showed prices of DRAM/NAND from 2001 to 2015. Gap is now 20x.\n\nComparing bandwidth to price per gigabytes for different storage technologies: Tape, HDD, SSD, 3D XPoint, DRAM, L3, L2, L1\n\nIntel diagram mentions PCM-based DIMMs (far memory) and DDR DIMMs (near memory).\n\nChart with latency for HDD SAS/SATA, SSD SAS/SATA, SSD NVMe, 3D XPoint NVMe – how much of it is the media, how much is the software stack?\n\n3D Xpoint’s place in the memory/storage hierarchy. IOPS x Access time. DRAM, 3D XPoint (Optane), NVMe SSD, SATA SSD\n\nGreat gains at low queue depth. 800GB SSD read IOPS using 16GB die. IOPS x queue depth of NAND vs. 3D XPoint.\n\nEconomic benefits: measuring $/write IOPS for SAS HDD, SATA SSD, PCIe SSD, 3D XPoint\n\nTiming is good because: DRAM is running out of speed, NVDIMMs are catching on, some sysadmins understand how to use flash to reduce DRAM needs\n\nTiming is bad because: Nobody can make it economically, no software supports SCM (storage class memory), new layers take time to establish Why should I care: better cost/perf ratio, lower power consumption (less DRAM, more perf/server, lower OpEx), in-memory DB starts to make sense\n\nWhen? Micron slide projects 3D XPoint at end of FY17 (two months ahead of CY). Same slide shows NAND production surpassing DRAM production in FY17.\n\nComparing average price per GB compared to the number of GB shipped over time. It takes a lot of shipments to lower price.\n\nLooking at the impact in the DRAM industry if this actually happens. DRAM slows down dramatically starting in FY17, as 3D XPoint revenues increase (optimistic).\n\nNext Generation Data Centers: Hyperconverged Architectures Impact On Storage\n\nMark OConnell, Distinguished Engineer, EMC\n\nHistory: Client/Server –> shared SANs –> Scale-Out systems\n\n>> Scale-Out systems: architecture, expansion, balancing\n\n>> Evolution of the application platform: physical servers à virtualization à Virtualized application farm\n\n>> Virtualized application farms and Storage: local storage à Shared Storage (SAN) à Scale-Out Storage à Hyper-converged\n\n>> Early hyper-converged systems: HDFS (Hadoop) à JVM/Tasks/HDFS in every node\n\nEffects of hyper-converged systems\n\n>> Elasticity (compute/storage density varies)\n\n>> App management, containers, app frameworks\n\n>> Storage provisioning: frameworks (openstack swift/cinder/manila), pure service architectures\n\n>> Hybrid cloud enablement. Apps as self-describing bundles. Storage as a dynamically bound service. Enables movement off-prem.\n\nImplications of Emerging Storage Technologies on Massive Scale Simulation Based Visual Effects\n\nYahya H. Mirza, CEO/CTO, Aclectic Systems Inc\n\nSteve Jobs quote: “You‘ve got to start with the customer experience and work back toward the technology”.\n\nProblem 1: Improve customer experience. Higher resolution, frame rate, throughput, etc.\n\nProblem 2: Production cost continues to rise.\n\nProblem 3: Time to render single frame remains constant.\n\nProblem 4: Render farm power and cooling increasing. Coherent shared memory model.\n\nHow do you reduce customer CapEx/OpEx. Low efficiency: 30% CPU. Prooblem is memory access latency and I/O.\n\nProduction workflow: modeling, animation/simulation/shading, lighting, rendering, compositing. More and more simulation.\n\nConcrete production experiment: 2005. Story boards. Attempt to create a short film. Putting himself in the customer’s shoes. Shot decomposition.\n\nReal 3-minute short costs $2 million. Animatic to pitch the project.\n\nCharacter modeling and development. Includes flesh and muscle simulation. A lot of it done procedurally.\n\nLooking at Disney’s “Big Hero 6”, DreamWorks’ “Puss in Boots” and Weta’s “The Hobbit”, including simulation costs, frame rate, resolution, size of files, etc.\n\nPhysically based rendering: global illumination effects, reflection, shadows. Comes down to light transport simulation, physically based materials description.\n\nExemplary VFX shot pipeline. VFX Tool (Houdini/Maya), Voxelized Geometry (OpenVDB), Scene description (Alembic), Simulation Engine (PhysBam), Simulation Farm (RenderFarm), Simulation Output (OpenVDB), Rendering Engine (Mantra), Render Farm (RenderFarm), Output format (OpenEXR), Compositor (Flame), Long-term storage.\n\nOne example: smoke simulation – reference model smoke/fire VFX. Complicated physical model. Hotspot algorithms: monte-carlo integration, ray-intersection test, linear algebra solver (multigrid).\n\nStorage implications. Compute storage (scene data, simulation data), Long term storage.\n\nIs public cloud computing viable for high-end VFX?\n\nDisney’s data center. 55K cores across 4 geos.\n\nVertically integrated systems are going to be more and more important. FPGAs, ARM-based servers.\n\nAclectic Colossus smoke demo. Showing 256x256x256.\n\nWe don’t want coherency; we don’t want sharing. Excited about Intel OmniPath.\n\nhttp://www.intel.com/content/www/us/en/high-performance-computing-fabrics/omni-path-architecture-fabric-overview.html\n\nHow Did Human Cells Build a Storage Engine?\n\nSanjay Joshi, CTO Life Sciences, EMC\n\nHuman cell, Nuclear DNA, Transcription and Translation, DNA Structure\n\nThe data structure: [char(3*10^9) human_genome] strand\n\n3 gigabases [(3*10^9)*2]/8 = ~750MB. With overlaps, ~1GB per cell. 15-70 trillion cells.\n\nActual files used to store genome are bigger, between 10GB and 4TB (includes lots of redundancy).\n\nGenome sequencing will surpass all other data types by 2040\n\nProtein coding portion is just a small portion of it. There’s a lot we don’t understand.\n\nNuclear DNA: Is it a file? Flat file system, distributed, asynchronous. Search header, interpret, compile, execute.\n\nNuclear DNA properties: Large:~20K genes/cell, Dynamic: append/overwrite/truncate, Semantics: strict, Consistent: No, Metadata: fixed, View: one-to-many\n\nMitochondrial DNA: Object? Distributed hash table, a ring with 32 partitions. Constant across generations.\n\nMitochondrial DNA: Small: ~40 genes/cell, Static: constancy, energy functions, Semantics: single origin, Consistent: Yes, Metadata: system based, View: one-to-one\n\nFile versus object. Comparing Nuclear DNA and Mitochondrial DNA characteristics.\n\nThe human body: 7500 names parts, 206 regularly occurring bones (newborns close to 300), ~640 skeletal muscles (320 pairs), 60+ organs, 37 trillion cells. Distributed cluster.\n\nMapping the ISO 7 layers to this system. Picture.\n\nFinite state machine: max 10^45 states at 4*10^53 state-changes/sec. 10^24 NOPS (nucleotide ops per second) across biosphere.\n\nConsensus in cell biology: Safety: under all conditions: apoptosis. Availability: billions of replicate copies. Not timing dependent: asynchronous. Command completion: 10 base errors in every 10,000 protein translation (10 AA/sec).\n\nObject vs. file. Object: Maternal, Static, Haploid. Small, Simple, Energy, Early. File: Maternal and paternal, Diploid. Scalable, Dynamic, Complex. All cells are female first.\n\nMove Objects to LTFS Tape Using HTTP Web Service Interface\n\nMatt Starr, Chief Technical Officer, Spectra Logic\n\nJeff Braunstein, Developer Evangelist, Spectra Logic\n\nWorldwide data growth: 2009 = 800 EB, 2015 = 6.5ZB, 2020 = 35ZB\n\nGenomics. 6 cows = 1TB of data. They keep it forever.\n\nVideo data. SD to Full HD to 4K UHD (4.2TB per hours) to 8K UHD. Also kept forever.\n\nIntel slide on the Internet minute. 90% of the people of the world never took a picture with anything but a camera phone.\n\nIOT – Total digital info create or replicated.\n\n$1000 genome scan take 780MB fully compressed. 2011 HiSeq-2000 scanner generates 20TB per month. Typical camera generates 105GB/day.\n\nMore and more examples.\n\nTape storage is the lowest cost. But it’s also complex to deploy. Comparing to Public and Private cloud…\n\nPitfalls of public cloud – chart of $/PB/day. OpEx per PB/day reaches very high for public cloud.\n\nRisk of public cloud: Amazon has 1 trillion objects. If they lose 1% it would 10 billion objects.\n\nRisk of public cloud: Nirvanix. VC pulled the plug in September 2013.\n\nCloud: Good: toolkits, naturally WAN friendly, user expectation: put it away.\n\nWhat if: Combine S3/Object with tape. Spectra S3 – Front end is REST, backend is LTFS tape.\n\nCost: $.09/GB. 7.2PB. Potentially a $0.20 two-copy archive.\n\nAutomated: App or user-built. Semi-Automated: NFI or scripting.\n\nInformation available at https://developer.spectralogic.com\n\nAll the tools you need to get started. Including simulator of the front end (BlackPearl) in a VM.\n\nS3 commands, plus data to write sequentially in bulk fashion.\n\nConfigure user for access, buckets.\n\nDeep storage browser (source code on GitHub) allows you to browse the simulated storage.\n\nSDK available in Java, C#, many others. Includes integration with Visual Studio (demonstrated).\n\nShowing sample application. 4 lines of code from the SDK to move a folder to tape storage.\n\nQ: Access times when not cached? Hours or minutes. Depends on if the tape is already in the drive. You can ask to pull those to cache, set priorities. By default GET has higher priority than PUT. 28TB or 56TB of cache.\n\nQ: Can we use CIFS/NFS? Yes, there is an NFI (Network File Interface) using CIFS/NFS, which talks to the cache machine. Manages time-outs.\n\nQ: Any protection against this being used as disk? System monitors health of the tape. Using an object-based interface helps.\n\nQ: Can you stage a file for some time, like 24h? There is a large cache. But there are no guarantees on the latency. Keeping it on cache is more like Glacier. What’s the trigger to bring the data?\n\nQ: Glacier? Considering support for it. Data policy to move to lower cost, move it back (takes time). Not a lot of product or customers demanding it. S3 has become the standard, not sure if Glacier will be that for archive.\n\nQ: Drives are a precious resource. How do you handle overload? By default, reads have precedence over writes. Writes usually can wait.\n\nTaxonomy of Differential Compression\n\nLiwei Ren, Scientific Adviser, Trend Micro\n\nMathematical model for describing file differences\n\nLossless data compression categories: data compression (one file), differential compression (two files), data deduplication (multiple files)\n\nPurposes: network data transfer acceleration and storage space reduction\n\nAreas for DC – mobile phones’ firmware over the air, incremental update of files for security software, file synchronization and transfer over WAN, executable files\n\nMath model – Diff procedure: Delta = T – R, Merge procedure: T = R + Delta. Model for reduced network bandwidth, reduced storage cost.\n\nApplications: backup, revision control system, patch management, firmware over the air, malware signature update, file sync and transfer, distributed file system, cloud data migration\n\nDiff model. Two operations: COPY (source address, size [, destination address] ), ADD (data block, size [, destination address] )\n\nHow to create the delta? How to encode the delta into a file? How to create the right sequence of COPY/ADD operations?\n\nTop task is an effective algorithm to identify common blocks. Not covering it here, since it would take more than half an hour…\n\nModeling a diff package. Example.\n\nHow do you measure the efficiency of an algorithm? You need a cost model.\n\nCategorizing: Local DC – LDC (xdelta, zdelta, bsdiff), Remote DC – RDC (rsync, RDC protocol, tsync), Iterative – IDC (proposed)\n\nCategorizing: Not-in-place merging: general files (xdelta, zdelta, bsdiff), executable files (bsdiff, courgette)\n\nCategorizing: In place merging: firmware as general files (FOTA), firmware as executable files (FOTA)\n\nTopics in depth: LDC vs RDC vs IDC for general files\n\nTopics in depth: LDC for executable files\n\nTopics in depth: LDC for in-place merging\n\nNew Consistent Hashing Algorithms for Data Storage\n\nJason Resch, Software Architect, Cleversafe\n\nIntroducing a new algorithm for hashing.\n\nHashing is useful. Used commonly is distributed storage, distributed caching.\n\nIndependent users can coordinate (readers know where writers would write without talking to them).\n\nTypically, resizing a Hash Table is inefficient. Showing example.\n\nThat’s why we need “Stable Hashing”. Showing example. Only a small portion of the keys need to be re-mapped.\n\nStable hashing becomes a necessity when system is stateful and/or transferring state is expensive,\n\nUsed in Caching/Routing (CARP), DHT/Storage (Gluster, DynamoDB, Cassandra, ceph, openstack)\n\nStable Hashing with Global Namespaces. If you have a file name, you know what node has the data.\n\nEliminates points of contention, no metadata systems. Namespace is fixed, but the system is dynamic.\n\nBalances read/write load across nodes, as well as storage utilization across nodes.\n\nPerfectly Stable Hashing (Rendezvous Hashing, Consistent Hashing). Precisely weighted (CARP, RUSH, CRUSH).\n\nIt would be nice to have something that would offer the characteristics of both.\n\nConsistent: buckets inserted in random positions. Keys maps to the next node greater than that key. With a new node, only neighbors as disrupted. But neighbor has to send data to new node, might not distribute keys evenly.\n\nRendezvous: Score = Hash (Bucket ID || Key). Bucket with the highest score wins. When adding a new node, some of the keys will move to it. Every node is disrupted evenly.\n\nCARP is rendezvous hashing with a twist. It multiples the scores by a “Load Factor” for each node. Allows for some nodes being more capable than others. Not perfectly stable: if node’s weighting changes or node is added, then all load factor must be recomputed.\n\nRUSH/CRUSH: Hierarchical tree, with each node assigned a probability to go left/right. CRUSH makes the tree match the fault domains of the system. Efficient to add nodes, but not to remove or re-weight nodes.\n\nNew algorithm: Weighted Rendezvous Hashing (WRH). Both perfectly stable and precisely weighted.\n\nWRH adjusts scores before weighting them. Unlike CARP, scores aren’t relatively scaled.\n\nNo unnecessary transfer of keys when adding/removing nodes. If adding node or increasing weight on node, other nodes will move keys to it, but nothing else. Transfers are equalized and perfectly efficient.\n\nWRH is simple to implement. Whole python code showed in one slide.\n\nAll the magic is in one line: “Score = 1.0 / -math.log(hash_f)” – Proof of correctness provided for the math inclined.\n\nHow Cleversafe uses WRH. System is grown by set of devices. Devices have a lifecycle: added, possibly expanded, then retired.\n\nDetailed explanation of the lifecycle and how keys move as nodes are added, expanded, retired.\n\nStorage Resource Map. Includes weight, hash_seed. Hash seed enables a clever trick to retire device sets more efficiently.\n\nQ: How to find data when things are being moved? If clients talk to the old node while keys are being moved. Old node will proxy the request to the new node.\n\nStorage Class Memory Support in the Windows Operating System\n\nNeal Christiansen, Principal Development Lead, Microsoft\n\nWindows support for non-volatile storage medium with RAM-like performance is a big change.\n\nStorage Class Memory (SCM): NVDIMM, 3D XPoint, others\n\nMicrosoft involved with the standardization efforts in this space.\n\nNew driver model necessary: SCM Bus Driver, SCM Disk Driver.\n\nWindows Goals for SCM: Support zero-copy access, run most user-mode apps unmodified, option for 100% backward compatibility (new types of failure modes), sector granular failure modes for app compat.\n\nApplications make lots of assumptions on the underlying storage\n\nSCM Storage Drivers will support BTT – Block Translation Table. Provides sector-level atomicity for writes.\n\nSCM is disruptive. Fastest performance and application compatibility can be conflicting goals.\n\nSCM-aware File Systems for Windows. Volume modes: block mode or DAS mode (chosen at format time).\n\nBlock Mode Volumes – maintain existing semantics, full application compatibility\n\nDAS Mode Volumes – introduce new concepts (memory mapped files, maximizes performance). Some existing functionality is lost. Supported by NTFS and ReFS.\n\nMemory Mapped IO in DAS mode. Application can create a memory mapped section. Allowed when volumes resides on SCM hardware and the volume has been formatted for DAS mode.\n\nMemory Mapped IO: True zero copy access. BTT is not used. No paging reads or paging writes.\n\nCached IO in DAS Mode: Cache manager creates a DAS-enabled cache map. Cache manager will copy directly between user’s buffer and SCM. Coherent with memory-mapped IO. App will see new failure patterns on power loss or system crash. No paging reads or paging writes.\n\nNon-cached IO in DAS Mode. Will send IO down the storage stack to the SCM driver. Will use BTT. Maintains existing storage semantics.\n\nIf you really want the performance, you will need to change your code.\n\nDAS mode eliminates traditional hook points used by the file system to implement features.\n\nFeatures not in DAS Mode: NTFS encryption, NTS compression, NTFS TxF, ReFS integrity streams, ReFS cluster band, ReFS block cloning, Bitlocker volume encryption, snapshot via VolSnap, mirrored or parity via storage spaces or dynamic disks\n\nSparse files won’t be there initially but will come in the future.\n\nUpdated at the time the file is memory mapped: file modification time, mark file as modified in the USN journal, directory change notification\n\nFile System Filters in DAS mode: no notification that a DAS volume is mounted, filter will indicate via a flag if they understand DAS mode semantics.\n\nApplication compatibility with filters in DAS mode: No opportunity for data transformation filters (encryption, compression). Anti-virus are minimally impacted, but will need to watch for creation of writeable mapped sections (no paging writes anymore).\n\nIntel NVLM library. Open source library implemented by Intel. Defines set of application APIs for directly manipulating files on SCM hardware.\n\nNVLM library available for Linux today via GitHub. Microsoft working with Intel on a Windows port.\n\nQ: XIP (Execute in place)? It’s important, but the plans have not solidified yet.\n\nQ: NUMA? Can be in NUMA nodes. Typically, the file system and cache are agnostic to NUMA.\n\nQ: Hyper-V? Not ready to talk about what we are doing in that area.\n\nQ: Roll-out plan? We have one, but not ready to talk about it yet.\n\nQ: Data forensics? We’ve yet to discuss this with that group. But we will.\n\nQ: How far are you to completion? It’s running and working today. But it is not complete.\n\nQ: Windows client? To begin, we’re targeting the server. Because it’s available there first.\n\nQ: Effect on performance? When we’re ready to announce the schedule, we will announce the performance. The data about SCM is out there. It’s fast!\n\nQ: Will you backport? Probably not. We generally move forward only. Not many systems with this kind of hardware will run a down level OS.\n\nQ: What languages for the Windows port of NVML? Andy will cover that in his talk tomorrow.\n\nQ: How fast will memory mapped be? Potentially as fast as DRAM, but depends on the underlying technology.\n\nThe Bw-Tree Key-Value Store and Its Applications to Server/Cloud Data Management in Production\n\nSudipta Sengupta, Principal Research Scientist, Microsoft Research\n\nThe B-Tree: key-ordered access to records. Balanced tree via page split and merge mechanisms.\n\nDesign tenets: Lock free operation (high concurrency), log-structure storage (exploit flash devices with fast random reads and inefficient random writes), delta updates to pages (reduce cache invalidation, garbage creation)\n\nBw-Tree Architecture: 3 layers: B-Tree (expose API, B-tree search/update, in-memory pages), Cache (logical page abstraction, move between memory and flash), Flash (reads/writes from/to storage, storage management).\n\nMapping table: Expose logical pages to access method layer. Isolates updates to single page. Structure for lock-free multi-threaded concurrency control.\n\nHighly concurrent page updates with Bw-Tree. Explaining the process using a diagram.\n\nBw-Tree Page Split: No hard threshold for splitting unlike in classical B-Tree. B-link structure allows “half-split” without locking.\n\nFlash SSDs: Log-Structured storage. Use log structure to exploit the benefits of flash and work around its quirks: random reads are fast, random in-place writes are expensive.\n\nLLAMA Log-Structured Store: Amortize cost of writes over many page updates. Random reads to fetch a “logical page”.\n\nDepart from tradition: logical page formed by linking together records on multiple physical pages on flash. Adapted from SkimpyStash.\n\nDetailed diagram comparing traditional page writing with the writing optimized storage organization with Bw-Tree.\n\nLLAMA: Optimized Logical Page Reads. Multiple delta records are packed when flushed together. Pages consolidated periodically in memory also get consolidated on flash when flushed.\n\nLLAMA: Garbage collection on flash. Two types of record units in the log: Valid or Orphaned. Garbage collection starts from the oldest portion of the log. Earliest written record on a logical page is encountered first.\n\nLLAMA: cache layer. Responsible for moving pages back and forth from storage.\n\nBw-Tree Checkpointing: Need to flush to buffer and to storage. LLAMA checkpoint for fast recovery.\n\nBw-Tree Fast Recovery. Restore mapping table from latest checkpoint region. Warm-up using sequential I/O.\n\nBw-Tree: Support for transactions. Part of the Deuteronomy Architecture.\n\nEnd-to-end crash recovery. Data component (DC) and transactional component (TC) recovery. DC happens before TC.\n\nBw-Tree in production: Key-sequential index in SQL Server in-memory database\n\nBw-Tree in production: Indexing engine in Azure DocumentDB. Resource governance is important (CPU, Memory, IOPS, Storage)\n\nBw-Tree in production: Sorted key-value store in Bing ObjectStore.\n\nSummary: Classic B-Tree redesigned for modern hardware and cloud. Lock-free, delta updating of pages, log-structure, flexible resource governor, transactional. Shipping in production.\n\nGoing forward: Layer transactional component (Deuteronomy Architecture, CIDR 2015), open-source the codebase\n\nReFS v2: Cloning, Projecting, and Moving Data\n\nJ.R. Tipton, Development Lead, Microsoft\n\nAgenda: ReFS v1 primer, ReFS v2 at a glance, motivations for ReFS v2, cloning, translation, transformation\n\nReFS v1 primer: Windows allocate-on-write file system, Merkel trees verify metadata integrity, online data correction from alternate copies, online chkdsk\n\nReFS v2: Available in Windows Server 2016 TP4. Efficient, reliable storage for VMs, efficient parity, write tiering, read caching, block cloning, optimizations\n\nMotivations for ReFS v2: cheap storage does not mean slow, VM density, VM provisioning, more hardware flavors (SLC, MLC, TLC flash, SMR)\n\nWrite performance. Magic does not work in a few environments (super fast hardware, small random writes, durable writes/FUA/sync/write-through)\n\nReFS Block Cloning: Clone any block of one file into any other block in another file. Full file clone, reorder some or all data, project data from one area into another without copy\n\nReFS Block Cloning: Metadata only operation. Copy-on-write used when needed (ReFS knows when).\n\nCloning examples: deleting a Hyper-V VM checkpoint, VM provisioning from image.\n\nCloning observations: app directed, avoids data copies, metadata operations, Hyper-V is the first but not the only one using this\n\nCloning is no free lunch: multiple valid copies will copy-on-write upon changes. metadata overhead to track state, slam dunk in most cases, but not all\n\nReFS cluster bands. Volume internally divvied up into bands that contain regular FS clusters (4KB, 64KB). Mostly invisible outside file system. Bands and clusters track independently (per-band metadata). Bands can come and go.\n\nReFS can move bands around (read/write/update band pointer). Efficient write caching and parity. Writes to bands in fast tier. Tracks heat per band. Moves bands between tiers. More efficient allocation. You can move from 100% triple mirroring to 95% parity.\n\nReFS cluster bands: small writes accumulate where writing is cheap (mirror, flash, log-structured arena), bands are later shuffled to tier where random writes are expensive (band transfers are fully sequential).\n\nReFS cluster bands: transformation. ReFS can do stuff to the data in a band (can happen in the background). Examples: band compaction (put cold bands together, squeeze out free space), band compression (decompress on read).\n\nReFS v2 summary: data cloning, data movement, data transformation. Smart when smart makes sense, switches to dumb when dumb is better. Takes advantages of hardware combinations. And lots of other stuff…\n\nInnovator, Disruptor or Laggard, Where Will Your Storage Applications Live? Next Generation Storage\n\nBev Crair, Vice President and General Manager, Storage Group, Intel\n\nThe world is changing: information growth, complexity, cloud, technology.\n\nGrowth: 44ZB of data in all systems. 15% of the data is stored, since perceived cost is low.\n\nEvery minute of every day: 2013 : 8h of of video uploaded to YouTube, 47,000 apps downloaded, 200 million e-mails\n\nEvery minute of every day: 2015 : 300h of of video uploaded to YouTube, 51,000 apps downloaded, 204 million e-mails\n\nData never sleeps: the internet in real time. tiles showing activities all around the internet.\n\nData use pattern changes: sense and generate, collect and communicate, analyze and optimize. Example: HADRON collider\n\nData use pattern changes: from collection to analyzing data, valuable data now reside outside the organization, analyzing and optimizing unstructured data\n\nCloud impact on storage solutions: business impact, technology impact. Everyone wants an easy button\n\nIntelligent storage: Deduplication, real-time compression, intelligent tiering, thin provisioning. All of this is a software problem.\n\nScale-out storage: From single system with internal network to nodes working together with an external network\n\nNon-Volatile Memory (NVM) accelerates the enterprise: Examples in Virtualization, Private Cloud, Database, Big Data and HPC\n\nPyramid: CPU, DRAM, Intel DIMM (3D XPoint), Intel SSD (3D XPoint), NAND SSD, HDD, …\n\nStorage Media latency going down dramatically. With NVM, the bottleneck is now mostly in the software stack.\n\nFuture storage architecture: complex chart with workloads for 2020 and beyond. New protocols, new ways to attach.\n\nIntel Storage Technologies. Not only hardware, but a fair amount of software. SPDK, NVMe driver, Acceleration Library, Lustre, others.\n\nWhy does faster storage matter? Genome testing for cancer takes weeks, and the cancer mutates. Genome is 10TB. If we can speed up the time it takes to test it to one day, it makes a huge difference and you can create a medicine that saves a person’s life. That’s why it matters.\n\nThe Long-Term Future of Solid State Storage\n\nJim Handy, General Director, Objective Analysis\n\nHow we got here? Why are we in the trouble we’re at right now? How do we get ahead of it? Where is it going tomorrow?\n\nEstablishing a schism: Memory is in bytes (DRAM, Cache, Flash?), Storage is in blocks (Disk Tape, DVD, SAN, NAS, Cloud, Flash)\n\nIs it really about block? Block, NAND page, DRAM pages, CPU cache lines. It’s all in pages anyway…\n\nIs there another differentiator? Volatile vs. Persistent. It’s confusing…\n\nWhat is an SSD? SSDs are nothing new. Going back to DEC Bulk Core.\n\nDisk interfaces create delays. SSD vs HDD latency chart. Time scale in milliseconds.\n\nZooming in to tens of microseconds. Different components of the SSD delay. Read time, Transfer time, Link transfer, platform and adapter, software\n\nNow looking at delays for MLC NAND ONFi2, ONFi3, PCIe x4 Gen3, future NVM on PCIe x4 Gen3\n\nChanging the scale to tens of microseconds on future NVM. Link Transfer, Platform & adapter and Software now accounts for most of the latency.\n\nHow to move ahead? Get rid of the disk interfaces (PCIe, NVMe, new technologies). Work on the software: SNIA.\n\nWhy now? DRAM Transfer rates. Chart transfer rates for SDRAM, DDR, DDR2, DDR3, DDR4. Designing the bus takes most of the time.\n\nDRAM running out of speed? We probably won’t see a DDR5. HMC or HBM a likely next step. Everything points to fixed memory sizes.\n\nNVM to the rescue. DRAM is not the only upgrade path. It became cheaper to use NAND flash than DRAM to upgrade a PC.\n\nNVM to be a new memory layer between DRAM & NAND: Intel/Micron 3D XPoint – “Optane”\n\nOne won’t kill the other. Future systems will have DRAM, NVM, NAND, HDD. None of them will go away…\n\nNew memories are faster than NAND. Chart with read bandwidth vs write bandwidth. Emerging NVRAM: FeRAM, eMRAM, RRAM, PRAM.\n\nComplex chart with emerging research memories. Clock frequency vs. Cell Area (cost).\n\nThe computer of tomorrow. Memory or storage? In the beginning (core memory), there was no distinction between the two.\n\nWe’re moving to an era where you can turn off the computer, turn it back on and there’s something in memory. Do you trust it?\n\nSCM – Storage Class Memory: high performance with archival properties. There are many other terms for it: Persistent Memory, Non-Volatile Memory.\n\nNew NVM has disruptively low latency: Log chart with latency budgets for HDD, SATA SSD, NVMe, Persistent. When you go below 10 microseconds (as Persistent does), context switching does not make sense.\n\nNon-blocking I/O. NUMA latencies up to 200ns have been tolerated. Latencies below these cause disruption.\n\nMemory mapped files eliminate file system latency.\n\nThe computer of tomorrow. Fixed DRAM size, upgradeable NVM (tomorrow’s DIMM), both flash and disk (flash on PCIe or own bus), much work needed on SCM software\n\nQ: Will all these layers survive? I believe so. There are potential improvements in all of them (cited a few on NAND, HDD).\n\nQ: Shouldn’t we drop one of the layers? Usually, adding layers (not removing them) is more interesting from a cost perspective.\n\nQ: Do we need a new protocol for SCM? NAND did well without much of that. Alternative memories could be put on a memory bus.\n\nConcepts on Moving From SAS connected JBOD to an Ethernet Connected JBOD\n\nJim Pinkerton, Partner Architect Lead, Microsoft\n\nWhat if we took a JBOD, a simple device, and just put it on Ethernet?\n\nRe-Thinking the Software-defined Storage conceptual model definition: compute nodes, storage nodes, flakey storage devices\n\nFront-end fabric (Ethernet, IB or FC), Back-end fabric (directly attached or shared storage)\n\nYesterday’s Storage Architecture: Still highly profitable. Compute nodes, traditional SAN/NAS box (shipped as an appliance)\n\nToday: Software Defined Storage (SDS) – “Converged”. Separate the storage service from the JBOD.\n\nToday: Software Defined Storage (SDS) – “Hyper-Converged” (H-C). Everything ships in a single box. Scale-out architecture.\n\nH-C appliances are a dream for the customer to install/use, but the $/GB storage is high.\n\nMicrosoft Cloud Platform System (CPS). Shipped as a packaged deal. Microsoft tested and guaranteed.\n\nSDS with DAS – Storage layer divided into storage front-end (FE) and storage back-end (BE). The two communicate over Ethernet.\n\nSDS Topologies. Going from Converged and Hyper-Converged to a future EBOD topology. From file/block access to device access.\n\nExpose the raw device over Ethernet. The raw device is flaky, but we love it. The storage FE will abstract that, add reliability.\n\nI would like to have an EBOD box that could provide the storage BE.\n\nEBOD works for a variety of access protocols and topologies. Examples: SMB3 “block”, Lustre object store, Ceph object store, NVMe fabric, T10 objects.\n\nShared SAS Interop. Nightmare experience (disk multi-path interop, expander multi-path interop, HBA distributed failure). This is why customers prefers appliances.\n\nTo share or not to share. We want to share, but we do not want shared SAS. Customer deployment is more straightforward, but you have more traffic on Ethernet.\n\nHyper-Scale cloud tension – fault domain rebuild time. Depends on number of disks behind a node and how much network you have.\n\nFault domain for storage is too big. Required network speed offsets cost benefits of greater density. Many large disks behind a single node becomes a problem.\n\nPrivate cloud tension – not enough disks. Entry points at 4 nodes, small number of disks. Again, fault domain is too large.\n\nGoals in refactoring SDS – Storage back-end is a “data mover” (EBOD). Storage front-end is “general purpose CPU”.\n\nEBOD goals – Can you hit a cost point that’s interesting? Reduce storage costs, reduce size of fault domain, build a more robust ecosystem of DAS. Keep topology simple, so customer can build it themselves.\n\nEBOD: High end box, volume box, capacity box.\n\nEBOD volume box should be close to what a JBOD costs. Basically like exposing raw disks.\n\nComparing current Hyper-Scale to EBOD. EBOD has an NIC and an SOC, in addition to the traditional expander in a JBOD.\n\nEBOD volume box – Small CPU and memory, dual 10GbE, SOC with RDMA NIC/SATA/SAS/PCIe, up to 20 devices, SFF-8639 connector, management (IPMI, DMTF Redfish?)\n\nVolume EBOD Proof Point – Intel Avaton, PCIe Gen 2, Chelsio 10GbE, SAS HBA, SAS SSD. Looking at random read IOPS (local, RDMA remote and non-RDMA remote). Max 159K IOPS w/RDMA, 122K IOPS w/o RDMA. Latency chart showing just a few msec.\n\nEBOD Performance Concept – Big CPU, Dual attach 40GbE, Possibly all NVME attach or SCM. Will show some of the results this afternoon.\n\nEBOD is an interesting approach that’s different from what we’re doing. But it’s nicely aligned with software-defined storage.\n\nPrice point of EBOD must be carefully managed, but the low price point enables a smaller fault domain.\n\nPlanning for the Next Decade of NVM Programming\n\nAndy Rudoff, SNIA NVM Programming TWG, Intel\n\nLooking at what’s coming up in the next decade, but will start with some history.\n\nComparison of data storage technologies. Emerging NV technologies with read times in the same order of magnitude as DRAM.\n\nMoving the focus to software latency when using future NVM.\n\nIs it memory or storage? It’s persistent (like storage) and byte-addressable (like memory).\n\nStorage vs persistent memory. Block IO vs. byte addressable, sync/async (DMA master) vs. sync (DMA slave). High capacity vs. growing capacity.\n\npmem: The new Tier. Byte addressable, but persistent. Not NAND. Can do small I/O. Can DMA to it.\n\nSNIA TWG (lots of companies). Defining the NVM programming model: NVM.PM.FILE mode and NVM.PM.VOLUME mode.\n\nAll the OSes created in the last 30 years have a memory mapped file.\n\nIs this stuff real? Why are we spending so much time on this? Yes – Intel 3D XPoint technology, the Intel DIMM. Showed a wafer on stage. 1000x faster than NAND. 1000X endurance of NAND, 10X denser than conventional memory. As much as 6TB of this stuff…\n\nTimeline: Big gap between NAND flash memory (1989) and 3D XPoint (2015).\n\nDiagram of of the model with Management, Block, File and Memory access. Link at the end to the diagram.\n\nDetecting pmem: Defined in the ACPI 6.0. Linux support upstream (generic DIMM driver, DAX, ext4+DAX, KVM). Neal talked about Windows support yesterday.\n\nHeavy OSV involvement in TWG, we wrote the spec together.\n\nWe don’t want every application to have to re-architecture itself. That’s why we have block and file there as well.\n\nThe next decade\n\nTransparency levels: increasing barrier to adoption. increasing leverage. Could do it in layers. For instance, could be file system only, without app modification. For instance, could modify just the JVM to get significant advantages without changing the apps.\n\nComparing to multiple cores in hardware and multi-threaded programming. Took a decade or longer, but it’s commonplace now.\n\nOne transparent example: pmem Paging. Paging from the OS page cache (diagrams).\n\nAttributes of paging : major page faults, memory looks much larger, page in must pick a victim, many enterprise apps opt-out, interesting example: Java GC.\n\nWhat would it look like if you paged to pmem instead of paging to storage. I don’t even care that it’s persistent, just that there’s a lot of it.\n\nI could kick a page out synchronously, probably faster than a context switch. But the app could access the data in pmem without swapping it in (that‘s new!). Could have policies for which app lives in which memory. The OS could manage that, with application transparency.\n\nWould this really work? It will when pmem costs less, performance is close, capacity is significant and it is reliable. “We’re going to need a bigger byte” to hold error information.\n\nNot just for pmem. Other memories technologies are emerging. High bandwidth memory, NUMA localities, different NVM technologies.\n\nExtending into user space: NVM Library – pmem.io (64-bit Linux Alpha release). Windows is working on it as well.\n\nThat is a non-transparent example. It’s hard (like multi-threading). Things can fail in interesting new ways.\n\nThe library makes it easier and some of it is transactional.\n\nNo kernel interception point, for things like replication. No chance to hook above or below the file system. You could do it in the library.\n\nNon-transparent use cases: volatile caching, in-memory database, storage appliance write cache, large byte-addressable data structures (hash table, dedup), HPC (checkpointing)\n\nSweet spots: middleware, libraries, in-kernel usages.\n\nBig challenge: middleware, libraries. Is it worth the complexity.\n\nBuilding a software ecosystem for pmem, cost vs. benefit challenge.\n\nPrepare yourself: lean NVM programming model, map use cases to pmem, contribute to the libraries, software ecosystem\n\nFS Design Around SMR: Seagate’s Journey and Reference System with EXT4\n\nAdrian Palmer, Drive Development Engineering, Seagate Technologies\n\nSNIA Tutorial. I’m talking about the standard, as opposed as the design of our drive.\n\nSMR is being embraced by everyone, since this is a major change, a game changes.\n\nFrom random writes to resemble the write profile of sequential-access tape.\n\n1 new condition: forward-write preferred. ZAC/ZBD spec: T10/13. Zones, SCSI ZBC standards, ATA ZAC standards.\n\nWhat is a file system? Essential software on a system, structured and unstructured data, stores metadata and data.\n\nBasic FS requirements: Write-in-place (superblock, known location on disk), Sequential write (journal), Unrestricted write type (random or sequential)\n\nDrive parameters: Sector (atomic unit of read/write access). Typically 512B size. Independently accessed. Read/write, no state.\n\nDrive parameters: Zone (atomic performant rewrite unit). Typically 256 MiB in size. Indirectly addressed via sector. Modified with ZAC/ZBD commands. Each zone has state (WritePointer, Condition, Size, Type).\n\nWrite Profiles. Conventional (random access), Tape (sequential access), Flash (sequential access, erase blocks), SMR HA/HM (sequential access, zones). SMR write profile is similar to Tape and Flash.\n\nAllocation containers. Drive capacities are increasing, location mapping is expensive. 1.56% with 512B blocks or 0.2% with 4KB blocks.\n\nRemap the block device as a… block device. Partitions (w*sector size), Block size (x*sector size), Group size (y*Block size), FS (z*group size, expressed as blocks).\n\nZones are a good fit to be matched with Groups. Absorb and mirror the metadata, don’t keep querying drive for metadata.\n\nSolving the sequential write problem. Separate the problem spaces with zones.\n\nDedicate zones to each problem space: user data, file records, indexes, superblock, trees, journal, allocation containers.\n\nGPT/Superblocks: First and last zone (convention, not guaranteed). Update infrequently, and at dismount. Looks at known location and WritePointer. Copy-on-update. Organized wipe and update algorithm.\n\nJournal/soft updates. Update very frequently, 2 or more zones, set up as a circular buffer. Checkpoint at each zone. Wipe and overwrite oldest zone. Can be used as NV cache for metadata. Requires lots of storage space for efficient use and NV.\n\nGroup descriptors: Infrequently changed. Changes on zone condition change, resize, free block counts. Write cached, butwritten at WritePointer. Organized as a B+Tree, not an indexed array. The B+Tree needs to be stored on-disk.\n\nFile Records: POSIX information (ctime, mtime, atime, msize, fs specific attributes), updated very frequently. Allows records to be modified in memory, written to journal cache, gather from journal, write to new blocks at WritePointer.\n\nMapping (file records to blocks). File ideally written as a single chunk (single pointer), but could become fragmented (multiple pointers). Can outgrow file record space, needs its own B+Tree. List can be in memory, in the journal, written out to disk at WritePointer.\n\nData: Copy-on-write. Allocator chooses blocks at WritePointer. Writes are broken at zone boundary, creating new command and new mapping fragment.\n\nCleanup: Cannot clean up as you go, need a separate step. Each zone will have holes. Garbage collection: Journal GC, Zones GC, Zone Compaction, Defragmentation.\n\nAdvanced features: indexes, queries, extended attributes, snapshots, checksums/parity, RAID/JBOD.\n\nAzure File Service: ‘Net Use’ the Cloud\n\nDavid Goebel, Software Engineer, Microsoft\n\nAgenda: features and API (what), scenarios enabled (why), design of an SMB server not backed by a conventional FS (how)\n\nIt’s not the Windows SMB server (srv2.sys). Uses Azure Tables and Azure Blobs for the actual files.\n\nEasier because we already have a highly available and distributed architecture.\n\nSMB 2.1 in preview since last summer. SMB 3.0 (encryption, persistent handles) in progress.\n\nAzure containers mapped as shares. Clients work unmodified out-of-the-box. We implemented the spec.\n\nShare namespace is coherently accessible\n\nMS-SMB2, not SMB1. Anticipates (but does not require) a traditional file system on the other side.\n\nIn some ways it’s harder, since what’s there is not a file system. We have multiple tables (for leases, locks, etc). Nice and clean.\n\nSMB is a stateful protocol, while REST is all stateless. Some state is immutable (like FileId), some state is transient (like open counts), some is maintained by the client (like CreateGuid), some state is ephemeral (connection).\n\nDiagram with the big picture. Includes DNS, load balancer, session setup & traffic, front-end node, azure tables and blobs.\n\nFront-end has ephemeral and immutable state. Back-end has solid and fluid durable state.\n\nDiagram with two clients accessing the same file and share, using locks, etc. All the state handled by the back-end.\n\nLosing a front-end node considered a regular event (happens during updates), the client simple reconnects, transparently.\n\nCurrent state, SMB 2.1 (SMB 3.0 in the works). 5TB per share and 1TB per file. 1,000 8KB IOPS per share, 60MB/sec per share. Some NTFS features not supported, some limitations on characters and path length (due to HTTP/REST restrictions).\n\nDemo: I’m actually running my talk using a PPTX file on Azure File. Robocopy to file share. Delete, watch via explorer (notifications working fine). Watching also via wireshark.\n\nCurrently Linux Support. Lists specific versions Ubuntu Server, Ubuntu Core, CentOS, Open SUSE, SUSE Linux Enterprise Server.\n\nWhy: They want to move to cloud, but they can’t change their apps. Existing file I/O applications. Most of what was written over the last 30 years “just works”. Minor caveats that will become more minor over time.\n\nDiscussed specific details about how permissions are currently implemented. ACL support is coming.\n\nExample: Encryption enabled scenario over the internet.\n\nWhat about REST? SMB and REST access the same data in the same namespace, so a gradual application transition without disruption is possible. REST for container, directory and file operations.\n\nThe durability game. Modified state that normally exists only in server memory, which must be durably committed.\n\nExamples of state tiering: ephemeral state, immutable state, solid durable state, fluid durable state.\n\nExample: Durable Handle Reconnect. Intended for network hiccups, but stretched to also handles front-end reconnects. Limited our ability because of SMB 2.1 protocol compliance.\n\nExample: Persistent Handles. Unlike durable handles, SMB 3 is actually intended to support transparent failover when a front-end dies. Seamless transparent failover.\n\nResource Links: Getting started blog (http://blogs.msdn.com/b/windowsazurestorage/archive/2014/05/12/introducing-microsoft-azure-file-service.aspx) , NTFS features currently not supported (https://msdn.microsoft.com/en-us/library/azure/dn744326.aspx), naming restrictions for REST compatibility (https://msdn.microsoft.com/library/azure/dn167011.aspx).\n\nSoftware Defined Storage – What Does it Look Like in 3 Years?\n\nRichard McDougall, Big Data and Storage Chief Scientist, VMware\n\nHow do you come up with a common, generic storage platform that serves the needs of application?\n\nBringing a definition of SDS. Major trends in hardware, what the apps are doing, cloud platforms\n\nStorage workloads map. Many apps on 4 quadrants on 2 axis: capacity (10’s of Terabytes to 10’s of Petabytes) and IOPS (1K to 1M)\n\nWhat are cloud-native applications? Developer access via API, continuous integration and deployment, built for scale, availability architected in the app, microservices instead of monolithic stacks, decoupled from infrastructure\n\nWhat do Linux containers need from storage? Copy/clone root images, isolated namespace, QoS controls\n\nOptions to deliver storage to containers: copy whole root tree (primitive), fast clone using shared read-only images, clone via “Another Union File System” (aufs), leverage native copy-on-write file system.\n\nShared data: Containers can share file system within host or across hots (new interest in distributed file systems)\n\nDocker storage abstractions for containers: non-persistent boot environment, persistent data (backed by block volumes)\n\nContainer storage use cases: unshared volumes, shared volumes, persist to external storage (API to cloud storage)\n\nEliminate the silos: converged big data platform. Diagram shows Hadoop, HBase, Impala, Pivotal HawQ, Cassandra, Mongo, many others. HDFS, MAPR, GPFS, POSIX, block storage. Storage system common across all these, with the right access mechanism.\n\nBack to the quadrants based on capacity and IOPS. Now with hardware solutions instead of software. Many flash appliances in the upper left (low capacity, high IOPS). Isilon in the lower right (high capacity, low IOPS).\n\nStorage media technologies in 2016. Pyramid with latency, capacity per device, capacity per host for each layer: DRAM (1TB/device, 4TB/host, ~100ns latency), NVM (1TB, 4TB, ~500ns), NVMe SSD (4TB, 48TB, ~10us), capacity SSD (16TB, 192TB, ~1ms), magnetic storage (32TB, 384TB, ~10ms), object storage (?, ?, ~1s).\n\nBack to the quadrants based on capacity and IOPS. Now with storage media technologies.\n\nDetails on the types of NVDIMM (NVIMM-N – Type 1, NVDIMM-F – Type 2, Type 4). Standards coming up for all of these. Needs work to virtualize those, so they show up properly inside VMs.\n\nIntel 3D XPoint Technology.\n\nWhat are the SDS solutions than can sit on top of all this? Back to quadrants with SDS solutions. Nexenta, Mentions ScaleiO, VSAN, ceph, Scality, MAPR, HDFS. Can you make one solution that works well for everything?\n\nWhat’s really behind a storage array? The value from the customer is that it’s all from one vendor and it all works. Nothing magic, but the vendor spent a ton of time on testing.\n\nTypes of SDS: Fail-over software on commodity servers (lists many vendors), complexity in hardware, interconnects. Issues with hardware compatibility.\n\nTypes of SDS: Software replication using servers + local disks. Simpler, but not very scalable.\n\nTypes of SDS: Caching hot core/cold edge. NVMe flash devices up front, something slower behind it (even cloud). Several solutions, mostly startups.\n\nTypes of SDS: Scale-out SDS. Scalable, fault-tolerant, rolling updates. More management, separate compute and storage silos. Model used by ceph, ScaleiO. Issues with hardware compatibility. You really need to test the hardware.\n\nTypes of SDS: Hyper-converged SDS. Easy management, scalable, fault-tolerant, rolling upgrades. Fixed compute to storage ration. Model used by VSAN, Nutanix. Amount of variance in hardware still a problem. Need to invest in HCL verification.\n\nStorage interconnects. Lots of discussion on what’s the right direction. Protocols (iSCSI, FC, FCoE, NVMe, NVMe over Fabrics), Hardware transports (FC, Ethernet, IB, SAS), Device connectivity (SATA, SAS, NVMe)\n\nNetwork. iSCSI, iSER, FCoE, RDMA over Ethernet, NVMe Fabrics. Can storage use the network? RDMA debate for years. We’re at a tipping point.\n\nDevice interconnects: HCA with SATA/SAS. NVMe SSD, NVM over PCIe. Comparing iSCSI, FCoE and NVMe over Ethernet.\n\nPCIe rack-level Fabric. Devices become addressable. PCIe rack-scale compute and storage, with host-to-host RDMA.\n\nNVMe – The new kid on the block. Support from various vendors. Quickly becoming the all-purpose stack for storage, becoming the universal standard for talking block.\n\nBeyond block: SDS Service Platforms. Back to the 4 quadrants, now with service platforms.\n\nToo many silos: block, object, database, key-value, big data. Each one is its own silo with its own machines, management stack, HCLs. No sharing of infrastructure.\n\nOption 1: Multi-purpose stack. Has everything we talked about, but it’s a compromise.\n\nOption 2: Common platform + ecosystem of services. Richest, best-of-breed services, on a single platform, manageable, shared resources.\n\nWhy the Storage You Have is Not the Storage Your Data Needs\n\nLaz Vekiarides, CTO and Co-founder, ClearSky Data\n\nClearSky Data is a tech company, consumes what we discussed in this conference.\n\nThe problem we’re trying to solve is the management of the storage silos\n\nEnterprise storage today. Chart: Capacity vs. $/TB. Flash, Mid-Range, Scale-Out. Complex, costly silos\n\nDescribe the lifecycle of the data, the many copies you make over time, the rebuilding and re-buying of infrastructure\n\nWhat enterprises want: buy just enough of the infrastructure, with enough performance, availability, security.\n\nCloud economics – pay only for the stuff that you use, you don’t have to see all the gear behind the storage, someone does the physical management\n\nTiering is a bad answer – Nothing remains static. How fast does hot data cool? How fast does it re-warm? What is the overhead to manage it? It’s a huge overhead. It’s not just a bandwidth problem.\n\nIt’s the latency, stupid. Data travels at the speed of light. Fast, but finite. Boston to San Francisco: 29.4 milliseconds of round-trip time (best case). Reality (with switches, routers, protocols, virtualization) is more like 70 ms.\n\nSo, where exactly is the cloud? Amazon East is near Ashburn, VA. Best case is 10ms RTT. Worst case is ~150ms (do"
    }
}
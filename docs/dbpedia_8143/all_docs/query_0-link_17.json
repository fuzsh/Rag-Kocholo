{
    "id": "dbpedia_8143_0",
    "rank": 17,
    "data": {
        "url": "https://learn.microsoft.com/en-us/azure/ai-services/openai/concepts/content-filter",
        "read_more_link": "",
        "language": "en",
        "title": "Azure OpenAI Service content filtering - Azure OpenAI",
        "top_image": "https://learn.microsoft.com/en-us/media/open-graph-image.png",
        "meta_img": "https://learn.microsoft.com/en-us/media/open-graph-image.png",
        "images": [],
        "movies": [],
        "keywords": [],
        "meta_keywords": [
            ""
        ],
        "tags": null,
        "authors": [],
        "publish_date": "2024-06-12T22:54:00+00:00",
        "summary": "",
        "meta_description": "Learn about the content filtering capabilities of Azure OpenAI in Azure AI services.",
        "meta_lang": "en",
        "meta_favicon": "",
        "meta_site_name": "",
        "canonical_link": "https://learn.microsoft.com/en-us/azure/ai-services/openai/concepts/content-filter",
        "text": "Content filtering\n\nAzure OpenAI Service includes a content filtering system that works alongside core models, including DALL-E image generation models. This system works by running both the prompt and completion through an ensemble of classification models aimed at detecting and preventing the output of harmful content. The content filtering system detects and takes action on specific categories of potentially harmful content in both input prompts and output completions. Variations in API configurations and application design might affect completions and thus filtering behavior.\n\nThe content filtering models for the hate, sexual, violence, and self-harm categories have been specifically trained and tested on the following languages: English, German, Japanese, Spanish, French, Italian, Portuguese, and Chinese. However, the service can work in many other languages, but the quality might vary. In all cases, you should do your own testing to ensure that it works for your application.\n\nIn addition to the content filtering system, the Azure OpenAI Service performs monitoring to detect content and/or behaviors that suggest use of the service in a manner that might violate applicable product terms. For more information about understanding and mitigating risks associated with your application, see the Transparency Note for Azure OpenAI. For more information about how data is processed for content filtering and abuse monitoring, see Data, privacy, and security for Azure OpenAI Service.\n\nThe following sections provide information about the content filtering categories, the filtering severity levels and their configurability, and API scenarios to be considered in application design and implementation.\n\nContent filtering categories\n\nThe content filtering system integrated in the Azure OpenAI Service contains:\n\nNeural multi-class classification models aimed at detecting and filtering harmful content; the models cover four categories (hate, sexual, violence, and self-harm) across four severity levels (safe, low, medium, and high). Content detected at the 'safe' severity level is labeled in annotations but isn't subject to filtering and isn't configurable.\n\nOther optional classification models aimed at detecting jailbreak risk and known content for text and code; these models are binary classifiers that flag whether user or model behavior qualifies as a jailbreak attack or match to known text or source code. The use of these models is optional, but use of protected material code model may be required for Customer Copyright Commitment coverage.\n\nRisk categories\n\nCategory Description Hate and Fairness Hate and fairness-related harms refer to any content that attacks or uses discriminatory language with reference to a person or Identity group based on certain differentiating attributes of these groups.\n\nThis includes, but is not limited to:\n\nRace, ethnicity, nationality\n\nGender identity groups and expression\n\nSexual orientation\n\nReligion\n\nPersonal appearance and body size\n\nDisability status\n\nHarassment and bullying\n\nSexual Sexual describes language related to anatomical organs and genitals, romantic relationships and sexual acts, acts portrayed in erotic or affectionate terms, including those portrayed as an assault or a forced sexual violent act against oneâs will.â¯\n\nâ¯This includes but is not limited to:\n\nVulgar content\n\nProstitution\n\nNudity and Pornography\n\nAbuse\n\nChild exploitation, child abuse, child grooming\n\nViolence Violence describes language related to physical actions intended to hurt, injure, damage, or kill someone or something; describes weapons, guns and related entities.\n\nThis includes, but isn't limited to:\n\nWeapons\n\nBullying and intimidation\n\nTerrorist and violent extremism\n\nStalking\n\nSelf-Harm Self-harm describes language related to physical actions intended to purposely hurt, injure, damage oneâs body or kill oneself.\n\nThis includes, but isn't limited to:\n\nEating Disorders\n\nBullying and intimidation\n\nProtected Material for Text* Protected material text describes known text content (for example, song lyrics, articles, recipes, and selected web content) that can be outputted by large language models. Protected Material for Code Protected material code describes source code that matches a set of source code from public repositories, which can be outputted by large language models without proper citation of source repositories.\n\n* If you're an owner of text material and want to submit text content for protection, file a request.\n\nPrompt Shields\n\nType Description Prompt Shield for User Prompt Attacks User prompt attacks are User Prompts designed to provoke the Generative AI model into exhibiting behaviors it was trained to avoid or to break the rules set in the System Message. Such attacks can vary from intricate roleplay to subtle subversion of the safety objective. Prompt Shield for Indirect Attacks Indirect Attacks, also referred to as Indirect Prompt Attacks or Cross-Domain Prompt Injection Attacks, are a potential vulnerability where third parties place malicious instructions inside of documents that the Generative AI system can access and process. Requires document embedding and formatting.\n\nText content\n\nImage content\n\nPrompt shield content\n\nConfigurability (preview)\n\nThe default content filtering configuration for the GPT model series is set to filter at the medium severity threshold for all four content harm categories (hate, violence, sexual, and self-harm) and applies to both prompts (text, multi-modal text/image) and completions (text). This means that content that is detected at severity level medium or high is filtered, while content detected at severity level low isn't filtered by the content filters. For DALL-E, the default severity threshold is set to low for both prompts (text) and completions (images), so content detected at severity levels low, medium, or high is filtered. The configurability feature is available in preview and allows customers to adjust the settings, separately for prompts and completions, to filter content for each content category at different severity levels as described in the table below:\n\nSeverity filtered Configurable for prompts Configurable for completions Descriptions Low, medium, high Yes Yes Strictest filtering configuration. Content detected at severity levels low, medium, and high is filtered. Medium, high Yes Yes Content detected at severity level low isn't filtered, content at medium and high is filtered. High Yes Yes Content detected at severity levels low and medium isn't filtered. Only content at severity level high is filtered. Requires approval1. No filters If approved1 If approved1 No content is filtered regardless of severity level detected. Requires approval1.\n\n1 For Azure OpenAI models, only customers who have been approved for modified content filtering have full content filtering control and can off turn content filters. Apply for modified content filters via this form: Azure OpenAI Limited Access Review: Modified Content Filters For Azure Government customers, please apply for modified content filters via this form: Azure Government - Request Modified Content Filtering for Azure OpenAI Service.\n\nConfigurable content filters for inputs (prompts) and outputs (completions) are available for the following Azure OpenAI models:\n\nGPT model series\n\nGPT-4 Turbo Vision GA* (turbo-2024-04-09)\n\nGPT-4o\n\nDALL-E 2 and 3\n\n*Only available for GPT-4 Turbo Vision GA, does not apply to GPT-4 Turbo Vision preview\n\nContent filtering configurations are created within a Resource in Azure AI Studio, and can be associated with Deployments. Learn more about configurability here.\n\nCustomers are responsible for ensuring that applications integrating Azure OpenAI comply with the Code of Conduct.\n\nScenario details\n\nWhen the content filtering system detects harmful content, you receive either an error on the API call if the prompt was deemed inappropriate, or the finish_reason on the response will be content_filter to signify that some of the completion was filtered. When building your application or system, you'll want to account for these scenarios where the content returned by the Completions API is filtered, which might result in content that is incomplete. How you act on this information will be application specific. The behavior can be summarized in the following points:\n\nPrompts that are classified at a filtered category and severity level will return an HTTP 400 error.\n\nNon-streaming completions calls won't return any content when the content is filtered. The finish_reason value is set to content_filter. In rare cases with longer responses, a partial result can be returned. In these cases, the finish_reason is updated.\n\nFor streaming completions calls, segments are returned back to the user as they're completed. The service continues streaming until either reaching a stop token, length, or when content that is classified at a filtered category and severity level is detected.\n\nScenario: You send a non-streaming completions call asking for multiple outputs; no content is classified at a filtered category and severity level\n\nThe table below outlines the various ways content filtering can appear:\n\nHTTP response code Response behavior 200 In the cases when all generation passes the filters as configured, no content moderation details are added to the response. The finish_reason for each generation will be either stop or length.\n\nExample request payload:\n\n{ \"prompt\":\"Text example\", \"n\": 3, \"stream\": false }\n\nExample response JSON:\n\n{ \"id\": \"example-id\", \"object\": \"text_completion\", \"created\": 1653666286, \"model\": \"davinci\", \"choices\": [ { \"text\": \"Response generated text\", \"index\": 0, \"finish_reason\": \"stop\", \"logprobs\": null } ] }\n\nScenario: Your API call asks for multiple responses (N>1) and at least one of the responses is filtered\n\nHTTP Response Code Response behavior 200 The generations that were filtered will have a finish_reason value of content_filter.\n\nExample request payload:\n\n{ \"prompt\":\"Text example\", \"n\": 3, \"stream\": false }\n\nExample response JSON:\n\n{ \"id\": \"example\", \"object\": \"text_completion\", \"created\": 1653666831, \"model\": \"ada\", \"choices\": [ { \"text\": \"returned text 1\", \"index\": 0, \"finish_reason\": \"length\", \"logprobs\": null }, { \"text\": \"returned text 2\", \"index\": 1, \"finish_reason\": \"content_filter\", \"logprobs\": null } ] }\n\nScenario: An inappropriate input prompt is sent to the completions API (either for streaming or non-streaming)\n\nHTTP Response Code Response behavior 400 The API call fails when the prompt triggers a content filter as configured. Modify the prompt and try again.\n\nExample request payload:\n\n{ \"prompt\":\"Content that triggered the filtering model\" }\n\nExample response JSON:\n\n\"error\": { \"message\": \"The response was filtered\", \"type\": null, \"param\": \"prompt\", \"code\": \"content_filter\", \"status\": 400 }\n\nScenario: You make a streaming completions call; no output content is classified at a filtered category and severity level\n\nHTTP Response Code Response behavior 200 In this case, the call streams back with the full generation and finish_reason will be either 'length' or 'stop' for each generated response.\n\nExample request payload:\n\n{ \"prompt\":\"Text example\", \"n\": 3, \"stream\": true }\n\nExample response JSON:\n\n{ \"id\": \"cmpl-example\", \"object\": \"text_completion\", \"created\": 1653670914, \"model\": \"ada\", \"choices\": [ { \"text\": \"last part of generation\", \"index\": 2, \"finish_reason\": \"stop\", \"logprobs\": null } ] }\n\nScenario: You make a streaming completions call asking for multiple completions and at least a portion of the output content is filtered\n\nHTTP Response Code Response behavior 200 For a given generation index, the last chunk of the generation includes a non-null finish_reason value. The value is content_filter when the generation was filtered.\n\nExample request payload:\n\n{ \"prompt\":\"Text example\", \"n\": 3, \"stream\": true }\n\nExample response JSON:\n\n{ \"id\": \"cmpl-example\", \"object\": \"text_completion\", \"created\": 1653670515, \"model\": \"ada\", \"choices\": [ { \"text\": \"Last part of generated text streamed back\", \"index\": 2, \"finish_reason\": \"content_filter\", \"logprobs\": null } ] }\n\nScenario: Content filtering system doesn't run on the completion\n\nHTTP Response Code Response behavior 200 If the content filtering system is down or otherwise unable to complete the operation in time, your request will still complete without content filtering. You can determine that the filtering wasn't applied by looking for an error message in the content_filter_result object.\n\nExample request payload:\n\n{ \"prompt\":\"Text example\", \"n\": 1, \"stream\": false }\n\nExample response JSON:\n\n{ \"id\": \"cmpl-example\", \"object\": \"text_completion\", \"created\": 1652294703, \"model\": \"ada\", \"choices\": [ { \"text\": \"generated text\", \"index\": 0, \"finish_reason\": \"length\", \"logprobs\": null, \"content_filter_result\": { \"error\": { \"code\": \"content_filter_error\", \"message\": \"The contents are not filtered\" } } } ] }\n\nAnnotations\n\nContent filters\n\nWhen annotations are enabled as shown in the code snippet below, the following information is returned via the API for the categories hate and fairness, sexual, violence, and self-harm:\n\ncontent filtering category (hate, sexual, violence, self_harm)\n\nthe severity level (safe, low, medium, or high) within each content category\n\nfiltering status (true or false).\n\nOptional models\n\nOptional models can be enabled in annotate (returns information when content was flagged, but not filtered) or filter mode (returns information when content was flagged and filtered).\n\nWhen annotations are enabled as shown in the code snippets below, the following information is returned by the API for optional models:\n\nModel Output User prompt attack detected (true or false),\n\nfiltered (true or false) indirect attacks detected (true or false),\n\nfiltered (true or false) protected material text detected (true or false),\n\nfiltered (true or false) protected material code detected (true or false),\n\nfiltered (true or false),\n\nExample citation of public GitHub repository where code snippet was found,\n\nThe license of the repository\n\nWhen displaying code in your application, we strongly recommend that the application also displays the example citation from the annotations. Compliance with the cited license may also be required for Customer Copyright Commitment coverage.\n\nSee the following table for the annotation availability in each API version:\n\nCategory 2024-02-01 GA 2024-04-01-preview 2023-10-01-preview 2023-06-01-preview Hate â â â â Violence â â â â Sexual â â â â Self-harm â â â â Prompt Shield for user prompt attacks â â â â Prompt Shield for indirect attacks â Protected material text â â â â Protected material code â â â â Profanity blocklist â â â â Custom blocklist â â â\n\nFor details on the inference REST API endpoints for Azure OpenAI and how to create Chat and Completions, follow Azure OpenAI Service REST API reference guidance. Annotations are returned for all scenarios when using any preview API version starting from 2023-06-01-preview, as well as the GA API version 2024-02-01.\n\nExample scenario: An input prompt containing content that is classified at a filtered category and severity level is sent to the completions API\n\n{ \"error\": { \"message\": \"The response was filtered due to the prompt triggering Azure Content management policy. Please modify your prompt and retry. To learn more about our content filtering policies please read our documentation: https://go.microsoft.com/fwlink/?linkid=21298766\", \"type\": null, \"param\": \"prompt\", \"code\": \"content_filter\", \"status\": 400, \"innererror\": { \"code\": \"ResponsibleAIPolicyViolation\", \"content_filter_result\": { \"hate\": { \"filtered\": true, \"severity\": \"high\" }, \"self-harm\": { \"filtered\": true, \"severity\": \"high\" }, \"sexual\": { \"filtered\": false, \"severity\": \"safe\" }, \"violence\": { \"filtered\":true, \"severity\": \"medium\" } } } } }\n\nDocument embedding in prompts\n\nA key aspect of Azure OpenAI's Responsible AI measures is the content safety system. This system runs alongside the core GPT model to monitor any irregularities in the model input and output. Its performance is improved when it can differentiate between various elements of your prompt like system input, user input, and AI assistant's output.\n\nFor enhanced detection capabilities, prompts should be formatted according to the following recommended methods.\n\nChat Completions API\n\nThe Chat Completion API is structured by definition. It consists of a list of messages, each with an assigned role.\n\nThe safety system parses this structured format and apply the following behavior:\n\nOn the latest âuserâ content, the following categories of RAI Risks will be detected:\n\nHate\n\nSexual\n\nViolence\n\nSelf-Harm\n\nPrompt shields (optional)\n\nThis is an example message array:\n\n{\"role\": \"system\", \"content\": \"Provide some context and/or instructions to the model.\"}, {\"role\": \"user\", \"content\": \"Example question goes here.\"}, {\"role\": \"assistant\", \"content\": \"Example answer goes here.\"}, {\"role\": \"user\", \"content\": \"First question/message for the model to actually respond to.\"}\n\nEmbedding documents in your prompt\n\nIn addition to detection on last user content, Azure OpenAI also supports the detection of specific risks inside context documents via Prompt Shields â Indirect Prompt Attack Detection. You should identify parts of the input that are a document (for example, retrieved website, email, etc.) with the following document delimiter.\n\n<documents> *insert your document content here* </documents>\n\nWhen you do so, the following options are available for detection on tagged documents:\n\nOn each tagged âdocumentâ content, detect the following categories:\n\nIndirect attacks (optional)\n\nHere's an example chat completion messages array:\n\n{\"role\": \"system\", \"content\": \"Provide some context and/or instructions to the model, including document context. \\\"\\\"\\\" <documents>\\n*insert your document content here*\\n<\\\\documents> \\\"\\\"\\\"\"}, {\"role\": \"user\", \"content\": \"First question/message for the model to actually respond to.\"}\n\nJSON escaping\n\nWhen you tag unvetted documents for detection, the document content should be JSON-escaped to ensure successful parsing by the Azure OpenAI safety system.\n\nFor example, see the following email body:\n\nHello JosÃ¨, I hope this email finds you well today.\n\nWith JSON escaping, it would read:\n\nHello Jos\\u00E9,\\nI hope this email finds you well today.\n\nThe escaped text in a chat completion context would read:\n\n{\"role\": \"system\", \"content\": \"Provide some context and/or instructions to the model, including document context. \\\"\\\"\\\" <documents>\\n Hello Jos\\\\u00E9,\\\\nI hope this email finds you well today. \\n<\\\\documents> \\\"\\\"\\\"\"}, {\"role\": \"user\", \"content\": \"First question/message for the model to actually respond to.\"}\n\nContent streaming\n\nThis section describes the Azure OpenAI content streaming experience and options. Customers can receive content from the API as it's generated, instead of waiting for chunks of content that have been verified to pass your content filters.\n\nDefault\n\nThe content filtering system is integrated and enabled by default for all customers. In the default streaming scenario, completion content is buffered, the content filtering system runs on the buffered content, and â depending on the content filtering configuration â content is either returned to the user if it doesn't violate the content filtering policy (Microsoft's default or a custom user configuration), or itâs immediately blocked and returns a content filtering error, without returning the harmful completion content. This process is repeated until the end of the stream. Content is fully vetted according to the content filtering policy before it's returned to the user. Content isn't returned token-by-token in this case, but in âcontent chunksâ of the respective buffer size.\n\nAsynchronous Filter\n\nCustomers can choose the Asynchronous Filter as an extra option, providing a new streaming experience. In this case, content filters are run asynchronously, and completion content is returned immediately with a smooth token-by-token streaming experience. No content is buffered, which allows for a fast streaming experience with zero latency associated with content safety.\n\nCustomers must understand that while the feature improves latency, it's a trade-off against the safety and real-time vetting of smaller sections of model output. Because content filters are run asynchronously, content moderation messages and policy violation signals are delayed, which means some sections of harmful content that would otherwise have been filtered immediately could be displayed to the user.\n\nAnnotations: Annotations and content moderation messages are continuously returned during the stream. We strongly recommend you consume annotations in your app and implement other AI content safety mechanisms, such as redacting content or returning other safety information to the user.\n\nContent filtering signal: The content filtering error signal is delayed. If there is a policy violation, itâs returned as soon as itâs available, and the stream is stopped. The content filtering signal is guaranteed within a ~1,000-character window of the policy-violating content.\n\nCustomer Copyright Commitment: Content that is retroactively flagged as protected material may not be eligible for Customer Copyright Commitment coverage.\n\nTo enable Asynchronous Filter in Azure OpenAI Studio, follow the Content filter how-to guide to create a new content filtering configuration, and select Asynchronous Filter in the Streaming section.\n\nComparison of content filtering modes\n\nCompare Streaming - Default Streaming - Asynchronous Filter Status GA Public Preview Eligibility All customers Customers approved for modified content filtering How to enable Enabled by default, no action needed Customers approved for modified content filtering can configure it directly in Azure OpenAI Studio (as part of a content filtering configuration, applied at the deployment level) Modality and availability Text; all GPT models Text; all GPT models Streaming experience Content is buffered and returned in chunks Zero latency (no buffering, filters run asynchronously) Content filtering signal Immediate filtering signal Delayed filtering signal (in up to ~1,000-character increments) Content filtering configurations Supports default and any customer-defined filter setting (including optional models) Supports default and any customer-defined filter setting (including optional models)\n\nAnnotations and sample responses\n\nPrompt annotation message\n\nThis is the same as default annotations.\n\ndata: { \"id\": \"\", \"object\": \"\", \"created\": 0, \"model\": \"\", \"prompt_filter_results\": [ { \"prompt_index\": 0, \"content_filter_results\": { ... } } ], \"choices\": [], \"usage\": null }\n\nCompletion token message\n\nCompletion messages are forwarded immediately. No moderation is performed first, and no annotations are provided initially.\n\ndata: { \"id\": \"chatcmpl-7rAJvsS1QQCDuZYDDdQuMJVMV3x3N\", \"object\": \"chat.completion.chunk\", \"created\": 1692905411, \"model\": \"gpt-35-turbo\", \"choices\": [ { \"index\": 0, \"finish_reason\": null, \"delta\": { \"content\": \"Color\" } } ], \"usage\": null }\n\nAnnotation message\n\nThe text field will always be an empty string, indicating no new tokens. Annotations will only be relevant to already-sent tokens. There may be multiple annotation messages referring to the same tokens.\n\n\"start_offset\" and \"end_offset\" are low-granularity offsets in text (with 0 at beginning of prompt) to mark which text the annotation is relevant to.\n\n\"check_offset\" represents how much text has been fully moderated. It's an exclusive lower bound on the \"end_offset\" values of future annotations. It's non-decreasing.\n\ndata: { \"id\": \"\", \"object\": \"\", \"created\": 0, \"model\": \"\", \"choices\": [ { \"index\": 0, \"finish_reason\": null, \"content_filter_results\": { ... }, \"content_filter_raw\": [ ... ], \"content_filter_offsets\": { \"check_offset\": 44, \"start_offset\": 44, \"end_offset\": 198 } } ], \"usage\": null }\n\nSample response stream (passes filters)\n\nBelow is a real chat completion response using Asynchronous Filter. Note how the prompt annotations aren't changed, completion tokens are sent without annotations, and new annotation messages are sent without tokensâthey're instead associated with certain content filter offsets.\n\n{\"temperature\": 0, \"frequency_penalty\": 0, \"presence_penalty\": 1.0, \"top_p\": 1.0, \"max_tokens\": 800, \"messages\": [{\"role\": \"user\", \"content\": \"What is color?\"}], \"stream\": true}\n\ndata: {\"id\":\"\",\"object\":\"\",\"created\":0,\"model\":\"\",\"prompt_annotations\":[{\"prompt_index\":0,\"content_filter_results\":{\"hate\":{\"filtered\":false,\"severity\":\"safe\"},\"self_harm\":{\"filtered\":false,\"severity\":\"safe\"},\"sexual\":{\"filtered\":false,\"severity\":\"safe\"},\"violence\":{\"filtered\":false,\"severity\":\"safe\"}}}],\"choices\":[],\"usage\":null} data: {\"id\":\"chatcmpl-7rCNsVeZy0PGnX3H6jK8STps5nZUY\",\"object\":\"chat.completion.chunk\",\"created\":1692913344,\"model\":\"gpt-35-turbo\",\"choices\":[{\"index\":0,\"finish_reason\":null,\"delta\":{\"role\":\"assistant\"}}],\"usage\":null} data: {\"id\":\"chatcmpl-7rCNsVeZy0PGnX3H6jK8STps5nZUY\",\"object\":\"chat.completion.chunk\",\"created\":1692913344,\"model\":\"gpt-35-turbo\",\"choices\":[{\"index\":0,\"finish_reason\":null,\"delta\":{\"content\":\"Color\"}}],\"usage\":null} data: {\"id\":\"chatcmpl-7rCNsVeZy0PGnX3H6jK8STps5nZUY\",\"object\":\"chat.completion.chunk\",\"created\":1692913344,\"model\":\"gpt-35-turbo\",\"choices\":[{\"index\":0,\"finish_reason\":null,\"delta\":{\"content\":\" is\"}}],\"usage\":null} data: {\"id\":\"chatcmpl-7rCNsVeZy0PGnX3H6jK8STps5nZUY\",\"object\":\"chat.completion.chunk\",\"created\":1692913344,\"model\":\"gpt-35-turbo\",\"choices\":[{\"index\":0,\"finish_reason\":null,\"delta\":{\"content\":\" a\"}}],\"usage\":null} ... data: {\"id\":\"\",\"object\":\"\",\"created\":0,\"model\":\"\",\"choices\":[{\"index\":0,\"finish_reason\":null,\"content_filter_results\":{\"hate\":{\"filtered\":false,\"severity\":\"safe\"},\"self_harm\":{\"filtered\":false,\"severity\":\"safe\"},\"sexual\":{\"filtered\":false,\"severity\":\"safe\"},\"violence\":{\"filtered\":false,\"severity\":\"safe\"}},\"content_filter_offsets\":{\"check_offset\":44,\"start_offset\":44,\"end_offset\":198}}],\"usage\":null} ... data: {\"id\":\"chatcmpl-7rCNsVeZy0PGnX3H6jK8STps5nZUY\",\"object\":\"chat.completion.chunk\",\"created\":1692913344,\"model\":\"gpt-35-turbo\",\"choices\":[{\"index\":0,\"finish_reason\":\"stop\",\"delta\":{}}],\"usage\":null} data: {\"id\":\"\",\"object\":\"\",\"created\":0,\"model\":\"\",\"choices\":[{\"index\":0,\"finish_reason\":null,\"content_filter_results\":{\"hate\":{\"filtered\":false,\"severity\":\"safe\"},\"self_harm\":{\"filtered\":false,\"severity\":\"safe\"},\"sexual\":{\"filtered\":false,\"severity\":\"safe\"},\"violence\":{\"filtered\":false,\"severity\":\"safe\"}},\"content_filter_offsets\":{\"check_offset\":506,\"start_offset\":44,\"end_offset\":571}}],\"usage\":null} data: [DONE]\n\nSample response stream (blocked by filters)\n\n{\"temperature\": 0, \"frequency_penalty\": 0, \"presence_penalty\": 1.0, \"top_p\": 1.0, \"max_tokens\": 800, \"messages\": [{\"role\": \"user\", \"content\": \"Tell me the lyrics to \\\"Hey Jude\\\".\"}], \"stream\": true}\n\ndata: {\"id\":\"\",\"object\":\"\",\"created\":0,\"model\":\"\",\"prompt_filter_results\":[{\"prompt_index\":0,\"content_filter_results\":{\"hate\":{\"filtered\":false,\"severity\":\"safe\"},\"self_harm\":{\"filtered\":false,\"severity\":\"safe\"},\"sexual\":{\"filtered\":false,\"severity\":\"safe\"},\"violence\":{\"filtered\":false,\"severity\":\"safe\"}}}],\"choices\":[],\"usage\":null} data: {\"id\":\"chatcmpl-8JCbt5d4luUIhYCI7YH4dQK7hnHx2\",\"object\":\"chat.completion.chunk\",\"created\":1699587397,\"model\":\"gpt-35-turbo\",\"choices\":[{\"index\":0,\"finish_reason\":null,\"delta\":{\"role\":\"assistant\"}}],\"usage\":null} data: {\"id\":\"chatcmpl-8JCbt5d4luUIhYCI7YH4dQK7hnHx2\",\"object\":\"chat.completion.chunk\",\"created\":1699587397,\"model\":\"gpt-35-turbo\",\"choices\":[{\"index\":0,\"finish_reason\":null,\"delta\":{\"content\":\"Hey\"}}],\"usage\":null} data: {\"id\":\"chatcmpl-8JCbt5d4luUIhYCI7YH4dQK7hnHx2\",\"object\":\"chat.completion.chunk\",\"created\":1699587397,\"model\":\"gpt-35-turbo\",\"choices\":[{\"index\":0,\"finish_reason\":null,\"delta\":{\"content\":\" Jude\"}}],\"usage\":null} data: {\"id\":\"chatcmpl-8JCbt5d4luUIhYCI7YH4dQK7hnHx2\",\"object\":\"chat.completion.chunk\",\"created\":1699587397,\"model\":\"gpt-35-turbo\",\"choices\":[{\"index\":0,\"finish_reason\":null,\"delta\":{\"content\":\",\"}}],\"usage\":null} ... data: {\"id\":\"chatcmpl-8JCbt5d4luUIhYCI7YH4dQK7hnHx2\",\"object\":\"chat.completion.chunk\",\"created\":1699587397,\"model\":\"gpt-35- turbo\",\"choices\":[{\"index\":0,\"finish_reason\":null,\"delta\":{\"content\":\" better\"}}],\"usage\":null} data: {\"id\":\"\",\"object\":\"\",\"created\":0,\"model\":\"\",\"choices\":[{\"index\":0,\"finish_reason\":null,\"content_filter_results\":{\"hate\":{\"filtered\":false,\"severity\":\"safe\"},\"self_harm\":{\"filtered\":false,\"severity\":\"safe\"},\"sexual\":{\"filtered\":false,\"severity\":\"safe\"},\"violence\":{\"filtered\":false,\"severity\":\"safe\"}},\"content_filter_offsets\":{\"check_offset\":65,\"start_offset\":65,\"end_offset\":1056}}],\"usage\":null} data: {\"id\":\"\",\"object\":\"\",\"created\":0,\"model\":\"\",\"choices\":[{\"index\":0,\"finish_reason\":\"content_filter\",\"content_filter_results\":{\"protected_material_text\":{\"detected\":true,\"filtered\":true}},\"content_filter_offsets\":{\"check_offset\":65,\"start_offset\":65,\"end_offset\":1056}}],\"usage\":null} data: [DONE]\n\nBest practices\n\nAs part of your application design, consider the following best practices to deliver a positive experience with your application while minimizing potential harms:\n\nDecide how you want to handle scenarios where your users send prompts containing content that is classified at a filtered category and severity level or otherwise misuse your application.\n\nCheck the finish_reason to see if a completion is filtered.\n\nCheck that there's no error object in the content_filter_result (indicating that content filters didn't run).\n\nIf you're using the protected material code model in annotate mode, display the citation URL when you're displaying the code in your application.\n\nNext steps"
    }
}
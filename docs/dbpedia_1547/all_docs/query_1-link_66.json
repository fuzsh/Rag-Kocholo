{
    "id": "dbpedia_1547_1",
    "rank": 66,
    "data": {
        "url": "https://www.oreilly.com/library/view/data-governance-the/9781492063483/ch01.html",
        "read_more_link": "",
        "language": "en",
        "title": "Data Governance: The Definitive Guide [Book]",
        "top_image": "https://www.oreilly.com/library/cover/9781492063483/1200w630h/",
        "meta_img": "https://www.oreilly.com/library/cover/9781492063483/1200w630h/",
        "images": [
            "https://cdn.oreillystatic.com/images/sitewide-headers/oreilly_logo_mark_red.svg",
            "https://www.oreilly.com/api/v2/epubs/9781492063483/files/assets/DGDG_0101.png",
            "https://www.oreilly.com/api/v2/epubs/9781492063483/files/assets/DGDG_0102.png",
            "https://www.oreilly.com/api/v2/epubs/9781492063483/files/assets/DGDG_0103.png",
            "https://www.oreilly.com/api/v2/epubs/9781492063483/files/assets/DGDG_0104.png",
            "https://www.oreilly.com/api/v2/epubs/9781492063483/files/assets/DGDG_0105.png",
            "https://www.oreilly.com/api/v2/epubs/9781492063483/files/assets/DGDG_0106.png",
            "https://www.oreilly.com/api/v2/epubs/9781492063483/files/assets/DGDG_0107.png",
            "https://www.oreilly.com/api/v2/epubs/9781492063483/files/assets/DGDG_0108.png",
            "https://www.oreilly.com/api/v2/epubs/9781492063483/files/assets/DGDG_0109.png",
            "https://www.oreilly.com/api/v2/epubs/9781492063483/files/assets/DGDG_0110.png",
            "https://www.oreilly.com/api/v2/epubs/9781492063483/files/assets/DGDG_0111.png",
            "https://www.oreilly.com/api/v2/epubs/9781492063483/files/assets/DGDG_0112.png",
            "https://www.oreilly.com/api/v2/epubs/9781492063483/files/assets/DGDG_0113.png",
            "https://www.oreilly.com/api/v2/epubs/9781492063483/files/assets/DGDG_0114.png",
            "https://cdn.oreillystatic.com/oreilly/images/app-store-logo.png",
            "https://cdn.oreillystatic.com/oreilly/images/google-play-logo.png",
            "https://cdn.oreillystatic.com/oreilly/images/roku-tv-logo.png",
            "https://cdn.oreillystatic.com/oreilly/images/amazon-appstore-logo.png",
            "https://cdn.oreillystatic.com/images/sitewide-headers/oreilly_logo_mark_red.svg",
            "https://cdn.oreillystatic.com/oreilly/images/report-software-architecture-patterns-553x420.jpg",
            "https://cdn.oreillystatic.com/oreilly/images/laptop-flat-topics-ml-1124x638.png"
        ],
        "movies": [],
        "keywords": [],
        "meta_keywords": [
            ""
        ],
        "tags": null,
        "authors": [
            "Dom Zippilli",
            "Evren Eryurek",
            "Uri Gilad",
            "Valliappa Lakshmanan",
            "Anita Kibunguchy-Grant",
            "Jessi Ashdown"
        ],
        "publish_date": null,
        "summary": "",
        "meta_description": "Chapter 1. What Is Data Governance? Data governance is, first and foremost, a data management function to ensure the quality, integrity, security, and usability of the data collected by an …  - Selection from Data Governance: The Definitive Guide [Book]",
        "meta_lang": "en",
        "meta_favicon": "//www.oreilly.com/favicon.ico",
        "meta_site_name": "O’Reilly Online Learning",
        "canonical_link": "https://www.oreilly.com/library/view/data-governance-the/9781492063483/ch01.html",
        "text": "Spotify Creates Discover Weekly\n\nAs an example of how well-governed data can lead to measurable benefits to an organization and how the availability of data can completely change an entire industry, consider the Spotify Discover Weekly feature. In the early 2010s, the way most people listened to music was to purchase physical/digital albums and rip them to create custom playlists. These playlists, consisting of songs you owned, was what you listened to.\n\nThere was also a large and thriving illegal music-sharing ecosystem that consisted of pirated singles that could be added to your playlists. In an effort to get the pirated music system under control, music labels allowed the sales of digital singles. As the size of people’s digital libraries grew, and as internet connections became more reliable, consumers became willing to keep their purchased tracks online and stream them to their audio devices. Music labels were also willing to “rent” out music when it was streamed. Instead of selling the song, the music labels would be paid each time the song was played.\n\nThis was how Spotify (which is now the world’s largest music streaming service) got started. It’s worth noting that Spotify owes its very existence to data governance. It got started as a way for music labels to get paid for their work—music piracy was decimating the music industry. Spotify’s entire business model was built around tracking the songs users played and reimbursing artists for those songs. The ability to prove that its handling of the data was trustworthy is the reason that Spotify became a viable music service in the first place.\n\nThe fact that Spotify was keeping tabs on which songs users played meant that it had data on what people listened to. Thus it was now possible for Spotify to recommend new songs to its listeners. Such recommendation algorithms key off of three things:\n\nFind other songs by the artists you listen to, or songs in the same genre (e.g., 1940s jazz). This is called content-based recommendation.\n\nFind users who like the same songs you do and recommend the songs that those users like. This is called collaborative filtering.\n\nUse models that analyze the raw audio files of songs you like and recommend songs that are similar. The raw audio captures many inherent features, such as the beat. If you tend to like music with a fast beat and with repeating tonal phrases, the algorithm can recommend other songs with a similar structure. This is called similarity matching.\n\nAt that point, Edward Newett, an engineer at Spotify, had an interesting idea: instead of recommending songs one at a time, what if Spotify created a playlist of recommendations? And so, every Monday, Spotify would recommend what they thought each individual user would like. This was called Discover Weekly.\n\nDiscover Weekly was a huge hit—within a year after its launch, more than 40 million people had used the service and streamed nearly five billion tracks. The deep personalization had worked. The music sounded familiar but was still novel. The service allowed music lovers to discover new titles, and new artists to find audiences, and it gave Spotify’s customers an event to look forward to every week.\n\nSpotify was able to use its recommendation algorithm to provide artists and music labels with insights about fans’ preferences. It could leverage the recommendation algorithm to expand users’ music preferences and introduce novel bands. This extra knowledge and marketing ability has allowed Spotify to negotiate with music publishers from a position of strength.\n\nNone of this would have been possible if Spotify had not assured its users that the information about their listening habits was being used in a responsible way to improve their own music-listening experience. European regulators are very protective of the privacy of EU citizens. Spotify, being based in Europe, could not have gotten its recommendation systems off the ground if it had not proven that it had robust privacy controls in place, and that it was ensuring that data scientists could devise algorithms but not breach data that could be tied to individuals.\n\nDiscover Weekly illustrates how data, properly governed, can create a well-loved brand and change the power dynamic in an entire industry. Spotify extended its recommendations with Spotify Wrapped, where listeners everywhere get a deep dive into their most memorable listening moments of the year. This is a great way to have people remember and share their most listened-to songs and artists (see Figure 1-1).\n\nHolistic Approach to Data Governance\n\nSeveral years ago, when smartphones with GPS sensors were becoming ubiquitous, one of the authors of this book was working on machine learning algorithms to predict the occurence of hail. Machine learning requires labeled data—something that was in short supply at the temporal and spatial resolution the research team needed. Our team hit on the idea of creating a mobile application that would allow citizen scientists to report hail at their location.2 This was our first encounter with making choices about what data to collect—until then, we had mostly been at the receiving end of whatever data the National Weather Service was collecting. Considering the rudimentary state of information security tools in an academic setting, we decided to forego all personally identifying information and make the reporting totally anonymous, even though this meant that certain types of reported information became somewhat unreliable. Even this anonymous data brought tremendous benefits—we started to evaluate hail algorithms at greater resolutions, and this improved the quality of our forecasts. This new dataset allowed us to calibrate existing datasets, thus enhancing the data quality of other datasets as well. The benefits went beyond data quality and started to accrue toward trustworthiness—involvement of citizen scientists was novel enough that National Public Radio carried a story about the project, emphasizing the anonymous nature of the data collection.3 The data governance lens had allowed us to carefully think about which report data to collect, improve the quality of enterprise data, enhance the quality of forecasts produced by the National Weather Service, and even contribute to the overall brand of our weather enterprise. This combination of effects—regulatory compliance, better data quality, new business opportunities, and enhanced trustworthiness—was the result of a holistic approach to data governance.\n\nFast-forward a few years, and now, at Google Cloud, we are all part of a team that builds technology for scalable cloud data warehouses and data lakes. One of the recurring concerns that our enterprise customers have is around what best practices and policies they should put in place to manage the classification, discovery, availability, accessibility, integrity, and security of their data—data governance—and customers approach it with the same sort of apprehension that our small team in academia did.\n\nYet the tools and capabilities that an enterprise has at its disposal to carry out data governance are quite powerful and diverse. We hope to convince you that you should not be afraid of data governance, and that properly applying data governance can open up new worlds of possibility. While you might initially approach data governance purely from a legal or regulatory compliance standpoint, applying governance policies can drive growth and lower costs.\n\nClassification and Access Control\n\nWhile the purpose of data governance is to increase the trustworthiness of enterprise data so as to derive business benefits, it remains the case that the primary activity associated with data governance involves classification and access control. Therefore, to understand the roles involved in data governance, it is helpful to consider a typical classification and access control setup.\n\nLet’s take the case of protecting the human resources information of employees, as shown in Figure 1-3.\n\nThe human resources information includes several data elements: each employee’s name, their date of hire, past salary payments, the bank account into which those salary payments were deposited, current salary, etc. Each of these data elements is protected in different ways, depending on the classification level. Potential classification levels might be public (things accessible by people not associated with the enterprise), external (things accessible by partners and vendors with authorized access to the enterprise internal systems), internal (things accessible by any employee of the organization), and restricted. For example, information about each employee’s salary payments and which bank account they were deposited into would be restricted to managers in the payroll processing group only. On the other hand, the restrictions could be more dynamic. An employee’s current salary might be visible only to their manager, and each manager might be able to see salary information only for their respective reports. The access control policy would specify what users can do when they access the data—whether they can create a new record, or read, update, or delete existing records.\n\nThe governance policy is typically specified by the group that is accountable for the data (here, the human resources department)—this group is often referred to as the governors. The policy itself might be implemented by the team that operates the database system or application (here, the information technology department), and so changes such as adding users to permitted groups are often carried out by the IT team—hence, members of that team are often referred to as approvers or data stewards. The people whose actions are being circumscribed or enabled by data governance are often referred to as users. In businesses where not all employees have access to enterprise data, the set of employees with access might be called knowledge workers to differentiate them from those without access..\n\nSome enterprises default to open—for example, when it comes to business data, the domain of authorized users may involve all knowledge workers in the enterprise. Other enterprises default to closed—business data may be available only to those with a need to know. Policies such as these are within the purview of the data governance board in the organization—there is no uniquely correct answer on which approach is best.\n\nManaging Discoverability, Security, and Accountability\n\nIn July 2019, Capital One, one of the largest issuers of consumer and small business credit cards, discovered that an outsider had been able to take advantage of a misconfigured web application firewall in its Apache web server. The attacker was able to obtain temporary credentials and access files containing personal information for Capital One customers.11 The resulting leak of information affected more than 100 million individuals who had applied for Capital One credit cards.\n\nTwo aspects of this leak limited the blast radius. First, the leak was of application data sent to Capital One, and so, while the information included names, social security numbers, bank account numbers, and addresses, it did not include log-in credentials that would have allowed the attacker to steal money. Second, the attacker was swiftly caught by the FBI, and the reason for the attacker being caught is why we include this anecdote in this book.\n\nBecause the files in question were stored in a public cloud storage bucket where every access to the files was logged, access logs were available to investigators after the fact. They were able to figure out the IP routes and narrow down the source of the attack to a few houses. While misconfigured IT systems that create security vulnerabilities can happen anywhere, attackers who steal admin credentials from on-premises systems will usually cover their tracks by modifying the system access logs. On the public cloud, though, these access logs are not modifiable because the attacker doesn’t have access to them.\n\nThis incident highlights a handful of lessons:\n\nMake sure that your data collection is purposeful. In addition, store as narrow a slice of the data as possible. It was fortunate that the data store of credit card applications did not also include the details of the resulting credit card accounts.\n\nTurn on organizational-level audit logs in your data warehouse. Had this not been done, it would not have been possible to catch the culprit.\n\nConduct periodic security audits of all open ports. If this is not done, no alerts will be raised about attempts to get past security safeguards.\n\nApply an additional layer of security to sensitive data within documents. Social security numbers, for example, should have been masked or tokenized using an artificial intelligence service capable of identifying PII data and redacting it.\n\nThe fourth best practice is an additional safeguard—arguably, if only absolutely necessary data is collected and stored, there would be no need for masking. However, most organizations have multiple uses of the data, and in some use cases, the decrypted social security number might be needed. In order to do such multi-use effectively, it is necessary to tag or label each attribute based on multiple categories to ensure the appropriate controls and security are placed on it. This tends to be a collaborative effort among many organizations within the company. It is worth noting that systems like these that remove data from consideration come with their own challenges and risks.12\n\nAs the data collected and retained by enterprises has grown, ensuring that best practices like these are well understood and implemented correctly has become more and more important. Such best practices and the policies and tools to implement them are at the heart of data governance.\n\nImproving Data Quality\n\nData governance is not just about security breaches. For data to be useful to an organization, it is necessary that the data be trustworthy. The quality of data matters, and much of data governance focuses on ensuring that the integrity of data can be trusted by downstream applications. This is especially hard when data is not owned by your organization and when that data is moving around.\n\nA good example of data governance activities improving data quality comes from the US Coast Guard (USCG). The USCG focuses on maritime search and rescue, ocean spill cleanup, maritime safety, and law enforcement. Our colleague Dom Zippilli was part of the team that proved the data governance concepts and techniques behind what became known as the Authoritative Vessel Identification Service (AVIS). The following sidebar about AVIS is in his words.\n\nThe USCG program is a handy reminder that data quality is something to strive for and constantly be on the watch for. The cleaner the data, the more likely it is to be usable for more critical use cases. In the USCG case, we see this in the usability of the data for search and rescue tasks as well .\n\nFostering Innovation\n\nA good data governance strategy, when set in motion, combines several factors that allow a business to extract more value from the data. Whether the goal is to improve operations, find additional sources of revenue, or even monetize data directly, a data governance strategy is an enabler of various value drivers in enterprises.\n\nA data governance strategy, if working well, is a combination of process (to make data available under governance), people (who manage policies and usher in data access across the organization, breaking silos where needed), and tools that facilitate the above by applying machine learning techniques to categorize data and indexing the data available for discovery.\n\nData governance ideally will allow all employees in the organization to access all data (subject to a governance process) under a set of governance rules (defined in greater detail below), while preserving the organization’s risk posture (i.e., no additional exposure or risks are introduced due to making data accessible under a governance strategy). Since the risk posture is maintained and possibly even improved with the additional controls data governance brings, one could argue there is only an upside to making data accessible. Giving all knowledge workers access to data, in a governed manner, can foster innovation by allowing individuals to rapidly prototype answers to questions based on the data that exists within the organization. This can lead to better decision making, better opportunity discovery, and a more productive organization overall.\n\nThe quality of the data available is another way to ascertain whether governance is well implemented in the organization. A part of data governance is a well-understood way to codify and inherit a “quality signal” on the data. This signal should tell potential data users and analysts whether the data was curated, whether it was normalized or missing, whether corrupt data was removed, and potentially how trustworthy the source for the data is. Quality signals are crucial when making decisions on potential uses of the data; for example, within machine learning training datasets.\n\nThe Tension Between Data Governance and Democratizing Data Analysis\n\nVery often, complete data democratization is thought of as conflicting with data governance. This conflict is not necessarily an axiom. Data democratization, in its most extreme interpretation, can mean that all analysts or knowledge workers can access all data, whatever class it may belong to. The access described here makes a modern organization uncomfortable when you consider specific examples, such as employee data (e.g., salaries) and customer data (e.g., customer names and addresses). Clearly, only specific people should be able to access data of the aforementioned types, and they should do so only within their specific job-related responsibilities.\n\nData governance is actually an enabler here, solving this tension. The key concept to keep in mind is that there are two layers to the data: the data itself (e.g., salaries) and the metadata (data about the data—e.g., “I have a table that contains salaries, but I won’t tell you anything further”).\n\nWith data governance, you can accomplish three things:\n\nAccess a metadata catalog, which includes an index of all the data managed (full democratization, in a way) and allows you to search for the existence of certain data. A good data catalog also includes certain access control rules that limit the bounds of the search (for example, I will be able to search “sales-related data,” but “HR” is out of my purview completely, and therefore even HR-metadata is inaccessible to me).\n\nGovern access to the data, which includes an acquisition process (described above) and a way to adhere to the principle of least access: once access is requested, provide access limited to the boundaries of the specific resource; don’t overshare.\n\nIndependently of the other steps, make an “audit trail” available to the data access request, the data access approval cycle, and the approver (data steward), as well as to all the subsequent access operations. This audit trail is data itself and therefore must comply with data governance.\n\nIn a way, data governance becomes the facility where you can enable data democratization, allowing more of your data to be accessible to more of the knowledge employee population, and therefore be an accelerator to the business in making the use of data easier and faster.\n\nBusiness outcomes, such as visibility into all parts of a supply chain, understanding of customer behavior on every online asset, tracking the success of a multipronged campaign, and the resulting customer journeys, are becoming more and more possible. Under governance, different business units will be able to pull data together, analyze it to achieve deeper insight, and react quickly to both local and global changes.\n\nLocation\n\nData locality is mostly relevant for global organizations that store and use data across the globe, but a deeper look into regulation reveals that the situation is not so simple. For example, if, for business reasons, you want to leverage a data center in a central location (say, in the US, next to your potential customers) but your company is a German company, regulation requires that data about employees remains on German soil; thus your data strategy just became more involved.\n\nThe need to store user data within sovereign boundaries is an increasingly common regulatory requirement. In 2016, the EU Parliament approved data sovereignty measures within GDPR, wherein the storage and processing of records about EU citizens and residents must be carried out in a manner that follows EU law. Specific classes of data (e.g., health records in Australia, telecommunications metadata in Germany, or payment data in India) may also be subject to data locality regulations; these go beyond mere sovereignty measures by requiring that all data processing and storage occur within the national boundaries. The major public cloud providers offer the ability to store your data in accordance with these regulations. It can be convenient to simply mark a dataset as being within the EU multi-region and know that you have both redundancy (because it’s a multi-region) and compliance (because data never leaves the EU). Implementing such a solution in your on-premises data center can be quite difficult, since it can be cost-prohibitive to build data centers in every sovereign location that you wish to do business in and that has locality regulations.\n\nAnother reason that location matters is that secure transaction-aware global access matters. As your customers travel or locate their own operations, they will require you to provide access to data and applications wherever they are. This can be difficult if your regulatory compliance begins and ends with colocating applications and data in regional silos. You need the ability to seamlessly apply compliance roles based on users, not just on applications. Running your applications in a public cloud that runs its own private fiber and offers end-to-end physical network security and global time synchronization (not all clouds do this) simplifies the architecture of your applications.\n\nEphemeral Compute\n\nIn order to have a single source of data and still be able to support enterprise applications, current and future, we need to make sure that the data is not stored within a compute cluster, or scaled in proportion to it. If our business is spiky, or if we require the ability to support interactive or occasional workloads, we will require infinitely scalable and readily burstable compute capability that is separate from storage architecture. This is possible only if our data processing and analytics architecture is serverless and/or clearly separates computes and storage.\n\nWhy do we need both data processing and analytics to be serverless? Because the utility of data is often realized only after a series of preparation, cleanup, and intelligence tools are applied to it. All these tools need to support separation of compute and storage and autoscaling in order to realize the benefits of a serverless analytics platform. It is not sufficient just to have a serverless data warehouse or application architecture that is built around serverless functions. You need your tooling frameworks themselves to be serverless. This is available only in the cloud.\n\nSecurity in a Hybrid World\n\nThe last point about having consistent policies that are easily applicable is key. Consistency and a single security pane are key benefits to hosting your enterprise software infrastructure on the cloud. However, such an all-or-nothing approach is unrealistic for most enterprises. If your business operates equipment (handheld devices, video cameras, point-of-sale registers, etc.) “on the edge,” it is often necessary to have some of your software infrastructure there as well. Sometimes, as with voting machines, regulatory compliance might require physical control of the equipment being used. Your legacy systems may not be ready to take advantage of the separation of compute and storage that the cloud offers. In these cases, you’d like to continue to operate on-premises. Systems that involve components that live in a public cloud and one other place—in two public clouds, or in a public cloud and on the edge, or a in public cloud and on-premises—are termed hybrid cloud systems.\n\nIt is possible to greatly expand the purview of your cloud security posture and policies by employing solutions that allow you to control both on-premises and cloud infrastructure using the same tooling. For example, if you have audited an on-premises application and its use of data, it is easier to approve that identical application running in the cloud than it is to reaudit a rewritten application. The cost of entry to this capability is to containerize your applications, and this might be a cost well worth paying for, for the governance benefits alone."
    }
}
{
    "id": "dbpedia_1547_1",
    "rank": 46,
    "data": {
        "url": "https://www.linkedin.com/posts/rahim-baig_generatieveai-llms-chatgpt-activity-7166028529506140160-utwm",
        "read_more_link": "",
        "language": "en",
        "title": "Mirza Rahim Baig on LinkedIn: #generatieveai #llms #chatgpt #artificialintelligence #rag",
        "top_image": "https://media.licdn.com/dms/image/v2/D4D22AQGexzG9Ncs4lg/feedshare-shrink_800/feedshare-shrink_800/0/1708514338476?e=2147483647&v=beta&t=pRRvmS7h4d-hoBu9wskco2zKEtbSlaZogjFqzYfWbsU",
        "meta_img": "https://media.licdn.com/dms/image/v2/D4D22AQGexzG9Ncs4lg/feedshare-shrink_800/feedshare-shrink_800/0/1708514338476?e=2147483647&v=beta&t=pRRvmS7h4d-hoBu9wskco2zKEtbSlaZogjFqzYfWbsU",
        "images": [
            "https://media.licdn.com/dms/image/v2/D4E16AQGTDs9rO8_eNQ/profile-displaybackgroundimage-shrink_200_800/profile-displaybackgroundimage-shrink_200_800/0/1709722990872?e=2147483647&v=beta&t=S75PCXsQecGllgmBhN8r7xyC4eKmfNIE_Bvpantp4as"
        ],
        "movies": [],
        "keywords": [],
        "meta_keywords": [
            ""
        ],
        "tags": null,
        "authors": [
            "Mirza Rahim Baig"
        ],
        "publish_date": "2024-02-21T11:18:59.779000+00:00",
        "summary": "",
        "meta_description": "ğ—¦ğ—¼ğ—ºğ—² ğ—¹ğ—²ğ˜€ğ˜€ğ—¼ğ—»ğ˜€ ğ—¹ğ—²ğ—®ğ—¿ğ—»ğ˜ ğ—³ğ—¿ğ—¼ğ—º ğ—ºğ˜† ğ—¼ğ˜„ğ—» ğ—½ğ—²ğ˜ ğ—½ğ—¿ğ—¼ğ—·ğ—²ğ—°ğ˜ ğ˜‚ğ˜€ğ—¶ğ—»ğ—´ ğ—Ÿğ—Ÿğ— ğ˜€ ğ—®ğ—»ğ—± ğ—® ğ—¥ğ—”ğ—š. ğŸ‘¨â€ğŸ“ \n\nThere are multiple choices to beâ€¦",
        "meta_lang": "en",
        "meta_favicon": "https://static.licdn.com/aero-v1/sc/h/al2o9zrvru7aqj8e1x2rzsrca",
        "meta_site_name": "",
        "canonical_link": "https://www.linkedin.com/posts/rahim-baig_generatieveai-llms-chatgpt-activity-7166028529506140160-utwm",
        "text": "5 Step-by-Step Suggestions for Adopting LLMs in Your Company (to get the best results while saving yourself headaches) ğŸ‘‰ 1. Prompt engineering: Keep it simple. Try your best to craft effective prompts. Prompts can be very powerful, and if they don't give you the desired result, try the second approach. But always start with prompt engineering first. Prompts are more powerful than you think. Don't believe me? Try googling the term \"Reverse Prompt Engineering\"! ğŸ‘‰ 2. RAG: Use the RAG approach. It basically means connecting your database to LLM. Most of the time, RAG and quality prompt engineering are all you need. However, I have seen many people using RAG from boilerplate code or common indexing libraries. Beware, if you do that, you may get poor results. Try to understand your data and build a sophisticated multi-stage vector search pipeline for that. You can easily spend a week on this problem alone. There is lots of room here for squeezing the best out of your database. ğŸ‘‰ 3. Fine-tuning: The last step is fine-tuning. If RAG doesn't give you the result you are looking for, fine-tune an LLM model. This is a bit costly, but you can still use RAG and quality prompt engineering on top of your fine-tuned model. Now, you have to think of hosting your model by yourself. Management overhead is free as well. It could be quite costly. That's why I told you to spend more time on steps one & two! Only come to step three after leaving no stone unturned. ğŸ‘‰ 4. Train one: If all three approaches are not giving you the best result, you may be trying to do too much with an LLM. LLMs may not be the best solution for your specific use case. In this case, try training a specific model for yourself from your data. Remember, this was the approach we used before LLMs were available. Break the task down. Train or find a specific model for each use case. One AI model to do everything is not the best approach. It's better to use multiple smaller models to achieve your result. ğŸ‘‰ 5. Keep Evaluating: If all these 4 steps fail to provide you with the best result, wait for Sam Altman and Zuck to do wonders again! ğŸ˜‚ Just kidding. Stay with the imperfect solution. It's better to have a weak AI than no AI at all. But, most of the time, if you don't get the best result, you can improve your solution by revisiting the previous steps. Keep evaluating your approaches and looking for ways to improve. #llm #llmops #rag #mlops\n\nCapturing the efficiency gains of LLMâ€™s starts with and is centered on your ability to build and implement prompt based workflowsâ€¦ Simply put, you need to build up your experience around using individual prompts as building blocks to construct the larger effects which you then orchestrate into complete workflows to produce high quality outputs. You do not need private models, specially fine tuned models, or super secure 3rd party services to do any of this type of development for your organization. You could literally have an unpaid college intern do this for you over a few weeks or months with nothing more than a $20/month ChatGPT or Claude account. But what does this development look like? The point is to find the prompts, test the effects of combination, and evaluate the combined workflows. It starts with dissecting your target business process into its fundamental action steps. Building out, testing, and refining individual prompts around each of these steps. Combining these prompts together into ever more complex structures, measuring effectiveness, and revising along the way. Ultimately, the task is complete when a minimum quality threshold is met by a sufficient output from a total workflow of all the afore-mentioned pieces. Remember this is all done in just plain English, so data analysis backgrounds help, but basic grammar fluency and many reps with the tools is the most beneficial. Hence a person more apt to wilding enjoying crossword puzzles would be a better choice than a data scientist for this task. Remember as unique as your businessâ€™s products or services may truly be, all your normal internal business processes are not. These boring but necessary processes are the perfect kind of tasks to start with, with many examples and easily understandable components for anyone to try automating with LLMs. They also carry the benefit of wide horizontal integration in your business when you achieve them. The point being to start somewhere, do the development as cheaply and efficiently as possible, then scale up and integrate your successes. This is the point where you might want to look at all those other model and tool options if they are applicable to additional scaling of efficiency gains to your successful workflows. This is what GenAI integration looks like for your business in the most cost efficient and business advantaged way. Go buy off the shelf products if you want, but learning and implementing your own solutions is the key to long term success. If you need help doing or setting up this kind of work, I also offer training and consulting services. â€œKnowing is Half the Battleâ€ ğŸ§  For more Insights, Resources, and Services Go to my website: https://lnkd.in/g8CFG8Xd\n\nğŸ¥ Check out my comprehensive guide on the most popular tools and frameworks for building LLM applications. ğŸ‘‰ The landscape for building LLM applications is rich with a variety of tools and technologies, each serving different needs and stages of the process. ğŸ‘‰Finding and picking the right tools and frameworks for your LLM app is key and takes time. Even if you're just starting out, knowing what's out there and how it all works together is super important! â›³ To simplify your decision process, I've compiled a detailed guide to help you navigate the large pool of options available for LLM application development. Link to the guide: https://lnkd.in/es4_SHra ğŸ”° LLM tools can be broadly classified into four main categories: â›³ Input Processing Tools: These tools are designed to handle data ingestion and prepare various inputs for your application. They include data pipelines and vector databases that are crucial for processing and preparing data for the LLM. â›³ LLM Development Tools: This category encompasses tools that aid in interacting with Large Language Models. This includes services for calling LLMs, fine-tuning them, conducting experiments, and managing orchestration. Examples include LLM providers, orchestration platforms, and computing and experimentation platforms. â›³ Output Tools: Post-processing tools that manage and refine the output from the LLM application fall into this category. They focus on processes after the LLM has generated its output, such as evaluation frameworks that assess the quality and relevance of the output. â›³ Application Tools: These are tools that manage all aspects of the LLM application, including hosting, monitoring etc. ğŸ¯ This guide will provide deeper insights into these types of tools, their various options, along with their advantages and disadvantages, giving you a comprehensive view of what's available for application building and how to best utilize these resources. ğŸ›‘ Please note that this guide is not comprehensive by any means, it is only supposed to give you an overview of the popular tools! In addition to categorizing these tools, I've differentiated between tools necessary for RAG versus those needed for fine-tuning LLMs. The guide has been linked below ğŸš¨ I share #genai content daily, follow along for the latest updates! #genai #freeresources #llms\n\nâ„¹ï¸ Prompt types in LLMs If you worked with any LLM, you have used prompt engineering to get the expected results. I was reading a paper {Language Models for Data Annotation.} [0] and noticed they had categorised the prompts into 5 techniques: â¡ï¸ Input-Output Prompting (IO) This is the fundamental way of interacting with an LLM, where a prompt is given to the model to get a specific output. Itâ€™s like asking a question and getting an answer. â¡ï¸ In-Context Learning (ICL) This method enhances IO by adding examples or demonstrations to the prompt, showing the LLM the expected answers. Itâ€™s like giving the model a hint by showing similar problems and solutions. â¡ï¸ Chain-of-Thought Prompting (CoT) This goes a step further by providing examples and a reasoning pathway that leads to the answer. It helps the model understand the answer and how to think about the problem. Example: Prompt to LLM: \"Here is a customer review: 'The product design is innovative, but it stopped working after two days, and customer service was unhelpful.' Break down your analysis step-by-step to determine if the sentiment is positive, neutral, or negative.\" Expected Chain of Thought from LLM: - \"The customer appreciates the product design, which indicates a positive aspect.\" - \"However, the product stopped working after two days, which is a significant negative point.\" - \"Additionally, the customer service was unhelpful, adding another negative aspect.\" - \"Despite the initial positive comment, the two negative points outweigh it.\" - \"Conclusion: The overall sentiment of the review is negative.\" Final Annotation Output: \"Negative\" â¡ï¸ Instruction Tuning (IT) This involves fine-tuning LLMs with specific instructions, making them better at following directions and generalizing across different tasks. Itâ€™s like teaching the model to understand and follow instructions more effectively. â¡ï¸ Alignment Tuning (AT) This fine-tunes LLMs to align their outputs with human preferences using human-labeled data and LLM-generated annotations. This helps align the modelâ€™s responses with what humans would expect or prefer. [0] Link to the paper https://lnkd.in/eYbyPsDX\n\nHere is a Checklist to consider when deploying an LLM Model. ğŸ“ Before Deployment Data Collection âœ… Dataset: Needs to be diverse and comprehensive and relevant to the LLM use case. âœ… Ensure data quality and representation (see previous posts). Pre-training âœ… Train the LLM on the collected data using unsupervised learning. âœ… Monitor the training process with a validation watch for over-fitting. âœ… Adjust hyperparameters as needed for optimal performance. Task-Specific Data Collection âœ… Collect labeled data specific to the tasks the LLM will perform (make sure this aligns with the use cases). Supervised Fine-Tuning âœ… Set up performance metrics. âœ… Fine-tune the pre-trained LLM on task-specific data. âœ… Evaluate and refine based on performance metrics. Evaluation âœ… Assess the model on a test set to ensure task-specific performance. âœ… Loop back to data collection or fine-tuning if results are unsatisfactory. Knowledge Base Preparation âœ… Compile a knowledge base for the RAG system to query during inference. Indexing âœ… Create embeddings for documents in the knowledge base and store them in a vector database. RAG System Setup âœ… Integrate the retrieval component with the LLM to enhance responses. Infrastructure Setup âœ… Ensure adequate computational resources and storage for hosting the model. Containerisation âœ… Package the model and its dependencies using tools like Docker for consistent deployment. API Development âœ… Develop APIs for interaction with the model, handling input/output processing efficiently. ğŸ“ After Deployment Performance Monitoring âœ… Continuously monitor model performance using metrics and real-time tools. âœ… Loop back to fine-tuning or RAG system setup if performance drops. Logging and Error Tracking âœ… Implement logging and error tracking to diagnose and resolve issues quickly. âœ… Redeploy the model after fixing significant errors. User Feedback Integration âœ… Collect and analyze user feedback to identify areas for improvement. âœ… Refine the model based on feedback. Regular Updates âœ… Periodically retrain or fine-tune the model with new data to keep it relevant. âœ… Revisit data collection, RAG system setup, or fine-tuning for continuous updates. Scalability Enhancements âœ… Optimize and scale the infrastructure to handle increased load and improve performance. âœ… Revisit infrastructure setup or API development if scalability issues arise. Follow for more information about LLM implementation and engineering. #LLM #MachineLearning #AI #Deployment #TechLeadership\n\nHave you encountered cases of Catastrophic Forgetting in LLMs? Catastrophic forgetting in #LLMs (or in #ML in general) happens when a model forgets previously learned information when it is trained on new information. This phenomenon is especially common in single task fine-tuned LLMs or when a model is trained on multiple tasks over time. Catastrophic Forgetting occurs during (instruction) fine-tuning as it alters all the weights of a pretrained #LLM, and hence models facing this issue may fail on tasks that they quite easily used to solve before fine tuning. Single task fine tuning generally requires just 500-1000 new data samples. As the model focuses solely on learning patterns required for the new task and generates better responses for the focused task, it may forget the broader understanding of other general tasks. This can be problematic if a LLM needs to be updated constantly on new information for new tasks over time. So, what are some ways using which you can reduce Catastrophic Forgetting? ğŸ¤“ First of all, evaluate if CF is even going to be a problem in your case. If the focus of your LLM is only on the task you're fine tuning it for, CF is probably not going to be a huge issue. ğŸ¤“ If you want to use the fine tuned LLM for other tasks too, you can avoid CF by performing multi-task fine tuning. In this case, you will be fine tuning your LLM on all the tasks of interest in one go. This will ensure enhanced capabilities of your fine tuned LLM on all the tasks. However, multi-task fine tuning requires you to provide 10K to 100K data samples across all tasks and can be an expensive operation. FLAN (Fine tuned Language Net) models are examples of multi-task fine tuned models. ğŸ¤“ Another solution is to use parameter efficient fine tuning (#PEFT) techniques instead of full fine tuning. PEFT freezes the weights of the initial layers of the pretrained LLM and only modifies the weights of the later layers, and hence may reduce the effects of CF. ğŸ¤“ Lastly, you can use regularization techniques to add a penalty term to the loss function, constraining the learning process to retain the knowledge of previous tasks. There are several other techniques like Learning Without Forgetting (LWF), Synaptic Intelligence (SI), Continual Learning with Memory Replay (CLMR) etc, which are out of the scope of this post and you can read more on them. In summary, CF impacts the general performance of a LLM when it is fine tuned on a single task. This is something that we need to carefully consider before going with the costly fine tuning tasks so we can get the best out of the LLM. Let me know your thoughts in the comments! Follow Kavana Venkatesh for more such content! #generativeai #fidelityassociate #neuralnetworks #deeplearning #genai #finetuning #largelanguagemodels #nlp #ai #datascience #emergingtech"
    }
}
{
    "id": "dbpedia_1153_2",
    "rank": 79,
    "data": {
        "url": "https://www.frontiersin.org/journals/earth-science/articles/10.3389/feart.2023.1136472/full",
        "read_more_link": "",
        "language": "en",
        "title": "Seismic Rigoletto: Hazards, risks and seismic roulette applications",
        "top_image": "https://images-provider.frontiersin.org/api/ipx/w=1200&f=png/https://www.frontiersin.org/files/Articles/1136472/feart-11-1136472-HTML-r4/image_m/feart-11-1136472-g001.jpg",
        "meta_img": "https://images-provider.frontiersin.org/api/ipx/w=1200&f=png/https://www.frontiersin.org/files/Articles/1136472/feart-11-1136472-HTML-r4/image_m/feart-11-1136472-g001.jpg",
        "images": [
            "https://loop.frontiersin.org/images/profile/92148/32",
            "https://loop.frontiersin.org/images/profile/1099729/32",
            "https://loop.frontiersin.org/images/profile/2219655/32",
            "https://www.frontiersin.org/article-pages/_nuxt/img/crossmark.5c8ec60.svg",
            "https://loop.frontiersin.org/images/profile/2149563/74",
            "https://loop.frontiersin.org/images/profile/1034279/74",
            "https://loop.frontiersin.org/cdn/images/profile/default_32.jpg",
            "https://www.frontiersin.org/files/Articles/1136472/feart-11-1136472-HTML-r4/image_m/feart-11-1136472-g001.jpg",
            "https://www.frontiersin.org/files/Articles/1136472/feart-11-1136472-HTML-r4/image_m/feart-11-1136472-g002.jpg",
            "https://www.frontiersin.org/files/Articles/1136472/feart-11-1136472-HTML-r4/image_m/feart-11-1136472-g003.jpg",
            "https://www.frontiersin.org/files/Articles/1136472/feart-11-1136472-HTML-r4/image_m/feart-11-1136472-g004.jpg",
            "https://www.frontiersin.org/files/Articles/1136472/feart-11-1136472-HTML-r4/image_m/feart-11-1136472-g005.jpg",
            "https://www.frontiersin.org/files/Articles/1136472/feart-11-1136472-HTML-r4/image_m/feart-11-1136472-g0x1.gif",
            "https://www.frontiersin.org/files/Articles/1136472/feart-11-1136472-HTML-r4/image_m/feart-11-1136472-g0x2.gif",
            "https://www.frontiersin.org/files/Articles/1136472/feart-11-1136472-HTML-r4/image_m/feart-11-1136472-g0x3.gif",
            "https://www.frontiersin.org/files/Articles/1136472/feart-11-1136472-HTML-r4/image_m/feart-11-1136472-g006.jpg",
            "https://www.frontiersin.org/files/Articles/1136472/feart-11-1136472-HTML-r4/image_m/feart-11-1136472-g007.jpg",
            "https://www.frontiersin.org/files/Articles/1136472/feart-11-1136472-HTML-r4/image_m/feart-11-1136472-g008.jpg",
            "https://www.frontiersin.org/files/Articles/1136472/feart-11-1136472-HTML-r4/image_m/feart-11-1136472-g009.jpg",
            "https://www.frontiersin.org/files/Articles/1136472/feart-11-1136472-HTML-r4/image_m/feart-11-1136472-t001.jpg",
            "https://www.frontiersin.org/files/Articles/1136472/feart-11-1136472-HTML-r4/image_m/feart-11-1136472-g010.jpg",
            "https://www.frontiersin.org/files/Articles/1136472/feart-11-1136472-HTML-r4/image_m/feart-11-1136472-g011.jpg",
            "https://www.frontiersin.org/files/Articles/1136472/feart-11-1136472-HTML-r4/image_m/feart-11-1136472-g012.jpg"
        ],
        "movies": [],
        "keywords": [],
        "meta_keywords": [
            "Complex dynamical systems",
            "Earthquake disaster",
            "Hazard analysis",
            "pattern recognition applications",
            "risk analysis",
            "Scenario simulation"
        ],
        "tags": null,
        "authors": [],
        "publish_date": null,
        "summary": "",
        "meta_description": "Neo-Deterministic Seismic Hazard Assessment (NDSHA), dating back to the turn of the Millennium, is the new multi-disciplinary scenario- and physics-based app...",
        "meta_lang": "en",
        "meta_favicon": "https://brand.frontiersin.org/m/ed3f9ce840a03d7/favicon_16-tenantFavicon-Frontiers.png",
        "meta_site_name": "Frontiers",
        "canonical_link": "https://www.frontiersin.org/journals/earth-science/articles/10.3389/feart.2023.1136472/full",
        "text": "Introduction\n\nNewly published Earthquakes and Sustainable Infrastructure (Panza et al., 2021) presents a new paradigm for seismic safety — comprehensively detailing in one volume the ‘state-of-the-art’ scientific knowledge on earthquakes and their related seismic risks, and the actions that can be taken to reliably ensure greater safety and sustainability. This book is appropriately dedicated to the centenary of Russian geophysicist Vladimir Keilis-Borok (1921–2013), whose mathematical insights have been seminal for the innovative paradigm of Neo-Deterministic Seismic Hazard Assessment (NDSHA). Dating back to the turn of the Millennium, NDSHA is the new multi-disciplinary scenario- and physics-based approach for the evaluation of seismic hazard and safety — guaranteeing “prevention rather than cure.”\n\nWhen earthquakes occur with a given magnitude (M), the shaking certainly does not depend on sporadic occurrences within the study area, nor on anti-seismic (earthquake resistant) design parameters scaled otherwise to probabilistic models of earthquake return period and likelihood — as adopted in the widespread application of the model-driven Probabilistic Seismic Hazard Analysis (PSHA), e.g., by the Global Earthquake Model (GEM) project and its recent spinoff Modello di Pericolosità Sismica (MPS19) for Italy.\n\nAn earthquake compatible with the seismogenic characteristics of a certain area, even if sporadic and therefore labelled as “unlikely”, can occur at any time, and the anti-seismic design parameters must take into account the magnitude values defined according to both the seismic history and the seismotectonics. Therefore, from a policy perspective of prevention, coherent and compatible with the most advanced theories in Earth Science, it is essential that at least the infrastructure installations and public structures are designed so as to resist (or sustain) future strong earthquakes and continue operation in their original capacities.\n\nThirty chapters of the book provide comprehensive reviews and updates of NDSHA research and applications so far in Africa, America, Asia and Europe — a collection of evidences and case histories that hopefully will persuade responsible people and authorities to consider these more reliable procedures for seismic hazard analyses and risk evaluation. Providing awareness that the use of PSHA may result in the design of unsafe buildings, NDSHA evaluations must be considered in the next versions of earthquake-resistant design standards and explicitly taken as the reference approach for both safety and sustainability.\n\nThe book fulfils essential needs of geophysicists, geochemists, seismic engineers, and all those working in disaster preparation and prevention; and is the only book to cover earthquake prediction and civil preparedness measures from a Neo-Deterministic (NDSHA) approach. In this review we focus on the lead chapter: Hazard, Risks, and Prediction by Vladimir Kossobokov (2021) — an understanding of which is essential in the applications of the state-of-the-art knowledge presented in the book’s 29 following chapters.\n\nScience should be able to warn people of looming Disaster\n\n« Science should be able to warn people of looming disaster, Vladimir Keilis-Borok believes.“My main trouble,” he says, “is my feeling of responsibility.” »(Los Angeles Times, 9 July 2012)\n\nNowadays, in our Big Data World, Science can disclose Natural Hazards, assess Risks, and deliver the state-of-the-art Knowledge of Looming Disasters in advance of catastrophes, along with useful Recommendations on the level of risks for decision-making with regard to engineering design, insurance, and emergency management. Science cannot remove, yet, people’s favor for fable and illusion regarding reality, as well as political denial, sincere ignorance, and conscientious negligence among decision-makers. The general conclusion above is confirmed by application and testing against Earthquake Reality, that the innovative methodology of Neo-Deterministic Seismic Hazard Assessment (NDSHA) “Guarantees Prevention Rather Than Cure.” NDSHA results are based on reliable seismic evidence, Pattern Recognition of Earthquake Prone Areas (PREPA), implications of the Unified Scaling Law for Earthquakes (USLE), and exhaustive scenario-based modeling of ground shaking.\n\nThe UN World Conference on Disaster Reduction, held January 18–22, 2005 in Kobe, Hyogo, Japan, formally adopted the Hyogo Framework for Action 2005–2015: “Building the Resilience of Nations and Communities to Disasters”, just days following the 26 December 2004, MW 9.2 Great Indian Ocean mega-earthquake and tsunami. During the Conference, a Statement (Kossobokov, 2005a) at the “Special Session on the Indian Ocean Disaster: risk reduction for a safer future” was urging or insistent on a possibility of a few mega-earthquakes of about the same magnitude MW 9.0 occurring globally within the next 5–10 years. This prediction was confirmed, unfortunately, by both the 27 February 2010, MW 8.8 mega-thrust offshore Maule, Chile and the 11 March 2011, MW 9.1 mega-thrust and tsunami off the Pacific coast of Tōhoku, Japan (Kossobokov, 2011; Ismail-Zadeh and Kossobokov, 2020).\n\nAn opportunity to reduce the impacts from both these earthquakes and tsunami disasters was missed. Davis et al. (2012) showed how the prediction information on expected world’s largest earthquakes provided by the M8 and MSc algorithms (Keilis-Borok and Kossobokov, 1990; Kossobokov et al., 1990), although limited to the intermediate-term span of years and middle-range location of a thousand km, can be used to reduce future impacts from the world’s largest earthquakes.\n\nThe primary reasons for having not used the prediction for improving preparations in advance of the Tōhoku earthquake “included: 1) inadequate links between emergency managers and the earthquake prediction information; and 2) no practiced application of existing methodologies to guide emergency preparedness and policy development on how to make important public safety decisions based on information provided for an intermediate-term and middle-range earthquake prediction having limited but known accuracy.” The Tōhoku case-study exemplifies how reasonable, prudent, and cost-effective decisions can be made to reduce damaging effects in a region, when given a reliable Time of Increased Probability (TIP) for the occurrence of a large earthquake and associated phenomena like tsunami, landslides, liquefaction, floods, fires, etc.\n\nThe Sendai Framework for Disaster Risk Reduction 2015–2030, a successor of the Hyogo Framework for Action, is a set of agreed-upon commitments to proactively ensure the prevention of “new” Disasters — through the timely implementation of integrated economic, structural, legal, social, health, cultural, educational, environmental, technological, political, and institutional measures (Briceño, 2014; Mitchell, 2014). Years after the 2005 Hyogo and 2015 Sendai Frameworks for Disaster Risk Reduction, countries are now following a range of different approaches and mitigation strategies, due to the variety of both societal systems and hazards. However, Gilbert White’s (2005) observation from the tragic tsunami beginnings of this heightened awareness that it was “important to recognize that no country in the world has achieved a completely effective policy for dealing with the rising tide of costs from natural hazards” is still largely true today.\n\nOur beliefs in models, myths can contradict real-world observations\n\nMoreover, the ongoing COVID-19 pandemic is an itchy and troubling global example of how public policies based on presumably both “the best science available” and also data of high quality nonetheless appear to be extremely difficult, uneven, and may sometimes lead to Disaster even in those countries that were supposedly well-prepared for such an emergency. In fact, the pandemic (https://coronavirus.jhu.edu/map.html), with a rapidly growing less-than-a-year death toll of 1,820,841 and 83,579,767 global cases reported on 1 January 2021, as of 13 February 2021, had the numbers alarmingly already raised to 2,385,203 and 108,289,000 respectively — and thereby sheds a sobering shower on our existing unperturbed and unchallenged myths about disasters (Mitchell, 2014). As of 2 September 2021, the totals had more than doubled, rising to 4,702,119 and 229,159,687 despite enormous efforts on vaccination. JHU has stopped collecting data as of March 10, 2023 when the death toll reached 6,881,955; total cases reached 676,609,955; and total vaccine doses administered reached 13,338,833,198.\n\nIn one disastrous outcome, “the anzen shinwa (“safety myth”) image portrayed by the Japanese government and electric power companies tended to stifle honest and open discussion of the risks” from nuclear power, in the years leading up to the 2011 Fukushima disaster (Nöggerath et al., 2011). Kaufmann and Penciakova (2011) illustrate how “countries with good governance”— for example, Chile in 2010, “can better prepare for and mitigate the devastating effects of natural disasters” through leadership and transparency. In exploring “Japan’s governance in an international context and its impact on the country’s crisis response,” they reveal how failures in the nuclear plant regulatory environment (including regulatory capture — wherein “the rulemaking process also appears to be riddled with conflict of interest”) led to an unmitigated disaster that was totally avoidable. See also (Saltelli et al., 2022).\n\nCan nothing be done to stop the increasing number of disasters?\n\nIs there any reason, when estimating long-term trends, for inventing the Myth that now “fewer people are dying in disasters” (Mitchell, 2014), if a pandemic like COVID-19 (or even a single deadly event like the 2004 Great Indian Ocean mega-earthquake and tsunami that killed 227,898 people) can push up significantly the expected average rate of death tolls? Is Climate Change now the biggest cause of disasters, since both vulnerable populations and infrastructures presently exist widespread in the areas exposed to extreme catastrophic events of different kinds?\n\nIs it true that nothing can be done to stop the increasing number of disasters, if, alternatively, a country can radically reduce its risks from disasters by appropriate investments, incentives, and political leadership? Unlike 30 years ago, Science presently does have the know-how to reduce damage from even the major hazardous events to the level of incidents rather than disasters.\n\nEvidently, we do not live in a black-and-white disaster world, and our beliefs, i.e., our mental models, or the “conceptualizations” that we “bring to the task” (pages 2–3 in Chu, 2014) in “initial basic principles” may unfortunately lead us to rather prefer models that contradict with our real-world observations. We know quite well the famous quotation that “all models are wrong, but some are useful” from George Box (1979), but too often we forget that some models are useless and some others are really harmful, especially, when viewed as complete substitutes for the original natural phenomenon (Gelfand, 1991).\n\nNowadays, in our Big Data World, where the global information storage capacity routinely surpasses a level of more than 6 Zettabytes (6 × 10+21 in optimally compressed bytes) per year, “open data”, together with the enormous amount of available pretty fast user-friendly software, provide unprecedented opportunities for the development and enhancement of pattern recognition studies — in particular, those studies applied to Earth System processes. However, a Big Data World alternatively opens up as well many wide avenues, narrow pathways, and even rabbit holes for finding and/or imagining deceptive associations (i.e., Quixote-like patterns that are not really there) in both inter- and trans-disciplinary data — therein then subsequently inflicting misleading inventions, predictions, and, regretfully, wrong decisions that eventually may lead to different kinds of disasters.\n\nThe core seed of disaster is risk\n\nThe “common language vocabulary” by itself is oftentimes confusing to common peoples’ understandings of well-intentioned messaging conveying importance of dangers and their likelihood, even though generally being both thought-provoking and pretty much instructive: see Cambridge Dictionary for Disaster; Hazard; Risk; Vulnerability; and Prediction (https://dictionary.cambridge.org/us/).\n\n“Although ‘hazard’ and ‘risk’ are commonly regarded as synonyms, it is useful to distinguish between them. Hazard can be thought of as the possibility that a dangerous phenomenon might occur, whereas risk is a measure of the loss to society that would result from the occurrence of the phenomenon. More concisely, ‘risk is a measure of the probability and severity of adverse effects’ (Lowrance, 1976; Peterson, 1988).” Seismic hazard refers to the natural phenomenon of earthquakes, ground motion in particular, which can cause harm. Seismic risk refers to the possibility of loss or injury caused by a seismic hazard.\n\nWe are all living in a risky world, and Figure 1 illustrates further our appetite for all the essential intertwined loops of Risk: defined in common language as “the chance of injury, damage, or loss.” The figure complements with the fifth basic component of Time the four components presented by Boissonnade and Shah (1984), who define Risk “as the likelihood of loss”. In insurance studies: a) the Exposure is defined as “the value of structures and contents, business interruption, lives, etc.”; and b) Vulnerability as the sensitivity to Hazard(s) at certain Location(s) — i.e., “the position of the exposure relative to the hazard.” Since Hazard is likely to cause damage and losses sometimes, the origin Time and duration of any hazardous event may become critical in its transformation to Disaster, as illustrated later.\n\nFIGURE 1\n\nNatural hazards\n\nIn the natural hazard realm, these dangerous and damaging phenomena may include earthquake, tsunami, flood, landslide, volcanic eruption, hurricane, tornado, wildfire, etc. Hazards (or possibilities that dangerous phenomena might occur) are especially “risky” when they are only thought of in terms of the perceived probabilities for their occurrence (i.e., low hazard or high hazard) — because here we really need to consider the components of Location, Time, and Exposure versus Vulnerability as well.\n\nWe also know quite well from experience that hazardous events may cascade — where (under certain circumstances) a primary event may initiate or cause further secondary, tertiary, etc. damages, disruptions, and losses — such as the recent August 26 2021 Hurricane Ida, a Category 4 storm that blasted ashore in Louisiana midday “knocking out power to all of New Orleans, blowing roofs off buildings and reversing the flow of the Mississippi River as it rushed from the Louisiana coast into one of the nation’s most important industrial corridors” in the middle of increasing Delta Variant infections/hospitalizations due to the ongoing COVID-19 pandemic. Thus, depending on both the particular risky situation and our response, a hazardous event scenario may either cause or not cause a Disaster.\n\nCan uncertainty be computed?\n\nWhile Risk can be computed, uncertainty cannot. So regretfully, the following statement, originally attributed to seismic hazard assessment some four decades ago, has not lost its relevance today, and still applies appropriately to present day situations we face in analyzing other potential damages and losses — for the timely implementation of integrated economic, structural, legal, social, health, cultural, educational, environmental, technological, political, and institutional measures:\n\nHowever, ignorance still exists on the seismic severity (usually expressed in intensity values) a site may expect in the future as well on the damage a structure may sustain for a given seismic intensity. (Boissonnade and Shah, 1984, p. 233)\n\nAnd while prediction is “the act of saying what you think will happen in the future: e.g., ‘I wouldn’t like to make any predictions about the result of this match.’”— even the advanced tools of data analysis may lead to wrong assessments, when inappropriately used to describe the phenomenon under study. A (self-) deceptive conclusion could be avoided by verification of candidate models against (reproducible) experiments on empirical data — and in no other way.\n\nRisk communication in disaster planning\n\nWhen decisions are made about required actions in response to prediction of a disaster, the choices made are usually based on a comparison of expected “black eyes” (risks/costs) and “feathers in caps” (benefits). If the latter exceed the former, it is reasonable to go forward. But each of decision-makers may have rather different opinions on hazards, risks, and outcomes of different decisions and, as it is well-known, even two experts (scientists, in particular) may have three or more opinions!\n\nTherefore, actual decisions sometimes (if not always) are not optimal, especially when there are alternative ways of gaining personal benefits or avoiding personal guilt. In many practical cases, decision makers do not have any opinion due to: i) ignorance in beyond-design circumstances; ii) denial of hazard and risk — based on misconceptions; and iii) a sense of personal responsibility to an impending disaster when it is too late to take effective countermeasures. As a result, since Prediction again is “the act of saying what you think will happen in the future: e.g., ‘I wouldn't like to make any predictions about the result of this match.’” — this mimicked view in policy decisions becomes a common way to avoid responsibility.\n\nSince there is already a lot of flexibility in common language that justifies the following disclaimer note: “Any opinions in the examples do not represent the opinion of the Cambridge Dictionary editors or of Cambridge University Press or its licensors.” — we note that many people, including scientists, do not well distinguish between ‘unpredictable’, ‘random’, and ‘haphazard’, which distinctions are, nevertheless, crucial for scientific reasoning and conclusions. In particular, Stark (2017, 2022) emphasizes that: “‘Random’ is a very precise statistical term of art” and that notions of probability can only apply “if the data have a random component.”\n\nRisk Modeling (Michel, 2018) is about the future of Exposure and necessarily convolves Hazard (where possibility now ≈ likelihood of an event) with the components of Location, Time, and Vulnerability (Cannon, 1993; McEntire et al., 2002; McEntire, 2004). Fischhof and Kadvany (2011) informatively note that Risk “shows how to evaluate claims about facts (what might happen) and about values (what might matter)”, further observing that as was previously noted with regard to the global COVID-19 pandemic, officially declared 11 Mar 2020 by the World Health Organization (WHO): “societies define themselves by how they define and manage dangers.” See also (SISMA-ASI (2009); Kaufmann and Penciakova, 2011; May, 2001; Berke and Beatley, 1992; Scawthorn, 2006; SISMA-ASI (2009); Wang, 2008; Wiggins, 1972; Bolt, 1991; Tierney, 2014).\n\nThus, an earthquake hazard with a presumed low-likelihood (or low probability) can nevertheless represent a high or even unacceptable risk (Berke and Beatley, 1992; May, 2001; Marincioni et al., 2012; Bela, 2014; Tanner et al., 2020), in addition to references cited in previous paragraph — and particularly for those cases noted in “Earthquakes and Sustainable Infrastructure” (Panza et al., 2021), the state-of-the-art approaches are “aimed at the level of natural risks for decision-making in regard to engineering design, insurance, and emergency management”.\n\nAnd while insurance can repair the damage, and while catastrophic reinsurance can even further spread the risk and keep first insurers solvent, lives can only be saved and infrastructure installations and public structures can only “resist (or sustain) future strong earthquakes and continue to operate in their original capacity” if they can withstand the shaking. An often unappreciated and complicating factor is that “earthquake risk is characteristically seen as ‘remote’ ”— with naturally rare earthquake events “resulting in low risk awareness and low risk reward (Michel, 2014).”\n\nVolcanic disasters: Nyiragongo and Mt. St. Helens\n\nThe recent 22 May 2021 Nyiragongo volcano (DR Congo) flank eruption is tellingly illustrative of a volcanic disaster. After just 19 years since the catastrophic January-February 2002 flank eruption, a new flank eruption began on 22 May 2021 (coincidentally on the same date as the Mw 9.5 1960 Chile earthquake, the largest recorded earthquake of the 20th century). As of 27 May 2021 “More than 230,000 displaced people are crowding neighboring towns and villages. Lack of clean water, food and medical supplies, as well as electricity in parts of Goma, are creating catastrophic conditions in many places. To add to all this misery, health authorities are worried about outbreaks of cholera — at least 35 suspected cases have been found so far.” (https://www.volcanodiscovery.com/nyira-gongo/eruption-may-2021/activity-update.html).\n\nUSGS volcanologist Donald Peterson, who witnessed first-hand the catastrophic 1980 Eruption of Mt. St. Helens in southwestern Washington state, United States, observed in a comprehensive review of “Volcanic Hazards and Public Response” (Peterson, 1988) that “although scientific understanding of volcanoes is advancing, eruptions continue to take a substantial toll of life and property.” And although “scientists sometimes tend to feel that the blame for poor decisions in emergency management lies chiefly with officials or journalists because of their failure to understand the threat,” he believes otherwise that “however, the underlying problem embraces a set of more complex issues comprising three pervasive factors: 1) the first factor is the volcano: signals given by restless volcanoes are often ambiguous and difficult to interpret, especially at long-quiescent volcanoes; 2) the second factor is people: people confront hazardous volcanoes in widely divergent ways, and many have difficulty in dealing with the uncertainties inherent in volcanic unrest; 3) the third factor is the scientists: volcanologists correctly place their highest priority on monitoring and hazard assessment, but they sometimes fail to explain clearly their conclusions to responsible officials and the public, which may lead to inadequate public response.” And since “of all groups in society, volcanologists have the clearest understanding of the hazards and vagaries of volcanic activity; they thereby assume an ethical obligation to convey effectively their knowledge to benefit all of society.”\n\nExplaining uncertainty; miscommunication and disasters\n\nCommon language vocabulary issues aside; “it is not easy to explain the uncertainties of volcanic hazards to people not familiar with volcanoes, and often these difficulties lead to confusion, misunderstanding, and strained relations between scientists and persons responsible for the public welfare, such as civil officials, land managers, and journalists” (Peterson, 1988) — and notably, the fatal 6 April 2009 Mw 6.3 earthquake disaster that occurred in the Abruzzi region of Central Italy, killing more than 300 people and wrecking the medieval heart of the city, is just such a case-in-point: showing that the above miscommunication reality will apply mutatis mutandis to earthquakes and other hazards. The 2009 L’Aquila earthquake had been preceded by much seismic activity beginning in October 2008, analogous to the preparatory rumblings of an awakening volcano. But even though it occurred in a zone defined at high seismic hazard, as charted on a map — high vulnerabilities combined with major failures in Disaster Risk Mitigation to produce both the tragic large losses and an ensuing legal prosecution of six scientists and one government official, “the L'Aquila Trial” (See Marincioni et al., 2012; Panza and Bela, 2020 and Supplementary Material therein).\n\nEffective communication\n\nIn comprehensively addressing the public response, Peterson “advanced the view that volcanologists should regard the development of effective communications with the public just as important a challenge as that of monitoring and understanding the volcanoes. We must apply the same degree of creativity and innovation to improving public understanding of volcanic hazards,” he believed, “as we apply to the problems of volcanic processes. Only then will our full obligation to society be satisfied.”\n\nTo be creatively most effective, in developing effective communications with the public (all people or groups not involved in the scientific study of volcanoes, earthquakes, etc.), Peterson offered these insights systematically researched and provided from the social sciences, which “deal with the interaction of people with all kinds of hazards.”\n\nSorensen and Mileti (1987), pages 14–53 showed that the response to a warning by a person (Figure 2) or group includes a series of steps that involve hearing a message, understanding it, believing it, personalizing it (that is, being convinced that it really applies to the individual), and finally taking action. Different people and different societies react in individual ways as they progress through these steps. The style of a warning message greatly influences the response it produces, and warnings are most effective if they are specific, consistent, accurate, certain, and clear (Sorensen and Mileti, 1987, page 20). If one or more of these attributes is missing, the message is more likely to be ignored or disbelieved.\n\nFIGURE 2\n\nWhat (we think) we know about earthquakes\n\nFor a reliable seismic hazard assessment, a specialist must be knowledgeable in understanding seismic effects:\n\n• An earthquake is a sudden movement that generates seismic waves inside the Earth and shakes the ground surface.\n\n• Although historical records on earthquakes are known from 2100 B.C., generally most of the earthquakes before the middle of the 18th century are lacking a reliable description, with a possible exception being the Catalogo Parametrico dei Terremoti Italiani (Gasperini et al., 2004) based upon both historical and instrumental data comprising an Italian Earthquakes Catalog more than a thousand years long.\n\n• Earthquakes are complex phenomena. Their extreme catastrophic nature has been known for centuries, due to resulting devastations recorded from many of them.\n\n• Their abruptness, along with their sporadic, irregular and apparently rare occurrences, all facilitate formation of the common perception that earthquakes are random and unpredictable phenomena.\n\nHowever, modern advances in seismology prove that this perceived random and unpredictable behavior is not really the case in a number of important aspects (Kossobokov, 2021).\n\nNowadays, the location of earthquake-prone sites is accurately mapped (Figure 3) due to rather accurate hypocenter determinations, along with estimates of their source size. The “seismic effects” of earthquakes that are needed for a Seismic Hazard Assessment (SHA) can be characterized from both physically felt and observed effects (Macroseismic Intensity), and also from instrumentally recorded earthquake records: a) seismograms and b) records of the actual ground shaking characterizing acceleration, velocity, and displacement — see chapters in Encyclopedia of Solid Earth Geophysics (Gupta, 2020).\n\nFIGURE 3\n\nA detailed historical review of earliest seismological attempts to quantify sizes of earthquake sources through a measure of their energy radiated into seismic waves, which occurred also in connection with the parallel development of the concept of earthquake magnitude, is supplied by Gutenberg and Richter (1949), Panza and Romanelli (2001), and Okal (2019). Figure 4 illustrates the commonly accepted notation of earthquake magnitude classes.\n\nFIGURE 4\n\nEllsworth (1990) offers these important caveats whenever performing a systematic SHA: a) “earthquakes are complex physical processes generated by sudden slip on faults, and as such they can only be grossly characterized by simple concepts”; and b) “Magnitude, as commonly used to compare the sizes of different earthquakes, also represents an extreme simplification (cf Felt Intensity: center of energy; Instrumental Seismometer: point of first rupture) of the earthquake process and by itself cannot fully characterize the size of any event. Traditionally, seismologists have developed a suite of magnitude scales, each with its own purpose and range of validity to measure an earthquake. Because no single magnitude scale can be systematically applied to the entire historical record, a summary magnitude, M, is introduced here to facilitate comparisons between events.”\n\nMany shaking intensity scales have been developed over a few centuries to measure the damaging results from earthquakes, of which the Modified Mercalli Intensity (MMI) is among the most commonly used. This scale, which maps the center of energy release for pre-instrumental records, classifies qualitatively the effects from an earthquake upon the Earth’s surface: ranging from “not felt” (intensity I); to “extreme” (intensity X), when most masonry and frame structures are destroyed with foundations; and finally, to “total destruction” (intensity XII on the MMI scale), when rolling waves are seen on the ground surface and objects are thrown upward into the air. We feel worth mentioning here also that, for the Mercalli Cancani Sieberg (MCS) intensity scale, a doubling of Peak Ground Acceleration (PGA) practically corresponds to one unit increment of Macroseismic Intensity (Cancani, 1904).\n\nNumerous approaches to the determination of an earthquake source size have resulted in a number of quantitative determinations of magnitude M based on instrumental, macro-seismic, and other data (Bormann, 2020). Charles Richter (1935) used the physically dimensionless logarithmic scale (because of the very large differences in displacement amplitude between different sized events) for his definition of magnitude М — that appears naturally appropriate due to apparent hierarchical organization of the lithosphere (which contains mobile blocks ranging from just the size of a grain ∼10−3 m across — on up to scale of tectonic plates ∼106 m). (Keilis-Borok, 1990; Sadovsky, 2004; Ranguelov, 2011; Ranguelov and Ivanov, 2017).\n\nIt is not surprising that, for shallow-depth earthquakes, the magnitude M (originally determined by Richter from the ground displacement recorded on a seismogram) is about two-thirds of the MMI intensity at the epicenter I0, thus М = ⅔ I0 + 1 (Gutenberg and Richter, 1956). Accordingly, then, a strong (M = 6.0) shallow earthquake may cause only negligible damage in buildings of good design and construction near the epicenter, but otherwise considerable damage in poorly built or badly designed structures.\n\nFigure 5 illustrates the global magnitude distribution by year for the time period 1963–2020, i.e., after installation of the analog World-Wide Network of Standard Seismograph Stations (WWNSS) https://science.sciencemag.org/content/174/4006/254 (top); and the empirical non-cumulative Gutenberg-Richter plot of N(M) for the entire 58 years of record, with b-value estimated at 0.998 (R2 = 0.977) for the best fit of log10N(M) = a + b × (8 – M) (bottom).\n\nFIGURE 5\n\nThe coastline of Britain and the seismic locus of earthquake epicenters\n\nThe set of earthquake epicenters or, in other words, the seismic locus, has the same fractal properties as the coastline of Britain. Benoit Mandelbrot (1967) notes: “Geographical curves are so involved in their detail that their lengths are often infinite or, rather, undefinable. However, many are statistically ‘self-similar’, meaning that each portion can be considered a reduced-scale image of the whole. Indeed, self-similarity methods are a potent tool in the study of chance phenomena, wherever they appear, from geostatistics to economics and physics. Therefore, scientists ought to consider dimension as a continuous quantity ranging from 0 to infinity.”\n\nFollowing the pioneering works by Mikhail A. Sadovsky (Sadovsky et al., 1982) and Keiiti Aki (Okubo and Aki, 1987), our understanding of the fractal nature of earthquakes and seismic processes has increasingly grown (Kossobokov, 2020) — along with a concomitantly scientifically revolutionary better understanding and mapping of both the Earth’s interior (Stacey and Davis, 2020), as well as of geophysical aspects of seismic waves propagation (Florsch et al., 1991; Fäh et al., 1993; La Mura et al., 2011; Iturrarán-Viveros and Sánchez-Sesma, 2020).\n\nNaturally, or as might be expected due to hierarchical organization of the lithosphere (referring to the size-related distribution of geographical/seismological phenomena), the number of earthquakes globally, or within a region, is scaled by magnitude, according to the Gutenberg-Richter Frequency-Magnitude (FM) relation (Gutenberg and Richter, 1944; 1954). Globally, and for the time period of 58 years shown in Figure 5 the slope (so called b-value) of the plot is about 1, so that each one unit change in magnitude between M = 5 and M = 9 results in approximately a 10-fold change in the number of earthquakes; there were approximately two hundred M = 7 earthquakes compared to about twenty M = 8 and 20,000 M = 5 earthquakes.\n\nThe Gutenberg-Richter relation is a power law and can be written as log10 N = a − bM. As shown in Figure 5, this is also a Pareto distribution, or a distribution with “fat” tails, which serves as a reminder that, in SHA, outliers always do exist as “possibilities” and must therefore be duly recognized and accounted for in seismic hazard [see e.g., Kanamori (2014; 2021)].\n\nGeneralized Gutenberg-Richter relationship and unified scaling law for earthquakes\n\nThe Gutenberg-Richter relationship just shown above was further generalized by Kossobokov and Mazhkenov (1988, 1994) to the following fractal form:\n\nwhere: i) N(M,L) is the expected annual number of main shocks of magnitude M within an area of linear size L; ii) the similarity coefficients A and B are similar to the a- and b-values from the classical Gutenberg-Richter law; iii) the newly added similarity coefficient C is the fractal dimension (D per Mandelbrot) of the set of epicenters; and iv) M–and M–are the limits of the magnitude range where this relationship holds. The three frequency-magnitude-spatial coefficients provide an insight into scaling properties of actual seismicity, and therefore they are of specific interest to seismologists working on seismic zonation and risk assessment.\n\nIt was shown that C is significantly different from 2, and that it correlates with the geometry of tectonic structures: i) high values of C (∼1.5) correspond to the regions of complex dense patterns of faults of different strikes and high degrees of fracturing, whereas; ii) lower values of C (∼1) are related to regions exhibiting a predominant linear major fault zone (which is consistent with rectifiable curves and straight lines, where D = 1).\n\nMoreover, for example, in the specific case of the Lake Baikal region in the mountainous Russian region of Siberia, north of the Mongolian border (with area of 1,500,000 km2 and C = 1.25), it was demonstrated (Kossobokov and Mazhkenov, 1988; Kossobokov and Mazhkenov, 1994) that: i) the inclusion of aseismic areas leads to underestimation of seismic activity in an area of 1,000 km2 by a factor of 15; and alternatively ii) when a characteristic of seismic activity over 1,000 km2 is computed for a grid 10 km × 10 km, this leads to overestimation by a factor greater than 2.\n\nEarlier, in order to avoid just such seismic activity bias, in a pilot study assessing seismic risk for 76 selected Largest Cities of the World in active seismic regions, Keilis-Borok et al. (1984) compared these two integral estimates: 1) the number of cities with population of one million or more affected in 30 years by strong motion of intensity I ≥ VIII; and 2) the total population in these cities — with the actual aftermaths of these past earthquakes — for reliable “validation of the results” showing specifically that: a) “available data may be sufficient to estimate the seismic risk for a large set of objects, while not for each separate object”; and b) “it indicates, that global seismic risk is rapidly increasing, presenting new unexplored problems.”\n\nThe Unified Scaling Law for Earthquakes (USLE), got its name later, when Bak et al. (2002) presented an alternative formulation from that above — making use of the inter-event time between the earthquake occurrences, instead of their annual number. Using the USGS/NEIC Global Hypocenters Data Base, 1964–2001, and a robust box-counting algorithm; Nekrasova and Kossobokov (2002) managed to map the values of A, B, and C in every 1°× 1° box on the Earth marked by record of earthquake occurrence, wherever the catalog of shallow earthquakes of M ≥ 4 permitted a reliable estimation. The results of this global mapping are available at the data repository of the International Seismological Centre (Nekrasova and Kossobokov, 2019).\n\nThe distribution of the number of seismic events by magnitudes — the Gutenberg-Richter frequency magnitude relation — is of paramount importance for seismic hazard assessment of a territory. Accordingly, the generalization of the original Gutenberg-Richter relation into the Unified Scaling Law for Earthquakes (USLE) as originally proposed in 1988 makes it possible now to take into account as well the pattern of epicentral distribution of seismic events, whenever changing the spatial scale of the analysis. This is extremely important for adequate downscaling of the frequency-of-occurrence into a smaller target area within any territory under study (e.g., into a megalopolis).\n\nAt the time, when Per Bak (Bak et al., 2002) suggested a dual formulation of USLE using the time between seismic events, the Institute of Earthquake Prediction Theory and Mathematical Geophysics of the Russian Academy of Sciences developed a modified algorithm for statistically improved, confident Scaling Coefficients Estimation (referred to as SCE) of the USLE parameters to be used for producing seismic hazard maps of territories prone to seismic effects. An updated brief review, focused on the use of the USLE approach in relation to assessment of seismic hazard and associated risks, is provided in (Nekrasova et al., 2020).\n\nMulti-scale seismicity model\n\nComplementary to USLE is the Multi-scale Seismicity Model (MSM) by Molchan et al. (1997). For a general use of the classical frequency-magnitude relation in seismic risk assessment, they formulated a multi-scale seismicity model that relies on the hypothesis that “only the ensemble of events that are geometrically small, compared with the elements of the seismotectonic regionalization, can be described by a log-linear FM relation.” It follows then that the seismic zonation must be performed at several scales, depending upon the self-similarity conditions of the seismic events and the log-linearity of the frequency-magnitude relation, within the magnitude range of interest. The analysis of worldwide seismicity, using the Global Centroid Moment Tensor (GCMT) Project catalog (where the seismic moment is recorded for the earthquake size) corroborates the idea and observation that a single FM relation is not universally applicable. The MSM of the FM relation has been tested in the Italian region, and MSM is one of the considered appropriate ingredients of NDSHA.\n\nEarthquake catalogs evidence clear patterns that there exists a space-time energy distribution of seismic events because: i) earthquakes do not happen everywhere, but preferentially in tectonically well-developed highly fractured fault zones within the Earth’s lithosphere; ii) earthquake sizes follow the Gutenberg-Richter relationship, which is a surprisingly robust power law (such that, for every magnitude M event, there are ∼10 magnitude M − 1 quakes — within an area that is large enough); iii) earthquakes cluster in time — in particular, seismologists observe: a) surges and swarms of earthquakes; b) seismically driven decreasing cascades of aftershocks; and c) less evident inverse cascade (energy increase), or crescendo of rising activity in foreshocks premonitory to the main shock.\n\nSince earthquake-related observations are generally limited to the recent-most decades (sometimes centuries in just a few rare cases), getting reasonable confidence limits on an objective estimate of the occurrence rate or inter-event times of a strong earthquake within any particular geographic location necessarily requires a geologic span of time that is unfortunately unreachable for instrumental, or even historical seismology [see, e.g., (Ellsworth, 1990; Beauval et al., 2008; Stark, 2017; 2022)]. That is why probability estimates in Probabilistic Seismic Hazard Analysis (PSHA) remain subjective values ranging between 0 and 1, derived from evidently imaginary (but enticingly both analytically and numerically tractable) unrealistic hypothetical models of seismicity.\n\nSeismic Roulette: Nature spins the wheel!\n\n“Look deep into nature, and then you will understand everything better”.- Albert Einstein\n\nRegretfully, most, if not all, of earthquake prediction claims can be characterized as “invented” windmills, wherein we see the earth “not as it is”, but “as it should be” due to very small, if any, samples of clearly defined evidence! Many prediction claims are hampered at their start from the misuse of Error Diagram and its analogues — ignoring the evident heterogeneity of earthquake distributions in space as well as in time. See e.g. (Bela and Panza, 2021).\n\nA rigorous mathematical formulation of a natural spatial measure of seismicity is given in (Kossobokov et al., 1999). This “Seismic Roulette null-hypothesis” (Kossobokov and Shebalin, 2003) (or the hypothesis that chance alone in a random process is responsible for the results) is a nice analogy for using the simple recipe that accounts for this spatial patternicity (Kossobokov, 2006a) using statistical tools available since Blaise Pascal (1623–1662):\n\nconsider a roulette “wheel” with as many sectors as the number of events in the best available catalog of earthquakes, one sector per earthquake epicenter event; make your best bet according to any prediction strategy: determining which events are inside a projected space-time “area of alarm” — and then place one chip upon each of the corresponding sectors.\n\nNature then spins the “wheel”, before introducing an energized target-seeking earthquake “ball”. If you play seismic roulette systematically, then you win and lose systematically (Figure 6). If you are smart enough, and your predictions are effective, the first will outscore the second.\n\nFIGURE 6\n\nHowever, if Seismic Roulette is not perfect in confirming your betting strategy (and thus alternatively is nullifying your hypothesis), and still you are smart enough to choose an effective strategy, then your wins will outscore your losses! And after a while . . . you can then use your best wisdom, or even now an “antipodal strategy”, wherein the earthquake “prediction problem” is examined from the standpoint of decision theory and goal optimization per Molchan (2003) — so as to win both systematically and statistically self-similarly in the future bets!\n\nThe results of just such a global “betting” test of the prediction algorithms M8 and MSc did confirm such an “imperfection” of Seismic Roulette (Seismic Roulette is not perfet!) in the recurrence of earthquakes in Nature (Ismail-Zadeh and Kossobokov, 2020); but these same results still suggest placing future bets can be useful, if used in a knowledgeable way for the benefit of the populations exposed to seismic hazard. Their accuracy is already enough for undertaking earthquake preparedness measures, which would prevent a considerable part of damage and human loss, although far from the total. And fortunately, the methodology linking prediction with disaster management strategies does already exist (Molchan, 1997).\n\nPattern recognition of earthquake prone areas\n\nIn lieu of local seismic observations long enough for trustworthy and reliable SHA, alternatively one may try using Pattern Recognition of Earthquake-Prone Areas (PREPA) based, however, on the appropriate geological and geophysical data sets that are available. This geomorphological pattern recognition approach (Gelfand et al., 1972; Kossobokov and Soloviev, 2018) is an especially useful preparedness and mitigation tool in seismic regions that have passed validation: i) first, by exhaustive retrospective testing; and then ii) by the decisive confirmation check afforded by actual strong earthquakes that have occurred. Validity of this pattern recognition PREPA methodology has been proven by the overall statistics of strong earthquake occurrences — after numerous publications of pattern recognition results encompassing both many seismic regions and also over many magnitude ranges (see Gorshkov et al., 2003; Gorshkov and Novikova, 2018 and references therein).\n\nThose who can’t model are doomed to reality!\n\nOne application of PREPA deserves a special comment. Regional pattern recognition problems solved by Gelfand et al. (1976) treated two different sets of natural recognition objects for the two overlapping regions: i) regularly spaced points along major strike-slip faults in California; and ii) intersections of morphostructural lineaments in California and adjacent territories of Nevada (Figure 7). They then drew from these the qualitative conclusion that areas prone to M ≥ 6.5 are characterized by proximity to the ends (or to intersections) of major faults, in association with both: a) low relief; and b) often also with some kind of downward neotectonic movement expressed in regional topography and geology — with their conclusion further supported by both PREPA classifications: i) points; and ii) intersections — wherein the same five groups of earthquake-prone areas show up in both cases. Slight differences are due to the fact that the study of intersections covers a larger territory. This supports the idea derived from recognition of points — that strong earthquake-prone intersections often associate with neotectonic subsidence on top of a background weak uplift.\n\nFIGURE 7\n\nAs evident from Figure 7, the PREPA termless prediction for California and Nevada has been statistically justified by the subsequent occurrence of 16 out of 17 magnitude 6.5+ earthquakes within a narrow vicinity of the 73 Dangerous D-intersections of morphostructural lineaments (union of yellow circles in Figure 7) determined by Gelfand et al. (1976) as prone to seismic events that large. The target earthquakes included the recent-most 15 May 2020, M6.5 Monte Cristo Range (NV) earthquake and 6 July 2019, M7.1 Ridgecrest (CA) main shock, i.e., the one exceptional near-miss within the study area since 1976. In fact, the first day cascading aftershocks for this event, as well as the entire 2019 Ridgecrest earthquake sequence, extend to the D-intersection. It is also notable that the Puente Hills thrust fault beneath metropolitan Los Angeles coincides exactly (Kossobokov, 2013) with the lineament drawn back in 1976, decades in advance of its “rediscovery” by the 1994 Northridge Earthquake (Shaw and Shearer, 1999).\n\nFinally (and importantly for seismic hazard assessment), PREPA is a readily available hazard-related quantity that can be naturally included in NDSHA, while so far, no comparable way exists to formulate a direct use for it within PSHA — wherein earthquake “possibilities” are instead viewed temporally by Senior Seismic Hazard Analysis Committee (SSHAC, 1997) of the United States Nuclear Regulatory Commission as “annual frequencies of exceedance of earthquake-caused ground motions [that, however] can be attained only with significant uncertainty.” Therefore, ahem . . . those who can’t model are doomed to reality!\n\nSeismic hazard and associated risks\n\n“At half-past two o'clock of a moonlit morning in March, I was awakened by a tremendous earthquake, and though I had never before enjoyed a storm of this sort, the strange thrilling motion could not be mistaken, and I ran out of my cabin, both glad and frightened, shouting, \"A noble earthquake! A noble earthquake!\" feeling sure I was going to learn something.”John Muir, The Yosemite, Chapter 4\n\nGround shaking may be frightening, but it may not necessarily kill people. For example, the earliest reported earthquake in California was on 28 July 1769, and was documented in diaries by the exploring expedition of Gaspar de Portola, enroute from San Diego to chart a land route to Monterey. While camped along the Santa Ana River, about 50 km southeast of Los Angeles, “a sharp earthquake was felt that ‘lasted about half-as-long as an Ave Maria.” Based on descriptions of the quake, it was likely a moderate or strong earthquake. Some described the shaking in expedition diaries as violent, and occurring for over the next several days, suggesting aftershocks. Although the magnitude and epicenter are unclear, by comparing these descriptions with more recent events, the quake may have been similar to the M 6.4 1933 Long Beach or the M 5.9 1987 Whittier Narrows earthquake (https://geologycafe.com/california/pp1515/chapter6.html#history).\n\nThe exploring party, personally uninjured and unimpeded in this M 5–6 earthquake event, noted not that the region portended high seismic hazard and landslide risk, but instead benignly rather that it appeared to be a good place for agriculture!\n\n“Earthquakes do not kill people, buildings do!” is a long-time refrain in the world of seismic hazard preparedness and earthquake engineering or do they? While inadequately designed and poorly constructed buildings, infrastructure and lifeline systems can kill people (Gere and Shah, 1984; Bilham, 2009), tsunamis and landslides are directly triggered earthquake phenomena that tragically do kill people, as well!\n\nTherefore, for reliably assessing the hazard and estimating the risk that a population is exposed to, one needs to know the possible distribution of earthquakes large enough to produce a primary damage state. The global map of the maximal magnitude (Mmax) observed during the last 57 years, as portrayed within 2.4° × 2.0° grid cells (Figure 8) could be used for this purpose, as a very rough approximation.\n\nFIGURE 8\n\nEarthquake vulnerability, intensity and disaster\n\nAn earthquake of about M ∼ 5 (Intensity VI on the MMI scale), may cause slight damage (if any) to an ordinary structure located nearby the epicenter; and therefore, cannot produce any significant loss. On the other hand, a strong earthquake (M 6.0–M 6.9) may result in a real disaster — as has happened on several occasions in the past. See e.g., the M 6.3 L’Aquila Earthquake of 6 April 2009 (Alexander, 2010).\n\nFor example, the 21 July 2003, M 6.0 Yunnan (China) earthquake and induced landslides: i) destroyed 264,878 buildings; ii) damaged 1,186,000 houses; iii) killed at least 16 people; and iv) injured 584. Moreover, v) a power station was damaged; vi) roads were blocked; and vii) 1,508 livestock were killed in the province. The resulting damage due to direct and indirect losses (consequences) of this earthquake was estimated at ∼ 75 million United States dollars. So, this is a rare case when a shallow M 6 earthquake (one at the fringe of the smallest threshold of potentially hazardous earthquakes) occurred at both a location and also at 23:16 local time that together unfortunately combined maximum Vulnerability × Exposure of the province (i.e., slopes prone to failure; buildings, houses, etc., that could not withstand the shaking; dense population at home; and livestock still sheltered in their facilities).\n\nHalf of a clock face on Modenesi’s Towers of Finale Emilia, Ferrara, Italy (Figure 9) — destroyed following an earthquake and aftershocks May 20–29, 2012. Felt Intensities exceeded VII, as depicted on the clock face after the main shock. It was the first strong earthquake “anywhere nearby” since the Ferrara quake of 1570. The relatively small number in only 7 fatalities, when a strong and unusually shallow M 6 earthquake struck the Emilia Romagna region of northern Italy, is connected with the event’s occurrence time at just after 4 a.m. — fortuitously very early on that Sunday morning 20 May 2012 — on account of the fact that “the affected region is home to countless historic churches, castles, and towers — many of which were damaged or toppled.” With so many vulnerable churches collapsed or severely damaged, an origin time in the late morning might have easily claimed hundreds of victims from worshipers participating in religious ceremonies (Panza et al., 2014).\n\nFIGURE 9\n\nThe world’s deadliest earthquakes since 2000\n\nTable 1 lists all eighteen of the World’s deadliest earthquakes since the year 2000 — where the number of fatalities in each case exceeded one thousand. Remembering the earlier comment by Ellsworth (1990), i.e., that “earthquakes are complex physical processes generated by sudden slip on faults, and as such they can only be grossly characterized by simple concepts.” — we note that the magnitude of any one of these disastrous events has a poor correlation with the loss of lives: i) the two deadliest earthquakes, namely, the M 9.0 Indian Ocean disaster of 2004, and the M 7.3 Haiti earthquake of 2010, differ in seismic energy by a factor >350 — but resulted in roughly the same death tolls of above 200,000 people; while ii) the death toll of a later occurring M 9.1 mega-thrust earthquake and tsunami off the coast of Tōhoku (Japan) in 2011 was 2 times lower than that for the strong crustal earthquake of only M 6.6 in Bam (Iran). See also (Bela, 2014) and references therein.\n\nTABLE 1\n\nThere is one single case showing a negative value ΔI0 that refers to the smallest of these 18 deadliest earthquakes: the M 6.1 earthquake that struck Hindu Kush (Afghanistan) on 25 March 2002 causing 1,000 + fatalities. A larger M 7.4 deep earthquake (at 200 + km depth) and at distance greater than 150 km, occurred within less than a month on 03.03.2002, causing at least 150 fatalities. The last column in Table 1 shows the difference ΔI0 between the real Macroseismic Intensity I0 EVENT and that predicted by the GSHAP Map I0 GSHAP. These computed ΔI0 = I0 EVENT − I0 GSHAP values are (in all but one case) positive — with their median value of 2.\n\nSeismic hazard mapping\n\nAn accurate characterization of seismic hazard at local scale requires use of detailed geologic maps of both active faults and past earthquake epicenter determinations. The typical seismic hazard assessment undertaken strives to provide a preventive determination of the ground motion characteristics that may be associated with future earthquakes — at regional, local, and even urban scales (Panza et al., 2013).\n\nThe first scientific seismic hazard assessment maps were deterministic in scope, and they were based on the observations that primary damage: i) decreases generally with the distance away from the earthquake source; and ii) is often correlated with the physical properties of underlying soils at a particular site, e.g., rock and gravel. In the 1970s, after publication of Engineering Seismic Risk Analysis by Alin Cornell (1968), the development of probabilistic seismic hazard maps became first fashionable, then preferred, and finally “required” — so that in the 1990s the probabilistic mapping of seismic hazard came to prevail over the heretofore deterministic cartography. For a chronologic history (and a Bibliographic Journey of that history), see in particular (Nishioka and Mualchin, 1996; 1997; Hanks, 1997; Bommer and Abrahamson, 2006; McGuire, 2008; Mualchin, 2011; Panza and Bela, 2020, especially Supplementary Material therein).\n\nGlobal Seismic Hazard Assessment Program (GSHAP) 1992–1999\n\nIn particular, a widespread application of PSHA began when the Global Seismic Hazard Assessment Program (GSHAP) was launched three decades ago in 1992 by the International Lithosphere Program (ILP) with the support of the International Council of Scientific Unions (ICSU), and also endorsed as a demonstration program within the framework of the United Nations International Decade for Natural Disaster Reduction (UN/IDNDR). The GSHAP project terminated in 1999 (Giardini, 1999) with publication of the final GSHAP Global Seismic Hazard Map (Giardini et al., 1999).\n\nHowever, following a number of publications critical of the PSHA technoscience paradigm, e.g., (Krinitzsky, 1993a; Krinitzsky, 1993b; Krinitzsky, 1995; Castanos and Lomnitz, 2002; Klügel, 2007; see also Udias, 2002), and pivotably the catastrophic 2010 Haiti earthquake, a systematic comparison of the GSHAP peak ground acceleration (PGA) estimates with those related to the actual earthquakes that had occurred disclosed gross inadequacy of this “probabilistic” product (Kossobokov, 2010a). The discrepancy between: a) the PGA on the GSHAP map; and b) accelerations at epicenters of 1,320 strong (M ≥ 6.0) earthquakes that happened after publication of the 1999 Map appeared to be a disservice to seismic zonation and associated building codes adopted in many countries on both national or regional scale (see Bommer and Abrahamson, 2006 and references therein). For fully half of these earthquakes, the PGA values on the map were surpassed by 1.7 m/s2 (0.2 g) or more within just 10 years of publication, which fact (of exceeding more than 50% of the PSHA hazard map values within just 10 years) evidently contradicts the GSHAP predicted “10% chance of exceedance in 50 years” — for the ground motion contours displayed on the map.\n\nThese problematic GSHAP results were naturally reported to a wide geophysical community at the Euroscience Open Forum (ESOF 2010) session on “Disaster prediction and management: Breaking a seismo-ill-logical circulus vitiosus”, and also at the Union sessions of the Meeting of the Americas and the American Geophysical Union (AGU) 2010 Fall Meeting (Kossobokov, 2010b; Kossobokov, 2010c; Kossobokov and Nekrasova, 2010; Soloviev and Kossobokov, 2010); and later at the EGI Community Forum (EGICF12) by Peresan et al. (2012a). Then, with finally the 11 March 2011, Mw 9.0 Tōhoku mega-earthquake and tsunami disaster, it became absolutely clear that the GSHAP Probabilistic Seismic Hazard Analysis—despite parascientific apologetics of its “legacy” advocated by Danciu and Giardini (2015) — is UNACCEPTABLE FOR ANY KIND OF RESPONSIBLE SEISMIC RISK EVALUATION AND KNOWLEDGEABLE DISASTER PREVENTION (Kossobokov and Nekrasova, 2012). See, e.g., (Wyss et al., 2012; Mulargia et al., 2017).\n\nUnsurprising surprises\n\nWhile, “like Sumatra in 2004, the power of the Tōhoku earthquake in 2011 took us by surprise (Wang, 2012)”, and made us question: “After decades of scientific research, how well or how badly are we doing in understanding subduction earthquakes?” ... In retrospect, the Tōhoku earthquake and its tsunami were consistent with what we had learned from comparative studies of different subduction zones–and therefore, “despite its wrenching pain”, the cascading 2011 Tōhoku Mw 9.0 Megathrust Earthquake — Tsunami — Fukushima Disaster (from both an earth and tsunami science perspective) was an “Unsurprising Surprise!”\n\nThe last column in Table 1 shows the difference ΔI0 between the real Macroseismic Intensity I0 EVENT and that predicted by GSHAP I0 GSHAP, which values are all (but one case) positive with average and median values of about 2! The same holds as well for all M ≥ 7.5 earthquakes, including the most recent 6 February 2023 coupled earthquakes in Turkey. This underestimation by two units on MMI scale can mean an event experience of “severe damage in substantial buildings with partial collapse” instead of a GSHAP forecast “highly likely” intensity of “slight damage to an ordinary structure.”\n\nMoreover, it should be noted that, in common sense, such a poor performance of the GSHAP product could have already been found at the time of its 1999 publication, and this should have been done by the contributors to the Program as the first order validation test of the GSHAP final map! The claim of a 10% chance of exceedance in 50 years is violated already in 1990–1999 for more than 40% of 2,200 strong M ≥ 6.0, for 94% of 242 significant M ≥ 7.0, and 100% for major M ≥ 7.5 earthquakes (Kossobokov and Nekrasova, 2012)! Note also that GSHAP directly overlapped the time when it was openly realized and discussed by the earthquake engineering community that “10% probability of exceedance in 50 years was too risky for a life-safety criterion” in the United States Building Codes (Frankel et al., 1996) — because earthquake-resistant design standards, when scaled to 10% in 50 years hazard curve ground motions, were insufficient protection when damaging major and great earthquakes did inevitably occur!\n\nSynthetic seismograms: Increasing the reliability of seismic hazard assessment\n\nOn the other hand, with our current knowledge of the physical processes of both earthquake generation and seismic wave propagation in anelastic attenuating media, we can increase the reliability of seismic hazard assessments by basing them instead on computation of synthetic seismograms — in terms of a more realistic modeling of ground motion (see e.g., Panza, 1985; Fäh et al., 1993; Panza et al., 1996; Panza et al., 2001; Panza and Romanelli, 2001; Paskaleva et al., 2007; Peresan et al., 2012b).\n\nNDSHA, which is immediately falsifiable by the occurrence of a damaging event with magnitude exceeding the predicted threshold, has so far been validated in all regions where hazard maps prepared under its methodology have existed at the time of later strong or larger occurring earthquakes. PSHA, by providing a minimum ground motion that has a commonly 10% or 2% chance of exceedance in 50 years in its hazard model, is therefore not falsifiable at the occurrence of any single event that far exceeds this minimum ground motion value, as shown in Table 1.\n\nFurthermore, such ambiguity (authoritatively calculated and endorsed) also provides a legal shield against both “unsurprising surprises”, as well as any responsibilities for ensuring satisfactory outcomes to civil populations for any such presumed unlikely and rare events–on the part of administrators, politicians, engineers and even scientists (e.g., “the L’Aquila Trial,” as previously mentioned).\n\nFinally, there are existing algorithms for the diagnosis of “times of increased probability” (TIP) that have also been proven reliable in the long-lasting and on-going earthquake prediction experiment that began in 1985 (Kossobokov et al., 1999; Kossobokov and Shebalin, 2003; Kossobokov, 2013) and these can deliver an intermediate-term Time and middle-range Space component to the newer Neo-Deterministic NDSHA approach in a more targeted public-safety centered evaluation of seismic hazard (Peresan et al., 2011; Peresan et al., 2012a). In some cases, additional geophysical observations can further help in reducing the spatial uncertainty to the narrow-range about tens of kilometers, e.g., (Crespi et al., 2020).\n\nAdvanced seismic hazard assessment\n\nThe results by Wyss et al. (2012) regarding “Errors in expected human losses due to incorrect seismic hazard estimates” are well in line with the two Topical Volumes Advanced Seismic Hazard Assessment edited by Panza et al. (2011), which supply multifaceted information on the modern tools for Seismic Hazard Assessment (SHA).\n\nThe contributors to these special issues make clear the significant differences between hazard, risk, hazard mitigation, and risk reduction (Klügel, 2011; Peresan et al., 2011; Wang, 2011; Zuccolo et al., 2011), which are of paramount importance as the critical arguments toward revising fundamentally our present existing hazard maps, risk estimates, and engineering practices.\n\nAll ideas have consequences. Therefore, any Standard Method must be Reliable in the first place! That is, it must be: a) good; b) right; and c) true! It must consider: i) the fragility of the local built environment; ii) soil conditions; and iii) furnish now far more informative risk/resiliency assessments of cities and metropolitan territories (Paskaleva et al., 2007; Trendafiloski et al., 2009). The consequences of the Maximum Credible Earthquake (MCE) should be the criteria for Reliable Seismic Hazard Assessment (RSHA), because “what can happen” is a more important consideration than “what gets approved” based on a hazard model (see again Kanamori, 2014, 2021). Incidentally, we note that MCE as practiced in NDSHA (per Rugarli et al., 2019) supplies for Japan an enveloping magnitude M 9.3.\n\nBackward into the future!\n\nIn spite of both: i) the numerous evidenced shortcomings of PSHA (see Stein et al., 2012; Wyss and Rosset, 2013 for a comprehensive discussion); and ii) its unreliable and poor performances — PSHA (emboldened now by 50 years of dangerous “sincere ignorance and conscientious stupidity”) is still widely applied at regional and global scale “to continue the vision of a global seismic hazard model” (Danciu and Giardini, 2015; Gerstenberger et al., 2020; Meletti et al., 2021). Regretfully, the Global Earthquake Model project (GEM, http://www.globalquakemodel.org/) is evidently still on the preferred “circulus vitiosus” — a situation in which the solution to a problem creates another problem. Recently, the GEM Foundation released Global Seismic Hazard Map (version 2018.1) that depicts the geographic distribution of the PGA “with a 10% probability of being exceeded in 50 years” and makes the same fatal errors of the GSHAP 1999 PSHA map — see also “Development of a global seismic risk model” (Silva et al., 2020).\n\nIn the recent AGU Reviews in Geophysics article, entitled “Probabilistic Seismic Hazard Analysis at Regional and National Scales: State of the Art and Future Challenges,” Gerstenberger et al. (2020): i) keep advocating the evident misuse of statistics by attributing any exposure of the fundamental flaws of PSHA (e.g., Castanos and Lomnitz, 2002; Klügel, 2007; Mulargia et al., 2017; Stark, 2017, 2022; Panza and Bela, 2020, etc.) to “subjective experts’ judgment”; and ii) keep ignoring both — a) the systematic failures-to-predict the magnitude of exceedance (Kossobokov and Nekrasova, 2012; Wyss et al., 2012; Wyss and Rosset, 2013; Wyss, 2015); as well as b) the already two-decades-long existing and much more reliable alternative of the neodeterministic approach (Panza et al., 2012; Panza et al., 2014; Kossobokov et al., 2015a; Nekrasova et al., 2015a; Kossobokov et al., 2015b).\n\nThe PSHA’s “State of the Art and Future Challenges” (which more correctly should have been alternatively released under the technoscience warning label “Reviews in Risk Modeling for Hazards and Disasters”, rather than a true scientific oriented “Reviews in Geophysics”) purposely “sincerely” missed referencing “NDSHA: A new paradigm for reliable seismic hazard assessment” (Panza and Bela, 2020), published online already about 2 months prior to the Gerstenberger et al. (2020) acceptance date (10 January 2020) and ignored as well Advanced Seismic Hazard Assessment, which was published in Pure and Applied Geophysics already 9 years prior (Panza et al., 2011).\n\nFurthermore, Jordan et al. (2014) have referenced (Peresan et al., 2012a), which reference fully reveals the qualities, attributes, and applicability of NDSHA to “Operational earthquake forecast/prediction” with direct attention called in the Abstract to the “very unsatisfactory global performance of Probabilistic Seismic Hazard Assessment at the occurrence of most of the recent destructive earthquakes.” Peresan et al. (2012a, p. 135) also discuss in detail the “Existing operational practice in Italy,” which has been “following an integrated neo-deterministic approach” since 2005.\n\nSupplementary Material in (Panza and Bela, 2020): Bibliographic Journey To A New Paradigm, provides detailed references in their chronologically developing order, so that one can see PSHA and NDSHA publications side-by-side over now more than two decades — as NDSHA effectively “built a new model that made the existing model obsolete!” Finally, one of just a few references critical of PSHA that were surprisingly included by Gerstenberger et al. (2020) did manage to state with absolute clarity: “Reliance on PSHA for decisions that affect public safety should cease” (Wyss and Rosset, 2013)!\n\nSeismic Roulette is a game of chance\n\nSeismic Roulette is a game of chance! It is true that we gamble against our will, but that does not make it any less of a game! Disastrous earthquakes are low-probability events locally; however, in any of the earthquake-prone areas worldwide, they reoccur as “unsurprising surprises” with certainty, i.e., with 100% probability sooner or later! Should we then synchronize our watches and historic clock towers. And then wait for another decade, while “Nature spins the wheel”, to find out that GEM is as wrong as GSHAP?\n\nThe Neo-Deterministic Seismic Hazard Assessment (NDSHA) methodology (Fäh et al., 1993; Panza et al., 2012; Peresan et al., 2012a; Bela and Panza, 2021; Panza and Bela, 2020 and references therein) has demonstrated its abilities to serve as the Standard Method for Reliable Seismic Hazard Assessment (RSHA). NDSHA, proposed some 20 years ago (Panza et al., 1996; Panza and Romanelli, 2001), has proven to both reliably and realistically simulate comprehensive sets of hazardous earthquake ground motions throughout many regions worldwide. NDSHA, in making use of: i) our present-day comprehensive physical knowledge of seismic source structures and processes; ii) the propagation of earthquake waves in heterogeneous anelastic media; and iii) site conditions — effectively accounts for the complex, essentially tensor nature of earthquake ground motions in the affected area. Therefore, NDSHA applications provide realistic synthetic time series of ground shaking at a given place, when the best available distribution of the potential earthquake sources can be used for scenario modelling.\n\nConservative estimates of the maximum credible seismic hazard are obtained when they are based on the actual empirical distribution of earthquake characteristics — supplemented further with i) the existing geologic, tectonic, macro- and paleo-seismic evidence, ii) the results of PREPA, and iii) the implications of USLE, accounting for the local fractal structure of the lithosphere. In fact, USLE allows for a comparison between PSHA and NDSHA by providing reliable estimates of PGA values associated with model earthquakes of maximal expected magnitude within 50 years (Nekrasova et al., 2014; Parvez et al., 2014; Nekrasova et al., 2015a; Nekrasova et al., 2015b; Kossobokov et al., 2020), it has been comparatively demonstrated that the NDSHA maps that use such estimates outscore GSHAP generated PSHA maps in identifying correctly the sites of moderate, strong, and significant earthquakes.\n\nSpecifically, the number of unacceptable errors (when PGA on a hazard map at the epicenter of a real earthquake is less, by factor 2 or greater, than PGA attributed to this earthquake) is several times larger for the GSHAP map than for the NDSHA — USLE derived map (e.g., PGA is 11.4, 1.7, and 2.5 times larger for strong earthquakes in Himalayas and surroundings, Lake Baikal region, and Central China, respectively, than on the GSHAP PGA hazard map). This cannot be attributed solely to the difference of the empirical probability distributions of the model PGA values within a region, although evidently the USLE model favors larger estimates in the Baikal and Central China regions. Note that at the regional scale of investigation, the GSHAP estimates of seismic hazard can be grossly underestimated in the areas of sparse explorations of seismically active faults, such as those to the east of the upper segment of the Baikal rift zone.\n\nEarthquake prediction\n\n“Science has not yet mastered prophecy. We predict too much for the next year and yet far too little for the next ten.”Neil Armstrong(Speech to a joint session of Congress, 16 September 1969)\n\nThe terms “Earthquake Forecast/Prediction” described in this section are focused primarily on Operational Earthquake Forecasting (OEF) and mean: i) first specifying the time, place, and energy (as a rule in terms of magnitude) of an anticipated seismic event with sufficient accuracy/precision to then ii) provide authoritative warning to those responsible for the undertaking of civil preparedness actions intended to: a) reduce loss-of-life and damage to property; and b) mitigate disruption to life lines and social fabric (i.e., harden community resilience).\n\nSome distinguish forecasting as prediction supplemented with probability of occurrence (Allen, 1976; NEPEC, 2016). In common everyday language, however, “forecast” and “prediction” are synonymous to the public when they are referring to earthquake phenomena — at least from a practical awareness and actionable viewpoint. Note, however, that estimates of earthquake probability or likelihood are the result of one’s usually subjective deterministic choice of probability model — which might mislead personal belief away from the actual phenomena under study (Gelfand, 1991).\n\nIn J.R.R. Tolkien’s fantasy adventure “The Hobbit: An Unexpected Journey” — the “necessity of identifying risk in any thorough plans in life” is underscored in making reference to the actual phenomenon: “It does not do to leave a live dragon out of your calculations, if you live near him.”\n\nTherefore, earthquake forecast/prediction is neither an easy nor a straight-forward task (but rather an unexpected journey) and therefore implies both an informed as well as a delicate application of statistics (Vere-Jones, 2001). Whenever the problems are very broad, as they particularly are here in the earthquake realm of geophysics and the earth sciences — it is always very important to distinguish the facts from the assumptions, so that one can fully understand the limitations of the assumptions, as a scientific safeguard against committing the equivalent of geophysical malpractice (see P.B. Stark’s “Thoughts on applied statistics” at https://www.stat.berkeley.edu/users/stark/other.htm).\n\nRegretfully, in many cases of Seismic Hazard Assessment (SHA): from i) time-independent (term-less); to ii) time-dependent probabilistic (PSHA); from iii) deterministic (DSHA, NDSHA); to also iv) Short-term Earthquake Forecasting (StEF) — since the claims of a high potential success of the prediction method are based on a flawed application of statistics, they are therefore hardly suitable for communication to responsible decision makers.\n\nMaking SHA claims (either time-independent or time-dependent) quantitatively probabilistic in the “frames” of the most popular objectivists’ viewpoint on probability (i.e., objective chance) — requires a long series of “yes/no” trials, which however cannot actually be obtained without an extended and rigorous testing of the method predictions against real (live) observations. Moreover, as pointed out by Stark (2017), (2022), the distinction between ‘random’, ‘haphazard’, and ‘unpredictable’ is crucial for scientific inference and applications in practice (see, e.g., Chipangura et al., 2019).\n\nPredicting the unpredictable\n\nBy the 1980s, the lithosphere of the Earth was recognized as a complex hierarchically self-organized non-linear dissipative system, with critical phase transitions manifested through larger earthquakes (Keilis-Borok, 1990) (see also more recent Wang et al., 2018; Bedford et al., 2020). Mathematically, the characteristics of such haphazard, apparently chaotic systems are nevertheless predictable up to a certain limit, and after substantial averaging (e.g., as in the abovementioned Keilis-Borok et al., 1984). Therefore, a “success” in forecasting disastrous earthquakes necessarily implies a successive step-by-step determination that narrows down the time interval, location area, and magnitude range of any incipient earthquake.\n\nSo far, none of the proposed short-term precursory signals have shown sufficient evidence to be used as a reliable precursor ahead of large impending earthquakes (Wyss, 1991; Wyss, 1997; Sornette et al., 2021). For example, when testing the West Pacific short-term forecast of earthquakes with magnitude MwHRV ≥5.8 of Jackson and Kagan (1999) against the catalog of earthquakes in the period from 10 April 2002 through 13 September 2004, the conclusion was drawn by Kossobokov (2006b) that the underlying method could be used for prediction of aftershocks, while it however does not outscore Seismic Roulette random guessing — when main shocks are considered. Note that the attribute “short-term” in (Jackson and Kagan, 1999) appears rather misleading, because even for reasonably small values of an alerted space-time volume, some places can remain at “short-term” alert for years.\n\nShort-Term Earthquake Probability (STEP)\n\nThe unfortunately poor performance of another Short-Term Earthquake Probability (STEP) model (based on earthquake clustering) by Gerstenberger et al. (2005) could have been anticipated before making operational the United States Geological Survey web site service, as well as the solicited publication announcement in Nature; Kossobokov (2005); Kossobokov (2006a), based on the 15 years of seismic record statistics provided in “Real-time forecasts of tomorrow’s earthquakes in California” by Gerstenberger et al., presented a “half-page proof” that suggests rejecting with confidence above 97% “the generic California clustering model” used in calculation of forecasts of expected ground shaking for tomorrow. The poor performance of STEP was eventually later further confirmed by Kossobokov (2008), because: in 1,060 days operation of the real-time forecasting, the five earthquakes of MMI ≥ VI in California occurred alternatively in the areas of the web-site’s lowest-risk (about 1/10,000 or less), while the extent of the observed areas of intensity VI for these events (about 100 cells in total) is by far less than the model expected value (about 850 cells). “A website, showing daily ground-shaking probabilities in California, was subsequently removed because of coding problems” (Cartlidge, 2014).\n\nAccuracy of short-term earthquake forecast/prediction tools\n\nRecently Zhang et al. (2021) apply the Error Diagram (Molchan, 1997; Molchan, 2003), weighted by an analogue of Seismic Roulette, to conclude that the correspondence between earthquakes and thermal infrared (TIR) anomalies, widely observed in decades of satellite imagery, is proven “absent” — because the claims for such are based on a uniform distribution of epicenters that they judge to be inadequate for an appropriate measure of the alarm [see also: Preface; Editors’ Introduction discussion on “Statistical Models and Causal Inference” in (Collier et al., 2010)].\n\nIn their reviews of the state of knowledge in the field of earthquake forecast/prediction — neither Jordan et al. (2011), nor Sornette et al. (2021) found any reliable methods for a short-term Operational Earthquake Forecasting (OEF) — but acknowledge that the deterministic pattern recognition approach is reliably efficient at: i) intermediate-term time scale of a few years; and ii) middle-range distance encompassing areas of a few target earthquake-source dimensions in diameter (see also Kossobokov, 2006a; Kossobokov, 2014; Ismail-Zadeh and Kossobokov, 2020; Kossobokov and Shchepalina, 2020).\n\nFor three decades the deterministic earthquake prediction algorithms have made use of clustering in seismic sequences, observable at different magnitude-space-time scales. For example, the M8 Algorithm (Keilis Borok and Kossobokov, 1990; Healy et al., 1992; Ismail-Zadeh and Kossobokov, 2020) diagnoses the “Time of Increased Probability” (TIP) from a multi-parametric analysis of a local system of blocks-and-faults in the traditional “phase space” of rate and rate differential — supplemented with earthquake-specific measures of earthquake source concentration and clustering. However, the M8 algorithm does not provide probability value, but rather a pattern of its increase above the level sufficient for efficient prediction of target earthquakes.\n\nAfter these many decades of rigid real time testing of both the validity and reliability of Global M8 predictions (Ismail-Zadeh and Kossobokov, 2020; Kossobokov and Shchepalina, 2020) and Regional CN algorithm predictions for Italy (Peresan et al., 2005) have been confirmed with a high statistical confidence (Peresan, 2018; Kossobokov, 2021).\n\nTherefore, the accuracy of these forecasting/prediction tools is sufficiently proven for efficiently undertaking precautionary civil earthquake preparedness measures. The theoretical framework for optimization of disaster preparedness measures, undertaken in response to reliable earthquake forecast/prediction, was developed under supervision of Leonid V. Kantorovich, the 1975 Nobel Laureate in Economic Sciences (Kantorovich et al., 1974; Kantorovich and Keilis-Borok, 1991).\n\nDavis et al. (2012) have shown further that prudent and cost-effective actions can be wisely undertaken, if the prediction certainty is known but not necessarily high. For example, the huge losses from the 6 Fukushima Nuclear Power Plants were on the order $100 billion, while the preventive costs of raising tsunami wall height, plus that of protective housing for generators to resist the potential flooding, were only about $10 million. As an epilogue (reminiscent of “the L’Aquila Trial”), in 2020 (https://www.theguardian.com/environment/fukushima) a “Japanese court found the government and TEPCO, the operator of the wrecked Fukushima nuclear plants, negligent for failing to take measures to prevent the 2011 nuclear disaster, and ordered them to pay 1 bn yen ($9.5 million) in damages to thousands of residents for their lost livelihoods.” Therefore, taking these preventative actions, as detailed by Davis et al. (2012), in response to the intermediate-term (Time) and middle-range (Space) prediction that was communicated to Japanese authorities in mid-2001 would have been cost-effective for the Fukushima nuclear power plants, even if the prediction had had a whopping 99.99% chance of being false alarm!\n\nTherefore, is a “Likely Impossibility” (or leaving a live dragon out of your calculations), a safe bet on which to place chips in Seismic Roulette 00? a) “In court, the government argued it was impossible to predict the tsunami or prevent the subsequent disaster”; and b) “The court said that the government could have taken measures to protect the site, based on expert assessments available in 2002 that indicated the possibility of a tsunami of more than 15 m.”\n\nOperational earthquake forecasting\n\nIn the Realm of Operational Earthquake Forecasting (OEF), which is “the dissemination of authoritative information about time-dependent earthquake probabilities to help communities prepare for potentially destructive earthquakes” (Jordan et al., 2014), within the broader OEF scheme shown in Figure 10, we believe one should try to use all reliable Geophysical Information, including but not limited to: i) Earthquakes; ii) GPS; iii) Gravity; iv) Electro-magnetic; and v) Geochemical input that might be relevant to origination of damaging ground shaking.\n\nFIGURE 10\n\nThis would allow for a true multi-disciplinary forecast/prediction so much needed in practice. Forecasting Information must be reliable, tested, and confirmed by evidence (Allen, 1976; Kossobokov et al., 2015b), such that the heretofore probabilistic-centered models (which have been focused primarily on “expert opinion” and weighting of different subjective “models of seismicity”) defer now rather to a more collaborative practice of “expert judgement”, which is incorporating all of the above forecasting requirements, and as is more simply illustrated in Figure 11. Ideally, good judgement comes from experience; and not from bad judgement and failed policy disasters, such as experienced at L’Aquila and Fukushima!\n\nFIGURE 11\n\nSeismology and computer science alone are not enough for a successful collaboration aimed at effective forecasting of larger earthquakes. OEF could be either deterministic, probabilistic, or a combination of both in interaction with user needs within the Realm of Risk Analysis and Mitigation. Naturally, the scheme applies as well to other natural hazards and can be further generalized. Note however, that ‘operational’ (in everyday language) means ‘ready to work correctly’; hence, it is obvious that SHA belongs to the OEF Realm as, we believe, the most important part of the OEF user interface shown in Figures 10, 11. See (Peresan et al., 2005; Peresan, 2018) for ‘operational’ forecast/prediction practices in Italy since 2005 and also (Crespi et al., 2020) for a recent example.\n\nPractitioners are positive that any reliable forecasting information can be i) effective, ii) complementary to design and construction of seismically resistant buildings and infrastructure, and iii) well appreciated by the population as a timely precautionary reminder and timely warning (Mileti and Fitzpatrick, 1992; Kossobokov et al., 2015b). Obviously, the spectrum of doable low-key preparedness options increases in the case of longer rather than short-term warnings. Theoretically speaking, while decision-makers should be aware of the full broad spectrum of possible actions, following general strategies of response to predictions by escalation or de-escalation of safety measures (depending on expected losses and magnitude-space-time accuracy of reliable forecasting), lives can only be saved (and more legal trials avoided) if buildings and infrastructures can withstand the shaking!\n\nEarthquake preparedness should not fluctuate on daily or weekly basis\n\nTherefore, per Wang and Rogers (2014), “Earthquake Preparedness Should Not Fluctuate on a Daily or Weekly Basis” from both public safety, as well as effective public messaging concerns. They say “although fully appreciating the noble intention of OEF and the scientific merits of the seismicity analyses it employs, we are concerned that its wide promotion may lead the public to believe that earthquake preparedness can fluctuate at timescales of days or weeks,” and we “question the claim that it should and can be made operational,” because:\n\n(1) Where OEF is based largely on clustering or potential foreshock sequences, it is not reliable, as the “majority of damaging earthquakes are not preceded by anomalous foreshocks.”\n\n(2) “The most objective measure of the usefulness of a short-term forecast is whether it can guide pre-seismic evacuation of unsafe buildings.”\n\n(3) But providing probability forecasts in percentages from very low to even 20% or 40% (and with authoritative uncertainties) may not mean much to a public that only dichotomizes Risk: Yes or No! (like “Shall I carry an umbrella today, or not?”).\n\n(4) “Crying earthquake is a potent way of blunting earthquake awareness and preparedness,” as well as disrupting the economy.\n\nSince the most sensible and reliable way to mitigate the hazard posed by unsafe and killer buildings requires that the public, the government, and society “make every effort to retrofit or replace an unsafe building to comply with earthquake resistance provisions,” while this is not simple, Wang and Rogers believe the scientific community should first and foremost help society to deal with these challenges, and not just champion short-term alternatives focusing on evacuations (i.e., “the practice of continual updating and dissemination of physics-based short-term (days) probabilities for the occurrence of damaging earthquakes”) to that specific evacuation end.\n\nReflecting on the damage and fatalities in L’Aquila, they note that the relevant questions to ask regarding reliable mitigation strategies are:\n\nWhy did those buildings collapse? What could have been done better in designing and implementing building codes? How should retrofitting regulations and practices be improved to reduce the vulnerability of old buildings? How can the methods of developing seismic-hazard models be further improved?\n\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n\nDuring this type of crisis, the scientific community should step up to guide public and government attention to the relevant questions asked above. It is our concern that their attention was guided farther away from these questions by the report of the International Commission on Earthquake Forecasting for Civil Protection (Jordan et al., 2011), which, in our view, incorrectly concludes that the L’Aquila incident demonstrated the need for OEF.\n\n“Society’s best strategy against the consequence of earthquakes,” they say, “is to focus on making the built environment earthquake resistant.” Naturally, when then Nature does spin the wheel, you can bet on that!\n\nEarthquake prediction in practice: Success and failure\n\nAs an example, the success in predicting in practice the devastating 1975 Haicheng earthquake (Zhang-li et al., 1984) remains the sole case of successful OEF decision-making interaction between scientists and administrators. A “lucky” intermediate-term (Time) guess: i) readied the province for an incipient large earthquake; ii) was followed by an escalation of civil preparedness from low-key actions at low-level alert to short-term monitoring of multidisciplinary observations; and iii) ultimately culminated with “red alert” status! — and evacuation of the city of Haicheng was ordered by Chinese officials early in the morning of February 4th — thereby saving most of approximately one million residents from consequences of the devastating M 7.5 shock that hit the area in the evening hours. Although 1,328 people nevertheless died, over 27,000 were injured, and thousands of buildings collapsed, the number of fatalities is just about 1% of the estimate, if the evacuation had not taken place.\n\nIn deadly contrast, the M 7.8 Great Tangshan Earthquake of 28 July 1976 was sadly “the greatest earthquake disaster in the history of the world”, and occurred with no warning (Huixian et al., 2002). It was generated by a fault running directly through the middle of the city, which is located in the extreme NE region of China abutting the iconic (and fractal) Bohai Bay indentation of the Chinese coastline. “Although the building code had seismic design requirements, Tangshan was in a zone requiring no earthquake design.” Therefore “red alert” timely evacuation would have been the one-and-only successful action for OEF!\n\nNotably: i) 85% of the buildings in Tangshan either collapsed or were “so seriously damaged as to be unusable; ” ii) infrastructure and agriculture were also seriously damaged as well; iii) the shock hit around 4:00 a.m. when the population was mostly at home asleep in vulnerable structures having no earthquake resistance; iv) the extreme intensity XI on MMI scale and subsequent aftershocks caused officially 242,419 victims (Huixian et al., 2002) (this death toll number placing The Great Tangshan Earthquake atop the list of deadliest earthquakes in a century).\n\nTherefore, in contrast to the very successful 1975 Haicheng earthquake forecast/prediction case, The Great Tangshan Earthquake turns out be a “cold shower” of disillusionment on both the Risk Umbrella and the presumed reliability of OEF, demonstrating particularly that consideration of MCE as practiced in NDSHA, per Rugarli et al. (2019), has to envelope all public safety considerations over and above, we believe, probability model perturbations of OEF responses.\n\nIt does not do to leave a live dragon out of your calculations\n\nThe Caltech EERL 2002–001 Report on The Great Tangshan Earthquake shows what can happen when an unexpected earthquake strikes an unprepared city, and it makes clear the need for earthquake preparedness even if the (subjectively modeled) probability of an earthquake is presumed to be low (Huixian et al., 2002).\n\nDecades later, a golden opportunity was tragically lost to advise the citizens of L'Aquila, Italy on low-key safety measures, when Commisione Grandi Rischi (CGR), or Grand Risk Commission, issued their politicized statement on 31 March 2009 — because at this time the situation was favorable for adequate reaction of the public to prudent (understandable, believable, and personal) information on the “increased probability” of an impending strong earthquake (see e.g., Mileti and Fitzpatrick, 1992; Marincioni et al., 2012; Wachinger et al., 2013).\n\nThe local authorities and emergency management agencies, in particular, were also ready to act in advance, but were advised to the contrary by CGR to do nothing! From a strictly scientific viewpoint, an adequate evaluation of the seismic crisis in the Abruzzo region of Central Italy east of Rome on 31 March 2009 could have actually provided a much more reliable forecast/prediction than the previous and celebratory 1975 Haicheng success story! — if seismological knowledge had not been misspresented by CGR:\n\n(I) According to scientific studies by the CGR members (Boschi et al., 1995; Dolce and Martinelli, 2005) L'Aquila was: a) at the highest seismic risk in Italy in the near future; and b) buildings in medieval L'Aquila were extremely vulnerable.\n\n(II) Therefore, given the November 2008 — March 2009 apparently unrelenting seismic activity shaking the Abruzzo region (culminating in March 2009, when over 100 tremors occurred in the vicinity of L’Aquila), any responsible scientific body should not have asked local people to “calm down and relax,” but should instead have claimed: a) evident “increase of seismic risk” within the area; and also advised b) raising the “alert level” from background “green” to “yellow”, at least, or even “orange”. Subsequently it has been shown that this cluster of seismic activity that was so alarming to the population was a foreshock sequence foreshadowing the M 6.3 L’Aquila main shock (Papadopoulos et al., 2010).\n\nCommunicating their already existing scientific knowledge “as is” and unabridged could have led to saving the lives of a significant part of the 309 people who perished under the rubbles of the devastating M 6.3 L'Aquila earthquake on 06 April 2009 at 03:32 a.m. Many victims would not have returned back to their homes to sleep, but would have instead remained outside their houses for the rest of the night, following the two premonitory tremors of M3.9 (2009/04/05 22:48 CET) and M3.5 (2009/04/06 00:39 CET) which preceded the fatal M6.3 (2009/04/06 03:32 CET) main shock by just 3 hours. See in particular ‘Voices from the seismic crater in the trial of The Major Risk Committee: a local counternarrative of “the L’Aquila Seven”’ by Pietrucci (2016).\n\n(1) On 22 October 2012 the Court of L’Aquila found all seven CGR members, who had been convened in L'Aquila on 31.03.2009 with “the aim of providing the citizens of Abruzzo with all the information available to the scientific community on the seismic activity of the last few weeks,” guilty of negligence, imprudence, and inexperience by their actions in providing incomplete information to the National Department of Civil Protection, to the Abruzzo Region Councilor for Civil Protection, to the Mayor of L'Aquila, and to the citizens of L'Aquila that had resulted in the deaths of people.\n\nThus, and what was commonly misunderstood at the time, the guilty parties were convicted neither for failing to forecast the earthquake, nor for failing to advise evacuation of the city. Rather they were convicted for having provided ‘inaccurate, incomplete, and contradictory information’ about the ongoing seismic activity and therefore undermining the safety of the population (see e.g., https://www.slideshare.net/dealexander/reflections-on-the-trial-of-the-laquila-seven).\n\nTheir providing of this “incomplete information” was as a result of: a) the statements made to the media; and b) the CGR draft report — both of which were: i) imprecise and contradictory on the nature, causes, danger and future developments of the seismic activity in question; ii) also in violation of the general legislation of the Law regarding the “discipline of information and communication” activities of public administrations at the time of said meeting; and iii) only approximate, generic and ineffective in relation to the activities and duties of “forecasting and prevention” (Alexander, 2014). In the Italian three-part legal system, after the Court of L’Aquila guilty verdict 1) above; 2) later the Court of Appeal acquitted six scientists; and 3) the Supreme Court ultimately confirmed this ruling.\n\nIt cannot be ruled out, however, that the CGR meeting on 31 March 2009 was convened with the explicit goal to calm down the disquieted public from both the ongoing seismic activity, and also the warnings of an amateur earthquake prediction scientist (Alexander, 2014; Imperiale and Vanclay, 2019). And the questions of whether scientists were used or “captured” (allowing their knowledge to be misused) or also complicit (by not taking action to correct misinformation that was the equivalent of geophysical malpractice) remains both itchy, as well as worrisome.\n\nMore details about so-called “L'Aquila Trial” (or trial of “the L’Aquila Seven”), and the political crisis in science it spawned, are given by Panza and Bela (2020) and Supplementary material therein. Information regarding the “AGU Statement: Investigation of Scientists and Officials in L'Aquila, Italy, Is Unfounded'' (including Comment and Reply) — are in (Dobran, 2010; Wasserburg, 2010); see also (Stark and Saltelli, 2018).\n\nCN prediction experiment in Italy: 8 target earthquakes\n\nSince the beginning of the real-time CN prediction experiment in Italy in July 2003 (Peresan et al., 2005), only 2 events (out of 8 target earthquakes) were missed. Namely, the 24 November 2004, M 5.5, Salò earthquake in southern Alps, Northern region (Figure 12) and the 6 April 2009, M 6.3, L’Aquila earthquake in the central part of the Apennines, Central region. The L’Aquila earthquake scored as a “failure to predict” just because its epicenter was located about 10 km outside the alarm territory of Northern region identified by CN algorithm for the corresponding time window (Peresan et al., 2011).\n\nFIGURE 12\n\nThe situation with the Salò earthquake, which again occurred within the Northern Region, was quite different. In fact, fully three target earthquakes hit the same region in a row, respectively those of September 2003 July 2004, and November 2004. Both the first and second earthquakes were correctly predicted by CN algorithm within the alarm window beginning in May 2001, but according to the protocols of the prediction experiment, however, when the seismic energy release within the alarm region surpassed the preset sufficiently high threshold after the second, July 2004 earthquake, the alarm was automatically terminated — thus resulting in a “failure to predict” for the third November 2004 Salò earthquake (Peresan, 2018).\n\nDiscussion and conclusions\n\n“Nothing is less predictable than the development of an active scientific field.”Charles Francis Richter\n\nCharles Richter, whose critical observation that “only fools and charlatans predict earthquakes” is often cited (see e."
    }
}
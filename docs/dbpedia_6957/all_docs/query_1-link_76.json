{
    "id": "dbpedia_6957_1",
    "rank": 76,
    "data": {
        "url": "https://ferretj.github.io/",
        "read_more_link": "",
        "language": "en",
        "title": "Bio",
        "top_image": "",
        "meta_img": "",
        "images": [
            "https://ferretj.github.io/me2021_redux.jpg"
        ],
        "movies": [],
        "keywords": [],
        "meta_keywords": [
            ""
        ],
        "tags": null,
        "authors": [
            "Johan Ferret"
        ],
        "publish_date": null,
        "summary": "",
        "meta_description": "Deep RL research.",
        "meta_lang": "en",
        "meta_favicon": "/favicon.ico?",
        "meta_site_name": "Johan Ferret",
        "canonical_link": "https://ferretj.github.io/",
        "text": "Hi there! I am a Research Scientist at Google DeepMind. I am doing research on Reinforcement Learning and LLMs and also contributing to large-scale efforts including Bard, Gemini and Gemma.\n\nPrior to that, I did my PhD at Google Brain and Inria Lille (Scool team, ex-SequeL). I worked on Reinforcement Learning, with a focus on credit assignment and interpretability. My advisors were Olivier Pietquin and Philippe Preux. I also collaborated with Matthieu Geist.\n\nMy PhD thesis is available for consultation (manuscript, slides).\n\nPrior to my PhD, I got an engineering degree in Computer Science and Applied Mathematics from Télécom Paris, and an M.Sc. in Machine Learning from École Polytechnique. I then worked for two years as a Research Engineer at DreamQuark.\n\nOutside of work, I make generative art (no GANs involved for now!). I also love music, roguelikes, high-intensity interval training and spicy food.\n\nMy CV is available online, and all my publications can be consulted here.\n\nPublications\n\nWARM: On the Benefits of Weight Averaged Reward Models\n\nAlexandre Ramé, Nino Vieillard, Léonard Hussenot, Robert Dadashi, Geoffrey Cideron, Olivier Bachem, Johan Ferret\n\nICML 2024\n\n[ paper ]\n\nRLAIF: Scaling Reinforcement Learning from Human Feedback with AI Feedback\n\nHarrison Lee, Samrat Phatale, Hassan Mansoor, Thomas Mesnard, Johan Ferret, Kellie Lu, Colton Bishop, Ethan Hall, Victor Carbune, Abhinav Rastogi, Sushant Prakash\n\nICML 2024\n\n[ paper ]\n\nA Survey of Temporal Credit Assignment in Deep Reinforcement Learning\n\nEduardo Pignatelli, Johan Ferret, Matthieu Geist, Hado van Hasselt, Olivier Pietquin, Laura Toni\n\nTMLR 2024\n\n[ paper ]\n\nFactually Consistent Summarization via Reinforcement Learning with Textual Entailment Feedback\n\nJohan Ferret*, Paul Roit*, Lior Shani*, Roee Aharoni, Geoffrey Cideron, Robert Dadashi, Matthieu Geist, Sertan Girgin, Léonard Hussenot, Orgad Keller, Nikola Momchev, Sabela Ramos, Piotr Stanczyk, Nino Vieillard, Olivier Bachem, Gal Elidan, Avinatan Hassidim, Olivier Pietquin, Idan Szpektor\n\nACL 2023\n\n[ paper | code soon! ]\n\nOn Actions that Matter: Credit Assignment and Interpretability in Reinforcement Learning\n\nJohan Ferret\n\nPhD thesis\n\n[ manuscript | slides ]\n\nLazy-MDPs: Towards Interpretable Reinforcement Learning by Learning When to Act\n\nJohan Ferret*, Alexis Jacq*, Olivier Pietquin, Matthieu Geist\n\nAAMAS 2022\n\n[ paper | code soon! ]\n\nThere is no Turning Back: A Self-Supervised Approach to Reversibility-Aware Reinforcement Learning\n\nJohan Ferret*, Nathan Grinsztajn*, Olivier Pietquin, Philippe Preux, Matthieu Geist\n\nNeurIPS 2021\n\n[ paper | blog post | slides | code ]\n\nSelf-Imitation Advantage Learning\n\nJohan Ferret, Olivier Pietquin, Matthieu Geist\n\nAAMAS 2021\n\n[ paper | slides | code ]\n\nAdversarially Guided Actor-Critic\n\nJohan Ferret*, Yannis Flet-Berliac*, Olivier Pietquin, Philippe Preux, Matthieu Geist\n\nICLR 2021\n\n[ paper | slides | video | code ]\n\nSelf-Attentional Credit Assignment for Transfer in Reinforcement Learning\n\nJohan Ferret, Raphaël Marinier, Matthieu Geist, Olivier Pietquin\n\nIJCAI 2020\n\n[ paper | slides | video ]\n\nPreprints\n\nWARP: On the Benefits of Weight Averaged Rewarded Policies\n\nAlexandre Ramé, Johan Ferret, Nino Vieillard, Robert Dadashi, Léonard Hussenot, Pierre-Louis Cedoz, Pier Giuseppe Sessa, Sertan Girgin, Arthur Douillard, Olivier Bachem\n\narxiv preprint\n\n[ paper ]\n\nBOND: Aligning LLMs with Best-of-N Distillation\n\nPier Giuseppe Sessa, Robert Dadashi, Léonard Hussenot, Johan Ferret, Nino Vieillard, Alexandre Ramé, Bobak Shariari, Sarah Perrin, Abe Friesen, Geoffrey Cideron, Sertan Girgin, Piotr Stanczyk, Andrea Michi, Danila Sinopalnikov, Sabela Ramos, Amélie Héliou, Aliaksei Severyn, Matt Hoffman, Nikola Momchev, Olivier Bachem\n\narxiv preprint\n\n[ paper ]\n\nConditioned Language Policy: A General Framework for Steerable Multi-Objective Finetuning\n\nKaiwen Wang, Rahul Kidambi, Ryan Sullivan, Alekh Agarwal, Christoph Dann, Andrea Michi, Marco Gelmi, Yunxuan Li, Raghav Gupta, Avinava Dubey, Alexandre Ramé, Johan Ferret, Geoffrey Cideron, Le Hou, Hongkun Yu, Amr Ahmed, Aranyak Mehta, Léonard Hussenot, Olivier Bachem, Edouard Leurent\n\narxiv preprint\n\n[ paper ]\n\nRecurrentGemma: Moving Past Transformers for Efficient Open Language Models\n\nGriffin Team, RLHF Team, Gemma Team\n\nTechnical report\n\n[ paper ]\n\nGemma: Open Models Based on Gemini Research and Technology\n\nGemma Team\n\nTechnical report\n\n[ paper ]\n\nDirect Language Model Alignment from Online AI Feedback\n\nShangmin Guo, Biao Zhang, Tianlin Liu, Tianqi Liu, Misha Khalman, Felipe Llinares, Alexandre Ramé, Thomas Mesnard, Yao Zhao, Bilal Piot, Johan Ferret, Mathieu Blondel\n\narxiv preprint\n\n[ paper ]\n\nGemini: A Family of Highly Capable Multimodal Models\n\nGemini Team\n\nTechnical report\n\n[ paper ]\n\nAcme: A Research Framework for Distributed Reinforcement Learning\n\nAcme Team\n\narxiv preprint\n\n[ paper | colab | code ]\n\nWorkshops\n\nAssessing the Zero-Shot Capabilities of LLMs for Action Evaluation in RL\n\nEduardo Pignatelli, Johan Ferret, Davide Paglieri, Samuel Coward, Tim Rocktäschel, Edward Grefenstette, Laura Toni\n\nAutoRL workshop, ICML 2024\n\n[ paper ]\n\nMore Efficient Exploration with Symbolic Priors on Action Sequence Equivalence\n\nNathan Grinsztajn, Toby Johnstone, Johan Ferret, Philippe Preux\n\nDeep Reinforcement Learning workshop, NeurIPS 2022\n\n[ paper ]\n\nOffline Credit Assignment in Deep Reinforcement Learning with Hindsight Discriminator Networks\n\nJohan Ferret, Olivier Pietquin, Matthieu Geist\n\nEWRL 2022\n\n[ paper ]"
    }
}
{
    "id": "dbpedia_2233_3",
    "rank": 66,
    "data": {
        "url": "https://nlenov.wordpress.com/category/scientific-activity/",
        "read_more_link": "",
        "language": "en",
        "title": "scientific activity",
        "top_image": "https://s0.wp.com/i/blank.jpg",
        "meta_img": "https://s0.wp.com/i/blank.jpg",
        "images": [
            "https://www.ascistance.co.uk/blog/wp-content/uploads/2021/02/Reactiongraph.png",
            "https://www.ascistance.co.uk/blog/wp-content/uploads/2021/02/Model-2.png",
            "https://www.ascistance.co.uk/blog/wp-content/uploads/2021/02/Timecourse-1024x682.png",
            "https://www.ascistance.co.uk/blog/wp-content/uploads/2021/02/Sensitivity_3D-1024x638.png",
            "https://www.ascistance.co.uk/blog/wp-content/uploads/2021/02/Slider-1024x735.png",
            "https://www.ascistance.co.uk/blog/wp-content/uploads/2021/02/Parameter_scan.png",
            "https://www.ascistance.co.uk/blog/wp-content/uploads/2021/02/optimisation.png",
            "https://www.ascistance.co.uk/blog/wp-content/uploads/2021/02/experiment-values.png",
            "https://www.ascistance.co.uk/blog/wp-content/uploads/2021/02/Experiment.png",
            "https://www.ascistance.co.uk/blog/wp-content/uploads/2021/02/Estimation-1.png",
            "https://nlenov.wordpress.com/wp-content/uploads/2020/05/spain-cfr-worldometer.png?w=300",
            "https://nlenov.wordpress.com/wp-content/uploads/2020/05/ifr_tests_10apr.png?w=300",
            "https://nlenov.wordpress.com/wp-content/uploads/2020/05/cfrvsage.jpg?w=300",
            "https://nlenov.wordpress.com/wp-content/uploads/2020/02/arcsinheq.png",
            "https://nlenov.wordpress.com/wp-content/uploads/2020/02/curves-768x552-1.png",
            "https://nlenov.wordpress.com/wp-content/uploads/2020/02/raw-768x552-1.png",
            "https://nlenov.wordpress.com/wp-content/uploads/2020/02/correctbaseline-768x552-1.png",
            "https://nlenov.wordpress.com/wp-content/uploads/2020/02/log-768x552-1.png",
            "https://nlenov.wordpress.com/wp-content/uploads/2020/02/arcsinh-768x552-1.png",
            "https://nlenov.wordpress.com/wp-content/uploads/2019/01/cpm1.png",
            "https://nlenov.wordpress.com/wp-content/uploads/2019/01/1celltypenocorr.png",
            "https://nlenov.wordpress.com/wp-content/uploads/2019/01/cells-1.png",
            "https://nlenov.wordpress.com/wp-content/uploads/2019/01/1celltypecorr.png",
            "https://nlenov.wordpress.com/wp-content/uploads/2019/01/2celltypesnocorr.png",
            "https://nlenov.wordpress.com/wp-content/uploads/2019/01/2celltypeswrongcorr.png",
            "https://nlenov.wordpress.com/wp-content/uploads/2019/01/cells3_1.png",
            "https://nlenov.wordpress.com/wp-content/uploads/2019/01/cells3_2.png",
            "https://nlenov.wordpress.com/wp-content/uploads/2019/01/2celltypesgoodcorr.png",
            "https://nlenov.wordpress.com/wp-content/uploads/2017/12/goertner.png",
            "https://nlenov.wordpress.com/wp-content/uploads/2018/10/image10.png",
            "https://nlenov.wordpress.com/wp-content/uploads/2018/10/image2.png",
            "https://nlenov.wordpress.com/wp-content/uploads/2018/10/image4.png",
            "https://nlenov.wordpress.com/wp-content/uploads/2018/10/image11.png",
            "https://nlenov.wordpress.com/wp-content/uploads/2018/10/image3.png",
            "https://nlenov.wordpress.com/wp-content/uploads/2018/10/image5.png",
            "https://nlenov.wordpress.com/wp-content/uploads/2018/10/image1.png",
            "https://nlenov.wordpress.com/wp-content/uploads/2018/10/image6.png",
            "https://nlenov.wordpress.com/wp-content/uploads/2018/10/image8.png",
            "https://nlenov.wordpress.com/wp-content/uploads/2018/10/image7.png",
            "https://nlenov.wordpress.com/wp-content/uploads/2017/02/citvsif1.png",
            "https://nlenov.wordpress.com/wp-content/uploads/2017/02/citvsifnosbml1.png",
            "https://nlenov.wordpress.com/wp-content/uploads/2017/02/citvsifcorryear1.png",
            "https://nlenov.wordpress.com/wp-content/uploads/2017/02/rcrvsif1.png",
            "https://nlenov.wordpress.com/wp-content/uploads/2017/02/rcrvscit1.png",
            "https://nlenov.wordpress.com/wp-content/uploads/2017/02/bi-h-indexvscit.png",
            "https://nlenov.wordpress.com/wp-content/uploads/2017/02/ranking-post1.png",
            "https://nlenov.wordpress.com/wp-content/uploads/2015/09/percent.png",
            "https://nlenov.wordpress.com/wp-content/uploads/2015/09/excellencedistrib.png",
            "https://nlenov.wordpress.com/wp-content/uploads/2015/09/sorting.png",
            "https://nlenov.wordpress.com/wp-content/uploads/2015/09/mulilevelsorting.png",
            "https://nlenov.wordpress.com/wp-content/uploads/2015/09/percent-cascade.png",
            "https://s2.wp.com/i/logo/wpcom-gray-white.png",
            "https://s2.wp.com/i/logo/wpcom-gray-white.png",
            "https://pixel.wp.com/b.gif?v=noscript"
        ],
        "movies": [],
        "keywords": [],
        "meta_keywords": [
            ""
        ],
        "tags": null,
        "authors": [],
        "publish_date": "2021-02-16T11:31:27+00:00",
        "summary": "",
        "meta_description": "Posts about scientific activity written by NicGambarde",
        "meta_lang": "en",
        "meta_favicon": "https://s1.wp.com/i/favicon.ico",
        "meta_site_name": "Phosphenes",
        "canonical_link": "https://nlenov.wordpress.com/category/scientific-activity/",
        "text": "A crucial part of any computational modelling is getting parameter values right. By computational model, I mean a mathematical description of a set of processes that we can then numerically simulate to reproduce or predict a system’s behaviour. There are many other kinds of computational or mathematical models used in computational biology, such as 3D models of macromolecules, statistical models, machine learning models and more. While some concepts dealt with in this blog post would actually be relevant, I want to limit the scope of this post to what is sometimes called “systems biology” models.\n\nSo, we have a model that describes chemical reactions (for instance). The model behaviour will dictate the values of some variables, e.g. substrate or product concentrations. We call those variables “dependent” (“independent variables” are variables whose values are decided before any numerical simulation or whose values do not depend on the mathematical model, such as “time” for standard chemical kinetics model). Another essential set of values that we have to fix before a simulation consists of the initial conditions, such as initial concentrations.\n\nThe quickest way to get cracking is to gather the variable values from previous models (found for instance in BioModels), from databases of biochemical parameters such as Brenda or SABIO-RK, or from patiently sieving scientific literature. But what if we want to improve the values of the variables? This blog post will explore a few possible ways forward using the modelling software tool COPASI, namely, sensitivity analysis, picking up variable values and looking at the results, parameter scans, optimisation, and parameter estimation.\n\nLoading a model in COPASI\n\nFirst, we need to have a model to play with. The post will use the model of MAPK signalling published by Huang and Ferrell in 1996. You can find it in BioModels where can download the SBML version and import it in COPASI. Throughout this post, I will assume the reader master the basic usage of COPASI (create reactions, run simple simulations, etc.). You will find some introductory videos about this user-friendly, versatile, and powerful tool on their website.\n\nThe model presents the three-level cascade activating MAP kinase. MAPK, MAPKK, and MAPKKK mean Mitogen-activated protein kinase, Mitogen-activated protein kinase kinase, and Mitogen-activated protein kinase kinase kinase, respectively. Each curved arrow below represents three elementary reactions: binding and unbinding of the protein to an enzyme, and catalysis (addition or removal of a phosphate).\n\nThe top input E1 (above) is called MAPKKK activator in the model. To visualise the results, we will follow the normalised values for the active (phosphorylated) forms of the enzymes K_PP_norm, KK_PP_norm and K_P_norm, that are just the sums of all the molecular species containing the active forms divided by the sums of all the molecular species containing the enzymes (NB: Throughout the screenshots, the red dots are added by myself and not part of COPASI’s GUI).\n\nLet’s run a numerical simulation of the model. Below you see the activation of the three enzymes, with the swift and supra-linear activation of the bottom one, MAPK, one of the hallmarks of the cascade (the others being an amplification of the signal and an ultrasensitive dose-response which allows to fully activate MAPK with only a touch of MAPKKK activation).\n\nSensitivity analysis\n\nThe first question we can ask ourselves is “What are the parameters that values affect the most the output of our system?”. To do so, we can run a sensitivity analysis. COPASI will vary a bit all the parameters and measure the consequences of these changes on the results of a selected task, here the steady-state of the non-constant concentrations of species.\n\nWe see that the most important effect is the impact of MAPKKK activator binding constant (k1) on the concentration of PP-MAPK, which happens to be the final output of the signalling cascade. This is quite relevant since the MAPKKK activator binding constant basically transmits the initial signal at the top of the cascade. You can click the small spreadsheet icon on the right to access coloured matrices of numerical results.\n\nTesting values\n\nAll right, now we know which parameter we want to fiddle with. The first thing we can do is visually look at the effect of modifying the value. We can do that interactively with a “slider“. Once in the timecourse panel, click on the slider icon in the toolbar. You can then create as many sliders as you want to set values manually. Here, I created a slider that can vary logarithmically (advised for most model parameters) between 1 and 1 million. The initial value, used to create the timecourse above, was 1000. We see that sliding to 100 changes the model’s behaviour quite dramatically, with very low enzyme activations. Moving it well above 1000 will show that we increase the speed of activation of the three enzymes, increase the activation of the top enzyme, albeit without significant gain on K-PP, our interesting output.\n\nParameter scans\n\nPlaying with sliders is great fun. But this is not very precise. And if we want to evaluate the effect of changing several parameters simultaneously, this can be extremely tedious. However, we can do that automatically thanks to COPASI’s parameter scans. We can actually repeat the simulation with the same value (useful to repeat stochastic simulations), systematically scan parameter values within a range, or sampling them from statistical distributions (and nest all of these). Below, I run a scan over the range defined above and observe the same effect. To visualise the scan’s results, I created a graph that plotted the active enzyme’s steady-state versus the activator binding constant.\n\nOptimisation\n\nAll that is good, but I still have to look at curves or numerical results to find out the best value for my parameter. Ideally, I would like COPASI to hand me directly the value. This is the role of optimisation. Optimisation if made up of two parts: the result I want to optimise and the parameter to change to optimise it. I will not discuss the possibility to optimise a value. There are many cases for which optimisation is just not possible. For instance, it is not possible to optimise the production of phosphorylated MAPK. Whatever upper bound we would fix for the activator binding constant, the optimal value would end up on this boundary. In this example, I decided to maximise the steady-state concentration of K_PP for a given concentration of KKK_P, i.e. getting the most bang for my buck. As before, the parameter I want to explore is the MAPKKK activator binding constant. I fix the same lower and upper bound as before. COPASI offers many algorithms to explore parameter values. Here, I chose Evolutionary Programming, which offers a good balance between accuracy and speed.\n\nThe optimal result is 231. Indeed, if we look at the parameter scan plot, we can see that with a binding constant of 231, we get an almost maximal activation of MAPK with minimal activation of MAPKKK. Why is this important? All those enzymes are highly connected and will act on downstream targets right, left, and centre. In order to minimise side effects, I want to activate (or inhibit) protein as little as necessary. Being efficient at low doses also helps with suboptimal bioavailability. And of course, using 100 times less of the stuff to get the same effect is certainly cheaper, particularly for biologics such as antibodies.\n\nParameter estimation\n\nWe are now reaching the holy grail of parameter search, which is parameter estimation from observed results. As with optimisation, this is not always possible. It is known as the identifiability problem. Using the initial model, I created a fake noisy set of measurements, which would, for instance, represent the results of Western blot or ELISA using antibodies against phosphorylated and total forms of RAF, MEF, and ERK, which are specific MAPKKK, MAPKK, and MAPK.\n\nI can load this file (immuno.txt on the screenshot) in COPASI, and map the experimental values (automatically recognised by COPASI) to variables of the model. Note that we can load several files, and each file can contain several experiments.\n\nI can then list the parameters I want to explore, here the usual activator binding constant, between 1 and 1 million. Note that we can use only some of the experiments in the estimation of given parameters. This allows building extremely powerful and flexible parameter estimations.\n\nWe can use the same algorithms used in optimisation to explore the parameter space. The plot represents the best solution. Dashed lines link experimental values, continuous lines represent the fitting values, and circles are the error values.\n\nThe value estimated by COPASI for the binding constant is 1009. The “experiment” was created with a value of 1000. Not bad.\n\nThis concludes our overview of parameter exploration with COPASI. Note that this only brushes up the topic and I hope I picked your curiosity enough for you to read more about it.\n\nCovid-19 affects all of us on a daily basis and discussions about the disease stir passions, whether among politicians, health professionals, scientists, or the general public. Commentary and debate, particularly about the seriousness of the pandemic and how to counter it, are often tainted by inaccuracy and even bad faith due to confusion between different concepts and misunderstanding of some of them. People talk about different things, compare things that are not comparable, and misrepresent what others are saying.\n\nI’m not a doctor or an epidemiologist. My opinions are my own, and I am certainly not in a position to provide advice. This post is not intended to take a position, but to provide some clarity on concepts that have been thrown around in pieces of news and in raging tweets.\n\nAre Covid-19 and SARS-CoV-2 the same thing?\n\nNo, Covid-19 and SARS-CoV-2 are not two names for the same thing. Covid-19 is a “disease“, which is a set of symptoms related to an identified common cause (as opposed to a “syndrome“, which is a set of symptoms with an unknown, uncertain, or irrelevant cause). This disease is due to the infection of humans with the SARS-CoV-2 virus. This infection is the cause of the disease, not the disease itself. A large part of the population shows no symptoms after SARS-Cov-2 infection, people are said to be “asymptomatic”. In other words, these people are not sick. According to studies, this population is estimated to be between one-third of those infected to several times the number of people having the disease.\n\nSo we can see that there is a big difference in seriousness depending on whether we’re talking about the virus infection or the disease. SARS-CoV-2 infection is generally not very serious compared, for example, to Ebola or rabies viral infections. On the other hand, because of the severe respiratory symptoms and the lack of treatment, Covid-19 disease is actually more dangerous than rabies.\n\nThe disease is diagnosed on the basis of symptoms, before hospitalization, during hospitalization, or post-mortem. Only a certain proportion of patients are tested for the presence of the virus. It is generally accepted that for all patients with symptoms of Covid-19 and positive for the virus, the virus is the triggering factor for the disease. But it must be remembered that we are all permanently infected with several viruses (sometimes several strains of the same virus). For patients who are not tested for the presence of the virus, the decision depends on local policies. Hence, for example, the debates on the very low mortality from Covid-19 in Russia, or the real extent of deaths in care homes.\n\nTo summarise, not all persons infected with SARS-CoV-2 are sick with Covid-19, and it is possible that in a small proportion of persons diagnosed as sick with Covid-19 (especially in post-mortem diagnoses) the disease was in fact not triggered by SARS-CoV-2.\n\nAbove, I used the word “mortality”. So it’s time for a little vocabulary check.\n\nMortality, Lethality, Case Fatality Rate, Infection Fatality Rate\n\nThe “mortality” of a disease is the number of people killed by that disease in a given population, including those without the disease. The “lethality” of a disease is the number of people killed by the disease in patients with the disease. The lethality of a disease does not depend on its “prevalence“, while this is the case with mortality. In most countries, seasonal flu has a much higher mortality rate than Ebola hemorrhagic fever, although the latter has a much higher case fatality rate.\n\nIn order to estimate the fatality rate of a given infectious disease, the observed values of deaths in the sick population must be matched. The observed value is the Case Fatality Rate (CFR). The estimated value is the Infection Fatality Rate (IFR). One would think that with a well-constructed and careful assessment these numbers are close. They are not. The first reason is described in the previous paragraph (the difference between infection detection and disease diagnosis). The second is that the case-fatality rate changes over time. The first patients diagnosed usually have severe forms, with high mortality. The observed case-fatality rate is therefore very high (an extreme case being 100% if the first patient dies). As the diagnosis is extended to a larger population, and as the management of patients improves, the proportion of people surviving the disease increases. And the case-fatality rate tends to the infection fatality rate i.e., the “true” case fatality rate. This evolution can be seen in the image below, representing the outcome of Covid-19 diseases in Spain (Source https://www.worldometers.info/coronavirus/country/spain/, 23 May 2020). The first patient recovered, and then for a period of time 50% of the patients died, which corresponds approximately to the fraction of deaths of Covid-19 patients on ventilators. Then, the recovery rate increases more or less steadily.\n\nIt is very important to understand that if 10% of the patients diagnosed with Covid-19 in a given area and at a given time die from the disease, it absolutely does not mean that 10% of the people infected with the virus will die from it.\n\nCan lethality be compared between countries? Influence of testing policies\n\nIn order to assess infection-fatality rates, it is, of course, necessary to measure case fatality rates and to be able to detect cases. There are two types of tests.\n\nSo-called serological tests detect the presence of antibodies in the blood that target the virus. These tests can tell if a person has been infected in the past. There are two main problems with these tests. First, at the moment we do not know exactly what proportion of infected people create antibodies. In the case of infected people who have developed Covid-19, it seems that the amount of antibodies is related to the severity of symptoms (probably because symptoms are related to the amount of virus, the “viral load“). The big unknown is for people who have not developed the disease. Second, these tests are generally not very reliable, and in particular, their “sensitivity” is not high enough (more details in this post).\n\nThe other type of test detects the presence of the virus in actively infected people. The problem with these tests is that they have to be repeated over and over again. For a few days after infection, the viral load is not sufficient to be detected. Also, after a few weeks, the virus is no longer detectable. A person who had symptoms of Covid-19 can now be negative. Nevertheless, this is the type of test used to calculate case fatality rates. It is therefore clear that the testing policy will influence the calculations. If a country only tests people entering the hospital, the case fatality rate will be higher than if a country tests the whole population. It is therefore not surprising that there is a very clear correlation between case-fatality rates and the number of tests performed per million inhabitants. The graph below is based on Worldometer data from 10 April 2020.\n\nCan lethality be compared between countries? Influence of age structure\n\nDifferent studies have attempted to calculate an overall lethality rate of Covid-19 for the whole population, see for example here, here, and here. These estimates are based on data obtained by different methods (e.g. disease diagnosis, virus detection, detection of antibodies to the virus) in disparate regions, and analysed in a variety of ways. Not surprisingly, the results are, to say the least, heterogeneous. What they have in common is the desire to determine a “universal” rate. Trying to determine a single lethality for a disease is a justifiable exercise. However, this rate can only be valid for a homogeneous population and will necessarily vary between populations, making comparisons difficult, if not irrelevant.\n\nThe first factor is the effect of age. Most respiratory diseases disproportionately affect the elderly. As a result, infections with the viruses that trigger these diseases, such as influenza, show highly age-dependent mortality. Similarly, an estimate of the lethality rate of Covid-19 in China was 0.0016% for children aged 0-9 years and gradually increased to 7.8% for those over 80 years of age, an increase of almost 5,000 times.\n\nIf lethality depends on age, so does mortality. However, the relationship is not direct because of the distribution of the population by age group (the population pyramid). There are many more people aged 60 to 85 than there are aged 85 to 110. Although Covid-19 is much more lethal in the latter population, there are more deaths in the former. Since different countries have different age pyramids, this will affect their overall estimated lethality, as shown in the figure below (borrowed from https://theconversation.com/the-coronavirus-looks-less-deadly-than-first-reported-but-its-definitely-not-just-a-flu-133526)\n\nHowever, beware, things are not that simple. As the UN showed as early as 1955, the global mortality curve is affected by life expectancy. Mortality at age 25 in a country with a life expectancy of 60 is similar to mortality at age 45 in a country with a life expectancy of 70. This is due to the underlying causes of lower life expectancy. Which brings us to the comorbidities.\n\nCan lethality be compared between countries? Influence of comorbidities\n\nStricto-sensu, comorbidities are the other disorders that will affect the outcome of the disease. However, for this post, I mean all the factors not related to SARS-CoV-2 infection that will affect the lethality of Covid-19. One research paper studied many of these factors in a large cohort of over 96,000 patients from six continents (the purpose of the study was to study the effect of drugs on Covid-19, but that is not our point here). If each year of life increases the risk of Covid-19 death by 1%, one BMI point increases it by 6%, having diabetes by 20%, smoking by 27% and being Hispanic by 50%! These different factors are of course not independent (and therefore not additive).\n\nThe impact of ethnicity may come in part from genetic predisposing factors, as well as environmental conditions. For example, it is clear that the transmission of SARS-CoV-2 is affected by temperature and humidity as well as air pollution. It is not impossible that the outcome of the disease may also be affected (possibly via respiratory co-morbidities).\n\nFinally, the state of health systems has a considerable impact on the number of Covid-19 deaths. While most patients have only mild symptoms (let alone asymptomatic people), a fraction of patients requires respiratory assistance. In the worst case, these people need to be intubated. A large fraction of these people survive. In health care systems with insufficient ventilator capacity, all of these patients die. This is regardless of the policies undertaken to contain or eradicate the disease. However, these policies are important to keep the number of patients with severe Covid-19 below the limits of the health care system (“flattening the curve”).\n\nHerd immunity collective or lockdown policies\n\nOne of the inexhaustible sources of sterile and acrimonious debate is the battle between advocates of “herd immunity” and those of “isolation”. The latter notion is familiar to everyone and fairly simple to understand. If people are isolated, through social distancing and confinement, they cannot be contaminated or contaminate others. Moreover, if we quarantine the sick long enough for them to recover and get rid of the virus, we can eradicate it. Obviously, in the case of SARS-CoV-2, the goal is no longer to eradicate it, as the number of infected people is too large and their geographical distribution too wide. The aim is to reduce the number of severe diseases as much as possible while waiting for a vaccine.\n\nHerd immunity means that a sufficient portion of the population is exposed to the virus and develops an immune response so that the chance of an unexposed person encountering an infected person is very low. For many viruses, the required proportion of the population is just over 80%. Note that this principle of herd immunity is a key aspect of vaccination campaigns. In order for a campaign to keep disease at bay, a certain portion of the population must be vaccinated. Building up such herd immunity also goes hand in hand with isolating vulnerable people until the required % of immunized people are reached (no one suggests, as is sometimes written, that the herd immunity strategy means sacrificing a % of the population corresponding to the IFR, i.e. almost 1% of the world’s population).\n\nEstimates of the % of the population that has developed immunity to SARS-CoV-2 infection vary between 1% and 25% depending on the study. A cohort study in Geneva observed a growth rate of 3% per week. This percentage is obviously far from sufficient for a society to rely on herd immunity. Does this mean that the idea of herd immunity is invalidated? Not at all. At best, it reflects the success of isolation policies. In the long term, the consequences of isolation policies might outweigh the consequences of SARS-CoV-2 infection, especially if effective vaccination or treatments, whether curative or prophylactic, are not forthcoming.\n\nThe important thing is that the two approaches are incompatible and therefore the consequences of the application of one cannot be used to judge the other. Moreover, between absolute quarantine and uncontrolled exposure, there is a continuum of possibility. For example, an interesting approach, based on the fact that Covid-19 is mainly severe in elderly subjects or those with co-morbidities, is the concept controlled avalanche based on voluntary infections.\n\nOf course, this post only deals with some of the concepts underpinning the heated debates about Covid-19. However, I hope you will use it as a starting point to explore the diversity of opinions available and avoid the pitfall of snap judgements. Everyone is in the same boat and is looking for the same thing: a resolution to this crisis with the fewest possible casualties and consequences.\n\nAn increasing share of the scientific publications in life sciences reports results obtained with software tools, based on the use of datasets already published and deposited in databases, and all sort of resources available online. The manner to properly cite those vary from journal to journal and from communities to communities. I know that the publishing industry is working on the case. They have been for thirty years. Their solutions, when they exist, are often heterogeneous, complicated, and impose a burden on both authors and readers, letting the former to ignore the guidelines and the latter to ignore the result. While people more clever than me work on building solid, perennial and consensus solutions, I propose to systematically add a simple table as supplementary material, with the list of all software tools, data resources and datasets used in the study, and how to access them.\n\nWhat is the current situation?\n\nSometimes, people do not cite the resource at all. Many papers mention “survey of the literature”, or even “survey of the literature using PubMed”. That might surprise some readers, but a decade ago, I had a discussion with three researchers in life sciences, two from pharma and one from academia. None of them was actually aware of PubMed. When probed to see how they accessed the literature, one was just contacting authors to get preprints, one was using a commercial solution, and the third was using a national bibliographic resource. So it is useful to cite PubMed if its use is at the core of your result (on top of recognising the good work of NCBI staff). Another example was a collaborator (admittedly about 20 years ago), who built an entire paper on sequence search and alignment using FASTA. When I told him he should cite FASTA in his manuscript, his answer was “why? This is just a software tool. And I wrote the name anyway.”\n\nSometimes, people just cite a paper describing the resource. This can be the initial paper, which then might be obsolete, the most recent paper, that might describe a version of the resource that is posterior to the one used in the present report, or a third party publication, like a review of similar resources. Sometimes, one cites only a URL leading to the resource or a certain version of the resource. This URL might not exist anymore, or might be too generic to allow accessing the very specific resource used in the report. Certain journals also ban the use of URLs in the main text, of the list of references.\n\nThese citations are peppered throughout the papers. In general, different approaches are used in the same manuscript. For instance, PubMed will not be cited, a software tool will be cited through a reference, a data resource through a URL and a dataset with an accession number (not a complete URI, just the record identifier). This forces the reader to find those cryptic references, transform them in something actionable, and then work on converting them in the real resources, i.e. recognising that a URL is obsolete, a citation mention paper describing the wrong version, etc.\n\nI suggest that everyone build ONE very simple table as follows, and submit it as supplementary material.\n\nNote 1: examples filled in are of course irrelevant here. They are just illustrative\n\nNote 2: the multiple entries for DESeq2 are meant to show alternative citation mechanisms\n\nTable X: Accessible resources used to perform this research\n\nResource Address Description Reference Software tools DESeq2 https://bioconductor.org/packages/release/bioc/html/DESeq2.html A Bioconductor package that allows finding which genes are differentially expressed between conditions. Love et al (2014). Genome Biology, 15, 550 DESeq2 doi:10.18129/B9.bioc.DESeq2 Software tool to find differentially expressed genes doi:10.1186/s13059-014-0550-8 DESeq2 https://github.com/mikelove/DESeq2 https://identifiers.org/pubmed/25516281 DESeq2 https://github.com/mikelove/DESeq2 32 [that would be the entry in the list of references of the study containing this table] Data resources GEO https://www.ncbi.nlm.nih.gov/geo/ Gene Expression Omnibus Barrett et al. NCBI GEO: archive for functional genomics data sets–update. Nucleic Acids Res. 2013 Jan;41(Database issue):D991-5. Datasets GSE2606 https://identifiers.org/geo/GSE2606 Gene expressing profiling of PC12 cells expressing the FGFR3 mutation responsible for thanatophoric dysplasia type II https://www.ncbi.nlm.nih.gov/pubmed/15843401\n\nResource: The name of the software tool, the database or the dataset as it is used in the paper.\n\nAddress: The real place on the interweb where we can find the very version of the resource used to perform the research. Not the latest version, not the reference version, not the most frequently cited version. The precise version that was used in the research.\n\nDescription: An optional description of the resource. Depending on the length of the table (in some of the papers I am currently writing or helping to write, there would be several dozens or even hundreds of rows)\n\nReference: The reference where people can find more information about the resource, or that was recommended as a way to cite the resources by its authors. Although this field is optional, it would be nice to fill it. If possible, this reference should be part of the main list of references in the papers reporting the research, and therefore this table would only contain a citation of this entry, such as [2], or Love et al (2014). However, this is not always possible. Some journals stricly limit the number of references in paper (thereby undermining the track records of resource producers, and also hindering research reproducibility).\n\nOnly the first two columns would be mandatory since they would be sufficient to discover the resource.\n\nEt voilà! Nothing more. I am conscious that one can add many layers of standardisation there. People involved in the FAIR universe would have plenty to say. But the main point is to have ONE table where the reader can find PRECISE and ACCURATE information about ALL THE RESOURCES used in a study.\n\nWe all know that one of the main sources of variability in molecular measurements are batch effects. We perform the “same” experiment twice, using the same protocol and the same piece of kit. We believe we control everything and eliminate all sorts of unwanted perturbations. But at the end of day, everything is different, luminosity, air pressure, what we ate at lunch etc. Fortunately, we can (sometimes, and partially) correct these effects down the line. Now, what happens if we want to correct the batch effects for only part of a dataset? When I needed to, I had to dig quite a bit to find out how we can do so. I thought the trick could be useful to others (why we would like to do so will become clear latter).\n\nNB: I use the example of single-cell RNA-seq in this post. However, the approach can of course be applied to all kind of data.\n\nFirst, let’s look at the batch correction applied to an entire dataset. I will use scRNA-seq data, generated with the Smart-Seq protocol. The details of the sample preparation and the generation of the datasets is not relevant for the current post. So I will start directly with the count tables. The datasets haver been cleaned though. Counts have been corrected for library sizes (Count Per Million reads) and “bad” cells have been removed (cells with lot fewer reads than most, here less than 500000, and cells whose reads mapped to less genes, here 9000). I then removed all the genes that did not show at least 10 CPM in at least one cell, and genes that did not vary by at least two-fold across the entire datasets. Finally, I took the log of the counts. Other approaches can be used, such as DESeq2’s rlog. It does not matter here. Let’s load the counts.\n\n# load counts cpm1 <- read.table(\"OneCellType.csv\", sep=\",\",fill=T, header=T,row.names=1)\n\nWe can look at the resulting table. Columns are cells. The first part of the name indicate the name of the 96-well plate while the second part are the coordinates of the well. We have two batches, one composed of 1 plate, EPI1, and the other composed of 2 plates, EPI2 and EPI3. In total, we have 232 cells and 24235 genes.\n\nAs we can see, many many cells show zeros, as expected with scRNA-seq. But no row is made up entirely of zeros. Let’s visualise the dataset with Principal Component Analysis.\n\n# transpose the count matrix; needed for PCA tcpm1<-t(cpm1) # Run the PCA PCA1<-prcomp(tcpm1) # Provide the variance of components library(factoextra) eigen1<-get_eig(PCA1) # increase the left margin to avoid cropping the labels par(mar=c(5.1, 5.1, 4.1, 2.1)) # Colour by plates. colour<-0 colour[grep(\"EPI1\",colnames(cpm1))]<-rgb(230/255,159/255,0/255) colour[grep(\"EPI2\",colnames(cpm1))]<-rgb(86/255,180/255,233/255) colour[grep(\"EPI3\",colnames(cpm1))]<-rgb(0/255,158/255,115/255) # Plot PC1 and PC2 (the first coordinates from PCA1$x) plot(PCA1$x,col=colour,pch=16, cex=1.5,cex.lab=1.5,cex.axis=1.5, xlab=sprintf(\"PC1 %.1f %%\",eigen1[1,2]), ylab=sprintf(\"PC2 %.1f %%\",eigen1[2,2]))\n\nThe resulting plot does not exhibit much structure, and the variance landscape is very flat, the first 2 components showing only about 5% of it. This is all good since all those cells are supposed to belong to the same cell-type. However, we can see that while EPI2 and EPI3 (blue and green) 0 the two plates processed in the same batch – overlap nicely, EPI1 (in orange) is off. And this batch effect aligns perfectly with PC1. Yes, the most important source of variability (albeit small) is the batch! So we need to correct for it. To do that, we will use the function ComBat from the Bioconductor package sva.\n\nFirst we create a table that link our cells with the batches.\n\n# load the package sva library(sva) # create a table with the cells and the batches they belong to cells<-data.frame(batch = c(rep(\"b1\",ncol(cpm1[,grep(\"EPI1\",colnames(cpm1))])), rep(\"b2\",ncol(cpm1[,grep(\"EPI2\",colnames(cpm1))])), rep(\"b2\",ncol(cpm1[,grep(\"EPI3\",colnames(cpm1))]))), row.names = colnames(cpm1))\n\nSince all cells belong to the same cell-type, we have only one column, linking each cell to its batch. We can now run the batch correction itself. Since we have only one variable in the model, we do not let anything out (~1). Then we replot the PCA.\n\n# model to use in the batch correction modcombat = model.matrix(~1,data=cells) bcor_cpm1 = ComBat(dat=cpm1,batch=cells$batch, mod=modcombat, par.prior=TRUE, prior.plots=FALSE) # redo the PCA tbcpm1<-t(bcor_cpm1) PCAb1<-prcomp(tbcpm1) eigenb1<-get_eig(PCAb1) plot(PCAb1$x,col=colour,pch=16, cex=1.5,cex.lab=1.5,cex.axis=1.5, xlab=sprintf(\"PC1 %.1f %%\",eigenb1[1,2]), ylab=sprintf(\"PC2 %.1f %%\",eigenb1[2,2]))\n\nSuccess! Now, all three plates, from the first and second batch, are merged together. Note the decrease of variance associated with PC1, from 3.1 % to 2.5 %. This was expected since the shift of EPI1 compared with EPI2 and EPI3 was along PC1 in the first place. Now, this is important. If the feature were were looking to analyse also aligned with PC1, we would have thrown the baby out with the bathwater. Fortunately, in our case, the interesting stuff aligned with PC2. And PC2 is almost not affected by the batch correction. Of course you cannot know that in advance. It required a series of iterative analyses to become aware of it. As a rule, the more orthogonal the feature is with the batch effect, the less it will be affected by the batch correction.\n\nNow, that was easy since all the cells belonged to the same cell-type. In fact, in the real study, that was not quite the case. The first batch contained one cell-type, while the second batch contained two cell types. So let’s load the complete dataset.\n\n# load counts cpm2 <- read.table(\"TwoCellTypes.csv\", sep=\",\",fill=T, header=T,row.names=1)\n\nWe have initially 326 cells and 25299 genes. In addition to the plates of EPI cells, we have now a plate of LPM cells (what EPI and LPM mean really does not matter here). We can now run a PCA again.\n\n# transpose data for PCA tcpm2_clean<-t(cpm2) # Principal Component Analysis PCA2<-prcomp(tcpm2) eigen2<-get_eig(PCA2) colour<-0 colour[grep(\"EPI1\",colnames(cpm2))]<-rgb(230/255,159/255,0/255) colour[grep(\"EPI2\",colnames(cpm2))]<-rgb(86/255,180/255,233/255) colour[grep(\"EPI3\",colnames(cpm2))]<-rgb(0/255,158/255,115/255) colour[grep(\"LPM\",colnames(cpm2))]<-rgb(0/255,0/255,0/255) plot(PCA2$x,col=colour,pch=16,cex=1.5, cex.lab=1.5,cex.axis=1.5, xlab=sprintf(\"PC1 %.1f %%\",eigen2[1,2]), ylab=sprintf(\"PC2 %.1f %%\",eigen2[2,2]))\n\nWe can see that again, while EPI2 and EPI3 are together, the cloud of EPI1 cell is slightly shifted upward. The plate composed of cells belonging to another cell type, plotted in black, is clearly different from the three EPI ones. Let’s try to batch correct to bring together EPI1, EPI2 and EPI3. The procedure is absolutely identical as the previous one, except we now integrate the new plate, resulting in 3 plates associated with batch b2.\n\n# create a table with the cells and the batches they belong to cells2<-data.frame(batch = c(rep(\"b1\",ncol(cpm2[,grep(\"EPI1\",colnames(cpm2))])), rep(\"b2\",ncol(cpm2[,grep(\"EPI2\",colnames(cpm2))])), rep(\"b2\",ncol(cpm2[,grep(\"EPI3\",colnames(cpm2))])), rep(\"b2\",ncol(cpm2[,grep(\"LPM1\",colnames(cpm2))]))), row.names = colnames(cpm2)) modcombat = model.matrix(~1,data=cells2) bcor_cpm2 = ComBat(dat=cpm2,batch=cells2$batch, mod=modcombat, par.prior=TRUE, prior.plots=FALSE) tbcpm2<-t(bcor_cpm2) PCAb2<-prcomp(tbcpm2) eigenb2<-get_eig(PCAb2) plot(PCAb2$x,col=colour,pch=16,cex=1.5,cex.lab=1.5,cex.axis=1.5, xlab=sprintf(\"PC1 %.1f %%\",eigenb2[1,2]),ylab=sprintf(\"PC2 %.1f %%\",eigenb2[2,2]))\n\nFail! EPI1 from batch1 has now been batch-corrected taking into account all three plates of batch b2, which include the LPM cells. As a result, EPI1 cells end up located somewhere in between EPI2/3 and LPM1. Clearly this is wrong. What we want is putting together EPI1 and EPI2/3, ignoring LPM1. We can do that without a problem, by declaring the cell-types as a variable of interest, that we will then ignore. But first we need to do something. The initial clean-up was performed on the entire dataset. Even if gene expression varied throughout the dataset, we could still have genes that do not vary either across EPI cells or across LPM cells. That would cause newest versions of ComBat to throw errors. So let’s remove the culprit genes.\n\n# sanity check on variance. remove genes which # variance is 0 either in EPI or in LPM varEPI <- apply(cpm2[,grep(\"EPI\",colnames(cpm2))], 1, var) varLPM <- apply(cpm2[,grep(\"LPM\",colnames(cpm2))], 1, var) cpm2 <- cpm2[-which(varEPI == 0 | varLPM == 0 ),]\n\nWe still have 20885 genes to play with, which is largely enough. Now we will create a table linking the cells and the batches, as before, but in addition add the cell-type they belong to.\n\n# create a table with the cells and the batches they belong to cells3<-data.frame(batch = c(rep(\"b1\",ncol(cpm2[,grep(\"EPI1\",colnames(cpm2))])), rep(\"b2\",ncol(cpm2[,grep(\"EPI2\",colnames(cpm2))])), rep(\"b2\",ncol(cpm2[,grep(\"EPI3\",colnames(cpm2))])), rep(\"b2\",ncol(cpm2[,grep(\"LPM1\",colnames(cpm2))]))), celltype = c(rep(\"epi\",ncol(cpm2[,grep(\"EPI\",colnames(cpm2))])), rep(\"lpm\",ncol(cpm2[,grep(\"LPM\",colnames(cpm2))]))), row.names = colnames(cpm2))\n\nWe can now instruct ComBat to correct for the batch but taking into account the cell-type in the model (~cells3$celltype).\n\nmodcombat = model.matrix(~cells3$celltype,data=cells3) bcor_cpm3 = ComBat(dat=cpm2,batch=cells3$batch, mod=modcombat, par.prior=TRUE, prior.plots=FALSE)\n\nAnd we run the PCA again.\n\ntbcpm3<-t(bcor_cpm3) PCAb3<-prcomp(tbcpm3) eigenb3<-get_eig(PCAb3) plot(PCAb3$x,col=colour,pch=16,cex=1.5, cex.lab=1.5,cex.axis=1.5, xlab=sprintf(\"PC1 %.1f %%\",eigenb3[1,2]), ylab=sprintf(\"PC2 %.1f %%\",eigenb3[2,2]))\n\nHooray! Now we have EPI1, EPI2 and EPI3 together, regardless of the batch, while LPM cells stand quietly to the side. We can export those coordinates for future use (e.g. clustering).\n\nwrite.csv(bcor_cpm3, file=\"batchCorrected.cvs\",quote=FALSE)\n\nVisual representation of biochemical pathways has been a key tool used to understand cellular and molecular systems for a long time. Any knowledge integration project involves a jigsaw puzzle step, where different pieces have to be put together. When Feynman cheekily wrote on his blackboard just before his death “What I cannot create I do not understand”, he meant that he only fully understood a system once he derived a (mathematical) model for it, and interestingly Feynman is also famous for one of the earliest standard graphical representations of reaction networks, namely the Feynman diagrams to represent models of subatomic particle interactions. The earliest metabolic “map” I possess comes from the 3rd edition of “Outlines of Biochemistry” by Gortner published in 1949. I would be happy to hear if you have older ones.\n\n(I let you find out all the inconsistencies, confusions and error-generating features in this map. This might be food for another text, but I believe this is a great example to support the creation of standards, best practices, and software tools!)\n\nUntil recently, those diagrams were mostly drawn by hand, initially on paper, then using drawing software. There was not so much thinking spent in consistency, visual semantics, or interoperability. This changed in the 1990s, as part as Systems Biology’s revival. The other thing that changed in the 1990s was the widespread use of computers and software tools to build and analyse models. The child of both trends was the development of standard computer-readable formats to represent biological networks.\n\nWhen drawing a knowledge representation map, one can divide the decision-making process, and therefore the things we need to encode in order to share the map, in three parts:\n\nWhat – How can people identify what I represent? A biochemical map is a network made up of nodes, linked by arcs. The network may contain only one type of nodes, for instance a protein-protein interaction network or an influence network, or be a bipartite graph, like a reaction network – one type of nodes representing the pools involved in the reactions, the other representing the reactions themselves. One decision is the shape to use for each node so that it carries a visual information about the nature of what it represents. Another concerns the arcs linking the nodes, that can also contain visual clues, such as directionality, sign, type of influence etc. All this must be encoded in some way, either semantically (a code identifying the type of glyphs, from an agreed-up list of codes), or graphical (embedding an image or describing the node).\n\nWhere – Once the glyphs are chosen, one needs to place them. The relative position of the information should not always carry much information, but there are some cases where it must, e.g. members of complexes, inclusion in compartments etc. And there is no denying that the relative position of glyphs is also used to convey more subjective information. For instance a linear chain of reactions induce the idea of a flow, much better than a set of reactions going randomly up and down, right and left. Another unwritten convention is to represent membrane signal transduction on the top of the maps, with the “end-result”, often effect on gene expression, at the bottom, with the idea of a cascading flux of information. The coordinates of the glyphs must then be shared as well.\n\nHow – Finally, the impact of a visual representation also depends on aesthetic factors. The relative size of glyphs and labels, thickness of arcs, the colours, shades and textures, all influence the facility with which viewers absorb the information contained in a map. Relying on such aspects to interpret the meaning of a map should be avoided, in particular if the map is to be shared between different media, where rendering could affect the final aspect. But wanting to keep this aspect as close as possible makes sense.\n\nA bit of history\n\nDifferent formats have been developed over the years to cover these different aspects with different accuracy and constraints. In order to understand why we have such a variety of description formats on offer, a bit of history might be useful. Being able to encode graphical representation of models in SBML was mentioned as early as 2000 (Andrew Finney. Possible Extensions to the Systems Biology Markup Language. 27 November 2000.).\n\nIn 2002, the group of Hiroaki Kitano presented a graphical editor for the Systems Biology Markup Language (SBML, Hucka et al 2003), called SBedit, and proposed extensions to SBML necessary for encoding maps (Tanimura et al. Proposal for SBEdit’s extension of SBML-Level-1. 8 July 2002). This software later became CellDesigner (Funahashi et al 2003), a full-featured modelling developing environment, using SBML as its native format. All graphical information is encoded in CellDesigner-specific annotations, using the SBML extension system. In addition to the layout (the where), CellDesigner proposed a set of standardised glyphs to use for representing different types of molecular entities and different relationships (the what) (Kitano et al 2003). At the same time, Herbert Sauro developed an extension to SBML to encode the maps designed in the software JDesigner (Herbert Sauro. JDesigner SBMLAnnotation. 8 January 2003). Both CellDesigner and JDesigner annotations could also encode the appearance of glyphs (how).\n\nIn 2003, Gauges et al (Gauges et al. Including Layout information in SBML files. 13 May 2003) proposed to split the description of the layout (the where) and the rendering (the what and the how), and to focus on the layout part in SBML (Gauges et al 2006). Eventually, this effort led to the development of two SBML Level 3 Packages, Layout (Gauges et al 2015) and Render (Bergmann et al 2017).\n\nOnce the SBML Layout annotations were finalised, the SBML and BioPAX communities came together to standardise visual representations for biochemical pathways. This led to the Systems Biology Graphical Notation, a set of three standard graphical languages with agreed upon symbols and rules to assemble them (the what, Le Novère et al 2009). While the shape of SBGN glyphs determine their meaning, neither their placement in the map nor their graphical attributes (colour, texture, edge thickness, the how) affect the map semantics. SBGN maps are ultimately images and can be exchanged as such, either in bitmaps or vector graphics. They are also graphs and can be exchanged using graph formats, such as GraphML. However, it was felt that sharing and editing SBGN maps would be much easier if more semantics was encoded rather than graphical details. This led to the development of SBGN-ML (van Iersel et al 2012), which not only encode the SBGN part of SBGN maps, but also the layout and size of graph elements.\n\nSo we have at least three solutions to encode biochemical maps using XML standards from the COMBINE community (Hucka et al 2015): 1) SBGN-ML, 2) SBML with Layout extension (controlled Layout annotations in Level 2 and Layout package in Level 3) and 3) SBML with proprietary extensions. Regarding the latter, we will only consider CellDesigner, for two reasons. Firstly, CellDesigner is the most used graphical model designer in systems biology (at the time of writing, the articles describing the software have been cited over 1000 times). Secondly, CellDesigner’ SBML extensions are used in other software tools. These solutions are not equivalent, they present different advantages and disadvantages, and round-tripping is in general not possible.\n\nSBGN-ML\n\nCuriously, despite its name, SBGN-ML does not explicitly describe the SBGN part of the maps (the what). Since the shape of nodes is a standard, it is only necessary to mention their type, and any supporting software will know which symbol to use. For instance, SBGN-ML will not specify that a protein X must be represented with a round-corner rectangle. It will only say that there is a macromolecule X at a certain position with given width and height. Any SBGN-supporting software must know that a macromolecule is represented by a round-corner rectangle. The consequence is that SBGN-ML cannot be used to encode maps using non-SBGN symbols. However, software tools can decide to use different symbols attributed to a given class of SBGN objects during the rendering of the maps. Instead of using a round-corner rectangle each time the class of a glyph is macromolecule, it could use a star. The resulting image would not be an SBGN map. But if modified, and saved back in SBGN-ML, it could be recognised by another supporting software. Such a behaviour is not to be encouraged if we want people to get used to SBGN symbols, but it provides a certain level of interoperability.\n\nWhat is explicitly described in SBGN-ML instead are the parts that are not regulated by SBGN itself, but are specific to the map. That include the size of the glyphs (bounding box), the textual labels, as well as the positions of glyphs (the where). SBGN-ML currently does not encode rendering properties such as text size, colours and textures (the how). But the language provides an element extension, analogous to the SBML annotation, that allows to augment the language. One can use this element to extend each glyph, or to encode styles, and the community started to do so in an agreed-upon manner.\n\nNote that SBGN-ML only encodes the graph. While there is a certain amount of biological semantics, linked to the identity of the glyphs, it is not a general purpose format that would encode advanced semantic of regulatory features, such as BioPAX (Demir et al. 2010), or mathematical relationships such as SBML. However, users can distribute SBML files along SBGN-ML files, for instance in a COMBINE Archive (Bergmann et al 2014). Unfortunately, there is currently no blessed way to map an SBML element, such as a particular species, to a given SBGN-ML glyph.\n\nSBML Level 3 + Layout and Render packages\n\nAs we mentioned before, SBML Level 3 provides two packages helping with the visual representations of networks: Layout (the where) and Render (the how). Contrarily to SBGN-ML, which is meant to describe maps in a standard graphical notation, the SBML Level 3 packages do not restrict the way one represents biochemical networks. This provides more flexibility to the user, but decreases the “stand-alone” semantics content of the representations. I.e. if non-standard symbols are used, their meaning must be defined in an external legend. It is of course possible to use only SBGN glyphs to encode maps. The visual rendering of such a file will be SBGN, but the automatic analysis of the underlying format will be harder.\n\nThe SBML Layout package permits to encode the position of objects, points, curves and bounding boxes. Curves can have complex shapes, encoded as Béziers curves. The package allows to distinguish between different general types of nodes such as compartments, molecular species, reactions and text. However, there is little biological semantics encoded by the shapes, either regarding the nodes (e.g. nothing distinguishes a simple chemical from a protein) or the edges (one cannot distinguish an inhibition from a stimulation). In addition, the SBML Render package permits to define styles that can be applied to types of glyphs. This includes colours and gradients, geometric shapes, properties of text, lines, line-endings etc. Render can encode a wide variety of graphical properties, and pave the gap to generic graphical formats such as SVG.\n\nIf we are trying to visualise a model, one advantage of using SBML packages is that all the information is included in a single file, providing an easy mapping between the model constructs and their representation. This goes a long way to solve the issue of the biological semantics mentioned above, since it can be retrieved from the SBML Core elements, linked to the Layout elements. Let’s note that while SBML Layout+Render do not encode the nature of the objects represented by the glyphs (the what) using specific structures, this can be retrieved via the attributes sboTerm of the corresponding SBML Core elements, using the appropriate values from the Systems Biology Ontology (Courtot et al 2011).\n\nCellDesigner notation\n\nCellDesigner uses SBML (currently Level 2) as its native language. However, it extended it with its own proprietary annotation, keeping the SBML perfectly valid (which is also the way software tools such as JDesigner operate). Visually, the CellDesigner notation is close to SBGN Process Descriptions, having been the strongest inspiration for the community effort. CellDesigner offers an SBGN-View mode, that produces graphs closer to pure SBGN PD.\n\nCellDesigner’s SBML extensions increase the semantics of SBML elements such as molecular species or regulatory arcs, in a way not dissimilar to SBGN-ML. In addition, it provides a description of each glyph linked to the SBML elements, covering the ground of SBML Layout and Render. The SBML extensions being specific to CellDesigner, they do not offer the flexibility of SBML Render. However, the limited spectrum of possibilities might makes the support easier.\n\nCellDesigner notation SBML Layout+Render SBGN-ML Encodes the what ✓ ✓ ✓ Encodes the where ✓ ✓ ✓ Encodes the how ✓ ✓ ✓ Contains the mathematical model part ✓ ✓ ✗ Writing supported by more than 1 tool ✗ ✓ ✓ Reading supported by more than 1 tool ✓ ✓ ✓ Is a community standard ✗ ✓ ✓\n\nExamples of usages and conversions\n\nNow let’s see the three formats in action. We start with SBGN-ML. First, we can load a model, for instance from BioModels (Chelliah et al 2015), in CellDesigner (version 4.4 at the time of writing). Here we will use the model BIOMD0000000010, an SBML version of the MAP kinase model described in Kholodenko et al (2000).\n\nFrom an SBML file that does not contain any visual representation, CellDesigner created one using its auto-layout functions. One can then export an SBGN-ML file. This SBGN-ML file can be imported for instance in Cytoscape (Shannon et al. 2003) 2.8 using the CySBGN plugin (Gonçalves et al 2013).\n\nThe position and size of nodes are conserved, but edges have different sizes (and the catalysis glyph is wrong). The same SBGN-ML file can be open in the online SBGN editor Newt.\n\nAn alternative to CellDesigner to produce the SBGN-ML map could be Vanted (Junker et al 2006, version 2.6.4 at the time of writing). Using the same model from BioModels, we can auto-layout the map (we used the organic layout here) and then convert the graph to SBGN using the SBGN-ED plugin (Czauderna et al 2010).\n\nThe map can then be saved as SBGN-ML, and as before opened in Newt.\n\nThe positions of the nodes are conserved. But the connection of edges is a bit different. In that case, Newt is slightly more SBGN compliant.\n\nNow, let’s start with a vanilla SBML file. We can import our BIOMD0000000010 model in COPASI (Hoops et al 2006, version 4.22 at the time of writing). COPASI now offers auto-layout capabilities, with possibilities of manually editing the resulting maps.\n\nNow, when we’ll export the model in SBML, it will contain the map encoded with the Layout and Render packages. When the model is uploaded in any software tool supporting the packages, we will retrieve the map. For instance, we can use the SBML Layout Viewer. Note that if the layout is conserved, it is not the case of the rendering.\n\nAlternatively, we can load the model to CellDesigner, and manually generate a nice map (NB: a CellDesigner plugin that can read SBML Layout was implemented during Google Summer of Code 2014 . It is part of the JSBML project).\n\nWe can create an SBML Layout using CellDesigner layout converter. When we import the model in COPASI we can visualise the map encoded in Layout. NB: the difference of appearance here is due to a problem in CellDesigner converter, not COPASI.\n\nThe same model can be loaded in the SBML Layout Viewer.\n\nHow do I choose between the formats?\n\nThere is unfortunately no unique solution at the moment. The main question one has to ask is what do we want to do with the visual maps?\n\nAre they meant to be a visual representation of an underlying model, the model being the important part, that needs to be exchanged? If that is the case, SBML packages or CellDesigner notation should be used.\n\nDoes the project mostly/only involves graphical representations, and those must be exchanged? CellDesigner or SBGN-ML would therefore be better.\n\nDoes the rendering of graphical elements matter? In that case, SBML packages or CellDesigner notations are currently better (but that is going to change soon).\n\nIs standardisation important for the project, in addition to immediate interoperability? If yes, SBML packages or SBGN-ML would be the way to go.\n\nAll those questions and more have to be clearly spelt out at the beginning of a project. The answer will quickly emerge from the answers.\n\nAcknowledgements\n\nThanks to Frank Bergmann, Andreas Dräger, Akira Funahashi, Sarah Keating, Herbert Sauro for help and corrections.\n\nReferences\n\nBergmann FT, Adams R, Moodie S, Cooper J, Glont M, Golebiewski M, Hucka M, Laibe C, Miller AK, Nickerson DP, Olivier BG, Rodriguez N, Sauro HM, Scharm M, Soiland-Reyes S, Waltemath D, Yvon F, Le Novère N (2015) COMBINE archive and OMEX format: one file to share all information to reproduce a modeling project. BMC Syst Biol 15, 369. doi:10.1186/s12859-014-0369-z\n\nBergmann FT, Keating SM, Gauges R, Sahle S, Wengler K (2017) Render, Version 1 Release 1. Available from COMBINE <http://identifiers.org/combine.specifications/sbml.level-3.version-1.render.version-1.release-1>\n\nChelliah V, Juty N, Ajmera I, Raza A, Dumousseau M, Glont M, Hucka M, Jalowicki G, Keating S, Knight-Schrijver V, Lloret-Villas A, Natarajan K, Pettit J-B, Rodriguez N, Schubert M, Wimalaratne S, Zhou Y, Hermjakob H, Le Novère N, Laibe C (2015) BioModels: ten year anniversary. Nucleic Acids Res 43(D1), D542-D548. doi:10.1093/nar/gku1181\n\nCourtot M, Juty N, Knüpfer C, Waltemath D, Zhukova A, Dräger A, Dumontier M, Finney A, Golebiewski M, Hastings J, Hoops S, Keating S, Kell DB, Kerrien S, Lawson J, Lister A, Lu J, Machne R, Mendes P, Pocock M, Rodriguez N, Villeger A, Wilkinson DJ, Wimalaratne S, Laibe C, Hucka M, Le Novère N. Controlled vocabularies and semantics in Systems Biology. Mol Syst Biol 7, 543. doi:10.1038/msb.2011.77\n\nCzauderna T, Klukas C, Schreiber F (2010) Editing, validating and translating of SBGN maps. Bioinformatics 26(18), 2340-2341. doi:10.1093/bioinformatics/btq407\n\nDemir E, Cary MP, Paley S, Fukuda K, Lemer C, Vastrik I, Wu G, D’Eustachio P, Schaefer C, Luciano J, Schacherer F, Martinez-Flores I, Hu Z, Jimenez-Jacinto V, Joshi-Tope G, Kandasamy K, Lopez-Fuentes AC, Mi H, Pichler E, Rodchenkov I, Splendiani A, Tkachev S, Zucker J, Gopinathrao G, Rajasimha H, Ramakrishnan R, Shah I, Syed M, Anwar N, Babur O, Blinov M, Brauner E, Corwin D, Donaldson S, Gibbons F, Goldberg R, Hornbeck P, Luna A, Murray-Rust P, Neumann E, Ruebenacker O, Samwald M, van Iersel M, Wimalaratne S, Allen K, Braun B, Carrillo M, Cheung KH, Dahlquist K, Finney A, Gillespie M, Glass E, Gong L, Haw R, Honig M, Hubaut O, Kane D, Krupa S, Kutmon M, Leonard J, Marks D, Merberg D, Petri V, Pico A, Ravenscroft D, Ren L, Shah N, Sunshine M, Tang R, Whaley R, Letovksy S, Buetow KH, Rzhetsky A, Schachter V, Sobral BS, Dogrusoz U, McWeeney S, Aladjem M, Birney E, Collado-Vides J, Goto S, Hucka M, Le Novère N, Maltsev N, Pandey A, Thomas P, Wingender E, Karp PD, Sander C, Bader GD (2010) The BioPAX Community Standard for Pathway Data Sharing. Nat Biotechnol, 28, 935–942. doi:10.1038/nbt.1666\n\nFunahashi A, Morohashi M, Kitano H, Tanimura N (2003) CellDesigner: a process diagram editor for gene-regulatory and biochemical networks. Biosilico 1 (5), 159-162\n\nGauges R, Rost U, Sahle S, Wegner K (2006) A model diagram layout extension for SBML. Bioinformatics 22(15), 1879-1885. doi:10.1093/bioinformatics/btl195\n\nGauges R, Rost U, Sahle S, Wengler K, Bergmann FT (2015) The Systems Biology Markup Language (SBML) Level 3 Package: Layout, Version 1 Core. J Integr Bioinform 12(2), 267. doi:10.2390/biecoll-jib-2015-267\n\nGonçalves E, van Iersel M, Saez-Rodriguez J (2013) CySBGN: A Cytoscape plug-in to integrate SBGN maps. BMC Bioinfo 14, 17. doi:10.1186/1471-2105-14-17\n\nHoops S, Sahle S, Gauges R, Lee C, Pahle J, Simus N, Singhal M, Xu L, Mendes P, Kummer U (2006) COPASI-a COmplex PAthway SImulator. Bioinformatics 22(24), 3067-3074. doi:10.1093/bioinformatics/btl485\n\nHucka M, Bolouri H, Finney A, Sauro HM, Doyle JC, Kitano H, Arkin AP, Bornstein BJ, Bray D, Cornish-Bowden A, Cuellar AA, Dronov S, Ginkel M, Gor V, Goryanin II, Hedley WJ, Hodgman TC, Hunter PJ, Juty NS, Kasberger JL, Kremling A, Kummer U, Le Novère N, Loew LM, Lucio D, Mendes P, Mjolsness ED, Nakayama Y, Nelson MR, Nielsen PF, Sakurada T, Schaff JC, Shapiro BE, Shimizu TS, Spence HD, Stelling J, Takahashi K, Tomita M, Wagner J, Wang J (2003) The Systems Biology Markup Language (SBML): A Medium for Representation and Exchange of Biochemical Network Models. Bioinformatics, 19, 524-531. doi:10.1093/bioinformatics/btg015\n\nHucka M, Nickerson DP, Bader G, Bergmann FT, Cooper J, Demir E, Garny A, Golebiewski M, Myers CJ, Schreiber F, Waltemath D, Le Novère N (2015) Promoting coordinated development of community-based information standards for modeling in biology: the COMBINE initiative. Frontiers Bioeng Biotechnol 3, 19. doi:10.3389/fbioe.2015.00019\n\nJunker BH, Klukas C, Schreiber F (2006) VANTED: A system for advanced data analysis and visualization in the context of biological networks. BMC Bioinfo 7, 109. doi:10.1186/1471-2105-7-109\n\nKholodenko BN (2000) Negative feedback and ultrasensitivity can bring about oscillations in the mitogen-activated protein kinase cascades. Eur J Biochem.267(6), 1583-1588. doi:10.1046/j.1432-1327.2000.01197.x\n\nKitano H (2003) A graphical notation for biochemical networks. Biosilico 1 (5), 169-176. doi:10.1016/S1478-5382(03)02380-1\n\nLe Novère N, Hucka M, Mi H., Moodie S, Shreiber F, Sorokin A, Demir E, Wegner K, Aladjem M, Wimalaratne S, Bergman FT, Gauges R, Ghazal P, Kawaji H, Li L, Matsuoka Y, Villéger A, Boyd SE, Calzone L, Courtot M, Dogrusoz U, Freeman T, Funahashi A, Ghosh S, Jouraku A, Kim S, Kolpakov F, Luna A, Sahle S, Schmidt E, Watterson S, Goryanin I, Kell DB, Sander C, Sauro H, Snoep JL, Kohn K, Kitano H (2009) The Systems Biology Graphical Notation. Nat Biotechnol 27, 735-741. doi:10.1038/nbt.1558\n\nShannon P, Markiel A, Ozier O, Baliga NS, Wang JT, Ramge D, Amin N, Schwikowski B, Ideker T (2003) Cytoscape: a software environment for integrated models of biomolecular interaction networks. Bioinformatics 13, 2498-2504. doi:10.1101/gr.1239303\n\nvan Iersel MP, Villéger AC, Czauderna T, Boyd SE, Bergmann FT, Luna A, Demir E, Sorokin A, Dogrusoz U, Matsuoka Y, Funahashi A, Aladjem MI, Mi H, Moodie SL, Kitano H, Le Novère N, Schreiber F (2012) Software support for SBGN maps: SBGN-ML and LibSBGN. Bioinformatics 28, 2016-2021. doi:10.1093/bioinformatics/bts270\n\nAs I get older, I am asked to sit on more senior grant panels. Larger projects generally mean more diverse research approaches. I am also offered to sit on multi-disciplinary panels, where the projects must feature different approaches, or even different fields. Finally, those panels are often made up of more “mature” scientists. As a result of all that:\n\n1) Panel members have less expertise, if any, in some of the fields covered by the applications. Even in their own field, they mostly master only part of the landscape. They left university decades ago, and kept abreast of developments via reading scientific literature. Since there are only 24 h in a day, we tend to focus our reading largely on what directly impacts our own research. When it comes to techniques, most of senior panel members left actual experiments/programming/equations for quite a while, and although they have an “academic” knowledge of the methods, they might not know the state of the art, or master the subtleties and pitfalls of given techniques.\n\n2) Panel members/reviewers have a shorter attention span. This is not only because of age (although let’s face it, there is some decrease in focus and stamina), but also because senior scientists have more commitments and are always running for deadlines (there is also the increasing number of projects to evaluate. In my latest panel, I was in charge of 25 projects). Finally, there might also be a bit of arrogance and “cannot be bothered” attitude.\n\nApplicants to large grants should take these facts into consideration. One area where it is particularly important is Systems Biology. I am not going to dwell on what is Systems Biology, but a subset includes the development of mathematical models and numerical simulations to reproduce the behaviour of biological systems. This is an area where I encountered systematic strategic errors in the way grant applications are written, that decreased their chances of being successfully funded comparatively to projects in other areas. Below are a few advices that could allow senior panels to appreciate your projects better. This is only my opinion, and I might be wrong about the impact of those mistakes. Also, some of the advices seem obvious. But nevertheless, they are not systematically followed.\n\nWhat is the question?\n\nBuilding models is great. Models are integrators of knowledge, and building a model that can reproduce known behaviours of a system is the best way to see if we understand it. Models can also suggest new avenues of research or treatment. But models must fulfill a purpose. This purpose is not obligatory part of the project submitted itself, but it must be mentioned there. Why are you building the model? What questions do you want to answer with the model simulations and analysis? Note that this is not specific to computational modelling. I saw rejection of projects describing very complex experimental techniques which use was not justified. Technical activity must be commensurate with expected benefits.\n\nHave a colleague to read your proposal, and ask them afterward “can you tell me what we are going to do with this model?” (Do not ask the question before, while handing them the application. Your colleague is clever and friendly. They will finds hints here and there, add their own ideas and put something together).\n\nClarity of the research plan\n\nI cannot count the number of time where after a lengthy introduction, a detailed experiment part, projects end up with “and we will build a model”, or even worse, the dreadful “we will use a systems biology approach”. WTF is a “systems biology approach”? I have called myself a systems biologist for the best part of two decades and have not a clue. It does not mean anything, or it means too many things.\n\nExplain what you are going to do and how. Which kind of modeling approach will you use? Why? Is this the best modeling approach considering the data you have and the questions you ask? If building a model, how will you build it? What will be the variables (e.g. which molecular species will be represented)? How will you relate them? Will you use numerical or logic approaches? Will you incorporate uncertainty? Will you study steady-states or kinetics? Will you use deterministic or stochastic simulations? Which software will you use? How are you going to analyse the data? How are you going to link the model to experimental evidence? Do you have a plan for parameterisation?\n\nDon’t overdo it. One does not describe generic molecular biology kits or culture media in senior grant applications (except if this is at the core of the project), so we do not need to describe technical details that will not affect the results and their interpretation. But give enough details to convince the reviewers or panel members who might actually know what all that is about. An experimental plan that would not precise the organism or cell line to be used has almost no chance of getting through. Same for modelling! And actually, you also need to add enough explanation to allow for non-specialists to understand what you are going to do. For instance, many people are baffled by genome-scale constraint-based modelling of metabolism, confusing flux balance analysis and metabolic flux analysis, and therefore misunderstanding the (absent) role of metabolite concentrations. They also mistake them with ODE models, concluding that they are too big to be parameterised.\n\nHave a colleague to read your proposal, and ask them afterward “Can you tell me which modelling method I will use and why it is the best for this project?”\n\nProvide preliminary data\n\nAlmost any experimental project comes packed with preliminary data that shows 1) why is the investigated question interesting, 2) why is the workplan feasible. It is no different with modelling. Why would people believe you? Past track record on other projects is not sufficient. At best it can show that you can do modelling. Good. But this is not enough. Remember, this blog post is particularly focused on large projects, with multiple lines of investigations and requesting large amounts of money. Often these projects have been written over many months or even a year. I know a few such projects for which the production of preliminary data required another, smaller, dedicated funding. So there is no excuse not to spend a sufficient effort benchmarking the modelling approaches you will use, and getting preliminary results, hopefully exciting and justifying a scale-up.\n\nDescribe the validation steps\n\nThis is a very important part, even more important than for experimental projects. A modeller cannot say “let the data talk”. Any number of models can lead to reasonable simulation results. That does not mean the model is the correct one, or that the results mean anything. You must convince the panel that you have a plan to check that your results are valid and are not a bunch of random numbers or graphs. How will you validate the results of your simulations and analysis? How will this validation feedback in your model design? How precisely will your predictions lead to new experiments? Who will do the validation? Where? When?\n\nHave a colleague to read your proposal, and ask them afterward “Can you draw me a workflow of the modelling part of this project, and identify the points of contact with the rest of the project?”\n\nThese were just some tips I came up with. Do you disagree with them? Would you add others?\n\nEvaluating the impact of research activity is a complex issue, that is guaranteed to stir hot debates whatever the audience or the context. The way evaluators have access to research output is mostly via publications, whatever the type – articles, books, conference proceedings, technical reports etc. It is important to note that in many fields of research, publications are not (or should not) themselves the output of research. In particular, in natural sciences and mathematics, the output of science comes under many guises such as theorems, software, datasets, techniques, chemical compounds and materials, patents etc. the publications being only a report on the research activity and its outcome. Nevertheless, as a consequence, a large part of the evaluation relies on the publications. The most obvious way to do so is by reading the publications, the so-called peer review. This is what is done before a manuscript is accepted for publication in scientific journals (and increasingly now after it is published). However, to assess funding applications, project achievements, individual and institution performances, most evaluations rely in part on the analysis of publication impact.\n\n[small digression. Let’s be clear about something. Everyone claiming that peer-review of papers is being used to evaluate funding applications, individuals for positions or promotions, or institute performance, is either an hypocrite or has never been part of such an evaluation committee. This never happens, for two reasons, one negative and one positive. The first one is that nobody would have time to perform such as exercise. Members of evaluation panels are often senior researchers, chosen for their recognized track records. They lead research groups and are completely over-committed. Reading a paper seriously, understanding its content, its novelty, takes a significant amount of time. The notion that we read dozens of papers from dozens of scientists for a given panel is just a fantasy. The second, positive, reason is that members of evaluation committees present a very limited collective expertise. For instance I am part of a committee covering the totality of the research spectrum. In this committee, there are only a handful of people covering the entirety of life sciences! It is VERY FORTUNATE that we are not actually judging the papers ourselves!]\n\nTo improve on arbitrary judgments based on unconscious bias triggered by journal names, and to complement evaluation by external reviewers, people try to use quantitative metrics, developed by the field called bibliometrics, and in particular citation analysis. For instance, The UK Research Excellence Framework (REF) provides guidance for the use of citation data. It is important to note that those metrics are not sufficient, and REF is actively assessing how best to use them.\n\nIn the field of natural sciences citation, a variety of metrics are used to evaluate the impact of articles, individuals and institutes, including citation counts, h-indices and impact factors (yes, this is very wrong, IF are meant for journals not for papers and authors). Recently, a new metrics has been proposed to assess the impact of a given article, the Relative Citation Ratio.\n\nScientists are inherently navel gazing (or maybe it is just me), and I was curious to see how all these correlated for me. So I collated my bibliometrics data using Google Scholar. First let’s look at the classic measurements. If I plot the citations of each paper versus the impact factor of the journal it was published in for the year of its publication, the correlation is not overwhelming …\n\nThe paper describing SBML is clearly an outlier and makes hard to judge the rest of the plot, so let’s discard it for the time being (yes, I should also discard the outliers in the other direction, but hey, this is a blog post, not a research paper …)\n\nNow, the correlation is clear, but still not overwhelming. The correlation seems to disappear for the highest impact factors, above 18. However, there is an obvious correction to bring to citation counts: recent papers are less cited than old papers. Because I am now a senior scientist, I tend to publish a bit more in papers of high impact factors. Examples are papers reporting the results of large collaborations and invited reviews. So we need to correct for paper age by dividing the counts with the number of year elapsed since their publication.\n\nIndeed, the correlation is clearer. But there is still a lot of noise. I would not say that choosing a higher impact factor is a foolproof way to getting more citations. And I would certainly not say that a paper in a high impact factor journal has necessarily a big impact!\n\nLet’s now turn to the Relative citation ratio. How does it compare to the Impact factor?\n\nWell, the correlation is quasi-identical to the one with the average citations per year. Which of course leads us to the main comparison, which is between the RCR and the citation counts.\n\nThe correlation is much better. The outlier with 37 citations and an RCR of 0 is actually an artifact of Google Scholar. Of course, the RCR offers more than just an improved citation count. For instance, it also compares a paper’s impact to the impact of all papers reporting research funded by the NIH. A problem of the current tool though, is that its citation data comes from the Web of Science databases. Those databases do not contain all the scientific journals. They do not record citations in books. And of course they are not open. The RCR is a neat tool, but considering the strong correlation with pure citations, at least in my case, I think just looking at the citation counts is actually a good easy to use proxy for impact.\n\nAll that focused on article per article impact. But would total citations be a good proxy to evaluate individual researchers? Continuing the navel gazing exercise, I extracted the data for people in my institute who set up a Google profile. I omitted the PhD students, because publication records and citations are too noisy. I divided the positions in department heads, tenure group leaders, tenure track group leaders (5 year positions, most often a first experience of group leader), senior research associates (indefinite contracts but not group leaders) and post-doctoral fellows.\n\nThe correlation between total citations and h-index is quite impressive. This is probably due to the fact that we do not have distortions due to anomalous papers (e.g. BLAST or Clustal in bioinformatics). The occasional highly cited papers (e.g. SBML in my case) are just averaged out. And what comes out clearly is that in the majority of cases, positions match publication impact. Are total citations or h-index the best predictor? We can plot the rank in both classifications.\n\nThe H-index seems to correlate a bit better with tenure, SRA and tenure track positions. The separation between tenure track and post-docs is more blurry because some post-docs are quite senior and have impressive CVs. But overall, the separations are quite clear. And so is the message. In my institute, there is little hope to become tenure track if you have less than 1000 citations and a single digit h-index. For tenure, the bar would be close to 3000 citations and h-index in the mid-tenth. When it comes to department heads we’re talking 10000 citations and an h-index of 50.\n\nNow all that is of course very focused on my field of research. Molecular, cellular and systems biology is a very peculiar community. The publication habits, the criteria of excellence, everything is very homogeneous, almost military. It is also a fairly inward looking community. Not only there are very little contacts with other sciences, but there are very little contacts with the other components of life sciences as well. A fair amount of its members are actually convinced all scientists in all fields are thinking and acting alike. They would be surprised, and dismayed, to witness what I once saw in a conference: German computer science students impersonating us, exchanging pompous sentences about journal articles, impact factors and citations. They had the time of their life. Very humbling.\n\nAll that to say that everything in this blog post should be taken with more than a grain of salt.\n\nOver the past decade or so, I have been part of quite a few grant panels, for national and international funding agencies. Each time, the funding agency felt compelled to reinvent all the procedures from scratch, and ignore whatever experience has been gained from thousands of such exercises in the past. The reason is always to be more efficient, and serve science by selecting the best projects, with the highest likelihood of impact. Invariably, the system put in place achieves the exact opposite.\n\nOne aspect in particular severely impacts the resulting outcome: the success rate. The success rate varies widely from one funding scheme to another. The most highly sought after funding sources, such as grants from the Human Frontier Science Project, barely reach a few % of success. For such competitive schemes, multi-stage selection systems are often put in place, with only a fraction of the projects going from one stage to the next. Now the interesting – and slightly depressing – facts are:\n\nthe fraction of projects moving from one stage to another seems completely random, and disconnected from the final success rate;\n\nthe length of the documentation required in the application is disconnected from the number of stages and success rate;\n\nthe number of panel members looking at the documentation also varies seemingly in a random fashion, and is certainly not related to the number and size of proposals.\n\nOne of the worst examples I saw of that situation was the selection of Horizon 2020 collaborative projects in 2015, where a first step selected 30% of the projects, and a second step selected 5% of these. Such fractions were not only wrong because not equal, with an increased selection pressure, but they were actually the wrong way around, with 70% of scientists writing small documents without success for the first stage and 95% of scientists writing very long applications without success for the second stage. An even worse example is the advanced ERC grants, where all applicants are asked a short and long project description. But the panel select the projects during step 1 using only the short description! Since only 1/3 of projects are sent for external reviews, 2/3 of the applicants wrote a long application that will never be read by anyone!!!\n\nWhat are the consequences:\n\nfrustration and dispiriting of scientists, that compounded the lack of research done while they were working on the grant applications;\n\nincreased workload on panel members, who had to read and evaluate a lot of documentation, 95% for nothing;\n\nenormous waste of taxpayers money on both sides of the fence;\n\nfunded projects that are almost certainly not the best.\n\nWaste of money\n\nSo, what does such a process cost? Let’s look at the panel side. Evaluating a 3-6 pages document, that outlines a project, takes maybe one hour per project. Let’s assume a project was read by two panel members. The H2020 call I was talking about above had 355 applications, of which 108 were selected for the second stage, 5 of them being funded. So, we are talking 710 hours of reading for the first stage. To which we need to add the panel meeting. We’ll assume a panel of 10 members, meeting once for 2 days (8 hours a day) plus travel (~10 h return). So 260 more hours. Total is 970 hours. This represents GBP 48500 (I took a very average salary for a PI, costing their institution GBP 50 per hour). To which we need to add travelling, accommodation and catering costs, about 5000 (again super conservative). Of these 53500, 35700 are wasted on failed applications.\n\nA complete application of 50-100 pages would require half a day (4 hours), hence 864 hours of reading for the lot, plus the panel meeting. Total is 1124, that is GBP 56200 plus 5000 of meeting. But … hold on, I forgot someone! For this second stage, the opinion of external experts will be sought. Now, I am not going to overestimate their amount of work. They are assumed to spend half a day on each proposal. But I will only count 2 hours. And each proposal is evaluated by 3 experts. So total is 93600, of which 89300 are wasted on failed applications.\n\nBut those are the costs on the panel side, the ones directly supported by some funding agencies (most do not fund reviewers’ time and some do not support panel members’ time).\n\nNow, on the applicant side, the one superbly ignored by the funding agencies … For the first stage, I will assume 10 people are involved, spending a day for most and a week for 2 of them (coordinator, grant officer). They also have a meeting to which 7 people travel and spend a night. The total spent for the 355 projects is 4 millions of which 2.8 millions are spent for failed applications. for the second stage, more people are involved, spending more time, let’s say 12 people spending a week on the project, and 3 spending 3 weeks. 10 people travel to a preparatory meeting. We are talking of a total expense of 9.2 millions, of which 5 are wasted on failed applications. The funding bodies could not care less about this money. They do not pay for this side of the process. The institution of the applicants (and therefore other funding bodies) do so.\n\nAdding panel and applicant spending, 9.4 MILLIONS pounds of taxpayers money have been spent on failed applications! Now the interesting fact is that this particular call had a total budget of 30 millions Euros, that is a bit more than 20 millions GPB. In other words, to distribute 2 of their pounds, the taxpayer spent another pound! ONE THIRD of this public money was spent without any scientific research being done.\n\nRandom selection of projects\n\nNow, that’s for the efficiency. Let’s move to the efficacy. Surely this very expensive process selected the best possible scientific projects? Being super selective means only the “crème de la crème” are selected? Not at all! This is misunderstanding how grant applications are selected.\n\n1) Within a panel, grant applications are distributed to a few of the panel members, sometimes called “introducing members”. This is generally (but not always) based on the expertise of those members, who can then evaluate the proposal and select suitable external reviewers. These introducing members have an enormous power. They are generally the only ones reading an application attentively enough to detect flaws. They are giving the initial score to a project, that will decide how it will be discussed in the panel meeting. Panel members have different habits to score projects. Some will provide a Gaussian distribution of scores. Some will only give highest scores to projects they want to discuss and lowest for the one they do not like. This will affect the global score, drawn from the combination of scores from various introducing members.\n\nIntroducing members are defending or destroying the application during the meeting. If the introducing member is negative, you’re doomed. If the introducing member is an expert in your field, you’re doomed. If the panel member is a shy individual, you’re doomed. If the panel member cannot be bothered or was depressed, you’re doomed. If the introducing panel is not an expert but saw an interesting talk in the domain a couple of weeks ago, you’re saved. If the panel member has a big voice, you’re saved. If the panel member is competitive and wants “his” projects to be funded so he beats the other panel members, you’re saved. So there is an enormous bias towards boasting, competitive, vocal introducing members.\n\nAs with every process in the universe, the noise (non-scientifically related component of the selection) increases in function of the square root of the signal (proportion of projects funded). If only a very few projects are selected among plenty, the effect of the introducing members on the whole selection will be proportionally bigger (although for any given project, it does not matter).\n\nVisual rendering of the selection of 30, 10, 3 and 1 % of proposals.\n\n2) Discussing a lot of projects during a panel meeting leads to temporal bias. We are more lenient at the beginning of the day, and more severe towards the end of the day. Not only do we get tired, nervous, dehydrated, we also tend to wield an axe rather than clippers to prune the good from the bad. While we find excuses and side interests to a lot of projects at the beginning of the day, the slight error or clumsy statement is damning when we reach tea time. Now, the more projects, the less likely it is that they will be discussed several times in a day,and therefore the more sensitive the process will be to the panel’s physiology.\n\n3) Recognising excellence from a grant application is not that easy. And the excellence of the projects is in general not linear. Many projects will be totally rubbish (oh come on! I am not the only one having been in a grant panel, have I?). But many will be excellent as well. With a few in between. Imagine a “sigmoid curve”. Selecting between the very best projects is very difficult. One needs more information to distinguish between close competitors (green box). While we do not need much to eliminate the hopeless ones (red box).\n\nSo, how do we fix this?\n\nA proposal: remember the rule of thirds\n\nThis idea is based on the way we actually rank proposals. Whatever selection I have to do among competitive pieces, I make three piles: NO, YES, MAYBE. The NO pile is made up of project I think should be rejected no matter what. The YES pile is made up of projects that I would be proud to have proposed. They’re excellent, and they should be funded. The MAYBE pile is … well maybe. We need more discussion, it depends on the funding etc. Because each project is read by several reviewers/panel members, there will be variation of scoring. But this noise should happen at the edge of the groups. One should then discuss the bottom of the YES pile, and the top of the MAYBE pile (see blue box on the excellence plot).\n\nSo, choosing which projects to fund should obey the rule of third: accept at least a third of them. If there is not enough money for at least 1/3, then a 2 stage process must be organised. If the money is too short to fund 1/9 of them, then a 3 stage process must be organised etc. At each stage, three equal piles are drawn, YES, NO, MAYBE.\n\nThe first stage should be strategic. For instance, each project is only described in a one page document. The panel chair and co-chair select within ALL proposals the ones that are suitable for the call. That way, since they see all proposals, they can balance topics, senior vs junior, gender etc. according to the policies of the funding bodies. This can be done very quickly, in a few days of intense work.\n\nThe second stage involves panel members. A project description must then include the science, track records etc. Each panel member has several projects, each project is evaluated by several members. Each member must have a significant share. That should be done fairly quickly since the descriptions are short, and no external opinion is sought.\n\nThe final stage involves external scientists. Only then does one require the full project descriptions.\n\nNote that the pile is the same height at each step: The less proposals, the longer the descriptions.\n\nHow does the progressive selection look?\n\nWhat are the costs for the EU call we used as example before?\n\nPanel side: The first step is done on 1 page. It involves 10 min by chair and co-chair. So, we are talking 119 hours of reading for the first stage. There is not panel meeting. The total expense is then ~5900.\n\nThe second stage is equivalent to the first stage previously. Evaluating a 3-6 pages document, that outlines a project, takes maybe one hour per project. Let’s assume a project was read by two panel members. On third of 355 projects are evaluated, that is 119. So, we are talking 238 hours of reading for the first stage. To which we need to add the panel meeting. We’ll assume a panel of 10 members, meeting once for 2 days (8 hours a day) plus travel (~10 h return). So 260 more hours. Total is 498 hours. This represents GBP 24900. To which we need to add travelling, accommodation and catering costs, about 5000 (again super conservative).\n\nThe complete application still requires half a day (4 hours). But we have only 40 of them hence 320 hours of reading plus the panel meeting. Total is 590, that is GBP 29500 plus 5000 of meeting. For this third stage, the opinion of external experts will be sought. As before, I assume they will spend 2 hours per proposal. And each proposal is evaluated by 3 experts. So total is 240 hours. Plus the panel meeting.\n\nTotal for the panel side is 5900+29900+4600. That is 81800, not a huge saving on the previous situation (still one year of PhD salary …).\n\nNow, on the applicant side, this is a completely different story. For the first stage, only one person is involved, the coordinator, spending 1 day. The total is therefore 142000 for the 355 projects. The second stage is now what was previously the first stage, except only 119 projects are involved. The total spent is 900600. The third stage is now like the second stage previously, except only 40 projects are evaluated. The total is 1920000.\n\nThe total for the applicant side is therefore 142000+900600+192000 = 3418600\n\nAdding panel and applicant spending, only 3500400 pounds of taxpayers money have been spent, a 2/3 economy!\n\nNow, should it have stopped here? No. This process was still not good, because only 12.5% of the projects have been selected during the last round. An even better process would have been to add yet another layer of selection. The second layer would have involved the panel members, but without meeting. The third layer (panel member and extended discussions during a meeting) would have selected 14 projects. The last exercise involving external reviewers, would have selected 5 amongst those. Only 42 reviewers would be needed (14*3) instead of a whooping 350 or so.\n\nThe process would perhaps be a bit longer (“perhaps”, because most of the time lost in those processes is NOT due to the evaluation, but to administrative treatment of applications and unnecessary delays between the different stages). But so much effort, money and anxiety saved! And so much more time for scientists to do research!\n\nUpdate January 2020: I just noticed a very different solution proposed on the Youtube channel Numberphile, designed to rank applications to use telescopes. Pretty clever, and perhaps cheaper!"
    }
}
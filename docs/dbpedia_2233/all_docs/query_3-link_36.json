{
    "id": "dbpedia_2233_3",
    "rank": 36,
    "data": {
        "url": "https://www.biorxiv.org/content/10.1101/2020.11.17.385203v1.full",
        "read_more_link": "",
        "language": "en",
        "title": "A Modular Workflow for Model Building, Analysis, and Parameter Estimation in Systems Biology and Neuroscience",
        "top_image": "https://www.biorxiv.org/sites/default/files/images/biorxiv_logo_homepage7-5-small.png",
        "meta_img": "https://www.biorxiv.org/sites/default/files/images/biorxiv_logo_homepage7-5-small.png",
        "images": [
            "https://www.biorxiv.org/sites/default/files/biorxiv_article.jpg",
            "https://www.biorxiv.org/sites/all/modules/contrib/panels_ajax_tab/images/loading.gif",
            "https://www.biorxiv.org/content/biorxiv/early/2020/11/18/2020.11.17.385203/F1.medium.gif",
            "https://www.biorxiv.org/content/biorxiv/early/2020/11/18/2020.11.17.385203/F1.medium.gif",
            "https://www.biorxiv.org/content/biorxiv/early/2020/11/18/2020.11.17.385203/F2.medium.gif",
            "https://www.biorxiv.org/content/biorxiv/early/2020/11/18/2020.11.17.385203/F2.medium.gif",
            "https://www.biorxiv.org/content/biorxiv/early/2020/11/18/2020.11.17.385203/F3.medium.gif",
            "https://www.biorxiv.org/content/biorxiv/early/2020/11/18/2020.11.17.385203/F3.medium.gif",
            "https://www.biorxiv.org/sites/default/files/highwire/biorxiv/early/2020/11/18/2020.11.17.385203/embed/graphic-6.gif",
            "https://www.biorxiv.org/sites/default/files/highwire/biorxiv/early/2020/11/18/2020.11.17.385203/embed/graphic-6.gif",
            "https://www.biorxiv.org/sites/default/files/highwire/biorxiv/early/2020/11/18/2020.11.17.385203/embed/inline-graphic-1.gif",
            "https://www.biorxiv.org/sites/default/files/highwire/biorxiv/early/2020/11/18/2020.11.17.385203/embed/inline-graphic-1.gif",
            "https://www.biorxiv.org/sites/default/files/highwire/biorxiv/early/2020/11/18/2020.11.17.385203/embed/inline-graphic-2.gif",
            "https://www.biorxiv.org/sites/default/files/highwire/biorxiv/early/2020/11/18/2020.11.17.385203/embed/inline-graphic-2.gif",
            "https://www.biorxiv.org/sites/default/files/highwire/biorxiv/early/2020/11/18/2020.11.17.385203/embed/inline-graphic-3.gif",
            "https://www.biorxiv.org/sites/default/files/highwire/biorxiv/early/2020/11/18/2020.11.17.385203/embed/inline-graphic-3.gif",
            "https://www.biorxiv.org/sites/default/files/highwire/biorxiv/early/2020/11/18/2020.11.17.385203/embed/inline-graphic-4.gif",
            "https://www.biorxiv.org/sites/default/files/highwire/biorxiv/early/2020/11/18/2020.11.17.385203/embed/inline-graphic-4.gif",
            "https://www.biorxiv.org/content/biorxiv/early/2020/11/18/2020.11.17.385203/F4.medium.gif",
            "https://www.biorxiv.org/content/biorxiv/early/2020/11/18/2020.11.17.385203/F4.medium.gif",
            "https://www.biorxiv.org/content/biorxiv/early/2020/11/18/2020.11.17.385203/F5.medium.gif",
            "https://www.biorxiv.org/content/biorxiv/early/2020/11/18/2020.11.17.385203/F5.medium.gif",
            "https://www.biorxiv.org/content/biorxiv/early/2020/11/18/2020.11.17.385203/F6.medium.gif",
            "https://www.biorxiv.org/content/biorxiv/early/2020/11/18/2020.11.17.385203/F6.medium.gif",
            "https://www.biorxiv.org/content/biorxiv/early/2020/11/18/2020.11.17.385203/F7.medium.gif",
            "https://www.biorxiv.org/content/biorxiv/early/2020/11/18/2020.11.17.385203/F7.medium.gif",
            "https://www.biorxiv.org/sites/all/modules/highwire/highwire/images/twitter.png",
            "https://www.biorxiv.org/sites/all/modules/highwire/highwire/images/fb-blue.png",
            "https://www.biorxiv.org/sites/all/modules/highwire/highwire/images/linkedin-32px.png",
            "https://www.biorxiv.org/sites/all/modules/highwire/highwire/images/mendeley.png"
        ],
        "movies": [],
        "keywords": [],
        "meta_keywords": [
            ""
        ],
        "tags": null,
        "authors": [
            "João P.G. Santos",
            "Kadri Pajo",
            "Daniel Trpevski",
            "Andrey Stepaniuk",
            "Olivia Eriksson",
            "Anu G. Nair",
            "Daniel Keller",
            "Jeanette Hellgren Kotaleski",
            "Andrei Kramer"
        ],
        "publish_date": "2020-11-17T00:00:00",
        "summary": "",
        "meta_description": "bioRxiv - the preprint server for biology, operated by Cold Spring Harbor Laboratory, a research and educational institution",
        "meta_lang": "en",
        "meta_favicon": "https://www.biorxiv.org/sites/default/files/images/favicon.ico",
        "meta_site_name": "bioRxiv",
        "canonical_link": "https://www.biorxiv.org/content/10.1101/2020.11.17.385203v1",
        "text": "Introduction\n\nComputational systems biology is a data-driven field concerned with building models of biological systems, most commonly the intracellular signaling cascades. Methods from systems biology have been proven valuable in neuroscience, particularly when studying the composition of synapses that can reveal the molecular mechanisms of plasticity, learning and various other neuronal processes (Bhalla and Iyengar 1999; Hellgren Kotaleski and Blackwell 2010; Li et al. 2012). A wide variety of different software and toolboxes, each with their own strengths and weaknesses, are available within the field. This diversity, however, can obstruct model reuse as interoperability between the different software packages and the convertibility between various file types is only solved in part. Interoperability can either mean that the model built in one simulator can be run in another or that both simulators interoperate at run-time either at the same or different scales (Cannon et al. 2014). The former is addressed by standardizing model descriptions, and in systems biology by the standard machine-readable model formats as XML-based SBML (Systems Biology Markup Language; Hucka et al. 2003) and CellML (Hedley et al. 2001), and human readable format as SBtab (Lubitz et al. 2016). An analogous model description language for neurons and networks is the NeuroML (Neural Open Markup Language; Gleeson et al. 2010).\n\nWe start by providing examples of the available systems biology tools. We then proceed to describe our approach in developing a modular workflow to address some of these interoperability issues and present simulation results of an example use case in various simulators and frameworks. Our workflow starts with a human-readable representation of the model that is easily accessible to everyone and proceeds through various conversions into different simulation environments: MATLAB, COPASI, NEURON, and STEPS. Specifically, we will describe the conversion tools we created for this purpose.\n\nExamples of Software and Toolboxes used in Systems Biology\n\nNo software package is perfectly suited for every task, some have programmable interfaces with scripting languages, like MATLAB’s SimBiology toolbox, some focus on providing a fixed array of functions that can be run via graphical user interfaces, like COPASI, although it now offers a Python toolbox for scripting (Welsh et al. 2018). Most toolboxes and software packages offer a mixture of the two approaches: fine-grained programmable interface as well as fixed high-level operations. The extremes of this spectrum are a powerful but inflexible high-level software on one side and a complex, harder to learn but very flexible library or toolbox with an API on the other. Some examples of general modeling toolboxes in MATLAB are the SBPOP/SBToolbox2 (Schmidt and Jirstrand 2006), and the PottersWheel Toolbox (Maiwald and Timmer 2008). Specifically, for Bayesian parameter estimation, there are the MCMCSTAT toolbox (Haario et al. 2006) in MATLAB, as well as pyABC (Klinger et al. 2018) and pyPESTO (Schälte et al. 2020) in Python, and the standalone Markov Chain Monte Carlo (MCMC) software GNU MCsim (Bois 2009). For simulations in neuroscience, examples are NEURON (Hines and Carnevale 1997) and STEPS (Hepburn et al. 2012). Both are used for simulations of neurons and can include reaction-diffusion systems and electro-physiology.\n\nThese software packages do not all use the same model definition formats. Most have some compatibility with SBML, others use their own formats (e.g. NEURON uses MOD files). Often an SBML file exported into one of these packages cannot be imported into another package without errors. More generally, SBML has to be translated into code that can be used in model simulations, i.e. the right-hand side of an ordinary differential equation. There are also several tools that facilitate the conversion between formats, e.g. the SBFC (The Systems Biology Format converter; Rodriguez et al. 2016) as well as the more general VFGEN (A Vector Field File Generator; Weckesser 2008).\n\nAll toolboxes and software packages have great strengths and short-comings, and each programming language has different sets of (freely) available libraries which makes the development (or use) of numerical methods more (or less) feasible than in another language. One such example is the R package CDvine for parameter dependency modeling using copulas and vines, which has recently been used for modeling parameter spaces in-between MCMC iterations (Eriksson et al. 2019). This package is not easily replaced in many other languages. The Julia language, however, has a far richer set of differential equation solvers than R or MATLAB. Each researcher must therefore make decisions that result in the best compromise for them. If a researcher is familiar with a given set of programming/scripting languages it is probably not reasonable to expect them to be able to collaborate with other groups in languages they do not know. For this reason, it is our firm opinion that the conversion of models between different formats is a very important task and it is equally important to use formats that people can pick-up easily. To make format conversion flexible intermediate files are a great benefit which leads to a modular approach with possible validation between modules.\n\nWe also have to consider the FAIR data principles - the findability, accessibility, interoperability and reusability of data and associated infrastructure (Wilkinson et al. 2016). Here, we would like to address the interoperability principle by having developed a workflow for building biochemical pathway models using existing tools and custom-made, short, freely available scripts for the storage and refinement of models in all phases of development, ensuring interchangeability with other formats and toolkits at every step in the pipeline with many standardized intermediate files (Fig. 1). To illustrate all the tools in the workflow we have used it to retune a part of a model previously developed in our research group to make it more compatible with various simulators and to create a concrete use case for others to reuse and modify.\n\nThe workflow\n\nWhile the standard model storage format in systems biology is SBML, it has a few drawbacks for the reasons mentioned above. Therefore, the workflow is centered around building an easy-to-use infrastructure with models and data expressed in a spreadsheet-based storage format called SBtab. We chose SBtab as the primary modeling source file because it is human-readable, it can contain both the model and the data, and because it is easy to write parsing scripts for it, such as a converter from SBtab to SBML using the libSBML1 interface in R. This ease of convertibility is used in another focus of the workflow, convertibility between SBtab and other common formats and simulation software, since in systems biology and in any other computational sciences, the lack of compatibility between different tools and formats can often pose problems. A partially working conversion tool between SBtab and SBML had already been developed by the SBtab team. However, it can currently only read one table at a time and does not produce any functional SBML files with our model example. To combat these shortcomings, we wrote scripts to convert the SBtab into SBML, either using the R language or MATLAB, and validating it successfully in COPASI.\n\nThe bulk of our workflow is available as MATLAB code, particularly the parameter estimation tools and functions for global sensitivity analysis (SA). SA can be used to determine the importance of different parameters in regulating different outputs. Local sensitivity analysis is based on partial derivatives and investigates the behavior of the output when parameters are perturbed in the close vicinity to a specific point in parameter space. Tools for local SA are already included in the MATLAB suite. Global SA (GSA), on the other hand, is based on statistical approaches and has a much broader range. GSA is more relevant for models that have a large uncertainty in their parameter estimates which is common for systems biology models where many of the parameters have not been precisely measured and the data are sparse.\n\nThe standard approach within biochemical modeling is to use deterministic simulations and ordinary differential equations (ODE) that follow the law of mass action as it is computationally efficient and provides good results for sufficiently large well-mixed biological systems. However, this approach has several restrictions in the case of neuronal biochemical cascades. First, such cascades are always subject to stochastic noise, which can be especially relevant in a compartment as small as a dendritic spine where the copy number of key molecules are small enough that the effect of randomness becomes significant (Bhalla 2004). For precise simulation of stochasticity in reaction networks several stochastic solvers are available, e.g. Gillespie’s Stochastic Simulation Algorithm (SSA) (Gillespie 1976) and explicit and implicit tau-leaping algorithms (Gillespie 2001). Second, the number of possible states of a biochemical cascade often grows exponentially with the number of simulated molecule types, such that it becomes difficult to represent all these states in the model. In this case, for efficient simulation the reactions in the model could be represented and simulated in a network free form using rule-based modeling approaches (Chylek et al. 2015). To tackle these problems, we developed the subcellular application, a web-based software component for model development. It allows the extension and validation of deterministic chemical reaction network-based models by simulating them with stochastic solvers for reaction-diffusion systems and network free solvers.\n\nAlthough the workflow is applicable to any biochemical pathway model our emphasis is on modeling biochemical signaling in neurons. Therefore, the last challenge we want to address is an important concept in the interoperability domain of computational neuroscience called multiscale modeling which concerns the integration of subcellular models into electrical models of single cells or microcircuits. This can be achieved either by run-time interoperability between two simulators of different systems or by expanding the capabilities of a single simulation platform as has been done with the NEURON software (McDougal et al. 2013). With this purpose in mind we have written a conversion function from SBtab to the MOD format which is used by NEURON. As such, the inputs and the outputs of a biochemical cascade can be linked to any of the biophysiological measures of the electrical neuron model.\n\nUse case\n\nAs a use case to illustrate the workflow we have chosen a previously developed pathway model of the emergence of eligibility trace observed in reinforcement learning in striatal direct pathway medium spiny neurons (MSN) that carry the D1 receptor (Nair et al. 2016). In this model, a synapse that receives excitatory input which leads to an increase in calcium concentration is potentiated only when the signal is followed by a reinforcing dopamine input. Fig. 2a represents a simplified model scheme illustrating these two signaling cascades, one starts with calcium as the input and the other one with dopamine. In simulation experiments the inputs are represented as a calcium train and a dopamine transient (Fig. 3a). Calcium input refers to a burst of 10 spikes at 10 Hz reaching 5 μM. Dopamine input is represented by a single transient of 1.5 μM. The first cascade (species in blue) features the calcium-dependent activation of Ca2+/calmodulin-dependent protein kinase II (CaMKII) and the subsequent phosphorylation of a generic CaMKII substrate which serves as a proxy for long term potentiation (LTP) and is the main output of the model. The second cascade (species in red) represents a G-protein dependent cascade following the dopamine input and resulting in the phosphorylation of the striatal dopamine- and cAMP-regulated phosphoprotein, 32 kDa (DARPP-32) that turns into an inhibitor of protein phosphatase 1 (PP1) which can dephosphorylate both CaMKII and its substrate. The phosphorylation of the substrate is maximal when two constraints are met. First, the time window between the calcium and dopamine inputs has to be short, corresponding to the input-interval constraint which is mediated by DARPP-32 via PP1 inhibition. Second, intracellular calcium elevation has to be followed by the dopamine input, corresponding to the input-order constraint that is mediated by another phosphoprotein, the cyclic AMP-regulated phosphoprotein, 21 kDa (ARPP-21), thanks to its ability to sequester calcium/calmodulin if dopamine arrives first (Fig. 3d).\n\nIn the originally published model, CaMKII is autophosphorylated in two compartments, both the cytosol and the post synaptic density (PSD), with a custom-written MATLAB rate function that was calculated based on the probability of two neighboring subunits being fully activated as described in Li et al. (2012). To make it possible to run the model in different software we replace the rate equation of autophosphorylation with a similar set of reactions in both compartments so that the model would only contain bimolecular reactions. The reactions represent a simplified version of the autophosphorylation reactions in Pepke et al. (2010), where in our case only the fully activated CaMKII can be phosphorylated. The same set of reactions is used in both compartments and the schematics is available in Fig. 2b along with the required six new parameters. We used our parameter estimation software to find and constrain parameters that preserved the behavior of the model. We used simulated data from the original model with different timings of the dopamine input relative to the calcium input (Fig. 2c) to obtain a comprehensive picture of its behavior which we want the updated model to reproduce.\n\nSBtab\n\nAs described above we have chosen the SBtab format for model and data storage. This format allows the storage of biochemical models and associated data in a single file and provides a set of syntax rules and conventions to structure data in a tabulated form making it easy to modify and share. To ensure interoperability, SBtab provides an online tool to convert the models into the SBML format. SBtab is suitable for storing data that comes in spreadsheet or table formats, e.g. concentration time series or dose response curves, and it also supports various annotations. The SBtab file is intended to be updated manually during the process of model building. Additional instructions on how to make SBtab files work well within our toolchain can be found in the Subcellular Workflow GitHub repository. Some of the columns and sheets that we use should be considered as extensions to the format. However, SBtab is easy to parse so adjustments to parsers can be made quickly.\n\nThe SBtab file should include separate sheets for compartments, compounds, reactions, assignment expressions, parameters, inputs, outputs, and experiments (as well as data tables). The use case model has 99 compounds, 138 reactions and 227 parameters. An example of the SBtab reaction table can be found in Table 1. An important thing to note here is that while most software tools use seconds as their default time units, the NEURON simulator which we use to incorporate the biochemical cascades into an electrical neuron model uses milliseconds instead. We found no easy way to convert the parameter units in SBtab into the file format used by NEURON and hence, the time unit we use in the SBtab sheets representing the model (but not the data) is millisecond.\n\nOne of our goals with this study was to reproduce the original model behavior after replacing a single module inside the model to convert the model to bimolecular reactions only. The data we used therefore represents the simulated time series (20 s) of the concentrations of four selected species in response to different input combinations using the original model. Each individual data sheet (named E0-E9, Fig. 2c) in SBtab represents the outputs of one experiment. Another sheet called Experiments allows to define the input parameters differently for each experimental setup. By setting the initial concentrations of the unused species to zero the data could be mapped to a specific sub-module of the model (the remaining species). In this case the initial conditions are the same for all experiments. The Experiments table can also support various annotations relevant to each dataset. We used nine different timings (corresponding to E0-E8) between the calcium and the dopamine signal starting with a dopamine signal preceding calcium by four seconds and finishing with dopamine following calcium after four seconds as this corresponds to the time frame originally used in model development (Δt={-4,-3,-2-1,0,1,2,3,4}). Additionally, we used simulations with calcium as the only input (E9) (Fig. 2). The time series of the input species are in a separate sheet following each experiment sheet. An example of how experimental data is stored can be found in Table 2.\n\nModel pre-processing tools\n\nModel building entails frequent changes to the model structure by adding new species, reactions, and parameters. This can result in the emergence or disappearance of Wegscheider cyclicity conditions that refer to the relationships between reaction rate coefficients arising from conditions of thermodynamic equilibrium (Wegscheider 1901; Vlad and Ross 2004). Identified thermodynamic constraints show parameter dependencies that follow from physical laws and can reduce the number of independent parameters. These conditions are frequently difficult to determine by human inspection, especially for large systems. Similarly, identifying conserved moieties, like conserved total concentration of a protein, allows the reduction of the ODE model size, which leads to increased performance. In order to address these model pre-processing needs our toolkit includes scripts in MATLAB/GNU Octave that use the stoichiometric matrix of the reaction network as an input to determine the thermodynamic constraints as described in Vlad and Ross (2004), and conservation laws. These diagnostic tools output any identified constraints that, if needed, are to be implemented manually before the parameter estimation step. It should be noted that such constraints need to be re-examined after each addition of new reactions as the structure of the model might change and make previously true constraints invalid. This is true for all major changes to the model.\n\nMATLAB tools\n\nThe bulk of our workflow is developed in MATLAB as it provides an easy-to-use biochemical modeling application with a graphical user interface called SimBiology along with a wide range of toolboxes for mathematical analysis. The workflow is divided into import, simulation, and analysis scripts. To ensure an easy and user-friendly usage, all operations are controlled by a single settings file where all specification options needing user input are represented as modifiable variables. An example settings file of the use case model along with instructive comments can be found in the GitHub repository.\n\nImport from SBtab to MATLAB\n\nThe scripts we have written first convert the SBtab into a .mat format, this file is then used to generate representations of the base model in three different formats: the MATLAB format, the SimBiology format and SBML (level 2 encoding), while also creating scripts coding the input and each experiment inside the newly created ‘Formulas’ and ‘Data’ folders, respectively. All these new files and folders are stored inside the main model folder.\n\nParameter estimation\n\nMATLAB offers a wide range of tools, such as various optimization algorithms, that can be utilized in model refinement. This allows e.g. the optimization for single point parameter estimation. For this step, we have also developed MATLAB code to automate the use of either of the listed methods with minimal user-defined input. Adjusted parameters can be manually updated in the SBtab or SimBiology model. Our scripts allow the use of Genetic Algorithm, Simulated Annealing, Pattern Search and Constrained Optimization for which MATLAB provides thorough documentation. The equation used to calculate the score for how well the model outputs fit the experimental data can be found below. In addition, we have incorporated a few other ways of calculating the score depending on the need (see documentation).\n\nHere, Y represents the simulation results from the original (validated) model and y are the simulations of the updated model under parameterization θ. The allowed mismatch τ between the two simulation results is analogous to the standard deviation of a Gaussian noise model in data fitting. The resulting F is the objective function for Particle Swarm optimization. The error is summed over n, the number of points in a given experimental output, m, the number of experimental outputs in an experiment which is four in our use case (see Fig. 3b), and l, the number of experiments (E0-E9 in this case) (see Fig. 2c).\n\nParameter estimation is generally based on experimental data. Here, however, we would like to demonstrate the workflow with a simple use case and therefore the data we used represents the simulated time series of the concentrations of several species using the original version of the model in SimBiology. After modifying the model, we minimized the difference between the old behavior and the updated model’s response through optimization. The simulation results from the old model can be considered as analogous to experimental data in a normal parameter estimation setting. Here we merely aim to make an updated model agree with its earlier iteration, which itself was adjusted based on experimental data. When changing a module in a model it is crucial to protect the unchanged parts which is why we performed parameter estimation using the key species that intersect the calcium and dopamine cascades, namely PP1, calmodulin and DARPP-32 (Fig. 2 and 3b). We performed an optimization with the Particle Swarm algorithm to parameterize the replaced module and the use of the simulated data of these four species proved to be sufficient for the particular task as adding more species did not improve the fits.\n\nValidation in Matlab\n\nParameter estimation resulted in a good fit for most of the species and the updated model was able to closely reproduce the results seen with the original model (Fig. 3c). The Subcellular Workflow includes the updated model in SBtab, SBML and MATLAB SimBiology along with a script that simulates the model in MATLAB and reproduces the traces of the updated model shown in Fig. 3c-d.\n\nGlobal sensitivity analysis\n\nIn many cases parameter estimation of biochemical pathway models does not result in one unique value for a parameter. Structural and practical unidentifiability (Raue et al. 2009) results in a large set of parameter values that all correspond to solutions with a good fit to the data, i.e. there is a large uncertainty in the parameter estimates (Eriksson et al. 2019). When this is the case local sensitivity analysis is not so informative, since this can be different depending on which point in parameter space it is performed at. A global sensitivity analysis (GSA), on the other hand, covers a larger range of the parameter space. Several methods for GSA exist (Zi 2011) but we have focused on a method by Sobol and Saltelli (Sobol 2001; Saltelli 2002; Saltelli 2004) as implemented by Halnes et al. (2009) which is based on the decomposition of variances (Saltelli 2004). Single parameters or subsets of parameters that have a large effect on the variance of the output get a high sensitivity score in this method. Intuitively, this method can be understood as varying all parameters but one (or a small subset) at the same time within a multivariate distribution to determine what effect this has on the output variance. If there is a large reduction in the variance, the parameter that was kept fixed is important for this output (Saltelli 2004).\n\nLet the vector Θ denote the parameters of the model, and y=f(Θ) be a scalar output from the model. In the sensitivity analysis Θ are stochastic variables, sampled from a multivariate distribution, whose variation gives a corresponding uncertainty of the output, quantified by the variance V(Y). In this setting the different Θi are assumed to be independent from each other. We consider two types of sensitivity indices, the first order effects Si and the total order effects STi. The first order effects describe how the uncertainty in the output depends on the parameter Θi, i.e. how much of the variance of the output can be explained by the parameter Θi alone. As an example, Si=0.1 means that 10% of the output variance can be explained by Θi alone. The total order effects give an indication on the interactive effect the parameter Θi has with the rest of the parameters on the output. Parameters are said to interact when their effect on the output cannot be expressed as a sum of their single effects on the output.\n\nThe first order sensitivity index of the parameter Θi is defined as , where Θ−i corresponds to all elements of Θ except Θi, and E(…|…) denotes conditional expected value. For first order sensitivity indices and for additive models this is an equality (all assuming that the distributions of the different Θi are independent from each other). The total order effects of the parameter Θi corresponds to , the sum of all STi is always larger than or equal to one . If there is a large difference between Si and STi this is an indication that this parameter takes part in interactions. For a detailed description see e.g. chapter 5 of Saltelli (2004).\n\nThe optimization described earlier takes place on log transformed parameter values (log10(Θ)) and for the sensitivity analysis we perform the sampling on a lognormal distribution, meaning that log10(Θ)~ N(μ, σ). Below we use μ =log10(Θ*) and sigma=0.1, where Θ* correspond to the optimal values received from the optimization. We have also implemented some alternative distributions that can be used. We illustrate this method using only the six parameters corresponding to the model module that has been replaced (Fig. 2) and the results can be seen in Fig. 4. Only four of the parameters (k216-k219/k222-k225; Fig. 4) seem to be important for the output within the investigated parameter region. In experiments 1 and 2 these four parameters are shown to have equal importance, whereas in experiments 4-6 the parameter k219/k225 has the largest influence. Also, there seem to be some interactive effects between the parameters as shown by STi analysis (Fig. 4b).\n\nCompatibility and validation with other simulation environments\n\nConversion to SBML and simulations in COPASI\n\nCOPASI is one of the more commonly used modeling environments in systems biology and it can read SBML files (Hoops et al. 2006). Our first validation step is to use the SBML model retrieved from the SBtab model in COPASI. As the online conversion tool from SBtab to SBML turned out difficult to understand we wrote a new conversion function that can be found in the GitHub repository of the paper. It interprets the biological model and converts it into plain ODEs in VFGEN’s custom format (.vf), the VFGEN file can then be used to create output in various languages2 (Weckesser 2008). The conversion script (written in R) converts the SBtab saved as a series of .tsv files or one .ods file into a VFGEN vector field file and as by-products also the SBML and a MOD file (see chapter Conversion to a MOD file and simulations in NEURON). To create an SBML model libsbml has to be installed with R bindings. The SBML file can be imported directly into COPASI, although it might be necessary to remove the superfluous unit definitions manually.\n\nAnother way to convert the model into SBML is through a single MATLAB SimBiology function. The models, however, are created with long ID’s that carry no biological information (the ID’s are similar to hexadecimal hashes) for all model components, the units are not properly recognized, and some units may just be incorrectly defined in the output. We, therefore, created a script in R that asks for default units for the model and replaces the ones in the SBML file. It fixes most issues (units, ID’s, and the time variable in assignments) allowing the model to be properly imported in3 COPASI. To illustrate that the SBML-converted model imported into COPASI produces the same results as in SimBiology, we used a simplified calcium input corresponding to one double exponential spike analogous to the dopamine transient and simulated the model with deterministic solvers in both COPASI (LSODA solver) and MATLAB SimBiology under similar conditions. Both simulation environments produced almost overlapping results (Fig. 5b-c) validating the converted model in the SBML format.\n\nSimulations in STEPS\n\nThe web-based subcellular application4 allows importing, combining and simulating models expressed in the BioNetGen language (BNGL; Harris et al. 2016). It supports the import of SBML (level 2) models and their transformation to rule-based BNGL form using Atomizer (Tapia and Faeder 2013). The BioNetGen file format was extended to provide diffusion parameters, links to tetrahedral meshes describing the geometry of model compartments, as well as the additional parameters for solvers and stimulation protocols required for spatially-distributed models. The subcellular application is integrated with the network free solver NFsim (Sneddon et al. 2011) and it supports simulations of spatially distributed systems using STEPS (Hepburn et al. 2012). STEPS provides spatial stochastic and deterministic solvers for simulations of reactions and diffusion on tetrahedral meshes. Furthermore, the subcellular application provides a number of facilities for the visualization of models’ geometries and the results of simulations.\n\nTo demonstrate the compatibility of the subcellular application with the workflow for model development described above, we imported the SBML version of the use case model to the subcellular application and simulated it with the STEPS TetOpSplit solver. We have used a simple two-compartmental spine model with a tetrahedral compartment corresponding to the spine compartment of the use case model. There is also a PSD compartment on one of the faces. The results of the model simulations with a STEPS solver were qualitatively similar to the results obtained with the deterministic model simulated in MATLAB. Examples of simulated time courses for molecule concentrations as shown in Fig. 3 in comparison with corresponding MATLAB curves are shown in Fig. 6.\n\nConversion to a MOD file and simulations in NEURON\n\nAs we suggested before, conversion between different modeling frameworks and formats facilitates collaboration. But conversion is made harder by the differences in the capabilities of different modeling packages. A model, such as the one above, could be useful for multiscale simulations investigating how network activity shapes synaptic plasticity. A large number of cellular level models are built and simulated in the NEURON environment which also supports simplified reaction-diffusion systems. It would thus be useful to be able to integrate a subcellular level model into a cellular level model specified using NEURON. Models in NEURON are built by adding features with MOD files that are written in the NMODL programming language which has very little documentation. While conversion from SBML to MOD is already possible via NeuroML (instructions can be found in Lindroos et al. 2018) it is not an automated or user-friendly approach and therefore we created a new conversion tool from SBtab to MOD which is the same one that creates an SBML file. As one of the options it is also possible to generate a MOD file with or without state variable substitution by observing conservation laws. An example on how to use the SBtab to VFGEN/MOD/SBML converter in R can be found in the GitHub repository. The MOD file is meant to be a starting point for the modeler as the limited subcellular model in SBtab form is not aware of its coupling to a larger model of the cell. The user must edit the resulting file manually to use it within a larger scope and assign a role to this model component.\n\nWe validate the biochemical cascade model and our conversion tools in NEURON (see Information Sharing Statement) by qualitatively reproducing the results obtained in MATLAB SimBiology. Our goal is to show that the cascade model can be integrated into a single neuron model and bridge spatial and temporal scales of system behavior by linking the output of the cascade to changes in the synaptic properties and ultimately to the electrical behavior of the neuron model. Therefore, the biochemical model in the MOD format was incorporated into a single biophysically detailed and compartmentalized D1 MSN model from Lindroos et al. (2018). To integrate the MOD file into single cell models a few user-specific modifications have to be made to interface this model with the larger electrochemical system. First, the input is modified so that the calcium burst is represented by calcium influx from the cell’s calcium channels and adjusted so that the overall calcium level would be similar to the input used in MATLAB simulations. The dopamine transient is represented by an assignment expression (available in the Expression table of the SBtab) that creates its double exponential form (this can be used in other languages as well). When simulating the model with the same input timing combinations as in the MATLAB experiments, we were able to qualitatively reproduce the substrate phosphorylation curve that illustrates both the input-order and input-interval constraints seen in Fig. 3d. Minor differences can be explained by differences in the calcium input and the solver as MATLAB simulations were performed with ode15s while NEURON uses cnexp.\n\nIn the D1 MSNs this biochemical signaling cascade causes synaptic strengthening via several mechanisms, one of which is the phosphorylation of AMPA receptors. As mentioned above, the model instead includes a generic substrate whose level of phosphorylation is the output of the cascade. For the purpose of illustrating the workflow with a proof-of-concept example, we have here linked the fraction of phosphorylated substrate to the AMPA receptor conductance (the conductance is scaled by (1 + fraction of the phosphorylated substrate), i.e. when there is very little substrate phosphorylation, very little change in the AMPA conductance is elicited, and vice versa). Modifying the model to include the reactions for AMPA receptor phosphorylation will be made in a future study.\n\nDiscussion\n\nIn order to address the growing need for interoperability in biochemical pathway modeling within the neuroscience field, we have developed a workflow that can be used to refine models in all phases of development, keeping in mind the fact that many of the users (including us) are natural scientists and not professional programmers. For the model and data storage we have chosen the SBtab format which can be easily read and modified by both modelers and experimentalists, and can be converted into other formats, e.g. SBML, MATLAB SimBiology or MOD. Our workflow is modularized into different steps allowing the use of each step depending on the need and ensuring interoperability with other tools, such as those described in a similar endeavor named FindSim (Viswan et al. 2018). There are distinct advantages to the workflow, by enforcing a common standard for information exchange, it inherently makes the models more generalizable and reduces the likelihood that simulation results are artifacts of a particular simulator, and nonetheless, it gives users the flexibility to leverage the strengths of each different simulation environment and provides distinct stages of processing that would not be possible in any single simulator. The presented workflow aims to use software components that are free (apart from MATLAB) and solve incremental sub-tasks within the workflow (with open standard intermediate files) to make the workflow easy to branch into scenarios we have not previously considered. It is also possible to circumvent MATLAB entirely, if desired (e.g. conversion from SBtab to a MOD file that is then used by NEURON).\n\nWhen deciding which software packages to use we find that an important aspect that must be considered is the cost and licensing. For some researchers price may be a relevant concern, in other cases a researcher may have to undergo considerable overhead to make their institution/lab purchase a license and possibly operate a license server. Other than MATLAB, we made the choice to disregard commercial products, keeping in line with the field’s trend towards open source platforms. We will also expand our tools to support the use of more complex geometries, with several compartments (which is also an SBML feature), and tetrahedral meshes that can be used with STEPS in the subcellular application. Such advanced geometries can in principle be defined within SBtab tables. When it comes to multiscale simulations, there is the possibility of using the Reaction-Diffusion module (RXD) in NEURON. Currently, however, it does not support the import of SBML as it lacks the concept of spatially extended models, but SBML support might be added to the future versions of RXD (McDougal et al. 2013).\n\nAnother interesting consideration is whether the user wants to define any model directly using rules (as in rule-based modeling). This would make the use of Atomizer unnecessary. The transformation from rules to classical reactions seems easier than the reverse, so even if a rule-based simulation is not necessary or too slow, a rule-based description may be shorter and more fundamental in terms of model translation.\n\nWhen it comes to model analysis, we have here implemented a functionality for global sensitivity analysis. This is a thorough, but, computationally demanding approach and the model needs to be run in parallel on a supercomputer. There are also faster but more approximate screening methods that could have been used (Saltelli 2004). In the future we also intend to incorporate uncertainty quantification into the workflow (Eriksson et al. 2019).\n\nIn summary, we have here presented a workflow for biochemical pathway modeling and provided a concrete use case of reward dependent synaptic plasticity. Multiscale models are crucial when trying to understand the brain using modeling and simulations, e.g. how network activity shapes synaptic plasticity or how neuromodulation might affect cellular excitability on sub second timescales. Structured approaches for bridging from detailed cellular level neuron models, to more simplified or abstract cellular-, network-, and even brain region models are developing (Amsalem et al. 2020; Carlu et al. 2020; Schmutz et al. 2020). In the currently illustrated workflow, we add to these efforts by bridging from the subcellular scale to the cellular level scale. We updated parts of the use case model to accomplish a model with only bimolecular reactions that are easier to represent in standards such as SBML. The idea is that users can look at our model as a concrete test case, rerun the workflow (or parts thereof) and then replace the current example model with their own models. In this particular use case, we specifically focused on creating scripts to achieve interoperability between human readable model specification standards and machine-readable standards, and, we also wanted to facilitate how a subcellular signaling model could be implemented in different solvers with different strengths, in this case both SimBiology in MATLAB, as well as STEPS and NEURON. NEURON is currently the most commonly used simulation software for detailed cellular level neuron models, and STEPS can, as said, simulate signaling cascades in arbitrary dendritic morphologies both in a deterministic and stochastic manner. MATLAB on the other hand has a large number of functions, for example for parameter estimation and we included an implementation for global sensitivity analysis. Several other software is, however, used within the computational neuroscience community for cellular or subcellular model simulations (Ray and Bhalla 2008; Oliveira et al. 2010; Resasco et al. 2012; Akar et al. 2019). To successively make as many of those tools interoperable with standards for both model and data specification, various parameter estimation and model analysis methods, visualization software, etc., will further facilitate the creation of FAIR multiscale modeling pipelines in the future."
    }
}